<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jimmy Song – eBPF 数据路径</title>
    <link>https://jimmysong.io/book/cilium-handbook/ebpf/</link>
    <description>Recent content in eBPF 数据路径 on Jimmy Song</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Fri, 17 Jun 2022 12:00:00 +0800</lastBuildDate>
    
	  <atom:link href="https://jimmysong.io/book/cilium-handbook/ebpf/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>eBPF 数据路径介绍</title>
      <link>https://jimmysong.io/book/cilium-handbook/ebpf/intro/</link>
      <pubDate>Fri, 17 Jun 2022 12:00:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/book/cilium-handbook/ebpf/intro/</guid>
      <description>
        
        
        &lt;p&gt;Linux 内核在网络堆栈中支持一组 BPF 钩子（hook），可用于运行 BPF 程序。Cilium 数据路径使用这些钩子来加载 BPF 程序，这些程序一起使用时会创建更高级别的网络结构。&lt;/p&gt;
&lt;p&gt;以下是 Cilium 使用的钩子列表和简要说明。有关每个钩子细节的更详尽的文档，请参阅 &lt;a href=&#34;https://docs.cilium.io/en/stable/bpf/#bpf-guide&#34; title=&#34;BPF 和 XDP 参考指南&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BPF 和 XDP 参考指南&lt;/a&gt;
。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;XDP&lt;/strong&gt;：XDP BPF 钩子位于网络驱动程序中的最早可能点，并在数据包接收时触发 BPF 程序的运行。这实现了可能的最佳数据包处理性能，因为程序在任何其他处理发生之前直接在数据包数据上运行。此钩子非常适合运行丢弃恶意或意外流量的过滤程序以及其他常见的 DDOS 保护机制。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;流量控制入口/出口&lt;/strong&gt;：附加到流量控制（traffic control，简称 TC）入口钩子的 BPF 程序附加到网络接口，与 XDP 相同，但将在网络堆栈完成数据包的初始处理后运行。该钩子在三层网络之前运行，但可以访问与数据包关联的大部分元数据。这非常适合进行本地节点处理，例如应用三层/四层端点策略并将流量重定向到端点。对于面向网络的设备，TC 入口钩子可以与上面的 XDP 钩子耦合。完成此操作后，可以合理地假设此时的大部分流量是合法的并以主机为目的地。&lt;/p&gt;
&lt;p&gt;容器通常使用称为 veth 对的虚拟设备，它充当将容器连接到主机的虚拟路由。通过附加到这个 veth 对的主机端的 TC 入口钩子，Cilium 可以监控和执行所有离开容器的流量的策略。通过将 BPF 程序附加到与每个容器关联的 veth 对，并将所有网络流量路由到主机端虚拟设备，同时将另一个 BPF 程序附加到 TC 入口钩子，Cilium 可以监控所有进入或离开节点的流量并执行策略。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;套接字操作&lt;/strong&gt;：套接字操作钩子附加到特定的 cgroup 并在 TCP 事件上运行。Cilium 将 BPF 套接字操作程序附加到根 cgroup 并使用它来监视 TCP 状态转换，特别是 ESTABLISHED 状态转换。如果 TCP 套接字具有节点本地对等节点（可能是本地代理），则当套接字转换为 ESTABLISHED 状态时，附加套接字发送 / 接收程序。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;套接字发送/接收&lt;/strong&gt;：套接字发送/接收钩子（socket send/recv hook）在 TCP 套接字执行的每个发送操作上运行。此时，钩子可以检查消息并丢弃消息、将消息发送到 TCP 层或将消息重定向到另一个套接字。Cilium 使用它来加速数据路径重定向，如下所述。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;将上述钩子与虚拟接口（&lt;code&gt;cilium_host&lt;/code&gt;、&lt;code&gt;cilium_net&lt;/code&gt;）、可选的 overlay 接口（&lt;code&gt;cilium_vxlan&lt;/code&gt;）、Linux 内核加密支持和用户空间代理（Envoy）相结合，Cilium 创建了以下网络对象。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;前置过滤器（Prefilter）&lt;/strong&gt;：前置过滤器对象运行一个 XDP 程序并提供一组前置过滤器规则，用于过滤来自网络的流量以获得最佳性能。具体来说，数据包要么被丢弃，例如当目标不是有效的端点时，要么被堆栈处理。这可以根据需要轻松扩展以构建新的前置过滤器标准/功能。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;端点策略&lt;/strong&gt;：端点策略对象实现 Cilium 端点强制。使用映射来查找与身份和策略相关的数据包，该层可以很好地扩展到许多端点。根据策略，该层可能会丢弃数据包、转发到本地端点、转发到服务对象或转发到七层策略对象以获取进一步的七层规则。这是 Cilium 数据路径中的主要对象，负责将数据包映射到身份并执行三层和四层策略。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;服务&lt;/strong&gt;：服务对象对对象接收的每个数据包的目标 IP 和可选的目标端口执行映射查找。如果找到匹配条目，数据包将被转发到配置的三层/四层端点之一。Service 块可用于使用 TC 入口钩子在任何接口上实现独立的负载均衡器，或者可以集成到端点策略对象中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;三层加密&lt;/strong&gt;：在入口时，三层加密对象标记要解密的数据包，将数据包传递给 Linux xfrm（转换）层进行解密，数据包解密后对象接收数据包，然后将其向上传递到堆栈以供其他人进一步处理对象。根据模式、直接路由或覆盖，这可能是 BPF 尾调用或将数据包传递给下一个对象的 Linux 路由堆栈。解密所需的密钥在 IPsec 标头中编码，因此在入口处我们不需要进行映射查找来查找解密密钥。&lt;/p&gt;
&lt;p&gt;在出口处，首先使用目标 IP 执行映射查找以确定数据包是否应加密，如果加密，则目标节点上可用的密钥。选择两个节点上可用的最新密钥，并将数据包标记为加密。然后将数据包传递到对其进行加密的 Linux xfrm 层。在接收到现在加密的数据包后，通过将其发送到 Linux 堆栈进行路由或在使用覆盖时进行直接尾调用，将其传递到下一层。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;套接字层强制&lt;/strong&gt;：套接字层强制使用两个钩子，套接字操作钩子和套接字发送/接收钩子来监视和附加到与 Cilium 托管端点关联的所有 TCP 套接字，包括任何 七层代理。套接字操作钩子将识别用于加速的候选套接字。这些包括所有本地节点连接（端点到端点）和任何到 Cilium 代理的连接。然后，这些已识别的连接将由套接字发送/接收钩子处理所有消息，并将使用 &lt;code&gt;sockmap&lt;/code&gt; 快速重定向加速。快速重定向确保 Cilium 中实现的所有策略对关联的套接字 / 端点映射有效，并假设它们直接将消息发送到对等套接字。这是允许的，因为 &lt;code&gt;sockmap&lt;/code&gt; &lt;code&gt;send/recv&lt;/code&gt; 钩子确保消息不需要由上述任何对象处理。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;七层策略&lt;/strong&gt;：七层策略对象将代理流量重定向到 Cilium 用户空间代理实例。Cilium 使用 Envoy 实例作为其用户空间代理。然后，Envoy 将根据配置的七层策略转发流量或生成适当的拒绝消息。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些组件相互连接，以创建 Cilium 使用的灵活高效的数据路径。下面我们展示了连接单个节点上的端点、入口到端点以及端点到出口网络设备的以下可能流程。在每种情况下，都有一个附加图表显示启用套接字层强制时可用的 TCP 加速路径。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>数据包流程</title>
      <link>https://jimmysong.io/book/cilium-handbook/ebpf/lifeofapacket/</link>
      <pubDate>Fri, 17 Jun 2022 12:00:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/book/cilium-handbook/ebpf/lifeofapacket/</guid>
      <description>
        
        
        &lt;h2 id=&#34;端点到端点&#34;&gt;端点到端点&lt;/h2&gt;
&lt;p&gt;首先，我们使用可选的七层出口和入口策略显示本地端点到端点的流程。随后是启用了套接字层强制的同一端点到端点流。为 TCP 流量启用套接字层实施后，启动连接的握手将遍历端点策略对象，直到 TCP 状态为 ESTABLISHED。然后在建立连接后，只需要七层策略对象。&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
    
      
        
          &lt;img src=&#34;https://jimmysong.io/book/cilium-handbook/ebpf/lifeofapacket/cilium_bpf_endpoint.svg&#34; data-img=&#34;/book/cilium-handbook/ebpf/lifeofapacket/cilium_bpf_endpoint.svg&#34; alt=&#34;image&#34; data-caption=&#34;&#34;&gt;
        
      
    
  
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;端点到出口&#34;&gt;端点到出口&lt;/h2&gt;
&lt;p&gt;接下来，我们使用可选的 overlay 网络显示本地端点到出口。在可选的覆盖网络中，网络流量被转发到与 overlay 网络对应的 Linux 网络接口。在默认情况下，overlay 接口名为 &lt;code&gt;cilium_vxlan&lt;/code&gt;。与上面类似，当启用套接字层强制并使用七层代理时，我们可以避免在端点和 TCP 流量的七层策略之间运行端点策略块。如果启用，可选的 L3 加密块将加密数据包。&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
    
      
        
          &lt;img src=&#34;https://jimmysong.io/book/cilium-handbook/ebpf/lifeofapacket/cilium_bpf_egress.svg&#34; data-img=&#34;/book/cilium-handbook/ebpf/lifeofapacket/cilium_bpf_egress.svg&#34; alt=&#34;image&#34; data-caption=&#34;端点到出口的流程&#34;&gt;
        
      
    
  
  &lt;figcaption&gt;端点到出口的流程&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;入口到端点&#34;&gt;入口到端点&lt;/h2&gt;
&lt;p&gt;最后，我们还使用可选的 overlay 网络显示到本地端点的入口。与上述套接字层强制类似，可用于避免代理和端点套接字之间的一组策略遍历。如果数据包在接收时被加密，则首先将其解密，然后通过正常流程进行处理。&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
    
      
        
          &lt;img src=&#34;https://jimmysong.io/book/cilium-handbook/ebpf/lifeofapacket/cilium_bpf_ingress.svg&#34; data-img=&#34;/book/cilium-handbook/ebpf/lifeofapacket/cilium_bpf_ingress.svg&#34; alt=&#34;image&#34; data-caption=&#34;入口到端点流程&#34;&gt;
        
      
    
  
  &lt;figcaption&gt;入口到端点流程&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;这就是数据包路径概述。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>eBPF Map</title>
      <link>https://jimmysong.io/book/cilium-handbook/ebpf/maps/</link>
      <pubDate>Fri, 17 Jun 2022 12:00:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/book/cilium-handbook/ebpf/maps/</guid>
      <description>
        
        
        &lt;p&gt;所有 BPF Map 都是有使用容量上限的。超出限制的插入将失败，从而限制了数据路径的可扩展性。下表显示了映射的默认值。每个限制都可以在源代码中更改。如果需要，将根据要求添加配置选项。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Map 名称&lt;/th&gt;
&lt;th&gt;范围&lt;/th&gt;
&lt;th&gt;默认限制&lt;/th&gt;
&lt;th&gt;规模影响&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;连接跟踪&lt;/td&gt;
&lt;td&gt;节点或端点&lt;/td&gt;
&lt;td&gt;1M TCP/256k UDP&lt;/td&gt;
&lt;td&gt;最大 1M 并发 TCP 连接，最大 256k 预期 UDP 应答&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NAT&lt;/td&gt;
&lt;td&gt;节点&lt;/td&gt;
&lt;td&gt;512k&lt;/td&gt;
&lt;td&gt;最大 512k NAT 条目&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;邻居表&lt;/td&gt;
&lt;td&gt;节点&lt;/td&gt;
&lt;td&gt;512k&lt;/td&gt;
&lt;td&gt;最大 512k 邻居条目&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;端点&lt;/td&gt;
&lt;td&gt;节点&lt;/td&gt;
&lt;td&gt;64k&lt;/td&gt;
&lt;td&gt;每个节点最多 64k 个本地端点 + 主机 IP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IP 缓存&lt;/td&gt;
&lt;td&gt;节点&lt;/td&gt;
&lt;td&gt;512k&lt;/td&gt;
&lt;td&gt;最大 256k 端点（IPv4+IPv6），最大 512k 端点（IPv4 或 IPv6）跨所有集群&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;负载均衡器&lt;/td&gt;
&lt;td&gt;节点&lt;/td&gt;
&lt;td&gt;64k&lt;/td&gt;
&lt;td&gt;跨所有集群的所有服务的最大 64k 累积后端&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;策略&lt;/td&gt;
&lt;td&gt;端点&lt;/td&gt;
&lt;td&gt;16k&lt;/td&gt;
&lt;td&gt;特定端点的最大允许身份 + 端口 + 协议对 16k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;代理 Map&lt;/td&gt;
&lt;td&gt;节点&lt;/td&gt;
&lt;td&gt;512k&lt;/td&gt;
&lt;td&gt;最大 512k 并发重定向 TCP 连接到代理&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;隧道&lt;/td&gt;
&lt;td&gt;节点&lt;/td&gt;
&lt;td&gt;64k&lt;/td&gt;
&lt;td&gt;跨所有集群最多 32k 节点（IPv4+IPv6）或 64k 节点（IPv4 或 IPv6）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IPv4 分片&lt;/td&gt;
&lt;td&gt;节点&lt;/td&gt;
&lt;td&gt;8k&lt;/td&gt;
&lt;td&gt;节点上同时传输的最大 8k 个分段数据报&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;会话亲和性&lt;/td&gt;
&lt;td&gt;节点&lt;/td&gt;
&lt;td&gt;64k&lt;/td&gt;
&lt;td&gt;来自不同客户端的最大 64k 关联&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IP 掩码&lt;/td&gt;
&lt;td&gt;节点&lt;/td&gt;
&lt;td&gt;16k&lt;/td&gt;
&lt;td&gt;基于 BPF 的 ip-masq-agent 使用的最大 16k IPv4 cidrs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;服务源范围&lt;/td&gt;
&lt;td&gt;节点&lt;/td&gt;
&lt;td&gt;64k&lt;/td&gt;
&lt;td&gt;跨所有服务的最大 64k 累积 LB 源范围&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Egess 策略&lt;/td&gt;
&lt;td&gt;端点&lt;/td&gt;
&lt;td&gt;16k&lt;/td&gt;
&lt;td&gt;跨所有集群的所有目标 CIDR 的最大 16k 端点&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;对于某些 BPF 映射，可以使用命令行选项 &lt;code&gt;cilium-agent&lt;/code&gt; 覆盖容量上限。可以使用 &lt;code&gt;--bpf-lb-map-max&lt;/code&gt;、 &lt;code&gt;--bpf-ct-global-tcp-max&lt;/code&gt;、&lt;code&gt;--bpf-ct-global-any-max&lt;/code&gt;、 &lt;code&gt;--bpf-nat-global-max&lt;/code&gt;、&lt;code&gt;--bpf-neigh-global-max&lt;/code&gt;、&lt;code&gt;--bpf-policy-map-max&lt;/code&gt;和 &lt;code&gt;--bpf-fragments-map-max&lt;/code&gt; 来设置给定容量。&lt;/p&gt;



&lt;div class=&#34;alert&#34;&gt;
  
  &lt;div class=&#34;alert-note-title py-1 px-2&#34;&gt;
    提示
  &lt;/div&gt;
  
  &lt;div class=&#34;alert-note py-1 px-2&#34;&gt;
    如果指定了&lt;code&gt;--bpf-ct-global-tcp-max&lt;/code&gt;和 / 或&lt;code&gt;--bpf-ct-global-any-max&lt;/code&gt; ，则 NAT 表大小 ( &lt;code&gt;--bpf-nat-global-max&lt;/code&gt;) 不得超过组合 CT 表大小（TCP + UDP）的 2/3。&lt;code&gt;--bpf-nat-global-max&lt;/code&gt; 如果未显式设置或使用动态 BPF Map 大小（见下文），这将自动设置。
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;使用 &lt;code&gt;--bpf-map-dynamic-size-ratio&lt;/code&gt; 标志，几个大型 BPF Map 的容量上限在代理启动时根据给定的总系统内存比率确定。例如，给定的 0.0025 比率导致 0.25% 的总系统内存用于这些映射。&lt;/p&gt;
&lt;p&gt;此标志会影响以下消耗系统中大部分内存的 BPF Map： &lt;code&gt;cilium_ct_{4,6}_global&lt;/code&gt;、&lt;code&gt;cilium_ct_{4,6}_any&lt;/code&gt;、 &lt;code&gt;cilium_nodeport_neigh{4,6}&lt;/code&gt;、&lt;code&gt;cilium_snat_v{4,6}_external&lt;/code&gt; 和 &lt;code&gt;cilium_lb{4,6}_reverse_sk&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kube-proxy&lt;/code&gt;根据机器拥有的内核数设置为 linux 连接跟踪表中的最大条目数。无论机器有多少内核，&lt;code&gt;kube-proxy&lt;/code&gt; 默认每个内核的最大条目数是 32768 和最小条目数是 131072。&lt;/p&gt;
&lt;p&gt;Cilium 有自己的连接跟踪表作为 BPF Map，并且此类映射的条目数是根据节点中的总内存量计算的，无论机器有多少内存，条目最少数是 131072。&lt;/p&gt;
&lt;p&gt;下表介绍了当 Cilium 配置为 &lt;code&gt;-bpf-map-dynamic-size-ratio: 0.0025&lt;/code&gt; 时，&lt;code&gt;kube-proxy&lt;/code&gt; 和 Cilium 为自己的连接跟踪表设置的数值：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;虚拟 CPU&lt;/th&gt;
&lt;th&gt;内存 (GiB)&lt;/th&gt;
&lt;th&gt;Kube-proxy CT 条目&lt;/th&gt;
&lt;th&gt;Cilium CT 条目&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;3.75&lt;/td&gt;
&lt;td&gt;131072&lt;/td&gt;
&lt;td&gt;131072&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;7.5&lt;/td&gt;
&lt;td&gt;131072&lt;/td&gt;
&lt;td&gt;131072&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;131072&lt;/td&gt;
&lt;td&gt;131072&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;td&gt;262144&lt;/td&gt;
&lt;td&gt;284560&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;524288&lt;/td&gt;
&lt;td&gt;569120&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;120&lt;/td&gt;
&lt;td&gt;1048576&lt;/td&gt;
&lt;td&gt;1138240&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;240&lt;/td&gt;
&lt;td&gt;2097152&lt;/td&gt;
&lt;td&gt;2276480&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;96&lt;/td&gt;
&lt;td&gt;360&lt;/td&gt;
&lt;td&gt;3145728&lt;/td&gt;
&lt;td&gt;4552960&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>Iptables 用法</title>
      <link>https://jimmysong.io/book/cilium-handbook/ebpf/iptables/</link>
      <pubDate>Fri, 17 Jun 2022 12:00:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/book/cilium-handbook/ebpf/iptables/</guid>
      <description>
        
        
        &lt;p&gt;根据所使用的 Linux 内核版本，eBPF 数据路径可以完全在 eBPF 中实现不同的功能集。如果某些所需功能不可用，则使用旧版 iptables 实现提供该功能。有关详细信息，请参阅 &lt;a href=&#34;https://docs.cilium.io/en/stable/operations/system_requirements/#features-kernel-matrix&#34; title=&#34;IPsec 要求。&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IPsec 要求。&lt;/a&gt;
&lt;/p&gt;
&lt;h2 id=&#34;kube-proxy-互操作性&#34;&gt;kube-proxy 互操作性&lt;/h2&gt;
&lt;p&gt;下图显示了 &lt;code&gt;kube-proxy&lt;/code&gt; 安装的 iptables 规则和 Cilium 安装的 iptables 规则的集成。&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
    
      
        
          &lt;img src=&#34;https://jimmysong.io/book/cilium-handbook/ebpf/iptables/kubernetes_iptables.svg&#34; data-img=&#34;/book/cilium-handbook/ebpf/iptables/kubernetes_iptables.svg&#34; alt=&#34;image&#34; data-caption=&#34;kube-proxy 与 Cilium 的 iptables 规则集成&#34;&gt;
        
      
    
  
  &lt;figcaption&gt;kube-proxy 与 Cilium 的 iptables 规则集成&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div class=&#34;cta-group&#34;&gt;
  
    &lt;a href=&#34;../../policy&#34;  class=&#34;btn btn-sm btn-primary&#34;&gt;下一章&lt;/a&gt;
  
  
&lt;/dv&gt;


      </description>
    </item>
    
  </channel>
</rss>
