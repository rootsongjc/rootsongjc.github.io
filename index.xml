<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jimmy Song&#39;s Blog</title>
    <link>http://rootsongjc.github.io/</link>
    <description>Recent content on Jimmy Song&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 15 Jul 2017 16:25:30 +0800</lastBuildDate>
    
	<atom:link href="http://rootsongjc.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>什么是云原生应用架构</title>
      <link>http://rootsongjc.github.io/blogs/what-is-cloud-native-application-architecture/</link>
      <pubDate>Sat, 15 Jul 2017 16:25:30 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/what-is-cloud-native-application-architecture/</guid>
      <description>本文译自Migrating to Cloud Native Application Architectures第一部分，归档到https://github.com/rootsongjc/migrating-to-cloud-native-application-architectures 。
第1章 云原生的崛起 软件正在吞噬这个世界。—Mark Andreessen
近些年来，在一些长期由领导者支配的行业中，这些领导者的领先地位已经岌岌可危，这都是由以这些行业为核心业务的软件公司造成的。像Square、Uber、Netflix、Airbnb和特斯拉这样的公司能够持续快速增长，并并拥有傲人的市场估值，成为它们所在行业的新领导者。这些创新公司有什么共同点？
 快速创新 持续可用的服务 弹性可扩展的Web 以移动为核心的用户体验  将软件迁移到云上是一种自演化，使用了云原生应用架构是这些公司能够如此具有破坏性的核心原因。通过云，任何能够按需、自助弹性提供和释放计算、网络和存储资源的计算环境。云的定义包括公有云(例如 Amazon Web Services、Google Cloud和Microsoft Azure)和私有云(例如 VMware vSphere和 OpenStack)。
本章中我们将探讨云原生应用架构的创新性，然后论证云原生应用架构的主要特性。
为何使用云原生应用架构 首先我们来阐述下将应用迁移到云原生架构的动机。
速度 天下武功，唯快不破，市场竞争亦是如此。想象一下，能够快速创新、实验并交付软件的企业，与使用传统软件交付模式的企业，谁将在市场竞争中胜出呢？
在传统企业中，为应用提供环境和部署新版本花费的时间通常以天、周或月来计算。这种速度严重限制了每个发行版可以承担的风险，因为修复这些错误往往跟发行一个新版本有差不多的耗时。
互联网公司经常提到它们每天极版次发布的实践。为什么频繁发布如此重要？如果你可以每天实现几百次发布，你们就可以几乎立即从错误的版本恢复过来。如果你可以立即从错误中恢复过来，你就能够承受更多的风险。如果你可以承受更多的风险，你就可以做更疯狂的试验——这些试验结果可能会成为你接下来的竞争优势。
基于云基础设置的弹性和自服务的特性天生就适应于这种工作方式。通过调用云服务API来提供新的应用程序环境比基于表单的手动过程要快几个数量级。然后通过另一个API调用将代码部署到新的环境中。将自服务和hook添加到团队的CI/CD服务器环境中进一步加快了速度。现在，我们可以回答精益大师Mary Poppendick提出的问题了——“如果只是改变了应用的一行代码，您的组织需要多长时间才能把应用部署到线上？“答案是几分钟或几秒钟。
你可以大胆想象 一下，如果你也可以达到这样的速度，你的团队、你的业务可以做哪些事情呢？
安全 光是速度快还是不够的。如果你开车是一开始就把油门踩到底，你将因此发生事故而付出惨痛的代价（有时甚至是致命的）！不同的交通方式如飞机和特快列车都会兼顾速度和安全性。云原生应用架构在快速变动的需求、稳定性、可用性和耐久性之间寻求平衡。这是可能的而且非常有必要同时实现的。
我们前面已经提到过，云原生应用架构可以让我们迅速地从错误中恢复。我们没有谈论如何预防错误，而在企业里往往在这一点上花费了大量的时间。在追寻速度的路上，大而全的前端升级，详尽的文档，架构复核委员会和漫长的回归测试周期在一次次成为我们的绊脚石。当然，之所以这样做都是出于好意。不幸的是，所有这些做法都不能提供一致可衡量的生产缺陷改善度量。
那么我们如何才能做到即安全又快速呢？
可视化
我们的架构必须为我们提供必要的工具，以便可以在发生故障时看到它。 我们需要观测一切的能力，建立一个“哪些是正常”的概况，检测与标准情况的偏差（包括绝对值和变化率），并确定哪些组件导致了这些偏差。功能丰富的指标、监控、警报、数据可视化框架和工具是所有云原生应用程序体系结构的核心。
故障隔离
为了限制与故障带来的风险，我们需要限制可能受到故障影响的组件或功能的范围。如果每次亚马逊的推荐引擎挂掉后人们就不能再在亚马逊上买产品，那将是灾难性的。单体架构通常就是这种类型的故障模式。云原生应用程序架构通常使用微服务。通过将系统拆解为微服务，我们可以将任何一个微服务的故障范围限制在这个微服务上，但还需要结合容错才能实现这一点。
容错
仅仅将系统拆解为可以独立部署的微服务还是不够的；还需要防止出现错误的组件将错误传递它所的依赖组件上造成级联故障。Mike Nygard 在他的《Release It! - Pragmatic Programmers》一书中描述了一些容错模型，最受欢迎的是断路器。软件断路器的工作原理就类似于电子断路器（保险丝）：断开它所保护的组件与故障系统之间的回路以防止级联故障。它还可以提供一个优雅的回退行为，比如回路断开的时候提供一组默认的产品推荐。我们将在“容错”一节详细讨论该模型。
自动恢复
凭借可视化、故障隔离和容错能力，我们拥有确定故障所需的工具，从故障中恢复，并在进行错误检测和故障恢复的过程中为客户提供合理的服务水平。一些故障很容易识别：它们在每次发生时呈现出相同的易于检测的模式。以服务健康检查为例，结果只有两个：健康或不健康，up或down。很多时候，每次遇到这样的故障时，我们都会采取相同的行动。在健康检查失败的情况下，我们通常只需重新启动或重新部署相关服务。云原生应用程序架构不要当应用在这些情况下无需手动干预。相反，他们会自动检测和恢复。换句话说，他们给电脑装上了寻呼机而不是人。
弹性扩展 随着需求的增加，我们必须扩大服务能力。过去我们通过垂直扩展来处理更多的需求：购买了更强悍的服务器。我们最终实现了自己的目标，但是步伐太慢，并且产生了更多的花费。这导致了基于高峰使用预测的容量规划。我们会问”这项服务需要多大的计算能力？”然后购买足够的硬件来满足这个要求。很多时候我们依然会判断错误，会在如黑色星期五这类事件中打破我们的可用容量规划。但是，更多的时候，我们将会遇到数以百计的服务器，它们的CPU都是空闲的，这会让资源使用率指标很难看。
创新型的公司通过以下两个开创性的举措来解决这个问题：
 它们不再继续购买更大型的服务器，取而代之的是用大量的更便宜机器来水平扩展应用实例。这些机器更容易获得，并且能够快速部署。 通过将大型服务器虚拟化成几个较小的服务器，并向其部署多个隔离的工作负载来改善现有大型服务器的资源利用率。  随着像亚马逊AWS这样的公有云基础设施的出现，这两个举措融合了起来。虚拟化工作被委托给云提供商，消费者只需要关注在大量的云服务器实例很想扩展它们的应用程序实例。最近，作为应用程序部署的单元，发生了另一个转变，从虚拟机转移到了容器。
由于公司不再需要大量启动资金来部署软件，所以向云的转变打开了更多创新之门。正在进行的维护还需要较少的资本投入，并且通过API进行配置不仅可以提高初始部署的速度，还可以最大限度地提高我们应对需求变化的速度。
不幸的是，所有这些好处都带有成本。相较于垂直扩展的应用，支持水平扩展的应用程序的架构必须不同。云的弹性要求应用程序的状态短暂性。我们不仅可以快速创建新的应用实例；我们也必须能够快速、安全地处置它们。这种需求是状态管理的问题：一次性与持久性相互作用如何？ 在大多数垂直架构中采用的诸如聚类会话和共享文件系统的传统方法并不能很好地支持水平扩展。
云原生应用程序架构的另一个标志是将状态外部化到内存数据网格，缓存和持久对象存储，同时保持应用程序实例本身基本上是无状态的。无状态应用程序可以快速创建和销毁，以及附加到外部状态管理器和脱离外部状态管理器，增强我们响应需求变化的能力。当然这也需要外部状态管理器自己来扩展。大多数云基础设施提供商已经认识到这一必要性，并提供了这类服务的健康菜单。</description>
    </item>
    
    <item>
      <title>Kubernetes中的数据持久化问题的一个案例讨论与解决方案探究</title>
      <link>http://rootsongjc.github.io/blogs/data-persistence-problem/</link>
      <pubDate>Tue, 11 Jul 2017 20:33:21 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/data-persistence-problem/</guid>
      <description> 本文已归档到kubernetes-handbook中的【最佳实践—运维管理】章节中，一切内容以kubernetes-handbook为准。
数据落盘问题的由来 这本质上是数据持久化问题，对于有些应用依赖持久化数据，比如应用自身产生的日志需要持久化存储的情况，需要保证容器里的数据不丢失，在Pod挂掉后，其他应用依然可以访问到这些数据，因此我们需要将数据持久化存储起来。
数据落盘问题解决方案 下面以一个应用的日志收集为例，该日志需要持久化收集到ElasticSearch集群中，如果不考虑数据丢失的情形，可以直接使用kubernetes-handbook中【应用日志收集】一节中的方法，但考虑到Pod挂掉时logstash（或filebeat）并没有收集完该pod内日志的情形，我们想到了如下这种解决方案，示意图如下：
 首先需要给数据落盘的应用划分node，即这些应用只调用到若干台主机上 给这若干台主机增加label 使用deamonset方式在这若干台主机上启动logstash的Pod（使用nodeSelector来限定在这几台主机上，我们在边缘节点启动的treafik也是这种模式） 将应用的数据通过volume挂载到宿主机上 Logstash（或者filebeat）收集宿主机上的数据，数据持久化不会丢失  Side-effect  首先kubernetes本身就提供了数据持久化的解决方案statefulset，不过需要用到公有云的存储货其他分布式存储，这一点在我们的私有云环境里被否定了。 需要管理主机的label，增加运维复杂度，但是具体问题具体对待 必须保证应用启动顺序，需要先启动logstash 为主机打label使用nodeSelector的方式限制了资源调度的范围  </description>
    </item>
    
    <item>
      <title>Kubernetes kubectl cheat sheat——Kubectl命令参考图</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-kubectl-cheatsheat/</link>
      <pubDate>Sat, 01 Jul 2017 14:44:23 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-kubectl-cheatsheat/</guid>
      <description>参考 https://kubernetes.io/docs/user-guide/kubectl/v1.6/ 绘制。
通过该图可以对kubernetes的客户端命令kubectl有个感性和大体的了解，具体使用方法请参考官方文档。
图片归档在kubernetes-handbook，以GitHub中的图片为准。
注：在移动设备上打开该链接后会提示图片太大无法查看请选择Desktop version方可查看原图。
今后我将陆续推出其他object的cheat sheet，敬请关注。</description>
    </item>
    
    <item>
      <title>微服务中的服务发现方式对比</title>
      <link>http://rootsongjc.github.io/blogs/service-discovery-in-microservices/</link>
      <pubDate>Fri, 30 Jun 2017 18:32:19 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/service-discovery-in-microservices/</guid>
      <description>在单体架构时，因为服务不会经常和动态迁移，所有服务地址可以直接在配置文件中配置，所以也不会有服务发现的问题。但是对于微服务来说，应用的拆分，服务之间的解耦，和服务动态扩展带来的服务迁移，服务发现就成了微服务中的一个关键问题。
服务发现分为客户端服务发现和服务端服务发现两种，架构如下图所示。
这两种架构都各有利弊，我们拿客户端服务发现软件Eureka和服务端服务发现架构Kubernetes/SkyDNS+Ingress LB+Traefik+PowerDNS为例说明。
   服务发现方案 Pros Cons     Eureka 使用简单，适用于java语言开发的项目，比服务端服务发现少一次网络跳转 对非Java语言的支持不够好，Consumer需要内置特定的服务发现客户端和发现逻辑   Kubernetes Consumer无需关注服务发现具体细节，只需知道服务的DNS域名即可 需要基础设施支撑，多了一次网络跳转，可能有性能损失    参考 谈服务发现的背景、架构以及落地方案</description>
    </item>
    
    <item>
      <title>使用Jenkins进行持续构建与发布应用到kubernetes集群中</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-jenkins-ci-cd/</link>
      <pubDate>Tue, 27 Jun 2017 20:52:57 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-jenkins-ci-cd/</guid>
      <description>（题图：正午@东直门 Jun 27，2017）
本文已归档到kubernetes-handbook中的【最佳实践—使用Jenkins进行持续构建与发布】章节中，一切内容以kubernetes-handbook为准。
我们基于Jenkins的CI/CD流程如下所示。
流程说明 应用构建和发布流程说明。
 用户向Gitlab提交代码，代码中必须包含Dockerfile； 将代码提交到远程仓库； 用户在发布应用时需要填写git仓库地址和分支、服务类型、服务名称、资源数量、实例个数等，确定后触发Jenkins自动构建； Jenkins的CI流水线自动编译代码并打包成docker镜像推送到Harbor镜像仓库； Jenkins的CI流水线中包括了自定义脚本，根据我们已准备好的kubernetes的YAML模板，将其中的变量替换成用户输入的选项； 生成应用的kubernetes YAML配置文件； 更新Ingress的配置，根据新部署的应用的名称，在ingress的配置文件中增加一条路由信息 更新PowerDNS，向其中插入一条DNS记录，IP地址是边缘节点的IP地址。关于边缘节点，请查看kubernetes-handbook中的【最佳实践——边缘节点配置】章节； Jenkins调用kubernetes的API，部署应用到kubernetes集群中。  关于应用的更新、滚动升级、灰度发布请留意博客中的后续文章或关注kubernetes-handbook的更新。</description>
    </item>
    
    <item>
      <title>云原生微服务治理框架Linkerd简介</title>
      <link>http://rootsongjc.github.io/blogs/linkerd-introduction/</link>
      <pubDate>Mon, 26 Jun 2017 21:02:13 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/linkerd-introduction/</guid>
      <description>（题图：青岛 May 26,2017）
前言 Linkerd是一个用于云原生应用的开源、可扩展的service mesh（一般翻译成服务网格，还有一种说法叫”服务啮合层“，见Istio：用于微服务的服务啮合层）。同时，Linkerd也是CNCF（云原生计算基金会）中的组件之一。
P.S 本文已归档到kubernetes-handbook中的【领域应用—微服务架构】章节中。
Linkerd是什么 Linkerd的出现是为了解决像twitter、google这类超大规模生产系统的复杂性问题。Linkerd不是通过控制服务之间的通信机制来解决这个问题，而是通过在服务实例之上添加一个抽象层来解决的。
Linkerd负责跨服务通信中最困难、易出错的部分，包括延迟感知、负载均衡、连接池、TLS、仪表盘、请求路由等——这些都会影响应用程序伸缩性、性能和弹性。
如何运行 Linkerd作为独立代理运行，无需特定的语言和库支持。应用程序通常会在已知位置运行linkerd实例，然后通过这些实例代理服务调用——即不是直接连接到目标服务，服务连接到它们对应的linkerd实例，并将它们视为目标服务。
在该层上，linkerd应用路由规则，与现有服务发现机制通信，对目标实例做负载均衡——与此同时调整通信并报告指标。
通过延迟调用linkerd的机制，应用程序代码与以下内容解耦：
 生产拓扑 服务发现机制 负载均衡和连接管理逻辑  应用程序也将从一致的全局流量控制系统中受益。这对于多语言应用程序尤其重要，因为通过库来实现这种一致性是非常困难的。
Linkerd实例可以作为sidecar（既为每个应用实体或每个主机部署一个实例）来运行。 由于linkerd实例是无状态和独立的，因此它们可以轻松适应现有的部署拓扑。它们可以与各种配置的应用程序代码一起部署，并且基本不需要去协调它们。
参考 Buoyant发布服务网格Linkerd的1.0版本
Linkerd documentation
Istio：用于微服务的服务啮合层</description>
    </item>
    
    <item>
      <title>Kubernetes Pod Cheat Sheet——Pod数据结构参考图</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-pod-cheetsheet/</link>
      <pubDate>Sat, 24 Jun 2017 14:20:39 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-pod-cheetsheet/</guid>
      <description>昨天晚上构思，今天花了一上午的时间翻阅了下kubernetes api reference，画了一个Kubernetes Pod Cheat Sheet。
从Pod的数据结构和API入手，管中窥豹，可见一斑。通过该图基本可以对kubernetes中这个最基本的object——Pod的功能和配置有一个感性的认识了，也许具体的某个组件的实现你不了解，但是从high level的视角来看待Pod整体有助于今后深入研究某个feature。
该图是根据kubernetes 1.6版本的Pod v1 core API绘制。
图片归档在kubernetes-handbook,请以GitHub中的图片为准。
注：在移动设备上打开该链接后会提示图片太大无法查看请选择Desktop version方可查看原图。
今后我将陆续推出其他object的cheat sheet，敬请关注。</description>
    </item>
    
    <item>
      <title>使用API blueprint创建API文档</title>
      <link>http://rootsongjc.github.io/blogs/creating-api-document-with-api-blueprint/</link>
      <pubDate>Fri, 23 Jun 2017 12:24:12 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/creating-api-document-with-api-blueprint/</guid>
      <description>在进行微服务开发的过程中，为了保证最终开发的系统跟最初的设计保持一致，约定RESTful接口之间的调用方法，我们需要将API设计文档化，因此我们引入了API Blueprint。
API Blueprint 是什么 API Blueprint 用来编写API文档的一种标记语言，类似于Markdown，因为是纯文本的，所以方便共享编辑，具体的语法规则可以在 API Blueprint documentation 查看，配合一些开源的工具可以把接口文档渲染成 html 再搭配一个静态服务器，方便于分享。
另外，配合一些工具，可以直接生成一个 mock data 数据，这样只要和后端的同学约定好接口格式，那么前端再开发的时候可以使用 mock data 数据来做测试，等到后端写好接口之后再做联调就可以了。
我们以Cloud Native Go书中的gogo-service示例里的apiary.apib文件为例。
该文件实际上是一个Markdown格式的文件，Github中原生支持该文件，使用Typora打开后是这样子。
在Visual Studio Code中有个API Element extension对于API Blueprint文件的支持也比较好。
生成静态页面和进行冒烟测试 我们分别使用开源的aglio和drakov来生成静态页面和进行冒烟测试。
aglio 是一个可以根据 api-blueprint 的文档生成静态 HTML 页面的工具。
其生成的工具不是简单的 markdown 到 html 的转换, 而是可以生成类似 rdoc 这样的拥有特定格式风格的查询文档。
在本地安装有node环境的情况下，使用下面的命令安装和使用aglio。
$ npm install -g aglio $ aglio -i apiary.apib -o api.html  打开api.html文件后，如图：
安装和使用drakov。
$ npm install -g drakov $ drakov -f apiary.</description>
    </item>
    
    <item>
      <title>使用Wercker进行持续构建与发布</title>
      <link>http://rootsongjc.github.io/blogs/continuous-integration-with-wercker/</link>
      <pubDate>Thu, 22 Jun 2017 18:08:51 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/continuous-integration-with-wercker/</guid>
      <description>本文介绍了wercker和它的基本用法，并用我GitHub上的magpie应用作为示例，讲解如何给GitHub项目增加wercker构建流程，并将生成的镜像自动上传到Docker Hub上。
注：本文参考了Cloud Native Go书中的”持续交付“章节。
CI工具 开源项目的构建离不开CI工具，你可能经常会在很多GitHub的开源项目首页上看到这样的东西：
这些图标都是CI工具提供的，可以直观的看到当前的构建状态，例如wercker中可以在Application-magpie-options中看到：
将文本框中的代码复制到你的项目的README文件中，就可以在项目主页上看到这样的标志了。
现在市面上有很多流行的CI/CD工具和DevOps工具有很多，这些工具提高了软件开发的效率，增加了开发人员的幸福感。这些工具有：
适用于GitHub上的开源项目，可以直接使用GitHub账户登陆，对于公开项目可以直接使用：Travis-ci、CircleCI、Wercker。从目前GitHub上开源项目的使用情况来看，Travis-ci的使用率更高一些。
适用于企业级的：Jenkins
不仅包括CI/CD功能的DevOps平台：JFrog、Spinnaker、Fabric8
Wercker简介 Wercker是一家为现代云服务提供容器化应用及微服务的快速开发、部署工具的初创企业，成立于2012年，总部位于荷兰阿姆斯特丹。其以容器为中心的平台可以对微服务和应用的开发进行自动化。开发者通过利用其命令行工具能够生成容器到桌面，然后自动生成应用并部署到各种云平台上面。其支持的平台包括Heroku、AWS以及Rackspace等。
Wercker于2016年获得450万美元A轮融资，此轮融资由Inkef Capital领投，Notion Capital跟投，融资所得将用于商业版产品的开发。此轮融资过后其总融资额为750万美元。
Wercker于2017年4月被Oracle甲骨文于收购。
为什么使用Wercker 所有的CI工具都可以在市面上获取，但为何要建议使用Wercker呢？依据云之道的准则评估了所有工具，发现Wercker正是我们需要的。
首先，无须在工作站中安装Wecker，仅安装一个命令行客户端即可，构建过程全部在云端进行。
其次，不用通过信用卡就可使用Wercker。当我们迫切希望简化流程时，这是一件令人赞叹的事。付款承诺这一条件大大增加了开发者的压力，这通常是不必要的。
最后，Wercker使用起来非常简单。它非常容易配置，不需要经过高级培训或拥有持续集成的博士学位，也不用制定专门的流程。
通过Wercker搭建CI环境只需经过三个基本步骤。
1．在Wercker网站中创建一个应用程序。
2．将wercker.yml添加到应用程序的代码库中。
3．选择打包和部署构建的位置。
如何使用 可以使用GitHub帐号直接登录Wercker，整个创建应用CI的流程一共3步。
一旦拥有了账户，那么只需简单地点击位于顶部的应用程序菜单，然后选择创建选项即可。如果系统提示是否要创建组织或应用程序，请选择应用程序。Wercker组织允许多个Wercker用户之间进行协作，而无须提供信用卡。下图为设置新应用程序的向导页面。
选择了GitHub中的repo之后，第二步配置访问权限，最后一步Wercker会尝试生成一个wercker.yml文件（后面会讨论）。不过至少对于Go应用程序来说，这个配置很少会满足要求，所以我们总是需要创建自己的Wercker配置文件。
安装Wercker命令行程序 这一步是可选的，如果你希望在本地进行wercker构建的话才需要在本地安装命令行程序。本地构建和云端构建都依赖于Docker的使用。基本上，代码会被置于所选择的docker镜像中（在wercker.yml中定义），然后再选择执行的内容和方法。
要在本地运行Wercker构建，需要使用Wercker CLI。有关如何安装和测试CLI的内容，请查看http://devcenter.wercker.com/docs/cli。Wercker更新文档的频率要比本书更高，所以请在本书中做个标记，然后根据Wercker网站的文档安装Wercker CLI。
如果已经正确安装了CLI，应该可以查询到CLI的版本，代码如下所示。
Version: 1.0.882 Compiled at: 2017-06-02 06:49:39 +0800 CST Git commit: da8bc056ed99e27b4b7a1b608078ddaf025a9dc4 No new version available  本地构建只要在项目的根目录下输入wercker build命令即可，wercker会自动下载依赖的docker镜像在本地运行所有构建流程。
创建Wercker配置文件wercker.yml Wercker配置文件是一个YAML文件，该文件必须在GitHub repo的最顶层目录，该文件主要包含三个部分，对应可用的三个主要管道。
 Dev：定义了开发管道的步骤列表。与所有管道一样，可以选定一个box用于构建，也可以全局指定一个box应用于所有管道。box可以是Wercker内置的预制Docker镜像之一，也可以是Docker Hub托管的任何Docker镜像。
 Build：定义了在Wercker构建期间要执行的步骤和脚本的列表。与许多其他服务（如Jenkins和TeamCity）不同，构建步骤位于代码库的配置文件中，而不是隐藏在服务配置里。
 Deploy：在这里可以定义构建的部署方式和位置。
Wercker中还有工作流的概念，通过使用分支、条件构建、多个部署目标和其他高级功能扩展了管道的功能，这些高级功能读着可以自己在wercker的网站中探索。
示例 我们以我用Go语言开发的管理yarn on docker集群的命令行工具magpie为例，讲解如何使用wercker自动构建，并产生docker镜像发布到Docker Hub中。</description>
    </item>
    
    <item>
      <title>kubernetes client-go包使用示例</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-client-go-sample/</link>
      <pubDate>Wed, 21 Jun 2017 19:23:45 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-client-go-sample/</guid>
      <description>(题图：青岛栈桥 May 26,2017)
前言 本文将归档到kubernetes-handbook的【开发指南—client-go示例】章节中，最终版本以kubernetes-handbook中为准。
本文中的代码见：https://github.com/rootsongjc/kubernetes-client-go-sample
client-go示例 访问kubernetes集群有几下几种方式：
   方式 特点 支持者     Kubernetes dashboard 直接通过Web UI进行操作，简单直接，可定制化程度低 官方支持   kubectl 命令行操作，功能最全，但是比较复杂，适合对其进行进一步的分装，定制功能，版本适配最好 官方支持   client-go 从kubernetes的代码中抽离出来的客户端包，简单易用，但需要小心区分kubernetes的API版本 官方支持   client-python python客户端，kubernetes-incubator 官方支持   Java client fabric8中的一部分，kubernetes的java客户端 redhat    下面，我们基于client-go，对Deployment升级镜像的步骤进行了定制，通过命令行传递一个Deployment的名字、应用容器名和新image名字的方式来升级。代码和使用方式见 https://github.com/rootsongjc/kubernetes-client-go-sample 。
kubernetes-client-go-sample 代码如下：
package main import ( &amp;quot;flag&amp;quot; &amp;quot;fmt&amp;quot; &amp;quot;os&amp;quot; &amp;quot;path/filepath&amp;quot; &amp;quot;k8s.io/apimachinery/pkg/api/errors&amp;quot; metav1 &amp;quot;k8s.io/apimachinery/pkg/apis/meta/v1&amp;quot; &amp;quot;k8s.io/client-go/kubernetes&amp;quot; &amp;quot;k8s.io/client-go/tools/clientcmd&amp;quot; ) func main() { var kubeconfig *string if home := homeDir(); home !</description>
    </item>
    
    <item>
      <title>kubernetes管理的容器命名规则解析</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-container-naming-rule/</link>
      <pubDate>Thu, 15 Jun 2017 21:52:38 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-container-naming-rule/</guid>
      <description>本文将归档到kubernetes-handbook的【运维管理-监控】章节中，最终版本以kubernetes-handbook中为准。
当我们通过cAdvisor获取到了容器的信息后，例如访问${NODE_IP}:4194/api/v1.3/docker获取的json结果中的某个容器包含如下字段：
&amp;quot;labels&amp;quot;: { &amp;quot;annotation.io.kubernetes.container.hash&amp;quot;: &amp;quot;f47f0602&amp;quot;, &amp;quot;annotation.io.kubernetes.container.ports&amp;quot;: &amp;quot;[{\&amp;quot;containerPort\&amp;quot;:80,\&amp;quot;protocol\&amp;quot;:\&amp;quot;TCP\&amp;quot;}]&amp;quot;, &amp;quot;annotation.io.kubernetes.container.restartCount&amp;quot;: &amp;quot;0&amp;quot;, &amp;quot;annotation.io.kubernetes.container.terminationMessagePath&amp;quot;: &amp;quot;/dev/termination-log&amp;quot;, &amp;quot;annotation.io.kubernetes.container.terminationMessagePolicy&amp;quot;: &amp;quot;File&amp;quot;, &amp;quot;annotation.io.kubernetes.pod.terminationGracePeriod&amp;quot;: &amp;quot;30&amp;quot;, &amp;quot;io.kubernetes.container.logpath&amp;quot;: &amp;quot;/var/log/pods/d8a2e995-3617-11e7-a4b0-ecf4bbe5d414/php-redis_0.log&amp;quot;, &amp;quot;io.kubernetes.container.name&amp;quot;: &amp;quot;php-redis&amp;quot;, &amp;quot;io.kubernetes.docker.type&amp;quot;: &amp;quot;container&amp;quot;, &amp;quot;io.kubernetes.pod.name&amp;quot;: &amp;quot;frontend-2337258262-771lz&amp;quot;, &amp;quot;io.kubernetes.pod.namespace&amp;quot;: &amp;quot;default&amp;quot;, &amp;quot;io.kubernetes.pod.uid&amp;quot;: &amp;quot;d8a2e995-3617-11e7-a4b0-ecf4bbe5d414&amp;quot;, &amp;quot;io.kubernetes.sandbox.id&amp;quot;: &amp;quot;843a0f018c0cef2a5451434713ea3f409f0debc2101d2264227e814ca0745677&amp;quot; },  这些信息其实都是kubernetes创建容器时给docker container打的Labels。
你是否想过这些label跟容器的名字有什么关系？当你在node节点上执行docker ps看到的容器名字又对应哪个应用的Pod呢？
在kubernetes代码中pkg/kubelet/dockertools/docker.go中的BuildDockerName方法定义了容器的名称规范。
这段容器名称定义代码如下：
// Creates a name which can be reversed to identify both full pod name and container name. // This function returns stable name, unique name and a unique id. // Although rand.Uint32() is not really unique, but it&#39;s enough for us because error will // only occur when instances of the same container in the same pod have the same UID.</description>
    </item>
    
    <item>
      <title>Kubernetes配置最佳实践</title>
      <link>http://rootsongjc.github.io/blogs/configuration-best-practice/</link>
      <pubDate>Wed, 14 Jun 2017 20:03:09 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/configuration-best-practice/</guid>
      <description>（题图：青岛 May 26,2017）
前言 本文档旨在汇总和强调用户指南、快速开始文档和示例中的最佳实践。该文档会很很活跃并持续更新中。如果你觉得很有用的最佳实践但是本文档中没有包含，欢迎给我们提Pull Request。
本文已上传到kubernetes-handbook中的第四章最佳实践章节，本文仅作归档，更新以kubernetes-handbook为准。
通用配置建议  定义配置文件的时候，指定最新的稳定API版本（目前是V1）。 在配置文件push到集群之前应该保存在版本控制系统中。这样当需要的时候能够快速回滚，必要的时候也可以快速的创建集群。 使用YAML格式而不是JSON格式的配置文件。在大多数场景下它们都可以作为数据交换格式，但是YAML格式比起JSON更易读和配置。 尽量将相关的对象放在同一个配置文件里。这样比分成多个文件更容易管理。参考guestbook-all-in-one.yaml文件中的配置（注意，尽管你可以在使用kubectl命令时指定配置文件目录，你也可以在配置文件目录下执行kubectl create——查看下面的详细信息）。 为了简化和最小化配置，也为了防止错误发生，不要指定不必要的默认配置。例如，省略掉ReplicationController的selector和label，如果你希望它们跟podTemplate中的label一样的话，因为那些配置默认是podTemplate的label产生的。更多信息请查看 guestbook app 的yaml文件和 examples 。 将资源对象的描述放在一个annotation中可以更好的内省。  裸奔的Pods vs Replication Controllers和 Jobs  如果有其他方式替代“裸奔的pod”（如没有绑定到replication controller 上的pod），那么就使用其他选择。在node节点出现故障时，裸奔的pod不会被重新调度。Replication Controller总是会重新创建pod，除了明确指定了restartPolicy: Never 的场景。Job 也许是比较合适的选择。  Services  通常最好在创建相关的replication controllers之前先创建service（没有这个必要吧？）你也可以在创建Replication Controller的时候不指定replica数量（默认是1），创建service后，在通过Replication Controller来扩容。这样可以在扩容很多个replica之前先确认pod是正常的。 除非时分必要的情况下（如运行一个node daemon），不要使用hostPort（用来指定暴露在主机上的端口号）。当你给Pod绑定了一个hostPort，该pod可被调度到的主机的受限了，因为端口冲突。如果是为了调试目的来通过端口访问的话，你可以使用 kubectl proxy and apiserver proxy 或者 kubectl port-forward。你可使用 Service 来对外暴露服务。如果你确实需要将pod的端口暴露到主机上，考虑使用 NodePort service。 跟hostPort一样的原因，避免使用 hostNetwork。 如果你不需要kube-proxy的负载均衡的话，可以考虑使用使用headless services。  使用Label  定义 labels 来指定应用或Deployment的 semantic attributes 。例如，不是将label附加到一组pod来显式表示某些服务（例如，service:myservice），或者显式地表示管理pod的replication controller（例如，controller:mycontroller），附加label应该是标示语义属性的标签， 例如{app:myapp,tier:frontend,phase:test,deployment:v3}。 这将允许您选择适合上下文的对象组——例如，所有的”tier:frontend“pod的服务或app是“myapp”的所有“测试”阶段组件。 有关此方法的示例，请参阅guestbook应用程序。  可以通过简单地从其service的选择器中省略特定于发行版本的标签，而不是更新服务的选择器来完全匹配replication controller的选择器，来实现跨越多个部署的服务，例如滚动更新。</description>
    </item>
    
    <item>
      <title>Cloud Native Go - 基于Go和React的web云服务构建指南</title>
      <link>http://rootsongjc.github.io/talks/cloud-native-go/</link>
      <pubDate>Tue, 06 Jun 2017 22:23:54 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/talks/cloud-native-go/</guid>
      <description>(题图：北京植物园桃花 Mar 26,2016)
Kevin Hoffman和Dan Nemeth著的《Cloud Native Go - 基于Go和React的web云原生应用构建指南》将由电子工业出版社出版。
简介 Cloud Native Go向开发人员展示如何构建大规模云应用程序，在满足当今客户的强大需求的同时还可以动态扩展来处理几乎任何规模的数据量、流量或用户。
Kevin Hoffman和Dan Nemeth详细描述了现代云原生应用程序，阐明了与快速、可靠的云原生开发相关的因素、规则和习惯。他们还介绍了Go这种“简单优雅”的高性能语言，它特别适合于云开发。
在本书中你将使用Go语言创建微服务，使用ReactJS和Flux添加前端Web组件，并掌握基于Go的高级云原生技术。Hoffman和Nemeth展示了如何使用Wercker、Docker和Dockerhub等工具构建持续交付管道; 自动推送应用程序到平台上; 并系统地监控生产中的应用程序性能。
 学习“云之道”：为什么开发好的云软件基本上是关于心态和规则 了解为什么使用Go语言是云本地微服务开发的理想选择 规划支持持续交付和部署的云应用程序 设计服务生态系统，然后以test-first的方式构建它们
 将正在进行的工作推送到云
 使用事件源和CQRS模式来响应大规模和高吞吐量
 安全的基于云的Web应用程序：做与不做的选择
 使用第三方消息传递供应商创建响应式云应用程序
 使用React和Flux构建大规模，云友好的GUI
 监控云中的动态扩展，故障转移和容错
  章节简介如下图。
关于作者 Kevin Hoffman通过现代化和以多种不同语言构建云原生服务的方式帮助企业将其应用程序引入云端。他10岁时开始编程，在重新组装的CommodoreVIC-20上自习BASIC。从那时起，他已经沉迷于构建软件，并花了很多时间学习语言、框架和模式。他已经构建了从遥控摄影无人机、仿生性安全系统、超低延迟金融应用程序到移动应用程序等一系列软件。他在构建需要与Pivotal Cloud Foundry配合使用的自定义组件时爱上了Go语言。
Kevin 是流行的幻想书系列（*The Sigilord Chronicles*，http://amzn.to/ 2fc8iES）的作者，他热切地期待着最终能够将自己对构建软件的热爱与对构建幻想世界的热爱结合起来。
Dan Nemeth目前在Pivotal担任咨询解决方案架构师，负责支持Pivotal Cloud Foundry。他从Commodore 64开始就一直在开发软件，从1995年起开始专业编码，使用ANSIC编写了用于本地ISP的CGI脚本。从那时起，他职业生涯的大部分时间里是作为独立顾问为从金融到制药行业提供解决方案，并使用当时流行的各种语言和框架。Dan最近接受了Go作为自己的归宿，并热情地将它用于所有的项目。
如果你发现Dan没在电脑前，他很可能就是在靠近安纳波利斯的水域玩帆船或飞钓。
下面先罗列下目录，以飨读者。
目录 云之道.. 1 云之道的优点.. 2
遵循简单.. 2
测试优先，测试一切.. 3
尽早发布，频繁发布.. 5
自动化一切.. 6</description>
    </item>
    
    <item>
      <title>微服务管理框架Istio简介</title>
      <link>http://rootsongjc.github.io/blogs/istio-overview/</link>
      <pubDate>Fri, 02 Jun 2017 11:27:57 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/istio-overview/</guid>
      <description>（题图：威海朱口 May 29,2017）
前言 本文已上传到kubernetes-handbook中的第五章微服务章节，本文仅作归档，更新以kubernetes-handbook为准。
Istio是由Google、IBM和Lyft开源的微服务管理、保护和监控框架。Istio为希腊语，意思是”起航“。
简介 使用istio可以很简单的创建具有负载均衡、服务间认证、监控等功能的服务网络，而不需要对服务的代码进行任何修改。你只需要在部署环境中，例如Kubernetes的pod里注入一个特别的sidecar proxy来增加对istio的支持，用来截获微服务之间的网络流量。
目前版本的istio只支持kubernetes，未来计划支持其他其他环境。
特性 使用istio的进行微服务管理有如下特性：
 流量管理：控制服务间的流量和API调用流，使调用更可靠，增强不同环境下的网络鲁棒性。 可观测性：了解服务之间的依赖关系和它们之间的性质和流量，提供快速识别定位问题的能力。 策略实施：通过配置mesh而不是以改变代码的方式来控制服务之间的访问策略。 服务识别和安全：提供在mesh里的服务可识别性和安全性保护。  未来将支持多种平台，不论是kubernetes、Mesos、还是云。同时可以集成已有的ACL、日志、监控、配额、审计等。
架构 Istio架构分为控制层和数据层。
 数据层：由一组智能代理（Envoy）作为sidecar部署，协调和控制所有microservices之间的网络通信。 控制层：负责管理和配置代理路由流量，以及在运行时执行的政策。  Envoy Istio使用Envoy代理的扩展版本，该代理是以C++开发的高性能代理，用于调解service mesh中所有服务的所有入站和出站流量。 Istio利用了Envoy的许多内置功能，例如动态服务发现，负载平衡，TLS终止，HTTP/2＆gRPC代理，断路器，运行状况检查，基于百分比的流量拆分分阶段上线，故障注入和丰富指标。
Envoy在kubernetes中作为pod的sidecar来部署。 这允许Istio将大量关于流量行为的信号作为属性提取出来，这些属性又可以在Mixer中用于执行策略决策，并发送给监控系统以提供有关整个mesh的行为的信息。 Sidecar代理模型还允许你将Istio功能添加到现有部署中，无需重新构建或重写代码。 更多信息参见设计目标。
Mixer Mixer负责在service mesh上执行访问控制和使用策略，并收集Envoy代理和其他服务的遥测数据。代理提取请求级属性，发送到mixer进行评估。有关此属性提取和策略评估的更多信息，请参见Mixer配置。 混音器包括一个灵活的插件模型，使其能够与各种主机环境和基础架构后端进行接口，从这些细节中抽象出Envoy代理和Istio管理的服务。
Istio Manager Istio-Manager用作用户和Istio之间的接口，收集和验证配置，并将其传播到各种Istio组件。它从Mixer和Envoy中抽取环境特定的实现细节，为他们提供独立于底层平台的用户服务的抽象表示。 此外，流量管理规则（即通用4层规则和七层HTTP/gRPC路由规则）可以在运行时通过Istio-Manager进行编程。
Istio-auth Istio-Auth提供强大的服务间和最终用户认证，使用相互TLS，内置身份和凭据管理。它可用于升级service mesh中的未加密流量，并为运营商提供基于服务身份而不是网络控制的策略的能力。 Istio的未来版本将增加细粒度的访问控制和审计，以使用各种访问控制机制（包括属性和基于角色的访问控制以及授权hook）来控制和监控访问你服务、API或资源的人员。
参考 Istio开源平台发布，Google、IBM和Lyft分别承担什么角色？
Istio：用于微服务的服务啮合层
Istio Overview</description>
    </item>
    
    <item>
      <title>微服务管理框架istio安装笔记</title>
      <link>http://rootsongjc.github.io/blogs/istio-installation/</link>
      <pubDate>Thu, 01 Jun 2017 20:18:57 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/istio-installation/</guid>
      <description>（题图：威海东部海湾 May 28,2017）
前言 本文已上传到kubernetes-handbook中的第五章微服务章节，本文仅作归档，更新以kubernetes-handbook为准。
本文根据官网的文档整理而成，步骤包括安装istio 0.1.5并创建一个bookinfo的微服务来测试istio的功能。
文中使用的yaml文件可以在kubernetes-handbook的manifests/istio目录中找到，所有的镜像都换成了我的私有镜像仓库地址，请根据官网的镜像自行修改。
安装环境  CentOS 7.3.1611 Docker 1.12.6 Kubernetes 1.6.0  安装 1.下载安装包
下载地址：https://github.com/istio/istio/releases
下载Linux版本的当前最新版安装包
wget https://github.com/istio/istio/releases/download/0.1.5/istio-0.1.5-linux.tar.gz  2.解压
解压后，得到的目录结构如下：
. ├── bin │ └── istioctl ├── install │ └── kubernetes │ ├── addons │ │ ├── grafana.yaml │ │ ├── prometheus.yaml │ │ ├── servicegraph.yaml │ │ └── zipkin.yaml │ ├── istio-auth.yaml │ ├── istio-rbac-alpha.yaml │ ├── istio-rbac-beta.yaml │ ├── istio.yaml │ ├── README.</description>
    </item>
    
    <item>
      <title>使用filebeat收集kubernetes中的应用日志</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-filebeat/</link>
      <pubDate>Wed, 17 May 2017 17:24:52 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-filebeat/</guid>
      <description>（题图：民生现代美术馆 May 14,2017）
前言 本文已同步更新到Github仓库kubernetes-handbook中。
昨天写了篇文章使用Logstash收集Kubernetes的应用日志，发现logstash十分消耗内存（大约500M），经人提醒改用filebeat（大约消耗10几M内存），因此重写一篇使用filebeat收集kubernetes中的应用日志。
在进行日志收集的过程中，我们首先想到的是使用Logstash，因为它是ELK stack中的重要成员，但是在测试过程中发现，Logstash是基于JDK的，在没有产生日志的情况单纯启动Logstash就大概要消耗500M内存，在每个Pod中都启动一个日志收集组件的情况下，使用logstash有点浪费系统资源，经人推荐我们选择使用Filebeat替代，经测试单独启动Filebeat容器大约会消耗12M内存，比起logstash相当轻量级。
方案选择 Kubernetes官方提供了EFK的日志收集解决方案，但是这种方案并不适合所有的业务场景，它本身就有一些局限性，例如：
 所有日志都必须是out前台输出，真实业务场景中无法保证所有日志都在前台输出 只能有一个日志输出文件，而真实业务场景中往往有多个日志输出文件 Fluentd并不是常用的日志收集工具，我们更习惯用logstash，现使用filebeat替代 我们已经有自己的ELK集群且有专人维护，没有必要再在kubernetes上做一个日志收集服务  基于以上几个原因，我们决定使用自己的ELK集群。
Kubernetes集群中的日志收集解决方案
   编号 方案 优点 缺点     1 每个app的镜像中都集成日志收集组件 部署方便，kubernetes的yaml文件无须特别配置，可以为每个app自定义日志收集配置 强耦合，不方便应用和日志收集组件升级和维护且会导致镜像过大   2 单独创建一个日志收集组件跟app的容器一起运行在同一个pod中 低耦合，扩展性强，方便维护和升级 需要对kubernetes的yaml文件进行单独配置，略显繁琐   3 将所有的Pod的日志都挂载到宿主机上，每台主机上单独起一个日志收集Pod 完全解耦，性能最高，管理起来最方便 需要统一日志收集规则，目录和输出方式    综合以上优缺点，我们选择使用方案二。
该方案在扩展性、个性化、部署和后期维护方面都能做到均衡，因此选择该方案。
我们创建了自己的logstash镜像。创建过程和使用方式见https://github.com/rootsongjc/docker-images
镜像地址：index.tenxcloud.com/jimmy/filebeat:5.4.0
测试 我们部署一个应用对logstash的日志收集功能进行测试。
创建应用yaml文件fielbeat-test.yaml。
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: filebeat-test namespace: default spec: replicas: 3 template: metadata: labels: k8s-app: filebeat-test spec: containers: - image: sz-pg-oam-docker-hub-001.</description>
    </item>
    
    <item>
      <title>使用Logstash收集Kubernetes的应用日志</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-logstash/</link>
      <pubDate>Tue, 16 May 2017 17:46:15 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-logstash/</guid>
      <description>（题图：798艺术区 May 14,2017）
前言 本文同步更新到Github仓库kubernetes-handbook中。
很多企业内部都有自己的ElasticSearch集群，我们没有必要在kubernetes集群内部再部署一个，而且这样还难于管理，因此我们考虑在容器里部署logstash收集日志到已有的ElasticSearch集群中。
方案选择 Kubernetes官方提供了EFK的日志收集解决方案，但是这种方案并不适合所有的业务场景，它本身就有一些局限性，例如：
 所有日志都必须是out前台输出，真实业务场景中无法保证所有日志都在前台输出 只能有一个日志输出文件，而真实业务场景中往往有多个日志输出文件 Fluentd并不是常用的日志收集工具，我们更习惯用logstash 我们已经有自己的ELK集群且有专人维护，没有必要再在kubernetes上做一个日志收集服务  基于以上几个原因，我们决定使用自己的ELK集群。
Kubernetes集群中的日志收集解决方案
   编号 方案 优点 缺点     1 每个app的镜像中都集成日志收集组件 部署方便，kubernetes的yaml文件无须特别配置，可以为每个app自定义日志收集配置 强耦合，不方便应用和日志收集组件升级和维护且会导致镜像过大   2 单独创建一个日志收集组件跟app的容器一起运行在同一个pod中 低耦合，扩展性强，方便维护和升级 需要对kubernetes的yaml文件进行单独配置，略显繁琐   3 将所有的Pod的日志都挂载到宿主机上，每台主机上单独起一个日志收集Pod 完全解耦，性能最高，管理起来最方便 需要统一日志收集规则，目录和输出方式    综合以上优缺点，我们选择使用方案二。
该方案在扩展性、个性化、部署和后期维护方面都能做到均衡，因此选择该方案。
我们创建了自己的logstash镜像。创建过程和使用方式见https://github.com/rootsongjc/docker-images
镜像地址：index.tenxcloud.com/jimmy/logstash:5.3.0
测试 我们部署一个应用对logstash的日志收集功能进行测试。
创建应用yaml文件logstash-test.yaml。
apiVersion: extensions/v1beta1 kind: Deployment metadata: name: logstash-test namespace: default spec: replicas: 3 template: metadata: labels: k8s-app: logstash-test spec: containers: - image: sz-pg-oam-docker-hub-001.</description>
    </item>
    
    <item>
      <title>Kubernete概念解析之Deployment</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-concept-deployment/</link>
      <pubDate>Sat, 13 May 2017 00:46:37 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-concept-deployment/</guid>
      <description>（题图：京广桥@北京国贸 Apr 30,2017）
前言 本文同步更新到Github仓库kubernetes-handbook中。
本文翻译自kubernetes官方文档：https://github.com/kubernetes/kubernetes.github.io/blob/master/docs/concepts/workloads/controllers/deployment.md
本文章根据2017年5月10日的Commit 8481c02翻译。
Deployment是Kubernetes中的一个非常重要的概念，从它开始是了解kubernetes中资源概念的一个很好的切入点，看到网上也没什么详细的说明文档，我就随手翻译了一下官方文档（Github中的文档），kubernetes官网上的文档还没有这个新。这篇文章对Deployment的概念解释的面面俱到十分详尽。
Deployment是什么？ Deployment为Pod和Replica Set（下一代Replication Controller）提供声明式更新。
你只需要在Deployment中描述你想要的目标状态是什么，Deployment controller就会帮你将Pod和Replica Set的实际状态改变到你的目标状态。你可以定义一个全新的Deployment，也可以创建一个新的替换旧的Deployment。
一个典型的用例如下：
 使用Deployment来创建ReplicaSet。ReplicaSet在后台创建pod。检查启动状态，看它是成功还是失败。 然后，通过更新Deployment的PodTemplateSpec字段来声明Pod的新状态。这会创建一个新的ReplicaSet，Deployment会按照控制的速率将pod从旧的ReplicaSet移动到新的ReplicaSet中。 如果当前状态不稳定，回滚到之前的Deployment revision。每次回滚都会更新Deployment的revision。 扩容Deployment以满足更高的负载。 暂停Deployment来应用PodTemplateSpec的多个修复，然后恢复上线。 根据Deployment 的状态判断上线是否hang住了。 清楚旧的不必要的ReplicaSet。  创建Deployment 下面是一个Deployment示例，它创建了一个Replica Set来启动3个nginx pod。
下载示例文件并执行命令：
$ kubectl create -f docs/user-guide/nginx-deployment.yaml --record deployment &amp;quot;nginx-deployment&amp;quot; created  将kubectl的 —record 的flag设置为 true可以在annotation中记录当前命令创建或者升级了该资源。这在未来会很有用，例如，查看在每个Deployment revision中执行了哪些命令。
然后立即执行getí将获得如下结果：
$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 0 0 0 1s  输出结果表明我们希望的repalica数是3（根据deployment中的.spec.replicas配置）当前replica数（ .status.replicas）是0, 最新的replica数（.status.updatedReplicas）是0，可用的replica数（.status.availableReplicas）是0。</description>
    </item>
    
    <item>
      <title>Kubernetes中的Rolling Update服务滚动升级</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-service-rolling-update/</link>
      <pubDate>Wed, 10 May 2017 17:14:20 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-service-rolling-update/</guid>
      <description>（题图：后海夜色 Apr 30,2017）
前言 本文已同步到gitbook kubernetes-handbook的第8章第1节。
本文说明在Kubernetes1.6中服务如何滚动升级，并对其进行测试。
当有镜像发布新版本，新版本服务上线时如何实现服务的滚动和平滑升级？
如果你使用ReplicationController创建的pod可以使用kubectl rollingupdate命令滚动升级，如果使用的是Deployment创建的Pod可以直接修改yaml文件后执行kubectl apply即可。
Deployment已经内置了RollingUpdate strategy，因此不用再调用kubectl rollingupdate命令，升级的过程是先创建新版的pod将流量导入到新pod上后销毁原来的旧的pod。
Rolling Update适用于Deployment、Replication Controller，官方推荐使用Deployment而不再使用Replication Controller。
使用ReplicationController时的滚动升级请参考官网说明：https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/
ReplicationController与Deployment的关系 ReplicationController和Deployment的RollingUpdate命令有些不同，但是实现的机制是一样的，关于这两个kind的关系我引用了ReplicationController与Deployment的区别中的部分内容如下，详细区别请查看原文。
ReplicationController Replication Controller为Kubernetes的一个核心内容，应用托管到Kubernetes之后，需要保证应用能够持续的运行，Replication Controller就是这个保证的key，主要的功能如下：
 确保pod数量：它会确保Kubernetes中有指定数量的Pod在运行。如果少于指定数量的pod，Replication Controller会创建新的，反之则会删除掉多余的以保证Pod数量不变。 确保pod健康：当pod不健康，运行出错或者无法提供服务时，Replication Controller也会杀死不健康的pod，重新创建新的。 弹性伸缩 ：在业务高峰或者低峰期的时候，可以通过Replication Controller动态的调整pod的数量来提高资源的利用率。同时，配置相应的监控功能（Hroizontal Pod Autoscaler），会定时自动从监控平台获取Replication Controller关联pod的整体资源使用情况，做到自动伸缩。 滚动升级：滚动升级为一种平滑的升级方式，通过逐步替换的策略，保证整体系统的稳定，在初始化升级的时候就可以及时发现和解决问题，避免问题不断扩大。  Deployment Deployment同样为Kubernetes的一个核心内容，主要职责同样是为了保证pod的数量和健康，90%的功能与Replication Controller完全一样，可以看做新一代的Replication Controller。但是，它又具备了Replication Controller之外的新特性：
 Replication Controller全部功能：Deployment继承了上面描述的Replication Controller全部功能。 事件和状态查看：可以查看Deployment的升级详细进度和状态。 回滚：当升级pod镜像或者相关参数的时候发现问题，可以使用回滚操作回滚到上一个稳定的版本或者指定的版本。 版本记录: 每一次对Deployment的操作，都能保存下来，给予后续可能的回滚使用。 暂停和启动：对于每一次升级，都能够随时暂停和启动。 多种升级方案：Recreate：删除所有已存在的pod,重新创建新的; RollingUpdate：滚动升级，逐步替换的策略，同时滚动升级时，支持更多的附加参数，例如设置最大不可用pod数量，最小升级间隔时间等等。  创建测试镜像 我们来创建一个特别简单的web服务，当你访问网页时，将输出一句版本信息。通过区分这句版本信息输出我们就可以断定升级是否完成。
所有配置和代码见Github上的manifests/test/rolling-update-test目录。
Web服务的代码main.go
package main import ( &amp;quot;fmt&amp;quot; &amp;quot;log&amp;quot; &amp;quot;net/http&amp;quot; ) func sayhello(w http.</description>
    </item>
    
    <item>
      <title>Kubernetes的边缘节点配置</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-edge-node-configuration/</link>
      <pubDate>Tue, 09 May 2017 12:59:19 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-edge-node-configuration/</guid>
      <description>（题图：南屏晚钟@圆明园 May 6,2017）
前言 为了配置kubernetes中的traefik ingress的高可用，对于kubernetes集群以外只暴露一个访问入口，需要使用keepalived排除单点问题。本文参考了kube-keepalived-vip，但并没有使用容器方式安装，而是直接在node节点上安装。
本文已同步到gitbook kubernetes-handbook的第2章第5节。
定义 首先解释下什么叫边缘节点（Edge Node），所谓的边缘节点即集群内部用来向集群外暴露服务能力的节点，集群外部的服务通过该节点来调用集群内部的服务，边缘节点是集群内外交流的一个Endpoint。
边缘节点要考虑两个问题
 边缘节点的高可用，不能有单点故障，否则整个kubernetes集群将不可用 对外的一致暴露端口，即只能有一个外网访问IP和端口  架构 为了满足边缘节点的以上需求，我们使用keepalived来实现。
在Kubernetes集群外部配置nginx来访问边缘节点的VIP。
选择Kubernetes的三个node作为边缘节点，并安装keepalived。
准备 复用kubernetes测试集群的三台主机。
172.20.0.113
172.20.0.114
172.20.0.115
安装 使用keepalived管理VIP，VIP是使用IPVS创建的，IPVS已经成为linux内核的模块，不需要安装
LVS的工作原理请参考：http://www.cnblogs.com/codebean/archive/2011/07/25/2116043.html
不使用镜像方式安装了，直接手动安装，指定三个节点为边缘节点（Edge node）。
因为我们的测试集群一共只有三个node，所有在在三个node上都要安装keepalived和ipvsadmin。
yum install keepalived ipvsadm  配置说明 需要对原先的traefik ingress进行改造，从以Deployment方式启动改成DeamonSet。还需要指定一个与node在同一网段的IP地址作为VIP，我们指定成172.20.0.119，配置keepalived前需要先保证这个IP没有被分配。。
 Traefik以DaemonSet的方式启动 通过nodeSelector选择边缘节点 通过hostPort暴露端口 当前VIP漂移到了172.20.0.115上 Traefik根据访问的host和path配置，将流量转发到相应的service上  配置keepalived 参考基于keepalived 实现VIP转移，lvs，nginx的高可用，配置keepalived。
keepalived的官方配置文档见：http://keepalived.org/pdf/UserGuide.pdf
配置文件/etc/keepalived/keepalived.conf文件内容如下：
! Configuration File for keepalived global_defs { notification_email { root@localhost } notification_email_from kaadmin@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id LVS_DEVEL } vrrp_instance VI_1 { state MASTER interface eth0 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 172.</description>
    </item>
    
    <item>
      <title>在kubernetes中使用glusterfs做持久化存储</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-with-glusterfs/</link>
      <pubDate>Thu, 04 May 2017 20:06:18 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-with-glusterfs/</guid>
      <description>（题图：无题@北京奥林匹克森林公园 May 1,2017）
前言 本文章已同步到kubernetes-handbook 7.1章节。
Kubernetes集群沿用跟我一起部署kubernetes1.6集群中的三台机器。
我们复用kubernetes集群的这三台主机做glusterfs存储。
以下步骤参考自：https://www.xf80.com/2017/04/21/kubernetes-glusterfs/
安装glusterfs 我们直接在物理机上使用yum安装，如果你选择在kubernetes上安装，请参考：https://github.com/gluster/gluster-kubernetes/blob/master/docs/setup-guide.md
# 先安装 gluster 源 $ yum install centos-release-gluster -y # 安装 glusterfs 组件 $ yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel ## 创建 glusterfs 目录 $ mkdir /opt/glusterd ## 修改 glusterd 目录 $ sed -i &#39;s/var\/lib/opt/g&#39; /etc/glusterfs/glusterd.vol # 启动 glusterfs $ systemctl start glusterd.service # 设置开机启动 $ systemctl enable glusterd.service #查看状态 $ systemctl status glusterd.service  配置 glusterfs # 配置 hosts $ vi /etc/hosts 172.</description>
    </item>
    
    <item>
      <title>Kubernetes网络和集群性能测试</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-performance-test/</link>
      <pubDate>Tue, 25 Apr 2017 22:14:49 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-performance-test/</guid>
      <description>（题图：无题@安贞门 Jun 18,2016）
前言 该测试是为了测试在不同的场景下，访问kubernetes的延迟以及kubernetes的性能。进行以下测试前，你需要有一个部署好的kubernetes集群，关于如何部署kuberentes1.6集群，请参考kubernetes-handbook。
准备 测试环境
在以下几种环境下进行测试：
 Kubernetes集群node节点上通过Cluster IP方式访问 Kubernetes集群内部通过service访问 Kubernetes集群外部通过traefik ingress暴露的地址访问  测试地址
Cluster IP: 10.254.149.31
Service Port：8000
Ingress Host：traefik.sample-webapp.io
测试工具
 Locust：一个简单易用的用户负载测试工具，用来测试web或其他系统能够同时处理的并发用户数。 curl kubemark 测试程序：sample-webapp，源码见Github kubernetes的分布式负载测试  测试说明
通过向sample-webapp发送curl请求获取响应时间，直接curl后的结果为：
$ curl &amp;quot;http://10.254.149.31:8000/&amp;quot; Welcome to the &amp;quot;Distributed Load Testing Using Kubernetes&amp;quot; sample web app  网络延迟测试 场景一、 Kubernetes集群node节点上通过Cluster IP访问 测试命令
curl -o /dev/null -s -w &#39;%{time_connect} %{time_starttransfer} %{time_total}&#39; &amp;quot;http://10.254.149.31:8000/&amp;quot;  10组测试结果
   No time_connect time_starttransfer time_total     1 0.</description>
    </item>
    
    <item>
      <title>运用kubernetes进行分布式负载测试</title>
      <link>http://rootsongjc.github.io/blogs/distributed-load-testing-using-kubernetes/</link>
      <pubDate>Mon, 24 Apr 2017 21:32:52 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/distributed-load-testing-using-kubernetes/</guid>
      <description>（题图：Kubrick Book Store Mar 25,2016）
前言 Github地址https://github.com/rootsongjc/distributed-load-testing-using-kubernetes
该教程描述如何在Kubernetes中进行分布式负载均衡测试，包括一个web应用、docker镜像和Kubernetes controllers/services。更多资料请查看Distributed Load Testing Using Kubernetes 。
注意：该测试是在我自己本地搭建的kubernetes集群上测试的，不需要使用Google Cloud Platform。
准备 不需要GCE及其他组件，你只需要有一个kubernetes集群即可。
如果你还没有kubernetes集群，可以参考kubernetes-handbook部署一个。
部署Web应用 sample-webapp 目录下包含一个简单的web测试应用。我们将其构建为docker镜像，在kubernetes中运行。你可以自己构建，也可以直接用这个我构建好的镜像index.tenxcloud.com/jimmy/k8s-sample-webapp:latest。
在kubernetes上部署sample-webapp。
$ cd kubernetes-config $ kubectl create -f sample-webapp-controller.yaml $ kubectl create -f kubectl create -f sample-webapp-service.yaml  部署Locust的Controller和Service locust-master和locust-work使用同样的docker镜像，修改cotnroller中spec.template.spec.containers.env字段中的value为你sample-webapp service的名字。
- name: TARGET_HOST value: http://sample-webapp:8000  创建Controller Docker镜像（可选） locust-master和locust-work controller使用的都是locust-tasks docker镜像。你可以直接下载gcr.io/cloud-solutions-http://olz1di9xf.bkt.clouddn.com/locust-tasks，也可以自己编译。自己编译大概要花几分钟时间，镜像大小为820M。
$ docker build -t index.tenxcloud.com/jimmy/locust-tasks:latest . $ docker push index.tenxcloud.com/jimmy/locust-tasks:latest  注意：我使用的是时速云的镜像仓库。
每个controller的yaml的spec.template.spec.containers.image 字段指定的是我的镜像：</description>
    </item>
    
    <item>
      <title>Kubernetes中的IP和服务发现体系</title>
      <link>http://rootsongjc.github.io/blogs/ip-and-service-discovry-in-kubernetes/</link>
      <pubDate>Mon, 24 Apr 2017 16:11:16 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/ip-and-service-discovry-in-kubernetes/</guid>
      <description>（题图：路边的野花@朝阳公园 Nov 8,2015）
Cluster IP 即Service的IP，通常在集群内部使用Service Name来访问服务，用户不需要知道该IP地址，kubedns会自动根据service name解析到服务的IP地址，将流量分发给Pod。
Service Name才是对外暴露服务的关键。
在kubeapi的配置中指定该地址范围。
默认配置
--service-cluster-ip-range=10.254.0.0/16 --service-node-port-range=30000-32767  Pod IP 通过配置flannel的network和subnet来实现。
默认配置
FLANNEL_NETWORK=172.30.0.0/16 FLANNEL_SUBNET=172.30.46.1/24  Pod的IP地址不固定，当pod重启时IP地址会变化。
该IP地址也是用户无需关心的。
但是Flannel会在本地生成相应IP段的虚拟网卡，为了防止和集群中的其他IP地址冲突，需要规划IP段。
主机/Node IP 物理机的IP地址，即kubernetes管理的物理机的IP地址。
$ kubectl get nodes NAME STATUS AGE VERSION 172.20.0.113 Ready 12d v1.6.0 172.20.0.114 Ready 12d v1.6.0 172.20.0.115 Ready 12d v1.6.0  服务发现 集群内部的服务发现
通过DNS即可发现，kubends是kubernetes的一个插件，不同服务之间可以直接使用service name访问。
通过sericename:port即可调用服务。
服务外部的服务发现
通过Ingress来实现，我们是用的Traefik来实现。
参考 Ingress解析
Kubernetes Traefik Ingress安装试用</description>
    </item>
    
    <item>
      <title>Kubernetes中的RBAC支持</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-rbac-support/</link>
      <pubDate>Fri, 21 Apr 2017 19:53:18 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-rbac-support/</guid>
      <description>（题图：无题 Apr 2,2016）
 在Kubernetes1.6版本中新增角色访问控制机制（Role-Based Access，RBAC）让集群管理员可以针对特定使用者或服务账号的角色，进行更精确的资源访问控制。在RBAC中，权限与角色相关联，用户通过成为适当角色的成员而得到这些角色的权限。这就极大地简化了权限的管理。在一个组织中，角色是为了完成各种工作而创造，用户则依据它的责任和资格来被指派相应的角色，用户可以很容易地从一个角色被指派到另一个角色。
 前言 本文翻译自RBAC Support in Kubernetes，转载自kubernetes中文社区，译者催总，Jimmy Song做了稍许修改。该文章是5天内了解Kubernetes1.6新特性的系列文章之一。
One of the highlights of the Kubernetes 1.6中的一个亮点时RBAC访问控制机制升级到了beta版本。RBAC，基于角色的访问控制机制，是用来管理kubernetes集群中资源访问权限的机制。使用RBAC可以很方便的更新访问授权策略而不用重启集群。
本文主要关注新特性和最佳实践。
RBAC vs ABAC 目前kubernetes中已经有一系列l 鉴权机制。鉴权的作用是，决定一个用户是否有权使用 Kubernetes API 做某些事情。它除了会影响 kubectl 等组件之外，还会对一些运行在集群内部并对集群进行操作的软件产生作用，例如使用了 Kubernetes 插件的 Jenkins，或者是利用 Kubernetes API 进行软件部署的 Helm。ABAC 和 RBAC 都能够对访问策略进行配置。
ABAC（Attribute Based Access Control）本来是不错的概念，但是在 Kubernetes 中的实现比较难于管理和理解，而且需要对 Master 所在节点的 SSH 和文件系统权限，而且要使得对授权的变更成功生效，还需要重新启动 API Server。
而 RBAC 的授权策略可以利用 kubectl 或者 Kubernetes API 直接进行配置。RBAC 可以授权给用户，让用户有权进行授权管理，这样就可以无需接触节点，直接进行授权管理。RBAC 在 Kubernetes 中被映射为 API 资源和操作。
因为 Kubernetes 社区的投入和偏好，相对于 ABAC 而言，RBAC 是更好的选择。</description>
    </item>
    
    <item>
      <title>Kubernetes traefik ingress安装试用</title>
      <link>http://rootsongjc.github.io/blogs/traefik-ingress-installation/</link>
      <pubDate>Thu, 20 Apr 2017 22:38:40 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/traefik-ingress-installation/</guid>
      <description>（题图：🐟@鱼缸 Sep 15,2016）
前言 昨天翻了下Ingress解析，然后安装试用了下traefik，过程已同步到kubernetes-handbook上，Github地址https://github.com/rootsongjc/kubernetes-handbook
Ingress简介 如果你还不了解，ingress是什么，可以先看下我翻译的Kubernetes官网上ingress的介绍Kubernetes Ingress解析。
理解Ingress
简单的说，ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到不同的service上。Ingress相当于nginx、apache等负载均衡方向代理服务器，其中还包括规则定义，即URL的路由信息，路由信息得的刷新由Ingress controller来提供。
理解Ingress Controller
Ingress Controller 实质上可以理解为是个监视器，Ingress Controller 通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少 pod，service 增加与减少等；当得到这些变化信息后，Ingress Controller 再结合下文的 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的作用。
部署Traefik 介绍traefik
Traefik是一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，可以实现自动化动态配置。目前支持Docker, Swarm, Mesos/Marathon, Mesos, Kubernetes, Consul, Etcd, Zookeeper, BoltDB, Rest API等等后端模型。
以下配置文件可以在kubernetes-handbookGitHub仓库中的manifests/traefik-ingress/目录下找到。
创建ingress-rbac.yaml
将用于service account验证。
apiVersion: v1 kind: ServiceAccount metadata: name: ingress namespace: kube-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: ingress subjects: - kind: ServiceAccount name: ingress namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.</description>
    </item>
    
    <item>
      <title>Kubernetes ingress解析</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-ingress-resource/</link>
      <pubDate>Wed, 19 Apr 2017 21:05:47 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-ingress-resource/</guid>
      <description>（题图：朝阳门银河SOHO Jan 31,2016）
前言 这是kubernete官方文档中Ingress Resource的翻译，因为最近工作中用到，文章也不长，也很好理解，索性翻译一下，也便于自己加深理解，同时造福kubernetes中文社区。后续准备使用Traefik来做Ingress controller，文章末尾给出了几个相关链接，实际使用案例正在摸索中，届时相关安装文档和配置说明将同步更新到kubernetes-handbook中。
术语
在本篇文章中你将会看到一些在其他地方被交叉使用的术语，为了防止产生歧义，我们首先来澄清下。
 节点：Kubernetes集群中的一台物理机或者虚拟机。 集群：位于Internet防火墙后的节点，这是kubernetes管理的主要计算资源。
 边界路由器：为集群强制执行防火墙策略的路由器。 这可能是由云提供商或物理硬件管理的网关。
 集群网络：一组逻辑或物理链接，可根据Kubernetes网络模型实现群集内的通信。 集群网络的实现包括Overlay模型的 flannel 和基于SDN的OVS。
 服务：使用标签选择器标识一组pod成为的Kubernetes服务。 除非另有说明，否则服务假定在集群网络内仅可通过虚拟IP访问。
  什么是Ingress？ 通常情况下，service和pod仅可在集群内部网络中通过IP地址访问。所有到达边界路由器的流量或被丢弃或被转发到其他地方。从概念上讲，可能像下面这样：
 internet | ------------ [ Services ]  Ingress是授权入站连接到达集群服务的规则集合。
 internet | [ Ingress ] --|-----|-- [ Services ]  你可以给Ingress配置提供外部可访问的URL、负载均衡、SSL、基于名称的虚拟主机等。用户通过POST Ingress资源到API server的方式来请求ingress。 Ingress controller负责实现Ingress，通常使用负载平衡器，它还可以配置边界路由和其他前端，这有助于以HA方式处理流量。
先决条件 在使用Ingress resource之前，有必要先了解下面几件事情。Ingress是beta版本的resource，在kubernetes1.1之前还没有。你需要一个Ingress Controller来实现Ingress，单纯的创建一个Ingress没有任何意义。
GCE/GKE会在master节点上部署一个ingress controller。你可以在一个pod中部署任意个自定义的ingress controller。你必须正确地annotate每个ingress，比如 运行多个ingress controller 和 关闭glbc.
确定你已经阅读了Ingress controller的beta版本限制。在非GCE/GKE的环境中，你需要在pod中部署一个controller。
Ingress Resource 最简化的Ingress配置：
1: apiVersion: extensions/v1beta1 2: kind: Ingress 3: metadata: 4: name: test-ingress 5: spec: 6: rules: 7: - http: 8: paths: 9: - path: /testpath 10: backend: 11: serviceName: test 12: servicePort: 80  如果你没有配置Ingress controller就将其POST到API server不会有任何用处</description>
    </item>
    
    <item>
      <title>Kubernetes Handbook发起</title>
      <link>http://rootsongjc.github.io/projects/kubernetes-handbook-startup/</link>
      <pubDate>Fri, 14 Apr 2017 19:33:27 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/projects/kubernetes-handbook-startup/</guid>
      <description>（题图：地坛公园 Sep 27,2015）
玩转Kubernetes，我就看kubernetes handbook！
GitHub地址：https://github.com/rootsongjc/kubernetes-handbook
文章同步更新到gitbook，方便大家浏览和下载PDF。
说明 文中涉及的配置文件和代码链接在gitbook中会无法打开，请下载github源码后，在MarkDown编辑器中打开，点击链接将跳转到你的本地目录。
如何使用 方式一：在线浏览
访问gitbook：https://www.gitbook.com/book/rootsongjc/kubernetes-handbook/
方式二：本地查看
 将代码克隆到本地 安装gitbook：Setup and Installation of GitBook 执行gitbook serve 在浏览器中访问http://localhost:4000  P.S 本书中也将记录业界动态和经验分享，欢迎分享你的独到见解。
加入Kubernetes交流群，请添加我微信jimmysong。</description>
    </item>
    
    <item>
      <title>Kubernetes1.6集群部署完全指南 ——二进制文件部署开启TLS基于CentOS7发布</title>
      <link>http://rootsongjc.github.io/projects/kubernetes-installation-document/</link>
      <pubDate>Thu, 13 Apr 2017 14:00:04 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/projects/kubernetes-installation-document/</guid>
      <description>（题图：清晨@首都机场 Aug 13,2016）
这可能是目前为止最详细的kubernetes安装文档了。
经过几天的安装、调试、整理，今天该文档终于发布了。
你可以在这里看到文档和配置文件和我一步步部署 kubernetes1.6 集群。
或者直接下载pdf版本（2.92M）。
Kubernetes的安装繁琐，步骤复杂，该文档能够帮助跳过很多坑，节约不少时间，我在本地环境上已经安装完成，有问题欢迎在GitHub上提issue。
安装的集群详情
 Kubernetes 1.6.0 Docker 1.12.5（使用yum安装） Etcd 3.1.5 Flanneld 0.7 vxlan 网络 TLS 认证通信 (所有组件，如 etcd、kubernetes master 和 node) RBAC 授权 kublet TLS BootStrapping kubedns、dashboard、heapster(influxdb、grafana)、EFK(elasticsearch、fluentd、kibana) 集群插件 私有docker镜像仓库harbor（请自行部署，harbor提供离线安装包，直接使用docker-compose启动即可）  该文档中包括以下几个步骤
 创建 TLS 通信所需的证书和秘钥 创建 kubeconfig 文件 创建三节点的高可用 etcd 集群 kubectl命令行工具 部署高可用 master 集群 部署 node 节点 kubedns 插件 Dashboard 插件 Heapster 插件 EFK 插件  </description>
    </item>
    
    <item>
      <title>在开启TLS的Kubernetes1.6集群上安装EFK</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-efk-installation-with-tls/</link>
      <pubDate>Thu, 13 Apr 2017 12:28:10 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-efk-installation-with-tls/</guid>
      <description>（题图：簋街 Jun 17,2016）
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，在开启了TLS验证的集群中部署EFK日志收集监控插件。
配置和安装 EFK 官方文件目录：cluster/addons/fluentd-elasticsearch
$ ls *.yaml es-controller.yaml es-service.yaml fluentd-es-ds.yaml kibana-controller.yaml kibana-service.yaml efk-rbac.yaml  同样EFK服务也需要一个efk-rbac.yaml文件，配置serviceaccount为efk。
已经修改好的 yaml 文件见：EFK
配置 es-controller.yaml $ diff es-controller.yaml.orig es-controller.yaml 24c24 &amp;lt; - image: gcr.io/google_containers/elasticsearch:v2.4.1-2 --- &amp;gt; - image: sz-pg-oam-docker-hub-001.tendcloud.com/library/elasticsearch:v2.4.1-2  配置 es-service.yaml 无需配置；
配置 fluentd-es-ds.yaml $ diff fluentd-es-ds.yaml.orig fluentd-es-ds.yaml 26c26 &amp;lt; image: gcr.io/google_containers/fluentd-elasticsearch:1.22 --- &amp;gt; image: sz-pg-oam-docker-hub-001.tendcloud.com/library/fluentd-elasticsearch:1.22  配置 kibana-controller.yaml $ diff kibana-controller.yaml.orig kibana-controller.yaml 22c22 &amp;lt; image: gcr.io/google_containers/kibana:v4.6.1-1 --- &amp;gt; image: sz-pg-oam-docker-hub-001.</description>
    </item>
    
    <item>
      <title>在开启TLS的Kubernetes1.6集群上安装heapster</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-heapster-installation-with-tls/</link>
      <pubDate>Wed, 12 Apr 2017 20:20:19 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-heapster-installation-with-tls/</guid>
      <description>（题图：大喵 Aug 8,2016）
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，在开启了TLS验证的集群中部署heapster，包括influxdb、grafana等组件。
配置和安装Heapster 到 heapster release 页面 下载最新版本的 heapster。
$ wget https://github.com/kubernetes/heapster/archive/v1.3.0.zip $ unzip v1.3.0.zip $ mv v1.3.0.zip heapster-1.3.0  文件目录： heapster-1.3.0/deploy/kube-config/influxdb
$ cd heapster-1.3.0/deploy/kube-config/influxdb $ ls *.yaml grafana-deployment.yaml grafana-service.yaml heapster-deployment.yaml heapster-service.yaml influxdb-deployment.yaml influxdb-service.yaml heapster-rbac.yaml  我们自己创建了heapster的rbac配置heapster-rbac.yaml。
已经修改好的 yaml 文件见：heapster
配置 grafana-deployment $ diff grafana-deployment.yaml.orig grafana-deployment.yaml 16c16 &amp;lt; image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 --- &amp;gt; image: sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-grafana-amd64:v4.0.2 40,41c40,41 &amp;lt; # value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/ &amp;lt; value: / --- &amp;gt; value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/ &amp;gt; #value: /  如果后续使用 kube-apiserver 或者 kubectl proxy 访问 grafana dashboard，则必须将 GF_SERVER_ROOT_URL 设置为 /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/，否则后续访问grafana时访问时提示找不到http://10.</description>
    </item>
    
    <item>
      <title>在开启TLS的Kubernetes1.6集群上安装dashboard</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation-with-tls/</link>
      <pubDate>Wed, 12 Apr 2017 15:53:39 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation-with-tls/</guid>
      <description>（题图：东直门桥 Aug 20,2016）
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，在开启了TLS验证的集群中部署dashboard。
感谢opsnull和ipchy的细心解答。
安装环境配置信息
 CentOS 7.2.1511 Docker 1.12.5 Flannel 0.7 Kubernetes 1.6.0  配置和安装 dashboard 官方文件目录：kubernetes/cluster/addons/dashboard
我们使用的文件
$ ls *.yaml dashboard-controller.yaml dashboard-service.yaml dashboard-rbac.yaml  已经修改好的 yaml 文件见：dashboard
由于 kube-apiserver 启用了 RBAC 授权，而官方源码目录的 dashboard-controller.yaml 没有定义授权的 ServiceAccount，所以后续访问 kube-apiserver 的 API 时会被拒绝，web中提示：
Forbidden (403) User &amp;quot;system:serviceaccount:kube-system:default&amp;quot; cannot list jobs.batch in the namespace &amp;quot;default&amp;quot;. (get jobs.batch)  增加了一个dashboard-rbac.yaml文件，定义一个名为 dashboard 的 ServiceAccount，然后将它和 Cluster Role view 绑定。
配置dashboard-service $ diff dashboard-service.yaml.orig dashboard-service.</description>
    </item>
    
    <item>
      <title>Kubernetes安装之kubedns配置</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-kubedns-installation/</link>
      <pubDate>Wed, 12 Apr 2017 13:04:45 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-kubedns-installation/</guid>
      <description>（题图：雨过天晴@北京定福庄 Aug 27,2016）
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，使用yaml文件部署kubedns。
安装环境配置信息
 CentOS 7.2.1511 Docker 1.12.5 Flannel 0.7 Kubernetes 1.6.0  安装和配置 kubedns 插件 官方的yaml文件目录：kubernetes/cluster/addons/dns。
该插件直接使用kubernetes部署，官方的配置文件中包含以下镜像：
gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.1 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.1 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1  我clone了上述镜像，上传到我的私有镜像仓库：
sz-pg-oam-docker-hub-001.tendcloud.com/library/k8s-dns-dnsmasq-nanny-amd64:1.14.1 sz-pg-oam-docker-hub-001.tendcloud.com/library/k8s-dns-kube-dns-amd64:1.14.1 sz-pg-oam-docker-hub-001.tendcloud.com/library/k8s-dns-sidecar-amd64:1.14.1  同时上传了一份到时速云备份：
index.tenxcloud.com/jimmy/k8s-dns-dnsmasq-nanny-amd64:1.14.1 index.tenxcloud.com/jimmy/k8s-dns-kube-dns-amd64:1.14.1 index.tenxcloud.com/jimmy/k8s-dns-sidecar-amd64:1.14.1  以下yaml配置文件中使用的是私有镜像仓库中的镜像。
kubedns-cm.yaml kubedns-sa.yaml kubedns-controller.yaml kubedns-svc.yaml  已经修改好的 yaml 文件见：github项目中的manifest/kubedns/目录。
系统预定义的 RoleBinding 预定义的 RoleBinding system:kube-dns 将 kube-system 命名空间的 kube-dns ServiceAccount 与 system:kube-dns Role 绑定， 该 Role 具有访问 kube-apiserver DNS 相关 API 的权限；
$ kubectl get clusterrolebindings system:kube-dns -o yaml apiVersion: rbac.</description>
    </item>
    
    <item>
      <title>Kubernetes node节点安装</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-node-installation/</link>
      <pubDate>Tue, 11 Apr 2017 22:20:31 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-node-installation/</guid>
      <description>（题图：太阳宫桥@北京东北三环 Dec 11,2016）
前言 这是和我一步步部署kubernetes集群项目(fork自opsnull)中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，部署node节点上的kube-proxy和kubelet，同时对之前部署的flannel改造。
安装环境配置信息
 CentOS7.2.1511 Docker 1.12.5 Flannel 0.7 Kubernetes 1.6.0  部署kubernetes node节点 kubernetes node 节点包含如下组件：
 Flanneld：参考我之前写的文章Kubernetes基于Flannel的网络配置，之前没有配置TLS，现在需要在serivce配置文件中增加TLS配置。 Docker1.12.5：docker的安装很简单，这里也不说了。 kubelet kube-proxy  下面着重讲kubelet和kube-proxy的安装，同时还要将之前安装的flannel集成TLS验证。
目录和文件 我们再检查一下三个节点上，经过前几步操作生成的配置文件。
$ ls /etc/kubernetes/ssl admin-key.pem admin.pem ca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem kubernetes-key.pem kubernetes.pem $ ls /etc/kubernetes/ apiserver bootstrap.kubeconfig config controller-manager kubelet kube-proxy.kubeconfig proxy scheduler ssl token.csv  配置Flanneld 参考我之前写的文章Kubernetes基于Flannel的网络配置，之前没有配置TLS，现在需要在serivce配置文件中增加TLS配置。
service配置文件/usr/lib/systemd/system/flanneld.service。
[Unit] Description=Flanneld overlay address etcd agent After=network.target After=network-online.target Wants=network-online.target After=etcd.service Before=docker.service [Service] Type=notify EnvironmentFile=/etc/sysconfig/flanneld EnvironmentFile=-/etc/sysconfig/docker-network ExecStart=/usr/bin/flanneld-start $FLANNEL_OPTIONS ExecStartPost=/usr/libexec/flannel/mk-docker-opts.</description>
    </item>
    
    <item>
      <title>Kubernetes高可用master节点安装</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-ha-master-installation/</link>
      <pubDate>Tue, 11 Apr 2017 19:55:56 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-ha-master-installation/</guid>
      <description>（题图：鬼见愁@北京西山 Sep 14,2015）
前言 这是和我一步步部署kubernetes集群项目((fork自opsnull))中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，部署master节点的kube-apiserver、kube-controller-manager和kube-scheduler的过程。
高可用kubernetes master节点安装 kubernetes master 节点包含的组件：
 kube-apiserver kube-scheduler kube-controller-manager  目前这三个组件需要部署在同一台机器上。
 kube-scheduler、kube-controller-manager 和 kube-apiserver 三者的功能紧密相关； 同时只能有一个 kube-scheduler、kube-controller-manager 进程处于工作状态，如果运行多个，则需要通过选举产生一个 leader；  本文档记录部署一个三个节点的高可用 kubernetes master 集群步骤。（后续创建一个 load balancer 来代理访问 kube-apiserver 的请求）
TLS 证书文件 pem和token.csv证书文件我们在TLS证书和秘钥这一步中已经创建过了。我们再检查一下。
$ ls /etc/kubernetes/ssl admin-key.pem admin.pem ca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem kubernetes-key.pem kubernetes.pem  下载最新版本的二进制文件 有两种下载方式
方式一
从 github release 页面 下载发布版 tarball，解压后再执行下载脚本
$ wget https://github.com/kubernetes/kubernetes/releases/download/v1.6.0/kubernetes.tar.gz $ tar -xzvf kubernetes.tar.gz ... $ cd kubernetes $ .</description>
    </item>
    
    <item>
      <title>Kubernetes安装之etcd高可用配置</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-etcd-ha-config/</link>
      <pubDate>Tue, 11 Apr 2017 15:21:39 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-etcd-ha-config/</guid>
      <description>（题图：北京夜景@西山）
前言 这是和我一步步部署kubernetes集群项目((fork自opsnull))中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，生成kubeconfig文件的过程。
创建高可用 etcd 集群 kuberntes 系统使用 etcd 存储所有数据，本文档介绍部署一个三节点高可用 etcd 集群的步骤，这三个节点复用 kubernetes master 机器，分别命名为sz-pg-oam-docker-test-001.tendcloud.com、sz-pg-oam-docker-test-002.tendcloud.com、sz-pg-oam-docker-test-003.tendcloud.com：
 sz-pg-oam-docker-test-001.tendcloud.com：172.20.0.113 sz-pg-oam-docker-test-002.tendcloud.com：172.20.0.114 sz-pg-oam-docker-test-003.tendcloud.com：172.20.0.115  TLS 认证文件 需要为 etcd 集群创建加密通信的 TLS 证书，这里复用以前创建的 kubernetes 证书
$ cp ca.pem kubernetes-key.pem kubernetes.pem /etc/kubernetes/ssl   kubernetes 证书的 hosts 字段列表中包含上面三台机器的 IP，否则后续证书校验会失败；  下载二进制文件 到 https://github.com/coreos/etcd/releases 页面下载最新版本的二进制文件
$ https://github.com/coreos/etcd/releases/download/v3.1.5/etcd-v3.1.5-linux-amd64.tar.gz $ tar -xvf etcd-v3.1.4-linux-amd64.tar.gz $ sudo mv etcd-v3.1.4-linux-amd64/etcd* /root/local/bin  创建 etcd 的 systemd unit 文件 注意替换 ETCD_NAME 和 INTERNAL_IP 变量的值；</description>
    </item>
    
    <item>
      <title>Kubernetes安装之创建kubeconfig文件</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-create-kubeconfig/</link>
      <pubDate>Tue, 11 Apr 2017 14:34:54 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-create-kubeconfig/</guid>
      <description>(题图：北海公园 May 8,2016)
前言 这是和我一步步部署kubernetes集群项目((fork自opsnull))中的一篇文章，下文是结合我之前部署kubernetes的过程产生的kuberentes环境，生成kubeconfig文件的过程。 kubelet、kube-proxy 等 Node 机器上的进程与 Master 机器的 kube-apiserver 进程通信时需要认证和授权； kubernetes 1.4 开始支持由 kube-apiserver 为客户端生成 TLS 证书的 TLS Bootstrapping 功能，这样就不需要为每个客户端生成证书了；该功能当前仅支持为 kubelet 生成证书。
创建 TLS Bootstrapping Token Token auth file
Token可以是任意的包涵128 bit的字符串，可以使用安全的随机数发生器生成。
export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &#39; &#39;) cat &amp;gt; token.csv &amp;lt;&amp;lt;EOF ${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&amp;quot;system:kubelet-bootstrap&amp;quot; EOF   后三行是一句，直接复制上面的脚本运行即可。
 将token.csv发到所有机器（Master 和 Node）的 /etc/kubernetes/ 目录。
$cp token.csv /etc/kubernetes/  创建 kubelet bootstrapping kubeconfig 文件 $ cd /etc/kubernetes $ export KUBE_APISERVER=&amp;quot;https://172.</description>
    </item>
    
    <item>
      <title>开源微服务管理平台fabric8简介</title>
      <link>http://rootsongjc.github.io/blogs/fabric8-introduction/</link>
      <pubDate>Mon, 10 Apr 2017 21:39:00 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/fabric8-introduction/</guid>
      <description>前言 无意中发现Fabric8这个对于Java友好的开源微服务管理平台。
其实这在这里发现的Achieving CI/CD with Kubernetes（by Ramit Surana,on February 17, 2017），其实是先在slideshare上看到的，pdf可以在此下载，大小2.04M。
大家可能以前听过一个叫做fabric的工具，那是一个 Python (2.5-2.7) 库和命令行工具，用来流水线化执行 SSH 以部署应用或系统管理任务。所以大家不要把fabric8跟fabric搞混，虽然它们之间有一些共同点，但两者完全不是同一个东西，fabric8不是fabric的一个版本。Fabric是用python开发的，fabric8是java开发的。
如果你想了解简化Fabric可以看它的中文官方文档。
Fabric8简介 fabric8是一个开源集成开发平台，为基于Kubernetes和Jenkins的微服务提供持续发布。
使用fabric可以很方便的通过Continuous Delivery pipelines创建、编译、部署和测试微服务，然后通过Continuous Improvement和ChatOps运行和管理他们。
Fabric8微服务平台提供：
 Developer Console，是一个富web应用，提供一个单页面来创建、编辑、编译、部署和测试微服务。 Continuous Integration and Continous Delivery，使用 Jenkins with a Jenkins Workflow Library更快和更可靠的交付软件。 Management，集中式管理Logging、Metrics, ChatOps、Chaos Monkey，使用Hawtio和Jolokia管理Java Containers。 Integration Integration Platform As A Service with deep visualisation of your Apache Camel integration services, an API Registry to view of all your RESTful and SOAP APIs and Fabric8 MQ provides Messaging As A Service based on Apache ActiveMQ。 Java Tools 帮助Java应用使用Kubernetes:  Maven Plugin for working with Kubernetes ，这真是极好的 Integration and System Testing of Kubernetes resources easily inside JUnit with Arquillian Java Libraries and support for CDI extensions for working with Kubernetes.</description>
    </item>
    
    <item>
      <title>Kubernetes安装之证书验证</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-tls-certificate/</link>
      <pubDate>Mon, 10 Apr 2017 17:28:41 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-tls-certificate/</guid>
      <description>（题图：铜牛@颐和园 Aug 25,2014）
前言 昨晚（Apr 9,2017）金山软件的opsnull发布了一个开源项目和我一步步部署kubernetes集群，下文是结合我之前部署kubernetes的过程打造的kubernetes环境和opsnull的文章创建 kubernetes 各组件 TLS 加密通信的证书和秘钥的实践。之前安装过程中一直使用的是非加密方式，一直到后来使用Fluentd和ElasticSearch收集Kubernetes集群日志时发现有权限验证问题，所以为了深入研究kubernentes。
Kubernentes中的身份验证 kubernetes 系统的各组件需要使用 TLS 证书对通信进行加密，本文档使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 和其它证书；
生成的 CA 证书和秘钥文件如下：
 ca-key.pem ca.pem kubernetes-key.pem kubernetes.pem kube-proxy.pem kube-proxy-key.pem admin.pem admin-key.pem  使用证书的组件如下：
 etcd：使用 ca.pem、kubernetes-key.pem、kubernetes.pem； kube-apiserver：使用 ca.pem、kubernetes-key.pem、kubernetes.pem； kubelet：使用 ca.pem； kube-proxy：使用 ca.pem、kube-proxy-key.pem、kube-proxy.pem； kubectl：使用 ca.pem、admin-key.pem、admin.pem；  kube-controller、kube-scheduler 当前需要和 kube-apiserver 部署在同一台机器上且使用非安全端口通信，故不需要证书。
安装 CFSSL 方式一：直接使用二进制源码包安装
$ wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 $ chmod +x cfssl_linux-amd64 $ sudo mv cfssl_linux-amd64 /root/local/bin/cfssl $ wget https://pkg.</description>
    </item>
    
    <item>
      <title>《云计算技术架构与实践（第二版）》读后感</title>
      <link>http://rootsongjc.github.io/blogs/cloud-computing-architecture-practice/</link>
      <pubDate>Sat, 08 Apr 2017 12:29:36 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/cloud-computing-architecture-practice/</guid>
      <description>（题图：长江三峡大坝@湖北宜昌 Apr 6,2015）
前言 最近（2017年3月）友人推荐了一本书，是华为的工程师写的《云计算架构与实践第二版》，正好在网上找到了这本书的pdf，分享给大家，点这里下载，书是文字版的，大小13.04MB，除了章节顺序有点问题外没有其他什么问题。这是该书的第二版，第一版2014年9月出版，第二版2016年9月出版，第二版的编者团队居然有50人之多😓
第二版分享了华为在云计算核心竞争力构建与价值转换方面的经验与建议，并补充了业界在公有云、私有云、行业云以及电信网络云化商用落地与技术应用方面的成功优秀实践。增加了对Docker容器与微服务敏捷迭代、大数据与数据库云化、行业建模与机器学习算法、混合云与管理自动化编排、云生态建设等方面的介绍。
第1章 云计算的商业动力与技术趋势 ​
​
​
​
​
​</description>
    </item>
    
    <item>
      <title>使用Fluentd和ElasticSearch收集Kubernetes集群日志</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-fluentd-elasticsearch-installation/</link>
      <pubDate>Fri, 07 Apr 2017 20:13:24 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-fluentd-elasticsearch-installation/</guid>
      <description>（题图：码头@古北水镇 Apr 30,2016）
前言 在安装好了Kubernetes集群、配置好了flannel网络、安装了Kubernetes Dashboard和配置Heapster监控插件后，还有一项重要的工作，为了调试和故障排查，还需要进行日志收集工作。
官方文档
Kubernetes Logging and Monitoring Cluster Activity
Logging Using Elasticsearch and Kibana：不过这篇文章是在GCE上配置的，参考价值不大。
容器日志的存在形式 目前容器日志有两种输出形式：
stdout,stderr标准输出
这种形式的日志输出我们可以直接使用docker logs查看日志，kubernetes集群中同样可以使用kubectl logs类似的形式查看日志。
日志文件记录
这种日志输出我们无法从以上方法查看日志内容，只能tail日志文件查看。
Fluentd介绍 Fluentd是使用Ruby编写的，通过在后端系统之间提供统一的日志记录层来从后端系统中解耦数据源。 此层允许开发人员和数据分析人员在生成日志时使用多种类型的日志。 统一的日志记录层可以让您和您的组织更好地使用数据，并更快地在您的软件上进行迭代。 也就是说fluentd是一个面向多种数据来源以及面向多种数据出口的日志收集器。另外它附带了日志转发的功能。
Fluentd收集的event由以下几个方面组成：
 Tag：字符串，中间用点隔开，如myapp.access Time：UNIX时间格式 Record：JSON格式  Fluentd特点  部署简单灵活 开源 经过验证的可靠性和性能 社区支持，插件较多 使用json格式事件格式 可拔插的架构设计 低资源要求 内置高可靠性  安装 查看cluster/addons/fluentd-elasticsearch插件目录，获取到需要用到的docker镜像名称。
$grep -rn &amp;quot;gcr.io&amp;quot; *.yaml es-controller.yaml:24: - image: gcr.io/google_containers/elasticsearch:v2.4.1-2 fluentd-es-ds.yaml:26: image: gcr.io/google_containers/fluentd-elasticsearch:1.22 kibana-controller.yaml:22: image: gcr.io/google_containers/kibana:v4.6.1-1  需要用到的镜像
 gcr.io/google_containers/kibana:v4.6.1-1 gcr.io/google_containers/elasticsearch:v2.4.1-2 gcr.</description>
    </item>
    
    <item>
      <title>Kubernetes的ConfigMap解析</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-configmap-introduction/</link>
      <pubDate>Thu, 06 Apr 2017 21:24:20 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-configmap-introduction/</guid>
      <description>（题图：龙形灯笼@古北水镇 Apr 30,2016）
前言 为什么要翻译这篇文章，是因为我在使用Fluentd和ElasticSearch收集Kubernetes集群日志的时候遇到了需要修改镜像中配置的问题，fluent-plugin-kubernetes_metadata里的需要的td-agent.conf文件。
其实ConfigMap功能在Kubernetes1.2版本的时候就有了，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。这些配置信息需要与docker image解耦，你总不能每修改一个配置就重做一个image吧？ConfigMap API给我们提供了向容器中注入配置信息的机制，ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制大对象。
ConfigMap概览 ConfigMap API资源用来保存key-value pair配置数据，这个数据可以在pods里使用，或者被用来为像controller一样的系统组件存储配置数据。虽然ConfigMap跟Secrets类似，但是ConfigMap更方便的处理不含敏感信息的字符串。 注意：ConfigMaps不是属性配置文件的替代品。ConfigMaps只是作为多个properties文件的引用。你可以把它理解为Linux系统中的/etc目录，专门用来存储配置文件的目录。下面举个例子，使用ConfigMap配置来创建Kuberntes Volumes，ConfigMap中的每个data项都会成为一个新文件。
kind: ConfigMap apiVersion: v1 metadata: creationTimestamp: 2016-02-18T19:14:38Z name: example-config namespace: default data: example.property.1: hello example.property.2: world example.property.file: |- property.1=value-1 property.2=value-2 property.3=value-3  data一栏包括了配置数据，ConfigMap可以被用来保存单个属性，也可以用来保存一个配置文件。 配置数据可以通过很多种方式在Pods里被使用。ConfigMaps可以被用来：
 设置环境变量的值 在容器里设置命令行参数 在数据卷里面创建config文件  用户和系统组件两者都可以在ConfigMap里面存储配置数据。
其实不用看下面的文章，直接从kubectl create configmap -h的帮助信息中就可以对ConfigMap究竟如何创建略知一二了。
Examples: # Create a new configmap named my-config based on folder bar kubectl create configmap my-config --from-file=path/to/bar # Create a new configmap named my-config with specified keys instead of file basenames on disk kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.</description>
    </item>
    
    <item>
      <title>TensorFlow深度学习手写数字识别初体验</title>
      <link>http://rootsongjc.github.io/blogs/tensorflow-and-deep-learning-without-a-phd/</link>
      <pubDate>Wed, 05 Apr 2017 21:52:01 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/tensorflow-and-deep-learning-without-a-phd/</guid>
      <description>（题图：禾雀 @北京动物园 Apr 3,2017）
前言 TensorFlow学习曲线是陡峭的，不是所有的IT从业人员都很容易参与的，你需要有一定的数学专业知识，对于对深度学习没有经验的程序员，要想了解这门技术，最快捷的途径是先运行一个示例，我们认识事物都是先从感性、到理性的思辨过程。
下面我们来跟随Martin Gorner的TensorFlow and Deep Learing Without a PhD来编写我们的第一个TensorFlow程序——手写数字识别，这篇文章的中文版没有博士学位如何玩转TensorFlow和深度学习于2017年3月13日发表在发表在机器之心上。这篇文章也是根据3月8日-10日的Google Cloud NEXT&amp;rsquo;17大会上Martin Gorner做的讲解整理而成的，教程 | 没有博士学位，照样玩转TensorFlow深度学习这篇文章是对Martin Gorner的简易教程的原文翻译，我们暂时不要求了解TensorFlow背后复杂的理论，我们先跟随这篇简易教程玩一把TensorFlow的手写数字识别。
如果你想深入了解这本后的原理的话，可以查看哈尔滨工业大学社会计算与信息检索研究中心翻译的《神经网络与深度学习》这本书，该书翻译自Neural Networks and Deep Learning的中文翻译，原文作者 Michael Nielsen，而且这还是一本免费的电子书，该书中系统讲解了使用神经网络识别手写数字背后的原理。该书托管在GitBook上，你可以点击这里直接下载该书中文版的PDF。
准备 下载代码
这个代码仓库里包含了手写数字识别和下载依赖的训练数据的代码，我们将只用到mnist_1.0_softmax.py这一个代码文件。整个mnist_1.0_softmax.py代码并不复杂，不算注释的话只有36行。
git clone https://github.com/martin-gorner/tensorflow-mnist-tutorial.git  下载完后，可以看到有一个INSTALL.txt，这篇文章是运行代码所必需的环境要求说明。
安装TensorFlow
我之前写过详细的TensorFlow安装教程TensorFlow实战（才云郑泽宇著）读书笔记——第二章TensorFlow环境搭建，这篇文章中主要讲怎样在docker里安装TensorFlow。
我使用的Mac而且还是python2.7，所以我这样安装：
pip install --upgrade tensorflow --user -U pip install --upgrade matplotlib --user -U  运行示例 运行手写数字训练示例。
python mnist_1.0_softmax.py  运行过程中你会看到一大段输出：
Collecting matplotlib Downloading matplotlib-2.0.0-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (12.8MB) 100% |████████████████████████████████| 12.8MB 26kB/s Requirement already up-to-date: pyparsing!</description>
    </item>
    
    <item>
      <title>Kubernetes heapster监控插件安装文档</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-heapster-installation/</link>
      <pubDate>Wed, 05 Apr 2017 18:41:19 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-heapster-installation/</guid>
      <description>（题图：嗑猫薄荷的白化孟加拉虎@北京动物园 Apr 3,2017）
前言 前面几篇文章中记录了我们安装好了Kubernetes集群、配置好了flannel网络、安装了Kubernetes Dashboard，但是还没法查看Pod的监控信息，虽然kubelet默认集成了cAdvisor（在每个node的4194端口可以查看到），但是很不方便，因此我们选择安装heapster。
安装 下载heapster的代码
直接现在Github上的最新代码。
git pull https://github.com/kubernetes/heapster.git  目前的最高版本是1.3.0。
在heapster/deploy/kube-config/influxdb目录下有几个yaml文件：
grafana-deployment.yaml grafana-service.yaml heapster-deployment.yaml heapster-service.yaml influxdb-deployment.yaml influxdb-service.yaml  我们再看下用了哪些镜像：
grafana-deployment.yaml:16: image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 heapster-deployment.yaml:16: image: gcr.io/google_containers/heapster-amd64:v1.3.0-beta.1 influxdb-deployment.yaml:16: image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1  下载镜像
我们下载好了这些images后，存储到私有镜像仓库里：
 sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-amd64:v1.3.0-beta.1 sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-grafana-amd64:v4.0.2 sz-pg-oam-docker-hub-001.tendcloud.com/library/heapster-influxdb-amd64:v1.1.1  我已经将官方镜像克隆到了时速云上，镜像地址：
 index.tenxcloud.com/jimmy/heapster-amd64:v1.3.0-beta.1 index.tenxcloud.com/jimmy/heapster-influxdb-amd64:v1.1.1 index.tenxcloud.com/jimmy/heapster-grafana-amd64:v4.0.2  需要的可以去下载，下载前需要用时速云账户登陆，然后再执行pull操作。
docker login index.tendcloud.com  配置 参考Run Heapster in a Kubernetes cluster with an InfluxDB backend and a Grafana UI和Configuring Source，需要修改yaml文件中的几个配置。
 首先修改三个deployment.yaml文件，将其中的镜像文件地址改成我们自己的私有镜像仓库的 修改heapster-deployment.</description>
    </item>
    
    <item>
      <title>Kubernetes Dashboard/Web UI安装全记录</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation/</link>
      <pubDate>Wed, 05 Apr 2017 14:28:51 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-dashboard-installation/</guid>
      <description>（题图：晒太阳的袋鼠@北京动物园 Apr 3,2017）
前言 前几天在CentOS7.2上安装Kubernetes1.6和安装好flannel网络配置，今天我们来安装下kuberentnes的dashboard。
Dashboard是Kubernetes的一个插件，代码在单独的开源项目里。1年前还是特别简单的一个UI，只能在上面查看pod的信息和部署pod而已，现在已经做的跟Docker Enterprise Edition的Docker Datacenter很像了。
安装Dashboard 官网的安装文档https://kubernetes.io/docs/user-guide/ui/，其实官网是让我们使用现成的image来用kubernetes部署即可。
首先需要一个kubernetes-dashboard.yaml的配置文件，可以直接在Github的src/deploy/kubernetes-dashboard.yaml下载。
我们能看下这个文件的内容：
# Copyright 2015 Google Inc. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the &amp;quot;License&amp;quot;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an &amp;quot;AS IS&amp;quot; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</description>
    </item>
    
    <item>
      <title>容器技术在大数据场景下的应用——Yarn on Docker</title>
      <link>http://rootsongjc.github.io/projects/yarn-on-docker/</link>
      <pubDate>Tue, 04 Apr 2017 00:19:04 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/projects/yarn-on-docker/</guid>
      <description>作者：宋净超 TalkingData云计算及大数据工程师
前言 我已就该话题已在2016年上海Qcon上发表过演讲，点此观看。
另外InfoQ网站上的文字版数据中心的Yarn on Docker集群方案，即本文。
项目代码开源在Github上：Magpie
当前数据中心存在的问题 数据中心中的应用一般独立部署，为了保证环境隔离与方便管理，保证应用最大资源 数据中心中普遍存在如下问题：
1.主机资源利用率低
2.部署和扩展复杂
3.资源隔离无法动态调整
4.无法快速响应业务
为何使用Yarnon Docker
彻底隔离队列 
• 为了合理利用Hadoopyarn的资源，队列间会互相抢占计算资源，造成重要任务阻塞
• 根据部门申请的机器数量划分Yarn集群方便财务管理
• 更细粒度的资源分配 统一的资源分配
• 每个NodeManager和容器都可以限定CPU、内存资源
• Yarn资源划分精确到CPU核数和内存大小 弹性伸缩性服务
• 每个容器中运行一个NodeManager，增减yarn资源只需增减容器个数
• 可以指定每个NodeManager拥有的计算资源多少，按需申请资源 给我们带来什么好处？  Swarm统一集群资源调度
 • 统一资源
• 增加Docker虚拟化层，降低运维成本
增加Hadoop集群资源利用率
• Fordatacenter：避免了静态资源隔离
• Forcluster：加强集群内部资源隔离
 系统架构  比如数据中心中运行的Hadoop集群，我们将HDFS依然运行在物理机上，即DataNode依然部署在实体机器上，将Yarn计算层运行在Docker容器中，整个系统使用二层资源调度，Spark、Flinek、MapReduce等应用运行在Yarn上。
  Swarm调度最底层的主机硬件资源，CPU和内存封装为Docker容器，容器中运行NodeManager，提供给Yarn集群，一个Swarm集群中可以运行多个Yarn集群，形成圈地式的Yarn计算集群。
具体流程
1.swarm node向swarm master注册主机资源并加入到swarmcluster中
2.swarm master向cluster申请资源请求启动容器
3.swarm根据调度策略选择在某个node上启动dockercontainer
4.swarm node的docker deamon根据容器启动参数启动相应资源大小的NodeManager
5.NodeManager自动向YARN的ResourceManager注册资源一个NodeManager资源添加完成。
 Swarm为数据中心做容器即主机资源调度，每个swarmnode的节点结构如图：
一个Swarmnode就是一台物理机，每台主机上可以起多个同类型的dockercontainer，每个container的资源都有限制包括CPU、内存NodeManager容器只需要考虑本身进程占用的资源和需要给主机预留资源。假如主机是24核64G，我们可以分给一个容器5核12G，NodeManager占用4核10G的资源提供给Yarn。</description>
    </item>
    
    <item>
      <title>两款图片处理工具——Google Guetzli和基于AI的Deep Photo Style Transfer</title>
      <link>http://rootsongjc.github.io/talks/picture-process/</link>
      <pubDate>Sun, 02 Apr 2017 20:27:00 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/talks/picture-process/</guid>
      <description>前言 如果你看过美剧「硅谷」会记得剧中主角们所在的创业公司PiedPipper，他们就是靠自己发明的视频压缩算法来跟大公司Hooli竞争的，这部剧现在已经发展到第4季，在腾讯视频上可以免费观看。
最近关注了两个图像处理的Open Source Projects。
 Google Guetzli 图像压缩工具 Luan Fujun&amp;rsquo;s Deep Photo Style Transfer 图像style转换工具  另外对于图像处理还处于Photoshop、Lightroom这种摄影后期和图像处理命令行工具ImageMagick的我来说，图像压缩，智能图像风格转换实乃上乘武功，不是我等凡夫俗子驾驭的了，但是乘兴而来，总不能败兴而归吧，下面我们来一探究竟。
Google Guetzli 聊聊架构微信公众号上有一篇介绍Google开源新算法，可将JPEG文件缩小35%文章。
我在Mac上试用了一下，安装很简单，只要一条命令：
brew install guetzli  但是当我拿一张22M大小的照片使用guetzli压缩的时候，我是绝望的，先后三次kill掉了进程。
因为实在是太慢了，也能是我软件对内存和CPU的利用率不高，效果你们自己看看。
原图是这个样子的，拍摄地点在景山上的，俯瞰紫禁城的绝佳位置。
guetzli --quality 84 --verbose 20160403052.jpg output.jpg  为什么quality要设置成84呢？因为只能设置为84+的quality，如果要设置的更低的话需要自己修改代码。
耗时了一个小时，后台进程信息。
这个是使用Squash压缩后的大小效果，压缩每张照片差不多只要3秒钟。
 Squash的logo就是个正在被剥皮的🍊，这是下载地址。
 压缩比分别为70%和30%。
压缩比70%后的细节放大图
压缩比30%的细节放大图
你看出什么区别了吗？反正我是没有。
下面再来看看耗时一个小时，千呼万唤始出来的guetzli压缩后的效果和使用squash压缩比为30%的效果对比。
左面是使用guetzli压缩后（4.1M），右面使用的squash压缩后（3.1M）的照片。
似乎还是没有什么区别啊？你看出来了吗？
Guetzli总结 可能是我使用Guetzli的方式不对，但是命令行里确实没有设置CPU和内存资源的选项啊，为啥压缩照片会这么慢呢？效果也并不出彩，不改代码的话照片质量只能设置成84以上，但是这个是Open Source的，使用的C++开发，可以研究下它的图像压缩算法。
Deep Photo Style Transfer 来自康奈尔大学的Luan Fujun开源的图像sytle转换工具，看了README的介绍，上面有很多图像风格转换的例子，真的很惊艳，市面上好像还没有这种能够在给定任意一张照片的情况下，自动将另一张照片转换成该照片的style。
这个工具使用Matlab和Lua开发，基于Torch运行的时候需要CUDA，cudnn，Matlab，环境实在太复杂，就没折腾，启动有人发布Docker镜像，已经有人提了issue。
如果它能够被商用，绝对是继Prisma后又一人工智能照片处理应用利器。
后记 是不是有了照片风格转换这个东西就不需要做照片后期了？只要选几张自己喜欢的风格照片，再鼠标点几下就可以完成照片处理了？摄影师要失业了？非也！照片风格东西本来就是很主观性的，每个人都有自己喜欢的风格，照相机发明后就有人说画家要失业了，其实不然，画画依然是创造性地劳动，只能说很多写实风格的画家要失业了。Deep Photo Style Transfer也许会成为Lightroom或者手机上一款app的功能，是一个不错的工具。也许还会成为像Prisma一样的现象级产品，who knows?🤷‍♂️</description>
    </item>
    
    <item>
      <title>Kubernetes基于flannel的网络配置</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-network-config/</link>
      <pubDate>Fri, 31 Mar 2017 11:05:18 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-network-config/</guid>
      <description>（题图：西安鼓楼 Oct 4,2014）
书接上文在CentOS中安装Kubernetes详细指南，这是一个系列文章，作为学习Kubernetes的心路历程吧。
本文主要讲解Kubernetes的网络配置，👆文中有一个安装Flannel的步骤，但是安装好后并没有相应的配置说明。
配置flannel 我们直接使用的yum安装的flannle，安装好后会生成/usr/lib/systemd/system/flanneld.service配置文件。
[Unit] Description=Flanneld overlay address etcd agent After=network.target After=network-online.target Wants=network-online.target After=etcd.service Before=docker.service [Service] Type=notify EnvironmentFile=/etc/sysconfig/flanneld EnvironmentFile=-/etc/sysconfig/docker-network ExecStart=/usr/bin/flanneld-start $FLANNEL_OPTIONS ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker Restart=on-failure [Install] WantedBy=multi-user.target RequiredBy=docker.service  可以看到flannel环境变量配置文件在/etc/sysconfig/flanneld。
# Flanneld configuration options # etcd url location. Point this to the server where etcd runs FLANNEL_ETCD_ENDPOINTS=&amp;quot;http://sz-pg-oam-docker-test-001.tendcloud.com:2379&amp;quot; # etcd config key. This is the configuration key that flannel queries # For address range assignment FLANNEL_ETCD_PREFIX=&amp;quot;/kube-centos/network&amp;quot; # Any additional options that you want to pass #FLANNEL_OPTIONS=&amp;quot;&amp;quot;   etcd的地址FLANNEL_ETCD_ENDPOINT etcd查询的目录，包含docker的IP地址段配置。FLANNEL_ETCD_PREFIX  在etcd中创建网络配置</description>
    </item>
    
    <item>
      <title>TensorFlow实战（才云郑泽宇著）读书笔记——第三章TensorFlow入门</title>
      <link>http://rootsongjc.github.io/blogs/tensorflow-practice-03/</link>
      <pubDate>Thu, 30 Mar 2017 21:34:33 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/tensorflow-practice-03/</guid>
      <description>（题图：扬州东关 May 24,2015）
这是我阅读才云科技郑泽宇的《TensorFlow实战Google深度学习框架》的读书笔记系列文章，按照文章的章节顺序来写的。整本书的笔记归档在这里。
P.S 本书的官方读者交流微信群（作者也在群里）已经超过100人，您可以先加我微信后我拉您进去，我的二维码在这里，或者直接搜索我的微信号jimmysong。
这一章从三个角度带大家入门。
分别是TensorFlow的
 计算模型 数据模型 运行模型  3.1 TensorFlow的计算模型——图计算 计算图是TensorFlow中的一个最基本的概念，TensorFlow中的所有计算都会转化成计算图上的节点。
其实TensorFlow的名字已经暗示了它的实现方式了，Tensor表示的是数据结构——张量，Flow表示数据流——Tensor通过数据流相互转化。
常用的方法
 在python中导入tensorflow：import tensorflow as tf 获取当前默认的计算图：tf.get_default_graph() 生成新的计算图：tf.Graph()  书中这里都有例子讲解，可以从Github中下载代码，或者如果你使用才云提供的docker镜像的方式安装的话，在jupyter中可以看到各个章节的代码。
定义两个不同的图
import tensorflow as tf g1 = tf.Graph() with g1.as_default(): v = tf.get_variable(&amp;quot;v&amp;quot;, [1], initializer = tf.zeros_initializer) # 设置初始值为0 g2 = tf.Graph() with g2.as_default(): v = tf.get_variable(&amp;quot;v&amp;quot;, [1], initializer = tf.ones_initializer()) # 设置初始值为1 with tf.Session(graph = g1) as sess: tf.global_variables_initializer().run() with tf.variable_scope(&amp;quot;&amp;quot;, reuse=True): print(sess.</description>
    </item>
    
    <item>
      <title>在CentOS上安装kubernetes详细指南</title>
      <link>http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/</link>
      <pubDate>Thu, 30 Mar 2017 20:44:20 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/kubernetes-installation-on-centos/</guid>
      <description>（题图：北京圆明园 Aug 25,2014）
作者：Jimmy Song，Peter Ma，2017年3月30日
最近决定从Docker Swarm Mode投入到Kubernetes的怀抱，对Docker的战略和企业化发展前景比较堪忧，而Kubernetes是CNCF的成员之一。
这篇是根据官方安装文档实践整理的，操作系统是纯净的CentOS7.2。
另外还有一个Peter Ma写的在CentOS上手动安装kubernetes的文档可以参考。
角色分配
下面以在三台主机上安装Kubernetes为例。
172.20.0.113 master/node kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy etcd flannel 172.20.0.114 node kubectl kube-proxy flannel 172.20.0.115 node kubectl kube-proxy flannel  第一台主机既作为master也作为node。
系统环境
 Centos 7.2.1511 docker 1.12.6 etcd 3.1.5 kubernetes 1.6.0 flannel 0.7.0-1  安装 下面给出两种安装方式：
 配置yum源后，使用yum安装，好处是简单，坏处也很明显，需要google更新yum源才能获得最新版本的软件，而所有软件的依赖又不能自己指定，尤其是你的操作系统版本如果低的话，使用yum源安装的kubernetes的版本也会受到限制。 使用二进制文件安装，好处是可以安装任意版本的kubernetes，坏处是配置比较复杂。  我们最终选择使用第二种方式安装。
本文的很多安装步骤和命令是参考的Kubernetes官网CentOS Manual Config文档。
第一种方式：CentOS系统中直接使用yum安装 给yum源增加一个Repo
[virt7-docker-common-release] name=virt7-docker-common-release baseurl=http://cbs.centos.org/repos/virt7-docker-common-release/x86_64/os/ gpgcheck=0  安装docker、kubernetes、etcd、flannel一步到位
yum -y install --enablerepo=virt7-docker-common-release kubernetes etcd flannel  安装好了之后需要修改一系列配置文件。</description>
    </item>
    
    <item>
      <title>Go语言中的并发编程总结</title>
      <link>http://rootsongjc.github.io/projects/golang-concurrency-summary/</link>
      <pubDate>Fri, 24 Mar 2017 08:36:29 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/projects/golang-concurrency-summary/</guid>
      <description>Go语言并发编程总结  Golang :不要通过共享内存来通信，而应该通过通信来共享内存。这句风靡在Go社区的话,说的就是 goroutine中的 channel。他在go并发编程中充当着类型安全的管道作用。
 1、通过golang中的 goroutine 与sync.Mutex进行并发同步 import( &amp;quot;fmt&amp;quot; &amp;quot;sync&amp;quot; &amp;quot;runtime&amp;quot; ) var count int =0; func counter(lock * sync.Mutex){ lock.Lock() count++ fmt.Println(count) lock.Unlock() } func main(){ lock:=&amp;amp;sync.Mutex{} for i:=0;i&amp;lt;10;i++{ //传递指针是为了防止 函数内的锁和 调用锁不一致 go counter(lock) } for{ lock.Lock() c:=count lock.Unlock() ///把时间片给别的goroutine 未来某个时刻运行该routine runtime.Gosched() if c&amp;gt;=10{ fmt.Println(&amp;quot;goroutine end&amp;quot;) break } } }  2、goroutine之间通过 channel进行通信 channel是和类型相关的 可以理解为 是一种类型安全的管道。
简单的channel 使用
package main import &amp;quot;fmt&amp;quot; func Count(ch chan int) { ch &amp;lt;- 1 fmt.</description>
    </item>
    
    <item>
      <title>Pivotal Cloud foundry快速开始指南</title>
      <link>http://rootsongjc.github.io/blogs/cloud-foundry-tryout/</link>
      <pubDate>Thu, 23 Mar 2017 22:54:18 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/cloud-foundry-tryout/</guid>
      <description>（题图：黄山日出后的云海 Oct 3,2013）
前言 最近研究了下Pivotal的Cloud foundry，CF本身是一款开源软件，很多PAAS厂商都加入了CF，我们用的是的PCF Dev（PCF Dev是一款可以在工作站上运行的轻量级PCF安装）来试用的，因为它可以部署在自己的环境里，而Pivotal Web Services只免费两个月，之后就要收费。这里有官方的详细教程。
开始 根据官网的示例，我们将运行一个Java程序示例。
安装命令行终端
下载后双击安装即可，然后执行cf help能够看到帮助。
安装PCF Dev
先下载，如果你没有Pivotal network账号的话，还需要注册个用户，然后用以下命令安装：
$./pcfdev-VERSION-osx &amp;amp;&amp;amp; \ cf dev start Less than 4096 MB of free memory detected, continue (y/N): &amp;gt; y Please sign in with your Pivotal Network account. Need an account? Join Pivotal Network: https://network.pivotal.io Email&amp;gt; 849122844@qq.com Password&amp;gt; Downloading VM... Progress: |+++++++++++++=======&amp;gt;| 100% VM downloaded. Allocating 4096 MB out of 16384 MB total system memory (3514 MB free).</description>
    </item>
    
    <item>
      <title>TensorFlow实战（才云郑泽宇著）读书笔记——第二章TensorFlow环境搭建</title>
      <link>http://rootsongjc.github.io/blogs/tensorflow-practice-02/</link>
      <pubDate>Thu, 23 Mar 2017 19:34:33 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/tensorflow-practice-02/</guid>
      <description>（题图：广州海珠桥 Aug 10,2014）
 这是我阅读才云科技郑泽宇的《TensorFlow实战Google深度学习框架》的读书笔记系列文章，按照文章的章节顺序来写的。整本书的笔记归档在这里。
 P.S 本书的官方读者交流微信群（作者也在群里）已经超过100人，您可以先加我微信后我拉您进去，我的二维码在这里，或者直接搜索我的微信号jimmysong。
睇完这一章后应该就可以自己搭建出一个TensorFlow的环境，我之前在docker里玩过，镜像比较大，下载慢一点，不过用起来很方便，如果你仅仅是想试用一下TensorFlow，看看它能干什么的话，可以直接在docker里试用一下。在Mac上安装的详细步骤，官方安装说明文档。
2.1 TensorFlow的主要依赖包 TensorFlow主要用到以下两个依赖：
 Protocol buffer：数据结构化工具。Google开源的结构化数据格式，用于网络传输数据时候的序列化和反序列化，使用的时候需要先定义schema，github地址https://github.com/google/protobuf。分布式TensorFlow使用到额gRPC也是使用Protocol Buffer来组织的， Bazel:自动化编译构建工具。Google开源的，github地址https://github.com/bazelbuild/bazel，它支持多语言、多平台、可重复编译和可伸缩，构建大型软件速度也是很快的。Bazel使用**项目空间**的形式管理编译的，每个项目空间需要包含[BUILD文件](https://github.com/tensorflow/tensorflow/blob/master/bower.BUILD)（定义编译目标）和[WORKSPACE](https://github.com/tensorflow/tensorflow/blob/master/WORKSPACE)文件（定义编译的依赖环境）。这两个文件都有点类似python语法。  2.2 TensorFlow安装 TensorFlow的安装方式包括docker镜像、pip安装、源码编译安装。
我选择最方便的docker镜像方式，其他方式对本地环境做很多配置，折腾起来比较麻烦。
我早就在docker中安装过TensorFlow0.9小试过牛刀。现在1.0.1版本已经released了。TensorFlow的所有版本都有对应的docker镜像发布在docker hub，可以直接docker pull安装。
为了和书中所用的镜像保持统一，我将使用caicloud提供的镜像，基于TensorFlow0.12.0（这个版本是2016年12月20日发布的），他们增加了一些其他机器学习工具包和TensorFlow可视化工具TensorBoard。
docker镜像方式安装
首先下载镜像，这个image比较大，下载下来比较费时间，我用了差不多15分钟吧。
docker pull cargo.caicloud.io/tensorflow/tensorflow:0.12.0  下载下来后我们再check下这个大小为1.41GB镜像的layers。
另外还有个nvidia版本的docker，可以将你电脑的GPU派山用场，我暂时没用到GPU，我电脑装的是docker17.03-ce，就不折腾GPU版本的TensorFlow了。
IMAGE CREATED CREATED BY SIZE COMMENT c8a8409297f2 5 weeks ago /bin/sh -c #(nop) CMD [&amp;quot;/run_tf.sh&amp;quot;] 0 B &amp;lt;missing&amp;gt; 5 weeks ago /bin/sh -c #(nop) COPY file:78332d36244852... 122 B &amp;lt;missing&amp;gt; 5 weeks ago /bin/sh -c #(nop) COPY dir:8b6ab7d235e3975.</description>
    </item>
    
    <item>
      <title>容器的应用场景</title>
      <link>http://rootsongjc.github.io/talks/container-applications-scenarios/</link>
      <pubDate>Thu, 23 Mar 2017 15:26:11 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/talks/container-applications-scenarios/</guid>
      <description>（题图：深圳大梅沙 Aug 1,2014）
 如果你对容器到底有什么用存在疑惑的话，推荐你看下我今天碰到的一篇阿里云的容器服务-产品简介-应用场景的文章，觉得比较好，把容器的典型应用场景都概括了，容器对于互联网的弹性扩展和微服务架构有很好的应用场景，P.S这里不是在帮阿里云做广告，这里的推荐搭配确实是很多常用的配置选项。
 DevOps 持续交付 最优化的持续交付流程
配合 Jenkins 帮您自动完成从代码提交到应用部署的 DevOps 完整流程，确保只有通过自动测试的代码才能交付和部署，高效替代业内部署复杂、迭代缓慢的传统方式。
能够实现：
 DevOps 自动化  实现从代码变更到代码构建，镜像构建和应用部署的全流程自动化。
 环境一致性  容器技术让您交付的不仅是代码，还有基于不可变架构的运行环境。
 持续反馈  每次集成或交付，都会第一时间将结果实时反馈。
推荐搭配使用：
云服务器 ECS + 容器服务
基于高性能计算的机器学习 专注机器学习本身，快速实现从 0 到 1
帮助数据工程师在 HPC 集群上轻松部署机器学习应用，跟踪试验和训练、发布模型，数据部署在分布式存储，无需关心繁琐部署运维，专注核心业务，快速从 0 到 1。
能够实现：
 快速弹性  一键部署机器学习应用，秒级启动和弹性伸缩。
 简单可控  一行配置轻松获取 GPU 计算能力，并且可以监控 GPU 的资源。
 深度整合  无缝接入阿里云存储、日志监控和安全基础架构能力。
推荐搭配使用：
高性能计算 (Alibaba Cloud HPC) + 容器服务 + 阿里云文件存储 NAS + 对象存储 OSS</description>
    </item>
    
    <item>
      <title>TensorFlow实战（才云郑泽宇著）读书笔记——第一章深度学习简介</title>
      <link>http://rootsongjc.github.io/blogs/tensorflow-practice-01/</link>
      <pubDate>Mon, 20 Mar 2017 22:04:33 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/tensorflow-practice-01/</guid>
      <description>（题图：TensofFlow实战图书封面）
🙏电子工业出版社编辑赠书，能够这么快的拿到这本书，也🙏才云科技的郑泽宇大哥耐心的写了这本书，能够让我等小白一窥深度学习的真容。另外要强烈推荐下这本书，这是本TensorFlow深度学习很好的入门书。书中提供的代码下载地址，整本书的笔记归档在这里。
P.S 本书的官方读者交流微信群（作者也在群里）已经超过100人，您可以先加我微信后我拉您进去，我的二维码在这里，或者直接搜索我的微信号jimmysong。
1.1 人工智能、机器学习与深度学习 这一节是讲解三者之间的关系。
首先以垃圾邮件分类问题引入机器学习的逻辑回归算法。
逻辑回归算法的准确性取决于训练数据中的特征的提取，以及训练的数据数量。
文章中又提了一个从实体中提取特征的例子：通过笛卡尔坐标系活极角坐标系来表示不同颜色的点，看看能否用一条直线划分。这个例子用来说明一旦解决了数据表达和特征提取，很多人工智能的问题就能迎刃而解。
深度学习是机器学习的一个分支，除了能够学习特征和任务之间的关联之外，还能自动从简单特征中提取更加复杂的特征，这是其区别于机器学习的关键点。
总的来说，人工智能&amp;gt;机器学习&amp;gt;深度学习。
1.2深度学习的发展历程 本节介绍了深度网络历史的三个发展阶段。
2012年的ImageNet图像分类竞赛上，深度学习系统AlexNet赢得冠军，自此深度学习作为深层神经网络的代名词而被人熟知。
1.3深度学习的应用 这一节讲的是深度学习的应用，首先还是从ImageNet的图像识别开始，应用到了OCR（提到了卷积神经网络）、语音识别（提到了混合搞高斯模型）、自然语言处理（提到了语料库、单词向量、机器翻译、情感分析）、人机对弈（提到了AlphaGO）。
1.4 深度学习工具介绍与对比 TensorFlow的渊源是Google大脑团队在2011年开发，在内部使用的DistBelief，并赢得了ImageNet 2014年的比赛，TF是其开源版本，还发表了一篇论文TensorFlow: Large-Scale Machine Learning on Heteogeneous Distributed systems，这就跟当年的HDFS、MapReduce一个套路啊。
Google还把它用来做RankBrain和很多其他的产品线上使用。
当然，还有很多其他的深度学习工具，比如Caffe、Deeplearning4j、Torch等不一而足。从各种指标来看，TensorFlow都是目前最受关注的深度学习框架。</description>
    </item>
    
    <item>
      <title>Docker源码分析第一篇——代码结构</title>
      <link>http://rootsongjc.github.io/blogs/docker-source-code-analysis-code-structure/</link>
      <pubDate>Sun, 19 Mar 2017 23:00:29 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-source-code-analysis-code-structure/</guid>
      <description>(题图：北京八达岭长城 Oct 1,2015)
前言 之前陆陆续续看过一点docker的源码，都未成体系，最近在研究Docker-17.03-CE，趁此机会研究下docker的源码，在网上找到一些相关资料，都比较过时了，发现*孙宏亮*大哥写过一本书叫《Docker源码分析》，而且之前也在InfoQ上陆续发过一些文章，虽然文章都比较老了，基于老的docker版本，但我认为依然有阅读的价值。起码能有这三方面收获：
 一是培养阅读源码的思维方式，为自己阅读docker源码提供借鉴。 二是可以了解docker版本的来龙去脉。 三还可以作为Go语言项目开发作为借鉴。  下载地址 鉴于这本书已经发行一年半了了，基于的docker版本还是1.2.0，而如今都到了1.13.0（docker17.03的老版本号），应该很少有人买了吧，可以说这本书的纸质版本的生命周期也差不多了吧。如果有人感兴趣可以下载pdf版本看看，Docker源码解析-机械工业出版社-孙宏亮著-2015年8月（完整文字版，大小25.86M），Docker源码解析-看云整理版（文字版，有缩略，大小7.62M）。
Out-of-date 有一点必须再次强调一下，这本书中的docker源码分析是基于docker1.2.0，而这个版本的docker源码在github上已经无法下载到了，github上available的最低版本的docker源码是1.4.1。
 顺便感叹一句，科技行业发展实在太快了，尤其是互联网，一本书能连续用上三年都不过时，如果这样的话那么这门技术恐怕都就要被淘汰了吧？
 总体架构 Docker总体上是用的是Client/Server模式，所有的命令都可以通过RESTful接口传递。
整个Docker软件的架构中可以分成三个角色：
 Daemon：常驻后台运行的进程，接收客户端请求，管理docker容器。 Client：命令行终端，包装命令发送API请求。 Engine：真正处理客户端请求的后端程序。  代码结构 Docker的代码结构比较清晰，分成的目录比较多，有以下这些：
 api：定义API，使用了Swagger2.0这个工具来生成API，配置文件在api/swagger.yaml builder：用来build docker镜像的包，看来历史比较悠久了 bundles：这个包是在进行docker源码编译和开发环境搭建的时候用到的，编译生成的二进制文件都在这里。 cli：使用cobra工具生成的docker客户端命令行解析器。 client：接收cli的请求，调用RESTful API中的接口，向server端发送http请求。 cmd：其中包括docker和dockerd两个包，他们分别包含了客户端和服务端的main函数入口。 container：容器的配置管理，对不同的platform适配。 contrib：这个目录包括一些有用的脚本、镜像和其他非docker core中的部分。 daemon：这个包中将docker deamon运行时状态expose出来。 distribution：负责docker镜像的pull、push和镜像仓库的维护。 dockerversion：编译的时候自动生成的。 docs：文档。这个目录已经不再维护，文档在另一个仓库里https://github.com/docker/docker.github.io/。 experimental：从docker1.13.0版本起开始增加了实验特性。 hack：创建docker开发环境和编译打包时用到的脚本和配置文件。 image：用于构建docker镜像的。 integration-cli：集成测试 layer：管理 union file system driver上的read-only和read-write mounts。 libcontainerd：访问内核中的容器系统调用。 man：生成man pages。 migrate：将老版本的graph目录转换成新的metadata。 oci：Open Container Interface库 opts：命令行的选项库。 pkg： plugin：docker插件后端实现包。 profiles：里面有apparmor和seccomp两个目录。用于内核访问控制。 project：项目管理的一些说明文档。 reference：处理docker store中镜像的reference。 registry：docker registry的实现。 restartmanager：处理重启后的动作。 runconfig：配置格式解码和校验。 vendor：各种依赖包。 volume：docker volume的实现。  下一篇将讲解docker的各个功能模块和原理。</description>
    </item>
    
    <item>
      <title>About Jimmy Song</title>
      <link>http://rootsongjc.github.io/about/</link>
      <pubDate>Sat, 18 Mar 2017 20:53:54 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/about/</guid>
      <description> About me  Jimmy Song Beijing, China WHUT rootsongjc@gmail.com Wechat: jimmysong Career: iFlytek, TalkingData Translator: 《Cloud Native Go — Building Web Applications and Microservices for the Cloud with Go and React 》  </description>
    </item>
    
    <item>
      <title>React入门</title>
      <link>http://rootsongjc.github.io/talks/react-tryout/</link>
      <pubDate>Sat, 18 Mar 2017 10:07:13 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/talks/react-tryout/</guid>
      <description>前言 前端无疑是2016年最火热的技术，没有之一，2017年依然🔥。现在不会点前端技术都不好意思出去见人。
各种前端mvc框架层出不穷，angular js，vue，React，前端组件化开发概念已经深入人心。作为开发者，学习下前端设计也是有必要的，一来页面有些小的设计问题可以自己解决，同时还能提高自己的审美，提高网站的ui设计水平。
今天看到一本书《React Up and Running》的中文版本《React快速上手开发》出版了，英文版可以在这里下载。最近我在翻译的书Cloud Native Go中的实例也使用了React来构建Web应用程序，因此在网上找了一些React资料学习下。
必备基础技能 前端技能汇总这个项目详细记录 了前端工程师牵涉到的各方面知识。在具备基本技能之后可以在里面找到学习 的方向，完善技能和知识面。
frontend-dev-bookmarks是老外总结的前端开发资源。覆盖面非常广。包括各种知识点、工具、技术，非常全面。
以下是个人觉得入门阶段应该熟练掌握的基础技能：
 HTML4，HTML5语法、标签、语义 CSS2.1，CSS3规范，与HTML结合实现各种布局、效果 Ecma-262定义的javascript的语言核心，原生客户端javascript，DOM操作，HTML5新增功能 一个成熟的客户端javascript库，推荐jquery 一门服务器端语言：如果有服务器端开发经验，使用已经会的语言即可，如果没有服务器端开发经验，熟悉Java可以选择Servlet，不熟悉的可以选PHP，能实现简单登陆注册功能就足够支持前端开发了，后续可能需要继续学习，最基本要求是实现简单的功能模拟， HTTP  在掌握以上基础技能之后，工作中遇到需要的技术也能快速学习。
基本开发工具 恰当的工具能有效提高学习效率，将重点放在知识本身，在出现问题时能快速定位并 解决问题，以下是个人觉得必备的前端开发工具：
 文本编辑器：推荐Sublime Text，支持各种插件、主题、设置，使用方便 浏览器：推荐Google Chrome，更新快，对前端各种标准提供了非常好的支持 调试工具：推荐Chrome自带的Chrome develop tools，可以轻松查看DOM结构、样式，通过控制台输出调试信息，调试javascript，查看网络等 辅助工具：PhotoShop编辑图片、取色，fireworks量尺寸，AlloyDesigner对比尺寸，以及前面的到的Chrome develop tools， 翻墙工具：Shadowsocks、云梯VPN  学习方法和学习目标 方法：
 入门阶段反复阅读经典书籍的中文版，书籍中的每一个例子都动手实现并在浏览器中查看效果 在具备一定基础之后可以上网搜各种教程、demo，了解各种功能的实际用法和常见功能的实现方法 阅读HTML，CSS，Javascript标准全面完善知识点 阅读前端牛人的博客、文章提升对知识的理解 善用搜索引擎  目标：
 熟记前面知识点部分的重要概念，结合学习经历得到自己的理解 熟悉常见功能的实现方法，如常见CSS布局，Tab控件等。  入门之路 以下是入门阶段不错的书籍和资料
 HTML先看《HTML &amp;amp; CSS: Design and Build Websites》1-9章，然后《HTML5: The Missing Manual》1-4章。 CSS先看《CSS: The Missing Manual》，然后《CSS权威指南》 javascript先看《javascript高级程序设计》，然后《javascript权威指南》 HTTP看HTTP权威指南 在整个学习过程中HTML CSS JavaScript会有很多地方需要互相结合，实际工作中也是这样，一个简单的功能模块都需要三者结合才能实现。 动手是学习的重要组成部分，书籍重点讲解知识点，例子可能不是很充足，这就需要利用搜索引擎寻找一些简单教程，照着教程实现功能。以下是一些比较好的教程网址  可以搜索各大公司前端校招笔试面试题作为练习题或者他人总结的前端面试题还有个人总结的面试题（带参考答案） http://code.</description>
    </item>
    
    <item>
      <title>零基础使用Hugo和GitHub Pages创建自己的博客</title>
      <link>http://rootsongjc.github.io/talks/building-github-pages-with-hugo/</link>
      <pubDate>Fri, 17 Mar 2017 22:08:25 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/talks/building-github-pages-with-hugo/</guid>
      <description>（题图：🦅 北京动物园 Oct 5,2015）
 亲，你还在为虚拟主机、域名、空间而发愁吗？你想拥有自己的网站吗？你想拥有一个分享知识、留住感动，为开源事业而奋斗终身吗？那么赶快拿起你手中的📱拨打16899168，不对，是看这篇文章吧，不用998，也不用168，这一切都是免费的，是的你没看错，真的不要钱！
 准备 当然还是需要你有一点电脑基础的，会不会编程不要紧，还要会一点英文，你需要先申请一下几个账号和安装一些软件环境：
 GitHub 这是必需的，因为你需要使用Github Pages来托管你的网站。而且你还需要安装git工具。创建一个以自己用户名命名的username.github.io的project。 七牛云存储 非必需，为了存储文件方便，建议申请一个，免费10G的存储空间，存储照片和一些小文件是足够的，可以用来做外链，方便存储和管理，这样你就不用把图片也托管到Github上了。流量也是不限的。我没有收七牛的一点好处，以为是我自己用的，所以推荐给大家，七牛还有命令行客户端，方便你上传和同步文件。如上的题图都是存储在七牛云中的。 百度统计 非必需，基本的网站数据分析，免费的，质量还行。还有微信公众号可以查看，这一点我发现腾讯分析居然都没有微信公众号，自家的产品咋都不推出微信客户端呢。顺便提一下，这个统计账号跟你的百度账号不是同一个东西，两者是两套体系，当然你可以和自己的百度账号关联。只需要在Web的Header中植入一段JS代码即可。 Hugo 必需的，静态网站生成工具，用来编译静态网站的。跟Hexo比起来我更喜欢这个工具。 Typro 非必需，但是强烈推荐，我最喜欢的免费的Markdown编辑器，hugo可以编译markdown格式为HTML，所以用它来写博客是最合适不过了。  好了注册好Github后你现在可以尽情的玩耍了！😄
Let&amp;rsquo;s rock&amp;amp;roll! 首先介绍下Hugo
Hugo是一种通用的网站框架。严格来说，Hugo应该被称作静态网站生成器。
静态网站生成器从字面上来理解，就是将你的内容生成静态网站。所谓“静态”的含义其实反映在网站页面的生成的时间。一般的web服务器（WordPress, Ghost, Drupal等等）在收到页面请求时，需要调用数据库生成页面（也就是HTML代码），再返回给用户请求。而静态网站则不需要在收到请求后生成页面，而是在整个网站建立起之前就将所有的页面全部生成完成，页面一经生成便称为静态文件，访问时直接返回现成的静态页面，不需要数据库的参与。
采用静态网站的维护也相当简单，实际上你根本不需要什么维护，完全不用考虑复杂的运行时间，依赖和数据库的问题。再有也不用担心安全性的问题，没有数据库，网站注入什么的也无从下手。
静态网站最大好处就是访问快速，不用每次重新生成页面。当然，一旦网站有任何更改，静态网站生成器需要重新生成所有的与更改相关的页面。然而对于小型的个人网站，项目主页等等，网站规模很小，重新生成整个网站也是非常快的。Hugo在速度方面做得非常好，Dan Hersam在他这个Hugo教程里提到，5000篇文章的博客，Hugo生成整个网站只花了6秒，而很多其他的静态网站生成器则需要几分钟的时间。我的博客目前文章只有几十篇，用Hugo生成整个网站只需要0.1秒。官方文档提供的数据是每篇页面的生成时间不到1ms。
认为对于个人博客来说，应该将时间花在内容上而不是各种折腾网站。Hugo会将Markdown格式的内容和设置好模版一起，生成漂亮干净的页面。挑选折腾好一个喜爱的模版，在Sublime Text里用Markdown写博客，再敲一行命令生成同步到服务器就OK了。整个体验是不是非常优雅简单还有点geek的味道呢？
了解Hugo 首先建立自己的网站，mysite是网站的路径
$ hugo new site mysite  然后进入该路径
$ cd mysite  在该目录下你可以看到以下几个目录和config.toml文件
 ▸ archetypes/ ▸ content/ ▸ layouts/ ▸ static/ config.toml  config.toml是网站的配置文件，包括baseurl, title, copyright等等网站参数。
这几个文件夹的作用分别是：
 archetypes：包括内容类型，在创建新内容时自动生成内容的配置 content：包括网站内容，全部使用markdown格式 layouts：包括了网站的模版，决定内容如何呈现 static：包括了css, js, fonts, media等，决定网站的外观  Hugo提供了一些完整的主题可以使用，下载这些主题：</description>
    </item>
    
    <item>
      <title>Contiv Ultimate-Docker17.03CE下思科docker网络插件contiv趟坑终极版</title>
      <link>http://rootsongjc.github.io/blogs/contiv-ultimate/</link>
      <pubDate>Fri, 17 Mar 2017 17:52:37 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/contiv-ultimate/</guid>
      <description>（题图：广州石牌桥 Aug 10,2014）
前几天写的几篇关于Contiv的文章已经把引入坑了😂
今天这篇文章将带领大家用正确的姿势编译和打包一个contiv netplugin。
 请一定要在Linux环境中编译。docker中编译也会报错，最好还是搞个虚拟🐔吧，最好还有VPN能翻墙。
 环境准备 我使用的是docker17.03-CE、安装了open vSwitch(这个包redhat的源里没有，需要自己的编译安装)，如果你懒得编译可以用我编译的rpm包，点这里下载。
编译 这一步是很容易失败的，有人提过issue-779
具体步骤
 创建一个link /go链接到你的GOPATH目录，下面编译的时候要用。 将源码的vender目录下的文件拷贝到$GOPATH/src目录。 执行编译  在netplugin目录下执行以下命令能够编译出二进制文件。
NET_CONTAINER_BUILD=1 make build  在你的/$GOPATH/bin目录下应该会有如下几个文件：
contivk8s github-release godep golint misspell modelgen netcontiv netctl netmaster netplugin  ⚠️编译过程中可能会遇到 有些包不存在或者需要翻墙下载。
打包 我们将其打包为docker plugin。
Makefile里用于创建plugin rootfs的命令是：
host-pluginfs-create: @echo dev: creating a docker v2plugin rootfs ... sh scripts/v2plugin_rootfs.sh  v2plugin_rootfs.sh这个脚本的内容：
#!/bin/bash # Script to create the docker v2 plugin # run this script from contiv/netplugin directory echo &amp;quot;Creating rootfs for v2plugin &amp;quot;, ${CONTIV_V2PLUGIN_NAME} cat install/v2plugin/config.</description>
    </item>
    
    <item>
      <title>Docker17.03-CE插件开发案例</title>
      <link>http://rootsongjc.github.io/blogs/docker-plugin-develop/</link>
      <pubDate>Wed, 15 Mar 2017 13:57:26 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-plugin-develop/</guid>
      <description>（题图：杭州吴山步道旁的墙壁 Oct 16,2016）
 当你看到这篇文章时，如果你也正在进行docker1.13+版本下的plugin开发，恭喜你也入坑了，如果你趟出坑，麻烦告诉你的方法，感恩不尽🙏
 看了文章后你可能会觉得，官网上的可能是个假🌰。虽然官网上的文档写的有点不对，不过你使用docker-ssh-volume的开源代码自己去构建plugin的还是可以成功的！
Docker plugin开发文档 首先docker官方给出了一个docker legacy plugin文档，这篇文章基本就是告诉你docker目前支持哪些插件，罗列了一系列连接，不过对不起，这些不是docker官方插件，有问题去找它们的开发者去吧😂
Docker plugin貌似开始使用了新的v2 plugin了，legacy版本的plugin可以能在后期被废弃。
从docker的源码plugin/store.go中可以看到：
/* allowV1PluginsFallback determines daemon&#39;s support for V1 plugins. * When the time comes to remove support for V1 plugins, flipping * this bool is all that will be needed. */ const allowV1PluginsFallback bool = true /* defaultAPIVersion is the version of the plugin API for volume, network, IPAM and authz. This is a very stable API.</description>
    </item>
    
    <item>
      <title>Docker 17.03-CE create plugin源码解析</title>
      <link>http://rootsongjc.github.io/blogs/docker-create-plugin/</link>
      <pubDate>Wed, 15 Mar 2017 12:09:26 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-create-plugin/</guid>
      <description>（题图：故宫 Apr 3,2016）
继续上一篇Docker17.03-CE插件开发的🌰，今天来看下docker create plugin的源码。
cli/command/plugin/create.go
Docker命令行docker plugin create调用的，使用的是cobra，这个命令行工具开发包很好用，推荐下。
执行这两个函数
func newCreateCommand(dockerCli *command.DockerCli) *cobra.Command //调用下面的函数，拼装成URL调用RESTful API接口 func runCreate(dockerCli *command.DockerCli, options pluginCreateOptions) error { ... if err = dockerCli.Client().PluginCreate(ctx, createCtx, createOptions); err != nil { return err } ... }  在api/server/router/plugin/plugin_routes.go中
func (pr *pluginRouter) createPlugin(ctx context.Context, w http.ResponseWriter, r *http.Request, vars map[string]string) error { ... if err := pr.backend.CreateFromContext(ctx, r.Body, options); err != nil { return err } ... }  createPlugin这个方法定义在api/server/route/plugin/backen.</description>
    </item>
    
    <item>
      <title>微服务设计读书笔记</title>
      <link>http://rootsongjc.github.io/talks/microservice-reading-notes/</link>
      <pubDate>Sat, 11 Mar 2017 15:45:27 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/talks/microservice-reading-notes/</guid>
      <description>(题图：青海湖畔 Jun 25,2016)
最近在看《微服务设计（Sam Newman著）》这本书，下载本书PDF(扫描版，高清49.17M)。作者是ThoughtWorks的Sam Newman。这本书中包括很多业界是用案例，比如Netflix和亚马逊。有兴趣的话大家一起看看讨论一下。😄
本书读者交流微信群二维码，扫码入群（3月18日前有效），如果二维码失效，请移步这里加我微信，拉你入群。
P.S 这本书比较偏理论，另外还有一本中国人写的书，《微服务架构与实践，王磊著，电子工业出版社》，下载本书的pdf，文字版，大小28.08M。这个人同样也是ThoughtWorks的，两个人的观点不谋而合，依然是便理论的东西。
Cloud Native Go - 基于Go和React的web云服务构建指南
这本书是我最近在翻译的，将由电子工业出版社出版，本书根据实际案例教你如何构建一个web微服务，是实践为服务架构的很好的参考。查看本书介绍。
1.微服务初探 什么是微服务？ 微服务（Microservices）这个词比较新颖，但是其实这种架构设计理念早就有了。微服务是一种分布式架构设计理念，为了推动细粒度服务的使用，这些服务要能协同工作，每个服务都有自己的生命周期。一个为服务就是一个独立的实体，可以独立的部署在PAAS平台上，也可以作为一个独立的进程在主机中运行。服务之间通过API访问，修改一个服务不会影响其它服务。
微服务的好处 微服务的好处有很多，包括:
 帮助你更快的采用新技术 解决技术异构的问题，因为是用API网络通信，可以使用不同的语言和技术开发不同的服务 增强系统弹性，服务的边界比较清晰，便于故障处理 方便扩展，比如使用容器技术，可以很方便的一次性启动很多个微服务 方便部署，因为微服务之间彼此独立，所以能够独立的部署单个服务而不影响其它服务，如果部署失败的话还可以回滚 别忘了康为定律，微服务可以很好契合解决组织架构问题 可重用，可随意组合 便于维护，可以随时重写服务，不必担心历史遗留问题  与面向服务架构SOA的关系 可以说微服务架构师SOA的一种，但是目前的大多数SOA做的都不好，在通信协议的选择、第三方中间件的选择、服务力度如何划分方面做的都不够好。
微服务与SOA的共同点
 都使用共享库，比如可重用的代码库 模块化，比如Java中的OSGI(Open Source Gateway Initiative)、Erlang中的模块化  2.架构师的职责 架构师应该关心是什么 架构师（Architect）在英文中和建筑师是同一个词，他们之间也有很多相同之处，架构师构建的是软件，而建筑师构建的是建筑。
终于看到了我翻译的*Cloud Native Go*第14章中引用的这本书的原话了。
软件的需求变更是来的那么快来的那么直接，不像建筑那样可以在设计好后按照设计图纸一步步的去建设。
架构师应该关心的是什么呢？
 保证系统适合开发人员在上面工作 关注服务之间的交互，不需要过于关注各个服务内部发生的事情，比如服务之间互相调用的接口，是使用protocol buffer呢，还是使用RESTful API，还是使用Java RMI，这个才是架构师需要关注的问题，至于服务内部究竟使用什么，那就看开发人员自己了，架构师更需要关注系统的边界和分区。 架构师应该与团队在一起，结对编程 🤓🤓 了解普通工作，知道普通的工作是什么样子，做一个代码架构师 😂  架构师应该做什么  提供原则指导实践，比如Heroku的12因素法则用来指导SAAS应用架构一样，微服务架构设计也要有一套原则。 提供要求标准，通过日志功能和监控对服务进行集中式管理，明确接口标准，提供安全性建议。 代码治理。为开发人员提供范例和服务代码模板。 解决技术债务。 集中治理和领导。维持良好的团队关系，当团队跑偏的时候及时纠正。  3.服务建模 以MusicCorp这家公司的服务为例子讲解。</description>
    </item>
    
    <item>
      <title>Docker v.s Kubernetes part2</title>
      <link>http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part2/</link>
      <pubDate>Fri, 10 Mar 2017 22:06:32 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part2/</guid>
      <description> （题图：河北承德兴隆县雾灵山京郊最佳星空拍摄点 July 9,2016)
本文是Docker v.s Kubernetes第二篇，续接上文Docker v.s Kuberntes Part1。
Kubernetes是典型的Master/Slave架构模式，本文简要的介绍kubenetes的架构和组件构成。
Kubernetes核心架构 master节点  apiserver：作为kubernetes系统的入口，封装了核心对象的增删改查操作，以RESTFul接口方式提供给外部客户和内部组件调用。它维护的REST对象将持久化到etcd（一个分布式强一致性的key/value存储）。 scheduler：负责集群的资源调度，为新建的Pod分配机器。这部分工作分出来变成一个组件，意味着可以很方便地替换成其他的调度器。 controller-manager：负责执行各种控制器，目前有两类：  endpoint-controller：定期关联service和Pod(关联信息由endpoint对象维护)，保证service到Pod的映射总是最新的。 replication-controller：定期关联replicationController和Pod，保证replicationController定义的复制数量与实际运行Pod的数量总是一致的。   node节点  kubelet：负责管控docker容器，如启动/停止、监控运行状态等。它会定期从etcd获取分配到本机的Pod，并根据Pod信息启动或停止相应的容器。同时，它也会接收apiserver的HTTP请求，汇报Pod的运行状态。 proxy：负责为Pod提供代理。它会定期从etcd获取所有的service，并根据service信息创建代理。当某个客户Pod要访问其他Pod时，访问请求会经过本机proxy做转发。  Kubernetes组件详细介绍 etcd 虽然不是Kubernetes的组件但是有必要提一下，etcd是一个分布式协同数据库，基于Go语言开发，CoreOS公司出品，使用raft一致性算法协同。Kubernetes的主数据库，在安装kubernetes之前就要先安装它，很多开源下项目都用到，老版本的docker swarm也用到了它。目前主要使用的是2.7.x版本，3.0+版本的API变化太大。
APIServer APIServer负责对外提供kubernetes API服务，它运行在master节点上。任何对资源的增删改查都要交给APIServer处理后才能提交给etcd。APIServer总体上由两部分组成：HTTP/HTTPS服务和一些功能性插件。这些功能性插件又分为两种：一部分与底层IaaS平台（Cloud Provide）相关；另一部分与资源管理控制（Admission Control）相关。
Scheduler Scheduler的作用是根据特定的调度算法将pod调度到node节点上，这一过程也被称为绑定。Scheduler调度器的输入是待调度的pod和可用的工作节点列表，输出则是一个已经绑定了pod的节点，这个节点是通过调度算法在工作节点列表中选择的最优节点。
工作节点从哪里来？工作节点并不是由Kubernetes创建，它是由IaaS平台创建，或者就是由用户管理的物理机或者虚拟机。但是Kubernetes会创建一个Node对象，用来描述这个工作节点。描述的具体信息由创建Node对象的配置文件给出。一旦用户创建节点的请求被成功处理，Kubernetes又会立即在内部创建一个node对象，再去检查该节点的健康状况。只有那些当前可用的node才会被认为是一个有效的节点并允许pod调度到上面运行。
工作节点可以通过资源配置文件或者kubectl命令行工具来创建。Kubernetes主要维护工作节点的两个属性：spec和status来描述一个工作节点的期望状态和当前状态。其中，所谓的当前状态信息由3个信息组成：HostIp、NodePhase和Node Condition。
工作节点的动态维护过程依靠Node Controller来完成，它是Kubernetes Controller Manager下属的一个控制器。它会一直不断的检查Kubernetes已知的每台node节点是否正常工作，如果一个之前已经失败的节点在这个检查循环中被检查为可以工作的，那么Node Controller会把这个节点添加到工作节点中，Node Controller会从工作节点中删除这个节点。
Controller Manager Controller Manager运行在集群的Master节点上，是基于pod API的一个独立服务，它重点实现service Endpoint（服务端点）的动态更新。管理着Kubernetes集群中各种控制节点，包括replication Controller和node Controller。
与APIServer相比，APIServer负责接受用户请求并创建对应的资源，而Controller Manager在系统中扮演的角色是在一旁旁默默的管控这些资源，确保他们永远保持在预期的状态。它采用各种管理器定时的对pod、节点等资源进行预设的检查，然后判断出于预期的是否一致，若不一致，则通知APIServer采取行动，比如重启、迁移、删除等。
kubelet kubelet组件工作在Kubernetes的node上，负责管理和维护在这台主机上运行着的所有容器。 kubelet与cAdvisor交互来抓取docker容器和主机的资源信息。 kubelet垃圾回收机制，包括容器垃圾回收和镜像垃圾回收。 kubelet工作节点状态同步。
kube-proxy kube-proxy提供两种功能:
 提供算法将客服端流量负载均衡到service对应的一组后端pod。 使用etcd的watch机制，实现服务发现功能，维护一张从service到endpoint的映射关系，从而保证后端pod的IP变化不会对访问者的访问造成影响。  </description>
    </item>
    
    <item>
      <title>Docker v.s Kubernetes part1</title>
      <link>http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part1/</link>
      <pubDate>Fri, 10 Mar 2017 21:09:47 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-vs-kubernetes-part1/</guid>
      <description> （题图：杭州西湖 Oct 16,2016）
前言 这一系列文章是对比kubernetes 和docker两者之间的差异，鉴于我之前从docker1.10.3起开始使用docker，对原生docker的了解比较多，最近又正在看Kunernetes权威指南（第二版）这本书（P.S感谢电子工业出版社的编辑朋友赠送此书）。这系列文章不是为了比较孰优孰劣，适合自己的才是最好的。
此系列文章中所说的docker指的是*17.03-ce*版本。
概念性的差别 Kubernetes
了解一样东西首先要高屋建瓴的了解它的概念，kubernetes包括以下几种资源对象：
 Pod Service Volume Namespace ReplicaSet Deployment StatefulSet DaemonSet Job  Docker
Docker的资源对象相对于kubernetes来说就简单多了，只有以下几个：
 Service Node Stack Docker  就这么简单，使用一个*docker-compose.yml*即可以启动一系列服务。当然简单的好处是便于理解和管理，但是在功能方面就没有kubernetes那么强大了。
功能性差别  Kubernetes 资源限制 CPU 100m千分之一核为单位，绝对值，requests 和limits，超过这个值可能被杀掉，资源限制力度比docker更细。 Pod中有个最底层的pause 容器，其他业务容器共用他的IP，docker因为没有这层概念，所以没法共用IP，而是使用overlay网络同处于一个网络里来通信。 Kubernetes在rc中使用环境变量传递配置（1.3版本是这样的，后续版本还没有研究过） Kuberentes Label 可以在开始和动态的添加修改，所有的资源对象都有，这一点docker也有，但是资源调度因为没有kubernetes那么层级，所有还是相对比较弱一些。 Kubernetes对象选择机制继续通过label selector，用于对象调度。 Kubernetes中有一个比较特别的镜像，叫做google_containers/pause，这个镜像是用来实现Pod概念的。 HPA horizontal pod autoscaling 横向移动扩容，也是一种资源对象，根据负载变化情况针对性的调整pod目标副本数。 Kubernetes中有三个IP，Node,Pod,Cluster IP的关系比较复杂，docker中没有Cluster IP的概念。 持久化存储，在Kubernetes中有Persistent volume 只能是网络存储，不属于任何node，独立于pod之外，而docker只能使用volume plugin。 多租户管理，kubernetes中有`Namespace，docker暂时没有多租户管理功能。  总体来说Docker架构更加简单，使用起来也没有那么多的配置，只需要每个结点都安装docker即可，调度和管理功能没kubernetes那么复杂。但是kubernetes本身就是一个通用的数据中心管理工具，不仅可以用来管理docker，*pod*这个概念里就可以运行不仅是docker了。
 以后的文章中将结合docker着重讲Kubernetes，基于1.3版本。
 </description>
    </item>
    
    <item>
      <title>Contiv入坑指南-v2plugin</title>
      <link>http://rootsongjc.github.io/blogs/contiv-v2plugin/</link>
      <pubDate>Fri, 10 Mar 2017 11:51:09 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/contiv-v2plugin/</guid>
      <description>(题图：上海交通大学 Oct 22,2016)
继续趟昨天挖的坑。
昨天的issue-776已经得到@gkvijay的回复，原来是因为没有安装contiv/v2plugin的缘故，所以create contiv network失败，我需要自己build一个docker plugin。
查看下这个commit里面有build v2plugin的脚本更改，所以直接调用以下命令就可以build自己的v2plugin。
前提你需要先build出netctl、netmaster、netplugin三个二进制文件并保存到bin目录下，如果你没自己build直接下载release里面的文件保存进去也行。
编译v2plugin插件 修改config.json插件配置文件
{ &amp;quot;manifestVersion&amp;quot;: &amp;quot;v0&amp;quot;, &amp;quot;description&amp;quot;: &amp;quot;Contiv network plugin for Docker&amp;quot;, &amp;quot;documentation&amp;quot;: &amp;quot;https://contiv.github.io&amp;quot;, &amp;quot;entrypoint&amp;quot;: [&amp;quot;/startcontiv.sh&amp;quot;], &amp;quot;network&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;host&amp;quot; }, &amp;quot;env&amp;quot;: [ { &amp;quot;Description&amp;quot;: &amp;quot;To enable debug mode, set to &#39;-debug&#39;&amp;quot;, &amp;quot;Name&amp;quot;: &amp;quot;dbg_flag&amp;quot;, &amp;quot;Settable&amp;quot;: [ &amp;quot;value&amp;quot; ], &amp;quot;Value&amp;quot;: &amp;quot;-debug&amp;quot; }, { &amp;quot;Description&amp;quot;: &amp;quot;VLAN uplink interface used by OVS&amp;quot;, &amp;quot;Name&amp;quot;: &amp;quot;iflist&amp;quot;, &amp;quot;Settable&amp;quot;: [ &amp;quot;value&amp;quot; ], &amp;quot;Value&amp;quot;: &amp;quot;&amp;quot; }, { &amp;quot;Description&amp;quot;: &amp;quot;Etcd or Consul cluster store url&amp;quot;, &amp;quot;Name&amp;quot;: &amp;quot;cluster_store&amp;quot;, &amp;quot;Settable&amp;quot;: [ &amp;quot;value&amp;quot; ], &amp;quot;Value&amp;quot;: &amp;quot;etcd://172.</description>
    </item>
    
    <item>
      <title>Contiv入坑指南-试用全记录</title>
      <link>http://rootsongjc.github.io/blogs/contiv-tryout/</link>
      <pubDate>Thu, 09 Mar 2017 14:23:04 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/contiv-tryout/</guid>
      <description>(题图：山东荣成滨海风力发电场 Jan 31,2017）
关于contiv的介绍请看我的上一篇文章Contiv Intro。
开发环境使用Vagrant搭建，昨天试用了下，真不知道它们是怎么想的，即然是docker插件为啥不直接在docker中开发呢，我有篇文章介绍如何搭建docker开发环境，可以在docker中开发docker，当然也可以用来开发contiv啊😄，只要下载一个docker镜像dockercore/docker:latest即可，不过有点大2.31G，使用阿里云的mirror下载倒是也划算，总比你自己部署一个开发环境节省时间。
Contiv概念解析 Contiv用于给容器创建和分配网路，可以创建策略管理容器的安全、带宽、优先级等，相当于一个SDN。
Group 按容器或Pod的功能给容器分配策略组，通常是按照容器/Pod的label来分组，应用组跟contiv的network不是一一对应的，可以很多应用组属于同一个network或IP subnet。
Polices 用来限定group的行为，contiv支持两种类型的policy：
 Bandwidth 限定应用组的资源使用上限 Isolation 资源组的访问权限  Group可以同时应用一个或多个policy，当有容器调度到该group里就会适用该group的policy。
Network IPv4或IPv6网络，可以配置subnet和gateway。
Contiv中的网络
在contiv中可以配置两种类型的网络
 application network：容器使用的网络 infrastructure network：host namespace的虚拟网络，比如基础设施监控网络  网络封装
Contiv中有两种类型的网络封装
 Routed：overlay topology和L3-routed BGP topology Bridged：layer2 VLAN  Tenant Tenant提供contiv中的namespace隔离。一个tenant可以有很多个network，每个network都有个subnet。该tenant中的用户可以使用它的任意network和subnet的IP。
物理网络中的tenant称作虚拟路由转发(VRF)。Contiv使用VLAN和VXLAN ID来实现外部网络访问，这取决你使用的是layer2、layer3还是Cisco ACI。
Contiv下载 Contiv的编译安装比较复杂，我们直接下载github上的release-1.0.0-beta.3-03-08-2017.18-51-20.UTC文件解压获得二进制文件安装。
 https://github.com/contiv/install/blob/master/README.md这个官方文档已经过时，不要看了。
 如果试用可以的话，我会后续写contiv开发环境搭建的文章。
这个release是2017年3月8日发布的，就在我写这篇文章的前一天。有个最重要的更新是支持docker1.13 swarm mode。
官方安装文档
下载解压后会得到如下几个文件：
 contivk8s k8s专用的 contrib 文件夹，里面有个netctl的bash脚本 netcontiv 这个命令就一个-version选项用来查看contiv的版本😓 netctl contiv命令行工具，用来配置网络、策略、服务负载均衡，使用说明 netmaster contiv的主节点服务 netplugin  下面的安装中用到的只有netctl、netmaster和netplugin这三个二进制文件。</description>
    </item>
    
    <item>
      <title>思科开源docker网络插件Contiv简介</title>
      <link>http://rootsongjc.github.io/blogs/contiv-guide/</link>
      <pubDate>Thu, 09 Mar 2017 11:28:34 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/contiv-guide/</guid>
      <description>(题图：北京蓝色港湾夜景 Feb 11,2017 元宵节)
Contiv是思科开发的docker网络插件，从2015年就开源了，业界通常拿它和Calico比较。貌似Contiv以前还开发过volume plugin，现在销声匿迹了，只有netplugin仍在活跃开发。
容器网络插件 Calico 与 Contiv Netplugin深入比较
还有篇文章讲解了docker网络方案的改进
Contiv Netplugin 简介 Contiv Netplugin 是来自思科的解决方案。编程语言为 Go。它基于 OpenvSwitch，以插件化的形式支持容器访问网络，支持 VLAN，Vxlan，多租户，主机访问控制策略等。作为思科整体支持容器基础设施contiv项目的网络部分，最大的亮点在于容器被赋予了 SDN 能力，实现对容器更细粒度，更丰富的访问控制功能。另外，对 Docker CNM 网络模型的支持，并内置了 IPAM 接口，不仅仅提供了一容器一 IP，而且容器的网络信息被记录的容器配置中，伴随着容器的整个生命周期，减低因为状态不同步造成网络信息丢失的风险。有别于 CNI，这种内聚化的设计有利于减少对第三方模块的依赖。随着项目的发展，除了 Docker，还提供了对 Kubernetes 以及 Mesos 的支持，即 CNI 接口。

 Netmaster 后台进程负责记录所有节点状态，保存网络信息，分配 IP 地址 Netplugin 后台进程作为每个宿主机上的 Agent 与 Docker 及 OVS 通信，处理来自 Docker 的请求，管理 OVS。Docker 方面接口为 remote driver，包括一系列 Docker 定义的 JSON-RPC(POST) 消息。OVS 方面接口为 remote ovsdb，也是 JSON-RPC 消息。以上消息都在 localhost 上处理。 集群管理依赖 etcd/serf</description>
    </item>
    
    <item>
      <title>开源镜像定义工具Packer简介</title>
      <link>http://rootsongjc.github.io/blogs/packer-intro/</link>
      <pubDate>Thu, 09 Mar 2017 10:58:42 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/packer-intro/</guid>
      <description>昨天研究了下Vagrant，感觉它的虚拟机ruby格式定义很麻烦，经人指点还有一个叫做packer的东西，也是Hashicorp这家公司出品的，今天看了下。
Packer是一款开源轻量级的镜像定义工具，可以根据一份定义文件生成多个平台的镜像，支持的平台有：
 Amazon EC2 (AMI). Both EBS-backed and instance-store AMIs Azure DigitalOcean Docker Google Compute Engine OpenStack Parallels QEMU. Both KVM and Xen images. VirtualBox VMware  Packer创造的镜像也能转换成Vagrant boxes。
Packer的镜像创建需要一个json格式的定义文件，例如quick-start.json
{ &amp;quot;variables&amp;quot;: { &amp;quot;access_key&amp;quot;: &amp;quot;{{env `AWS_ACCESS_KEY_ID`}}&amp;quot;, &amp;quot;secret_key&amp;quot;: &amp;quot;{{env `AWS_SECRET_ACCESS_KEY`}}&amp;quot; }, &amp;quot;builders&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;amazon-ebs&amp;quot;, &amp;quot;access_key&amp;quot;: &amp;quot;{{user `access_key`}}&amp;quot;, &amp;quot;secret_key&amp;quot;: &amp;quot;{{user `secret_key`}}&amp;quot;, &amp;quot;region&amp;quot;: &amp;quot;us-east-1&amp;quot;, &amp;quot;source_ami&amp;quot;: &amp;quot;ami-af22d9b9&amp;quot;, &amp;quot;instance_type&amp;quot;: &amp;quot;t2.micro&amp;quot;, &amp;quot;ssh_username&amp;quot;: &amp;quot;ubuntu&amp;quot;, &amp;quot;ami_name&amp;quot;: &amp;quot;packer-example {{timestamp}}&amp;quot; }] }  使用packer build quick-start.json可以在AWS上build一个AIM镜像。
Packer的详细文档：https://www.packer.io/docs/</description>
    </item>
    
    <item>
      <title>Vagrant介绍-从使用到放弃完全指南</title>
      <link>http://rootsongjc.github.io/blogs/vagrant-intro/</link>
      <pubDate>Wed, 08 Mar 2017 20:40:08 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/vagrant-intro/</guid>
      <description>（题图：北京地铁13号线光熙家园夜景 Mar 5,2017）
起源 久闻Vagrant大名，之前经常看到有开源项目使用它作为分布式开发的环境配置。
因为今天在看contiv正好里面使用vagrant搭建的开发测试环境，所以顺便了解下。它的Vagrantfile文件中定义了三台主机。并安装了很多依赖软件，如consul、etcd、docker、go等，整的比较复杂。
➜ netplugin git:(master) ✗ vagrant status Current machine states: netplugin-node1 running (virtualbox) netplugin-node2 running (virtualbox) netplugin-node3 running (virtualbox) This environment represents multiple VMs. The VMs are all listed above with their current state. For more information about a specific VM, run `vagrant status NAME`.  Vagrant是hashicorp这家公司的产品，这家公司主要做数据中心PAAS和虚拟化，其名下大名鼎鼎的产品有Consul、Vault、Nomad、Terraform。他们的产品都是基于Open Source的Github地址。
用途 Vagrant是用来管理虚拟机的，如VirtualBox、VMware、AWS等，主要好处是可以提供一个可配置、可移植和复用的软件环境，可以使用shell、chef、puppet等工具部署。所以vagrant不能单独使用，如果你用它来管理自己的开发环境的话，必须在自己的电脑里安装了虚拟机软件，我使用的是virtualbox。
Vagrant提供一个命令行工具vagrant，通过这个命令行工具可以直接启动一个虚拟机，当然你需要提前定义一个Vagrantfile文件，这有点类似Dockerfile之于docker了。
跟docker类比这来看vagrant就比较好理解了，vagrant也是用来提供一致性环境的，vagrant本身也提供一个镜像源，使用vagrant init hashicorp/precise64就可以初始化一个Ubuntu 12.04的镜像。
用法 你可以下载安装文件来安装vagrant，也可以使用RubyGem安装，它是用Ruby开发的。
Vagrantfile
Vagrantfile是用来定义vagrant project的，使用ruby语法，不过你不必了解ruby就可以写一个Vagrantfile。
看个例子，选自https://github.com/fenbox/Vagrantfile
# -*- mode: ruby -*- # vi: set ft=ruby : # All Vagrant configuration is done below.</description>
    </item>
    
    <item>
      <title>Docker技术选型</title>
      <link>http://rootsongjc.github.io/projects/docker-tech-selection/</link>
      <pubDate>Wed, 08 Mar 2017 10:37:01 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/projects/docker-tech-selection/</guid>
      <description>回顾历史  多少次我回过头看看走过的路，你还在小村旁。
 去年基于docker1.11对Hadoop yarn进行了docker化改造，详情请看大数据集群虚拟化-Yarn on docker始末，我将这个事件命名为magpie，因为它就像是喜鹊一样收集着各种各样的资源搭建自己的小窝。magpie还是有很多事情可以做的，大数据集群的虚拟化也不会止步，它仅仅是对其做了初步的探索，对于资源利用率和管理方面的优化还有很长的路要走，Yarn本身就是做为大数据集群的资源管理调度角色出现的，一开始是为调度MapReduce，后来的spark、hive、tensrflow、HAWQ、slide等等不一而足陆续出现。但是用它来管理docker似乎还是有点过重，还不如用kubernetes、marathon、nomad、swarm等。
但是在微服务方面docker1.11的很多弊端或者说缺点就暴露了出来，首先docker1.11原生并不带cluster管理，需要配合·docker swarm、kubernetes、marathon等才能管理docker集群。之前的对于docker的使用方式基本就是按照虚拟机的方式使用的，固定IP有悖于微服务的原则。
我们基于docker1.11和shrike二层网络模式，还有shipyard来做集群管理，shipyard只是一个简单的docker集群管理的WebUI，基本都是调用docker API，唯一做了一点docker原生没有的功能就是scale容器，而且只支持到docker1.11，早已停止开发。我抛弃了shipyard，它的页面功能基本可有可无，我自己开发的magpie一样可以管理yarn on docker集群。
Docker Swarm有如下几个缺点
 对于大规模集群的管理效率太低，当管理上百个node的时候经常出现有节点状态不同步的问题，比如主机重启后容器已经Exited了，但是master让然认为是Running状态，必须重启所有master节点才行。 没有中心化Node管理功能，必须登录到每台node上手动启停swarm-agent。 集群管理功能实在太太太简陋，查看所有node状态只能用docker info而且那个格式就不提了，shipyard里有处理这个格式的代码，我copy到了magpie里，彻底抛弃shipyard了。 Docker swarm的集群管理概念缺失，因为docker一开始设计的时候就不是用来管理集群的，所以出现了swarm，但是只能使用docker-compose来编排服务，但是无法在swarm集群中使用我们自定义的mynet网络，compose issue-4233，compose也已经被docker官方废弃（最近一年docker发展的太快了，原来用python写的compose已经被用go重构为libcompose直接集成到swarm mode里了），而且docker1.11里也没有像kubernetes那样service的单位，在docker1.11所有的管理都是基于docker容器的。  Docker Swarm的问题也是shipyard的问题，谁让shipyard直接调用docker的API呢。当然，在后续版本的docker里以上问题都已经不是问题，docker已经越来越像kubernetes，不论是在设计理念上还是在功能上，甚至还发行了企业版，以后每个月发布一个版本。
技术选型 主要对比Docker1.11和Docker17.03-ce版本。
首先有一点需要了解的是，docker1.12+带来的swarm mode，你可以使用一个命令直接启动一个复杂的stack，其中包括了服务编排和所有的服务配置，这是一个投票应用的例子。
下表对比了docker1.11和docker17.03-ce
   版本 docker1.11 docker17.03-ce     基本单位 docker容器 docker容器、service、stack   服务编排 compose，不支持docker swarm的mynet网络 改造后的compose，支持stack中完整的服务编排   网络模型 Host、bridge、overlay、mynet 默认支持跨主机的overlay网络，创建单个容器时也可以attach到已有的overla网络中   插件 没有插件管理命令，但是可以手动创建和管理 有插件管理命令，可以手动创建和从docker hub中下载，上传插件到自己的私有镜像仓库   升级 不支持平滑升级，重启docker原来的容器也会停掉 可以停止docker engine但不影响已启动的容器   弹性伸缩 不支持 service内置功能   服务发现 监听docker event增删DNS 内置服务发现，根据DNS负载均衡   节点管理 手动启停 中心化管理node节点   服务升级 手动升级 service内置功能   负载均衡 本身不支持 Swarm mode内部DNS轮寻    基于以上对比，使用docker17.</description>
    </item>
    
    <item>
      <title>Docker源码编译和开发环境搭建</title>
      <link>http://rootsongjc.github.io/blogs/docker-dev-env/</link>
      <pubDate>Mon, 06 Mar 2017 17:03:19 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-dev-env/</guid>
      <description>看了下网上其他人写的docker开发环境搭建，要么是在ubuntu下搭建，要么就是使用官方说明的build docker-dev镜像的方式一步步搭建的，甚是繁琐，docker hub上有一个docker官方推出的dockercore/docker镜像，其实这就是官网上所说的docker-dev镜像，不过以前的那个deprecated了，使用目前这个镜像搭建docker开发环境是最快捷的了。
想要修改docker源码和做docker定制开发的同学可以参考下。
官方指导文档：https://docs.docker.com/opensource/code/
设置docker开发环境：https://docs.docker.com/opensource/project/set-up-dev-env/
docker的编译实质上是在docker容器中运行docker。
因此在本地编译docker的前提是需要安装了docker，还需要用git把代码pull下来。
创建分支 为了方便以后给docker提交更改，我们从docker官方fork一个分支。
git clone https://github.com/rootsongjc/docker.git git config --local user.name &amp;quot;Jimmy Song&amp;quot; git config --local user.email &amp;quot;rootsongjc@gmail.com&amp;quot; git remote add upstream https://github.com/docker/docker.git git config --local -l git remote -v git checkout -b dry-run-test touch TEST.md vim TEST.md git status git add TEST.md git commit -am &amp;quot;Making a dry run test.&amp;quot; git push --set-upstream origin dry-run-test  然后就可以在dry-run-test这个分支下工作了。
配置docker开发环境 官网上说需要先清空自己电脑上已有的容器和镜像。
docker开发环境本质上是创建一个docker镜像，镜像里包含了docker的所有开发运行环境，本地代码通过挂载的方式放到容器中运行，下面这条命令会自动创建这样一个镜像。
在dry-run-test分支下执行
make BIND_DIR=.</description>
    </item>
    
    <item>
      <title>12因素法则</title>
      <link>http://rootsongjc.github.io/blogs/12-factor-app/</link>
      <pubDate>Mon, 27 Feb 2017 22:32:40 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/12-factor-app/</guid>
      <description>Twelve-factor App 简介 如今，软件通常会作为一种服务来交付，它们被称为网络应用程序，或软件即服务（SaaS）。12-Factor 为构建如下的 SaaS 应用提供了方法论：
 使用标准化流程自动配置，从而使新的开发者花费最少的学习成本加入这个项目。 和操作系统之间尽可能的划清界限，在各个系统中提供最大的可移植性。 适合部署在现代的云计算平台，从而在服务器和系统管理方面节省资源。 将开发环境和生产环境的差异降至最低，并使用持续交付实施敏捷开发。 可以在工具、架构和开发流程不发生明显变化的前提下实现扩展。  这套理论适用于任意语言和后端服务（数据库、消息队列、缓存等）开发的应用程序。
背景 本文的贡献者者参与过数以百计的应用程序的开发和部署，并通过 Heroku 平台间接见证了数十万应用程序的开发，运作以及扩展的过程。
本文综合了我们关于 SaaS 应用几乎所有的经验和智慧，是开发此类应用的理想实践标准，并特别关注于应用程序如何保持良性成长，开发者之间如何进行有效的代码协作，以及如何 避免软件污染 。
我们的初衷是分享在现代软件开发过程中发现的一些系统性问题，并加深对这些问题的认识。我们提供了讨论这些问题时所需的共享词汇，同时使用相关术语给出一套针对这些问题的广义解决方案。本文格式的灵感来自于 Martin Fowler 的书籍： *Patterns of Enterprise Application Architecture* ， *Refactoring* 。
12-factors I. 基准代码 一份基准代码，多份部署 II. 依赖 显式声明依赖关系 III. 配置 在环境中存储配置 IV. 后端服务 把后端服务当作附加资源 V. 构建，发布，运行 严格分离构建和运行 VI. 进程 以一个或多个无状态进程运行应用 VII. 端口绑定 通过端口绑定提供服务 VIII. 并发 通过进程模型进行扩展 IX. 易处理 快速启动和优雅终止可最大化健壮性 X. 开发环境与线上环境等价 尽可能的保持开发，预发布，线上环境相同 XI. 日志 把日志当作事件流 XII.</description>
    </item>
    
    <item>
      <title>Docker Service Discovery</title>
      <link>http://rootsongjc.github.io/blogs/docker-service-discovery/</link>
      <pubDate>Mon, 27 Feb 2017 18:27:07 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-service-discovery/</guid>
      <description>Prior to Docker 1.12 release, setting up Swarm cluster needed some sort of service discovery backend. There are multiple discovery backends available like hosted discovery service, using a static file describing the cluster, etcd, consul, zookeeper or using static list of IP address.

Thanks to Docker 1.12 Swarm Mode, we don’t have to depend upon these external tools and complex configurations. Docker Engine 1.12 runs it’s own internal DNS service to route services by name.</description>
    </item>
    
    <item>
      <title>Docker内置DNS</title>
      <link>http://rootsongjc.github.io/blogs/docker-embedded-dns/</link>
      <pubDate>Mon, 27 Feb 2017 18:23:42 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/docker-embedded-dns/</guid>
      <description>本文主要介绍了Docker容器的DNS配置及其注意点，重点对docker 1.10发布的embedded DNS server进行了源码分析，看看embedded DNS server到底是个啥，它是如何工作的。
Configure container DNS DNS in default bridge network    Options Description     -h HOSTNAME or –hostname=HOSTNAME 在该容器启动时，将HOSTNAME设置到容器内的/etc/hosts, /etc/hostname, /bin/bash提示中。   –link=CONTAINER_NAME or ID:ALIAS 在该容器启动时，将ALIAS和CONTAINER_NAME/ID对应的容器IP添加到/etc/hosts. 如果 CONTAINER_NAME/ID有多个IP地址 ？   –dns=IP_ADDRESS… 在该容器启动时，将nameserver IP_ADDRESS添加到容器内的/etc/resolv.conf中。可以配置多个。   –dns-search=DOMAIN… 在该容器启动时，将DOMAIN添加到容器内/etc/resolv.conf的dns search列表中。可以配置多个。   –dns-opt=OPTION… 在该容器启动时，将OPTION添加到容器内/etc/resolv.conf中的options选项中，可以配置多个。     说明：
 如果docker run时不含--dns=IP_ADDRESS..., --dns-search=DOMAIN..., or --dns-opt=OPTION...参数，docker daemon会将copy本主机的/etc/resolv.conf，然后对该copy进行处理（将那些/etc/resolv.conf中ping不通的nameserver项给抛弃）,处理完成后留下的部分就作为该容器内部的/etc/resolv.conf。因此，如果你想利用宿主机中的/etc/resolv.conf配置的nameserver进行域名解析，那么你需要宿主机中该dns service配置一个宿主机内容器能ping通的IP。 如果宿主机的/etc/resolv.conf内容发生改变，docker daemon有一个对应的file change notifier会watch到这一变化，然后根据容器状态采取对应的措施：  如果容器状态为stopped，则立刻根据宿主机的/etc/resolv.</description>
    </item>
    
    <item>
      <title>Raft一致性算法</title>
      <link>http://rootsongjc.github.io/blogs/raft/</link>
      <pubDate>Mon, 27 Feb 2017 10:47:14 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/blogs/raft/</guid>
      <description>这是一个动画演示版的Raft一致性算法的说明，很直观，推荐观看。 http://thesecretlivesofdata.com/raft/
P.S Raft一致性算法在很多软件中都有应用，如Docker（Swarm Mode）、Ectd等，Hadoop生态圈里的Zookeeper用的是艰深的Paxos算法。</description>
    </item>
    
    <item>
      <title>TalkingData Annual Meeting</title>
      <link>http://rootsongjc.github.io/talks/td-annual-meeting/</link>
      <pubDate>Sun, 26 Feb 2017 20:18:54 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/talks/td-annual-meeting/</guid>
      <description>TalkingData Annual Meeting 2017 Dayin Theater, Beijing Friday, Feb 24, 2017
Photo by Jimmy Song</description>
    </item>
    
    <item>
      <title>My github pages</title>
      <link>http://rootsongjc.github.io/projects/my-github-pages/</link>
      <pubDate>Wed, 22 Feb 2017 16:56:04 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/projects/my-github-pages/</guid>
      <description> My Open Source Project  Magpie - Magpie is a command line tool for deploying and managing Yarn on Docker cluster. Docker IPAM plugin - Docker network plugin to make a L2 flat network. Docker practice - Docker in practice Go practice - Go in practice Linux practice - Linux in practice :) Team management - About team management  </description>
    </item>
    
  </channel>
</rss>