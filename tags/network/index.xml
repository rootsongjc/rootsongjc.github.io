<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Network on Jimmy&#39;s blog</title>
    <link>http://rootsongjc.github.io/tags/network/index.xml</link>
    <description>Recent content in Network on Jimmy&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://rootsongjc.github.io/tags/network/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Contiv Intro</title>
      <link>http://rootsongjc.github.io/post/contiv_guide/</link>
      <pubDate>Thu, 09 Mar 2017 11:28:34 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/post/contiv_guide/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://olz1di9xf.bkt.clouddn.com/2017021162.jpg&#34; alt=&#34;蓝色港湾&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(题图：北京蓝色港湾夜景)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Contiv&lt;/strong&gt;是思科开发的docker网络插件，从2015年就开源了，业界通常拿它和Calico比较。貌似Contiv以前还开发过volume plugin，现在销声匿迹了，只有netplugin仍在活跃开发。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://dockone.io/article/1935&#34;&gt;容器网络插件 Calico 与 Contiv Netplugin深入比较&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;还有篇文章讲解了&lt;a href=&#34;http://blog.dataman-inc.com/shurenyun-docker-133/&#34;&gt;docker网络方案的改进&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;contiv-netplugin-简介&#34;&gt;Contiv Netplugin 简介&lt;/h3&gt;

&lt;p&gt;Contiv Netplugin 是来自思科的解决方案。编程语言为 Go。它基于 OpenvSwitch，以插件化的形式支持容器访问网络，支持 VLAN，Vxlan，多租户，主机访问控制策略等。作为思科整体支持容器基础设施contiv项目的网络部分，最大的亮点在于容器被赋予了 SDN 能力，实现对容器更细粒度，更丰富的访问控制功能。另外，对 Docker CNM 网络模型的支持，并内置了 IPAM 接口，不仅仅提供了一容器一 IP，而且容器的网络信息被记录的容器配置中，伴随着容器的整个生命周期，减低因为状态不同步造成网络信息丢失的风险。有别于 CNI，这种内聚化的设计有利于减少对第三方模块的依赖。随着项目的发展，除了 Docker，还提供了对 Kubernetes 以及 Mesos 的支持，即 CNI 接口。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://dockerone.com/uploads/article/20161221/c737f78ce7c50c84e49648aaf771a6b4.png&#34;&gt;&lt;img src=&#34;http://dockerone.com/uploads/article/20161221/c737f78ce7c50c84e49648aaf771a6b4.png&#34; alt=&#34;9260E9B7-43C0-48B8-B5C7-CF8B952959D2.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Netmaster 后台进程负责记录所有节点状态，保存网络信息，分配 IP 地址&lt;/li&gt;
&lt;li&gt;Netplugin 后台进程作为每个宿主机上的 Agent 与 Docker 及 OVS 通信，处理来自 Docker 的请求，管理 OVS。Docker 方面接口为 remote driver，包括一系列 Docker 定义的 JSON-RPC(POST) 消息。OVS 方面接口为 remote ovsdb，也是 JSON-RPC 消息。以上消息都在 localhost 上处理。&lt;/li&gt;
&lt;li&gt;集群管理依赖 etcd/serf&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;http://dockerone.com/uploads/article/20161221/852b276222482c4740b690eb7f078409.png&#34;&gt;&lt;img src=&#34;http://dockerone.com/uploads/article/20161221/852b276222482c4740b690eb7f078409.png&#34; alt=&#34;580469BC-468C-49C8-B29E-8B88143AFE0A.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;netplugin的优势&#34;&gt;Netplugin的优势&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;较早支持CNM模型。与已有的网络基础设施兼容性较高，改造影响小。基于VLAN的平行扩展与现有网络结构地位对等&lt;/li&gt;
&lt;li&gt;SDN能力，能够对容器的网络访问做更精细的控制&lt;/li&gt;
&lt;li&gt;多租户支持，具备未来向混合云/公有云迁移的潜力&lt;/li&gt;
&lt;li&gt;代码规模不大，逻辑结构清晰，并发好，VLAN在公司内部有开发部署运维实践经验，稳定性经过生产环境验证&lt;/li&gt;
&lt;li&gt;&lt;u&gt;&lt;strong&gt;京东&lt;/strong&gt;基于相同的技术栈（OVS + VLAN）已支持10w+ 容器的运行。&lt;/u&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;next&#34;&gt;Next&lt;/h3&gt;

&lt;p&gt;后续文章会讲解contiv netplugin的环境配置和开发。目前还在1.0-beta版本。&lt;strong&gt;Docker store&lt;/strong&gt;上提供了contiv插件的&lt;a href=&#34;https://store.docker.com/plugins/803eecee-0780-401a-a454-e9523ccf86b3&#34;&gt;下载地址&lt;/a&gt;。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>docker service discovery</title>
      <link>http://rootsongjc.github.io/post/docker-service-discovery/</link>
      <pubDate>Mon, 27 Feb 2017 18:27:07 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/post/docker-service-discovery/</guid>
      <description>&lt;p&gt;Prior to Docker 1.12 release, setting up Swarm cluster needed some sort of &lt;a href=&#34;https://docs.docker.com/v1.11/swarm/discovery/&#34;&gt;service discovery backend&lt;/a&gt;. There are multiple discovery backends available like hosted discovery service, using a static file describing the cluster, etcd, consul, zookeeper or using static list of IP address.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pic-intro.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pic-intro.png&#34; alt=&#34;pic-intro&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Thanks to Docker 1.12 Swarm Mode&lt;/strong&gt;, we don’t have to depend upon these external tools and complex configurations. &lt;a href=&#34;https://github.com/docker/docker/releases/tag/v1.12.0-rc5&#34;&gt;Docker Engine 1.12&lt;/a&gt; runs it’s own internal DNS service to route services by name.Swarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How does it help?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When you create a service and provide a name for it, you can use just that name as a target hostname, and it’s going to be automatically resolved to the proper container IP of the service. In short, within the swarm, containers can simply reference other services via their names and the built-in DNS will be used to find the appropriate IP and port automatically. It is important to note that if the service has multiple replicas, &lt;strong&gt;the requests would be round-robin load-balanced&lt;/strong&gt;. This would still work if you didn’t forward any ports when you created your docker services.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic10.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic10.png&#34; alt=&#34;Pic10&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Embedded DNS is not a new concept. It was first included under Docker 1.10 release. Please note that DNS lookup for containers connected to user-defined networks works differently compared to the containers connected to &lt;code&gt;default bridge&lt;/code&gt; network. As of Docker 1.10, the docker daemon implements an embedded DNS server which provides built-in service discovery for any container created with a valid &lt;code&gt;name&lt;/code&gt; or &lt;code&gt;net-alias&lt;/code&gt; or aliased by &lt;code&gt;link&lt;/code&gt;. Moreover,container name configured using &lt;code&gt;--name&lt;/code&gt; is used to discover a container within an user-defined docker network. The embedded DNS server maintains the mapping between the container name and its IP address (on the network the container is connected to).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How does Embedded DNS resolve unqualified names?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic22.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic22.png&#34; alt=&#34;Pic22&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;With Docker 1.12 release, a new API called “service” is being included which clearly talks about the functionality of service discovery.  It is important to note that Service discovery is scoped within the network. What it really means is –  If you have redis application and web client as two separate services , you combine into single application and put them into same network.If you try build your application in such a way that you are trying to reach to redis through name “redis”,it will always resolve to name “redis”. Reason – both of these services are part of the same network. You don’t need to be inside the application trying to resolve this service using FQDN. Reason – FQDN name is not going to be portable which in turn, makes your application non-portable.&lt;/p&gt;

&lt;p&gt;Internally, there is a listener opened inside the container itself. If we try to enter into the container which is providing a service discovery and look at /etc/resolv.conf, we will find that the nameserver entry holds something really different like 127.0.0.11.This is nothing but a loopback address. So, whenever resolver tried to resolve, it will resolve to 127.0.0.11 and this request is rightly trapped.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-12.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-12.png&#34; alt=&#34;Pic-12&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once this request is trapped, it is sent to particular random UDP / TCP port currently being listened under the docker daemon. Consequently, the socket is to be created inside the namespace. When DNS server and daemon gets the request, it knows that this is coming from which specific network, hence gets aware of  the context of from where it is coming from.Once it knows the context, it can generate the appropriate DNS response.&lt;/p&gt;

&lt;p&gt;To demonstrate Service Discovery  under Docker 1.12, I have upgraded Docker 1.12.rc5 to 1.12.0 GA version. The swarm cluster look like:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pico01.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pico01.png&#34; alt=&#34;Pico01&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have created a network called “collabnet” for the new services as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-2.jpg&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-2.jpg&#34; alt=&#34;Pic-2&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let’s create a service called “wordpressdb” under collabnet network :&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-mysql.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-mysql.png&#34; alt=&#34;pico-mysql&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can list the running tasks(containers) and the node on which these containers are running on:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-4.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic-4.png&#34; alt=&#34;Pic-4&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let’s create another service called “wordpressapp” under the same network:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-app.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-app.png&#34; alt=&#34;pico-app&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now, we can list out the number of services running on our swarm cluster as shown below.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-2.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-2.png&#34; alt=&#34;pico-2&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have scaled out the number of wordpressapp and wordpressdb just for demonstration purpose.&lt;/p&gt;

&lt;p&gt;Let’s consider my master node where I have two of the containers running as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pico-1.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pico-1.png&#34; alt=&#34;Pico-1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I can reach out one service(wordpressapp) from another service(wordpressapp) through just service-name as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-last.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-last.png&#34; alt=&#34;pico-last&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also, I can reach out to particular container by its name from other container running different service but on the same network. As shown below, I can reach out to wordpressapp.3.6f8bthp container via wordpressdb.7.e62jl57qqu running wordpressdb.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-tasktoo.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-tasktoo.png&#34; alt=&#34;pico-tasktoo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The below picture depicts the Service Discovery in a nutshell:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic23.png&#34; alt=&#34;Pic23&#34; /&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Every service has Virtual IP(VIP) associated which can be derived as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pic-list.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pic-list.png&#34; alt=&#34;pic-list&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As shown above, each service has an IP address and this IP address maps to multiple container IP address associated with that service. It is important to note that service IP associated with a service does not change even though containers associated with the service dies/ restarts.&lt;/p&gt;

&lt;p&gt;Few important points to remember:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;VIP based services use Linux IPVS load balancing to route to the backend containers. This works only for TCP/UDP protocols. When you use DNS-RR mode services don’t have a VIP allocated. Instead service names resolves to one of the backend container IPs randomly.&lt;/li&gt;
&lt;li&gt;Ping not working for VIP is as designed. Technically, IPVS is a TCP/UDP load-balancer, while ping uses ICMP and hence IPVS is not going to load-balance the ping request.&lt;/li&gt;
&lt;li&gt;For VIP based services the reason ping works on the local node is because the VIP is added a 2nd IP address on the overlay network interface&lt;/li&gt;
&lt;li&gt;You can any of the tools like  dig, nslookup or wget -O- &lt;service name&gt; to demonstrate the service discovery functionality&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below picture depicts that the network is the scope of service discoverability which means that when you have a service running on one network , it is scoped to that network and won’t be able to reach out to different service running on different network(unless it is part of that network).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/SD.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/SD.png&#34; alt=&#34;SD&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let’s dig little further introducing Load-balancing aspect too. To see what is basically enabling the load-balancing functionality, we can go into sandbox of each containers and see how it has been resolved.&lt;/p&gt;

&lt;p&gt;Let’s pick up the two containers running on the master node. We can see the sandbox running through the following command:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-namespace.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-namespace.png&#34; alt=&#34;pico-namespace&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Under /var/run/docker/netns, you will find various namespaces. The namespaces marked with x-{id} represents network namespace managed by the overlay network driver for its operation (such as creating a bridge, terminating vxlan tunnel, etc…). They don’t represent the container network namespace. Since it is managed by the driver, it is not recommended to manipulate anything within this namespace. But if you are curious on the deep dive, then you can use the “nsenter” tool to understand more about this internal namespace.&lt;/p&gt;

&lt;p&gt;We can enter into sandbox through the nsenter utility:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-mangle.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/pico-mangle.png&#34; alt=&#34;pico-mangle&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In case you faced an error stating “nsenter: reassociate to namespace ‘ns/net’ failed: Invalid argument”, I suggest to look at &lt;a href=&#34;http://tinyurl.com/gu5rsw9&#34;&gt;this&lt;/a&gt; workaround.&lt;/p&gt;

&lt;p&gt;10.0.3.4 service IP is marked 0x108 using iptables OUTPUT chain. ipvs uses this marking and load balances it to containers 10.0.3.5 and 10.0.3.6 as shown below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/ipvs.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/ipvs.png&#34; alt=&#34;ipvs&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here are key takeaways from this entire post:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic34.png&#34;&gt;&lt;img src=&#34;http://collabnix.com/wp-content/uploads/2016/07/Pic34.png&#34; alt=&#34;Pic34&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;From &lt;a href=&#34;http://collabnix.com/archives/1504&#34;&gt;http://collabnix.com/archives/1504&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>docker embedded dns</title>
      <link>http://rootsongjc.github.io/post/docker-embedded-dns/</link>
      <pubDate>Mon, 27 Feb 2017 18:23:42 +0800</pubDate>
      
      <guid>http://rootsongjc.github.io/post/docker-embedded-dns/</guid>
      <description>

&lt;p&gt;本文主要介绍了&lt;a href=&#34;http://lib.csdn.net/base/docker&#34;&gt;Docker&lt;/a&gt;容器的DNS配置及其注意点，重点对docker 1.10发布的embedded DNS server进行了源码分析，看看embedded DNS server到底是个啥，它是如何工作的。&lt;/p&gt;

&lt;h2 id=&#34;configure-container-dns&#34;&gt;Configure container DNS&lt;/h2&gt;

&lt;h3 id=&#34;dns-in-default-bridge-network&#34;&gt;DNS in default bridge network&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Options&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;-h HOSTNAME or –hostname=HOSTNAME&lt;/td&gt;
&lt;td&gt;在该容器启动时，将HOSTNAME设置到容器内的/etc/hosts, /etc/hostname, /bin/bash提示中。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–link=CONTAINER_NAME or ID:ALIAS&lt;/td&gt;
&lt;td&gt;在该容器启动时，将ALIAS和CONTAINER_NAME/ID对应的容器IP添加到/etc/hosts. 如果 CONTAINER_NAME/ID有多个IP地址 ？&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns=IP_ADDRESS…&lt;/td&gt;
&lt;td&gt;在该容器启动时，将&lt;code&gt;nameserver IP_ADDRESS&lt;/code&gt;添加到容器内的/etc/resolv.conf中。可以配置多个。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns-search=DOMAIN…&lt;/td&gt;
&lt;td&gt;在该容器启动时，将DOMAIN添加到容器内/etc/resolv.conf的dns search列表中。可以配置多个。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns-opt=OPTION…&lt;/td&gt;
&lt;td&gt;在该容器启动时，将OPTION添加到容器内/etc/resolv.conf中的options选项中，可以配置多个。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;说明：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果docker run时不含&lt;code&gt;--dns=IP_ADDRESS..., --dns-search=DOMAIN..., or --dns-opt=OPTION...&lt;/code&gt;参数，docker daemon会将copy本主机的/etc/resolv.conf，然后对该copy进行处理（将那些/etc/resolv.conf中ping不通的nameserver项给抛弃）,处理完成后留下的部分就作为该容器内部的/etc/resolv.conf。因此，如果你想利用宿主机中的/etc/resolv.conf配置的nameserver进行域名解析，那么你需要宿主机中该dns service配置一个宿主机内容器能ping通的IP。&lt;/li&gt;
&lt;li&gt;如果宿主机的/etc/resolv.conf内容发生改变，docker daemon有一个对应的file change notifier会watch到这一变化，然后根据容器状态采取对应的措施：

&lt;ul&gt;
&lt;li&gt;如果容器状态为stopped，则立刻根据宿主机的/etc/resolv.conf内容更新容器内的/etc/resolv.conf.&lt;/li&gt;
&lt;li&gt;如果容器状态为running，则容器内的/etc/resolv.conf将不会改变，直到该容器状态变为stopped.&lt;/li&gt;
&lt;li&gt;如果容器启动后修改过容器内的/etc/resolv.conf，则不会对该容器进行处理，否则可能会丢失已经完成的修改，无论该容器为什么状态。 
如果容器启动时，用了–dns, –dns-search, or –dns-opt选项，其启动时已经修改了宿主机的/etc/resolv.conf过滤后的内容，因此docker daemon永远不会更新这种容器的/etc/resolv.conf。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;注意&lt;/strong&gt;: docker daemon监控宿主机/etc/resolv.conf的这个file change notifier的实现是依赖linux内核的inotify特性，而inotfy特性不兼容overlay fs，因此使用overlay fs driver的docker deamon将无法使用该/etc/resolv.conf自动更新的功能。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;embedded-dns-in-user-defined-networks&#34;&gt;Embedded DNS in user-defined networks&lt;/h3&gt;

&lt;p&gt;在docker 1.10版本中，docker daemon实现了一个叫做&lt;code&gt;embedded DNS server&lt;/code&gt;的东西，用来当你创建的容器满足以下条件时：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用自定义网络；&lt;/li&gt;
&lt;li&gt;容器创建时候通过&lt;code&gt;--name&lt;/code&gt;,&lt;code&gt;--network-alias&lt;/code&gt; or &lt;code&gt;--link&lt;/code&gt;提供了一个name；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;docker daemon就会利用embedded DNS server对整个自定义网络中所有容器进行名字解析（你可以理解为一个网络中的一种服务发现）。&lt;/p&gt;

&lt;p&gt;因此当你启动容器时候满足以上条件时，该容器的域名解析就不应该去考虑容器内的/etc/hosts, /etc/resolv.conf，应该保持其不变，甚至为空，将需要解析的域名都配置到对应embedded DNS server中。具体配置参数及说明如下：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Options&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;–name=CONTAINER-NAME&lt;/td&gt;
&lt;td&gt;在该容器启动时，会将CONTAINER-NAME和该容器的IP配置到该容器连接到的自定义网络中的embedded DNS server中，由它提供该自定义网络范围内的域名解析&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–network-alias=ALIAS&lt;/td&gt;
&lt;td&gt;将容器的name-ip map配置到容器连接到的其他网络的embedded DNS server中。PS：一个容器可能连接到多个网络中。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–link=CONTAINER_NAME:ALIAS&lt;/td&gt;
&lt;td&gt;在该容器启动时，将ALIAS和CONTAINER_NAME/ID对应的容器IP配置到该容器连接到的自定义网络中的embedded DNS server中，但仅限于配置了该link的容器能解析这条rule。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns=[IP_ADDRESS…]&lt;/td&gt;
&lt;td&gt;当embedded DNS server无法解析该容器的某个dns query时，会将请求foward到这些–dns配置的IP_ADDRESS DNS Server，由它们进一步进行域名解析。注意，这些–dns配置到&lt;code&gt;nameserver IP_ADDRESS&lt;/code&gt;全部由对应的embedded DNS server管理，并不会更新到容器内的/etc/resolv.conf.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns-search=DOMAIN…&lt;/td&gt;
&lt;td&gt;在该容器启动时，会将–dns-search配置的DOMAIN们配置到the embedded DNS server，并不会更新到容器内的/etc/resolv.conf。&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;–dns-opt=OPTION…&lt;/td&gt;
&lt;td&gt;在该容器启动时，会将–dns-opt配置的OPTION们配置到the embedded DNS server，并不会更新到容器内的/etc/resolv.conf。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;说明：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果docker run时不含&lt;code&gt;--dns=IP_ADDRESS..., --dns-search=DOMAIN..., or --dns-opt=OPTION...&lt;/code&gt;参数，docker daemon会将copy本主机的/etc/resolv.conf，然后对该copy进行处理（将那些/etc/resolv.conf中ping不通的nameserver项给抛弃）,处理完成后留下的部分就作为该容器内部的/etc/resolv.conf。因此，如果你想利用宿主机中的/etc/resolv.conf配置的nameserver进行域名解析，那么你需要宿主机中该dns service配置一个宿主机内容器能ping通的IP。&lt;/li&gt;
&lt;li&gt;注意容器内/etc/resolv.conf中配置的DNS server，只有当the embedded DNS server无法解析某个name时，才会用到。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;embedded-dns-server源码分析&#34;&gt;embedded DNS server源码分析&lt;/h2&gt;

&lt;p&gt;所有embedded DNS server相关的代码都在libcontainer项目中，几个最主要的文件分别是&lt;code&gt;/libnetwork/resolver.Go&lt;/code&gt;,&lt;code&gt;/libnetwork/resolver_unix.go&lt;/code&gt;,&lt;code&gt;sandbox_dns_unix.go&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;OK, 先来看看embedded DNS server对象在docker中的定义：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Go&#34;&gt;libnetwork/resolver.go

// resolver implements the Resolver interface
type resolver struct {
    sb         *sandbox
    extDNSList [maxExtDNS]extDNSEntry
    server     *dns.Server
    conn       *net.UDPConn
    tcpServer  *dns.Server
    tcpListen  *net.TCPListener
    err        error
    count      int32
    tStamp     time.Time
    queryLock  sync.Mutex
}

// Resolver represents the embedded DNS server in Docker. It operates
// by listening on container&#39;s loopback interface for DNS queries.
type Resolver interface {
    // Start starts the name server for the container
    Start() error
    // Stop stops the name server for the container. Stopped resolver
    // can be reused after running the SetupFunc again.
    Stop()
    // SetupFunc() provides the setup function that should be run
    // in the container&#39;s network namespace.
    SetupFunc() func()
    // NameServer() returns the IP of the DNS resolver for the
    // containers.
    NameServer() string
    // SetExtServers configures the external nameservers the resolver
    // should use to forward queries
    SetExtServers([]string)
    // ResolverOptions returns resolv.conf options that should be set
    ResolverOptions() []string
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可见，resolver就是embedded DNS server，每个resolver都bind一个sandbox，并定义了一个对应的dns.Server，还定义了外部DNS对象列表，但embedded DNS server无法解析某个name时，就会forward到那些外部DNS。&lt;/p&gt;

&lt;p&gt;Resolver Interface定义了embedded DNS server必须实现的接口，这里会重点关注SetupFunc()和Start()，见下文分析。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;dns.Server的实现，全部交给github.com/miekg/dns，限于篇幅，这里我将不会跟进去分析。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从整个&lt;a href=&#34;http://lib.csdn.net/base/docker&#34;&gt;Container&lt;/a&gt; create的流程上来看，docker daemon对embedded DNS server的处理是从endpoint Join a sandbox开始的:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;libnetwork/endpoint.go


func (ep *endpoint) Join(sbox Sandbox, options ...EndpointOption) error {
    ...

    return ep.sbJoin(sb, options...)
}


func (ep *endpoint) sbJoin(sb *sandbox, options ...EndpointOption) error {
    ...

    if err = sb.populateNetworkResources(ep); err != nil {
        return err
    }

    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sandbox join a sandbox的流程中，会调用sandbox. populateNetworkResources做网络资源的设置，这其中就包括了embedded DNS server的启动。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Go&#34;&gt;libnetwork/sandbox.go
func (sb *sandbox) populateNetworkResources(ep *endpoint) error {
    ...
    if ep.needResolver() {
        sb.startResolver(false)
    }
    ...
}


libnetwork/sandbox_dns_unix.go
func (sb *sandbox) startResolver(restore bool) {
    sb.resolverOnce.Do(func() {
        var err error
        sb.resolver = NewResolver(sb)
        defer func() {
            if err != nil {
                sb.resolver = nil
            }
        }()

        // In the case of live restore container is already running with
        // right resolv.conf contents created before. Just update the
        // external DNS servers from the restored sandbox for embedded
        // server to use.
        if !restore {
            err = sb.rebuildDNS()
            if err != nil {
                log.Errorf(&amp;quot;Updating resolv.conf failed for container %s, %q&amp;quot;, sb.ContainerID(), err)
                return
            }
        }
        sb.resolver.SetExtServers(sb.extDNS)

        sb.osSbox.InvokeFunc(sb.resolver.SetupFunc())
        if err = sb.resolver.Start(); err != nil {
            log.Errorf(&amp;quot;Resolver Setup/Start failed for container %s, %q&amp;quot;, sb.ContainerID(), err)
        }
    })
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;sandbox.startResolver是流程关键:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;通过sanbdox.rebuildDNS生成了container内的/etc/resolv.conf&lt;/li&gt;
&lt;li&gt;通过resolver.SetExtServers(sb.extDNS)设置embedded DNS server的forward DNS list&lt;/li&gt;
&lt;li&gt;通过resolver.SetupFunc()启动两个随机可用端口作为embedded DNS server（127.0.0.11）的TCP和UDP Linstener&lt;/li&gt;
&lt;li&gt;通过resolver.Start()对容器内的iptable进行设置(见下)，并通过miekg/dns启动一个nameserver在53端口提供服务。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面我将逐一介绍上面的各个步骤。&lt;/p&gt;

&lt;h3 id=&#34;sanbdox-rebuilddns&#34;&gt;sanbdox.rebuildDNS&lt;/h3&gt;

&lt;p&gt;sanbdox.rebuildDNS负责构建容器内的resolv.conf，构建规则就是第一节江参数配置时候提到的：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Save the external name servers in resolv.conf in the sandbox&lt;/li&gt;
&lt;li&gt;Add only the embedded server’s IP to container’s resolv.conf&lt;/li&gt;
&lt;li&gt;If the embedded server needs any resolv.conf options add it to the current list&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-Go&#34;&gt;libnetwork/sandbox_dns_unix.go

func (sb *sandbox) rebuildDNS() error {
    currRC, err := resolvconf.GetSpecific(sb.config.resolvConfPath)
    if err != nil {
        return err
    }

    // localhost entries have already been filtered out from the list
    // retain only the v4 servers in sb for forwarding the DNS queries
    sb.extDNS = resolvconf.GetNameservers(currRC.Content, types.IPv4)

    var (
        dnsList        = []string{sb.resolver.NameServer()}
        dnsOptionsList = resolvconf.GetOptions(currRC.Content)
        dnsSearchList  = resolvconf.GetSearchDomains(currRC.Content)
    )

    dnsList = append(dnsList, resolvconf.GetNameservers(currRC.Content, types.IPv6)...)

    resOptions := sb.resolver.ResolverOptions()

dnsOpt:
    ...
    dnsOptionsList = append(dnsOptionsList, resOptions...)

    _, err = resolvconf.Build(sb.config.resolvConfPath, dnsList, dnsSearchList, dnsOptionsList)
    return err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;resolver-setextservers&#34;&gt;resolver.SetExtServers&lt;/h3&gt;

&lt;p&gt;设置embedded DNS server的forward DNS list, 当embedded DNS server不能解析某name时，就会将请求forward到ExtServers。代码很简单，不多废话。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Go&#34;&gt;libnetwork/resolver.go
func (r *resolver) SetExtServers(dns []string) {
    l := len(dns)
    if l &amp;gt; maxExtDNS {
        l = maxExtDNS
    }
    for i := 0; i &amp;lt; l; i++ {
        r.extDNSList[i].ipStr = dns[i]
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;resolver-setupfunc&#34;&gt;resolver.SetupFunc&lt;/h3&gt;

&lt;p&gt;启动两个随机可用端口作为embedded DNS server（127.0.0.11）的TCP和UDP Linstener。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;libnetwork/resolver.go

func (r *resolver) SetupFunc() func() {
    return (func() {
        var err error

        // DNS operates primarily on UDP
        addr := &amp;amp;net.UDPAddr{
            IP: net.ParseIP(resolverIP),
        }

        r.conn, err = net.ListenUDP(&amp;quot;udp&amp;quot;, addr)
        ...

        // Listen on a TCP as well
        tcpaddr := &amp;amp;net.TCPAddr{
            IP: net.ParseIP(resolverIP),
        }

        r.tcpListen, err = net.ListenTCP(&amp;quot;tcp&amp;quot;, tcpaddr)
        ...
    })
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;resolver-start&#34;&gt;resolver.Start&lt;/h3&gt;

&lt;p&gt;resolver.Start中两个重要步骤，分别是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;setupIPTable设置容器内的iptables&lt;/li&gt;
&lt;li&gt;启动dns nameserver在53端口开始提供域名解析服务&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;func (r *resolver) Start() error {
    ...
    if err := r.setupIPTable(); err != nil {
        return fmt.Errorf(&amp;quot;setting up IP table rules failed: %v&amp;quot;, err)
    }
    ...
    tcpServer := &amp;amp;dns.Server{Handler: r, Listener: r.tcpListen}
    r.tcpServer = tcpServer
    go func() {
        tcpServer.ActivateAndServe()
    }()
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;先来看看怎么设置容器内的iptables的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (r *resolver) setupIPTable() error {
    ...
    // 获取setupFunc()时的两个本地随机监听端口
    laddr := r.conn.LocalAddr().String()
    ltcpaddr := r.tcpListen.Addr().String()

    cmd := &amp;amp;exec.Cmd{
        Path:   reexec.Self(),
        // 将这两个端口传给setup-resolver命令并启动执行
        Args:   append([]string{&amp;quot;setup-resolver&amp;quot;}, r.sb.Key(), laddr, ltcpaddr),
        Stdout: os.Stdout,
        Stderr: os.Stderr,
    }
    if err := cmd.Run(); err != nil {
        return fmt.Errorf(&amp;quot;reexec failed: %v&amp;quot;, err)
    }
    return nil
}

// init时就注册setup-resolver对应的handler
func init() {
    reexec.Register(&amp;quot;setup-resolver&amp;quot;, reexecSetupResolver)
}

// setup-resolver对应的handler定义
func reexecSetupResolver() {
    ...
    // 封装iptables数据
    _, ipPort, _ := net.SplitHostPort(os.Args[2])
    _, tcpPort, _ := net.SplitHostPort(os.Args[3])
    rules := [][]string{
        {&amp;quot;-t&amp;quot;, &amp;quot;nat&amp;quot;, &amp;quot;-I&amp;quot;, outputChain, &amp;quot;-d&amp;quot;, resolverIP, &amp;quot;-p&amp;quot;, &amp;quot;udp&amp;quot;, &amp;quot;--dport&amp;quot;, dnsPort, &amp;quot;-j&amp;quot;, &amp;quot;DNAT&amp;quot;, &amp;quot;--to-destination&amp;quot;, os.Args[2]},
        {&amp;quot;-t&amp;quot;, &amp;quot;nat&amp;quot;, &amp;quot;-I&amp;quot;, postroutingchain, &amp;quot;-s&amp;quot;, resolverIP, &amp;quot;-p&amp;quot;, &amp;quot;udp&amp;quot;, &amp;quot;--sport&amp;quot;, ipPort, &amp;quot;-j&amp;quot;, &amp;quot;SNAT&amp;quot;, &amp;quot;--to-source&amp;quot;, &amp;quot;:&amp;quot; + dnsPort},
        {&amp;quot;-t&amp;quot;, &amp;quot;nat&amp;quot;, &amp;quot;-I&amp;quot;, outputChain, &amp;quot;-d&amp;quot;, resolverIP, &amp;quot;-p&amp;quot;, &amp;quot;tcp&amp;quot;, &amp;quot;--dport&amp;quot;, dnsPort, &amp;quot;-j&amp;quot;, &amp;quot;DNAT&amp;quot;, &amp;quot;--to-destination&amp;quot;, os.Args[3]},
        {&amp;quot;-t&amp;quot;, &amp;quot;nat&amp;quot;, &amp;quot;-I&amp;quot;, postroutingchain, &amp;quot;-s&amp;quot;, resolverIP, &amp;quot;-p&amp;quot;, &amp;quot;tcp&amp;quot;, &amp;quot;--sport&amp;quot;, tcpPort, &amp;quot;-j&amp;quot;, &amp;quot;SNAT&amp;quot;, &amp;quot;--to-source&amp;quot;, &amp;quot;:&amp;quot; + dnsPort},
    }
    ...

    // insert outputChain and postroutingchain
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在reexecSetupResolver()中清楚的定义了iptables添加outputChain 和postroutingchain，将到容器内的dns query请求重定向到embedded DNS server(127.0.0.11)上的udp/tcp两个随机可用端口，embedded DNS server(127.0.0.11)的返回数据则重定向到容器内的53端口，这样完成了整个dns query请求。&lt;/p&gt;

&lt;p&gt;模型图如下： 
&lt;img src=&#34;http://img.blog.csdn.net/20170105215440792?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&#34; alt=&#34;这里写图片描述&#34; /&gt;&lt;/p&gt;

&lt;p&gt;贴一张实例图： 
&lt;img src=&#34;http://img.blog.csdn.net/20170105215310369?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&#34; alt=&#34;这里写图片描述&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.blog.csdn.net/20170105215322635?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&#34; alt=&#34;这里写图片描述&#34; /&gt;&lt;/p&gt;

&lt;p&gt;到这里，关于embedded DNS server的源码分析就结束了。当然，其中还有很多细节，就留给读者自己走读代码了。&lt;/p&gt;

&lt;h2 id=&#34;福利&#34;&gt;福利&lt;/h2&gt;

&lt;p&gt;从该时序图中看看embedded DNS server的操作在整个容器create流程中的位置。
&lt;img src=&#34;http://img.blog.csdn.net/20170105215401307?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvV2FsdG9uV2FuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&#34; alt=&#34;这里写图片描述&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>