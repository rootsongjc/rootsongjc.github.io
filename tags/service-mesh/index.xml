<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jimmy Song - 专注于探索后 Kubernetes 时代的云原生新范式 – Service Mesh</title>
    <link>https://jimmysong.io/tags/service-mesh/</link>
    <description>Recent content in Service Mesh on Jimmy Song - 专注于探索后 Kubernetes 时代的云原生新范式</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <copyright>&amp;copy; 2017-2022 Jimmy Song 保留所有权利</copyright>
    <lastBuildDate>Wed, 15 Jun 2022 20:27:49 +0800</lastBuildDate>
    
	  <atom:link href="https://jimmysong.io/tags/service-mesh/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>云原生社区著《深入理解 Istio》正式上市开售</title>
      <link>https://jimmysong.io/blog/istio-service-mesh-book/</link>
      <pubDate>Wed, 15 Jun 2022 20:27:49 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/istio-service-mesh-book/</guid>
      <description>
        
        
        &lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;istio-book.jpg&#34; alt=&#34;云原生社区最新力作 —— 《深入理解 Istio》上市开售&#34; data-caption=&#34;云原生社区最新力作 —— 《深入理解 Istio》上市开售&#34;&gt;
  
  &lt;figcaption&gt;云原生社区最新力作 —— 《深入理解 Istio》上市开售&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;2017 年 5 月，Google、IBM 和 Lyft 联合 &lt;a href=&#34;https://istio.io/latest/news/releases/0.x/announcing-0.1/&#34; title=&#34;宣布&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;宣布&lt;/a&gt;
 将 &lt;a href=&#34;https://istio.io&#34; title=&#34;Istio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio&lt;/a&gt;
 开源，不知不觉中距今已5年有余。在这5年多的时间里，Istio 项目从一颗种子长成了参天大树。尤其是在 2018 年 Istio 1.0 版本发布的接下来两年里，国内有多本关于 Istio 服务网格的图书上市。在 Istio 图书出版领域，我国走在了世界的前列。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;istio-open-source-history.jpg&#34; alt=&#34;Istio 开源时间线&#34; data-caption=&#34;Istio 开源时间线&#34;&gt;
  
  &lt;figcaption&gt;Istio 开源时间线&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;h2 id=&#34;服务网格云原生的核心技术之一&#34;&gt;服务网格：云原生的核心技术之一&lt;/h2&gt;
&lt;p&gt;如今在国内，Istio 几乎可以作为服务网格的代名词，作为 &lt;a href=&#34;https://github.com/cncf/toc/blob/main/DEFINITION.md&#34; title=&#34;CNCF（云原生计算基金会）定义的云原生&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CNCF（云原生计算基金会）定义的云原生&lt;/a&gt;
关键技术之一，服务网格发展至今经历了以下几个阶段。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;探索阶段：2017 —2018 年&lt;/li&gt;
&lt;li&gt;早期采用者阶段：2019—2020 年&lt;/li&gt;
&lt;li&gt;大规模落地及生态发展阶段：2021 年至今&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2018 年，CNCF 对云原生的定义是：云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式 API。&lt;/p&gt;
&lt;p&gt;可见，CNCF 将服务网格加入了云原生定义中，即服务网格是云原生的代表性技术之一。如今， Google 正在将 Istio 捐献给 CNCF，我们有理由相信，成为 CNCF 项目后，Istio 的社区会开放，它未来的发展之路也会更顺畅。&lt;/p&gt;
&lt;h2 id=&#34;服务网格与云原生应用&#34;&gt;服务网格与云原生应用&lt;/h2&gt;
&lt;p&gt;云原生的发展方兴未艾，虽然不断有新的技术和产品出现，但作为整个云原生技术栈的一部分，服务网格在过去一年里不断夯实了它作为“云原生网络基础设施”的定位。下图展示了云原生技术栈模型，其中的每一层都有一些代表性的技术来定义标准。作为新时代的中间件，服务网格与其他云原生技术交相辉映，如 Dapr（分布式应用程序运行时）定义了云原生中间件的能力模型，OAM 定义了云原生应用程序模型等，而服务网格定义了云原生七层网络模型。&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;&lt;img src=&#34;model.jpg&#34; loading=&#34;lazy&#34; decoding=&#34;async&#34;
         alt=&#34;云原生应用模型&#34;
         data-caption=&#34;云原生应用模型&#34; width=&#34;80%&#34;/&gt;&lt;figcaption&gt;
            云原生应用模型
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;为什么需要服务网格&#34;&gt;为什么需要服务网格&lt;/h2&gt;
&lt;p&gt;使用服务网格并非意味着与 Kubernetes 决裂，而是自然而然的事情。Kubernetes 的本质是通过声明配置对应用进行生命周期管理，而服务网格的本质是提供应用间的流量控制和安全性管理，以及可观察性。 假如已经使用 Kubernetes 构建了稳定的微服务平台，那么如何设置服务间调用的负载均衡和流量控制呢？&lt;/p&gt;
&lt;p&gt;Envoy 创造的 xDS 协议被众多开源软件所支持，如 Istio、Linkerd、MOSN 等。Envoy 对服务网格或云原生而言最大的贡献就是定义了 xDS。Envoy 本质上是一个网络代理，是通过 API 配置的现代版代理，基于它衍生出了很多不同的使用场景，如 API 网关、服务网格中的 sidecar 代理和边缘代理。&lt;/p&gt;
&lt;p&gt;技术发展从 Kubernetes 到 Istio，概括来讲有以下原因。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes 的本质是应用的生命周期管理，具体来说，就是应用的部署和管理（扩缩容、自 动恢复、发布）。&lt;/li&gt;
&lt;li&gt;Kubernetes 为微服务提供了可扩展、高弹性的部署和管理平台。&lt;/li&gt;
&lt;li&gt;服务网格的基础是透明代理，先通过 sidecar 代理拦截微服务间的流量，再通过控制平面配置管理微服务的行为。如今，服务网格的部署模式也迎来了新的挑战，sidecar 已经不是服务网格所必须的，基于 gRPC 的无代理的服务网格也在测试中。&lt;/li&gt;
&lt;li&gt;xDS 定义了服务网格配置的协议标准，目前基于 gRPC 的 xDS 也正在开发中。&lt;/li&gt;
&lt;li&gt;服务网格将流量管理从 Kubernetes 中解耦，服务网格内部的流量无须 kube-proxy 组件的支持， 通过接近微服务应用层的抽象，管理服务间的流量，实现安全性和可观察性功能。&lt;/li&gt;
&lt;li&gt;服务网格是对 Kubernetes 中 service 更上层的抽象，它的下一步是 Serverless，这也是Google 在 Istio 之后紧接着推出基于 Kubernetes 和 Istio 之上的 Knative 的原因。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;以社区之名成就开源&#34;&gt;以社区之名成就开源&lt;/h2&gt;
&lt;p&gt;2018 年 5 月，在蚂蚁金服的支持下，ServiceMesher 社区成立。随后，国内刮起了服务网格的旋风，由社区领导的 Istio 官方文档翻译工作也进入白热化阶段。&lt;/p&gt;
&lt;p&gt;随着时间的推移，我感受到系统介绍 Istio 的中文资料匮乏，于是在 2018 年 9 月开始构思写一本关于 Istio 的图书，并在 GitHub 上发起了 Istio Handbook 的开源电子书项目。几个月后，随着服务网格技术的推广及 ServiceMesher 社区规模的扩大，我在社区的线上线下活动中结识了很多同样热衷于 Istio 和服务网格技术的朋友。我们一致决定，一起写一本 Istio 的开源电子书，将社区积累的宝贵文章和经验集结成系统的文字，分享给广大开发者。&lt;/p&gt;
&lt;p&gt;2019 年 3 月，在社区管理委员会的组织下，几十位成员自愿参与并开始共同撰写此书。2020 年 5 月，为了更好地推广云原生技术，丰富社区分享的技术内容，我们成立了云原生社区，并将原有的 ServiceMesher 社区纳入其中，社区运营的内容也从服务网格技术扩展到更加全面的云原生技术。&lt;/p&gt;
&lt;p&gt;2020 年 10 月，这本书主要的内容贡献者组成了编委会，成员分别有我、马若飞、王佰平、王炜、罗广明、赵化冰、钟华和郭旭东。我们在出版社的指导与帮助下，对本书进行了后续的版本升级、完善、优化等工作。经过反复的迭代，这本《深入理解 Isito：云原生服务网格进阶实战》终于和大家见面了。&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;&lt;img src=&#34;cover.jpg&#34; loading=&#34;lazy&#34; decoding=&#34;async&#34;
         alt=&#34;图书封面&#34;
         data-caption=&#34;图书封面&#34; width=&#34;70%&#34;/&gt;&lt;figcaption&gt;
            《深入理解 Istio —— 云原生服务网格进阶实战》封面
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;关于本书&#34;&gt;关于本书&lt;/h2&gt;
&lt;p&gt;Istio 在 1.5 版本后有了重大的架构变化，同时引入或改进了多项功能，例如，引入了智能 DNS 代理、新的资源对象，改进了对虚拟机的支持等。&lt;/p&gt;
&lt;p&gt;本书以 Istio 新版本为基础编写而成，在持续追踪 Istio 社区最新动向的基础上，力求为读者提 供最新、最全面的内容。另外，本书的多位作者都是一线的开发或运维工程师，具有丰富的 Istio 实战经验， 为本书提供了翔实、宝贵的参考案例。&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;&lt;img src=&#34;feature.jpg&#34; loading=&#34;lazy&#34; decoding=&#34;async&#34;
         alt=&#34;本书特色&#34;
         data-caption=&#34;本书特色&#34; width=&#34;70%&#34;/&gt;
&lt;/figure&gt;

&lt;figure class=&#34;mx-auto text-center&#34;&gt;&lt;img src=&#34;target-reader.jpg&#34; loading=&#34;lazy&#34; decoding=&#34;async&#34;
         alt=&#34;面向读者&#34;
         data-caption=&#34;面向读者&#34; width=&#34;70%&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;目前，这本书已经在京东平台上线，要想了解更多有关 Istio 的相关知识，就来读一读这本《深入理解 Isito：云原生服务网格进阶实战》吧！&lt;/p&gt;
&lt;p&gt;京东 618，满 100 减 50，扫码即购！&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;&lt;a href=&#34;https://item.jd.com/13200745.html&#34;&gt;&lt;img src=&#34;qrcode.jpg&#34; loading=&#34;lazy&#34; decoding=&#34;async&#34;
         alt=&#34;购书二维码&#34;
         data-caption=&#34;购书二维码&#34; width=&#34;30%&#34;/&gt;&lt;/a&gt;&lt;figcaption&gt;
            &lt;p&gt;&lt;a href=&#34;https://item.jd.com/13200745.html&#34; title=&#34;点此购买&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;点此购买&lt;/a&gt;&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


      </description>
    </item>
    
    <item>
      <title>请暂时抛弃使用 eBPF 取代服务网格和 sidecar 模式的幻想</title>
      <link>https://jimmysong.io/blog/ebpf-sidecar-and-service-mesh/</link>
      <pubDate>Sat, 11 Jun 2022 11:08:49 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/ebpf-sidecar-and-service-mesh/</guid>
      <description>
        
        
        &lt;p&gt;最近 eBPF 技术在云原生社区中持续火热，在我翻译了《&lt;a href=&#34;https://lib.jimmysong.io/what-is-ebpf/&#34; title=&#34;什么是 eBPF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;什么是 eBPF&lt;/a&gt;
》之后，当阅读“云原生环境中的 eBPF”之后就一直在思考 eBPF 在云原生环境中究竟处于什么地位，发挥什么样的作用。当时我评论说“eBPF 开启了上帝视角，可以看到主机上所有的活动，而 sidecar 只能观测到 pod 内的活动，只要搞好进程隔离，基于 eBPF 的 proxy per-node 才是最佳选择”，再看到 William Morgan 的&lt;a href=&#34;https://buoyant.io/2022/06/07/ebpf-sidecars-and-the-future-of-the-service-mesh/&#34; title=&#34;这篇文章&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这篇文章&lt;/a&gt;
 &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;之后，让我恍然大悟。下面节选翻译了文章我比统同意的观点，即 eBPF 无法替代服务网格和 sidecar，感兴趣的读者可以阅读 William 的原文。&lt;/p&gt;
&lt;h2 id=&#34;什么是-ebpf&#34;&gt;什么是 eBPF&lt;/h2&gt;
&lt;p&gt;在过去，如果你想让应用程序处理网络数据包，那是不可能的。因为应用程序运行在 Linux 用户空间，它是不能直接访问主机的网络缓冲区。缓冲区是由内核管理的，受到内核保护，内核需要确保进程隔离，进程之间不能直接读取对方的网络数据包。正确的做法是，应用程序通过系统调用（syscall）来请求网络数据包信息，这本质上是内核 API 调用——应用程序调用 syscall，内核检查应用程序是否有权限获得其请求的数据包；如果有，就把返回数据包。&lt;/p&gt;
&lt;p&gt;有了 eBPF 之后，应用程序不再需要 syscall，数据包不需要在内核空间和用户空间之间来回交互传递。而是我们将代码直接交给内核，让内核自己执行，这样就可以让代码全速运行，效率更高。eBPF 允许应用程序和内核以安全的方式共享内存，eBPF 允许应用程序直接向内核提交代码，目标都是通过超越系统调用的方式来实现性能提升。&lt;/p&gt;
&lt;p&gt;eBPF 不是银弹，你不能用 eBPF 运行任意程序，实际上 eBPF 可以做的事情是非常有限的。&lt;/p&gt;
&lt;h2 id=&#34;ebpf-的局限性&#34;&gt;eBPF 的局限性&lt;/h2&gt;
&lt;p&gt;eBPF 的局限性也是因为内核造成的。内核中运行的应用程序应当有自己的租户，这些租户之间会争抢系统的内存、磁盘和网络，内核的职责就是隔离和调度这些应用程序的资源，同时内核还要保护确认应用程序的权限，保护其不被其他程序破坏。&lt;/p&gt;
&lt;p&gt;因为我们直接将 eBPF 代码交给内核执行，这绕过了内核安全保护（如 syscall），内核将面临直接的安全风险。为了保护内核，所有 eBPF 程序要想运行都必须先通过一个&lt;strong&gt;验证器&lt;/strong&gt;。但是要想自动验证程序是很困难的，验证器可能会过度限制程序的功能。比如 eBPF 程序不能是阻塞的，不能有无限循环，不能超过预定的大小；其复杂性也受到限制，验证器会评估所有可能的执行路径，如果 eBPF 程序不能在某些范围内完成，或者不能证明每个循环都有一个退出条件，那么验证器就不会允许该程序运行。有很多应用程序都违反了这些限制，要想将它们作为 eBPF 程序来运行的话，要么重写以满足验证器的需求，要么给内核打补丁，来绕过一些验证（这可能比较困难）。不过随着内核版本的升级，这些验证器也变得更加智能，限制也逐渐变得宽松，也有一些创造性的方法来绕过这些限制。&lt;/p&gt;
&lt;p&gt;但总的来说，eBPF 程序能做的事情非常有限。对于一些重量级事件的处理，例如处理全局范围内的 HTTP/2 流量，或者 TLS 握手协商不能在纯 eBPF 环境中完成。充其量，eBPF 可以做其中的一小部分工作，然后调用用户空间应用程序来处理对于 eBPF 来说过于复杂而无法处理的部分。&lt;/p&gt;
&lt;h2 id=&#34;ebpf-与服务网格的关系&#34;&gt;eBPF 与服务网格的关系&lt;/h2&gt;
&lt;p&gt;因为上文所述的 eBPF 的各项限制，七层流量仍然需要用户空间的网络代理来完成，eBPF 并不能替代服务网格。eBPF 可以与 CNI（容器网络接口）一起运行，处理三层/四层流量，而服务网格处理七层流量。&lt;/p&gt;
&lt;h3 id=&#34;每个主机一个代理的模式比-sidecar-更糟&#34;&gt;每个主机一个代理的模式比 sidecar 更糟&lt;/h3&gt;
&lt;p&gt;对于每个主机一个代理（per-host）的模式，服务网格的早期实践者 Linkerd 1.x 就是这么用的，笔者也是从那个时候开始关注服务网格，Linkerd 1.x 还使用了 JVM 虚拟机！但是经过 Linkerd 1.x 的用户实践证明，这种模式相对于 sidecar 模式，对于运维和安全来说会更糟糕。&lt;/p&gt;
&lt;p&gt;为什么说 sidecar 模式比 per-host 模式更好呢？因为 sidecar 模式有以下几个优势，这是 per-host 模式所不具备的：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;代理的资源消耗随着应用程序的负载而变化。随着实例流量的增加，sidecar 会消耗更多的资源，就像应用程序一样。如果应用程序的流量非常小，那么 sidecar 就不需要消耗很多资源。Kubernetes 现有的管理资源消耗的机制，如资源请求和限制以及 OOM kill，都会继续工作。&lt;/li&gt;
&lt;li&gt;代理失败的爆炸半径只限于一个 pod。代理失败与应用失败相同，由 Kubernetes 负责处理失败的 pod。&lt;/li&gt;
&lt;li&gt;代理维护。例如代理版本的升级，是通过如滚动更新，灰度发布等应用程序本身相同的机制完成的。&lt;/li&gt;
&lt;li&gt;安全边界很清楚（而且很小）：在 pod 级别。Sidecar 在应用程序实例的同一安全上下文中运行。它是 pod 的一部分，与应用程序具有一样的 IP 地址。Sidecar 执行策略，并将 mTLS 应用于进出该 pod 的流量，而且它只需要该 pod 的密钥。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;而对于 per-host 模式，就没有上述好处了。代理与应用程序 pod 完全解耦，处理主机上所有 pod 的流量，这样会代理各种问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;代理消耗的资源是高度可变的，这取决于在某个时间点 Kubernetes 调度了多少个 pod 在该主机上。你无法有效的预测特定代理的资源消耗情况，这样代理就有崩溃的风险（原文是这么说的，这点笔者还是存疑的，希望有点读者能解帮忙解释下）。&lt;/li&gt;
&lt;li&gt;主机上 pod 之间的流量争抢问题。因为主机上的所有流量都经过同一个代理，如果有一个应用程序 pod 的流量极高，消耗了代理的所有资源，主机上的其他应用程序就有被饿死的危险。&lt;/li&gt;
&lt;li&gt;代理的爆炸半径很大，而且是不断变化的。代理的故障和升级现在影响到随机的应用程序集合中的一个随机的 pod 子集，意味着任何故障或维护任务都有难以预测的风险。&lt;/li&gt;
&lt;li&gt;使得安全问题更加复杂。以 TLS 为例，主机上的代理必须包含该主机上所有应用程序的密钥，这使得它成为一个新的攻击媒介，容易受到&lt;a href=&#34;https://en.wikipedia.org/wiki/Confused_deputy_problem&#34; title=&#34;混淆代理&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;混淆代理&lt;/a&gt;
问题的影响——代理中的任何 CVE 或漏洞都是潜在的密钥泄露风险。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;简而言之，sidecar 模式继续贯彻了容器级别的隔离保护——内核可以在容器级别执行所有安全保护和公平的多租户调度。容器的隔离仍然可以完美的运行，而 per-host 模式却破坏了这一切，重新引入了争抢式的多租户隔离问题。&lt;/p&gt;
&lt;p&gt;当然 per-host 也不是一无是处，该模式最大的好处是可以成数量级的减少代理的数量，减少网络跳数，这也就减少了资源消耗和网络延迟。但是与该模式带来的运维和安全性问题相比，这些优势都是次要的。我们也可以通过持续优化 sidecar 来弥补 sidecar 模式在这方面的不足，而 per-host 模式的缺陷确是致命性的。&lt;/p&gt;
&lt;p&gt;其实归根结底还是回到了争抢式多租户问题上，那么能否利用现有的内核解决方案，改进一下 per-host 模式中的代理，让其支持多租户呢？比如改造 Envoy 代理，使其支持多租户模式。虽然从理论来说这是可行的，但是工作量巨大，Matt Klein 也觉得不值得这样做 &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;，还不如使用容器来实现租户隔离。而且即使让 per-host 模式中的代理支持了多租户，仍然还有爆炸半径和安全问题需要解决。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;不管有没有 eBPF，在可预见的未来，服务网格都会基于运行在用户空间的 sidecar 代理（proxyless 模式除外）。Sidecar 模式虽然也有弊端，但它依然是既能保持容器隔离和操作的优势，又能处理云原生网络复杂性的最优方案。eBPF 的能力将来是否会发展到可以处理七层网络流量，从而替代服务网格和 sidecar，也许吧，但那一天可能很遥远。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;William Morgan 的 &lt;a href=&#34;https://buoyant.io/2022/06/07/ebpf-sidecars-and-the-future-of-the-service-mesh/&#34; title=&#34;eBPF, sidecars, and the future of the service mesh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;eBPF, sidecars, and the future of the service mesh&lt;/a&gt;
 这篇文章正好回答了我的关于 eBPF、sidecar 的疑问。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;关于 per-host 模式中的代理改造问题，Twitter 上有一个精彩的&lt;a href=&#34;https://twitter.com/mattklein123/status/1522925333053272065&#34; title=&#34;讨论&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;讨论&lt;/a&gt;
。&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;

      </description>
    </item>
    
    <item>
      <title>Istio 1.13 有哪些值得注意的更新？</title>
      <link>https://jimmysong.io/blog/what-is-new-in-istio-1-13/</link>
      <pubDate>Tue, 22 Mar 2022 17:16:50 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/what-is-new-in-istio-1-13/</guid>
      <description>
        
        
        &lt;p&gt;2022 年 2 月 Istio 发布 &lt;a href=&#34;https://istio.io/latest/news/releases/1.13.x/announcing-1.13/&#34; title=&#34;1.13.0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1.13.0&lt;/a&gt;
 和 &lt;a href=&#34;https://istio.io/latest/news/releases/1.13.x/announcing-1.13.1/&#34; title=&#34;1.13.1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1.13.1&lt;/a&gt;
，这篇博客将想你介绍这两个版本中有哪些值得注意的新特性。&lt;/p&gt;
&lt;p&gt;Istio 1.13 是 2022 年的第一个版本，不出意外的话，Istio 团队会依然按照每个季度的频率发布新版本。总体来看，这个版本中的新特性包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对 Kubernetes 更新版本的支持&lt;/li&gt;
&lt;li&gt;引入了一个新的 API——ProxyConfig，用来配置 sidecar proxy&lt;/li&gt;
&lt;li&gt;完善了 Telemetry API&lt;/li&gt;
&lt;li&gt;支持多网络网关的基于主机名的负载均衡器&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;对-kubernetes-版本的支持&#34;&gt;对 Kubernetes 版本的支持&lt;/h2&gt;
&lt;p&gt;我经常看到有人在社区里问 Istio 支持哪些 Kubernetes 版本，其实 Istio 官网中已经明确列出了支持的 Kubernetes 版本，你可以在&lt;a href=&#34;https://istio.io/latest/docs/releases/supported-releases/#support-status-of-istio-releases&#34; title=&#34;这里&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这里&lt;/a&gt;
看到，Istio 1.13 支持 Kubernetes 1.20、1.21、1.22 和 1.23 版本，并在 Kubernetes 1.16、1.17、1.18、1.19 中测试过，但并得到官方支持。&lt;/p&gt;
&lt;p&gt;在配置 Istio 的时候，其实还有很多检查列表，我将他们都记录到了 &lt;a href=&#34;https://github.com/tetratelabs/istio-cheatsheet&#34; title=&#34;Istio cheatsheet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio cheatsheet&lt;/a&gt;
 中，这个项目中整理了很多关于配置 Istio、资源对象的使用、常见问题处理等相关的 cheatsheet，将于近期上线，敬请期待。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;istio-cheatsheet.jpg&#34; alt=&#34;Istio cheatsheet 页面截图&#34; data-caption=&#34;Istio cheatsheet 页面截图&#34;&gt;
  
  &lt;figcaption&gt;Istio cheatsheet 页面截图&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;h2 id=&#34;引入新的-proxyconfig-api&#34;&gt;引入新的 ProxyConfig API&lt;/h2&gt;
&lt;p&gt;在 Istio 1.13 版本之前，如果你想自定义 sidecar proxy 的配置，有两种方式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;方式一：MeshConfig&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;使用 &lt;code&gt;MeshConfig&lt;/code&gt;，在 Mesh 级别使用 IstioOperator 来修改。例如，使用下面的配置来修改 &lt;code&gt;istiod&lt;/code&gt; 的默认发现端口。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;install.istio.io/v1alpha1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;IstioOperator&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;meshConfig&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;	  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;defaultConfig&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;discoveryAddress&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;istiod:15012&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;方式二：Pod 中的 annotation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;你也可以在 Pod 级别使用 annotation 的方式自定义配置，例如在 Pod 中增加下面的配置同样可以修改工作负载所有连接的 &lt;code&gt;istiod&lt;/code&gt; 的默认端口。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;anannotations&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;proxy.istio.io/config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sd&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;discoveryAddress&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;istiod:15012&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;当你同时使用了以上两种方式配置了 sidecar，&lt;code&gt;annotations&lt;/code&gt; 中设置的字段将完全覆盖 &lt;code&gt;MeshConfig&lt;/code&gt; 默认的字段。关于 &lt;code&gt;ProxyConfig&lt;/code&gt; 的所有配置项请参考 &lt;a href=&#34;https://istio.io/latest/docs/reference/config/istio.mesh.v1alpha1/#ProxyConfig&#34; title=&#34;Istio 文档&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio 文档&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;新方式：&lt;code&gt;ProxyConfig&lt;/code&gt; API&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;但是在 1.13 版本中，新增了一个顶级自定义资源 &lt;code&gt;ProxyConfig&lt;/code&gt;，你可以一站式的在一个地方来自定义 sidecar proxy 的配置，你可以通过指定 namespace、使用 &lt;code&gt;selector&lt;/code&gt; 来选择工作负载的范围，就像其他 CRD 一样。目前 Istio 对该 API 的支持有限，关于 &lt;code&gt;ProxyConfig&lt;/code&gt; API 的详细信息请参考 &lt;a href=&#34;https://istio.io/latest/docs/reference/config/networking/proxy-config/&#34; title=&#34;Istio 文档&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio 文档&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;但是不论你用哪种方式自定义 sidecar proxy 的配置，该配置都无法动态生效，需要重启工作负载才可以生效。例如，对于上面的配置，因为你修改了 &lt;code&gt;istiod&lt;/code&gt; 的默认端口，mesh 中的所有工作负载都需要重启才可以与 control plane 建立连接。&lt;/p&gt;
&lt;h2 id=&#34;telemetry-api&#34;&gt;Telemetry API&lt;/h2&gt;
&lt;p&gt;在 Istio 服务网格中，很多扩展和自定义的配置都是通过 &lt;a href=&#34;https://istio.io/latest/docs/reference/config/istio.mesh.v1alpha1/#MeshConfig-ExtensionProvider&#34; title=&#34;&amp;lt;code&amp;gt;MeshConfig&amp;lt;/code&amp;gt;&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;MeshConfig&lt;/code&gt;&lt;/a&gt;
 的方式来完成的。可观察性的三种类型 Metric、遥测和日志，分别可以对接不同的提供者，&lt;a href=&#34;https://istio.io/latest/docs/tasks/observability/telemetry/&#34; title=&#34;Telemetry API&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Telemetry API&lt;/a&gt;
 可以让你有一个一站式的灵活的配置它们。与 ProxyConfig API 类似，Telemetry API 也遵循着工作负载选择器&amp;gt;本地命名空间&amp;gt;根配置命名空间的配置层级关系。该 API 是在 Istio 1.11 中引入，在该版本中得到了进一步完善，增加了 &lt;code&gt;OpenTelemetry&lt;/code&gt; 日志、过滤访问日志以及自定义跟踪服务名称的支持。详见 &lt;a href=&#34;https://istio.io/latest/docs/reference/config/telemetry/&#34; title=&#34;Telemetry 配置&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Telemetry 配置&lt;/a&gt;
。&lt;/p&gt;
&lt;h2 id=&#34;自动解析多网络网关主机名&#34;&gt;自动解析多网络网关主机名&lt;/h2&gt;
&lt;p&gt;2021 年 9 月，Istio 社区里&lt;a href=&#34;https://szabo.jp/2021/09/22/multicluster-istio-on-eks/&#34; title=&#34;有人报告&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;有人报告&lt;/a&gt;
，在 AWS EKS 中运行多集群多主的 Istio 时，出现 EKS 的负载均衡器无法解析的问题。对于多集群多网络的网格，跨集群边界的服务负载，需要通过专用的东西向网关，以间接的方式通讯。你可以按照 &lt;a href=&#34;https://istio.io/latest/docs/setup/install/multicluster/multi-primary_multi-network/&#34; title=&#34;Istio 官网上的说明&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio 官网上的说明&lt;/a&gt;
配置多网络的 primary-remote 集群，Istio 会根据主机名自动解析负载均衡器的 IP 地址。&lt;/p&gt;
&lt;h2 id=&#34;istio-1131-修复重大安全漏洞&#34;&gt;Istio 1.13.1 修复重大安全漏洞&lt;/h2&gt;
&lt;p&gt;当月，Istio 1.13.1 发布，修复了一个已知的&lt;a href=&#34;https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=CVE-2022-23635&#34; title=&#34;重大漏洞&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;重大漏洞&lt;/a&gt;
，该漏洞可能导致未经认证的控制平面拒绝服务攻击。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;primary-remote-cluster-mesh.jpg&#34; alt=&#34;跨网络的主从集群&#34; data-caption=&#34;跨网络的主从集群&#34;&gt;
  
  &lt;figcaption&gt;跨网络的主从集群&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;在安装多网络的 &lt;a href=&#34;https://istio.io/latest/docs/setup/install/multicluster/multi-primary_multi-network/&#34; title=&#34;primary-remote&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;primary-remote&lt;/a&gt;
 模式的 Istio 网格时，为了让 remote Kubernetes 集群能够访问控制平面，需要在 primary 集群中安装一个东西向的 Gateway，将控制平面 &lt;code&gt;istiod&lt;/code&gt; 的 15012 端口暴露到互联网。攻击者可能向该端口发送特制的消息，导致控制平面崩溃。如果你设置了防火墙，只允许来自部分 IP 的流量访问该端口，将可以缩小该问题的影响范围。建议你立即升级到 Istio 1.13.1 来彻底解决该问题。&lt;/p&gt;
&lt;h2 id=&#34;istiocon-2022&#34;&gt;IstioCon 2022&lt;/h2&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;istiocon-2022.jpg&#34; alt=&#34;IstioCon 2022&#34; data-caption=&#34;IstioCon 2022&#34;&gt;
  
  &lt;figcaption&gt;IstioCon 2022&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;最后，作为上一届和本届 IstioCon 的筹备委员会成员之一，我号召大家报名参加 4 月 25 日在线上举行的 &lt;a href=&#34;https://events.istio.io/istiocon-2022/&#34; title=&#34;IstioCon 2022&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IstioCon 2022&lt;/a&gt;
！IstioCon 2022是一个以行业为重点的活动，一个连接贡献者和用户的平台，讨论Istio在不同架构设置中的用途，有哪些限制，以及项目的下一步发展方向。主要的焦点将是在最终用户公司，因为我们期待着分享多样化的案例研究，展示如何在生产中使用Istio。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>服务网格 2021 年终盘点：实用当先，生态为本</title>
      <link>https://jimmysong.io/blog/service-mesh-2021/</link>
      <pubDate>Tue, 11 Jan 2022 17:16:50 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/service-mesh-2021/</guid>
      <description>
        
        
        &lt;p&gt;随着服务网格架构理念的深入人心，它的适用场景也慢慢为众人所了解，社区中也不乏争论，甚至是质疑的声音。笔者以在云原生和服务网格社区中多年的观察，将从亲历者的角度总结服务网格在 2021 年的进展。因为当前在国内 Istio 几乎是服务网格的代名词，本文也将主要从 Istio 的技术和生态层面来解读服务网格在 2021 年的发展。&lt;/p&gt;
&lt;h2 id=&#34;服务网格云原生的核心技术之一&#34;&gt;服务网格：云原生的核心技术之一&lt;/h2&gt;
&lt;p&gt;作为 &lt;a href=&#34;https://github.com/cncf/toc/blob/main/DEFINITION.md&#34; title=&#34;CNCF 定义的云原生&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CNCF 定义的云原生&lt;/a&gt;
关键技术之一，服务网格发展至今已经有五个年头了，其发展经历了以下几个时期：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;探索阶段：2017 年-2018 年&lt;/li&gt;
&lt;li&gt;早期采用者阶段：2019 年-2020 年&lt;/li&gt;
&lt;li&gt;大规模落地及生态发展阶段：2021 年至今&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果根据&lt;a href=&#34;https://thinkinsights.net/strategy/crossing-the-chasm/&#34; title=&#34;“跨越鸿沟”理论&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“跨越鸿沟”理论&lt;/a&gt;
，服务网格已经跨越了“鸿沟”，处于“早期大众”和“晚期大众”阶段之间。根据&lt;a href=&#34;https://cloudnative.to/sig-istio/big-talk/overview.html&#34; title=&#34;《Istio 大咖说》&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;《Istio 大咖说》&lt;/a&gt;
观众中的反馈来看，用户已不再盲从于新技术，开始辩证的考虑&lt;a href=&#34;https://cloudnative.to/sig-istio/begin/before-you-begin.html&#34; title=&#34;是否真的需要引入服务网格&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;是否真的需要引入服务网格&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;008i3skNly1gyc468pf0sj318g0p0tax.jpg&#34; alt=&#34;跨越鸿沟理论&#34; data-caption=&#34;跨越鸿沟理论&#34;&gt;
  
  &lt;figcaption&gt;跨越鸿沟理论&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;云原生的发展方兴未艾，虽然不断有新的技术和产品出现，但作为整个云原生技术栈的一部分，服务网格在过去一年里不断夯实了它作为“云原生网络基础设施”的定位。下图展示了云原生技术栈模型，其中每一层有一些代表性的技术来定义标准。作为新时代的中间件，服务网格与其他云原生技术交相辉映，如 Dapr（分布式应用程序运行时）定义云原生中间件的能力模型，OAM 定义云原生应用程序模型等，而服务网格定义的是云原生七层网络模型。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;008i3skNly1gyc4698fi8j30w40u0adk.jpg&#34; alt=&#34;云原生技术栈&#34; data-caption=&#34;云原生技术栈&#34;&gt;
  
  &lt;figcaption&gt;云原生技术栈&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;h2 id=&#34;社区焦点&#34;&gt;社区焦点&lt;/h2&gt;
&lt;p&gt;过去一年中，社区的焦点主要集中在以下几个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;性能优化&lt;/strong&gt;：服务网格在大规模应用场景下的性能问题；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;协议扩展&lt;/strong&gt;：让服务网格支持任意七层网络协议；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;部署模式&lt;/strong&gt;：Proxyless vs Node 模式 vs Sidecar 模式；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;引入 eBPF&lt;/strong&gt;：将服务网格的部分能力下沉到内核层；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;性能优化&#34;&gt;性能优化&lt;/h3&gt;
&lt;p&gt;Istio 设计之初的目标就是通过“原协议转发”的方式服务于服务间流量，让服务网格尽可能对应用程序“透明”，从而使用了 &lt;a href=&#34;https://jimmysong.io/blog/envoy-sidecar-injection-in-istio-service-mesh-deep-dive/&#34; title=&#34;IPtables 劫持流量&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IPtables 劫持流量&lt;/a&gt;
，根据&lt;a href=&#34;https://istio.io/latest/zh/blog/2019/performance-best-practices/&#34; title=&#34;社区提供的测试结果&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;社区提供的测试结果&lt;/a&gt;
，对于在 16 个连接上具有 1000 RPS 的网格，Istio 1.2 仅增加了 3 毫秒的基准延迟。但是，因为 IPtables conntrack 模块所固有的问题，随着网格规模的扩大，Istio 的性能问题开始显现。关于 Istio sidecar 的资源占用及网络延迟的性能优化，社区给出了以下解决方案：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sidecar 配置：通过手动或在控制平面增加一个 Operator 的方式来配置服务的依赖项，可以减少向 Sidecar 中下发的服务配置数量，从而降低数据平面的资源占用；为了更加自动和智能地配置 Sidecar，开源项目 &lt;a href=&#34;https://cloudnative.to/blog/smart-istio-management-plane-slime/&#34; title=&#34;Slime&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slime&lt;/a&gt;
 及 &lt;a href=&#34;https://github.com/aeraki-framework/aeraki&#34; title=&#34;Aeraki&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Aeraki&lt;/a&gt;
 都给出了各自的配置懒加载方案；&lt;/li&gt;
&lt;li&gt;引入 eBPF：eBPF 可以作为优化服务网格性能的一种可行性方案，有基于 Cilium 的初创公司甚至激进的提出&lt;a href=&#34;https://cloudnative.to/blog/ebpf-solve-service-mesh-sidecar/&#34; title=&#34;使用 eBPF/Cilium 完全替换 Sidecar 代理&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;使用 eBPF/Cilium 完全替换 Sidecar 代理&lt;/a&gt;
的策略，但事实上 Envoy 代理/xDS 协议已经成为服务网格实现的实际代理，且很好的支持七层协议。eBPF 可用来改善网络性能，但复杂的协议协商、解析和用户扩展在用户侧依然很难实现。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;协议扩展&#34;&gt;协议扩展&lt;/h3&gt;
&lt;p&gt;如何扩展 Istio 一直以来就是一个老大难的问题。Istio 的可扩展包含两方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;协议层面：让 Istio 支持所有七层协议&lt;/li&gt;
&lt;li&gt;生态层面：让 Istio 可以运行更多的插件&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Istio 使用的是 Envoy 作为数据平面，扩展 Istio 本质上就是对 Envoy 功能的扩展。Istio 官方目前给出的方案是使用 WebAssembly，并在&lt;a href=&#34;https://cloudnative.to/blog/istio-wasm-extensions-and-ecosystem/&#34; title=&#34; Istio 1.12 引入 Wasm 插件配置 API 用于扩展 Istio 生态&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt; Istio 1.12 引入 Wasm 插件配置 API 用于扩展 Istio 生态&lt;/a&gt;
，Istio 的扩展机制使用 &lt;a href=&#34;https://github.com/proxy-wasm/spec&#34; title=&#34;Proxy-Wasm 应用二进制接口（ABI）&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Proxy-Wasm 应用二进制接口（ABI）&lt;/a&gt;
规范，提供了一套代理无关的流媒体 API 和实用功能，可以用任何有合适 SDK 的语言来实现。截至目前，Proxy-Wasm 的 SDK 有 AssemblyScript（类似 TypeScript）、C++、Rust、Zig 和 Go（使用 TinyGo WebAssembly 系统接口）。&lt;/p&gt;
&lt;p&gt;目前 WebAssembly 扩展应用还比较少，很多企业选择自定义 CRD，基于 Istio 构建服务网格管理平面。另外，让 Istio 支持异构环境，适用于一切工作负载，如虚拟机、容器，这个对于终端用户来说也有很强的需求，因为这可以让用户很方便的从传统负载迁移应用到服务网格中。最后是多集群、多网格的混合云流量管理，这个属于比较高阶的需求了。&lt;/p&gt;
&lt;h3 id=&#34;部署模式&#34;&gt;部署模式&lt;/h3&gt;
&lt;p&gt;在服务网格概念兴起之初就有 Per-node 和 Sidecar 模式之争，他们的代表分别是 Linkerd 和 Istio。后来 eBPF 提出将服务网格下沉的内核，从而演化出了更多的服务网格部署模式，如下图所示。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;service-mesh-arch.png&#34; alt=&#34;服务网格的部署模式&#34; data-caption=&#34;服务网格的部署模式&#34;&gt;
  
  &lt;figcaption&gt;服务网格的部署模式&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;下表中详细对比了这四种部署方式，它们各有优劣，具体选择哪种根据实际情况而定。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;模式&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;内存开销&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;安全性&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;故障域&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;运维&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Sidecar 代理&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;因为为每个 pod 都注入一个代理，所以开销最大。&lt;/td&gt;
&lt;td&gt;由于 sidecar 必须与工作负载一起部署，工作负载有可能绕过 sidecar。&lt;/td&gt;
&lt;td&gt;Pod 级别隔离，如果有代理出现故障，只影响到 Pod 中的工作负载。&lt;/td&gt;
&lt;td&gt;可以单独升级某个工作负载的 sidecar 而不影响其他工作负载。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;节点共享代理&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;每个节点上只有一个代理，为该节点上的所有工作负载所共享，开销小。&lt;/td&gt;
&lt;td&gt;对加密内容和私钥的管理存在安全隐患。&lt;/td&gt;
&lt;td&gt;节点级别隔离，如果共享代理升级时出现版本冲突、配置冲突或扩展不兼容等问题，则可能会影响该节点上的所有工作负载。&lt;/td&gt;
&lt;td&gt;不需要考虑注入 Sidecar 的问题。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Service Account/节点共享代理&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;服务账户/身份下的所有工作负载都使用共享代理，开销小。&lt;/td&gt;
&lt;td&gt;工作负载和代理之间的连接的认证及安全性无法保障。&lt;/td&gt;
&lt;td&gt;节点和服务账号之间级别隔离，故障同“节点共享代理”。&lt;/td&gt;
&lt;td&gt;同“节点共享代理”。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;带有微代理的共享远程代理&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;因为为每个 pod 都注入一个微代理，开销比较大。&lt;/td&gt;
&lt;td&gt;微代理专门处理 mTLS，不负责 L7 路由，可以保障安全性。&lt;/td&gt;
&lt;td&gt;当需要应用7层策略时，工作负载实例的流量会被重定向到L7代理上，若不需要，则可以直接绕过。该L7代理可以采用共享节点代理、每个服务账户代理，或者远程代理的方式运行。&lt;/td&gt;
&lt;td&gt;同“Sidecar 代理”。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;figcaption class=&#34;text-center&#34;&gt;
    服务网格的部署模式
&lt;/figcaption&gt;

&lt;h3 id=&#34;生态发展&#34;&gt;生态发展&lt;/h3&gt;
&lt;p&gt;2021 年，Istio 社区也是精彩纷呈，举办了系列的活动，还发布了系列教程：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2 月，首个 Istio 发行版，&lt;a href=&#34;https://istio.tetratelabs.io/&#34; title=&#34; Tetrate Istio Distro（TID）&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt; Tetrate Istio Distro（TID）&lt;/a&gt;
 发布；&lt;/li&gt;
&lt;li&gt;2 月，第一届 &lt;a href=&#34;https://events.istio.io/istiocon-2021/&#34; title=&#34;IstioCon&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IstioCon&lt;/a&gt;
 在线上举办，2000 多人参与了会议；&lt;/li&gt;
&lt;li&gt;3 月，首个免费的线上 &lt;a href=&#34;https://academy.tetrate.io/courses/istio-fundamentals-zh&#34; title=&#34;Istio 基础教程&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio 基础教程&lt;/a&gt;
发布；&lt;/li&gt;
&lt;li&gt;5 月，首个 &lt;a href=&#34;https://academy.tetrate.io/courses/certified-istio-administrator&#34; title=&#34;Istio 管理员认证考试（CIAT）&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio 管理员认证考试（CIAT）&lt;/a&gt;
发布；&lt;/li&gt;
&lt;li&gt;5 月，ServiceMeshCon Europe 在线上举办；&lt;/li&gt;
&lt;li&gt;7 月，&lt;a href=&#34;https://istio.io/latest/zh/blog/2021/istiomeetups-china/&#34; title=&#34;Istio Meetup China&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio Meetup China&lt;/a&gt;
 在北京举办，100 多人现场参加；&lt;/li&gt;
&lt;li&gt;10 月，ServiceMeshCon North America 在洛杉矶举办；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外还有众多与 Istio 服务网格相关的项目开源，如下表所示。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;项目名称&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;开源时间&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;类别&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;描述&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;主导公司&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Star 数量&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;与 Istio 的关系&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/envoyproxy/envoy&#34; title=&#34;Envoy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Envoy&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;2016年 9 月&lt;/td&gt;
&lt;td&gt;网络代理&lt;/td&gt;
&lt;td&gt;云原生高性能边缘/中间服务代理&lt;/td&gt;
&lt;td&gt;Lyft&lt;/td&gt;
&lt;td&gt;18700&lt;/td&gt;
&lt;td&gt;默认的数据平面&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/istio/istio/&#34; title=&#34;Istio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;2017 年 5 月&lt;/td&gt;
&lt;td&gt;服务网格&lt;/td&gt;
&lt;td&gt;连接、保护、控制和观察服务。&lt;/td&gt;
&lt;td&gt;Google&lt;/td&gt;
&lt;td&gt;29100&lt;/td&gt;
&lt;td&gt;控制平面&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/linkerd/linkerd2&#34; title=&#34;Linkerd2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Linkerd2&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;2017 年 12 月&lt;/td&gt;
&lt;td&gt;服务网格&lt;/td&gt;
&lt;td&gt;适用于 Kubernetes 的轻量级服务网格。&lt;/td&gt;
&lt;td&gt;Buoyant&lt;/td&gt;
&lt;td&gt;7900&lt;/td&gt;
&lt;td&gt;服务网格的另一种实现&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/emissary-ingress/emissary&#34; title=&#34;Emissary Gateway&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Emissary Gateway&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;2018 年 2 月&lt;/td&gt;
&lt;td&gt;网关&lt;/td&gt;
&lt;td&gt;用于微服务的 Kubernetes 原生 API 网关，基于 Envoy 构建&lt;/td&gt;
&lt;td&gt;Ambassador&lt;/td&gt;
&lt;td&gt;3600&lt;/td&gt;
&lt;td&gt;可连接 Istio&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/apache/apisix&#34; title=&#34;APISIX&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;APISIX&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;2019 年 6 月&lt;/td&gt;
&lt;td&gt;网关&lt;/td&gt;
&lt;td&gt;云原生 API 网关&lt;/td&gt;
&lt;td&gt;API7&lt;/td&gt;
&lt;td&gt;8100&lt;/td&gt;
&lt;td&gt;可作为 Istio 的数据平面运行也可以单独作为网关&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/mosn/mosn&#34; title=&#34;MOSN&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MOSN&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;2019 年 12 月&lt;/td&gt;
&lt;td&gt;代理&lt;/td&gt;
&lt;td&gt;云原生边缘网关及代理&lt;/td&gt;
&lt;td&gt;蚂蚁&lt;/td&gt;
&lt;td&gt;3500&lt;/td&gt;
&lt;td&gt;可作为 Istio 数据平面&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/slime-io/slime&#34; title=&#34;Slime&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slime&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;2021 年 1月&lt;/td&gt;
&lt;td&gt;扩展&lt;/td&gt;
&lt;td&gt;基于 Istio 的智能服务网格管理器&lt;/td&gt;
&lt;td&gt;网易&lt;/td&gt;
&lt;td&gt;236&lt;/td&gt;
&lt;td&gt;为 Istio 增加一个管理平面&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/tetratelabs/getmesh&#34; title=&#34;Tetrate Istio Distro&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tetrate Istio Distro&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;2021 年 2 月&lt;/td&gt;
&lt;td&gt;工具&lt;/td&gt;
&lt;td&gt;Istio 集成和命令行管理工具&lt;/td&gt;
&lt;td&gt;Tetrate&lt;/td&gt;
&lt;td&gt;95&lt;/td&gt;
&lt;td&gt;第一个 Istio 开源发行版和多版本管理工具&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/aeraki-framework/aeraki&#34; title=&#34;Aeraki&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Aeraki&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;2021 年 3 月&lt;/td&gt;
&lt;td&gt;扩展&lt;/td&gt;
&lt;td&gt;管理 Istio 的任何七层负载&lt;/td&gt;
&lt;td&gt;腾讯&lt;/td&gt;
&lt;td&gt;330&lt;/td&gt;
&lt;td&gt;扩展多协议支持&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/mosn/layotto/&#34; title=&#34;Layotto&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Layotto&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;2021 年 6 月&lt;/td&gt;
&lt;td&gt;运行时&lt;/td&gt;
&lt;td&gt;云原生应用运行时&lt;/td&gt;
&lt;td&gt;蚂蚁&lt;/td&gt;
&lt;td&gt;393&lt;/td&gt;
&lt;td&gt;可以作为 Istio 的数据平面&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/hango-io/hango-gateway&#34; title=&#34;Hango Gateway&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hango Gateway&lt;/a&gt;
&lt;/td&gt;
&lt;td&gt;2021 年 8 月&lt;/td&gt;
&lt;td&gt;网关&lt;/td&gt;
&lt;td&gt;基于 Envoy 和 Istio 构建的 API 网关&lt;/td&gt;
&lt;td&gt;网易&lt;/td&gt;
&lt;td&gt;253&lt;/td&gt;
&lt;td&gt;可与 Istio 集成&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;figcaption class=&#34;text-center&#34;&gt;
    Istio 开源生态
&lt;/figcaption&gt;

&lt;div class=&#34;alert&#34;&gt;

&lt;div class=&#34;alert-note py-1 px-2&#34;&gt;
  注：数据统计截止到 2022 年 1 月 6 日。
&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;回望 2021 年，我们可以看出用户对服务网格的追求更趋实用，作为云原生网络的基础设施，其地位得到进一步夯实，更重要的是服务网格生态渐起。展望 2022 年，有两个值得关注的技术是 eBPF 和 WebAssembly。我们有理由相信，更多的服务网格实践优秀案例出现，在生态和标准化上更进一步。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/ebpf-solve-service-mesh-sidecar/&#34; title=&#34;告别 Sidecar——使用 eBPF 解锁内核级服务网格&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;告别 Sidecar——使用 eBPF 解锁内核级服务网格&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/smart-istio-management-plane-slime/&#34; title=&#34;网易开源 Istio 扩展项目 Slime 简介——基于 Istio 的智能服务网格管理器&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;网易开源 Istio 扩展项目 Slime 简介——基于 Istio 的智能服务网格管理器&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/istio-wasm-extensions-and-ecosystem/&#34; title=&#34;Istio 1.12 引入 Wasm 插件配置 API 用于扩展 Istio 生态&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio 1.12 引入 Wasm 插件配置 API 用于扩展 Istio 生态&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/grpc-proxyless-service-mesh/&#34; title=&#34;基于 GRPC 和 Istio 的无 Sidecar 代理的服务网格&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;基于 GRPC 和 Istio 的无 Sidecar 代理的服务网格&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/how-ebpf-streamlines-the-service-mesh/&#34; title=&#34;eBPF 如何简化服务网格&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;eBPF 如何简化服务网格&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/sig-istio/begin/before-you-begin.html&#34; title=&#34;使用 Isito 前的考虑要素&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;使用 Isito 前的考虑要素&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>网易开源 Istio 扩展项目 Slime 简介——基于 Istio 的智能服务网格管理器</title>
      <link>https://jimmysong.io/blog/slime-intro/</link>
      <pubDate>Wed, 24 Nov 2021 14:43:27 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/slime-intro/</guid>
      <description>
        
        
        &lt;p&gt;最近我在研究 Istio 生态中的开源项目，&lt;a href=&#34;https://github.com/slime-io/slime/&#34; title=&#34;Slime&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slime&lt;/a&gt;
 这个项目开源与 2021 年初，是由网易数帆微服务团队开源的一款基于 Istio 的智能网格管理器。Slime 基于 Kubernetes Operator 实现，可作为 Istio 的 CRD 管理器，无须对 Istio 做任何定制化改造，就可以定义动态的服务治理策略，从而达到自动便捷使用 Istio 和 Envoy 高阶功能的目的。&lt;/p&gt;
&lt;h2 id=&#34;slime-试图解决的问题&#34;&gt;Slime 试图解决的问题&lt;/h2&gt;
&lt;p&gt;Slime 项目的诞生主要为了解决以下问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;网格内所有服务配置全量下到所有 Sidecar Proxy，导致其消耗大量资源使得应用性能变差的问题&lt;/li&gt;
&lt;li&gt;如何在 Istio 中实现高阶扩展的问题：比如扩展 HTTP 插件；根据服务的资源使用率做到自适应限流&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Slime 解决以上问题的答案是构建 Istio 的控制平面，具体做法是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;构建可拔插控制器&lt;/li&gt;
&lt;li&gt;数据平面监控&lt;/li&gt;
&lt;li&gt;CRD 转换&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过以上方式 Slime 可以实现&lt;strong&gt;配置懒加载&lt;/strong&gt;和&lt;strong&gt;插件管理器&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;slime-架构&#34;&gt;Slime 架构&lt;/h2&gt;
&lt;p&gt;Slime 内部分为三大模块，其架构图如下所示。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;slime-internal-arch.jpg&#34; alt=&#34;Slime 内部架构图&#34; data-caption=&#34;Slime 内部架构图&#34;&gt;
  
  &lt;figcaption&gt;Slime 内部架构图&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;Slime 内部三大组件为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;slime-boot&lt;/code&gt;：在 Kubernetes 上部署 Slime 模块的 operator。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;slime-controller&lt;/code&gt;：Slime 的核心组件，监听 Slime CRD 并将其转换为Istio CRD。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;slime-metric&lt;/code&gt;：用于获取服务 metrics 信息的组件，&lt;code&gt;slime-controller&lt;/code&gt; 会根据其获取的信息动态调整服务治理规则。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;目前 Slime 内置了三个控制器子模块：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;配置懒加载（按需加载）&lt;/strong&gt;：用户无须手动配置 &lt;code&gt;SidecarScope&lt;/code&gt;，Istio 可以按需加载服务配置和服务发现信息；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HTTP 插件管理&lt;/strong&gt;：使用新的 CRD——&lt;code&gt;pluginmanager/envoyplugin&lt;/code&gt; 包装了可读性，摒弃了可维护性较差的 &lt;code&gt;envoyfilter&lt;/code&gt;，使得插件扩展更为便捷；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自适应限流&lt;/strong&gt;：结合监控信息自动调整限流策略；&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;什么是 SidecarScope？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;SidecarScope 是在 Istio 1.1 版本中引入的，它并不是一个直接面向用户的配置项，而是 Sidecar 资源的包装器，具体来说就是 &lt;a href=&#34;../config/networking/sidecar.md&#34; title=&#34;Sidecar 资源&#34;&gt;Sidecar 资源&lt;/a&gt;
中的 &lt;code&gt;egress&lt;/code&gt; 选项。通过该配置可以减少 Istio 向 Sidecar 下发的数据量，例如只向某个命名空间中的某些服务下发某些 hosts 的访问配置，从而提高应用提高性能。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;使用-slime-作为-istio-的控制平面&#34;&gt;使用 Slime 作为 Istio 的控制平面&lt;/h2&gt;
&lt;p&gt;为了解决这些问题，Slime 在 Istio 之上构建了更高层次的抽象，相当于为 Istio 构建了一层管理平面，其工作流程图如下所示。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;slime-flow-chart.jpg&#34; alt=&#34;Slime 工作流程图&#34; data-caption=&#34;Slime 工作流程图&#34;&gt;
  
  &lt;figcaption&gt;Slime 工作流程图&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;具体步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Slime Operator 根据管理员的配置在 Kubernetes 中完成 Slime 组件的初始化；&lt;/li&gt;
&lt;li&gt;开发者创建符合 Slime CRD 规范的配置并应用到 Kubernetes 集群中；&lt;/li&gt;
&lt;li&gt;Slime 查询 Prometheus 中保存的相关服务的监控数据，结合 Slime CRD 中自适应部分的配置，将 Slime CRD 转换为 Istio CRD，同时将其推送到 Global Proxy 中；&lt;/li&gt;
&lt;li&gt;Istio 监听 Istio CRD 的创建；&lt;/li&gt;
&lt;li&gt;Istio 将 Sidecar Proxy 的配置信息推送到数据平面相应的 Sidecar Proxy 中；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;以上只是一个对 Slime 工作流程的一个笼统的介绍，更多详细信息请参考 &lt;a href=&#34;https://github.com/slime-io/slime/&#34; title=&#34;Slime GitHub&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slime GitHub&lt;/a&gt;
。&lt;/p&gt;
&lt;h2 id=&#34;配置懒加载&#34;&gt;配置懒加载&lt;/h2&gt;
&lt;p&gt;为了解决数据平面中 Sidecar Proxy 资源消耗过大及网络延迟问题，Slime 使用了配置懒加载（按需加载 Sidecar 配置）的方案。该方案的核心思想是向每个 Sidecar Proxy 中只下发其所 Pod 中服务所需的配置，而不是将网格中的所有服务信息全量下发。所以 Slime 需要获取每个服务的调用关系这样才能得到其所需的 Sidecar Proxy 配置。&lt;/p&gt;
&lt;p&gt;Slime 实现 Sidecar Proxy 配置懒加载的方法是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;让数据平面中的所有服务的首次调用都通过一个 Global Proxy，该 Proxy 可以记录所有服务的调用和依赖信息，根据该依赖信息更新 Istio 中 Sidecar 资源的配置；&lt;/li&gt;
&lt;li&gt;当某个服务的调用链被 VirtualService 中的路由信息重新定义时， Global Proxy 原有记录就失效了，需要一个新的数据结构来维护该服务的调用关系。Slime 创建了名为 &lt;code&gt;ServiceFence&lt;/code&gt;  的 CRD 来维护服务调用关系以解决服务信息缺失问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;使用-global-proxy-初始化服务调用拓扑&#34;&gt;使用 Global Proxy 初始化服务调用拓扑&lt;/h3&gt;
&lt;p&gt;Slime 在数据平面中部署 Global Proxy（也叫做 Global Sidecar，但其与应用的 Pod 不是一对一的关系，笔者更倾向于称其为 Global Proxy），该代理同样使用 Envoy 构建，在每个需要启动配置懒加载的命名空间中部署一个或在整个网格中只部署一个，所有缺失服务发现信息的调用（你也可以手动配置服务调用关系），都会被兜底路由劫持到 Global Proxy，经过其首次转发后，Slime 便可感知到被调用方的信息，然后根据其对应服务的 VirtualService，找到服务名和真实后端的映射关系，将两者的都加入 SidecarScope，以后该服务的调用就不再需要经过 Global Proxy 了。&lt;/p&gt;
&lt;h3 id=&#34;使用-servicefence-维护服务调用拓扑&#34;&gt;使用 ServiceFence 维护服务调用拓扑&lt;/h3&gt;
&lt;p&gt;在使用 Global Proxy 初始化服务调用拓扑后，一旦服务调用链有变动的话怎么办？对此 Slime 创建了 ServiceFence 的 CRD。使用 ServiceFence 可以维护服务名和后端服务的映射关系。Slime 根据其对应服务的 VirtualService，找到 Kubernetes 服务名和真实后端（host）的映射关系，将两者的都加入 Sidecar 的配置中。ServiceFence 管理生成的 SidecarScope 的生命周期，自动清理长时间不用的调用关系，从而避免上述问题。&lt;/p&gt;
&lt;h3 id=&#34;如何开启配置懒加载&#34;&gt;如何开启配置懒加载&lt;/h3&gt;
&lt;p&gt;配置懒加载功能对于终端用户是透明的，只需要 Kubernetes  Service 上打上 &lt;code&gt;istio.dependency.servicefence/status:&amp;quot;true&amp;quot;&lt;/code&gt; 的标签，表明该服务需要开启配置懒加载，剩下的事情交给 Slime Operator 来完成即可。&lt;/p&gt;
&lt;h2 id=&#34;http-插件管理&#34;&gt;HTTP 插件管理&lt;/h2&gt;
&lt;p&gt;Istio 中的插件扩展只能通过 EnvoyFilter 来实现，因为它是 xDS 层面的配置，管理和维护这样的配置需要耗费大量的精力，也极容易出错。因此，Slime 在 EnvoyFilter 的基础上做了一层面向插件的抽象。&lt;/p&gt;
&lt;p&gt;Slime 共有两个 CRD 用于 HTTP 插件管理，分别是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PluginManager&lt;/strong&gt;：配置为哪些负载开启哪些插件，插件的配置顺序即为执行顺序；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EnvoyPlugin&lt;/strong&gt;：EnvoyPlugin 不关心每个插件的具体配置，具体配置会被放在 EnvoyFilter 资源的 &lt;code&gt;patch.typed_config&lt;/code&gt; 结构中透传），EnvoyPlugin 的核心思想是将插件配置在需要的维度中做聚合，从而限定插件的生鲜范围。这样做一方面更加贴合插件使用者的习惯，另一方面也降低了上层配置的冗余，&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;关于 Slime 中插件管理的详细使用方式请见 &lt;a href=&#34;https://github.com/slime-io/slime/blob/master/doc/zh/plugin_manager.md&#34; title=&#34;Slime GitHub&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slime GitHub&lt;/a&gt;
。&lt;/p&gt;
&lt;h2 id=&#34;自适应限流&#34;&gt;自适应限流&lt;/h2&gt;
&lt;p&gt;Envoy 内置的限流组件功能单一，只能以实例维度配置限流值，无法做到根据应用负载的自适应限流。Slime 通过与 Prometheus metric server 对接，实时的获取监控情况，来动态配置限流值。&lt;/p&gt;
&lt;p&gt;Slime 自适应限流的流程图如下所示。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;slime-smart-limiter.jpg&#34; alt=&#34;Slime 的自适应限流流程图&#34; data-caption=&#34;Slime 的自适应限流流程图&#34;&gt;
  
  &lt;figcaption&gt;Slime 的自适应限流流程图&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;Slime 的自适应限流的流程分为两部分，一部分为 SmartLimiter 到 EnvoyFilter 的转换，另一部分为获取监控数据。目前 Slime 支持从 Kubernetes Metric Server 获取服务的CPU、内存、副本数等数据。Slime 还对外提供了一套监控数据接口（Metric Discovery Server），通过 MDS，可以将自定义的监控指标同步给限流组件。&lt;/p&gt;
&lt;p&gt;Slime 创建的 CRD &lt;code&gt;SmartLimiter&lt;/code&gt; 用于配置自适应限流。其的配置是接近自然语义，例如希望在 CPU 超过 80% 时触发服务 A 的访问限制，限额为 30QPS，对应的SmartLimiter 定义如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;microservice.netease.com/v1alpha1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;SmartLimiter&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;descriptors&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;action&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;fill_interval&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;seconds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;quota&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;30/{pod}&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# 30为该服务的额度，将其均分给每个 pod，加入有 3 个 pod，则每个 pod 的限流为 10&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;condition&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{cpu}&amp;gt;0.8&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# 根据监控项{cpu}的值自动填充该模板&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;更多&#34;&gt;更多&lt;/h2&gt;
&lt;p&gt;Slime 开源于 2021 年初，本文发稿时该项目仍处于初级阶段，本文大量参考了杨笛航在云原生社区中的分享 &lt;a href=&#34;https://cloudnative.to/blog/netease-slime/&#34; title=&#34;Slime：让 Istio 服务网格变得更加高效与智能&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slime：让 Istio 服务网格变得更加高效与智能&lt;/a&gt;
 及 Slime 的 &lt;a href=&#34;https://github.com/slime-io/slime&#34; title=&#34;GitHub&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;
。感兴趣的读者可以关注下这个项目的 GitHub，进一步了解它。&lt;/p&gt;
&lt;p&gt;另外欢迎关注服务网格和 Istio 的朋友加入&lt;a href=&#34;https://cloudnative.to/sig-istio/&#34; title=&#34;云原生社区 Istio SIG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;云原生社区 Istio SIG&lt;/a&gt;
，一起参与讨论和交流。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/netease-slime/&#34; title=&#34;Slime：让 Istio 服务网格变得更加高效与智能 - cloudnative.to&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slime：让 Istio 服务网格变得更加高效与智能 - cloudnative.to&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/slime-io/slime/blob/master/README_ZH.md&#34; title=&#34;Slime GitHub 文档 - github.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slime GitHub 文档 - github.com&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://istio.io/latest/docs/reference/config/networking/sidecar/&#34; title=&#34;Sidecar - istio.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sidecar - istio.io&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>如何理解 Istio Ingress， 它与 API Gateway 有什么区别？</title>
      <link>https://jimmysong.io/blog/istio-servicemesh-api-gateway/</link>
      <pubDate>Fri, 06 Aug 2021 10:22:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/istio-servicemesh-api-gateway/</guid>
      <description>
        
        
        &lt;p&gt;API 网关作为客户端访问后端的入口，已经存在很长时间了，它主要是用来管理”南北向“的流量；近几年服务网格开始流行，它主要是管理系统内部，即“东西向”流量，而像 Istio 这样的服务网格还内置了网关，从而将系统内外部的流量纳入了统一管控。这经常给初次接触 Istio 的人带来困惑——服务网格与 API 网关之间是什么关系？是不是使用了 Istio 就可以替代了 API 网关？Istio 的 API 网关是如何运作的？有哪些方式暴露 Istio mesh 中的服务？这篇文章给为你解答。&lt;/p&gt;
&lt;h2 id=&#34;主要观点&#34;&gt;主要观点&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;服务网格诞生的初衷是为了解决分布式应用的内部流量的管理问题，而在此之前 API 网关已存在很久了。&lt;/li&gt;
&lt;li&gt;虽然 Istio 中内置了Gateway，但是你仍可以使用自定义的 Ingress Controller 来代理外部流量。&lt;/li&gt;
&lt;li&gt;API 网关和服务网格正朝着融合的方向发展。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;如何暴露-istio-mesh-中的服务&#34;&gt;如何暴露 Istio mesh 中的服务？&lt;/h2&gt;
&lt;p&gt;下图展示了使用 Istio Gateway、Kubernetes Ingress、API Gateway 及 NodePort/LB 暴露 Istio mesh 中服务的四种方式。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;access-cluster.svg&#34; alt=&#34;暴露 Kubernetes 中服务的几种方式&#34; data-caption=&#34;暴露 Kubernetes 中服务的几种方式&#34;&gt;
  
  &lt;figcaption&gt;暴露 Kubernetes 中服务的几种方式&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;其中阴影表示的是 Istio mesh，mesh 中的的流量属于集群内部（东西向）流量，而客户端访问 Kubernetes 集群内服务的流量属于外部（南北向）流量。不过因为 Ingress、Gateway 也是部署在 Kubernetes 集群内的，这些节点访问集群内其他服务的流量就难以归属了。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;方式&lt;/th&gt;
&lt;th&gt;控制器&lt;/th&gt;
&lt;th&gt;功能&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;NodePort/LoadBalancer&lt;/td&gt;
&lt;td&gt;Kubernetes&lt;/td&gt;
&lt;td&gt;负载均衡&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kubernetes Ingress&lt;/td&gt;
&lt;td&gt;Ingress Controller&lt;/td&gt;
&lt;td&gt;负载均衡、TLS、虚拟主机、流量路由&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Istio Gateway&lt;/td&gt;
&lt;td&gt;Istio&lt;/td&gt;
&lt;td&gt;负载均衡、TLS、虚拟主机、高级流量路由、其他 Istio 的高级功能&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;API 网关&lt;/td&gt;
&lt;td&gt;API Gateway&lt;/td&gt;
&lt;td&gt;负载均衡、TLS、虚拟主机、流量路由、API 生命周期管理、权限认证、数据聚合、账单和速率限制&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;由于 NodePort/LoadBalancer 是 Kubernetes 内置的基本的暴露服务的方式，本文就不讨论这种方式了。下文将对其他三种方式分别作出说明。&lt;/p&gt;
&lt;h2 id=&#34;使用-kubernetes-ingress-暴露服务&#34;&gt;使用 Kubernetes Ingress 暴露服务&lt;/h2&gt;
&lt;p&gt;我们都知道 Kubernetes 集群的客户端是无法直接访问 Pod 的 IP 地址的，因为 Pod 是处于 Kubernetes 内置的一个网络平面中。我们可以将 Kubernetes 内的服务使用 NodePort 或者 LoadBlancer 的方式暴露到集群以外。同时为了支持虚拟主机、隐藏和节省 IP 地址，可以使用 &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/&#34; title=&#34;Ingress&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ingress&lt;/a&gt;
 来暴露 Kubernetes 中的服务。Kubernetes Ingress 原理如下图所示。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;ingress.svg&#34; alt=&#34;使用 Kubernetes Ingress 暴露服务&#34; data-caption=&#34;使用 Kubernetes Ingress 暴露服务&#34;&gt;
  
  &lt;figcaption&gt;使用 Kubernetes Ingress 暴露服务&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;简单的说，Ingress 就是从 Kubernetes 集群外访问集群的入口，将用户的 URL 请求转发到不同的服务上。Ingress 相当于 Nginx、Apache 等负载均衡方向代理服务器，其中还包括规则定义，即 URL 的路由信息，路由信息得的刷新由 &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-controllers&#34; title=&#34;Ingress controller&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ingress controller&lt;/a&gt;
来提供。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;networking.k8s.io/v1beta1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Ingress&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;annotations&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kubernetes.io/ingress.class&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;istio&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ingress&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;rules&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;host&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;httpbin.example.com&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;http&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;paths&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/status/*&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;backend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;serviceName&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;httpbin&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;servicePort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8000&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;上面的例子中的 &lt;code&gt;kubernetes.io/ingress.class: istio&lt;/code&gt; 注解表明该 Ingress 使用的 Istio Ingress Controller。&lt;/p&gt;
&lt;h2 id=&#34;使用-istio-gateway-暴露服务&#34;&gt;使用 Istio Gateway 暴露服务&lt;/h2&gt;
&lt;p&gt;我们都知道 Istio 是继承 Kubernetes 之后发展出来的一个流行的服务网格实现，它实现了 Kubernetes 没有的一些功能，请参考&lt;a href=&#34;https://jimmysong.io/blog/what-is-istio-and-why-does-kubernetes-need-it/&#34; title=&#34;什么是 Istio？为什么 Kubernetes 需要 Istio？&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;什么是 Istio？为什么 Kubernetes 需要 Istio？&lt;/a&gt;
简要来说，正是因为 Istio 补足了 Kubernetes 对于云原生应用的流量管理、可观察性和安全方面的短板，使得流量管理变得对应用程序透明，使这部分功能从应用程序中转移到了平台层，成为了云原生基础设施。&lt;/p&gt;
&lt;p&gt;Istio 0.8 以前版本中使用 Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/&#34; title=&#34;Ingress&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ingress&lt;/a&gt;
 来作为流量入口，其中使用 Envoy 作为 Ingress Controller。在 Istio 0.8 及以后的版本中，Istio 创建了 Gateway 对象。Gateway 和 VirtualService 用于表示 Istio Ingress 的配置模型，Istio Ingress 的缺省实现则采用了和 sidecar 相同的 Envoy 代理。通过该方式，Istio 控制面用一致的配置模型同时控制了入口网关和内部的 sidecar 代理。这些配置包括路由规则，策略检查、遥测收集以及其他服务管控功能。&lt;/p&gt;
&lt;p&gt;Istio Gateway 的功能与 Kubernetes Ingress 类似，它负责进出集群的南北流量。Istio Gateway 描述了一个负载均衡器，用于承载进出服务网格边缘的连接。该规范描述了一组开放端口和这些端口所使用的协议，以及用于负载均衡的 SNI 配置等。&lt;/p&gt;
&lt;p&gt;Istio Gateway 资源本身只能配置L4到L6的功能，例如暴露的端口、TLS 设置等；但 Gateway 可与 VirtualService 绑定，在VirtualService 中可以配置七层路由规则，例如按比例和版本的流量路由，故障注入，HTTP 重定向，HTTP 重写等所有Mesh内部支持的路由规则。&lt;/p&gt;
&lt;p&gt;下面是一个 Gateway 与 VirtualService 绑定的示例。拥有 &lt;code&gt;istio: ingressgateway&lt;/code&gt; 标签的 pod 将作为 Ingress Gateway 并路由对 &lt;code&gt;httpbin.example.com&lt;/code&gt; 虚拟主机的 80 端口的 HTTP 访问，这相当于给 Kubernetes 敞开了一个外部访问的入口。这与使用 Kubernetes Ingress 最大的区别就是，需要我们手动将VirtualService与Gateway 绑定，并指定 Gateway 所在的 pod。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;networking.istio.io/v1alpha3&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Gateway&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;httpbin-gateway&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;selector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;istio&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ingressgateway&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;servers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;number&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;80&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;http&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;protocol&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;HTTP&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;hosts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;s2&#34;&gt;&amp;#34;httpbin.example.com&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;下面这个 VirtualService 通过 &lt;code&gt;gateways&lt;/code&gt; 与上面的网关绑定在了一起，以接受来自该网关的流量。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;networking.istio.io/v1alpha3&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;VirtualService&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;httpbin&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;hosts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;s2&#34;&gt;&amp;#34;httpbin.example.com&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;gateways&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;httpbin-gateway&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;http&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;match&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;uri&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;prefix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/status&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;route&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;destination&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;number&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8000&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;host&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;httpbin&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;使用-api-网关暴露服务&#34;&gt;使用 API 网关暴露服务&lt;/h2&gt;
&lt;p&gt;API 网关是位于客户端和后端服务之间的 API 管理工具，一种将客户端接口与后端实现分离的方式，在微服务中得到了广泛的应用。当客户端发出请求时，API 网关会将其分解为多个请求，然后将它们路由到正确的位置，生成响应，并跟踪所有内容。&lt;/p&gt;
&lt;p&gt;API Gateway 是微服务架构体系中的一类型特殊服务，它是所有微服务的入口，它的职责是执行路由请求、协议转换、聚合数据、认证、限流、熔断等。大多数企业 API 都是通过 API 网关部署的。API 网关通常会处理跨 API 服务系统的常见任务，例如用户身份验证、速率限制和统计信息。&lt;/p&gt;
&lt;p&gt;在网格中可以有一个或多个 API Gateway。API 网关的职责有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;请求路由和版本控制&lt;/li&gt;
&lt;li&gt;方便单体应用到微服务的过渡&lt;/li&gt;
&lt;li&gt;权限认证&lt;/li&gt;
&lt;li&gt;数据聚合：监控和计费&lt;/li&gt;
&lt;li&gt;协议转换&lt;/li&gt;
&lt;li&gt;消息和缓存&lt;/li&gt;
&lt;li&gt;安全和报警&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上很多基本功能比如路由和权限认证通过 Istio Gateway 也可以实现，只是在功能的丰富度和扩展性方面有些成熟的 API Gateway 可能更占优势，不过在 Istio mesh 中再引入 API Gateway 也可能带来一些弊端。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;引入了 API Gateway，需要考虑 API Gateway 本身的部署、运维、负载均衡等场景，增加了后端服务的复杂度&lt;/li&gt;
&lt;li&gt;API Gateway 中承载了大量的接口适配，导致难以维护&lt;/li&gt;
&lt;li&gt;对于部分场景，增加了一跳可能导致性能的降低&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;在 Istio mesh 中你可以使用多种 Kubernetes Ingress Controller 来充当入口网关，当然你还可以直接使用 Istio 内置的 Istio 网关，对于策略控制、流量管理和用量监控可以直接通过 Istio 网关来完成，这样做的好处是通过 Istio 的控制平面来直接管理网关，而不需要再借助其他工具。但是对于 API 生命周期管理、复杂的计费、协议转换和认证等功能，传统的 API 网关可能更适合你。所以，你可以根据自己的需求来选择，也可以组合使用。&lt;/p&gt;
&lt;p&gt;目前有些传统的反向代理也在向 Service Mesh 方向发展，如 Nginx 构建了 &lt;a href=&#34;https://www.nginx.com/products/nginx-service-mesh/&#34; title=&#34;Nginx Service Mesh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nginx Service Mesh&lt;/a&gt;
，Traefik 构建了 &lt;a href=&#34;https://traefik.io/traefik-mesh/&#34; title=&#34;Traefik Mesh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Traefik Mesh&lt;/a&gt;
。还有的 API 网关产品也向 Service Mesh 方向挺进，比如 Kong 发展出了 &lt;a href=&#34;https://kuma.io&#34; title=&#34;Kuma&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kuma&lt;/a&gt;
。在未来，我们会看到更多 API 网关、反向代理和服务网格的融合产品出现。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/evolving-kubernetes-networking-with-the-gateway-api/&#34; title=&#34;利用 Gateway API 发展 Kubernetes 网络&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;利用 Gateway API 发展 Kubernetes 网络&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/how-to-pick-gateway-for-service-mesh/&#34; title=&#34;如何为服务网格选择入口网关？&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;如何为服务网格选择入口网关？&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/service-mesh-and-api-gateway/&#34; title=&#34;Service Mesh 和 API Gateway 关系深度探讨&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Service Mesh 和 API Gateway 关系深度探讨&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/using-traefik-ingress-controller-with-istio-service-mesh/&#34; title=&#34;在 Istio 服务网格中使用 Traefik Ingress Controller&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;在 Istio 服务网格中使用 Traefik Ingress Controller&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>服务网格之旅——使用 Kubernetes 和 Istio Service Mesh 构建混合云</title>
      <link>https://jimmysong.io/blog/multicluster-management-with-kubernetes-and-istio/</link>
      <pubDate>Mon, 12 Jul 2021 22:22:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/multicluster-management-with-kubernetes-and-istio/</guid>
      <description>
        
        
        &lt;p&gt;这篇文章将带你了解使用 Kubernetes 和 Istio Service Mesh 构建多集群及混合云的过程和需要考虑的问题。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes&#34;&gt;Kubernetes&lt;/h2&gt;
&lt;p&gt;使用 Kubernetes 可以快速部署一个分布式环境，实现了云的互操作性，统一了云上的控制平面。并提供了 Service、Ingress 和 &lt;a href=&#34;https://kubernetes.io/blog/2021/04/22/evolving-kubernetes-networking-with-the-gateway-api/&#34; title=&#34;Gateway&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gateway&lt;/a&gt;
 等资源对象来处理应用程序的流量。如下图所示，Kubernetes 中默认使用 Service 做服务注册和发现，服务之间可以使用服务名称来访问。Kubernetes API Server 与集群内的每个节点上的 &lt;code&gt;kube-proxy&lt;/code&gt; 组件通信，为节点创建 iptables 规则，并将请求转发到其他 pod 上。&lt;/p&gt;
&lt;p&gt;假定现在客户端要访问 Kubernetes 中的服务，首先请求会发送到 Ingress/Gateway 上，然后根据 Ingress/Gateway 里的路由配置转发到后端服务上（图中是服务 A），接着服务 A 对服务 B 请求的流量转发轮询到服务 B 的实例上。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;008i3skNly1gsgg6a11l1j31lu0u042s.jpg&#34; alt=&#34;Kubernetes&#34; data-caption=&#34;Kubernetes&#34;&gt;
  
  &lt;figcaption&gt;Kubernetes&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-多集群管理&#34;&gt;Kubernetes 多集群管理&lt;/h2&gt;
&lt;p&gt;多集群管理最常见的使用场景包括服务流量负载均衡、隔离开发和生产环境、解耦数据处理和数据存储、跨云备份和灾难恢复、灵活分配计算资源、跨区域服务的低延迟访问以及避免厂商锁定等。一个企业内部往往有多个 Kubernetes 集群，由 MultiCluster SIG 开发的 KubeFed 实现 Kubernetes 集群联邦可以实现多集群管理的功能，这使得所有 Kubernetes 集群都通过同一个接口来管理。&lt;/p&gt;
&lt;p&gt;在使用集群联邦时需要解决以下几个通用问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;配置需要联邦哪些集群&lt;/li&gt;
&lt;li&gt;需要在集群中传播的 API 资源&lt;/li&gt;
&lt;li&gt;配置 API 资源如何分配到不同的集群&lt;/li&gt;
&lt;li&gt;对集群中 DNS 记录注册以实现跨集群的服务发现&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面是 KubeSphere 的多集群架构，也是最常用的一种 Kubernetes 多集群管理架构，其中 Host Cluster 作为控制平面，有两个成员集群，分别是 West 和 East。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;008i3skNly1gsgg7a2ojvj31aa0u0491.jpg&#34; alt=&#34;Multicluster&#34; data-caption=&#34;Multicluster&#34;&gt;
  
  &lt;figcaption&gt;Multicluster&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;Host 集群需要能够访问 Member 集群的 API Server，Member 集群之间的网络连通性没有要求。管理集群 Host Cluster 独立于其所管理的成员集群，Member Cluster 并不知道 Host Cluster 存在，这样做的好处是当控制平面发生故障时不会影响到成员集群，已经部署的负载仍然可以正常运行，不会受到影响。&lt;/p&gt;
&lt;p&gt;Host 集群同时承担着 API 入口的作用，由 Host Cluster 将对 Member 集群的资源请求转发到 Member 集群，这样做的目的是方便聚合，而且也利于做统一的权限认证。我们看到在 Host Cluster 中有联邦控制平面，其中的 Push Reconciler 会将联邦集群中身份、角色及角色绑定传播到所有成员集群中。&lt;/p&gt;
&lt;h2 id=&#34;istio&#34;&gt;Istio&lt;/h2&gt;
&lt;p&gt;当我们在 Kubernetes 中运行着多语言、多版本的微服务，并需要更细粒度的金丝雀发布和统一的安全策略管理，实现服务间的可观察性时，可以考虑使用 Istio 服务网格。Istio 通过向应用程序 Pod 中注入 sidecar proxy，缺省使用 IPTables 透明得拦截进出应用程序的所有流量，从而实现了应用层到集群中其他启用服务网格的服务的智能应用感知负载均衡，并绕过了初级的 kube-proxy 负载均衡。Istio 控制平面与 Kubernetes API Server 通信可以获取集群中所有注册的服务信息。&lt;/p&gt;
&lt;p&gt;下图展示了 Istio 的基本原理，其中所有节点属于同一个 Kubernetes 集群。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;008i3skNly1gsgg6sdrk2j32v60u0qbb.jpg&#34; alt=&#34;Istio Service Mesh&#34; data-caption=&#34;Istio Service Mesh&#34;&gt;
  
  &lt;figcaption&gt;Istio Service Mesh&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;你可能最终会有至少几个Kubernetes集群，每个集群都承载着微服务。Istio 的多集群部署根据网络隔离、主备情况存在多种&lt;a href=&#34;https://istio.io/latest/docs/setup/install/multicluster/&#34; title=&#34;部署模式&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;部署模式&lt;/a&gt;
，可以使用 Istio Operator 部署时通过声明来指定。集群中的这些微服务之间的通信可以通过服务网格来加强。在集群内部，Istio提供通用的通信模式，以提高弹性、安全性和可观察性。&lt;/p&gt;
&lt;p&gt;以上都是关于 Kubernetes 上的应用负载管理，但是对于虚拟机上遗留应用，如何在同一个平面中管理？如何管理多集群中的流量划分、网关和安全性呢？&lt;/p&gt;
&lt;h2 id=&#34;管理平面&#34;&gt;管理平面&lt;/h2&gt;
&lt;p&gt;在 Istio 之上再增加一层抽象，将网关、流量和安全分组管理，并将它们应用到不同的集群和命名空间上。下图展示的是 &lt;a href=&#34;https://www.tetrate.io/tetrate-service-bridge/&#34; title=&#34;Tetrate Service Bridge&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tetrate Service Bridge&lt;/a&gt;
 的多租户模型，利用 NGAC 来管理用户的访问权限，同时也有利于构建零信任网络。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;008i3skNly1gsgg8ndcajj31il0u00z9.jpg&#34; alt=&#34;Management Plane&#34; data-caption=&#34;Management Plane&#34;&gt;
  
  &lt;figcaption&gt;Management Plane&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;Istio 提供了工作负载识别，并由强大的 mTLS 加密保护。这种零信任模型比基于源 IP 等拓扑信息来信任工作负载更好。在 Istio 之上构建一个多集群管理的通用控制平面，然后再增加一个管理平面来管理多集群，提供多租户、管理配置、可观察性等功能。&lt;/p&gt;
&lt;p&gt;下图展示的是 Tetrate Service Bridge 的架构图。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;008i3skNly1gsgg951mknj314g0u0dnf.jpg&#34; alt=&#34;Tetrate Service Bridge&#34; data-caption=&#34;Tetrate Service Bridge&#34;&gt;
  
  &lt;figcaption&gt;Tetrate Service Bridge&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;使用 Kubernetes 实现了异构集群的互操作性，Istio 将容器化负载和虚拟机负载纳入到一个同一个控制平面内，统一管理集群内的流量、安全和可观察性。但是，随着集群数量、网络环境和用户权限的越发复杂，人们还需要在 Istio 的控制平面至上再构建一层管理平面来进行混合云管理。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>如何调试 Kubernetes 中的微服务 ——proxy、sidecar 还是 service mesh？</title>
      <link>https://jimmysong.io/blog/how-to-debug-microservices-in-kubernetes-with-proxy-sidecar-or-service-mesh/</link>
      <pubDate>Mon, 05 Jul 2021 22:22:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/how-to-debug-microservices-in-kubernetes-with-proxy-sidecar-or-service-mesh/</guid>
      <description>
        
        
        &lt;p&gt;Kubernetes 可以说是目前为止用来运行微服务的最佳载体，但是在调试 Kubernetes 环境中的微服务时的体验可能就没那么友好了。本文将带你了解如何调试 Kubernetes 中的微服务，介绍常用的工具，以及 Istio 的引入为微服务的调试带来的变革。&lt;/p&gt;
&lt;h2 id=&#34;调试微服务与传统单体应用有巨大的不同&#34;&gt;调试微服务与传统单体应用有巨大的不同&lt;/h2&gt;
&lt;p&gt;微服务的调试是一直长期困扰软件开发人员的问题，这在传统的单体应用中不存在，因为开发者可以利用 IDE 中的调试器，为应用程序增加断点、修改环境变量，单步执行等，这些都为软件调试提供了巨大帮助。随着 Kubernetes 的流行，微服务的调试就成了一个棘手的问题，其中相比传统单体应用的调试多了以下问题：&lt;/p&gt;
&lt;h3 id=&#34;多依赖&#34;&gt;多依赖&lt;/h3&gt;
&lt;p&gt;一个微服务往往依赖多个其他微服务，在调试某个微服务时，如何部署其他依赖服务以快速搭建一套最新的 stagging 环境？&lt;/p&gt;
&lt;h3 id=&#34;从本地机器访问&#34;&gt;从本地机器访问&lt;/h3&gt;
&lt;p&gt;微服务在开发者的本地电脑上运行时，通常无法直接访问到 Kubernetes 集群中的服务，如何像调试本地服务一样调试部署在 Kubernetes 集群中的微服务？&lt;/p&gt;
&lt;h3 id=&#34;开发效率低下&#34;&gt;开发效率低下&lt;/h3&gt;
&lt;p&gt;通常情况下，代码从更新到构建成镜像再推送到集群中需要一个漫长的过程，如何加快开发速度？&lt;/p&gt;
&lt;p&gt;我们一起来看下哪些工具能够解决以上问题。&lt;/p&gt;
&lt;h2 id=&#34;工具&#34;&gt;工具&lt;/h2&gt;
&lt;p&gt;调试 Kubernetes 中的微服务的主要解决方案有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proxy：在 Kubernetes 集群和本地调试终端中部署一个代理，通过构建一个 VPN，使得本地应用可以直接访问到 Kubernetes 中的服务；&lt;/li&gt;
&lt;li&gt;Sidecar：替换原来应用容器的镜像为开发镜像，可以在这个容器中中对该服务进行调试，同时在要调试的微服务 pod 中注入一个 sidecar 作为辅助工具来同步代码；&lt;/li&gt;
&lt;li&gt;服务网格：要想了解应用的整体情况，就需要在所有微服务中注入 sidecar，这样你就可以获得一个监控全局状态的仪表板；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面是实现以上解决方案的三个典型的开源项目，它们分别从不同的角度可以帮助你调试微服务。&lt;/p&gt;
&lt;h3 id=&#34;proxy-模式telepresence&#34;&gt;Proxy 模式：Telepresence&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.telepresence.io/&#34; title=&#34;Telesprence&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Telesprence&lt;/a&gt;
 本质上是一个本地代理，该代理将 Kubernetes 集群中的数据卷、环境变量、网络都代理到了本地。下图展示的是 Teleprence 的主要使用场景。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;telepresence.jpg&#34; alt=&#34;Proxy 模式：Telepresence&#34; data-caption=&#34;Proxy 模式：Telepresence&#34;&gt;
  
  &lt;figcaption&gt;Proxy 模式：Telepresence&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;用户需要在本地自主地执行 &lt;code&gt;telepresence&lt;/code&gt; 命令，它会自动将代理部署到 Kubernetes 中，有了该代理之后：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本地的服务就可以完整的访问到 Kubernetes 集群中的其他服务、环境变量、Secret、ConfigMap 等；&lt;/li&gt;
&lt;li&gt;集群中的服务还能直接访问到本地暴露出来的端点；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是这种方式仍然不够连贯，还需要用户在本地调试时运行多次命令，而且在某些网络环境下可能无法与 Kubernetes 集群建立 VPN 连接。&lt;/p&gt;
&lt;h3 id=&#34;sidecar-模式nocalhost&#34;&gt;Sidecar 模式：Nocalhost&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://nocalhost.dev/&#34; title=&#34;Nocalhost&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nocalhost&lt;/a&gt;
 是一个基于 Kubernetes 的云端开发环境。要想使用它，你只需要在你的 IDE——VS Code 中安装一个插件即可扩展 Kubernetes，并缩短开发反馈周期。通过为不同的用户创建不同的 namespace，并使用 ServiceAccount 绑定到不同用户角身上时，就可以实现开发环境隔离。同时，Nocalhost 还提供了 Web 控制台和 API，方便管理员来管理不同的开发环境。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;sidecar-nocalhost.jpg&#34; alt=&#34;Sidecar 模式：Nocalhost&#34; data-caption=&#34;Sidecar 模式：Nocalhost&#34;&gt;
  
  &lt;figcaption&gt;Sidecar 模式：Nocalhost&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;h4 id=&#34;测试&#34;&gt;测试&lt;/h4&gt;
&lt;p&gt;参考 &lt;a href=&#34;https://nocalhost.dev/getting-started.html&#34; title=&#34;Nocalhost 文档&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nocalhost 文档&lt;/a&gt;
，我们在 macOS 上安装 Nocalhost，并使用 Minikube 来演示如何调试。&lt;/p&gt;
&lt;p&gt;执行下面的命令安装 Nocalhost 客户端并查看 &lt;code&gt;nhctl&lt;/code&gt; 命令行工具的版本。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;brew install nocalhost/repo/nocalhost
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;nhctl version
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我们假设你机的 &lt;code&gt;kubeconfig&lt;/code&gt; 文件位于 &lt;code&gt;~/.kube/config&lt;/code&gt;（若不在此位置需要在下面的命令中使用 &lt;code&gt;--kubeconfig&lt;/code&gt; 手动指定） 并拥有 Kubernetes 集群的 admin 角色，执行下面的命令使用 Helm3 在 Kubernetes 上安装 Nocalhost 服务端。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;nhctl init demo -n nocalhost 
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;执行下面的命令启动 Minikube 隧道并查看 Nocalhost web 端地址。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;minikube tunnel
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubectl get service nocalhost-web
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在浏览器中访问 &lt;code&gt;http://&amp;lt;EXTERNAL-IP&amp;gt;&lt;/code&gt; 即可，用户名/密码为：&lt;code&gt;admin@admin.com/123456&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;要想在 VS Code 中使用，你还想需要创建一个 ServiceAccount 并绑定 admin 角色，然后将该 ServiceAccount 作为 Kubeconfig 文件导出。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubectl create serviceaccount my-service-account
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubectl create rolebinding admin --clusterrole&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;admin --serviceaccount&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;default:my-service-account
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;只要你有一个 Kubernetes 集群，并有集群的 admin 权限，就可以参考 Nocalhost 的文档快速开始试用。在 VS Code 中使用 Nocalhost 插件时需要先为插件中配置 Kubernetes 集群。选择你刚导出的 Kubeconfig 文件或者直接复制文件中的内容粘贴到配置里。然后选择你需要测试的服务，并选择对应的 Dev Container，VS Code 会自动打开一个新的代码窗口。&lt;/p&gt;
&lt;p&gt;下面是以 Istio 官方提供的 &lt;a href=&#34;https://istio.io/latest/docs/examples/bookinfo/&#34; title=&#34;bookinfo 示例&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bookinfo 示例&lt;/a&gt;
为例，你可以在本地 IDE 中打开克隆下来的代码，然后点击代码文件旁边的锤子即可进入开发模式。选择对应的 DevContainer，nocalhost 会自动向 pod 中注入一个开发容器 sidecar，并在终端中自动进入该容器，如下图所示。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;nocalhost-vs-code.jpg&#34; alt=&#34;Nocalhost VS code 界面&#34; data-caption=&#34;Nocalhost VS code 界面&#34;&gt;
  
  &lt;figcaption&gt;Nocalhost VS code 界面&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;在开发模式中，本地修改代码，无需重新构建镜像，远端开发环境实时生效，这样可以极大的加快开发速度。同时，Nocalhost 还提供了服务端，可用于开发环境和用户权限进行管理，如下图所示。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;nocalhost-web-admin.jpg&#34; alt=&#34;Nocalhost web 端&#34; data-caption=&#34;Nocalhost web 端&#34;&gt;
  
  &lt;figcaption&gt;Nocalhost web 端&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;h3 id=&#34;service-mesh-模式istio&#34;&gt;Service Mesh 模式：Istio&lt;/h3&gt;
&lt;p&gt;以上使用 proxy 和 sidecar 的方式，一次只能对一个服务进行调试，如果想要掌握服务的全局状况，比如获取的服务的指标，以及通过分布式追踪了解服务的依赖和调用流程，对服务的性能进行调试。这些&lt;a href=&#34;https://istio.io/latest/zh/docs/concepts/observability/&#34; title=&#34;可观察性&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;可观察性&lt;/a&gt;
的功能，需要为所有服务统一注入 sidecar 来实现。&lt;/p&gt;
&lt;p&gt;而且，当你的服务正处于从虚拟机迁移到 Kubernetes 的过程中时，使用 Istio 可以将虚拟机与 Kubernetes 纳入一个网络平面中（如下图所示），方便开发者调试和做渐进式的迁移。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;istio-service-mesh.jpg&#34; alt=&#34;Serivce Mesh 模式：Istio&#34; data-caption=&#34;Serivce Mesh 模式：Istio&#34;&gt;
  
  &lt;figcaption&gt;Serivce Mesh 模式：Istio&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;当然要获得这些好处也不是一点“代价”也不没有的，引入 Istio 后，你的 Kubernetes  service 需要遵守 Istio 的&lt;a href=&#34;https://istio.io/latest/zh/docs/ops/deployment/requirements/&#34; title=&#34;命名规范&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;命名规范&lt;/a&gt;
，学习使用 &lt;a href=&#34;https://istio.io/latest/docs/ops/diagnostic-tools/istioctl-analyze/&#34; title=&#34;Istioctl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istioctl&lt;/a&gt;
 命令行和日志的方式来调试微服务。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用 &lt;code&gt;istioctl analyze&lt;/code&gt; 命令来调试集群中的微服务部署情况，可以使用 YAML 文件来检查某个命名空间或整个集群中的资源部署情况。&lt;/li&gt;
&lt;li&gt;使用 &lt;code&gt;istioctl proxy-config secret&lt;/code&gt;  来调试 service mesh 中的 pod 的 secret 被正确的加载并有效。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Istio 的配置信息在大型的集群部署中传播将会耗时更长并且可能有几秒钟的延迟时间，sidecar 的引入会给服务间调用带来一定延迟。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;在应用微服务化和从虚拟机迁移到 Kubernetes 的过程中，开发者需要很多观念和习惯上的转变。通过 proxy 在本地跟 Kubernetes 间构建 VPN，可以方便开发者像调试本地服务一样调试 Kubernetes 中的服务。通过向 pod 中注入 sidecar，可以实现实时调试，加快开发进度。最后，Istio service mesh 真正实现了全局的可观察性，你还可以使用像 &lt;a href=&#34;https://www.tetrate.io/tetrate-service-bridge/&#34; title=&#34;Tetrate Service Bridge&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tetrate Service Bridge&lt;/a&gt;
 这样的工具来管理异构平台，帮助你渐渐地从单体应用过度到微服务。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Istio 开源四周年回顾与展望</title>
      <link>https://jimmysong.io/blog/istio-4-year-birthday/</link>
      <pubDate>Mon, 24 May 2021 08:00:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/istio-4-year-birthday/</guid>
      <description>
        
        
        &lt;p&gt;Istio 是由 &lt;a href=&#34;https://tetrate.io/&#34; title=&#34;Tetrate&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tetrate&lt;/a&gt;
 创始人 Varun Talwar 和谷歌首席工程师 Louis Ryan 命名并在 2017 年 5 月 24 日开源。今天是 Istio 开源四周年，让我们一起来回顾一下 Istio 四年来的发展并展望一下它的未来。&lt;/p&gt;
&lt;h2 id=&#34;istio-的开源历史&#34;&gt;Istio 的开源历史&lt;/h2&gt;
&lt;p&gt;2017 年是 Kubernetes 结束容器编排之战的一年，Google 为了巩固在云原生领域的优势，并弥补 Kubernetes 在服务间流量管理方面的劣势，趁势开源了 Istio。下面是截止目前 Istio 历史上最重要的几次版本发布。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;日期&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;版本&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2017-05-24&lt;/td&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;td&gt;正式开源，该版本发布时仅一个命令行工具。确立了功能范围和 sidecar 部署模式，确立的 Envoy 作为默认 sidecar proxy 的地位。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2017-10-10&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;支持多运行时环境，如虚拟机。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018-06-01&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;API 重构。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018-07-31&lt;/td&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;td&gt;生产就绪，此后 Istio 团队被大规模重组。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019-03-19&lt;/td&gt;
&lt;td&gt;1.1&lt;/td&gt;
&lt;td&gt;企业就绪，支持多 Kubernetes 集群，性能优化。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020-03-03&lt;/td&gt;
&lt;td&gt;1.5&lt;/td&gt;
&lt;td&gt;回归单体架构，支持 WebAssembly 扩展，使得 Istio 的生态更加强大。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020-11-18&lt;/td&gt;
&lt;td&gt;1.8&lt;/td&gt;
&lt;td&gt;正式放弃 Mixer，进一步完善对虚拟机的支持。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Istio 开源后经过了一年时间的发展，在 1.0 发布的前两个月发布了 0.8 版本，这是对 API 的一次大规模重构。而在 2018 年 7 月底发布 1.0 时， Istio 达到了生产可用的临界点，此后 Google 对 Istio 团队进行了大规模重组，多家以 Istio 为基础的 Service Mesh &lt;a href=&#34;https://istio.io/latest/about/ecosystem/#providers&#34; title=&#34;创业公司&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;创业公司&lt;/a&gt;
诞生，可以说 2018 年是服务网格行业诞生的元年。&lt;/p&gt;
&lt;p&gt;2019年 3 月 Istio 1.1 发布，而这距离 1.0 发布已经过去了近 9 个月，这已经远远超出一个开源项目的平均发布周期。我们知道迭代和进化速度是基础软件的核心竞争力，此后 Istio 开始以每个季度一个版本的固定&lt;a href=&#34;https://istio.io/v1.7/about/release-cadence/&#34; title=&#34;发布节奏&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;发布节奏&lt;/a&gt;
，并在 2019 年成为了 &lt;a href=&#34;https://octoverse.github.com/#fastest-growing-oss-projects-by-contributors&#34; title=&#34;GitHub 增长最快的十大项目中排名第 4 名&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub 增长最快的十大项目中排名第 4 名&lt;/a&gt;
！&lt;/p&gt;
&lt;h2 id=&#34;istio-社区&#34;&gt;Istio 社区&lt;/h2&gt;
&lt;p&gt;Istio 开源四年来，已经在 GitHub 上收获了 2.7 万颗星，获得了大量的&lt;a href=&#34;https://istio.io/latest/about/case-studies/&#34; title=&#34;社区用户&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;社区用户&lt;/a&gt;
。下图是 &lt;a href=&#34;https://github.com/istio/istio&#34; title=&#34;Istio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio&lt;/a&gt;
 的 GitHub star 数增长情况。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;008i3skNly1gqtm7n2hm1j31me0n2tag.jpg&#34; alt=&#34;&#34; data-caption=&#34;&#34;&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;2020 年 Istio 的项目管理开始走向成熟，治理方式也到了进化的阶段。2020 年，Istio 社区进行了第一次&lt;a href=&#34;https://istio.io/latest/blog/2020/steering-election-results/&#34; title=&#34;管委会选举&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;管委会选举&lt;/a&gt;
，还把商标转让给了 &lt;a href=&#34;https://istio.io/latest/blog/2020/open-usage/&#34; title=&#34;Open Usage Commons&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open Usage Commons&lt;/a&gt;
。首届 &lt;a href=&#34;https://events.istio.io/istiocon-2021/&#34; title=&#34;IstioCon&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IstioCon&lt;/a&gt;
 在 2021 年 2 月份成功举办，几千人参加了线上会议。在中国也有大量的 Istio 社区用户，2021 年也会有线下面对面的 Istio 社区 meetup 在中国举办。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;008i3skNly1gquicfqg14j31lw0smwl2.jpg&#34; alt=&#34;&#34; data-caption=&#34;&#34;&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;根据 CNCF 2020 年调查，46% 的组织在生产中使用服务网格或计划在未来 12 个月内使用。Istio 是在生产中使用的最多的网格。&lt;/p&gt;
&lt;h2 id=&#34;未来&#34;&gt;未来&lt;/h2&gt;
&lt;p&gt;经过 4 年的发展，围绕 Istio 不仅形成了庞大的用户群，还诞生了多家 Istio 供应商，你可以在最近改版的 &lt;a href=&#34;https://istio.io&#34; title=&#34;Istio 的官网首页&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio 的官网首页&lt;/a&gt;
中看到。在最近几个版本中，Istio 已经将发展中心转移到了提升 Day 2 Operation 体验上来了。我们还希望看到更多的 Istio 的采纳路径建议、案例研究、学习资料、培训及认证（例如来自 Tetrate 的业界的第一个 &lt;a href=&#34;https://academy.tetrate.io/courses/certified-istio-administrator&#34; title=&#34;Istio 管理员认证&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio 管理员认证&lt;/a&gt;
），这些都将有利于 Istio 的推广和采用。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>为什么在使用了 Kubernetes 后你可能还需要 Istio？</title>
      <link>https://jimmysong.io/blog/why-do-you-need-istio-when-you-already-have-kubernetes/</link>
      <pubDate>Wed, 07 Apr 2021 08:27:17 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/why-do-you-need-istio-when-you-already-have-kubernetes/</guid>
      <description>
        
        
        &lt;p&gt;如果你听说过服务网格，并尝试过 &lt;a href=&#34;https://istio.io/&#34; title=&#34;Istio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio&lt;/a&gt;
，你可能有以下问题。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为什么 Istio 要在 Kubernetes 上运行？&lt;/li&gt;
&lt;li&gt;Kubernetes 和服务网格在云原生应用架构中分别扮演什么角色？&lt;/li&gt;
&lt;li&gt;Istio 扩展了 Kubernetes 的哪些方面？它解决了哪些问题？&lt;/li&gt;
&lt;li&gt;Kubernetes、Envoy 和 Istio 之间是什么关系？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本文将带大家了解 Kubernetes 和 Istio 的内部工作原理。此外，我会介绍 Kubernetes 中的负载均衡方法，并解释为什么有了 Kubernetes 后还需要 Istio。&lt;/p&gt;
&lt;p&gt;Kubernetes 本质上是通过声明式配置来实现应用生命周期管理，而服务网格本质上是提供应用间的流量、安全管理和可观察性。如果你已经使用 Kubernetes 搭建了一个稳定的应用平台，那么如何设置服务间调用的负载均衡和流量控制？是否有这样一个通用的工具或者说平台（非 SDK），可以实现？这就需要用到服务网格了。&lt;/p&gt;
&lt;p&gt;Envoy 引入了 xDS 协议，这个协议得到了各种开源软件的支持，比如 Istio、&lt;a href=&#34;https://mosn.io/&#34; title=&#34;MOSN&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MOSN&lt;/a&gt;
 等。Envoy 将 xDS 贡献给服务网格或云原生基础设施。Envoy 本质上是一个现代版的代理，可以通过 API 进行配置，在此基础上衍生出许多不同的使用场景–比如 API Gateway、服务网格中的 sidecar 代理和边缘代理。&lt;/p&gt;
&lt;p&gt;本文包含以下内容。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kube-proxy 的作用描述。&lt;/li&gt;
&lt;li&gt;Kubernetes 在微服务管理方面的局限性。&lt;/li&gt;
&lt;li&gt;Istio 服务网格的功能介绍。&lt;/li&gt;
&lt;li&gt;Kubernetes、Envoy 和 Istio 服务网格中一些概念的比较。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kubernetes-vs-service-mesh&#34;&gt;Kubernetes vs Service Mesh&lt;/h2&gt;
&lt;p&gt;下图显示了 Kubernetes 中的服务访问关系和服务网格（每个 pod 模型一个 sidecar）。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;008eGmZEly1gpb7knfo4dj31hk0redrz.jpg&#34; alt=&#34;Kubernetes vs Service Mesh&#34; data-caption=&#34;Kubernetes vs Service Mesh&#34;&gt;
  
  &lt;figcaption&gt;Kubernetes vs Service Mesh&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;h3 id=&#34;流量转发&#34;&gt;流量转发&lt;/h3&gt;
&lt;p&gt;Kubernetes 集群中的每个节点都部署了一个 kube-proxy 组件，该组件与 Kubernetes API Server 进行通信，获取集群中的服务信息，然后设置 iptables 规则，将服务请求直接发送到对应的 Endpoint（属于同一组服务的 pod）。&lt;/p&gt;
&lt;h3 id=&#34;服务发现&#34;&gt;服务发现&lt;/h3&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;008eGmZEly1gpb7knwb79j30kq0fcjs9.jpg&#34; alt=&#34;Service Discovery&#34; data-caption=&#34;Service Discovery&#34;&gt;
  
  &lt;figcaption&gt;Service Discovery&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;Istio 可以跟踪 Kubernetes 中的服务注册，也可以在控制平面中通过平台适配器与其他服务发现系统对接；然后生成数据平面的配置（使用 CRD，这些配置存储在 etcd 中），数据平面的透明代理。数据平面的透明代理以 sidecar 容器的形式部署在每个应用服务的 pod 中，这些代理都需要请求控制平面同步代理配置。代理之所以 “透明”，是因为应用容器完全不知道代理的存在。过程中的 kube-proxy 组件也需要拦截流量，只不过 kube-proxy 拦截的是进出 Kubernetes 节点的流量–而 sidecar 代理拦截的是进出 pod 的流量。&lt;/p&gt;
&lt;h3 id=&#34;服务网格的劣势&#34;&gt;服务网格的劣势&lt;/h3&gt;
&lt;p&gt;由于 Kubernetes 的每个节点上都运行着很多 pod，所以在每个 pod 中放入原有的 kube-proxy 路由转发功能，会增加响应延迟–由于 sidecar 拦截流量时跳数更多，消耗更多的资源。为了对流量进行精细化管理，将增加一系列新的抽象功能。这将进一步增加用户的学习成本，但随着技术的普及，这种情况会慢慢得到缓解。&lt;/p&gt;
&lt;h3 id=&#34;服务网格的优势&#34;&gt;服务网格的优势&lt;/h3&gt;
&lt;p&gt;kube-proxy 的设置是全局的，无法对每个服务进行细粒度的控制，而 service mesh 通过 sidecar proxy 的方式将 Kubernetes 中的流量控制从服务层中抽离出来–可以实现更大的弹性。&lt;/p&gt;
&lt;h3 id=&#34;kube-proxy-的不足之处&#34;&gt;Kube-proxy 的不足之处&lt;/h3&gt;
&lt;p&gt;首先，如果转发的 pod 不能正常服务，它不会自动尝试其他 pod。每个 pod 都有一个健康检查机制，当一个 pod 出现健康问题时，kubelet 会重启 pod，kube-proxy 会删除相应的转发规则。另外，节点 Port 类型的服务不能添加 TLS 或更复杂的消息路由机制。&lt;/p&gt;
&lt;p&gt;Kube-proxy 实现了一个 Kubernetes 服务的多个 pod 实例之间的流量负载均衡，但如何对这些服务之间的流量进行精细化控制–比如将流量按百分比划分给不同的应用版本（这些应用版本都是同一个服务的一部分，但在不同的部署上），或者做金丝雀发布（灰度发布）和蓝绿发布？&lt;/p&gt;
&lt;p&gt;Kubernetes 社区给出了一个使用 Deployment 做&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments&#34; title=&#34;金丝雀发布&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;金丝雀发布&lt;/a&gt;
的方法，本质上是通过修改 pod 的标签来给部署的服务分配不同的 pod。&lt;/p&gt;
&lt;h3 id=&#34;kubernetes-ingress-vs-istio-gateway&#34;&gt;Kubernetes Ingress vs Istio Gateway&lt;/h3&gt;
&lt;p&gt;如上所述，kube-proxy 只能在 Kubernetes 集群内路由流量。Kubernetes 集群的 pod 位于 CNI 创建的网络中。Ingress 是在 Kubernetes 中创建的资源对象，用于集群外部的通信。它由位于 Kubernetes 边缘节点上的入口控制器驱动，负责管理南北向流量。Ingress 必须与各种 Ingress 控制器对接，比如 &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34; title=&#34;nginx ingress 控制器&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nginx ingress 控制器&lt;/a&gt;
和 &lt;a href=&#34;https://traefik.io/&#34; title=&#34;traefik&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;traefik&lt;/a&gt;
。Ingress 只适用于 HTTP 流量，使用简单。它只能通过匹配有限的字段来路由流量——如服务、端口、HTTP 路径等。这使得它无法对 TCP 流量进行路由，如 MySQL、Redis 和各种 RPC。这就是为什么你会看到人们在 ingress 资源注释中写 Nginx 配置语言的原因（注：使用 Nginx Ingress Controller 可以通过 &lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/&#34; title=&#34;配置 ConfigMap 和 Service 的方式&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;配置 ConfigMap 和 Service 的方式&lt;/a&gt;
来变通支持 TCP 和 UDP  流量转发）。直接路由南北流量的唯一通行方法是使用服务的 LoadBalancer 或 NodePort，前者需要云厂商支持，后者需要额外的端口管理。&lt;/p&gt;
&lt;p&gt;Istio Gateway 的功能与 Kubernetes Ingress 类似，它负责进出集群的南北流量。Istio Gateway 描述了一个负载均衡器，用于承载进出服务网格边缘的连接。该规范描述了一组开放端口和这些端口所使用的协议，以及用于负载均衡的 SNI 配置等。Gateway 是一个 CRD 扩展，它也重用了 sidecar 代理的功能；详细配置请参见 &lt;a href=&#34;https://istio.io/latest/docs/reference/config/networking/gateway/&#34; title=&#34;Istio 网站&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio 网站&lt;/a&gt;
。&lt;/p&gt;
&lt;h2 id=&#34;envoy&#34;&gt;Envoy&lt;/h2&gt;
&lt;p&gt;Envoy 是 Istio 中默认的 sidecar 代理。Istio 基于 Enovy 的 xDS 协议扩展了其控制平面。在讨论 Envoy 的 xDS 协议之前，我们需要先熟悉 Envoy 的基本术语。下面是 Envoy 的架构图。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;envoy-arch.jpg&#34; alt=&#34;Envoy 架构图&#34; data-caption=&#34;Envoy 架构图&#34;&gt;
  
  &lt;figcaption&gt;Envoy 架构图&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;h3 id=&#34;基础概念&#34;&gt;基础概念&lt;/h3&gt;
&lt;p&gt;以下是 Enovy 中你应该知道的基本术语。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;下游。下游主机连接到 Envoy，发送请求，并接收响应，即发送请求的主机。&lt;/li&gt;
&lt;li&gt;上游：上游主机。上游主机接收来自 Envoy 的连接和请求，并返回响应；即接收请求的主机。&lt;/li&gt;
&lt;li&gt;Listener：监听器。监听器是一个命名的网络地址（如端口、UNIX 域套接字等）；下游客户端可以连接到这些监听器。Envoy 将一个或多个监听器暴露给下游主机进行连接。&lt;/li&gt;
&lt;li&gt;集群。集群是一组逻辑上相同的上游主机，Envoy 连接到它们。Envoy 通过服务发现来发现集群的成员。可以选择通过主动的健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略来决定集群中哪个成员的请求路由。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 Envoy 中可以设置多个监听器，每个监听器可以设置一个过滤链（过滤链表），而且过滤链是可扩展的，这样我们可以更方便地操纵流量的行为–比如设置加密、私有 RPC 等。&lt;/p&gt;
&lt;p&gt;xDS 协议是由 Envoy 提出的，是 Istio 中默认的 sidecar 代理，但只要实现了 xDS 协议，理论上也可以作为 Istio 中的 sidecar 代理 —— 比如蚂蚁集团开源的 &lt;a href=&#34;https://mosn.io&#34; title=&#34;MOSN&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MOSN&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cdn.thenewstack.io/media/2021/03/b800bf17-image3.png&#34; title=&#34;
  &amp;lt;figure class=&amp;#34;mx-auto text-center&amp;#34;&amp;gt;
  &amp;lt;img src=&amp;#34;008eGmZEly1gpb7kk7wk4j31060lqgqx.jpg&amp;#34; alt=&amp;#34;img&amp;#34; data-caption=&amp;#34;img&amp;#34;&amp;gt;
  
  &amp;lt;figcaption&amp;gt;img&amp;lt;/figcaption&amp;gt;
  
  &amp;lt;/figure&amp;gt;

&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;008eGmZEly1gpb7kk7wk4j31060lqgqx.jpg&#34; alt=&#34;img&#34; data-caption=&#34;img&#34;&gt;
  
  &lt;figcaption&gt;img&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Istio 是一个功能非常丰富的服务网格，包括以下功能。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;流量管理。这是 Istio 最基本的功能。&lt;/li&gt;
&lt;li&gt;策略控制。实现访问控制系统、遥测采集、配额管理、计费等功能。&lt;/li&gt;
&lt;li&gt;可观察性。在 sidecar 代理中实现。&lt;/li&gt;
&lt;li&gt;安全认证。由 Citadel 组件进行密钥和证书管理。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;istio-中的流量管理&#34;&gt;Istio 中的流量管理&lt;/h2&gt;
&lt;p&gt;Istio 中定义了以下 CRD 来帮助用户进行流量管理。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;网关。网关描述了一个运行在网络边缘的负载均衡器，用于接收传入或传出的 HTTP/TCP 连接。&lt;/li&gt;
&lt;li&gt;虚拟服务（VirtualService）。VirtualService 实际上是将 Kubernetes 服务连接到 Istio 网关。它还可以执行额外的操作，例如定义一组流量路由规则，以便在主机寻址时应用。&lt;/li&gt;
&lt;li&gt;DestinationRule。DestinationRule 定义的策略决定了流量被路由后的访问策略。简单来说，它定义了流量的路由方式。其中，这些策略可以定义为负载均衡配置、连接池大小和外部检测（用于识别和驱逐负载均衡池中不健康的主机）配置。&lt;/li&gt;
&lt;li&gt;EnvoyFilter。EnvoyFilter 对象描述了代理服务的过滤器，可以自定义 Istio Pilot 生成的代理配置。这种配置一般很少被主用户使用。&lt;/li&gt;
&lt;li&gt;ServiceEntry。默认情况下，Istio 服务 Mesh 中的服务无法发现 Mesh 之外的服务。ServiceEntry 可以在 Istio 内部的服务注册表中添加额外的条目，从而允许 Mesh 中自动发现的服务访问并路由到这些手动添加的服务。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kubernetes-vs-xds-vs-istio&#34;&gt;Kubernetes vs xDS vs Istio&lt;/h2&gt;
&lt;p&gt;在回顾了 Kubernetes 的 kube-proxy 组件、xDS 和 Istio 对流量管理的抽象后，现在我们仅从流量管理的角度来看看这三个组件 / 协议的比较（注意，三者并不完全等同）。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;xDS&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Istio service mesh&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Endpoint&lt;/td&gt;
&lt;td&gt;Endpoint&lt;/td&gt;
&lt;td&gt;WorkloadEntry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service&lt;/td&gt;
&lt;td&gt;Route&lt;/td&gt;
&lt;td&gt;VirtualService&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-proxy&lt;/td&gt;
&lt;td&gt;Route&lt;/td&gt;
&lt;td&gt;DestinationRule&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-proxy&lt;/td&gt;
&lt;td&gt;Listener&lt;/td&gt;
&lt;td&gt;EnvoyFilter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ingress&lt;/td&gt;
&lt;td&gt;Listener&lt;/td&gt;
&lt;td&gt;Gateway&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service&lt;/td&gt;
&lt;td&gt;Cluster&lt;/td&gt;
&lt;td&gt;ServiceEntry&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;核心观点&#34;&gt;核心观点&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes 的本质是应用生命周期管理，具体来说就是部署和管理（伸缩、自动恢复、发布）。&lt;/li&gt;
&lt;li&gt;Kubernetes 为微服务提供了一个可扩展、高弹性的部署和管理平台。&lt;/li&gt;
&lt;li&gt;服务网格是基于透明代理，通过 sidecar 代理拦截服务之间的流量，然后通过控制平面配置管理它们的行为。&lt;/li&gt;
&lt;li&gt;服务网格将流量管理与 Kubernetes 解耦，不需要 kube-proxy 组件来支持服务网格内的流量；通过提供更接近微服务应用层的抽象来管理服务间的流量、安全性和可观察性。&lt;/li&gt;
&lt;li&gt;xDS 是服务网格的协议标准之一。&lt;/li&gt;
&lt;li&gt;服务网格是 Kubernetes 中服务的一个更高层次的抽象。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;如果说 Kubernetes 管理的对象是一个 pod，那么服务网格管理的对象就是一个服务，所以用 Kubernetes 管理微服务，然后应用服务网格就可以了。如果你连服务都不想管理，那就用 &lt;a href=&#34;https://knative.dev/&#34; title=&#34;Knative&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Knative&lt;/a&gt;
 这样的无服务器平台，不过这是后话。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>云原生初学者入门必读</title>
      <link>https://jimmysong.io/blog/must-read-for-cloud-native-beginner/</link>
      <pubDate>Sun, 18 Oct 2020 14:18:40 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/must-read-for-cloud-native-beginner/</guid>
      <description>
        
        
        &lt;h2 id=&#34;为什么写这篇文章&#34;&gt;为什么写这篇文章&lt;/h2&gt;
&lt;p&gt;看到这个标题后，大家可能会问“都已经 2020 年了，Kubernetes 开源有 6 年时间了，为什么还要写一篇 Kubernetes 入门的文章？”我想说的是，Kubernetes 还远远没有达到我们想象的那么普及。众多的开发者，平时忙于各自的业务开发，学习新技术的时间有限；还有大量的学生群体，可能还仅仅停留在“知道有这门技术”的阶段，远远没有入门。这篇文章将助于各位有志于从事云原生领域工作或需要了解该领域背景的人群快速入门 Kubernetes 和云原生。&lt;/p&gt;
&lt;p&gt;因为云原生的知识体系过于庞杂，本文主要讲解容器、Kubernetes 及服务网格的入门概念，关于云原生的更多细节将在后续文章中推出。&lt;/p&gt;
&lt;h2 id=&#34;引言&#34;&gt;引言&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/&#34; title=&#34;Kubernetes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes&lt;/a&gt;
 一词来自希腊语，意思是“飞行员”或“舵手”。这个名字很贴切，Kubernetes 可以帮助你在波涛汹涌的容器海洋中航行。&lt;/p&gt;
&lt;p&gt;Kubernetes 是做什么的？什么是 Docker？什么是容器编排？Kubernetes 是如何工作和扩展的？你可能还有很多其他的问题，本文将一一为你解答。&lt;/p&gt;
&lt;p&gt;这篇文章适合初学者，尤其是那些工作忙碌，没有办法抽出太多时间来了解 Kubernetes 和云原生的开发者们，希望本文可以帮助你进入 Kubernetes 的世界。&lt;/p&gt;
&lt;p&gt;简而言之，Kubernetes 提供了一个平台或工具来帮助你快速协调或扩展容器化应用，特别是在 &lt;a href=&#34;https://docker.com/&#34; title=&#34;Docker&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Docker&lt;/a&gt;
 容器。让我们深入了解一下这些概念。&lt;/p&gt;
&lt;h2 id=&#34;容器和容器化&#34;&gt;容器和容器化&lt;/h2&gt;
&lt;p&gt;那么什么是容器呢？&lt;/p&gt;
&lt;p&gt;要讨论容器化首先要谈到虚拟机 (VM)，顾名思义，虚拟机就是可以远程连接的虚拟服务器，比如 AWS 的 EC2 或阿里云的 ECS。&lt;/p&gt;
&lt;p&gt;接下来，假如你要在虚拟机上运行一个网络应用——包括一个 MySQL 数据库、一个 Vue 前端和一些 Java 库，在 Ubuntu 操作系统 (OS) 上运行。你不用熟悉其中的每一个技术——你只要记住，一个应用程序由各种组件、服务和库组成，它们运行在操作系统上。&lt;/p&gt;
&lt;p&gt;现在，将应用程序打包成一个虚拟机镜像，这个镜像中包括了 Ubuntu 操作系统。这使得虚拟机变得非常笨重——通常有几个 G 的大小。&lt;/p&gt;
&lt;p&gt;虚拟机镜像包含了整个操作系统及所有的库，对应用程序来说，这个镜像过于臃肿，其中大部分组件并没有被应用程序直接调用。如果你需要重新创建、备份或扩展这个应用程序，就需要复制整个环境（虚拟机镜像），在新环境中启动应用通常需要几十秒甚至几分钟时间。如果你想单独升级应用中的某个组件，比如说 Vue 应用，就需要重建整个虚拟机镜像。另外，如果你的两个应用依赖同一个底层镜像，升级底层镜像会同时影响这两个应用，而有时候，你只需要升级其中一个应用的依赖而已。这就是所谓的“依赖陷阱”。&lt;/p&gt;
&lt;p&gt;解决这个问题的办法就是容器。容器是继虚拟机之后更高层次的抽象，在这层抽象中，整个应用程序的每个组件被单独打包成一个个独立的单元，这个单元就是所谓的容器。通过这种方式，可以将代码和应用服务从底层架构中分离出来，实现了完全的可移植性（在任何操作系统或环境上运行应用的能力）。所以在上面的例子中，Ubuntu 操作系统就是一个单元（容器）。MySQL 数据库是另一个容器，Vue 环境和随之而来的库也是一个容器。&lt;/p&gt;
&lt;p&gt;但是，MySQL 数据库是如何自己“运行”的？数据库本身肯定也要在操作系统上运行吧？没错！&lt;/p&gt;
&lt;p&gt;更高层次的容器，比如 MySQL 容器，实际上会包含必要的库来与底层的操作系统容器通信和集成。所以你可以把容器看成是整个应用堆栈中的一层，每层都依赖于下层的单元。而这就类似于船舶或港口中集装箱的堆叠方式，每个容器的稳定性都依赖于下面的容器的支持。所以应用容器的核心是一个受控的执行环境。它们允许你从头开始定义整个环境，从操作系统开始，到你要使用的各个版本的库，再到你要添加的代码版本。&lt;/p&gt;
&lt;p&gt;与容器相关的一个重要概念是&lt;strong&gt;微服务&lt;/strong&gt;。将应用程序的各个组件拆分并打包成独立的服务，这样每个组件都可以很容易地被替换、升级、调试。上面的例子中，我们会为 Vue 前端创建一个微服务，为 MySQL 数据库创建另一个微服务，为 Java 中间件部分创建另一个微服务，以此类推。很明显，微服务与容器化是相辅相成的。&lt;/p&gt;
&lt;h2 id=&#34;从-docker-开始&#34;&gt;从 Docker 开始&lt;/h2&gt;
&lt;p&gt;现在你已经对容器有一定了解了吧？Docker 是最常用的容器化工具，也是最流行的容器运行时。&lt;/p&gt;
&lt;p&gt;Docker 开源于 2013 年。用于打包和创建容器，管理基于容器的应用。所有 Linux 发行版、Windows 和 macOS 都支持 Docker。&lt;/p&gt;
&lt;p&gt;还有其他的容器化工具，如 &lt;a href=&#34;https://coreos.com/rkt/&#34; title=&#34;CoreOS rkt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CoreOS rkt&lt;/a&gt;
、&lt;a href=&#34;http://mesos.apache.org/documentation/latest/mesos-containerizer/&#34; title=&#34;Mesos Containerizer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mesos Containerizer&lt;/a&gt;
 和 &lt;a href=&#34;https://linuxcontainers.org/&#34; title=&#34;LXC&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LXC&lt;/a&gt;
。但是目前，绝大多数的容器化应用都是在 Docker 上运行的。&lt;/p&gt;
&lt;h2 id=&#34;再到-kubernetes&#34;&gt;再到 Kubernetes&lt;/h2&gt;
&lt;p&gt;首先，简单介绍一下历史。Kubernetes 是 Google 基于其内部容器调度平台 Borg 的经验开发的。2014 年开源，并作为 CNCF（云原生计算基金会）的核心发起项目。&lt;/p&gt;
&lt;p&gt;那么 Kubernetes 又跟容器是什么关系呢？让我们再回到上面的例子。假设我们的应用爆火，每天的注册用户越来越多。&lt;/p&gt;
&lt;p&gt;现在，我们需要增加后端资源，使浏览我们网站的用户在浏览页面时加载时间不会过长或者超时。最简单的方式就是增加容器的数量，然后使用负载均衡器将传入的负载（以用户请求的形式）分配给容器。&lt;/p&gt;
&lt;p&gt;这样做虽然行之有效，但也只能在用户规模有限的情况下使用。当用户请求达到几十万或几百万时，这种方法也是不可扩展的。你需要管理几十个也许是几百个负载均衡器，这本身就是另一个令人头疼的问题。如果我们想对网站或应用进行任何升级，也会遇到问题，因为负载均衡不会考虑到应用升级的问题。我们需要单独配置每个负载均衡器，然后升级该均衡器所服务的容器。想象一下，当你有 20 个负载均衡器和每周 5 或 6 个小的更新时，你将不得不进行大量的手工劳动。&lt;/p&gt;
&lt;p&gt;我们需要的是一种可以一次性将变更传递给所有受控容器的方法，同时也需要一种可以轻松地调度可用容器的方法，这个过程还必须要是自动化的，这正是 Kubernetes 所做的事情。&lt;/p&gt;
&lt;p&gt;接下来，我们将探讨 Kubernetes 究竟是如何工作的，它的各种组件和服务，以及更多关于如何使用 Kubernetes 来编排、管理和监控容器化环境。为了简单起见，假设我们使用的是 Docker 容器，尽管如前所述，Kubernetes 除了支持 Docker 之外，还支持其他几种容器平台。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-架构和组件&#34;&gt;Kubernetes 架构和组件&lt;/h2&gt;
&lt;p&gt;首先，最重要的是你需要认识到 Kubernetes 利用了“期望状态”原则。就是说，你定义了组件的期望状态，而 Kubernetes 要将它们始终调整到这个状态。&lt;/p&gt;
&lt;p&gt;例如，你想让你的 Web 服务器始终运行在 4 个容器中，以达到负载均衡的目的，你的数据库复制到 3 个不同的容器中，以达到冗余的目的。这就是你想要的状态。如果这 7 个容器中的任何一个出现故障，Kubernetes 引擎会检测到这一点，并自动创建出一个新的容器，以确保维持所需的状态。&lt;/p&gt;
&lt;p&gt;现在我们来定义一些 Kubernetes 的重要组件。&lt;/p&gt;
&lt;p&gt;当你第一次设置 Kubernetes 时，你会创建一个集群。所有其他组件都是集群的一部分。你也可以创建多个虚拟集群，称为命名空间 (namespace)，它们是同一个物理集群的一部分。这与你可以在同一物理服务器上创建多个虚拟机的方式非常相似。如果你不需要，也没有明确定义的命名空间，那么你的集群将在始终存在的默认命名空间中创建。&lt;/p&gt;
&lt;p&gt;Kubernetes 运行在节点 (node) 上，节点是集群中的单个机器。如果你有自己的硬件，节点可能对应于物理机器，但更可能对应于在云中运行的虚拟机。节点是部署你的应用或服务的地方，是 Kubernetes 工作的地方。有 2 种类型的节点——master 节点和 worker 节点，所以说 Kubernetes 是主从结构的。&lt;/p&gt;
&lt;p&gt;主节点是一个控制其他所有节点的特殊节点。一方面，它和集群中的任何其他节点一样，这意味着它只是另一台机器或虚拟机。另一方面，它运行着控制集群其他部分的软件。它向集群中的所有其他节点发送消息，将工作分配给它们，工作节点向主节点上的 API Server 汇报。&lt;/p&gt;
&lt;p&gt;Master 节点本身也包含一个名为 API Server 的组件。这个 API 是节点与控制平面通信的唯一端点。API Server 至关重要，因为这是 worker 节点和 master 节点就 pod、deployment 和所有其他 Kubernetes API 对象的状态进行通信的点。&lt;/p&gt;
&lt;p&gt;Woker 节点是 Kubernetes 中真正干活的节点。当你在应用中部署容器或 pod（稍后定义）时，其实是在将它们部署到 worker 节点上运行。Worker 节点托管和运行一个或多个容器的资源。&lt;/p&gt;
&lt;p&gt;Kubernetes 中的逻辑而非物理的工作单位称为 pod。一个 pod 类似于 Docker 中的容器。记得我们在前面讲到，容器可以让你创建独立、隔离的工作单元，可以独立运行。但是要创建复杂的应用程序，比如 Web 服务器，你经常需要结合多个容器，然后在一个 pod 中一起运行和管理。这就是 pod 的设计目的——一个 pod 允许你把多个容器，并指定它们如何组合在一起来创建应用程序。而这也进一步明确了 Docker 和 Kubernetes 之间的关系——一个 Kubernetes pod 通常包含一个或多个 Docker 容器，所有的容器都作为一个单元来管理。&lt;/p&gt;
&lt;p&gt;Kubernetes 中的 service 是一组逻辑上的 pod。把一个 service 看成是一个 pod 的逻辑分组，它提供了一个单一的 IP 地址和 DNS 名称，你可以通过它访问服务内的所有 pod。有了服务，就可以非常容易地设置和管理负载均衡，当你需要扩展 Kubernetes pod 时，这对你有很大的帮助，我们很快就会看到。&lt;/p&gt;
&lt;p&gt;ReplicationController 或 ReplicaSet 是 Kubernetes 的另一个关键功能。它是负责实际管理 pod 生命周期的组件——当收到指令时或 pod 离线或意外停止时启动 pod，也会在收到指示时杀死 pod，也许是因为用户负载减少。所以换句话说，ReplicationController 有助于实现我们所期望的指定运行的 pod 数量的状态。&lt;/p&gt;
&lt;h2 id=&#34;什么是-kubectl&#34;&gt;什么是 Kubectl？&lt;/h2&gt;
&lt;p&gt;kubectl 是一个命令行工具，用于与 Kubernetes 集群和其中的 pod 通信。使用它你可以查看集群的状态，列出集群中的所有 pod，进入 pod 中执行命令等。你还可以使用 YAML 文件定义资源对象，然后使用 kubectl 将其应用到集群中。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-中的自动扩展&#34;&gt;Kubernetes 中的自动扩展&lt;/h2&gt;
&lt;p&gt;请记住，我们使用 Kubernetes 而不是直接使用 Docker 的原因之一，是因为 Kubernetes 能够自动扩展应用实例的数量以满足工作负载的需求。&lt;/p&gt;
&lt;p&gt;自动缩放是通过集群设置来实现的，当服务需求增加时，增加节点数量，当需求减少时，则减少节点数量。但也要记住，节点是 “物理” 结构——我们把“物理”放在引号里，因为要记住，很多时候，它们实际上是虚拟机。&lt;/p&gt;
&lt;p&gt;无论如何，节点是物理机器的事实意味着我们的云平台必须允许 Kubernetes 引擎创建新机器。各种云提供商对 Kubernetes 支持基本都满足这一点。&lt;/p&gt;
&lt;p&gt;我们再继续说一些概念，这次是和网络有关的。&lt;/p&gt;
&lt;h2 id=&#34;什么是-kubernetes-ingress-和-egress&#34;&gt;什么是 kubernetes Ingress 和 Egress？&lt;/h2&gt;
&lt;p&gt;外部用户或应用程序与 Kubernetes pod 交互，就像 pod 是一个真正的服务器一样。我们需要设置安全规则允许哪些流量可以进入和离开“服务器”，就像我们为托管应用程序的服务器定义安全规则一样。&lt;/p&gt;
&lt;p&gt;进入 Kubernetes pod 的流量称为 Ingress，而从 pod 到集群外的出站流量称为 egress。我们创建入口策略和出口策略的目的是限制不需要的流量进入和流出服务。而这些策略也是定义 pod 使用的端口来接受传入和传输传出数据 / 流量的地方。&lt;/p&gt;
&lt;h2 id=&#34;什么是-ingress-controller&#34;&gt;什么是 Ingress Controller？&lt;/h2&gt;
&lt;p&gt;但是在定义入口和出口策略之前，你必须首先启动被称为 Ingress Controller（入口控制器）的组件；这个在集群中默认不启动。有不同类型的入口控制器，Kubernetes 项目默认只支持 Google Cloud 和开箱即用的 Nginx 入口控制器。通常云供应商都会提供自己的入口控制器。&lt;/p&gt;
&lt;h2 id=&#34;什么是-replica-和-replicaset&#34;&gt;什么是 Replica 和 ReplicaSet？&lt;/h2&gt;
&lt;p&gt;为了保证应用程序的弹性，需要在不同节点上创建多个 pod 的副本。这些被称为 Replica。假设你所需的状态策略是“让名为 webserver-1 的 pod 始终维持在 3 个副本”，这意味着 ReplicationController 或 ReplicaSet 将监控活动副本的数量，如果其中有任何一个 replica 因任何原因不可用（例如节点的故障），那么 Deployment Controller 将自动创建一个新的系统（定义如下）。&lt;/p&gt;
&lt;p&gt;所需状态是在 deployment 中定义的。 Master 节点的中有一个子系统叫做 Deployment Controller，负责实际执行并使当前状态不断趋向于所需状态。&lt;/p&gt;
&lt;p&gt;因此，举例来说，如果你目前有 2 个 pod 的副本，而你所希望的状态应该有 3 个，那么 Replication Controller 或 ReplicaSet 会自动检测到这个要求，并指示 Deployment Controller 根据预定义的设置部署一个新的 pod。&lt;/p&gt;
&lt;h2 id=&#34;什么是服务网格&#34;&gt;什么是服务网格？&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://jimmysong.io/blog/what-is-a-service-mesh/&#34; title=&#34;服务网格 (Service Mesh)&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;服务网格 (Service Mesh)&lt;/a&gt;
 用于管理服务之间的网络流量，是云原生的网络基础设施层，也是 &lt;a href=&#34;https://jimmysong.io/blog/post-kubernetes-era/&#34; title=&#34;Kubernetes 次世代的云原生应用&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes 次世代的云原生应用&lt;/a&gt;
 的重要组成部分。&lt;/p&gt;
&lt;p&gt;服务网格利用容器之间的网络设置来控制或改变应用程序中不同组件之间的交互。下面，我们用一个例子来说明。假设你想测试 Nginx 的新版本，检查它是否与你的 Web 应用兼容。你用新的 Nginx 版本创建了一个新的容器 (Container2)，并从当前容器 (Container1) 中复制了当前的 Nginx webserver 配置。但你不想影响组成 web 应用的其他微服务（假设每个容器对应一个单独的微服务）——就是 MySQL 数据库、Node.js 前端、负载均衡器等。&lt;/p&gt;
&lt;p&gt;所以使用服务网格，你可以立即只把 webserver 微服务改成 Container2（新 Nginx 版本的那个）进行测试。如果确定它不能工作，比如因为它导致网站出现一些兼容性问题，那么你就调用服务网格来快速切换回原来的 Container1。而这一切都不需要对其他容器进行任何配置变更——这些变更对其他容器是完全透明的。&lt;/p&gt;
&lt;p&gt;如果没有服务网格，对容器来说这项工作将十分繁琐，因为这涉及到逐一更改所有其他容器上的配置，将它们所包含的服务从 Container1 指向 Container2，然后在测试失败后，将它们全部改回来。&lt;/p&gt;
&lt;p&gt;在前面这部分 Kubernetes 指南中，我们介绍了一些与 Kubernetes 网络相关的概念。Kubernetes 中的网络可能很棘手，很难理解，如果你刚刚开始，你可能需要一些实践来理解这里。关于服务网格的更多内容请参考 &lt;a href=&#34;https://www.servicemesher.com/istio-handbook&#34; title=&#34;Istio Handbook——Istio 服务网格进阶实战&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio Handbook——Istio 服务网格进阶实战&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;在下一部分中，我们将展开更多关于 Kubernetes 的话题：如何开始学习 Kubernetes，如何在本地安装和测试 Kubernetes，以及 Kubernetes 的一些优秀的监控工具。&lt;/p&gt;
&lt;h2 id=&#34;如何学习-kubernetes&#34;&gt;如何学习 Kubernetes？&lt;/h2&gt;
&lt;p&gt;自学 Kubernetes 知识基本上有三种不同的途径，我们在这里只提供了一个指导大纲。&lt;/p&gt;
&lt;h3 id=&#34;一从零开始学习和安装-kubernetes&#34;&gt;一、从零开始学习和安装 Kubernetes&lt;/h3&gt;
&lt;p&gt;要想真正掌握 Kubernetes，最好的办法莫过于自己从头开始安装 Kubernetes。不过要注意的是，从零开始安装 Kubernetes 并不是一件容易的事情。安装 Kubernetes 并不是简单的“下载文件 -&amp;gt; 点击安装”式的操作，Kubernetes 由多个组件组成，这些组件必须单独安装和配置。而在此之前，你也需要相当的技术储备来做安装前的准备，比如熟悉 Linux 操作系统。如果你决定使用这种方式学习的话，推荐你阅读 &lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook&#34; title=&#34;Kubernetes Handbook——Kubernetes 中文指南 / 云原生架构实践手册&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes Handbook——Kubernetes 中文指南 / 云原生架构实践手册&lt;/a&gt;
。此外，请记住，尽管 Kubernetes 作为一个开源解决方案在技术上是免费的，但它确实有一些隐藏的成本，只不过对初学者来说可能并不明显。&lt;/p&gt;
&lt;h3 id=&#34;二kubernetes-自托管解决方案&#34;&gt;二、Kubernetes 自托管解决方案&lt;/h3&gt;
&lt;p&gt;这些解决方案样是一些工具和实用程序，大大简化了在本地计算机上安装和配置小型 Kubernetes 集群的任务。它们是学习 Kubernetes 的好方法，同时对于新手来说也不会太难，又足够小巧可以到安装在个人电脑上。最流行的自托管 Kubernetes 工具和环境是 &lt;a href=&#34;https://github.com/kubernetes/minikube&#34; title=&#34;Minikube&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Minikube&lt;/a&gt;
、&lt;a href=&#34;https://github.com/ubuntu/microk8s&#34; title=&#34;MicroK8s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MicroK8s&lt;/a&gt;
、&lt;a href=&#34;https://docs.docker.com/docker-for-windows/kubernetes/&#34; title=&#34;Docker Desktop&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Docker Desktop&lt;/a&gt;
 和 &lt;a href=&#34;https://github.com/kubernetes-sigs/kind&#34; title=&#34;Kind&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kind&lt;/a&gt;
。这些解决方案往往有一些限制，例如，Minikube 只允许创建一个节点。尽管有这些缺点，但这些工具还是非常值得推荐，因为它们将易学性和成本效益结合起来，对于刚开始使用 Kubernetes 的初学者来说，是一个很好的选择。&lt;/p&gt;
&lt;h3 id=&#34;三云托管的解决方案&#34;&gt;三、云托管的解决方案&lt;/h3&gt;
&lt;p&gt;如今各大云供应商都提供了定制化的 Kubernetes 解决方案来。你也可以通过线上教学平台如 &lt;a href=&#34;https://katacoda.com/&#34; title=&#34;Katacoda&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Katacoda&lt;/a&gt;
 上的免费课程来学习 Kubernetes，它们都是云托管的，你不需要自己安装，只不过你需要云供应商的集群需要付费。&lt;/p&gt;
&lt;h2 id=&#34;本地测试和调试-kubernetes&#34;&gt;本地测试和调试 Kubernetes&lt;/h2&gt;
&lt;p&gt;作为本地安装 Kubernetes 的一部分，你很可能还需要一些测试和调试能力，以确保一切都在顺利运行，特别是定义入口和出口策略等棘手的任务。此外，还有 Kubernetes 附加组件的生态系统，你可能想使用这些组件来扩展 Kubernetes 集群的功能。添加所有这些都需要进行更多的测试，以确保它们能与你的 Kubernetes 集群完美的集成。&lt;/p&gt;
&lt;p&gt;用于在本地开发和调试 Kubernetes 服务的工具有：&lt;a href=&#34;https://github.com/microsoft/mindaro&#34; title=&#34;Microsoft Bridge to Kubernetes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Microsoft Bridge to Kubernetes&lt;/a&gt;
 和 &lt;a href=&#34;https://github.com/telepresenceio/telepresence&#34; title=&#34;telepresence&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;telepresence&lt;/a&gt;
。这些工具可以让你在本地运行单个服务，同时将该服务连接到远程 Kubernetes 集群。这样你就可以让自己的本地机器作为 Kubernetes 集群中的一部分来运行——这对于在本地而不是在生产集群上开发服务非常有用。&lt;/p&gt;
&lt;p&gt;Kubernetes 项目也了解到了 Kubernetes 安装对端到端 (E2E) 测试的需求。为此，项目核心团队一直在确保在最近的版本中更恰当地支持 E2E 测试。这包括诸如允许测试重用和纳入更多附加组件和驱动程序的测试等。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-监控工具&#34;&gt;Kubernetes 监控工具&lt;/h2&gt;
&lt;p&gt;Kubernetes 提供了应用程序在集群的每个层次上的资源使用情况的详细信息——容器、pod、服务。这些详细信息使你能够评估应用程序的性能，确定哪些瓶颈可以解决以提高整体性能。&lt;/p&gt;
&lt;p&gt;毕竟，监控可以帮助你了解应用和集群运行情况的详细信息，这对于学习 Kubernetes 是十分有帮助的。&lt;/p&gt;
&lt;p&gt;Kubernetes 包含两个内置度量收集工具用于监控：&lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/&#34; title=&#34;资源管道和全度量管道&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;资源管道和全度量管道&lt;/a&gt;
。资源管道是一个较低级和较有限的工具，主要集中在与各种控制器相关的指标上。全指标管道，顾名思义，从几乎所有集群组件中获取并显示更丰富的指标。&lt;/p&gt;
&lt;p&gt;还有一些第三方工具可以安装并集成到 Kubernetes 集群中。对于 Kubernetes 来说，最普遍使用的两个工具是 Prometheus 和 Grafana。&lt;/p&gt;
&lt;h3 id=&#34;prometheus-监控&#34;&gt;Prometheus 监控&lt;/h3&gt;
&lt;p&gt;Prometheus 是一个功能丰富的开源监控和警报工具。Prometheus 包含一个内部数据存储用来收集指标，如生成的时间序列数据。Prometheus 还拥有众多插件，允许它将数据暴露给各种外部解决方案，并从其他数据源导入数据，包括所有主要公有云监控解决方案。&lt;/p&gt;
&lt;h3 id=&#34;grafana-仪表盘&#34;&gt;Grafana 仪表盘&lt;/h3&gt;
&lt;p&gt;Grafana 是一个优秀的仪表盘、分析和数据可视化工具。它没有 Prometheus 的全功能数据收集能力，但 Prometheus 又没有 Grafana 的数据呈现界面。事实上，他们最好是结合在一起使用——Prometheus 负责数据收集和汇总，Grafana 负责数据展示。它们共同创造了一个强大的组合，涵盖了数据收集、基本警报和可视化。&lt;/p&gt;
&lt;h3 id=&#34;高级警报&#34;&gt;高级警报&lt;/h3&gt;
&lt;p&gt;对于高级警报，你可以添加 &lt;a href=&#34;https://www.nagios.org/&#34; title=&#34;Nagios&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nagios&lt;/a&gt;
 或 &lt;a href=&#34;https://github.com/prometheus/alertmanager&#34; title=&#34;Prometheus Alertmanager&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prometheus Alertmanager&lt;/a&gt;
 等工具。这些警报工具通常有大量的集成。你可以为自定义值班团队，然后定义你想要监控的参数，例如“当任何 pod 不可用时”或“当任何节点无法访问时”、“当容量达到 90%”等，然后通过电子邮件、短信、手机应用提醒、电话呼叫等方式向值班人员发送自定义通知。你还可以创建升级策略，比如，如果一个被定义为“危急”的警报在 10 分钟内没有值班人员确认，那么就将警报升级（发送警报）到该人员的经理。&lt;/p&gt;
&lt;p&gt;现在，你应该已经对 Docker 和 Kubernetes 有了大体的认识。了解了 Kubernetes 的作用，知道它是如何进行容器化应用部署和管理的。&lt;/p&gt;
&lt;p&gt;调试和监控技术不仅仅是运维需要，你也可以把它当作学习方式。有什么比边做边学更好呢？&lt;/p&gt;
&lt;p&gt;请记住，如果你的应用规模太小，而且预计用户需求不会有太大变化或重大波动（比如一个只在公司内部使用的应用），那么 Kubernetes 对你来说可能没有必要，这种情况下，直接使用 Docker 就足够了。&lt;/p&gt;
&lt;h2 id=&#34;更多&#34;&gt;更多&lt;/h2&gt;
&lt;p&gt;云原生领域的开源项目众多（见 &lt;a href=&#34;https://jimmysong.io/awesome-cloud-native&#34; title=&#34;Awesome Cloud Native/云原生开源项目大全&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Awesome Cloud Native/云原生开源项目大全&lt;/a&gt;
），其中有大量的优秀项目可供我们学习。此外，Kubernetes 开源已经多年时间，网上有大量的学习资料，业界出版过很多&lt;a href=&#34;https://jimmysong.io/cloud-native/note/books/&#34; title=&#34;书籍&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;书籍&lt;/a&gt;
，建议大家通过阅读&lt;a href=&#34;https://kubernetes.io&#34; title=&#34;官方文档&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;官方文档&lt;/a&gt;
和实践来学习，也可以参考我编写的&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34; title=&#34;Kubernetes Handbook——Kubernetes 中文指南 / 云原生架构实践手册&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes Handbook——Kubernetes 中文指南 / 云原生架构实践手册&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;推荐大家加入我发起创办的&lt;a href=&#34;https://cloudnative.to&#34; title=&#34;云原生社区&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;云原生社区&lt;/a&gt;
，这是一个立足中国，放眼世界的云原生终端用户社区，致力于云原生技术的传播和应用。云原生社区主办的&lt;a href=&#34;https://github.com/cloudnativeto/academy&#34; title=&#34;云原生学院&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;云原生学院&lt;/a&gt;
定期邀请云原生和开源领域的大咖在 B 站上进行直播分享，成员自发组织了多个 SIG（特别兴趣小组）进行讨论学习。欢迎加入我们，共同学习和交流云原生技术。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>云原生应用之路</title>
      <link>https://jimmysong.io/blog/from-kubernetes-to-cloud-native/</link>
      <pubDate>Wed, 20 Dec 2017 15:08:02 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/from-kubernetes-to-cloud-native/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;从Kubernetes到Cloud Native——云原生应用之路&lt;/strong&gt;，这是我最近在 &lt;a href=&#34;http://bj2017.archsummit.com/presentation/306&#34; title=&#34;ArchSummit2017北京站&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ArchSummit2017北京站&lt;/a&gt;
 和 &lt;a href=&#34;https://www.kubernetes.org.cn/3211.html&#34; title=&#34;数人云&amp;amp;amp;TalkingData合办的Service Mesh is comming meetup&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;数人云&amp;amp;TalkingData合办的Service Mesh is comming meetup&lt;/a&gt;
 中分享的话题。&lt;/p&gt;
&lt;p&gt;本文简要介绍了容器技术发展的路径，为何Kubernetes的出现是容器技术发展到这一步的必然选择，而为何Kuberentes又将成为云原生应用的基石。&lt;/p&gt;
&lt;p&gt;我的分享按照这样的主线展开：容器-&amp;gt;Kubernetes-&amp;gt;微服务-&amp;gt;Cloud Native（云原生）-&amp;gt;Service Mesh（服务网格）-&amp;gt;使用场景-&amp;gt;Open Source（开源）。&lt;/p&gt;
&lt;h2 id=&#34;容器&#34;&gt;容器&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;容器——Cloud Native的基石&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;容器最初是通过开发者工具而流行，可以使用它来做隔离的开发测试环境和持续集成环境，这些都是因为容器轻量级，易于配置和使用带来的优势，docker和docker-compose这样的工具极大的方便的了应用开发环境的搭建，开发者就像是化学家一样在其中小心翼翼的进行各种调试和开发。&lt;/p&gt;
&lt;p&gt;随着容器的在开发者中的普及，已经大家对CI流程的熟悉，容器周边的各种工具蓬勃发展，俨然形成了一个小生态，在2016年达到顶峰，下面这张是我画的容器生态图：&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/container-ecosystem.png&#34; alt=&#34;容器生态图 Container ecosystem&#34; data-caption=&#34;容器生态图 Container ecosystem&#34;&gt;
  
  &lt;figcaption&gt;容器生态图 Container ecosystem&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;该生态涵盖了容器应用中从镜像仓库、服务编排、安全管理、持续集成与发布、存储和网络管理等各个方面，随着在单主机中运行容器的成熟，集群管理和容器编排成为容器技术亟待解决的问题。譬如化学家在实验室中研究出来的新产品，如何推向市场，进行大规模生产，成了新的议题。&lt;/p&gt;
&lt;h2 id=&#34;为什么使用kubernetes&#34;&gt;为什么使用Kubernetes&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Kubernetes——让容器应用进入大规模工业生产。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kubernetes是容器编排系统的事实标准&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在单机上运行容器，无法发挥它的最大效能，只有形成集群，才能最大程度发挥容器的良好隔离、资源分配与编排管理的优势，而对于容器的编排管理，Swarm、Mesos和Kubernetes的大战已经基本宣告结束，kubernetes成为了无可争议的赢家。&lt;/p&gt;
&lt;p&gt;下面这张图是Kubernetes的架构图（图片来自网络），其中显示了组件之间交互的接口CNI、CRI、OCI等，这些将Kubernetes与某款具体产品解耦，给用户最大的定制程度，使得Kubernetes有机会成为跨云的真正的云原生应用的操作系统。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/kubernetes-high-level-component-archtecture.jpg&#34; alt=&#34;Kuberentes架构&#34; data-caption=&#34;Kuberentes架构&#34;&gt;
  
  &lt;figcaption&gt;Kuberentes架构&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;随着Kubernetes的日趋成熟，“Kubernetes is becoming boring”，基于该“操作系统”之上构建的适用于不同场景的应用将成为新的发展方向，就像我们将石油开采出来后，提炼出汽油、柴油、沥青等等，所有的材料都将找到自己的用途，Kubernetes也是，毕竟我们谁也不是为了部署和管理容器而用Kubernetes，承载其上的应用才是价值之所在。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;云原生的核心目标&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/cloud-native-core-target.jpg&#34; alt=&#34;Cloud Native Core target&#34; data-caption=&#34;Cloud Native Core target&#34;&gt;
  
  &lt;figcaption&gt;Cloud Native Core target&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;云已经可以为我们提供稳定可以唾手可得的基础设施，但是业务上云成了一个难题，Kubernetes的出现与其说是从最初的容器编排解决方案，倒不如说是为了解决应用上云（即云原生应用）这个难题。&lt;/p&gt;
&lt;p&gt;包括微服务和FaaS/Serverless架构，都可以作为云原生应用的架构。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/redpoint-faas-landscape.jpg&#34; alt=&#34;FaaS Landscape&#34; data-caption=&#34;FaaS Landscape&#34;&gt;
  
  &lt;figcaption&gt;FaaS Landscape&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;但就2017年为止，kubernetes的主要使用场景也主要作为应用开发测试环境、CI/CD和运行Web应用这几个领域，如下图&lt;a href=&#34;http://thenewstack.io&#34; title=&#34;TheNewStack&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TheNewStack&lt;/a&gt;
的Kubernetes生态状况调查报告所示。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5mxr6fxtj31kw11q484.jpg&#34; alt=&#34;Workloads running on Kubernetes&#34; data-caption=&#34;Workloads running on Kubernetes&#34;&gt;
  
  &lt;figcaption&gt;Workloads running on Kubernetes&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;另外基于Kubernetes的构建PaaS平台和Serverless也处于爆发的准备的阶段，如下图中Gartner的报告中所示：&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5my2jtxzj315o0z8dkr.jpg&#34; alt=&#34;Gartner技术爆发趋势图2017&#34; data-caption=&#34;Gartner技术爆发趋势图2017&#34;&gt;
  
  &lt;figcaption&gt;Gartner技术爆发趋势图2017&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;当前各大公有云如Google GKE、微软Azure ACS、亚马逊EKS（2018年上线）、VmWare、Pivotal、腾讯云、阿里云等都提供了Kuberentes服务。&lt;/p&gt;
&lt;h2 id=&#34;微服务&#34;&gt;微服务&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;微服务——Cloud Native的应用架构。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;下图是&lt;a href=&#34;https://developers.redhat.com/blog/author/bibryam/&#34; title=&#34;Bilgin Ibryam&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bilgin Ibryam&lt;/a&gt;
给出的微服务中应该关心的主题，图片来自&lt;a href=&#34;https://developers.redhat.com/blog/2016/12/09/spring-cloud-for-microservices-compared-to-kubernetes/&#34; title=&#34;RedHat Developers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RedHat Developers&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/microservices-concerns.jpg&#34; alt=&#34;Microservices concerns&#34; data-caption=&#34;Microservices concerns&#34;&gt;
  
  &lt;figcaption&gt;Microservices concerns&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;微服务带给我们很多开发和部署上的灵活性和技术多样性，但是也增加了服务调用的开销、分布式系统管理、调试与服务治理方面的难题。&lt;/p&gt;
&lt;p&gt;当前最成熟最完整的微服务框架可以说非&lt;a href=&#34;https://spring.io/&#34; title=&#34;Spring&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spring&lt;/a&gt;
莫属，而Spring又仅限于Java语言开发，其架构本身又跟Kubernetes存在很多重合的部分，如何探索将Kubernetes作为微服务架构平台就成为一个热点话题。&lt;/p&gt;
&lt;p&gt;就拿微服务中最基础的&lt;strong&gt;服务注册发现&lt;/strong&gt;功能来说，其方式分为&lt;strong&gt;客户端服务发现&lt;/strong&gt;和&lt;strong&gt;服务端服务发现&lt;/strong&gt;两种，Java应用中常用的方式是使用Eureka和Ribbon做服务注册发现和负载均衡，这属于客户端服务发现，而在Kubernetes中则可以使用DNS、Service和Ingress来实现，不需要修改应用代码，直接从网络层面来实现。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/service-discovery-in-microservices.png&#34; alt=&#34;两种服务发现方式&#34; data-caption=&#34;两种服务发现方式&#34;&gt;
  
  &lt;figcaption&gt;两种服务发现方式&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;h2 id=&#34;cloud-native&#34;&gt;Cloud Native&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;DevOps——通向云原生的云梯&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;CNCF（云原生计算基金会）给出了云原生应用的三大特征：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;容器化包装&lt;/strong&gt;：软件应用的进程应该包装在容器中独立运行。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动态管理&lt;/strong&gt;：通过集中式的编排调度系统来动态的管理和调度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;微服务化&lt;/strong&gt;：明确服务间的依赖，互相解耦。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图是我整理的关于云原生所需要的能力和特征。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/cloud-native-architecutre-mindnode.jpg&#34; alt=&#34;Cloud Native Features&#34; data-caption=&#34;Cloud Native Features&#34;&gt;
  
  &lt;figcaption&gt;Cloud Native Features&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cncf.io&#34; title=&#34;CNCF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CNCF&lt;/a&gt;
所托管的应用（目前已达12个），即朝着这个目标发展，其公布的&lt;a href=&#34;https://github.com/cncf/landscape&#34; title=&#34;Cloud Native Landscape&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Native Landscape&lt;/a&gt;
，给出了云原生生态的参考体系。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5myp6ednj31kw0w0u0x.jpg&#34; alt=&#34;Cloud Native Landscape v1.0&#34; data-caption=&#34;Cloud Native Landscape v1.0&#34;&gt;
  
  &lt;figcaption&gt;Cloud Native Landscape v1.0&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;使用Kubernetes构建云原生应用&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们都是知道Heroku推出了适用于PaaS的&lt;a href=&#34;https://12factor.net/&#34; title=&#34;12 factor app&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;12 factor app&lt;/a&gt;
的规范，包括如下要素：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基准代码&lt;/li&gt;
&lt;li&gt;依赖管理&lt;/li&gt;
&lt;li&gt;配置&lt;/li&gt;
&lt;li&gt;后端服务&lt;/li&gt;
&lt;li&gt;构建，发布，运行&lt;/li&gt;
&lt;li&gt;无状态进程&lt;/li&gt;
&lt;li&gt;端口绑定&lt;/li&gt;
&lt;li&gt;并发&lt;/li&gt;
&lt;li&gt;易处理&lt;/li&gt;
&lt;li&gt;开发环境与线上环境等价&lt;/li&gt;
&lt;li&gt;日志作为事件流&lt;/li&gt;
&lt;li&gt;管理进程&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;另外还有补充的三点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;API声明管理&lt;/li&gt;
&lt;li&gt;认证和授权&lt;/li&gt;
&lt;li&gt;监控与告警&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果落实的具体的工具，请看下图，使用Kubernetes构建云原生架构：&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/building-cloud-native-architecture-with-kubernetes.png&#34; alt=&#34;Building a Cloud Native Architecture with Kubernetes followed 12 factor app&#34; data-caption=&#34;Building a Cloud Native Architecture with Kubernetes followed 12 factor app&#34;&gt;
  
  &lt;figcaption&gt;Building a Cloud Native Architecture with Kubernetes followed 12 factor app&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;结合这12因素对开发或者改造后的应用适合部署到Kubernetes之上，基本流程如下图所示：&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/creating-kubernetes-native-app.jpg&#34; alt=&#34;Creating Kubernetes native app&#34; data-caption=&#34;Creating Kubernetes native app&#34;&gt;
  
  &lt;figcaption&gt;Creating Kubernetes native app&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;迁移到云架构&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;迁移到云端架构，相对单体架构来说会带来很多挑战。比如自动的持续集成与发布、服务监控的变革、服务暴露、权限的管控等。这些具体细节请参考&lt;strong&gt;Kubernetes-handbook&lt;/strong&gt;中的说明：&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34; title=&#34;https://jimmysong.io/kubernetes-handbook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://jimmysong.io/kubernetes-handbook&lt;/a&gt;
，在此就不细节展开，另外推荐一本我翻译的由Pivotal出品的电子书——&lt;a href=&#34;https://content.pivotal.io/ebooks/migrating-to-cloud-native-application-architectures&#34; title=&#34;Migrating to Cloud Native Application Architectures&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Migrating to Cloud Native Application Architectures&lt;/a&gt;
，地址：&lt;a href=&#34;https://jimmysong.io/migrating-to-cloud-native-application-architectures/&#34; title=&#34;https://jimmysong.io/migrating-to-cloud-native-application-architectures/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://jimmysong.io/migrating-to-cloud-native-application-architectures/&lt;/a&gt;
。&lt;/p&gt;
&lt;h2 id=&#34;service-mesh&#34;&gt;Service Mesh&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Services for show, meshes for a pro.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Kubernetes中的应用将作为微服务运行，但是Kuberentes本身并没有给出微服务治理的解决方案，比如服务的限流、熔断、良好的灰度发布支持等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Service mesh可以用来做什么&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Traffic Management：API网关&lt;/li&gt;
&lt;li&gt;Observability：服务调用和性能分析&lt;/li&gt;
&lt;li&gt;Policy Enforcement：控制服务访问策略&lt;/li&gt;
&lt;li&gt;Service Identity and Security：安全保护&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Service mesh的特点&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;专用的基础设施层&lt;/li&gt;
&lt;li&gt;轻量级高性能网络代理&lt;/li&gt;
&lt;li&gt;提供安全的、快速的、可靠地服务间通讯&lt;/li&gt;
&lt;li&gt;扩展kubernetes的应用负载均衡机制，实现灰度发布&lt;/li&gt;
&lt;li&gt;完全解耦于应用，应用可以无感知，加速应用的微服务和云原生转型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用Service Mesh将可以有效的治理Kuberentes中运行的服务，当前开源的Service Mesh有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linkderd：&lt;a href=&#34;https://linkerd.io&#34; title=&#34;https://linkerd.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://linkerd.io&lt;/a&gt;
，由最早提出Service Mesh的公司&lt;a href=&#34;https://buoyant.io&#34; title=&#34;Buoyant&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Buoyant&lt;/a&gt;
开源，创始人来自Twitter&lt;/li&gt;
&lt;li&gt;Envoy：&lt;a href=&#34;https://www.envoyproxy.io/&#34; title=&#34;https://www.envoyproxy.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.envoyproxy.io/&lt;/a&gt;
，Lyft开源的，可以在Istio中使用Sidecar模式运行&lt;/li&gt;
&lt;li&gt;Istio：&lt;a href=&#34;https://istio.io&#34; title=&#34;https://istio.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://istio.io&lt;/a&gt;
，由Google、IBM、Lyft联合开发并开源&lt;/li&gt;
&lt;li&gt;Conduit：&lt;a href=&#34;https://conduit.io&#34; title=&#34;https://conduit.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://conduit.io&lt;/a&gt;
，同样由Buoyant开源的轻量级的基于Kubernetes的Service Mesh&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外还有很多其它的Service Mesh鱼贯而出，请参考&lt;a href=&#34;https://jimmysong.io/awesome-cloud-native&#34; title=&#34;awesome-cloud-native&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;awesome-cloud-native&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Istio VS Linkerd&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Linkerd和Istio是最早开源的Service Mesh，它们都支持Kubernetes，下面是它们之间的一些特性对比。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Feature&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Istio&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Linkerd&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;部署架构&lt;/td&gt;
&lt;td&gt;Envoy/Sidecar&lt;/td&gt;
&lt;td&gt;DaemonSets&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;易用性&lt;/td&gt;
&lt;td&gt;复杂&lt;/td&gt;
&lt;td&gt;简单&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;支持平台&lt;/td&gt;
&lt;td&gt;kuberentes&lt;/td&gt;
&lt;td&gt;kubernetes/mesos/Istio/local&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;当前版本&lt;/td&gt;
&lt;td&gt;0.3.0&lt;/td&gt;
&lt;td&gt;1.3.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;是否已有生产部署&lt;/td&gt;
&lt;td&gt;否&lt;/td&gt;
&lt;td&gt;是&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;关于两者的架构可以参考各自的官方文档，我只从其在kubernetes上的部署结构来说明其区别。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/istio-vs-linkerd.jpg&#34; alt=&#34;istio vs linkerd&#34; data-caption=&#34;istio vs linkerd&#34;&gt;
  
  &lt;figcaption&gt;istio vs linkerd&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;Istio的组件复杂，可以分别部署的kubernetes集群中，但是作为核心路由组件&lt;strong&gt;Envoy&lt;/strong&gt;是以&lt;strong&gt;Sidecar&lt;/strong&gt;形式与应用运行在同一个Pod中的，所有进入该Pod中的流量都需要先经过Envoy。&lt;/p&gt;
&lt;p&gt;Linker的部署十分简单，本身就是一个镜像，使用Kubernetes的&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/concepts/daemonset.html&#34; title=&#34;DaemonSet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DaemonSet&lt;/a&gt;
方式在每个node节点上运行。&lt;/p&gt;
&lt;p&gt;更多信息请参考&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34; title=&#34;kubernetes-handbook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubernetes-handbook&lt;/a&gt;
。&lt;/p&gt;
&lt;h2 id=&#34;使用场景&#34;&gt;使用场景&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Cloud Native的大规模工业生产&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;GitOps&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;给开发者带来最大配置和上线的灵活性，践行DevOps流程，改善研发效率，下图这样的情况将更少发生。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5mzj8rj6j318g1ewtfc.jpg&#34; alt=&#34;Deployment pipeline&#34; data-caption=&#34;Deployment pipeline&#34;&gt;
  
  &lt;figcaption&gt;Deployment pipeline&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;我们知道Kubernetes中的所有应用的部署都是基于YAML文件的，这实际上就是一种&lt;strong&gt;Infrastructure as code&lt;/strong&gt;，完全可以通过Git来管控基础设施和部署环境的变更。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Big Data&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Spark现在已经非官方支持了基于Kuberentes的原生调度，其具有以下特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes原生调度：与yarn、mesos同级&lt;/li&gt;
&lt;li&gt;资源隔离，粒度更细：以namespace来划分用户&lt;/li&gt;
&lt;li&gt;监控的变革：单次任务资源计量&lt;/li&gt;
&lt;li&gt;日志的变革：pod的日志收集&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Feature&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Yarn&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;queue&lt;/td&gt;
&lt;td&gt;queue&lt;/td&gt;
&lt;td&gt;namespace&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;instance&lt;/td&gt;
&lt;td&gt;ExcutorContainer&lt;/td&gt;
&lt;td&gt;Executor Pod&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;td&gt;host&lt;/td&gt;
&lt;td&gt;plugin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;heterogeneous&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;security&lt;/td&gt;
&lt;td&gt;RBAC&lt;/td&gt;
&lt;td&gt;ACL&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;下图是在Kubernetes上运行三种调度方式的spark的单个节点的应用部分对比：&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/spark-on-kubernetes-with-different-schedulers.jpg&#34; alt=&#34;Spark on Kubernetes with different schedulers&#34; data-caption=&#34;Spark on Kubernetes with different schedulers&#34;&gt;
  
  &lt;figcaption&gt;Spark on Kubernetes with different schedulers&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;从上图中可以看到在Kubernetes上使用YARN调度、standalone调度和kubernetes原生调度的方式，每个node节点上的Pod内的spark Executor分布，毫无疑问，使用kubernetes原生调度的spark任务才是最节省资源的。&lt;/p&gt;
&lt;p&gt;提交任务的语句看起来会像是这样的：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;./spark-submit &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --deploy-mode cluster &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --class com.talkingdata.alluxio.hadooptest &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --master k8s://https://172.20.0.113:6443 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --kubernetes-namespace spark-cluster &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.driverEnv.SPARK_USER&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;hadoop &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.driverEnv.HADOOP_USER_NAME&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;hadoop &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executorEnv.HADOOP_USER_NAME&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;hadoop &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executorEnv.SPARK_USER&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;hadoop &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.authenticate.driver.serviceAccountName&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.driver.memory&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;100G &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executor.memory&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;10G &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.driver.cores&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;30&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executor.cores&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.driver.maxResultSize&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;10240m &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.driver.limit.cores&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;32&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.executor.limit.cores&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.executor.memoryOverhead&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;2g &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executor.instances&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;5&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.app.name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark-pi &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.driver.docker.image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark-driver:v2.1.0-kubernetes-0.3.1-1 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.executor.docker.image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark-executor:v2.1.0-kubernetes-0.3.1-1 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.initcontainer.docker.image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark-init:v2.1.0-kubernetes-0.3.1-1 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.resourceStagingServer.uri&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;http://172.20.0.114:31000 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;~/Downloads/tendcloud_2.10-1.0.jar
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;关于支持Kubernetes原生调度的Spark请参考：https://jimmysong.io/spark-on-k8s/&lt;/p&gt;
&lt;h2 id=&#34;open-source&#34;&gt;Open Source&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Contributing is Not only about code, it is about helping a community.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;下图是我们刚调研准备使用Kubernetes时候的调研方案选择。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5mzywc83j31fk1i8qg4.jpg&#34; alt=&#34;Kubernetes solutions&#34; data-caption=&#34;Kubernetes solutions&#34;&gt;
  
  &lt;figcaption&gt;Kubernetes solutions&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;对于一个初次接触Kubernetes的人来说，看到这样一个庞大的架构选型时会望而生畏，但是Kubernetes的开源社区帮助了我们很多。&lt;/p&gt;
&lt;p&gt;
  &lt;figure class=&#34;mx-auto text-center&#34;&gt;
  &lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/kubernetes-sigs.jpg&#34; alt=&#34;Kubernetes SIG&#34; data-caption=&#34;Kubernetes SIG&#34;&gt;
  
  &lt;figcaption&gt;Kubernetes SIG&lt;/figcaption&gt;
  
  &lt;/figure&gt;

&lt;/p&gt;
&lt;p&gt;我组建了&lt;strong&gt;K8S&amp;amp;Cloud Native实战&lt;/strong&gt;微信群，参与了k8smeetup、KEUC2017、Kubernetes 官方文档的翻译工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;有用的资料和链接&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to&#34; title=&#34;云原生社区&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;云原生社区&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34; title=&#34;Kubernetes Handbook - Kubernetes 和云原生应用架构实践手册&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes Handbook - Kubernetes 和云原生应用架构实践手册&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/awesome-cloud-native/&#34; title=&#34;Cloud native开源生态&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud native开源生态&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/book/migrating-to-cloud-native-application-architectures/&#34; title=&#34;迁移到云原生应用架构|电子书&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;迁移到云原生应用架构|电子书&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
