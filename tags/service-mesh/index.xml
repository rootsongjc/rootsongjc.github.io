<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jimmy Song - 云原生|开源|社区 – service mesh</title>
    <link>https://jimmysong.io/tags/service-mesh/</link>
    <description>Recent content in service mesh on Jimmy Song - 云原生|开源|社区</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <copyright>Copyright &amp;copy; 2021 Jimmy Song 保留所有权利；&lt;/br&gt;基于 Hugo [educenter](https://github.com/themefisher/educenter-hugo)  主题构建</copyright>
    <lastBuildDate>Wed, 24 Nov 2021 14:43:27 +0800</lastBuildDate>
    
	  <atom:link href="https://jimmysong.io/tags/service-mesh/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>网易开源 Istio 扩展项目 Slime 简介——基于 Istio 的智能服务网格管理器</title>
      <link>https://jimmysong.io/blog/slime-intro/</link>
      <pubDate>Wed, 24 Nov 2021 14:43:27 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/slime-intro/</guid>
      <description>
        
        
        &lt;p&gt;最近我在研究 Istio 生态中的开源项目，&lt;a href=&#34;https://github.com/slime-io/slime/&#34;&gt;Slime&lt;/a&gt; 这个项目开源与 2021 年初，是由网易数帆微服务团队开源的一款基于 Istio 的智能网格管理器。Slime 基于 Kubernetes Operator 实现，可作为 Istio 的 CRD 管理器，无须对 Istio 做任何定制化改造，就可以定义动态的服务治理策略，从而达到自动便捷使用 Istio 和 Envoy 高阶功能的目的。&lt;/p&gt;
&lt;h2 id=&#34;slime-试图解决的问题&#34;&gt;Slime 试图解决的问题&lt;/h2&gt;
&lt;p&gt;Slime 项目的诞生主要为了解决以下问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;网格内所有服务配置全量下到所有 Sidecar Proxy，导致其消耗大量资源使得应用性能变差的问题&lt;/li&gt;
&lt;li&gt;如何在 Istio 中实现高阶扩展的问题：比如扩展 HTTP 插件；根据服务的资源使用率做到自适应限流&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Slime 解决以上问题的答案是构建 Istio 的控制平面，具体做法是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;构建可拔插控制器&lt;/li&gt;
&lt;li&gt;数据平面监控&lt;/li&gt;
&lt;li&gt;CRD 转换&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过以上方式 Slime 可以实现&lt;strong&gt;配置懒加载&lt;/strong&gt;和&lt;strong&gt;插件管理器&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;slime-架构&#34;&gt;Slime 架构&lt;/h2&gt;
&lt;p&gt;Slime 内部分为三大模块，其架构图如下所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;slime-internal-arch.jpg&#34; alt=&#34;Slime 内部架构图&#34;&gt;&lt;/p&gt;
&lt;p&gt;Slime 内部三大组件为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;slime-boot&lt;/code&gt;：在 Kubernetes 上部署 Slime 模块的 operator。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;slime-controller&lt;/code&gt;：Slime 的核心组件，监听 Slime CRD 并将其转换为Istio CRD。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;slime-metric&lt;/code&gt;：用于获取服务 metrics 信息的组件，&lt;code&gt;slime-controller&lt;/code&gt; 会根据其获取的信息动态调整服务治理规则。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;目前 Slime 内置了三个控制器子模块：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;配置懒加载（按需加载）&lt;/strong&gt;：用户无须手动配置 &lt;code&gt;SidecarScope&lt;/code&gt;，Istio 可以按需加载服务配置和服务发现信息；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HTTP 插件管理&lt;/strong&gt;：使用新的 CRD——&lt;code&gt;pluginmanager/envoyplugin&lt;/code&gt; 包装了可读性，摒弃了可维护性较差的 &lt;code&gt;envoyfilter&lt;/code&gt;，使得插件扩展更为便捷；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自适应限流&lt;/strong&gt;：结合监控信息自动调整限流策略；&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;什么是 SidecarScope？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;SidecarScope 是在 Istio 1.1 版本中引入的，它并不是一个直接面向用户的配置项，而是 Sidecar 资源的包装器，具体来说就是 &lt;a href=&#34;../config/networking/sidecar.md&#34;&gt;Sidecar 资源&lt;/a&gt;中的 &lt;code&gt;egress&lt;/code&gt; 选项。通过该配置可以减少 Istio 向 Sidecar 下发的数据量，例如只向某个命名空间中的某些服务下发某些 hosts 的访问配置，从而提高应用提高性能。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;使用-slime-作为-istio-的控制平面&#34;&gt;使用 Slime 作为 Istio 的控制平面&lt;/h2&gt;
&lt;p&gt;为了解决这些问题，Slime 在 Istio 之上构建了更高层次的抽象，相当于为 Istio 构建了一层管理平面，其工作流程图如下所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;slime-flow-chart.jpg&#34; alt=&#34;Slime 工作流程图&#34;&gt;&lt;/p&gt;
&lt;p&gt;具体步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Slime Operator 根据管理员的配置在 Kubernetes 中完成 Slime 组件的初始化；&lt;/li&gt;
&lt;li&gt;开发者创建符合 Slime CRD 规范的配置并应用到 Kubernetes 集群中；&lt;/li&gt;
&lt;li&gt;Slime 查询 Prometheus 中保存的相关服务的监控数据，结合 Slime CRD 中自适应部分的配置，将 Slime CRD 转换为 Istio CRD，同时将其推送到 Global Proxy 中；&lt;/li&gt;
&lt;li&gt;Istio 监听 Istio CRD 的创建；&lt;/li&gt;
&lt;li&gt;Istio 将 Sidecar Proxy 的配置信息推送到数据平面相应的 Sidecar Proxy 中；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;以上只是一个对 Slime 工作流程的一个笼统的介绍，更多详细信息请参考 &lt;a href=&#34;https://github.com/slime-io/slime/&#34;&gt;Slime GitHub&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;配置懒加载&#34;&gt;配置懒加载&lt;/h2&gt;
&lt;p&gt;为了解决数据平面中 Sidecar Proxy 资源消耗过大及网络延迟问题，Slime 使用了配置懒加载（按需加载 Sidecar 配置）的方案。该方案的核心思想是向每个 Sidecar Proxy 中只下发其所 Pod 中服务所需的配置，而不是将网格中的所有服务信息全量下发。所以 Slime 需要获取每个服务的调用关系这样才能得到其所需的 Sidecar Proxy 配置。&lt;/p&gt;
&lt;p&gt;Slime 实现 Sidecar Proxy 配置懒加载的方法是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;让数据平面中的所有服务的首次调用都通过一个 Global Proxy，该 Proxy 可以记录所有服务的调用和依赖信息，根据该依赖信息更新 Istio 中 Sidecar 资源的配置；&lt;/li&gt;
&lt;li&gt;当某个服务的调用链被 VirtualService 中的路由信息重新定义时， Global Proxy 原有记录就失效了，需要一个新的数据结构来维护该服务的调用关系。Slime 创建了名为 &lt;code&gt;ServiceFence&lt;/code&gt;  的 CRD 来维护服务调用关系以解决服务信息缺失问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;使用-global-proxy-初始化服务调用拓扑&#34;&gt;使用 Global Proxy 初始化服务调用拓扑&lt;/h3&gt;
&lt;p&gt;Slime 在数据平面中部署 Global Proxy（也叫做 Global Sidecar，但其与应用的 Pod 不是一对一的关系，笔者更倾向于称其为 Global Proxy），该代理同样使用 Envoy 构建，在每个需要启动配置懒加载的命名空间中部署一个或在整个网格中只部署一个，所有缺失服务发现信息的调用（你也可以手动配置服务调用关系），都会被兜底路由劫持到 Global Proxy，经过其首次转发后，Slime 便可感知到被调用方的信息，然后根据其对应服务的 VirtualService，找到服务名和真实后端的映射关系，将两者的都加入 SidecarScope，以后该服务的调用就不再需要经过 Global Proxy 了。&lt;/p&gt;
&lt;h3 id=&#34;使用-servicefence-维护服务调用拓扑&#34;&gt;使用 ServiceFence 维护服务调用拓扑&lt;/h3&gt;
&lt;p&gt;在使用 Global Proxy 初始化服务调用拓扑后，一旦服务调用链有变动的话怎么办？对此 Slime 创建了 ServiceFence 的 CRD。使用 ServiceFence 可以维护服务名和后端服务的映射关系。Slime 根据其对应服务的 VirtualService，找到 Kubernetes 服务名和真实后端（host）的映射关系，将两者的都加入 Sidecar 的配置中。ServiceFence 管理生成的 SidecarScope 的生命周期，自动清理长时间不用的调用关系，从而避免上述问题。&lt;/p&gt;
&lt;h3 id=&#34;如何开启配置懒加载&#34;&gt;如何开启配置懒加载&lt;/h3&gt;
&lt;p&gt;配置懒加载功能对于终端用户是透明的，只需要 Kubernetes  Service 上打上 &lt;code&gt;istio.dependency.servicefence/status:&amp;quot;true&amp;quot;&lt;/code&gt; 的标签，表明该服务需要开启配置懒加载，剩下的事情交给 Slime Operator 来完成即可。&lt;/p&gt;
&lt;h2 id=&#34;http-插件管理&#34;&gt;HTTP 插件管理&lt;/h2&gt;
&lt;p&gt;Istio 中的插件扩展只能通过 EnvoyFilter 来实现，因为它是 xDS 层面的配置，管理和维护这样的配置需要耗费大量的精力，也极容易出错。因此，Slime 在 EnvoyFilter 的基础上做了一层面向插件的抽象。&lt;/p&gt;
&lt;p&gt;Slime 共有两个 CRD 用于 HTTP 插件管理，分别是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PluginManager&lt;/strong&gt;：配置为哪些负载开启哪些插件，插件的配置顺序即为执行顺序；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EnvoyPlugin&lt;/strong&gt;：EnvoyPlugin 不关心每个插件的具体配置，具体配置会被放在 EnvoyFilter 资源的 &lt;code&gt;patch.typed_config&lt;/code&gt; 结构中透传），EnvoyPlugin 的核心思想是将插件配置在需要的维度中做聚合，从而限定插件的生鲜范围。这样做一方面更加贴合插件使用者的习惯，另一方面也降低了上层配置的冗余，&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;关于 Slime 中插件管理的详细使用方式请见 &lt;a href=&#34;https://github.com/slime-io/slime/blob/master/doc/zh/plugin_manager.md&#34;&gt;Slime GitHub&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;自适应限流&#34;&gt;自适应限流&lt;/h2&gt;
&lt;p&gt;Envoy 内置的限流组件功能单一，只能以实例维度配置限流值，无法做到根据应用负载的自适应限流。Slime 通过与 Prometheus metric server 对接，实时的获取监控情况，来动态配置限流值。&lt;/p&gt;
&lt;p&gt;Slime 自适应限流的流程图如下所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;slime-smart-limiter.jpg&#34; alt=&#34;Slime 的自适应限流流程图&#34;&gt;&lt;/p&gt;
&lt;p&gt;Slime 的自适应限流的流程分为两部分，一部分为 SmartLimiter 到 EnvoyFilter 的转换，另一部分为获取监控数据。目前 Slime 支持从 Kubernetes Metric Server 获取服务的CPU、内存、副本数等数据。Slime 还对外提供了一套监控数据接口（Metric Discovery Server），通过 MDS，可以将自定义的监控指标同步给限流组件。&lt;/p&gt;
&lt;p&gt;Slime 创建的 CRD &lt;code&gt;SmartLimiter&lt;/code&gt; 用于配置自适应限流。其的配置是接近自然语义，例如希望在 CPU 超过 80% 时触发服务 A 的访问限制，限额为 30QPS，对应的SmartLimiter 定义如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;microservice.netease.com/v1alpha1&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;kind&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;SmartLimiter&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;metadata&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;name&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;a&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;namespace&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;default&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;spec&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;descriptors&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;-&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;action&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;fill_interval&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;seconds&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;quota&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;30/{pod}&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# 30为该服务的额度，将其均分给每个 pod，加入有 3 个 pod，则每个 pod 的限流为 10&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;condition&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{cpu}&amp;gt;0.8&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# 根据监控项{cpu}的值自动填充该模板&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;更多&#34;&gt;更多&lt;/h2&gt;
&lt;p&gt;Slime 开源于 2021 年初，本文发稿时该项目仍处于初级阶段，本文大量参考了杨笛航在云原生社区中的分享 &lt;a href=&#34;https://cloudnative.to/blog/netease-slime/&#34;&gt;Slime：让 Istio 服务网格变得更加高效与智能&lt;/a&gt; 及 Slime 的 &lt;a href=&#34;https://github.com/slime-io/slime&#34;&gt;GitHub&lt;/a&gt;。感兴趣的读者可以关注下这个项目的 GitHub，进一步了解它。&lt;/p&gt;
&lt;p&gt;另外欢迎关注服务网格和 Istio 的朋友加入&lt;a href=&#34;https://cloudnative.to/sig-istio/&#34;&gt;云原生社区 Istio SIG&lt;/a&gt;，一起参与讨论和交流。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/netease-slime/&#34;&gt;Slime：让 Istio 服务网格变得更加高效与智能 - cloudnative.to&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/slime-io/slime/blob/master/README_ZH.md&#34;&gt;Slime GitHub 文档 - github.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://istio.io/latest/docs/reference/config/networking/sidecar/&#34;&gt;Sidecar - istio.io&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>如何理解 Istio Ingress， 它与 API Gateway 有什么区别？</title>
      <link>https://jimmysong.io/blog/istio-servicemesh-api-gateway/</link>
      <pubDate>Fri, 06 Aug 2021 10:22:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/istio-servicemesh-api-gateway/</guid>
      <description>
        
        
        &lt;p&gt;API 网关作为客户端访问后端的入口，已经存在很长时间了，它主要是用来管理”南北向“的流量；近几年服务网格开始流行，它主要是管理系统内部，即“东西向”流量，而像 Istio 这样的服务网格还内置了网关，从而将系统内外部的流量纳入了统一管控。这经常给初次接触 Istio 的人带来困惑——服务网格与 API 网关之间是什么关系？是不是使用了 Istio 就可以替代了 API 网关？Istio 的 API 网关是如何运作的？有哪些方式暴露 Istio mesh 中的服务？这篇文章给为你解答。&lt;/p&gt;
&lt;h2 id=&#34;主要观点&#34;&gt;主要观点&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;服务网格诞生的初衷是为了解决分布式应用的内部流量的管理问题，而在此之前 API 网关已存在很久了。&lt;/li&gt;
&lt;li&gt;虽然 Istio 中内置了Gateway，但是你仍可以使用自定义的 Ingress Controller 来代理外部流量。&lt;/li&gt;
&lt;li&gt;API 网关和服务网格正朝着融合的方向发展。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;如何暴露-istio-mesh-中的服务&#34;&gt;如何暴露 Istio mesh 中的服务？&lt;/h2&gt;
&lt;p&gt;下图展示了使用 Istio Gateway、Kubernetes Ingress、API Gateway 及 NodePort/LB 暴露 Istio mesh 中服务的四种方式。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;api-gateway-istio-service-mesh.jpg&#34; alt=&#34;暴露 Kubernetes 中服务的几种方式&#34;&gt;&lt;/p&gt;
&lt;p&gt;其中阴影表示的是 Istio mesh，mesh 中的的流量属于集群内部（东西向）流量，而客户端访问 Kubernetes 集群内服务的流量属于外部（南北向）流量。不过因为 Ingress、Gateway 也是部署在 Kubernetes 集群内的，这些节点访问集群内其他服务的流量就难以归属了。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;方式&lt;/th&gt;
&lt;th&gt;控制器&lt;/th&gt;
&lt;th&gt;功能&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;NodePort/LoadBalancer&lt;/td&gt;
&lt;td&gt;Kubernetes&lt;/td&gt;
&lt;td&gt;负载均衡&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kubernetes Ingress&lt;/td&gt;
&lt;td&gt;Ingress Controller&lt;/td&gt;
&lt;td&gt;负载均衡、TLS、虚拟主机、流量路由&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Istio Gateway&lt;/td&gt;
&lt;td&gt;Istio&lt;/td&gt;
&lt;td&gt;负载均衡、TLS、虚拟主机、高级流量路由、其他 Istio 的高级功能&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;API 网关&lt;/td&gt;
&lt;td&gt;API Gateway&lt;/td&gt;
&lt;td&gt;负载均衡、TLS、虚拟主机、流量路由、API 生命周期管理、权限认证、数据聚合、账单和速率限制&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;由于 NodePort/LoadBalancer 是 Kubernetes 内置的基本的暴露服务的方式，本文就不讨论这种方式了。下文将对其他三种方式分别作出说明。&lt;/p&gt;
&lt;h2 id=&#34;使用-kubernetes-ingress-暴露服务&#34;&gt;使用 Kubernetes Ingress 暴露服务&lt;/h2&gt;
&lt;p&gt;我们都知道 Kubernetes 集群的客户端是无法直接访问 Pod 的 IP 地址的，因为 Pod 是处于 Kubernetes 内置的一个网络平面中。我们可以将 Kubernetes 内的服务使用 NodePort 或者 LoadBlancer 的方式暴露到集群以外。同时为了支持虚拟主机、隐藏和节省 IP 地址，可以使用 &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/&#34;&gt;Ingress&lt;/a&gt; 来暴露 Kubernetes 中的服务。Kubernetes Ingress 原理如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;kubernetes-ingress.jpg&#34; alt=&#34;使用 Kubernetes Ingress 暴露服务&#34;&gt;&lt;/p&gt;
&lt;p&gt;简单的说，Ingress 就是从 Kubernetes 集群外访问集群的入口，将用户的 URL 请求转发到不同的服务上。Ingress 相当于 Nginx、Apache 等负载均衡方向代理服务器，其中还包括规则定义，即 URL 的路由信息，路由信息得的刷新由 &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-controllers&#34;&gt;Ingress controller&lt;/a&gt;来提供。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;networking.k8s.io/v1beta1&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;kind&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;Ingress&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;metadata&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;annotations&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;kubernetes.io/ingress.class&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;istio&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;name&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;ingress&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;spec&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;rules&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;-&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;host&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;httpbin.example.com&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;http&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;paths&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;-&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;path&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;/status/*&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;backend&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;serviceName&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;httpbin&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;servicePort&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8000&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;上面的例子中的 &lt;code&gt;kubernetes.io/ingress.class: istio&lt;/code&gt; 注解表明该 Ingress 使用的 Istio Ingress Controller。&lt;/p&gt;
&lt;h2 id=&#34;使用-istio-gateway-暴露服务&#34;&gt;使用 Istio Gateway 暴露服务&lt;/h2&gt;
&lt;p&gt;我们都知道 Istio 是继承 Kubernetes 之后发展出来的一个流行的服务网格实现，它实现了 Kubernetes 没有的一些功能，请参考&lt;a href=&#34;https://jimmysong.io/blog/what-is-istio-and-why-does-kubernetes-need-it/&#34;&gt;什么是 Istio？为什么 Kubernetes 需要 Istio？&lt;/a&gt;简要来说，正是因为 Istio 补足了 Kubernetes 对于云原生应用的流量管理、可观察性和安全方面的短板，使得流量管理变得对应用程序透明，使这部分功能从应用程序中转移到了平台层，成为了云原生基础设施。&lt;/p&gt;
&lt;p&gt;Istio 0.8 以前版本中使用 Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/&#34;&gt;Ingress&lt;/a&gt; 来作为流量入口，其中使用 Envoy 作为 Ingress Controller。在 Istio 0.8 及以后的版本中，Istio 创建了 Gateway 对象。Gateway 和 VirtualService 用于表示 Istio Ingress 的配置模型，Istio Ingress 的缺省实现则采用了和 sidecar 相同的 Envoy 代理。通过该方式，Istio 控制面用一致的配置模型同时控制了入口网关和内部的 sidecar 代理。这些配置包括路由规则，策略检查、遥测收集以及其他服务管控功能。&lt;/p&gt;
&lt;p&gt;Istio Gateway 的功能与 Kubernetes Ingress 类似，它负责进出集群的南北流量。Istio Gateway 描述了一个负载均衡器，用于承载进出服务网格边缘的连接。该规范描述了一组开放端口和这些端口所使用的协议，以及用于负载均衡的 SNI 配置等。&lt;/p&gt;
&lt;p&gt;Istio Gateway 资源本身只能配置L4到L6的功能，例如暴露的端口、TLS 设置等；但 Gateway 可与 VirtualService 绑定，在VirtualService 中可以配置七层路由规则，例如按比例和版本的流量路由，故障注入，HTTP 重定向，HTTP 重写等所有Mesh内部支持的路由规则。&lt;/p&gt;
&lt;p&gt;下面是一个 Gateway 与 VirtualService 绑定的示例。拥有 &lt;code&gt;istio: ingressgateway&lt;/code&gt; 标签的 pod 将作为 Ingress Gateway 并路由对 &lt;code&gt;httpbin.example.com&lt;/code&gt; 虚拟主机的 80 端口的 HTTP 访问，这相当于给 Kubernetes 敞开了一个外部访问的入口。这与使用 Kubernetes Ingress 最大的区别就是，需要我们手动将VirtualService与Gateway 绑定，并指定 Gateway 所在的 pod。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;networking.istio.io/v1alpha3&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;kind&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;Gateway&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;metadata&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;name&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;httpbin-gateway&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;spec&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;selector&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;istio&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;ingressgateway&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;servers&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;-&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;port&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;number&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;80&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;name&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;http&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;protocol&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;HTTP&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;hosts&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;-&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;httpbin.example.com&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;下面这个 VirtualService 通过 &lt;code&gt;gateways&lt;/code&gt; 与上面的网关绑定在了一起，以接受来自该网关的流量。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;networking.istio.io/v1alpha3&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;kind&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;VirtualService&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;metadata&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;name&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;httpbin&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;spec&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;hosts&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;-&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;httpbin.example.com&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;gateways&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;-&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;httpbin-gateway&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;http&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;-&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;match&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;-&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;uri&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;prefix&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;/status&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;route&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;-&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;destination&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;port&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;number&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8000&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;host&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;httpbin&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;使用-api-网关暴露服务&#34;&gt;使用 API 网关暴露服务&lt;/h2&gt;
&lt;p&gt;API 网关是位于客户端和后端服务之间的 API 管理工具，一种将客户端接口与后端实现分离的方式，在微服务中得到了广泛的应用。当客户端发出请求时，API 网关会将其分解为多个请求，然后将它们路由到正确的位置，生成响应，并跟踪所有内容。&lt;/p&gt;
&lt;p&gt;API Gateway 是微服务架构体系中的一类型特殊服务，它是所有微服务的入口，它的职责是执行路由请求、协议转换、聚合数据、认证、限流、熔断等。大多数企业 API 都是通过 API 网关部署的。API 网关通常会处理跨 API 服务系统的常见任务，例如用户身份验证、速率限制和统计信息。&lt;/p&gt;
&lt;p&gt;在网格中可以有一个或多个 API Gateway。API 网关的职责有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;请求路由和版本控制&lt;/li&gt;
&lt;li&gt;方便单体应用到微服务的过渡&lt;/li&gt;
&lt;li&gt;权限认证&lt;/li&gt;
&lt;li&gt;数据聚合：监控和计费&lt;/li&gt;
&lt;li&gt;协议转换&lt;/li&gt;
&lt;li&gt;消息和缓存&lt;/li&gt;
&lt;li&gt;安全和报警&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上很多基本功能比如路由和权限认证通过 Istio Gateway 也可以实现，只是在功能的丰富度和扩展性方面有些成熟的 API Gateway 可能更占优势，不过在 Istio mesh 中再引入 API Gateway 也可能带来一些弊端。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;引入了 API Gateway，需要考虑 API Gateway 本身的部署、运维、负载均衡等场景，增加了后端服务的复杂度&lt;/li&gt;
&lt;li&gt;API Gateway 中承载了大量的接口适配，导致难以维护&lt;/li&gt;
&lt;li&gt;对于部分场景，增加了一跳可能导致性能的降低&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;在 Istio mesh 中你可以使用多种 Kubernetes Ingress Controller 来充当入口网关，当然你还可以直接使用 Istio 内置的 Istio 网关，对于策略控制、流量管理和用量监控可以直接通过 Istio 网关来完成，这样做的好处是通过 Istio 的控制平面来直接管理网关，而不需要再借助其他工具。但是对于 API 声明周期管理、复杂的计费、协议转换和认证等功能，传统的 API 网关可能更适合你。所以，你可以根据自己的需求来选择，也可以组合使用。&lt;/p&gt;
&lt;p&gt;目前有些传统的反向代理也在向 Service Mesh 方向发展，如 Nginx 构建了 &lt;a href=&#34;https://www.nginx.com/products/nginx-service-mesh/&#34;&gt;Nginx Service Mesh&lt;/a&gt;，Traefik 构建了 &lt;a href=&#34;https://traefik.io/traefik-mesh/&#34;&gt;Traefik Mesh&lt;/a&gt;。还有的 API 网关产品也向 Service Mesh 方向挺进，比如 Kong 发展出了 &lt;a href=&#34;https://kuma.io&#34;&gt;Kuma&lt;/a&gt;。在未来，我们会看到更多 API 网关、反向代理和服务网格的融合产品出现。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/evolving-kubernetes-networking-with-the-gateway-api/&#34;&gt;利用 Gateway API 发展 Kubernetes 网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/how-to-pick-gateway-for-service-mesh/&#34;&gt;如何为服务网格选择入口网关？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/service-mesh-and-api-gateway/&#34;&gt;Service Mesh 和 API Gateway 关系深度探讨&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/using-traefik-ingress-controller-with-istio-service-mesh/&#34;&gt;在 Istio 服务网格中使用 Traefik Ingress Controller&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>服务网格之旅——使用 Kubernetes 和 Istio Service Mesh 构建混合云</title>
      <link>https://jimmysong.io/blog/multicluster-management-with-kubernetes-and-istio/</link>
      <pubDate>Mon, 12 Jul 2021 22:22:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/multicluster-management-with-kubernetes-and-istio/</guid>
      <description>
        
        
        &lt;p&gt;这篇文章将带你了解使用 Kubernetes 和 Istio Service Mesh 构建多集群及混合云的过程和需要考虑的问题。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes&#34;&gt;Kubernetes&lt;/h2&gt;
&lt;p&gt;使用 Kubernetes 可以快速部署一个分布式环境，实现了云的互操作性，统一了云上的控制平面。并提供了 Service、Ingress 和 &lt;a href=&#34;https://kubernetes.io/blog/2021/04/22/evolving-kubernetes-networking-with-the-gateway-api/&#34;&gt;Gateway&lt;/a&gt; 等资源对象来处理应用程序的流量。如下图所示，Kubernetes 中默认使用 Service 做服务注册和发现，服务之间可以使用服务名称来访问。Kubernetes API Server 与集群内的每个节点上的 &lt;code&gt;kube-proxy&lt;/code&gt; 组件通信，为节点创建 iptables 规则，并将请求转发到其他 pod 上。&lt;/p&gt;
&lt;p&gt;假定现在客户端要访问 Kubernetes 中的服务，首先请求会发送到 Ingress/Gateway 上，然后根据 Ingress/Gateway 里的路由配置转发到后端服务上（图中是服务 A），接着服务 A 对服务 B 请求的流量转发轮询到服务 B 的实例上。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008i3skNly1gsgg6a11l1j31lu0u042s.jpg&#34; alt=&#34;Kubernetes&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-多集群管理&#34;&gt;Kubernetes 多集群管理&lt;/h2&gt;
&lt;p&gt;多集群管理最常见的使用场景包括服务流量负载均衡、隔离开发和生产环境、解耦数据处理和数据存储、跨云备份和灾难恢复、灵活分配计算资源、跨区域服务的低延迟访问以及避免厂商锁定等。一个企业内部往往有多个 Kubernetes 集群，由 MultiCluster SIG 开发的 KubeFed 实现 Kubernetes 集群联邦可以实现多集群管理的功能，这使得所有 Kubernetes 集群都通过同一个接口来管理。&lt;/p&gt;
&lt;p&gt;在使用集群联邦时需要解决以下几个通用问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;配置需要联邦哪些集群&lt;/li&gt;
&lt;li&gt;需要在集群中传播的 API 资源&lt;/li&gt;
&lt;li&gt;配置 API 资源如何分配到不同的集群&lt;/li&gt;
&lt;li&gt;对集群中 DNS 记录注册以实现跨集群的服务发现&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面是 KubeSphere 的多集群架构，也是最常用的一种 Kubernetes 多集群管理架构，其中 Host Cluster 作为控制平面，有两个成员集群，分别是 West 和 East。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008i3skNly1gsgg7a2ojvj31aa0u0491.jpg&#34; alt=&#34;Multicluster&#34;&gt;&lt;/p&gt;
&lt;p&gt;Host 集群需要能够访问 Member 集群的 API Server，Member 集群之间的网络连通性没有要求。管理集群 Host Cluster 独立于其所管理的成员集群，Member Cluster 并不知道 Host Cluster 存在，这样做的好处是当控制平面发生故障时不会影响到成员集群，已经部署的负载仍然可以正常运行，不会受到影响。&lt;/p&gt;
&lt;p&gt;Host 集群同时承担着 API 入口的作用，由 Host Cluster 将对 Member 集群的资源请求转发到 Member 集群，这样做的目的是方便聚合，而且也利于做统一的权限认证。我们看到在 Host Cluster 中有联邦控制平面，其中的 Push Reconciler 会将联邦集群中身份、角色及角色绑定传播到所有成员集群中。&lt;/p&gt;
&lt;h2 id=&#34;istio&#34;&gt;Istio&lt;/h2&gt;
&lt;p&gt;当我们在 Kubernetes 中运行着多语言、多版本的微服务，并需要更细粒度的金丝雀发布和统一的安全策略管理，实现服务间的可观察性时，可以考虑使用 Istio 服务网格。Istio 通过向应用程序 Pod 中注入 sidecar proxy，缺省使用 IPTables 透明得拦截进出应用程序的所有流量，从而实现了应用层到集群中其他启用服务网格的服务的智能应用感知负载均衡，并绕过了初级的 kube-proxy 负载均衡。Istio 控制平面与 Kubernetes API Server 通信可以获取集群中所有注册的服务信息。&lt;/p&gt;
&lt;p&gt;下图展示了 Istio 的基本原理，其中所有节点属于同一个 Kubernetes 集群。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008i3skNly1gsgg6sdrk2j32v60u0qbb.jpg&#34; alt=&#34;Istio Service Mesh&#34;&gt;&lt;/p&gt;
&lt;p&gt;你可能最终会有至少几个Kubernetes集群，每个集群都承载着微服务。Istio 的多集群部署根据网络隔离、主备情况存在多种&lt;a href=&#34;https://istio.io/latest/docs/setup/install/multicluster/&#34;&gt;部署模式&lt;/a&gt;，可以使用 Istio Operator 部署时通过声明来指定。集群中的这些微服务之间的通信可以通过服务网格来加强。在集群内部，Istio提供通用的通信模式，以提高弹性、安全性和可观察性。&lt;/p&gt;
&lt;p&gt;以上都是关于 Kubernetes 上的应用负载管理，但是对于虚拟机上遗留应用，如何在同一个平面中管理？如何管理多集群中的流量划分、网关和安全性呢？&lt;/p&gt;
&lt;h2 id=&#34;管理平面&#34;&gt;管理平面&lt;/h2&gt;
&lt;p&gt;在 Istio 之上再增加一层抽象，将网关、流量和安全分组管理，并将它们应用到不同的集群和命名空间上。下图展示的是 &lt;a href=&#34;https://www.tetrate.io/tetrate-service-bridge/&#34;&gt;Tetrate Service Bridge&lt;/a&gt; 的多租户模型，利用 NGAC 来管理用户的访问权限，同时也有利于构建零信任网络。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008i3skNly1gsgg8ndcajj31il0u00z9.jpg&#34; alt=&#34;Management Plane&#34;&gt;&lt;/p&gt;
&lt;p&gt;Istio 提供了工作负载识别，并由强大的 mTLS 加密保护。这种零信任模型比基于源 IP 等拓扑信息来信任工作负载更好。在 Istio 之上构建一个多集群管理的通用控制平面，然后再增加一个管理平面来管理多集群，提供多租户、管理配置、可观察性等功能。&lt;/p&gt;
&lt;p&gt;下图展示的是 Tetrate Service Bridge 的架构图。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008i3skNly1gsgg951mknj314g0u0dnf.jpg&#34; alt=&#34;Tetrate Service Bridge&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;使用 Kubernetes 实现了异构集群的互操作性，Istio 将容器化负载和虚拟机负载纳入到一个同一个控制平面内，统一管理集群内的流量、安全和可观察性。但是，随着集群数量、网络环境和用户权限的越发复杂，人们还需要在 Istio 的控制平面至上再构建一层管理平面来进行混合云管理。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>如何调试 Kubernetes 中的微服务 ——proxy、sidecar 还是 service mesh？</title>
      <link>https://jimmysong.io/blog/how-to-debug-microservices-in-kubernetes-with-proxy-sidecar-or-service-mesh/</link>
      <pubDate>Mon, 05 Jul 2021 22:22:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/how-to-debug-microservices-in-kubernetes-with-proxy-sidecar-or-service-mesh/</guid>
      <description>
        
        
        &lt;p&gt;Kubernetes 可以说是目前为止用来运行微服务的最佳载体，但是在调试 Kubernetes 环境中的微服务时的体验可能就没那么友好了。本文将带你了解如何调试 Kubernetes 中的微服务，介绍常用的工具，以及 Istio 的引入为微服务的调试带来的变革。&lt;/p&gt;
&lt;h2 id=&#34;调试微服务与传统单体应用有巨大的不同&#34;&gt;调试微服务与传统单体应用有巨大的不同&lt;/h2&gt;
&lt;p&gt;微服务的调试是一直长期困扰软件开发人员的问题，这在传统的单体应用中不存在，因为开发者可以利用 IDE 中的调试器，为应用程序增加断点、修改环境变量，单步执行等，这些都为软件调试提供了巨大帮助。随着 Kubernetes 的流行，微服务的调试就成了一个棘手的问题，其中相比传统单体应用的调试多了以下问题：&lt;/p&gt;
&lt;h3 id=&#34;多依赖&#34;&gt;多依赖&lt;/h3&gt;
&lt;p&gt;一个微服务往往依赖多个其他微服务，在调试某个微服务时，如何部署其他依赖服务以快速搭建一套最新的 stagging 环境？&lt;/p&gt;
&lt;h3 id=&#34;从本地机器访问&#34;&gt;从本地机器访问&lt;/h3&gt;
&lt;p&gt;微服务在开发者的本地电脑上运行时，通常无法直接访问到 Kubernetes 集群中的服务，如何像调试本地服务一样调试部署在 Kubernetes 集群中的微服务？&lt;/p&gt;
&lt;h3 id=&#34;开发效率低下&#34;&gt;开发效率低下&lt;/h3&gt;
&lt;p&gt;通常情况下，代码从更新到构建成镜像再推送到集群中需要一个漫长的过程，如何加快开发速度？&lt;/p&gt;
&lt;p&gt;我们一起来看下哪些工具能够解决以上问题。&lt;/p&gt;
&lt;h2 id=&#34;工具&#34;&gt;工具&lt;/h2&gt;
&lt;p&gt;调试 Kubernetes 中的微服务的主要解决方案有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proxy：在 Kubernetes 集群和本地调试终端中部署一个代理，通过构建一个 VPN，使得本地应用可以直接访问到 Kubernetes 中的服务；&lt;/li&gt;
&lt;li&gt;Sidecar：替换原来应用容器的镜像为开发镜像，可以在这个容器中中对该服务进行调试，同时在要调试的微服务 pod 中注入一个 sidecar 作为辅助工具来同步代码；&lt;/li&gt;
&lt;li&gt;服务网格：要想了解应用的整体情况，就需要在所有微服务中注入 sidecar，这样你就可以获得一个监控全局状态的仪表板；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面是实现以上解决方案的三个典型的开源项目，它们分别从不同的角度可以帮助你调试微服务。&lt;/p&gt;
&lt;h3 id=&#34;proxy-模式telepresence&#34;&gt;Proxy 模式：Telepresence&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.telepresence.io/&#34;&gt;Telesprence&lt;/a&gt; 本质上是一个本地代理，该代理将 Kubernetes 集群中的数据卷、环境变量、网络都代理到了本地。下图展示的是 Teleprence 的主要使用场景。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;telepresence.jpg&#34; alt=&#34;Proxy 模式：Telepresence&#34;&gt;&lt;/p&gt;
&lt;p&gt;用户需要在本地自主地执行 &lt;code&gt;telepresence&lt;/code&gt; 命令，它会自动将代理部署到 Kubernetes 中，有了该代理之后：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本地的服务就可以完整的访问到 Kubernetes 集群中的其他服务、环境变量、Secret、ConfigMap 等；&lt;/li&gt;
&lt;li&gt;集群中的服务还能直接访问到本地暴露出来的端点；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是这种方式仍然不够连贯，还需要用户在本地调试时运行多次命令，而且在某些网络环境下可能无法与 Kubernetes 集群建立 VPN 连接。&lt;/p&gt;
&lt;h3 id=&#34;sidecar-模式nocalhost&#34;&gt;Sidecar 模式：Nocalhost&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://nocalhost.dev/&#34;&gt;Nocalhost&lt;/a&gt; 是一个基于 Kubernetes 的云端开发环境。要想使用它，你只需要在你的 IDE——VS Code 中安装一个插件即可扩展 Kubernetes，并缩短开发反馈周期。通过为不同的用户创建不同的 namespace，并使用 ServiceAccount 绑定到不同用户角身上时，就可以实现开发环境隔离。同时，Nocalhost 还提供了 Web 控制台和 API，方便管理员来管理不同的开发环境。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;sidecar-nocalhost.jpg&#34; alt=&#34;Sidecar 模式：Nocalhost&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;测试&#34;&gt;测试&lt;/h4&gt;
&lt;p&gt;参考 &lt;a href=&#34;https://nocalhost.dev/getting-started.html&#34;&gt;Nocalhost 文档&lt;/a&gt;，我们在 macOS 上安装 Nocalhost，并使用 Minikube 来演示如何调试。&lt;/p&gt;
&lt;p&gt;执行下面的命令安装 Nocalhost 客户端并查看 &lt;code&gt;nhctl&lt;/code&gt; 命令行工具的版本。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;brew install nocalhost/repo/nocalhost

nhctl version
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我们假设你机的 &lt;code&gt;kubeconfig&lt;/code&gt; 文件位于 &lt;code&gt;~/.kube/config&lt;/code&gt;（若不在此位置需要在下面的命令中使用 &lt;code&gt;--kubeconfig&lt;/code&gt; 手动指定） 并拥有 Kubernetes 集群的 admin 角色，执行下面的命令使用 Helm3 在 Kubernetes 上安装 Nocalhost 服务端。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;nhctl init demo -n nocalhost 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;执行下面的命令启动 Minikube 隧道并查看 Nocalhost web 端地址。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;minikube tunnel
kubectl get service nocalhost-web
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在浏览器中访问 &lt;code&gt;http://&amp;lt;EXTERNAL-IP&amp;gt;&lt;/code&gt; 即可，用户名/密码为：&lt;code&gt;admin@admin.com/123456&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;要想在 VS Code 中使用，你还想需要创建一个 ServiceAccount 并绑定 admin 角色，然后将该 ServiceAccount 作为 Kubeconfig 文件导出。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl create serviceaccount my-service-account
kubectl create rolebinding admin --clusterrole&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;admin --serviceaccount&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;default:my-service-account
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;只要你有一个 Kubernetes 集群，并有集群的 admin 权限，就可以参考 Nocalhost 的文档快速开始试用。在 VS Code 中使用 Nocalhost 插件时需要先为插件中配置 Kubernetes 集群。选择你刚导出的 Kubeconfig 文件或者直接复制文件中的内容粘贴到配置里。然后选择你需要测试的服务，并选择对应的 Dev Container，VS Code 会自动打开一个新的代码窗口。&lt;/p&gt;
&lt;p&gt;下面是以 Istio 官方提供的 &lt;a href=&#34;https://istio.io/latest/docs/examples/bookinfo/&#34;&gt;bookinfo 示例&lt;/a&gt;为例，你可以在本地 IDE 中打开克隆下来的代码，然后点击代码文件旁边的锤子即可进入开发模式。选择对应的 DevContainer，nocalhost 会自动向 pod 中注入一个开发容器 sidecar，并在终端中自动进入该容器，如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;nocalhost-vs-code.jpg&#34; alt=&#34;Nocalhost VS code 界面&#34;&gt;&lt;/p&gt;
&lt;p&gt;在开发模式中，本地修改代码，无需重新构建镜像，远端开发环境实时生效，这样可以极大的加快开发速度。同时，Nocalhost 还提供了服务端，可用于开发环境和用户权限进行管理，如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;nocalhost-web-admin.jpg&#34; alt=&#34;Nocalhost web 端&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;service-mesh-模式istio&#34;&gt;Service Mesh 模式：Istio&lt;/h3&gt;
&lt;p&gt;以上使用 proxy 和 sidecar 的方式，一次只能对一个服务进行调试，如果想要掌握服务的全局状况，比如获取的服务的指标，以及通过分布式追踪了解服务的依赖和调用流程，对服务的性能进行调试。这些&lt;a href=&#34;https://istio.io/latest/zh/docs/concepts/observability/&#34;&gt;可观察性&lt;/a&gt;的功能，需要为所有服务统一注入 sidecar 来实现。&lt;/p&gt;
&lt;p&gt;而且，当你的服务正处于从虚拟机迁移到 Kubernetes 的过程中时，使用 Istio 可以将虚拟机与 Kubernetes 纳入一个网络平面中（如下图所示），方便开发者调试和做渐进式的迁移。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;istio-service-mesh.jpg&#34; alt=&#34;Serivce Mesh 模式：Istio&#34;&gt;&lt;/p&gt;
&lt;p&gt;当然要获得这些好处也不是一点“代价”也不没有的，引入 Istio 后，你的 Kubernetes  service 需要遵守 Istio 的&lt;a href=&#34;https://istio.io/latest/zh/docs/ops/deployment/requirements/&#34;&gt;命名规范&lt;/a&gt;，学习使用 &lt;a href=&#34;https://istio.io/latest/docs/ops/diagnostic-tools/istioctl-analyze/&#34;&gt;Istioctl&lt;/a&gt; 命令行和日志的方式来调试微服务。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用 &lt;code&gt;istioctl analyze&lt;/code&gt; 命令来调试集群中的微服务部署情况，可以使用 YAML 文件来检查某个命名空间或整个集群中的资源部署情况。&lt;/li&gt;
&lt;li&gt;使用 &lt;code&gt;istioctl proxy-config secret&lt;/code&gt;  来调试 service mesh 中的 pod 的 secret 被正确的加载并有效。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Istio 的配置信息在大型的集群部署中传播将会耗时更长并且可能有几秒钟的延迟时间，sidecar 的引入会给服务间调用带来一定延迟。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;在应用微服务化和从虚拟机迁移到 Kubernetes 的过程中，开发者需要很多观念和习惯上的转变。通过 proxy 在本地跟 Kubernetes 间构建 VPN，可以方便开发者像调试本地服务一样调试 Kubernetes 中的服务。通过向 pod 中注入 sidecar，可以实现实时调试，加快开发进度。最后，Istio service mesh 真正实现了全局的可观察性，你还可以使用像 &lt;a href=&#34;https://www.tetrate.io/tetrate-service-bridge/&#34;&gt;Tetrate Service Bridge&lt;/a&gt; 这样的工具来管理异构平台，帮助你渐渐地从单体应用过度到微服务。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Istio 开源四周年回顾与展望</title>
      <link>https://jimmysong.io/blog/istio-4-year-birthday/</link>
      <pubDate>Mon, 24 May 2021 08:00:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/istio-4-year-birthday/</guid>
      <description>
        
        
        &lt;p&gt;Istio 是由 &lt;a href=&#34;https://tetrate.io/&#34;&gt;Tetrate&lt;/a&gt; 创始人 Varun Talwar 和谷歌首席工程师 Louis Ryan 命名并在 2017 年 5 月 24 日开源。今天是 Istio 开源四周年，让我们一起来回顾一下 Istio 四年来的发展并展望一下它的未来。&lt;/p&gt;
&lt;h2 id=&#34;istio-的开源历史&#34;&gt;Istio 的开源历史&lt;/h2&gt;
&lt;p&gt;2017 年是 Kubernetes 结束容器编排之战的一年，Google 为了巩固在云原生领域的优势，并弥补 Kubernetes 在服务间流量管理方面的劣势，趁势开源了 Istio。下面是截止目前 Istio 历史上最重要的几次版本发布。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;日期&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;版本&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2017-05-24&lt;/td&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;td&gt;正式开源，该版本发布时仅一个命令行工具。确立了功能范围和 sidecar 部署模式，确立的 Envoy 作为默认 sidecar proxy 的地位。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2017-10-10&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;支持多运行时环境，如虚拟机。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018-06-01&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;API 重构。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018-07-31&lt;/td&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;td&gt;生产就绪，此后 Istio 团队被大规模重组。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019-03-19&lt;/td&gt;
&lt;td&gt;1.1&lt;/td&gt;
&lt;td&gt;企业就绪，支持多 Kubernetes 集群，性能优化。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020-03-03&lt;/td&gt;
&lt;td&gt;1.5&lt;/td&gt;
&lt;td&gt;回归单体架构，支持 WebAssembly 扩展，使得 Istio 的生态更加强大。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020-11-18&lt;/td&gt;
&lt;td&gt;1.8&lt;/td&gt;
&lt;td&gt;正式放弃 Mixer，进一步完善对虚拟机的支持。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Istio 开源后经过了一年时间的发展，在 1.0 发布的前两个月发布了 0.8 版本，这是对 API 的一次大规模重构。而在 2018 年 7 月底发布 1.0 时， Istio 达到了生产可用的临界点，此后 Google 对 Istio 团队进行了大规模重组，多家以 Istio 为基础的 Service Mesh &lt;a href=&#34;https://istio.io/latest/about/ecosystem/#providers&#34;&gt;创业公司&lt;/a&gt;诞生，可以说 2018 年是服务网格行业诞生的元年。&lt;/p&gt;
&lt;p&gt;2019年 3 月 Istio 1.1 发布，而这距离 1.0 发布已经过去了近 9 个月，这已经远远超出一个开源项目的平均发布周期。我们知道迭代和进化速度是基础软件的核心竞争力，此后 Istio 开始以每个季度一个版本的固定&lt;a href=&#34;https://istio.io/v1.7/about/release-cadence/&#34;&gt;发布节奏&lt;/a&gt;，并在 2019 年成为了 &lt;a href=&#34;https://octoverse.github.com/#fastest-growing-oss-projects-by-contributors&#34;&gt;GitHub 增长最快的十大项目中排名第 4 名&lt;/a&gt;！&lt;/p&gt;
&lt;h2 id=&#34;istio-社区&#34;&gt;Istio 社区&lt;/h2&gt;
&lt;p&gt;Istio 开源四年来，已经在 GitHub 上收获了 2.7 万颗星，获得了大量的&lt;a href=&#34;https://istio.io/latest/about/case-studies/&#34;&gt;社区用户&lt;/a&gt;。下图是 &lt;a href=&#34;https://github.com/istio/istio&#34;&gt;Istio&lt;/a&gt; 的 GitHub star 数增长情况。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008i3skNly1gqtm7n2hm1j31me0n2tag.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;2020 年 Istio 的项目管理开始走向成熟，治理方式也到了进化的阶段。2020 年，Istio 社区进行了第一次&lt;a href=&#34;https://istio.io/latest/blog/2020/steering-election-results/&#34;&gt;管委会选举&lt;/a&gt;，还把商标转让给了 &lt;a href=&#34;https://istio.io/latest/blog/2020/open-usage/&#34;&gt;Open Usage Commons&lt;/a&gt;。首届 &lt;a href=&#34;https://events.istio.io/istiocon-2021/&#34;&gt;IstioCon&lt;/a&gt; 在 2021 年 2 月份成功举办，几千人参加了线上会议。在中国也有大量的 Istio 社区用户，2021 年也会有线下面对面的 Istio 社区 meetup 在中国举办。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008i3skNly1gquicfqg14j31lw0smwl2.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;根据 CNCF 2020 年调查，46% 的组织在生产中使用服务网格或计划在未来 12 个月内使用。Istio 是在生产中使用的最多的网格。&lt;/p&gt;
&lt;h2 id=&#34;未来&#34;&gt;未来&lt;/h2&gt;
&lt;p&gt;经过 4 年的发展，围绕 Istio 不仅形成了庞大的用户群，还诞生了多家 Istio 供应商，你可以在最近改版的 &lt;a href=&#34;https://istio.io&#34;&gt;Istio 的官网首页&lt;/a&gt;中看到。在最近几个版本中，Istio 已经将发展中心转移到了提升 Day 2 Operation 体验上来了。我们还希望看到更多的 Istio 的采纳路径建议、案例研究、学习资料、培训及认证（例如来自 Tetrate 的业界的第一个 &lt;a href=&#34;https://academy.tetrate.io/courses/certified-istio-administrator&#34;&gt;Istio 管理员认证&lt;/a&gt;），这些都将有利于 Istio 的推广和采用。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>为什么在使用了 Kubernetes 后你可能还需要 Istio？</title>
      <link>https://jimmysong.io/blog/why-do-you-need-istio-when-you-already-have-kubernetes/</link>
      <pubDate>Wed, 07 Apr 2021 08:27:17 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/why-do-you-need-istio-when-you-already-have-kubernetes/</guid>
      <description>
        
        
        &lt;p&gt;如果你听说过服务网格，并尝试过 &lt;a href=&#34;https://istio.io/&#34;&gt;Istio&lt;/a&gt;，你可能有以下问题。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为什么 Istio 要在 Kubernetes 上运行？&lt;/li&gt;
&lt;li&gt;Kubernetes 和服务网格在云原生应用架构中分别扮演什么角色？&lt;/li&gt;
&lt;li&gt;Istio 扩展了 Kubernetes 的哪些方面？它解决了哪些问题？&lt;/li&gt;
&lt;li&gt;Kubernetes、Envoy 和 Istio 之间是什么关系？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本文将带大家了解 Kubernetes 和 Istio 的内部工作原理。此外，我会介绍 Kubernetes 中的负载均衡方法，并解释为什么有了 Kubernetes 后还需要 Istio。&lt;/p&gt;
&lt;p&gt;Kubernetes 本质上是通过声明式配置来实现应用生命周期管理，而服务网格本质上是提供应用间的流量、安全管理和可观察性。如果你已经使用 Kubernetes 搭建了一个稳定的应用平台，那么如何设置服务间调用的负载均衡和流量控制？是否有这样一个通用的工具或者说平台（非 SDK），可以实现？这就需要用到服务网格了。&lt;/p&gt;
&lt;p&gt;Envoy 引入了 xDS 协议，这个协议得到了各种开源软件的支持，比如 Istio、&lt;a href=&#34;https://mosn.io/&#34;&gt;MOSN&lt;/a&gt; 等。Envoy 将 xDS 贡献给服务网格或云原生基础设施。Envoy 本质上是一个现代版的代理，可以通过 API 进行配置，在此基础上衍生出许多不同的使用场景–比如 API Gateway、服务网格中的 sidecar 代理和边缘代理。&lt;/p&gt;
&lt;p&gt;本文包含以下内容。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kube-proxy 的作用描述。&lt;/li&gt;
&lt;li&gt;Kubernetes 在微服务管理方面的局限性。&lt;/li&gt;
&lt;li&gt;Istio 服务网格的功能介绍。&lt;/li&gt;
&lt;li&gt;Kubernetes、Envoy 和 Istio 服务网格中一些概念的比较。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kubernetes-vs-service-mesh&#34;&gt;Kubernetes vs Service Mesh&lt;/h2&gt;
&lt;p&gt;下图显示了 Kubernetes 中的服务访问关系和服务网格（每个 pod 模型一个 sidecar）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008eGmZEly1gpb7knfo4dj31hk0redrz.jpg&#34; alt=&#34;Kubernetes vs Service Mesh&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;流量转发&#34;&gt;流量转发&lt;/h3&gt;
&lt;p&gt;Kubernetes 集群中的每个节点都部署了一个 kube-proxy 组件，该组件与 Kubernetes API Server 进行通信，获取集群中的服务信息，然后设置 iptables 规则，将服务请求直接发送到对应的 Endpoint（属于同一组服务的 pod）。&lt;/p&gt;
&lt;h3 id=&#34;服务发现&#34;&gt;服务发现&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;008eGmZEly1gpb7knwb79j30kq0fcjs9.jpg&#34; alt=&#34;Service Discovery&#34;&gt;&lt;/p&gt;
&lt;p&gt;Istio 可以跟踪 Kubernetes 中的服务注册，也可以在控制平面中通过平台适配器与其他服务发现系统对接；然后生成数据平面的配置（使用 CRD，这些配置存储在 etcd 中），数据平面的透明代理。数据平面的透明代理以 sidecar 容器的形式部署在每个应用服务的 pod 中，这些代理都需要请求控制平面同步代理配置。代理之所以 “透明”，是因为应用容器完全不知道代理的存在。过程中的 kube-proxy 组件也需要拦截流量，只不过 kube-proxy 拦截的是进出 Kubernetes 节点的流量–而 sidecar 代理拦截的是进出 pod 的流量。&lt;/p&gt;
&lt;h3 id=&#34;服务网格的劣势&#34;&gt;服务网格的劣势&lt;/h3&gt;
&lt;p&gt;由于 Kubernetes 的每个节点上都运行着很多 pod，所以在每个 pod 中放入原有的 kube-proxy 路由转发功能，会增加响应延迟–由于 sidecar 拦截流量时跳数更多，消耗更多的资源。为了对流量进行精细化管理，将增加一系列新的抽象功能。这将进一步增加用户的学习成本，但随着技术的普及，这种情况会慢慢得到缓解。&lt;/p&gt;
&lt;h3 id=&#34;服务网格的优势&#34;&gt;服务网格的优势&lt;/h3&gt;
&lt;p&gt;kube-proxy 的设置是全局的，无法对每个服务进行细粒度的控制，而 service mesh 通过 sidecar proxy 的方式将 Kubernetes 中的流量控制从服务层中抽离出来–可以实现更大的弹性。&lt;/p&gt;
&lt;h3 id=&#34;kube-proxy-的不足之处&#34;&gt;Kube-proxy 的不足之处&lt;/h3&gt;
&lt;p&gt;首先，如果转发的 pod 不能正常服务，它不会自动尝试其他 pod。每个 pod 都有一个健康检查机制，当一个 pod 出现健康问题时，kubelet 会重启 pod，kube-proxy 会删除相应的转发规则。另外，节点 Port 类型的服务不能添加 TLS 或更复杂的消息路由机制。&lt;/p&gt;
&lt;p&gt;Kube-proxy 实现了一个 Kubernetes 服务的多个 pod 实例之间的流量负载均衡，但如何对这些服务之间的流量进行精细化控制–比如将流量按百分比划分给不同的应用版本（这些应用版本都是同一个服务的一部分，但在不同的部署上），或者做金丝雀发布（灰度发布）和蓝绿发布？&lt;/p&gt;
&lt;p&gt;Kubernetes 社区给出了一个使用 Deployment 做&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments&#34;&gt;金丝雀发布&lt;/a&gt;的方法，本质上是通过修改 pod 的标签来给部署的服务分配不同的 pod。&lt;/p&gt;
&lt;h3 id=&#34;kubernetes-ingress-vs-istio-gateway&#34;&gt;Kubernetes Ingress vs Istio Gateway&lt;/h3&gt;
&lt;p&gt;如上所述，kube-proxy 只能在 Kubernetes 集群内路由流量。Kubernetes 集群的 pod 位于 CNI 创建的网络中。一个 ingress—— 一个在 Kubernetes 中创建的资源对象 - 被创建用于集群外部的通信。它由位于 Kubernetes 边缘节点上的入口控制器驱动，负责管理南北向流量。Ingress 必须与各种 Ingress 控制器对接，比如 &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34;&gt;nginx ingress 控制器&lt;/a&gt;和 &lt;a href=&#34;https://traefik.io/&#34;&gt;traefik&lt;/a&gt;。Ingress 只适用于 HTTP 流量，使用简单。它只能通过匹配有限的字段来路由流量–如服务、端口、HTTP 路径等。这使得它无法对 TCP 流量进行路由，如 MySQL、Redis 和各种 RPC。这就是为什么你会看到人们在 ingress 资源注释中写 nginx 配置语言的原因。直接路由南北流量的唯一方法是使用服务的 LoadBalancer 或 NodePort，前者需要云厂商支持，后者需要额外的端口管理。&lt;/p&gt;
&lt;p&gt;Istio Gateway 的功能与 Kubernetes Ingress 类似，它负责进出集群的南北流量。Istio Gateway 描述了一个负载均衡器，用于承载进出服务网格边缘的连接。该规范描述了一组开放端口和这些端口所使用的协议，以及用于负载均衡的 SNI 配置等。Gateway 是一个 CRD 扩展，它也重用了 sidecar 代理的功能；详细配置请参见 &lt;a href=&#34;https://istio.io/latest/docs/reference/config/networking/gateway/&#34;&gt;Istio 网站&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;envoy&#34;&gt;Envoy&lt;/h2&gt;
&lt;p&gt;Envoy 是 Istio 中默认的 sidecar 代理。Istio 基于 Enovy 的 xDS 协议扩展了其控制平面。在讨论 Envoy 的 xDS 协议之前，我们需要先熟悉 Envoy 的基本术语。以下是 Envoy 中的基本术语及其数据结构的列表，更多细节请参考 &lt;a href=&#34;https://envoyproxy.io/&#34;&gt;Envoy 文档&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008eGmZEly1gpb7koah95j31450tetta.jpg&#34; alt=&#34;Envoy&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;基础概念&#34;&gt;基础概念&lt;/h3&gt;
&lt;p&gt;以下是 Enovy 中你应该知道的基本术语。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;下游。下游主机连接到 Envoy，发送请求，并接收响应，即发送请求的主机。&lt;/li&gt;
&lt;li&gt;上游：上游主机。上游主机接收来自 Envoy 的连接和请求，并返回响应；即接收请求的主机。&lt;/li&gt;
&lt;li&gt;Listener：监听器。监听器是一个命名的网络地址（如端口、UNIX 域套接字等）；下游客户端可以连接到这些监听器。Envoy 将一个或多个监听器暴露给下游主机进行连接。&lt;/li&gt;
&lt;li&gt;集群。集群是一组逻辑上相同的上游主机，Envoy 连接到它们。Envoy 通过服务发现来发现集群的成员。可以选择通过主动的健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略来决定集群中哪个成员的请求路由。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 Envoy 中可以设置多个监听器，每个监听器可以设置一个过滤链（过滤链表），而且过滤链是可扩展的，这样我们可以更方便地操纵流量的行为–比如设置加密、私有 RPC 等。&lt;/p&gt;
&lt;p&gt;xDS 协议是由 Envoy 提出的，是 Istio 中默认的 sidecar 代理，但只要实现了 xDS 协议，理论上也可以作为 Istio 中的 sidecar 代理 —— 比如蚂蚁集团开源的 &lt;a href=&#34;https://mosn.io&#34;&gt;MOSN&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cdn.thenewstack.io/media/2021/03/b800bf17-image3.png&#34;&gt;&lt;img src=&#34;008eGmZEly1gpb7kk7wk4j31060lqgqx.jpg&#34; alt=&#34;img&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Istio 是一个功能非常丰富的服务网格，包括以下功能。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;流量管理。这是 Istio 最基本的功能。&lt;/li&gt;
&lt;li&gt;策略控制。实现访问控制系统、遥测采集、配额管理、计费等功能。&lt;/li&gt;
&lt;li&gt;可观察性。在 sidecar 代理中实现。&lt;/li&gt;
&lt;li&gt;安全认证。由 Citadel 组件进行密钥和证书管理。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;istio-中的流量管理&#34;&gt;Istio 中的流量管理&lt;/h2&gt;
&lt;p&gt;Istio 中定义了以下 CRD 来帮助用户进行流量管理。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;网关。网关描述了一个运行在网络边缘的负载均衡器，用于接收传入或传出的 HTTP/TCP 连接。&lt;/li&gt;
&lt;li&gt;虚拟服务（VirtualService）。VirtualService 实际上是将 Kubernetes 服务连接到 Istio 网关。它还可以执行额外的操作，例如定义一组流量路由规则，以便在主机寻址时应用。&lt;/li&gt;
&lt;li&gt;DestinationRule。DestinationRule 定义的策略决定了流量被路由后的访问策略。简单来说，它定义了流量的路由方式。其中，这些策略可以定义为负载均衡配置、连接池大小和外部检测（用于识别和驱逐负载均衡池中不健康的主机）配置。&lt;/li&gt;
&lt;li&gt;EnvoyFilter。EnvoyFilter 对象描述了代理服务的过滤器，可以自定义 Istio Pilot 生成的代理配置。这种配置一般很少被主用户使用。&lt;/li&gt;
&lt;li&gt;ServiceEntry。默认情况下，Istio 服务 Mesh 中的服务无法发现 Mesh 之外的服务。ServiceEntry 可以在 Istio 内部的服务注册表中添加额外的条目，从而允许 Mesh 中自动发现的服务访问并路由到这些手动添加的服务。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kubernetes-vs-xds-vs-istio&#34;&gt;Kubernetes vs xDS vs Istio&lt;/h2&gt;
&lt;p&gt;在回顾了 Kubernetes 的 kube-proxy 组件、xDS 和 Istio 对流量管理的抽象后，现在我们仅从流量管理的角度来看看这三个组件 / 协议的比较（注意，三者并不完全等同）。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;xDS&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Istio service mesh&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Endpoint&lt;/td&gt;
&lt;td&gt;Endpoint&lt;/td&gt;
&lt;td&gt;WorkloadEntry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service&lt;/td&gt;
&lt;td&gt;Route&lt;/td&gt;
&lt;td&gt;VirtualService&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-proxy&lt;/td&gt;
&lt;td&gt;Route&lt;/td&gt;
&lt;td&gt;DestinationRule&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-proxy&lt;/td&gt;
&lt;td&gt;Listener&lt;/td&gt;
&lt;td&gt;EnvoyFilter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ingress&lt;/td&gt;
&lt;td&gt;Listener&lt;/td&gt;
&lt;td&gt;Gateway&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service&lt;/td&gt;
&lt;td&gt;Cluster&lt;/td&gt;
&lt;td&gt;ServiceEntry&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;核心观点&#34;&gt;核心观点&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes 的本质是应用生命周期管理，具体来说就是部署和管理（伸缩、自动恢复、发布）。&lt;/li&gt;
&lt;li&gt;Kubernetes 为微服务提供了一个可扩展、高弹性的部署和管理平台。&lt;/li&gt;
&lt;li&gt;服务网格是基于透明代理，通过 sidecar 代理拦截服务之间的流量，然后通过控制平面配置管理它们的行为。&lt;/li&gt;
&lt;li&gt;服务网格将流量管理与 Kubernetes 解耦，不需要 kube-proxy 组件来支持服务网格内的流量；通过提供更接近微服务应用层的抽象来管理服务间的流量、安全性和可观察性。&lt;/li&gt;
&lt;li&gt;xDS 是服务网格的协议标准之一。&lt;/li&gt;
&lt;li&gt;服务网格是 Kubernetes 中服务的一个更高层次的抽象。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;如果说 Kubernetes 管理的对象是一个 pod，那么服务网格管理的对象就是一个服务，所以用 Kubernetes 管理微服务，然后应用服务网格就可以了。如果你连服务都不想管理，那就用 &lt;a href=&#34;https://knative.dev/&#34;&gt;Knative&lt;/a&gt; 这样的无服务器平台，不过这是后话。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>云原生初学者入门必读</title>
      <link>https://jimmysong.io/blog/must-read-for-cloud-native-beginner/</link>
      <pubDate>Sun, 18 Oct 2020 14:18:40 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/must-read-for-cloud-native-beginner/</guid>
      <description>
        
        
        &lt;h2 id=&#34;为什么写这篇文章&#34;&gt;为什么写这篇文章&lt;/h2&gt;
&lt;p&gt;看到这个标题后，大家可能会问“都已经 2020 年了，Kubernetes 开源有 6 年时间了，为什么还要写一篇 Kubernetes 入门的文章？”我想说的是，Kubernetes 还远远没有达到我们想象的那么普及。众多的开发者，平时忙于各自的业务开发，学习新技术的时间有限；还有大量的学生群体，可能还仅仅停留在“知道有这门技术”的阶段，远远没有入门。这篇文章将助于各位有志于从事云原生领域工作或需要了解该领域背景的人群快速入门 Kubernetes 和云原生。&lt;/p&gt;
&lt;p&gt;因为云原生的知识体系过于庞杂，本文主要讲解容器、Kubernetes 及服务网格的入门概念，关于云原生的更多细节将在后续文章中推出。&lt;/p&gt;
&lt;h2 id=&#34;引言&#34;&gt;引言&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; 一词来自希腊语，意思是“飞行员”或“舵手”。这个名字很贴切，Kubernetes 可以帮助你在波涛汹涌的容器海洋中航行。&lt;/p&gt;
&lt;p&gt;Kubernetes 是做什么的？什么是 Docker？什么是容器编排？Kubernetes 是如何工作和扩展的？你可能还有很多其他的问题，本文将一一为你解答。&lt;/p&gt;
&lt;p&gt;这篇文章适合初学者，尤其是那些工作忙碌，没有办法抽出太多时间来了解 Kubernetes 和云原生的开发者们，希望本文可以帮助你进入 Kubernetes 的世界。&lt;/p&gt;
&lt;p&gt;简而言之，Kubernetes 提供了一个平台或工具来帮助你快速协调或扩展容器化应用，特别是在 &lt;a href=&#34;https://docker.com/&#34;&gt;Docker&lt;/a&gt; 容器。让我们深入了解一下这些概念。&lt;/p&gt;
&lt;h2 id=&#34;容器和容器化&#34;&gt;容器和容器化&lt;/h2&gt;
&lt;p&gt;那么什么是容器呢？&lt;/p&gt;
&lt;p&gt;要讨论容器化首先要谈到虚拟机 (VM)，顾名思义，虚拟机就是可以远程连接的虚拟服务器，比如 AWS 的 EC2 或阿里云的 ECS。&lt;/p&gt;
&lt;p&gt;接下来，假如你要在虚拟机上运行一个网络应用——包括一个 MySQL 数据库、一个 Vue 前端和一些 Java 库，在 Ubuntu 操作系统 (OS) 上运行。你不用熟悉其中的每一个技术——你只要记住，一个应用程序由各种组件、服务和库组成，它们运行在操作系统上。&lt;/p&gt;
&lt;p&gt;现在，将应用程序打包成一个虚拟机镜像，这个镜像中包括了 Ubuntu 操作系统。这使得虚拟机变得非常笨重——通常有几个 G 的大小。&lt;/p&gt;
&lt;p&gt;虚拟机镜像包含了整个操作系统及所有的库，对应用程序来说，这个镜像过于臃肿，其中大部分组件并没有被应用程序直接调用。如果你需要重新创建、备份或扩展这个应用程序，就需要复制整个环境（虚拟机镜像），在新环境中启动应用通常需要几十秒甚至几分钟时间。如果你想单独升级应用中的某个组件，比如说 Vue 应用，就需要重建整个虚拟机镜像。另外，如果你的两个应用依赖同一个底层镜像，升级底层镜像会同时影响这两个应用，而有时候，你只需要升级其中一个应用的依赖而已。这就是所谓的“依赖陷阱”。&lt;/p&gt;
&lt;p&gt;解决这个问题的办法就是容器。容器是继虚拟机之后更高层次的抽象，在这层抽象中，整个应用程序的每个组件被单独打包成一个个独立的单元，这个单元就是所谓的容器。通过这种方式，可以将代码和应用服务从底层架构中分离出来，实现了完全的可移植性（在任何操作系统或环境上运行应用的能力）。所以在上面的例子中，Ubuntu 操作系统就是一个单元（容器）。MySQL 数据库是另一个容器，Vue 环境和随之而来的库也是一个容器。&lt;/p&gt;
&lt;p&gt;但是，MySQL 数据库是如何自己“运行”的？数据库本身肯定也要在操作系统上运行吧？没错！&lt;/p&gt;
&lt;p&gt;更高层次的容器，比如 MySQL 容器，实际上会包含必要的库来与底层的操作系统容器通信和集成。所以你可以把容器看成是整个应用堆栈中的一层，每层都依赖于下层的单元。而这就类似于船舶或港口中集装箱的堆叠方式，每个容器的稳定性都依赖于下面的容器的支持。所以应用容器的核心是一个受控的执行环境。它们允许你从头开始定义整个环境，从操作系统开始，到你要使用的各个版本的库，再到你要添加的代码版本。&lt;/p&gt;
&lt;p&gt;与容器相关的一个重要概念是&lt;strong&gt;微服务&lt;/strong&gt;。将应用程序的各个组件拆分并打包成独立的服务，这样每个组件都可以很容易地被替换、升级、调试。上面的例子中，我们会为 Vue 前端创建一个微服务，为 MySQL 数据库创建另一个微服务，为 Java 中间件部分创建另一个微服务，以此类推。很明显，微服务与容器化是相辅相成的。&lt;/p&gt;
&lt;h2 id=&#34;从-docker-开始&#34;&gt;从 Docker 开始&lt;/h2&gt;
&lt;p&gt;现在你已经对容器有一定了解了吧？Docker 是最常用的容器化工具，也是最流行的容器运行时。&lt;/p&gt;
&lt;p&gt;Docker 开源于 2013 年。用于打包和创建容器，管理基于容器的应用。所有 Linux 发行版、Windows 和 macOS 都支持 Docker。&lt;/p&gt;
&lt;p&gt;还有其他的容器化工具，如 &lt;a href=&#34;https://coreos.com/rkt/&#34;&gt;CoreOS rkt&lt;/a&gt;、&lt;a href=&#34;http://mesos.apache.org/documentation/latest/mesos-containerizer/&#34;&gt;Mesos Containerizer&lt;/a&gt; 和 &lt;a href=&#34;https://linuxcontainers.org/&#34;&gt;LXC&lt;/a&gt;。但是目前，绝大多数的容器化应用都是在 Docker 上运行的。&lt;/p&gt;
&lt;h2 id=&#34;再到-kubernetes&#34;&gt;再到 Kubernetes&lt;/h2&gt;
&lt;p&gt;首先，简单介绍一下历史。Kubernetes 是 Google 基于其内部容器调度平台 Borg 的经验开发的。2014 年开源，并作为 CNCF（云原生计算基金会）的核心发起项目。&lt;/p&gt;
&lt;p&gt;那么 Kubernetes 又跟容器是什么关系呢？让我们再回到上面的例子。假设我们的应用爆火，每天的注册用户越来越多。&lt;/p&gt;
&lt;p&gt;现在，我们需要增加后端资源，使浏览我们网站的用户在浏览页面时加载时间不会过长或者超时。最简单的方式就是增加容器的数量，然后使用负载均衡器将传入的负载（以用户请求的形式）分配给容器。&lt;/p&gt;
&lt;p&gt;这样做虽然行之有效，但也只能在用户规模有限的情况下使用。当用户请求达到几十万或几百万时，这种方法也是不可扩展的。你需要管理几十个也许是几百个负载均衡器，这本身就是另一个令人头疼的问题。如果我们想对网站或应用进行任何升级，也会遇到问题，因为负载均衡不会考虑到应用升级的问题。我们需要单独配置每个负载均衡器，然后升级该均衡器所服务的容器。想象一下，当你有 20 个负载均衡器和每周 5 或 6 个小的更新时，你将不得不进行大量的手工劳动。&lt;/p&gt;
&lt;p&gt;我们需要的是一种可以一次性将变更传递给所有受控容器的方法，同时也需要一种可以轻松地调度可用容器的方法，这个过程还必须要是自动化的，这正是 Kubernetes 所做的事情。&lt;/p&gt;
&lt;p&gt;接下来，我们将探讨 Kubernetes 究竟是如何工作的，它的各种组件和服务，以及更多关于如何使用 Kubernetes 来编排、管理和监控容器化环境。为了简单起见，假设我们使用的是 Docker 容器，尽管如前所述，Kubernetes 除了支持 Docker 之外，还支持其他几种容器平台。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-架构和组件&#34;&gt;Kubernetes 架构和组件&lt;/h2&gt;
&lt;p&gt;首先，最重要的是你需要认识到 Kubernetes 利用了“期望状态”原则。就是说，你定义了组件的期望状态，而 Kubernetes 要将它们始终调整到这个状态。&lt;/p&gt;
&lt;p&gt;例如，你想让你的 Web 服务器始终运行在 4 个容器中，以达到负载均衡的目的，你的数据库复制到 3 个不同的容器中，以达到冗余的目的。这就是你想要的状态。如果这 7 个容器中的任何一个出现故障，Kubernetes 引擎会检测到这一点，并自动创建出一个新的容器，以确保维持所需的状态。&lt;/p&gt;
&lt;p&gt;现在我们来定义一些 Kubernetes 的重要组件。&lt;/p&gt;
&lt;p&gt;当你第一次设置 Kubernetes 时，你会创建一个集群。所有其他组件都是集群的一部分。你也可以创建多个虚拟集群，称为命名空间 (namespace)，它们是同一个物理集群的一部分。这与你可以在同一物理服务器上创建多个虚拟机的方式非常相似。如果你不需要，也没有明确定义的命名空间，那么你的集群将在始终存在的默认命名空间中创建。&lt;/p&gt;
&lt;p&gt;Kubernetes 运行在节点 (node) 上，节点是集群中的单个机器。如果你有自己的硬件，节点可能对应于物理机器，但更可能对应于在云中运行的虚拟机。节点是部署你的应用或服务的地方，是 Kubernetes 工作的地方。有 2 种类型的节点——master 节点和 worker 节点，所以说 Kubernetes 是主从结构的。&lt;/p&gt;
&lt;p&gt;主节点是一个控制其他所有节点的特殊节点。一方面，它和集群中的任何其他节点一样，这意味着它只是另一台机器或虚拟机。另一方面，它运行着控制集群其他部分的软件。它向集群中的所有其他节点发送消息，将工作分配给它们，工作节点向主节点上的 API Server 汇报。&lt;/p&gt;
&lt;p&gt;Master 节点本身也包含一个名为 API Server 的组件。这个 API 是节点与控制平面通信的唯一端点。API Server 至关重要，因为这是 worker 节点和 master 节点就 pod、deployment 和所有其他 Kubernetes API 对象的状态进行通信的点。&lt;/p&gt;
&lt;p&gt;Woker 节点是 Kubernetes 中真正干活的节点。当你在应用中部署容器或 pod（稍后定义）时，其实是在将它们部署到 worker 节点上运行。Worker 节点托管和运行一个或多个容器的资源。&lt;/p&gt;
&lt;p&gt;Kubernetes 中的逻辑而非物理的工作单位称为 pod。一个 pod 类似于 Docker 中的容器。记得我们在前面讲到，容器可以让你创建独立、隔离的工作单元，可以独立运行。但是要创建复杂的应用程序，比如 Web 服务器，你经常需要结合多个容器，然后在一个 pod 中一起运行和管理。这就是 pod 的设计目的——一个 pod 允许你把多个容器，并指定它们如何组合在一起来创建应用程序。而这也进一步明确了 Docker 和 Kubernetes 之间的关系——一个 Kubernetes pod 通常包含一个或多个 Docker 容器，所有的容器都作为一个单元来管理。&lt;/p&gt;
&lt;p&gt;Kubernetes 中的 service 是一组逻辑上的 pod。把一个 service 看成是一个 pod 的逻辑分组，它提供了一个单一的 IP 地址和 DNS 名称，你可以通过它访问服务内的所有 pod。有了服务，就可以非常容易地设置和管理负载均衡，当你需要扩展 Kubernetes pod 时，这对你有很大的帮助，我们很快就会看到。&lt;/p&gt;
&lt;p&gt;ReplicationController 或 ReplicaSet 是 Kubernetes 的另一个关键功能。它是负责实际管理 pod 生命周期的组件——当收到指令时或 pod 离线或意外停止时启动 pod，也会在收到指示时杀死 pod，也许是因为用户负载减少。所以换句话说，ReplicationController 有助于实现我们所期望的指定运行的 pod 数量的状态。&lt;/p&gt;
&lt;h2 id=&#34;什么是-kubectl&#34;&gt;什么是 Kubectl？&lt;/h2&gt;
&lt;p&gt;kubectl 是一个命令行工具，用于与 Kubernetes 集群和其中的 pod 通信。使用它你可以查看集群的状态，列出集群中的所有 pod，进入 pod 中执行命令等。你还可以使用 YAML 文件定义资源对象，然后使用 kubectl 将其应用到集群中。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-中的自动扩展&#34;&gt;Kubernetes 中的自动扩展&lt;/h2&gt;
&lt;p&gt;请记住，我们使用 Kubernetes 而不是直接使用 Docker 的原因之一，是因为 Kubernetes 能够自动扩展应用实例的数量以满足工作负载的需求。&lt;/p&gt;
&lt;p&gt;自动缩放是通过集群设置来实现的，当服务需求增加时，增加节点数量，当需求减少时，则减少节点数量。但也要记住，节点是 “物理” 结构——我们把“物理”放在引号里，因为要记住，很多时候，它们实际上是虚拟机。&lt;/p&gt;
&lt;p&gt;无论如何，节点是物理机器的事实意味着我们的云平台必须允许 Kubernetes 引擎创建新机器。各种云提供商对 Kubernetes 支持基本都满足这一点。&lt;/p&gt;
&lt;p&gt;我们再继续说一些概念，这次是和网络有关的。&lt;/p&gt;
&lt;h2 id=&#34;什么是-kubernetes-ingress-和-egress&#34;&gt;什么是 kubernetes Ingress 和 Egress？&lt;/h2&gt;
&lt;p&gt;外部用户或应用程序与 Kubernetes pod 交互，就像 pod 是一个真正的服务器一样。我们需要设置安全规则允许哪些流量可以进入和离开“服务器”，就像我们为托管应用程序的服务器定义安全规则一样。&lt;/p&gt;
&lt;p&gt;进入 Kubernetes pod 的流量称为 Ingress，而从 pod 到集群外的出站流量称为 egress。我们创建入口策略和出口策略的目的是限制不需要的流量进入和流出服务。而这些策略也是定义 pod 使用的端口来接受传入和传输传出数据 / 流量的地方。&lt;/p&gt;
&lt;h2 id=&#34;什么是-ingress-controller&#34;&gt;什么是 Ingress Controller？&lt;/h2&gt;
&lt;p&gt;但是在定义入口和出口策略之前，你必须首先启动被称为 Ingress Controller（入口控制器）的组件；这个在集群中默认不启动。有不同类型的入口控制器，Kubernetes 项目默认只支持 Google Cloud 和开箱即用的 Nginx 入口控制器。通常云供应商都会提供自己的入口控制器。&lt;/p&gt;
&lt;h2 id=&#34;什么是-replica-和-replicaset&#34;&gt;什么是 Replica 和 ReplicaSet？&lt;/h2&gt;
&lt;p&gt;为了保证应用程序的弹性，需要在不同节点上创建多个 pod 的副本。这些被称为 Replica。假设你所需的状态策略是“让名为 webserver-1 的 pod 始终维持在 3 个副本”，这意味着 ReplicationController 或 ReplicaSet 将监控活动副本的数量，如果其中有任何一个 replica 因任何原因不可用（例如节点的故障），那么 Deployment Controller 将自动创建一个新的系统（定义如下）。&lt;/p&gt;
&lt;p&gt;所需状态是在 deployment 中定义的。 Master 节点的中有一个子系统叫做 Deployment Controller，负责实际执行并使当前状态不断趋向于所需状态。&lt;/p&gt;
&lt;p&gt;因此，举例来说，如果你目前有 2 个 pod 的副本，而你所希望的状态应该有 3 个，那么 Replication Controller 或 ReplicaSet 会自动检测到这个要求，并指示 Deployment Controller 根据预定义的设置部署一个新的 pod。&lt;/p&gt;
&lt;h2 id=&#34;什么是服务网格&#34;&gt;什么是服务网格？&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://jimmysong.io/blog/what-is-a-service-mesh/&#34;&gt;服务网格 (Service Mesh)&lt;/a&gt; 用于管理服务之间的网络流量，是云原生的网络基础设施层，也是 &lt;a href=&#34;https://jimmysong.io/blog/post-kubernetes-era/&#34;&gt;Kubernetes 次世代的云原生应用&lt;/a&gt; 的重要组成部分。&lt;/p&gt;
&lt;p&gt;服务网格利用容器之间的网络设置来控制或改变应用程序中不同组件之间的交互。下面，我们用一个例子来说明。假设你想测试 Nginx 的新版本，检查它是否与你的 Web 应用兼容。你用新的 Nginx 版本创建了一个新的容器 (Container2)，并从当前容器 (Container1) 中复制了当前的 Nginx webserver 配置。但你不想影响组成 web 应用的其他微服务（假设每个容器对应一个单独的微服务）——就是 MySQL 数据库、Node.js 前端、负载均衡器等。&lt;/p&gt;
&lt;p&gt;所以使用服务网格，你可以立即只把 webserver 微服务改成 Container2（新 Nginx 版本的那个）进行测试。如果确定它不能工作，比如因为它导致网站出现一些兼容性问题，那么你就调用服务网格来快速切换回原来的 Container1。而这一切都不需要对其他容器进行任何配置变更——这些变更对其他容器是完全透明的。&lt;/p&gt;
&lt;p&gt;如果没有服务网格，对容器来说这项工作将十分繁琐，因为这涉及到逐一更改所有其他容器上的配置，将它们所包含的服务从 Container1 指向 Container2，然后在测试失败后，将它们全部改回来。&lt;/p&gt;
&lt;p&gt;在前面这部分 Kubernetes 指南中，我们介绍了一些与 Kubernetes 网络相关的概念。Kubernetes 中的网络可能很棘手，很难理解，如果你刚刚开始，你可能需要一些实践来理解这里。关于服务网格的更多内容请参考 &lt;a href=&#34;https://www.servicemesher.com/istio-handbook&#34;&gt;Istio Handbook——Istio 服务网格进阶实战&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;在下一部分中，我们将展开更多关于 Kubernetes 的话题：如何开始学习 Kubernetes，如何在本地安装和测试 Kubernetes，以及 Kubernetes 的一些优秀的监控工具。&lt;/p&gt;
&lt;h2 id=&#34;如何学习-kubernetes&#34;&gt;如何学习 Kubernetes？&lt;/h2&gt;
&lt;p&gt;自学 Kubernetes 知识基本上有三种不同的途径，我们在这里只提供了一个指导大纲。&lt;/p&gt;
&lt;h3 id=&#34;一从零开始学习和安装-kubernetes&#34;&gt;一、从零开始学习和安装 Kubernetes&lt;/h3&gt;
&lt;p&gt;要想真正掌握 Kubernetes，最好的办法莫过于自己从头开始安装 Kubernetes。不过要注意的是，从零开始安装 Kubernetes 并不是一件容易的事情。安装 Kubernetes 并不是简单的“下载文件 -&amp;gt; 点击安装”式的操作，Kubernetes 由多个组件组成，这些组件必须单独安装和配置。而在此之前，你也需要相当的技术储备来做安装前的准备，比如熟悉 Linux 操作系统。如果你决定使用这种方式学习的话，推荐你阅读 &lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook&#34;&gt;Kubernetes Handbook——Kubernetes 中文指南 / 云原生架构实践手册&lt;/a&gt;。此外，请记住，尽管 Kubernetes 作为一个开源解决方案在技术上是免费的，但它确实有一些隐藏的成本，只不过对初学者来说可能并不明显。&lt;/p&gt;
&lt;h3 id=&#34;二kubernetes-自托管解决方案&#34;&gt;二、Kubernetes 自托管解决方案&lt;/h3&gt;
&lt;p&gt;这些解决方案样是一些工具和实用程序，大大简化了在本地计算机上安装和配置小型 Kubernetes 集群的任务。它们是学习 Kubernetes 的好方法，同时对于新手来说也不会太难，又足够小巧可以到安装在个人电脑上。最流行的自托管 Kubernetes 工具和环境是 &lt;a href=&#34;https://github.com/kubernetes/minikube&#34;&gt;Minikube&lt;/a&gt;、&lt;a href=&#34;https://github.com/ubuntu/microk8s&#34;&gt;MicroK8s&lt;/a&gt;、&lt;a href=&#34;https://docs.docker.com/docker-for-windows/kubernetes/&#34;&gt;Docker Desktop&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/kubernetes-sigs/kind&#34;&gt;Kind&lt;/a&gt;。这些解决方案往往有一些限制，例如，Minikube 只允许创建一个节点。尽管有这些缺点，但这些工具还是非常值得推荐，因为它们将易学性和成本效益结合起来，对于刚开始使用 Kubernetes 的初学者来说，是一个很好的选择。&lt;/p&gt;
&lt;h3 id=&#34;三云托管的解决方案&#34;&gt;三、云托管的解决方案&lt;/h3&gt;
&lt;p&gt;如今各大云供应商都提供了定制化的 Kubernetes 解决方案来。你也可以通过线上教学平台如 &lt;a href=&#34;https://katacoda.com/&#34;&gt;Katacoda&lt;/a&gt; 上的免费课程来学习 Kubernetes，它们都是云托管的，你不需要自己安装，只不过你需要云供应商的集群需要付费。&lt;/p&gt;
&lt;h2 id=&#34;本地测试和调试-kubernetes&#34;&gt;本地测试和调试 Kubernetes&lt;/h2&gt;
&lt;p&gt;作为本地安装 Kubernetes 的一部分，你很可能还需要一些测试和调试能力，以确保一切都在顺利运行，特别是定义入口和出口策略等棘手的任务。此外，还有 Kubernetes 附加组件的生态系统，你可能想使用这些组件来扩展 Kubernetes 集群的功能。添加所有这些都需要进行更多的测试，以确保它们能与你的 Kubernetes 集群完美的集成。&lt;/p&gt;
&lt;p&gt;用于在本地开发和调试 Kubernetes 服务的工具有：&lt;a href=&#34;https://github.com/microsoft/mindaro&#34;&gt;Microsoft Bridge to Kubernetes&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/telepresenceio/telepresence&#34;&gt;telepresence&lt;/a&gt;。这些工具可以让你在本地运行单个服务，同时将该服务连接到远程 Kubernetes 集群。这样你就可以让自己的本地机器作为 Kubernetes 集群中的一部分来运行——这对于在本地而不是在生产集群上开发服务非常有用。&lt;/p&gt;
&lt;p&gt;Kubernetes 项目也了解到了 Kubernetes 安装对端到端 (E2E) 测试的需求。为此，项目核心团队一直在确保在最近的版本中更恰当地支持 E2E 测试。这包括诸如允许测试重用和纳入更多附加组件和驱动程序的测试等。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-监控工具&#34;&gt;Kubernetes 监控工具&lt;/h2&gt;
&lt;p&gt;Kubernetes 提供了应用程序在集群的每个层次上的资源使用情况的详细信息——容器、pod、服务。这些详细信息使你能够评估应用程序的性能，确定哪些瓶颈可以解决以提高整体性能。&lt;/p&gt;
&lt;p&gt;毕竟，监控可以帮助你了解应用和集群运行情况的详细信息，这对于学习 Kubernetes 是十分有帮助的。&lt;/p&gt;
&lt;p&gt;Kubernetes 包含两个内置度量收集工具用于监控：&lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/&#34;&gt;资源管道和全度量管道&lt;/a&gt;。资源管道是一个较低级和较有限的工具，主要集中在与各种控制器相关的指标上。全指标管道，顾名思义，从几乎所有集群组件中获取并显示更丰富的指标。&lt;/p&gt;
&lt;p&gt;还有一些第三方工具可以安装并集成到 Kubernetes 集群中。对于 Kubernetes 来说，最普遍使用的两个工具是 Prometheus 和 Grafana。&lt;/p&gt;
&lt;h3 id=&#34;prometheus-监控&#34;&gt;Prometheus 监控&lt;/h3&gt;
&lt;p&gt;Prometheus 是一个功能丰富的开源监控和警报工具。Prometheus 包含一个内部数据存储用来收集指标，如生成的时间序列数据。Prometheus 还拥有众多插件，允许它将数据暴露给各种外部解决方案，并从其他数据源导入数据，包括所有主要公有云监控解决方案。&lt;/p&gt;
&lt;h3 id=&#34;grafana-仪表盘&#34;&gt;Grafana 仪表盘&lt;/h3&gt;
&lt;p&gt;Grafana 是一个优秀的仪表盘、分析和数据可视化工具。它没有 Prometheus 的全功能数据收集能力，但 Prometheus 又没有 Grafana 的数据呈现界面。事实上，他们最好是结合在一起使用——Prometheus 负责数据收集和汇总，Grafana 负责数据展示。它们共同创造了一个强大的组合，涵盖了数据收集、基本警报和可视化。&lt;/p&gt;
&lt;h3 id=&#34;高级警报&#34;&gt;高级警报&lt;/h3&gt;
&lt;p&gt;对于高级警报，你可以添加 &lt;a href=&#34;https://www.nagios.org/&#34;&gt;Nagios&lt;/a&gt; 或 &lt;a href=&#34;https://github.com/prometheus/alertmanager&#34;&gt;Prometheus Alertmanager&lt;/a&gt; 等工具。这些警报工具通常有大量的集成。你可以为自定义值班团队，然后定义你想要监控的参数，例如“当任何 pod 不可用时”或“当任何节点无法访问时”、“当容量达到 90%”等，然后通过电子邮件、短信、手机应用提醒、电话呼叫等方式向值班人员发送自定义通知。你还可以创建升级策略，比如，如果一个被定义为“危急”的警报在 10 分钟内没有值班人员确认，那么就将警报升级（发送警报）到该人员的经理。&lt;/p&gt;
&lt;p&gt;现在，你应该已经对 Docker 和 Kubernetes 有了大体的认识。了解了 Kubernetes 的作用，知道它是如何进行容器化应用部署和管理的。&lt;/p&gt;
&lt;p&gt;调试和监控技术不仅仅是运维需要，你也可以把它当作学习方式。有什么比边做边学更好呢？&lt;/p&gt;
&lt;p&gt;请记住，如果你的应用规模太小，而且预计用户需求不会有太大变化或重大波动（比如一个只在公司内部使用的应用），那么 Kubernetes 对你来说可能没有必要，这种情况下，直接使用 Docker 就足够了。&lt;/p&gt;
&lt;h2 id=&#34;更多&#34;&gt;更多&lt;/h2&gt;
&lt;p&gt;云原生领域的开源项目众多（见 &lt;a href=&#34;https://jimmysong.io/awesome-cloud-native&#34;&gt;Awesome Cloud Native/云原生开源项目大全&lt;/a&gt;），其中有大量的优秀项目可供我们学习。此外，Kubernetes 开源已经多年时间，网上有大量的学习资料，业界出版过很多&lt;a href=&#34;https://jimmysong.io/cloud-native/note/books/&#34;&gt;书籍&lt;/a&gt;，建议大家通过阅读&lt;a href=&#34;https://kubernetes.io&#34;&gt;官方文档&lt;/a&gt;和实践来学习，也可以参考我编写的&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34;&gt;Kubernetes Handbook——Kubernetes 中文指南 / 云原生架构实践手册&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;推荐大家加入我发起创办的&lt;a href=&#34;https://cloudnative.to&#34;&gt;云原生社区&lt;/a&gt;，这是一个立足中国，放眼世界的云原生终端用户社区，致力于云原生技术的传播和应用。云原生社区主办的&lt;a href=&#34;https://github.com/cloudnativeto/academy&#34;&gt;云原生学院&lt;/a&gt;定期邀请云原生和开源领域的大咖在 B 站上进行直播分享，成员自发组织了多个 SIG（特别兴趣小组）进行讨论学习。欢迎加入我们，共同学习和交流云原生技术。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>云原生应用之路</title>
      <link>https://jimmysong.io/blog/from-kubernetes-to-cloud-native/</link>
      <pubDate>Wed, 20 Dec 2017 15:08:02 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/from-kubernetes-to-cloud-native/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;从Kubernetes到Cloud Native——云原生应用之路&lt;/strong&gt;，这是我最近在 &lt;a href=&#34;http://bj2017.archsummit.com/presentation/306&#34;&gt;ArchSummit2017北京站&lt;/a&gt; 和 &lt;a href=&#34;https://www.kubernetes.org.cn/3211.html&#34;&gt;数人云&amp;amp;TalkingData合办的Service Mesh is comming meetup&lt;/a&gt; 中分享的话题。&lt;/p&gt;
&lt;p&gt;本文简要介绍了容器技术发展的路径，为何Kubernetes的出现是容器技术发展到这一步的必然选择，而为何Kuberentes又将成为云原生应用的基石。&lt;/p&gt;
&lt;p&gt;我的分享按照这样的主线展开：容器-&amp;gt;Kubernetes-&amp;gt;微服务-&amp;gt;Cloud Native（云原生）-&amp;gt;Service Mesh（服务网格）-&amp;gt;使用场景-&amp;gt;Open Source（开源）。&lt;/p&gt;
&lt;h2 id=&#34;容器&#34;&gt;容器&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;容器——Cloud Native的基石&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;容器最初是通过开发者工具而流行，可以使用它来做隔离的开发测试环境和持续集成环境，这些都是因为容器轻量级，易于配置和使用带来的优势，docker和docker-compose这样的工具极大的方便的了应用开发环境的搭建，开发者就像是化学家一样在其中小心翼翼的进行各种调试和开发。&lt;/p&gt;
&lt;p&gt;随着容器的在开发者中的普及，已经大家对CI流程的熟悉，容器周边的各种工具蓬勃发展，俨然形成了一个小生态，在2016年达到顶峰，下面这张是我画的容器生态图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/container-ecosystem.png&#34; alt=&#34;容器生态图 Container ecosystem&#34;&gt;&lt;/p&gt;
&lt;p&gt;该生态涵盖了容器应用中从镜像仓库、服务编排、安全管理、持续集成与发布、存储和网络管理等各个方面，随着在单主机中运行容器的成熟，集群管理和容器编排成为容器技术亟待解决的问题。譬如化学家在实验室中研究出来的新产品，如何推向市场，进行大规模生产，成了新的议题。&lt;/p&gt;
&lt;h2 id=&#34;为什么使用kubernetes&#34;&gt;为什么使用Kubernetes&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Kubernetes——让容器应用进入大规模工业生产。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kubernetes是容器编排系统的事实标准&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在单机上运行容器，无法发挥它的最大效能，只有形成集群，才能最大程度发挥容器的良好隔离、资源分配与编排管理的优势，而对于容器的编排管理，Swarm、Mesos和Kubernetes的大战已经基本宣告结束，kubernetes成为了无可争议的赢家。&lt;/p&gt;
&lt;p&gt;下面这张图是Kubernetes的架构图（图片来自网络），其中显示了组件之间交互的接口CNI、CRI、OCI等，这些将Kubernetes与某款具体产品解耦，给用户最大的定制程度，使得Kubernetes有机会成为跨云的真正的云原生应用的操作系统。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/kubernetes-high-level-component-archtecture.jpg&#34; alt=&#34;Kuberentes架构&#34;&gt;&lt;/p&gt;
&lt;p&gt;随着Kubernetes的日趋成熟，“Kubernetes is becoming boring”，基于该“操作系统”之上构建的适用于不同场景的应用将成为新的发展方向，就像我们将石油开采出来后，提炼出汽油、柴油、沥青等等，所有的材料都将找到自己的用途，Kubernetes也是，毕竟我们谁也不是为了部署和管理容器而用Kubernetes，承载其上的应用才是价值之所在。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;云原生的核心目标&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/cloud-native-core-target.jpg&#34; alt=&#34;Cloud Native Core target&#34;&gt;&lt;/p&gt;
&lt;p&gt;云已经可以为我们提供稳定可以唾手可得的基础设施，但是业务上云成了一个难题，Kubernetes的出现与其说是从最初的容器编排解决方案，倒不如说是为了解决应用上云（即云原生应用）这个难题。&lt;/p&gt;
&lt;p&gt;包括微服务和FaaS/Serverless架构，都可以作为云原生应用的架构。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/redpoint-faas-landscape.jpg&#34; alt=&#34;FaaS Landscape&#34;&gt;&lt;/p&gt;
&lt;p&gt;但就2017年为止，kubernetes的主要使用场景也主要作为应用开发测试环境、CI/CD和运行Web应用这几个领域，如下图&lt;a href=&#34;http://thenewstack.io&#34;&gt;TheNewStack&lt;/a&gt;的Kubernetes生态状况调查报告所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5mxr6fxtj31kw11q484.jpg&#34; alt=&#34;Workloads running on Kubernetes&#34;&gt;&lt;/p&gt;
&lt;p&gt;另外基于Kubernetes的构建PaaS平台和Serverless也处于爆发的准备的阶段，如下图中Gartner的报告中所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5my2jtxzj315o0z8dkr.jpg&#34; alt=&#34;Gartner技术爆发趋势图2017&#34;&gt;&lt;/p&gt;
&lt;p&gt;当前各大公有云如Google GKE、微软Azure ACS、亚马逊EKS（2018年上线）、VmWare、Pivotal、腾讯云、阿里云等都提供了Kuberentes服务。&lt;/p&gt;
&lt;h2 id=&#34;微服务&#34;&gt;微服务&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;微服务——Cloud Native的应用架构。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;下图是&lt;a href=&#34;https://developers.redhat.com/blog/author/bibryam/&#34;&gt;Bilgin Ibryam&lt;/a&gt;给出的微服务中应该关心的主题，图片来自&lt;a href=&#34;https://developers.redhat.com/blog/2016/12/09/spring-cloud-for-microservices-compared-to-kubernetes/&#34;&gt;RedHat Developers&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/microservices-concerns.jpg&#34; alt=&#34;Microservices concerns&#34;&gt;&lt;/p&gt;
&lt;p&gt;微服务带给我们很多开发和部署上的灵活性和技术多样性，但是也增加了服务调用的开销、分布式系统管理、调试与服务治理方面的难题。&lt;/p&gt;
&lt;p&gt;当前最成熟最完整的微服务框架可以说非&lt;a href=&#34;https://spring.io/&#34;&gt;Spring&lt;/a&gt;莫属，而Spring又仅限于Java语言开发，其架构本身又跟Kubernetes存在很多重合的部分，如何探索将Kubernetes作为微服务架构平台就成为一个热点话题。&lt;/p&gt;
&lt;p&gt;就拿微服务中最基础的&lt;strong&gt;服务注册发现&lt;/strong&gt;功能来说，其方式分为&lt;strong&gt;客户端服务发现&lt;/strong&gt;和&lt;strong&gt;服务端服务发现&lt;/strong&gt;两种，Java应用中常用的方式是使用Eureka和Ribbon做服务注册发现和负载均衡，这属于客户端服务发现，而在Kubernetes中则可以使用DNS、Service和Ingress来实现，不需要修改应用代码，直接从网络层面来实现。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/service-discovery-in-microservices.png&#34; alt=&#34;两种服务发现方式&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;cloud-native&#34;&gt;Cloud Native&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;DevOps——通向云原生的云梯&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;CNCF（云原生计算基金会）给出了云原生应用的三大特征：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;容器化包装&lt;/strong&gt;：软件应用的进程应该包装在容器中独立运行。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动态管理&lt;/strong&gt;：通过集中式的编排调度系统来动态的管理和调度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;微服务化&lt;/strong&gt;：明确服务间的依赖，互相解耦。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图是我整理的关于云原生所需要的能力和特征。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/cloud-native-architecutre-mindnode.jpg&#34; alt=&#34;Cloud Native Features&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cncf.io&#34;&gt;CNCF&lt;/a&gt;所托管的应用（目前已达12个），即朝着这个目标发展，其公布的&lt;a href=&#34;https://github.com/cncf/landscape&#34;&gt;Cloud Native Landscape&lt;/a&gt;，给出了云原生生态的参考体系。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5myp6ednj31kw0w0u0x.jpg&#34; alt=&#34;Cloud Native Landscape v1.0&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;使用Kubernetes构建云原生应用&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们都是知道Heroku推出了适用于PaaS的&lt;a href=&#34;https://12factor.net/&#34;&gt;12 factor app&lt;/a&gt;的规范，包括如下要素：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基准代码&lt;/li&gt;
&lt;li&gt;依赖管理&lt;/li&gt;
&lt;li&gt;配置&lt;/li&gt;
&lt;li&gt;后端服务&lt;/li&gt;
&lt;li&gt;构建，发布，运行&lt;/li&gt;
&lt;li&gt;无状态进程&lt;/li&gt;
&lt;li&gt;端口绑定&lt;/li&gt;
&lt;li&gt;并发&lt;/li&gt;
&lt;li&gt;易处理&lt;/li&gt;
&lt;li&gt;开发环境与线上环境等价&lt;/li&gt;
&lt;li&gt;日志作为事件流&lt;/li&gt;
&lt;li&gt;管理进程&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;另外还有补充的三点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;API声明管理&lt;/li&gt;
&lt;li&gt;认证和授权&lt;/li&gt;
&lt;li&gt;监控与告警&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果落实的具体的工具，请看下图，使用Kubernetes构建云原生架构：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/building-cloud-native-architecture-with-kubernetes.png&#34; alt=&#34;Building a Cloud Native Architecture with Kubernetes followed 12 factor app&#34;&gt;&lt;/p&gt;
&lt;p&gt;结合这12因素对开发或者改造后的应用适合部署到Kubernetes之上，基本流程如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/creating-kubernetes-native-app.jpg&#34; alt=&#34;Creating Kubernetes native app&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;迁移到云架构&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;迁移到云端架构，相对单体架构来说会带来很多挑战。比如自动的持续集成与发布、服务监控的变革、服务暴露、权限的管控等。这些具体细节请参考&lt;strong&gt;Kubernetes-handbook&lt;/strong&gt;中的说明：&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34;&gt;https://jimmysong.io/kubernetes-handbook&lt;/a&gt;，在此就不细节展开，另外推荐一本我翻译的由Pivotal出品的电子书——&lt;a href=&#34;https://content.pivotal.io/ebooks/migrating-to-cloud-native-application-architectures&#34;&gt;Migrating to Cloud Native Application Architectures&lt;/a&gt;，地址：&lt;a href=&#34;https://jimmysong.io/migrating-to-cloud-native-application-architectures/&#34;&gt;https://jimmysong.io/migrating-to-cloud-native-application-architectures/&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;service-mesh&#34;&gt;Service Mesh&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Services for show, meshes for a pro.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Kubernetes中的应用将作为微服务运行，但是Kuberentes本身并没有给出微服务治理的解决方案，比如服务的限流、熔断、良好的灰度发布支持等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Service mesh可以用来做什么&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Traffic Management：API网关&lt;/li&gt;
&lt;li&gt;Observability：服务调用和性能分析&lt;/li&gt;
&lt;li&gt;Policy Enforcement：控制服务访问策略&lt;/li&gt;
&lt;li&gt;Service Identity and Security：安全保护&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Service mesh的特点&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;专用的基础设施层&lt;/li&gt;
&lt;li&gt;轻量级高性能网络代理&lt;/li&gt;
&lt;li&gt;提供安全的、快速的、可靠地服务间通讯&lt;/li&gt;
&lt;li&gt;扩展kubernetes的应用负载均衡机制，实现灰度发布&lt;/li&gt;
&lt;li&gt;完全解耦于应用，应用可以无感知，加速应用的微服务和云原生转型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用Service Mesh将可以有效的治理Kuberentes中运行的服务，当前开源的Service Mesh有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linkderd：&lt;a href=&#34;https://linkerd.io&#34;&gt;https://linkerd.io&lt;/a&gt;，由最早提出Service Mesh的公司&lt;a href=&#34;https://buoyant.io&#34;&gt;Buoyant&lt;/a&gt;开源，创始人来自Twitter&lt;/li&gt;
&lt;li&gt;Envoy：&lt;a href=&#34;https://www.envoyproxy.io/&#34;&gt;https://www.envoyproxy.io/&lt;/a&gt;，Lyft开源的，可以在Istio中使用Sidecar模式运行&lt;/li&gt;
&lt;li&gt;Istio：&lt;a href=&#34;https://istio.io&#34;&gt;https://istio.io&lt;/a&gt;，由Google、IBM、Lyft联合开发并开源&lt;/li&gt;
&lt;li&gt;Conduit：&lt;a href=&#34;https://conduit.io&#34;&gt;https://conduit.io&lt;/a&gt;，同样由Buoyant开源的轻量级的基于Kubernetes的Service Mesh&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外还有很多其它的Service Mesh鱼贯而出，请参考&lt;a href=&#34;https://jimmysong.io/awesome-cloud-native&#34;&gt;awesome-cloud-native&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Istio VS Linkerd&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Linkerd和Istio是最早开源的Service Mesh，它们都支持Kubernetes，下面是它们之间的一些特性对比。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Feature&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Istio&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Linkerd&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;部署架构&lt;/td&gt;
&lt;td&gt;Envoy/Sidecar&lt;/td&gt;
&lt;td&gt;DaemonSets&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;易用性&lt;/td&gt;
&lt;td&gt;复杂&lt;/td&gt;
&lt;td&gt;简单&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;支持平台&lt;/td&gt;
&lt;td&gt;kuberentes&lt;/td&gt;
&lt;td&gt;kubernetes/mesos/Istio/local&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;当前版本&lt;/td&gt;
&lt;td&gt;0.3.0&lt;/td&gt;
&lt;td&gt;1.3.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;是否已有生产部署&lt;/td&gt;
&lt;td&gt;否&lt;/td&gt;
&lt;td&gt;是&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;关于两者的架构可以参考各自的官方文档，我只从其在kubernetes上的部署结构来说明其区别。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/istio-vs-linkerd.jpg&#34; alt=&#34;istio vs linkerd&#34;&gt;&lt;/p&gt;
&lt;p&gt;Istio的组件复杂，可以分别部署的kubernetes集群中，但是作为核心路由组件&lt;strong&gt;Envoy&lt;/strong&gt;是以&lt;strong&gt;Sidecar&lt;/strong&gt;形式与应用运行在同一个Pod中的，所有进入该Pod中的流量都需要先经过Envoy。&lt;/p&gt;
&lt;p&gt;Linker的部署十分简单，本身就是一个镜像，使用Kubernetes的&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/concepts/daemonset.html&#34;&gt;DaemonSet&lt;/a&gt;方式在每个node节点上运行。&lt;/p&gt;
&lt;p&gt;更多信息请参考&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34;&gt;kubernetes-handbook&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;使用场景&#34;&gt;使用场景&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Cloud Native的大规模工业生产&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;GitOps&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;给开发者带来最大配置和上线的灵活性，践行DevOps流程，改善研发效率，下图这样的情况将更少发生。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5mzj8rj6j318g1ewtfc.jpg&#34; alt=&#34;Deployment pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;我们知道Kubernetes中的所有应用的部署都是基于YAML文件的，这实际上就是一种&lt;strong&gt;Infrastructure as code&lt;/strong&gt;，完全可以通过Git来管控基础设施和部署环境的变更。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Big Data&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Spark现在已经非官方支持了基于Kuberentes的原生调度，其具有以下特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes原生调度：与yarn、mesos同级&lt;/li&gt;
&lt;li&gt;资源隔离，粒度更细：以namespace来划分用户&lt;/li&gt;
&lt;li&gt;监控的变革：单次任务资源计量&lt;/li&gt;
&lt;li&gt;日志的变革：pod的日志收集&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Feature&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Yarn&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;queue&lt;/td&gt;
&lt;td&gt;queue&lt;/td&gt;
&lt;td&gt;namespace&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;instance&lt;/td&gt;
&lt;td&gt;ExcutorContainer&lt;/td&gt;
&lt;td&gt;Executor Pod&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;td&gt;host&lt;/td&gt;
&lt;td&gt;plugin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;heterogeneous&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;security&lt;/td&gt;
&lt;td&gt;RBAC&lt;/td&gt;
&lt;td&gt;ACL&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;下图是在Kubernetes上运行三种调度方式的spark的单个节点的应用部分对比：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/spark-on-kubernetes-with-different-schedulers.jpg&#34; alt=&#34;Spark on Kubernetes with different schedulers&#34;&gt;&lt;/p&gt;
&lt;p&gt;从上图中可以看到在Kubernetes上使用YARN调度、standalone调度和kubernetes原生调度的方式，每个node节点上的Pod内的spark Executor分布，毫无疑问，使用kubernetes原生调度的spark任务才是最节省资源的。&lt;/p&gt;
&lt;p&gt;提交任务的语句看起来会像是这样的：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;./spark-submit &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --deploy-mode cluster &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --class com.talkingdata.alluxio.hadooptest &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --master k8s://https://172.20.0.113:6443 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --kubernetes-namespace spark-cluster &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.driverEnv.SPARK_USER&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;hadoop &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.driverEnv.HADOOP_USER_NAME&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;hadoop &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executorEnv.HADOOP_USER_NAME&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;hadoop &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executorEnv.SPARK_USER&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;hadoop &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.authenticate.driver.serviceAccountName&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.driver.memory&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;100G &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executor.memory&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;10G &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.driver.cores&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;30&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executor.cores&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.driver.maxResultSize&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;10240m &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.driver.limit.cores&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;32&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.executor.limit.cores&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.executor.memoryOverhead&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;2g &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executor.instances&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;5&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.app.name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark-pi &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.driver.docker.image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark-driver:v2.1.0-kubernetes-0.3.1-1 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.executor.docker.image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark-executor:v2.1.0-kubernetes-0.3.1-1 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.initcontainer.docker.image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark-init:v2.1.0-kubernetes-0.3.1-1 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.resourceStagingServer.uri&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;http://172.20.0.114:31000 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;~/Downloads/tendcloud_2.10-1.0.jar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;关于支持Kubernetes原生调度的Spark请参考：https://jimmysong.io/spark-on-k8s/&lt;/p&gt;
&lt;h2 id=&#34;open-source&#34;&gt;Open Source&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Contributing is Not only about code, it is about helping a community.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;下图是我们刚调研准备使用Kubernetes时候的调研方案选择。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5mzywc83j31fk1i8qg4.jpg&#34; alt=&#34;Kubernetes solutions&#34;&gt;&lt;/p&gt;
&lt;p&gt;对于一个初次接触Kubernetes的人来说，看到这样一个庞大的架构选型时会望而生畏，但是Kubernetes的开源社区帮助了我们很多。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/kubernetes-sigs.jpg&#34; alt=&#34;Kubernetes SIG&#34;&gt;&lt;/p&gt;
&lt;p&gt;我组建了&lt;strong&gt;K8S&amp;amp;Cloud Native实战&lt;/strong&gt;微信群，参与了k8smeetup、KEUC2017、Kubernetes 官方文档的翻译工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;有用的资料和链接&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to&#34;&gt;云原生社区&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34;&gt;Kubernetes Handbook - Kubernetes 和云原生应用架构实践手册&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/awesome-cloud-native/&#34;&gt;Cloud native开源生态&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/book/migrating-to-cloud-native-application-architectures/&#34;&gt;迁移到云原生应用架构|电子书&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
