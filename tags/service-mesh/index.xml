<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jimmy Song - 云原生|开源|社区 – Service Mesh</title>
    <link>https://jimmysong.io/tags/service-mesh/</link>
    <description>Recent content in Service Mesh on Jimmy Song - 云原生|开源|社区</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <copyright>Copyright &amp;copy; 2021 Jimmy Song 保留所有权利；&lt;/br&gt;基于 Hugo [educenter](https://github.com/themefisher/educenter-hugo)  主题构建</copyright>
    <lastBuildDate>Mon, 24 May 2021 08:00:00 +0800</lastBuildDate>
    
	  <atom:link href="https://jimmysong.io/tags/service-mesh/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Istio 开源四周年回顾与展望</title>
      <link>https://jimmysong.io/blog/istio-4-year-birthday/</link>
      <pubDate>Mon, 24 May 2021 08:00:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/istio-4-year-birthday/</guid>
      <description>
        
        
        &lt;p&gt;Istio 是由 &lt;a href=&#34;https://tetrate.io/&#34;&gt;Tetrate&lt;/a&gt; 创始人 Varun Talwar 和谷歌首席工程师 Louis Ryan 命名并在 2017 年 5 月 24 日开源。今天是 Istio 开源四周年，让我们一起来回顾一下 Istio 四年来的发展并展望一下它的未来。&lt;/p&gt;
&lt;h2 id=&#34;istio-的开源历史&#34;&gt;Istio 的开源历史&lt;/h2&gt;
&lt;p&gt;2017 年是 Kubernetes 结束容器编排之战的一年，Google 为了巩固在云原生领域的优势，并弥补 Kubernetes 在服务间流量管理方面的劣势，趁势开源了 Istio。下面是截止目前 Istio 历史上最重要的几次版本发布。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;日期&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;版本&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2017-05-24&lt;/td&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;td&gt;正式开源，该版本发布时仅一个命令行工具。确立了功能范围和 sidecar 部署模式，确立的 Envoy 作为默认 sidecar proxy 的地位。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2017-10-10&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;支持多运行时环境，如虚拟机。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018-06-01&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;API 重构。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018-07-31&lt;/td&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;td&gt;生产就绪，此后 Istio 团队被大规模重组。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019-03-19&lt;/td&gt;
&lt;td&gt;1.1&lt;/td&gt;
&lt;td&gt;企业就绪，支持多 Kubernetes 集群，性能优化。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020-03-03&lt;/td&gt;
&lt;td&gt;1.5&lt;/td&gt;
&lt;td&gt;回归单体架构，支持 WebAssembly 扩展，使得 Istio 的生态更加强大。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020-11-18&lt;/td&gt;
&lt;td&gt;1.8&lt;/td&gt;
&lt;td&gt;正式放弃 Mixer，进一步完善对虚拟机的支持。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Istio 开源后经过了一年时间的发展，在 1.0 发布的前两个月发布了 0.8 版本，这是对 API 的一次大规模重构。而在 2018 年 7 月底发布 1.0 时， Istio 达到了生产可用的临界点，此后 Google 对 Istio 团队进行了大规模重组，多家以 Istio 为基础的 Service Mesh &lt;a href=&#34;https://istio.io/latest/about/ecosystem/#providers&#34;&gt;创业公司&lt;/a&gt;诞生，可以说 2018 年是服务网格行业诞生的元年。&lt;/p&gt;
&lt;p&gt;2019年 3 月 Istio 1.1 发布，而这距离 1.0 发布已经过去了近 9 个月，这已经远远超出一个开源项目的平均发布周期。我们知道迭代和进化速度是基础软件的核心竞争力，此后 Istio 开始以每个季度一个版本的固定&lt;a href=&#34;https://istio.io/v1.7/about/release-cadence/&#34;&gt;发布节奏&lt;/a&gt;，并在 2019 年成为了 &lt;a href=&#34;https://octoverse.github.com/#fastest-growing-oss-projects-by-contributors&#34;&gt;GitHub 增长最快的十大项目中排名第 4 名&lt;/a&gt;！&lt;/p&gt;
&lt;h2 id=&#34;istio-社区&#34;&gt;Istio 社区&lt;/h2&gt;
&lt;p&gt;Istio 开源四年来，已经在 GitHub 上收获了 2.7 万颗星，获得了大量的&lt;a href=&#34;https://istio.io/latest/about/case-studies/&#34;&gt;社区用户&lt;/a&gt;。下图是 &lt;a href=&#34;https://github.com/istio/istio&#34;&gt;Istio&lt;/a&gt; 的 GitHub star 数增长情况。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008i3skNly1gqtm7n2hm1j31me0n2tag.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;2020 年 Istio 的项目管理开始走向成熟，治理方式也到了进化的阶段。2020 年，Istio 社区进行了第一次&lt;a href=&#34;https://istio.io/latest/blog/2020/steering-election-results/&#34;&gt;管委会选举&lt;/a&gt;，还把商标转让给了 &lt;a href=&#34;https://istio.io/latest/blog/2020/open-usage/&#34;&gt;Open Usage Commons&lt;/a&gt;。首届 &lt;a href=&#34;https://events.istio.io/istiocon-2021/&#34;&gt;IstioCon&lt;/a&gt; 在 2021 年 2 月份成功举办，几千人参加了线上会议。在中国也有大量的 Istio 社区用户，2021 年也会有线下面对面的 Istio 社区 meetup 在中国举办。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008i3skNly1gquicfqg14j31lw0smwl2.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;根据 CNCF 2020 年调查，46% 的组织在生产中使用服务网格或计划在未来 12 个月内使用。Istio 是在生产中使用的最多的网格。&lt;/p&gt;
&lt;h2 id=&#34;未来&#34;&gt;未来&lt;/h2&gt;
&lt;p&gt;经过 4 年的发展，围绕 Istio 不仅形成了庞大的用户群，还诞生了多家 Istio 供应商，你可以在最近改版的 &lt;a href=&#34;https://istio.io&#34;&gt;Istio 的官网首页&lt;/a&gt;中看到。在最近几个版本中，Istio 已经将发展中心转移到了提升 Day 2 Operation 体验上来了。我们还希望看到更多的 Istio 的采纳路径建议、案例研究、学习资料、培训及认证（例如来自 Tetrate 的业界的第一个 &lt;a href=&#34;https://academy.tetrate.io/courses/certified-istio-administrator&#34;&gt;Istio 管理员认证&lt;/a&gt;），这些都将有利于 Istio 的推广和采用。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>为什么在使用了 Kubernetes 后你可能还需要 Istio？</title>
      <link>https://jimmysong.io/blog/why-do-you-need-istio-when-you-already-have-kubernetes/</link>
      <pubDate>Wed, 07 Apr 2021 08:27:17 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/why-do-you-need-istio-when-you-already-have-kubernetes/</guid>
      <description>
        
        
        &lt;p&gt;如果你听说过服务网格，并尝试过 &lt;a href=&#34;https://istio.io/&#34;&gt;Istio&lt;/a&gt;，你可能有以下问题。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为什么 Istio 要在 Kubernetes 上运行？&lt;/li&gt;
&lt;li&gt;Kubernetes 和服务网格在云原生应用架构中分别扮演什么角色？&lt;/li&gt;
&lt;li&gt;Istio 扩展了 Kubernetes 的哪些方面？它解决了哪些问题？&lt;/li&gt;
&lt;li&gt;Kubernetes、Envoy 和 Istio 之间是什么关系？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本文将带大家了解 Kubernetes 和 Istio 的内部工作原理。此外，我会介绍 Kubernetes 中的负载均衡方法，并解释为什么有了 Kubernetes 后还需要 Istio。&lt;/p&gt;
&lt;p&gt;Kubernetes 本质上是通过声明式配置来实现应用生命周期管理，而服务网格本质上是提供应用间的流量、安全管理和可观察性。如果你已经使用 Kubernetes 搭建了一个稳定的应用平台，那么如何设置服务间调用的负载均衡和流量控制？是否有这样一个通用的工具或者说平台（非 SDK），可以实现？这就需要用到服务网格了。&lt;/p&gt;
&lt;p&gt;Envoy 引入了 xDS 协议，这个协议得到了各种开源软件的支持，比如 Istio、&lt;a href=&#34;https://mosn.io/&#34;&gt;MOSN&lt;/a&gt; 等。Envoy 将 xDS 贡献给服务网格或云原生基础设施。Envoy 本质上是一个现代版的代理，可以通过 API 进行配置，在此基础上衍生出许多不同的使用场景–比如 API Gateway、服务网格中的侧车代理和边缘代理。&lt;/p&gt;
&lt;p&gt;本文包含以下内容。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kube-proxy 的作用描述。&lt;/li&gt;
&lt;li&gt;Kubernetes 在微服务管理方面的局限性。&lt;/li&gt;
&lt;li&gt;Istio 服务网格的功能介绍。&lt;/li&gt;
&lt;li&gt;Kubernetes、Envoy 和 Istio 服务网格中一些概念的比较。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kubernetes-vs-service-mesh&#34;&gt;Kubernetes vs Service Mesh&lt;/h2&gt;
&lt;p&gt;下图显示了 Kubernetes 中的服务访问关系和服务网格（每个 pod 模型一个 sidecar）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008eGmZEly1gpb7knfo4dj31hk0redrz.jpg&#34; alt=&#34;Kubernetes vs Service Mesh&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;流量转发&#34;&gt;流量转发&lt;/h3&gt;
&lt;p&gt;Kubernetes 集群中的每个节点都部署了一个 kube-proxy 组件，该组件与 Kubernetes API Server 进行通信，获取集群中的服务信息，然后设置 iptables 规则，将服务请求直接发送到对应的 Endpoint（属于同一组服务的 pod）。&lt;/p&gt;
&lt;h3 id=&#34;服务发现&#34;&gt;服务发现&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;008eGmZEly1gpb7knwb79j30kq0fcjs9.jpg&#34; alt=&#34;Service Discovery&#34;&gt;&lt;/p&gt;
&lt;p&gt;Istio 可以跟踪 Kubernetes 中的服务注册，也可以在控制平面中通过平台适配器与其他服务发现系统对接；然后生成数据平面的配置（使用 CRD，这些配置存储在 etcd 中），数据平面的透明代理。数据平面的透明代理以 sidecar 容器的形式部署在每个应用服务的 pod 中，这些代理都需要请求控制平面同步代理配置。代理之所以 “透明”，是因为应用容器完全不知道代理的存在。过程中的 kube-proxy 组件也需要拦截流量，只不过 kube-proxy 拦截的是进出 Kubernetes 节点的流量–而 sidecar 代理拦截的是进出 pod 的流量。&lt;/p&gt;
&lt;h3 id=&#34;服务网格的劣势&#34;&gt;服务网格的劣势&lt;/h3&gt;
&lt;p&gt;由于 Kubernetes 的每个节点上都运行着很多 pod，所以在每个 pod 中放入原有的 kube-proxy 路由转发功能，会增加响应延迟–由于侧车拦截流量时跳数更多，消耗更多的资源。为了对流量进行精细化管理，将增加一系列新的抽象功能。这将进一步增加用户的学习成本，但随着技术的普及，这种情况会慢慢得到缓解。&lt;/p&gt;
&lt;h3 id=&#34;服务网格的优势&#34;&gt;服务网格的优势&lt;/h3&gt;
&lt;p&gt;kube-proxy 的设置是全局的，无法对每个服务进行细粒度的控制，而 service mesh 通过 sidecar proxy 的方式将 Kubernetes 中的流量控制从服务层中抽离出来–可以实现更大的弹性。&lt;/p&gt;
&lt;h3 id=&#34;kube-proxy-的不足之处&#34;&gt;Kube-proxy 的不足之处&lt;/h3&gt;
&lt;p&gt;首先，如果转发的 pod 不能正常服务，它不会自动尝试其他 pod。每个 pod 都有一个健康检查机制，当一个 pod 出现健康问题时，kubelet 会重启 pod，kube-proxy 会删除相应的转发规则。另外，节点 Port 类型的服务不能添加 TLS 或更复杂的消息路由机制。&lt;/p&gt;
&lt;p&gt;Kube-proxy 实现了一个 Kubernetes 服务的多个 pod 实例之间的流量负载均衡，但如何对这些服务之间的流量进行精细化控制–比如将流量按百分比划分给不同的应用版本（这些应用版本都是同一个服务的一部分，但在不同的部署上），或者做金丝雀发布（灰度发布）和蓝绿发布？&lt;/p&gt;
&lt;p&gt;Kubernetes 社区给出了一个使用 Deployment 做&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments&#34;&gt;金丝雀发布&lt;/a&gt;的方法，本质上是通过修改 pod 的标签来给部署的服务分配不同的 pod。&lt;/p&gt;
&lt;h3 id=&#34;kubernetes-ingress-vs-istio-gateway&#34;&gt;Kubernetes Ingress vs Istio Gateway&lt;/h3&gt;
&lt;p&gt;如上所述，kube-proxy 只能在 Kubernetes 集群内路由流量。Kubernetes 集群的 pods 位于 CNI 创建的网络中。一个 ingress—— 一个在 Kubernetes 中创建的资源对象 - 被创建用于集群外部的通信。它由位于 Kubernetes 边缘节点上的入口控制器驱动，负责管理南北向流量。Ingress 必须与各种 Ingress 控制器对接，比如 &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34;&gt;nginx ingress 控制器&lt;/a&gt;和 &lt;a href=&#34;https://traefik.io/&#34;&gt;traefik&lt;/a&gt;。Ingress 只适用于 HTTP 流量，使用简单。它只能通过匹配有限的字段来路由流量–如服务、端口、HTTP 路径等。这使得它无法对 TCP 流量进行路由，如 MySQL、Redis 和各种 RPC。这就是为什么你会看到人们在 ingress 资源注释中写 nginx 配置语言的原因。直接路由南北流量的唯一方法是使用服务的 LoadBalancer 或 NodePort，前者需要云厂商支持，后者需要额外的端口管理。&lt;/p&gt;
&lt;p&gt;Istio Gateway 的功能与 Kubernetes Ingress 类似，它负责进出集群的南北流量。Istio Gateway 描述了一个负载平衡器，用于承载进出网状结构边缘的连接。该规范描述了一组开放端口和这些端口所使用的协议，以及用于负载均衡的 SNI 配置等。Gateway 是一个 CRD 扩展，它也重用了 sidecar 代理的功能；详细配置请参见 &lt;a href=&#34;https://istio.io/latest/docs/reference/config/networking/gateway/&#34;&gt;Istio 网站&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;envoy&#34;&gt;Envoy&lt;/h2&gt;
&lt;p&gt;Envoy 是 Istio 中默认的 sidecar 代理。Istio 基于 Enovy 的 xDS 协议扩展了其控制平面。在讨论 Envoy 的 xDS 协议之前，我们需要先熟悉 Envoy 的基本术语。以下是 Envoy 中的基本术语及其数据结构的列表，更多细节请参考 &lt;a href=&#34;https://envoyproxy.io/&#34;&gt;Envoy 文档&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008eGmZEly1gpb7koah95j31450tetta.jpg&#34; alt=&#34;Envoy&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;基础概念&#34;&gt;基础概念&lt;/h3&gt;
&lt;p&gt;以下是 Enovy 中你应该知道的基本术语。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;下游。下游主机连接到 Envoy，发送请求，并接收响应，即发送请求的主机。&lt;/li&gt;
&lt;li&gt;上游：上游主机。上游主机接收来自 Envoy 的连接和请求，并返回响应；即接收请求的主机。&lt;/li&gt;
&lt;li&gt;Listener：监听器。监听器是一个命名的网络地址（如端口、UNIX 域套接字等）；下游客户端可以连接到这些监听器。Envoy 将一个或多个监听器暴露给下游主机进行连接。&lt;/li&gt;
&lt;li&gt;集群。集群是一组逻辑上相同的上游主机，Envoy 连接到它们。Envoy 通过服务发现来发现集群的成员。可以选择通过主动的健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略来决定集群中哪个成员的请求路由。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 Envoy 中可以设置多个监听器，每个监听器可以设置一个过滤链（过滤链表），而且过滤链是可扩展的，这样我们可以更方便地操纵流量的行为–比如设置加密、私有 RPC 等。&lt;/p&gt;
&lt;p&gt;xDS 协议是由 Envoy 提出的，是 Istio 中默认的 sidecar 代理，但只要实现了 xDS 协议，理论上也可以作为 Istio 中的 sidecar 代理 —— 比如蚂蚁集团开源的 &lt;a href=&#34;https://mosn.io&#34;&gt;MOSN&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cdn.thenewstack.io/media/2021/03/b800bf17-image3.png&#34;&gt;&lt;img src=&#34;008eGmZEly1gpb7kk7wk4j31060lqgqx.jpg&#34; alt=&#34;img&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Istio 是一个功能非常丰富的服务网格，包括以下功能。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;流量管理。这是 Istio 最基本的功能。&lt;/li&gt;
&lt;li&gt;策略控制。实现访问控制系统、遥测采集、配额管理、计费等功能。&lt;/li&gt;
&lt;li&gt;可观察性。在 sidecar 代理中实现。&lt;/li&gt;
&lt;li&gt;安全认证。由 Citadel 组件进行密钥和证书管理。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;istio-中的流量管理&#34;&gt;Istio 中的流量管理&lt;/h2&gt;
&lt;p&gt;Istio 中定义了以下 CRD 来帮助用户进行流量管理。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;网关。网关描述了一个运行在网络边缘的负载均衡器，用于接收传入或传出的 HTTP/TCP 连接。&lt;/li&gt;
&lt;li&gt;虚拟服务（VirtualService）。VirtualService 实际上是将 Kubernetes 服务连接到 Istio 网关。它还可以执行额外的操作，例如定义一组流量路由规则，以便在主机寻址时应用。&lt;/li&gt;
&lt;li&gt;DestinationRule。DestinationRule 定义的策略决定了流量被路由后的访问策略。简单来说，它定义了流量的路由方式。其中，这些策略可以定义为负载均衡配置、连接池大小和外部检测（用于识别和驱逐负载均衡池中不健康的主机）配置。&lt;/li&gt;
&lt;li&gt;EnvoyFilter。EnvoyFilter 对象描述了代理服务的过滤器，可以自定义 Istio Pilot 生成的代理配置。这种配置一般很少被主用户使用。&lt;/li&gt;
&lt;li&gt;ServiceEntry。默认情况下，Istio 服务 Mesh 中的服务无法发现 Mesh 之外的服务。ServiceEntry 可以在 Istio 内部的服务注册表中添加额外的条目，从而允许 Mesh 中自动发现的服务访问并路由到这些手动添加的服务。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kubernetes-vs-xds-vs-istio&#34;&gt;Kubernetes vs xDS vs Istio&lt;/h2&gt;
&lt;p&gt;在回顾了 Kubernetes 的 kube-proxy 组件、xDS 和 Istio 对流量管理的抽象后，现在我们仅从流量管理的角度来看看这三个组件 / 协议的比较（注意，三者并不完全等同）。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;xDS&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Istio service mesh&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Endpoint&lt;/td&gt;
&lt;td&gt;Endpoint&lt;/td&gt;
&lt;td&gt;WorkloadEntry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service&lt;/td&gt;
&lt;td&gt;Route&lt;/td&gt;
&lt;td&gt;VirtualService&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-proxy&lt;/td&gt;
&lt;td&gt;Route&lt;/td&gt;
&lt;td&gt;DestinationRule&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-proxy&lt;/td&gt;
&lt;td&gt;Listener&lt;/td&gt;
&lt;td&gt;EnvoyFilter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ingress&lt;/td&gt;
&lt;td&gt;Listener&lt;/td&gt;
&lt;td&gt;Gateway&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service&lt;/td&gt;
&lt;td&gt;Cluster&lt;/td&gt;
&lt;td&gt;ServiceEntry&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;核心观点&#34;&gt;核心观点&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes 的本质是应用生命周期管理，具体来说就是部署和管理（伸缩、自动恢复、发布）。&lt;/li&gt;
&lt;li&gt;Kubernetes 为微服务提供了一个可扩展、高弹性的部署和管理平台。&lt;/li&gt;
&lt;li&gt;服务网格是基于透明代理，通过侧车代理拦截服务之间的流量，然后通过控制平面配置管理它们的行为。&lt;/li&gt;
&lt;li&gt;服务网格将流量管理与 Kubernetes 解耦，不需要 kube-proxy 组件来支持服务网格内的流量；通过提供更接近微服务应用层的抽象来管理服务间的流量、安全性和可观察性。&lt;/li&gt;
&lt;li&gt;xDS 是服务网格的协议标准之一。&lt;/li&gt;
&lt;li&gt;服务网格是 Kubernetes 中服务的一个更高层次的抽象。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;如果说 Kubernetes 管理的对象是一个 pod，那么服务网格管理的对象就是一个服务，所以用 Kubernetes 管理微服务，然后应用服务网格就可以了。如果你连服务都不想管理，那就用 &lt;a href=&#34;https://knative.dev/&#34;&gt;Knative&lt;/a&gt; 这样的无服务器平台，不过这是后话。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>云原生初学者入门必读</title>
      <link>https://jimmysong.io/blog/must-read-for-cloud-native-beginner/</link>
      <pubDate>Sun, 18 Oct 2020 14:18:40 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/must-read-for-cloud-native-beginner/</guid>
      <description>
        
        
        &lt;h2 id=&#34;为什么写这篇文章&#34;&gt;为什么写这篇文章&lt;/h2&gt;
&lt;p&gt;看到这个标题后，大家可能会问“都已经 2020 年了，Kubernetes 开源有 6 年时间了，为什么还要写一篇 Kubernetes 入门的文章？”我想说的是，Kubernetes 还远远没有达到我们想象的那么普及。众多的开发者，平时忙于各自的业务开发，学习新技术的时间有限；还有大量的学生群体，可能还仅仅停留在“知道有这门技术”的阶段，远远没有入门。这篇文章将助于各位有志于从事云原生领域工作或需要了解该领域背景的人群快速入门 Kubernetes 和云原生。&lt;/p&gt;
&lt;p&gt;因为云原生的知识体系过于庞杂，本文主要讲解容器、Kubernetes 及服务网格的入门概念，关于云原生的更多细节将在后续文章中推出。&lt;/p&gt;
&lt;h2 id=&#34;引言&#34;&gt;引言&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; 一词来自希腊语，意思是“飞行员”或“舵手”。这个名字很贴切，Kubernetes 可以帮助你在波涛汹涌的容器海洋中航行。&lt;/p&gt;
&lt;p&gt;Kubernetes 是做什么的？什么是 Docker？什么是容器编排？Kubernetes 是如何工作和扩展的？你可能还有很多其他的问题，本文将一一为你解答。&lt;/p&gt;
&lt;p&gt;这篇文章适合初学者，尤其是那些工作忙碌，没有办法抽出太多时间来了解 Kubernetes 和云原生的开发者们，希望本文可以帮助你进入 Kubernetes 的世界。&lt;/p&gt;
&lt;p&gt;简而言之，Kubernetes 提供了一个平台或工具来帮助你快速协调或扩展容器化应用，特别是在 &lt;a href=&#34;https://docker.com/&#34;&gt;Docker&lt;/a&gt; 容器。让我们深入了解一下这些概念。&lt;/p&gt;
&lt;h2 id=&#34;容器和容器化&#34;&gt;容器和容器化&lt;/h2&gt;
&lt;p&gt;那么什么是容器呢？&lt;/p&gt;
&lt;p&gt;要讨论容器化首先要谈到虚拟机 (VM)，顾名思义，虚拟机就是可以远程连接的虚拟服务器，比如 AWS 的 EC2 或阿里云的 ECS。&lt;/p&gt;
&lt;p&gt;接下来，假如你要在虚拟机上运行一个网络应用——包括一个 MySQL 数据库、一个 Vue 前端和一些 Java 库，在 Ubuntu 操作系统 (OS) 上运行。你不用熟悉其中的每一个技术——你只要记住，一个应用程序由各种组件、服务和库组成，它们运行在操作系统上。&lt;/p&gt;
&lt;p&gt;现在，将应用程序打包成一个虚拟机镜像，这个镜像中包括了 Ubuntu 操作系统。这使得虚拟机变得非常笨重——通常有几个 G 的大小。&lt;/p&gt;
&lt;p&gt;虚拟机镜像包含了整个操作系统及所有的库，对应用程序来说，这个镜像过于臃肿，其中大部分组件并没有被应用程序直接调用。如果你需要重新创建、备份或扩展这个应用程序，就需要复制整个环境（虚拟机镜像），在新环境中启动应用通常需要几十秒甚至几分钟时间。如果你想单独升级应用中的某个组件，比如说 Vue 应用，就需要重建整个虚拟机镜像。另外，如果你的两个应用依赖同一个底层镜像，升级底层镜像会同时影响这两个应用，而有时候，你只需要升级其中一个应用的依赖而已。这就是所谓的“依赖陷阱”。&lt;/p&gt;
&lt;p&gt;解决这个问题的办法就是容器。容器是继虚拟机之后更高层次的抽象，在这层抽象中，整个应用程序的每个组件被单独打包成一个个独立的单元，这个单元就是所谓的容器。通过这种方式，可以将代码和应用服务从底层架构中分离出来，实现了完全的可移植性（在任何操作系统或环境上运行应用的能力）。所以在上面的例子中，Ubuntu 操作系统就是一个单元（容器）。MySQL 数据库是另一个容器，Vue 环境和随之而来的库也是一个容器。&lt;/p&gt;
&lt;p&gt;但是，MySQL 数据库是如何自己“运行”的？数据库本身肯定也要在操作系统上运行吧？没错！&lt;/p&gt;
&lt;p&gt;更高层次的容器，比如 MySQL 容器，实际上会包含必要的库来与底层的操作系统容器通信和集成。所以你可以把容器看成是整个应用堆栈中的一层，每层都依赖于下层的单元。而这就类似于船舶或港口中集装箱的堆叠方式，每个容器的稳定性都依赖于下面的容器的支持。所以应用容器的核心是一个受控的执行环境。它们允许你从头开始定义整个环境，从操作系统开始，到你要使用的各个版本的库，再到你要添加的代码版本。&lt;/p&gt;
&lt;p&gt;与容器相关的一个重要概念是&lt;strong&gt;微服务&lt;/strong&gt;。将应用程序的各个组件拆分并打包成独立的服务，这样每个组件都可以很容易地被替换、升级、调试。上面的例子中，我们会为 Vue 前端创建一个微服务，为 MySQL 数据库创建另一个微服务，为 Java 中间件部分创建另一个微服务，以此类推。很明显，微服务与容器化是相辅相成的。&lt;/p&gt;
&lt;h2 id=&#34;从-docker-开始&#34;&gt;从 Docker 开始&lt;/h2&gt;
&lt;p&gt;现在你已经对容器有一定了解了吧？Docker 是最常用的容器化工具，也是最流行的容器运行时。&lt;/p&gt;
&lt;p&gt;Docker 开源于 2013 年。用于打包和创建容器，管理基于容器的应用。所有 Linux 发行版、Windows 和 macOS 都支持 Docker。&lt;/p&gt;
&lt;p&gt;还有其他的容器化工具，如 &lt;a href=&#34;https://coreos.com/rkt/&#34;&gt;CoreOS rkt&lt;/a&gt;、&lt;a href=&#34;http://mesos.apache.org/documentation/latest/mesos-containerizer/&#34;&gt;Mesos Containerizer&lt;/a&gt; 和 &lt;a href=&#34;https://linuxcontainers.org/&#34;&gt;LXC&lt;/a&gt;。但是目前，绝大多数的容器化应用都是在 Docker 上运行的。&lt;/p&gt;
&lt;h2 id=&#34;再到-kubernetes&#34;&gt;再到 Kubernetes&lt;/h2&gt;
&lt;p&gt;首先，简单介绍一下历史。Kubernetes 是 Google 基于其内部容器调度平台 Borg 的经验开发的。2014 年开源，并作为 CNCF（云原生计算基金会）的核心发起项目。&lt;/p&gt;
&lt;p&gt;那么 Kubernetes 又跟容器是什么关系呢？让我们再回到上面的例子。假设我们的应用爆火，每天的注册用户越来越多。&lt;/p&gt;
&lt;p&gt;现在，我们需要增加后端资源，使浏览我们网站的用户在浏览页面时加载时间不会过长或者超时。最简单的方式就是增加容器的数量，然后使用负载均衡器将传入的负载（以用户请求的形式）分配给容器。&lt;/p&gt;
&lt;p&gt;这样做虽然行之有效，但也只能在用户规模有限的情况下使用。当用户请求达到几十万或几百万时，这种方法也是不可扩展的。你需要管理几十个也许是几百个负载均衡器，这本身就是另一个令人头疼的问题。如果我们想对网站或应用进行任何升级，也会遇到问题，因为负载均衡不会考虑到应用升级的问题。我们需要单独配置每个负载均衡器，然后升级该均衡器所服务的容器。想象一下，当你有 20 个负载均衡器和每周 5 或 6 个小的更新时，你将不得不进行大量的手工劳动。&lt;/p&gt;
&lt;p&gt;我们需要的是一种可以一次性将变更传递给所有受控容器的方法，同时也需要一种可以轻松地调度可用容器的方法，这个过程还必须要是自动化的，这正是 Kubernetes 所做的事情。&lt;/p&gt;
&lt;p&gt;接下来，我们将探讨 Kubernetes 究竟是如何工作的，它的各种组件和服务，以及更多关于如何使用 Kubernetes 来编排、管理和监控容器化环境。为了简单起见，假设我们使用的是 Docker 容器，尽管如前所述，Kubernetes 除了支持 Docker 之外，还支持其他几种容器平台。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-架构和组件&#34;&gt;Kubernetes 架构和组件&lt;/h2&gt;
&lt;p&gt;首先，最重要的是你需要认识到 Kubernetes 利用了“期望状态”原则。就是说，你定义了组件的期望状态，而 Kubernetes 要将它们始终调整到这个状态。&lt;/p&gt;
&lt;p&gt;例如，你想让你的 Web 服务器始终运行在 4 个容器中，以达到负载均衡的目的，你的数据库复制到 3 个不同的容器中，以达到冗余的目的。这就是你想要的状态。如果这 7 个容器中的任何一个出现故障，Kubernetes 引擎会检测到这一点，并自动创建出一个新的容器，以确保维持所需的状态。&lt;/p&gt;
&lt;p&gt;现在我们来定义一些 Kubernetes 的重要组件。&lt;/p&gt;
&lt;p&gt;当你第一次设置 Kubernetes 时，你会创建一个集群。所有其他组件都是集群的一部分。你也可以创建多个虚拟集群，称为命名空间 (namespace)，它们是同一个物理集群的一部分。这与你可以在同一物理服务器上创建多个虚拟机的方式非常相似。如果你不需要，也没有明确定义的命名空间，那么你的集群将在始终存在的默认命名空间中创建。&lt;/p&gt;
&lt;p&gt;Kubernetes 运行在节点 (node) 上，节点是集群中的单个机器。如果你有自己的硬件，节点可能对应于物理机器，但更可能对应于在云中运行的虚拟机。节点是部署你的应用或服务的地方，是 Kubernetes 工作的地方。有 2 种类型的节点——master 节点和 worker 节点，所以说 Kubernetes 是主从结构的。&lt;/p&gt;
&lt;p&gt;主节点是一个控制其他所有节点的特殊节点。一方面，它和集群中的任何其他节点一样，这意味着它只是另一台机器或虚拟机。另一方面，它运行着控制集群其他部分的软件。它向集群中的所有其他节点发送消息，将工作分配给它们，工作节点向主节点上的 API Server 汇报。&lt;/p&gt;
&lt;p&gt;Master 节点本身也包含一个名为 API Server 的组件。这个 API 是节点与控制平面通信的唯一端点。API Server 至关重要，因为这是 worker 节点和 master 节点就 pod、deployment 和所有其他 Kubernetes API 对象的状态进行通信的点。&lt;/p&gt;
&lt;p&gt;Woker 节点是 Kubernetes 中真正干活的节点。当你在应用中部署容器或 pod（稍后定义）时，其实是在将它们部署到 worker 节点上运行。Worker 节点托管和运行一个或多个容器的资源。&lt;/p&gt;
&lt;p&gt;Kubernetes 中的逻辑而非物理的工作单位称为 pod。一个 pod 类似于 Docker 中的容器。记得我们在前面讲到，容器可以让你创建独立、隔离的工作单元，可以独立运行。但是要创建复杂的应用程序，比如 Web 服务器，你经常需要结合多个容器，然后在一个 pod 中一起运行和管理。这就是 pod 的设计目的——一个 pod 允许你把多个容器，并指定它们如何组合在一起来创建应用程序。而这也进一步明确了 Docker 和 Kubernetes 之间的关系——一个 Kubernetes pod 通常包含一个或多个 Docker 容器，所有的容器都作为一个单元来管理。&lt;/p&gt;
&lt;p&gt;Kubernetes 中的 service 是一组逻辑上的 pod。把一个 service 看成是一个 pod 的逻辑分组，它提供了一个单一的 IP 地址和 DNS 名称，你可以通过它访问服务内的所有 pod。有了服务，就可以非常容易地设置和管理负载均衡，当你需要扩展 Kubernetes pod 时，这对你有很大的帮助，我们很快就会看到。&lt;/p&gt;
&lt;p&gt;ReplicationController 或 ReplicaSet 是 Kubernetes 的另一个关键功能。它是负责实际管理 pod 生命周期的组件——当收到指令时或 pod 离线或意外停止时启动 pod，也会在收到指示时杀死 pod，也许是因为用户负载减少。所以换句话说，ReplicationController 有助于实现我们所期望的指定运行的 pod 数量的状态。&lt;/p&gt;
&lt;h2 id=&#34;什么是-kubectl&#34;&gt;什么是 Kubectl？&lt;/h2&gt;
&lt;p&gt;kubectl 是一个命令行工具，用于与 Kubernetes 集群和其中的 pod 通信。使用它你可以查看集群的状态，列出集群中的所有 pod，进入 pod 中执行命令等。你还可以使用 YAML 文件定义资源对象，然后使用 kubectl 将其应用到集群中。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-中的自动扩展&#34;&gt;Kubernetes 中的自动扩展&lt;/h2&gt;
&lt;p&gt;请记住，我们使用 Kubernetes 而不是直接使用 Docker 的原因之一，是因为 Kubernetes 能够自动扩展应用实例的数量以满足工作负载的需求。&lt;/p&gt;
&lt;p&gt;自动缩放是通过集群设置来实现的，当服务需求增加时，增加节点数量，当需求减少时，则减少节点数量。但也要记住，节点是 “物理” 结构——我们把“物理”放在引号里，因为要记住，很多时候，它们实际上是虚拟机。&lt;/p&gt;
&lt;p&gt;无论如何，节点是物理机器的事实意味着我们的云平台必须允许 Kubernetes 引擎创建新机器。各种云提供商对 Kubernetes 支持基本都满足这一点。&lt;/p&gt;
&lt;p&gt;我们再继续说一些概念，这次是和网络有关的。&lt;/p&gt;
&lt;h2 id=&#34;什么是-kubernetes-ingress-和-egress&#34;&gt;什么是 kubernetes Ingress 和 Egress？&lt;/h2&gt;
&lt;p&gt;外部用户或应用程序与 Kubernetes pod 交互，就像 pod 是一个真正的服务器一样。我们需要设置安全规则允许哪些流量可以进入和离开“服务器”，就像我们为托管应用程序的服务器定义安全规则一样。&lt;/p&gt;
&lt;p&gt;进入 Kubernetes pod 的流量称为 Ingress，而从 pod 到集群外的出站流量称为 egress。我们创建入口策略和出口策略的目的是限制不需要的流量进入和流出服务。而这些策略也是定义 pod 使用的端口来接受传入和传输传出数据 / 流量的地方。&lt;/p&gt;
&lt;h2 id=&#34;什么是-ingress-controller&#34;&gt;什么是 Ingress Controller？&lt;/h2&gt;
&lt;p&gt;但是在定义入口和出口策略之前，你必须首先启动被称为 Ingress Controller（入口控制器）的组件；这个在集群中默认不启动。有不同类型的入口控制器，Kubernetes 项目默认只支持 Google Cloud 和开箱即用的 Nginx 入口控制器。通常云供应商都会提供自己的入口控制器。&lt;/p&gt;
&lt;h2 id=&#34;什么是-replica-和-replicaset&#34;&gt;什么是 Replica 和 ReplicaSet？&lt;/h2&gt;
&lt;p&gt;为了保证应用程序的弹性，需要在不同节点上创建多个 pod 的副本。这些被称为 Replica。假设你所需的状态策略是“让名为 webserver-1 的 pod 始终维持在 3 个副本”，这意味着 ReplicationController 或 ReplicaSet 将监控活动副本的数量，如果其中有任何一个 replica 因任何原因不可用（例如节点的故障），那么 Deployment Controller 将自动创建一个新的系统（定义如下）。&lt;/p&gt;
&lt;p&gt;所需状态是在 deployment 中定义的。 Master 节点的中有一个子系统叫做 Deployment Controller，负责实际执行并使当前状态不断趋向于所需状态。&lt;/p&gt;
&lt;p&gt;因此，举例来说，如果你目前有 2 个 pod 的副本，而你所希望的状态应该有 3 个，那么 Replication Controller 或 ReplicaSet 会自动检测到这个要求，并指示 Deployment Controller 根据预定义的设置部署一个新的 pod。&lt;/p&gt;
&lt;h2 id=&#34;什么是服务网格&#34;&gt;什么是服务网格？&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://jimmysong.io/blog/what-is-a-service-mesh/&#34;&gt;服务网格 (Service Mesh)&lt;/a&gt; 用于管理服务之间的网络流量，是云原生的网络基础设施层，也是 &lt;a href=&#34;https://jimmysong.io/blog/post-kubernetes-era/&#34;&gt;Kubernetes 次世代的云原生应用&lt;/a&gt; 的重要组成部分。&lt;/p&gt;
&lt;p&gt;服务网格利用容器之间的网络设置来控制或改变应用程序中不同组件之间的交互。下面，我们用一个例子来说明。假设你想测试 Nginx 的新版本，检查它是否与你的 Web 应用兼容。你用新的 Nginx 版本创建了一个新的容器 (Container2)，并从当前容器 (Container1) 中复制了当前的 Nginx webserver 配置。但你不想影响组成 web 应用的其他微服务（假设每个容器对应一个单独的微服务）——就是 MySQL 数据库、Node.js 前端、负载均衡器等。&lt;/p&gt;
&lt;p&gt;所以使用服务网格，你可以立即只把 webserver 微服务改成 Container2（新 Nginx 版本的那个）进行测试。如果确定它不能工作，比如因为它导致网站出现一些兼容性问题，那么你就调用服务网格来快速切换回原来的 Container1。而这一切都不需要对其他容器进行任何配置变更——这些变更对其他容器是完全透明的。&lt;/p&gt;
&lt;p&gt;如果没有服务网格，对容器来说这项工作将十分繁琐，因为这涉及到逐一更改所有其他容器上的配置，将它们所包含的服务从 Container1 指向 Container2，然后在测试失败后，将它们全部改回来。&lt;/p&gt;
&lt;p&gt;在前面这部分 Kubernetes 指南中，我们介绍了一些与 Kubernetes 网络相关的概念。Kubernetes 中的网络可能很棘手，很难理解，如果你刚刚开始，你可能需要一些实践来理解这里。关于服务网格的更多内容请参考 &lt;a href=&#34;https://www.servicemesher.com/istio-handbook&#34;&gt;Istio Handbook——Istio 服务网格进阶实战&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;在下一部分中，我们将展开更多关于 Kubernetes 的话题：如何开始学习 Kubernetes，如何在本地安装和测试 Kubernetes，以及 Kubernetes 的一些优秀的监控工具。&lt;/p&gt;
&lt;h2 id=&#34;如何学习-kubernetes&#34;&gt;如何学习 Kubernetes？&lt;/h2&gt;
&lt;p&gt;自学 Kubernetes 知识基本上有三种不同的途径，我们在这里只提供了一个指导大纲。&lt;/p&gt;
&lt;h3 id=&#34;一从零开始学习和安装-kubernetes&#34;&gt;一、从零开始学习和安装 Kubernetes&lt;/h3&gt;
&lt;p&gt;要想真正掌握 Kubernetes，最好的办法莫过于自己从头开始安装 Kubernetes。不过要注意的是，从零开始安装 Kubernetes 并不是一件容易的事情。安装 Kubernetes 并不是简单的“下载文件 -&amp;gt; 点击安装”式的操作，Kubernetes 由多个组件组成，这些组件必须单独安装和配置。而在此之前，你也需要相当的技术储备来做安装前的准备，比如熟悉 Linux 操作系统。如果你决定使用这种方式学习的话，推荐你阅读 &lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook&#34;&gt;Kubernetes Handbook——Kubernetes 中文指南 / 云原生架构实践手册&lt;/a&gt;。此外，请记住，尽管 Kubernetes 作为一个开源解决方案在技术上是免费的，但它确实有一些隐藏的成本，只不过对初学者来说可能并不明显。&lt;/p&gt;
&lt;h3 id=&#34;二kubernetes-自托管解决方案&#34;&gt;二、Kubernetes 自托管解决方案&lt;/h3&gt;
&lt;p&gt;这些解决方案样是一些工具和实用程序，大大简化了在本地计算机上安装和配置小型 Kubernetes 集群的任务。它们是学习 Kubernetes 的好方法，同时对于新手来说也不会太难，又足够小巧可以到安装在个人电脑上。最流行的自托管 Kubernetes 工具和环境是 &lt;a href=&#34;https://github.com/kubernetes/minikube&#34;&gt;Minikube&lt;/a&gt;、&lt;a href=&#34;https://github.com/ubuntu/microk8s&#34;&gt;MicroK8s&lt;/a&gt;、&lt;a href=&#34;https://docs.docker.com/docker-for-windows/kubernetes/&#34;&gt;Docker Desktop&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/kubernetes-sigs/kind&#34;&gt;Kind&lt;/a&gt;。这些解决方案往往有一些限制，例如，Minikube 只允许创建一个节点。尽管有这些缺点，但这些工具还是非常值得推荐，因为它们将易学性和成本效益结合起来，对于刚开始使用 Kubernetes 的初学者来说，是一个很好的选择。&lt;/p&gt;
&lt;h3 id=&#34;三云托管的解决方案&#34;&gt;三、云托管的解决方案&lt;/h3&gt;
&lt;p&gt;如今各大云供应商都提供了定制化的 Kubernetes 解决方案来。你也可以通过线上教学平台如 &lt;a href=&#34;https://katacoda.com/&#34;&gt;Katacoda&lt;/a&gt; 上的免费课程来学习 Kubernetes，它们都是云托管的，你不需要自己安装，只不过你需要云供应商的集群需要付费。&lt;/p&gt;
&lt;h2 id=&#34;本地测试和调试-kubernetes&#34;&gt;本地测试和调试 Kubernetes&lt;/h2&gt;
&lt;p&gt;作为本地安装 Kubernetes 的一部分，你很可能还需要一些测试和调试能力，以确保一切都在顺利运行，特别是定义入口和出口策略等棘手的任务。此外，还有 Kubernetes 附加组件的生态系统，你可能想使用这些组件来扩展 Kubernetes 集群的功能。添加所有这些都需要进行更多的测试，以确保它们能与你的 Kubernetes 集群完美的集成。&lt;/p&gt;
&lt;p&gt;用于在本地开发和调试 Kubernetes 服务的工具有：&lt;a href=&#34;https://github.com/microsoft/mindaro&#34;&gt;Microsoft Bridge to Kubernetes&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/telepresenceio/telepresence&#34;&gt;telepresence&lt;/a&gt;。这些工具可以让你在本地运行单个服务，同时将该服务连接到远程 Kubernetes 集群。这样你就可以让自己的本地机器作为 Kubernetes 集群中的一部分来运行——这对于在本地而不是在生产集群上开发服务非常有用。&lt;/p&gt;
&lt;p&gt;Kubernetes 项目也了解到了 Kubernetes 安装对端到端 (E2E) 测试的需求。为此，项目核心团队一直在确保在最近的版本中更恰当地支持 E2E 测试。这包括诸如允许测试重用和纳入更多附加组件和驱动程序的测试等。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-监控工具&#34;&gt;Kubernetes 监控工具&lt;/h2&gt;
&lt;p&gt;Kubernetes 提供了应用程序在集群的每个层次上的资源使用情况的详细信息——容器、pod、服务。这些详细信息使你能够评估应用程序的性能，确定哪些瓶颈可以解决以提高整体性能。&lt;/p&gt;
&lt;p&gt;毕竟，监控可以帮助你了解应用和集群运行情况的详细信息，这对于学习 Kubernetes 是十分有帮助的。&lt;/p&gt;
&lt;p&gt;Kubernetes 包含两个内置度量收集工具用于监控：&lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/&#34;&gt;资源管道和全度量管道&lt;/a&gt;。资源管道是一个较低级和较有限的工具，主要集中在与各种控制器相关的指标上。全指标管道，顾名思义，从几乎所有集群组件中获取并显示更丰富的指标。&lt;/p&gt;
&lt;p&gt;还有一些第三方工具可以安装并集成到 Kubernetes 集群中。对于 Kubernetes 来说，最普遍使用的两个工具是 Prometheus 和 Grafana。&lt;/p&gt;
&lt;h3 id=&#34;prometheus-监控&#34;&gt;Prometheus 监控&lt;/h3&gt;
&lt;p&gt;Prometheus 是一个功能丰富的开源监控和警报工具。Prometheus 包含一个内部数据存储用来收集指标，如生成的时间序列数据。Prometheus 还拥有众多插件，允许它将数据暴露给各种外部解决方案，并从其他数据源导入数据，包括所有主要公有云监控解决方案。&lt;/p&gt;
&lt;h3 id=&#34;grafana-仪表盘&#34;&gt;Grafana 仪表盘&lt;/h3&gt;
&lt;p&gt;Grafana 是一个优秀的仪表盘、分析和数据可视化工具。它没有 Prometheus 的全功能数据收集能力，但 Prometheus 又没有 Grafana 的数据呈现界面。事实上，他们最好是结合在一起使用——Prometheus 负责数据收集和汇总，Grafana 负责数据展示。它们共同创造了一个强大的组合，涵盖了数据收集、基本警报和可视化。&lt;/p&gt;
&lt;h3 id=&#34;高级警报&#34;&gt;高级警报&lt;/h3&gt;
&lt;p&gt;对于高级警报，你可以添加 &lt;a href=&#34;https://www.nagios.org/&#34;&gt;Nagios&lt;/a&gt; 或 &lt;a href=&#34;https://github.com/prometheus/alertmanager&#34;&gt;Prometheus Alertmanager&lt;/a&gt; 等工具。这些警报工具通常有大量的集成。你可以为自定义值班团队，然后定义你想要监控的参数，例如“当任何 pod 不可用时”或“当任何节点无法访问时”、“当容量达到 90%”等，然后通过电子邮件、短信、手机应用提醒、电话呼叫等方式向值班人员发送自定义通知。你还可以创建升级策略，比如，如果一个被定义为“危急”的警报在 10 分钟内没有值班人员确认，那么就将警报升级（发送警报）到该人员的经理。&lt;/p&gt;
&lt;p&gt;现在，你应该已经对 Docker 和 Kubernetes 有了大体的认识。了解了 Kubernetes 的作用，知道它是如何进行容器化应用部署和管理的。&lt;/p&gt;
&lt;p&gt;调试和监控技术不仅仅是运维需要，你也可以把它当作学习方式。有什么比边做边学更好呢？&lt;/p&gt;
&lt;p&gt;请记住，如果你的应用规模太小，而且预计用户需求不会有太大变化或重大波动（比如一个只在公司内部使用的应用），那么 Kubernetes 对你来说可能没有必要，这种情况下，直接使用 Docker 就足够了。&lt;/p&gt;
&lt;h2 id=&#34;更多&#34;&gt;更多&lt;/h2&gt;
&lt;p&gt;云原生领域的开源项目众多（见 &lt;a href=&#34;https://jimmysong.io/awesome-cloud-native&#34;&gt;Awesome Cloud Native/云原生开源项目大全&lt;/a&gt;），其中有大量的优秀项目可供我们学习。此外，Kubernetes 开源已经多年时间，网上有大量的学习资料，业界出版过很多&lt;a href=&#34;https://jimmysong.io/cloud-native/note/books/&#34;&gt;书籍&lt;/a&gt;，建议大家通过阅读&lt;a href=&#34;https://kubernetes.io&#34;&gt;官方文档&lt;/a&gt;和实践来学习，也可以参考我编写的&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34;&gt;Kubernetes Handbook——Kubernetes 中文指南 / 云原生架构实践手册&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;推荐大家加入我发起创办的&lt;a href=&#34;https://cloudnative.to&#34;&gt;云原生社区&lt;/a&gt;，这是一个立足中国，放眼世界的云原生终端用户社区，致力于云原生技术的传播和应用。云原生社区主办的&lt;a href=&#34;https://github.com/cloudnativeto/academy&#34;&gt;云原生学院&lt;/a&gt;定期邀请云原生和开源领域的大咖在 B 站上进行直播分享，成员自发组织了多个 SIG（特别兴趣小组）进行讨论学习。欢迎加入我们，共同学习和交流云原生技术。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>云原生应用之路</title>
      <link>https://jimmysong.io/blog/from-kubernetes-to-cloud-native/</link>
      <pubDate>Wed, 20 Dec 2017 15:08:02 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/from-kubernetes-to-cloud-native/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;从Kubernetes到Cloud Native——云原生应用之路&lt;/strong&gt;，这是我最近在 &lt;a href=&#34;http://bj2017.archsummit.com/presentation/306&#34;&gt;ArchSummit2017北京站&lt;/a&gt; 和 &lt;a href=&#34;https://www.kubernetes.org.cn/3211.html&#34;&gt;数人云&amp;amp;TalkingData合办的Service Mesh is comming meetup&lt;/a&gt; 中分享的话题。&lt;/p&gt;
&lt;p&gt;本文简要介绍了容器技术发展的路径，为何Kubernetes的出现是容器技术发展到这一步的必然选择，而为何Kuberentes又将成为云原生应用的基石。&lt;/p&gt;
&lt;p&gt;我的分享按照这样的主线展开：容器-&amp;gt;Kubernetes-&amp;gt;微服务-&amp;gt;Cloud Native（云原生）-&amp;gt;Service Mesh（服务网格）-&amp;gt;使用场景-&amp;gt;Open Source（开源）。&lt;/p&gt;
&lt;h2 id=&#34;容器&#34;&gt;容器&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;容器——Cloud Native的基石&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;容器最初是通过开发者工具而流行，可以使用它来做隔离的开发测试环境和持续集成环境，这些都是因为容器轻量级，易于配置和使用带来的优势，docker和docker-compose这样的工具极大的方便的了应用开发环境的搭建，开发者就像是化学家一样在其中小心翼翼的进行各种调试和开发。&lt;/p&gt;
&lt;p&gt;随着容器的在开发者中的普及，已经大家对CI流程的熟悉，容器周边的各种工具蓬勃发展，俨然形成了一个小生态，在2016年达到顶峰，下面这张是我画的容器生态图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/container-ecosystem.png&#34; alt=&#34;容器生态图 Container ecosystem&#34;&gt;&lt;/p&gt;
&lt;p&gt;该生态涵盖了容器应用中从镜像仓库、服务编排、安全管理、持续集成与发布、存储和网络管理等各个方面，随着在单主机中运行容器的成熟，集群管理和容器编排成为容器技术亟待解决的问题。譬如化学家在实验室中研究出来的新产品，如何推向市场，进行大规模生产，成了新的议题。&lt;/p&gt;
&lt;h2 id=&#34;为什么使用kubernetes&#34;&gt;为什么使用Kubernetes&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Kubernetes——让容器应用进入大规模工业生产。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kubernetes是容器编排系统的事实标准&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在单机上运行容器，无法发挥它的最大效能，只有形成集群，才能最大程度发挥容器的良好隔离、资源分配与编排管理的优势，而对于容器的编排管理，Swarm、Mesos和Kubernetes的大战已经基本宣告结束，kubernetes成为了无可争议的赢家。&lt;/p&gt;
&lt;p&gt;下面这张图是Kubernetes的架构图（图片来自网络），其中显示了组件之间交互的接口CNI、CRI、OCI等，这些将Kubernetes与某款具体产品解耦，给用户最大的定制程度，使得Kubernetes有机会成为跨云的真正的云原生应用的操作系统。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/kubernetes-high-level-component-archtecture.jpg&#34; alt=&#34;Kuberentes架构&#34;&gt;&lt;/p&gt;
&lt;p&gt;随着Kubernetes的日趋成熟，“Kubernetes is becoming boring”，基于该“操作系统”之上构建的适用于不同场景的应用将成为新的发展方向，就像我们将石油开采出来后，提炼出汽油、柴油、沥青等等，所有的材料都将找到自己的用途，Kubernetes也是，毕竟我们谁也不是为了部署和管理容器而用Kubernetes，承载其上的应用才是价值之所在。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;云原生的核心目标&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/cloud-native-core-target.jpg&#34; alt=&#34;Cloud Native Core target&#34;&gt;&lt;/p&gt;
&lt;p&gt;云已经可以为我们提供稳定可以唾手可得的基础设施，但是业务上云成了一个难题，Kubernetes的出现与其说是从最初的容器编排解决方案，倒不如说是为了解决应用上云（即云原生应用）这个难题。&lt;/p&gt;
&lt;p&gt;包括微服务和FaaS/Serverless架构，都可以作为云原生应用的架构。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/redpoint-faas-landscape.jpg&#34; alt=&#34;FaaS Landscape&#34;&gt;&lt;/p&gt;
&lt;p&gt;但就2017年为止，kubernetes的主要使用场景也主要作为应用开发测试环境、CI/CD和运行Web应用这几个领域，如下图&lt;a href=&#34;http://thenewstack.io&#34;&gt;TheNewStack&lt;/a&gt;的Kubernetes生态状况调查报告所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5mxr6fxtj31kw11q484.jpg&#34; alt=&#34;Workloads running on Kubernetes&#34;&gt;&lt;/p&gt;
&lt;p&gt;另外基于Kubernetes的构建PaaS平台和Serverless也处于爆发的准备的阶段，如下图中Gartner的报告中所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5my2jtxzj315o0z8dkr.jpg&#34; alt=&#34;Gartner技术爆发趋势图2017&#34;&gt;&lt;/p&gt;
&lt;p&gt;当前各大公有云如Google GKE、微软Azure ACS、亚马逊EKS（2018年上线）、VmWare、Pivotal、腾讯云、阿里云等都提供了Kuberentes服务。&lt;/p&gt;
&lt;h2 id=&#34;微服务&#34;&gt;微服务&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;微服务——Cloud Native的应用架构。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;下图是&lt;a href=&#34;https://developers.redhat.com/blog/author/bibryam/&#34;&gt;Bilgin Ibryam&lt;/a&gt;给出的微服务中应该关心的主题，图片来自&lt;a href=&#34;https://developers.redhat.com/blog/2016/12/09/spring-cloud-for-microservices-compared-to-kubernetes/&#34;&gt;RedHat Developers&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/microservices-concerns.jpg&#34; alt=&#34;Microservices concerns&#34;&gt;&lt;/p&gt;
&lt;p&gt;微服务带给我们很多开发和部署上的灵活性和技术多样性，但是也增加了服务调用的开销、分布式系统管理、调试与服务治理方面的难题。&lt;/p&gt;
&lt;p&gt;当前最成熟最完整的微服务框架可以说非&lt;a href=&#34;https://spring.io/&#34;&gt;Spring&lt;/a&gt;莫属，而Spring又仅限于Java语言开发，其架构本身又跟Kubernetes存在很多重合的部分，如何探索将Kubernetes作为微服务架构平台就成为一个热点话题。&lt;/p&gt;
&lt;p&gt;就拿微服务中最基础的&lt;strong&gt;服务注册发现&lt;/strong&gt;功能来说，其方式分为&lt;strong&gt;客户端服务发现&lt;/strong&gt;和&lt;strong&gt;服务端服务发现&lt;/strong&gt;两种，Java应用中常用的方式是使用Eureka和Ribbon做服务注册发现和负载均衡，这属于客户端服务发现，而在Kubernetes中则可以使用DNS、Service和Ingress来实现，不需要修改应用代码，直接从网络层面来实现。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/service-discovery-in-microservices.png&#34; alt=&#34;两种服务发现方式&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;cloud-native&#34;&gt;Cloud Native&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;DevOps——通向云原生的云梯&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;CNCF（云原生计算基金会）给出了云原生应用的三大特征：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;容器化包装&lt;/strong&gt;：软件应用的进程应该包装在容器中独立运行。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动态管理&lt;/strong&gt;：通过集中式的编排调度系统来动态的管理和调度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;微服务化&lt;/strong&gt;：明确服务间的依赖，互相解耦。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图是我整理的关于云原生所需要的能力和特征。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/cloud-native-architecutre-mindnode.jpg&#34; alt=&#34;Cloud Native Features&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cncf.io&#34;&gt;CNCF&lt;/a&gt;所托管的应用（目前已达12个），即朝着这个目标发展，其公布的&lt;a href=&#34;https://github.com/cncf/landscape&#34;&gt;Cloud Native Landscape&lt;/a&gt;，给出了云原生生态的参考体系。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5myp6ednj31kw0w0u0x.jpg&#34; alt=&#34;Cloud Native Landscape v1.0&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;使用Kubernetes构建云原生应用&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们都是知道Heroku推出了适用于PaaS的&lt;a href=&#34;https://12factor.net/&#34;&gt;12 factor app&lt;/a&gt;的规范，包括如下要素：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基准代码&lt;/li&gt;
&lt;li&gt;依赖管理&lt;/li&gt;
&lt;li&gt;配置&lt;/li&gt;
&lt;li&gt;后端服务&lt;/li&gt;
&lt;li&gt;构建，发布，运行&lt;/li&gt;
&lt;li&gt;无状态进程&lt;/li&gt;
&lt;li&gt;端口绑定&lt;/li&gt;
&lt;li&gt;并发&lt;/li&gt;
&lt;li&gt;易处理&lt;/li&gt;
&lt;li&gt;开发环境与线上环境等价&lt;/li&gt;
&lt;li&gt;日志作为事件流&lt;/li&gt;
&lt;li&gt;管理进程&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;另外还有补充的三点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;API声明管理&lt;/li&gt;
&lt;li&gt;认证和授权&lt;/li&gt;
&lt;li&gt;监控与告警&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果落实的具体的工具，请看下图，使用Kubernetes构建云原生架构：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/building-cloud-native-architecture-with-kubernetes.png&#34; alt=&#34;Building a Cloud Native Architecture with Kubernetes followed 12 factor app&#34;&gt;&lt;/p&gt;
&lt;p&gt;结合这12因素对开发或者改造后的应用适合部署到Kubernetes之上，基本流程如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/creating-kubernetes-native-app.jpg&#34; alt=&#34;Creating Kubernetes native app&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;迁移到云架构&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;迁移到云端架构，相对单体架构来说会带来很多挑战。比如自动的持续集成与发布、服务监控的变革、服务暴露、权限的管控等。这些具体细节请参考&lt;strong&gt;Kubernetes-handbook&lt;/strong&gt;中的说明：&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34;&gt;https://jimmysong.io/kubernetes-handbook&lt;/a&gt;，在此就不细节展开，另外推荐一本我翻译的由Pivotal出品的电子书——&lt;a href=&#34;https://content.pivotal.io/ebooks/migrating-to-cloud-native-application-architectures&#34;&gt;Migrating to Cloud Native Application Architectures&lt;/a&gt;，地址：&lt;a href=&#34;https://jimmysong.io/migrating-to-cloud-native-application-architectures/&#34;&gt;https://jimmysong.io/migrating-to-cloud-native-application-architectures/&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;service-mesh&#34;&gt;Service Mesh&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Services for show, meshes for a pro.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Kubernetes中的应用将作为微服务运行，但是Kuberentes本身并没有给出微服务治理的解决方案，比如服务的限流、熔断、良好的灰度发布支持等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Service mesh可以用来做什么&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Traffic Management：API网关&lt;/li&gt;
&lt;li&gt;Observability：服务调用和性能分析&lt;/li&gt;
&lt;li&gt;Policy Enforcement：控制服务访问策略&lt;/li&gt;
&lt;li&gt;Service Identity and Security：安全保护&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Service mesh的特点&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;专用的基础设施层&lt;/li&gt;
&lt;li&gt;轻量级高性能网络代理&lt;/li&gt;
&lt;li&gt;提供安全的、快速的、可靠地服务间通讯&lt;/li&gt;
&lt;li&gt;扩展kubernetes的应用负载均衡机制，实现灰度发布&lt;/li&gt;
&lt;li&gt;完全解耦于应用，应用可以无感知，加速应用的微服务和云原生转型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用Service Mesh将可以有效的治理Kuberentes中运行的服务，当前开源的Service Mesh有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linkderd：&lt;a href=&#34;https://linkerd.io&#34;&gt;https://linkerd.io&lt;/a&gt;，由最早提出Service Mesh的公司&lt;a href=&#34;https://buoyant.io&#34;&gt;Buoyant&lt;/a&gt;开源，创始人来自Twitter&lt;/li&gt;
&lt;li&gt;Envoy：&lt;a href=&#34;https://www.envoyproxy.io/&#34;&gt;https://www.envoyproxy.io/&lt;/a&gt;，Lyft开源的，可以在Istio中使用Sidecar模式运行&lt;/li&gt;
&lt;li&gt;Istio：&lt;a href=&#34;https://istio.io&#34;&gt;https://istio.io&lt;/a&gt;，由Google、IBM、Lyft联合开发并开源&lt;/li&gt;
&lt;li&gt;Conduit：&lt;a href=&#34;https://conduit.io&#34;&gt;https://conduit.io&lt;/a&gt;，同样由Buoyant开源的轻量级的基于Kubernetes的Service Mesh&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外还有很多其它的Service Mesh鱼贯而出，请参考&lt;a href=&#34;https://jimmysong.io/awesome-cloud-native&#34;&gt;awesome-cloud-native&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Istio VS Linkerd&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Linkerd和Istio是最早开源的Service Mesh，它们都支持Kubernetes，下面是它们之间的一些特性对比。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Feature&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Istio&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Linkerd&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;部署架构&lt;/td&gt;
&lt;td&gt;Envoy/Sidecar&lt;/td&gt;
&lt;td&gt;DaemonSets&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;易用性&lt;/td&gt;
&lt;td&gt;复杂&lt;/td&gt;
&lt;td&gt;简单&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;支持平台&lt;/td&gt;
&lt;td&gt;kuberentes&lt;/td&gt;
&lt;td&gt;kubernetes/mesos/Istio/local&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;当前版本&lt;/td&gt;
&lt;td&gt;0.3.0&lt;/td&gt;
&lt;td&gt;1.3.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;是否已有生产部署&lt;/td&gt;
&lt;td&gt;否&lt;/td&gt;
&lt;td&gt;是&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;关于两者的架构可以参考各自的官方文档，我只从其在kubernetes上的部署结构来说明其区别。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/istio-vs-linkerd.jpg&#34; alt=&#34;istio vs linkerd&#34;&gt;&lt;/p&gt;
&lt;p&gt;Istio的组件复杂，可以分别部署的kubernetes集群中，但是作为核心路由组件&lt;strong&gt;Envoy&lt;/strong&gt;是以&lt;strong&gt;Sidecar&lt;/strong&gt;形式与应用运行在同一个Pod中的，所有进入该Pod中的流量都需要先经过Envoy。&lt;/p&gt;
&lt;p&gt;Linker的部署十分简单，本身就是一个镜像，使用Kubernetes的&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/concepts/daemonset.html&#34;&gt;DaemonSet&lt;/a&gt;方式在每个node节点上运行。&lt;/p&gt;
&lt;p&gt;更多信息请参考&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34;&gt;kubernetes-handbook&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;使用场景&#34;&gt;使用场景&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Cloud Native的大规模工业生产&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;GitOps&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;给开发者带来最大配置和上线的灵活性，践行DevOps流程，改善研发效率，下图这样的情况将更少发生。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5mzj8rj6j318g1ewtfc.jpg&#34; alt=&#34;Deployment pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;我们知道Kubernetes中的所有应用的部署都是基于YAML文件的，这实际上就是一种&lt;strong&gt;Infrastructure as code&lt;/strong&gt;，完全可以通过Git来管控基础设施和部署环境的变更。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Big Data&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Spark现在已经非官方支持了基于Kuberentes的原生调度，其具有以下特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes原生调度：与yarn、mesos同级&lt;/li&gt;
&lt;li&gt;资源隔离，粒度更细：以namespace来划分用户&lt;/li&gt;
&lt;li&gt;监控的变革：单次任务资源计量&lt;/li&gt;
&lt;li&gt;日志的变革：pod的日志收集&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Feature&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Yarn&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;queue&lt;/td&gt;
&lt;td&gt;queue&lt;/td&gt;
&lt;td&gt;namespace&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;instance&lt;/td&gt;
&lt;td&gt;ExcutorContainer&lt;/td&gt;
&lt;td&gt;Executor Pod&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;td&gt;host&lt;/td&gt;
&lt;td&gt;plugin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;heterogeneous&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;security&lt;/td&gt;
&lt;td&gt;RBAC&lt;/td&gt;
&lt;td&gt;ACL&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;下图是在Kubernetes上运行三种调度方式的spark的单个节点的应用部分对比：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/spark-on-kubernetes-with-different-schedulers.jpg&#34; alt=&#34;Spark on Kubernetes with different schedulers&#34;&gt;&lt;/p&gt;
&lt;p&gt;从上图中可以看到在Kubernetes上使用YARN调度、standalone调度和kubernetes原生调度的方式，每个node节点上的Pod内的spark Executor分布，毫无疑问，使用kubernetes原生调度的spark任务才是最节省资源的。&lt;/p&gt;
&lt;p&gt;提交任务的语句看起来会像是这样的：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;./spark-submit &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --deploy-mode cluster &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --class com.talkingdata.alluxio.hadooptest &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --master k8s://https://172.20.0.113:6443 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --kubernetes-namespace spark-cluster &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.driverEnv.SPARK_USER&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;hadoop &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.driverEnv.HADOOP_USER_NAME&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;hadoop &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executorEnv.HADOOP_USER_NAME&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;hadoop &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executorEnv.SPARK_USER&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;hadoop &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.authenticate.driver.serviceAccountName&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.driver.memory&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;100G &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executor.memory&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;10G &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.driver.cores&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;30&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executor.cores&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.driver.maxResultSize&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;10240m &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.driver.limit.cores&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;32&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.executor.limit.cores&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.executor.memoryOverhead&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;2g &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executor.instances&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;5&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.app.name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark-pi &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.driver.docker.image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark-driver:v2.1.0-kubernetes-0.3.1-1 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.executor.docker.image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark-executor:v2.1.0-kubernetes-0.3.1-1 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.initcontainer.docker.image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark-init:v2.1.0-kubernetes-0.3.1-1 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.resourceStagingServer.uri&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;http://172.20.0.114:31000 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;~/Downloads/tendcloud_2.10-1.0.jar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;关于支持Kubernetes原生调度的Spark请参考：https://jimmysong.io/spark-on-k8s/&lt;/p&gt;
&lt;h2 id=&#34;open-source&#34;&gt;Open Source&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Contributing is Not only about code, it is about helping a community.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;下图是我们刚调研准备使用Kubernetes时候的调研方案选择。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5mzywc83j31fk1i8qg4.jpg&#34; alt=&#34;Kubernetes solutions&#34;&gt;&lt;/p&gt;
&lt;p&gt;对于一个初次接触Kubernetes的人来说，看到这样一个庞大的架构选型时会望而生畏，但是Kubernetes的开源社区帮助了我们很多。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/kubernetes-sigs.jpg&#34; alt=&#34;Kubernetes SIG&#34;&gt;&lt;/p&gt;
&lt;p&gt;我组建了&lt;strong&gt;K8S&amp;amp;Cloud Native实战&lt;/strong&gt;微信群，参与了k8smeetup、KEUC2017、Kubernetes 官方文档的翻译工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;有用的资料和链接&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to&#34;&gt;云原生社区&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34;&gt;Kubernetes Handbook - Kubernetes 和云原生应用架构实践手册&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/awesome-cloud-native/&#34;&gt;Cloud native开源生态&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/book/migrating-to-cloud-native-application-architectures/&#34;&gt;迁移到云原生应用架构|电子书&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
