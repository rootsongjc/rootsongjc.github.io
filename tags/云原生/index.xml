<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jimmy Song – 云原生</title>
    <link>https://jimmysong.io/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/</link>
    <description>Recent content in 云原生 on Jimmy Song</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Tue, 16 Apr 2024 12:54:49 +0800</lastBuildDate>
    
	  <atom:link href="https://jimmysong.io/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>深入解读 CNCF 推出的云原生 AI 白皮书</title>
      <link>https://jimmysong.io/blog/cloud-native-ai-whitepaper/</link>
      <pubDate>Tue, 16 Apr 2024 12:54:49 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/cloud-native-ai-whitepaper/</guid>
      <description>
        
        
        &lt;p&gt;2024 年 3 月，在 KubeCon EU 期间，云原生计算基金会（CNCF）发布了首份关于云原生人工智能（CNAI）的详细&lt;a href=&#34;https://www.cncf.io/reports/cloud-native-artificial-intelligence-whitepaper/&#34; title=&#34;白皮书&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;白皮书&lt;/a&gt;
。这份报告详尽地探讨了将云原生技术与人工智能融合的当前状态、面临的挑战、以及未来的发展方向。本文将对这份白皮书的核心内容进行深入解读。&lt;/p&gt;
&lt;h2 id=&#34;什么是云原生-ai&#34;&gt;什么是云原生 AI？&lt;/h2&gt;
&lt;p&gt;云原生 AI 指的是利用云原生技术原则来构建和部署人工智能应用和工作负载的方法。这包括利用微服务、容器化、声明式 API 和持续集成/持续部署（CI/CD）等云原生技术来增强 AI 应用的可扩展性、可复用性和可操作性。&lt;/p&gt;
&lt;p&gt;下图展示了云原生 AI 的架构，图片根据该白皮书重新绘制。&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
    
      
        
          &lt;img src=&#34;https://jimmysong.io/blog/cloud-native-ai-whitepaper/cloud-native-ai.svg&#34; data-img=&#34;/blog/cloud-native-ai-whitepaper/cloud-native-ai.svg&#34; alt=&#34;image&#34; data-caption=&#34;云原生 AI 架构&#34;&gt;
        
      
    
  
  &lt;figcaption&gt;云原生 AI 架构&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;云原生-ai-与云原生技术之间的关系&#34;&gt;云原生 AI 与云原生技术之间的关系&lt;/h2&gt;
&lt;p&gt;云原生技术提供了一个灵活、可扩展的平台，使得开发和运行 AI 应用变得更加高效。通过容器化和微服务架构，开发人员可以快速迭代和部署 AI 模型，同时保证系统的高可用性和可扩展性。Kubernetes 和其他云原生工具提供了必要的支持，例如资源调度、自动扩缩容和服务发现等。&lt;/p&gt;
&lt;p&gt;白皮书中给出了两个例子说明云原生 AI 与云原生技术的关系，即在云原生基础架构上运行 AI：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/hugging-face-endpoints-on-azure&#34; title=&#34;Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openai.com/research/scaling-kubernetes-to-7500-nodes&#34; title=&#34;OpenAI Scaling Kubernetes to 7,500 nodes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI Scaling Kubernetes to 7,500 nodes&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;云原生-ai-的挑战&#34;&gt;云原生 AI 的挑战&lt;/h2&gt;
&lt;p&gt;尽管云原生技术为 AI 应用提供了坚实的基础，但在将 AI 工作负载与云原生平台整合时，仍然存在一些挑战。这些挑战包括数据准备的复杂性、模型训练的资源需求、以及在多租户环境中保持模型的安全性和隔离性。此外，云原生环境中的资源管理和调度对于大规模 AI 应用尤其关键，需要进一步优化以支持高效的模型训练和推理。&lt;/p&gt;
&lt;h2 id=&#34;云原生-ai-的发展路径&#34;&gt;云原生 AI 的发展路径&lt;/h2&gt;
&lt;p&gt;白皮书中提出了几条云原生 AI 的发展路径，包括改进资源调度算法以更好地支持 AI 负载、开发新的服务网格技术以提高 AI 应用的性能和安全性，以及通过开源项目和社区合作来推动云原生 AI 技术的创新和标准化。&lt;/p&gt;
&lt;h2 id=&#34;云原生-ai-技术景观图&#34;&gt;云原生 AI 技术景观图&lt;/h2&gt;
&lt;p&gt;云原生 AI 涉及到多种技术，从容器和微服务到服务网格和无服务器计算。Kubernetes 是部署和管理 AI 应用的关键平台，而 Istio、Envoy 等服务网格技术则提供了强大的流量管理和安全功能。此外，Prometheus 和 Grafana 等监控工具对于维护 AI 应用的性能和可靠性至关重要。&lt;/p&gt;
&lt;p&gt;下面是白皮书中给出的云原生 AI 景观图。&lt;/p&gt;
&lt;h3 id=&#34;general-orchestration&#34;&gt;General Orchestration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes&lt;/li&gt;
&lt;li&gt;Volcano&lt;/li&gt;
&lt;li&gt;Armada&lt;/li&gt;
&lt;li&gt;Kuberay&lt;/li&gt;
&lt;li&gt;Nvidia NeMo&lt;/li&gt;
&lt;li&gt;Yunikorn&lt;/li&gt;
&lt;li&gt;Kueue&lt;/li&gt;
&lt;li&gt;Flame&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;distributed-training&#34;&gt;Distributed Training&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Kubeflow Training Operator&lt;/li&gt;
&lt;li&gt;Pytorch DDP&lt;/li&gt;
&lt;li&gt;TensorFlow Distributed&lt;/li&gt;
&lt;li&gt;Open MPI&lt;/li&gt;
&lt;li&gt;DeepSpeed&lt;/li&gt;
&lt;li&gt;Megatron&lt;/li&gt;
&lt;li&gt;Horovod&lt;/li&gt;
&lt;li&gt;Apla&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ml-serving&#34;&gt;ML Serving&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Kserve&lt;/li&gt;
&lt;li&gt;Seldon&lt;/li&gt;
&lt;li&gt;VLLM&lt;/li&gt;
&lt;li&gt;TGT&lt;/li&gt;
&lt;li&gt;Skypilot&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cicd---delivery&#34;&gt;CI/CD - Delivery&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Kubeflow Pipelines&lt;/li&gt;
&lt;li&gt;Mlflow&lt;/li&gt;
&lt;li&gt;TFX&lt;/li&gt;
&lt;li&gt;BentoML&lt;/li&gt;
&lt;li&gt;MLRun&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-science&#34;&gt;Data Science&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Jupyter&lt;/li&gt;
&lt;li&gt;Kubeflow Notebooks&lt;/li&gt;
&lt;li&gt;PyTorch&lt;/li&gt;
&lt;li&gt;TensorFlow&lt;/li&gt;
&lt;li&gt;Apache Zeppelin&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;workload-observability&#34;&gt;Workload Observability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Prometheus&lt;/li&gt;
&lt;li&gt;Influxdb&lt;/li&gt;
&lt;li&gt;Grafana&lt;/li&gt;
&lt;li&gt;Weights and Biases (wandb)&lt;/li&gt;
&lt;li&gt;OpenTelemetry&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;automl&#34;&gt;AutoML&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Hyperopt&lt;/li&gt;
&lt;li&gt;Optuna&lt;/li&gt;
&lt;li&gt;Kubeflow Katib&lt;/li&gt;
&lt;li&gt;NNI&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;governance--policy&#34;&gt;Governance &amp;amp; Policy&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Kyverno&lt;/li&gt;
&lt;li&gt;Kyverno-JSON&lt;/li&gt;
&lt;li&gt;OPA/Gatekeeper&lt;/li&gt;
&lt;li&gt;StackRox Minder&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-architecture&#34;&gt;Data Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ClickHouse&lt;/li&gt;
&lt;li&gt;Apache Pinot&lt;/li&gt;
&lt;li&gt;Apache Druid&lt;/li&gt;
&lt;li&gt;Cassandra&lt;/li&gt;
&lt;li&gt;ScyllaDB&lt;/li&gt;
&lt;li&gt;Hadoop HDFS&lt;/li&gt;
&lt;li&gt;Apache HBase&lt;/li&gt;
&lt;li&gt;Presto&lt;/li&gt;
&lt;li&gt;Trino&lt;/li&gt;
&lt;li&gt;Apache Spark&lt;/li&gt;
&lt;li&gt;Apache Flink&lt;/li&gt;
&lt;li&gt;Kafka&lt;/li&gt;
&lt;li&gt;Pulsar&lt;/li&gt;
&lt;li&gt;Fluid&lt;/li&gt;
&lt;li&gt;Memcached&lt;/li&gt;
&lt;li&gt;Redis&lt;/li&gt;
&lt;li&gt;Alluxio&lt;/li&gt;
&lt;li&gt;Apache Superset&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;vector-databases&#34;&gt;Vector Databases&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Milvus&lt;/li&gt;
&lt;li&gt;Chroma&lt;/li&gt;
&lt;li&gt;Weaviate&lt;/li&gt;
&lt;li&gt;Quadrant&lt;/li&gt;
&lt;li&gt;Pinecone&lt;/li&gt;
&lt;li&gt;Extensions
&lt;ul&gt;
&lt;li&gt;Redis&lt;/li&gt;
&lt;li&gt;Postgres SQL&lt;/li&gt;
&lt;li&gt;ElasticSearch&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;modelllm-observability&#34;&gt;Model/LLM Observability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Trulens&lt;/li&gt;
&lt;li&gt;Langfuse&lt;/li&gt;
&lt;li&gt;Deepchecks&lt;/li&gt;
&lt;li&gt;OpenLLMetry&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;最后，笔者梳理了以下关键观点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;开源社区的推动作用&lt;/strong&gt;：白皮书明确指出开源社区对云原生 AI 进步的推动作用，其中包括通过开源项目和广泛的合作来加速创新和降低成本。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;云原生技术的重要性&lt;/strong&gt;：云原生 AI 是按照云原生原则构建和部署的，突出了可重复性和可扩展性的重要性。云原生技术为 AI 应用提供了高效的开发和运行环境，特别是在资源调度和服务可伸缩性方面。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;存在的挑战&lt;/strong&gt;：尽管云原生 AI 带来了诸多优势，但在数据准备、模型训练资源需求以及模型安全性和隔离性方面，仍面临不少挑战。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;未来发展方向&lt;/strong&gt;：白皮书提出的发展路径包括优化资源调度算法以支持 AI 负载，开发新的服务网格技术以提升性能和安全性，以及利用开源项目和社区合作进一步促进技术创新和标准化。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;关键技术组件&lt;/strong&gt;：云原生 AI 涉及的关键技术包括容器、微服务、服务网格和无服务器计算等，其中 Kubernetes 扮演着 AI 应用部署和管理的中心角色，Istio 和 Envoy 等服务网格技术提供了必要的流量管理和安全保障。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有关详情，请下载 &lt;a href=&#34;https://www.cncf.io/reports/cloud-native-artificial-intelligence-whitepaper/&#34; title=&#34;云原生 AI 白皮书&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;云原生 AI 白皮书&lt;/a&gt;
。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>“寒武纪大爆发”之后的云原生，2021 年走向何处？</title>
      <link>https://jimmysong.io/blog/cloud-native-2021/</link>
      <pubDate>Thu, 28 Jan 2021 08:34:40 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/cloud-native-2021/</guid>
      <description>
        
        
        &lt;p&gt;很荣幸收到 CSDN 的邀请，接受”云原生人物志“专栏采访，其实我从 2017 年起就已经在撰写 &lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/appendix/kubernetes-and-cloud-native-summary-in-2017-and-outlook-for-2018.html&#34; title=&#34;Kubernetes 和云原生年度总结和新年展望&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes 和云原生年度总结和新年展望&lt;/a&gt;
，今天在此聊抒己见，欢迎大家讨论和指正。&lt;/p&gt;
&lt;h2 id=&#34;云原生在演进&#34;&gt;云原生在演进&lt;/h2&gt;
&lt;p&gt;云原生是一种行为方式和设计理念，究其本质，凡是能够提高云上资源利用率和应用交付效率的行为或方式都是云原生的。云计算的发展史就是一部云原生化的历史。Kubernetes 开启了云原生 1.0 的序幕，服务网格 Istio 的出现，引领了后 Kubernetes 时代的微服务，serverless 的再次兴起，使得&lt;strong&gt;云原生从基础设施层不断向应用架构层挺进&lt;/strong&gt;，我们正处于一个云原生 2.0 的新时代。&lt;/p&gt;
&lt;h2 id=&#34;业界动向&#34;&gt;业界动向&lt;/h2&gt;
&lt;p&gt;最近国内的一些云厂商，如阿里云、腾讯云、华为云陆续发布了各自的云原生相关的架构和实践白皮书。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2020 年 7，中国信通院发布了《云原生产业白皮书（2020）》。&lt;/li&gt;
&lt;li&gt;2020 年 12 月 20 日，在腾讯 2020 Techo Park 开发者大会上，腾讯云正式发布了《云原生最佳实践路线图》，同时发布的还有一份 3 万多字的《腾讯云原生路线图手册》。&lt;/li&gt;
&lt;li&gt;2020 年 12 月 23 日，阿里云原生实战峰会上发布了《云原生架构白皮书》。&lt;/li&gt;
&lt;li&gt;2020 年 12 月 30 日，华为云在深圳的 TechWave 云原生 2.0 技术峰会上联合 Forrester 发布了《云原生白皮书：拥抱云原生优先战略》。&lt;/li&gt;
&lt;li&gt;2021 年初，阿里巴巴达摩院发布 2021 十大科技趋势，其中将“云原生重塑 IT 技术体系”作为 2021 年技术预测之一。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;云原生项目的寒武纪大爆发&#34;&gt;云原生项目的“寒武纪大爆发”&lt;/h2&gt;
&lt;p&gt;云原生已历经”寒武纪大爆发“，标志是从 2018 年 Kubernetes 毕业 后走向深耕路线。云原生领域的开源项目层出不穷，令人眼花缭乱，见我收集的 Awesome Cloud Native。&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
    
      
        
          
          &lt;picture&gt;
           &lt;source srcset=&#34;https://jimmysong.io/blog/cloud-native-2021/008eGmZEly1gn37vq5g81j30q906dmyk_hu77e1435a9e181e9cc063a70f81a805ab_28410_945x229_resize_q75_h2_lanczos.webp&#34; type=&#34;image/webp&#34;&gt;
           &lt;img src=&#34;https://jimmysong.io/blog/cloud-native-2021/008eGmZEly1gn37vq5g81j30q906dmyk.jpg&#34; data-img=&#34;/blog/cloud-native-2021/008eGmZEly1gn37vq5g81j30q906dmyk.jpg&#34; data-width=&#34;945&#34; data-height=&#34;229&#34; alt=&#34;image&#34; data-caption=&#34;云原生发展阶段&#34;&gt;
          &lt;/picture&gt;
        
      
    
  
  &lt;figcaption&gt;云原生发展阶段&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;2020 年 CNCF 共接纳了 35 个项目加入基金会，并且有多个项目毕业或晋级，CNCF 托管的项目总数达到了 80 多个。&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
    
      
        
          
          &lt;picture&gt;
           &lt;source srcset=&#34;https://jimmysong.io/blog/cloud-native-2021/008eGmZEly1gn37weeu5lj30q90ivalh_huc75e20abcc0f566c75d569880b55a916_150398_945x679_resize_q75_h2_lanczos.webp&#34; type=&#34;image/webp&#34;&gt;
           &lt;img src=&#34;https://jimmysong.io/blog/cloud-native-2021/008eGmZEly1gn37weeu5lj30q90ivalh.jpg&#34; data-img=&#34;/blog/cloud-native-2021/008eGmZEly1gn37weeu5lj30q90ivalh.jpg&#34; data-width=&#34;945&#34; data-height=&#34;679&#34; alt=&#34;image&#34; data-caption=&#34;图片来自 CNCF 年度报告 2020&#34;&gt;
          &lt;/picture&gt;
        
      
    
  
  &lt;figcaption&gt;图片来自 CNCF 年度报告 2020&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;云原生之争实际上是标准之争&#34;&gt;云原生之争实际上是标准之争&lt;/h2&gt;
&lt;p&gt;PC 端操作系统 Windows 占据上风，移动端是 iOS 和 Android，服务器端是 Linux，而云计算商用分布式操作系统呢？答案是 Kubernetes。&lt;/p&gt;
&lt;p&gt;2020 年 Kubernete 宣布将&lt;a href=&#34;https://blog.csdn.net/csdnnews/article/details/110520682&#34; title=&#34;在 v1.20 版本之后弃用 Docker&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;在 v1.20 版本之后弃用 Docker&lt;/a&gt;
，实际上 Docker 本来就不是 Kubernetes 中默认和唯一的的容器运行时了，实际上只要是支持 CRI（Container Runtime Interface）或 OCI（Open Container Initiative）标准的容器运行时都可以在 Kubernetes 中运行。如下图所示，容器，英文是 container，也是集装箱的意思，其实集装箱不止一种型号，根据运送的货物的不同特性可以制定了多种集装箱类型。而这个容器类型是标准只能是由 Kubernetes 来定，否则只能是削足适履。&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
    
      
        
          
          &lt;picture&gt;
           &lt;source srcset=&#34;https://jimmysong.io/blog/cloud-native-2021/008eGmZEly1gn37vqbb2lj30q90enwpl_hudd11a7ec3825c26e9f43f83125d0c7cf_85025_945x527_resize_q75_h2_lanczos.webp&#34; type=&#34;image/webp&#34;&gt;
           &lt;img src=&#34;https://jimmysong.io/blog/cloud-native-2021/008eGmZEly1gn37vqbb2lj30q90enwpl.jpg&#34; data-img=&#34;/blog/cloud-native-2021/008eGmZEly1gn37vqbb2lj30q90enwpl.jpg&#34; data-width=&#34;945&#34; data-height=&#34;527&#34; alt=&#34;image&#34; data-caption=&#34;各种容器类型&#34;&gt;
          &lt;/picture&gt;
        
      
    
  
  &lt;figcaption&gt;各种容器类型&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Kubernetes 统一了云上的资源对象制定和调度的标准，只要在其标准之上开发 CRD 和 Operator 即可。但是这也仅限于单个应用的管理，如何管理复杂的多集群和混合云环境，如何管理应用间流量，如何如何保证调用链的安全？以 Istio 为代表的服务网格就是为了解决这个问题。&lt;/p&gt;
&lt;h2 id=&#34;云原生趋势云上应用管理&#34;&gt;云原生趋势：云上应用管理&lt;/h2&gt;
&lt;p&gt;Kubernetes 奠定了云原生基础设施的基础，随着而来的监控、存储、AI、大数据等技术的迁移，从单个应用层面来说已经日趋成熟，而在&lt;strong&gt;使用云原生架构尤其是对云上应用的管理&lt;/strong&gt;，而在异构环境、多集群、混合云等已成为常态的情况下，&lt;strong&gt;如何对云上的应用进行管理，成为棘手的事情&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;Kubernetes 以其开创新的声明式 API 和调节器模式，奠定了云原生的基础。我们看到 Google 的项目 Anthos，Azure 的 Arc，AWS 最近开源的 EKS-D，它们都是着重在混合云管理，让云无处不在。另外，服务网格（Service Mesh）经过两年的推广和发酵，将会看到越来越多的应用。&lt;/p&gt;
&lt;h2 id=&#34;云原生与开源社区&#34;&gt;云原生与开源社区&lt;/h2&gt;
&lt;p&gt;目前&lt;strong&gt;企业云原生化转型最缺乏的东西 —— 套路和组合拳&lt;/strong&gt;。对于基础软件，企业往往会选择开源项目并根据自身需求进行改造，而云原生的开源项目又有很多，企业不是没有选择，而是选择太多，以致于无从下手。就像下面教你如何画猫头鹰的示例。我们可以将企业的云原生化的愿景想象成是这只猫头鹰，这些开源项目就像步骤一中圆，你可能想当然的认为只要用了 Kubernetes 就是云原生了，这就像画了两个圆，而剩余部分没有人教你如何完成。&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
    
      
        
          
          &lt;picture&gt;
           &lt;source srcset=&#34;https://jimmysong.io/blog/cloud-native-2021/008eGmZEly1gn37vqshfnj30q90hh44y_hu18e77de0aeb9907df74ac42d6b90d8de_36525_945x629_resize_q75_h2_lanczos.webp&#34; type=&#34;image/webp&#34;&gt;
           &lt;img src=&#34;https://jimmysong.io/blog/cloud-native-2021/008eGmZEly1gn37vqshfnj30q90hh44y.jpg&#34; data-img=&#34;/blog/cloud-native-2021/008eGmZEly1gn37vqshfnj30q90hh44y.jpg&#34; data-width=&#34;945&#34; data-height=&#34;629&#34; alt=&#34;image&#34; data-caption=&#34;如何画猫头鹰&#34;&gt;
          &lt;/picture&gt;
        
      
    
  
  &lt;figcaption&gt;如何画猫头鹰&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;开源社区的核心是面向开发者，就是向开发者灌输如何来画好这只“猫头鹰”的。开源不意味着免费和做慈善，使用开源也是有代价的。&lt;strong&gt;开源社区存在的意义是平衡开发者、终端用户及供应商之间的共同利益&lt;/strong&gt;，而一个中立的开源社区有利于发挥开源的生态优势。&lt;/p&gt;
&lt;p&gt;近年来随着云原生大热，在美国诞生了大量该领域的初创公司，他们基于 AWS、谷歌云、Azure 等提供各种云原生的解决方案，从每次 KubeCon 的赞助商规模上就可以窥知一二。国内该领域的公司目前还不多，而云原生终端用户社区的公司规模上依然跟国外的公司数量有不小的差距。&lt;/p&gt;
&lt;p&gt;云原生社区就是在这样的背景下于 2020 年初由我发起，开始筹备并在 5 月 12 号正式成立，致力于推广云原生技术，构建开发者生态。云原生社区采取 SIG（特别兴趣小组）和 WG（工作组）的组织形式，基于开源项目和不同的专业领域构建研讨组，与厂商合作定期举办线下 meetup，并邀请社区的专家们定期在 B 站的云原生学院进行直播。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;开源应该关注的是终端用户和开发者生态，用 Apache Way 来说就是“社区大于代码”，没有社区的项目是难以长久的。因此我们可以看到国内一些云厂商开源项目之后也会积极投入运营，举行各种各样的活动。我们看到在云原生的推广过程中，CNCF 起到的相当大的作用，2020 年国内也有类似的基金会成立，我们希望看到更多中立的基金会和社区的成立，更多的厂商参与其中，为终端用户提供更佳的解决方案。&lt;/p&gt;
&lt;p&gt;最后感谢 CSDN 宋慧编辑和「CSDN 云计算」的邀请。&lt;/p&gt;
&lt;p&gt;往期报道见：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/csdnnews/article/details/112293560&#34; title=&#34;梁胜：做开源项目的贡献者没有意义&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;梁胜：做开源项目的贡献者没有意义&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/FL63Zv9Zou86950w/article/details/110433443&#34; title=&#34;华为云 CTO 张宇昕：云原生已经进入深水区&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;华为云 CTO 张宇昕：云原生已经进入深水区&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/csdnnews/article/details/110508201&#34; title=&#34;APISIX 温铭：开源的本质是要拿开发者的杠杆&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;APISIX 温铭：开源的本质是要拿开发者的杠杆&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;个人介绍&#34;&gt;个人介绍&lt;/h2&gt;
&lt;p&gt;在我的职业生涯里先后从事过 Java 开发、大数据运维、DevOps、开源管理等工作，个人爱好是研究并推广开源技术及理念，摄影和旅行。目前在企业级服务网格初创公司 Tetrate 担任 Developer Advocate，同时作为中立的云原生终端用户社区 —— 云原生社区（Cloud Native Community）的负责人。&lt;/p&gt;
&lt;p&gt;我的整个职业生涯都是与开源息息相关的，渊源可以追溯到大学时期。大学时我就开始使用 Linux 系统（Ubuntu）学习，刚进入职场的时候面向的也是 Hadoop 的开源生态及各种开源中间件，2015 起开始接触 Docker，2016 年开始进入云原生领域，2017 年开始写 Kubernetes 领域的第一本开源中文电子书《&lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook&#34; title=&#34;Kubernetes Handbook——Kubernetes 中文指南 / 云原生应用架构实践手册&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes Handbook——Kubernetes 中文指南 / 云原生应用架构实践手册&lt;/a&gt;
》，本书直到如今仍在更新，2018 年在蚂蚁集团做开源管理及服务网格社区 ServiceMesher，2020 年加入基于 Istio、Envoy 和 Apache SkyWalking 等开源项目而构建企业级服务网格的初创公司 Tetrate。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>云原生初学者入门必读</title>
      <link>https://jimmysong.io/blog/must-read-for-cloud-native-beginner/</link>
      <pubDate>Sun, 18 Oct 2020 14:18:40 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/must-read-for-cloud-native-beginner/</guid>
      <description>
        
        
        &lt;h2 id=&#34;为什么写这篇文章&#34;&gt;为什么写这篇文章&lt;/h2&gt;
&lt;p&gt;看到这个标题后，大家可能会问“都已经 2020 年了，Kubernetes 开源有 6 年时间了，为什么还要写一篇 Kubernetes 入门的文章？”我想说的是，Kubernetes 还远远没有达到我们想象的那么普及。众多的开发者，平时忙于各自的业务开发，学习新技术的时间有限；还有大量的学生群体，可能还仅仅停留在“知道有这门技术”的阶段，远远没有入门。这篇文章将助于各位有志于从事云原生领域工作或需要了解该领域背景的人群快速入门 Kubernetes 和云原生。&lt;/p&gt;
&lt;p&gt;因为云原生的知识体系过于庞杂，本文主要讲解容器、Kubernetes 及服务网格的入门概念，关于云原生的更多细节将在后续文章中推出。&lt;/p&gt;
&lt;h2 id=&#34;引言&#34;&gt;引言&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/&#34; title=&#34;Kubernetes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes&lt;/a&gt;
 一词来自希腊语，意思是“飞行员”或“舵手”。这个名字很贴切，Kubernetes 可以帮助你在波涛汹涌的容器海洋中航行。&lt;/p&gt;
&lt;p&gt;Kubernetes 是做什么的？什么是 Docker？什么是容器编排？Kubernetes 是如何工作和扩展的？你可能还有很多其他的问题，本文将一一为你解答。&lt;/p&gt;
&lt;p&gt;这篇文章适合初学者，尤其是那些工作忙碌，没有办法抽出太多时间来了解 Kubernetes 和云原生的开发者们，希望本文可以帮助你进入 Kubernetes 的世界。&lt;/p&gt;
&lt;p&gt;简而言之，Kubernetes 提供了一个平台或工具来帮助你快速协调或扩展容器化应用，特别是在 &lt;a href=&#34;https://docker.com/&#34; title=&#34;Docker&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Docker&lt;/a&gt;
 容器。让我们深入了解一下这些概念。&lt;/p&gt;
&lt;h2 id=&#34;容器和容器化&#34;&gt;容器和容器化&lt;/h2&gt;
&lt;p&gt;那么什么是容器呢？&lt;/p&gt;
&lt;p&gt;要讨论容器化首先要谈到虚拟机 (VM)，顾名思义，虚拟机就是可以远程连接的虚拟服务器，比如 AWS 的 EC2 或阿里云的 ECS。&lt;/p&gt;
&lt;p&gt;接下来，假如你要在虚拟机上运行一个网络应用——包括一个 MySQL 数据库、一个 Vue 前端和一些 Java 库，在 Ubuntu 操作系统 (OS) 上运行。你不用熟悉其中的每一个技术——你只要记住，一个应用程序由各种组件、服务和库组成，它们运行在操作系统上。&lt;/p&gt;
&lt;p&gt;现在，将应用程序打包成一个虚拟机镜像，这个镜像中包括了 Ubuntu 操作系统。这使得虚拟机变得非常笨重——通常有几个 G 的大小。&lt;/p&gt;
&lt;p&gt;虚拟机镜像包含了整个操作系统及所有的库，对应用程序来说，这个镜像过于臃肿，其中大部分组件并没有被应用程序直接调用。如果你需要重新创建、备份或扩展这个应用程序，就需要复制整个环境（虚拟机镜像），在新环境中启动应用通常需要几十秒甚至几分钟时间。如果你想单独升级应用中的某个组件，比如说 Vue 应用，就需要重建整个虚拟机镜像。另外，如果你的两个应用依赖同一个底层镜像，升级底层镜像会同时影响这两个应用，而有时候，你只需要升级其中一个应用的依赖而已。这就是所谓的“依赖陷阱”。&lt;/p&gt;
&lt;p&gt;解决这个问题的办法就是容器。容器是继虚拟机之后更高层次的抽象，在这层抽象中，整个应用程序的每个组件被单独打包成一个个独立的单元，这个单元就是所谓的容器。通过这种方式，可以将代码和应用服务从底层架构中分离出来，实现了完全的可移植性（在任何操作系统或环境上运行应用的能力）。所以在上面的例子中，Ubuntu 操作系统就是一个单元（容器）。MySQL 数据库是另一个容器，Vue 环境和随之而来的库也是一个容器。&lt;/p&gt;
&lt;p&gt;但是，MySQL 数据库是如何自己“运行”的？数据库本身肯定也要在操作系统上运行吧？没错！&lt;/p&gt;
&lt;p&gt;更高层次的容器，比如 MySQL 容器，实际上会包含必要的库来与底层的操作系统容器通信和集成。所以你可以把容器看成是整个应用堆栈中的一层，每层都依赖于下层的单元。而这就类似于船舶或港口中集装箱的堆叠方式，每个容器的稳定性都依赖于下面的容器的支持。所以应用容器的核心是一个受控的执行环境。它们允许你从头开始定义整个环境，从操作系统开始，到你要使用的各个版本的库，再到你要添加的代码版本。&lt;/p&gt;
&lt;p&gt;与容器相关的一个重要概念是&lt;strong&gt;微服务&lt;/strong&gt;。将应用程序的各个组件拆分并打包成独立的服务，这样每个组件都可以很容易地被替换、升级、调试。上面的例子中，我们会为 Vue 前端创建一个微服务，为 MySQL 数据库创建另一个微服务，为 Java 中间件部分创建另一个微服务，以此类推。很明显，微服务与容器化是相辅相成的。&lt;/p&gt;
&lt;h2 id=&#34;从-docker-开始&#34;&gt;从 Docker 开始&lt;/h2&gt;
&lt;p&gt;现在你已经对容器有一定了解了吧？Docker 是最常用的容器化工具，也是最流行的容器运行时。&lt;/p&gt;
&lt;p&gt;Docker 开源于 2013 年。用于打包和创建容器，管理基于容器的应用。所有 Linux 发行版、Windows 和 macOS 都支持 Docker。&lt;/p&gt;
&lt;p&gt;还有其他的容器化工具，如 &lt;a href=&#34;https://coreos.com/rkt/&#34; title=&#34;CoreOS rkt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CoreOS rkt&lt;/a&gt;
、&lt;a href=&#34;http://mesos.apache.org/documentation/latest/mesos-containerizer/&#34; title=&#34;Mesos Containerizer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mesos Containerizer&lt;/a&gt;
 和 &lt;a href=&#34;https://linuxcontainers.org/&#34; title=&#34;LXC&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LXC&lt;/a&gt;
。但是目前，绝大多数的容器化应用都是在 Docker 上运行的。&lt;/p&gt;
&lt;h2 id=&#34;再到-kubernetes&#34;&gt;再到 Kubernetes&lt;/h2&gt;
&lt;p&gt;首先，简单介绍一下历史。Kubernetes 是 Google 基于其内部容器调度平台 Borg 的经验开发的。2014 年开源，并作为 CNCF（云原生计算基金会）的核心发起项目。&lt;/p&gt;
&lt;p&gt;那么 Kubernetes 又跟容器是什么关系呢？让我们再回到上面的例子。假设我们的应用爆火，每天的注册用户越来越多。&lt;/p&gt;
&lt;p&gt;现在，我们需要增加后端资源，使浏览我们网站的用户在浏览页面时加载时间不会过长或者超时。最简单的方式就是增加容器的数量，然后使用负载均衡器将传入的负载（以用户请求的形式）分配给容器。&lt;/p&gt;
&lt;p&gt;这样做虽然行之有效，但也只能在用户规模有限的情况下使用。当用户请求达到几十万或几百万时，这种方法也是不可扩展的。你需要管理几十个也许是几百个负载均衡器，这本身就是另一个令人头疼的问题。如果我们想对网站或应用进行任何升级，也会遇到问题，因为负载均衡不会考虑到应用升级的问题。我们需要单独配置每个负载均衡器，然后升级该均衡器所服务的容器。想象一下，当你有 20 个负载均衡器和每周 5 或 6 个小的更新时，你将不得不进行大量的手工劳动。&lt;/p&gt;
&lt;p&gt;我们需要的是一种可以一次性将变更传递给所有受控容器的方法，同时也需要一种可以轻松地调度可用容器的方法，这个过程还必须要是自动化的，这正是 Kubernetes 所做的事情。&lt;/p&gt;
&lt;p&gt;接下来，我们将探讨 Kubernetes 究竟是如何工作的，它的各种组件和服务，以及更多关于如何使用 Kubernetes 来编排、管理和监控容器化环境。为了简单起见，假设我们使用的是 Docker 容器，尽管如前所述，Kubernetes 除了支持 Docker 之外，还支持其他几种容器平台。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-架构和组件&#34;&gt;Kubernetes 架构和组件&lt;/h2&gt;
&lt;p&gt;首先，最重要的是你需要认识到 Kubernetes 利用了“期望状态”原则。就是说，你定义了组件的期望状态，而 Kubernetes 要将它们始终调整到这个状态。&lt;/p&gt;
&lt;p&gt;例如，你想让你的 Web 服务器始终运行在 4 个容器中，以达到负载均衡的目的，你的数据库复制到 3 个不同的容器中，以达到冗余的目的。这就是你想要的状态。如果这 7 个容器中的任何一个出现故障，Kubernetes 引擎会检测到这一点，并自动创建出一个新的容器，以确保维持所需的状态。&lt;/p&gt;
&lt;p&gt;现在我们来定义一些 Kubernetes 的重要组件。&lt;/p&gt;
&lt;p&gt;当你第一次设置 Kubernetes 时，你会创建一个集群。所有其他组件都是集群的一部分。你也可以创建多个虚拟集群，称为命名空间 (namespace)，它们是同一个物理集群的一部分。这与你可以在同一物理服务器上创建多个虚拟机的方式非常相似。如果你不需要，也没有明确定义的命名空间，那么你的集群将在始终存在的默认命名空间中创建。&lt;/p&gt;
&lt;p&gt;Kubernetes 运行在节点 (node) 上，节点是集群中的单个机器。如果你有自己的硬件，节点可能对应于物理机器，但更可能对应于在云中运行的虚拟机。节点是部署你的应用或服务的地方，是 Kubernetes 工作的地方。有 2 种类型的节点——master 节点和 worker 节点，所以说 Kubernetes 是主从结构的。&lt;/p&gt;
&lt;p&gt;主节点是一个控制其他所有节点的特殊节点。一方面，它和集群中的任何其他节点一样，这意味着它只是另一台机器或虚拟机。另一方面，它运行着控制集群其他部分的软件。它向集群中的所有其他节点发送消息，将工作分配给它们，工作节点向主节点上的 API Server 汇报。&lt;/p&gt;
&lt;p&gt;Master 节点本身也包含一个名为 API Server 的组件。这个 API 是节点与控制平面通信的唯一端点。API Server 至关重要，因为这是 worker 节点和 master 节点就 pod、deployment 和所有其他 Kubernetes API 对象的状态进行通信的点。&lt;/p&gt;
&lt;p&gt;Woker 节点是 Kubernetes 中真正干活的节点。当你在应用中部署容器或 pod（稍后定义）时，其实是在将它们部署到 worker 节点上运行。Worker 节点托管和运行一个或多个容器的资源。&lt;/p&gt;
&lt;p&gt;Kubernetes 中的逻辑而非物理的工作单位称为 pod。一个 pod 类似于 Docker 中的容器。记得我们在前面讲到，容器可以让你创建独立、隔离的工作单元，可以独立运行。但是要创建复杂的应用程序，比如 Web 服务器，你经常需要结合多个容器，然后在一个 pod 中一起运行和管理。这就是 pod 的设计目的——一个 pod 允许你把多个容器，并指定它们如何组合在一起来创建应用程序。而这也进一步明确了 Docker 和 Kubernetes 之间的关系——一个 Kubernetes pod 通常包含一个或多个 Docker 容器，所有的容器都作为一个单元来管理。&lt;/p&gt;
&lt;p&gt;Kubernetes 中的 service 是一组逻辑上的 pod。把一个 service 看成是一个 pod 的逻辑分组，它提供了一个单一的 IP 地址和 DNS 名称，你可以通过它访问服务内的所有 pod。有了服务，就可以非常容易地设置和管理负载均衡，当你需要扩展 Kubernetes pod 时，这对你有很大的帮助，我们很快就会看到。&lt;/p&gt;
&lt;p&gt;ReplicationController 或 ReplicaSet 是 Kubernetes 的另一个关键功能。它是负责实际管理 pod 生命周期的组件——当收到指令时或 pod 离线或意外停止时启动 pod，也会在收到指示时杀死 pod，也许是因为用户负载减少。所以换句话说，ReplicationController 有助于实现我们所期望的指定运行的 pod 数量的状态。&lt;/p&gt;
&lt;h2 id=&#34;什么是-kubectl&#34;&gt;什么是 Kubectl？&lt;/h2&gt;
&lt;p&gt;kubectl 是一个命令行工具，用于与 Kubernetes 集群和其中的 pod 通信。使用它你可以查看集群的状态，列出集群中的所有 pod，进入 pod 中执行命令等。你还可以使用 YAML 文件定义资源对象，然后使用 kubectl 将其应用到集群中。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-中的自动扩展&#34;&gt;Kubernetes 中的自动扩展&lt;/h2&gt;
&lt;p&gt;请记住，我们使用 Kubernetes 而不是直接使用 Docker 的原因之一，是因为 Kubernetes 能够自动扩展应用实例的数量以满足工作负载的需求。&lt;/p&gt;
&lt;p&gt;自动缩放是通过集群设置来实现的，当服务需求增加时，增加节点数量，当需求减少时，则减少节点数量。但也要记住，节点是“物理”结构——我们把“物理”放在引号里，因为要记住，很多时候，它们实际上是虚拟机。&lt;/p&gt;
&lt;p&gt;无论如何，节点是物理机器的事实意味着我们的云平台必须允许 Kubernetes 引擎创建新机器。各种云提供商对 Kubernetes 支持基本都满足这一点。&lt;/p&gt;
&lt;p&gt;我们再继续说一些概念，这次是和网络有关的。&lt;/p&gt;
&lt;h2 id=&#34;什么是-kubernetes-ingress-和-egress&#34;&gt;什么是 kubernetes Ingress 和 Egress？&lt;/h2&gt;
&lt;p&gt;外部用户或应用程序与 Kubernetes pod 交互，就像 pod 是一个真正的服务器一样。我们需要设置安全规则允许哪些流量可以进入和离开“服务器”，就像我们为托管应用程序的服务器定义安全规则一样。&lt;/p&gt;
&lt;p&gt;进入 Kubernetes pod 的流量称为 Ingress，而从 pod 到集群外的出站流量称为 egress。我们创建入口策略和出口策略的目的是限制不需要的流量进入和流出服务。而这些策略也是定义 pod 使用的端口来接受传入和传输传出数据 / 流量的地方。&lt;/p&gt;
&lt;h2 id=&#34;什么是-ingress-controller&#34;&gt;什么是 Ingress Controller？&lt;/h2&gt;
&lt;p&gt;但是在定义入口和出口策略之前，你必须首先启动被称为 Ingress Controller（入口控制器）的组件；这个在集群中默认不启动。有不同类型的入口控制器，Kubernetes 项目默认只支持 Google Cloud 和开箱即用的 Nginx 入口控制器。通常云供应商都会提供自己的入口控制器。&lt;/p&gt;
&lt;h2 id=&#34;什么是-replica-和-replicaset&#34;&gt;什么是 Replica 和 ReplicaSet？&lt;/h2&gt;
&lt;p&gt;为了保证应用程序的弹性，需要在不同节点上创建多个 pod 的副本。这些被称为 Replica。假设你所需的状态策略是“让名为 webserver-1 的 pod 始终维持在 3 个副本”，这意味着 ReplicationController 或 ReplicaSet 将监控活动副本的数量，如果其中有任何一个 replica 因任何原因不可用（例如节点的故障），那么 Deployment Controller 将自动创建一个新的系统（定义如下）。&lt;/p&gt;
&lt;p&gt;所需状态是在 deployment 中定义的。Master 节点的中有一个子系统叫做 Deployment Controller，负责实际执行并使当前状态不断趋向于所需状态。&lt;/p&gt;
&lt;p&gt;因此，举例来说，如果你目前有 2 个 pod 的副本，而你所希望的状态应该有 3 个，那么 Replication Controller 或 ReplicaSet 会自动检测到这个要求，并指示 Deployment Controller 根据预定义的设置部署一个新的 pod。&lt;/p&gt;
&lt;h2 id=&#34;什么是服务网格&#34;&gt;什么是服务网格？&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://jimmysong.io/blog/what-is-a-service-mesh/&#34; title=&#34;服务网格 (Service Mesh)&#34;&gt;服务网格 (Service Mesh)&lt;/a&gt;
 用于管理服务之间的网络流量，是云原生的网络基础设施层，也是 &lt;a href=&#34;https://jimmysong.io/blog/post-kubernetes-era/&#34; title=&#34;Kubernetes 次世代的云原生应用&#34;&gt;Kubernetes 次世代的云原生应用&lt;/a&gt;
 的重要组成部分。&lt;/p&gt;
&lt;p&gt;服务网格利用容器之间的网络设置来控制或改变应用程序中不同组件之间的交互。下面，我们用一个例子来说明。假设你想测试 Nginx 的新版本，检查它是否与你的 Web 应用兼容。你用新的 Nginx 版本创建了一个新的容器 (Container2)，并从当前容器 (Container1) 中复制了当前的 Nginx webserver 配置。但你不想影响组成 web 应用的其他微服务（假设每个容器对应一个单独的微服务）——就是 MySQL 数据库、Node.js 前端、负载均衡器等。&lt;/p&gt;
&lt;p&gt;所以使用服务网格，你可以立即只把 webserver 微服务改成 Container2（新 Nginx 版本的那个）进行测试。如果确定它不能工作，比如因为它导致网站出现一些兼容性问题，那么你就调用服务网格来快速切换回原来的 Container1。而这一切都不需要对其他容器进行任何配置变更——这些变更对其他容器是完全透明的。&lt;/p&gt;
&lt;p&gt;如果没有服务网格，对容器来说这项工作将十分繁琐，因为这涉及到逐一更改所有其他容器上的配置，将它们所包含的服务从 Container1 指向 Container2，然后在测试失败后，将它们全部改回来。&lt;/p&gt;
&lt;p&gt;在前面这部分 Kubernetes 指南中，我们介绍了一些与 Kubernetes 网络相关的概念。Kubernetes 中的网络可能很棘手，很难理解，如果你刚刚开始，你可能需要一些实践来理解这里。&lt;/p&gt;
&lt;p&gt;在下一部分中，我们将展开更多关于 Kubernetes 的话题：如何开始学习 Kubernetes，如何在本地安装和测试 Kubernetes，以及 Kubernetes 的一些优秀的监控工具。&lt;/p&gt;
&lt;h2 id=&#34;如何学习-kubernetes&#34;&gt;如何学习 Kubernetes？&lt;/h2&gt;
&lt;p&gt;自学 Kubernetes 知识基本上有三种不同的途径，我们在这里只提供了一个指导大纲。&lt;/p&gt;
&lt;h3 id=&#34;一从零开始学习和安装-kubernetes&#34;&gt;一、从零开始学习和安装 Kubernetes&lt;/h3&gt;
&lt;p&gt;要想真正掌握 Kubernetes，最好的办法莫过于自己从头开始安装 Kubernetes。不过要注意的是，从零开始安装 Kubernetes 并不是一件容易的事情。安装 Kubernetes 并不是简单的“下载文件 -&amp;gt; 点击安装”式的操作，Kubernetes 由多个组件组成，这些组件必须单独安装和配置。而在此之前，你也需要相当的技术储备来做安装前的准备，比如熟悉 Linux 操作系统。如果你决定使用这种方式学习的话，推荐你阅读 &lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook&#34; title=&#34;Kubernetes Handbook——Kubernetes 中文指南 / 云原生架构实践手册&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes Handbook——Kubernetes 中文指南 / 云原生架构实践手册&lt;/a&gt;
。此外，请记住，尽管 Kubernetes 作为一个开源解决方案在技术上是免费的，但它确实有一些隐藏的成本，只不过对初学者来说可能并不明显。&lt;/p&gt;
&lt;h3 id=&#34;二kubernetes-自托管解决方案&#34;&gt;二、Kubernetes 自托管解决方案&lt;/h3&gt;
&lt;p&gt;这些解决方案样是一些工具和实用程序，大大简化了在本地计算机上安装和配置小型 Kubernetes 集群的任务。它们是学习 Kubernetes 的好方法，同时对于新手来说也不会太难，又足够小巧可以到安装在个人电脑上。最流行的自托管 Kubernetes 工具和环境是 &lt;a href=&#34;https://github.com/kubernetes/minikube&#34; title=&#34;Minikube&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Minikube&lt;/a&gt;
、&lt;a href=&#34;https://github.com/ubuntu/microk8s&#34; title=&#34;MicroK8s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MicroK8s&lt;/a&gt;
、&lt;a href=&#34;https://docs.docker.com/docker-for-windows/kubernetes/&#34; title=&#34;Docker Desktop&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Docker Desktop&lt;/a&gt;
 和 &lt;a href=&#34;https://github.com/kubernetes-sigs/kind&#34; title=&#34;Kind&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kind&lt;/a&gt;
。这些解决方案往往有一些限制，例如，Minikube 只允许创建一个节点。尽管有这些缺点，但这些工具还是非常值得推荐，因为它们将易学性和成本效益结合起来，对于刚开始使用 Kubernetes 的初学者来说，是一个很好的选择。&lt;/p&gt;
&lt;h3 id=&#34;三云托管的解决方案&#34;&gt;三、云托管的解决方案&lt;/h3&gt;
&lt;p&gt;如今各大云供应商都提供了定制化的 Kubernetes 解决方案来。你也可以通过线上教学平台如 &lt;a href=&#34;https://katacoda.com/&#34; title=&#34;Katacoda&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Katacoda&lt;/a&gt;
 上的免费课程来学习 Kubernetes，它们都是云托管的，你不需要自己安装，只不过你需要云供应商的集群需要付费。&lt;/p&gt;
&lt;h2 id=&#34;本地测试和调试-kubernetes&#34;&gt;本地测试和调试 Kubernetes&lt;/h2&gt;
&lt;p&gt;作为本地安装 Kubernetes 的一部分，你很可能还需要一些测试和调试能力，以确保一切都在顺利运行，特别是定义入口和出口策略等棘手的任务。此外，还有 Kubernetes 附加组件的生态系统，你可能想使用这些组件来扩展 Kubernetes 集群的功能。添加所有这些都需要进行更多的测试，以确保它们能与你的 Kubernetes 集群完美的集成。&lt;/p&gt;
&lt;p&gt;用于在本地开发和调试 Kubernetes 服务的工具有：&lt;a href=&#34;https://github.com/microsoft/mindaro&#34; title=&#34;Microsoft Bridge to Kubernetes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Microsoft Bridge to Kubernetes&lt;/a&gt;
 和 &lt;a href=&#34;https://github.com/telepresenceio/telepresence&#34; title=&#34;telepresence&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;telepresence&lt;/a&gt;
。这些工具可以让你在本地运行单个服务，同时将该服务连接到远程 Kubernetes 集群。这样你就可以让自己的本地机器作为 Kubernetes 集群中的一部分来运行——这对于在本地而不是在生产集群上开发服务非常有用。&lt;/p&gt;
&lt;p&gt;Kubernetes 项目也了解到了 Kubernetes 安装对端到端 (E2E) 测试的需求。为此，项目核心团队一直在确保在最近的版本中更恰当地支持 E2E 测试。这包括诸如允许测试重用和纳入更多附加组件和驱动程序的测试等。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-监控工具&#34;&gt;Kubernetes 监控工具&lt;/h2&gt;
&lt;p&gt;Kubernetes 提供了应用程序在集群的每个层次上的资源使用情况的详细信息——容器、pod、服务。这些详细信息使你能够评估应用程序的性能，确定哪些瓶颈可以解决以提高整体性能。&lt;/p&gt;
&lt;p&gt;毕竟，监控可以帮助你了解应用和集群运行情况的详细信息，这对于学习 Kubernetes 是十分有帮助的。&lt;/p&gt;
&lt;p&gt;Kubernetes 包含两个内置度量收集工具用于监控：&lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/&#34; title=&#34;资源管道和全度量管道&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;资源管道和全度量管道&lt;/a&gt;
。资源管道是一个较低级和较有限的工具，主要集中在与各种控制器相关的指标上。全指标管道，顾名思义，从几乎所有集群组件中获取并显示更丰富的指标。&lt;/p&gt;
&lt;p&gt;还有一些第三方工具可以安装并集成到 Kubernetes 集群中。对于 Kubernetes 来说，最普遍使用的两个工具是 Prometheus 和 Grafana。&lt;/p&gt;
&lt;h3 id=&#34;prometheus-监控&#34;&gt;Prometheus 监控&lt;/h3&gt;
&lt;p&gt;Prometheus 是一个功能丰富的开源监控和警报工具。Prometheus 包含一个内部数据存储用来收集指标，如生成的时间序列数据。Prometheus 还拥有众多插件，允许它将数据暴露给各种外部解决方案，并从其他数据源导入数据，包括所有主要公有云监控解决方案。&lt;/p&gt;
&lt;h3 id=&#34;grafana-仪表盘&#34;&gt;Grafana 仪表盘&lt;/h3&gt;
&lt;p&gt;Grafana 是一个优秀的仪表盘、分析和数据可视化工具。它没有 Prometheus 的全功能数据收集能力，但 Prometheus 又没有 Grafana 的数据呈现界面。事实上，他们最好是结合在一起使用——Prometheus 负责数据收集和汇总，Grafana 负责数据展示。它们共同创造了一个强大的组合，涵盖了数据收集、基本警报和可视化。&lt;/p&gt;
&lt;h3 id=&#34;高级警报&#34;&gt;高级警报&lt;/h3&gt;
&lt;p&gt;对于高级警报，你可以添加 &lt;a href=&#34;https://www.nagios.org/&#34; title=&#34;Nagios&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nagios&lt;/a&gt;
 或 &lt;a href=&#34;https://github.com/prometheus/alertmanager&#34; title=&#34;Prometheus Alertmanager&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prometheus Alertmanager&lt;/a&gt;
 等工具。这些警报工具通常有大量的集成。你可以为自定义值班团队，然后定义你想要监控的参数，例如“当任何 pod 不可用时”或“当任何节点无法访问时”、“当容量达到 90%”等，然后通过电子邮件、短信、手机应用提醒、电话呼叫等方式向值班人员发送自定义通知。你还可以创建升级策略，比如，如果一个被定义为“危急”的警报在 10 分钟内没有值班人员确认，那么就将警报升级（发送警报）到该人员的经理。&lt;/p&gt;
&lt;p&gt;现在，你应该已经对 Docker 和 Kubernetes 有了大体的认识。了解了 Kubernetes 的作用，知道它是如何进行容器化应用部署和管理的。&lt;/p&gt;
&lt;p&gt;调试和监控技术不仅仅是运维需要，你也可以把它当作学习方式。有什么比边做边学更好呢？&lt;/p&gt;
&lt;p&gt;请记住，如果你的应用规模太小，而且预计用户需求不会有太大变化或重大波动（比如一个只在公司内部使用的应用），那么 Kubernetes 对你来说可能没有必要，这种情况下，直接使用 Docker 就足够了。&lt;/p&gt;
&lt;h2 id=&#34;更多&#34;&gt;更多&lt;/h2&gt;
&lt;p&gt;云原生领域的开源项目众多（见 &lt;a href=&#34;https://jimmysong.io/awesome-cloud-native&#34; title=&#34;Awesome Cloud Native/云原生开源项目大全&#34;&gt;Awesome Cloud Native/云原生开源项目大全&lt;/a&gt;
），其中有大量的优秀项目可供我们学习。此外，Kubernetes 开源已经多年时间，网上有大量的学习资料，业界出版过很多书籍，建议大家通过阅读&lt;a href=&#34;https://kubernetes.io&#34; title=&#34;官方文档&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;官方文档&lt;/a&gt;
和实践来学习，也可以参考我编写的&lt;a href=&#34;https://jimmysong.io/book/kubernetes-handbook&#34; title=&#34;Kubernetes Handbook——Kubernetes 中文指南 / 云原生架构实践手册&#34;&gt;Kubernetes Handbook——Kubernetes 中文指南 / 云原生架构实践手册&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;推荐大家加入我发起创办的&lt;a href=&#34;https://cloudnative.to&#34; title=&#34;云原生社区&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;云原生社区&lt;/a&gt;
，这是一个立足中国，放眼世界的云原生终端用户社区，致力于云原生技术的传播和应用。云原生社区主办的&lt;a href=&#34;https://github.com/cloudnativeto/academy&#34; title=&#34;云原生学院&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;云原生学院&lt;/a&gt;
定期邀请云原生和开源领域的大咖在 B 站上进行直播分享，成员自发组织了多个 SIG（特别兴趣小组）进行讨论学习。欢迎加入我们，共同学习和交流云原生技术。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Kubernetes 次世代的云原生应用</title>
      <link>https://jimmysong.io/blog/post-kubernetes-era/</link>
      <pubDate>Mon, 01 Jun 2020 18:13:19 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/post-kubernetes-era/</guid>
      <description>
        
        
        &lt;p&gt;Kubernetes 自开源至今已经走过六个年头了，&lt;a href=&#34;https://cloudnative.to/blog/cloud-native-era/&#34; title=&#34;云原生时代&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;云原生时代&lt;/a&gt;
也已到来，我关注云原生领域也四年有余了，最近开始思考云原生的未来走向，特此撰写本文作为&lt;a href=&#34;https://jimmysong.io/guide-to-cloud-native-app&#34; title=&#34;《云原生应用白皮书》&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;《云原生应用白皮书》&lt;/a&gt;
的开篇，更多关于云原生应用的介绍请转到白皮书中浏览。&lt;/p&gt;
&lt;h2 id=&#34;重点&#34;&gt;重点&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;云原生基础设施已渡过了野蛮生长期，正朝着统一应用标准方向迈进。&lt;/li&gt;
&lt;li&gt;Kubernetes 的原语无法完整描述云原生应用体系，且在资源的配置上开发与运维功能耦合严重。&lt;/li&gt;
&lt;li&gt;Operator 在扩展了 Kubernetes 生态的同时导致云原生应用碎片化，亟需一个统一的应用定义标准。&lt;/li&gt;
&lt;li&gt;OAM 的本质是将云原生应用定义中的研发、运维关注点分离，资源对象进行进一步抽象，化繁为简，包罗万象。&lt;/li&gt;
&lt;li&gt;“Kubernetes 次世代”是指在 Kubernetes 成为基础设施层标准之后，云原生生态的关注点正在向应用层过度，近两年来火热的 Service Mesh 正是该过程中的一次有力探索，而基于 Kubernetes 的云原生&lt;strong&gt;应用&lt;/strong&gt;架构的时代即将到来。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Kubernetes 已成为云原生应用的既定运行平台，本文以 Kubernetes 为默认平台展开，包括云原生应用的分层模型。&lt;/p&gt;
&lt;h2 id=&#34;云原生的不同发展阶段&#34;&gt;云原生的不同发展阶段&lt;/h2&gt;
&lt;p&gt;Kubernetes 从开源至今已经走过快&lt;a href=&#34;https://jimmysong.io/cloud-native/memo/open-source/&#34; title=&#34;六个年头&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;六个年头&lt;/a&gt;
（2014 年 6 月开源）了，可以说是 Kubernetes 的诞生开启了整个云原生的时代。我粗略的将云原生的发展划分为以下几个时期。&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
    
      
        
          
          &lt;picture&gt;
           &lt;source srcset=&#34;https://jimmysong.io/blog/post-kubernetes-era/cloud-native-stages_hu0953b587bd346fd1129ec57414444e0c_66753_2096x508_resize_q75_h2_lanczos_3.webp&#34; type=&#34;image/webp&#34;&gt;
           &lt;img src=&#34;https://jimmysong.io/blog/post-kubernetes-era/cloud-native-stages.png&#34; data-img=&#34;/blog/post-kubernetes-era/cloud-native-stages.png&#34; data-width=&#34;2096&#34; data-height=&#34;508&#34; alt=&#34;image&#34; data-caption=&#34;云原生的发展阶段&#34;&gt;
          &lt;/picture&gt;
        
      
    
  
  &lt;figcaption&gt;云原生的发展阶段&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;第一阶段：孵化期（2014 年）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2014 年，Google 开源 Kubernetes，在此之前的 2013 年，Docker 开源，DevOps、微服务已变得十分流行，云原生的概念已经初出茅庐。在开源了 Kubernetes 之后，Google 联合其他厂商发起成立了 CNCF，并将 Kubernetes 作为初创项目捐献给了 CNCF。CNCF 作为云原生的背后推手，开始推广 Kubernetes。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第二阶段：高速发展期（2015 年 - 2016 年）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这几年间，Kubernetes 保持着高速发展，并于 2017 年打败了 Docker Swarm、Mesos，确立了容器编排工具领导者的地位。CRD 和 Operator 模式的诞生，大大增强了 Kubernetes 的扩展性，促进了周边生态的繁荣。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第三阶段：野蛮生长期（2017 年 - 2018 年）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2016 年之后的云原生基本都默认运行在 Kubernetes 平台上，2017、2018 年 Google 主导的 Istio、Knative 相继开源，这些开源项目都大量利用了 Kubernetes 的 Operator 进行了扩展，Istio 刚发布时就有 50 多个 CRD 定义。Istio 号称是&lt;a href=&#34;https://jimmysong.io/blog/service-mesh-the-microservices-in-post-kubernetes-era/&#34; title=&#34;后 Kubernetes 时代的微服务&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;后 Kubernetes 时代的微服务&lt;/a&gt;
，它的出现第一次使得云原生以服务（应用）为中心。Knative 是 Google 在基于 Kubernetes 之上开源的 Serverless 领域的一次尝试。2018 年 Kubernetes 正式从 CNCF &lt;a href=&#34;https://www.cncf.io/blog/2018/03/06/kubernetes-first-cncf-project-graduate/&#34; title=&#34;毕业&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;毕业&lt;/a&gt;
，Prometheus、Envoy 也陆续从 CNCF 毕业。CNCF 也与 2018 年修改了 charter，对云原生进行了重定义，从原来的三要素：”应用容器化；面向微服务架构；应用支持容器的编排调度“，修改为”云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式 API“。这一年，我曾写过两篇 Kubernetes 及云原生发展的年终总结和展望，见 &lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/appendix/kubernetes-and-cloud-native-summary-in-2017-and-outlook-for-2018.html&#34; title=&#34;2017 年&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2017 年&lt;/a&gt;
和 &lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/appendix/kubernetes-and-cloud-native-summary-in-2018-and-outlook-for-2019.html&#34; title=&#34;2018 年&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018 年&lt;/a&gt;
的预测和总结。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第四阶段：普及推广期（2019 年至今）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;经过几年的发展，Kubernetes 已经得到的大规模的应用，云原生的概念开始深入人心，Kubernetes 号称是云原生的操作系统，基于 Operator 模式的生态大放异彩。整合 Kubernetes 和云基础设施，研发和运维关注点分离。Kubernetes 到 Service Mesh（后 Kubernetes 时代的微服务），基于 Kubernetes 的 Serverless 都在快速发展，OAM 诞生，旨在定义云原生应用标准。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-开辟了云原生时代&#34;&gt;Kubernetes 开辟了云原生时代&lt;/h2&gt;
&lt;p&gt;Kubernetes 开源之初就继承了 Google 内部调度系统 Borg 的经验，屏蔽掉了底层物理机、虚拟机之间的差异，经过几年时间的发展成为了容器编排标准，进而统一了 PaaS 平台的基础设施层。&lt;/p&gt;
&lt;p&gt;下图是 Kubernetes 原生内置的可以应用到一个 Pod 上的所有控制器、资源对象等。&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
    
      
        
          
          &lt;picture&gt;
           &lt;source srcset=&#34;https://jimmysong.io/blog/post-kubernetes-era/kubernetes-concepts_huc7f9b9987a9d4903fe8c8a4e926f7b29_121210_800x596_resize_q75_h2_lanczos_3.webp&#34; type=&#34;image/webp&#34;&gt;
           &lt;img src=&#34;https://jimmysong.io/blog/post-kubernetes-era/kubernetes-concepts.png&#34; data-img=&#34;/blog/post-kubernetes-era/kubernetes-concepts.png&#34; data-width=&#34;800&#34; data-height=&#34;596&#34; alt=&#34;image&#34; data-caption=&#34;Kubernetes 概念&#34;&gt;
          &lt;/picture&gt;
        
      
    
  
  &lt;figcaption&gt;Kubernetes 概念&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;图片来自图书 &lt;a href=&#34;https://www.redhat.com/cms/managed-files/cm-oreilly-kubernetes-patterns-ebook-f19824-201910-en.pdf&#34; title=&#34;Kubernetes Patterns（O’Reilly）&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes Patterns（O’Reilly）&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Kubernetes 作为云原生基础设施设计之初遵循了以下原则：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基础设施即代码（声明式 API）&lt;/li&gt;
&lt;li&gt;不可变基础设施&lt;/li&gt;
&lt;li&gt;幂等性&lt;/li&gt;
&lt;li&gt;调节器模式（Operator 的原理）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中声明式 API 可谓开创了云原生时代的基调，而调节器模式是 Kubernetes 区别于其他&lt;a href=&#34;https://jimmysong.io/cloud-native-infra/evolution-of-cloud-native-developments.html&#34; title=&#34;云部署形式&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;云部署形式&lt;/a&gt;
的主要区别之一，这也为后来的 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/54633203&#34; title=&#34;Operator 框架的诞生&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Operator 框架的诞生&lt;/a&gt;
打下了基础。&lt;/p&gt;
&lt;h3 id=&#34;声明式-api&#34;&gt;声明式 API&lt;/h3&gt;
&lt;p&gt;根据声明式 API 可以做应用编排，定义组件间的依赖，通常使用人类易读的 YAML 文件来表示。但是，YAML 文件声明的字段真的就是最终的状态吗？有没有可能动态改变？&lt;/p&gt;
&lt;p&gt;我们在创建 &lt;code&gt;Deployment&lt;/code&gt; 时会指定 Pod 的副本数，但是其实际副本数并不一定是一成不变的。假如集群中还有定义 HPA，那么 Pod 的副本数就可能随着一些外界因素（比如内存、CPU 使用率或者自定义 metric）而改变，而且如果集群中还有运行自定义的控制器话，那么也有可能修改应用的实例数量。在有多个控制器同时控制某个资源对象时，如何确保控制器之间不会发生冲突，资源对象的状态可预期？可以使用&lt;a href=&#34;https://kubernetes.io/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#monitoring-admission-webhooks&#34; title=&#34;动态准入控制&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;动态准入控制&lt;/a&gt;
来达到这一点。&lt;/p&gt;
&lt;h3 id=&#34;kubernetes-原生应用&#34;&gt;Kubernetes 原生应用&lt;/h3&gt;
&lt;p&gt;我们都知道要想运行一个应用至少需要以下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用的业务逻辑（代码）、运行时（可运行的二进制文件、字节码或脚本）。&lt;/li&gt;
&lt;li&gt;应用的配置注入（配置文件、环境变量等），身份、路由、服务暴露等满足应用的安全性和可访问性。&lt;/li&gt;
&lt;li&gt;应用的生命周期管理（各种 Controller 登场）。&lt;/li&gt;
&lt;li&gt;可观察性、可运维、网络和资源及环境依赖、隔离性等。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图展示了基于 Kubernetes 原语及 PaaS 平台资源的 Kubernetes 原生应用的组成。&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
    
      
        
          &lt;img src=&#34;https://jimmysong.io/blog/post-kubernetes-era/kubernetes-native-application-motion.gif&#34; data-img=&#34;/blog/post-kubernetes-era/kubernetes-native-application-motion.gif&#34; data-width=&#34;600&#34; data-height=&#34;334&#34; alt=&#34;image&#34; data-caption=&#34;Kubernetes 原生应用&#34;&gt;
        
      
    
  
  &lt;figcaption&gt;Kubernetes 原生应用&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;我们都知道 Kubernetes 提供了大量的&lt;a href=&#34;https://kubernetes.io/docs/concepts/&#34; title=&#34;原语&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;原语&lt;/a&gt;
，用户可以基于这些原语来编排服务，管理应用的生命周期。上图展示的是基于 Kubernetes 原生应用可以使用的 Kubernetes 原语、扩展及平台层资源，从内向外的对象跟应用程序（业务逻辑）的关联度依次降低，到最外层基本只剩下平台资源依赖，已经与 Kubernetes 几乎没有关系了。该图里仅展示了部分资源和对象（包含阿里巴巴开源的 &lt;a href=&#34;https://github.com/openkruise/kruise&#34; title=&#34;OpenKruise&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenKruise&lt;/a&gt;
、Istio），实际上 &lt;a href=&#34;https://operatorhub.io/&#34; title=&#34;Operator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Operator&lt;/a&gt;
 资源之丰富，也是 Kubernetes 生态如此繁荣的原因之一。&lt;/p&gt;
&lt;p&gt;Kubernetes 本身的原语、资源对象、配置、常用的 CRD 扩展有几十、上百个之多。开发者需要了解这些复杂的概念吗？我只是想部署一个应用而已！不用所对于应用开发者，即使对于基础实施开发和运维人员也需要很陡峭的学习曲线才能完全掌握它。&lt;/p&gt;
&lt;p&gt;我将 Kubernetes 原生应用所需要的定义和资源进行了分层：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心层&lt;/strong&gt;：应用逻辑、服务定义、生命周期控制；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隔离与服务访问层&lt;/strong&gt;：资源限制与隔离、配置、身份、路由规则等；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;调度层&lt;/strong&gt;：各种调度控制器，这也是 Kubernetes 原生应用的主要扩展层；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;资源层&lt;/strong&gt;：提供网络、存储和其他平台资源；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而这些不同的层，完全可以将其职责分配给相应的人员，比如核心层是由应用程序开发者负责，将其职责分离，可以很大程度上降低开发和运维的复杂度。&lt;/p&gt;
&lt;p&gt;云原生应用落实到 Kubernetes 平台之上，仅仅利用 Kubernetes 的对象原语已很难描述一个复杂的应用程序，所以诞生了各种各样的 Operator，但这也仅仅解决了单个应用的定义，对于应用的打包封装则无能为力。&lt;/p&gt;
&lt;p&gt;同一个资源对象又有多种实现方式，比如 Ingress 就有 &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1DnsHtdHbxjvHmxvlu7VhzWcWgLAn_Mc5L1WlhLDA__k/edit#gid=0&#34; title=&#34;10 多种实现&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10 多种实现&lt;/a&gt;
，PV 就更不用说，对于对于开发者究竟如何选择，平台如何管理，这都是让人很头疼的问题。而且有时候平台所提供的扩展能力还可能会有冲突，这些能力有的可能互不相干，有的可能会有正交，有的可能完全重合。且应用本身与运维特性之间存在太多耦合，不便于复用。&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
    
      
        
          &lt;img src=&#34;https://jimmysong.io/blog/post-kubernetes-era/resources-motion.gif&#34; data-img=&#34;/blog/post-kubernetes-era/resources-motion.gif&#34; data-width=&#34;600&#34; data-height=&#34;363&#34; alt=&#34;image&#34; data-caption=&#34;资源交集动画&#34;&gt;
        
      
    
  
  &lt;figcaption&gt;资源交集动画&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;上图中不同颜色的方框代表不同的资源类别，红线框代表不能为一个资源同时应用该配置，否则会出现冲突，不同的颜色上面是一个动画，展示的是部分资源组合。图中仅包含了部分 Kubernetes 中的原语和 Istio 中的资源对象组合及自定义扩展，实际上用户可以根据应用的自身特点，基于 Kubernetes 原语和 CRD 创建出千变万化的组合。&lt;/p&gt;
&lt;p&gt;为了管理这些应用诞生出了众多的 &lt;a href=&#34;https://github.com/operator-framework/awesome-operators&#34; title=&#34;Operator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Operator&lt;/a&gt;
。Kubernetes 1.7 版本以来就引入了&lt;a href=&#34;https://kubernetes.io/docs/concepts/api-extension/custom-resources/&#34; title=&#34;自定义控制器&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;自定义控制器&lt;/a&gt;
的概念，该功能可以让开发人员扩展添加新功能，更新现有的功能，并且可以自动执行一些管理任务，这些自定义的控制器就像 Kubernetes 原生的组件一样，Operator 直接使用 Kubernetes API 进行开发，也就是说它们可以根据这些控制器内部编写的自定义规则来监控集群、更改 Pods/Services、对正在运行的应用进行扩缩容。&lt;/p&gt;
&lt;p&gt;Operator 的本质是一种调节器模式（Reconciler Pattern）的应用，跟 Kubernetes 本身的实现模式是一样的，用于管理云原生应用，协调应用的实际状态达到预期状态。&lt;/p&gt;
&lt;p&gt;调节器模式的四个原则：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;所有的输入和输出都使用数据结构。&lt;/li&gt;
&lt;li&gt;确保数据结构是不可变的。&lt;/li&gt;
&lt;li&gt;保持资源映射简单。&lt;/li&gt;
&lt;li&gt;使实际状态符合预期状态。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;云原生应用走向碎片化&#34;&gt;云原生应用走向碎片化&lt;/h2&gt;
&lt;p&gt;利用声明式 API 及调节器模式，理论上可以在 Kubernetes 上部署任何可声明应用，但是在 Operator 出现之前，管理 Kubernetes 上的有状态应用一直是一个难题，随着 Operator 模式的确立，该难题已得以解决，并促进了 Kubernetes 生态的进一步发展。随着该生态的繁荣，有一种碎片化的特征正在显现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;云原生应用碎片化的体现&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Operator 模式将运维人员的反应式经验转化成基于 &lt;code&gt;Reconcile&lt;/code&gt; 模式的代码，统一了有状态应用的管理模式，极大得扩展了 Kubernetes 应用生态。&lt;/li&gt;
&lt;li&gt;开发者在引用 Operator 所提供的能力时没有统一的视图，加大了基础设施运维与开发者之间的沟通成本。&lt;/li&gt;
&lt;li&gt;Operator 总体上治理松散，没有统一的管控机制，在同时应用时可能导致互相冲突或无法预期的结果发生。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;有状态应用管理难题&#34;&gt;有状态应用管理难题&lt;/h3&gt;
&lt;p&gt;Kubernetes 对于无状态应用的管理很出色，但是对于有状态应用就不是那么回事了。虽然 StatefulSet 可以帮助管理有状态应用，但是这还远远不够，有状态应用往往有复杂的依赖。声明式的 API 里往往要加载着大量的配置和启动脚本，才能实现一个复杂应用的 Kubernetes 化。&lt;/p&gt;
&lt;p&gt;例如在 2017 年初，Operator Framework 出现之前，需要使用大量的 &lt;code&gt;ConfigMap&lt;/code&gt;、复杂的启动脚本才能&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/guide/migrating-hadoop-yarn-to-kubernetes.html&#34; title=&#34;在 Kubernetes 上定义 Hadoop YARN&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;在 Kubernetes 上定义 Hadoop YARN&lt;/a&gt;
 和&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/usecases/running-spark-with-kubernetes-native-scheduler.html&#34; title=&#34;运行 Spark&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;运行 Spark&lt;/a&gt;
。虽然 &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&#34; title=&#34;&amp;lt;code&amp;gt;StatefulSet&amp;lt;/code&amp;gt;&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;StatefulSet&lt;/code&gt;&lt;/a&gt;
 号称可以解决有状态应用的部署问题，但是它主要是保证了 Pod 的在启动、伸缩时的顺序和使 Pod 具有稳定的标识。但是很多分布式应用来说并不仅依靠启动顺序就可以保证其状态，根据其在分布式应用中的角色不同（master/worker）而需要有大量的自定义配置，在没有 Operator 之前这些配置通常是通过一些自定义脚本来实现，这些脚本可能存在于应用镜像中，也可以通过 &lt;code&gt;ConfigMap&lt;/code&gt; 挂在到容器运行时，但无论如何这些脚本都可能因为散落在各处，这些脚本还是面向过程的，跟在 Kubernetes 诞生之前的运维方式毫无二致，这极其不便于版本控制和运维管理。&lt;/p&gt;
&lt;h3 id=&#34;operator-统一了-kubernetes-应用运维框架&#34;&gt;Operator 统一了 Kubernetes 应用运维框架&lt;/h3&gt;
&lt;p&gt;Operator 大大增强了 Kubernetes 的可扩展性，丰富了以 Kubernetes 为基础的云原生生态，许多原先不是为 Kubernetes 而构建的应用纷纷通过&lt;a href=&#34;https://zhuanlan.zhihu.com/p/54633203&#34; title=&#34;构建自己的 Operator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;构建自己的 Operator&lt;/a&gt;
 迁移到 Kubernetes 上。还有一些直接基于 Kubernetes 构建的 Service Mesh、Serverless 框架，它们应用 Operator 模式（如 &lt;a href=&#34;https://istio.io&#34; title=&#34;Istio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio&lt;/a&gt;
、&lt;a href=&#34;https://knative.dev&#34; title=&#34;Knative&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Knative&lt;/a&gt;
），试图成为云原生应用的基础设施层，补齐 Kubernetes 在服务治理、无服务架构等方面的短板，随着大量的 CRD、Operator 控制器的出现，而 Kubernetes 却无法以应用的视角来管理这些能力及其背后零散的 CRD，这使得云原生应用碎片化。&lt;/p&gt;
&lt;p&gt;Operator 百花齐放，在没有一个大一统的视图之前，各个控制器之间存在着这样的关系：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;独立&lt;/strong&gt;：互不干涉，比如 Controller 与服务发现之间就不存在冲突。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可组合&lt;/strong&gt;：例如 &lt;code&gt;Service&lt;/code&gt;、&lt;code&gt;VirtualService&lt;/code&gt;、&lt;code&gt;DestinationRule&lt;/code&gt; 同属一类资源（可访问性与路由），就是可组合的（后两者是 Istio 中的 CRD，用于流量管理）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;有冲突&lt;/strong&gt;：例如图中的 &lt;code&gt;CronHorizontalPodAutoscaler&lt;/code&gt;（CRD）、&lt;code&gt;HorizontalPodAutoscaler&lt;/code&gt;（Kubernetes 内置），同时使用可能导致无法意料的情况发生。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;正是以为这样复杂的关系，导致其无法做到开箱即用，还需要基础设施团队基于云原生社区和生态自己构建出来的，比如&lt;a href=&#34;https://jimmysong.io/awesome-cloud-native/#application-delivery&#34; title=&#34;应用交付领域&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;应用交付领域&lt;/a&gt;
的系列开源项目。&lt;/p&gt;
&lt;h2 id=&#34;云原生应用管理工具-helm&#34;&gt;云原生应用管理工具 Helm&lt;/h2&gt;
&lt;p&gt;Kubernetes 之上有很多能力缺失，比如应用构建、发布、管理和运维等，Helm 的出现主要补偿了应用打包和版本管理的缺陷。其中云原生应用的配置包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用程序启动时加载的配置文件；&lt;/li&gt;
&lt;li&gt;应用程序的运维配置，如资源申请限额；&lt;/li&gt;
&lt;li&gt;应用程序的服务发现配置；&lt;/li&gt;
&lt;li&gt;应用程序的工作负载、发布策略、依赖等；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些配置可以存在于 &lt;code&gt;ConfigMap&lt;/code&gt;、&lt;code&gt;Deployment&lt;/code&gt;、&lt;code&gt;Service&lt;/code&gt;、&lt;code&gt;Ingress&lt;/code&gt; 等 Kubernetes 的多个资源文件中，如何保证应用程序的复用性？应用程序之间有依赖该如何解决？这是时候你可能自然的想到了 Helm。&lt;/p&gt;
&lt;p&gt;云原生应用打包和发布管理&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Helm 通过 chart 模板，提高了应用程序的复用性并解决了部分依赖问题；&lt;/li&gt;
&lt;li&gt;Chart 仓库提供了云原生应用程序的统一管控视图；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Release&lt;/code&gt; 概念的引入，使得云原生应用版本化管理进一步加强；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Helm 主要关注的是 &lt;a href=&#34;https://12factor.net/zh_cn/&#34; title=&#34;12 因素应用&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;12 因素应用&lt;/a&gt;
法则&lt;a href=&#34;https://12factor.net/zh_cn/build-release-run&#34; title=&#34;构建、发布、运行&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;构建、发布、运行&lt;/a&gt;
这一原则中的”发布”这一环节。下图是 Helm v3 的架构图。&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
    
      
        
          
          &lt;picture&gt;
           &lt;source srcset=&#34;https://jimmysong.io/blog/post-kubernetes-era/helm-chart_hu9afbbdedb02edb818b6556857f76303a_42376_600x429_resize_q75_h2_lanczos_3.webp&#34; type=&#34;image/webp&#34;&gt;
           &lt;img src=&#34;https://jimmysong.io/blog/post-kubernetes-era/helm-chart.png&#34; data-img=&#34;/blog/post-kubernetes-era/helm-chart.png&#34; data-width=&#34;600&#34; data-height=&#34;429&#34; alt=&#34;image&#34; data-caption=&#34;Helm3 架构&#34;&gt;
          &lt;/picture&gt;
        
      
    
  
  &lt;figcaption&gt;Helm3 架构&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Helm 可以安装本地或者远程的 chart，当 chart 安装到 Kubernetes 中后就会创建一个 release，每次更新该 chart 的配置并执行 &lt;code&gt;helm upgrade&lt;/code&gt;，release 的版本数就会加 1，开发者可以升级 chart 或回滚到历史版本。&lt;/p&gt;
&lt;h3 id=&#34;打包配置和发布&#34;&gt;打包、配置和发布&lt;/h3&gt;
&lt;p&gt;Helm 和 chart 的主要作用是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用程序封装&lt;/li&gt;
&lt;li&gt;版本管理&lt;/li&gt;
&lt;li&gt;依赖检查&lt;/li&gt;
&lt;li&gt;便于应用程序分发&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;打包&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Helm 采用 &lt;a href=&#34;https://helm.sh/docs/topics/charts/&#34; title=&#34;Chart&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chart&lt;/a&gt;
 的格式来标准化描述应用，可以将目录打包成版本化的压缩包进行部署理论上一个 Chart 是可以嵌套若干个 Chart 并定义依赖关系，组织形式非常灵活。Helm chart 用于打包 Kubernetes 原生应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;配置&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;应用配置参数，在 Chart 中由 &lt;code&gt;values.yaml&lt;/code&gt; 和命令行参数组成。Chart 采用 Go Template 的特性和 &lt;code&gt;values.yaml&lt;/code&gt; 对部署的模板文件进行参数渲染，也可以通过 &lt;code&gt;helm&lt;/code&gt; 命令 &lt;code&gt;--set key=value&lt;/code&gt; 的方式进行参数赋值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;发布&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Release 代表 Chart 在集群中的运行实例，Helm 围绕 Release 对应用提供了强大的生命周期管理能力，包括 Release 的查询、安装、更新、删除、回滚等。&lt;/p&gt;
&lt;h2 id=&#34;云原生应用&#34;&gt;云原生应用&lt;/h2&gt;
&lt;p&gt;以上关注的点都是基于 Kubernetes 原语的实现，虽然基于 Kubernetes 构建的 PaaS 平台部分屏蔽了底层基础设施的差异，但是仍有很多云服务是无法通过 Kubernetes 创建，或者需要提前创建供 Kubernetes 原生应用使用的，这些应用通常不运行在 Kubernetes 集群中。因此创建和管理一个云原生应用程序需要考虑以下方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;运行时：ECS、Docker、KataContainer、gVisor 等；&lt;/li&gt;
&lt;li&gt;资源隔离性：多租户、VPC、Namespace、防火墙；&lt;/li&gt;
&lt;li&gt;资源调度：各种类型的 controller；&lt;/li&gt;
&lt;li&gt;网络可达性：Service、Ingress、Egress、Gateway、VirtualService、DestinationRule、LoadBalancer、ServiceEntry 等；&lt;/li&gt;
&lt;li&gt;可观测性：日志、分布式追踪、指标；&lt;/li&gt;
&lt;li&gt;安全性：SecurityPolicy、NetworkPolicy、AuthorizationPolicy；&lt;/li&gt;
&lt;li&gt;平台资源申请：数据库、存储等；&lt;/li&gt;
&lt;li&gt;运行与隔离：ECS、Docker、KataContainer 等；&lt;/li&gt;
&lt;li&gt;资源分配和调度：各种控制器；&lt;/li&gt;
&lt;li&gt;环境隔离：Namespace、多租户、VPC、防火墙、LimitRange、Resources；&lt;/li&gt;
&lt;li&gt;可访问性：Service、Ingress、Egress、Gateway、LoadBalancer、VirtualService、DestinationRule、ServiceEntry；&lt;/li&gt;
&lt;li&gt;状态管理：Operator；&lt;/li&gt;
&lt;li&gt;可观察性：日志、监控、指标；&lt;/li&gt;
&lt;li&gt;安全性：SecurityPolicy、ServiceAccount；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;云原生应用分层模型&#34;&gt;云原生应用分层模型&lt;/h3&gt;
&lt;p&gt;那么究竟如何来给云原生应用分层，化繁就简？近几年来，基于 Kubernetes 的应用呈爆炸式发展，光是在&lt;a href=&#34;https://jimmysong.io/awesome-cloud-native/#application-delivery&#34; title=&#34;应用交付领域&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;应用交付领域&lt;/a&gt;
的开源项目就达几十个之多。下图展示我根据这些项目的特性而绘制的 App Delivery Landscape。&lt;/p&gt;
&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
    
      
        
          
          &lt;picture&gt;
           &lt;source srcset=&#34;https://jimmysong.io/blog/post-kubernetes-era/cloud-native-app_hu7d47b535d0003e689bf1fda99b24e11d_32198_1054x514_resize_q75_h2_lanczos_3.webp&#34; type=&#34;image/webp&#34;&gt;
           &lt;img src=&#34;https://jimmysong.io/blog/post-kubernetes-era/cloud-native-app.png&#34; data-img=&#34;/blog/post-kubernetes-era/cloud-native-app.png&#34; data-width=&#34;1054&#34; data-height=&#34;514&#34; alt=&#34;image&#34; data-caption=&#34;云原生应用的分层模型&#34;&gt;
          &lt;/picture&gt;
        
      
    
  
  &lt;figcaption&gt;云原生应用的分层模型&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;应用定义和包装&lt;/strong&gt;：云原生应用的最上层，直接定义云原生应用的组成形式，解决云原生应用之间的依赖关系，并封装成发布包，如 Helm、CNAB，还有云原生变成语言 Pulumi 和 Ballerina，基于 API 的方式来编排云原生应用；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;负载定义&lt;/strong&gt;：基于 Kubernetes Operator，大多是 Serverless 负载，既负责了负载的定义又负责了生命周期管理。&lt;a href=&#34;https://istio.io&#34; title=&#34;Istio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio&lt;/a&gt;
 是比较特殊的存在，它不仅管理服务间的流量，还负责安全性、可观察性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;应用发布和上线&lt;/strong&gt;：关注应用的构建和发布、GitOps、发布策略等，这也是云原生应用全景中最丰富的部分之一；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kubernetes 原语&lt;/strong&gt;：Kubernetes 本身提供的原语，Operator 基于此构建；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上为我个人分类的云原生应用全景模型，仅限于 Kubernetes 之上的应用，对于其他非 Kubernetes 应用非本文的考虑范围。另外，CNCF SIG App Delivery 中也给出的云原生应用的分层模型，其模型将非 Kubernetes 应用场景也纳入了考虑，详见：&lt;a href=&#34;https://docs.google.com/document/d/1gMhRz4vEwiHa3uD8DqFKHGTSxrVJNgkLG2WZWvi9lXo/edit#heading=h.h9so53gv5zen&#34; title=&#34;The Dictionary of Cloud-Native App Delivery&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Dictionary of Cloud-Native App Delivery&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;Platform/Kuberntes，Kubernetes 仅仅是屏蔽了平台的一些差异，但是对于最上层的应用来说，没有涉及，用户需要自己来基于各种开源组件来搭积木。&lt;/p&gt;
&lt;h3 id=&#34;oam开放应用模型&#34;&gt;OAM（开放应用模型）&lt;/h3&gt;
&lt;p&gt;那么以上这么多应用有哪些共性，能不能再进一步抽象呢？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有应用是都以容器作为运行时环境（ContainerizedWorkload），这是 OAM 中的核心 Workload 类型；&lt;/li&gt;
&lt;li&gt;在应用发布和上线方面，有些是属于应用的运维特征，需要根据实际需求组合和变更，这些是持续变动的部分；&lt;/li&gt;
&lt;li&gt;要实现某些复杂的应用管控，需要使用到多个 CRD 的组合，比如 Istio 中的让流量根据百分比切分到不同的而服务，就需要部署 Istio Operator，并声明 &lt;code&gt;VirtualService&lt;/code&gt;、&lt;code&gt;DestinationRule&lt;/code&gt;，二者同时使用；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个 &lt;code&gt;ApplicationConfiguration&lt;/code&gt; 的 Runtime 的正常流程应该是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用开发者创建自己的 &lt;code&gt;Component&lt;/code&gt;，在 &lt;code&gt;Component&lt;/code&gt; 中描述要应用相关的信息，如应用名称、镜像配置、环境变量等，应用到 Kubernetes cluster 中；&lt;/li&gt;
&lt;li&gt;运维创建各种运维策略，如发布策略、网络策略等等，发布时由 AppConfig 对象关联要发布的 &lt;code&gt;Component&lt;/code&gt; 和本次的运维策略，apply 到集群中，集群的 OAM operator watch 到一次 &lt;code&gt;ApplicationConfiguration&lt;/code&gt;的下发，生成 &lt;code&gt;Component&lt;/code&gt; 对应的 &lt;code&gt;Workload&lt;/code&gt; 和 &lt;code&gt;Trait&lt;/code&gt;，&lt;code&gt;Trait&lt;/code&gt; controller 将本次的 &lt;code&gt;Trait&lt;/code&gt; 策略应用到本次要管理的 &lt;code&gt;Workload&lt;/code&gt; 当中，最终到达终态，完成一次发布。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;OAM 是对 Kubernetes 友好的，一样采用声明式 API 的理念开发。如果你已经编写了现成的 CRD Operator，可以平滑的接入到 OAM 体系中。OAM 以应用为中心，高度可扩展，扩展点包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Workload：扩展各种运行时类型，不仅限于容器运行时，还可以定义更多其他运行时，比如 Serverless 负载、虚拟机、数据库、网络等；例如，Pod、无服务器函数、数据存储、消息队列或任何其他类型的工作负载，这些都是应用程序开发人员需要设计一个完整的应用程序所需要的，可以直接引用 Kubernetes 的 CRD；&lt;/li&gt;
&lt;li&gt;Trait：各种运维规则，比如扩缩容、流量控制、安全性；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;生态&#34;&gt;生态&lt;/h3&gt;
&lt;p&gt;以前 CNCF 的主要关注群体大多是基础设施领域的技术人员，但是自 2019 年 9 月，&lt;a href=&#34;https://www.infoq.cn/article/Cdw7ISlEqKilGyN9V3Pj&#34; title=&#34;CNCF 宣布成立 SIG App Delivery&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CNCF 宣布成立 SIG App Delivery&lt;/a&gt;
 后，CNCF 正在将应用开发者和运维人员更紧密的联系在一起。&lt;a href=&#34;https://github.com/cncf/sig-app-delivery&#34; title=&#34;应用交付 SIG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;应用交付 SIG&lt;/a&gt;
 的使命是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在与开发、分发、部署、管理和运行安全的云原生应用相关的领域进行合作，目标是以云原生方式交付应用。&lt;/li&gt;
&lt;li&gt;发展信息资源，包括指南、教程和白皮书，让社区了解最佳实践和应用交付的价值。&lt;/li&gt;
&lt;li&gt;识别合适的项目和现状的差距，定期向 TOC 更新，并以结构化的方式向 TOC 提出行动建议。这包括帮助 TOC 评估和对潜在的新项目进行尽职调查。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前 OAM 定义的云原生应用模型已有以下项目支持。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://crossplane.io/&#34; title=&#34;Crossplane&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Crossplane&lt;/a&gt;
：这是一个开源的 Kubernetes 扩展组件，适用于主流公有云平台，使用 &lt;code&gt;kubectl&lt;/code&gt; 配置和管理基础架构、服务和应用。对于 OAM 的支持详见&lt;a href=&#34;https://crossplane.io/docs/v0.11/getting-started/run-applications.html&#34; title=&#34;运行应用程序&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;运行应用程序&lt;/a&gt;
。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://googlecontainertools.github.io/kpt/&#34; title=&#34;KPT&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KPT&lt;/a&gt;
：Kpt（发音为 &amp;ldquo;keep&amp;rdquo;）是一个在资源配置之上构建声明性工作流的开源工具。它的 git + YAML 架构意味着它只需与现有的工具、框架和平台一起工作。Kpt 包括了获取、显示、自定义、更新、验证和应用 Kubernetes 配置的解决方案。对 OAM 的支持详见 &lt;a href=&#34;https://googlecontainertools.github.io/kpt/guides/ecosystem/oam/&#34; title=&#34;使用 kpt 来管理由开放应用模型（OAM）定义的自定义 Kubernetes 应用程序&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;使用 kpt 来管理由开放应用模型（OAM）定义的自定义 Kubernetes 应用程序&lt;/a&gt;
。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;应用交付领域相关的开源项目还有很多，详见 &lt;a href=&#34;https://jimmysong.io/awesome-cloud-native/#application-delivery&#34; title=&#34;Awesome Cloud Native&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Awesome Cloud Native&lt;/a&gt;
。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;基于 Kubernetes 的云原生生态发展至今已有 6 年时间，当前已步入了普及推广阶段。可以说谁云原生应用定义的制高点，就可以掌握云原生的未来。从前我们是新技术浪潮的追随者，现在我们抓住时代的基于，参与标准制定、引领云原生的浪潮！欢迎加入 &lt;a href=&#34;https://oam.dev/&#34; title=&#34;OAM 社区&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OAM 社区&lt;/a&gt;
，一起参与进来，把国人参与指定的标准推向世界。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.ibm.com/technologies/containers/blogs/kubernetes-helm-3/&#34; title=&#34;Do you know what’s in Helm 3? - developer.ibm.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do you know what’s in Helm 3? - developer.ibm.com&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.redhat.com/cms/managed-files/cm-oreilly-kubernetes-patterns-ebook-f19824-201910-en.pdf&#34; title=&#34;O’Reilly: Kubernetes patterns for designing cloud-native apps - redhat.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;O’Reilly: Kubernetes patterns for designing cloud-native apps - redhat.com&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/document/d/1gMhRz4vEwiHa3uD8DqFKHGTSxrVJNgkLG2WZWvi9lXo/edit#heading=h.h9so53gv5zen&#34; title=&#34;The Dictionary of Cloud-Native App Delivery - docs.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Dictionary of Cloud-Native App Delivery - docs.google.com&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.infoq.cn/article/Cdw7ISlEqKilGyN9V3Pj&#34; title=&#34;CNCF 宣布成立应用交付领域小组，正式开启云原生应用时代 - infoq.cn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CNCF 宣布成立应用交付领域小组，正式开启云原生应用时代 - infoq.cn&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/c7A8lOdAKkW25GoqmwOgWg&#34; title=&#34;OAM v1alpha2 新版发布：平衡标准与可扩展性，灵活接入 CRD operator - mp.weixin.qq.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OAM v1alpha2 新版发布：平衡标准与可扩展性，灵活接入 CRD operator - mp.weixin.qq.com&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/54633203&#34; title=&#34;Kubernetes API 与 Operator，不为人知的开发者战争 - zhuanlan.zhihu.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes API 与 Operator，不为人知的开发者战争 - zhuanlan.zhihu.com&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/cloud-native-era/&#34; title=&#34;云原生时代——投资人视角下的云原生趋势思考 - cloudnative.to&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;云原生时代——投资人视角下的云原生趋势思考 - cloudnative.to&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
