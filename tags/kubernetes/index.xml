<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jimmy Song - Cloud Native | Open Source | Community – kubernetes</title>
    <link>https://jimmysong.io/tags/kubernetes/</link>
    <description>Recent content in kubernetes on Jimmy Song - Cloud Native | Open Source | Community</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <copyright>Copyright &amp;copy; 2020 Jimmy Song 保留所有权利；&lt;/br&gt;基于 Hugo [educenter](https://github.com/themefisher/educenter-hugo)  主题构建</copyright>
    <lastBuildDate>Mon, 01 Jun 2020 18:13:19 +0800</lastBuildDate>
    
	  <atom:link href="https://jimmysong.io/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Kubernetes 次世代的云原生应用</title>
      <link>https://jimmysong.io/blog/post-kubernetes-era/</link>
      <pubDate>Mon, 01 Jun 2020 18:13:19 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/post-kubernetes-era/</guid>
      <description>
        
        
        &lt;p&gt;Kubernetes 自开源至今已经走过六个年头了，&lt;a href=&#34;https://cloudnative.to/blog/cloud-native-era/&#34;&gt;云原生时代&lt;/a&gt;也已到来，我关注云原生领域也四年有余了，最近开始思考云原生的未来走向，特此撰写本文作为&lt;a href=&#34;https://jimmysong.io/guide-to-cloud-native-app&#34;&gt;《云原生应用白皮书》&lt;/a&gt;的开篇，更多关于云原生应用的介绍请转到白皮书中浏览。&lt;/p&gt;
&lt;h2 id=&#34;重点&#34;&gt;重点&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;云原生基础设施已渡过了野蛮生长期，正朝着统一应用标准方向迈进。&lt;/li&gt;
&lt;li&gt;Kubernetes 的原语无法完整描述云原生应用体系，且在资源的配置上开发与运维功能耦合严重。&lt;/li&gt;
&lt;li&gt;Operator 在扩展了 Kubernetes 生态的同时导致云原生应用碎片化，亟需一个统一的应用定义标准。&lt;/li&gt;
&lt;li&gt;OAM 的本质是将云原生应用定义中的研发、运维关注点分离，资源对象进行进一步抽象，化繁为简，包罗万象。&lt;/li&gt;
&lt;li&gt;“Kubernetes 次世代”是指在 Kubernetes 成为基础设施层标准之后，云原生生态的关注点正在向应用层过度，近两年来火热的 Service Mesh 正是该过程中的一次有力探索，而基于 Kubernetes 的云原生&lt;strong&gt;应用&lt;/strong&gt;架构的时代即将到来。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Kubernetes 已成为云原生应用的既定运行平台，本文以 Kubernetes 为默认平台展开，包括云原生应用的分层模型。&lt;/p&gt;
&lt;h2 id=&#34;云原生的不同发展阶段&#34;&gt;云原生的不同发展阶段&lt;/h2&gt;
&lt;p&gt;Kubernetes 从开源至今已经走过快&lt;a href=&#34;https://jimmysong.io/cloud-native/memo/open-source/&#34;&gt;六个年头&lt;/a&gt;（2014 年 6 月开源）了，可以说是 Kubernetes 的诞生开启了整个云原生的时代。我粗略的将云原生的发展划分为以下几个时期。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;cloud-native-stages.png&#34; alt=&#34;云原生的发展阶段&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一阶段：孵化期（2014 年）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2014 年，Google 开源 Kubernetes，在此之前的 2013 年，Docker 开源，DevOps、微服务已变得十分流行，云原生的概念已经初出茅庐。在开源了 Kubernetes 之后，Google 联合其他厂商发起成立了 CNCF，并将 Kubernetes 作为初创项目捐献给了 CNCF。CNCF 作为云原生的背后推手，开始推广 Kubernetes。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第二阶段：高速发展期（2015 年 - 2016 年）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这几年间，Kubernetes 保持着高速发展，并于 2017 年打败了 Docker Swarm、Mesos，确立了容器编排工具领导者的地位。CRD 和 Operator 模式的诞生，大大增强了 Kubernetes 的扩展性，促进了周边生态的繁荣。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第三阶段：野蛮生长期（2017 年 - 2018 年）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2106 年之后的云原生基本都默认运行在 Kubernetes 平台上，2017、2018 年 Google 主导的 Istio、Knative 相继开源，这些开源项目都大量利用了 Kubernetes 的 Operator 进行了扩展，Istio 刚发布时就有 50 多个 CRD 定义。Istio 号称是&lt;a href=&#34;https://jimmysong.io/blog/service-mesh-the-microservices-in-post-kubernetes-era/&#34;&gt;后 Kubernetes 时代的微服务&lt;/a&gt;，它的出现第一次使得云原生以服务（应用）为中心。Knative 是 Google 在基于 Kubernetes 之上开源的 Serverless 领域的一次尝试。2018 年 Kubernetes 正式从 CNCF &lt;a href=&#34;https://www.cncf.io/blog/2018/03/06/kubernetes-first-cncf-project-graduate/&#34;&gt;毕业&lt;/a&gt;，Prometheus、Envoy 也陆续从 CNCF 毕业。CNCF 也与 2018 年修改了 charter，对云原生进行了重定义，从原来的三要素：”应用容器化；面向微服务架构；应用支持容器的编排调度“，修改为”云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API“。这一年，我曾写过两篇 Kubernetes 及云原生发展的年终总结和展望，见 &lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/appendix/kubernetes-and-cloud-native-summary-in-2017-and-outlook-for-2018.html&#34;&gt;2017 年&lt;/a&gt;和 &lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/appendix/kubernetes-and-cloud-native-summary-in-2018-and-outlook-for-2019.html&#34;&gt;2018 年&lt;/a&gt;的预测和总结。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第四阶段：普及推广期（2019 年至今）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;经过几年的发展，Kubernetes 已经得到的大规模的应用，云原生的概念开始深入人心，Kubernetes 号称是云原生的操作系统，基于 Operator 模式的生态大放异彩。整合 Kubernetes 和云基础设施，研发和运维关注点分离。Kubernetes 到 Service Mesh（后 Kubernetes 时代的微服务），基于 Kubernetes 的 Serverless 都在快速发展，OAM 诞生，旨在定义云原生应用标准。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-开辟了云原生时代&#34;&gt;Kubernetes 开辟了云原生时代&lt;/h2&gt;
&lt;p&gt;Kubernetes 开源之初就继承了 Google 内部调度系统 Borg 的经验，屏蔽掉了底层物理机、虚拟机之间的差异，经过几年时间的发展成为了容器编排标准，进而统一了 PaaS 平台的基础设施层。&lt;/p&gt;
&lt;p&gt;下图是Kubernetes 原生内置的可以应用到一个 Pod 上的所有控制器、资源对象等。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;kubernetes-concepts.png&#34; alt=&#34;Kubernetes 概念&#34;&gt;&lt;/p&gt;
&lt;p&gt;图片来自图书 &lt;a href=&#34;https://www.redhat.com/cms/managed-files/cm-oreilly-kubernetes-patterns-ebook-f19824-201910-en.pdf&#34;&gt;Kubernetes Patterns（O’Reilly）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kubernetes 作为云原生基础设施设计之初遵循了以下原则：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基础设施即代码（声明式 API）&lt;/li&gt;
&lt;li&gt;不可变基础设施&lt;/li&gt;
&lt;li&gt;幂等性&lt;/li&gt;
&lt;li&gt;调节器模式（Operator 的原理）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中声明式 API 可谓开创了云原生时代的基调，而调节器模式是 Kubernetes 区别于其他&lt;a href=&#34;https://jimmysong.io/cloud-native-infra/evolution-of-cloud-native-developments.html&#34;&gt;云部署形式&lt;/a&gt;的主要区别之一，这也为后来的 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/54633203&#34;&gt;Operator 框架的诞生&lt;/a&gt;打下了基础。&lt;/p&gt;
&lt;h3 id=&#34;声明式-api&#34;&gt;声明式 API&lt;/h3&gt;
&lt;p&gt;根据声明式 API 可以做应用编排，定义组件间的依赖，通常使用人类易读的 YAML 文件来表示。但是，YAML 文件声明的字段真的就是最终的状态吗？有没有可能动态改变？&lt;/p&gt;
&lt;p&gt;我们在创建 &lt;code&gt;Deployment&lt;/code&gt; 时会指定 Pod 的副本数，但是其实际副本数并不一定是一成不变的。假如集群中还有定义 HPA，那么 Pod 的副本数就可能随着一些外界因素（比如内存、CPU 使用率或者自定义 metric）而改变，而且如果集群中还有运行自定义的控制器话，那么也有可能修改应用的实例数量。在有多个控制器同时控制某个资源对象时，如何确保控制器之间不会发生冲突，资源对象的状态可预期？可以使用&lt;a href=&#34;https://kubernetes.io/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#monitoring-admission-webhooks&#34;&gt;动态准入控制&lt;/a&gt;来达到这一点。&lt;/p&gt;
&lt;h3 id=&#34;kubernetes-原生应用&#34;&gt;Kubernetes 原生应用&lt;/h3&gt;
&lt;p&gt;我们都知道要想运行一个应用至少需要以下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用的业务逻辑（代码）、运行时（可运行的二进制文件、字节码或脚本）。&lt;/li&gt;
&lt;li&gt;应用的配置注入（配置文件、环境变量等），身份、路由、服务暴露等满足应用的安全性和可访问性。&lt;/li&gt;
&lt;li&gt;应用的生命周期管理（各种 Controller 登场）。&lt;/li&gt;
&lt;li&gt;可观察性、可运维、网络和资源及环境依赖、隔离性等。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图展示了基于 Kubernetes 原语及 PaaS 平台资源的 Kubernetes 原生应用的组成。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;kubernetes-native-application-motion.gif&#34; alt=&#34;Kubernetes 原生应用&#34;&gt;&lt;/p&gt;
&lt;p&gt;我们都知道 Kubernetes 提供了大量的&lt;a href=&#34;https://kubernetes.io/docs/concepts/&#34;&gt;原语&lt;/a&gt;，用户可以基于这些原语来编排服务，管理应用的生命周期。上图展示的是基于 Kubernetes 原生应用可以使用的 Kubernetes 原语、扩展及平台层资源，从内向外的对象跟应用程序（业务逻辑）的关联度依次降低，到最外层基本只剩下平台资源依赖，已经与 Kubernetes 几乎没有关系了。该图里仅展示了部分资源和对象（包含阿里巴巴开源的 &lt;a href=&#34;https://github.com/openkruise/kruise&#34;&gt;OpenKruise&lt;/a&gt;、Istio），实际上 &lt;a href=&#34;https://operatorhub.io/&#34;&gt;Operator&lt;/a&gt; 资源之丰富，也是 Kubernetes 生态如此繁荣的原因之一。&lt;/p&gt;
&lt;p&gt;Kubernetes 本身的原语、资源对象、配置、常用的 CRD 扩展有几十、上百个之多。开发者需要了解这些复杂的概念吗？我只是想部署一个应用而已！不用所对于应用开发者，即使对于基础实施开发和运维人员也需要很陡峭的学习曲线才能完全掌握它。&lt;/p&gt;
&lt;p&gt;我将 Kubernetes 原生应用所需要的定义和资源进行了分层：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心层&lt;/strong&gt;：应用逻辑、服务定义、生命周期控制；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隔离与服务访问层&lt;/strong&gt;：资源限制与隔离、配置、身份、路由规则等；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;调度层&lt;/strong&gt;：各种调度控制器，这也是 Kubernetes 原生应用的主要扩展层；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;资源层&lt;/strong&gt;：提供网络、存储和其他平台资源；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而这些不同的层，完全可以将其职责分配给相应的人员，比如核心层是由应用程序开发者负责，将其职责分离，可以很大程度上降低开发和运维的复杂度。&lt;/p&gt;
&lt;p&gt;云原生应用落实到 Kubernetes 平台之上，仅仅利用 Kubernetes 的对象原语已很难描述一个复杂的应用程序，所以诞生了各种各样的 Operator，但这也仅仅解决了单个应用的定义，对于应用的打包封装则无能为力。&lt;/p&gt;
&lt;p&gt;同一个资源对象又有多种实现方式，比如 Ingress 就有 &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1DnsHtdHbxjvHmxvlu7VhzWcWgLAn_Mc5L1WlhLDA__k/edit#gid=0&#34;&gt;10 多种实现&lt;/a&gt;，PV 就更不用说，对于对于开发者究竟如何选择，平台如何管理，这都是让人很头疼的问题。而且有时候平台所提供的扩展能力还可能会有冲突，这些能力有的可能互不相干，有的可能会有正交，有的可能完全重合。且应用本身与运维特性之间存在太多耦合，不便于复用。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;resources-motion.gif&#34; alt=&#34;资源交集动画&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图中不同颜色的方框代表不同的资源类别，红线框代表不能为一个资源同时应用该配置，否则会出现冲突，不同的颜色上面是一个动画，展示的是部分资源组合。图中仅包含了部分 Kubernetes 中的原语和 Istio 中的资源对象组合及自定义扩展，实际上用户可以根据应用的自身特点，基于 Kubernetes 原语和 CRD 创建出千变万化的组合。&lt;/p&gt;
&lt;p&gt;为了管理这些应用诞生出了众多的 &lt;a href=&#34;https://github.com/operator-framework/awesome-operators&#34;&gt;Operator&lt;/a&gt;。Kubernetes 1.7 版本以来就引入了&lt;a href=&#34;https://kubernetes.io/docs/concepts/api-extension/custom-resources/&#34;&gt;自定义控制器&lt;/a&gt;的概念，该功能可以让开发人员扩展添加新功能，更新现有的功能，并且可以自动执行一些管理任务，这些自定义的控制器就像 Kubernetes 原生的组件一样，Operator 直接使用 Kubernetes API进行开发，也就是说它们可以根据这些控制器内部编写的自定义规则来监控集群、更改 Pods/Services、对正在运行的应用进行扩缩容。&lt;/p&gt;
&lt;p&gt;Operator 的本质是一种调节器模式（Reconciler Pattern）的应用，跟 Kubernetes 本身的实现模式是一样的，用于管理云原生应用，协调应用的实际状态达到预期状态。&lt;/p&gt;
&lt;p&gt;调节器模式的四个原则：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;所有的输入和输出都使用数据结构。&lt;/li&gt;
&lt;li&gt;确保数据结构是不可变的。&lt;/li&gt;
&lt;li&gt;保持资源映射简单。&lt;/li&gt;
&lt;li&gt;使实际状态符合预期状态。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;云原生应用走向碎片化&#34;&gt;云原生应用走向碎片化&lt;/h2&gt;
&lt;p&gt;利用声明式 API 及调节器模式，理论上可以在 Kubernetes 上部署任何可声明应用，但是在 Operator 出现之前，管理 Kubernetes 上的有状态应用一直是一个难题，随着 Operator 模式的确立，该难题已得以解决，并促进了 Kubernetes 生态的进一步发展。随着该生态的繁荣，有一种碎片化的特征正在显现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;云原生应用碎片化的体现&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Operator 模式将运维人员的反应式经验转化成基于 &lt;code&gt;Reconcile&lt;/code&gt; 模式的代码，统一了有状态应用的管理模式，极大得扩展了 Kubernetes 应用生态。&lt;/li&gt;
&lt;li&gt;开发者在引用 Operator 所提供的能力时没有统一的视图，加大了基础设施运维与开发者之间的沟通成本。&lt;/li&gt;
&lt;li&gt;Operator 总体上治理松散，没有统一的管控机制，在同时应用时可能导致互相冲突或无法预期的结果发生。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;有状态应用管理难题&#34;&gt;有状态应用管理难题&lt;/h3&gt;
&lt;p&gt;Kubernetes 对于无状态应用的管理很出色，但是对于有状态应用就不是那么回事了。虽然 StatefulSet 可以帮助管理有状态应用，但是这还远远不够，有状态应用往往有复杂的依赖。声明式的 API 里往往要加载着大量的配置和启动脚本，才能实现一个复杂应用的 Kubernetes 化。&lt;/p&gt;
&lt;p&gt;例如在 2017 年初，Operator Framework 出现之前，需要使用大量的 &lt;code&gt;ConfigMap&lt;/code&gt;、复杂的启动脚本才能&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/guide/migrating-hadoop-yarn-to-kubernetes.html&#34;&gt;在 Kubernetes 上定义 Hadoop YARN&lt;/a&gt; 和&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/usecases/running-spark-with-kubernetes-native-scheduler.html&#34;&gt;运行 Spark&lt;/a&gt;。虽然 &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&#34;&gt;&lt;code&gt;StatefulSet&lt;/code&gt;&lt;/a&gt; 号称可以解决有状态应用的部署问题，但是它主要是保证了 Pod 的在启动、伸缩时的顺序和使 Pod 具有稳定的标识。但是很多分布式应用来说并不仅依靠启动顺序就可以保证其状态，根据其在分布式应用中的角色不同（master/worker）而需要有大量的自定义配置，在没有 Operator 之前这些配置通常是通过一些自定义脚本来实现，这些脚本可能存在于应用镜像中，也可以通过 &lt;code&gt;ConfigMap&lt;/code&gt; 挂在到容器运行时，但无论如何这些脚本都可能因为散落在各处，这些脚本还是面向过程的，跟在 Kubernetes 诞生之前的运维方式毫无二致，这极其不便于版本控制和运维管理。&lt;/p&gt;
&lt;h3 id=&#34;operator-统一了-kubernetes-应用运维框架&#34;&gt;Operator 统一了 Kubernetes 应用运维框架&lt;/h3&gt;
&lt;p&gt;Operator 大大增强了 Kubernetes 的可扩展性，丰富了以 Kubernetes 为基础的云原生生态，许多原先不是为 Kubernetes 而构建的应用纷纷通过&lt;a href=&#34;https://zhuanlan.zhihu.com/p/54633203&#34;&gt;构建自己的 Operator&lt;/a&gt; 迁移到 Kubernetes 上。还有一些直接基于 Kubernetes 构建的 Service Mesh、Serverless 框架，它们应用 Operator 模式（如 &lt;a href=&#34;https://istio.io&#34;&gt;Istio&lt;/a&gt;、&lt;a href=&#34;https://knative.dev&#34;&gt;Knative&lt;/a&gt;），试图成为云原生应用的基础设施层，补齐 Kubernetes 在服务治理、无服务架构等方面的短板，随着大量的 CRD、Operator 控制器的出现，而 Kubernetes 却无法以应用的视角来管理这些能力及其背后零散的 CRD，这使得云原生应用碎片化。&lt;/p&gt;
&lt;p&gt;Operator 百花齐放，在没有一个大一统的视图之前，各个控制器之间存在着这样的关系：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;独立&lt;/strong&gt;：互不干涉，比如 Controller 与服务发现之间就不存在冲突。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可组合&lt;/strong&gt;：例如 &lt;code&gt;Service&lt;/code&gt;、&lt;code&gt;VirtualService&lt;/code&gt;、&lt;code&gt;DestinationRule&lt;/code&gt; 同属一类资源（可访问性与路由），就是可组合的（后两者是 Istio 中的 CRD，用于流量管理）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;有冲突&lt;/strong&gt;：例如图中的 &lt;code&gt;CronHorizontalPodAutoscaler&lt;/code&gt;（CRD）、&lt;code&gt;HorizontalPodAutoscaler&lt;/code&gt;（Kubernetes 内置），同时使用可能导致无法意料的情况发生。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;正是以为这样复杂的关系，导致其无法做到开箱即用，还需要基础设施团队基于云原生社区和生态自己构建出来的，比如&lt;a href=&#34;https://jimmysong.io/awesome-cloud-native/#application-delivery&#34;&gt;应用交付领域&lt;/a&gt;的系列开源项目。&lt;/p&gt;
&lt;h2 id=&#34;云原生应用管理工具-helm&#34;&gt;云原生应用管理工具 Helm&lt;/h2&gt;
&lt;p&gt;Kubernetes 之上有很多能力缺失，比如应用构建、发布、管理和运维等，Helm 的出现主要补偿了应用打包和版本管理的缺陷。其中云原生应用的配置包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用程序启动时加载的配置文件；&lt;/li&gt;
&lt;li&gt;应用程序的运维配置，如资源申请限额；&lt;/li&gt;
&lt;li&gt;应用程序的服务发现配置；&lt;/li&gt;
&lt;li&gt;应用程序的工作负载、发布策略、依赖等；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些配置可以存在于 &lt;code&gt;ConfigMap&lt;/code&gt;、&lt;code&gt;Deployment&lt;/code&gt;、&lt;code&gt;Service&lt;/code&gt;、&lt;code&gt;Ingress&lt;/code&gt; 等 Kubernetes 的多个资源文件中，如何保证应用程序的复用性？应用程序之间有依赖该如何解决？这是时候你可能自然的想到了 Helm。&lt;/p&gt;
&lt;p&gt;云原生应用打包和发布管理&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Helm 通过 chart 模板，提高了应用程序的复用性并解决了部分依赖问题；&lt;/li&gt;
&lt;li&gt;Chart 仓库提供了云原生应用程序的统一管控视图；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Release&lt;/code&gt; 概念的引入，使得云原生应用版本化管理进一步加强；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Helm 主要关注的是 &lt;a href=&#34;https://12factor.net/zh_cn/&#34;&gt;12 因素应用&lt;/a&gt;法则&lt;a href=&#34;https://12factor.net/zh_cn/build-release-run&#34;&gt;构建、发布、运行&lt;/a&gt;这一原则中的”发布”这一环节。下图是 Helm v3 的架构图。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;helm-chart.png&#34; alt=&#34;Helm3 架构&#34;&gt;&lt;/p&gt;
&lt;p&gt;Helm 可以安装本地或者远程的 chart，当 chart 安装到 Kubernetes 中后就会创建一个 release，每次更新该 chart 的配置并执行 &lt;code&gt;helm upgrade&lt;/code&gt;， release 的版本数就会加 1，开发者可以升级 chart 或回滚到历史版本。&lt;/p&gt;
&lt;h3 id=&#34;打包配置和发布&#34;&gt;打包、配置和发布&lt;/h3&gt;
&lt;p&gt;Helm 和 chart 的主要作用是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用程序封装&lt;/li&gt;
&lt;li&gt;版本管理&lt;/li&gt;
&lt;li&gt;依赖检查&lt;/li&gt;
&lt;li&gt;便于应用程序分发&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;打包&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Helm 采用 &lt;a href=&#34;https://helm.sh/docs/topics/charts/&#34;&gt;Chart&lt;/a&gt; 的格式来标准化描述应用，可以将目录打包成版本化的压缩包进行部署理论上一个 Chart 是可以嵌套若干个 Chart 并定义依赖关系，组织形式非常灵活。Helm chart 用于打包 Kubernetes 原生应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;配置&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;应用配置参数，在 Chart 中由 &lt;code&gt;values.yaml&lt;/code&gt; 和命令行参数组成。Chart 采用 Go Template 的特性和 &lt;code&gt;values.yaml&lt;/code&gt; 对部署的模板文件进行参数渲染，也可以通过 &lt;code&gt;helm&lt;/code&gt; 命令 &lt;code&gt;--set key=value&lt;/code&gt; 的方式进行参数赋值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;发布&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Release 代表 Chart 在集群中的运行实例，Helm 围绕 Release 对应用提供了强大的生命周期管理能力，包括 Release 的查询、安装、更新、删除、回滚等。&lt;/p&gt;
&lt;h2 id=&#34;云原生应用&#34;&gt;云原生应用&lt;/h2&gt;
&lt;p&gt;以上关注的点都是基于 Kubernetes 原语的实现，虽然基于 Kubernetes 构建的 PaaS 平台部分屏蔽了底层基础设施的差异，但是仍有很多云服务是无法通过 Kubernetes 创建，或者需要提前创建供 Kubernetes 原生应用使用的，这些应用通常不运行在 Kubernetes 集群中。因此创建和管理一个云原生应用程序需要考虑以下方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;运行时：ECS、Docker、KataContainer、gVisor 等；&lt;/li&gt;
&lt;li&gt;资源隔离性：多租户、VPC、Namespace、防火墙；&lt;/li&gt;
&lt;li&gt;资源调度：各种类型的 controller；&lt;/li&gt;
&lt;li&gt;网络可达性：Service、Ingress、Egress、Gateway、VirtualService、DestinationRule、LoadBalancer、ServiceEntry 等；&lt;/li&gt;
&lt;li&gt;可观测性：日志、分布式追踪、指标；&lt;/li&gt;
&lt;li&gt;安全性：SecurityPolicy、NetworkPolicy、AuthorizationPolicy；&lt;/li&gt;
&lt;li&gt;平台资源申请：数据库、存储等；&lt;/li&gt;
&lt;li&gt;运行与隔离：ECS、Docker、KataContainer 等；&lt;/li&gt;
&lt;li&gt;资源分配和调度：各种控制器；&lt;/li&gt;
&lt;li&gt;环境隔离：Namespace、多租户、VPC、防火墙、LimitRange、Resources；&lt;/li&gt;
&lt;li&gt;可访问性：Service、Ingress、Egress、Gateway、LoadBalancer、VirtualService、DestinationRule、ServiceEntry；&lt;/li&gt;
&lt;li&gt;状态管理：Operator；&lt;/li&gt;
&lt;li&gt;可观察性：日志、监控、指标；&lt;/li&gt;
&lt;li&gt;安全性：SecurityPolicy、ServiceAccount；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;云原生应用分层模型&#34;&gt;云原生应用分层模型&lt;/h3&gt;
&lt;p&gt;那么究竟如何来给云原生应用分层，化繁就简？近几年来，基于 Kubernetes 的应用呈爆炸式发展，光是在&lt;a href=&#34;https://jimmysong.io/awesome-cloud-native/#application-delivery&#34;&gt;应用交付领域&lt;/a&gt;的开源项目就达几十个之多。下图展示我根据这些项目的特性而绘制的 App Delivery Landscape。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;cloud-native-app.png&#34; alt=&#34;云原生应用的分层模型&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;应用定义和包装&lt;/strong&gt;：云原生应用的最上层，直接定义云原生应用的组成形式，解决云原生应用之间的依赖关系，并封装成发布包，如 Helm、CNAB，还有云原生变成语言 Pulumi 和 Ballerina，基于 API 的方式来编排云原生应用；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;负载定义&lt;/strong&gt;：基于 Kubernetes Operator，大多是 Serverless 负载，既负责了负载的定义又负责了生命周期管理。&lt;a href=&#34;https://istio.io&#34;&gt;Istio&lt;/a&gt; 是比较特殊的存在，它不仅管理服务间的流量，还负责安全性、可观察性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;应用发布和上线&lt;/strong&gt;：关注应用的构建和发布、GitOps、发布策略等，这也是云原生应用全景中最丰富的部分之一；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kubernetes 原语&lt;/strong&gt;：Kubernetes 本身提供的原语，Operator 基于此构建；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上为我个人分类的云原生应用全景模型，仅限于 Kubernetes 之上的应用，对于其他非 Kubernetes 应用非本文的考虑范围。另外，CNCF SIG App Delivery 中也给出的云原生应用的分层模型，其模型将非 Kubernetes 应用场景也纳入了考虑，详见：&lt;a href=&#34;https://docs.google.com/document/d/1gMhRz4vEwiHa3uD8DqFKHGTSxrVJNgkLG2WZWvi9lXo/edit#heading=h.h9so53gv5zen&#34;&gt;The Dictionary of Cloud-Native App Delivery&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;Platform/Kuberntes，Kubernetes 仅仅是屏蔽了平台的一些差异，但是对于最上层的应用来说，没有涉及，用户需要自己来基于各种开源组件来搭积木。&lt;/p&gt;
&lt;h3 id=&#34;oam开放应用模型&#34;&gt;OAM（开放应用模型）&lt;/h3&gt;
&lt;p&gt;那么以上这么多应用有哪些共性，能不能再进一步抽象呢？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有应用是都以容器作为运行时环境（ContainerizedWorkload），这是 OAM 中的核心 Workload 类型；&lt;/li&gt;
&lt;li&gt;在应用发布和上线方面，有些是属于应用的运维特征，需要根据实际需求组合和变更，这些是持续变动的部分；&lt;/li&gt;
&lt;li&gt;要实现某些复杂的应用管控，需要使用到多个 CRD 的组合，比如 Istio 中的让流量根据白分比切分到不同的而服务，就需要部署 Istio Operator，并声明 &lt;code&gt;VirtualService&lt;/code&gt;、&lt;code&gt;DestinationRule&lt;/code&gt;，二者同时使用；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个 &lt;code&gt;ApplicationConfiguration&lt;/code&gt; 的 Runtime 的正常流程应该是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用开发者创建自己的 &lt;code&gt;Component&lt;/code&gt;，在 &lt;code&gt;Component&lt;/code&gt; 中描述要应用相关的信息，如应用名称、镜像配置、环境变量等，应用到 Kubernetes cluster 中；&lt;/li&gt;
&lt;li&gt;运维创建各种运维策略，如发布策略、网络策略等等，发布时由 AppConfig 对象关联要发布的 &lt;code&gt;Component&lt;/code&gt; 和本次的运维策略，apply 到集群中，集群的 OAM operator watch 到一次 &lt;code&gt;ApplicationConfiguration&lt;/code&gt;的下发，生成 &lt;code&gt;Component&lt;/code&gt; 对应的 &lt;code&gt;Workload&lt;/code&gt; 和 &lt;code&gt;Trait&lt;/code&gt;，&lt;code&gt;Trait&lt;/code&gt; controller 将本次的 &lt;code&gt;Trait&lt;/code&gt; 策略应用到本次要管理的 &lt;code&gt;Workload&lt;/code&gt; 当中，最终到达终态，完成一次发布。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;OAM 是对 Kubernetes 友好的，一样采用声明式 API 的理念开发。如果你已经编写了现成的 CRD Operator，可以平滑的接入到 OAM 体系中。OAM 以应用为中心，高度可扩展，扩展点包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Workload：扩展各种运行时类型，不仅限于容器运行时，还可以定义更多其他运行时，比如 Serverless 负载、虚拟机、数据库、网络等；例如，Pod、无服务器函数、数据存储、消息队列或任何其他类型的工作负载，这些都是应用程序开发人员需要设计一个完整的应用程序所需要的，可以直接引用 Kubernetes 的 CRD；&lt;/li&gt;
&lt;li&gt;Trait：各种运维规则，比如扩缩容、流量控制、安全性；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;生态&#34;&gt;生态&lt;/h3&gt;
&lt;p&gt;以前 CNCF 的主要关注群体大多是基础设施领域的技术人员，但是自 2019 年 9 月，&lt;a href=&#34;https://www.infoq.cn/article/Cdw7ISlEqKilGyN9V3Pj&#34;&gt;CNCF 宣布成立 SIG App Delivery&lt;/a&gt; 后，CNCF 正在将应用开发者和运维人员更紧密的联系在一起。&lt;a href=&#34;https://github.com/cncf/sig-app-delivery&#34;&gt;应用交付 SIG&lt;/a&gt; 的使命是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在与开发、分发、部署、管理和运行安全的云原生应用相关的领域进行合作，目标是以云原生方式交付应用。&lt;/li&gt;
&lt;li&gt;发展信息资源，包括指南、教程和白皮书，让社区了解最佳实践和应用交付的价值。&lt;/li&gt;
&lt;li&gt;识别合适的项目和现状的差距，定期向 TOC 更新，并以结构化的方式向 TOC 提出行动建议。这包括帮助 TOC 评估和对潜在的新项目进行尽职调查。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前 OAM 定义的云原生应用模型已有以下项目支持。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://crossplane.io/&#34;&gt;Crossplane&lt;/a&gt;：这是一个开源的 Kubernetes 扩展组件，适用于主流公有云平台，使用 &lt;code&gt;kubectl&lt;/code&gt; 配置和管理基础架构、服务和应用。对于 OAM 的支持详见&lt;a href=&#34;https://crossplane.io/docs/v0.11/getting-started/run-applications.html&#34;&gt;运行应用程序&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://googlecontainertools.github.io/kpt/&#34;&gt;KPT&lt;/a&gt;：Kpt（发音为 &amp;ldquo;keep&amp;rdquo;）是一个在资源配置之上构建声明性工作流的开源工具。它的 git + YAML 架构意味着它只需与现有的工具、框架和平台一起工作。Kpt 包括了获取、显示、自定义、更新、验证和应用 Kubernetes 配置的解决方案。对 OAM 的支持详见 &lt;a href=&#34;https://googlecontainertools.github.io/kpt/guides/ecosystem/oam/&#34;&gt;使用 kpt 来管理由开放应用模型（OAM）定义的自定义 Kubernetes 应用程序&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;应用交付领域相关的开源项目还有很多，详见 &lt;a href=&#34;https://jimmysong.io/awesome-cloud-native/#application-delivery&#34;&gt;Awesome Cloud Native&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;基于 Kubernetes 的云原生生态发展至今已有 6 年时间，当前已步入了普及推广阶段。可以说谁云原生应用定义的制高点，就可以掌握云原生的未来。从前我们是新技术浪潮的追随者，现在我们抓住时代的基于，参与标准制定、引领云原生的浪潮！欢迎加入 &lt;a href=&#34;https://oam.dev/&#34;&gt;OAM 社区&lt;/a&gt;，一起参与进来，把国人参与指定的标准推向世界。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.ibm.com/technologies/containers/blogs/kubernetes-helm-3/&#34;&gt;Do you know what’s in Helm 3? - developer.ibm.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.redhat.com/cms/managed-files/cm-oreilly-kubernetes-patterns-ebook-f19824-201910-en.pdf&#34;&gt;O’Reilly: Kubernetes patterns for designing cloud-native apps - redhat.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/document/d/1gMhRz4vEwiHa3uD8DqFKHGTSxrVJNgkLG2WZWvi9lXo/edit#heading=h.h9so53gv5zen&#34;&gt;The Dictionary of Cloud-Native App Delivery - docs.google.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.infoq.cn/article/Cdw7ISlEqKilGyN9V3Pj&#34;&gt;CNCF 宣布成立应用交付领域小组，正式开启云原生应用时代 - infoq.cn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/c7A8lOdAKkW25GoqmwOgWg&#34;&gt;OAM v1alpha2 新版发布：平衡标准与可扩展性，灵活接入 CRD operator - mp.weixin.qq.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/54633203&#34;&gt;Kubernetes API 与 Operator，不为人知的开发者战争 - zhuanlan.zhihu.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/cloud-native-era/&#34;&gt;云原生时代——投资人视角下的云原生趋势思考 - cloudnative.to&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>云原生应用之路</title>
      <link>https://jimmysong.io/blog/from-kubernetes-to-cloud-native/</link>
      <pubDate>Wed, 20 Dec 2017 15:08:02 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/from-kubernetes-to-cloud-native/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;从Kubernetes到Cloud Native——云原生应用之路&lt;/strong&gt;，这是我最近在 &lt;a href=&#34;http://bj2017.archsummit.com/presentation/306&#34;&gt;ArchSummit2017北京站&lt;/a&gt; 和 &lt;a href=&#34;https://www.kubernetes.org.cn/3211.html&#34;&gt;数人云&amp;amp;TalkingData合办的Service Mesh is comming meetup&lt;/a&gt; 中分享的话题。&lt;/p&gt;
&lt;p&gt;本文简要介绍了容器技术发展的路径，为何Kubernetes的出现是容器技术发展到这一步的必然选择，而为何Kuberentes又将成为云原生应用的基石。&lt;/p&gt;
&lt;p&gt;我的分享按照这样的主线展开：容器-&amp;gt;Kubernetes-&amp;gt;微服务-&amp;gt;Cloud Native（云原生）-&amp;gt;Service Mesh（服务网格）-&amp;gt;使用场景-&amp;gt;Open Source（开源）。&lt;/p&gt;
&lt;h2 id=&#34;容器&#34;&gt;容器&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;容器——Cloud Native的基石&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;容器最初是通过开发者工具而流行，可以使用它来做隔离的开发测试环境和持续集成环境，这些都是因为容器轻量级，易于配置和使用带来的优势，docker和docker-compose这样的工具极大的方便的了应用开发环境的搭建，开发者就像是化学家一样在其中小心翼翼的进行各种调试和开发。&lt;/p&gt;
&lt;p&gt;随着容器的在开发者中的普及，已经大家对CI流程的熟悉，容器周边的各种工具蓬勃发展，俨然形成了一个小生态，在2016年达到顶峰，下面这张是我画的容器生态图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/container-ecosystem.png&#34; alt=&#34;容器生态图 Container ecosystem&#34;&gt;&lt;/p&gt;
&lt;p&gt;该生态涵盖了容器应用中从镜像仓库、服务编排、安全管理、持续集成与发布、存储和网络管理等各个方面，随着在单主机中运行容器的成熟，集群管理和容器编排成为容器技术亟待解决的问题。譬如化学家在实验室中研究出来的新产品，如何推向市场，进行大规模生产，成了新的议题。&lt;/p&gt;
&lt;h2 id=&#34;为什么使用kubernetes&#34;&gt;为什么使用Kubernetes&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Kubernetes——让容器应用进入大规模工业生产。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kubernetes是容器编排系统的事实标准&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在单机上运行容器，无法发挥它的最大效能，只有形成集群，才能最大程度发挥容器的良好隔离、资源分配与编排管理的优势，而对于容器的编排管理，Swarm、Mesos和Kubernetes的大战已经基本宣告结束，kubernetes成为了无可争议的赢家。&lt;/p&gt;
&lt;p&gt;下面这张图是Kubernetes的架构图（图片来自网络），其中显示了组件之间交互的接口CNI、CRI、OCI等，这些将Kubernetes与某款具体产品解耦，给用户最大的定制程度，使得Kubernetes有机会成为跨云的真正的云原生应用的操作系统。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/kubernetes-high-level-component-archtecture.jpg&#34; alt=&#34;Kuberentes架构&#34;&gt;&lt;/p&gt;
&lt;p&gt;随着Kubernetes的日趋成熟，“Kubernetes is becoming boring”，基于该“操作系统”之上构建的适用于不同场景的应用将成为新的发展方向，就像我们将石油开采出来后，提炼出汽油、柴油、沥青等等，所有的材料都将找到自己的用途，Kubernetes也是，毕竟我们谁也不是为了部署和管理容器而用Kubernetes，承载其上的应用才是价值之所在。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;云原生的核心目标&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/cloud-native-core-target.jpg&#34; alt=&#34;Cloud Native Core target&#34;&gt;&lt;/p&gt;
&lt;p&gt;云已经可以为我们提供稳定可以唾手可得的基础设施，但是业务上云成了一个难题，Kubernetes的出现与其说是从最初的容器编排解决方案，倒不如说是为了解决应用上云（即云原生应用）这个难题。&lt;/p&gt;
&lt;p&gt;包括微服务和FaaS/Serverless架构，都可以作为云原生应用的架构。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/redpoint-faas-landscape.jpg&#34; alt=&#34;FaaS Landscape&#34;&gt;&lt;/p&gt;
&lt;p&gt;但就2017年为止，kubernetes的主要使用场景也主要作为应用开发测试环境、CI/CD和运行Web应用这几个领域，如下图&lt;a href=&#34;http://thenewstack.io&#34;&gt;TheNewStack&lt;/a&gt;的Kubernetes生态状况调查报告所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5mxr6fxtj31kw11q484.jpg&#34; alt=&#34;Workloads running on Kubernetes&#34;&gt;&lt;/p&gt;
&lt;p&gt;另外基于Kubernetes的构建PaaS平台和Serverless也处于爆发的准备的阶段，如下图中Gartner的报告中所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5my2jtxzj315o0z8dkr.jpg&#34; alt=&#34;Gartner技术爆发趋势图2017&#34;&gt;&lt;/p&gt;
&lt;p&gt;当前各大公有云如Google GKE、微软Azure ACS、亚马逊EKS（2018年上线）、VmWare、Pivotal、腾讯云、阿里云等都提供了Kuberentes服务。&lt;/p&gt;
&lt;h2 id=&#34;微服务&#34;&gt;微服务&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;微服务——Cloud Native的应用架构。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;下图是&lt;a href=&#34;https://developers.redhat.com/blog/author/bibryam/&#34;&gt;Bilgin Ibryam&lt;/a&gt;给出的微服务中应该关心的主题，图片来自&lt;a href=&#34;https://developers.redhat.com/blog/2016/12/09/spring-cloud-for-microservices-compared-to-kubernetes/&#34;&gt;RedHat Developers&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/microservices-concerns.jpg&#34; alt=&#34;Microservices concerns&#34;&gt;&lt;/p&gt;
&lt;p&gt;微服务带给我们很多开发和部署上的灵活性和技术多样性，但是也增加了服务调用的开销、分布式系统管理、调试与服务治理方面的难题。&lt;/p&gt;
&lt;p&gt;当前最成熟最完整的微服务框架可以说非&lt;a href=&#34;https://spring.io/&#34;&gt;Spring&lt;/a&gt;莫属，而Spring又仅限于Java语言开发，其架构本身又跟Kubernetes存在很多重合的部分，如何探索将Kubernetes作为微服务架构平台就成为一个热点话题。&lt;/p&gt;
&lt;p&gt;就拿微服务中最基础的&lt;strong&gt;服务注册发现&lt;/strong&gt;功能来说，其方式分为&lt;strong&gt;客户端服务发现&lt;/strong&gt;和&lt;strong&gt;服务端服务发现&lt;/strong&gt;两种，Java应用中常用的方式是使用Eureka和Ribbon做服务注册发现和负载均衡，这属于客户端服务发现，而在Kubernetes中则可以使用DNS、Service和Ingress来实现，不需要修改应用代码，直接从网络层面来实现。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/service-discovery-in-microservices.png&#34; alt=&#34;两种服务发现方式&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;cloud-native&#34;&gt;Cloud Native&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;DevOps——通向云原生的云梯&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;CNCF（云原生计算基金会）给出了云原生应用的三大特征：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;容器化包装&lt;/strong&gt;：软件应用的进程应该包装在容器中独立运行。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动态管理&lt;/strong&gt;：通过集中式的编排调度系统来动态的管理和调度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;微服务化&lt;/strong&gt;：明确服务间的依赖，互相解耦。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图是我整理的关于云原生所需要的能力和特征。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/cloud-native-architecutre-mindnode.jpg&#34; alt=&#34;Cloud Native Features&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cncf.io&#34;&gt;CNCF&lt;/a&gt;所托管的应用（目前已达12个），即朝着这个目标发展，其公布的&lt;a href=&#34;https://github.com/cncf/landscape&#34;&gt;Cloud Native Landscape&lt;/a&gt;，给出了云原生生态的参考体系。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5myp6ednj31kw0w0u0x.jpg&#34; alt=&#34;Cloud Native Landscape v1.0&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;使用Kubernetes构建云原生应用&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们都是知道Heroku推出了适用于PaaS的&lt;a href=&#34;https://12factor.net/&#34;&gt;12 factor app&lt;/a&gt;的规范，包括如下要素：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基准代码&lt;/li&gt;
&lt;li&gt;依赖管理&lt;/li&gt;
&lt;li&gt;配置&lt;/li&gt;
&lt;li&gt;后端服务&lt;/li&gt;
&lt;li&gt;构建，发布，运行&lt;/li&gt;
&lt;li&gt;无状态进程&lt;/li&gt;
&lt;li&gt;端口绑定&lt;/li&gt;
&lt;li&gt;并发&lt;/li&gt;
&lt;li&gt;易处理&lt;/li&gt;
&lt;li&gt;开发环境与线上环境等价&lt;/li&gt;
&lt;li&gt;日志作为事件流&lt;/li&gt;
&lt;li&gt;管理进程&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;另外还有补充的三点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;API声明管理&lt;/li&gt;
&lt;li&gt;认证和授权&lt;/li&gt;
&lt;li&gt;监控与告警&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果落实的具体的工具，请看下图，使用Kubernetes构建云原生架构：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/building-cloud-native-architecture-with-kubernetes.png&#34; alt=&#34;Building a Cloud Native Architecture with Kubernetes followed 12 factor app&#34;&gt;&lt;/p&gt;
&lt;p&gt;结合这12因素对开发或者改造后的应用适合部署到Kubernetes之上，基本流程如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/creating-kubernetes-native-app.jpg&#34; alt=&#34;Creating Kubernetes native app&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;迁移到云架构&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;迁移到云端架构，相对单体架构来说会带来很多挑战。比如自动的持续集成与发布、服务监控的变革、服务暴露、权限的管控等。这些具体细节请参考&lt;strong&gt;Kubernetes-handbook&lt;/strong&gt;中的说明：&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34;&gt;https://jimmysong.io/kubernetes-handbook&lt;/a&gt;，在此就不细节展开，另外推荐一本我翻译的由Pivotal出品的电子书——&lt;a href=&#34;https://content.pivotal.io/ebooks/migrating-to-cloud-native-application-architectures&#34;&gt;Migrating to Cloud Native Application Architectures&lt;/a&gt;，地址：&lt;a href=&#34;https://jimmysong.io/migrating-to-cloud-native-application-architectures/&#34;&gt;https://jimmysong.io/migrating-to-cloud-native-application-architectures/&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;service-mesh&#34;&gt;Service Mesh&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Services for show, meshes for a pro.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Kubernetes中的应用将作为微服务运行，但是Kuberentes本身并没有给出微服务治理的解决方案，比如服务的限流、熔断、良好的灰度发布支持等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Service mesh可以用来做什么&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Traffic Management：API网关&lt;/li&gt;
&lt;li&gt;Observability：服务调用和性能分析&lt;/li&gt;
&lt;li&gt;Policy Enforcement：控制服务访问策略&lt;/li&gt;
&lt;li&gt;Service Identity and Security：安全保护&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Service mesh的特点&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;专用的基础设施层&lt;/li&gt;
&lt;li&gt;轻量级高性能网络代理&lt;/li&gt;
&lt;li&gt;提供安全的、快速的、可靠地服务间通讯&lt;/li&gt;
&lt;li&gt;扩展kubernetes的应用负载均衡机制，实现灰度发布&lt;/li&gt;
&lt;li&gt;完全解耦于应用，应用可以无感知，加速应用的微服务和云原生转型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用Service Mesh将可以有效的治理Kuberentes中运行的服务，当前开源的Service Mesh有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linkderd：&lt;a href=&#34;https://linkerd.io&#34;&gt;https://linkerd.io&lt;/a&gt;，由最早提出Service Mesh的公司&lt;a href=&#34;https://buoyant.io&#34;&gt;Buoyant&lt;/a&gt;开源，创始人来自Twitter&lt;/li&gt;
&lt;li&gt;Envoy：&lt;a href=&#34;https://www.envoyproxy.io/&#34;&gt;https://www.envoyproxy.io/&lt;/a&gt;，Lyft开源的，可以在Istio中使用Sidecar模式运行&lt;/li&gt;
&lt;li&gt;Istio：&lt;a href=&#34;https://istio.io&#34;&gt;https://istio.io&lt;/a&gt;，由Google、IBM、Lyft联合开发并开源&lt;/li&gt;
&lt;li&gt;Conduit：&lt;a href=&#34;https://conduit.io&#34;&gt;https://conduit.io&lt;/a&gt;，同样由Buoyant开源的轻量级的基于Kubernetes的Service Mesh&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外还有很多其它的Service Mesh鱼贯而出，请参考&lt;a href=&#34;https://jimmysong.io/awesome-cloud-native&#34;&gt;awesome-cloud-native&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Istio VS Linkerd&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Linkerd和Istio是最早开源的Service Mesh，它们都支持Kubernetes，下面是它们之间的一些特性对比。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Feature&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Istio&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Linkerd&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;部署架构&lt;/td&gt;
&lt;td&gt;Envoy/Sidecar&lt;/td&gt;
&lt;td&gt;DaemonSets&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;易用性&lt;/td&gt;
&lt;td&gt;复杂&lt;/td&gt;
&lt;td&gt;简单&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;支持平台&lt;/td&gt;
&lt;td&gt;kuberentes&lt;/td&gt;
&lt;td&gt;kubernetes/mesos/Istio/local&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;当前版本&lt;/td&gt;
&lt;td&gt;0.3.0&lt;/td&gt;
&lt;td&gt;1.3.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;是否已有生产部署&lt;/td&gt;
&lt;td&gt;否&lt;/td&gt;
&lt;td&gt;是&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;关于两者的架构可以参考各自的官方文档，我只从其在kubernetes上的部署结构来说明其区别。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/istio-vs-linkerd.jpg&#34; alt=&#34;istio vs linkerd&#34;&gt;&lt;/p&gt;
&lt;p&gt;Istio的组件复杂，可以分别部署的kubernetes集群中，但是作为核心路由组件&lt;strong&gt;Envoy&lt;/strong&gt;是以&lt;strong&gt;Sidecar&lt;/strong&gt;形式与应用运行在同一个Pod中的，所有进入该Pod中的流量都需要先经过Envoy。&lt;/p&gt;
&lt;p&gt;Linker的部署十分简单，本身就是一个镜像，使用Kubernetes的&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/concepts/daemonset.html&#34;&gt;DaemonSet&lt;/a&gt;方式在每个node节点上运行。&lt;/p&gt;
&lt;p&gt;更多信息请参考&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34;&gt;kubernetes-handbook&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;使用场景&#34;&gt;使用场景&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Cloud Native的大规模工业生产&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;GitOps&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;给开发者带来最大配置和上线的灵活性，践行DevOps流程，改善研发效率，下图这样的情况将更少发生。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5mzj8rj6j318g1ewtfc.jpg&#34; alt=&#34;Deployment pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;我们知道Kubernetes中的所有应用的部署都是基于YAML文件的，这实际上就是一种&lt;strong&gt;Infrastructure as code&lt;/strong&gt;，完全可以通过Git来管控基础设施和部署环境的变更。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Big Data&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Spark现在已经非官方支持了基于Kuberentes的原生调度，其具有以下特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes原生调度：与yarn、mesos同级&lt;/li&gt;
&lt;li&gt;资源隔离，粒度更细：以namespace来划分用户&lt;/li&gt;
&lt;li&gt;监控的变革：单次任务资源计量&lt;/li&gt;
&lt;li&gt;日志的变革：pod的日志收集&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Feature&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Yarn&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;queue&lt;/td&gt;
&lt;td&gt;queue&lt;/td&gt;
&lt;td&gt;namespace&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;instance&lt;/td&gt;
&lt;td&gt;ExcutorContainer&lt;/td&gt;
&lt;td&gt;Executor Pod&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;td&gt;host&lt;/td&gt;
&lt;td&gt;plugin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;heterogeneous&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;security&lt;/td&gt;
&lt;td&gt;RBAC&lt;/td&gt;
&lt;td&gt;ACL&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;下图是在Kubernetes上运行三种调度方式的spark的单个节点的应用部分对比：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/spark-on-kubernetes-with-different-schedulers.jpg&#34; alt=&#34;Spark on Kubernetes with different schedulers&#34;&gt;&lt;/p&gt;
&lt;p&gt;从上图中可以看到在Kubernetes上使用YARN调度、standalone调度和kubernetes原生调度的方式，每个node节点上的Pod内的spark Executor分布，毫无疑问，使用kubernetes原生调度的spark任务才是最节省资源的。&lt;/p&gt;
&lt;p&gt;提交任务的语句看起来会像是这样的：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;./spark-submit &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --deploy-mode cluster &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --class com.talkingdata.alluxio.hadooptest &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --master k8s://https://172.20.0.113:6443 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --kubernetes-namespace spark-cluster &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.driverEnv.SPARK_USER&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;hadoop &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.driverEnv.HADOOP_USER_NAME&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;hadoop &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executorEnv.HADOOP_USER_NAME&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;hadoop &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executorEnv.SPARK_USER&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;hadoop &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.authenticate.driver.serviceAccountName&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.driver.memory&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;100G &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executor.memory&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;10G &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.driver.cores&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;30&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executor.cores&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.driver.maxResultSize&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;10240m &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.driver.limit.cores&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;32&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.executor.limit.cores&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.executor.memoryOverhead&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;2g &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.executor.instances&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;5&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.app.name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark-pi &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.driver.docker.image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark-driver:v2.1.0-kubernetes-0.3.1-1 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.executor.docker.image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark-executor:v2.1.0-kubernetes-0.3.1-1 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.initcontainer.docker.image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;spark-init:v2.1.0-kubernetes-0.3.1-1 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --conf spark.kubernetes.resourceStagingServer.uri&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;http://172.20.0.114:31000 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;~/Downloads/tendcloud_2.10-1.0.jar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;关于支持Kubernetes原生调度的Spark请参考：https://jimmysong.io/spark-on-k8s/&lt;/p&gt;
&lt;h2 id=&#34;open-source&#34;&gt;Open Source&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Contributing is Not only about code, it is about helping a community.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;下图是我们刚调研准备使用Kubernetes时候的调研方案选择。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/0069RVTdgy1fv5mzywc83j31fk1i8qg4.jpg&#34; alt=&#34;Kubernetes solutions&#34;&gt;&lt;/p&gt;
&lt;p&gt;对于一个初次接触Kubernetes的人来说，看到这样一个庞大的架构选型时会望而生畏，但是Kubernetes的开源社区帮助了我们很多。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/kubernetes-sigs.jpg&#34; alt=&#34;Kubernetes SIG&#34;&gt;&lt;/p&gt;
&lt;p&gt;我组建了&lt;strong&gt;K8S&amp;amp;Cloud Native实战&lt;/strong&gt;微信群，参与了k8smeetup、KEUC2017、&lt;a href=&#34;https://github.com/kubernetes/kubernetes-docs-cn&#34;&gt;kubernetes-docs-cn&lt;/a&gt; Kubernetes官方中文文档项目。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;有用的资料和链接&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我的博客： &lt;a href=&#34;https://jimmysong.io&#34;&gt;https://jimmysong.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;微信群：k8s&amp;amp;cloud native实战群（见：&lt;a href=&#34;https://jimmysong.io/about&#34;&gt;https://jimmysong.io/about&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;Meetup：k8smeetup&lt;/li&gt;
&lt;li&gt;Cloud Native Go - 基于Go和React云原生Web应用开发：https://jimmysong.io/cloud-native-go&lt;/li&gt;
&lt;li&gt;Gitbook：&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34;&gt;https://jimmysong.io/kubernetes-handbook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Cloud native开源生态：&lt;a href=&#34;https://jimmysong.io/awesome-cloud-native/&#34;&gt;https://jimmysong.io/awesome-cloud-native/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;资料分享整理：&lt;a href=&#34;https://github.com/rootsongjc/cloud-native-slides-share&#34;&gt;https://github.com/rootsongjc/cloud-native-slides-share&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;迁移到云原生架构：&lt;a href=&#34;https://jimmysong.io/migrating-to-cloud-native-application-architectures/&#34;&gt;https://jimmysong.io/migrating-to-cloud-native-application-architectures/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;KubeCon + CloudNativeCon 2018年11月14-15日 上海&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>使用Helm安装Nginx ingress</title>
      <link>https://jimmysong.io/blog/install-nginx-ingress-with-helm/</link>
      <pubDate>Fri, 27 Oct 2017 19:10:59 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/install-nginx-ingress-with-helm/</guid>
      <description>
        
        
        &lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34;&gt;Nginx ingress&lt;/a&gt; 使用ConfigMap来管理Nginx配置，nginx是大家熟知的代理和负载均衡软件，比起&lt;a href=&#34;https://traefik.io&#34;&gt;Traefik&lt;/a&gt;来说功能更加强大，我们使用&lt;a href=&#34;http://helm.sh&#34;&gt;helm&lt;/a&gt;来部署，&lt;a href=&#34;https://github.com/kubernetes/charts&#34;&gt;chart&lt;/a&gt;保存在私有的仓库中，helm安装使用见&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/practice/helm.html&#34;&gt;使用Helm管理kubernetes应用&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;安装时需要用到的镜像有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sophos/nginx-vts-exporter:v0.6&lt;/li&gt;
&lt;li&gt;gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.15&lt;/li&gt;
&lt;li&gt;gcr.io/google_containers/defaultbackend:1.3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;gcr.io中的那个两个镜像我复制了一份到时速云，可供大家下载：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;index.tenxcloud.com/jimmy/defaultbackend:1.3&lt;/li&gt;
&lt;li&gt;index.tenxcloud.com/jimmy/nginx-ingress-controller:0.9.0-beta.15&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Docker hub上的那个镜像可以直接下载，所有的安装时需要的配置保存在&lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook/blob/master/manifests/nginx-ingress&#34;&gt;../manifests/nginx-ingress&lt;/a&gt;目录下。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;安装nginx-ingress chart到本地repo中&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;修改&lt;code&gt;values.yaml&lt;/code&gt;配置，启用RBAC支持，相关配置见&lt;a href=&#34;https://github.com/kubernetes/charts/tree/master/stable/nginx-ingress#configuration&#34;&gt;nginx-ingress chart&lt;/a&gt;。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm package .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;查看niginx-ingress&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ helm search nginx-ingress
NAME                	VERSION	DESCRIPTION
local/nginx-ingress 	0.8.9  	An nginx Ingress controller that uses ConfigMap...
stable/nginx-ingress	0.8.9  	An nginx Ingress controller that uses ConfigMap...
stable/nginx-lego   	0.3.0  	Chart &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; nginx-ingress-controller and kube-lego
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;使用helm部署nginx-ingress&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ helm install --name nginx-ingress local/nginx-ingress
NAME:   nginx-ingress
LAST DEPLOYED: Fri Oct &lt;span class=&#34;m&#34;&gt;27&lt;/span&gt; 18:26:58 &lt;span class=&#34;m&#34;&gt;2017&lt;/span&gt;
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt; rbac.authorization.k8s.io/v1beta1/Role
NAME                         KIND
nginx-ingress-nginx-ingress  Role.v1beta1.rbac.authorization.k8s.io

&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt; rbac.authorization.k8s.io/v1beta1/RoleBinding
nginx-ingress-nginx-ingress  RoleBinding.v1beta1.rbac.authorization.k8s.io

&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt; v1/Service
NAME                                         CLUSTER-IP      EXTERNAL-IP  PORT&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;S&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;                     AGE
nginx-ingress-nginx-ingress-controller       10.254.100.108  &amp;lt;nodes&amp;gt;      80:30484/TCP,443:31053/TCP  1s
nginx-ingress-nginx-ingress-default-backend  10.254.58.156   &amp;lt;none&amp;gt;       80/TCP                      &lt;span class=&#34;nv&#34;&gt;1s&lt;/span&gt;

&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt; extensions/v1beta1/Deployment
NAME                                         DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
nginx-ingress-nginx-ingress-default-backend  &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          1s
nginx-ingress-nginx-ingress-controller       &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          &lt;span class=&#34;nv&#34;&gt;1s&lt;/span&gt;

&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt; v1/ConfigMap
NAME                                    DATA  AGE
nginx-ingress-nginx-ingress-controller  &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;     &lt;span class=&#34;nv&#34;&gt;1s&lt;/span&gt;

&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt; v1/ServiceAccount
NAME                         SECRETS  AGE
nginx-ingress-nginx-ingress  &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;        &lt;span class=&#34;nv&#34;&gt;1s&lt;/span&gt;

&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt; rbac.authorization.k8s.io/v1beta1/ClusterRole
NAME                         KIND
nginx-ingress-nginx-ingress  ClusterRole.v1beta1.rbac.authorization.k8s.io

&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt; rbac.authorization.k8s.io/v1beta1/ClusterRoleBinding
nginx-ingress-nginx-ingress  ClusterRoleBinding.v1beta1.rbac.authorization.k8s.io


NOTES:
The nginx-ingress controller has been installed.
Get the application URL by running these commands:
  &lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;HTTP_NODE_PORT&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl --namespace default get services -o &lt;span class=&#34;nv&#34;&gt;jsonpath&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{.spec.ports[0].nodePort}&amp;#34;&lt;/span&gt; nginx-ingress-nginx-ingress-controller&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
  &lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;HTTPS_NODE_PORT&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl --namespace default get services -o &lt;span class=&#34;nv&#34;&gt;jsonpath&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{.spec.ports[1].nodePort}&amp;#34;&lt;/span&gt; nginx-ingress-nginx-ingress-controller&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
  &lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;NODE_IP&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl --namespace default get nodes -o &lt;span class=&#34;nv&#34;&gt;jsonpath&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{.items[0].status.addresses[1].address}&amp;#34;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;

  &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;Visit http://&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$NODE_IP&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$HTTP_NODE_PORT&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; to access your application via HTTP.&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
  &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;Visit https://&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$NODE_IP&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$HTTPS_NODE_PORT&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; to access your application via HTTPS.&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;

An example Ingress that makes use of the controller:

  apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    annotations:
      kubernetes.io/ingress.class: nginx
    name: example
    namespace: foo
  spec:
    rules:
      - host: www.example.com
        http:
          paths:
            - backend:
                serviceName: exampleService
                servicePort: &lt;span class=&#34;m&#34;&gt;80&lt;/span&gt;
              path: /
    &lt;span class=&#34;c1&#34;&gt;# This section is only required if TLS is to be enabled for the Ingress&lt;/span&gt;
    tls:
        - hosts:
            - www.example.com
          secretName: example-tls

If TLS is enabled &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; the Ingress, a Secret containing the certificate and key must also be provided:

  apiVersion: v1
  kind: Secret
  metadata:
    name: example-tls
    namespace: foo
  data:
    tls.crt: &amp;lt;base64 encoded cert&amp;gt;
    tls.key: &amp;lt;base64 encoded key&amp;gt;
  type: kubernetes.io/tls
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;访问Nginx&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;首先获取Nginx的地址，从我们使用helm安装nginx-ingress命令的输出中那个可以看到提示，根据提示执行可以看到nginx的http和https地址：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;  &lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;HTTP_NODE_PORT&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl --namespace default get services -o &lt;span class=&#34;nv&#34;&gt;jsonpath&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{.spec.ports[0].nodePort}&amp;#34;&lt;/span&gt; nginx-ingress-nginx-ingress-controller&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
  &lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;HTTPS_NODE_PORT&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl --namespace default get services -o &lt;span class=&#34;nv&#34;&gt;jsonpath&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{.spec.ports[1].nodePort}&amp;#34;&lt;/span&gt; nginx-ingress-nginx-ingress-controller&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
  &lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;NODE_IP&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl --namespace default get nodes -o &lt;span class=&#34;nv&#34;&gt;jsonpath&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{.items[0].status.addresses[1].address}&amp;#34;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;

  &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;Visit http://&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$NODE_IP&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$HTTP_NODE_PORT&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; to access your application via HTTP.&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
  &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;Visit https://&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$NODE_IP&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$HTTPS_NODE_PORT&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; to access your application via HTTPS.&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
  Visit http://172.20.0.113:30484 to access your application via HTTP.
  Visit https://172.20.0.113:31053 to access your application via HTTPS.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;http地址：http://172.20.0.113:30484&lt;/li&gt;
&lt;li&gt;https地址：https://172.20.0.113:31053&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们分别在http和https地址上测试一下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/healthz&lt;/code&gt;返回200&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/&lt;/code&gt;返回404错误&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl -v http://172.20.0.113:30484/healthz
&lt;span class=&#34;c1&#34;&gt;# 返回200&lt;/span&gt;
curl -v http://172.20.0.113:30484/
&lt;span class=&#34;c1&#34;&gt;# 返回404&lt;/span&gt;
curl -v --insecure http://172.20.0.113:30484/healthz
&lt;span class=&#34;c1&#34;&gt;# 返回200&lt;/span&gt;
curl -v --insecure http://172.20.0.113:30484/
&lt;span class=&#34;c1&#34;&gt;# 返回404&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;删除nginx-ingress&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm delete --purge nginx-ingress
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用&lt;code&gt;--purge&lt;/code&gt;参数可以彻底删除release不留下记录，否则下一次部署的时候不能使用重名的release。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34;&gt;Ingress-nginx github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/charts/tree/master/stable/nginx-ingress&#34;&gt;Nginx chart configuration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/practice/helm.html&#34;&gt;使用Helm管理kubernetes应用&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>docker用户过渡到kubectl命令行指南</title>
      <link>https://jimmysong.io/blog/docker-cli-to-kubectl/</link>
      <pubDate>Sat, 16 Sep 2017 20:54:06 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/docker-cli-to-kubectl/</guid>
      <description>
        
        
        &lt;p&gt;对于没有使用过 kubernetes 的 docker 用户，如何快速掌握 kubectl 命令？kubectl 跟 docker 命令之间有什么区别和联系？&lt;/p&gt;
&lt;p&gt;在本文中，我们将向 docker-cli 用户介绍 Kubernetes 命令行如何与 api 进行交互。该命令行工具——kubectl，被设计成 docker-cli 用户所熟悉的样子，但是它们之间又存在一些必要的差异。该文档将向您展示每个 docker 子命令和 kubectl 与其等效的命令。&lt;/p&gt;
&lt;p&gt;在使用 kubernetes 集群的时候，docker 命令通常情况是不需要用到的，只有在调试程序或者容器的时候用到，我们基本上使用 kubectl 命令即可，所以在操作 kubernetes 的时候我们抛弃原先使用 docker 时的一些观念。&lt;/p&gt;
&lt;h4 id=&#34;docker-run&#34;&gt;docker run&lt;/h4&gt;
&lt;p&gt;如何运行一个 nginx Deployment 并将其暴露出来？ 查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl&#34;&gt;kubectl run&lt;/a&gt; 。&lt;/p&gt;
&lt;p&gt;使用 docker 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ docker run -d --restart&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;always -e &lt;span class=&#34;nv&#34;&gt;DOMAIN&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;cluster --name nginx-app -p 80:80 nginx
a9ec34d9878748d2f33dc20cb25c714ff21da8d40558b45bfaec9955859075d0
$ docker ps
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS                         NAMES
a9ec34d98787        nginx               &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;nginx -g &amp;#39;daemon of   2 seconds ago       Up 2 seconds        0.0.0.0:80-&amp;gt;80/tcp, 443/tcp   nginx-app 
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 kubectl 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# start the pod running nginx&lt;/span&gt;
$ kubectl run --image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;nginx nginx-app --port&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;80&lt;/span&gt; --env&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;DOMAIN=cluster&amp;#34;&lt;/span&gt;
deployment &lt;span class=&#34;s2&#34;&gt;&amp;#34;nginx-app&amp;#34;&lt;/span&gt; created
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在大于等于 1.2 版本 Kubernetes 集群中，使用&lt;code&gt;kubectl run&lt;/code&gt; 命令将创建一个名为 &amp;ldquo;nginx-app&amp;rdquo; 的 Deployment。如果您运行的是老版本，将会创建一个 replication controller。 如果您想沿用旧的行为，使用 &lt;code&gt;--generation=run/v1&lt;/code&gt; 参数，这样就会创建 replication controller。查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl/&#34;&gt;&lt;code&gt;kubectl run&lt;/code&gt;&lt;/a&gt; 获取更多详细信息。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# expose a port through with a service&lt;/span&gt;
$ kubectl expose deployment nginx-app --port&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;80&lt;/span&gt; --name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;nginx-http
service &lt;span class=&#34;s2&#34;&gt;&amp;#34;nginx-http&amp;#34;&lt;/span&gt; exposed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在 kubectl 命令中，我们创建了一个 &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment&#34;&gt;Deployment&lt;/a&gt;，这将保证有 N 个运行 nginx 的 pod（N 代表 spec 中声明的 replica 数，默认为 1）。我们还创建了一个 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/services&#34;&gt;service&lt;/a&gt;，使用 selector 匹配具有相应的 selector 的 Deployment。查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/quick-start&#34;&gt;快速开始&lt;/a&gt; 获取更多信息。&lt;/p&gt;
&lt;p&gt;默认情况下镜像会在后台运行，与&lt;code&gt;docker run -d ...&lt;/code&gt; 类似，如果您想在前台运行，使用：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl run &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;-i&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;--tty&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; --attach &amp;lt;name&amp;gt; --image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;lt;image&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;与 &lt;code&gt;docker run ...&lt;/code&gt; 不同的是，如果指定了 &lt;code&gt;--attach&lt;/code&gt; ，我们将连接到 &lt;code&gt;stdin&lt;/code&gt;，&lt;code&gt;stdout&lt;/code&gt; 和 &lt;code&gt;stderr&lt;/code&gt;，而不能控制具体连接到哪个输出流（&lt;code&gt;docker -a ...&lt;/code&gt;）。&lt;/p&gt;
&lt;p&gt;因为我们使用 Deployment 启动了容器，如果您终止了连接到的进程（例如 &lt;code&gt;ctrl-c&lt;/code&gt;），容器将会重启，这跟 &lt;code&gt;docker run -it&lt;/code&gt;不同。 如果想销毁该 Deployment（和它的 pod），您需要运行 &lt;code&gt;kubeclt delete deployment &amp;lt;name&amp;gt;&lt;/code&gt;。&lt;/p&gt;
&lt;h4 id=&#34;docker-ps&#34;&gt;docker ps&lt;/h4&gt;
&lt;p&gt;如何列出哪些正在运行？查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl&#34;&gt;kubectl get&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;使用 docker 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ docker ps
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS                         NAMES
a9ec34d98787        nginx               &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;nginx -g &amp;#39;daemon of   About an hour ago   Up About an hour    0.0.0.0:80-&amp;gt;80/tcp, 443/tcp   nginx-app
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 kubectl 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl get po
NAME              READY     STATUS    RESTARTS   AGE
nginx-app-5jyvm   1/1       Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          1h
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;docker-attach&#34;&gt;docker attach&lt;/h4&gt;
&lt;p&gt;如何连接到已经运行在容器中的进程？查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl&#34;&gt;kubectl attach&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;使用 docker 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ docker ps
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS                         NAMES
a9ec34d98787        nginx               &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;nginx -g &amp;#39;daemon of   8 minutes ago       Up 8 minutes        0.0.0.0:80-&amp;gt;80/tcp, 443/tcp   nginx-app
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&lt;/span&gt;$&lt;span class=&#34;s2&#34;&gt; docker attach a9ec34d98787
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;...
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 kubectl 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl get pods
NAME              READY     STATUS    RESTARTS   AGE
nginx-app-5jyvm   1/1       Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          10m
$ kubectl attach -it nginx-app-5jyvm
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;docker-exec&#34;&gt;docker exec&lt;/h4&gt;
&lt;p&gt;如何在容器中执行命令？查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl/&#34;&gt;kubectl exec&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;使用 docker 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ docker ps
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS                         NAMES
a9ec34d98787        nginx               &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;nginx -g &amp;#39;daemon of   8 minutes ago       Up 8 minutes        0.0.0.0:80-&amp;gt;80/tcp, 443/tcp   nginx-app
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&lt;/span&gt;$&lt;span class=&#34;s2&#34;&gt; docker exec a9ec34d98787 cat /etc/hostname
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;a9ec34d98787
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 kubectl 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl get po
NAME              READY     STATUS    RESTARTS   AGE
nginx-app-5jyvm   1/1       Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          10m
$ kubectl &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; nginx-app-5jyvm -- cat /etc/hostname
nginx-app-5jyvm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;执行交互式命令怎么办？&lt;/p&gt;
&lt;p&gt;使用 docker 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ docker &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; -ti a9ec34d98787 /bin/sh
&lt;span class=&#34;c1&#34;&gt;# exit&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 kubectl 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; -ti nginx-app-5jyvm -- /bin/sh      
&lt;span class=&#34;c1&#34;&gt;# exit&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;更多信息请查看 &lt;a href=&#34;https://kubernetes.io/docs/tasks/kubectl/get-shell-running-container&#34;&gt;获取运行中容器的 Shell 环境&lt;/a&gt;。&lt;/p&gt;
&lt;h4 id=&#34;docker-logs&#34;&gt;docker logs&lt;/h4&gt;
&lt;p&gt;如何查看运行中进程的 stdout/stderr？查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl/&#34;&gt;kubectl logs&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;使用 docker 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ docker logs -f a9e
192.168.9.1 - - &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;14/Jul/2015:01:04:02 +0000&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;GET / HTTP/1.1&amp;#34;&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;200&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;612&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;curl/7.35.0&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt;
192.168.9.1 - - &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;14/Jul/2015:01:04:03 +0000&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;GET / HTTP/1.1&amp;#34;&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;200&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;612&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;curl/7.35.0&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 kubectl 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl logs -f nginx-app-zibvs
10.240.63.110 - - &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;14/Jul/2015:01:09:01 +0000&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;GET / HTTP/1.1&amp;#34;&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;200&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;612&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;curl/7.26.0&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt;
10.240.63.110 - - &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;14/Jul/2015:01:09:02 +0000&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;GET / HTTP/1.1&amp;#34;&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;200&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;612&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;curl/7.26.0&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;现在是时候提一下 pod 和容器之间的细微差别了；默认情况下如果 pod 中的进程退出 pod 也不会终止，相反它将会重启该进程。这类似于 docker run 时的 &lt;code&gt;--restart=always&lt;/code&gt; 选项， 这是主要差别。在 docker 中，进程的每个调用的输出都是被连接起来的，但是对于 kubernetes，每个调用都是分开的。要查看以前在 kubernetes 中执行的输出，请执行以下操作：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl logs --previous nginx-app-zibvs
10.240.63.110 - - &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;14/Jul/2015:01:09:01 +0000&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;GET / HTTP/1.1&amp;#34;&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;200&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;612&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;curl/7.26.0&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt;
10.240.63.110 - - &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;14/Jul/2015:01:09:02 +0000&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;GET / HTTP/1.1&amp;#34;&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;200&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;612&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;curl/7.26.0&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;查看 &lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/logging&#34;&gt;记录和监控集群活动&lt;/a&gt; 获取更多信息。&lt;/p&gt;
&lt;h4 id=&#34;docker-stop-和-docker-rm&#34;&gt;docker stop 和 docker rm&lt;/h4&gt;
&lt;p&gt;如何停止和删除运行中的进程？查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl/&#34;&gt;kubectl delete&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;使用 docker 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ docker ps
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS                         NAMES
a9ec34d98787        nginx               &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;nginx -g &amp;#39;daemon of   22 hours ago        Up 22 hours         0.0.0.0:80-&amp;gt;80/tcp, 443/tcp   nginx-app
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&lt;/span&gt;$&lt;span class=&#34;s2&#34;&gt; docker stop a9ec34d98787
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;a9ec34d98787
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&lt;/span&gt;$&lt;span class=&#34;s2&#34;&gt; docker rm a9ec34d98787
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;a9ec34d98787
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 kubectl 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl get deployment nginx-app
NAME        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-app   &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;            &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           2m
$ kubectl get po -l &lt;span class=&#34;nv&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;nginx-app
NAME                         READY     STATUS    RESTARTS   AGE
nginx-app-2883164633-aklf7   1/1       Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          2m
$ kubectl delete deployment nginx-app
deployment &lt;span class=&#34;s2&#34;&gt;&amp;#34;nginx-app&amp;#34;&lt;/span&gt; deleted
$ kubectl get po -l &lt;span class=&#34;nv&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;nginx-app
&lt;span class=&#34;c1&#34;&gt;# Return nothing&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;请注意，我们不直接删除 pod。使用 kubectl 命令，我们要删除拥有该 pod 的 Deployment。如果我们直接删除pod，Deployment 将会重新创建该 pod。&lt;/p&gt;
&lt;h4 id=&#34;docker-login&#34;&gt;docker login&lt;/h4&gt;
&lt;p&gt;在 kubectl 中没有对 &lt;code&gt;docker login&lt;/code&gt; 的直接模拟。如果您有兴趣在私有镜像仓库中使用 Kubernetes，请参阅 &lt;a href=&#34;https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry&#34;&gt;使用私有镜像仓库&lt;/a&gt;。&lt;/p&gt;
&lt;h4 id=&#34;docker-version&#34;&gt;docker version&lt;/h4&gt;
&lt;p&gt;如何查看客户端和服务端的版本？查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl/&#34;&gt;kubectl version&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;使用 docker 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ docker version
Client version: 1.7.0
Client API version: 1.19
Go version &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;client&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;: go1.4.2
Git commit &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;client&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;: 0baf609
OS/Arch &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;client&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;: linux/amd64
Server version: 1.7.0
Server API version: 1.19
Go version &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;server&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;: go1.4.2
Git commit &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;server&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;: 0baf609
OS/Arch &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;server&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;: linux/amd64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 kubectl 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl version
Client Version: version.Info&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;Major:&lt;span class=&#34;s2&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt;, Minor:&lt;span class=&#34;s2&#34;&gt;&amp;#34;6&amp;#34;&lt;/span&gt;, GitVersion:&lt;span class=&#34;s2&#34;&gt;&amp;#34;v1.6.9+a3d1dfa6f4335&amp;#34;&lt;/span&gt;, GitCommit:&lt;span class=&#34;s2&#34;&gt;&amp;#34;9b77fed11a9843ce3780f70dd251e92901c43072&amp;#34;&lt;/span&gt;, GitTreeState:&lt;span class=&#34;s2&#34;&gt;&amp;#34;dirty&amp;#34;&lt;/span&gt;, BuildDate:&lt;span class=&#34;s2&#34;&gt;&amp;#34;2017-08-29T20:32:58Z&amp;#34;&lt;/span&gt;, OpenPaasKubernetesVersion:&lt;span class=&#34;s2&#34;&gt;&amp;#34;v1.03.02&amp;#34;&lt;/span&gt;, GoVersion:&lt;span class=&#34;s2&#34;&gt;&amp;#34;go1.7.5&amp;#34;&lt;/span&gt;, Compiler:&lt;span class=&#34;s2&#34;&gt;&amp;#34;gc&amp;#34;&lt;/span&gt;, Platform:&lt;span class=&#34;s2&#34;&gt;&amp;#34;linux/amd64&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
Server Version: version.Info&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;Major:&lt;span class=&#34;s2&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt;, Minor:&lt;span class=&#34;s2&#34;&gt;&amp;#34;6&amp;#34;&lt;/span&gt;, GitVersion:&lt;span class=&#34;s2&#34;&gt;&amp;#34;v1.6.9+a3d1dfa6f4335&amp;#34;&lt;/span&gt;, GitCommit:&lt;span class=&#34;s2&#34;&gt;&amp;#34;9b77fed11a9843ce3780f70dd251e92901c43072&amp;#34;&lt;/span&gt;, GitTreeState:&lt;span class=&#34;s2&#34;&gt;&amp;#34;dirty&amp;#34;&lt;/span&gt;, BuildDate:&lt;span class=&#34;s2&#34;&gt;&amp;#34;2017-08-29T20:32:58Z&amp;#34;&lt;/span&gt;, OpenPaasKubernetesVersion:&lt;span class=&#34;s2&#34;&gt;&amp;#34;v1.03.02&amp;#34;&lt;/span&gt;, GoVersion:&lt;span class=&#34;s2&#34;&gt;&amp;#34;go1.7.5&amp;#34;&lt;/span&gt;, Compiler:&lt;span class=&#34;s2&#34;&gt;&amp;#34;gc&amp;#34;&lt;/span&gt;, Platform:&lt;span class=&#34;s2&#34;&gt;&amp;#34;linux/amd64&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;docker-info&#34;&gt;docker info&lt;/h4&gt;
&lt;p&gt;如何获取有关环境和配置的各种信息？查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl/&#34;&gt;kubectl cluster-info&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;使用 docker 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ docker info
Containers: &lt;span class=&#34;m&#34;&gt;40&lt;/span&gt;
Images: &lt;span class=&#34;m&#34;&gt;168&lt;/span&gt;
Storage Driver: aufs
 Root Dir: /usr/local/google/docker/aufs
 Backing Filesystem: extfs
 Dirs: &lt;span class=&#34;m&#34;&gt;248&lt;/span&gt;
 Dirperm1 Supported: &lt;span class=&#34;nb&#34;&gt;false&lt;/span&gt;
Execution Driver: native-0.2
Logging Driver: json-file
Kernel Version: 3.13.0-53-generic
Operating System: Ubuntu 14.04.2 LTS
CPUs: &lt;span class=&#34;m&#34;&gt;12&lt;/span&gt;
Total Memory: 31.32 GiB
Name: k8s-is-fun.mtv.corp.google.com
ID: ADUV:GCYR:B3VJ:HMPO:LNPQ:KD5S:YKFQ:76VN:IANZ:7TFV:ZBF4:BYJO
WARNING: No swap limit support
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 kubectl 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl cluster-info
Kubernetes master is running at https://108.59.85.141
KubeDNS is running at https://108.59.85.141/api/v1/namespaces/kube-system/services/kube-dns/proxy
KubeUI is running at https://108.59.85.141/api/v1/namespaces/kube-system/services/kube-ui/proxy
Grafana is running at https://108.59.85.141/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy
Heapster is running at https://108.59.85.141/api/v1/namespaces/kube-system/services/monitoring-heapster/proxy
InfluxDB is running at https://108.59.85.141/api/v1/namespaces/kube-system/services/monitoring-influxdb/proxy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;本文同时归档到 &lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34;&gt;kubernetes-handbook&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rootsongjc/kubernetes.github.io/blob/master/docs/user-guide/docker-cli-to-kubectl.md&#34;&gt;阅读原文&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>记一本关于kubernetes management design patterns的书</title>
      <link>https://jimmysong.io/blog/book-kubernetes-management-design-patterns/</link>
      <pubDate>Thu, 20 Jul 2017 18:21:18 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/book-kubernetes-management-design-patterns/</guid>
      <description>
        
        
        &lt;p&gt;下面是这本书的基本信息。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;书名： Kubernetes Management Design Patterns: With Docker, CoreOS Linux, and Other Platforms&lt;/li&gt;
&lt;li&gt;Amazon购买链接：&lt;a href=&#34;https://www.amazon.com/Kubernetes-Management-Design-Patterns-Platforms-ebook/dp/B01MZDO0BD/ref=pd_sbs_351_4?_encoding=UTF8&amp;amp;psc=1&amp;amp;refRID=79F47CR67EEESD35S2VF&#34;&gt;链接&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;作者：Deepak Vohra&lt;/li&gt;
&lt;li&gt;发行日期：2017年1月20日&lt;/li&gt;
&lt;li&gt;出版社：Apress&lt;/li&gt;
&lt;li&gt;页数：399&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;简介&#34;&gt;简介&lt;/h3&gt;
&lt;p&gt;Kubernetes引领容器集群管理进入一个全新的阶段；学习如何在CoreOS上配置和管理kubernetes集群；使用适当的管理模式，如ConfigMaps、Autoscaling、弹性资源使用和高可用性配置。讨论了kubernetes的一些其他特性，如日志、调度、滚动升级、volume、服务类型和跨多个云供应商zone。&lt;/p&gt;
&lt;p&gt;Kubernetes中的最小模块化单位是Pod，它是拥有共同的文件系统和网络的系列容器的集合。Pod的抽象层可以对容器使用设计模式，就像面向对象设计模式一样。容器能够提供与软件对象（如模块化或包装，抽象和重用）相同的优势。&lt;/p&gt;
&lt;p&gt;在大多数章节中使用的都是CoreOS Linux，其他讨论的平台有CentOS，OpenShift，Debian 8（jessie），AWS和Debian 7 for Google Container Engine。&lt;/p&gt;
&lt;p&gt;使用CoreOS主要是因为Docker已经在CoreOS上开箱即用。CoreOS：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持大多数云提供商（包括Amazon AWS EC2和Google Cloud Platform）和虚拟化平台（如VMWare和VirtualBox）&lt;/li&gt;
&lt;li&gt;提供Cloud-Config，用于声明式配置OS，如网络配置（flannel），存储（etcd）和用户帐户&lt;/li&gt;
&lt;li&gt;为容器化应用提供生产级基础架构，包括自动化，安全性和可扩展性&lt;/li&gt;
&lt;li&gt;引领容器行业标准，并建立了应用程序标准&lt;/li&gt;
&lt;li&gt;提供最先进的容器仓库，Quay&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Docker于2013年3月开源，现已称为最流行的容器平台。kubernetes于2014年6月开源，现在已经成为最流行的容器集群管理平台。第一个稳定版CoreOS Linux于2014年7月发行，现已成为最流行的容器操作系统之一。&lt;/p&gt;
&lt;h3 id=&#34;你将学到什么&#34;&gt;你将学到什么&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;使用docker和kubernetes&lt;/li&gt;
&lt;li&gt;在AWS和CoreOS上创建kubernetes集群&lt;/li&gt;
&lt;li&gt;应用集群管理设计模式&lt;/li&gt;
&lt;li&gt;使用多个云供应商zone&lt;/li&gt;
&lt;li&gt;使用Ansible管理kubernetes&lt;/li&gt;
&lt;li&gt;基于kubernetes的PAAS平台OpenShift&lt;/li&gt;
&lt;li&gt;创建高可用网站&lt;/li&gt;
&lt;li&gt;构建高可用用的kubernetes master集群&lt;/li&gt;
&lt;li&gt;使用volume、configmap、serivce、autoscaling和rolling update&lt;/li&gt;
&lt;li&gt;管理计算资源&lt;/li&gt;
&lt;li&gt;配置日志和调度&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;谁适合读这本书&#34;&gt;谁适合读这本书&lt;/h3&gt;
&lt;p&gt;Linux管理员、CoreOS管理员、应用程序开发者、容器即服务（CAAS）开发者。阅读这本书需要Linux和Docker的前置知识。介绍Kubernetes的知识，例如创建集群，创建Pod，创建service以及创建和缩放replication controller。还需要一些关于使用Amazon Web Services（AWS）EC2，CloudFormation和VPC的必备知识。&lt;/p&gt;
&lt;h3 id=&#34;关于作者&#34;&gt;关于作者&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Deepak Vohra&lt;/strong&gt; is an Oracle Certified Associate and a Sun Certified Java Programmer. Deepak has published in Oracle Magazine, OTN, IBM developerWorks, ONJava, DevSource,  WebLogic Developer’s Journal, XML Journal, Java Developer’s Journal, FTPOnline, and devx.&lt;/p&gt;
&lt;h3 id=&#34;目录&#34;&gt;目录&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;第一部分：平台
&lt;ul&gt;
&lt;li&gt;第1章：Kuberentes on AWS&lt;/li&gt;
&lt;li&gt;第2章：kubernetes on CoreOS on AWS&lt;/li&gt;
&lt;li&gt;第3章：kubernetes on Google Cloud Platform&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;第二部分：管理和配置
&lt;ul&gt;
&lt;li&gt;第4章：使用多个可用区&lt;/li&gt;
&lt;li&gt;第5章：使用Tectonic Console&lt;/li&gt;
&lt;li&gt;第6章：使用volume&lt;/li&gt;
&lt;li&gt;第7章：使用service&lt;/li&gt;
&lt;li&gt;第8章：使用Rolling updte&lt;/li&gt;
&lt;li&gt;第9章：在node上调度pod&lt;/li&gt;
&lt;li&gt;第10章：配置计算资源&lt;/li&gt;
&lt;li&gt;第11章：使用ConfigMap&lt;/li&gt;
&lt;li&gt;第12章：使用资源配额&lt;/li&gt;
&lt;li&gt;第13章：使用Autoscaling&lt;/li&gt;
&lt;li&gt;第14章：配置logging&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;第三部分：高可用
&lt;ul&gt;
&lt;li&gt;第15章：在OpenShift中使用HA master&lt;/li&gt;
&lt;li&gt;第16章：开发高可用网站&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;个人评价&#34;&gt;个人评价&lt;/h3&gt;
&lt;p&gt;本书更像是一本参考手册，对于想在公有云中（如AWS、Google Cloud Platform）中尝试Kubernetes的人会有所帮助，而对于想使用kubernetes进行自己的私有云建设，或想了解kubernetes的实现原理和技术细节的人来说，就不适合了。对我来说，本书中有个别几个章节可以参考，如高可用，但还是使用OpenShift实现的。总之，如果你使用AWS这样的公有云，对操作系统没有特别要求，可以接受CoreOS的话，那么可以看看这本书。本来本书会对kubernetes中的各种应用模式能够有个详解，但是从书中我并没有找到。&lt;/p&gt;
&lt;p&gt;本书有两个优点，一个是每个章节都给出了问题的起因和kubernetes的解决方案，二是几乎所有的命令和操作都附有截图，说明很详细。&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
