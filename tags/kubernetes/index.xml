<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jimmy Song - 专注于探索后 Kubernetes 时代的云原生新范式 – Kubernetes</title>
    <link>https://jimmysong.io/tags/kubernetes/</link>
    <description>Recent content in Kubernetes on Jimmy Song - 专注于探索后 Kubernetes 时代的云原生新范式</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <copyright>&amp;copy; 2017-2022 Jimmy Song 保留所有权利</copyright>
    <lastBuildDate>Fri, 06 Aug 2021 10:22:00 +0800</lastBuildDate>
    
	  <atom:link href="https://jimmysong.io/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>如何理解 Istio Ingress， 它与 API Gateway 有什么区别？</title>
      <link>https://jimmysong.io/blog/istio-servicemesh-api-gateway/</link>
      <pubDate>Fri, 06 Aug 2021 10:22:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/istio-servicemesh-api-gateway/</guid>
      <description>
        
        
        &lt;p&gt;API 网关作为客户端访问后端的入口，已经存在很长时间了，它主要是用来管理”南北向“的流量；近几年服务网格开始流行，它主要是管理系统内部，即“东西向”流量，而像 Istio 这样的服务网格还内置了网关，从而将系统内外部的流量纳入了统一管控。这经常给初次接触 Istio 的人带来困惑——服务网格与 API 网关之间是什么关系？是不是使用了 Istio 就可以替代了 API 网关？Istio 的 API 网关是如何运作的？有哪些方式暴露 Istio mesh 中的服务？这篇文章给为你解答。&lt;/p&gt;
&lt;h2 id=&#34;主要观点&#34;&gt;主要观点&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;服务网格诞生的初衷是为了解决分布式应用的内部流量的管理问题，而在此之前 API 网关已存在很久了。&lt;/li&gt;
&lt;li&gt;虽然 Istio 中内置了Gateway，但是你仍可以使用自定义的 Ingress Controller 来代理外部流量。&lt;/li&gt;
&lt;li&gt;API 网关和服务网格正朝着融合的方向发展。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;如何暴露-istio-mesh-中的服务&#34;&gt;如何暴露 Istio mesh 中的服务？&lt;/h2&gt;
&lt;p&gt;下图展示了使用 Istio Gateway、Kubernetes Ingress、API Gateway 及 NodePort/LB 暴露 Istio mesh 中服务的四种方式。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/istio-servicemesh-api-gateway/access-cluster.svg&#34; data-img=&#34;/blog/istio-servicemesh-api-gateway/access-cluster.svg&#34; alt=&#34;image&#34; data-caption=&#34;暴露 Kubernetes 中服务的几种方式&#34;&gt;
    
  
  &lt;figcaption&gt;暴露 Kubernetes 中服务的几种方式&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;其中阴影表示的是 Istio mesh，mesh 中的的流量属于集群内部（东西向）流量，而客户端访问 Kubernetes 集群内服务的流量属于外部（南北向）流量。不过因为 Ingress、Gateway 也是部署在 Kubernetes 集群内的，这些节点访问集群内其他服务的流量就难以归属了。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;方式&lt;/th&gt;
&lt;th&gt;控制器&lt;/th&gt;
&lt;th&gt;功能&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;NodePort/LoadBalancer&lt;/td&gt;
&lt;td&gt;Kubernetes&lt;/td&gt;
&lt;td&gt;负载均衡&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kubernetes Ingress&lt;/td&gt;
&lt;td&gt;Ingress Controller&lt;/td&gt;
&lt;td&gt;负载均衡、TLS、虚拟主机、流量路由&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Istio Gateway&lt;/td&gt;
&lt;td&gt;Istio&lt;/td&gt;
&lt;td&gt;负载均衡、TLS、虚拟主机、高级流量路由、其他 Istio 的高级功能&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;API 网关&lt;/td&gt;
&lt;td&gt;API Gateway&lt;/td&gt;
&lt;td&gt;负载均衡、TLS、虚拟主机、流量路由、API 生命周期管理、权限认证、数据聚合、账单和速率限制&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;由于 NodePort/LoadBalancer 是 Kubernetes 内置的基本的暴露服务的方式，本文就不讨论这种方式了。下文将对其他三种方式分别作出说明。&lt;/p&gt;
&lt;h2 id=&#34;使用-kubernetes-ingress-暴露服务&#34;&gt;使用 Kubernetes Ingress 暴露服务&lt;/h2&gt;
&lt;p&gt;我们都知道 Kubernetes 集群的客户端是无法直接访问 Pod 的 IP 地址的，因为 Pod 是处于 Kubernetes 内置的一个网络平面中。我们可以将 Kubernetes 内的服务使用 NodePort 或者 LoadBlancer 的方式暴露到集群以外。同时为了支持虚拟主机、隐藏和节省 IP 地址，可以使用 &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/&#34; title=&#34;Ingress&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ingress&lt;/a&gt;
 来暴露 Kubernetes 中的服务。Kubernetes Ingress 原理如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/istio-servicemesh-api-gateway/ingress.svg&#34; data-img=&#34;/blog/istio-servicemesh-api-gateway/ingress.svg&#34; alt=&#34;image&#34; data-caption=&#34;使用 Kubernetes Ingress 暴露服务&#34;&gt;
    
  
  &lt;figcaption&gt;使用 Kubernetes Ingress 暴露服务&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;简单的说，Ingress 就是从 Kubernetes 集群外访问集群的入口，将用户的 URL 请求转发到不同的服务上。Ingress 相当于 Nginx、Apache 等负载均衡方向代理服务器，其中还包括规则定义，即 URL 的路由信息，路由信息得的刷新由 &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-controllers&#34; title=&#34;Ingress controller&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ingress controller&lt;/a&gt;
来提供。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;networking.k8s.io/v1beta1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Ingress&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;annotations&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kubernetes.io/ingress.class&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;istio&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ingress&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;rules&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;host&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;httpbin.example.com&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;http&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;paths&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/status/*&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;backend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;serviceName&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;httpbin&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;servicePort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8000&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;上面的例子中的 &lt;code&gt;kubernetes.io/ingress.class: istio&lt;/code&gt; 注解表明该 Ingress 使用的 Istio Ingress Controller。&lt;/p&gt;
&lt;h2 id=&#34;使用-istio-gateway-暴露服务&#34;&gt;使用 Istio Gateway 暴露服务&lt;/h2&gt;
&lt;p&gt;我们都知道 Istio 是继承 Kubernetes 之后发展出来的一个流行的服务网格实现，它实现了 Kubernetes 没有的一些功能，请参考&lt;a href=&#34;https://jimmysong.io/blog/what-is-istio-and-why-does-kubernetes-need-it/&#34; title=&#34;什么是 Istio？为什么 Kubernetes 需要 Istio？&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;什么是 Istio？为什么 Kubernetes 需要 Istio？&lt;/a&gt;
简要来说，正是因为 Istio 补足了 Kubernetes 对于云原生应用的流量管理、可观察性和安全方面的短板，使得流量管理变得对应用程序透明，使这部分功能从应用程序中转移到了平台层，成为了云原生基础设施。&lt;/p&gt;
&lt;p&gt;Istio 0.8 以前版本中使用 Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/&#34; title=&#34;Ingress&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ingress&lt;/a&gt;
 来作为流量入口，其中使用 Envoy 作为 Ingress Controller。在 Istio 0.8 及以后的版本中，Istio 创建了 Gateway 对象。Gateway 和 VirtualService 用于表示 Istio Ingress 的配置模型，Istio Ingress 的缺省实现则采用了和 sidecar 相同的 Envoy 代理。通过该方式，Istio 控制面用一致的配置模型同时控制了入口网关和内部的 sidecar 代理。这些配置包括路由规则，策略检查、遥测收集以及其他服务管控功能。&lt;/p&gt;
&lt;p&gt;Istio Gateway 的功能与 Kubernetes Ingress 类似，它负责进出集群的南北流量。Istio Gateway 描述了一个负载均衡器，用于承载进出服务网格边缘的连接。该规范描述了一组开放端口和这些端口所使用的协议，以及用于负载均衡的 SNI 配置等。&lt;/p&gt;
&lt;p&gt;Istio Gateway 资源本身只能配置L4到L6的功能，例如暴露的端口、TLS 设置等；但 Gateway 可与 VirtualService 绑定，在VirtualService 中可以配置七层路由规则，例如按比例和版本的流量路由，故障注入，HTTP 重定向，HTTP 重写等所有Mesh内部支持的路由规则。&lt;/p&gt;
&lt;p&gt;下面是一个 Gateway 与 VirtualService 绑定的示例。拥有 &lt;code&gt;istio: ingressgateway&lt;/code&gt; 标签的 pod 将作为 Ingress Gateway 并路由对 &lt;code&gt;httpbin.example.com&lt;/code&gt; 虚拟主机的 80 端口的 HTTP 访问，这相当于给 Kubernetes 敞开了一个外部访问的入口。这与使用 Kubernetes Ingress 最大的区别就是，需要我们手动将VirtualService与Gateway 绑定，并指定 Gateway 所在的 pod。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;networking.istio.io/v1alpha3&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Gateway&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;httpbin-gateway&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;selector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;istio&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ingressgateway&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;servers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;number&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;80&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;http&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;protocol&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;HTTP&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;hosts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;s2&#34;&gt;&amp;#34;httpbin.example.com&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;下面这个 VirtualService 通过 &lt;code&gt;gateways&lt;/code&gt; 与上面的网关绑定在了一起，以接受来自该网关的流量。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;networking.istio.io/v1alpha3&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;VirtualService&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;httpbin&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;hosts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;s2&#34;&gt;&amp;#34;httpbin.example.com&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;gateways&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;httpbin-gateway&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;http&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;match&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;uri&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;prefix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/status&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;route&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;destination&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;number&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8000&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;host&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;httpbin&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;使用-api-网关暴露服务&#34;&gt;使用 API 网关暴露服务&lt;/h2&gt;
&lt;p&gt;API 网关是位于客户端和后端服务之间的 API 管理工具，一种将客户端接口与后端实现分离的方式，在微服务中得到了广泛的应用。当客户端发出请求时，API 网关会将其分解为多个请求，然后将它们路由到正确的位置，生成响应，并跟踪所有内容。&lt;/p&gt;
&lt;p&gt;API Gateway 是微服务架构体系中的一类型特殊服务，它是所有微服务的入口，它的职责是执行路由请求、协议转换、聚合数据、认证、限流、熔断等。大多数企业 API 都是通过 API 网关部署的。API 网关通常会处理跨 API 服务系统的常见任务，例如用户身份验证、速率限制和统计信息。&lt;/p&gt;
&lt;p&gt;在网格中可以有一个或多个 API Gateway。API 网关的职责有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;请求路由和版本控制&lt;/li&gt;
&lt;li&gt;方便单体应用到微服务的过渡&lt;/li&gt;
&lt;li&gt;权限认证&lt;/li&gt;
&lt;li&gt;数据聚合：监控和计费&lt;/li&gt;
&lt;li&gt;协议转换&lt;/li&gt;
&lt;li&gt;消息和缓存&lt;/li&gt;
&lt;li&gt;安全和报警&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上很多基本功能比如路由和权限认证通过 Istio Gateway 也可以实现，只是在功能的丰富度和扩展性方面有些成熟的 API Gateway 可能更占优势，不过在 Istio mesh 中再引入 API Gateway 也可能带来一些弊端。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;引入了 API Gateway，需要考虑 API Gateway 本身的部署、运维、负载均衡等场景，增加了后端服务的复杂度&lt;/li&gt;
&lt;li&gt;API Gateway 中承载了大量的接口适配，导致难以维护&lt;/li&gt;
&lt;li&gt;对于部分场景，增加了一跳可能导致性能的降低&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;在 Istio mesh 中你可以使用多种 Kubernetes Ingress Controller 来充当入口网关，当然你还可以直接使用 Istio 内置的 Istio 网关，对于策略控制、流量管理和用量监控可以直接通过 Istio 网关来完成，这样做的好处是通过 Istio 的控制平面来直接管理网关，而不需要再借助其他工具。但是对于 API 生命周期管理、复杂的计费、协议转换和认证等功能，传统的 API 网关可能更适合你。所以，你可以根据自己的需求来选择，也可以组合使用。&lt;/p&gt;
&lt;p&gt;目前有些传统的反向代理也在向 Service Mesh 方向发展，如 Nginx 构建了 &lt;a href=&#34;https://www.nginx.com/products/nginx-service-mesh/&#34; title=&#34;Nginx Service Mesh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nginx Service Mesh&lt;/a&gt;
，Traefik 构建了 &lt;a href=&#34;https://traefik.io/traefik-mesh/&#34; title=&#34;Traefik Mesh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Traefik Mesh&lt;/a&gt;
。还有的 API 网关产品也向 Service Mesh 方向挺进，比如 Kong 发展出了 &lt;a href=&#34;https://kuma.io&#34; title=&#34;Kuma&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kuma&lt;/a&gt;
。在未来，我们会看到更多 API 网关、反向代理和服务网格的融合产品出现。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/evolving-kubernetes-networking-with-the-gateway-api/&#34; title=&#34;利用 Gateway API 发展 Kubernetes 网络&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;利用 Gateway API 发展 Kubernetes 网络&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/how-to-pick-gateway-for-service-mesh/&#34; title=&#34;如何为服务网格选择入口网关？&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;如何为服务网格选择入口网关？&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/service-mesh-and-api-gateway/&#34; title=&#34;Service Mesh 和 API Gateway 关系深度探讨&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Service Mesh 和 API Gateway 关系深度探讨&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/using-traefik-ingress-controller-with-istio-service-mesh/&#34; title=&#34;在 Istio 服务网格中使用 Traefik Ingress Controller&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;在 Istio 服务网格中使用 Traefik Ingress Controller&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>服务网格之旅——使用 Kubernetes 和 Istio Service Mesh 构建混合云</title>
      <link>https://jimmysong.io/blog/multicluster-management-with-kubernetes-and-istio/</link>
      <pubDate>Mon, 12 Jul 2021 22:22:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/multicluster-management-with-kubernetes-and-istio/</guid>
      <description>
        
        
        &lt;p&gt;这篇文章将带你了解使用 Kubernetes 和 Istio Service Mesh 构建多集群及混合云的过程和需要考虑的问题。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes&#34;&gt;Kubernetes&lt;/h2&gt;
&lt;p&gt;使用 Kubernetes 可以快速部署一个分布式环境，实现了云的互操作性，统一了云上的控制平面。并提供了 Service、Ingress 和 &lt;a href=&#34;https://kubernetes.io/blog/2021/04/22/evolving-kubernetes-networking-with-the-gateway-api/&#34; title=&#34;Gateway&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gateway&lt;/a&gt;
 等资源对象来处理应用程序的流量。如下图所示，Kubernetes 中默认使用 Service 做服务注册和发现，服务之间可以使用服务名称来访问。Kubernetes API Server 与集群内的每个节点上的 &lt;code&gt;kube-proxy&lt;/code&gt; 组件通信，为节点创建 iptables 规则，并将请求转发到其他 pod 上。&lt;/p&gt;
&lt;p&gt;假定现在客户端要访问 Kubernetes 中的服务，首先请求会发送到 Ingress/Gateway 上，然后根据 Ingress/Gateway 里的路由配置转发到后端服务上（图中是服务 A），接着服务 A 对服务 B 请求的流量转发轮询到服务 B 的实例上。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/multicluster-management-with-kubernetes-and-istio/008i3skNly1gsgg6a11l1j31lu0u042s.jpg&#34; data-img=&#34;/blog/multicluster-management-with-kubernetes-and-istio/008i3skNly1gsgg6a11l1j31lu0u042s.jpg&#34; data-width=&#34;2082&#34; data-height=&#34;1080&#34; alt=&#34;image&#34; data-caption=&#34;Kubernetes&#34;&gt;
    
  
  &lt;figcaption&gt;Kubernetes&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-多集群管理&#34;&gt;Kubernetes 多集群管理&lt;/h2&gt;
&lt;p&gt;多集群管理最常见的使用场景包括服务流量负载均衡、隔离开发和生产环境、解耦数据处理和数据存储、跨云备份和灾难恢复、灵活分配计算资源、跨区域服务的低延迟访问以及避免厂商锁定等。一个企业内部往往有多个 Kubernetes 集群，由 MultiCluster SIG 开发的 KubeFed 实现 Kubernetes 集群联邦可以实现多集群管理的功能，这使得所有 Kubernetes 集群都通过同一个接口来管理。&lt;/p&gt;
&lt;p&gt;在使用集群联邦时需要解决以下几个通用问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;配置需要联邦哪些集群&lt;/li&gt;
&lt;li&gt;需要在集群中传播的 API 资源&lt;/li&gt;
&lt;li&gt;配置 API 资源如何分配到不同的集群&lt;/li&gt;
&lt;li&gt;对集群中 DNS 记录注册以实现跨集群的服务发现&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面是 KubeSphere 的多集群架构，也是最常用的一种 Kubernetes 多集群管理架构，其中 Host Cluster 作为控制平面，有两个成员集群，分别是 West 和 East。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/multicluster-management-with-kubernetes-and-istio/008i3skNly1gsgg7a2ojvj31aa0u0491.jpg&#34; data-img=&#34;/blog/multicluster-management-with-kubernetes-and-istio/008i3skNly1gsgg7a2ojvj31aa0u0491.jpg&#34; data-width=&#34;1666&#34; data-height=&#34;1080&#34; alt=&#34;image&#34; data-caption=&#34;Multicluster&#34;&gt;
    
  
  &lt;figcaption&gt;Multicluster&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Host 集群需要能够访问 Member 集群的 API Server，Member 集群之间的网络连通性没有要求。管理集群 Host Cluster 独立于其所管理的成员集群，Member Cluster 并不知道 Host Cluster 存在，这样做的好处是当控制平面发生故障时不会影响到成员集群，已经部署的负载仍然可以正常运行，不会受到影响。&lt;/p&gt;
&lt;p&gt;Host 集群同时承担着 API 入口的作用，由 Host Cluster 将对 Member 集群的资源请求转发到 Member 集群，这样做的目的是方便聚合，而且也利于做统一的权限认证。我们看到在 Host Cluster 中有联邦控制平面，其中的 Push Reconciler 会将联邦集群中身份、角色及角色绑定传播到所有成员集群中。&lt;/p&gt;
&lt;h2 id=&#34;istio&#34;&gt;Istio&lt;/h2&gt;
&lt;p&gt;当我们在 Kubernetes 中运行着多语言、多版本的微服务，并需要更细粒度的金丝雀发布和统一的安全策略管理，实现服务间的可观察性时，可以考虑使用 Istio 服务网格。Istio 通过向应用程序 Pod 中注入 sidecar proxy，缺省使用 IPTables 透明得拦截进出应用程序的所有流量，从而实现了应用层到集群中其他启用服务网格的服务的智能应用感知负载均衡，并绕过了初级的 kube-proxy 负载均衡。Istio 控制平面与 Kubernetes API Server 通信可以获取集群中所有注册的服务信息。&lt;/p&gt;
&lt;p&gt;下图展示了 Istio 的基本原理，其中所有节点属于同一个 Kubernetes 集群。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/multicluster-management-with-kubernetes-and-istio/008i3skNly1gsgg6sdrk2j32v60u0qbb.jpg&#34; data-img=&#34;/blog/multicluster-management-with-kubernetes-and-istio/008i3skNly1gsgg6sdrk2j32v60u0qbb.jpg&#34; data-width=&#34;3714&#34; data-height=&#34;1080&#34; alt=&#34;image&#34; data-caption=&#34;Istio Service Mesh&#34;&gt;
    
  
  &lt;figcaption&gt;Istio Service Mesh&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;你可能最终会有至少几个Kubernetes集群，每个集群都承载着微服务。Istio 的多集群部署根据网络隔离、主备情况存在多种&lt;a href=&#34;https://istio.io/latest/docs/setup/install/multicluster/&#34; title=&#34;部署模式&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;部署模式&lt;/a&gt;
，可以使用 Istio Operator 部署时通过声明来指定。集群中的这些微服务之间的通信可以通过服务网格来加强。在集群内部，Istio提供通用的通信模式，以提高弹性、安全性和可观察性。&lt;/p&gt;
&lt;p&gt;以上都是关于 Kubernetes 上的应用负载管理，但是对于虚拟机上遗留应用，如何在同一个平面中管理？如何管理多集群中的流量划分、网关和安全性呢？&lt;/p&gt;
&lt;h2 id=&#34;管理平面&#34;&gt;管理平面&lt;/h2&gt;
&lt;p&gt;在 Istio 之上再增加一层抽象，将网关、流量和安全分组管理，并将它们应用到不同的集群和命名空间上。下图展示的是 &lt;a href=&#34;https://www.tetrate.io/tetrate-service-bridge/&#34; title=&#34;Tetrate Service Bridge&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tetrate Service Bridge&lt;/a&gt;
 的多租户模型，利用 NGAC 来管理用户的访问权限，同时也有利于构建零信任网络。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/multicluster-management-with-kubernetes-and-istio/008i3skNly1gsgg8ndcajj31il0u00z9.jpg&#34; data-img=&#34;/blog/multicluster-management-with-kubernetes-and-istio/008i3skNly1gsgg8ndcajj31il0u00z9.jpg&#34; data-width=&#34;1965&#34; data-height=&#34;1080&#34; alt=&#34;image&#34; data-caption=&#34;Management Plane&#34;&gt;
    
  
  &lt;figcaption&gt;Management Plane&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Istio 提供了工作负载识别，并由强大的 mTLS 加密保护。这种零信任模型比基于源 IP 等拓扑信息来信任工作负载更好。在 Istio 之上构建一个多集群管理的通用控制平面，然后再增加一个管理平面来管理多集群，提供多租户、管理配置、可观察性等功能。&lt;/p&gt;
&lt;p&gt;下图展示的是 Tetrate Service Bridge 的架构图。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/multicluster-management-with-kubernetes-and-istio/008i3skNly1gsgg951mknj314g0u0dnf.jpg&#34; data-img=&#34;/blog/multicluster-management-with-kubernetes-and-istio/008i3skNly1gsgg951mknj314g0u0dnf.jpg&#34; data-width=&#34;1456&#34; data-height=&#34;1080&#34; alt=&#34;image&#34; data-caption=&#34;Tetrate Service Bridge&#34;&gt;
    
  
  &lt;figcaption&gt;Tetrate Service Bridge&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;使用 Kubernetes 实现了异构集群的互操作性，Istio 将容器化负载和虚拟机负载纳入到一个同一个控制平面内，统一管理集群内的流量、安全和可观察性。但是，随着集群数量、网络环境和用户权限的越发复杂，人们还需要在 Istio 的控制平面至上再构建一层管理平面来进行混合云管理。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Istio 开源四周年回顾与展望</title>
      <link>https://jimmysong.io/blog/istio-4-year-birthday/</link>
      <pubDate>Mon, 24 May 2021 08:00:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/istio-4-year-birthday/</guid>
      <description>
        
        
        &lt;p&gt;Istio 是由 &lt;a href=&#34;https://tetrate.io/&#34; title=&#34;Tetrate&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tetrate&lt;/a&gt;
 创始人 Varun Talwar 和谷歌首席工程师 Louis Ryan 命名并在 2017 年 5 月 24 日开源。今天是 Istio 开源四周年，让我们一起来回顾一下 Istio 四年来的发展并展望一下它的未来。&lt;/p&gt;
&lt;h2 id=&#34;istio-的开源历史&#34;&gt;Istio 的开源历史&lt;/h2&gt;
&lt;p&gt;2017 年是 Kubernetes 结束容器编排之战的一年，Google 为了巩固在云原生领域的优势，并弥补 Kubernetes 在服务间流量管理方面的劣势，趁势开源了 Istio。下面是截止目前 Istio 历史上最重要的几次版本发布。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;日期&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;版本&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2017-05-24&lt;/td&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;td&gt;正式开源，该版本发布时仅一个命令行工具。确立了功能范围和 sidecar 部署模式，确立的 Envoy 作为默认 sidecar proxy 的地位。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2017-10-10&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;支持多运行时环境，如虚拟机。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018-06-01&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;API 重构。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018-07-31&lt;/td&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;td&gt;生产就绪，此后 Istio 团队被大规模重组。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019-03-19&lt;/td&gt;
&lt;td&gt;1.1&lt;/td&gt;
&lt;td&gt;企业就绪，支持多 Kubernetes 集群，性能优化。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020-03-03&lt;/td&gt;
&lt;td&gt;1.5&lt;/td&gt;
&lt;td&gt;回归单体架构，支持 WebAssembly 扩展，使得 Istio 的生态更加强大。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020-11-18&lt;/td&gt;
&lt;td&gt;1.8&lt;/td&gt;
&lt;td&gt;正式放弃 Mixer，进一步完善对虚拟机的支持。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Istio 开源后经过了一年时间的发展，在 1.0 发布的前两个月发布了 0.8 版本，这是对 API 的一次大规模重构。而在 2018 年 7 月底发布 1.0 时， Istio 达到了生产可用的临界点，此后 Google 对 Istio 团队进行了大规模重组，多家以 Istio 为基础的 Service Mesh &lt;a href=&#34;https://istio.io/latest/about/ecosystem/#providers&#34; title=&#34;创业公司&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;创业公司&lt;/a&gt;
诞生，可以说 2018 年是服务网格行业诞生的元年。&lt;/p&gt;
&lt;p&gt;2019年 3 月 Istio 1.1 发布，而这距离 1.0 发布已经过去了近 9 个月，这已经远远超出一个开源项目的平均发布周期。我们知道迭代和进化速度是基础软件的核心竞争力，此后 Istio 开始以每个季度一个版本的固定&lt;a href=&#34;https://istio.io/v1.7/about/release-cadence/&#34; title=&#34;发布节奏&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;发布节奏&lt;/a&gt;
，并在 2019 年成为了 &lt;a href=&#34;https://octoverse.github.com/#fastest-growing-oss-projects-by-contributors&#34; title=&#34;GitHub 增长最快的十大项目中排名第 4 名&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub 增长最快的十大项目中排名第 4 名&lt;/a&gt;
！&lt;/p&gt;
&lt;h2 id=&#34;istio-社区&#34;&gt;Istio 社区&lt;/h2&gt;
&lt;p&gt;Istio 开源四年来，已经在 GitHub 上收获了 2.7 万颗星，获得了大量的&lt;a href=&#34;https://istio.io/latest/about/case-studies/&#34; title=&#34;社区用户&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;社区用户&lt;/a&gt;
。下图是 &lt;a href=&#34;https://github.com/istio/istio&#34; title=&#34;Istio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio&lt;/a&gt;
 的 GitHub star 数增长情况。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/istio-4-year-birthday/008i3skNly1gqtm7n2hm1j31me0n2tag.jpg&#34; data-img=&#34;/blog/istio-4-year-birthday/008i3skNly1gqtm7n2hm1j31me0n2tag.jpg&#34; data-width=&#34;2102&#34; data-height=&#34;830&#34; alt=&#34;image&#34; data-caption=&#34;&#34;&gt;
    
  
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;2020 年 Istio 的项目管理开始走向成熟，治理方式也到了进化的阶段。2020 年，Istio 社区进行了第一次&lt;a href=&#34;https://istio.io/latest/blog/2020/steering-election-results/&#34; title=&#34;管委会选举&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;管委会选举&lt;/a&gt;
，还把商标转让给了 &lt;a href=&#34;https://istio.io/latest/blog/2020/open-usage/&#34; title=&#34;Open Usage Commons&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open Usage Commons&lt;/a&gt;
。首届 &lt;a href=&#34;https://events.istio.io/istiocon-2021/&#34; title=&#34;IstioCon&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IstioCon&lt;/a&gt;
 在 2021 年 2 月份成功举办，几千人参加了线上会议。在中国也有大量的 Istio 社区用户，2021 年也会有线下面对面的 Istio 社区 meetup 在中国举办。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/istio-4-year-birthday/008i3skNly1gquicfqg14j31lw0smwl2.jpg&#34; data-img=&#34;/blog/istio-4-year-birthday/008i3skNly1gquicfqg14j31lw0smwl2.jpg&#34; data-width=&#34;2084&#34; data-height=&#34;1030&#34; alt=&#34;image&#34; data-caption=&#34;&#34;&gt;
    
  
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;根据 CNCF 2020 年调查，46% 的组织在生产中使用服务网格或计划在未来 12 个月内使用。Istio 是在生产中使用的最多的网格。&lt;/p&gt;
&lt;h2 id=&#34;未来&#34;&gt;未来&lt;/h2&gt;
&lt;p&gt;经过 4 年的发展，围绕 Istio 不仅形成了庞大的用户群，还诞生了多家 Istio 供应商，你可以在最近改版的 &lt;a href=&#34;https://istio.io&#34; title=&#34;Istio 的官网首页&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio 的官网首页&lt;/a&gt;
中看到。在最近几个版本中，Istio 已经将发展中心转移到了提升 Day 2 Operation 体验上来了。我们还希望看到更多的 Istio 的采纳路径建议、案例研究、学习资料、培训及认证（例如来自 Tetrate 的业界的第一个 &lt;a href=&#34;https://academy.tetrate.io/courses/certified-istio-administrator&#34; title=&#34;Istio 管理员认证&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio 管理员认证&lt;/a&gt;
），这些都将有利于 Istio 的推广和采用。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>什么是 Istio？为什么 Kubernetes 需要 Istio？</title>
      <link>https://jimmysong.io/blog/what-is-istio-and-why-does-kubernetes-need-it/</link>
      <pubDate>Wed, 28 Apr 2021 09:06:14 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/what-is-istio-and-why-does-kubernetes-need-it/</guid>
      <description>
        
        
        &lt;p&gt;Istio 是当前&lt;a href=&#34;https://www.cncf.io/blog/2020/03/04/2019-cncf-survey-results-are-here-deployments-are-growing-in-size-and-speed-as-cloud-native-adoption-becomes-mainstream/&#34; title=&#34;最流行的服务网格实现&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;最流行的服务网格实现&lt;/a&gt;
，它是在 Kubernetes 的基础上开发的，它跟 Kubernetes 在云原生应用的生态中拥有着不同的定位。本文不是直接为你介绍 Istio 具有哪些功能，而是先向你介绍 Istio 诞生的历史条件，然后带你从 Kubernetes 与 Istio 的分工开始，了解什么是 Istio。&lt;/p&gt;
&lt;p&gt;要想解释什么是 Istio，还得先了解 Istio 是在什么样的情况下出现的——即为什么会有 Istio？&lt;/p&gt;
&lt;p&gt;容器作为云原生应用的交付物，既解决了环境一致性的问题，又可以更细粒度的限制应用资源，但是随着微服务和 DevOps 的流行，容器作为微服务的载体得以广泛应用。2014 年，Google 开源了 Kubernetes，随后几年得到迅猛发展，在 2017 年奠定了容器编排调度标准的地位。Kubernetes 作为一种容器编排调度工具，解决了分布式应用程序的部署和调度问题。因为一台单机的资源有限，而互联网应用可能因为用户规模的急速扩张，或用户属性的不同在不同时间段会出现流量洪峰，因此对计算资源的弹性要求比较高。而一台单机显然无法满足一个如何规模庞大的应用，反之，对于一个规模很小的应用也没必要占用整台主机，那将导致巨大的浪费。&lt;/p&gt;
&lt;p&gt;简而言之，Kubernetes 定义服务的最终状态，并使系统自动地达到和维持在该状态。那么在应用部署完成后，如何管理服务上的流量呢？下面我们将看下 Kubernetes 中如何做服务管理，及在 Istio 中的变化。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-中如何做服务管理&#34;&gt;Kubernetes 中如何做服务管理？&lt;/h2&gt;
&lt;p&gt;下图展示的是 Kubernetes 中的服务模型。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/what-is-istio-and-why-does-kubernetes-need-it/service-model.jpg&#34; data-img=&#34;/blog/what-is-istio-and-why-does-kubernetes-need-it/service-model.jpg&#34; data-width=&#34;1920&#34; data-height=&#34;1200&#34; alt=&#34;image&#34; data-caption=&#34;Kubernetes 服务模型&#34;&gt;
    
  
  &lt;figcaption&gt;Kubernetes 服务模型&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;从上图中我们可以看出：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;同一个服务的的不同示例可能被调度到不同的节点上；&lt;/li&gt;
&lt;li&gt;Kubernetes 通过 Service 对象将一个服务的多个实例组合在了一起，统一对外服务；&lt;/li&gt;
&lt;li&gt;Kubernetes 在每个 node 中安装了 &lt;code&gt;kube-proxy&lt;/code&gt;  组件来转发流量，它拥有的简单的负载均衡功能；&lt;/li&gt;
&lt;li&gt;Kubernetes 集群外部流量可以通过 Ingress 进入集群中（Kubernetes 还有其他几种暴露服务的方式，如 NodePort、LoadBalancer 等）；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Kubernetes 是用于资源集约管理的工具。但在为应用分配好资源后，如何保证应用的健壮性、冗余性，如何实现更细粒度的流量划分（不是根据服务中实例个数来实现），如何保障服务的安全性，如何进行多集群管理等，这些问题 Kubernetes 都不能很好地解决。&lt;/p&gt;
&lt;p&gt;服务具有多个版本，需要迭代和上线，在新版发布的时候需要切分流量，实现金丝雀发布；同时我们应该假定服务是不可靠的，可能因为各种原因导致请求失败，需要面向失败来编程，如何监控应用程序的指标，了解每个请求的耗时和状态？Istio 的发起这们就想到了在每个 pod 中注入一个代理，将代理的配置通过一个控制平面集中分发，然后将从 pod 中应用容器发起的每个请求都劫持到 sidecar 代理中，然后转发，这样不就可以完美的解决以上问题了吗？Kubernetes 优秀的架构和可扩展性，例如 CRD，pod 内的部署模式，可以完美的解决大量 sidecar 的注入和管理问题，使得 Istio 的实现成为可能。&lt;/p&gt;
&lt;h2 id=&#34;istio-的基本原理&#34;&gt;Istio 的基本原理&lt;/h2&gt;
&lt;p&gt;下图是 Istio 中的服务模型，它既可以支持 Kubernetes 中的工作负载，又可以支持虚拟机。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/what-is-istio-and-why-does-kubernetes-need-it/istio.jpg&#34; data-img=&#34;/blog/what-is-istio-and-why-does-kubernetes-need-it/istio.jpg&#34; data-width=&#34;1999&#34; data-height=&#34;1134&#34; alt=&#34;image&#34; data-caption=&#34;Istio&#34;&gt;
    
  
  &lt;figcaption&gt;Istio&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;从图中我们可以看出：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Istiod 作为控制平面，将配置下发给所有的 sidecar proxy 和 gateway（为了美观，图中没有画 Istiod 及 sidecar 之间的连接）&lt;/li&gt;
&lt;li&gt;Istio 不再使用 &lt;code&gt;kube-proxy&lt;/code&gt; 组件做流量转发，而是依托在每个 pod 中注入的 sidecar proxy，所有的 proxy 组成了 Istio 的数据平面；&lt;/li&gt;
&lt;li&gt;应用程序管理员可以和管理 Kubernetes 中的工作负载一样，通过声明式 API 操作 Istio mesh 中流量的行为；&lt;/li&gt;
&lt;li&gt;Ingress 被 Gateway 资源所替代，Gateway 是一种特殊的 proxy，实际上也是复用的 Sidecar proxy；&lt;/li&gt;
&lt;li&gt;可以在虚拟机中安装 sidecar proxy，将虚拟机引入的 Istio mesh 中；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;实际上在 Istio 之前，人们可以使用 SpringCloud、Netflix OSS 等，通过在应用程序中集成 SDK，编程的方式来管理应用程序中的流量。但是这通常会有编程语言限制，而且在 SDK 升级的时候，需要修改代码并重新上线应用，会增大人力负担。Istio 使得流量管理变得对应用程序透明，使这部分功能从应用程序中转移到了平台层，成为了云原生基础设施。&lt;/p&gt;
&lt;p&gt;正是因为 Istio 补足了 Kubernetes 对于云原生应用的流量管理、可观察性和安全方面的短板，在 2017 年由 Google、IBM 和 Lyft 共同发起的这个服务网格开源项目，并在三年来取得了长足的发展。关于 Istio 核心功能的介绍可以参考 &lt;a href=&#34;https://istio.io/latest/docs/concepts/what-is-istio/&#34; title=&#34;Istio 文档&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio 文档&lt;/a&gt;
。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Service Mesh 相当于云原生时代的 TCP/IP，解决应用程序网络通信、安全及可见性问题；&lt;/li&gt;
&lt;li&gt;Istio 是目前最流行的 service mesh 实现，依托于 Kubernetes，但也可以扩展到虚拟机负载；&lt;/li&gt;
&lt;li&gt;Istio 的核心由控制平面和数据平面组成，Envoy 是默认的数据平面代理；&lt;/li&gt;
&lt;li&gt;Istio 作为云原生基础设施的网络层，对应用透明。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>为什么在使用了 Kubernetes 后你可能还需要 Istio？</title>
      <link>https://jimmysong.io/blog/why-do-you-need-istio-when-you-already-have-kubernetes/</link>
      <pubDate>Wed, 07 Apr 2021 08:27:17 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/why-do-you-need-istio-when-you-already-have-kubernetes/</guid>
      <description>
        
        
        &lt;p&gt;如果你听说过服务网格，并尝试过 &lt;a href=&#34;https://istio.io/&#34; title=&#34;Istio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio&lt;/a&gt;
，你可能有以下问题。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为什么 Istio 要在 Kubernetes 上运行？&lt;/li&gt;
&lt;li&gt;Kubernetes 和服务网格在云原生应用架构中分别扮演什么角色？&lt;/li&gt;
&lt;li&gt;Istio 扩展了 Kubernetes 的哪些方面？它解决了哪些问题？&lt;/li&gt;
&lt;li&gt;Kubernetes、Envoy 和 Istio 之间是什么关系？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本文将带大家了解 Kubernetes 和 Istio 的内部工作原理。此外，我会介绍 Kubernetes 中的负载均衡方法，并解释为什么有了 Kubernetes 后还需要 Istio。&lt;/p&gt;
&lt;p&gt;Kubernetes 本质上是通过声明式配置来实现应用生命周期管理，而服务网格本质上是提供应用间的流量、安全管理和可观察性。如果你已经使用 Kubernetes 搭建了一个稳定的应用平台，那么如何设置服务间调用的负载均衡和流量控制？是否有这样一个通用的工具或者说平台（非 SDK），可以实现？这就需要用到服务网格了。&lt;/p&gt;
&lt;p&gt;Envoy 引入了 xDS 协议，这个协议得到了各种开源软件的支持，比如 Istio、&lt;a href=&#34;https://mosn.io/&#34; title=&#34;MOSN&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MOSN&lt;/a&gt;
 等。Envoy 将 xDS 贡献给服务网格或云原生基础设施。Envoy 本质上是一个现代版的代理，可以通过 API 进行配置，在此基础上衍生出许多不同的使用场景–比如 API Gateway、服务网格中的 sidecar 代理和边缘代理。&lt;/p&gt;
&lt;p&gt;本文包含以下内容。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kube-proxy 的作用描述。&lt;/li&gt;
&lt;li&gt;Kubernetes 在微服务管理方面的局限性。&lt;/li&gt;
&lt;li&gt;Istio 服务网格的功能介绍。&lt;/li&gt;
&lt;li&gt;Kubernetes、Envoy 和 Istio 服务网格中一些概念的比较。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kubernetes-vs-service-mesh&#34;&gt;Kubernetes vs Service Mesh&lt;/h2&gt;
&lt;p&gt;下图显示了 Kubernetes 中的服务访问关系和服务网格（每个 pod 模型一个 sidecar）。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/why-do-you-need-istio-when-you-already-have-kubernetes/008eGmZEly1gpb7knfo4dj31hk0redrz.jpg&#34; data-img=&#34;/blog/why-do-you-need-istio-when-you-already-have-kubernetes/008eGmZEly1gpb7knfo4dj31hk0redrz.jpg&#34; data-width=&#34;1928&#34; data-height=&#34;986&#34; alt=&#34;image&#34; data-caption=&#34;Kubernetes vs Service Mesh&#34;&gt;
    
  
  &lt;figcaption&gt;Kubernetes vs Service Mesh&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;流量转发&#34;&gt;流量转发&lt;/h3&gt;
&lt;p&gt;Kubernetes 集群中的每个节点都部署了一个 kube-proxy 组件，该组件与 Kubernetes API Server 进行通信，获取集群中的服务信息，然后设置 iptables 规则，将服务请求直接发送到对应的 Endpoint（属于同一组服务的 pod）。&lt;/p&gt;
&lt;h3 id=&#34;服务发现&#34;&gt;服务发现&lt;/h3&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/why-do-you-need-istio-when-you-already-have-kubernetes/008eGmZEly1gpb7knwb79j30kq0fcjs9.jpg&#34; data-img=&#34;/blog/why-do-you-need-istio-when-you-already-have-kubernetes/008eGmZEly1gpb7knwb79j30kq0fcjs9.jpg&#34; data-width=&#34;746&#34; data-height=&#34;552&#34; alt=&#34;image&#34; data-caption=&#34;Service Discovery&#34;&gt;
    
  
  &lt;figcaption&gt;Service Discovery&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Istio 可以跟踪 Kubernetes 中的服务注册，也可以在控制平面中通过平台适配器与其他服务发现系统对接；然后生成数据平面的配置（使用 CRD，这些配置存储在 etcd 中），数据平面的透明代理。数据平面的透明代理以 sidecar 容器的形式部署在每个应用服务的 pod 中，这些代理都需要请求控制平面同步代理配置。代理之所以 “透明”，是因为应用容器完全不知道代理的存在。过程中的 kube-proxy 组件也需要拦截流量，只不过 kube-proxy 拦截的是进出 Kubernetes 节点的流量–而 sidecar 代理拦截的是进出 pod 的流量。&lt;/p&gt;
&lt;h3 id=&#34;服务网格的劣势&#34;&gt;服务网格的劣势&lt;/h3&gt;
&lt;p&gt;由于 Kubernetes 的每个节点上都运行着很多 pod，所以在每个 pod 中放入原有的 kube-proxy 路由转发功能，会增加响应延迟–由于 sidecar 拦截流量时跳数更多，消耗更多的资源。为了对流量进行精细化管理，将增加一系列新的抽象功能。这将进一步增加用户的学习成本，但随着技术的普及，这种情况会慢慢得到缓解。&lt;/p&gt;
&lt;h3 id=&#34;服务网格的优势&#34;&gt;服务网格的优势&lt;/h3&gt;
&lt;p&gt;kube-proxy 的设置是全局的，无法对每个服务进行细粒度的控制，而 service mesh 通过 sidecar proxy 的方式将 Kubernetes 中的流量控制从服务层中抽离出来–可以实现更大的弹性。&lt;/p&gt;
&lt;h3 id=&#34;kube-proxy-的不足之处&#34;&gt;Kube-proxy 的不足之处&lt;/h3&gt;
&lt;p&gt;首先，如果转发的 pod 不能正常服务，它不会自动尝试其他 pod。每个 pod 都有一个健康检查机制，当一个 pod 出现健康问题时，kubelet 会重启 pod，kube-proxy 会删除相应的转发规则。另外，节点 Port 类型的服务不能添加 TLS 或更复杂的消息路由机制。&lt;/p&gt;
&lt;p&gt;Kube-proxy 实现了一个 Kubernetes 服务的多个 pod 实例之间的流量负载均衡，但如何对这些服务之间的流量进行精细化控制–比如将流量按百分比划分给不同的应用版本（这些应用版本都是同一个服务的一部分，但在不同的部署上），或者做金丝雀发布（灰度发布）和蓝绿发布？&lt;/p&gt;
&lt;p&gt;Kubernetes 社区给出了一个使用 Deployment 做&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments&#34; title=&#34;金丝雀发布&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;金丝雀发布&lt;/a&gt;
的方法，本质上是通过修改 pod 的标签来给部署的服务分配不同的 pod。&lt;/p&gt;
&lt;h3 id=&#34;kubernetes-ingress-vs-istio-gateway&#34;&gt;Kubernetes Ingress vs Istio Gateway&lt;/h3&gt;
&lt;p&gt;如上所述，kube-proxy 只能在 Kubernetes 集群内路由流量。Kubernetes 集群的 pod 位于 CNI 创建的网络中。Ingress 是在 Kubernetes 中创建的资源对象，用于集群外部的通信。它由位于 Kubernetes 边缘节点上的入口控制器驱动，负责管理南北向流量。Ingress 必须与各种 Ingress 控制器对接，比如 &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34; title=&#34;nginx ingress 控制器&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nginx ingress 控制器&lt;/a&gt;
和 &lt;a href=&#34;https://traefik.io/&#34; title=&#34;traefik&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;traefik&lt;/a&gt;
。Ingress 只适用于 HTTP 流量，使用简单。它只能通过匹配有限的字段来路由流量——如服务、端口、HTTP 路径等。这使得它无法对 TCP 流量进行路由，如 MySQL、Redis 和各种 RPC。这就是为什么你会看到人们在 ingress 资源注释中写 Nginx 配置语言的原因（注：使用 Nginx Ingress Controller 可以通过 &lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/&#34; title=&#34;配置 ConfigMap 和 Service 的方式&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;配置 ConfigMap 和 Service 的方式&lt;/a&gt;
来变通支持 TCP 和 UDP  流量转发）。直接路由南北流量的唯一通行方法是使用服务的 LoadBalancer 或 NodePort，前者需要云厂商支持，后者需要额外的端口管理。&lt;/p&gt;
&lt;p&gt;Istio Gateway 的功能与 Kubernetes Ingress 类似，它负责进出集群的南北流量。Istio Gateway 描述了一个负载均衡器，用于承载进出服务网格边缘的连接。该规范描述了一组开放端口和这些端口所使用的协议，以及用于负载均衡的 SNI 配置等。Gateway 是一个 CRD 扩展，它也重用了 sidecar 代理的功能；详细配置请参见 &lt;a href=&#34;https://istio.io/latest/docs/reference/config/networking/gateway/&#34; title=&#34;Istio 网站&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio 网站&lt;/a&gt;
。&lt;/p&gt;
&lt;h2 id=&#34;envoy&#34;&gt;Envoy&lt;/h2&gt;
&lt;p&gt;Envoy 是 Istio 中默认的 sidecar 代理。Istio 基于 Enovy 的 xDS 协议扩展了其控制平面。在讨论 Envoy 的 xDS 协议之前，我们需要先熟悉 Envoy 的基本术语。下面是 Envoy 的架构图。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/why-do-you-need-istio-when-you-already-have-kubernetes/envoy-arch.jpg&#34; data-img=&#34;/blog/why-do-you-need-istio-when-you-already-have-kubernetes/envoy-arch.jpg&#34; data-width=&#34;1492&#34; data-height=&#34;1080&#34; alt=&#34;image&#34; data-caption=&#34;Envoy 架构图&#34;&gt;
    
  
  &lt;figcaption&gt;Envoy 架构图&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;基础概念&#34;&gt;基础概念&lt;/h3&gt;
&lt;p&gt;以下是 Enovy 中你应该知道的基本术语。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;下游。下游主机连接到 Envoy，发送请求，并接收响应，即发送请求的主机。&lt;/li&gt;
&lt;li&gt;上游：上游主机。上游主机接收来自 Envoy 的连接和请求，并返回响应；即接收请求的主机。&lt;/li&gt;
&lt;li&gt;Listener：监听器。监听器是一个命名的网络地址（如端口、UNIX 域套接字等）；下游客户端可以连接到这些监听器。Envoy 将一个或多个监听器暴露给下游主机进行连接。&lt;/li&gt;
&lt;li&gt;集群。集群是一组逻辑上相同的上游主机，Envoy 连接到它们。Envoy 通过服务发现来发现集群的成员。可以选择通过主动的健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略来决定集群中哪个成员的请求路由。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 Envoy 中可以设置多个监听器，每个监听器可以设置一个过滤链（过滤链表），而且过滤链是可扩展的，这样我们可以更方便地操纵流量的行为–比如设置加密、私有 RPC 等。&lt;/p&gt;
&lt;p&gt;xDS 协议是由 Envoy 提出的，是 Istio 中默认的 sidecar 代理，但只要实现了 xDS 协议，理论上也可以作为 Istio 中的 sidecar 代理 —— 比如蚂蚁集团开源的 &lt;a href=&#34;https://mosn.io&#34; title=&#34;MOSN&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MOSN&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cdn.thenewstack.io/media/2021/03/b800bf17-image3.png&#34; title=&#34;&amp;lt;figure class=&amp;#34;mx-auto text-center&amp;#34;&amp;gt;
  
  
  
  
    
    &amp;lt;img src=&amp;#34;/blog/why-do-you-need-istio-when-you-already-have-kubernetes/008eGmZEly1gpb7kk7wk4j31060lqgqx.jpg&amp;#34; data-img=&amp;#34;/blog/why-do-you-need-istio-when-you-already-have-kubernetes/008eGmZEly1gpb7kk7wk4j31060lqgqx.jpg&amp;#34; data-width=&amp;#34;1302&amp;#34; data-height=&amp;#34;782&amp;#34; alt=&amp;#34;image&amp;#34; data-caption=&amp;#34;img&amp;#34;&amp;gt;
    
  
  &amp;lt;figcaption&amp;gt;img&amp;lt;/figcaption&amp;gt;
&amp;lt;/figure&amp;gt;
&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/why-do-you-need-istio-when-you-already-have-kubernetes/008eGmZEly1gpb7kk7wk4j31060lqgqx.jpg&#34; data-img=&#34;/blog/why-do-you-need-istio-when-you-already-have-kubernetes/008eGmZEly1gpb7kk7wk4j31060lqgqx.jpg&#34; data-width=&#34;1302&#34; data-height=&#34;782&#34; alt=&#34;image&#34; data-caption=&#34;img&#34;&gt;
    
  
  &lt;figcaption&gt;img&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Istio 是一个功能非常丰富的服务网格，包括以下功能。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;流量管理。这是 Istio 最基本的功能。&lt;/li&gt;
&lt;li&gt;策略控制。实现访问控制系统、遥测采集、配额管理、计费等功能。&lt;/li&gt;
&lt;li&gt;可观察性。在 sidecar 代理中实现。&lt;/li&gt;
&lt;li&gt;安全认证。由 Citadel 组件进行密钥和证书管理。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;istio-中的流量管理&#34;&gt;Istio 中的流量管理&lt;/h2&gt;
&lt;p&gt;Istio 中定义了以下 CRD 来帮助用户进行流量管理。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;网关。网关描述了一个运行在网络边缘的负载均衡器，用于接收传入或传出的 HTTP/TCP 连接。&lt;/li&gt;
&lt;li&gt;虚拟服务（VirtualService）。VirtualService 实际上是将 Kubernetes 服务连接到 Istio 网关。它还可以执行额外的操作，例如定义一组流量路由规则，以便在主机寻址时应用。&lt;/li&gt;
&lt;li&gt;DestinationRule。DestinationRule 定义的策略决定了流量被路由后的访问策略。简单来说，它定义了流量的路由方式。其中，这些策略可以定义为负载均衡配置、连接池大小和外部检测（用于识别和驱逐负载均衡池中不健康的主机）配置。&lt;/li&gt;
&lt;li&gt;EnvoyFilter。EnvoyFilter 对象描述了代理服务的过滤器，可以自定义 Istio Pilot 生成的代理配置。这种配置一般很少被主用户使用。&lt;/li&gt;
&lt;li&gt;ServiceEntry。默认情况下，Istio 服务 Mesh 中的服务无法发现 Mesh 之外的服务。ServiceEntry 可以在 Istio 内部的服务注册表中添加额外的条目，从而允许 Mesh 中自动发现的服务访问并路由到这些手动添加的服务。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kubernetes-vs-xds-vs-istio&#34;&gt;Kubernetes vs xDS vs Istio&lt;/h2&gt;
&lt;p&gt;在回顾了 Kubernetes 的 kube-proxy 组件、xDS 和 Istio 对流量管理的抽象后，现在我们仅从流量管理的角度来看看这三个组件 / 协议的比较（注意，三者并不完全等同）。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;xDS&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Istio service mesh&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Endpoint&lt;/td&gt;
&lt;td&gt;Endpoint&lt;/td&gt;
&lt;td&gt;WorkloadEntry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service&lt;/td&gt;
&lt;td&gt;Route&lt;/td&gt;
&lt;td&gt;VirtualService&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-proxy&lt;/td&gt;
&lt;td&gt;Route&lt;/td&gt;
&lt;td&gt;DestinationRule&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-proxy&lt;/td&gt;
&lt;td&gt;Listener&lt;/td&gt;
&lt;td&gt;EnvoyFilter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ingress&lt;/td&gt;
&lt;td&gt;Listener&lt;/td&gt;
&lt;td&gt;Gateway&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service&lt;/td&gt;
&lt;td&gt;Cluster&lt;/td&gt;
&lt;td&gt;ServiceEntry&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;核心观点&#34;&gt;核心观点&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes 的本质是应用生命周期管理，具体来说就是部署和管理（伸缩、自动恢复、发布）。&lt;/li&gt;
&lt;li&gt;Kubernetes 为微服务提供了一个可扩展、高弹性的部署和管理平台。&lt;/li&gt;
&lt;li&gt;服务网格是基于透明代理，通过 sidecar 代理拦截服务之间的流量，然后通过控制平面配置管理它们的行为。&lt;/li&gt;
&lt;li&gt;服务网格将流量管理与 Kubernetes 解耦，不需要 kube-proxy 组件来支持服务网格内的流量；通过提供更接近微服务应用层的抽象来管理服务间的流量、安全性和可观察性。&lt;/li&gt;
&lt;li&gt;xDS 是服务网格的协议标准之一。&lt;/li&gt;
&lt;li&gt;服务网格是 Kubernetes 中服务的一个更高层次的抽象。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;如果说 Kubernetes 管理的对象是一个 pod，那么服务网格管理的对象就是一个服务，所以用 Kubernetes 管理微服务，然后应用服务网格就可以了。如果你连服务都不想管理，那就用 &lt;a href=&#34;https://knative.dev/&#34; title=&#34;Knative&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Knative&lt;/a&gt;
 这样的无服务器平台，不过这是后话。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>云原生初学者入门必读</title>
      <link>https://jimmysong.io/blog/must-read-for-cloud-native-beginner/</link>
      <pubDate>Sun, 18 Oct 2020 14:18:40 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/must-read-for-cloud-native-beginner/</guid>
      <description>
        
        
        &lt;h2 id=&#34;为什么写这篇文章&#34;&gt;为什么写这篇文章&lt;/h2&gt;
&lt;p&gt;看到这个标题后，大家可能会问“都已经 2020 年了，Kubernetes 开源有 6 年时间了，为什么还要写一篇 Kubernetes 入门的文章？”我想说的是，Kubernetes 还远远没有达到我们想象的那么普及。众多的开发者，平时忙于各自的业务开发，学习新技术的时间有限；还有大量的学生群体，可能还仅仅停留在“知道有这门技术”的阶段，远远没有入门。这篇文章将助于各位有志于从事云原生领域工作或需要了解该领域背景的人群快速入门 Kubernetes 和云原生。&lt;/p&gt;
&lt;p&gt;因为云原生的知识体系过于庞杂，本文主要讲解容器、Kubernetes 及服务网格的入门概念，关于云原生的更多细节将在后续文章中推出。&lt;/p&gt;
&lt;h2 id=&#34;引言&#34;&gt;引言&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/&#34; title=&#34;Kubernetes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes&lt;/a&gt;
 一词来自希腊语，意思是“飞行员”或“舵手”。这个名字很贴切，Kubernetes 可以帮助你在波涛汹涌的容器海洋中航行。&lt;/p&gt;
&lt;p&gt;Kubernetes 是做什么的？什么是 Docker？什么是容器编排？Kubernetes 是如何工作和扩展的？你可能还有很多其他的问题，本文将一一为你解答。&lt;/p&gt;
&lt;p&gt;这篇文章适合初学者，尤其是那些工作忙碌，没有办法抽出太多时间来了解 Kubernetes 和云原生的开发者们，希望本文可以帮助你进入 Kubernetes 的世界。&lt;/p&gt;
&lt;p&gt;简而言之，Kubernetes 提供了一个平台或工具来帮助你快速协调或扩展容器化应用，特别是在 &lt;a href=&#34;https://docker.com/&#34; title=&#34;Docker&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Docker&lt;/a&gt;
 容器。让我们深入了解一下这些概念。&lt;/p&gt;
&lt;h2 id=&#34;容器和容器化&#34;&gt;容器和容器化&lt;/h2&gt;
&lt;p&gt;那么什么是容器呢？&lt;/p&gt;
&lt;p&gt;要讨论容器化首先要谈到虚拟机 (VM)，顾名思义，虚拟机就是可以远程连接的虚拟服务器，比如 AWS 的 EC2 或阿里云的 ECS。&lt;/p&gt;
&lt;p&gt;接下来，假如你要在虚拟机上运行一个网络应用——包括一个 MySQL 数据库、一个 Vue 前端和一些 Java 库，在 Ubuntu 操作系统 (OS) 上运行。你不用熟悉其中的每一个技术——你只要记住，一个应用程序由各种组件、服务和库组成，它们运行在操作系统上。&lt;/p&gt;
&lt;p&gt;现在，将应用程序打包成一个虚拟机镜像，这个镜像中包括了 Ubuntu 操作系统。这使得虚拟机变得非常笨重——通常有几个 G 的大小。&lt;/p&gt;
&lt;p&gt;虚拟机镜像包含了整个操作系统及所有的库，对应用程序来说，这个镜像过于臃肿，其中大部分组件并没有被应用程序直接调用。如果你需要重新创建、备份或扩展这个应用程序，就需要复制整个环境（虚拟机镜像），在新环境中启动应用通常需要几十秒甚至几分钟时间。如果你想单独升级应用中的某个组件，比如说 Vue 应用，就需要重建整个虚拟机镜像。另外，如果你的两个应用依赖同一个底层镜像，升级底层镜像会同时影响这两个应用，而有时候，你只需要升级其中一个应用的依赖而已。这就是所谓的“依赖陷阱”。&lt;/p&gt;
&lt;p&gt;解决这个问题的办法就是容器。容器是继虚拟机之后更高层次的抽象，在这层抽象中，整个应用程序的每个组件被单独打包成一个个独立的单元，这个单元就是所谓的容器。通过这种方式，可以将代码和应用服务从底层架构中分离出来，实现了完全的可移植性（在任何操作系统或环境上运行应用的能力）。所以在上面的例子中，Ubuntu 操作系统就是一个单元（容器）。MySQL 数据库是另一个容器，Vue 环境和随之而来的库也是一个容器。&lt;/p&gt;
&lt;p&gt;但是，MySQL 数据库是如何自己“运行”的？数据库本身肯定也要在操作系统上运行吧？没错！&lt;/p&gt;
&lt;p&gt;更高层次的容器，比如 MySQL 容器，实际上会包含必要的库来与底层的操作系统容器通信和集成。所以你可以把容器看成是整个应用堆栈中的一层，每层都依赖于下层的单元。而这就类似于船舶或港口中集装箱的堆叠方式，每个容器的稳定性都依赖于下面的容器的支持。所以应用容器的核心是一个受控的执行环境。它们允许你从头开始定义整个环境，从操作系统开始，到你要使用的各个版本的库，再到你要添加的代码版本。&lt;/p&gt;
&lt;p&gt;与容器相关的一个重要概念是&lt;strong&gt;微服务&lt;/strong&gt;。将应用程序的各个组件拆分并打包成独立的服务，这样每个组件都可以很容易地被替换、升级、调试。上面的例子中，我们会为 Vue 前端创建一个微服务，为 MySQL 数据库创建另一个微服务，为 Java 中间件部分创建另一个微服务，以此类推。很明显，微服务与容器化是相辅相成的。&lt;/p&gt;
&lt;h2 id=&#34;从-docker-开始&#34;&gt;从 Docker 开始&lt;/h2&gt;
&lt;p&gt;现在你已经对容器有一定了解了吧？Docker 是最常用的容器化工具，也是最流行的容器运行时。&lt;/p&gt;
&lt;p&gt;Docker 开源于 2013 年。用于打包和创建容器，管理基于容器的应用。所有 Linux 发行版、Windows 和 macOS 都支持 Docker。&lt;/p&gt;
&lt;p&gt;还有其他的容器化工具，如 &lt;a href=&#34;https://coreos.com/rkt/&#34; title=&#34;CoreOS rkt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CoreOS rkt&lt;/a&gt;
、&lt;a href=&#34;http://mesos.apache.org/documentation/latest/mesos-containerizer/&#34; title=&#34;Mesos Containerizer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mesos Containerizer&lt;/a&gt;
 和 &lt;a href=&#34;https://linuxcontainers.org/&#34; title=&#34;LXC&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LXC&lt;/a&gt;
。但是目前，绝大多数的容器化应用都是在 Docker 上运行的。&lt;/p&gt;
&lt;h2 id=&#34;再到-kubernetes&#34;&gt;再到 Kubernetes&lt;/h2&gt;
&lt;p&gt;首先，简单介绍一下历史。Kubernetes 是 Google 基于其内部容器调度平台 Borg 的经验开发的。2014 年开源，并作为 CNCF（云原生计算基金会）的核心发起项目。&lt;/p&gt;
&lt;p&gt;那么 Kubernetes 又跟容器是什么关系呢？让我们再回到上面的例子。假设我们的应用爆火，每天的注册用户越来越多。&lt;/p&gt;
&lt;p&gt;现在，我们需要增加后端资源，使浏览我们网站的用户在浏览页面时加载时间不会过长或者超时。最简单的方式就是增加容器的数量，然后使用负载均衡器将传入的负载（以用户请求的形式）分配给容器。&lt;/p&gt;
&lt;p&gt;这样做虽然行之有效，但也只能在用户规模有限的情况下使用。当用户请求达到几十万或几百万时，这种方法也是不可扩展的。你需要管理几十个也许是几百个负载均衡器，这本身就是另一个令人头疼的问题。如果我们想对网站或应用进行任何升级，也会遇到问题，因为负载均衡不会考虑到应用升级的问题。我们需要单独配置每个负载均衡器，然后升级该均衡器所服务的容器。想象一下，当你有 20 个负载均衡器和每周 5 或 6 个小的更新时，你将不得不进行大量的手工劳动。&lt;/p&gt;
&lt;p&gt;我们需要的是一种可以一次性将变更传递给所有受控容器的方法，同时也需要一种可以轻松地调度可用容器的方法，这个过程还必须要是自动化的，这正是 Kubernetes 所做的事情。&lt;/p&gt;
&lt;p&gt;接下来，我们将探讨 Kubernetes 究竟是如何工作的，它的各种组件和服务，以及更多关于如何使用 Kubernetes 来编排、管理和监控容器化环境。为了简单起见，假设我们使用的是 Docker 容器，尽管如前所述，Kubernetes 除了支持 Docker 之外，还支持其他几种容器平台。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-架构和组件&#34;&gt;Kubernetes 架构和组件&lt;/h2&gt;
&lt;p&gt;首先，最重要的是你需要认识到 Kubernetes 利用了“期望状态”原则。就是说，你定义了组件的期望状态，而 Kubernetes 要将它们始终调整到这个状态。&lt;/p&gt;
&lt;p&gt;例如，你想让你的 Web 服务器始终运行在 4 个容器中，以达到负载均衡的目的，你的数据库复制到 3 个不同的容器中，以达到冗余的目的。这就是你想要的状态。如果这 7 个容器中的任何一个出现故障，Kubernetes 引擎会检测到这一点，并自动创建出一个新的容器，以确保维持所需的状态。&lt;/p&gt;
&lt;p&gt;现在我们来定义一些 Kubernetes 的重要组件。&lt;/p&gt;
&lt;p&gt;当你第一次设置 Kubernetes 时，你会创建一个集群。所有其他组件都是集群的一部分。你也可以创建多个虚拟集群，称为命名空间 (namespace)，它们是同一个物理集群的一部分。这与你可以在同一物理服务器上创建多个虚拟机的方式非常相似。如果你不需要，也没有明确定义的命名空间，那么你的集群将在始终存在的默认命名空间中创建。&lt;/p&gt;
&lt;p&gt;Kubernetes 运行在节点 (node) 上，节点是集群中的单个机器。如果你有自己的硬件，节点可能对应于物理机器，但更可能对应于在云中运行的虚拟机。节点是部署你的应用或服务的地方，是 Kubernetes 工作的地方。有 2 种类型的节点——master 节点和 worker 节点，所以说 Kubernetes 是主从结构的。&lt;/p&gt;
&lt;p&gt;主节点是一个控制其他所有节点的特殊节点。一方面，它和集群中的任何其他节点一样，这意味着它只是另一台机器或虚拟机。另一方面，它运行着控制集群其他部分的软件。它向集群中的所有其他节点发送消息，将工作分配给它们，工作节点向主节点上的 API Server 汇报。&lt;/p&gt;
&lt;p&gt;Master 节点本身也包含一个名为 API Server 的组件。这个 API 是节点与控制平面通信的唯一端点。API Server 至关重要，因为这是 worker 节点和 master 节点就 pod、deployment 和所有其他 Kubernetes API 对象的状态进行通信的点。&lt;/p&gt;
&lt;p&gt;Woker 节点是 Kubernetes 中真正干活的节点。当你在应用中部署容器或 pod（稍后定义）时，其实是在将它们部署到 worker 节点上运行。Worker 节点托管和运行一个或多个容器的资源。&lt;/p&gt;
&lt;p&gt;Kubernetes 中的逻辑而非物理的工作单位称为 pod。一个 pod 类似于 Docker 中的容器。记得我们在前面讲到，容器可以让你创建独立、隔离的工作单元，可以独立运行。但是要创建复杂的应用程序，比如 Web 服务器，你经常需要结合多个容器，然后在一个 pod 中一起运行和管理。这就是 pod 的设计目的——一个 pod 允许你把多个容器，并指定它们如何组合在一起来创建应用程序。而这也进一步明确了 Docker 和 Kubernetes 之间的关系——一个 Kubernetes pod 通常包含一个或多个 Docker 容器，所有的容器都作为一个单元来管理。&lt;/p&gt;
&lt;p&gt;Kubernetes 中的 service 是一组逻辑上的 pod。把一个 service 看成是一个 pod 的逻辑分组，它提供了一个单一的 IP 地址和 DNS 名称，你可以通过它访问服务内的所有 pod。有了服务，就可以非常容易地设置和管理负载均衡，当你需要扩展 Kubernetes pod 时，这对你有很大的帮助，我们很快就会看到。&lt;/p&gt;
&lt;p&gt;ReplicationController 或 ReplicaSet 是 Kubernetes 的另一个关键功能。它是负责实际管理 pod 生命周期的组件——当收到指令时或 pod 离线或意外停止时启动 pod，也会在收到指示时杀死 pod，也许是因为用户负载减少。所以换句话说，ReplicationController 有助于实现我们所期望的指定运行的 pod 数量的状态。&lt;/p&gt;
&lt;h2 id=&#34;什么是-kubectl&#34;&gt;什么是 Kubectl？&lt;/h2&gt;
&lt;p&gt;kubectl 是一个命令行工具，用于与 Kubernetes 集群和其中的 pod 通信。使用它你可以查看集群的状态，列出集群中的所有 pod，进入 pod 中执行命令等。你还可以使用 YAML 文件定义资源对象，然后使用 kubectl 将其应用到集群中。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-中的自动扩展&#34;&gt;Kubernetes 中的自动扩展&lt;/h2&gt;
&lt;p&gt;请记住，我们使用 Kubernetes 而不是直接使用 Docker 的原因之一，是因为 Kubernetes 能够自动扩展应用实例的数量以满足工作负载的需求。&lt;/p&gt;
&lt;p&gt;自动缩放是通过集群设置来实现的，当服务需求增加时，增加节点数量，当需求减少时，则减少节点数量。但也要记住，节点是 “物理” 结构——我们把“物理”放在引号里，因为要记住，很多时候，它们实际上是虚拟机。&lt;/p&gt;
&lt;p&gt;无论如何，节点是物理机器的事实意味着我们的云平台必须允许 Kubernetes 引擎创建新机器。各种云提供商对 Kubernetes 支持基本都满足这一点。&lt;/p&gt;
&lt;p&gt;我们再继续说一些概念，这次是和网络有关的。&lt;/p&gt;
&lt;h2 id=&#34;什么是-kubernetes-ingress-和-egress&#34;&gt;什么是 kubernetes Ingress 和 Egress？&lt;/h2&gt;
&lt;p&gt;外部用户或应用程序与 Kubernetes pod 交互，就像 pod 是一个真正的服务器一样。我们需要设置安全规则允许哪些流量可以进入和离开“服务器”，就像我们为托管应用程序的服务器定义安全规则一样。&lt;/p&gt;
&lt;p&gt;进入 Kubernetes pod 的流量称为 Ingress，而从 pod 到集群外的出站流量称为 egress。我们创建入口策略和出口策略的目的是限制不需要的流量进入和流出服务。而这些策略也是定义 pod 使用的端口来接受传入和传输传出数据 / 流量的地方。&lt;/p&gt;
&lt;h2 id=&#34;什么是-ingress-controller&#34;&gt;什么是 Ingress Controller？&lt;/h2&gt;
&lt;p&gt;但是在定义入口和出口策略之前，你必须首先启动被称为 Ingress Controller（入口控制器）的组件；这个在集群中默认不启动。有不同类型的入口控制器，Kubernetes 项目默认只支持 Google Cloud 和开箱即用的 Nginx 入口控制器。通常云供应商都会提供自己的入口控制器。&lt;/p&gt;
&lt;h2 id=&#34;什么是-replica-和-replicaset&#34;&gt;什么是 Replica 和 ReplicaSet？&lt;/h2&gt;
&lt;p&gt;为了保证应用程序的弹性，需要在不同节点上创建多个 pod 的副本。这些被称为 Replica。假设你所需的状态策略是“让名为 webserver-1 的 pod 始终维持在 3 个副本”，这意味着 ReplicationController 或 ReplicaSet 将监控活动副本的数量，如果其中有任何一个 replica 因任何原因不可用（例如节点的故障），那么 Deployment Controller 将自动创建一个新的系统（定义如下）。&lt;/p&gt;
&lt;p&gt;所需状态是在 deployment 中定义的。 Master 节点的中有一个子系统叫做 Deployment Controller，负责实际执行并使当前状态不断趋向于所需状态。&lt;/p&gt;
&lt;p&gt;因此，举例来说，如果你目前有 2 个 pod 的副本，而你所希望的状态应该有 3 个，那么 Replication Controller 或 ReplicaSet 会自动检测到这个要求，并指示 Deployment Controller 根据预定义的设置部署一个新的 pod。&lt;/p&gt;
&lt;h2 id=&#34;什么是服务网格&#34;&gt;什么是服务网格？&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://jimmysong.io/blog/what-is-a-service-mesh/&#34; title=&#34;服务网格 (Service Mesh)&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;服务网格 (Service Mesh)&lt;/a&gt;
 用于管理服务之间的网络流量，是云原生的网络基础设施层，也是 &lt;a href=&#34;https://jimmysong.io/blog/post-kubernetes-era/&#34; title=&#34;Kubernetes 次世代的云原生应用&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes 次世代的云原生应用&lt;/a&gt;
 的重要组成部分。&lt;/p&gt;
&lt;p&gt;服务网格利用容器之间的网络设置来控制或改变应用程序中不同组件之间的交互。下面，我们用一个例子来说明。假设你想测试 Nginx 的新版本，检查它是否与你的 Web 应用兼容。你用新的 Nginx 版本创建了一个新的容器 (Container2)，并从当前容器 (Container1) 中复制了当前的 Nginx webserver 配置。但你不想影响组成 web 应用的其他微服务（假设每个容器对应一个单独的微服务）——就是 MySQL 数据库、Node.js 前端、负载均衡器等。&lt;/p&gt;
&lt;p&gt;所以使用服务网格，你可以立即只把 webserver 微服务改成 Container2（新 Nginx 版本的那个）进行测试。如果确定它不能工作，比如因为它导致网站出现一些兼容性问题，那么你就调用服务网格来快速切换回原来的 Container1。而这一切都不需要对其他容器进行任何配置变更——这些变更对其他容器是完全透明的。&lt;/p&gt;
&lt;p&gt;如果没有服务网格，对容器来说这项工作将十分繁琐，因为这涉及到逐一更改所有其他容器上的配置，将它们所包含的服务从 Container1 指向 Container2，然后在测试失败后，将它们全部改回来。&lt;/p&gt;
&lt;p&gt;在前面这部分 Kubernetes 指南中，我们介绍了一些与 Kubernetes 网络相关的概念。Kubernetes 中的网络可能很棘手，很难理解，如果你刚刚开始，你可能需要一些实践来理解这里。关于服务网格的更多内容请参考 &lt;a href=&#34;https://www.servicemesher.com/istio-handbook&#34; title=&#34;Istio Handbook——Istio 服务网格进阶实战&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio Handbook——Istio 服务网格进阶实战&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;在下一部分中，我们将展开更多关于 Kubernetes 的话题：如何开始学习 Kubernetes，如何在本地安装和测试 Kubernetes，以及 Kubernetes 的一些优秀的监控工具。&lt;/p&gt;
&lt;h2 id=&#34;如何学习-kubernetes&#34;&gt;如何学习 Kubernetes？&lt;/h2&gt;
&lt;p&gt;自学 Kubernetes 知识基本上有三种不同的途径，我们在这里只提供了一个指导大纲。&lt;/p&gt;
&lt;h3 id=&#34;一从零开始学习和安装-kubernetes&#34;&gt;一、从零开始学习和安装 Kubernetes&lt;/h3&gt;
&lt;p&gt;要想真正掌握 Kubernetes，最好的办法莫过于自己从头开始安装 Kubernetes。不过要注意的是，从零开始安装 Kubernetes 并不是一件容易的事情。安装 Kubernetes 并不是简单的“下载文件 -&amp;gt; 点击安装”式的操作，Kubernetes 由多个组件组成，这些组件必须单独安装和配置。而在此之前，你也需要相当的技术储备来做安装前的准备，比如熟悉 Linux 操作系统。如果你决定使用这种方式学习的话，推荐你阅读 &lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook&#34; title=&#34;Kubernetes Handbook——Kubernetes 中文指南 / 云原生架构实践手册&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes Handbook——Kubernetes 中文指南 / 云原生架构实践手册&lt;/a&gt;
。此外，请记住，尽管 Kubernetes 作为一个开源解决方案在技术上是免费的，但它确实有一些隐藏的成本，只不过对初学者来说可能并不明显。&lt;/p&gt;
&lt;h3 id=&#34;二kubernetes-自托管解决方案&#34;&gt;二、Kubernetes 自托管解决方案&lt;/h3&gt;
&lt;p&gt;这些解决方案样是一些工具和实用程序，大大简化了在本地计算机上安装和配置小型 Kubernetes 集群的任务。它们是学习 Kubernetes 的好方法，同时对于新手来说也不会太难，又足够小巧可以到安装在个人电脑上。最流行的自托管 Kubernetes 工具和环境是 &lt;a href=&#34;https://github.com/kubernetes/minikube&#34; title=&#34;Minikube&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Minikube&lt;/a&gt;
、&lt;a href=&#34;https://github.com/ubuntu/microk8s&#34; title=&#34;MicroK8s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MicroK8s&lt;/a&gt;
、&lt;a href=&#34;https://docs.docker.com/docker-for-windows/kubernetes/&#34; title=&#34;Docker Desktop&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Docker Desktop&lt;/a&gt;
 和 &lt;a href=&#34;https://github.com/kubernetes-sigs/kind&#34; title=&#34;Kind&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kind&lt;/a&gt;
。这些解决方案往往有一些限制，例如，Minikube 只允许创建一个节点。尽管有这些缺点，但这些工具还是非常值得推荐，因为它们将易学性和成本效益结合起来，对于刚开始使用 Kubernetes 的初学者来说，是一个很好的选择。&lt;/p&gt;
&lt;h3 id=&#34;三云托管的解决方案&#34;&gt;三、云托管的解决方案&lt;/h3&gt;
&lt;p&gt;如今各大云供应商都提供了定制化的 Kubernetes 解决方案来。你也可以通过线上教学平台如 &lt;a href=&#34;https://katacoda.com/&#34; title=&#34;Katacoda&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Katacoda&lt;/a&gt;
 上的免费课程来学习 Kubernetes，它们都是云托管的，你不需要自己安装，只不过你需要云供应商的集群需要付费。&lt;/p&gt;
&lt;h2 id=&#34;本地测试和调试-kubernetes&#34;&gt;本地测试和调试 Kubernetes&lt;/h2&gt;
&lt;p&gt;作为本地安装 Kubernetes 的一部分，你很可能还需要一些测试和调试能力，以确保一切都在顺利运行，特别是定义入口和出口策略等棘手的任务。此外，还有 Kubernetes 附加组件的生态系统，你可能想使用这些组件来扩展 Kubernetes 集群的功能。添加所有这些都需要进行更多的测试，以确保它们能与你的 Kubernetes 集群完美的集成。&lt;/p&gt;
&lt;p&gt;用于在本地开发和调试 Kubernetes 服务的工具有：&lt;a href=&#34;https://github.com/microsoft/mindaro&#34; title=&#34;Microsoft Bridge to Kubernetes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Microsoft Bridge to Kubernetes&lt;/a&gt;
 和 &lt;a href=&#34;https://github.com/telepresenceio/telepresence&#34; title=&#34;telepresence&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;telepresence&lt;/a&gt;
。这些工具可以让你在本地运行单个服务，同时将该服务连接到远程 Kubernetes 集群。这样你就可以让自己的本地机器作为 Kubernetes 集群中的一部分来运行——这对于在本地而不是在生产集群上开发服务非常有用。&lt;/p&gt;
&lt;p&gt;Kubernetes 项目也了解到了 Kubernetes 安装对端到端 (E2E) 测试的需求。为此，项目核心团队一直在确保在最近的版本中更恰当地支持 E2E 测试。这包括诸如允许测试重用和纳入更多附加组件和驱动程序的测试等。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-监控工具&#34;&gt;Kubernetes 监控工具&lt;/h2&gt;
&lt;p&gt;Kubernetes 提供了应用程序在集群的每个层次上的资源使用情况的详细信息——容器、pod、服务。这些详细信息使你能够评估应用程序的性能，确定哪些瓶颈可以解决以提高整体性能。&lt;/p&gt;
&lt;p&gt;毕竟，监控可以帮助你了解应用和集群运行情况的详细信息，这对于学习 Kubernetes 是十分有帮助的。&lt;/p&gt;
&lt;p&gt;Kubernetes 包含两个内置度量收集工具用于监控：&lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/&#34; title=&#34;资源管道和全度量管道&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;资源管道和全度量管道&lt;/a&gt;
。资源管道是一个较低级和较有限的工具，主要集中在与各种控制器相关的指标上。全指标管道，顾名思义，从几乎所有集群组件中获取并显示更丰富的指标。&lt;/p&gt;
&lt;p&gt;还有一些第三方工具可以安装并集成到 Kubernetes 集群中。对于 Kubernetes 来说，最普遍使用的两个工具是 Prometheus 和 Grafana。&lt;/p&gt;
&lt;h3 id=&#34;prometheus-监控&#34;&gt;Prometheus 监控&lt;/h3&gt;
&lt;p&gt;Prometheus 是一个功能丰富的开源监控和警报工具。Prometheus 包含一个内部数据存储用来收集指标，如生成的时间序列数据。Prometheus 还拥有众多插件，允许它将数据暴露给各种外部解决方案，并从其他数据源导入数据，包括所有主要公有云监控解决方案。&lt;/p&gt;
&lt;h3 id=&#34;grafana-仪表盘&#34;&gt;Grafana 仪表盘&lt;/h3&gt;
&lt;p&gt;Grafana 是一个优秀的仪表盘、分析和数据可视化工具。它没有 Prometheus 的全功能数据收集能力，但 Prometheus 又没有 Grafana 的数据呈现界面。事实上，他们最好是结合在一起使用——Prometheus 负责数据收集和汇总，Grafana 负责数据展示。它们共同创造了一个强大的组合，涵盖了数据收集、基本警报和可视化。&lt;/p&gt;
&lt;h3 id=&#34;高级警报&#34;&gt;高级警报&lt;/h3&gt;
&lt;p&gt;对于高级警报，你可以添加 &lt;a href=&#34;https://www.nagios.org/&#34; title=&#34;Nagios&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nagios&lt;/a&gt;
 或 &lt;a href=&#34;https://github.com/prometheus/alertmanager&#34; title=&#34;Prometheus Alertmanager&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prometheus Alertmanager&lt;/a&gt;
 等工具。这些警报工具通常有大量的集成。你可以为自定义值班团队，然后定义你想要监控的参数，例如“当任何 pod 不可用时”或“当任何节点无法访问时”、“当容量达到 90%”等，然后通过电子邮件、短信、手机应用提醒、电话呼叫等方式向值班人员发送自定义通知。你还可以创建升级策略，比如，如果一个被定义为“危急”的警报在 10 分钟内没有值班人员确认，那么就将警报升级（发送警报）到该人员的经理。&lt;/p&gt;
&lt;p&gt;现在，你应该已经对 Docker 和 Kubernetes 有了大体的认识。了解了 Kubernetes 的作用，知道它是如何进行容器化应用部署和管理的。&lt;/p&gt;
&lt;p&gt;调试和监控技术不仅仅是运维需要，你也可以把它当作学习方式。有什么比边做边学更好呢？&lt;/p&gt;
&lt;p&gt;请记住，如果你的应用规模太小，而且预计用户需求不会有太大变化或重大波动（比如一个只在公司内部使用的应用），那么 Kubernetes 对你来说可能没有必要，这种情况下，直接使用 Docker 就足够了。&lt;/p&gt;
&lt;h2 id=&#34;更多&#34;&gt;更多&lt;/h2&gt;
&lt;p&gt;云原生领域的开源项目众多（见 &lt;a href=&#34;https://jimmysong.io/awesome-cloud-native&#34; title=&#34;Awesome Cloud Native/云原生开源项目大全&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Awesome Cloud Native/云原生开源项目大全&lt;/a&gt;
），其中有大量的优秀项目可供我们学习。此外，Kubernetes 开源已经多年时间，网上有大量的学习资料，业界出版过很多&lt;a href=&#34;https://jimmysong.io/cloud-native/note/books/&#34; title=&#34;书籍&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;书籍&lt;/a&gt;
，建议大家通过阅读&lt;a href=&#34;https://kubernetes.io&#34; title=&#34;官方文档&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;官方文档&lt;/a&gt;
和实践来学习，也可以参考我编写的&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34; title=&#34;Kubernetes Handbook——Kubernetes 中文指南 / 云原生架构实践手册&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes Handbook——Kubernetes 中文指南 / 云原生架构实践手册&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;推荐大家加入我发起创办的&lt;a href=&#34;https://cloudnative.to&#34; title=&#34;云原生社区&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;云原生社区&lt;/a&gt;
，这是一个立足中国，放眼世界的云原生终端用户社区，致力于云原生技术的传播和应用。云原生社区主办的&lt;a href=&#34;https://github.com/cloudnativeto/academy&#34; title=&#34;云原生学院&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;云原生学院&lt;/a&gt;
定期邀请云原生和开源领域的大咖在 B 站上进行直播分享，成员自发组织了多个 SIG（特别兴趣小组）进行讨论学习。欢迎加入我们，共同学习和交流云原生技术。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Kubernetes 次世代的云原生应用</title>
      <link>https://jimmysong.io/blog/post-kubernetes-era/</link>
      <pubDate>Mon, 01 Jun 2020 18:13:19 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/post-kubernetes-era/</guid>
      <description>
        
        
        &lt;p&gt;Kubernetes 自开源至今已经走过六个年头了，&lt;a href=&#34;https://cloudnative.to/blog/cloud-native-era/&#34; title=&#34;云原生时代&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;云原生时代&lt;/a&gt;
也已到来，我关注云原生领域也四年有余了，最近开始思考云原生的未来走向，特此撰写本文作为&lt;a href=&#34;https://jimmysong.io/guide-to-cloud-native-app&#34; title=&#34;《云原生应用白皮书》&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;《云原生应用白皮书》&lt;/a&gt;
的开篇，更多关于云原生应用的介绍请转到白皮书中浏览。&lt;/p&gt;
&lt;h2 id=&#34;重点&#34;&gt;重点&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;云原生基础设施已渡过了野蛮生长期，正朝着统一应用标准方向迈进。&lt;/li&gt;
&lt;li&gt;Kubernetes 的原语无法完整描述云原生应用体系，且在资源的配置上开发与运维功能耦合严重。&lt;/li&gt;
&lt;li&gt;Operator 在扩展了 Kubernetes 生态的同时导致云原生应用碎片化，亟需一个统一的应用定义标准。&lt;/li&gt;
&lt;li&gt;OAM 的本质是将云原生应用定义中的研发、运维关注点分离，资源对象进行进一步抽象，化繁为简，包罗万象。&lt;/li&gt;
&lt;li&gt;“Kubernetes 次世代”是指在 Kubernetes 成为基础设施层标准之后，云原生生态的关注点正在向应用层过度，近两年来火热的 Service Mesh 正是该过程中的一次有力探索，而基于 Kubernetes 的云原生&lt;strong&gt;应用&lt;/strong&gt;架构的时代即将到来。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Kubernetes 已成为云原生应用的既定运行平台，本文以 Kubernetes 为默认平台展开，包括云原生应用的分层模型。&lt;/p&gt;
&lt;h2 id=&#34;云原生的不同发展阶段&#34;&gt;云原生的不同发展阶段&lt;/h2&gt;
&lt;p&gt;Kubernetes 从开源至今已经走过快&lt;a href=&#34;https://jimmysong.io/cloud-native/memo/open-source/&#34; title=&#34;六个年头&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;六个年头&lt;/a&gt;
（2014 年 6 月开源）了，可以说是 Kubernetes 的诞生开启了整个云原生的时代。我粗略的将云原生的发展划分为以下几个时期。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/post-kubernetes-era/cloud-native-stages.png&#34; data-img=&#34;/blog/post-kubernetes-era/cloud-native-stages.png&#34; data-width=&#34;2096&#34; data-height=&#34;508&#34; alt=&#34;image&#34; data-caption=&#34;云原生的发展阶段&#34;&gt;
    
  
  &lt;figcaption&gt;云原生的发展阶段&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一阶段：孵化期（2014 年）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2014 年，Google 开源 Kubernetes，在此之前的 2013 年，Docker 开源，DevOps、微服务已变得十分流行，云原生的概念已经初出茅庐。在开源了 Kubernetes 之后，Google 联合其他厂商发起成立了 CNCF，并将 Kubernetes 作为初创项目捐献给了 CNCF。CNCF 作为云原生的背后推手，开始推广 Kubernetes。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第二阶段：高速发展期（2015 年 - 2016 年）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这几年间，Kubernetes 保持着高速发展，并于 2017 年打败了 Docker Swarm、Mesos，确立了容器编排工具领导者的地位。CRD 和 Operator 模式的诞生，大大增强了 Kubernetes 的扩展性，促进了周边生态的繁荣。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第三阶段：野蛮生长期（2017 年 - 2018 年）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2016 年之后的云原生基本都默认运行在 Kubernetes 平台上，2017、2018 年 Google 主导的 Istio、Knative 相继开源，这些开源项目都大量利用了 Kubernetes 的 Operator 进行了扩展，Istio 刚发布时就有 50 多个 CRD 定义。Istio 号称是&lt;a href=&#34;https://jimmysong.io/blog/service-mesh-the-microservices-in-post-kubernetes-era/&#34; title=&#34;后 Kubernetes 时代的微服务&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;后 Kubernetes 时代的微服务&lt;/a&gt;
，它的出现第一次使得云原生以服务（应用）为中心。Knative 是 Google 在基于 Kubernetes 之上开源的 Serverless 领域的一次尝试。2018 年 Kubernetes 正式从 CNCF &lt;a href=&#34;https://www.cncf.io/blog/2018/03/06/kubernetes-first-cncf-project-graduate/&#34; title=&#34;毕业&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;毕业&lt;/a&gt;
，Prometheus、Envoy 也陆续从 CNCF 毕业。CNCF 也与 2018 年修改了 charter，对云原生进行了重定义，从原来的三要素：”应用容器化；面向微服务架构；应用支持容器的编排调度“，修改为”云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API“。这一年，我曾写过两篇 Kubernetes 及云原生发展的年终总结和展望，见 &lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/appendix/kubernetes-and-cloud-native-summary-in-2017-and-outlook-for-2018.html&#34; title=&#34;2017 年&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2017 年&lt;/a&gt;
和 &lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/appendix/kubernetes-and-cloud-native-summary-in-2018-and-outlook-for-2019.html&#34; title=&#34;2018 年&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018 年&lt;/a&gt;
的预测和总结。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第四阶段：普及推广期（2019 年至今）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;经过几年的发展，Kubernetes 已经得到的大规模的应用，云原生的概念开始深入人心，Kubernetes 号称是云原生的操作系统，基于 Operator 模式的生态大放异彩。整合 Kubernetes 和云基础设施，研发和运维关注点分离。Kubernetes 到 Service Mesh（后 Kubernetes 时代的微服务），基于 Kubernetes 的 Serverless 都在快速发展，OAM 诞生，旨在定义云原生应用标准。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-开辟了云原生时代&#34;&gt;Kubernetes 开辟了云原生时代&lt;/h2&gt;
&lt;p&gt;Kubernetes 开源之初就继承了 Google 内部调度系统 Borg 的经验，屏蔽掉了底层物理机、虚拟机之间的差异，经过几年时间的发展成为了容器编排标准，进而统一了 PaaS 平台的基础设施层。&lt;/p&gt;
&lt;p&gt;下图是Kubernetes 原生内置的可以应用到一个 Pod 上的所有控制器、资源对象等。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/post-kubernetes-era/kubernetes-concepts.png&#34; data-img=&#34;/blog/post-kubernetes-era/kubernetes-concepts.png&#34; data-width=&#34;800&#34; data-height=&#34;596&#34; alt=&#34;image&#34; data-caption=&#34;Kubernetes 概念&#34;&gt;
    
  
  &lt;figcaption&gt;Kubernetes 概念&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;图片来自图书 &lt;a href=&#34;https://www.redhat.com/cms/managed-files/cm-oreilly-kubernetes-patterns-ebook-f19824-201910-en.pdf&#34; title=&#34;Kubernetes Patterns（O’Reilly）&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes Patterns（O’Reilly）&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Kubernetes 作为云原生基础设施设计之初遵循了以下原则：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基础设施即代码（声明式 API）&lt;/li&gt;
&lt;li&gt;不可变基础设施&lt;/li&gt;
&lt;li&gt;幂等性&lt;/li&gt;
&lt;li&gt;调节器模式（Operator 的原理）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中声明式 API 可谓开创了云原生时代的基调，而调节器模式是 Kubernetes 区别于其他&lt;a href=&#34;https://jimmysong.io/cloud-native-infra/evolution-of-cloud-native-developments.html&#34; title=&#34;云部署形式&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;云部署形式&lt;/a&gt;
的主要区别之一，这也为后来的 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/54633203&#34; title=&#34;Operator 框架的诞生&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Operator 框架的诞生&lt;/a&gt;
打下了基础。&lt;/p&gt;
&lt;h3 id=&#34;声明式-api&#34;&gt;声明式 API&lt;/h3&gt;
&lt;p&gt;根据声明式 API 可以做应用编排，定义组件间的依赖，通常使用人类易读的 YAML 文件来表示。但是，YAML 文件声明的字段真的就是最终的状态吗？有没有可能动态改变？&lt;/p&gt;
&lt;p&gt;我们在创建 &lt;code&gt;Deployment&lt;/code&gt; 时会指定 Pod 的副本数，但是其实际副本数并不一定是一成不变的。假如集群中还有定义 HPA，那么 Pod 的副本数就可能随着一些外界因素（比如内存、CPU 使用率或者自定义 metric）而改变，而且如果集群中还有运行自定义的控制器话，那么也有可能修改应用的实例数量。在有多个控制器同时控制某个资源对象时，如何确保控制器之间不会发生冲突，资源对象的状态可预期？可以使用&lt;a href=&#34;https://kubernetes.io/zh/docs/reference/access-authn-authz/extensible-admission-controllers/#monitoring-admission-webhooks&#34; title=&#34;动态准入控制&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;动态准入控制&lt;/a&gt;
来达到这一点。&lt;/p&gt;
&lt;h3 id=&#34;kubernetes-原生应用&#34;&gt;Kubernetes 原生应用&lt;/h3&gt;
&lt;p&gt;我们都知道要想运行一个应用至少需要以下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用的业务逻辑（代码）、运行时（可运行的二进制文件、字节码或脚本）。&lt;/li&gt;
&lt;li&gt;应用的配置注入（配置文件、环境变量等），身份、路由、服务暴露等满足应用的安全性和可访问性。&lt;/li&gt;
&lt;li&gt;应用的生命周期管理（各种 Controller 登场）。&lt;/li&gt;
&lt;li&gt;可观察性、可运维、网络和资源及环境依赖、隔离性等。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图展示了基于 Kubernetes 原语及 PaaS 平台资源的 Kubernetes 原生应用的组成。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/post-kubernetes-era/kubernetes-native-application-motion.gif&#34; data-img=&#34;/blog/post-kubernetes-era/kubernetes-native-application-motion.gif&#34; data-width=&#34;600&#34; data-height=&#34;334&#34; alt=&#34;image&#34; data-caption=&#34;Kubernetes 原生应用&#34;&gt;
    
  
  &lt;figcaption&gt;Kubernetes 原生应用&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;我们都知道 Kubernetes 提供了大量的&lt;a href=&#34;https://kubernetes.io/docs/concepts/&#34; title=&#34;原语&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;原语&lt;/a&gt;
，用户可以基于这些原语来编排服务，管理应用的生命周期。上图展示的是基于 Kubernetes 原生应用可以使用的 Kubernetes 原语、扩展及平台层资源，从内向外的对象跟应用程序（业务逻辑）的关联度依次降低，到最外层基本只剩下平台资源依赖，已经与 Kubernetes 几乎没有关系了。该图里仅展示了部分资源和对象（包含阿里巴巴开源的 &lt;a href=&#34;https://github.com/openkruise/kruise&#34; title=&#34;OpenKruise&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenKruise&lt;/a&gt;
、Istio），实际上 &lt;a href=&#34;https://operatorhub.io/&#34; title=&#34;Operator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Operator&lt;/a&gt;
 资源之丰富，也是 Kubernetes 生态如此繁荣的原因之一。&lt;/p&gt;
&lt;p&gt;Kubernetes 本身的原语、资源对象、配置、常用的 CRD 扩展有几十、上百个之多。开发者需要了解这些复杂的概念吗？我只是想部署一个应用而已！不用所对于应用开发者，即使对于基础实施开发和运维人员也需要很陡峭的学习曲线才能完全掌握它。&lt;/p&gt;
&lt;p&gt;我将 Kubernetes 原生应用所需要的定义和资源进行了分层：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心层&lt;/strong&gt;：应用逻辑、服务定义、生命周期控制；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隔离与服务访问层&lt;/strong&gt;：资源限制与隔离、配置、身份、路由规则等；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;调度层&lt;/strong&gt;：各种调度控制器，这也是 Kubernetes 原生应用的主要扩展层；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;资源层&lt;/strong&gt;：提供网络、存储和其他平台资源；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而这些不同的层，完全可以将其职责分配给相应的人员，比如核心层是由应用程序开发者负责，将其职责分离，可以很大程度上降低开发和运维的复杂度。&lt;/p&gt;
&lt;p&gt;云原生应用落实到 Kubernetes 平台之上，仅仅利用 Kubernetes 的对象原语已很难描述一个复杂的应用程序，所以诞生了各种各样的 Operator，但这也仅仅解决了单个应用的定义，对于应用的打包封装则无能为力。&lt;/p&gt;
&lt;p&gt;同一个资源对象又有多种实现方式，比如 Ingress 就有 &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1DnsHtdHbxjvHmxvlu7VhzWcWgLAn_Mc5L1WlhLDA__k/edit#gid=0&#34; title=&#34;10 多种实现&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10 多种实现&lt;/a&gt;
，PV 就更不用说，对于对于开发者究竟如何选择，平台如何管理，这都是让人很头疼的问题。而且有时候平台所提供的扩展能力还可能会有冲突，这些能力有的可能互不相干，有的可能会有正交，有的可能完全重合。且应用本身与运维特性之间存在太多耦合，不便于复用。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/post-kubernetes-era/resources-motion.gif&#34; data-img=&#34;/blog/post-kubernetes-era/resources-motion.gif&#34; data-width=&#34;600&#34; data-height=&#34;363&#34; alt=&#34;image&#34; data-caption=&#34;资源交集动画&#34;&gt;
    
  
  &lt;figcaption&gt;资源交集动画&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;上图中不同颜色的方框代表不同的资源类别，红线框代表不能为一个资源同时应用该配置，否则会出现冲突，不同的颜色上面是一个动画，展示的是部分资源组合。图中仅包含了部分 Kubernetes 中的原语和 Istio 中的资源对象组合及自定义扩展，实际上用户可以根据应用的自身特点，基于 Kubernetes 原语和 CRD 创建出千变万化的组合。&lt;/p&gt;
&lt;p&gt;为了管理这些应用诞生出了众多的 &lt;a href=&#34;https://github.com/operator-framework/awesome-operators&#34; title=&#34;Operator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Operator&lt;/a&gt;
。Kubernetes 1.7 版本以来就引入了&lt;a href=&#34;https://kubernetes.io/docs/concepts/api-extension/custom-resources/&#34; title=&#34;自定义控制器&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;自定义控制器&lt;/a&gt;
的概念，该功能可以让开发人员扩展添加新功能，更新现有的功能，并且可以自动执行一些管理任务，这些自定义的控制器就像 Kubernetes 原生的组件一样，Operator 直接使用 Kubernetes API进行开发，也就是说它们可以根据这些控制器内部编写的自定义规则来监控集群、更改 Pods/Services、对正在运行的应用进行扩缩容。&lt;/p&gt;
&lt;p&gt;Operator 的本质是一种调节器模式（Reconciler Pattern）的应用，跟 Kubernetes 本身的实现模式是一样的，用于管理云原生应用，协调应用的实际状态达到预期状态。&lt;/p&gt;
&lt;p&gt;调节器模式的四个原则：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;所有的输入和输出都使用数据结构。&lt;/li&gt;
&lt;li&gt;确保数据结构是不可变的。&lt;/li&gt;
&lt;li&gt;保持资源映射简单。&lt;/li&gt;
&lt;li&gt;使实际状态符合预期状态。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;云原生应用走向碎片化&#34;&gt;云原生应用走向碎片化&lt;/h2&gt;
&lt;p&gt;利用声明式 API 及调节器模式，理论上可以在 Kubernetes 上部署任何可声明应用，但是在 Operator 出现之前，管理 Kubernetes 上的有状态应用一直是一个难题，随着 Operator 模式的确立，该难题已得以解决，并促进了 Kubernetes 生态的进一步发展。随着该生态的繁荣，有一种碎片化的特征正在显现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;云原生应用碎片化的体现&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Operator 模式将运维人员的反应式经验转化成基于 &lt;code&gt;Reconcile&lt;/code&gt; 模式的代码，统一了有状态应用的管理模式，极大得扩展了 Kubernetes 应用生态。&lt;/li&gt;
&lt;li&gt;开发者在引用 Operator 所提供的能力时没有统一的视图，加大了基础设施运维与开发者之间的沟通成本。&lt;/li&gt;
&lt;li&gt;Operator 总体上治理松散，没有统一的管控机制，在同时应用时可能导致互相冲突或无法预期的结果发生。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;有状态应用管理难题&#34;&gt;有状态应用管理难题&lt;/h3&gt;
&lt;p&gt;Kubernetes 对于无状态应用的管理很出色，但是对于有状态应用就不是那么回事了。虽然 StatefulSet 可以帮助管理有状态应用，但是这还远远不够，有状态应用往往有复杂的依赖。声明式的 API 里往往要加载着大量的配置和启动脚本，才能实现一个复杂应用的 Kubernetes 化。&lt;/p&gt;
&lt;p&gt;例如在 2017 年初，Operator Framework 出现之前，需要使用大量的 &lt;code&gt;ConfigMap&lt;/code&gt;、复杂的启动脚本才能&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/guide/migrating-hadoop-yarn-to-kubernetes.html&#34; title=&#34;在 Kubernetes 上定义 Hadoop YARN&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;在 Kubernetes 上定义 Hadoop YARN&lt;/a&gt;
 和&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/usecases/running-spark-with-kubernetes-native-scheduler.html&#34; title=&#34;运行 Spark&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;运行 Spark&lt;/a&gt;
。虽然 &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&#34; title=&#34;&amp;lt;code&amp;gt;StatefulSet&amp;lt;/code&amp;gt;&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;StatefulSet&lt;/code&gt;&lt;/a&gt;
 号称可以解决有状态应用的部署问题，但是它主要是保证了 Pod 的在启动、伸缩时的顺序和使 Pod 具有稳定的标识。但是很多分布式应用来说并不仅依靠启动顺序就可以保证其状态，根据其在分布式应用中的角色不同（master/worker）而需要有大量的自定义配置，在没有 Operator 之前这些配置通常是通过一些自定义脚本来实现，这些脚本可能存在于应用镜像中，也可以通过 &lt;code&gt;ConfigMap&lt;/code&gt; 挂在到容器运行时，但无论如何这些脚本都可能因为散落在各处，这些脚本还是面向过程的，跟在 Kubernetes 诞生之前的运维方式毫无二致，这极其不便于版本控制和运维管理。&lt;/p&gt;
&lt;h3 id=&#34;operator-统一了-kubernetes-应用运维框架&#34;&gt;Operator 统一了 Kubernetes 应用运维框架&lt;/h3&gt;
&lt;p&gt;Operator 大大增强了 Kubernetes 的可扩展性，丰富了以 Kubernetes 为基础的云原生生态，许多原先不是为 Kubernetes 而构建的应用纷纷通过&lt;a href=&#34;https://zhuanlan.zhihu.com/p/54633203&#34; title=&#34;构建自己的 Operator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;构建自己的 Operator&lt;/a&gt;
 迁移到 Kubernetes 上。还有一些直接基于 Kubernetes 构建的 Service Mesh、Serverless 框架，它们应用 Operator 模式（如 &lt;a href=&#34;https://istio.io&#34; title=&#34;Istio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio&lt;/a&gt;
、&lt;a href=&#34;https://knative.dev&#34; title=&#34;Knative&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Knative&lt;/a&gt;
），试图成为云原生应用的基础设施层，补齐 Kubernetes 在服务治理、无服务架构等方面的短板，随着大量的 CRD、Operator 控制器的出现，而 Kubernetes 却无法以应用的视角来管理这些能力及其背后零散的 CRD，这使得云原生应用碎片化。&lt;/p&gt;
&lt;p&gt;Operator 百花齐放，在没有一个大一统的视图之前，各个控制器之间存在着这样的关系：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;独立&lt;/strong&gt;：互不干涉，比如 Controller 与服务发现之间就不存在冲突。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可组合&lt;/strong&gt;：例如 &lt;code&gt;Service&lt;/code&gt;、&lt;code&gt;VirtualService&lt;/code&gt;、&lt;code&gt;DestinationRule&lt;/code&gt; 同属一类资源（可访问性与路由），就是可组合的（后两者是 Istio 中的 CRD，用于流量管理）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;有冲突&lt;/strong&gt;：例如图中的 &lt;code&gt;CronHorizontalPodAutoscaler&lt;/code&gt;（CRD）、&lt;code&gt;HorizontalPodAutoscaler&lt;/code&gt;（Kubernetes 内置），同时使用可能导致无法意料的情况发生。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;正是以为这样复杂的关系，导致其无法做到开箱即用，还需要基础设施团队基于云原生社区和生态自己构建出来的，比如&lt;a href=&#34;https://jimmysong.io/awesome-cloud-native/#application-delivery&#34; title=&#34;应用交付领域&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;应用交付领域&lt;/a&gt;
的系列开源项目。&lt;/p&gt;
&lt;h2 id=&#34;云原生应用管理工具-helm&#34;&gt;云原生应用管理工具 Helm&lt;/h2&gt;
&lt;p&gt;Kubernetes 之上有很多能力缺失，比如应用构建、发布、管理和运维等，Helm 的出现主要补偿了应用打包和版本管理的缺陷。其中云原生应用的配置包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用程序启动时加载的配置文件；&lt;/li&gt;
&lt;li&gt;应用程序的运维配置，如资源申请限额；&lt;/li&gt;
&lt;li&gt;应用程序的服务发现配置；&lt;/li&gt;
&lt;li&gt;应用程序的工作负载、发布策略、依赖等；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些配置可以存在于 &lt;code&gt;ConfigMap&lt;/code&gt;、&lt;code&gt;Deployment&lt;/code&gt;、&lt;code&gt;Service&lt;/code&gt;、&lt;code&gt;Ingress&lt;/code&gt; 等 Kubernetes 的多个资源文件中，如何保证应用程序的复用性？应用程序之间有依赖该如何解决？这是时候你可能自然的想到了 Helm。&lt;/p&gt;
&lt;p&gt;云原生应用打包和发布管理&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Helm 通过 chart 模板，提高了应用程序的复用性并解决了部分依赖问题；&lt;/li&gt;
&lt;li&gt;Chart 仓库提供了云原生应用程序的统一管控视图；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Release&lt;/code&gt; 概念的引入，使得云原生应用版本化管理进一步加强；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Helm 主要关注的是 &lt;a href=&#34;https://12factor.net/zh_cn/&#34; title=&#34;12 因素应用&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;12 因素应用&lt;/a&gt;
法则&lt;a href=&#34;https://12factor.net/zh_cn/build-release-run&#34; title=&#34;构建、发布、运行&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;构建、发布、运行&lt;/a&gt;
这一原则中的”发布”这一环节。下图是 Helm v3 的架构图。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/post-kubernetes-era/helm-chart.png&#34; data-img=&#34;/blog/post-kubernetes-era/helm-chart.png&#34; data-width=&#34;600&#34; data-height=&#34;429&#34; alt=&#34;image&#34; data-caption=&#34;Helm3 架构&#34;&gt;
    
  
  &lt;figcaption&gt;Helm3 架构&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Helm 可以安装本地或者远程的 chart，当 chart 安装到 Kubernetes 中后就会创建一个 release，每次更新该 chart 的配置并执行 &lt;code&gt;helm upgrade&lt;/code&gt;， release 的版本数就会加 1，开发者可以升级 chart 或回滚到历史版本。&lt;/p&gt;
&lt;h3 id=&#34;打包配置和发布&#34;&gt;打包、配置和发布&lt;/h3&gt;
&lt;p&gt;Helm 和 chart 的主要作用是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用程序封装&lt;/li&gt;
&lt;li&gt;版本管理&lt;/li&gt;
&lt;li&gt;依赖检查&lt;/li&gt;
&lt;li&gt;便于应用程序分发&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;打包&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Helm 采用 &lt;a href=&#34;https://helm.sh/docs/topics/charts/&#34; title=&#34;Chart&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chart&lt;/a&gt;
 的格式来标准化描述应用，可以将目录打包成版本化的压缩包进行部署理论上一个 Chart 是可以嵌套若干个 Chart 并定义依赖关系，组织形式非常灵活。Helm chart 用于打包 Kubernetes 原生应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;配置&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;应用配置参数，在 Chart 中由 &lt;code&gt;values.yaml&lt;/code&gt; 和命令行参数组成。Chart 采用 Go Template 的特性和 &lt;code&gt;values.yaml&lt;/code&gt; 对部署的模板文件进行参数渲染，也可以通过 &lt;code&gt;helm&lt;/code&gt; 命令 &lt;code&gt;--set key=value&lt;/code&gt; 的方式进行参数赋值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;发布&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Release 代表 Chart 在集群中的运行实例，Helm 围绕 Release 对应用提供了强大的生命周期管理能力，包括 Release 的查询、安装、更新、删除、回滚等。&lt;/p&gt;
&lt;h2 id=&#34;云原生应用&#34;&gt;云原生应用&lt;/h2&gt;
&lt;p&gt;以上关注的点都是基于 Kubernetes 原语的实现，虽然基于 Kubernetes 构建的 PaaS 平台部分屏蔽了底层基础设施的差异，但是仍有很多云服务是无法通过 Kubernetes 创建，或者需要提前创建供 Kubernetes 原生应用使用的，这些应用通常不运行在 Kubernetes 集群中。因此创建和管理一个云原生应用程序需要考虑以下方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;运行时：ECS、Docker、KataContainer、gVisor 等；&lt;/li&gt;
&lt;li&gt;资源隔离性：多租户、VPC、Namespace、防火墙；&lt;/li&gt;
&lt;li&gt;资源调度：各种类型的 controller；&lt;/li&gt;
&lt;li&gt;网络可达性：Service、Ingress、Egress、Gateway、VirtualService、DestinationRule、LoadBalancer、ServiceEntry 等；&lt;/li&gt;
&lt;li&gt;可观测性：日志、分布式追踪、指标；&lt;/li&gt;
&lt;li&gt;安全性：SecurityPolicy、NetworkPolicy、AuthorizationPolicy；&lt;/li&gt;
&lt;li&gt;平台资源申请：数据库、存储等；&lt;/li&gt;
&lt;li&gt;运行与隔离：ECS、Docker、KataContainer 等；&lt;/li&gt;
&lt;li&gt;资源分配和调度：各种控制器；&lt;/li&gt;
&lt;li&gt;环境隔离：Namespace、多租户、VPC、防火墙、LimitRange、Resources；&lt;/li&gt;
&lt;li&gt;可访问性：Service、Ingress、Egress、Gateway、LoadBalancer、VirtualService、DestinationRule、ServiceEntry；&lt;/li&gt;
&lt;li&gt;状态管理：Operator；&lt;/li&gt;
&lt;li&gt;可观察性：日志、监控、指标；&lt;/li&gt;
&lt;li&gt;安全性：SecurityPolicy、ServiceAccount；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;云原生应用分层模型&#34;&gt;云原生应用分层模型&lt;/h3&gt;
&lt;p&gt;那么究竟如何来给云原生应用分层，化繁就简？近几年来，基于 Kubernetes 的应用呈爆炸式发展，光是在&lt;a href=&#34;https://jimmysong.io/awesome-cloud-native/#application-delivery&#34; title=&#34;应用交付领域&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;应用交付领域&lt;/a&gt;
的开源项目就达几十个之多。下图展示我根据这些项目的特性而绘制的 App Delivery Landscape。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/post-kubernetes-era/cloud-native-app.png&#34; data-img=&#34;/blog/post-kubernetes-era/cloud-native-app.png&#34; data-width=&#34;1054&#34; data-height=&#34;514&#34; alt=&#34;image&#34; data-caption=&#34;云原生应用的分层模型&#34;&gt;
    
  
  &lt;figcaption&gt;云原生应用的分层模型&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;应用定义和包装&lt;/strong&gt;：云原生应用的最上层，直接定义云原生应用的组成形式，解决云原生应用之间的依赖关系，并封装成发布包，如 Helm、CNAB，还有云原生变成语言 Pulumi 和 Ballerina，基于 API 的方式来编排云原生应用；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;负载定义&lt;/strong&gt;：基于 Kubernetes Operator，大多是 Serverless 负载，既负责了负载的定义又负责了生命周期管理。&lt;a href=&#34;https://istio.io&#34; title=&#34;Istio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio&lt;/a&gt;
 是比较特殊的存在，它不仅管理服务间的流量，还负责安全性、可观察性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;应用发布和上线&lt;/strong&gt;：关注应用的构建和发布、GitOps、发布策略等，这也是云原生应用全景中最丰富的部分之一；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kubernetes 原语&lt;/strong&gt;：Kubernetes 本身提供的原语，Operator 基于此构建；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上为我个人分类的云原生应用全景模型，仅限于 Kubernetes 之上的应用，对于其他非 Kubernetes 应用非本文的考虑范围。另外，CNCF SIG App Delivery 中也给出的云原生应用的分层模型，其模型将非 Kubernetes 应用场景也纳入了考虑，详见：&lt;a href=&#34;https://docs.google.com/document/d/1gMhRz4vEwiHa3uD8DqFKHGTSxrVJNgkLG2WZWvi9lXo/edit#heading=h.h9so53gv5zen&#34; title=&#34;The Dictionary of Cloud-Native App Delivery&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Dictionary of Cloud-Native App Delivery&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;Platform/Kuberntes，Kubernetes 仅仅是屏蔽了平台的一些差异，但是对于最上层的应用来说，没有涉及，用户需要自己来基于各种开源组件来搭积木。&lt;/p&gt;
&lt;h3 id=&#34;oam开放应用模型&#34;&gt;OAM（开放应用模型）&lt;/h3&gt;
&lt;p&gt;那么以上这么多应用有哪些共性，能不能再进一步抽象呢？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有应用是都以容器作为运行时环境（ContainerizedWorkload），这是 OAM 中的核心 Workload 类型；&lt;/li&gt;
&lt;li&gt;在应用发布和上线方面，有些是属于应用的运维特征，需要根据实际需求组合和变更，这些是持续变动的部分；&lt;/li&gt;
&lt;li&gt;要实现某些复杂的应用管控，需要使用到多个 CRD 的组合，比如 Istio 中的让流量根据百分比切分到不同的而服务，就需要部署 Istio Operator，并声明 &lt;code&gt;VirtualService&lt;/code&gt;、&lt;code&gt;DestinationRule&lt;/code&gt;，二者同时使用；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个 &lt;code&gt;ApplicationConfiguration&lt;/code&gt; 的 Runtime 的正常流程应该是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用开发者创建自己的 &lt;code&gt;Component&lt;/code&gt;，在 &lt;code&gt;Component&lt;/code&gt; 中描述要应用相关的信息，如应用名称、镜像配置、环境变量等，应用到 Kubernetes cluster 中；&lt;/li&gt;
&lt;li&gt;运维创建各种运维策略，如发布策略、网络策略等等，发布时由 AppConfig 对象关联要发布的 &lt;code&gt;Component&lt;/code&gt; 和本次的运维策略，apply 到集群中，集群的 OAM operator watch 到一次 &lt;code&gt;ApplicationConfiguration&lt;/code&gt;的下发，生成 &lt;code&gt;Component&lt;/code&gt; 对应的 &lt;code&gt;Workload&lt;/code&gt; 和 &lt;code&gt;Trait&lt;/code&gt;，&lt;code&gt;Trait&lt;/code&gt; controller 将本次的 &lt;code&gt;Trait&lt;/code&gt; 策略应用到本次要管理的 &lt;code&gt;Workload&lt;/code&gt; 当中，最终到达终态，完成一次发布。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;OAM 是对 Kubernetes 友好的，一样采用声明式 API 的理念开发。如果你已经编写了现成的 CRD Operator，可以平滑的接入到 OAM 体系中。OAM 以应用为中心，高度可扩展，扩展点包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Workload：扩展各种运行时类型，不仅限于容器运行时，还可以定义更多其他运行时，比如 Serverless 负载、虚拟机、数据库、网络等；例如，Pod、无服务器函数、数据存储、消息队列或任何其他类型的工作负载，这些都是应用程序开发人员需要设计一个完整的应用程序所需要的，可以直接引用 Kubernetes 的 CRD；&lt;/li&gt;
&lt;li&gt;Trait：各种运维规则，比如扩缩容、流量控制、安全性；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;生态&#34;&gt;生态&lt;/h3&gt;
&lt;p&gt;以前 CNCF 的主要关注群体大多是基础设施领域的技术人员，但是自 2019 年 9 月，&lt;a href=&#34;https://www.infoq.cn/article/Cdw7ISlEqKilGyN9V3Pj&#34; title=&#34;CNCF 宣布成立 SIG App Delivery&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CNCF 宣布成立 SIG App Delivery&lt;/a&gt;
 后，CNCF 正在将应用开发者和运维人员更紧密的联系在一起。&lt;a href=&#34;https://github.com/cncf/sig-app-delivery&#34; title=&#34;应用交付 SIG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;应用交付 SIG&lt;/a&gt;
 的使命是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在与开发、分发、部署、管理和运行安全的云原生应用相关的领域进行合作，目标是以云原生方式交付应用。&lt;/li&gt;
&lt;li&gt;发展信息资源，包括指南、教程和白皮书，让社区了解最佳实践和应用交付的价值。&lt;/li&gt;
&lt;li&gt;识别合适的项目和现状的差距，定期向 TOC 更新，并以结构化的方式向 TOC 提出行动建议。这包括帮助 TOC 评估和对潜在的新项目进行尽职调查。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前 OAM 定义的云原生应用模型已有以下项目支持。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://crossplane.io/&#34; title=&#34;Crossplane&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Crossplane&lt;/a&gt;
：这是一个开源的 Kubernetes 扩展组件，适用于主流公有云平台，使用 &lt;code&gt;kubectl&lt;/code&gt; 配置和管理基础架构、服务和应用。对于 OAM 的支持详见&lt;a href=&#34;https://crossplane.io/docs/v0.11/getting-started/run-applications.html&#34; title=&#34;运行应用程序&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;运行应用程序&lt;/a&gt;
。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://googlecontainertools.github.io/kpt/&#34; title=&#34;KPT&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KPT&lt;/a&gt;
：Kpt（发音为 &amp;ldquo;keep&amp;rdquo;）是一个在资源配置之上构建声明性工作流的开源工具。它的 git + YAML 架构意味着它只需与现有的工具、框架和平台一起工作。Kpt 包括了获取、显示、自定义、更新、验证和应用 Kubernetes 配置的解决方案。对 OAM 的支持详见 &lt;a href=&#34;https://googlecontainertools.github.io/kpt/guides/ecosystem/oam/&#34; title=&#34;使用 kpt 来管理由开放应用模型（OAM）定义的自定义 Kubernetes 应用程序&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;使用 kpt 来管理由开放应用模型（OAM）定义的自定义 Kubernetes 应用程序&lt;/a&gt;
。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;应用交付领域相关的开源项目还有很多，详见 &lt;a href=&#34;https://jimmysong.io/awesome-cloud-native/#application-delivery&#34; title=&#34;Awesome Cloud Native&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Awesome Cloud Native&lt;/a&gt;
。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;基于 Kubernetes 的云原生生态发展至今已有 6 年时间，当前已步入了普及推广阶段。可以说谁云原生应用定义的制高点，就可以掌握云原生的未来。从前我们是新技术浪潮的追随者，现在我们抓住时代的基于，参与标准制定、引领云原生的浪潮！欢迎加入 &lt;a href=&#34;https://oam.dev/&#34; title=&#34;OAM 社区&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OAM 社区&lt;/a&gt;
，一起参与进来，把国人参与指定的标准推向世界。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.ibm.com/technologies/containers/blogs/kubernetes-helm-3/&#34; title=&#34;Do you know what’s in Helm 3? - developer.ibm.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do you know what’s in Helm 3? - developer.ibm.com&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.redhat.com/cms/managed-files/cm-oreilly-kubernetes-patterns-ebook-f19824-201910-en.pdf&#34; title=&#34;O’Reilly: Kubernetes patterns for designing cloud-native apps - redhat.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;O’Reilly: Kubernetes patterns for designing cloud-native apps - redhat.com&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/document/d/1gMhRz4vEwiHa3uD8DqFKHGTSxrVJNgkLG2WZWvi9lXo/edit#heading=h.h9so53gv5zen&#34; title=&#34;The Dictionary of Cloud-Native App Delivery - docs.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Dictionary of Cloud-Native App Delivery - docs.google.com&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.infoq.cn/article/Cdw7ISlEqKilGyN9V3Pj&#34; title=&#34;CNCF 宣布成立应用交付领域小组，正式开启云原生应用时代 - infoq.cn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CNCF 宣布成立应用交付领域小组，正式开启云原生应用时代 - infoq.cn&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/c7A8lOdAKkW25GoqmwOgWg&#34; title=&#34;OAM v1alpha2 新版发布：平衡标准与可扩展性，灵活接入 CRD operator - mp.weixin.qq.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OAM v1alpha2 新版发布：平衡标准与可扩展性，灵活接入 CRD operator - mp.weixin.qq.com&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/54633203&#34; title=&#34;Kubernetes API 与 Operator，不为人知的开发者战争 - zhuanlan.zhihu.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes API 与 Operator，不为人知的开发者战争 - zhuanlan.zhihu.com&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/blog/cloud-native-era/&#34; title=&#34;云原生时代——投资人视角下的云原生趋势思考 - cloudnative.to&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;云原生时代——投资人视角下的云原生趋势思考 - cloudnative.to&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>CentOS7 官方 Docker 发行版现重大 Bug</title>
      <link>https://jimmysong.io/blog/docker-exec-bug-on-centos7/</link>
      <pubDate>Thu, 06 Dec 2018 21:08:02 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/docker-exec-bug-on-centos7/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/openshift/origin/issues/21590&#34; title=&#34;Cannot ssh into a running pod/container &amp;amp;ndash; rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused &amp;amp;ldquo;process_linux.go:110: decoding init error from pipe caused &amp;amp;quot;read parent: connection reset by peer&amp;amp;quot;&amp;amp;rdquo; command terminated with exit code 126 #21590&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cannot ssh into a running pod/container &amp;ndash; rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused &amp;ldquo;process_linux.go:110: decoding init error from pipe caused &amp;quot;read parent: connection reset by peer&amp;quot;&amp;rdquo; command terminated with exit code 126 #21590&lt;/a&gt;
&lt;/p&gt;
&lt;h2 id=&#34;bug-影响&#34;&gt;Bug 影响&lt;/h2&gt;
&lt;p&gt;如果你使用的是 CentOS7，需要用到 &lt;code&gt;kubectl exec&lt;/code&gt; 或者为 Pod 配置了&lt;strong&gt;基于命令返回值&lt;/strong&gt;的健康检查（非常用的 HTTP Get 方式）的话，该 Bug 将导致命令返回错误，Pod 无法正常启动，引起大规模故障，而且也无法使用 &lt;code&gt;kubectl exec&lt;/code&gt; 或者 &lt;code&gt;docker exec&lt;/code&gt; 与容器交互。&lt;/p&gt;
&lt;p&gt;例如下面的健康检查配置：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;   &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;livenessProbe&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;     &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;exec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;       &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;command&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;       &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;/usr/local/bin/sidecar-injector&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;       &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;probe&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;       &lt;/span&gt;- --&lt;span class=&#34;l&#34;&gt;probe-path=/health&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;       &lt;/span&gt;- --&lt;span class=&#34;l&#34;&gt;interval=4s&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;     &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;failureThreshold&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;     &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;initialDelaySeconds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;     &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;periodSeconds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;     &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;successThreshold&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;     &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;timeoutSeconds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;   &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;readinessProbe&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;     &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;exec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;       &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;command&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;       &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;/usr/local/bin/sidecar-injector&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;       &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;probe&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;       &lt;/span&gt;- --&lt;span class=&#34;l&#34;&gt;probe-path=/health&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;       &lt;/span&gt;- --&lt;span class=&#34;l&#34;&gt;interval=4s&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;     &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;failureThreshold&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;     &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;initialDelaySeconds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;     &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;periodSeconds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;     &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;successThreshold&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;     &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;timeoutSeconds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;以上 YAML 配置摘自 &lt;a href=&#34;https://istio.io/zh&#34; title=&#34;Istio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio&lt;/a&gt;
 发行版中的 &lt;code&gt;istio-demo.yaml&lt;/code&gt; 文件。&lt;/p&gt;
&lt;h2 id=&#34;bug-成因&#34;&gt;Bug 成因&lt;/h2&gt;
&lt;p&gt;根据 &lt;a href=&#34;https://bugzilla.redhat.com/show_bug.cgi?id=1655214&#34; title=&#34;RedHat 的 Bug 报告&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RedHat 的 Bug 报告&lt;/a&gt;
，导致该 Bug 的原因是：&lt;/p&gt;
&lt;p&gt;CentOS7 发行版中的 Docker 使用的 docker-runc 二进制文件使用旧版本的 golang 构建的，这里面一些可能导致  &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSS/FIPS_Mode_-_an_explanation&#34; title=&#34;FIPS 模式&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FIPS 模式&lt;/a&gt;
崩溃的错误。&lt;/p&gt;
&lt;p&gt;至于该 Bug 是如何触发的官方只是说因为某些镜像导致的。&lt;/p&gt;
&lt;h2 id=&#34;发现过程&#34;&gt;发现过程&lt;/h2&gt;
&lt;p&gt;本周 &lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/appendix/kubernetes-1.13-changelog.html&#34; title=&#34;Kubernetes 1.13&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes 1.13&lt;/a&gt;
 发布，想着更新下我的 &lt;a href=&#34;https://github.com/rootsongjc/kubernetes-vagrant-centos-cluster&#34; title=&#34;kubernetes-vagrant-centos-cluster&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubernetes-vagrant-centos-cluster&lt;/a&gt;
 使用 Vagrant 和 VirtualBox 在本地搭建分布式 Kubernetes 1.13 集群和 &lt;a href=&#34;https://istio.io/zh&#34; title=&#34;Istio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio&lt;/a&gt;
 Service Mesh 的最新版本 1.0.4， 可是在安装 Istio 的时候发现 Istio 有两个 Pod 启动不起来，&lt;code&gt;istio-sidecar-injector&lt;/code&gt; 和 &lt;code&gt;istio-galley&lt;/code&gt; 这两个 Pod，检查其启动过程，发现它们都是因为 Readiness Probe 和 Liveness Probe 失败导致的。再联想到之前安装较老版本的 Istio 的时候也遇到该问题，见 &lt;a href=&#34;https://github.com/istio/istio/pull/6610&#34; title=&#34;Increase health probe interval #6610&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Increase health probe interval #6610&lt;/a&gt;
 通过增加健康检查的时间间隔可以解决该问题，可是经过反复的测试后发现还是不行。然后我想到先去掉健康检查，然后我手动使用 &lt;code&gt;kubectl exec&lt;/code&gt; 来执行健康检查的命令，解决却遇到下面的错误：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; -it istio-sidecar-injector-6fc974b6c8-pts4t -- istio-sidecar-injector-b484dfcbb-9x9l9 probe --probe-path&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/health --interval&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;4s
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Cannot ssh into a running pod/container -- rpc error: &lt;span class=&#34;nv&#34;&gt;code&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;desc&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; oci runtime error: &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; failed: container_linux.go:247: starting container process caused &lt;span class=&#34;s2&#34;&gt;&amp;#34;process_linux.go:110: decoding init error from pipe caused &amp;#34;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;read&lt;/span&gt; parent: connection reset by peer&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;command&lt;/span&gt; terminated with &lt;span class=&#34;nb&#34;&gt;exit&lt;/span&gt; code &lt;span class=&#34;m&#34;&gt;126&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;然后直接到 Pod 所在的主机使用 &lt;code&gt;docker exec&lt;/code&gt; 命令执行，依然报上面的错误，我就确定这不是 Kubernetes 的问题了。更何况前之前 &lt;a href=&#34;https://github.com/rootsongjc/kubernetes-vagrant-centos-cluster&#34; title=&#34;kubernetes-vagrant-centos-cluster&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubernetes-vagrant-centos-cluster&lt;/a&gt;
 屡试不爽，突然出现问题，有点让人摸不着头脑。知道我搜到了这个四天前才有人提出的 &lt;a href=&#34;https://github.com/openshift/origin/issues/21590&#34; title=&#34;issue&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;issue&lt;/a&gt;
。根据网友反馈，现在 &lt;a href=&#34;https://github.com/rootsongjc/kubernetes-vagrant-centos-cluster&#34; title=&#34;kubernetes-vagrant-centos-cluster&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubernetes-vagrant-centos-cluster&lt;/a&gt;
 中已经通过降级 Docker 的方式临时修复了该问题，并支持 Kubernetes 1.13 和  Istio 1.0.4，欢迎试用。&lt;/p&gt;
&lt;h2 id=&#34;解决方法&#34;&gt;解决方法&lt;/h2&gt;
&lt;p&gt;有两种解决方法，都需要替换 Docker 版本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一、降级到旧的 RedHat CentOS 官方源中的 Docker 版本&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;将 RedHat 官方源中的 Docker 版本降级，这样做的好处是所有的配置无需改动，参考 &lt;a href=&#34;https://github.com/openshift/origin/issues/21590&#34; title=&#34;https://github.com/openshift/origin/issues/21590&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/openshift/origin/issues/21590&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;查看 Docker 版本：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ rpm -qa &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; grep -i docker
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;docker-common-1.13.1-84.git07f3374.el7.centos.x86_64
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;docker-client-1.13.1-84.git07f3374.el7.centos.x86_64
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;docker-1.13.1-84.git07f3374.el7.centos.x86_64
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;降级 Docker 版本。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;降级之后再查看 Docker 版本：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ rpm -qa &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; grep -i docker
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;docker-common-1.13.1-75.git8633870.el7.centos.x86_64
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;docker-1.13.1-75.git8633870.el7.centos.x86_64
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;docker-client-1.13.1-75.git8633870.el7.centos.x86_64
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;此为临时解决方法，RedHat 也在着手解决该问题，为了可能会提供补丁，见 &lt;a href=&#34;https://bugzilla.redhat.com/show_bug.cgi?id=1655214&#34; title=&#34;&amp;lt;strong&amp;gt;Bug 1655214&amp;lt;/strong&amp;gt;&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bug 1655214&lt;/strong&gt;&lt;/a&gt;
 - docker exec does not work with registry.access.redhat.com/rhel7:7.3。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;二、更新到 Docker-CE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;众所周知，Docker 自1.13版本之后更改了版本的命名方式，也提供了官方的 CentOS 源，替换为 Docker-CE 亦可解决该问题，不过 Docker-CE 的配置可能会与 Docker 1.13 有所不同，所以可能需要修改配置文件。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/guide/configure-liveness-readiness-probes.html&#34; title=&#34;配置Pod的liveness和readiness探针 - jimmysong.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;配置Pod的liveness和readiness探针 - jimmysong.io&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bugzilla.redhat.com/show_bug.cgi?id=1655214&#34; title=&#34;Bug 1655214 - docker exec does not work with registry.access.redhat.com/rhel7:7.3 - redhat.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bug 1655214 - docker exec does not work with registry.access.redhat.com/rhel7:7.3 - redhat.com&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rootsongjc/kubernetes-vagrant-centos-cluster&#34; title=&#34;kubernetes-vagrant-centos-cluster - github.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubernetes-vagrant-centos-cluster - github.com&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSS/FIPS_Mode_-_an_explanation&#34; title=&#34;FIPS Mode - an explanation - mozilla.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FIPS Mode - an explanation - mozilla.org&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>配置Kubernetes DNS服务kube-dns</title>
      <link>https://jimmysong.io/blog/configuring-kubernetes-kube-dns/</link>
      <pubDate>Wed, 03 Jan 2018 16:16:01 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/configuring-kubernetes-kube-dns/</guid>
      <description>
        
        
        &lt;p&gt;在我们安装Kubernetes集群的时候就已经安装了kube-dns插件，这个插件也是官方推荐安装的。通过将 Service 注册到 DNS 中，Kuberentes 可以为我们提供一种简单的服务注册发现与负载均衡方式。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://coredns.io&#34; title=&#34;CoreDNS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CoreDNS&lt;/a&gt;
作为CNCF中的托管的一个项目，在Kuberentes1.9版本中，使用kubeadm方式安装的集群可以通过以下命令直接安装CoreDNS。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubeadm init --feature-gates&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;CoreDNS&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;true&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;您也可以使用CoreDNS替换Kubernetes插件kube-dns，可以使用 Pod 部署也可以独立部署，请参考&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/coredns/&#34; title=&#34;Using CoreDNS for Service Discovery&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Using CoreDNS for Service Discovery&lt;/a&gt;
，下文将介绍如何配置kube-dns。&lt;/p&gt;
&lt;p&gt;本文已归档到&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34; title=&#34;kubernetes-handbook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubernetes-handbook&lt;/a&gt;
中。&lt;/p&gt;
&lt;h2 id=&#34;kube-dns&#34;&gt;kube-dns&lt;/h2&gt;
&lt;p&gt;kube-dns是Kubernetes中的一个内置插件，目前作为一个独立的开源项目维护，见https://github.com/kubernetes/dns。&lt;/p&gt;
&lt;p&gt;下文中给出了配置 DNS Pod 的提示和定义 DNS 解析过程以及诊断 DNS 问题的指南。&lt;/p&gt;
&lt;h2 id=&#34;前提要求&#34;&gt;前提要求&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes 1.6 及以上版本。&lt;/li&gt;
&lt;li&gt;集群必须使用 &lt;code&gt;kube-dns&lt;/code&gt; 插件进行配置。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kube-dns-介绍&#34;&gt;kube-dns 介绍&lt;/h2&gt;
&lt;p&gt;从 Kubernetes v1.3 版本开始，使用 [cluster add-on 插件管理器回自动启动内置的 DNS。&lt;/p&gt;
&lt;p&gt;Kubernetes DNS pod 中包括 3 个容器：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kubedns&lt;/code&gt;：&lt;code&gt;kubedns&lt;/code&gt; 进程监视 Kubernetes master 中的 Service 和 Endpoint 的变化，并维护内存查找结构来服务DNS请求。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dnsmasq&lt;/code&gt;：&lt;code&gt;dnsmasq&lt;/code&gt; 容器添加 DNS 缓存以提高性能。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sidecar&lt;/code&gt;：&lt;code&gt;sidecar&lt;/code&gt; 容器在执行双重健康检查（针对 &lt;code&gt;dnsmasq&lt;/code&gt; 和 &lt;code&gt;kubedns&lt;/code&gt;）时提供单个健康检查端点（监听在10054端口）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DNS  pod 具有静态 IP 并作为 Kubernetes 服务暴露出来。该静态 IP 分配后，kubelet 会将使用 &lt;code&gt;--cluster-dns = &amp;lt;dns-service-ip&amp;gt;&lt;/code&gt; 标志配置的 DNS 传递给每个容器。&lt;/p&gt;
&lt;p&gt;DNS 名称也需要域名。本地域可以使用标志 &lt;code&gt;--cluster-domain = &amp;lt;default-local-domain&amp;gt;&lt;/code&gt; 在 kubelet 中配置。&lt;/p&gt;
&lt;p&gt;Kubernetes集群DNS服务器基于 &lt;a href=&#34;https://github.com/skynetservices/skydns&#34; title=&#34;SkyDNS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SkyDNS&lt;/a&gt;
 库。它支持正向查找（A 记录），服务查找（SRV 记录）和反向 IP 地址查找（PTR 记录）&lt;/p&gt;
&lt;h2 id=&#34;kube-dns-支持的-dns-格式&#34;&gt;kube-dns 支持的 DNS 格式&lt;/h2&gt;
&lt;p&gt;kube-dns 将分别为 service 和 pod 生成不同格式的 DNS 记录。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Service&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A记录：生成&lt;code&gt;my-svc.my-namespace.svc.cluster.local&lt;/code&gt;域名，解析成 IP 地址，分为两种情况：
&lt;ul&gt;
&lt;li&gt;普通 Service：解析成 ClusterIP&lt;/li&gt;
&lt;li&gt;Headless Service：解析为指定 Pod 的 IP 列表&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SRV记录：为命名的端口（普通 Service 或 Headless Service）生成 &lt;code&gt;_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster.local&lt;/code&gt; 的域名&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Pod&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A记录：生成域名 &lt;code&gt;pod-ip.my-namespace.pod.cluster.local&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kube-dns-存根域名&#34;&gt;kube-dns 存根域名&lt;/h2&gt;
&lt;p&gt;可以在 Pod 中指定 hostname 和 subdomain：&lt;code&gt;hostname.custom-subdomain.default.svc.cluster.local&lt;/code&gt;，例如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Pod&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;busybox&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;busybox&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;hostname&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;busybox-1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;subdomain&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;busybox-subdomain&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;containers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;busybox&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;busybox&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;command&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;sleep&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;s2&#34;&gt;&amp;#34;3600&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;该 Pod 的域名是 &lt;code&gt;busybox-1.busybox-subdomain.default.svc.cluster.local&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id=&#34;继承节点的-dns&#34;&gt;继承节点的 DNS&lt;/h2&gt;
&lt;p&gt;运行 Pod 时，kubelet 将预先配置集群 DNS 服务器到 Pod 中，并搜索节点自己的 DNS 设置路径。如果节点能够解析特定于较大环境的 DNS 名称，那么 Pod 应该也能够解析。请参阅下面的&lt;a href=&#34;#known-issues&#34; title=&#34;已知问题&#34;&gt;已知问题&lt;/a&gt;
以了解警告。&lt;/p&gt;
&lt;p&gt;如果您不想要这个，或者您想要为 Pod 设置不同的 DNS 配置，您可以给 kubelet 指定 &lt;code&gt;--resolv-conf&lt;/code&gt; 标志。将该值设置为 &amp;quot;&amp;quot; 意味着 Pod 不继承 DNS。将其设置为有效的文件路径意味着 kubelet 将使用此文件而不是 &lt;code&gt;/etc/resolv.conf&lt;/code&gt; 用于 DNS 继承。&lt;/p&gt;
&lt;h2 id=&#34;配置存根域和上游-dns-服务器&#34;&gt;配置存根域和上游 DNS 服务器&lt;/h2&gt;
&lt;p&gt;通过为 kube-dns （&lt;code&gt;kube-system:kube-dns&lt;/code&gt;）提供一个 ConfigMap，集群管理员能够指定自定义存根域和上游 nameserver。&lt;/p&gt;
&lt;p&gt;例如，下面的 ConfigMap 建立了一个 DNS 配置，它具有一个单独的存根域和两个上游 nameserver：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ConfigMap&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;kube-dns&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;kube-system&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;stubDomains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sd&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;{&lt;span class=&#34;nt&#34;&gt;“acme.local”&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;l&#34;&gt;“1.2.3.4”]}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;upstreamNameservers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sd&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;l&#34;&gt;“8.8.8.8”, “8.8.4.4”]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如上面指定的那样，带有“.acme.local”后缀的 DNS 请求被转发到 1.2.3.4 处监听的 DNS。Google Public DNS 为上游查询提供服务。&lt;/p&gt;
&lt;p&gt;下表描述了如何将具有特定域名的查询映射到其目标DNS服务器：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;域名&lt;/th&gt;
&lt;th&gt;响应查询的服务器&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;kubernetes.default.svc.cluster.local&lt;/td&gt;
&lt;td&gt;kube-dns&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;foo.acme.local&lt;/td&gt;
&lt;td&gt;自定义 DNS (1.2.3.4)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;widget.com&lt;/td&gt;
&lt;td&gt;上游 DNS (8.8.8.8 或 8.8.4.4)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;查看 &lt;a href=&#34;#configmap-options&#34; title=&#34;ConfigMap 选项&#34;&gt;ConfigMap 选项&lt;/a&gt;
 获取更多关于配置选项格式的详细信息。&lt;/p&gt;
&lt;h3 id=&#34;对-pod-的影响&#34;&gt;对 Pod 的影响&lt;/h3&gt;
&lt;p&gt;自定义的上游名称服务器和存根域不会影响那些将自己的 &lt;code&gt;dnsPolicy&lt;/code&gt; 设置为 &lt;code&gt;Default&lt;/code&gt; 或者 &lt;code&gt;None&lt;/code&gt; 的 Pod。&lt;/p&gt;
&lt;p&gt;如果 Pod 的 &lt;code&gt;dnsPolicy&lt;/code&gt; 设置为 “&lt;code&gt;ClusterFirst&lt;/code&gt;”，则其名称解析将按其他方式处理，具体取决于存根域和上游 DNS 服务器的配置。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;未进行自定义配置&lt;/strong&gt;：没有匹配上配置的集群域名后缀的任何请求，例如 “www.kubernetes.io”，将会被转发到继承自节点的上游 nameserver。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;进行自定义配置&lt;/strong&gt;：如果配置了存根域和上游 DNS 服务器（和在 &lt;a href=&#34;#configuring-stub-domain-and-upstream-dns-servers&#34; title=&#34;前面例子&#34;&gt;前面例子&lt;/a&gt;
 配置的一样），DNS 查询将根据下面的流程进行路由：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;查询首先被发送到 kube-dns 中的 DNS 缓存层。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;从缓存层，检查请求的后缀，并转发到合适的 DNS 上，基于如下的示例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;具有集群后缀的名字&lt;/em&gt; （例如 “.cluster.local”）：请求被发送到 kube-dns。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;具有存根域后缀的名字&lt;/em&gt; （例如 “.acme.local”）：请求被发送到配置的自定义 DNS 解析器（例如：监听在 1.2.3.4）。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;不具有能匹配上后缀的名字&lt;/em&gt; （例如 “widget.com”）：请求被转发到上游 DNS（例如：Google 公共 DNS 服务器，8.8.8.8 和 8.8.4.4）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  &lt;img src=&#34;https://d33wubrfki0l68.cloudfront.net/340889cb80e81dcd19a16bc34697a7907e2b229a/24ad0/docs/tasks/administer-cluster/dns-custom-nameservers/dns.png&#34; data-img=&#34;https://d33wubrfki0l68.cloudfront.net/340889cb80e81dcd19a16bc34697a7907e2b229a/24ad0/docs/tasks/administer-cluster/dns-custom-nameservers/dns.png&#34; alt=&#34;image&#34; data-caption=&#34;DNS lookup flow&#34;&gt;
  
  
  &lt;figcaption&gt;DNS lookup flow&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;configmap-选项&#34;&gt;ConfigMap 选项&lt;/h2&gt;
&lt;p&gt;kube-dns &lt;code&gt;kube-system:kube-dns&lt;/code&gt; ConfigMap 的选项如下所示：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;字段&lt;/th&gt;
&lt;th&gt;格式&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;stubDomains&lt;/code&gt;（可选）&lt;/td&gt;
&lt;td&gt;使用 DNS 后缀 key 的 JSON map（例如 “acme.local”），以及 DNS IP 的 JSON 数组作为 value。&lt;/td&gt;
&lt;td&gt;目标 nameserver 可能是一个 Kubernetes Service。例如，可以运行自己的 dnsmasq 副本，将 DNS 名字暴露到 ClusterDNS namespace 中。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;upstreamNameservers&lt;/code&gt;（可选）&lt;/td&gt;
&lt;td&gt;DNS IP 的 JSON 数组。&lt;/td&gt;
&lt;td&gt;注意：如果指定，则指定的值会替换掉被默认从节点的 &lt;code&gt;/etc/resolv.conf&lt;/code&gt; 中获取到的 nameserver。限制：最多可以指定三个上游 nameserver。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;示例&#34;&gt;示例&lt;/h3&gt;
&lt;h4 id=&#34;示例存根域&#34;&gt;示例：存根域&lt;/h4&gt;
&lt;p&gt;在这个例子中，用户有一个 Consul DNS 服务发现系统，他们希望能够与 kube-dns 集成起来。 Consul 域名服务器地址为 10.150.0.1，所有的 Consul 名字具有后缀 “.consul.local”。 要配置 Kubernetes，集群管理员只需要简单地创建一个 ConfigMap 对象，如下所示：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ConfigMap&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;kube-dns&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;kube-system&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;stubDomains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sd&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;{&lt;span class=&#34;nt&#34;&gt;“consul.local”&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;l&#34;&gt;“10.150.0.1”]}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意，集群管理员不希望覆盖节点的上游 nameserver，所以他们不会指定可选的 &lt;code&gt;upstreamNameservers&lt;/code&gt; 字段。&lt;/p&gt;
&lt;h4 id=&#34;示例上游-nameserver&#34;&gt;示例：上游 nameserver&lt;/h4&gt;
&lt;p&gt;在这个示例中，集群管理员不希望显式地强制所有非集群 DNS 查询进入到他们自己的 nameserver 172.16.0.1。 而且这很容易实现：他们只需要创建一个 ConfigMap，&lt;code&gt;upstreamNameservers&lt;/code&gt; 字段指定期望的 nameserver 即可。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ConfigMap&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;kube-dns&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;kube-system&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;upstreamNameservers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sd&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;l&#34;&gt;“172.16.0.1”]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;调试-dns-解析&#34;&gt;调试 DNS 解析&lt;/h2&gt;
&lt;h3 id=&#34;创建一个简单的-pod-用作测试环境&#34;&gt;创建一个简单的 Pod 用作测试环境&lt;/h3&gt;
&lt;p&gt;创建一个名为 busybox.yaml 的文件，其中包括以下内容：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Pod&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;busybox&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;containers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;busybox&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;busybox&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;command&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;sleep&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;s2&#34;&gt;&amp;#34;3600&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;imagePullPolicy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;IfNotPresent&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;restartPolicy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Always&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用该文件创建 Pod 并验证其状态：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl create -f busybox.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pod &lt;span class=&#34;s2&#34;&gt;&amp;#34;busybox&amp;#34;&lt;/span&gt; created
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl get pods busybox
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME      READY     STATUS    RESTARTS   AGE
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;busybox   1/1       Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          &amp;lt;some-time&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;该 Pod 运行后，您可以在它的环境中执行 &lt;code&gt;nslookup&lt;/code&gt;。如果您看到类似如下的输出，表示 DNS 正在正确工作。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; -ti busybox -- nslookup kubernetes.default
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Server:    10.0.0.10
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Address 1: 10.0.0.10
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Name:      kubernetes.default
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Address 1: 10.0.0.1
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果 &lt;code&gt;nslookup&lt;/code&gt; 命令失败，检查如下内容：&lt;/p&gt;
&lt;h3 id=&#34;首先检查本地-dns-配置&#34;&gt;首先检查本地 DNS 配置&lt;/h3&gt;
&lt;p&gt;查看下 resolv.conf 文件。（参考&lt;a href=&#34;#inheriting-dns-from-the-node&#34; title=&#34;集成节点的 DNS&#34;&gt;集成节点的 DNS&lt;/a&gt;
和 下面的&lt;a href=&#34;#known-issues&#34; title=&#34;已知问题&#34;&gt;已知问题&lt;/a&gt;
获取更多信息）&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; busybox cat /etc/resolv.conf
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;验证搜索路径和名称服务器设置如下（请注意，搜索路径可能因不同的云提供商而异）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;search default.svc.cluster.local svc.cluster.local cluster.local google.internal c.gce_project_id.internal
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;nameserver 10.0.0.10
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;options ndots:5
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果看到如下错误表明错误来自 kube-dns 或相关服务：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; -ti busybox -- nslookup kubernetes.default
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Server:    10.0.0.10
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Address 1: 10.0.0.10
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;nslookup: can&lt;span class=&#34;s1&#34;&gt;&amp;#39;t resolve &amp;#39;&lt;/span&gt;kubernetes.default&lt;span class=&#34;err&#34;&gt;&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;或者&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; -ti busybox -- nslookup kubernetes.default
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Server:    10.0.0.10
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;nslookup: can&lt;span class=&#34;s1&#34;&gt;&amp;#39;t resolve &amp;#39;&lt;/span&gt;kubernetes.default&lt;span class=&#34;err&#34;&gt;&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;检查-dns-pod-是否在运行&#34;&gt;检查 DNS pod 是否在运行&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;kubectl get pods&lt;/code&gt; 命令验证 DNS pod 是否正在运行。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl get pods --namespace&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kube-system -l k8s-app&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kube-dns
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME                    READY     STATUS    RESTARTS   AGE
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kube-dns-v19-ezo1y      3/3       Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;           1h
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果您看到没有 Pod 运行或者 Pod 处于 失败/完成 状态，DNS 插件可能没有部署到您的当前环境中，您需要手动部署。&lt;/p&gt;
&lt;h3 id=&#34;检查-dns-pod-中的错误&#34;&gt;检查 DNS pod 中的错误&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;kubectl logs&lt;/code&gt; 命令查看 DNS 守护进程的日志。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl logs --namespace&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kube-system &lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl get pods --namespace&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kube-system -l k8s-app&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kube-dns -o name&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt; -c kubedns
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl logs --namespace&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kube-system &lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl get pods --namespace&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kube-system -l k8s-app&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kube-dns -o name&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt; -c dnsmasq
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl logs --namespace&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kube-system &lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl get pods --namespace&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kube-system -l k8s-app&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kube-dns -o name&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt; -c sidecar
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;看看有没有可疑的日志。以字母“&lt;code&gt;W&lt;/code&gt;”，“&lt;code&gt;E&lt;/code&gt;”，“&lt;code&gt;F&lt;/code&gt;”开头的代表警告、错误和失败。请搜索具有这些日志级别的条目，并使用 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues&#34; title=&#34;kubernetes issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubernetes issues&lt;/a&gt;
来报告意外错误。&lt;/p&gt;
&lt;h3 id=&#34;dns-服务启动了吗&#34;&gt;DNS 服务启动了吗？&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;kubectl get service&lt;/code&gt; 命令验证 DNS 服务是否启动。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl get svc --namespace&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kube-system
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME          CLUSTER-IP     EXTERNAL-IP   PORT&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;S&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;             AGE
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kube-dns      10.0.0.10      &amp;lt;none&amp;gt;        53/UDP,53/TCP        1h
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果您已经创建了该服务或它本应该默认创建但没有出现，参考&lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/&#34; title=&#34;调试服务&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;调试服务&lt;/a&gt;
获取更多信息。&lt;/p&gt;
&lt;h3 id=&#34;dns-端点暴露出来了吗&#34;&gt;DNS 端点暴露出来了吗？&lt;/h3&gt;
&lt;p&gt;您可以使用&lt;code&gt;kubectl get endpoints&lt;/code&gt;命令验证 DNS 端点是否被暴露。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl get ep kube-dns --namespace&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kube-system
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME       ENDPOINTS                       AGE
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kube-dns   10.180.3.17:53,10.180.3.17:53    1h
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果您没有看到端点，查看&lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/&#34; title=&#34;调试服务&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;调试服务&lt;/a&gt;
文档中的端点部分。&lt;/p&gt;
&lt;p&gt;获取更多的 Kubernetes DNS 示例，请参考 Kubernetes GitHub 仓库中的&lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/cluster-dns&#34; title=&#34;cluster-dns示例&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cluster-dns示例&lt;/a&gt;
。&lt;/p&gt;
&lt;h2 id=&#34;已知问题&#34;&gt;已知问题&lt;/h2&gt;
&lt;p&gt;Kubernetes安装时不会将节点的 resolv.conf 文件配置为默认使用集群 DNS，因为该过程本身是特定于发行版的。这一步应该放到最后实现。&lt;/p&gt;
&lt;p&gt;Linux 的 libc 不可思议的卡住（&lt;a href=&#34;https://bugzilla.redhat.com/show_bug.cgi?id=168253&#34; title=&#34;查看该2005年起暴出来的bug&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;查看该2005年起暴出来的bug&lt;/a&gt;
）限制只能有 3 个 DNS &lt;code&gt;nameserver&lt;/code&gt; 记录和 6 个 DNS &lt;code&gt;search&lt;/code&gt; 记录。Kubernetes 需要消耗 1 个 &lt;code&gt;nameserver&lt;/code&gt; 记录和 3 个 &lt;code&gt;search&lt;/code&gt; 记录。这意味着如果本地安装已经使用 3 个 &lt;code&gt;nameserver&lt;/code&gt; 或使用 3 个以上的 &lt;code&gt;search&lt;/code&gt; 记录，那么其中一些设置将会丢失。有个部分解决该问题的方法，就是节点可以运行 &lt;code&gt;dnsmasq&lt;/code&gt;，它将提供更多的 &lt;code&gt;nameserver&lt;/code&gt; 条目，但不会有更多的 &lt;code&gt;search&lt;/code&gt; 条目。您也可以使用 kubelet 的 &lt;code&gt;--resolv-conf&lt;/code&gt; 标志。&lt;/p&gt;
&lt;p&gt;如果您使用的是 Alpine 3.3 或更低版本作为基础映像，由于已知的 Alpine 问题，DNS 可能无法正常工作。点击&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/30215&#34; title=&#34;这里&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这里&lt;/a&gt;
查看更多信息。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-集群联邦多可用区支持&#34;&gt;Kubernetes 集群联邦（多可用区支持）&lt;/h2&gt;
&lt;p&gt;Kubernetes 1.3 版本起引入了支持多站点 Kubernetes 安装的集群联邦支持。这需要对 Kubernetes 集群 DNS 服务器处理 DNS 查询的方式进行一些小的（向后兼容的）更改，以便于查找联邦服务（跨多个 Kubernetes 集群）。有关集群联邦和多站点支持的更多详细信息，请参阅&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/federation/&#34; title=&#34;集群联邦管理员指南&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;集群联邦管理员指南&lt;/a&gt;
。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/&#34; title=&#34;Configure DNS Service&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Configure DNS Service&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/&#34; title=&#34;Service 和 Pod 的 DNS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Service 和 Pod 的 DNS&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/&#34; title=&#34;自动扩容集群中的 DNS 服务&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;自动扩容集群中的 DNS 服务&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/coredns/&#34; title=&#34;Using CoreDNS for Service Discovery&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Using CoreDNS for Service Discovery&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>从外部访问 Kubernetes 中的 Pod</title>
      <link>https://jimmysong.io/blog/accessing-kubernetes-pods-from-outside-of-the-cluster/</link>
      <pubDate>Tue, 21 Nov 2017 20:13:01 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/accessing-kubernetes-pods-from-outside-of-the-cluster/</guid>
      <description>
        
        
        &lt;p&gt;本文主要讲解访问 kubenretes 中的 Pod 和 Serivce 的几种方式，包括如下几种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hostNetwork&lt;/li&gt;
&lt;li&gt;hostPort&lt;/li&gt;
&lt;li&gt;NodePort&lt;/li&gt;
&lt;li&gt;LoadBalancer&lt;/li&gt;
&lt;li&gt;Ingress&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;说是暴露 Pod 其实跟暴露 Service 是一回事，因为 Pod 就是 Service 的 backend。&lt;/p&gt;
&lt;h2 id=&#34;hostnetwork-true&#34;&gt;hostNetwork: true&lt;/h2&gt;
&lt;p&gt;这是一种直接定义 Pod 网络的方式。&lt;/p&gt;
&lt;p&gt;如果在 Pod 中使用 &lt;code&gt;hostNetwork:true&lt;/code&gt; 配置的话，在这种 pod 中运行的应用程序可以直接看到 pod 启动的主机的网络接口。在主机的所有网络接口上都可以访问到该应用程序。以下是使用主机网络的 pod 的示例定义：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Pod&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;influxdb&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;hostNetwork&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;containers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;influxdb&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;influxdb&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;部署该 Pod：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl create -f influxdb-hostnetwork.yml
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;访问该 pod 所在主机的 8086 端口：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;curl -v http://&lt;span class=&#34;nv&#34;&gt;$POD_IP&lt;/span&gt;:8086/ping
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;将看到 204 No Content 的 204 返回码，说明可以正常访问。&lt;/p&gt;
&lt;p&gt;注意每次启动这个 Pod 的时候都可能被调度到不同的节点上，所有外部访问 Pod 的 IP 也是变化的，而且调度 Pod 的时候还需要考虑是否与宿主机上的端口冲突，因此一般情况下除非您知道需要某个特定应用占用特定宿主机上的特定端口时才使用 &lt;code&gt;hostNetwork: true&lt;/code&gt; 的方式。&lt;/p&gt;
&lt;p&gt;这种 Pod 的网络模式有一个用处就是可以将网络插件包装在 Pod 中然后部署在每个宿主机上，这样该 Pod 就可以控制该宿主机上的所有网络。&lt;/p&gt;
&lt;h2 id=&#34;hostport&#34;&gt;hostPort&lt;/h2&gt;
&lt;p&gt;这是一种直接定义 Pod 网络的方式。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;hostPort&lt;/code&gt; 是直接将容器的端口与所调度的节点上的端口路由，这样用户就可以通过宿主机的 IP 加上来访问 Pod 了，如:。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Pod&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;influxdb&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;containers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;influxdb&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;influxdb&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ports&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;containerPort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8086&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;hostPort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8086&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这样做有个缺点，因为 Pod 重新调度的时候该 Pod 被调度到的宿主机可能会变动，这样就变化了，用户必须自己维护一个 Pod 与所在宿主机的对应关系。&lt;/p&gt;
&lt;p&gt;这种网络方式可以用来做 nginx &lt;a href=&#34;https://github.com/kubernetes/ingress/tree/master/controllers/nginx&#34; title=&#34;Ingress controller&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ingress controller&lt;/a&gt;
 。外部流量都需要通过 kubenretes node 节点的 80 和 443 端口。&lt;/p&gt;
&lt;h2 id=&#34;nodeport&#34;&gt;NodePort&lt;/h2&gt;
&lt;p&gt;NodePort 在 kubenretes 里是一个广泛应用的服务暴露方式。Kubernetes 中的 service 默认情况下都是使用的 &lt;code&gt;ClusterIP&lt;/code&gt; 这种类型，这样的 service 会产生一个 ClusterIP，这个 IP 只能在集群内部访问，要想让外部能够直接访问 service，需要将 service type 修改为 &lt;code&gt;nodePort&lt;/code&gt;。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Pod&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;influxdb&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;influxdb&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;containers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;influxdb&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;influxdb&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ports&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;containerPort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8086&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;同时还可以给 service 指定一个 &lt;code&gt;nodePort&lt;/code&gt; 值，范围是 30000-32767，这个值在 API server 的配置文件中，用 &lt;code&gt;--service-node-port-range&lt;/code&gt; 定义。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Service&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;influxdb&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;NodePort&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ports&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8086&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;nodePort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;30000&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;selector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;influxdb&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;集群外就可以使用 kubernetes 任意一个节点的 IP 加上 30000 端口访问该服务了。kube-proxy 会自动将流量以 round-robin 的方式转发给该 service 的每一个 pod。&lt;/p&gt;
&lt;p&gt;这种服务暴露方式，无法让你指定自己想要的应用常用端口，不过可以在集群上再部署一个反向代理作为流量入口。&lt;/p&gt;
&lt;h2 id=&#34;loadbalancer&#34;&gt;LoadBalancer&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;LoadBalancer&lt;/code&gt; 只能在 service 上定义。这是公有云提供的负载均衡器，如 AWS、Azure、CloudStack、GCE 等。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Service&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;influxdb&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;LoadBalancer&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ports&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8086&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;selector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;influxdb&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;查看服务：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl get svc influxdb
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME       CLUSTER-IP     EXTERNAL-IP     PORT&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;S&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;          AGE
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;influxdb   10.97.121.42   10.13.242.236   8086:30051/TCP   39s
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;内部可以使用 ClusterIP 加端口来访问服务，如 19.97.121.42:8086。&lt;/p&gt;
&lt;p&gt;外部可以用以下两种方式访问该服务：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用任一节点的 IP 加 30051 端口访问该服务&lt;/li&gt;
&lt;li&gt;使用 &lt;code&gt;EXTERNAL-IP&lt;/code&gt; 来访问，这是一个 VIP，是云供应商提供的负载均衡器 IP，如 10.13.242.236:8086。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ingress&#34;&gt;Ingress&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Ingress&lt;/code&gt; 是自 kubernetes1.1 版本后引入的资源类型。必须要部署 &lt;a href=&#34;https://github.com/kubernetes/ingress/tree/master/controllers/nginx&#34; title=&#34;Ingress controller&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ingress controller&lt;/a&gt;
 才能创建 Ingress 资源，Ingress controller 是以一种插件的形式提供。Ingress controller 是部署在 Kubernetes 之上的 Docker 容器。它的 Docker 镜像包含一个像 nginx 或 HAProxy 的负载均衡器和一个控制器守护进程。控制器守护程序从 Kubernetes 接收所需的 Ingress 配置。它会生成一个 nginx 或 HAProxy 配置文件，并重新启动负载平衡器进程以使更改生效。换句话说，Ingress controller 是由 Kubernetes 管理的负载均衡器。&lt;/p&gt;
&lt;p&gt;Kubernetes Ingress 提供了负载平衡器的典型特性：HTTP 路由，粘性会话，SSL 终止，SSL 直通，TCP 和 UDP 负载平衡等。目前并不是所有的 Ingress controller 都实现了这些功能，需要查看具体的 Ingress controller 文档。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;extensions/v1beta1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Ingress&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;influxdb&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;rules&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;host&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;influxdb.kube.example.com&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;http&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;paths&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;backend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;              &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;serviceName&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;influxdb&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;              &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;servicePort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8086&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;外部访问 URL &lt;code&gt;http://influxdb.kube.example.com/ping&lt;/code&gt; 访问该服务，入口就是 80 端口，然后 Ingress controller 直接将流量转发给后端 Pod，不需再经过 kube-proxy 的转发，比 LoadBalancer 方式更高效。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;总的来说 Ingress 是一个非常灵活和越来越得到厂商支持的服务暴露方式，包括 Nginx、HAProxy、Traefik，还有各种 Service Mesh，而其它服务暴露方式可以更适用于服务调试、特殊应用的部署。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docker 用户过渡到 kubectl 命令行指南</title>
      <link>https://jimmysong.io/blog/docker-cli-to-kubectl/</link>
      <pubDate>Sat, 16 Sep 2017 20:54:06 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/docker-cli-to-kubectl/</guid>
      <description>
        
        
        &lt;p&gt;对于没有使用过 kubernetes 的 docker 用户，如何快速掌握 kubectl 命令？kubectl 跟 docker 命令之间有什么区别和联系？&lt;/p&gt;
&lt;p&gt;在本文中，我们将向 docker-cli 用户介绍 Kubernetes 命令行如何与 api 进行交互。该命令行工具——kubectl，被设计成 docker-cli 用户所熟悉的样子，但是它们之间又存在一些必要的差异。该文档将向您展示每个 docker 子命令和 kubectl 与其等效的命令。&lt;/p&gt;
&lt;p&gt;在使用 kubernetes 集群的时候，docker 命令通常情况是不需要用到的，只有在调试程序或者容器的时候用到，我们基本上使用 kubectl 命令即可，所以在操作 kubernetes 的时候我们抛弃原先使用 docker 时的一些观念。&lt;/p&gt;
&lt;h2 id=&#34;docker-run&#34;&gt;docker run&lt;/h2&gt;
&lt;p&gt;如何运行一个 nginx Deployment 并将其暴露出来？ 查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl&#34; title=&#34;kubectl run&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubectl run&lt;/a&gt;
 。&lt;/p&gt;
&lt;p&gt;使用 docker 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ docker run -d --restart&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;always -e &lt;span class=&#34;nv&#34;&gt;DOMAIN&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;cluster --name nginx-app -p 80:80 nginx
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;a9ec34d9878748d2f33dc20cb25c714ff21da8d40558b45bfaec9955859075d0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ docker ps
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS                         NAMES
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;a9ec34d98787        nginx               &lt;span class=&#34;s2&#34;&gt;&amp;#34;nginx -g &amp;#39;daemon of   2 seconds ago       Up 2 seconds        0.0.0.0:80-&amp;gt;80/tcp, 443/tcp   nginx-app 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 kubectl 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# start the pod running nginx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl run --image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;nginx nginx-app --port&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;80&lt;/span&gt; --env&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;DOMAIN=cluster&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;deployment &lt;span class=&#34;s2&#34;&gt;&amp;#34;nginx-app&amp;#34;&lt;/span&gt; created
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在大于等于 1.2 版本 Kubernetes 集群中，使用&lt;code&gt;kubectl run&lt;/code&gt; 命令将创建一个名为 &amp;ldquo;nginx-app&amp;rdquo; 的 Deployment。如果您运行的是老版本，将会创建一个 replication controller。 如果您想沿用旧的行为，使用 &lt;code&gt;--generation=run/v1&lt;/code&gt; 参数，这样就会创建 replication controller。查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl/&#34; title=&#34;&amp;lt;code&amp;gt;kubectl run&amp;lt;/code&amp;gt;&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;kubectl run&lt;/code&gt;&lt;/a&gt;
 获取更多详细信息。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# expose a port through with a service&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl expose deployment nginx-app --port&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;80&lt;/span&gt; --name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;nginx-http
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;service &lt;span class=&#34;s2&#34;&gt;&amp;#34;nginx-http&amp;#34;&lt;/span&gt; exposed
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在 kubectl 命令中，我们创建了一个 &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment&#34; title=&#34;Deployment&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deployment&lt;/a&gt;
，这将保证有 N 个运行 nginx 的 pod（N 代表 spec 中声明的 replica 数，默认为 1）。我们还创建了一个 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/services&#34; title=&#34;service&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;service&lt;/a&gt;
，使用 selector 匹配具有相应的 selector 的 Deployment。查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/quick-start&#34; title=&#34;快速开始&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;快速开始&lt;/a&gt;
 获取更多信息。&lt;/p&gt;
&lt;p&gt;默认情况下镜像会在后台运行，与&lt;code&gt;docker run -d ...&lt;/code&gt; 类似，如果您想在前台运行，使用：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubectl run &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;-i&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;--tty&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; --attach &amp;lt;name&amp;gt; --image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;lt;image&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;与 &lt;code&gt;docker run ...&lt;/code&gt; 不同的是，如果指定了 &lt;code&gt;--attach&lt;/code&gt; ，我们将连接到 &lt;code&gt;stdin&lt;/code&gt;，&lt;code&gt;stdout&lt;/code&gt; 和 &lt;code&gt;stderr&lt;/code&gt;，而不能控制具体连接到哪个输出流（&lt;code&gt;docker -a ...&lt;/code&gt;）。&lt;/p&gt;
&lt;p&gt;因为我们使用 Deployment 启动了容器，如果您终止了连接到的进程（例如 &lt;code&gt;ctrl-c&lt;/code&gt;），容器将会重启，这跟 &lt;code&gt;docker run -it&lt;/code&gt;不同。 如果想销毁该 Deployment（和它的 pod），您需要运行 &lt;code&gt;kubeclt delete deployment &amp;lt;name&amp;gt;&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id=&#34;docker-ps&#34;&gt;docker ps&lt;/h2&gt;
&lt;p&gt;如何列出哪些正在运行？查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl&#34; title=&#34;kubectl get&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubectl get&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;使用 docker 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ docker ps
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS                         NAMES
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;a9ec34d98787        nginx               &lt;span class=&#34;s2&#34;&gt;&amp;#34;nginx -g &amp;#39;daemon of   About an hour ago   Up About an hour    0.0.0.0:80-&amp;gt;80/tcp, 443/tcp   nginx-app
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 kubectl 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl get po
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME              READY     STATUS    RESTARTS   AGE
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;nginx-app-5jyvm   1/1       Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          1h
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;docker-attach&#34;&gt;docker attach&lt;/h2&gt;
&lt;p&gt;如何连接到已经运行在容器中的进程？查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl&#34; title=&#34;kubectl attach&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubectl attach&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;使用 docker 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ docker ps
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS                         NAMES
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;a9ec34d98787        nginx               &lt;span class=&#34;s2&#34;&gt;&amp;#34;nginx -g &amp;#39;daemon of   8 minutes ago       Up 8 minutes        0.0.0.0:80-&amp;gt;80/tcp, 443/tcp   nginx-app
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&lt;/span&gt;$&lt;span class=&#34;s2&#34;&gt; docker attach a9ec34d98787
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 kubectl 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl get pods
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME              READY     STATUS    RESTARTS   AGE
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;nginx-app-5jyvm   1/1       Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          10m
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl attach -it nginx-app-5jyvm
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;docker-exec&#34;&gt;docker exec&lt;/h2&gt;
&lt;p&gt;如何在容器中执行命令？查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl/&#34; title=&#34;kubectl exec&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubectl exec&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;使用 docker 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ docker ps
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS                         NAMES
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;a9ec34d98787        nginx               &lt;span class=&#34;s2&#34;&gt;&amp;#34;nginx -g &amp;#39;daemon of   8 minutes ago       Up 8 minutes        0.0.0.0:80-&amp;gt;80/tcp, 443/tcp   nginx-app
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&lt;/span&gt;$&lt;span class=&#34;s2&#34;&gt; docker exec a9ec34d98787 cat /etc/hostname
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;a9ec34d98787
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 kubectl 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl get po
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME              READY     STATUS    RESTARTS   AGE
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;nginx-app-5jyvm   1/1       Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          10m
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; nginx-app-5jyvm -- cat /etc/hostname
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;nginx-app-5jyvm
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;执行交互式命令怎么办？&lt;/p&gt;
&lt;p&gt;使用 docker 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ docker &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; -ti a9ec34d98787 /bin/sh
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# exit&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 kubectl 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; -ti nginx-app-5jyvm -- /bin/sh      
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# exit&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;更多信息请查看 &lt;a href=&#34;https://kubernetes.io/docs/tasks/kubectl/get-shell-running-container&#34; title=&#34;获取运行中容器的 Shell 环境&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;获取运行中容器的 Shell 环境&lt;/a&gt;
。&lt;/p&gt;
&lt;h2 id=&#34;docker-logs&#34;&gt;docker logs&lt;/h2&gt;
&lt;p&gt;如何查看运行中进程的 stdout/stderr？查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl/&#34; title=&#34;kubectl logs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubectl logs&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;使用 docker 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ docker logs -f a9e
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;192.168.9.1 - - &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;14/Jul/2015:01:04:02 +0000&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;GET / HTTP/1.1&amp;#34;&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;200&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;612&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;curl/7.35.0&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;192.168.9.1 - - &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;14/Jul/2015:01:04:03 +0000&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;GET / HTTP/1.1&amp;#34;&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;200&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;612&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;curl/7.35.0&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 kubectl 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl logs -f nginx-app-zibvs
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;10.240.63.110 - - &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;14/Jul/2015:01:09:01 +0000&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;GET / HTTP/1.1&amp;#34;&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;200&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;612&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;curl/7.26.0&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;10.240.63.110 - - &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;14/Jul/2015:01:09:02 +0000&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;GET / HTTP/1.1&amp;#34;&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;200&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;612&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;curl/7.26.0&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;现在是时候提一下 pod 和容器之间的细微差别了；默认情况下如果 pod 中的进程退出 pod 也不会终止，相反它将会重启该进程。这类似于 docker run 时的 &lt;code&gt;--restart=always&lt;/code&gt; 选项， 这是主要差别。在 docker 中，进程的每个调用的输出都是被连接起来的，但是对于 kubernetes，每个调用都是分开的。要查看以前在 kubernetes 中执行的输出，请执行以下操作：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl logs --previous nginx-app-zibvs
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;10.240.63.110 - - &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;14/Jul/2015:01:09:01 +0000&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;GET / HTTP/1.1&amp;#34;&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;200&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;612&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;curl/7.26.0&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;10.240.63.110 - - &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;14/Jul/2015:01:09:02 +0000&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;GET / HTTP/1.1&amp;#34;&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;200&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;612&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;curl/7.26.0&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;查看 &lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/logging&#34; title=&#34;记录和监控集群活动&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;记录和监控集群活动&lt;/a&gt;
 获取更多信息。&lt;/p&gt;
&lt;h2 id=&#34;docker-stop-和-docker-rm&#34;&gt;docker stop 和 docker rm&lt;/h2&gt;
&lt;p&gt;如何停止和删除运行中的进程？查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl/&#34; title=&#34;kubectl delete&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubectl delete&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;使用 docker 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ docker ps
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS                         NAMES
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;a9ec34d98787        nginx               &lt;span class=&#34;s2&#34;&gt;&amp;#34;nginx -g &amp;#39;daemon of   22 hours ago        Up 22 hours         0.0.0.0:80-&amp;gt;80/tcp, 443/tcp   nginx-app
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&lt;/span&gt;$&lt;span class=&#34;s2&#34;&gt; docker stop a9ec34d98787
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;a9ec34d98787
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&lt;/span&gt;$&lt;span class=&#34;s2&#34;&gt; docker rm a9ec34d98787
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;a9ec34d98787
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 kubectl 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl get deployment nginx-app
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;nginx-app   &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;            &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           2m
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl get po -l &lt;span class=&#34;nv&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;nginx-app
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME                         READY     STATUS    RESTARTS   AGE
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;nginx-app-2883164633-aklf7   1/1       Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          2m
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl delete deployment nginx-app
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;deployment &lt;span class=&#34;s2&#34;&gt;&amp;#34;nginx-app&amp;#34;&lt;/span&gt; deleted
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl get po -l &lt;span class=&#34;nv&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;nginx-app
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Return nothing&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;请注意，我们不直接删除 pod。使用 kubectl 命令，我们要删除拥有该 pod 的 Deployment。如果我们直接删除pod，Deployment 将会重新创建该 pod。&lt;/p&gt;
&lt;h2 id=&#34;docker-login&#34;&gt;docker login&lt;/h2&gt;
&lt;p&gt;在 kubectl 中没有对 &lt;code&gt;docker login&lt;/code&gt; 的直接模拟。如果您有兴趣在私有镜像仓库中使用 Kubernetes，请参阅 &lt;a href=&#34;https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry&#34; title=&#34;使用私有镜像仓库&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;使用私有镜像仓库&lt;/a&gt;
。&lt;/p&gt;
&lt;h2 id=&#34;docker-version&#34;&gt;docker version&lt;/h2&gt;
&lt;p&gt;如何查看客户端和服务端的版本？查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl/&#34; title=&#34;kubectl version&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubectl version&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;使用 docker 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ docker version
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Client version: 1.7.0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Client API version: 1.19
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Go version &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;client&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;: go1.4.2
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Git commit &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;client&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;: 0baf609
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;OS/Arch &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;client&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;: linux/amd64
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Server version: 1.7.0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Server API version: 1.19
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Go version &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;server&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;: go1.4.2
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Git commit &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;server&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;: 0baf609
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;OS/Arch &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;server&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;: linux/amd64
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 kubectl 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl version
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Client Version: version.Info&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;Major:&lt;span class=&#34;s2&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt;, Minor:&lt;span class=&#34;s2&#34;&gt;&amp;#34;6&amp;#34;&lt;/span&gt;, GitVersion:&lt;span class=&#34;s2&#34;&gt;&amp;#34;v1.6.9+a3d1dfa6f4335&amp;#34;&lt;/span&gt;, GitCommit:&lt;span class=&#34;s2&#34;&gt;&amp;#34;9b77fed11a9843ce3780f70dd251e92901c43072&amp;#34;&lt;/span&gt;, GitTreeState:&lt;span class=&#34;s2&#34;&gt;&amp;#34;dirty&amp;#34;&lt;/span&gt;, BuildDate:&lt;span class=&#34;s2&#34;&gt;&amp;#34;2017-08-29T20:32:58Z&amp;#34;&lt;/span&gt;, OpenPaasKubernetesVersion:&lt;span class=&#34;s2&#34;&gt;&amp;#34;v1.03.02&amp;#34;&lt;/span&gt;, GoVersion:&lt;span class=&#34;s2&#34;&gt;&amp;#34;go1.7.5&amp;#34;&lt;/span&gt;, Compiler:&lt;span class=&#34;s2&#34;&gt;&amp;#34;gc&amp;#34;&lt;/span&gt;, Platform:&lt;span class=&#34;s2&#34;&gt;&amp;#34;linux/amd64&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Server Version: version.Info&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;Major:&lt;span class=&#34;s2&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt;, Minor:&lt;span class=&#34;s2&#34;&gt;&amp;#34;6&amp;#34;&lt;/span&gt;, GitVersion:&lt;span class=&#34;s2&#34;&gt;&amp;#34;v1.6.9+a3d1dfa6f4335&amp;#34;&lt;/span&gt;, GitCommit:&lt;span class=&#34;s2&#34;&gt;&amp;#34;9b77fed11a9843ce3780f70dd251e92901c43072&amp;#34;&lt;/span&gt;, GitTreeState:&lt;span class=&#34;s2&#34;&gt;&amp;#34;dirty&amp;#34;&lt;/span&gt;, BuildDate:&lt;span class=&#34;s2&#34;&gt;&amp;#34;2017-08-29T20:32:58Z&amp;#34;&lt;/span&gt;, OpenPaasKubernetesVersion:&lt;span class=&#34;s2&#34;&gt;&amp;#34;v1.03.02&amp;#34;&lt;/span&gt;, GoVersion:&lt;span class=&#34;s2&#34;&gt;&amp;#34;go1.7.5&amp;#34;&lt;/span&gt;, Compiler:&lt;span class=&#34;s2&#34;&gt;&amp;#34;gc&amp;#34;&lt;/span&gt;, Platform:&lt;span class=&#34;s2&#34;&gt;&amp;#34;linux/amd64&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;docker-info&#34;&gt;docker info&lt;/h2&gt;
&lt;p&gt;如何获取有关环境和配置的各种信息？查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl/&#34; title=&#34;kubectl cluster-info&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubectl cluster-info&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;使用 docker 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ docker info
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Containers: &lt;span class=&#34;m&#34;&gt;40&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Images: &lt;span class=&#34;m&#34;&gt;168&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Storage Driver: aufs
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; Root Dir: /usr/local/google/docker/aufs
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; Backing Filesystem: extfs
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; Dirs: &lt;span class=&#34;m&#34;&gt;248&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; Dirperm1 Supported: &lt;span class=&#34;nb&#34;&gt;false&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Execution Driver: native-0.2
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Logging Driver: json-file
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Kernel Version: 3.13.0-53-generic
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Operating System: Ubuntu 14.04.2 LTS
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;CPUs: &lt;span class=&#34;m&#34;&gt;12&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Total Memory: 31.32 GiB
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Name: k8s-is-fun.mtv.corp.google.com
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ID: ADUV:GCYR:B3VJ:HMPO:LNPQ:KD5S:YKFQ:76VN:IANZ:7TFV:ZBF4:BYJO
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;WARNING: No swap limit support
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 kubectl 命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl cluster-info
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Kubernetes master is running at https://108.59.85.141
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;KubeDNS is running at https://108.59.85.141/api/v1/namespaces/kube-system/services/kube-dns/proxy
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;KubeUI is running at https://108.59.85.141/api/v1/namespaces/kube-system/services/kube-ui/proxy
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Grafana is running at https://108.59.85.141/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Heapster is running at https://108.59.85.141/api/v1/namespaces/kube-system/services/monitoring-heapster/proxy
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;InfluxDB is running at https://108.59.85.141/api/v1/namespaces/kube-system/services/monitoring-influxdb/proxy
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;本文同时归档到 &lt;a href=&#34;https://jimmysong.io/kubernetes-handbook&#34; title=&#34;kubernetes-handbook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubernetes-handbook&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rootsongjc/kubernetes.github.io/blob/master/docs/user-guide/docker-cli-to-kubectl.md&#34; title=&#34;阅读原文&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;阅读原文&lt;/a&gt;
&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>每位 CTO 都该知道的关于 Kubernetes 的三件事</title>
      <link>https://jimmysong.io/blog/3-things-every-cto-should-know-about-kubernetes/</link>
      <pubDate>Sun, 10 Sep 2017 16:43:27 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/3-things-every-cto-should-know-about-kubernetes/</guid>
      <description>
        
        
        &lt;p&gt;Kubernetes 是一个自动发布、扩缩容和管理容器化应用的开源软件。&lt;/p&gt;
&lt;p&gt;尽管kubernetes非常强大，有如此多有用的技术特性，但是工具从来都不会被隔离起来单独使用，这要取决与底层基础架构，使用它的团队等等。&lt;/p&gt;
&lt;p&gt;在将kubernetes应用到生产环境（如果想成功的引入生产的话）之前，每个CTO都应该了解这三件事。&lt;/p&gt;
&lt;h3 id=&#34;1-你需要有坚实的基础设施&#34;&gt;1. 你需要有坚实的基础设施&lt;/h3&gt;
&lt;p&gt;大多数组织在运行kubernetes时遇到的第一个问题是在其下运行的平台。无论是基于VMware的私有云还是像AWS这样的公共云，您的平台需要稳定运行一段时间，并且具备以下基础设施基础知识：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;配置：根据需要创建虚拟机，或者直接使用裸机&lt;/li&gt;
&lt;li&gt;网络：DNS、负载均衡、VPC/VLAN、防火墙、安全组等&lt;/li&gt;
&lt;li&gt;存储：NFS/EFS/EBS/Ceph等，通过API创建&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果这些基础设施建设不到位，那么在尝试部署和运行kubernetes群集时，您将会遇到许多问题。&lt;/p&gt;
&lt;p&gt;根据经验，我们通常向客户推荐从像AWS这样的公共云提供商开始，然后配备了一些Hashicorp工具，如Terraform和Packer，以实现坚实的基础设施。&lt;/p&gt;
&lt;h3 id=&#34;2-您需要打造一支强大的团队&#34;&gt;2. 您需要打造一支强大的团队&lt;/h3&gt;
&lt;p&gt;让企业内部实现容器编排的能力很一个很有挑战的事情。&lt;/p&gt;
&lt;p&gt;打造是一个全面的团队，包括一些具有非常强大的Ops背景的成员，可以让他们去调试一些底层的东西，还有一些自动化工程师将负责设置和集群管理的日常工作，更多的研究人员将确保CI/CD流水线顺利运行以保证开发人员有一个很好的体验。&lt;/p&gt;
&lt;p&gt;下面是构建团队的几个建议：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;找到已经在尝试使用容器的团队，也许他们以前用过Docker Swarm或Rancher。他们可能已经渴望使用Kubernetes，并且愿意努力实施。&lt;/li&gt;
&lt;li&gt;给您的开发和运维团队做关于容器和容器编排方面的培训。&lt;/li&gt;
&lt;li&gt;聘请新人才。有时候，您可能会发现，最好的选择是建立一个全新的团队，这样他们就不会被当前流程所淹没，还可以向其他团队展示未来的样子。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;3-依托社区&#34;&gt;3. 依托社区&lt;/h3&gt;
&lt;p&gt;Kubernetes能够坐上了容器编排系统的头把交椅的主要原因是社区的支持。&lt;/p&gt;
&lt;p&gt;Kubernetes最初是基于Google的borg，borg具有非常丰富的功能集，现在已经是一个成熟的框架——这对kubernetes来说是非常有利的 ，但其成功的主要原因是已经形成了积极支持它的社区。&lt;/p&gt;
&lt;p&gt;下面是一些关于如何参与社区的小贴士：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;加入kubernetes slack channel，现在里面已经有21,000人，http://slack.k8s.io&lt;/li&gt;
&lt;li&gt;参与一个SIG（特别兴趣小组），这里面包括从在AWS上运行kubernetes到管理大数据集群。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.meetup.com/topics/kubernetes/&#34; title=&#34;参加meetup&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;参加meetup&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/hashtag/kubernetes&#34; title=&#34;关注#kubernetes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;关注#kubernetes&lt;/a&gt;
 关注那些主流传道者，有个人要特别关注下那就是&lt;a href=&#34;https://twitter.com/kelseyhightower&#34; title=&#34;Kelsey Hightower&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kelsey Hightower&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;走向成功&#34;&gt;走向成功&lt;/h3&gt;
&lt;p&gt;在拥有了坚如磐石的平台，熟练和多样化的团队，以及与Kubernetes社区不断增长的关系后，您将有处理通向成功道路上的遇到的任何问题的资本，克服成长过程中的痛苦。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.contino.io/insights/3-things-every-cto-should-know-about-kubernets&#34; title=&#34;原文地址&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;原文地址&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;作者: Marcus Maxwell&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>适用于 Kubernetes 的应用开发与部署流程详解</title>
      <link>https://jimmysong.io/blog/deploy-applications-in-kubernetes/</link>
      <pubDate>Thu, 20 Jul 2017 19:41:53 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/deploy-applications-in-kubernetes/</guid>
      <description>
        
        
        &lt;p&gt;本文已归档在&lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook&#34; title=&#34;kubernetes-handbook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubernetes-handbook&lt;/a&gt;
中的第3章【用户指南】中，一切更新以kubernetes-handbook中为准。&lt;/p&gt;
&lt;p&gt;为了详细说明，我特意写了两个示例程序放在GitHub中，模拟应用开发流程：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rootsongjc/k8s-app-monitor-test&#34; title=&#34;k8s-app-monitor-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;k8s-app-monitor-test&lt;/a&gt;
：生成模拟的监控数据，发送http请求，获取json返回值&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rootsongjc/k8s-app-monitor-agent&#34; title=&#34;K8s-app-monitor-agent&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;K8s-app-monitor-agent&lt;/a&gt;
：获取监控数据并绘图，访问浏览器获取图表&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;API文档见&lt;a href=&#34;https://github.com/rootsongjc/k8s-app-monitor-test&#34; title=&#34;k8s-app-monitor-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;k8s-app-monitor-test&lt;/a&gt;
中的&lt;code&gt;api.html&lt;/code&gt;文件，该文档在API blueprint中定义，使用&lt;a href=&#34;https://github.com/danielgtaylor/aglio&#34; title=&#34;aglio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;aglio&lt;/a&gt;
生成，打开后如图所示：&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/deploy-applications-in-kubernetes/k8s-app-monitor-test-api-doc.jpg&#34; data-img=&#34;/blog/deploy-applications-in-kubernetes/k8s-app-monitor-test-api-doc.jpg&#34; data-width=&#34;958&#34; data-height=&#34;941&#34; alt=&#34;image&#34; data-caption=&#34;API文档&#34;&gt;
    
  
  &lt;figcaption&gt;API文档&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;关于服务发现&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;K8s-app-monitor-agent&lt;/code&gt;服务需要访问&lt;code&gt;k8s-app-monitor-test&lt;/code&gt;服务，这就涉及到服务发现的问题，我们在代码中直接写死了要访问的服务的内网DNS地址（kubedns中的地址，即&lt;code&gt;k8s-app-monitor-test.default.svc.cluster.local&lt;/code&gt;）。&lt;/p&gt;
&lt;p&gt;我们知道Kubernetes在启动Pod的时候为容器注入环境变量，这些环境变量在所有的 namespace 中共享（环境变量是不断追加的，新启动的Pod中将拥有老的Pod中所有的环境变量，而老的Pod中的环境变量不变）。但是既然使用这些环境变量就已经可以访问到对应的service，那么获取应用的地址信息，究竟是使用变量呢？还是直接使用DNS解析来发现？&lt;/p&gt;
&lt;p&gt;答案是使用DNS，详细说明见&lt;a href=&#34;https://jimmysong.io/posts/exploring-kubernetes-env-with-docker/&#34; title=&#34;Kubernetes中的服务发现与Docker容器间的环境变量传递源码探究&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes中的服务发现与Docker容器间的环境变量传递源码探究&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;打包镜像&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;因为我使用wercker自动构建，构建完成后自动打包成docker镜像并上传到docker hub中（需要提前在docker hub中创建repo），如何使用 wercker 做持续构建与发布，并集成docker hub插件请参考&lt;a href=&#34;https://jimmysong.io/posts/continuous-integration-with-wercker/&#34; title=&#34;使用Wercker进行持续构建与发布&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;使用Wercker进行持续构建与发布&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://app.wercker.com/jimmysong/k8s-app-monitor-agent/&#34; title=&#34;查看详细构建流程&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;查看详细构建流程&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/deploy-applications-in-kubernetes/k8s-app-monitor-agent-wercker.jpg&#34; data-img=&#34;/blog/deploy-applications-in-kubernetes/k8s-app-monitor-agent-wercker.jpg&#34; data-width=&#34;3316&#34; data-height=&#34;1536&#34; alt=&#34;image&#34; data-caption=&#34;wercker&#34;&gt;
    
  
  &lt;figcaption&gt;wercker&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;生成了如下两个docker镜像：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;jimmysong/k8s-app-monitor-test:latest&lt;/li&gt;
&lt;li&gt;jimmysong/k8s-app-monitor-agent:latest&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;启动服务&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所有的kubernetes应用启动所用的yaml配置文件都保存在那两个GitHub仓库的&lt;code&gt;manifest.yaml&lt;/code&gt;文件中。&lt;/p&gt;
&lt;p&gt;分别在两个GitHub目录下执行&lt;code&gt;kubectl create -f manifest.yaml&lt;/code&gt;即可启动服务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;外部访问&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;服务启动后需要更新ingress配置，在&lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook/blob/master/manifests/traefik-ingress/ingress.yaml&#34; title=&#34;ingress.yaml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ingress.yaml&lt;/a&gt;
文件中增加以下几行：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Yaml&#34; data-lang=&#34;Yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;host&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;k8s-app-monitor-agent.jimmysong.io&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;http&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;paths&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;backend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;serviceName&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;k8s-app-monitor-agent&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;servicePort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8080&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;保存后，然后执行&lt;code&gt;kubectl replace -f ingress.yaml&lt;/code&gt;即可刷新ingress。&lt;/p&gt;
&lt;p&gt;修改本机的&lt;code&gt;/etc/hosts&lt;/code&gt;文件，在其中加入以下一行：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-ini&#34; data-lang=&#34;ini&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;172.20.0.119 k8s-app-monitor-agent.jimmysong.io&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;当然你也可以加入到DNS中，为了简单起见我使用hosts。&lt;/p&gt;
&lt;p&gt;详见&lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook/blob/master/practice/edge-node-configuration.md&#34; title=&#34;边缘节点配置&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;边缘节点配置&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;在浏览器中访问http://k8s-app-monitor-agent.jimmysong.io&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/deploy-applications-in-kubernetes/k8s-app-monitor-agent.jpg&#34; data-img=&#34;/blog/deploy-applications-in-kubernetes/k8s-app-monitor-agent.jpg&#34; data-width=&#34;1015&#34; data-height=&#34;579&#34; alt=&#34;image&#34; data-caption=&#34;图表&#34;&gt;
    
  
  &lt;figcaption&gt;图表&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;刷新页面将获得新的图表。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/posts/continuous-integration-with-wercker/&#34; title=&#34;使用Wercker进行持续构建与发布&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;使用Wercker进行持续构建与发布&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://app.wercker.com/jimmysong/k8s-app-monitor-agent/&#34; title=&#34;示例的项目代码服务器端&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;示例的项目代码服务器端&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rootsongjc/k8s-app-monitor-agent&#34; title=&#34;示例项目代码前端&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;示例项目代码前端&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/&#34; title=&#34;kubernetes-handbok&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubernetes-handbok&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook/blob/master/practice/edge-node-configuration.md&#34; title=&#34;边缘节点配置&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;边缘节点配置&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>记一本关于kubernetes management design patterns的书</title>
      <link>https://jimmysong.io/blog/book-kubernetes-management-design-patterns/</link>
      <pubDate>Thu, 20 Jul 2017 18:21:18 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/book-kubernetes-management-design-patterns/</guid>
      <description>
        
        
        &lt;p&gt;下面是这本书的基本信息。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;书名： Kubernetes Management Design Patterns: With Docker, CoreOS Linux, and Other Platforms&lt;/li&gt;
&lt;li&gt;Amazon购买链接：&lt;a href=&#34;https://www.amazon.com/Kubernetes-Management-Design-Patterns-Platforms-ebook/dp/B01MZDO0BD/ref=pd_sbs_351_4?_encoding=UTF8&amp;amp;psc=1&amp;amp;refRID=79F47CR67EEESD35S2VF&#34; title=&#34;链接&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;链接&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;作者：Deepak Vohra&lt;/li&gt;
&lt;li&gt;发行日期：2017年1月20日&lt;/li&gt;
&lt;li&gt;出版社：Apress&lt;/li&gt;
&lt;li&gt;页数：399&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;简介&#34;&gt;简介&lt;/h3&gt;
&lt;p&gt;Kubernetes引领容器集群管理进入一个全新的阶段；学习如何在CoreOS上配置和管理kubernetes集群；使用适当的管理模式，如ConfigMaps、Autoscaling、弹性资源使用和高可用性配置。讨论了kubernetes的一些其他特性，如日志、调度、滚动升级、volume、服务类型和跨多个云供应商zone。&lt;/p&gt;
&lt;p&gt;Kubernetes中的最小模块化单位是Pod，它是拥有共同的文件系统和网络的系列容器的集合。Pod的抽象层可以对容器使用设计模式，就像面向对象设计模式一样。容器能够提供与软件对象（如模块化或包装，抽象和重用）相同的优势。&lt;/p&gt;
&lt;p&gt;在大多数章节中使用的都是CoreOS Linux，其他讨论的平台有CentOS，OpenShift，Debian 8（jessie），AWS和Debian 7 for Google Container Engine。&lt;/p&gt;
&lt;p&gt;使用CoreOS主要是因为Docker已经在CoreOS上开箱即用。CoreOS：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持大多数云提供商（包括Amazon AWS EC2和Google Cloud Platform）和虚拟化平台（如VMWare和VirtualBox）&lt;/li&gt;
&lt;li&gt;提供Cloud-Config，用于声明式配置OS，如网络配置（flannel），存储（etcd）和用户帐户&lt;/li&gt;
&lt;li&gt;为容器化应用提供生产级基础架构，包括自动化，安全性和可扩展性&lt;/li&gt;
&lt;li&gt;引领容器行业标准，并建立了应用程序标准&lt;/li&gt;
&lt;li&gt;提供最先进的容器仓库，Quay&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Docker于2013年3月开源，现已称为最流行的容器平台。kubernetes于2014年6月开源，现在已经成为最流行的容器集群管理平台。第一个稳定版CoreOS Linux于2014年7月发行，现已成为最流行的容器操作系统之一。&lt;/p&gt;
&lt;h3 id=&#34;你将学到什么&#34;&gt;你将学到什么&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;使用docker和kubernetes&lt;/li&gt;
&lt;li&gt;在AWS和CoreOS上创建kubernetes集群&lt;/li&gt;
&lt;li&gt;应用集群管理设计模式&lt;/li&gt;
&lt;li&gt;使用多个云供应商zone&lt;/li&gt;
&lt;li&gt;使用Ansible管理kubernetes&lt;/li&gt;
&lt;li&gt;基于kubernetes的PAAS平台OpenShift&lt;/li&gt;
&lt;li&gt;创建高可用网站&lt;/li&gt;
&lt;li&gt;构建高可用用的kubernetes master集群&lt;/li&gt;
&lt;li&gt;使用volume、configmap、serivce、autoscaling和rolling update&lt;/li&gt;
&lt;li&gt;管理计算资源&lt;/li&gt;
&lt;li&gt;配置日志和调度&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;谁适合读这本书&#34;&gt;谁适合读这本书&lt;/h3&gt;
&lt;p&gt;Linux管理员、CoreOS管理员、应用程序开发者、容器即服务（CAAS）开发者。阅读这本书需要Linux和Docker的前置知识。介绍Kubernetes的知识，例如创建集群，创建Pod，创建service以及创建和缩放replication controller。还需要一些关于使用Amazon Web Services（AWS）EC2，CloudFormation和VPC的必备知识。&lt;/p&gt;
&lt;h3 id=&#34;关于作者&#34;&gt;关于作者&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Deepak Vohra&lt;/strong&gt; is an Oracle Certified Associate and a Sun Certified Java Programmer. Deepak has published in Oracle Magazine, OTN, IBM developerWorks, ONJava, DevSource,  WebLogic Developer’s Journal, XML Journal, Java Developer’s Journal, FTPOnline, and devx.&lt;/p&gt;
&lt;h3 id=&#34;目录&#34;&gt;目录&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;第一部分：平台
&lt;ul&gt;
&lt;li&gt;第1章：Kuberentes on AWS&lt;/li&gt;
&lt;li&gt;第2章：kubernetes on CoreOS on AWS&lt;/li&gt;
&lt;li&gt;第3章：kubernetes on Google Cloud Platform&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;第二部分：管理和配置
&lt;ul&gt;
&lt;li&gt;第4章：使用多个可用区&lt;/li&gt;
&lt;li&gt;第5章：使用Tectonic Console&lt;/li&gt;
&lt;li&gt;第6章：使用volume&lt;/li&gt;
&lt;li&gt;第7章：使用service&lt;/li&gt;
&lt;li&gt;第8章：使用Rolling updte&lt;/li&gt;
&lt;li&gt;第9章：在node上调度pod&lt;/li&gt;
&lt;li&gt;第10章：配置计算资源&lt;/li&gt;
&lt;li&gt;第11章：使用ConfigMap&lt;/li&gt;
&lt;li&gt;第12章：使用资源配额&lt;/li&gt;
&lt;li&gt;第13章：使用Autoscaling&lt;/li&gt;
&lt;li&gt;第14章：配置logging&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;第三部分：高可用
&lt;ul&gt;
&lt;li&gt;第15章：在OpenShift中使用HA master&lt;/li&gt;
&lt;li&gt;第16章：开发高可用网站&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;个人评价&#34;&gt;个人评价&lt;/h3&gt;
&lt;p&gt;本书更像是一本参考手册，对于想在公有云中（如AWS、Google Cloud Platform）中尝试Kubernetes的人会有所帮助，而对于想使用kubernetes进行自己的私有云建设，或想了解kubernetes的实现原理和技术细节的人来说，就不适合了。对我来说，本书中有个别几个章节可以参考，如高可用，但还是使用OpenShift实现的。总之，如果你使用AWS这样的公有云，对操作系统没有特别要求，可以接受CoreOS的话，那么可以看看这本书。本来本书会对kubernetes中的各种应用模式能够有个详解，但是从书中我并没有找到。&lt;/p&gt;
&lt;p&gt;本书有两个优点，一个是每个章节都给出了问题的起因和kubernetes的解决方案，二是几乎所有的命令和操作都附有截图，说明很详细。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Kubernetes中的服务发现与docker容器间的环境变量传递源码探究</title>
      <link>https://jimmysong.io/blog/exploring-kubernetes-env-with-docker/</link>
      <pubDate>Wed, 19 Jul 2017 23:15:01 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/exploring-kubernetes-env-with-docker/</guid>
      <description>
        
        
        &lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;
&lt;p&gt;今天创建了两个kubernetes示例应用：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rootsongjc/k8s-app-monitor-test&#34; title=&#34;k8s-app-monitor-test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;k8s-app-monitor-test&lt;/a&gt;
：启动server用来产生metrics&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rootsongjc/k8s-app-monitor-agent&#34; title=&#34;k8s-app-monitor-agent&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;k8s-app-monitor-agent&lt;/a&gt;
：获取metrics并绘图，显示在web上&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;注：相关的kubernetes应用&lt;code&gt;manifest.yaml&lt;/code&gt;文件分别见以上两个应用的GitHub 。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;当我查看Pod中的环境变量信息时，例如kubernetes中的service &lt;code&gt;k8s-app-monitor-test&lt;/code&gt;注入的环境变量时，包括了以下变量：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-ini&#34; data-lang=&#34;ini&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;K8S_APP_MONITOR_TEST_PORT_3000_TCP_ADDR&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;10.254.56.68&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;K8S_APP_MONITOR_TEST_PORT&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;tcp://10.254.56.68:3000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;K8S_APP_MONITOR_TEST_PORT_3000_TCP_PROTO&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;tcp&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;K8S_APP_MONITOR_TEST_SERVICE_PORT_HTTP&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;3000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;K8S_APP_MONITOR_TEST_PORT_3000_TCP_PORT&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;3000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;K8S_APP_MONITOR_TEST_PORT_3000_TCP&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;tcp://10.254.56.68:3000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;K8S_APP_MONITOR_TEST_SERVICE_HOST&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;10.254.56.68&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;K8S_APP_MONITOR_TEST_SERVICE_PORT&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;3000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我们知道Kubernetes在启动Pod的时候为容器注入环境变量，这些环境变量将在该Pod所在的namespace中共享。但是既然使用这些环境变量就已经可以访问到对应的service，那么获取应用的地址信息，究竟是使用变量呢？还是直接使用DNS解析来发现？下面我们从代码中来寻求答案。&lt;/p&gt;
&lt;p&gt;如果不想看下面的文字，可以直接看图。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/exploring-kubernetes-env-with-docker/kubernetes-service-discovery-with-dns-or-env.png&#34; data-img=&#34;/blog/exploring-kubernetes-env-with-docker/kubernetes-service-discovery-with-dns-or-env.png&#34; data-width=&#34;1029&#34; data-height=&#34;1127&#34; alt=&#34;image&#34; data-caption=&#34;kubernetes中传递ENV的探索过程&#34;&gt;
    
  
  &lt;figcaption&gt;kubernetes中传递ENV的探索过程&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;探索&#34;&gt;探索&lt;/h2&gt;
&lt;p&gt;docker的&lt;code&gt;docker/engine-api/types/container/config.go&lt;/code&gt;中的&lt;code&gt;Config&lt;/code&gt;结构体中有对环境变量的定义：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Go&#34; data-lang=&#34;Go&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// Config contains the configuration data about a container.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// It should hold only portable information about the container.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// Here, &amp;#34;portable&amp;#34; means &amp;#34;independent from the host we are running on&amp;#34;.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// Non-portable information *should* appear in HostConfig.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// All fields added to this struct must be marked `omitempty` to keep getting
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// predictable hashes from the old `v1Compatibility` configuration.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;type&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;Config&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;nx&#34;&gt;Hostname&lt;/span&gt;        &lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;                &lt;span class=&#34;c1&#34;&gt;// Hostname
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;	&lt;span class=&#34;nx&#34;&gt;Domainname&lt;/span&gt;      &lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;                &lt;span class=&#34;c1&#34;&gt;// Domainname
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;	&lt;span class=&#34;nx&#34;&gt;User&lt;/span&gt;            &lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;                &lt;span class=&#34;c1&#34;&gt;// User that will run the command(s) inside the container
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;nx&#34;&gt;Env&lt;/span&gt;             &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;              &lt;span class=&#34;c1&#34;&gt;// List of environment variable to set in the container
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;	&lt;span class=&#34;nx&#34;&gt;Cmd&lt;/span&gt;             &lt;span class=&#34;nx&#34;&gt;strslice&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;StrSlice&lt;/span&gt;     &lt;span class=&#34;c1&#34;&gt;// Command to run when starting the container
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;	&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Kubernetes中在&lt;code&gt;pkg/kubelet/container/runtime.go&lt;/code&gt;中的&lt;code&gt;RunContainerOptions&lt;/code&gt;结构体中定义：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// RunContainerOptions specify the options which are necessary for running containers
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;type&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;RunContainerOptions&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;c1&#34;&gt;// The environment variables list.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;	&lt;span class=&#34;nx&#34;&gt;Envs&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;EnvVar&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  	&lt;span class=&#34;c1&#34;&gt;// The mounts for the containers.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;	&lt;span class=&#34;nx&#34;&gt;Mounts&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Mount&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;c1&#34;&gt;// The host devices mapped into the containers.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Kubelet向容器中注入环境变量的配置是在下面的方法中定义：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-ini&#34; data-lang=&#34;ini&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;pkg/kubelet/kuberuntime/kuberuntime_container.go&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Go&#34; data-lang=&#34;Go&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// generateContainerConfig generates container config for kubelet runtime v1.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;func&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;m&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;kubeGenericRuntimeManager&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;generateContainerConfig&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;container&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Container&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;pod&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Pod&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;restartCount&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;podIP&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;imageRef&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;runtimeapi&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;ContainerConfig&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;error&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nx&#34;&gt;opts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;err&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;runtimeHelper&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;GenerateRunContainerOptions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;pod&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;container&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;podIP&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;c1&#34;&gt;// set environment variables
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;	&lt;span class=&#34;nx&#34;&gt;envs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;make&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;runtimeapi&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;KeyValue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;opts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Envs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;range&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;opts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Envs&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;		&lt;span class=&#34;nx&#34;&gt;e&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;opts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Envs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;		&lt;span class=&#34;nx&#34;&gt;envs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;runtimeapi&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;KeyValue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;			&lt;span class=&#34;nx&#34;&gt;Key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;   &lt;span class=&#34;nx&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;			&lt;span class=&#34;nx&#34;&gt;Value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;		&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;nx&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Envs&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;envs&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;nil&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;kubelet的&lt;code&gt;pkg/kubelet/kubelet_pods.go&lt;/code&gt;的如下方法中生成了&lt;code&gt;RunContainerOptions&lt;/code&gt;：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Go&#34; data-lang=&#34;Go&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// GenerateRunContainerOptions generates the RunContainerOptions, which can be used by
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// the container runtime to set parameters for launching a container.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;func&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;kl&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Kubelet&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;GenerateRunContainerOptions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;pod&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Pod&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;container&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Container&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;podIP&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;kubecontainer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;RunContainerOptions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;bool&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;error&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;nx&#34;&gt;opts&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;kubecontainer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;RunContainerOptions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;CgroupParent&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;cgroupParent&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;nx&#34;&gt;opts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Envs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;err&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;kl&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;makeEnvironmentVariables&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;pod&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;container&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;podIP&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;opts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;useClusterFirstPolicy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;nil&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我们再看下&lt;code&gt;makeEnvironmentVariables(pod, container, podIP)&lt;/code&gt;方法中又做了什么（该方法也在&lt;code&gt;pkg/kubelet/kubelet_pods.go&lt;/code&gt;文件中）。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Go&#34; data-lang=&#34;Go&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// Make the environment variables for a pod in the given namespace.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kd&#34;&gt;func&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;kl&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Kubelet&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;makeEnvironmentVariables&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;pod&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Pod&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;container&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;Container&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;podIP&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;([]&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;kubecontainer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;EnvVar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;error&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;kd&#34;&gt;var&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;kubecontainer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;EnvVar&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;c1&#34;&gt;// Note:  These are added to the docker Config, but are not included in the checksum computed
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;	&lt;span class=&#34;c1&#34;&gt;// by dockertools.BuildDockerName(...).  That way, we can still determine whether an
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;	&lt;span class=&#34;c1&#34;&gt;// v1.Container is already running by its hash. (We don&amp;#39;t want to restart a container just
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;	&lt;span class=&#34;c1&#34;&gt;// because some service changed.)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;	&lt;span class=&#34;c1&#34;&gt;//
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;	&lt;span class=&#34;c1&#34;&gt;// Note that there is a race between Kubelet seeing the pod and kubelet seeing the service.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;	&lt;span class=&#34;c1&#34;&gt;// To avoid this users can: (1) wait between starting a service and starting; or (2) detect
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;	&lt;span class=&#34;c1&#34;&gt;// missing service env var and exit and be restarted; or (3) use DNS instead of env vars
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;	&lt;span class=&#34;c1&#34;&gt;// and keep trying to resolve the DNS name of the service (recommended).
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;	&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;该代码段比较长，kubernetes究竟如何将环境变量注入到docker容器中的奥秘就在这里，按图索骥到了这里，从代码注释中已经可以得出结论，使用DNS解析而不要使用环境变量来做服务发现，究竟为何这样做，改天我们再详细解读。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Kubernetes中的数据持久化问题</title>
      <link>https://jimmysong.io/blog/data-persistence-problem/</link>
      <pubDate>Tue, 11 Jul 2017 20:33:21 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/data-persistence-problem/</guid>
      <description>
        
        
        &lt;h2 id=&#34;数据落盘问题的由来&#34;&gt;数据落盘问题的由来&lt;/h2&gt;
&lt;p&gt;这本质上是数据持久化问题，对于有些应用依赖持久化数据，比如应用自身产生的日志需要持久化存储的情况，需要保证容器里的数据不丢失，在Pod挂掉后，其他应用依然可以访问到这些数据，因此我们需要将数据持久化存储起来。&lt;/p&gt;
&lt;h2 id=&#34;数据落盘问题解决方案&#34;&gt;数据落盘问题解决方案&lt;/h2&gt;
&lt;p&gt;下面以一个应用的日志收集为例，该日志需要持久化收集到ElasticSearch集群中，如果不考虑数据丢失的情形，可以直接使用&lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook&#34; title=&#34;kubernetes-handbook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubernetes-handbook&lt;/a&gt;
中【应用日志收集】一节中的方法，但考虑到Pod挂掉时logstash（或filebeat）并没有收集完该pod内日志的情形，我们想到了如下这种解决方案，示意图如下：&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/data-persistence-problem/log-persistence-logstash.png&#34; data-img=&#34;/blog/data-persistence-problem/log-persistence-logstash.png&#34; data-width=&#34;789&#34; data-height=&#34;418&#34; alt=&#34;image&#34; data-caption=&#34;日志持久化收集解决方案示意图&#34;&gt;
    
  
  &lt;figcaption&gt;日志持久化收集解决方案示意图&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先需要给数据落盘的应用划分node，即这些应用只调用到若干台主机上&lt;/li&gt;
&lt;li&gt;给这若干台主机增加label&lt;/li&gt;
&lt;li&gt;使用&lt;code&gt;deamonset&lt;/code&gt;方式在这若干台主机上启动logstash的Pod（使用nodeSelector来限定在这几台主机上，我们在边缘节点启动的&lt;code&gt;treafik&lt;/code&gt;也是这种模式）&lt;/li&gt;
&lt;li&gt;将应用的数据通过volume挂载到宿主机上&lt;/li&gt;
&lt;li&gt;Logstash（或者filebeat）收集宿主机上的数据，数据持久化不会丢失&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;side-effect&#34;&gt;Side-effect&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;首先kubernetes本身就提供了数据持久化的解决方案statefulset，不过需要用到公有云的存储货其他分布式存储，这一点在我们的私有云环境里被否定了。&lt;/li&gt;
&lt;li&gt;需要管理主机的label，增加运维复杂度，但是具体问题具体对待&lt;/li&gt;
&lt;li&gt;必须保证应用启动顺序，需要先启动logstash&lt;/li&gt;
&lt;li&gt;为主机打label使用nodeSelector的方式限制了资源调度的范围&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;本文已归档到&lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook/&#34; title=&#34;kubernetes-handbook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubernetes-handbook&lt;/a&gt;
中的【最佳实践—运维管理】章节中，一切内容以kubernetes-handbook为准。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Kubernetes配置最佳实践</title>
      <link>https://jimmysong.io/blog/configuration-best-practice/</link>
      <pubDate>Wed, 14 Jun 2017 20:03:09 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/configuration-best-practice/</guid>
      <description>
        
        
        &lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;
&lt;p&gt;本文档旨在汇总和强调用户指南、快速开始文档和示例中的最佳实践。该文档会很很活跃并持续更新中。如果你觉得很有用的最佳实践但是本文档中没有包含，欢迎给我们提Pull Request。&lt;/p&gt;
&lt;p&gt;本文已上传到&lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook&#34; title=&#34;kubernetes-handbook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubernetes-handbook&lt;/a&gt;
中的第四章最佳实践章节，本文仅作归档，更新以&lt;a href=&#34;https://github.com/rootsongjc/kubernetes-handbook&#34; title=&#34;kubernetes-handbook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubernetes-handbook&lt;/a&gt;
为准。&lt;/p&gt;
&lt;h2 id=&#34;通用配置建议&#34;&gt;通用配置建议&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;定义配置文件的时候，指定最新的稳定API版本（目前是V1）。&lt;/li&gt;
&lt;li&gt;在配置文件push到集群之前应该保存在版本控制系统中。这样当需要的时候能够快速回滚，必要的时候也可以快速的创建集群。&lt;/li&gt;
&lt;li&gt;使用YAML格式而不是JSON格式的配置文件。在大多数场景下它们都可以作为数据交换格式，但是YAML格式比起JSON更易读和配置。&lt;/li&gt;
&lt;li&gt;尽量将相关的对象放在同一个配置文件里。这样比分成多个文件更容易管理。参考&lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/examples/guestbook/all-in-one/guestbook-all-in-one.yaml&#34; title=&#34;guestbook-all-in-one.yaml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;guestbook-all-in-one.yaml&lt;/a&gt;
文件中的配置（注意，尽管你可以在使用&lt;code&gt;kubectl&lt;/code&gt;命令时指定配置文件目录，你也可以在配置文件目录下执行&lt;code&gt;kubectl create&lt;/code&gt;——查看下面的详细信息）。&lt;/li&gt;
&lt;li&gt;为了简化和最小化配置，也为了防止错误发生，不要指定不必要的默认配置。例如，省略掉&lt;code&gt;ReplicationController&lt;/code&gt;的selector和label，如果你希望它们跟&lt;code&gt;podTemplate&lt;/code&gt;中的label一样的话，因为那些配置默认是&lt;code&gt;podTemplate&lt;/code&gt;的label产生的。更多信息请查看 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/examples/guestbook/&#34; title=&#34;guestbook app&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;guestbook app&lt;/a&gt;
 的yaml文件和 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/examples/guestbook/frontend-deployment.yaml&#34; title=&#34;examples&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;examples&lt;/a&gt;
 。&lt;/li&gt;
&lt;li&gt;将资源对象的描述放在一个annotation中可以更好的内省。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;裸奔的pods-vs-replication-controllers和-jobs&#34;&gt;裸奔的Pods vs Replication Controllers和 Jobs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;如果有其他方式替代“裸奔的pod”（如没有绑定到&lt;a href=&#34;https://kubernetes.io/docs/user-guide/replication-controller&#34; title=&#34;replication controller &#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;replication controller &lt;/a&gt;
上的pod），那么就使用其他选择。在node节点出现故障时，裸奔的pod不会被重新调度。Replication Controller总是会重新创建pod，除了明确指定了&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy&#34; title=&#34;&amp;lt;code&amp;gt;restartPolicy: Never&amp;lt;/code&amp;gt;&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;restartPolicy: Never&lt;/code&gt;&lt;/a&gt;
 的场景。&lt;a href=&#34;https://kubernetes.io/docs/concepts/jobs/run-to-completion-finite-workloads/&#34; title=&#34;Job&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Job&lt;/a&gt;
 也许是比较合适的选择。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;services&#34;&gt;Services&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;通常最好在创建相关的&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/&#34; title=&#34;replication controllers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;replication controllers&lt;/a&gt;
之前先创建&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/&#34; title=&#34;service&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;service&lt;/a&gt;
（没有这个必要吧？）你也可以在创建Replication Controller的时候不指定replica数量（默认是1），创建service后，在通过Replication Controller来扩容。这样可以在扩容很多个replica之前先确认pod是正常的。&lt;/li&gt;
&lt;li&gt;除非时分必要的情况下（如运行一个node daemon），不要使用&lt;code&gt;hostPort&lt;/code&gt;（用来指定暴露在主机上的端口号）。当你给Pod绑定了一个&lt;code&gt;hostPort&lt;/code&gt;，该pod可被调度到的主机的受限了，因为端口冲突。如果是为了调试目的来通过端口访问的话，你可以使用 &lt;a href=&#34;https://kubernetes.io/docs/tasks/access-kubernetes-api/http-proxy-access-api/&#34; title=&#34;kubectl proxy and apiserver proxy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubectl proxy and apiserver proxy&lt;/a&gt;
 或者 &lt;a href=&#34;https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/&#34; title=&#34;kubectl port-forward&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubectl port-forward&lt;/a&gt;
。你可使用 Service 来对外暴露服务。如果你确实需要将pod的端口暴露到主机上，考虑使用 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/services/#type-nodeport&#34; title=&#34;NodePort&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NodePort&lt;/a&gt;
 service。&lt;/li&gt;
&lt;li&gt;跟&lt;code&gt;hostPort&lt;/code&gt;一样的原因，避免使用 &lt;code&gt;hostNetwork&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;如果你不需要kube-proxy的负载均衡的话，可以考虑使用使用&lt;a href=&#34;https://kubernetes.io/docs/user-guide/services/#headless-services&#34; title=&#34;headless services&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;headless services&lt;/a&gt;
。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;使用label&#34;&gt;使用Label&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;定义 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/labels/&#34; title=&#34;labels&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;labels&lt;/a&gt;
 来指定应用或Deployment的 &lt;strong&gt;semantic attributes&lt;/strong&gt; 。例如，不是将label附加到一组pod来显式表示某些服务（例如，&lt;code&gt;service:myservice&lt;/code&gt;），或者显式地表示管理pod的replication controller（例如，&lt;code&gt;controller:mycontroller&lt;/code&gt;），附加label应该是标示语义属性的标签， 例如&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-ini&#34; data-lang=&#34;ini&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;{app:myapp,tier:frontend,phase:test,deployment:v3}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;这将允许您选择适合上下文的对象组——例如，所有的”tier:frontend“pod的服务或app是“myapp”的所有“测试”阶段组件。 有关此方法的示例，请参阅&lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/examples/guestbook/&#34; title=&#34;guestbook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;guestbook&lt;/a&gt;
应用程序。可以通过简单地从其service的选择器中省略特定于发行版本的标签，而不是更新服务的选择器来完全匹配replication controller的选择器，来实现跨越多个部署的服务，例如滚动更新。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;为了滚动升级的方便，在Replication Controller的名字中包含版本信息，例如作为名字的后缀。设置一个&lt;code&gt;version&lt;/code&gt;标签页是很有用的。滚动更新创建一个新的controller而不是修改现有的controller。因此，version含混不清的controller名字就可能带来问题。查看&lt;a href=&#34;https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/&#34; title=&#34;Rolling Update Replication Controller&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rolling Update Replication Controller&lt;/a&gt;
文档获取更多关于滚动升级命令的信息。&lt;/p&gt;
&lt;p&gt;注意 &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/&#34; title=&#34;Deployment&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deployment&lt;/a&gt;
 对象不需要再管理 replication controller 的版本名。Deployment 中描述了对象的期望状态，如果对spec的更改被应用了话，Deployment controller 会以控制的速率来更改实际状态到期望状态。（Deployment目前是 &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-groups&#34; title=&#34;&amp;lt;code&amp;gt;extensions&amp;lt;/code&amp;gt; API Group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;extensions&lt;/code&gt; API Group&lt;/a&gt;
的一部分）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;利用label做调试。因为Kubernetes replication controller和service使用label来匹配pods，这允许你通过移除pod中的label的方式将其从一个controller或者service中移除，原来的controller会创建一个新的pod来取代移除的pod。这是一个很有用的方式，帮你在一个隔离的环境中调试之前的“活着的” pod。查看 &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/&#34; title=&#34;&amp;lt;code&amp;gt;kubectl label&amp;lt;/code&amp;gt;&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;kubectl label&lt;/code&gt;&lt;/a&gt;
 命令。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;容器镜像&#34;&gt;容器镜像&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/containers/images/&#34; title=&#34;默认容器镜像拉取策略&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;默认容器镜像拉取策略&lt;/a&gt;
 是 &lt;code&gt;IfNotPresent&lt;/code&gt;, 当本地已存在该镜像的时候 &lt;a href=&#34;https://kubernetes.io/docs/admin/kubelet/&#34; title=&#34;Kubelet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubelet&lt;/a&gt;
 不会再从镜像仓库拉取。如果你希望总是从镜像仓库中拉取镜像的话，在yaml文件中指定镜像拉取策略为&lt;code&gt;Always&lt;/code&gt;（ &lt;code&gt;imagePullPolicy: Always&lt;/code&gt;）或者指定镜像的tag为 &lt;code&gt;:latest&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;如果你没有将镜像标签指定为&lt;code&gt;:latest&lt;/code&gt;，例如指定为&lt;code&gt;myimage:v1&lt;/code&gt;，当该标签的镜像进行了更新，kubelet也不会拉取该镜像。你可以在每次镜像更新后都生成一个新的tag（例如&lt;code&gt;myimage:v2&lt;/code&gt;），在配置文件中明确指定该版本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在生产环境下部署容器应该尽量避免使用&lt;code&gt;:latest&lt;/code&gt;标签，因为这样很难追溯到底运行的是哪个版本的容器和回滚。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;使用kubectl&#34;&gt;使用kubectl&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;尽量使用 &lt;code&gt;kubectl create -f &amp;lt;directory&amp;gt;&lt;/code&gt;  。kubeclt会自动查找该目录下的所有后缀名为&lt;code&gt;.yaml&lt;/code&gt;、&lt;code&gt;.yml&lt;/code&gt;和&lt;code&gt;.json&lt;/code&gt;文件并将它们传递给&lt;code&gt;create&lt;/code&gt;命令。&lt;/li&gt;
&lt;li&gt;使用 &lt;code&gt;kubectl delete&lt;/code&gt; 而不是 &lt;code&gt;stop&lt;/code&gt;. &lt;code&gt;Delete&lt;/code&gt; 是 &lt;code&gt;stop&lt;/code&gt;的超集，&lt;code&gt;stop&lt;/code&gt; 已经被弃用。&lt;/li&gt;
&lt;li&gt;使用 kubectl bulk 操作（通过文件或者label）来get和delete。查看&lt;a href=&#34;https://kubernetes.io/docs/user-guide/labels/#label-selectors&#34; title=&#34;label selectors &#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;label selectors &lt;/a&gt;
和 &lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#using-labels-effectively&#34; title=&#34;using labels effectively&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;using labels effectively&lt;/a&gt;
。&lt;/li&gt;
&lt;li&gt;使用 &lt;code&gt;kubectl run&lt;/code&gt; 和 &lt;code&gt;expose&lt;/code&gt; 命令快速创建直有耽搁容器的Deployment。查看 &lt;a href=&#34;https://kubernetes.io/docs/user-guide/quick-start/&#34; title=&#34;quick start guide&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quick start guide&lt;/a&gt;
中的示例。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/overview/&#34; title=&#34;Configuration Best Practices&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Configuration Best Practices&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>使用 Kubernetes 进行分布式负载测试</title>
      <link>https://jimmysong.io/blog/distributed-load-testing-using-kubernetes/</link>
      <pubDate>Mon, 24 Apr 2017 21:32:52 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/distributed-load-testing-using-kubernetes/</guid>
      <description>
        
        
        &lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;
&lt;p&gt;本示例来自 &lt;a href=&#34;https://github.com/rootsongjc/distributed-load-testing-using-kubernetes&#34; title=&#34;GitHub - distributed-load-testing-using-kubernetes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub - distributed-load-testing-using-kubernetes&lt;/a&gt;
。&lt;/p&gt;
&lt;p&gt;该教程描述如何在&lt;a href=&#34;http://kubernetes.io&#34; title=&#34;Kubernetes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes&lt;/a&gt;
中进行分布式负载均衡测试，包括一个web应用、docker镜像和Kubernetes controllers/services。更多资料请查看&lt;a href=&#34;http://cloud.google.com/solutions/distributed-load-testing-using-kubernetes&#34; title=&#34;Distributed Load Testing Using Kubernetes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Distributed Load Testing Using Kubernetes&lt;/a&gt;
 。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意：该测试是在我自己本地搭建的kubernetes集群上测试的，不需要使用Google Cloud Platform。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;准备&#34;&gt;准备&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;不需要GCE及其他组件，你只需要有一个kubernetes集群即可。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;部署web应用&#34;&gt;部署Web应用&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;sample-webapp&lt;/code&gt; 目录下包含一个简单的web测试应用。我们将其构建为docker镜像，在kubernetes中运行。你可以自己构建，也可以直接用这个我构建好的镜像&lt;code&gt;index.tenxcloud.com/jimmy/k8s-sample-webapp:latest&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;在kubernetes上部署sample-webapp。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ &lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; kubernetes-config
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl create -f sample-webapp-controller.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl create -f kubectl create -f sample-webapp-service.yaml
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;部署locust的controller和service&#34;&gt;部署Locust的Controller和Service&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;locust-master&lt;/code&gt;和&lt;code&gt;locust-work&lt;/code&gt;使用同样的docker镜像，修改cotnroller中&lt;code&gt;spec.template.spec.containers.env&lt;/code&gt;字段中的value为你&lt;code&gt;sample-webapp&lt;/code&gt; service的名字。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TARGET_HOST&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;http://sample-webapp:8000&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;创建controller-docker镜像可选&#34;&gt;创建Controller Docker镜像（可选）&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;locust-master&lt;/code&gt;和&lt;code&gt;locust-work&lt;/code&gt; controller使用的都是&lt;code&gt;locust-tasks&lt;/code&gt; docker镜像。你可以直接下载，也可以自己编译。自己编译大概要花几分钟时间，镜像大小为820M。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ docker build -t index.tenxcloud.com/jimmy/locust-tasks:latest .
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ docker push index.tenxcloud.com/jimmy/locust-tasks:latest
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：我使用的是时速云的镜像仓库。&lt;/p&gt;
&lt;p&gt;每个controller的yaml的&lt;code&gt;spec.template.spec.containers.image&lt;/code&gt; 字段指定的是我的镜像：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;index.tenxcloud.com/jimmy/locust-tasks:latest&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;部署locust-master&#34;&gt;部署locust-master&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl create -f locust-master-controller.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl create -f locust-master-service.yaml
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;部署locust-worker&#34;&gt;部署locust-worker&lt;/h3&gt;
&lt;p&gt;Now deploy &lt;code&gt;locust-worker-controller&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl create -f locust-worker-controller.yaml
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;你可以很轻易的给work扩容，通过命令行方式：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl scale --replicas&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;20&lt;/span&gt; replicationcontrollers locust-worker
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;当然你也可以通过WebUI：Dashboard - Workloads - Replication Controllers - &lt;strong&gt;ServiceName&lt;/strong&gt; - Scale来扩容。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/distributed-load-testing-using-kubernetes/dashbaord-scale.jpg&#34; data-img=&#34;/blog/distributed-load-testing-using-kubernetes/dashbaord-scale.jpg&#34; data-width=&#34;3268&#34; data-height=&#34;1896&#34; alt=&#34;image&#34; data-caption=&#34;Dashboard&#34;&gt;
    
  
  &lt;figcaption&gt;Dashboard&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;配置traefik&#34;&gt;配置Traefik&lt;/h3&gt;
&lt;p&gt;参考&lt;a href=&#34;https://jimmysong.io/posts/traefik-ingress-installation/&#34; title=&#34;kubernetes的traefik ingress安装&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubernetes的traefik ingress安装&lt;/a&gt;
，在&lt;code&gt;ingress.yaml&lt;/code&gt;中加入如下配置：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Yaml&#34; data-lang=&#34;Yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;host&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;traefik.locust.io&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;http&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;paths&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;backend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;serviceName&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;locust-master&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;servicePort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8089&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;然后执行&lt;code&gt;kubectl replace -f ingress.yaml&lt;/code&gt;即可更新traefik。&lt;/p&gt;
&lt;p&gt;通过Traefik的dashboard就可以看到刚增加的&lt;code&gt;traefik.locust.io&lt;/code&gt;节点。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/distributed-load-testing-using-kubernetes/traefik-dashboard-locust.jpg&#34; data-img=&#34;/blog/distributed-load-testing-using-kubernetes/traefik-dashboard-locust.jpg&#34; data-width=&#34;2300&#34; data-height=&#34;1898&#34; alt=&#34;image&#34; data-caption=&#34;Traefik dashboard&#34;&gt;
    
  
  &lt;figcaption&gt;Traefik dashboard&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;执行测试&#34;&gt;执行测试&lt;/h2&gt;
&lt;p&gt;打开&lt;code&gt;http://traefik.locust.io&lt;/code&gt;页面，点击&lt;code&gt;Edit&lt;/code&gt;输入伪造的用户数和用户每秒发送的请求个数，点击&lt;code&gt;Start Swarming&lt;/code&gt;就可以开始测试了。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/distributed-load-testing-using-kubernetes/locust-start-swarming.jpg&#34; data-img=&#34;/blog/distributed-load-testing-using-kubernetes/locust-start-swarming.jpg&#34; data-width=&#34;2050&#34; data-height=&#34;1166&#34; alt=&#34;image&#34; data-caption=&#34;启动 locust&#34;&gt;
    
  
  &lt;figcaption&gt;启动 locust&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在测试过程中调整&lt;code&gt;sample-webapp&lt;/code&gt;的pod个数（默认设置了1个pod），观察pod的负载变化情况。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/distributed-load-testing-using-kubernetes/sample-webapp-rc.jpg&#34; data-img=&#34;/blog/distributed-load-testing-using-kubernetes/sample-webapp-rc.jpg&#34; data-width=&#34;3252&#34; data-height=&#34;1906&#34; alt=&#34;image&#34; data-caption=&#34;示例 Web 应用&#34;&gt;
    
  
  &lt;figcaption&gt;示例 Web 应用&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;从一段时间的观察中可以看到负载被平均分配给了3个pod。&lt;/p&gt;
&lt;p&gt;在locust的页面中可以实时观察也可以下载测试结果。&lt;/p&gt;
&lt;p&gt;&lt;figure class=&#34;mx-auto text-center&#34;&gt;
  
  
  
  
    
    &lt;img src=&#34;https://jimmysong.io/blog/distributed-load-testing-using-kubernetes/locust-dashboard.jpg&#34; data-img=&#34;/blog/distributed-load-testing-using-kubernetes/locust-dashboard.jpg&#34; data-width=&#34;2086&#34; data-height=&#34;784&#34; alt=&#34;image&#34; data-caption=&#34;Locust dashboard&#34;&gt;
    
  
  &lt;figcaption&gt;Locust dashboard&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/solutions/distributed-load-testing-using-kubernetes&#34; title=&#34;Distributed Load Testing Using Kubernetes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Distributed Load Testing Using Kubernetes&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.csdn.net/article/2015-07-07/2825155&#34; title=&#34;运用Kubernetes进行分布式负载测试&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;运用Kubernetes进行分布式负载测试&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Kubernetes中的IP和服务发现体系</title>
      <link>https://jimmysong.io/blog/ip-and-service-discovry-in-kubernetes/</link>
      <pubDate>Mon, 24 Apr 2017 16:11:16 +0800</pubDate>
      
      <guid>https://jimmysong.io/blog/ip-and-service-discovry-in-kubernetes/</guid>
      <description>
        
        
        &lt;h2 id=&#34;cluster-ip&#34;&gt;Cluster IP&lt;/h2&gt;
&lt;p&gt;即Service的IP，通常在集群内部使用Service Name来访问服务，用户不需要知道该IP地址，kubedns会自动根据service name解析到服务的IP地址，将流量分发给Pod。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Service Name才是对外暴露服务的关键。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在kubeapi的配置中指定该地址范围。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;默认配置&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-ini&#34; data-lang=&#34;ini&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;--service-cluster-ip-range&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;10.254.0.0/16&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;--service-node-port-range&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;30000-32767&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;pod-ip&#34;&gt;Pod IP&lt;/h2&gt;
&lt;p&gt;通过配置flannel的&lt;code&gt;network&lt;/code&gt;和&lt;code&gt;subnet&lt;/code&gt;来实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;默认配置&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-ini&#34; data-lang=&#34;ini&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;FLANNEL_NETWORK&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;172.30.0.0/16&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;na&#34;&gt;FLANNEL_SUBNET&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;172.30.46.1/24&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Pod的IP地址&lt;!-- raw HTML omitted --&gt;不固定&lt;!-- raw HTML omitted --&gt;，当pod重启时IP地址会变化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;该IP地址也是用户无需关心的。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;但是Flannel会在本地生成相应IP段的虚拟网卡，为了防止和集群中的其他IP地址冲突，需要规划IP段。&lt;/p&gt;
&lt;h2 id=&#34;主机node-ip&#34;&gt;主机/Node IP&lt;/h2&gt;
&lt;p&gt;物理机的IP地址，即kubernetes管理的物理机的IP地址。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl get nodes
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME           STATUS    AGE       VERSION
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;172.20.0.113   Ready     12d       v1.6.0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;172.20.0.114   Ready     12d       v1.6.0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;172.20.0.115   Ready     12d       v1.6.0
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;服务发现&#34;&gt;服务发现&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;集群内部的服务发现&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;通过DNS即可发现，kubends是kubernetes的一个插件，不同服务之间可以直接使用service name访问。&lt;/p&gt;
&lt;p&gt;通过&lt;code&gt;sericename:port&lt;/code&gt;即可调用服务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;服务外部的服务发现&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;通过Ingress来实现，我们是用的&lt;strong&gt;Traefik&lt;/strong&gt;来实现。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/posts/kubernetes-ingress-resource/&#34; title=&#34;Ingress解析&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ingress解析&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/posts/traefik-ingress-installation/&#34; title=&#34;Kubernetes Traefik Ingress安装试用&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes Traefik Ingress安装试用&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
