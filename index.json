[{"content":"Istio 在刚开源的时候就定义了几十个 CRD，其中用于流量治理的有 RouteRule、DestinationPolicy、EgressRule 等，后来推出了 v1alpha3 API 使用 VirtualService 和 DestinationRule 等取代了之前的 API。但是这些资源对象的定义，并不像 Kubernetes 中那么直观，反而会有些难以理解，比如 VirtualService，只看名字你可能认为只是第一个了一个“虚拟的服务”，但实际并非如此。\n本文将为你通过与实际的交通做类比，直观简要的介绍 Istio 中的两个核心的用于流量治理的对象——VirtualService 和 DestinationRule。\n流量 vs 交通 很多刚接触 Istio 的人可能对 VirtualService 和 DestinationRule 和两个资源对象不是很理解，如果我们将环境仅限定于 Kubernetes 集群，我们可以将路由比喻成现实世界中的交通，有很多车辆在道路上行驶，这就是流量。DestinationRule 相当于开辟了道路/路径/车道，确保了两点之间可达并控制车道的数量、宽度等。而 VirtualService 就像是红绿灯和道路标线，指挥车辆向哪行驶和如何行驶。也就是说如果你只定义了 DR 将不会对流量产生任何影响，因为你没有指挥流量怎么走，所以你必须定义 VirtualService 才可以控制流量的走向。\n一句话来概括：DestinationRule 打通了两地之间的通路，犹如修路架桥通隧道，同时控制车道设路障；VirtualService 做车辆指挥调度。\n请看下面这张将流量与交通的对比图，可以帮助你更直观的理解这种比喻。\n流量与交通对比图 流量治理 下面列举了你分别可以在这两个资源对象上做的流量治理行为。\nVirtualService\n路由：金丝雀发布、基于用户身、URI、Header 等匹配路由等； 错误注入：HTTP 错误代码注入、HTTP 延时注入； 流量切分：基于百分比的流量切分路由； 流量镜像：将一定百分比的流量镜像发送到其他集群； 超时：设置超时时间，超过设置的时间请求将失败； 重试：设置重试策略，如触发条件、重试次数、间隔时间等； DestinationRule\n负载均衡：设置负载均衡策略，如简单负载均衡、区域感知负载均衡、区域权重负载均衡； 熔断（Circuit Breaking）：通过异常点检测（Outlier Detection）和连接池设置将异常节点从负载均衡池中剔除； 总结 本文将抽象的流量治理与现实中的交通管理相类比，帮助你更直观的理解 Istio 中的流量管理对象 VirtualService 和 DestinationRule。同时介绍了基于它们可以进行的流量治理功能。VirtualService 主要用于设置路由规则，而服务弹性（超时、重试、熔断等）需要靠它和 DestinationRule 来共同维持。\n参考 Introducing the Istio v1alpha3 routing API - istio.io Traffic Management - istio.io ","relpermalink":"/blog/understand-istio-vs-and-dr/","summary":"本文将 Istio 中的流量治理与现实世界中的交通管理类比，可以帮助你快速理解 VirtualService 和 DestinationRule 设置的功能。","title":"如何理解 Istio 中的 VirtualService 和 DestinationRule？"},{"content":"开始之前 截止到撰写本文时 Istio 的最高版本为 1.15.2，1.13 版本的官方支持已经结束。请对照 Istio 文档中的发布状态描述 确定是否需要对 Istio 进行升级。\nIstio 官网上给出了升级 Istio 的几种方式 ：\n金丝雀升级 原地升级 使用 Helm 升级 但实际上，为了减少在升级时对网格内业务的影响，建议在升级 Istio 的时候，使用 canary upgrade ，它比 in-place upgrade 更加安全，而且支持回滚。使用 canary upgrade 支持跨越两个小版本，而 in-place upgrade 必须一个一个小版本的升级。不论使用哪种方式，其中 Ingress Gateway 都是 in-place upgrade 的。\nIstio 官方文档 对升级的步骤描述的不是很详细，本文是对官方文档的一个补充，在升级完成后有两个注意事项：\n为需要自动 sidecar 注入的 namespace 打上对应的 label； 删除原有的 validatingwebhookconfiguration 并添加新的； 下面是详细的升级步骤。\n升级步骤 使用的是以下命令安装的 canary 版本：\n# 将新版本的 revision 命名为 canary istioctl install --set revision=canary # 取消原先自动注入 sidecar 的 namespace 中的 label 并设置新的 label，这样该 namespace 就可以注入 canary 版本对应的 sidecar kubectl label namespace test-ns istio-injection- istio.io/rev=canary # 重启数据平面中的工作负载，将完成新版本的 sidecar 自动注入 kubectl rollout restart deployment -n test-ns 注意在升级完成后，为新的 namespace 开启 sidecar 自动注入时，需要给 namespace 打上安装 canary Istio 时候设置的 label，执行下面的命令：\nkubectl label namespace new-ns istio-injection- istio.io/rev=canary Istio 升级完成后的注意事项 在升级完成后，还有一些注意事项。例如如果你已经为其他 namespace 打上了 sidecar 自动注入的 label，请一定要将它删掉，并将 label 设置为 istio.io/rev=canary，因为可以保证在 pod 中注入新版被 sidecar，并且连接到新版的 Istiod。\n另外，你需要把最早安装 Istio 时设置的 ValidatingWebhookConfiguration 删掉，执行下面的命令：\nkubectl delete validatingwebhookconfiguration istiod-default-validator 关于 ValidatingWebhookConfiguration 在你安装新版本的 Istio 的时候，会自动创建一个名为 istio-validator-canary-istio-system 的 ValidatingWebhookConfiguration，该配置的目的是在创建和更新 Istio CR 的时候，先检测所有连接的 Istiod 是否有效。关于动态准入控制的详细描述请见 Kubernetes 文档 。 因为在安装新版本 Istio 的时候，安装了新的 istio-validator-canary-istio-system。如果你不将旧的删除话，你在创建 Istio CR 的时候将会看到如下错误。\nError from server (InternalError): error when creating \u0026#34;samples/bookinfo/networking/bookinfo-gateway.yaml\u0026#34;: Internal error occurred: failed calling webhook \u0026#34;validation.istio.io\u0026#34;: failed to call webhook: Post \u0026#34;https://istiod.istio-system.svc:443/validate?timeout=10s\u0026#34;: service \u0026#34;istiod\u0026#34; not found 以上内容在 Istio 的官方文档中里并没有说明，但是在 Istio Issue-36526 中有提及。\n参考 动态准入控制 - kubernetes.io Istio Supported Releases - istio.io Canary Upgrades - istio.io ","relpermalink":"/blog/istio-canary-upgrade/","summary":"本文详述了使用金丝雀升级 Istio 的步骤及升级后的注意事项。","title":"如何不停机升级 Istio？"},{"content":"国庆假期除了去浙江和安徽玩了一圈之外，还抽空翻译了一本电子书。本书译自 Solving the Bottom Turtle — a SPIFFE Way to Establish Trust in Your Infrastructure via Universal Identity。你可以选择在线阅读（推荐）：https://lib.jimmysong.io/spiffe/ ，或者下载本书中文版 PDF（关注「几米宋」公众号在后台回复 SPIFFE 即可获得下载链接）。\n目录 SPIFFE 的历史和动机 收益 身份背后的通用概念 SPIFFE 和 SPIRE 概念介绍 开始前的准备 设计一个 SPIRE 部署 与其他系统集成 使用 SPIFFE 身份通知授权 SPIFFE 与其他安全技术对比 从业者故事 关于本书 本书介绍了服务身份的 SPIFFE 标准，以及 SPIFFE 的参考实现 SPIRE。这些项目为现代异构基础设施提供了一个统一的身份控制平面。这两个项目都是开源的，是云原生计算基金会（CNCF）的一部分。\n随着企业发展他们的应用架构以充分利用新的基础设施技术，他们的安全模式也必须不断发展。软件已经从一个盒子上的单片机发展到几十或几百个紧密联系的微服务，这些微服务可能分布在公共云或私人数据中心的数千个虚拟机上。在这个新的基础设施世界里，SPIFFE 和 SPIRE 帮助保持系统的安全。\n本书努力提炼 SPIFFE 和 SPIRE 的最重要的专家的经验，以提供对身份问题的深刻理解，帮助你解决这个问题。通过这些项目，开发和运维可以使用新的基础设施技术构建软件，同时让安全团队从昂贵和耗时的人工安全流程中解脱出来。\n关于零号乌龟 访问控制、秘密管理和身份都是相互依赖的。大规模地管理秘密需要有效的访问控制；实施访问控制需要身份；证明身份需要拥有一个秘密。保护一个秘密需要想出一些办法来保护另一个秘密，这就需要保护那个秘密，以此类推。\n这让人想起一个著名的轶事：一个女人打断了一位哲学家的讲座，告诉他世界是在乌龟的背上。当哲学家问她乌龟靠的是什么时，她说：“还是乌龟！\u0026#34;。找到底层的乌龟，即所有其他安全所依赖的坚实基础，是 SPIFFE 和 SPIRE 项目的目标。\n本书封面上的 “零号乌龟” 就是这只底层乌龟。零代表了数据中心和云计算的安全基础。零号是值得信赖的，愉快地支持所有其他的乌龟。\nSPIFFE 和 SPIRE 是帮助你为你的组织找到底层乌龟的项目。通过这本书中的工具，我们希望你也能为 “底层乌龟” 找到一个家。\n","relpermalink":"/notice/spiffe-book/","summary":"目前能找到的关于 SPIFEE 最完整系统的资料了，我特地将它翻译了一下。","title":"《零信任的基石：使用 SPIFFE 为基础设施创建通用身份》翻译电子书分享"},{"content":"今天 Istio 社区推出了 Ambient Mesh ，这是一种新的 Istio 数据平面模式，旨在简化操作、扩大应用兼容性并降低基础设施成本。用户可以选择将 Ambient Mesh 集成到其基础设施的网格数据平面，放弃 sidecar 代理，同时保持 Istio 的零信任安全、遥测和流量管理等核心功能。该模式目前还是预览版，Istio 社区准备在未来几个月内将其推向生产就绪。\nAmbient Mesh 推出的消息对于社区来说可能显得有些突然，但其实关于 sidecar 模式对于资源的消耗过大，以及简化服务网格的呼声在社区里已经存在很久了，Google 从多年前就在寻求 HBONE（HTTP-Based Overlay Network Environment，基于 HTTP 的重叠网络环境）解决方案，还有社区提出的多种 sidecar 部署模式 、proxyless 模式 等都是为了解决这个问题。\n关于 Ambient 模式的看法 本文我将谈谈对 ambient 模式的几点看法：\n关于 Ambient Mesh 的命名：我觉得叫做 Ambient Mode 会更好，有些接触 Istio 的初学者可能会觉得它是一种全新的不同于 Istio 的 service mesh；另外关于这个模式的中文翻译，如果直接翻译成“环境网格”似乎让人很难理解，我还想到了其他词汇，如“外围”、“氛围”、“周围”、“环绕”、”情景”等，没有一个汉语词汇可以准确表达这个 ambient 的含义，因为相对于 sidecar 模式，ambient 模式对应用程序 pod 没有侵入性，暂且将其称之为外围模式。 Ambient Mode 的本质：它的本质是分离 sidecar proxy（Envoy）中的 L4 和 L7 功能，让一部分仅需要安全功能的用户可以最小阻力（低资源消耗、运维成本）地使用 Istio service mesh。 Ambient Mode 的意义：因为它 sidecar 模式兼容，用户在采纳 Ambient Mode 获得了 mTLS 和有限的可观察性及 TPC 路由等 L4 功能，之后可以更方便的过度到 sidecar mode 以获得完全的 L7 功能。这给用户采纳 Istio 提供了更多模式选择，优化了 Istio 采纳路径。 Ambient Mode 的坏处：Proxyless、sidecar、ambient 模式，使得 Istio 越来越复杂，用户理解起来更加费力；控制平面为了支持多种数据平面部署模式，其实现将更加复杂。 与其他 service mesh 的关系：有的 service mesh 从原先的 per-proxy per-node 模式转变为 sidecar mode，如 Linkerd；还有的从 CNI 做到 service mesh，如 Cilium 使用 per-proxy per-node 模式；如今 Istio 在 sidecar mode 的基础上增加了 ambient mode，这也是目前唯一同时支持这两种部署模式的 service mesh，为用户提供了多样的选择。 安全问题：虽然 Istio 服务网格 ambient 模式安全详解 说明了ambient 模式的设计主旨是为了将应用程序与数据平面分离，让安全覆盖层的组件（ztunnel）处于类似于 CNI 的网格底层，考虑到 ztunnel 有限的 L4 攻击面，该模式的安全风险是可以接受的；但是，ztunnel 作为 DaemonSet 部署在每个节点上，需要处理和分发调度到该节点上的所有 pod 的证书来建立 mTLS 连接，一旦 一个 ztunnel 被攻破，它的爆炸半径确实是大于一个 sidecar，安全详解的博客中说 Envoy 的 CVE 问题会影响所有 sidecar，升级 sidecar 也会带来很大的运营成本，所以权衡之下选择 ambient 模式，安全问题再次给用户造成了困惑，不过最终选择的权利还是在用户自己。 Ambient 模式的限制 目前 ambient 模式的代码位于 Istio 代码库的 experimental-ambient 分支 ，根据 Matt Klein 和 Louis Ryan 的说法 ，ztunnel 和 Waypoint proxy 是用 Envoy 实现的，其中 ztunnel 是精简后的 Envoy，只负责 L4 功能且继续使用 xDS 协议来控制。但是 ambient 模式依然有很多限制 ，例如：\n不支持 EnvoyFilter； 直接对 Pod IP 而不是 service 的请求在某些情况下将无效； Ambient 模式下的服务无法通过 LoadBalancer 和 NodePort 方式访问，不过你可以部署一个入口网关（未启用 ambient 模式）以从外部访问服务； 不支持 Calico CNI 和 Dataplane V2 CNI； 这里 有安装 ambient 模式的详细环境要求。\n更多 以上就是笔者对 ambient 模式（外围模式）的看法，该模式还处于试验阶段，但绝不是玩具，据信已在某些场景试验过。笔者也将继续追踪该模式的最新进展，请保持关注。也欢迎更多关注 Istio 的朋友加入云原生社区 Istio 讨论群 ，与社区大咖一起探讨，或者在本文下面评论聊聊你的看法。\n","relpermalink":"/blog/istio-ambient-mode/","summary":"本文阐述了笔者对于 Istio 新推出的 ambient mesh（外围模式）的几点看法。","title":"关于 Istio 推出 ambient 数据平面模式的看法"},{"content":"昨天 COSS（Commercial Open Source）公司的创始人也是投资者 Joseph（JJ） Jacks 给我发消息，让我看下他们新发布的这个报告。\nJJ 给我发的报告 我早就知道 JJ 创业搞了一个 COSS 投资公司并创立了一个基金，这次他整理的这个全球 COSS 公司融资数据还是比较全面的。\n什么是 COSS 公司？ COSS 是英文 Commercial Open Source Software（商业化开源软件）的简称，COSS 公司指的依靠这些 COSS 而生存的公司，即如果没有这些软件，这家公司就不会存在，例如：\n如果没有 Kafka 就没有 Confluent； 如果没有 Hadoop 就没有 Cloudera； 如果没有 Spark，就没有 Databricks； 如果没有 Git，就没有 GitLab； 如果没有 Linux，就没有 Red Hat、SUSE 等； COSS 与 SaaS 类公司的区别见下图。\nCOSS 与 SaaS 类公司对比 关于 COSS 公司融资的一些数据 自 2020 年一月至 2022 年 8 月（32 个月）这段时间内全球公开的 COSS 公司融资数据如下：\n500+ 次融资 360+ 家 COSS 初创公司 340+ 个开源项目 230+ 家风投公司参与 总融资 240+ 亿美元 平均每月融资 7.5 亿美元 2021 年 2 月和 2021 年 9 月单月融资规模最大 更多详细数据请阅读博客 Global VC Funding In COSS: $24B+ Raised From Jan 2020 to August 2022 和原始数据表格 。\n中国的开源商业化公司 近两年中国也涌现了一系列的开源商业化（COSS）公司，例如：\n开源软件 公司名称 Apache ShardingSphere SphereEx TDengine 涛思数据 OceanBase OceanBase Apache APISIX API7 Apache Pulsar StreamNative Apache DolphinScheduler 未知 Nebula Graph 悦数科技 Milvus Zilliz 表：中国的 COSS 公司 以上仅列举了部分笔者观察到的 COSS 公司，如有遗漏，欢迎补充。\n总结 在 2020 年之前国内也有不少开源商业化公司，比如 PingCAP、Kylingence、EasyStack 等，2020 年后似乎在全球都有一种 COSS 创业的趋势，国内的开源商业化相对于国外还处于比较早期的阶段，一是有影响力的开源项目太少，二是国内用户尚未对商业化开源软件的付费的习惯，还有一些其他政策和法规问题。\n","relpermalink":"/blog/coss-vc-funding-since-2020/","summary":"近日 COSS 社区发布了自 2020 年 1 月以来的全球风险投资报告，全球共有超过 240 亿美元开源商业化软件公司融资。","title":"自 2020 年以来全球的开源商业化软件融资情况"},{"content":"Istio 是基于容器的云原生技术栈的三大核心技术之一，另外两个是 Kubernetes 和 Knative。其中 Kubernetes 和 Knative 早已支持了 arm64 架构，甚至连 Istio 的数据平面 Envoy 早在 1.16 版本 就已支持 arm64 架构（2020 年 10 月）。随着 Istio 1.15 的发布 ，你可以开箱即用得在 arm64 架构上部署 Istio，不需要自己来编译 arm 架构的镜像。\n在 Istio 1.15 之前如何在 arm 架构上安装 Istio？ Istio 默认使用 Docker Hub 作为生产镜像仓库，Google Container Registry 作为生产和测试仓库。对于 1.14 及以前的版本，Istio 官方的镜像仓库中只有 amd64 架构的镜像，如果你的 Kubernetes 集群是运行在 arm 架构下，在安装 Istio 时会出现出现如下错误：\nexec user process caused: exec format error 这时你需要为 Istio 安装重新指定一个包含 arm64 架构镜像的仓库，在安装 Istio 时执行下面的命令指定该镜像仓库：\n$ istioctl install --set profile=demo --set hub=docker.io/mydockerhub -y 此时要想在 arm64 架构上使用 Istio，你可以使用 Istio 社区中有人为 Istio 单独构建了 arm64 架构的镜像 ，或者自己构建镜像。\nIstio 为了支持 arm 做了哪些工作？ 为了让 Istio 支持 arm，需要将以下二进制文件或者镜像基于 arm 架构编译：\nistioctl：这是最简单的部分，只需要使用 Go 语言的交叉编译即可，Istio 的早期版本就已经支持； pilot：控制平面 Istiod 中运行的镜像； proxyv2：在 Ingress Gateway、Egress Gateway 和 Sidecar 中使用的镜像，通过 Kubernetes mutating webhook 自动注入； Istio 数据平面中的 Envoy 是从 Envoy 官方仓库中 fork 出来的，但是 Envoy 早就支持了 arm64，为什么 Istio 官方还不支持呢？这是因为一方面 Istio 的官方 CI 环境 prow.istio.io 运行在 GKE 上的，而 GKE 上并没有 arm64 架构的环境，所以无法执行测试。直到 2022 年 7 月 GKE 才正式提供 arm64 架构的虚拟机，那时才可以方便的编译和测试 arm64 架构的 Istio，详见 Run your Arm workloads on Google Kubernetes Engine with Tau T2A VMs 。\n注意 Istio 官方仅提供了 amd64 和 arm64 架构的镜像，不支持 arm32。 至于 arm 架构的镜像构建，可以使用 Docker BuildKit 来实现多平台构建，你可以使用下面的命令编译指定 arm 平台架构的镜像：\ndocker buildx build --platform linux/arm64 关于 docker buildx 的详细信息请参考 Docker 文档 。\n你可以像往常一样来安装 Istio，Kubernetes Node 会根据节点的架构自动拉起对应平台架构的镜像。\n","relpermalink":"/blog/istio-arm64-support/","summary":"随着 Istio 1.15 的发布，你可以很方便得在 arm64 架构上部署 Istio。","title":"Istio 1.15 新增对 arm64 架构处理器的支持"},{"content":"今天云原生社区里有好几个人问我 servicemesher.com 网站为什么访问不了了，我想在这里做一个统一的回复。\n通知 简单来说，该 servicemesher.com 归蚂蚁集团所有，2020 年时 ServiceMesher 与云原生社区合并后就该网站已停止更新，所有博客资料都已迁移到了云原生社区 。今天蚂蚁正式停止运营该网站，是时候与开通运营了四年多的 servicemesher.com 网站说再见了，谢谢大家的参与！ 关于 servicemesher.com servicemesher.com 的历史分为以下几个阶段：\n活跃期：2018 年 5 月 - 2021 年 2 月 停止维护：2021 年 3 月至 2022 年 8 月 停止运营：2022 年 8 月 24 日 servicemesher.com 网站建立于 2018 年 5 月，由蚂蚁集团建立（服务器资源、域名均由蚂蚁集团提供）并社区化运营。在该网站运营的近 3 年时间里，共发布了几百篇社区原创或翻译的服务网格和云原生相关博客，为服务网格技术在中国的启蒙发挥了重要作用。同时网站中还发布了第一份完整翻译的《Envoy 中文文档》（现已有更新的中文版本，见云原生社区翻译版 ）、《Kantive 入门》（因为原文基于的 Knative 版本太老，现已过时）、《Istio 服务网格进阶实战》（本书已出版发行，查看详情 ）等翻译或原创的电子书籍。\n随着时间的推移，服务网格技术早已成为云原生技术栈中不可或缺的一部分，因此笔者在 2020 年 5 月成立了云原生社区 ，ServiceMesher 与云原生社区合并，servicemesher.com 网站也于 2021 年 2 月停止维护（见此通知 ，但还可以访问，目前你依然可以在 GitHub 上查看它的代码），所有博客都已迁移到了云原生社区官网 cloudnative.to 。\n欢迎之前参与 ServiceMesher 的朋友和其他云原生技术爱好者关注和参与到云原生社区 中来，谢谢。\n","relpermalink":"/notice/servicemesher-website-halt/","summary":"是时候与开通运营了四年多的 servicemesher.com 网站说再见了。","title":"关于 servicemesher.com 停止运营的通知"},{"content":"前言 这篇回顾文章可能来的有点晚了，之前在云原生社区中国就有好几次有人问我，什么时候可以在 B 站上观看到 IstioCon 2022 的录像（IstioCon 组委会早已将录像上传到了 YouTube，但是国内有些人可能无法访问 YouTube），正好最近我跟会务人员要到了会议录像，将其全部上传到了 B 站。\n视频回放与 PPT B 站观看视频回放 IstioCon 2022 Sessions 大部分内容已上传到 B 站，除了以下三个，有人已经提前上传到了 B 站。\n有三个视频撞车了 PPT 可以直接通过上面的链接选择你感兴趣的话题下载。\n总结 IstioCon 2022 今年是第二届了，于 4 月 25 日到 29 日在线上举行，笔者作为 IstioCon 的中文场组织者之一参与了本届活动中文场开场及圆桌讨论环节，我们讨论的议题是《Istio 的开源生态展望》，下面是论坛嘉宾。\n中文场开场演讲：Istio 的开源生态展望论坛嘉宾 最近两年来，围绕 Istio，在中国涌现出了几个代表性的开源项目，比如腾讯开源的 Aeraki、DaoCloud 开源的 Merbridge 还有网易开源的 Slime 等。基于 Istio 开发的扩展，我看到的基本都是来自中国，这一点可以说是中国特色。\n本次 IstioCon 的关键词如下：\n零信任 多集群 Proxyless eBPF Gateway 安全 同时，Google Cloud 的 VP Eric Brewer 宣布了一个大消息，将 Istio 捐献给 CNCF，一旦成功，那么由 Google 主导的三个开源项目 Kubernetes、Istio、Knative 将成为 CNCF 中容器编排、服务网格、Serverless 等 Kubernetes 技术栈的三驾马车。\nKubernetes 技术栈的三驾马车 关于 Istio 你应该了解 Istio 只适用于特别场景与规模 不会一口吃出一个胖子，要一个一个版本的渐进式提高 理解 Envoy 对于应用 Istio 特别重要 用户不想再学习另一套 CRD，请考虑在 Istio 之上增加一层抽象 不要低估 Day 2 operation 需要消耗的精力 Istio 在 2022 年的计划 本届大会上还公布了 Istio 在 2022 年的关注点。\n稳定性与重新定义 Istio 的 API 平面 推动当前特性和 API 到稳定版 将 API 配置从 MeshConfig 中移到数据平面中并使其稳定 继续帮助 Kubernetes API 定义与 Istio API 对齐（Kubernetes Gateway API） 继续增强 Telemetry API：增加对使用 OpenTelemetry 等供应商的日志记录的支持，过滤访问日志，以及自定义跟踪服务名称 增强升级与故障排查 推动基于修订标签的升级到稳定版 将 Helm 安装推广到 Beta 版 在 istioctl 中添加更多的分析器，并扩展当前的分析器，包括针对 Kubernetes 集群以外环境的分析器 用户希望用服务网格来排除服务故障，而不是对服务网格进行故障排除 增强可扩展性 Wasm Plugin 自定义授权 增加标准化集成点以加入自定义的 CA 或网关 扩展 Istio 的使用场景 支持 IPv6 和双栈网络（Dual Stack Networking） 支持 ARM 扩展使用 gRPC 的 proxyless Istio 性能增强：增量 xDS、降低 sidecar 延迟 什么是双栈网络？ 双协议栈技术就是指在一台设备上同时启用 IPv4 协议栈和 IPv6 协议栈。 这样的话，这台设备既能和 IPv4 网络通信，又能和 IPv6 网络通信。 如果这台设备是一个路由器，那么这台路由器的不同接口上，分别配置了 IPv4 地址和 IPv6 地址，并很可能分别连接了 IPv4 网络和 IPv6 网络。Kubernetes 中支持 IPv4/IPv6 双协议栈，详见 Kubernetes 文档 。 安全加固 默认尽可能安全 继续完善安全最佳实践文档 推动 distroless 镜像 软件 BOM（Bill of Materials，依赖服务清单） 增加格外的模糊测试 其他增强 用自动化使升级更容易：让 Istio 的升级像其他升级一样工作，发布升级自动化的参考实现 多规模和大规模集群 对于开发者来说，我们已经将每周的工作小组会议合并到。对于开发者来说，我们已经将每周的工作组会议合并，一个在美国，一个在亚太地区。 写在最后 希望 Istio 正式进入 CNCF 的那天早日到来，我也希望能够在社区里看到更多源于 Istio 的终端案例分享，也欢迎大家加入到云原生社区 中来，我们有专门的 Istio 讨论群。\n","relpermalink":"/blog/istiocon-2022-recap/","summary":"本文总结了 IstioCon 2022 并分享了 Istio 社区 2022 年的工作重点，同时公布了 B 站视频链接和 PPT 下载地址。","title":"IstioCon 2022 回顾及录像、PPT 分享"},{"content":" 关于本文 本文根据笔者在 GIAC 深圳 2022 年大会上的的演讲《Beyond Istio OSS —— Istio 的现状及未来》 整理而成，演讲幻灯片见 腾讯文档 。 本文回顾了 Istio 开源近五年来的发展，并展望了 Istio 服务网格的未来方向。本文的主要观点如下：\n因为 Kubernetes、微服务、DevOps 及云原生架构的流行，导致服务网格技术的兴起； Kubernetes 和可编程代理，为 Istio 的出现打下了坚实的基础； 虽然 eBPF 可以加速 Istio 中的透明流量劫持，但无法取代服务网格中的 sidecar； Istio 的未来在于构建基于混合云的零信任网络； Istio 诞生的前夜 2013 年起，随着移动互联网的爆发，企业对应用迭代的效率要求更高，应用程序架构开始从单体转向微服务，DevOps 也开始变得流行。同年随着 Docker 的开源，解决了应用封装和隔离的问题，使得应用在编排系统中调度变得更容易。2014 年 Kubernetes、Spring Boot 开源，Spring 框架开发微服务应用开始流行，在接下来的几年间大批的 RPC 中间件开源项目出现，如 Google 在 2016 年发布 gRPC 1.0，蚂蚁在 2018 年开源 SOFAStack 等，微服务框架百花齐放。为了节约成本，增加开发效率，使应用更具弹性，越来越多的企业正在迁移上云，但这不仅仅是将应用搬到云上那么简单，为了更高效地利用云计算，一套「云原生」方法和理念也呼之欲出。\nIstio 开源时间线 Istio 开源发展时间线如下图所示。\nIstio 开源发展时间线示意图 下面我们来简单回顾下 Istio 开源大事件：\n2016 年 9 月：因为 Envoy 是 Istio 中的重要组成，Istio 的开源时间线应该有 Envoy 一部分。起初 Envoy 在 Lyft 内部仅作为边缘代理，开源前已在 Lyft 内部得到大规模生产验证并受到了 Google 工程师的注意 1，那时候 Google 正打算推出一个服务网格的开源项目。2017 年，Lyft 将 Envoy 捐献给了 CNCF 。 2017 年 5 月：Istio 由 Google、IBM 和 Lyft 联合宣布开源 2。一开始就使用了微服务架构，确定了数据平面和控制平面的组成以及 Sidecar 模式。 2018 年 3 月：Kubernetes 顺利的成为从 CNCF 中第一个毕业的项目，变得越来越「无聊」，基础 API 已经定型，CNCF 正式将服务网格（Service Mesh）写入到了云原生的第二版定义 3 中。笔者当前就职的公司 Tetrate ，也是在那时由 Google Istio 初创团队创业成立的。服务网格在中国开始爆发，ServiceMesher 社区也在蚂蚁集团的支持下成立，在中国布道服务网格技术。 2018 年 7 月：Istio 1.0 发布，号称「生产可用」，Istio 团队重组。 2020 年 3 月：Istio 1.5 发布，架构回归单体，发布周期确定，每三个月发布一个大版本，API 趋于稳定。 2020 年至今：Istio 的发展主要着重于 Day 2 Operation 4、性能优化和扩展性发面，多个围绕 Istio 生态的开源项目开始出现，例如 Slime 、Areaki 、Merbridge 。 为什么 Istio 会在 Kubernetes 之后出现？ 微服务和容器化之后，异构语言使用的增加，服务的数量激增，容器的生命周期变短是导致服务网格出现的根本原因。\n我们先来看下服务从部署在 Kubernetes 到 Istio 中架构的变迁，然后再探讨架构演进过程中 Istio 的需求，下文假定读者已了解 Kubernetes 和 Istio 的架构 。\nKubernetes 到 Istio 的架构改变示意图 从 Kubernetes 到 Istio，概括的讲应用的部署架构有如下特点：\nKubernetes 管理应用的生命周期，具体来说，就是应用的部署和管理（扩缩容、自动恢复、发布策略）；\n基于 Kubernetes 的自动 sidecar 注入，实现了透明流量拦截。先通过 sidecar 代理拦截到微服务间流量，再通过控制平面配置管理微服务的行为。如今服务网格的部署模式也迎来了新的挑战，sidecar 已经不是 Istio 服务网格所必须的，基于 gRPC 的无代理的服务网格 5 也在测试中。\n服务网格将流量管理从 Kubernetes 中解耦，服务网格内部的流量无须 kube-proxy 组件的支持， 通过类似于微服务应用层的抽象，管理服务间的流量，实现安全性和可观察性功能。\n控制平面通过 xDS 协议发放代理配置给数据平面，已实现 xDS 的代理有 Envoy 和蚂蚁开源的 MOSN 。\nKubernetes 集群外部的客户端访问集群内部服务时，原先是通过 Kubernetes Ingress ，在有了 Istio 之后，会通过 Gateway 来访问 6。\nKubernetes 容器编排与可编程代理 Envoy 为 Istio 的出现打下了坚实的基础。\n从上面 Kubernetes 到 Istio 的架构的转变的描述中，我们可以看到为了让开发者最小成本地管理服务间的流量，Istio 需要解决三个问题：\n透明劫持应用间的流量：Istio 开源最初的目标是成为网络基础设施，就像水和电人类的基础设施一样，我们使用水电不需要关心如何取水和发电，只需要打开水龙头，按下开关即可。透明流量劫持对于开发者来说，就像使用水和电，不需要修改应用程序就可以快速使用 Istio 带来的流量管理能力； 代理集群的运维：如何为每个应用注入一个代理，同时高效地管理这些分布式的 sidecar 代理； 可编程代理：代理可以通过 API 动态配置，还要有出色的性能与可扩展性； 以上三个条件对于 Istio 服务网格来说缺一不可，而且，从中我们可以看到，这些要求基本都是对于 sidecar 代理的要求，这个代理的选择将直接影响该项目的走向与成败。为了解决以上三个问题，Istio 选择了 Kubernetes 容器编排和可编程代理 Envoy。\n透明流量劫持 如果你使用的是如 gRPC 这类中间件开发微服务，在程序中集成 SDK 后，SDK 中的拦截器会自动为你拦截流量，如下图所示。\ngRPC 的拦截器示意图 如何让 Kubernetes pod 中的流量都通过代理呢？答案是在每个应用程序 pod 中注入一个代理，与应用共享网络空间，再通过修改 pod 内的流量路径，让所有进出 pod 的流量都经过 sidecar，其架构如下图所示。\nIstio 中的透明流量劫持示意图 从图中我们可以看到其中有一套非常复杂的 iptables 流量劫持逻辑（详见 Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解 ），使用 iptables 的好处是适用于任何 Linux 操作系统。但是这也带来了一些副作用：\nIstio 网格中所有的服务都需要在进出 pod 时都增加了一个网络跳跃点（hop），虽然每次 hop 可能只有两三毫秒，但是随着网格中服务和服务间的依赖增加，这种延迟可能会显著增加，对于那种追求低延迟的服务可能就不适用于服务网格了； 因为 Istio 向数据平面中注入了大量的 sidecar，尤其是当服务数量增大时，控制平面需要下发更多的 Envoy 代理配置到数据平面，这样会使数据平面占用大量的系统内存和网络资源； 针对这两个问题，如何优化服务网格呢？\n使用 proxyless 模式：取消 sidecar 代理，重新回到 SDK； 优化数据平面：减少下发到数据平面的配置的频率和大小； eBPF：使用 eBPF 优化网络劫持； 本文将在后面性能优化 一节讲解这些细节。\nSidecar 运维管理 Istio 是在 Kubernetes 的基础上构建的，它可以利用 Kubernetes 的容器编排和生命周期管理，在 Kubernetes 创建 pod 时，通过准入控制器自动向 pod 中注入 sidecar。\n为了解决 Sidecar 的资源消耗问题，有人为服务网格提出了有四种部署模式，如下图所示。\n服务网格的四种部署模式示意图 下表中详细对比了这四种部署方式，它们各有优劣，具体选择哪种根据实际情况而定。\n模式 内存开销 安全性 故障域 运维 Sidecar 代理 因为为每个 pod 都注入一个代理，所以开销最大。 由于 sidecar 必须与工作负载一起部署，工作负载有可能绕过 sidecar。 Pod 级别隔离，如果有代理出现故障，只影响到 Pod 中的工作负载。 可以单独升级某个工作负载的 sidecar 而不影响其他工作负载。 节点共享代理 每个节点上只有一个代理，为该节点上的所有工作负载所共享，开销小。 对加密内容和私钥的管理存在安全隐患。 节点级别隔离，如果共享代理升级时出现版本冲突、配置冲突或扩展不兼容等问题，则可能会影响该节点上的所有工作负载。 不需要考虑注入 Sidecar 的问题。 Service Account / 节点共享代理 服务账户 / 身份下的所有工作负载都使用共享代理，开销小。 工作负载和代理之间的连接的认证及安全性无法保障。 节点和服务账号之间级别隔离，故障同 “节点共享代理”。 同 “节点共享代理”。 带有微代理的共享远程代理 因为为每个 pod 都注入一个微代理，开销比较大。 微代理专门处理 mTLS，不负责 L7 路由，可以保障安全性。 当需要应用 7 层策略时，工作负载实例的流量会被重定向到 L7 代理上，若不需要，则可以直接绕过。该 L7 代理可以采用共享节点代理、每个服务账户代理，或者远程代理的方式运行。 同 “Sidecar 代理”。 服务网格的四种部署模式对比 可编程代理 Flomesh 的张晓辉曾在 为什么需要可编程代理 博客中详细说明了代理软件的发展演化过程，我下面将引用他的一些观点，说明可编程代理 Envoy 在 Istio 中的关键作用。\n下图展示了代理从配置到可编程模式的演化过程，及每个阶段中的代表性代理软件。\n代理软件的演化示意图 整个代理演化过程都是随着应用从本地和单体，越来越走向大规模和分布式。下面我将简要概括代理软件的发展过程：\n配置文件时代：几乎所有软件都有配置文件，代理软件因为其相对复杂的功能，更离不开配置文件。该阶段的代理主要使用 C 语言开发，包括其扩展模块，突出的代理本身的能力。这也是我们使用代理最原始最基础的形式，这些代理包括 Nginx、Apache HTTP Server、Squid 等； 配置语言时代：这个时代的代理，更具扩展性和灵活性，比如动态数据获取和配套的逻辑判断。代表性代理包括扩 Varnish 和 HAProxy； 脚本语言时代：从脚本语言的引入开始，代理软件才真正走向的可编程，我们可以更方便的使用脚本在代理中增加动态逻辑，增加了开发效率。代表性的代理是 Nginx 及其支持的脚本语言； 集群时代：随着云计算的普及，大规模部署和动态配置 API 成了代理所必需的能力，而且随着网络流量的增加，大规模代理集群也应运而生。这个时代的代表性代理有 Envoy、Kong 等； 云原生时代：多租户、弹性、异构混合云、多集群、安全和可观测，这些都是云原生时代对代理所提出的更高要求，代表性软件有 Istio、Linkerd、Pypi ，它们都为代理构建了控制平面。 这些都是服务网格吗？ 现在我将列举一些流行的服务网格开源项目，让我们一起探索服务网格的发展规律和本质。下表对比了当前流行的服务网格开源项目 7。\n对比项 Istio Linkerd Consul Connect Traefik Mesh Kuma Open Service Mesh (OSM) 当前版本 1.14 2.11 1.12 1.4 1.5 1.0 …","relpermalink":"/blog/beyond-istio-oss/","summary":"本文从云原生大背景下重新审视 Istio，讲解 Istio 诞生，在云原生技术栈中的地位及发展方向。","title":"Beyond Istio OSS —— Istio 服务网格的现状与未来"},{"content":" 读者须知 SPIRE 支持是 Istio 1.14 的新特性，请确保安装 1.14 及以上版本的 Istio。 安装 Istio 到 GitHub 上下载 Istio 安装包，解压后，建议使用 istioctl 安装 Istio。\nistioctl install --set profile=demo 如果你安装了低于 1.14 版本的 Istio，请先升级 Istio。\n安装 SPIRE 我们使用 Istio 提供的快速安装方式：\nkubectl apply -f samples/security/spire/spire-quickstart.yaml 这将安装以下组件：\nnamespace/spire created csidriver.storage.k8s.io/csi.spiffe.io created customresourcedefinition.apiextensions.k8s.io/spiffeids.spiffeid.spiffe.io created clusterrolebinding.rbac.authorization.k8s.io/k8s-workload-registrar-role-binding created clusterrole.rbac.authorization.k8s.io/k8s-workload-registrar-role created configmap/k8s-workload-registrar created serviceaccount/spire-server created configmap/trust-bundle created clusterrole.rbac.authorization.k8s.io/spire-server-trust-role created clusterrolebinding.rbac.authorization.k8s.io/spire-server-trust-role-binding created configmap/spire-server created statefulset.apps/spire-server created service/spire-server created serviceaccount/spire-agent created clusterrole.rbac.authorization.k8s.io/spire-agent-cluster-role created clusterrolebinding.rbac.authorization.k8s.io/spire-agent-cluster-role-binding created configmap/spire-agent created daemonset.apps/spire-agent created 需要给打 patch，这是为了让所有 sidecar 和 Ingress 都可以共享 SPIRE agent 的 UNIX Domain Socket。\nistioctl install --skip-confirmation -f - \u0026lt;\u0026lt;EOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system spec: profile: default meshConfig: trustDomain: example.org values: global: # This is used to customize the sidecar template sidecarInjectorWebhook: templates: spire: | spec: containers: - name: istio-proxy volumeMounts: - name: workload-socket mountPath: /run/secrets/workload-spiffe-uds readOnly: true volumes: - name: workload-socket csi: driver: \u0026#34;csi.spiffe.io\u0026#34; components: ingressGateways: - name: istio-ingressgateway enabled: true label: istio: ingressgateway k8s: overlays: - apiVersion: apps/v1 kind: Deployment name: istio-ingressgateway patches: - path: spec.template.spec.volumes.[name:workload-socket] value: name: workload-socket csi: driver: \u0026#34;csi.spiffe.io\u0026#34; - path: spec.template.spec.containers.[name:istio-proxy].volumeMounts.[name:workload-socket] value: name: workload-socket mountPath: \u0026#34;/run/secrets/workload-spiffe-uds\u0026#34; readOnly: true EOF 安装好 Istio 和 SPIRE 后，我们就可以注册负载了。\n自动注册 Kubernetes 负载 在快速安装 SPIRE 的时候，我们已经安装了 SPRIE Kubernetes Workload Registrar ，也就是说我们已经开启自动负载注册。现在检查一下 SPIRE 是否给负载颁发了身份证明。\nkubectl exec -i -t spire-server-0 -n spire -c spire-server -- /bin/sh -c \u0026#34;bin/spire-server entry show -socketPath /run/spire/sockets/server.sock\u0026#34; 你将看到例如下面这样的结果：\n# Node Entry ID : 3f17c8be-1379-4b7c-9a01-90805165d59f SPIFFE ID : spiffe://example.org/k8s-workload-registrar/demo-cluster/node/gke-jimmy-cluster-default-pool-d5041909-3atb Parent ID : spiffe://example.org/spire/server Revision : 0 TTL : default Selector : k8s_psat:agent_node_uid:cbab5123-b32f-49d0-89f2-0a7e4d2b0edd Selector : k8s_psat:cluster:demo-cluster # Ingress gateway Entry ID : ffc76b2e-e602-4ad3-8069-993ffbf4440e SPIFFE ID : spiffe://example.org/ns/istio-system/sa/istio-ingressgateway-service-account Parent ID : spiffe://example.org/k8s-workload-registrar/demo-cluster/node/gke-jimmy-cluster-default-pool-d5041909-0ucb Revision : 1 TTL : default Selector : k8s:node-name:gke-jimmy-cluster-default-pool-d5041909-0ucb Selector : k8s:ns:istio-system Selector : k8s:pod-uid:be32b10a-b5a4-4716-adaf-eab778f58c13 DNS name : istio-ingressgateway-6989cbc776-87qb6 DNS name : istio-ingressgateway.istio-system.svc # SPIRE Server Entry ID : 54444848-95ec-4b4d-a7c5-902a4049c96a SPIFFE ID : spiffe://example.org/ns/spire/sa/spire-server Parent ID : spiffe://example.org/k8s-workload-registrar/demo-cluster/node/gke-jimmy-cluster-default-pool-d5041909-0ucb Revision : 1 TTL : default Selector : k8s:node-name:gke-jimmy-cluster-default-pool-d5041909-0ucb Selector : k8s:ns:spire Selector : k8s:pod-uid:4917defc-9b5a-42d8-9b98-c0e0a48c0313 DNS name : spire-server-0 DNS name : spire-server.spire.svc 每个 node 的 parent ID 是 spiffe://example.org/ns/spire/sa/spire-server，而 spiffe://example.org/ns/spire/sa/spire-server 的 parent 是 spiffe://example.org/k8s-workload-registrar/demo-cluster/node/gke-jimmy-cluster-default-pool-d5041909-0ucb。\n也就是说工作负载的层级是这样的：\nSPIRE Server：spiffe://example.org/spire/server\nKubernetes 节点：spiffe://example.org/k8s-workload-registrar/demo-cluster/node/\n普通服务账户：spiffe://example.org/{namespace}/spire/sa/{service_acount}\nSPIRE 通过标签选择器选择工作负载，为它们创建 SPIFFE ID。\n工作负载的 SPIFEE ID 格式为 spiffe://\u0026lt;trust.domain\u0026gt;/ns/\u0026lt;namespace\u0026gt;/sa/\u0026lt;service-account\u0026gt;。\n部署应用 下面我们部署一个应用，然后检查下 SPIRE 为该应提供的身份。\n使用下面的命令部署 Istio 提供的示例应用 sleep：\nistioctl kube-inject --filename samples/security/spire/sleep-spire.yaml | kubectl apply -f - 因为我们在安装 Istio 的时候打了补丁，做了 SPIRE 自动注入，然后所有 pod 都会共享 SPIRE Agent 中的 UNIX Domain Socket：/run/secrets/workload-spiffe-uds/socket。获取正在运行的 sleep pod 的 YAML，我们可以看到其中有 volume 配置如下：\nvolumes: - name: workload-socket csi: driver: csi.spiffe.io 以及 volumeMounts 如下： …","relpermalink":"/blog/how-to-integrate-spire-with-istio/","summary":"本文将带你一步一步在 Istio 中集成 SPIRE 身份认证。","title":"如何在 Istio 中集成 SPIRE？"},{"content":"近日 InfoQ 发布了 DevOps and Cloud InfoQ Trends Report – June 2022 ，因为报告中所覆盖的技术领域过于宽泛，本文仅仅是对这篇报告的一点个人解读。我基本认同我关注的这些技术中「创新者」和「后期大众」阶段技术的划分。\n报告概括 下面这段节选自原文的 takeaways：\n数据可观察性将帮助企业更好地了解和排除其数据密集型系统的故障。 云原生应用采用无服务器和分布式SQL数据库的情况也越来越多。 FinOps将走向成熟。 eBPF和WASM是令人振奋的新技术，它们被用来在服务网格内开启可观察性、监控和安全的新方法。我们认为这处于创新者阶段。 低代码或无代码平台继续成熟，特别是用于内部工具和自动化用途。 我们还看到「开发者体验作为决策驱动力」的趋势得到了更多的关注，特别是在云平台领域。「平台工程师」的角色正在许多规模的组织中出现，以支持相关平台抽象、API和工具的建设。 报告解读 这篇报告为什么命名为 「DevOps 和云」我就不太清楚了，我觉得把名字换成「云计算」、「云原生」也是可以的，可能是为了延续之前的报告风格吧，毕竟 InfoQ 已经推出过很多期此类报告了。这类报告都是根据「鸿沟理论」将当前流行的技术分成以下阶段：\n创新者 早期采用者 早期大众 后期大众 落后者 不过 InfoQ 的报告中没有「落后者」这个阶段。\n什么是鸿沟理论？ 鸿沟理论指的就是高科技产品在市场营销过程中遭遇的最大障碍：高科技企业的早期市场和主流市场之间存在着一条巨大的鸿沟，能否顺利跨越鸿沟并进入主流市场，成功赢得实用主义者的支持，就决定了一项高科技产品的成败。实际上每项新技术都会经历鸿沟。关键在予采取适当的策略令高科技企业成功地 “跨越鸿沟”，摩尔在这本书中就告诉了人们一些欠经考验的制胜秘诀。 下图展示的是跨越鸿沟理论中不同阶段人群的分类及占比。\n跨越鸿沟理论中不同阶段人群的分类及占比 我们来看下 InfoQ 6 月新出的「云和 DevOps」趋势报告。\n软件开发云和 DevOps 趋势图（2022 年 6 月） 我们可以看到像低代码、eBPF、Data Mesh、WASM 已经出现在创新者视线里了。Service Mesh 还在「早期采用者」阶段，这点比我预想的要慢好多，我以为服务网格已经跨越鸿沟了，你觉得呢？\n","relpermalink":"/blog/cloud-and-devops-trend-2022/","summary":"近日 InfoQ 发布了 DevOps and Cloud InfoQ Trends Report – June 2022，因为报告中所覆盖的技术领域过于宽泛，本文仅仅是对这篇报告的一点个人解读。","title":"2022 年云和 DevOps 趋势报告"},{"content":"零信任（Zero Trust）是一种安全理念，而不是一种所有安全团队都要遵循的最佳实践。零信任概念的提出是为了给云原生世界带来更安全的网络。零信任是一种理论状态，即网络内的所有消费者都不仅没有任何权限，而且也不具备对周围网络的感知。\n零信任的基础 零信任网络中的所有用户，包括机器和人类，都需要通过一个密码学验证的身份。要是实践零信任，需要从引入用户身份开始，然后考虑限制用户的最小访问权限。零信任实践的基础是认证和授权，这是比起传统的安全策略来说，零信任中的认证变得更加严格，而授权将变得更加细化。举个例子，传统的授权是：“用户 A 可以访问数据中心 D 吗”，而在零信任的框架下将变成“在某个特定的时间点，在某个特定的地区，使用某个特定的设备的用户 A 可以访问某个特定应用中的某个特定文件吗？”\n越来越细化的授权 在 Kubernetes 中，我们使用 RBAC 来管理权限。所有用户都是以组为基础来授予或拒绝访问权限，单个用户（服务账户）会被授予过多的访问权限。零信任的一个重要特征就是更细的粒度，基于角色来授予访问权限是不够安全的。我们需要细化用户对单一资源在限定时间内的访问权限。这正好与微服务背后的原则相契合——随着服务和数据被分解成更小的部分，就有可能允许我们细化地位服务授予访问权限。\n有时间限制的授权 关于授权，我们往往会存在一种误解，即一个用户一旦被认证和授权，他就成了一个“受信任”的用户，该用户就可以随时的对系统进行访问。然后，在零信任网络中，没有受信任的用户或设备。用户的每一次访问都需要经过认证和授权。而且，授权还会有一个时间窗口，用户只能在这个规定的实践窗口中执行规定的动作。\n如何在企业内实行零信任网络 因为网络是企业系统的命脉之一，牵一发而动全身，要在企业内实行零信任网络，通常需要战略高层管理人员接纳零信任，通过自上而下的方式强加给安全团队。然后渐进式的改进你的网络，从一个关键业务开始，使其变成零信任的。\n在零信任网络里，默认是拒绝一切访问。需要在应用程序开发中，积极主动的允许来自应用程序的某些适当的请求。身份是零信任的基础，而不是网络。零信任的关注对象是访问点、身份认证与授权和攻击面。对于云原生应用，因为它们的生命周期短暂且是动态的，为了实现零信任，你必须为每个访问点配置一个规则，不断更新应用程序的证书和访问规则，这时候手动配置几乎是不可能的，你必须实现自动化。\n在 IstioCon 2022 的主题演讲 有提到，Istio 正在成为零信任的一个重要组成部分。其中最主要的是面向身份的控制，而不是面向网络的控制。这方面的核心原则在谷歌白皮书《BeyondProd：云原生安全的新方法》 上有描写 。\n如果我们能将身份概念扩展到用户，并为我们提供灵活而丰富的策略机制来指定、监控和跟踪访问控制，我们就能达到一个可操作的零信任架构 —— 将用户、服务和数据统一到一个管理层。我所工作的公司 Tetrate 创建了 Tetrate Service Bridge —— 可供大型组织使用的管理平面，也是践行了零信任的理念。\n总结 零信任是一种安全理念，它的基础是认证和授权。但比起传统网络安全方法来说，零信任具有如下特点：\n系统中的所有工作负载都有一个密码学验证的身份 零信任网络默认拒绝所有访问 具有更细粒度的访问授权。 为了在云原生应用中实行零信任，你需要：\n自上而下的推行 从关键业务入口 建立自动化工具 参考 下面有一些资料你可以参考：\n写给 Kubernetes 工程师的 mTLS 指南 利用服务网格为基于微服务的应用程序实施 DevSecOps Istio 捐献给 CNCF 意味着什么？ ","relpermalink":"/blog/what-is-zero-trust/","summary":"本文向你介绍零信任 —— 什么是零信任，它跟传统网络安全的区别，如何在企业内实施零信任。","title":"什么是零信任？"},{"content":"今年 6 月初，Istio 1.14 发布 ，该版本中最值得关注的特性是新增对 SPIRE 的支持。SPIFFE 和 SPIRE 都是 CNCF 孵化项目，其中 SPIRE 是 SPIFFE 的实现之一。本文将带你了解 SPIRE 对于零信任架构的意义，以及 Istio 是为何使用 SPIRE 实现身份认证。\nKubernetes 中的身份认证 我们都知道 Istio 最初是基于 Kubernetes 建立起来的，在谈在 Istio 中使用 SPIRE 做身份认证之前，我们先来看下 Kubernetes 中如何做身份认证。\n我们来看一个 pod 的 token 的例子，下面是 default 命名空间下 sleep pod 的 Service Account 的 token。\napiVersion: v1 data: ca.crt: {CA_CRT} namespace: ZGVmYXVsdA== token: {TOKEN_STRING} kind: Secret metadata: annotations: kubernetes.io/service-account.name: sleep kubernetes.io/service-account.uid: 2c0d00e8-13a2-48d0-9ff8-f987f3325ecf creationTimestamp: \u0026#34;2022-06-14T03:01:35Z\u0026#34; name: sleep-token-gwhwd namespace: default resourceVersion: \u0026#34;244535398\u0026#34; uid: b8822ceb-9553-4a17-96dc-d525bbaed0e0 type: kubernetes.io/service-account-token 我们看到其中有 ca.crt 和 token 字段，如果这个 token 被窃取，会有什么后果？Kubernetes 中使用 Service Account 来管理 Pod 的身份，然后利用 RBAC 指定具有某 Service Account 的 Pod 对 Kubernetes API 的权限。Service Account 的 token 存储在 Secret 中，token 中并不包含工作负载所运行的节点、pod 的声明，一旦 token 被窃取破坏者就获得了该账户的所有权限，伪装成该用户窃取信息或破坏。\n一个 token 只能在一个集群中标记负载身份，Istio 同时支持 Kubernetes 环境和虚拟机，还有多集群多网格，如何统一这些异构环境中的工作负载身份？这时，一个统一的工作负载身份标准就呼之欲出了。\nSPIFFE 与 SPIRE 简介 SPIFFE 的目的是基于零信任的理念，建立一个开放、统一的工作负载身份标准，这有助于建立一个零信任的全面身份化的数据中心网络。SPIFFE 的核心是通过简单 API 定义了一个短期的加密身份文件 SVID，用作工作负载认证时使用的身份文件，例如建立 TLS 连接或签署和验证 JWT 令牌等。SPIRE 可以根据管理员定义的策略自动轮换 X.509 SVID 证书和秘钥。Istio 可以通过 SPIRE 动态的消费工作负载标识，SPIRE 可以动态的提供工作负载标识。\n下面我将为你简单介绍一下与 SPIFFE 相关的一些术语。\nSPIFFE（Secure Production Identity Framework For Everyone）是一套身份认证标准。 SPIRE（SPIFFE Runtime Environment） 是 SPIFFE 标准的一套生产就绪实现。 SVID（SPIFFE Verifiable Identity Document）是工作负载向资源或调用者证明其身份的文件。SVID 包含一个 SPIFFE ID，代表了服务的身份。它将 SPIFFE ID 编码在一个可加密验证的文件中，目前支持两种格式：X.509 证书或 JWT 令牌。 SPIFFE ID 是一个统一资源标识符（URI），其格式如下：spiffe://trust_domain/workload_identifier。 SPIRE 包含 Server 和 Agent 两个部分，它们的作用如下。\nSPIRE Server\n身份映射 节点认证 SVID 颁发 SPIRE Agent\n工作负载认证 提供工作负载 API SPIFFE 与零信任安全 零信任的本质是以身份为中心的动态访问控制。动态证书轮换、动态证书下发、动态权限控制。SPIFFE 解决的是标识工作负载的问题。\n在虚拟机时代我们可能根据一个 IP 地址和端口来标识一个工作负载，基于 IP 地址标识存在多个服务共享一个 IP 地址，IP 地址伪造和访问控制列表过大等问题。到了 Kubernetes 时代，容器的生命周期是短暂的，我们无法再用 IP 地址来标识负载，而是通过 pod 或 service 名称。但是，不同的云、软件平台对工作负载标识的方法不同，相互之间存在兼容性问题。尤其是在异构混合云的中，同时存在虚拟机和容器的工作负载。这时，建立一个细粒度、具有互操作性的标识系统，将具有重要意义。\n在 Istio 中使用 SPIRE 做身份认证 Istio 会利用 SPIRE 为每个工作负载提供一个唯一标识，服务网格中的工作负载在进行对等身份认证、请求身份认证和授权策略都会使用到服务标识，用于验证访问是否被允许。SPIRE 原生支持 Envoy SDS API，SPIRE Agent 中的通过与工作负载中共享的 UNIX Domain Socket 通信，为工作负载颁发 SVID。请参考 Istio 文档 了解如何在 Istio 中使用 SPIRE 做身份认证。\nSDS 最重要的好处就是简化了证书管理。如果没有这个特性，在 Kubernetes deployment 中，证书就必须以 secret 的方式被创建，然后挂载进代理容器。如果证书过期了，就需要更新 secret 且代理容器需要被重新部署。如果使用 SDS，Istio 可以使用 SDS 服务器会将证书推送给所有的 Envoy 实例。如果证书过期了，服务器仅需要将新证书推送至 Envoy 实例，Envoy 将会立即使用新证书且不需要重新部署代理容器。\n下图展示了 Istio 中使用 SPIRE 进行身份认证的架构。\nIstio 中使用 SPIRE 进行身份认证的架构图 在 Kubernetes 集群中的 spire 命名空间中使用 StatefulSet 部署 SPIRE Server 和 Kubernetes Workload Registrar，使用 DaemonSet 资源为每个节点部署一个 SPIRE Agent。假设你在安装 Kubernetes 时使用的是默认的 DNS 名称 cluster.local，Kubernetes Workload Registar 会为 Istio Mesh 中的工作负载创建如下格式的身份：\nSPRRE Server：spiffe://cluster.local/ns/spire/sa/server SPIRE Agent：spiffe://cluster.local/ns/spire/sa/spire-agent Kubernetes Node：spiffe://cluster.local/k8s-workload-registrar/demo-cluster/node/ Kubernetes Worload Pod：spiffe://cluster.local/{namespace}/spire/sa/{service_acount} 这样不论是节点还是每个工作负载都有它们全局唯一的身份，而且还可以根据集群 （信任域）扩展。\nIstio 中的工作负载身份验证过程如下图所示。\nIstio 服务网格中的工作负载身份认证过程示意图 详细过程如下：\n工作负载的 sidecar 中的 pilot-agent 会通过共享的 UDS 调用 SPIRE Agent 来获取 SVID SPIRE Agent 询问 Kubernetes（准确的说是节点上的 kubelet）获取负载的信息 Kubelet 将从 API server 查询到的信息返回给工作负载验证器 验证器将 kubelet 返回的结果与 sidecar 共享的身份信息比对，如果相同，则将正确的 SVID 缓存返回给工作负载，如果不同，则身份认证失败 关于工作负载的注册和认证的详细过程请参考 SPIRE 文档 。\n总结 身份是零信任网络的基础，SPIFFE 统一了异构环境下的身份标准。在 Istio 中不论我们是否使用 SPIRE，身份验证对于工作负载来说是不会有任何感知的。通过 SPIRE 来为工作负载提供身份验证，可以有效的管理工作负载的身份，为实现零信任网络打好基础。\n","relpermalink":"/blog/why-istio-need-spire/","summary":"本文将带你了解 SPIRE 对于零信任架构的意义，以及 Istio 是为什么使用 SPIRE 实现身份认证。","title":"为什么 Istio 要使用 SPIRE 做身份认证？"},{"content":"最近开始对 eBPF 技术和 Cilium 开源项目颇为感兴趣，也翻译了一本 eBPF 相关的资料《什么是 eBPF 》， 以及撰写了 请暂时抛弃使用 eBPF 取代服务网格和 sidecar 模式的幻想 的文章，想要再深入了解一下 Cilium 和 eBPF 这样的技术就服务网格领域究竟能发挥什么样的作用，那么就先从 Cilium 开始吧！\n这本《Cilium 中文指南》目前译自 Cilium 官方文档 ，根据 1.11 版本，节选了以下章节：\n概念：描述了 Cilium 的组件以及部署 Cilium 的不同模式。 提供高层次的 运行一个完整的 Cilium 部署并理解其行为所需的高层次理解。 开始：快速开始使用 Cilium。 策略：详细介绍了策略语言结构和支持的格式。 内部原理：介绍了一些组件的内部细节。 其他未翻译部分主要涉及命令、API 及运维，本指南未来会加入笔者个人观点及其他内容，欢迎阅读和评论 。\n","relpermalink":"/notice/cilium-handbook/","summary":"一本关于 Cilium 和 eBPF 的电子书发布。","title":"《Cilium 中文指南》发布"},{"content":" 云原生社区最新力作 —— 《深入理解 Istio》上市开售 2017 年 5 月，Google、IBM 和 Lyft 联合 宣布 将 Istio 开源，不知不觉中距今已5年有余。在这5年多的时间里，Istio 项目从一颗种子长成了参天大树。尤其是在 2018 年 Istio 1.0 版本发布的接下来两年里，国内有多本关于 Istio 服务网格的图书上市。在 Istio 图书出版领域，我国走在了世界的前列。\nIstio 开源时间线 服务网格：云原生的核心技术之一 如今在国内，Istio 几乎可以作为服务网格的代名词，作为 CNCF（云原生计算基金会）定义的云原生 关键技术之一，服务网格发展至今经历了以下几个阶段。\n探索阶段：2017 —2018 年 早期采用者阶段：2019—2020 年 大规模落地及生态发展阶段：2021 年至今 2018 年，CNCF 对云原生的定义是：云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式 API。\n可见，CNCF 将服务网格加入了云原生定义中，即服务网格是云原生的代表性技术之一。如今， Google 正在将 Istio 捐献给 CNCF，我们有理由相信，成为 CNCF 项目后，Istio 的社区会开放，它未来的发展之路也会更顺畅。\n服务网格与云原生应用 云原生的发展方兴未艾，虽然不断有新的技术和产品出现，但作为整个云原生技术栈的一部分，服务网格在过去一年里不断夯实了它作为“云原生网络基础设施”的定位。下图展示了云原生技术栈模型，其中的每一层都有一些代表性的技术来定义标准。作为新时代的中间件，服务网格与其他云原生技术交相辉映，如 Dapr（分布式应用程序运行时）定义了云原生中间件的能力模型，OAM 定义了云原生应用程序模型等，而服务网格定义了云原生七层网络模型。\n云原生应用技术栈 为什么需要服务网格 使用服务网格并非意味着与 Kubernetes 决裂，而是自然而然的事情。Kubernetes 的本质是通过声明配置对应用进行生命周期管理，而服务网格的本质是提供应用间的流量控制和安全性管理，以及可观察性。 假如已经使用 Kubernetes 构建了稳定的微服务平台，那么如何设置服务间调用的负载均衡和流量控制呢？\nEnvoy 创造的 xDS 协议被众多开源软件所支持，如 Istio、Linkerd、MOSN 等。Envoy 对服务网格或云原生而言最大的贡献就是定义了 xDS。Envoy 本质上是一个网络代理，是通过 API 配置的现代版代理，基于它衍生出了很多不同的使用场景，如 API 网关、服务网格中的 sidecar 代理和边缘代理。\n技术发展从 Kubernetes 到 Istio，概括来讲有以下原因。\nKubernetes 的本质是应用的生命周期管理，具体来说，就是应用的部署和管理（扩缩容、自 动恢复、发布）。 Kubernetes 为微服务提供了可扩展、高弹性的部署和管理平台。 服务网格的基础是透明代理，先通过 sidecar 代理拦截微服务间的流量，再通过控制平面配置管理微服务的行为。如今，服务网格的部署模式也迎来了新的挑战，sidecar 已经不是服务网格所必须的，基于 gRPC 的无代理的服务网格也在测试中。 xDS 定义了服务网格配置的协议标准，目前基于 gRPC 的 xDS 也正在开发中。 服务网格将流量管理从 Kubernetes 中解耦，服务网格内部的流量无须 kube-proxy 组件的支持， 通过接近微服务应用层的抽象，管理服务间的流量，实现安全性和可观察性功能。 服务网格是对 Kubernetes 中 service 更上层的抽象，它的下一步是 Serverless，这也是Google 在 Istio 之后紧接着推出基于 Kubernetes 和 Istio 之上的 Knative 的原因。 以社区之名成就开源 2018 年 5 月，在蚂蚁金服的支持下，ServiceMesher 社区成立。随后，国内刮起了服务网格的旋风，由社区领导的 Istio 官方文档翻译工作也进入白热化阶段。\n随着时间的推移，我感受到系统介绍 Istio 的中文资料匮乏，于是在 2018 年 9 月开始构思写一本关于 Istio 的图书，并在 GitHub 上发起了 Istio Handbook 的开源电子书项目。几个月后，随着服务网格技术的推广及 ServiceMesher 社区规模的扩大，我在社区的线上线下活动中结识了很多同样热衷于 Istio 和服务网格技术的朋友。我们一致决定，一起写一本 Istio 的开源电子书，将社区积累的宝贵文章和经验集结成系统的文字，分享给广大开发者。\n2019 年 3 月，在社区管理委员会的组织下，几十位成员自愿参与并开始共同撰写此书。2020 年 5 月，为了更好地推广云原生技术，丰富社区分享的技术内容，我们成立了云原生社区，并将原有的 ServiceMesher 社区纳入其中，社区运营的内容也从服务网格技术扩展到更加全面的云原生技术。\n2020 年 10 月，这本书主要的内容贡献者组成了编委会，成员分别有我、马若飞、王佰平、王炜、罗广明、赵化冰、钟华和郭旭东。我们在出版社的指导与帮助下，对本书进行了后续的版本升级、完善、优化等工作。经过反复的迭代，这本《深入理解 Isito：云原生服务网格进阶实战》终于和大家见面了。\n《深入理解 Istio —— 云原生服务网格进阶实战》封面 关于本书 Istio 在 1.5 版本后有了重大的架构变化，同时引入或改进了多项功能，例如，引入了智能 DNS 代理、新的资源对象，改进了对虚拟机的支持等。\n本书以 Istio 新版本为基础编写而成，在持续追踪 Istio 社区最新动向的基础上，力求为读者提 供最新、最全面的内容。另外，本书的多位作者都是一线的开发或运维工程师，具有丰富的 Istio 实战经验， 为本书提供了翔实、宝贵的参考案例。\n目前，这本书已经在京东平台上线，要想了解更多有关 Istio 的相关知识，就来读一读这本《深入理解 Isito：云原生服务网格进阶实战》吧！\n京东 618，满 100 减 50，扫码即购！\n点此购买\n","relpermalink":"/blog/istio-service-mesh-book/","summary":"三年磨一剑，云原生社区著《深入理解 Istio —— 云原生服务网格进阶实战》正式上市开售啦！","title":"云原生社区著《深入理解 Istio》正式上市开售"},{"content":"最近 eBPF 技术在云原生社区中持续火热，在我翻译了《什么是 eBPF 》之后，当阅读“云原生环境中的 eBPF”之后就一直在思考 eBPF 在云原生环境中究竟处于什么地位，发挥什么样的作用。当时我评论说“eBPF 开启了上帝视角，可以看到主机上所有的活动，而 sidecar 只能观测到 pod 内的活动，只要搞好进程隔离，基于 eBPF 的 proxy per-node 才是最佳选择”，再看到 William Morgan 的这篇文章 1之后，让我恍然大悟。下面节选翻译了文章我比统同意的观点，即 eBPF 无法替代服务网格和 sidecar，感兴趣的读者可以阅读 William 的原文。\n什么是 eBPF 在过去，如果你想让应用程序处理网络数据包，那是不可能的。因为应用程序运行在 Linux 用户空间，它是不能直接访问主机的网络缓冲区。缓冲区是由内核管理的，受到内核保护，内核需要确保进程隔离，进程之间不能直接读取对方的网络数据包。正确的做法是，应用程序通过系统调用（syscall）来请求网络数据包信息，这本质上是内核 API 调用——应用程序调用 syscall，内核检查应用程序是否有权限获得其请求的数据包；如果有，就把返回数据包。\n有了 eBPF 之后，应用程序不再需要 syscall，数据包不需要在内核空间和用户空间之间来回交互传递。而是我们将代码直接交给内核，让内核自己执行，这样就可以让代码全速运行，效率更高。eBPF 允许应用程序和内核以安全的方式共享内存，eBPF 允许应用程序直接向内核提交代码，目标都是通过超越系统调用的方式来实现性能提升。\neBPF 不是银弹，你不能用 eBPF 运行任意程序，实际上 eBPF 可以做的事情是非常有限的。\neBPF 的局限性 eBPF 的局限性也是因为内核造成的。内核中运行的应用程序应当有自己的租户，这些租户之间会争抢系统的内存、磁盘和网络，内核的职责就是隔离和调度这些应用程序的资源，同时内核还要保护确认应用程序的权限，保护其不被其他程序破坏。\n因为我们直接将 eBPF 代码交给内核执行，这绕过了内核安全保护（如 syscall），内核将面临直接的安全风险。为了保护内核，所有 eBPF 程序要想运行都必须先通过一个验证器。但是要想自动验证程序是很困难的，验证器可能会过度限制程序的功能。比如 eBPF 程序不能是阻塞的，不能有无限循环，不能超过预定的大小；其复杂性也受到限制，验证器会评估所有可能的执行路径，如果 eBPF 程序不能在某些范围内完成，或者不能证明每个循环都有一个退出条件，那么验证器就不会允许该程序运行。有很多应用程序都违反了这些限制，要想将它们作为 eBPF 程序来运行的话，要么重写以满足验证器的需求，要么给内核打补丁，来绕过一些验证（这可能比较困难）。不过随着内核版本的升级，这些验证器也变得更加智能，限制也逐渐变得宽松，也有一些创造性的方法来绕过这些限制。\n但总的来说，eBPF 程序能做的事情非常有限。对于一些重量级事件的处理，例如处理全局范围内的 HTTP/2 流量，或者 TLS 握手协商不能在纯 eBPF 环境中完成。充其量，eBPF 可以做其中的一小部分工作，然后调用用户空间应用程序来处理对于 eBPF 来说过于复杂而无法处理的部分。\neBPF 与服务网格的关系 因为上文所述的 eBPF 的各项限制，七层流量仍然需要用户空间的网络代理来完成，eBPF 并不能替代服务网格。eBPF 可以与 CNI（容器网络接口）一起运行，处理三层/四层流量，而服务网格处理七层流量。\n每个主机一个代理的模式比 sidecar 更糟 对于每个主机一个代理（per-host）的模式，服务网格的早期实践者 Linkerd 1.x 就是这么用的，笔者也是从那个时候开始关注服务网格，Linkerd 1.x 还使用了 JVM 虚拟机！但是经过 Linkerd 1.x 的用户实践证明，这种模式相对于 sidecar 模式，对于运维和安全来说会更糟糕。\n为什么说 sidecar 模式比 per-host 模式更好呢？因为 sidecar 模式有以下几个优势，这是 per-host 模式所不具备的：\n代理的资源消耗随着应用程序的负载而变化。随着实例流量的增加，sidecar 会消耗更多的资源，就像应用程序一样。如果应用程序的流量非常小，那么 sidecar 就不需要消耗很多资源。Kubernetes 现有的管理资源消耗的机制，如资源请求和限制以及 OOM kill，都会继续工作。 代理失败的爆炸半径只限于一个 pod。代理失败与应用失败相同，由 Kubernetes 负责处理失败的 pod。 代理维护。例如代理版本的升级，是通过如滚动更新，灰度发布等应用程序本身相同的机制完成的。 安全边界很清楚（而且很小）：在 pod 级别。Sidecar 在应用程序实例的同一安全上下文中运行。它是 pod 的一部分，与应用程序具有一样的 IP 地址。Sidecar 执行策略，并将 mTLS 应用于进出该 pod 的流量，而且它只需要该 pod 的密钥。 而对于 per-host 模式，就没有上述好处了。代理与应用程序 pod 完全解耦，处理主机上所有 pod 的流量，这样会代理各种问题：\n代理消耗的资源是高度可变的，这取决于在某个时间点 Kubernetes 调度了多少个 pod 在该主机上。你无法有效的预测特定代理的资源消耗情况，这样代理就有崩溃的风险（原文是这么说的，这点笔者还是存疑的，希望有点读者能解帮忙解释下）。 主机上 pod 之间的流量争抢问题。因为主机上的所有流量都经过同一个代理，如果有一个应用程序 pod 的流量极高，消耗了代理的所有资源，主机上的其他应用程序就有被饿死的危险。 代理的爆炸半径很大，而且是不断变化的。代理的故障和升级现在影响到随机的应用程序集合中的一个随机的 pod 子集，意味着任何故障或维护任务都有难以预测的风险。 使得安全问题更加复杂。以 TLS 为例，主机上的代理必须包含该主机上所有应用程序的密钥，这使得它成为一个新的攻击媒介，容易受到混淆代理 问题的影响——代理中的任何 CVE 或漏洞都是潜在的密钥泄露风险。 简而言之，sidecar 模式继续贯彻了容器级别的隔离保护——内核可以在容器级别执行所有安全保护和公平的多租户调度。容器的隔离仍然可以完美的运行，而 per-host 模式却破坏了这一切，重新引入了争抢式的多租户隔离问题。\n当然 per-host 也不是一无是处，该模式最大的好处是可以成数量级的减少代理的数量，减少网络跳数，这也就减少了资源消耗和网络延迟。但是与该模式带来的运维和安全性问题相比，这些优势都是次要的。我们也可以通过持续优化 sidecar 来弥补 sidecar 模式在这方面的不足，而 per-host 模式的缺陷确是致命性的。\n其实归根结底还是回到了争抢式多租户问题上，那么能否利用现有的内核解决方案，改进一下 per-host 模式中的代理，让其支持多租户呢？比如改造 Envoy 代理，使其支持多租户模式。虽然从理论来说这是可行的，但是工作量巨大，Matt Klein 也觉得不值得这样做 2，还不如使用容器来实现租户隔离。而且即使让 per-host 模式中的代理支持了多租户，仍然还有爆炸半径和安全问题需要解决。\n总结 不管有没有 eBPF，在可预见的未来，服务网格都会基于运行在用户空间的 sidecar 代理（proxyless 模式除外）。Sidecar 模式虽然也有弊端，但它依然是既能保持容器隔离和操作的优势，又能处理云原生网络复杂性的最优方案。eBPF 的能力将来是否会发展到可以处理七层网络流量，从而替代服务网格和 sidecar，也许吧，但那一天可能很遥远。\n参考 William Morgan 的 eBPF, sidecars, and the future of the service mesh 这篇文章正好回答了我的关于 eBPF、sidecar 的疑问。 ↩︎\n关于 per-host 模式中的代理改造问题，Twitter 上有一个精彩的讨论 。 ↩︎\n","relpermalink":"/blog/ebpf-sidecar-and-service-mesh/","summary":"不管有没有 eBPF，在可预见的未来，服务网格都会基于运行在用户空间的 sidecar 代理（proxyless 模式除外）。","title":"请暂时抛弃使用 eBPF 取代服务网格和 sidecar 模式的幻想"},{"content":"《什么是 eBPF —— 新一代网络、安全和可观测性工具介绍》译自 O’Reilly 发布的报告 “What is eBPF”，作者是 Liz Rice，由 JImmy Song 翻译，英文原版可以在 O’Reilly 网站 上获取，中文版请在 云原生资料库 中阅读。\n","relpermalink":"/notice/what-is-ebpf/","summary":"新翻译了一本 O'Reilly 出品的报告。","title":"《什么是 eBPF》翻译电子书"},{"content":"不知道大家听说没有 PingCAP 推出的一个 OSSInsight.io 网站，可以根据 GitHub 上的事件，提供开源软件洞察，这个项目也开源在 GitHub 上。它可以提供以下方面的洞察能力，有点类似于 Google Analytics、Trends：\n比较 GitHub 仓库历史 Star 趋势图 开发者地理位置分布 开发者贡献时间热力图 编码活力，如每月 PR 数量、代码行数变化 分类趋势排名 网站截图 以下图片来自 OSSInsight 博客 ，展示了该网站的一些功能。\nKubernetes 和 Moby 的标记 star 的人员地理分布 K8s（上）和 Moby（下）的月度推送和提交 分类排名 你可以在首页输入一个 GitHub 仓库，查看该仓库的一些洞察信息。我查看了我的 rootsongjc/kubernetes-handbook 之后，发现它还以获得关注者的公司信息，如下图。\nrootsongjc/kubernetes-handbook 关注者的公司分布 这个网站有点类似于 CNCF 推出的 DevStats ，不过 DevStats 只能洞察 CNCF 托管的项目。\nDevStats 页面 评论 OSSInsight 也可以算是 CHAOSS 类软件的一种，比如 Linux 基金会下的 CHAOSS （Community Health Analytics Open Source Software）工作组有一个开源项目 GrimoireLab 就是做软件开发分析的。\nGrimoireLab 网站页面 如果你关注开源和技术趋势的话，网上还有一些类似的 GitHub 趋势网站，大家可以根据自己的需要选用。\n","relpermalink":"/blog/oss-insight/","summary":"推荐一个 PingCAP 推出的 OSSInsight.io 网站，可以根据 GitHub 上的事件，提供开源软件洞察，这个项目本身也开源在 GitHub 上。","title":"开源项目千千万，如何发现好项目？"},{"content":" 近日听闻 O’Reilly 将永久关闭在线学习网站 KataCoda，对于广大程序员和学习者来说，这无疑是一件痛心疾首的事情，以后我们再也看不到那只会变成的功夫猫了。\nKataCoda 简介 KataCoda 成立于 2016 年，它是一个在线学习平台，提供了上百个交互课程，用户可以登录免费学习。另外用户还可以基于 KataCoda 提供的基础镜像来构建和发布自己的在线课程，这一切都是免费的。\n你可以根据提示，在一个临时的容器环境中操作，输入命令、观察结果，这种实时的反馈式学习方式，让你不需要再为准备环境而操心，大大降低了众多技术的上手门槛，可以说这种方式对于计算机技术教育来说是一种“革命式”的。\nKataCoda 网站界面 这些交互环境没有任何网络限制，你可以访问任何网站，还可以构建临时的公开网站让互联网中的所有用户访问。这种便利可以说是云原生或者容器时代赋予我们的，\nO’Reilly 为什么关闭 KataCoda？ O’Reilly 与 2019 年底收购了 KataCoda，如今关闭该网站应该也实属无奈。在 O’Reilly 官网发布的 仅在 O’Reilly 内部利用 Katacoda 技术以及关闭 katacoda.com 的决定 这篇博客中，我们可以获得以下数据：\nKataCoda 有 28 万会员 KataCoda 上有超过 387,866 名独立用户已花费超过 74,711 小时在平台上学习 O’Reilly 有 280 万会员 以上数据仍然无法支撑 KataCoda 高昂的运营成本，主要是因为免费课程被滥用，比如用来挖矿，发送不良信息（所有免费课程连接互联网没有任何限制而且网速极快）。\nKataCoda 关闭之后怎么办？ KataCoda 上的很多免费课程其实都有在 GitHub 上开源，只有有另一个平台来托管，这些课程就可以继续使用。KataCoda 关闭后，还有众多交互式课程平台可以选择，比如下面这两个：\nKillercoda CloudYuga Instruqt 关于 O’Reilly 关闭 KataCoda 你有什么想法，欢迎在下面留言评论。\n","relpermalink":"/blog/goodbye-katacoda/","summary":"O'Reilly 宣布将于 6 月 15 日关闭在线学习网站 KataCoda。","title":"再见 KataCoda！"},{"content":"本文将指导你如何在 macOS 上编译 Istio 二进制文件和 Docker 镜像。\n构建前的准备 在正式开始构建前，参考这篇文档 ，以下是我的构建环境信息：\nmacOS 12.3.1 Darwin AMD64 Docker Desktop 4.8.1(78998) Docker Engine v20.10.14 开始构建 参考这篇文档 编译 Istio。\n首先在 GitHub 上 下载 Istio 代码，将代码下载到 $GOPATH/src/istio.io/istio 目录下，下文中的命令都在该根目录下执行。\n编译成二进制文件 执行下面的命令下载 Istio 依赖的包，这些包将下载到 vendor 目录下：\ngo mod vendor 然后执行下面的命令构建 Istio：\nsudo make build 如果你没有在命令前加 sudo，你可能遇到下面的错误：\nfatal: unsafe repository (\u0026#39;/work\u0026#39; is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work fatal: unsafe repository (\u0026#39;/work\u0026#39; is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work Makefile.core.mk:170: *** \u0026#34;TAG cannot be empty\u0026#34;. Stop. make: *** [build] Error 2 即使你按照提示执行了 git config --global --add safe.directory /work 在编译过程中还是会出现错误。\n构建完的二进制文件将保存在 out 目录下，其目录结构如下：\nout ├── darwin_amd64 │ ├── bug-report │ ├── client │ ├── envoy │ ├── extauthz │ ├── install-cni │ ├── istio-cni │ ├── istio-cni-taint │ ├── istio-iptables │ ├── istio_is_init │ ├── istioctl │ ├── logs │ ├── operator │ ├── pilot-agent │ ├── pilot-discovery │ ├── release │ └── server └── linux_amd64 ├── envoy ├── envoy-centos ├── logs └── release 同时构建出了 linux_amd64 和 darwin_amd64 架构的二进制文件。\n编译成 Docker 镜像 执行下面的将 Istio 编译成 Docker 镜像：\nsudo make build 编译根据你的网络情况，大概耗时 3 到 5 分钟。编译完成后，执行下面的命令你将看到 Istio 的 Docker 镜像。\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE localhost:5000/app_sidecar_centos_7 latest 2044037df94b 51 seconds ago 524MB localhost:5000/app_sidecar_ubuntu_jammy latest 5d8ae5ed55b7 About a minute ago 362MB localhost:5000/proxyv2 latest d4679412385f About a minute ago 243MB localhost:5000/install-cni latest 78f46d5771d2 About a minute ago 270MB localhost:5000/istioctl latest c38130a5adc8 About a minute ago 190MB localhost:5000/pilot latest 2aa9185ec202 About a minute ago 190MB localhost:5000/app latest 473adafaeb8d About a minute ago 188MB localhost:5000/operator latest 9ac1fedcdd12 About a minute ago 191MB localhost:5000/ext-authz latest 1fb5aaf20791 About a minute ago 117MB localhost:5000/app_sidecar_debian_11 latest 61376a02b95d 2 minutes ago 407MB localhost:5000/app_sidecar_ubuntu_xenial latest 7e8efe666611 2 minutes ago 418MB 编译出镜像以后，你就可以修改镜像名字并推送到自己的镜像仓库里了。\n总结 以上就是在 macOS 上构建 Istio 的过程，如果你已经下载好了构建所需要的的 Docker 镜像，那么构建时间将不超过一分钟，构建 Docker 镜像也只需要几分钟时间。\n参考 Using the Code Base - github.com ","relpermalink":"/blog/how-to-build-istio/","summary":"本文将指导你如何在 macOS 上编译 Istio。","title":"如何编译 Istio？"},{"content":"本文最早是基于 Istio 1.11 撰写，之后随着 Istio 的版本陆续更新，最新更新时间为 2022 年 5 月 12 日，关于本文历史版本的更新说明请见文章最后。本文记录了详细的实践过程，力图能够让读者复现，因此事无巨细，想要理解某个部分过程的读者可以使用目录跳转到对应的小节阅读。\n为了使读者能够更加直观的了解本文中执行的操作，在阅读本文前你也可以先观看下 Istio Workshop 第八讲视频 。\n观看视频点击观看\n为了理解本文希望你先阅读以下内容：\n理解 iptables Istio 数据平面 Pod 启动过程详解 内容介绍 本文基于 Istio 1.13 版本，将为大家介绍以下内容：\n什么是 sidecar 模式和它的优势在哪里。 Istio 中是如何做 sidecar 注入的。 Sidecar 代理是如何做透明流量劫持的。 iptables 的路由规则。 Envoy 代理是如何路由流量到上游的。 请大家结合下图理解本文中的内容，本图基于 Istio 官方提供的 Bookinfo 示例绘制，展示的是 reviews Pod 的内部结构，包括 Linux Kernel 空间中的 iptables 规则、Sidecar 容器、应用容器。\nIstio 流量劫持示意图 productpage 访问 reviews Pod，入站流量处理过程对应于图示上的步骤：1、2、3、4、Envoy Inbound Handler、5、6、7、8、应用容器。\nreviews Pod 访问 rating 服务的出站流量处理过程对应于图示上的步骤是：9、10、11、12、Envoy Outbound Handler、13、14、15。\n注意：图中的路径 16 近用于路由规则说明，它不不出现在当前示例中。实际上仅当 Pod 内发出的对当前 Pod 内的服务访问的时候才会途径它。\n上图中关于流量路由部分，包含：\nproductpage 服务请求访问 http://reviews.default.svc.cluster.local:9080/，当流量进入 reviews Pod 内部时，流量是如何被 iptables 劫持到 Envoy 代理被 Inbound Handler 处理的； reviews 请求访问 ratings 服务的 Pod，应用程序发出的出站流量被 iptables 劫持到 Envoy 代理的 Outbound Handler 的处理。 在阅读下文时，请大家确立以下已知点：\n首先，productpage 发出的对 reivews 的访问流量，是在 Envoy 已经通过 EDS 选择出了要请求的 reviews 服务的某个 Pod，知晓了其 IP 地址，直接向该 IP 发送的 TCP 连接请求。 reviews 服务有三个版本，每个版本有一个实例，三个版本中的 sidecar 工作步骤类似，下文只以其中一个 Pod 中的 sidecar 流量转发步骤来说明。 所有进入 reviews Pod 的 TCP 流量都根据 Pod 中的 iptables 规则转发到了 Envoy 代理的 15006 端口，然后经过 Envoy 的处理确定转发给 Pod 内的应用容器还是透传。 Sidecar 模式 将应用程序的功能划分为单独的进程运行在同一个最小调度单元中（例如 Kubernetes 中的 Pod）可以被视为 sidecar 模式。如下图所示，sidecar 模式允许您在应用程序旁边添加更多功能，而无需额外第三方组件配置或修改应用程序代码。\nSidecar 模式示意图 就像连接了 Sidecar 的三轮摩托车一样，在软件架构中， Sidecar 连接到父应用并且为其添加扩展或者增强功能。Sidecar 应用与主应用程序松散耦合。它可以屏蔽不同编程语言的差异，统一实现微服务的可观察性、监控、日志记录、配置、断路器等功能。\n使用 Sidecar 模式的优势 使用 sidecar 模式部署服务网格时，无需在节点上运行代理，但是集群中将运行多个相同的 sidecar 副本。在 sidecar 部署方式中，每个应用的容器旁都会部署一个伴生容器（如 Envoy 或 MOSN），这个容器称之为 sidecar 容器。Sidecar 接管进出应用容器的所有流量。在 Kubernetes 的 Pod 中，在原有的应用容器旁边注入一个 Sidecar 容器，两个容器共享存储、网络等资源，可以广义的将这个包含了 sidecar 容器的 Pod 理解为一台主机，两个容器共享主机资源。\n因其独特的部署结构，使得 sidecar 模式具有以下优势：\n将与应用业务逻辑无关的功能抽象到共同基础设施，降低了微服务代码的复杂度。 因为不再需要编写相同的第三方组件配置文件和代码，所以能够降低微服务架构中的代码重复度。 Sidecar 可独立升级，降低应用程序代码和底层平台的耦合度。 Sidecar 注入示例分析 以 Istio 官方提供的 bookinfo 中 productpage 的 YAML 为例，关于 bookinfo 应用的详细 YAML 配置请参考 bookinfo.yaml 。\n下文将从以下几个方面讲解：\nSidecar 容器的注入 iptables 规则的创建 路由的详细过程 apiVersion: apps/v1 kind: Deployment metadata: name: productpage-v1 labels: app: productpage version: v1 spec: replicas: 1 selector: matchLabels: app: productpage version: v1 template: metadata: labels: app: productpage version: v1 spec: serviceAccountName: bookinfo-productpage containers: - name: productpage image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9080 volumeMounts: - name: tmp mountPath: /tmp volumes: - name: tmp emptyDir: {} 再查看下 productpage 容器的 Dockerfile 。\nFROM python:3.7.4-slim COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt COPY test-requirements.txt ./ RUN pip install --no-cache-dir -r test-requirements.txt COPY productpage.py /opt/microservices/ COPY tests/unit/* /opt/microservices/ COPY templates /opt/microservices/templates COPY static /opt/microservices/static COPY requirements.txt /opt/microservices/ ARG flood_factor ENV FLOOD_FACTOR ${flood_factor:-0} EXPOSE 9080 WORKDIR /opt/microservices RUN python -m unittest discover USER 1 CMD [\u0026#34;python\u0026#34;, \u0026#34;productpage.py\u0026#34;, \u0026#34;9080\u0026#34;] 我们看到 Dockerfile 中没有配置 ENTRYPOINT，所以 CMD 的配置 python productpage.py 9080 将作为默认的 ENTRYPOINT，记住这一点，再看下注入 sidecar 之后的配置。\n$ istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml 我们只截取其中与 productpage 相关的 Deployment 配置中的部分 YAML 配置。\ncontainers: - image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 # 应用镜像 name: productpage ports: - containerPort: 9080 - args: - proxy - sidecar - --domain - $(POD_NAMESPACE).svc.cluster.local - --configPath - /etc/istio/proxy - --binaryPath - /usr/local/bin/envoy - --serviceCluster - productpage.$(POD_NAMESPACE) - --drainDuration - 45s - --parentShutdownDuration - 1m0s - --discoveryAddress - istiod.istio-system.svc:15012 - --zipkinAddress - zipkin.istio-system:9411 - --proxyLogLevel=warning - --proxyComponentLogLevel=misc:error - --connectTimeout - 10s - --proxyAdminPort - \u0026#34;15000\u0026#34; - --concurrency - \u0026#34;2\u0026#34; - --controlPlaneAuthPolicy - NONE - --dnsRefreshRate - 300s - --statusPort - \u0026#34;15020\u0026#34; - --trust-domain=cluster.local - --controlPlaneBootstrap=false image: docker.io/istio/proxyv2:1.5.1 # sidecar proxy name: istio-proxy ports: - containerPort: 15090 name: http-envoy-prom protocol: TCP initContainers: - command: - istio-iptables - -p - \u0026#34;15001\u0026#34; - -z - \u0026#34;15006\u0026#34; - -u - \u0026#34;1337\u0026#34; - -m - REDIRECT - -i - \u0026#39;*\u0026#39; - -x - \u0026#34;\u0026#34; - -b - \u0026#39;*\u0026#39; - -d - 15090,15020 image: docker.io/istio/proxyv2:1.5.1 # init 容器 name: istio-init Istio 给应用 Pod 注入的配置主要包括：\nInit 容器 istio-init：用于 pod 中设置 iptables 端口转发 Sidecar 容器 istio-proxy：运行 sidecar 代理，如 Envoy 或 MOSN。 iptables 规则注入解析 为了查看 iptables 配置，我们需要登陆到 sidecar 容器中使用 root 用户来查看，因为 kubectl 无法使用特权模式来远程操作 docker 容器，所以我们需要登陆到 productpage pod 所在的主机上使用 docker 命令登陆容器中查看。\n如果您使用 minikube 部署的 Kubernetes， …","relpermalink":"/blog/sidecar-injection-iptables-and-traffic-routing/","summary":"本文基于 Istio 1.13 版本，介绍了 sidecar 模式及其优势 sidecar 如何注入到数据平面，Envoy 如何做流量劫持和路由转发的，包括 Inbound 流量和 Outbound 流量。","title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解"},{"content":"本文将为你讲解：\nIstio 中 sidecar 自动注入过程 Istio 中的 init 容器启动过程 启用了 Sidecar 自动注入的 Pod 的启动流程 下图中展示了 Istio 数据平面中的 Pod 启动完后的组件。\nIstio 数据平面 Pod 内部组件 Istio 中的 sidecar 注入 Istio 中提供了以下两种 sidecar 注入方式：\n使用 istioctl 手动注入。 基于 Kubernetes 的 突变 webhook 准入控制器（mutating webhook addmission controller 的自动 sidecar 注入方式。 不论是手动注入还是自动注入，sidecar 的注入过程都需要遵循如下步骤：\nKubernetes 需要了解待注入的 sidecar 所连接的 Istio 集群及其配置； Kubernetes 需要了解待注入的 sidecar 容器本身的配置，如镜像地址、启动参数等； Kubernetes 根据 sidecar 注入模板和以上配置填充 sidecar 的配置参数，将以上配置注入到应用容器的一侧； 使用下面的命令可以手动注入 sidecar。\nistioctl kube-inject -f ${YAML_FILE} | kuebectl apply -f - 该命令会使用 Istio 内置的 sidecar 配置来注入，下面使用 Istio详细配置请参考 Istio 官网 。\n注入完成后您将看到 Istio 为原有 pod template 注入了 initContainer 及 sidecar proxy相关的配置。\nInit 容器 Init 容器是一种专用容器，它在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。\n一个 Pod 中可以指定多个 Init 容器，如果指定了多个，那么 Init 容器将会按顺序依次运行。只有当前面的 Init 容器必须运行成功后，才可以运行下一个 Init 容器。当所有的 Init 容器运行完成后，Kubernetes 才初始化 Pod 和运行应用容器。\nInit 容器使用 Linux Namespace，所以相对应用程序容器来说具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用程序容器则不能。\n在 Pod 启动过程中，Init 容器会按顺序在网络和数据卷初始化之后启动。每个容器必须在下一个容器启动之前成功退出。如果由于运行时或失败退出，将导致容器启动失败，它会根据 Pod 的 restartPolicy 指定的策略进行重试。然而，如果 Pod 的 restartPolicy 设置为 Always，Init 容器失败时会使用 RestartPolicy 策略。\n在所有的 Init 容器没有成功之前，Pod 将不会变成 Ready 状态。Init 容器的端口将不会在 Service中进行聚集。 正在初始化中的 Pod 处于 Pending 状态，但应该会将 Initializing 状态设置为 true。Init 容器运行完成以后就会自动终止。\n关于 Init 容器的详细信息请参考 Init 容器 - Kubernetes 中文指南/云原生应用架构实践手册 。\nInit 容器解析 Istio 在 pod 中注入的 Init 容器名为 istio-init，我们在上面 Istio 注入完成后的 YAML 文件中看到了该容器的启动命令是：\nistio-iptables -p 15001 -z 15006 -u 1337 -m REDIRECT -i \u0026#39;*\u0026#39; -x \u0026#34;\u0026#34; -b \u0026#39;*\u0026#39; -d 15090,15020 我们再检查下该容器的 Dockerfile 看看 ENTRYPOINT 是怎么确定启动时执行的命令。\n# 前面的内容省略 # The pilot-agent will bootstrap Envoy. ENTRYPOINT [\u0026#34;/usr/local/bin/pilot-agent\u0026#34;] 我们看到 istio-init 容器的入口是 /usr/local/bin/istio-iptables 命令行，该命令行工具的代码的位置在 Istio 源码仓库的 tools/istio-iptables 目录。\n注意：在 Istio 1.1 版本时还是使用 isito-iptables.sh 命令行来操作 IPtables。\nInit 容器启动入口 Init 容器的启动入口是 istio-iptables 命令行，该命令行工具的用法如下：\n$ istio-iptables [flags] -p: 指定重定向所有 TCP 流量的 sidecar 端口（默认为 $ENVOY_PORT = 15001） -m: 指定入站连接重定向到 sidecar 的模式，“REDIRECT” 或 “TPROXY”（默认为 $ISTIO_INBOUND_INTERCEPTION_MODE) -b: 逗号分隔的入站端口列表，其流量将重定向到 Envoy（可选）。使用通配符 “*” 表示重定向所有端口。为空时表示禁用所有入站重定向（默认为 $ISTIO_INBOUND_PORTS） -d: 指定要从重定向到 sidecar 中排除的入站端口列表（可选），以逗号格式分隔。使用通配符“*” 表示重定向所有入站流量（默认为 $ISTIO_LOCAL_EXCLUDE_PORTS） -o：逗号分隔的出站端口列表，不包括重定向到 Envoy 的端口。 -i: 指定重定向到 sidecar 的 IP 地址范围（可选），以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量。空列表将禁用所有出站重定向（默认为 $ISTIO_SERVICE_CIDR） -x: 指定将从重定向中排除的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量（默认为 $ISTIO_SERVICE_EXCLUDE_CIDR）。 -k：逗号分隔的虚拟接口列表，其入站流量（来自虚拟机的）将被视为出站流量。 -g：指定不应用重定向的用户的 GID。(默认值与 -u param 相同) -u：指定不应用重定向的用户的 UID。通常情况下，这是代理容器的 UID（默认值是 1337，即 istio-proxy 的 UID）。 -z: 所有进入 pod/VM 的 TCP 流量应被重定向到的端口（默认 $INBOUND_CAPTURE_PORT = 15006）。 以上传入的参数都会重新组装成 iptables 规则，关于该命令的详细用法请访问 tools/istio-iptables/pkg/cmd/root.go 。\n该容器存在的意义就是让 sidecar 代理可以拦截所有的进出 pod 的流量，15090 端口（Mixer 使用）和 15092 端口（Ingress Gateway）除外的所有入站（inbound）流量重定向到 15006 端口（sidecar），再拦截应用容器的出站（outbound）流量经过 sidecar 处理（通过 15001 端口监听）后再出站。关于 Istio 中端口用途请参考 Istio 官方文档 。\n命令解析\n这条启动命令的作用是：\n将应用容器的所有流量都转发到 sidecar 的 15006 端口。 使用 istio-proxy 用户身份运行， UID 为 1337，即 sidecar 所处的用户空间，这也是 istio-proxy 容器默认使用的用户，见 YAML 配置中的 runAsUser 字段。 使用默认的 REDIRECT 模式来重定向流量。 将所有出站流量都重定向到 sidecar 代理（通过 15001 端口）。 因为 Init 容器初始化完毕后就会自动终止，因为我们无法登陆到容器中查看 iptables 信息，但是 Init 容器初始化结果会保留到应用容器和 sidecar 容器中。\nPod 启动流程 启用了 Sidecar 自动注入的 Pod 启动流程如下：\nInit 容器先启动，向 Pod 中注入 iptables 规则，进行透明流量拦截。 随后，Kubernetes 会根据 Pod Spec 中容器的声明顺序依次启动容器，但这是非阻塞的，无法保证第一个容器启动完成后才启动下一个。istio-proxy 容器启动时，pilot-agent 将作为 PID 1 号进程，它是 Linux 用户空间的第一个进程，负责拉起其他进程和处理僵尸进程。pilot-agent 将生成 Envoy bootstrap 配置并拉起 envoy 进程；应用容器几乎跟 istio-proxy 容器同时启动，为了防止 Pod 内的容器在还没启动好的情况而接收到外界流量，这时候就绪探针就派上用场了。Kubernetes 会在 istio-proxy 容器的 15021 端口进行就绪检查，直到 isito-proxy 启动完成后 kubelet 才会将流量路由到 Pod 内。 在 Pod 启动完成后，pilot-agent 将变为守护进程监视系统其他进程，除此之外，该进程还为 Envoy 提供 Bootstrap 配置、证书、健康检查、配置热加载、身份支持及进程生命周期管理等。 Pod 内容器启动顺序问题 在 Pod 启动的过程中存在容器启动顺序问题，假设下面这种情况，应用容器先启动，请求其他服务，这时候 istio-proxy 容器还没启动完成，那么该请求将会失败，如果你的应用的健壮性不足，甚至可能导致应用容器崩溃，进而 Pod 重启。对于这种情况的解决方案是：\n修改应用程序，增加超时重试。 增加应用容器中进程的启动延迟，比如增加 sleep 时间。 在应用容器中增加一个 postStart 配置，检测应用进程是否启动完成，只有当检测成功时，Kubernetes 才会将 Pod 的状态标记为 Running。 总结 这篇文章带领大家了解了 Istio 数据平面中的 Pod 启动过程，还有因为 Pod 内容器启动顺序带来的问题。\n参考 istio 常见问题: Sidecar 启动顺序问题 - imroc.cc ","relpermalink":"/blog/istio-pod-process-lifecycle/","summary":"本文将为你讲解 Istio 的 Init 容器、Pod 内部进程及启动过程。","title":"Istio 数据平面 Pod 启动过程详解"},{"content":"iptables 作为 Linux 内核中的重要功能，有着广泛的应用，在 Istio 中默认就是利用 iptables 做透明流量劫持的。理解 iptables，对于我们理解 Istio 的运作有十分重要的作用。本文将为大家简单介绍下 iptbles。\niptables 简介 iptables 是 Linux 内核中的防火墙软件 netfilter 的管理工具，位于用户空间，同时也是 netfilter 的一部分。Netfilter 位于内核空间，不仅有网络地址转换的功能，也具备数据包内容修改、以及数据包过滤等防火墙功能。\n在了解 Init 容器初始化的 iptables 之前，我们先来了解下 iptables 和规则配置。\n下图展示了 iptables 调用链。\niptables 调用链 图片来源 iptables 中的表 Init 容器中使用的的 iptables 版本是 v1.6.0，共包含 5 张表：\nraw 用于配置数据包，raw 中的数据包不会被系统跟踪。 filter 是用于存放所有与防火墙相关操作的默认表。 nat 用于 网络地址转换 （例如：端口转发）。 mangle 用于对特定数据包的修改（参考损坏数据包 ）。 security 用于强制访问控制 网络规则。 注：在本示例中只用到了 nat 表。\n不同的表中的具有的链类型如下表所示：\n规则名称 raw filter nat mangle security PREROUTING ✓ ✓ ✓ INPUT ✓ ✓ ✓ OUTPUT ✓ ✓ ✓ ✓ ✓ POSTROUTING ✓ ✓ FORWARD ✓ ✓ ✓ 理解 iptables 规则 查看 istio-proxy 容器中的默认的 iptables 规则，默认查看的是 filter 表中的规则。\n$ iptables -L -v Chain INPUT (policy ACCEPT 350K packets, 63M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 18M packets, 1916M bytes) pkts bytes target prot opt in out source destination 我们看到三个默认的链，分别是 INPUT、FORWARD 和 OUTPUT，每个链中的第一行输出表示链名称（在本例中为INPUT/FORWARD/OUTPUT），后跟默认策略（ACCEPT）。\n每条链中都可以添加多条规则，规则是按照顺序从前到后执行的。我们来看下规则的表头定义。\npkts：处理过的匹配的报文数量 bytes：累计处理的报文大小（字节数） target：如果报文与规则匹配，指定目标就会被执行。 prot：协议，例如 tdp、udp、icmp 和 all。 opt：很少使用，这一列用于显示 IP 选项。 in：入站网卡。 out：出站网卡。 source：流量的源 IP 地址或子网，或者是 anywhere。 destination：流量的目的地 IP 地址或子网，或者是 anywhere。 还有一列没有表头，显示在最后，表示规则的选项，作为规则的扩展匹配条件，用来补充前面的几列中的配置。prot、opt、in、out、source 和 destination 和显示在 destination 后面的没有表头的一列扩展条件共同组成匹配规则。当流量匹配这些规则后就会执行 target。\ntarget 支持的类型\ntarget 类型包括 ACCEPT、REJECT、DROP、LOG 、SNAT、MASQUERADE、DNAT、REDIRECT、RETURN 或者跳转到其他规则等。只要执行到某一条链中只有按照顺序有一条规则匹配后就可以确定报文的去向了，除了 RETURN 类型，类似编程语言中的 return 语句，返回到它的调用点，继续执行下一条规则。target 支持的配置详解请参考 iptables 详解（1）：iptables 概念 。\n总结 以上就是对 iptables 的简要介绍，你已经了解了 iptables 是怎样运行的，规则链及其执行顺序。\n","relpermalink":"/blog/understanding-iptables/","summary":"本文将为大家简单介绍下 iptables，其中的表及执行顺序。","title":"理解 iptables"},{"content":"云原生资料库地址：https://lib.jimmysong.io 云原生资料库项目是一个文档项目，使用 Wowchemy 主题构建，开源在 GitHub 上：https://github.com/rootsongjc/cloud-native-library 同时笔者也对网站首页、菜单和目录结构进行了调整，网站上的图书栏目将使用新的主题来维护，发布在云原生资料库-书目列表 中。\n云原生资料库定位 云原生资料库收集了笔者自 2017 年以来的发表和翻译的云原生相关图书、资料等，是对已发布的的十几本资料的一个梳理和补充。原有的资料会继续保存在原仓库中，但不再继续维护。\n另外本站活动栏目也进行了改版，迁移到了新的主题页面 。\n","relpermalink":"/notice/cloud-native-public-library/","summary":"一站式云原生资料库，是对已发布资料的梳理。","title":"云原生资料库发布"},{"content":"更新时间：2022年03月22日\n我们正在中国招聘 Team Leader 和开发工程师，全球远程办公，让你工作更自由！\nTetrate logo 你想要加入由世界级工程师组成的团队吗？使用 Istio 、Envoy 、Apache SkyWalking 等开源项目来定义下一代云原生网络。下面是我们正在招聘的部分职位。\n分布式系统工程师 - Go 语言（管理平面团队） 我们正在为跨越传统和现代基础设施的关键任务企业应用建立一个安全、强大、高可用的服务网格平台。管理平面团队负责构建平台的主要负责：用户配置、观察和与网格交互的公共 API，为混合和多云服务网格结构提供多租户和安全性的主要工作流。\n在复杂的大型基础设施和灵活但可管理的、有意义的 API 之间架起桥梁是具有挑战性的。我们正在寻找具有强大的分布式系统经验的工程师加入，帮助建立运行时，使平台能够扩展。\n职责\n在平台核心运行时及其 API 的架构方面进行合作。 设计并实现一个分布图的架构，特别关注数据复制、传播和 CAP 定理的挑战。 为 NGAC（下一代访问控制）的生产就绪的参考实施作出贡献。 设计和实施新的 NGAC 访问策略。 针对不同的使用情况进行图的优化，如图的遍历和图的减少。 要求\n基于基本原理的问题解决能力；以职能驱动决策，以第一原则为基础的思维方式。我们不以 “职称 “为导向，我们重视结果而不是过程。 表现出对行动的偏爱；推动行动，高质量地按时完成。 在寻找最佳创意时，你没有自我意识；你在你的专业之外做出了有效的贡献；你从团队的最佳结果的角度来考虑解决问题。 充满好奇心，善于从模糊不清的地方看到机会。 理解关注细节与注重细节之间的区别。 重视自主性和结果而非过程。 有使用 Go 构建和测试软件的经验。 有编写 gRPC API 的经验。 有分布式系统的经验，了解 CAP 定理。 有分布式数据库的经验者优先。 对分布式系统技术，如复制日志或 CRDs 有深刻的理解。 熟悉 Kubernetes、Istio 和 Envoy 等服务网格技术。 对网络协议、概念、分布式系统的一致性属性、识别和协调配置漂移的技术有良好的理解。 有为开源项目做贡献的经验者优先。 有授权系统（如 RBAC、ABAC）方面的经验为佳。 工作地点\n全球范围远程办公，也可在旧金山、波士顿、东京和印度尼西亚万隆 / 坦噶尔的办公室工作。\n软件工程师（开发人员生产力团队） Tetrate 的可维护性团队负责确保工程组织的许多其他部分不仅拥有成功所需的工具，而且在 5 年后他们将继续热爱他们的代码库。这包括评估我们的构建系统和工具来发展它们。与 Bazel、Rust 中的自定义工具以及用各种语言编写的各种质量的测试一起工作。\n这个角色将用你的想法和工具不断提高 Tetrate 的基础设施的规模效率和开发人员的生产力。如果你关心开发人员的经验，对生产力的热情，以及在测试方面的工作经验，你会发现这个角色很适合你。\n职责\n了解开发人员的工作流程和构建系统，以改善构建时间，使用 Bazel 工作。 设计、开发和提供分布式工程构建工具和平台，用于各种代码库语言。 为代码库设计新的提示器，以帮助执行高质量的 API 使用。 修复脆弱的端到端测试，并确保我们的基础设施能够跟上我们的开发人员的步伐。 要求\n熟悉 Rust、Go 或 JavaScript 任意一种语言。 熟练使用 Bazel，了解其可靠性 / 可追溯性。 你习惯于为 Build、云端缓存创建额外的工具，并帮助开发者使用他们的 IDE。 熟悉测试的挑战，并乐于使测试正确性和可靠性。 重视自主性和结果而非过程。 有系统的解决问题的方法，再加上出色的沟通技巧和主人翁意识。 表现出对行动的偏爱，推动行动，高质量地按时完成任务。 工作地点\n全球范围远程办公，也可在旧金山、波士顿、东京和印度尼西亚万隆 / 坦噶尔的办公室工作。\n软件工程师 / 站点可靠性工程师（云服务团队） Tetrate 的云服务团队负责建立和运营基于 SaaS 的应用程序。包括 SRE 和软件工程的各个方面，如发布工程、消除劳累和建立基础设施抽象；使用现代工具，如 Pulumi 和主要在 AWS 上的无服务器优先方法。\n要求\n与应用团队合作，为关键任务的客户系统设计、开发和提供高可用性、安全的服务 建立变更管理管道 通过自动化识别和消除重复劳动 具有在任何一个公有云供应商上提供高可用性网络服务的经验 熟悉基础设施即代码的工具，如 Terraform 或 Pulumi 熟悉 Go、TypeScript 或 JavaScript 重视自主性和注重结果而非过程 有系统的解决问题的方法，加上优秀的沟通技巧和主人翁意识 表现出对行动的偏爱，推动行动，高质量地按时完成。 工作地点\n全球范围内远程办公。\n软件工程师 - Go 语言 加入一个由世界级工程师组成的团队，使用 Istio、Envoy 和一些开放项目来定义下一代的云原生网络服务。我们正在寻找在使用 Golang 和 gRPC 构建分布式系统方面有经验的后端软件工程师。\n职责\n设计和实现公共 API，用于配置服务网格的所有方面：网络、安全和可观察性。 实施将对客户产生直接影响的新产品功能。 帮助研究不同的新技术，如 NGAC（下一代访问控制）。 要求\n基于基本原理的问题解决能力；以职能驱动决策，以第一原则为基础的思维方式。我们不以 “职称 “为导向，我们重视结果而不是过程。 表现出对行动的偏爱；推动行动，高质量地按时完成。 在寻找最佳创意时，你没有自我意识；你在你的专业之外做出了有效的贡献；你从团队的最佳结果的角度来考虑解决问题。 充满好奇心，善于从模糊不清的地方看到机会。 理解关注细节与注重细节之间的区别。 重视自主性和结果而非过程。 有使用 Go 构建和测试软件的经验。 有编写 gRPC API 的经验。 有使用 SQL 关系数据库和 Key/Value 数据存储的经验。 熟悉 Kubernetes，服务网格技术，如 Istio 和 Envoy 对网络协议、概念、分布式系统的一致性属性、识别和协调配置漂移的技术有良好的理解。 有为开源项目做贡献的经验者优先。 有认证技术（如 OIDC）和授权系统（如 RBAC、ABAC）的经验者优先。 工作地点\n全球范围远程办公，也可在旧金山、波士顿、东京和印度尼西亚万隆 / 坦噶尔的办公室工作。\nIstio 上游贡献者（Go 语言） 我们的团队正物色在分布式系统方面拥有资深经验的工程师。我们正为众多无论对传统或现代建设来说皆首屈一指的企业级应用系统打造一个安全、稳固，且接触面广的服务网格平台。如果你是 Istio 的支持者，并希望为社区贡献更多力量，你将能通过这份工作定期为 Istio 上游作出重大贡献，机会难得。\n要求\n具备以基本面为基础的解决问题的能力、「第一性原理」思维，并能以功能为首要考虑，推动决策。 能以行动为先，避免分析瘫痪。能在工作结束前时刻保持动力，按时完成工作。 能不囿于自我地进行构想，以优秀的意念为先。 求知若渴，能时刻在不明朗的情境中发掘机遇。 明白「留意细节」和「注重细节」的区别。 注重自主性，结果导向。 能在专长以外作出有效贡献。 拥有运用 Go 语言建立分布式系统平台的经验。 熟悉 Kubernetes，以及 Istio 及 Envoy 等服务网格技术对网络协议、概念、分布式系统一致性的特质有透彻的了解及掌握，拥有分辨及抑制配置漂移的技巧。 如有贡献开源项目的经验则更佳。 如熟悉 WebAssembly 则更佳。 如熟悉 Go 语言、硬件 / 软件负载均衡（F5、NGINX）、HSM module、Active Directory/LDAP 更佳。 分布式系统工程师，企业基础架构（数据平面）Go 或 C++ 开发者 我们正在寻找有使用 Golang 和 gRPC 构建分布式系统经验的后端工程师。我们正在为财富 500 强企业的关键业务构建安全、高可用的服务网格（Service Mesh）平台，横跨传统和现代基础设施。您应具备较强的分布式系统和网络的基础知识。熟悉 Kubernetes、Istio 和 Envoy 等技术，有为开源项目做贡献的经验更佳。\n要求\n有使用 C++、Golang、gRPC 构建分布式系统平台的经验。 熟悉 Kubernetes，Istio、Envoy 等服务网格技术。 对网络协议、概念、分布式系统的一致性、识别和协调配置漂移的技术有很好的理解。 有为开源项目做贡献的经验更佳。 熟悉以下内容更佳：WebAssembly、Authorization、NGAC、RBAC、ABAC。 熟悉硬件 / 软件负载均衡器（F5、NGINX）、HSM 模块、Active Directory/LDAP 者更佳。 站点可靠性工程师（SRE） 站点可靠性工程（SRE）将软件和系统工程结合起来，构建和运行可扩展、大规模分布式、容错系统。作为团队的一员，你将致力于确保 Tetrate 平台具有适合用户需求的可靠性 / 正常运行时间，以及快速的改进速度。此外，我们的工程工作主要集中在建设基础设施，提高平台故障排除能力，并通过自动化减少人工干预。\n要求\n有系统的解决问题的方法，加上优秀的沟通技巧，有主人翁意识 / 完成感和自我导向的动力。 熟悉分布式系统（有状态和 / 或无状态）和网络的操作、调试和故障排除。 熟悉 Kubernetes、服务网格技术（如 Istio 和 Envoy）能够调试、优化代码、自动化日常任务。 至少有以下一种语言的编程经验：C++、Rust、Python、Go。 熟悉使用 SLO 和 SLI 以规范的方式量化故障和可用性的概念。 有性能分析和调优经验者优先。 工作地点 我们的产业遍布全球，在中国、印尼、印度、日本、美国、加拿大、爱尔兰、荷兰、西班牙和乌克兰都有业务。我们支持远程工作，并在旧金山、波士顿、巴塞罗那、 万隆 / 坦格朗（印尼）等地设有办公室。\n对英语的要求 我们鼓励口头英语和书面英语的非同步沟通，除 Team Leader 外，受聘者不必操流利英语。\n投递简历 请将展示你代码风格的 GitHub 或在线链接与你的英文简历一起发送至：careers@tetrate.io （并抄送给 jimmy@tetrate.io ） 或联系 Jimmy Song 了解详情。\nTetrate 介绍 基于 Istio、Envoy、Apache SkyWalking 等开源项目，Tetrate 的旗舰产品 Tetrate Service Bridge （TSB）可以实现传统和现代工作负载的桥接。在任何环境下，客户都可以为所有工作负载获得一致的内置可观察性、运行时安全性和流量管理。\n除了技术之外，Tetrate 还带来了一个世界级的团队，领导开放的 Istio、Envoy、Apache SkyWalking 等项目，提供企业可以用来实现人员和流程现代化的最佳实践。\nTetrate 的工作氛围 Forbes Tetrate 被认定为2022 年福布斯美国最佳创业公司雇主\nTetrate 2018 年创立于硅谷，当年获得 1250 万美元 A 轮融资，2021 年获得 4000 万美元 B 轮融资 。我们的团队主要来自加拿大、中国、印度、印尼、爱尔兰、日本、新西兰、新加坡、西班牙、荷兰和乌克兰。我们的招聘目的很简单，我们期望物色最佳人才，不论背景与居住地。我们并不提供一个头衔或角色，反而更着重于要解决的问题。若可透过实时通话，讨论彼此的交集和兴趣领域，则更为理想。你可以自由选择工作地点，弹性的休假时间和工作日程。我们每年会组织 3 到 4 次的线下团队聚会，最近几次分别举办于旧金山、西雅图、巴塞罗那、圣迭戈、华盛顿和万隆（2020 年以来因为疫情原因暂停）。团队建设活动能拉近团员间的距离，有利于你在远程工作时的协作。\nTetrate 公司名称来历 Tetrate 是数学术语 Tetration （迭代幂次） 的变体，Tetrate 员工自称 Tetrand。想要 …","relpermalink":"/notice/tetrate-recruit/","summary":"注册在硅谷，全球化远程办公。","title":"企业级服务网格提供商 Tetrate 公司招聘"},{"content":"在我的前两篇博客中：\nIstio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解 Sidecar 中的流量类型及 iptables 规则详解 我向你详细介绍了 Istio 数据平面中的流量，但数据平面并不能孤立的存在，本文将向你展示 Istio 中的控制平面和数据平面各组件的端口及其功能，有助于你了解这些流量之间的关系及故障排查。\nIstio 中的组件及端口示意图 按照习惯，我们首先展示一个全局示意图。下图展示的是 Istio 数据平面中 sidecar 的组成，以及与其交互的对象。\nIstio sidecar 组成示意图 我们可以使用 nsenter 命令进入Bookinfo 示例的 productpage Pod的网络空间，查看其内部监听的端口信息。\nIstio sidecar 中监听的端口信息 从图中我们可以看到除了 productpage 应用本身监听的 9080 端口以外，Sidecar 容器还有监听大量的其他端口，如 15000、15001、15004、15006、15021、15090 等，你可以在 Istio 文档 上了解 Istio 中使用的端口。\n我们再进入 productpage Pod 中，使用 lsof -i 命令查看它打开的端口，如下图所示。\nProductpage Pod 中打开的端口 我们可以看到其中有 pilot-agent 与 istiod 建立了 TCP 连接，上文中所述的监听中的端口，还有在 Pod 内部建立的 TCP 连接，这些连接对应了文章开头的示意图。\nSidecar 容器（istio-proxy ）的根进程是 pilot-agent，启动命令如下图所示：\nSidecar 中的进程 从图中我们可以看到，它 pilot-agent 进程的 PID 是 1，是它拉起了 envoy 进程。\n在 istiod 的 Pod 中查看它打开的端口，如下图所示。\nIstiod 中的端口 我们可以看到其中的监听的端口、进程间和远程通信连接。\nIstio 中各端口的功能概述 这些端口在你进行问题排查时可以起着举足轻重的作用。下面将根据端口所在的组件和功能分类描述。\nIstiod 中的端口 Istiod 中的端口相对比较少且功能单一：\n9876：ControlZ 用户界面，暴露 istiod 的进程信息 8080：istiod 调试端口，通过该端口可以查询网格的配置和状态信息 15010：暴露 xDS API 和颁发纯文本证书 15012：功能同 15010 端口，但使用 TLS 通信 15014：暴露控制平面的指标给 Prometheus 15017：Sidecar 注入和配置校验端口 Sidecar 中的端口 从上文中，我们看到 sidecar 中有众多端口：\n15000：Envoy 管理接口 ，你可以用它来查询和修改 Envoy 代理的的配置，详情请参考 Envoy 文档 。 15001：用于处理出站流量。 15004：调试端口，将在下文中解释。 15006：用于处理入站流量。 15020：汇总统计数据，对 Envoy 和 DNS 代理进行健康检查，调试 pilot-agent 进程，将在下文中详细解释。 15021：用于 sidecar 健康检查，以判断已注入 Pod 是否准备好接收流量。我们在该端口的 /healthz/ready 路径上设置了就绪探针，Istio 把 sidecar 的就绪检测交给了 kubelet，最大化利用 Kubernetes 平台自身的功能。envoy 进程将健康检查路由到 pilot-agent 进程的 15020 端口，实际的健康检查将发生在那里。 15053：本地 DNS 代理，用于解析 Kubernetes DNS 解析不了的集群内部域名的场景。 15090：Envoy Prometheus 查询端口，pilot-agent 将通过此端口收集统计信息。 以上端口可以分为以下几类：\n负责进程间通信，例如 15001、15006、15053 负责健康检查和信息统计，例如 150021、15090 调试：15000、15004 下文将对几个重点端口详解。\n15000 端口 15000 是 Envoy 的 Admin 接口，该接口允许我们修改 Envoy，并获得一个视图和查询指标和配置。\n管理接口由一个具有多个端点的 REST API 和一个简单的用户界面组成，你可以使用下面的命令开启 productpage Pod 中的 Envoy 管理接口视图。\nkubectl -n default port-forward deploy/productpage-v1 15000 在浏览器中访问 http://localhost:15000，你将看到 Envoy Admin 界面如下图所示。\nEnvoy Admin 界面 15004 端口 通过 pilot-agent 代理 istiod 8080 端口上的调试端点，你可以进入数据平面 Pod 中访问 localhost 的 15004 端口查询网格信息，其效果与下面的 8080 端口等同。\n8080 端口 你还可以在本地转发 istiod 8080 端口，请运行下面的命令。\nkubectl -n istio-system port-forward deploy/istiod 8080 在浏览器中访问 http://localhost:8080/debug，你将看到调试端点，如下图所示。\nPilot 调试控制台 当然，这只是一种获取网格信息和调试网格的方式，你还可以使用 istioctl 命令或 Kiali 来调试，那样将更加高效和直观。\n15020 端口 15020 端口有三大功能：\n汇总统计数据：查询 15090 端口获取 envoy 的指标，也可以配置查询应用程序的指标，将 envoy、应用程序和自身的指标汇总以供 Prometheus 收集。对应的调试端点是 /stats/prometheus。 对 Envoy 和 DNS 代理进行健康检查：对应的调试端点是 /healthz/ready 和 /app-health。 调试 pilot-agent 进程：对应的调试端点是 /quitquitquit、debug/ndsz 和 /debug/pprof。 下图展示的是使用本地端口转发后，在浏览器中打开 http://localhost:15020/debug/pprof 看到的调试信息。\npprof 端点 图中信息展示的是 pilot-agent 的堆栈信息。\n总结 通过对 Istio 中各组件端口的了解，你应该对 Istio 中各组件的关系及其内部流量有了更进一步的认识，熟悉这些端口的功能，有助于对网格的故障排除。\n","relpermalink":"/blog/istio-components-and-ports/","summary":"本文将向你介绍 Istio 控制平面和数据平面的各个端口及功能。","title":"Istio 中的各组件端口及功能详解"},{"content":"我在之前的一篇博客中 讲解过 Istio 中 sidecar 的注入、使用 iptables 进行透明流量拦截及流量路由的详细过程，并以 Bookinfo 示例中的 productpage 服务访问 reviews 服务，和 reviews 服务访问 ratings 服务为例绘制了透明流量劫持示意图。在那个示意图中仅展示了 reviews pod 接收流量和对外访问的路由，实际上 sidecar 内的流量远不止于此。\nISTIO_OUTPUT 规则 在所有的 iptables 调用链中最复杂的一个是 ISTIO_OUTPUT，其中共有 9 条规则如下：\nRule target in out source destination 1 RETURN any lo 127.0.0.6 anywhere 2 ISTIO_IN_REDIRECT any lo anywhere !localhost owner UID match 1337 3 RETURN any lo anywhere anywhere !owner UID match 1337 4 RETURN any any anywhere anywhere owner UID match 1337 5 ISTIO_IN_REDIRECT any lo anywhere !localhost owner GID match 1337 6 RETURN any lo anywhere anywhere !owner GID match 1337 7 RETURN any any anywhere anywhere owner GID match 1337 8 RETURN any any anywhere localhost 9 ISTIO_REDIRECT any any anywhere anywhere 本文将向你展示 Istio sidecar 中的六种流量类型及其 iptables 规则， 以示意图的形式带你一览其全貌，其中详细指出了路由具体使用的是 ISTIO_OUTPUT 中的哪一条规则。\nSidecar 中的 iptables 流量路由 Sidecar 中的流量可以划分为以下几类：\n远程服务访问本地服务：Remote Pod -\u0026gt; Local Pod 本地服务访问远程服务：Local Pod -\u0026gt; Remote Pod Prometheus 抓取本地服务的 metrics：Prometheus -\u0026gt; Local Pod 本地 Pod 服务间的流量：Local Pod -\u0026gt; Local Pod Envoy 内部的进程间 TCP 流量 Sidecar 到 Istiod 的流量 下面将依次解释每个场景下 Sidecar 内的 iptables 路由规则。\n类型一：Remote Pod -\u0026gt; Local Pod 以下是远程服务、应用或客户端访问数据平面本地 Pod IP 的 iptables 规则。\nRemote Pod -\u0026gt; RREROUTING -\u0026gt; ISTIO_INBOUND -\u0026gt; ISTIO_IN_REDIRECT -\u0026gt; Envoy 15006（Inbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 1 -\u0026gt; POSTROUTING -\u0026gt; Local Pod\n我们看到流量只经过一次 Envoy 15006 Inbound 端口。这种场景下的 iptables 规则的示意图如下。\nRemote Pod 到 Local Pod 类型二：Local Pod -\u0026gt; Remote Pod 以下是本地 Pod IP 访问远程服务经过的 iptables 规则。\nLocal Pod-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 9 -\u0026gt; ISTIO_REDIRECT -\u0026gt; Envoy 15001（Outbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 4 -\u0026gt; POSTROUTING -\u0026gt; Remote Pod\n我们看到流量只经过 Envoy 15001 Outbound 端口。\nLocal Pod 到 Remote Pod 以上两种场景中的流量都只经过一次 Envoy，因为该 Pod 中只有发出或接受请求一种场景发生。\n类型三：Prometheus -\u0026gt; Local Pod Prometheus 抓取数据平面 metrics 的流量不会也无须经过 Envoy 代理。\n这些流量通过的 iptables 规则如下。\nPrometheus-\u0026gt; RREROUTING -\u0026gt; ISTIO_INBOUND（对目的地为 15020、15090 端口流量将转到 INPUT）-\u0026gt; INPUT -\u0026gt; Local Pod\n这种场景下的 iptables 规则的示意图如下。\nPrometheus 到 Local Pod 类型四：Local Pod -\u0026gt; Local Pod 一个 Pod 可能同时存在两个或多个服务，如果 Local Pod 访问的服务也在该当前 Pod 上，流量会依次经过 Envoy 15001 和 Envoy 15006 端口最后到达本地 Pod 的服务端口上。\n这些流量通过的 iptables 规则如下。\nLocal Pod-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 9 -\u0026gt; ISTIO_REDIRECT -\u0026gt; Envoy 15001（Outbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 2 -\u0026gt; ISTIO_IN_REDIRECT -\u0026gt; Envoy 15006（Inbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 1 -\u0026gt; POSTROUTING -\u0026gt; Local Pod\nLocal Pod 到 Local Pod 类型五：Envoy 内部的进程间 TCP 流量 Envoy 内部进程的 UID 和 GID 为 1337，它们之间的流量将使用 lo 网卡，使用 localhost 域名来通信。\n这些流量通过的 iptables 规则如下。\nEnvoy 进程（Localhost） -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 8 -\u0026gt; POSTROUTING -\u0026gt; Envoy 进程（Localhost）\nEnvoy 内部的进程间 TCP 流量 类型六：Sidecar 到 Istiod 的流量 Sidecar 需要访问 Istiod 以同步配置，pilot-agent 进程会向 Istiod 发送请求，以同步配置。\n这些流量通过的 iptables 规则如下。\npilot-agent 进程 -\u0026gt; OUTPUT -\u0026gt; Istio_OUTPUT RULE 9 -\u0026gt; Envoy 15001 (Outbound Handler) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 4 -\u0026gt; POSTROUTING -\u0026gt; Istiod\nSidecar 到 Istiod 的流量 总结 Istio 注入在 Pod 内或虚拟机中安装的所有 sidecar 代理组成了服务网格的数据平面，也是 Istio 的主要工作负载所在地，通过 Istio 中的透明流量劫持 及这篇博客，相信你一定对 sidecar 代理中的流量有了一个深刻的了解，但这还只是管中窥豹，略见一斑，在我的下一篇博客 中，我将带你了解 Envoy 中各个组件的端口及其功能，这样可以让我们对 Istio 中的流量有一个更全面的了解。\n","relpermalink":"/blog/istio-sidecar-traffic-types/","summary":"本文将向你展示 Istio sidecar 中的六种流量类型及其 iptables 规则，并以示意图的形式带你一览其全貌。","title":"Istio sidecar 中的流量类型及 iptables 规则详解"},{"content":"在 2022 年 4 月 25 日， IstioCon 2022 开幕的当天，Istio 社区宣布正在申请将项目捐献给 CNCF ，这是 Istio 项目的一个里程碑，企业级服务网格公司 Tetrate 的 CEO/Istio 项目联合创始人 Varun Talwar 对此进行了解读。\n以下是来自 Varun 对 Istio 捐献给 CNCF 的解读 。\n将 Istio 纳入 CNCF，使得 Istio 和 Envoy 的发展更容易同步推进。它还有助于将 Istio 与 Envoy 一起定位为 CNCF 验证的 “云原生技术栈” 的一部分。根据 CNCF 的年度调查 ，到目前为止，Istio 是生产中最受欢迎和使用最多的服务网格。有 20 多家不同的公司在推动 Istio 社区的发展，这一宣布为 CNCF 管理下的持续创新和增长创造了条件。\n2016：Istio 的起源 我想借此机会解释一下 Istio 的起源。Istio 来自谷歌的 API 平台团队，名为 One Platform。(今天，具有讽刺意味的是，Istio 是美国政府项目 Platform One 的一部分，它使用 Tetrate 产品和服务）。一个平台利用了谷歌所有的基础设施优势（stubby、monarch、loas 等），并增加了最初的服务管理经验，并将其全部暴露给应用团队。\n每个团队都会编写他们的方案和方法，并定义他们的 “One Platform API”。一旦与 API 平台团队达成一致，各团队就不必再处理任何跨领域的问题，因为 Istio 处理了这些服务：流量管理、弹性、可观察性（使用具有一致名词的每个服务的预建仪表板）、认证、授权、速率限制等等。\nIstio 的想法来自于此；我们基本上采用了 One Platform 的想法，将 Envoy 加入其中（作为一个更好的数据平面），并将其与 LOAS 服务身份概念相结合，也就是今天世人所知的 Spiffe）。我们把这个想法告诉了 12 家公司，他们都很喜欢这个想法。这些公司包括大型互联网公司、金融服务公司和科技公司，特别是 SaaS 供应商。\n2017：形成核心 2017 年 5 月的，Istio 在 Gluecon 上首次公布 。0.1 展示了 Istio 的潜力，引发了大量的关注和讨论。\n2018-2019：稳定核心，增加能力 接下来的两年里，我们收集了客户的需求，将使用反馈内化，并稳定了核心功能。此外，我们还做出了一些关键的架构决定，如定义多集群模型，并将代码重新架构为一个单一的二进制文件，以方便使用。\n2020：团结社区 随着 Istio 的采用和用户生态系统的发展，人们对管理和商标保护的担忧也越来越大。然而，正如我们在这里 所提到的，作为一个社区保持团结是项目成功的关键。我可以自豪地说，Istio 就是这样做的。因此，今天加入 CNCF 的行动是发展社区和建立最终用户信任的又一步骤。\n2021：向 Wasm 和其他领域发展 人们对加入其他基础设施，如虚拟机、功能和裸机工作负载，以及使用 Wasm 等技术的定制和其他功能作为本地 API 的兴趣越来越大，这样用户就不必再使用 Envoy 过滤器了。2021 年见证了其中一些功能的建立和推广。\n“Varun Talwar 是项目的创始人之一，他一直认为 Istio 是云原生生态系统的一个重要组成部分。今天的公告验证了他对项目的愿景，我要感谢 Tetrate 成为 Istio 和我们社区的有力支持者。\u0026#34;——Louis Ryan（Istio 联合创始人，谷歌工程负责人）\n零信任的基础 关于零信任的话题已经有很多讨论，但很少有明确的说法。正如 Eric Brewer 今天在 IstioCon 的主题演讲 中提到的，Istio 正在成为零信任的一个重要组成部分。其中最主要的是面向身份的控制，而不是面向网络的控制。这方面的核心原则在谷歌白皮书《BeyondProd：云原生安全的新方法》 。\n然而，作为一个行业，这里有更多的事情要做。我们需要确保我们可以把应用用户和数据服务都带进来。如果我们能将身份概念扩展到用户，并为我们提供灵活而丰富的策略机制来指定、监控和跟踪访问控制，我们就能达到一个可操作的零信任结构 —— 一个将用户、服务和数据统一到一个管理层的结构。我在 2020 年为美国国家标准与技术研究院（NIST）举办的围绕信任云原生应用的主题演讲中也提到了这一点。这就是为什么我们在 Tetrate 创建了 Tetrate Service Bridge —— 一个管理平面，使大型组织可操作。\nTetrate Service Bridge 的基础是：\n用户、服务和数据的身份。每个人都有一个加密身份，构成所有政策的骨干。 策略和访问控制。定义 Istio 策略，也包括应用和组织策略，包括用户和设备，以及大规模管理它们的能力。 自动化。在运行时自动化、测量和持续监测策略的能力。 如果我们能让企业以这种方式为云原生工作负载部署和运营安全，我们就能作为一个行业取得巨大进步。\n人才 归根结底，没有高素质、富有创造性的人才，任何项目或技术都不会成为主流。在 Tetrate，我们相信我们需要对社区进行有关这项技术的教育，并为负责任的采用路径做出贡献。因此，我们提供世界级的认证和免费的在线培训课程，使社区中的任何人都可以在 academy.terate.io 轻松参加 Istio 和 Envoy 的初级和高级课程。\n我们 Tetrate 的所有人，特别是我自己，都期待着下一步的发展，我们将始终支持 Istio 项目和社区。\n","relpermalink":"/blog/istio-has-applied-to-join-the-cncf/","summary":"来自 Tetrate CEO、Istio 联合创始人 Varun Talwar 的解读。","title":"Istio 捐献给 CNCF 意味着什么？"},{"content":"Tetrate 是企业级服务网格领域的主要玩家之一，是 Istio、Envoy 和 SkyWalking 开源项目的发起者或主要参与者。本文将向你介绍 Tetrate 发起的几个开源项目：\nTetrate Istio Distro/GetMesh ：Tetrate Istio 发行版 wazero ：使用 Go 语言编写的无需平依赖性的 WebAssembly 运行时 func-e ：Envoy 构建命令行 istio-security-analyzer ：Istio 安全扫描工具 Tetrate Istio Distro/GetMesh Tetrate Istio 发行版，又名 GetMesh，为 Kubernetes 或应用平台安装和管理经过审核的 Istio。\n最简单的安装、操作和升级 Istio 的方法 为您的应用和云平台进行测试和加固 用户、生态系统和合作伙伴的社区中心 GetMesh 是一个命令行工具，你可以用它来：\n强制获取 Istio 的认证版本，并只允许安装 Istio 的兼容版本 允许在多个 istioctl 版本之间无缝切换 符合 FIPS 标准 通过整合多个来源的验证库，提供基于平台的 Istio 配置验证 使用一些云提供商证书管理系统来创建 Istio CA 证书，用于签署服务网格管理工作负载 提供附加的与云提供商多个集成点 使用下面的命令就可以安装 GetMesh：\ncurl -sL https://istio.tetratelabs.io/getmesh/install.sh | bash 注意：如果你位于中国大陆，执行上面的命令需要翻墙。\n想要了解更多关于 Tetrate Istio Distro/GetMesh 的信息请访问 https://istio.tetratelabs.io wazero wazero 是一个用 Go 语言编写的符合 WebAssembly 1.0（20191205） 规范的运行时。\nWebAssembly 是一种安全运行用其他语言编译的代码的方法。运行时 执行 WebAssembly 模块（Wasm），它通常是以 .wasm 为扩展名的二进制文件。\nwazero 仅依赖 Go 语言而无依赖，且不依赖 CGO。你可以运行其他语言的应用程序，但仍然保持交叉编译。也就是说它可以嵌入到应用程序中，而不依赖特定的操作系统。这是 wazero 与其他 WebAssembly 运行时的主要区别。wazero 还可以在 Docker 的 scratch 镜像 中运行。\n想要了解更多关于 wazero 的信息请访问：https://github.com/tetratelabs/wazero func-e func-e 是一个用来安装和运行 Envoy 代理的命令行工具。func-e（发音为funky）允许你快速查看 Envoy 的可用版本并进行试用。这使得你很容易验证在生产中使用的配置。每次你结束运行时，都会以你的名义获取运行时状态的快照。这使得知识共享和故障排除更加容易，特别是在升级时。\n想要了解更多关于 func-e 的信息请访问：https://github.com/tetratelabs/func-e Istio Security Analyzer Istio Security Analyzer 是一个用于 Istio 安全性分析的命令行工具。该工具可以：\n确保配置遵守 Istio 安全最佳实践。 检查正在运行的 Istio 版本，看是否有任何已知的 CVE 问题。 想要了解更多关于 Istio Security Analyzer 的信息请访问：https://github.com/tetratelabs/istio-security-analyzer 更多 Tetrate 开源的项目请访问：https://github.com/tetratelabs ","relpermalink":"/blog/tetrate-open-source-projects/","summary":"本文介绍了企业级服务网格公司 Tetrate 的几个围绕服务网格领域的开源项目。","title":"Tetrate 公司开源项目介绍"},{"content":"去年第一届 IstioCon 在线上成功举办，今天第二届 IstioCon 也要开始了，今年的会议依旧是线上举办，分为英文场和中文场，注册参会地址：https://events.istio.io/istiocon-2022/ 注册后你可以在大会官网 （需翻墙）上收看直播，也可以在 Istio 社区的 Bililibi 直播间 收看。\n中文场将在北京时间 4 月 26 日（周二）上午 9 点开始，本次大会的中国组织者（徐中虎、Iris Ding 和我）将进行开场，然后在 9:15 分我将和马若飞、赵化冰、刘齐均）有一个关于《Istio 开源生态展望 》的圆桌论坛，欢迎大家收看。\n","relpermalink":"/notice/istiocon-2022-program/","summary":"欢迎注册第二届 IstioCon","title":"IstioCon 2022（4月25-29，线上会议）"},{"content":"2022 年 2 月 Istio 发布 1.13.0 和 1.13.1 ，这篇博客将想你介绍这两个版本中有哪些值得注意的新特性。\nIstio 1.13 是 2022 年的第一个版本，不出意外的话，Istio 团队会依然按照每个季度的频率发布新版本。总体来看，这个版本中的新特性包括：\n对 Kubernetes 更新版本的支持 引入了一个新的 API——ProxyConfig，用来配置 sidecar proxy 完善了 Telemetry API 支持多网络网关的基于主机名的负载均衡器 对 Kubernetes 版本的支持 我经常看到有人在社区里问 Istio 支持哪些 Kubernetes 版本，其实 Istio 官网中已经明确列出了支持的 Kubernetes 版本，你可以在这里 看到，Istio 1.13 支持 Kubernetes 1.20、1.21、1.22 和 1.23 版本，并在 Kubernetes 1.16、1.17、1.18、1.19 中测试过，但并得到官方支持。\n在配置 Istio 的时候，其实还有很多检查列表，我将他们都记录到了 Istio cheatsheet 中，这个项目中整理了很多关于配置 Istio、资源对象的使用、常见问题处理等相关的 cheatsheet，将于近期上线，敬请期待。\nIstio cheatsheet 页面截图 引入新的 ProxyConfig API 在 Istio 1.13 版本之前，如果你想自定义 sidecar proxy 的配置，有两种方式。\n方式一：MeshConfig\n使用 MeshConfig，在 Mesh 级别使用 IstioOperator 来修改。例如，使用下面的配置来修改 istiod 的默认发现端口。\napVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: defaultConfig: discoveryAddress: istiod:15012 方式二：Pod 中的 annotation\n你也可以在 Pod 级别使用 annotation 的方式自定义配置，例如在 Pod 中增加下面的配置同样可以修改工作负载所有连接的 istiod 的默认端口。\nanannotations: proxy.istio.io/config: | discoveryAddress: istiod:15012 当你同时使用了以上两种方式配置了 sidecar，annotations 中设置的字段将完全覆盖 MeshConfig 默认的字段。关于 ProxyConfig 的所有配置项请参考 Istio 文档 。\n新方式：ProxyConfig API\n但是在 1.13 版本中，新增了一个顶级自定义资源 ProxyConfig，你可以一站式的在一个地方来自定义 sidecar proxy 的配置，你可以通过指定 namespace、使用 selector 来选择工作负载的范围，就像其他 CRD 一样。目前 Istio 对该 API 的支持有限，关于 ProxyConfig API 的详细信息请参考 Istio 文档 。\n但是不论你用哪种方式自定义 sidecar proxy 的配置，该配置都无法动态生效，需要重启工作负载才可以生效。例如，对于上面的配置，因为你修改了 istiod 的默认端口，mesh 中的所有工作负载都需要重启才可以与 control plane 建立连接。\nTelemetry API 在 Istio 服务网格中，很多扩展和自定义的配置都是通过 MeshConfig 的方式来完成的。可观察性的三种类型 Metric、遥测和日志，分别可以对接不同的提供者，Telemetry API 可以让你有一个一站式的灵活的配置它们。与 ProxyConfig API 类似，Telemetry API 也遵循着工作负载选择器\u0026gt;本地命名空间\u0026gt;根配置命名空间的配置层级关系。该 API 是在 Istio 1.11 中引入，在该版本中得到了进一步完善，增加了 OpenTelemetry 日志、过滤访问日志以及自定义跟踪服务名称的支持。详见 Telemetry 配置 。\n自动解析多网络网关主机名 2021 年 9 月，Istio 社区里有人报告 ，在 AWS EKS 中运行多集群多主的 Istio 时，出现 EKS 的负载均衡器无法解析的问题。对于多集群多网络的网格，跨集群边界的服务负载，需要通过专用的东西向网关，以间接的方式通讯。你可以按照 Istio 官网上的说明 配置多网络的 primary-remote 集群，Istio 会根据主机名自动解析负载均衡器的 IP 地址。\nIstio 1.13.1 修复重大安全漏洞 当月，Istio 1.13.1 发布，修复了一个已知的重大漏洞 ，该漏洞可能导致未经认证的控制平面拒绝服务攻击。\n跨网络的主从集群 在安装多网络的 primary-remote 模式的 Istio 网格时，为了让 remote Kubernetes 集群能够访问控制平面，需要在 primary 集群中安装一个东西向的 Gateway，将控制平面 istiod 的 15012 端口暴露到互联网。攻击者可能向该端口发送特制的消息，导致控制平面崩溃。如果你设置了防火墙，只允许来自部分 IP 的流量访问该端口，将可以缩小该问题的影响范围。建议你立即升级到 Istio 1.13.1 来彻底解决该问题。\nIstioCon 2022 IstioCon 2022 最后，作为上一届和本届 IstioCon 的筹备委员会成员之一，我号召大家报名参加 4 月 25 日在线上举行的 IstioCon 2022 ！IstioCon 2022是一个以行业为重点的活动，一个连接贡献者和用户的平台，讨论Istio在不同架构设置中的用途，有哪些限制，以及项目的下一步发展方向。主要的焦点将是在最终用户公司，因为我们期待着分享多样化的案例研究，展示如何在生产中使用Istio。\n","relpermalink":"/blog/what-is-new-in-istio-1-13/","summary":"2022 年 2 月 Istio 发布 Istio 1.13.0 和 Istio 1.13.1，这篇博客将想你介绍这两个版本中有哪些值得注意的新特性。","title":"Istio 1.13 有哪些值得注意的更新？"},{"content":"本书译自美国国家标准标准与技术研究院（NIST）Special Publication 800-204C。\n关于本书 作者：Ramaswamy Chandramouli\n审阅\n计算机安全司信息技术实验室 美国商务部 Gina M. Raimondo，秘书 国家标准和技术研究所 James K. Olthoff，履行负责标准和技术的商务部副部长兼国家标准和技术研究所所长的非专属职能和职责 原出版物可在此 免费获取，中文版请在此阅读 。\n","relpermalink":"/notice/service-mesh-devsecops/","summary":"本书译自美国国家标准标准与技术研究院（NIST）SP 800-204C","title":"《利用服务网格为基于微服务的应用程序实施 DevSecOps》电子书发布"},{"content":"每当夜深人静的时候，你是否觉得在桥都做云原生技术很孤单寂寞，没有人懂你，没有人在你需要帮助的时候给你一个拥抱然后说：傻孩子，你不知道重庆有个云原生社区吗，那里是你的归宿。是的，我们的线下交友活动又来了，本次活动由云原生社区重庆站联合中国DevOps社区共同举办，邀请了三位重庆地区的云原生实践者分享他们的故事，参加活动你可以学到知识，认识朋友，还有众多礼品可以拿到手软，赶快免费报名吧！点击报名 ！\n时间：2022 年 3 月 12 日 地点：重庆市渝北区两江幸福广场 MK SPACE 以下是详细日程。\n端到端DevOps：从GitLab Flow到GitOps 时间：14:20 - 15:00\n讲师介绍\n刘剑桥，十余年软件领域从业经验，曾在华为，云从等企业任职。主导和参与过DevOps一体化平台、云原生平台、混合云管理平台等领域架构设计和落地，在DevOps领域有着丰富的实践经验。目前在极狐GitLab任解决方案架构师。\n话题大纲\n通过GitLab Flow和GitOps搞定从代码提交到K8S部署的全流程。帮助企业以代码定义、可追溯、更安全的方式上云。 云原生在IoT边缘计算场景应用与实践 时间：15:00 - 15:40\n讲师介绍\n姜仁杰，现就职于中移物联网有限公司，负责5G+IoT、5G+边缘计算产品及行业解决方案研发，对IoT、边缘计算、云原生有丰富的经验。\n话题大纲\n物联网边缘计算平台场景介绍 云原生对边缘计算场景支持 应用与实践 DevOps: 从降本增效到业务赋能 时间：16:00 - 16:40\n讲师介绍\n张鹄干，猪八戒网DevOps工程师，重庆地区DevOps与云原生的先行者，在相关领域具有丰富的实战经验。\n话题大纲\n以猪八戒网为背景，讲解devops发展趋势，如何做到从降本增效的原始需求过渡到实现为业务赋能的终极目标。 本次活动赞助商：极狐GITLAB\n","relpermalink":"/notice/chongqing-meetup/","summary":"3 月 12 日，由云原生社区和中国 DevOps 社区联合主办。","title":"云原生社区重庆站聚会"},{"content":"从 2022 年 2 月 10 日起，我开始以每周一个视频的频率来推出 Istio Workshop/教程，这系列视频每个时长在 2 到 3 分钟，由 Tetrate 、云原生社区 和 Jimmy Song 联合出品，本课程将打破常规，将图文阅读、视频学习、动手实验及社区讨论等模式相结合，适用于有一定 Istio 基础的中高阶人群，关注云原生社区 B 站 、公众号及视频号将及时获得更新。\n","relpermalink":"/notice/istio-workshop/","summary":"由 Tetrate、云原生社区和我共同出品的 Istio 的教程视频","title":"Istio Workshop 系列 Istio 视频教程发布"},{"content":" 封面 Service Mesh Summit 2022 首届服务网格峰会，将于 2022 年 3 月 19 日（星期六），在上海舜元会议中心（近 2 号线淞虹路）举行。请转到活动行 免费报名。本次活动由云原生社区主办，赞助商有 API7 、Tetrate 、Flomesh 。\nService Mesh Summit 2022 服务网格峰会日程表 日程表 以下是详细日程。\n多集群流量调度 ——Flomesh 时间：9:00 - 9:45\n讲师介绍\n张晓辉，Flomesh 高级云原生架构师。资深码农，有多年的微服务和基础架构的实践经验。主要工作涉及微服务、容器、Kubernetes、DevOps 等。目前在 Flomesh 负责流量相关产品和技术布道。\n话题大纲\n背景：集群数量增加带来的跨集群通信问题 Flomesh 的跨集群服务治理方案介绍及原理 详细介绍服务治理的相关功能 听众收获\n了解跨集群网络通信的整体架构及实现原理 了解可编程的统一数据面 Pipy 在方案中的角色 了解跨集群的服务治理方案及相关功能 Apache APISIX 借助 Service Mesh 实现统一技术栈的全流量管理 时间：9:45 - 10:30\n讲师介绍\n张晋涛，API7.ai 云原生技术专家，Apache APISIX PMC， Kubernetes ingress-nginx reviewer，containerd/Docker/Helm/Kubernetes/KIND 等众多开源项目 contributor， 『K8S 生态周报』的维护者，微软 MVP。对 Docker 和 Kubernetes 等容器化技术有大量实践和深入源码的研究，业内多个知名大会讲师，PyCon China 核心组织者，著有 《Kubernetes 上手实践》、《Docker 核心知识必知必会》和《Kubernetes 安全原理与实践》等专栏。 公众号：MoeLove。\n话题大纲\nService Mesh 领域的现状 实践 Service Mesh 中遇到的问题 Apache APISIX 在 Service Mesh 中实现全流量管理 面向未来的架构演进 听众收获\n了解当前 Service Mesh 领域的现状及一些痛点 了解 Apache APISIX 如何在 Service Mesh 领域中实现统一的全流量管理 Apache APISIX 在 Service Mesh 领域的探索和实践 从开源 Istio 到企业级服务：如何在企业中落地服务网格 时间：10:30 - 11:15\n讲师介绍\n胡渐飞，Tetrate 工程师，之前在 Google 从事 Istio 相关开发。\n话题大纲\nIstio 在企业中的实际落地应用 企业级客户在面对开源 Istio 的时候有什么样的考量 Tetrate 如何解决这些开源 Istio 中遇到的问题 听众收获\nTetrate 作为服务网格的顶级供应商之一，成立 4 年来服务大量的中大企业客户，通过本次分享，你了解到这些大型企业在考虑使用服务网格时候的顾虑和需求。以及作为一个商业化公司，Tetrate 对开源有什么样的经验和看法。\n如何实现 Istio 服务网格自定义扩展功能 时间：11:15 -12:00\n讲师介绍\n曾宇星，阿里云云原生架构师，技术专家，长期从事服务端开发和架构工作，10 多年分布式领域后台开发经验，主要关注于云原生、高性能、高可用分布式架构。有多年 ServiceMesh 、Envoy 网关、Kubernetes 容器平台等云原生领域相关开发工作经验。 目前在阿里云服务网格团队从事 ServiceMesh 云产品研发和架构设计工作。\n话题大纲\n基于 Istio 的几种服务网格自定义扩展方式介绍 ， 囊括 In-process 和 Out-Of-Process 的几种实现方式，具体包括 wasm (WebAssembly)、lua、golang、native c++、grpc proxyless 等 以上几种扩展方式对比及其背后的实现原理，包括 envoyfilter 和 xds 协议扩展 阿里云服务网格云产品（asm) 是如何将这几种扩展方式集成到产品中的，云产品用户如何使用满足自定义扩展需求 听众收获\n了解 Istio 服务网格的控制面和数据面相关知识和交互原理 了解 Istio 自定义扩展的几种实现方式和对比，如何选型和快速满足自定义扩展需求 基于阿里云服务网格 ASM， 如何快速落地服务网格自定义扩展需求 Aeraki Mesh 在冬奥会视频应用中的产品落地实践 时间：13:30 - 14:15\n讲师介绍\n赵化冰，腾讯云工程师，Aeraki Mesh 创始人。\n覃士林，腾讯后台开发，推动腾讯融媒体业务全量上云，主导微服务化体系建设，推动传统运维方式向云原生转型。\n话题大纲\nAeraki Mesh MetaProtocol 协议扩展方案介绍 腾讯融媒体采用 Aeraki Mesh 在冬奥会直播中的服务网格实践 听众收获\n如何基于 Aeraki Mesh 的协议扩展能力在服务网格中管理一个私有协议。\n轻舟服务网格的无侵入增强 Istio 经验 时间：14:15 - 15:00\n讲师介绍\n方志恒，网易数帆架构师，负责轻舟 Service Mesh，先后参与阿里和网易 Service Mesh 建设及相关产品演进。具有多年 Istio 控制面 管理维护、功能拓展和性能优化经验。\n话题大纲\n围绕 slime 的孵化和版本演进，介绍轻舟服务网格的无侵入增强 Istio 的经验，分享 slime 的后续规划。\n听众收获\n了解 Istio 生产落地中可能遇到的问题和应对 了解 Istio 的扩展方式和思路 了解 slime 是如何以无侵入的方式增强 Istio 了解 slime 在工程化方面的改进 了解 slime 的 roadmap 以及最近大版本的功能特性 服务网格中 TLS 的加速和优化 时间：15:15 - 16:00\n讲师介绍\n胡伟，英特尔先进技术与软件部门，负责云计算开源软件开发和合作伙伴技术合作，聚焦云原生软件基于英特尔架构在性能和安全方面的技术创新。\n赵复生，英特尔先进技术与软件部门，负责领导云计算开源软件社区开发和客户技术支持工作，聚焦云原生服务网格软件基于英特尔架构的技术创新。\n话题大纲\n大规模微服务场景下 TLS 的挑战 英特尔加解密加速技术 Crypto Acceleration 介绍 Crypto Acceleration 在服务网格中的实施方案介绍 案例和优化成果分享 听众收获\n听众可以了解到如何利用英特尔的 Crypto Acceleration 技术大幅提升服务网格中 TLS 安全通信的性能。\n字节跳动 Service Mesh 性能优化的实践与思考 时间：16:00 - 16:45\n讲师介绍\n徐佳玮，字节跳动基础架构研发工程师，长期专注于高性能网络、服务治理等领域，目前在字节架构基础架构服务框架团队从事 Service Mesh 研发工作。\n话题大纲\n字节跳动 Service Mesh 落地的现状 字节跳动 Service Mesh 在高性能网络方向的实践和探索 对其他性能优化技术的思考和展望 听众收获\nService Mesh 如何提高网络传输效率，在性能上在哪些方面可以优化，未来可以和哪些技术结合？希望能够引发更多的思考。\n蚂蚁集团 Service Mesh 进展回顾与展望 时间：16:45: 17:30\n讲师介绍\n石建伟，花名卓与，蚂蚁集团高级技术专家，专注服务领域中间件多年，包括微服务、Service Mesh、配置中心、服务注册中心，目前主要负责蚂蚁内部 Service Mesh、SOFARPC、Layotto、SOFAGateway 等产品。\n话题大纲\n练内功：内部集群规模化扩张带来的技术挑战，长连接膨胀，服务治理智能化演进和人工介入说再见 定标准：Mesh 化是终点么？ 应用运行时标准 建通路：让业务飞速发展的秘诀 看未来：云原生运行时的下一个五年 听众收获\n了解业界前沿 Service Mesh 演进趋势，大规模集群的 Mesh 化优化方向，服务治理自适应限流、自适应流量调度，MOSN 与 Envoy 融合的尝试。 了解跨云无厂商绑定的新技术方案，应用运行时，多云探索。 ","relpermalink":"/notice/service-mesh-summit-2022/","summary":"首届服务网格峰会，由云原生社区主办，正在报名中。","title":"Service Mesh Summit 服务网格峰会 2022 正在报名中"},{"content":" Envoy 基础教程 Envoy 代理是一个开源的边缘和服务代理，是当今现代云原生应用的重要组成部分，Booking.com、Pinterest 和 Airbnb 等大公司都在生产中使用了它。Tetrate 是 Envoy 的顶级贡献者，开发了 Envoy 基础教程 ，这是一个免费的培训，有结业证书，以帮助企业更快地采用该技术。DevOps、SRE、开发人员和其他社区成员能够通过概念文本、实际实验和测验轻松学习 Envoy。Tetrate 也是受欢迎的 Istio 基础教程 和开源项目 Func-e 的创建者，这使得采用 Envoy 更加容易。\n“我对 Tetrate 的 Envoy 基础教程和认证感到兴奋”，Envoy Proxy 的创建者、Lyft 的高级工程师 Matt Klein 说。“该课程组织的很好，有关于 Envoy 应用的信息，以及带有逐步说明和测验的实验。完成全部课程后，你将获得一份证书的奖励。最重要的是，培训是完全免费的。我向想学习和使用 Envoy Proxy 的人强烈推荐这个课程”。\nEnvoy 基础教程结业证书 Envoy 是构建服务网格的默认选择 CNCF 的项目 Envoy Proxy 是最流行的 sidecar 和 ingress 实现。它是多个服务网格项目的默认 sidecar，包括 Istio、Open Service Mesh 和 AppMesh。根据 CNCF 的 2020 年调查 ，Envoy 作为 Ingress 提供者的使用率增加了 116%，共有 37% 的受访者在生产中使用 Envoy Proxy。\nEnvoy 最初是在 Lyft 建立的，作为一个代理，作为大规模微服务服务网格的通用数据平面。其理念是让 Envoy sidecar 在你的应用中的每个服务旁边运行，将网络从应用中抽象出来。它可以作为边缘网关、服务网格和混合网络的桥梁。有了 Envoy，公司可以通过提供更灵活的发布流程和高可用、高弹性的基础设施来扩展其微服务。\nEnvoy 具有丰富的网络相关功能，如重试、超时、流量路由和镜像、TLS 终止、可观察性等。由于所有的网络流量都流经 Envoy 代理，因此可以观察流量和问题区域，对性能进行微调，从中找出任何延迟来源。由于其众多的功能和庞大的 API ，用户可能会对广泛而全面的 文档 感到不知所措，特别是对于不熟悉代理和刚刚开始 Envoy 之旅的初学者。因此，我们决定创建一个课程，介绍 Envoy 的基本概念和内部结构，使用户能够更快地学习。\n“在过去几年中，Envoy 的采用率迅速提高。这意味着获得易于学习的资源，为用户提供快速扩展学习的能力，对保持 Envoy 的采用更加容易至关重要”，Tetrate 联合创始人 Varun Talwar 说。\n关于 Envoy 基础教程 免费的 Envoy 基础教程 由 8 个模块组成，每个模块内有多个视频课程和实验。\nEnvoy 基础教程中的模块 该课程从介绍 Envoy 开始，解释了 HTTP 连接管理器过滤器、集群、监听器、日志、管理界面和扩展 Envoy 等概念。每个模块都包括带有分步说明的动手实验。实验使学习者能够练习所解释的概念，如\nEnvoy 的动态配置 断路器 流量拆分 镜像请求 全局和局部速率限制 HTTP tap filter 使用 Lua 脚本和 Wasm 扩展 Envoy 等 Envoy 基础教程课程大纲 每个模块后的小测验帮助你评估自己对知识的掌握。完成课程和所有测验后，你会收到一份完成证书。在 Tetrate Academy 网站 上注册免费的 Envoy 基础课程 ，开始学习。\n注意：该课程为双语版，见中文版 。\n更多 Envoy 学习资源 5 分钟内开始使用 Envoy（博客） Envoy 和 Envoy 可扩展性的基础知识（博客） Envoy 入门：基于文件的动态配置 (博客) Istio Weekly：Envoy 的基本原理（视频） Istio Weekly：开发 Envoy Wasm 扩展（视频） Envoy 详解（视频） Lyft 的 Envoy：拥抱服务网格（视频） 人人都可以为 Envoy 做贡献（视频） 学员反馈 关于 Tetrate Tetrate 由 Istio 创始人和 Envoy 维护者创办，旨在重新构想应用网络，是一家管理现代混合云应用基础设施复杂性的企业级服务网格公司。其旗舰产品 Tetrate Service Bridge 为多集群、多租户和多云部署提供了一个全面的、企业就绪的服务网格平台。客户在任何环境下都能获得一致的、内嵌的可观察性、运行时的安全性和流量管理。Tetrate 仍然是开源项目 Istio 和 Envoy Proxy 的主要贡献者，其团队包括 Envoy 的高级维护人员。了解更多信息，请访问 tetrate.io 。\n","relpermalink":"/notice/envoy-fundamental-courses/","summary":"本教程由企业级服务网格提供商 Tetrate 出品。","title":"Tetrate 推出免费的 Envoy 基础教程"},{"content":"虎年春节期间，我翻译了 O’Reilly 出品的报告 The Future of Observablity with OpeTelemetry ，作者 Ted Young。现在中文版已翻译完成，可以下载和在线阅读。\n《OpenTelemetry 可观测性的未来》中文版封面 如何阅读本书 您可以使用以下方式阅读：\n在线阅读 下载 PDF 关于本书 本书内容包括：\nOpenTelemetry 如何满足库作者、应用程序拥有者、运维和响应者的需求 应用程序的不同角色如何围绕 OpenTelemetry 来协同和独立工作 关于在组织中采用和管理 OpenTelemetry 的实用建议 关于作者 Ted Young 是 OpenTelemetry 项目的联合创始人之一。在过去的二十年里，他设计并建立了各种大规模的分布式系统，包括可视化 FX 管道和容器调度系统。他目前在 Lightstep 公司担任开发者教育总监，住在俄勒冈州波特兰的一个小农场里。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区 可观测性讨论组（微信群）参与讨论交流，加入前请先填写入群申请问卷 后联系 Jimmy Song 入群。\n","relpermalink":"/notice/opentelemetry-obervability-ebook/","summary":"本书将帮助你理解可观测性、OpenTelemetry 及如何在企业内推广它。","title":"《OpenTelemetry 可观测性的未来》中文版发布"},{"content":"去年第一届 IstioCon 在线上成功举办，今天第二届 IstioCon 演讲议题也征集了，欢迎大家提交。\nIstioCon 2022 IstioCon 接受中文和英文议题，本次活动为线上分享，议题提交截止时间为 2022 年 3 月 3 日（北京时间，下同），提交及要求详见：https://sessionize.com/istiocon-2022/ 其他重要日期有：\n议题开放征集时间：2022 年 1 月 28 日 议题征集截止时间：2022 年 3 月 4 日 议程发布时间：2022 年 3 月 29 日 会议日期：2022 年 4 月 26 - 30 日 本次活动由 Istio 社区主办，我们期待您的精彩分享。\n","relpermalink":"/notice/istiocon-2022/","summary":"第二届 IstioCon 议题正在征集中。","title":"IstioCon 2022 讲师和议题正在征集中"},{"content":"随着服务网格架构理念的深入人心，它的适用场景也慢慢为众人所了解，社区中也不乏争论，甚至是质疑的声音。笔者以在云原生和服务网格社区中多年的观察，将从亲历者的角度总结服务网格在 2021 年的进展。因为当前在国内 Istio 几乎是服务网格的代名词，本文也将主要从 Istio 的技术和生态层面来解读服务网格在 2021 年的发展。\n服务网格：云原生的核心技术之一 作为 CNCF 定义的云原生 关键技术之一，服务网格发展至今已经有五个年头了，其发展经历了以下几个时期：\n探索阶段：2017 年-2018 年 早期采用者阶段：2019 年-2020 年 大规模落地及生态发展阶段：2021 年至今 如果根据“跨越鸿沟”理论 ，服务网格已经跨越了“鸿沟”，处于“早期大众”和“晚期大众”阶段之间。根据《Istio 大咖说》 观众中的反馈来看，用户已不再盲从于新技术，开始辩证的考虑是否真的需要引入服务网格 。\n跨越鸿沟理论 云原生的发展方兴未艾，虽然不断有新的技术和产品出现，但作为整个云原生技术栈的一部分，服务网格在过去一年里不断夯实了它作为“云原生网络基础设施”的定位。下图展示了云原生技术栈模型，其中每一层有一些代表性的技术来定义标准。作为新时代的中间件，服务网格与其他云原生技术交相辉映，如 Dapr（分布式应用程序运行时）定义云原生中间件的能力模型，OAM 定义云原生应用程序模型等，而服务网格定义的是云原生七层网络模型。\n云原生技术栈 社区焦点 过去一年中，社区的焦点主要集中在以下几个方面：\n性能优化：服务网格在大规模应用场景下的性能问题； 协议扩展：让服务网格支持任意七层网络协议； 部署模式：Proxyless vs Node 模式 vs Sidecar 模式； 引入 eBPF：将服务网格的部分能力下沉到内核层； 性能优化 Istio 设计之初的目标就是通过“原协议转发”的方式服务于服务间流量，让服务网格尽可能对应用程序“透明”，从而使用了 IPtables 劫持流量 ，根据社区提供的测试结果 ，对于在 16 个连接上具有 1000 RPS 的网格，Istio 1.2 仅增加了 3 毫秒的基准延迟。但是，因为 IPtables conntrack 模块所固有的问题，随着网格规模的扩大，Istio 的性能问题开始显现。关于 Istio sidecar 的资源占用及网络延迟的性能优化，社区给出了以下解决方案：\nSidecar 配置：通过手动或在控制平面增加一个 Operator 的方式来配置服务的依赖项，可以减少向 Sidecar 中下发的服务配置数量，从而降低数据平面的资源占用；为了更加自动和智能地配置 Sidecar，开源项目 Slime 及 Aeraki 都给出了各自的配置懒加载方案； 引入 eBPF：eBPF 可以作为优化服务网格性能的一种可行性方案，有基于 Cilium 的初创公司甚至激进的提出使用 eBPF/Cilium 完全替换 Sidecar 代理 的策略，但事实上 Envoy 代理/xDS 协议已经成为服务网格实现的实际代理，且很好的支持七层协议。eBPF 可用来改善网络性能，但复杂的协议协商、解析和用户扩展在用户侧依然很难实现。 协议扩展 如何扩展 Istio 一直以来就是一个老大难的问题。Istio 的可扩展包含两方面：\n协议层面：让 Istio 支持所有七层协议 生态层面：让 Istio 可以运行更多的插件 Istio 使用的是 Envoy 作为数据平面，扩展 Istio 本质上就是对 Envoy 功能的扩展。Istio 官方目前给出的方案是使用 WebAssembly，并在 Istio 1.12 引入 Wasm 插件配置 API 用于扩展 Istio 生态 ，Istio 的扩展机制使用 Proxy-Wasm 应用二进制接口（ABI） 规范，提供了一套代理无关的流媒体 API 和实用功能，可以用任何有合适 SDK 的语言来实现。截至目前，Proxy-Wasm 的 SDK 有 AssemblyScript（类似 TypeScript）、C++、Rust、Zig 和 Go（使用 TinyGo WebAssembly 系统接口）。\n目前 WebAssembly 扩展应用还比较少，很多企业选择自定义 CRD，基于 Istio 构建服务网格管理平面。另外，让 Istio 支持异构环境，适用于一切工作负载，如虚拟机、容器，这个对于终端用户来说也有很强的需求，因为这可以让用户很方便的从传统负载迁移应用到服务网格中。最后是多集群、多网格的混合云流量管理，这个属于比较高阶的需求了。\n部署模式 在服务网格概念兴起之初就有 Per-node 和 Sidecar 模式之争，他们的代表分别是 Linkerd 和 Istio。后来 eBPF 提出将服务网格下沉的内核，从而演化出了更多的服务网格部署模式，如下图所示。\n服务网格的部署模式 下表中详细对比了这四种部署方式，它们各有优劣，具体选择哪种根据实际情况而定。\n模式 内存开销 安全性 故障域 运维 Sidecar 代理 因为为每个 pod 都注入一个代理，所以开销最大。 由于 sidecar 必须与工作负载一起部署，工作负载有可能绕过 sidecar。 Pod 级别隔离，如果有代理出现故障，只影响到 Pod 中的工作负载。 可以单独升级某个工作负载的 sidecar 而不影响其他工作负载。 节点共享代理 每个节点上只有一个代理，为该节点上的所有工作负载所共享，开销小。 对加密内容和私钥的管理存在安全隐患。 节点级别隔离，如果共享代理升级时出现版本冲突、配置冲突或扩展不兼容等问题，则可能会影响该节点上的所有工作负载。 不需要考虑注入 Sidecar 的问题。 Service Account/节点共享代理 服务账户/身份下的所有工作负载都使用共享代理，开销小。 工作负载和代理之间的连接的认证及安全性无法保障。 节点和服务账号之间级别隔离，故障同“节点共享代理”。 同“节点共享代理”。 带有微代理的共享远程代理 因为为每个 pod 都注入一个微代理，开销比较大。 微代理专门处理 mTLS，不负责 L7 路由，可以保障安全性。 当需要应用7层策略时，工作负载实例的流量会被重定向到L7代理上，若不需要，则可以直接绕过。该L7代理可以采用共享节点代理、每个服务账户代理，或者远程代理的方式运行。 同“Sidecar 代理”。 服务网格的部署模式 生态发展 2021 年，Istio 社区也是精彩纷呈，举办了系列的活动，还发布了系列教程：\n2 月，首个 Istio 发行版， Tetrate Istio Distro（TID） 发布； 2 月，第一届 IstioCon 在线上举办，2000 多人参与了会议； 3 月，首个免费的线上 Istio 基础教程 发布； 5 月，首个 Istio 管理员认证考试（CIAT） 发布； 5 月，ServiceMeshCon Europe 在线上举办； 7 月，Istio Meetup China 在北京举办，100 多人现场参加； 10 月，ServiceMeshCon North America 在洛杉矶举办； 此外还有众多与 Istio 服务网格相关的项目开源，如下表所示。\n项目名称 开源时间 类别 描述 主导公司 Star 数量 与 Istio 的关系 Envoy 2016年 9 月 网络代理 云原生高性能边缘/中间服务代理 Lyft 18700 默认的数据平面 Istio 2017 年 5 月 服务网格 连接、保护、控制和观察服务。 Google 29100 控制平面 Linkerd2 2017 年 12 月 服务网格 适用于 Kubernetes 的轻量级服务网格。 Buoyant 7900 服务网格的另一种实现 Emissary Gateway 2018 年 2 月 网关 用于微服务的 Kubernetes 原生 API 网关，基于 Envoy 构建 Ambassador 3600 可连接 Istio APISIX 2019 年 6 月 网关 云原生 API 网关 API7 8100 可作为 Istio 的数据平面运行也可以单独作为网关 MOSN 2019 年 12 月 代理 云原生边缘网关及代理 蚂蚁 3500 可作为 Istio 数据平面 Slime 2021 年 1月 扩展 基于 Istio 的智能服务网格管理器 网易 236 为 Istio 增加一个管理平面 Tetrate Istio Distro 2021 年 2 月 工具 Istio 集成和命令行管理工具 Tetrate 95 第一个 Istio 开源发行版和多版本管理工具 Aeraki 2021 年 3 月 扩展 管理 Istio 的任何七层负载 腾讯 330 扩展多协议支持 Layotto 2021 年 6 月 运行时 云原生应用运行时 蚂蚁 393 可以作为 Istio 的数据平面 Hango Gateway 2021 年 8 月 网关 基于 Envoy 和 Istio 构建的 API 网关 网易 253 可与 Istio 集成 Istio 开源生态 注：数据统计截止到 2022 年 1 月 6 日。 总结 回望 2021 年，我们可以看出用户对服务网格的追求更趋实用，作为云原生网络的基础设施，其地位得到进一步夯实，更重要的是服务网格生态渐起。展望 2022 年，有两个值得关注的技术是 eBPF 和 WebAssembly。我们有理由相信，更多的服务网格实践优秀案例出现，在生态和标准化上更进一步。\n参考 告别 Sidecar——使用 eBPF 解锁内核级服务网格 网易开源 Istio 扩展项目 Slime 简介——基于 Istio 的智能服务网格管理器 Istio 1.12 引入 Wasm 插件配置 API 用于扩展 Istio 生态 基于 GRPC 和 Istio 的无 Sidecar 代理的服务网格 eBPF 如何简化服务网格 使用 Isito 前的考虑要素 ","relpermalink":"/blog/service-mesh-2021/","summary":"本文带您回顾了 2021 年服务网格的发展。","title":"服务网格 2021 年终盘点：实用当先，生态为本"},{"content":"随着跨入 2022 年，我们想分享下 2021 年 Tetrate 在您的支持下创造的一些亮点和达到的里程碑。自 2018 年 3 月成立以来，Tetrate 一直在增长其能力，以实现其设定的目标 —— 重新定义应用网络。今年是我们的第四个年头，由 Sapphire Ventures 领导的 B 轮融资和我们最近被指定为 Gartner 云计算的 Cool Vendor。以下是 Tetrate 在 2021 年完成的主要里程碑。\nTetrate 在 B 轮融资 中筹集了 4000 万美元 ，由 Sapphire Ventures 领投，Scale Venture Partners 、NTTVC 以及之前的投资者 Dell Technologies Capital 、Intel Capital 、8VC 和 Samsung NEXT 跟投。 Tetrate 与 NIST 合作举办了 第二届多云会议（Multi-cloud Conference） ，政府和行业的安全领导人在会上分享了最佳实践、演示和现实世界的使用案例，这些案例利用服务网格提供零信任架构（ZTA）并加速跨多云的 DevSecOps。Tetrate 还继续与 NIST 合作，在 Tetrate Service Bridge 实施下一代访问控制（NGAC），并为微服务定义零信任安全标准 。第三届年会 将于 2022 年 1 月 26-27 日举行。 在开源方面，Tetrate 仍然是 Istio 和 Envoy 的主要贡献者，并且是 Apache SkyWalking APM 和可观察性平台的主要支持者。Tetrate 是 Envoy 的第一大公司贡献者（按提交量计算 ），并对 Wasm 扩展功能、符合 SPIFFE 的 TLS 证书 验证功能以及 Proxy-Wasm C++ host 项目做出了显著贡献。我们还发布了 Func-e CLI （以前称为 GetEnvoy），使运行 Envoy 变得简单，并使开发人员能够快速运行不同的 Envoy 版本。对于 Istio，最流行的开源服务网格，Tetrate 发布了 GetMesh CLI ，作为 Tetrate Istio Distro 的一部分，它允许用户开始使用 Istio，并确保他们使用可信的、支持的 Istio 版本。Tetrate 工程师还帮助建立了 Istio 1.12 中宣布的 alpha 版本的 Wasm 插件功能 。 Tetrate 增强了其旗舰产品 Tetrate Service Bridge 。TSB 在多个集群中抽象出多个计算资源（虚拟机、k8s、云），允许企业使用一个非常性感的管理平面来控制、保护和观察其应用。最新的版本以 API 管理为特色 —— 它将 API 网关和服务网格整合到一个应用连接平台中，并提供一个托管版本的管理平面，因此客户只需带来他们的计算集群。TSB 由生产级和大规模运行的技术驱动，如 Envoy 、Istio 、SkyWalking 、Zipkin 和 下一代访问控制（NGAC） 。 Tetrate 支持世界各地的开源社区，并继续将社区和项目用户的教育作为核心责任。2021 年，我们创建了 Tetrate 学院 ，提供广受好评的免费课程 Istio Fundamentals 英文及中文教程 和 Envoy Fundamentals ，以及 Tetrate 认证 Istio 管理员（CIAT） 认证考试。我们组织了几十个网络研讨会（包括 YouTube 直播的 Istio Weekly 和 Istio 大咖说 （Bilibili 直播），赞助了 KubeCon、EnvoyCon NA、IstioCon 和云原生 Wasm Day，并与 NIST 共同主办了一个会议，以及世界各地的现场和虚拟聚会。 Tetrate 向亚太地区扩张 ，7 月和 12 月分别在新加坡和印度开设了办事处。 Tetrate 扩大了与 AWS 的合作关系 ，加入了 AWS ISV Accelerate 计划，并将其产品放在 Amazon Marketplace 上。Tetrate 现在与 Amazon EKS 和 EKS Anywhere 合作，为企业内部和云中的 Kubernetes 应用带来无缝连接和管理。 2021 年，Tetrate 的团队、客户数量和收入都翻了一番！我们的团队人数超过了 100 人。雇用了不同的职能部门：主要是工程和客户体验，但也有产品、营销和商务体验团队。我们有一个全明星的、分布在全球的团队，致力于客户、卓越技术、开源。Tetrate 与优秀的顾问 、新老客户和合作伙伴一起支持并分享我们的成长和成功。 Tetrate 被选为 Gartner 云计算领域优秀供应商 。Gartner 报告说，Tetrate 被选中是因为它提供了一个跨多云环境的服务网格解决方案，提供全面的应用连接、安全、可观察性和可靠性 —— 所有这些都来自一个平台层。“它的市场差异化源于其统一的网格功能、多云支持、灵活的运营模式和对开源的承诺”。 感谢我们的社区参与这个旅程，并与我们一起期待 2022 年！还有，我们正在招人 ！\n","relpermalink":"/notice/tetrate-2021/","summary":"随着跨入 2022 年，我们想分享下 2021 年 Tetrate 在您的支持下创造的一些亮点和达到的里程碑。","title":"Tetrate 2021 年度发展里程碑回顾"},{"content":"最近为某网站撰写服务网格技术的 2021 年总结，笔者关注该领域也有 4 年时间了，再结合自己最近这几年对云原生行业发展的观察，越发觉得《跨越鸿沟》（Crossing the chasm）这本书中所写的新技术的推广生命周期一一应验了。虽然该理论由 Jeffery Moore 于 1991 年提出，距今已有 30 年时间，但该理论至今依然奏效，另外该理论也在 CNCF 项目 的成熟度划分中得到应用，还有人指出过云原生技术需要跨越的鸿沟 。本文将为读者分享一些关于” 鸿沟理论 “有关的一些知识，希望能够引发大家对于新技术推广的一些思考。\n什么是鸿沟理论？ 鸿沟理论指的就是高科技产品在市场营销过程中遭遇的最大障碍：高科技企业的早期市场和主流市场之间存在着一条巨大的鸿沟，能否顺利跨越鸿沟并进入主流市场，成功赢得实用主义者的支持，就决定了一项高科技产品的成败。实际上每项新技术都会经历鸿沟。关键在予采取适当的策略令高科技企业成功地 “跨越鸿沟”，摩尔在这本书中就告诉了人们一些欠经考验的制胜秘诀。\n前言 基于经典的钟形曲线分布，“跨越鸿沟 \u0026#34; 是一个将新技术的采用随时间推移而可视化的概念：从一小撮早期采用者开始，经过大规模的中端市场，最终进入最抗拒变化的消费者手中。\n1962 年，社会学家 Everett Rogers 出版了 Diffusion of Innovasions （创新扩散） 一书。在这本书中，他根据消费者的购买行为，将他们分为不同的群体。他以 500 多项扩散研究的结果为基础进行分类。今天，这个模型被称为 “技术采用生命周期”。这个模型全面地描述了新技术产品或创新的采用或接受情况。在《跨越鸿沟》一书中，杰弗里・摩尔根据扩散生命周期中的客户群体，阐述了成功锁定主流消费者的营销技巧。\n客户群体 基于人口学和心理学特征，客户群保护以下五种：\n创新者 早期采用者 早期大众 后期大众 落后者 跨越鸿沟 图：”鸿沟理论“客户分布情况\n分布情况 正如可以观察到的，技术采用的生命周期有一个钟形曲线。各个分界线大约相当于标准差的落点。这意味着：\n创新者约占总人口的 2.5% 早期采用者约占 13.5% 早期大众和后期大众均为 34% 落后者占剩余的 16% 每个群体都代表着一个独特的心理特征，即心理和人口特征的组合。因此，针对这些群体的营销需要与其他群体完全不同的策略。营销人员通过更好地了解这些群体之间的差异，可以通过正确的营销技术更好地锁定所有这些消费者。\n创新者 创新者是技术爱好者。这是第一个有可能投资于你产品的消费者群体。创新者积极地追求新的产品和技术。有时，他们甚至在公司启动正式的营销计划之前就开始寻求创新。这是因为技术在他们的生活或业务中占据了核心利益。对于这个客户群体来说，产品功能组合的完整性或性能是次要的。\n不幸的是，在任何特定的细分市场中，都没有很多创新者（大约 2.5%）。通常情况下，他们不愿意为新产品付出很多。尽管如此，赢得他们是很重要的，因为他们的认可为市场上的其他消费者提供了必要的保证。此外，技术爱好者可以作为一个测试小组，在面向主流市场之前进行必要的修改。\n早期采用者 和创新者一样，早期采用者也是有远见的人，他们在新产品的生命周期的早期就接受了新产品的概念。然而，与创新者不同，他们不是技术专家。相反，他们是有远见的人，不只是在寻找一种改进，而且是一种革命性的突破。因此，他们愿意承担高风险，尝试新事物。他们是对价格最不敏感的客户群体，对产品的功能设置和性能要求很高。\n早期采用者在做出购买决定时不依赖成熟的参考资料。相反，他们更愿意依靠自己的直觉和眼光。此外，他们愿意作为其他采用者群体的参考。由于有远见的人善于提醒其他人群，他们是最重要的争取对象。\n早期大众 这个客户群由实用主义者组成。前两个采用者群体属于早期市场。然而，为了获得真正的成功，一个公司必须从早期大众开始，赢得主流市场。这些实用主义者与早期采用者有一些相同的能力，能够与技术产生联系。然而，他们受到强烈的实用意识的驱动。他们知道，很多发明最终会成为过眼云烟。因此，在自己投资之前，他们更期望等待，看看其他客户对该技术的使用情况如何。他们希望在进行大量投资之前看到成熟的参考资料。因为这部分人很多（大约 34%），对于任何努力争取大量利润和增长的企业来说，赢得这些人的支持是最基本的。\n后期大众 这个群体主要由保守派组成。后期大众作为一个群体与早期大众一样大（占总人口的 34%）。他们与早期大众有着同样的担忧。此外，他们对传统的信仰远远多于对进步的信仰。早期大众的顾客如果决定购买新技术产品，他们对自己处理该产品的能力感到满意。相比之下，“后期大众 \u0026#34; 的成员则不然。因此，这些保守派更愿意等到某样东西已经成为一种惯例时才购买。\n落后者 这个群体是由怀疑论者组成的。这一部分人占总数的 16%。这些人根本不希望与新技术有任何关系。他们唯一一次购买技术产品是当它被深埋在另一个产品中时。这些持怀疑态度的人强烈认为，颠覆性的创新很少能实现他们的承诺。他们总是担心意外的后果。从市场发展的角度来看，落后者通常被认为是不值得追求的。然而，他们对产品功能设置和性能的批评为技术公司提供了宝贵的反馈。\n鸿沟 在技术采用生命周期中，你可以看到早期采用者和早期大众群体之间的差距。这个差距代表了技术必须跨越的鸿沟。它标志着将左边的群体作为右边的客户群的参考基础而产生的可信度差距。鸿沟的存在是因为消费者信任属于他们自己的采用者群体的人的推荐。\n当然，这给技术公司带来了一个具有挑战性的困境。\n如果他们还没有从你这里买过东西，你怎么能利用首选参考群体的人呢？\n换句话说，将一个群体的客户作为其他群体的参考是无效的。因此，鸿沟就是这样产生的！\n由于从早期采用者到早期大众的飞跃意味着从早期市场到主流市场的过渡，跨越鸿沟对于新推出的产品 / 技术真正实现市场成功是最重要的。\n总结 根据摩尔的说法，成功跨越鸿沟可以通过首先瞄准早期大众中一个非常具体的利基市场来实现。组织试图跨越鸿沟的唯一目标应该是在主流市场上获得一个桥头堡，以创造一个可供参考的实用主义客户群。在这里，细分就是一切：将你所有的营销资源集中在一个特定的细分市场上，并确保你在这个特定的细分市场上成为领导者，然后再去做下一个细分市场。这就是所谓的 “大鱼小池 \u0026#34; 的方法。营销漏斗或 AIDA 模型是一个很好的营销框架，它可以帮助为潜在客户挑选正确的营销技术。此外，确保你的产品提供一个完整的解决方案，并且服务水平高（即整个产品解决方案）。实用主义者对你的产品的用户体验将最终决定他们是否也会激起他们的同行。一旦你在早期大众的不同部分建立了强大的口碑，你就成功地跨越了鸿沟。\n更多 你觉得服务网格目前处于鸿沟理论的哪个阶段呢？你又是何种受众？\n参考 《Crossing the Chasm》丨 NOTES - jianshu.com 灵雀云 CTO 陈恺：从 “鸿沟理论” 看云原生，哪些技术能够跨越鸿沟？ - infoq.cn ","relpermalink":"/blog/crossing-the-chasm/","summary":"本文将为读者分享一些关于“鸿沟理论”有关的一些知识，希望能够引发大家对于新技术推广的一些思考。","title":"跨越鸿沟：理解鸿沟理论"},{"content":"最近我在研究 Istio 生态中的开源项目，Slime 这个项目开源与 2021 年初，是由网易数帆微服务团队开源的一款基于 Istio 的智能网格管理器。Slime 基于 Kubernetes Operator 实现，可作为 Istio 的 CRD 管理器，无须对 Istio 做任何定制化改造，就可以定义动态的服务治理策略，从而达到自动便捷使用 Istio 和 Envoy 高阶功能的目的。\nSlime 试图解决的问题 Slime 项目的诞生主要为了解决以下问题：\n网格内所有服务配置全量下到所有 Sidecar Proxy，导致其消耗大量资源使得应用性能变差的问题 如何在 Istio 中实现高阶扩展的问题：比如扩展 HTTP 插件；根据服务的资源使用率做到自适应限流 Slime 解决以上问题的答案是构建 Istio 的控制平面，具体做法是：\n构建可拔插控制器 数据平面监控 CRD 转换 通过以上方式 Slime 可以实现配置懒加载和插件管理器。\nSlime 架构 Slime 内部分为三大模块，其架构图如下所示。\nSlime 内部架构图 Slime 内部三大组件为：\nslime-boot：在 Kubernetes 上部署 Slime 模块的 operator。 slime-controller：Slime 的核心组件，监听 Slime CRD 并将其转换为Istio CRD。 slime-metric：用于获取服务 metrics 信息的组件，slime-controller 会根据其获取的信息动态调整服务治理规则。 目前 Slime 内置了三个控制器子模块：\n配置懒加载（按需加载）：用户无须手动配置 SidecarScope，Istio 可以按需加载服务配置和服务发现信息； HTTP 插件管理：使用新的 CRD——pluginmanager/envoyplugin 包装了可读性，摒弃了可维护性较差的 envoyfilter，使得插件扩展更为便捷； 自适应限流：结合监控信息自动调整限流策略； 什么是 SidecarScope？\nSidecarScope 是在 Istio 1.1 版本中引入的，它并不是一个直接面向用户的配置项，而是 Sidecar 资源的包装器，具体来说就是 Sidecar 资源 中的 egress 选项。通过该配置可以减少 Istio 向 Sidecar 下发的数据量，例如只向某个命名空间中的某些服务下发某些 hosts 的访问配置，从而提高应用提高性能。\n使用 Slime 作为 Istio 的控制平面 为了解决这些问题，Slime 在 Istio 之上构建了更高层次的抽象，相当于为 Istio 构建了一层管理平面，其工作流程图如下所示。\nSlime 工作流程图 具体步骤如下：\nSlime Operator 根据管理员的配置在 Kubernetes 中完成 Slime 组件的初始化； 开发者创建符合 Slime CRD 规范的配置并应用到 Kubernetes 集群中； Slime 查询 Prometheus 中保存的相关服务的监控数据，结合 Slime CRD 中自适应部分的配置，将 Slime CRD 转换为 Istio CRD，同时将其推送到 Global Proxy 中； Istio 监听 Istio CRD 的创建； Istio 将 Sidecar Proxy 的配置信息推送到数据平面相应的 Sidecar Proxy 中； 以上只是一个对 Slime 工作流程的一个笼统的介绍，更多详细信息请参考 Slime GitHub 。\n配置懒加载 为了解决数据平面中 Sidecar Proxy 资源消耗过大及网络延迟问题，Slime 使用了配置懒加载（按需加载 Sidecar 配置）的方案。该方案的核心思想是向每个 Sidecar Proxy 中只下发其所 Pod 中服务所需的配置，而不是将网格中的所有服务信息全量下发。所以 Slime 需要获取每个服务的调用关系这样才能得到其所需的 Sidecar Proxy 配置。\nSlime 实现 Sidecar Proxy 配置懒加载的方法是：\n让数据平面中的所有服务的首次调用都通过一个 Global Proxy，该 Proxy 可以记录所有服务的调用和依赖信息，根据该依赖信息更新 Istio 中 Sidecar 资源的配置； 当某个服务的调用链被 VirtualService 中的路由信息重新定义时， Global Proxy 原有记录就失效了，需要一个新的数据结构来维护该服务的调用关系。Slime 创建了名为 ServiceFence 的 CRD 来维护服务调用关系以解决服务信息缺失问题。 使用 Global Proxy 初始化服务调用拓扑 Slime 在数据平面中部署 Global Proxy（也叫做 Global Sidecar，但其与应用的 Pod 不是一对一的关系，笔者更倾向于称其为 Global Proxy），该代理同样使用 Envoy 构建，在每个需要启动配置懒加载的命名空间中部署一个或在整个网格中只部署一个，所有缺失服务发现信息的调用（你也可以手动配置服务调用关系），都会被兜底路由劫持到 Global Proxy，经过其首次转发后，Slime 便可感知到被调用方的信息，然后根据其对应服务的 VirtualService，找到服务名和真实后端的映射关系，将两者的都加入 SidecarScope，以后该服务的调用就不再需要经过 Global Proxy 了。\n使用 ServiceFence 维护服务调用拓扑 在使用 Global Proxy 初始化服务调用拓扑后，一旦服务调用链有变动的话怎么办？对此 Slime 创建了 ServiceFence 的 CRD。使用 ServiceFence 可以维护服务名和后端服务的映射关系。Slime 根据其对应服务的 VirtualService，找到 Kubernetes 服务名和真实后端（host）的映射关系，将两者的都加入 Sidecar 的配置中。ServiceFence 管理生成的 SidecarScope 的生命周期，自动清理长时间不用的调用关系，从而避免上述问题。\n如何开启配置懒加载 配置懒加载功能对于终端用户是透明的，只需要 Kubernetes Service 上打上 istio.dependency.servicefence/status:\u0026#34;true\u0026#34; 的标签，表明该服务需要开启配置懒加载，剩下的事情交给 Slime Operator 来完成即可。\nHTTP 插件管理 Istio 中的插件扩展只能通过 EnvoyFilter 来实现，因为它是 xDS 层面的配置，管理和维护这样的配置需要耗费大量的精力，也极容易出错。因此，Slime 在 EnvoyFilter 的基础上做了一层面向插件的抽象。\nSlime 共有两个 CRD 用于 HTTP 插件管理，分别是：\nPluginManager：配置为哪些负载开启哪些插件，插件的配置顺序即为执行顺序； EnvoyPlugin：EnvoyPlugin 不关心每个插件的具体配置，具体配置会被放在 EnvoyFilter 资源的 patch.typed_config 结构中透传），EnvoyPlugin 的核心思想是将插件配置在需要的维度中做聚合，从而限定插件的生鲜范围。这样做一方面更加贴合插件使用者的习惯，另一方面也降低了上层配置的冗余， 关于 Slime 中插件管理的详细使用方式请见 Slime GitHub 。\n自适应限流 Envoy 内置的限流组件功能单一，只能以实例维度配置限流值，无法做到根据应用负载的自适应限流。Slime 通过与 Prometheus metric server 对接，实时的获取监控情况，来动态配置限流值。\nSlime 自适应限流的流程图如下所示。\nSlime 的自适应限流流程图 Slime 的自适应限流的流程分为两部分，一部分为 SmartLimiter 到 EnvoyFilter 的转换，另一部分为获取监控数据。目前 Slime 支持从 Kubernetes Metric Server 获取服务的CPU、内存、副本数等数据。Slime 还对外提供了一套监控数据接口（Metric Discovery Server），通过 MDS，可以将自定义的监控指标同步给限流组件。\nSlime 创建的 CRD SmartLimiter 用于配置自适应限流。其的配置是接近自然语义，例如希望在 CPU 超过 80% 时触发服务 A 的访问限制，限额为 30QPS，对应的SmartLimiter 定义如下：\napiVersion: microservice.netease.com/v1alpha1 kind: SmartLimiter metadata: name: a namespace: default spec: descriptors: - action: fill_interval: seconds: 1 quota: \u0026#34;30/{pod}\u0026#34; # 30为该服务的额度，将其均分给每个 pod，加入有 3 个 pod，则每个 pod 的限流为 10 condition: \u0026#34;{cpu}\u0026gt;0.8\u0026#34; # 根据监控项{cpu}的值自动填充该模板 更多 Slime 开源于 2021 年初，本文发稿时该项目仍处于初级阶段，本文大量参考了杨笛航在云原生社区中的分享 Slime：让 Istio 服务网格变得更加高效与智能 及 Slime 的 GitHub 。感兴趣的读者可以关注下这个项目的 GitHub，进一步了解它。\n另外欢迎关注服务网格和 Istio 的朋友加入云原生社区 Istio SIG ，一起参与讨论和交流。\n参考 Slime：让 Istio 服务网格变得更加高效与智能 - cloudnative.to Slime GitHub 文档 - github.com Sidecar - istio.io ","relpermalink":"/blog/slime-intro/","summary":"本文介绍的是由网易数帆微服务团队开源的一款基于 Istio 的智能网格管理器 Slime。","title":"网易开源 Istio 扩展项目 Slime 简介——基于 Istio 的智能服务网格管理器"},{"content":"本文根据 2021 年 11 月 22 日晚我应极客邦邀请在「极客时间训练营」的直播分享《云原生漫谈：聊聊 Service Mesh 的现状》整理而成。\n本来极客时间是想邀请我分享云原生的，但我觉得那个范围太大，在一次分享中只能泛泛而谈，无法聚焦到一个具体的点，因此我想还是先聚焦在服务网格这一个专题上吧。云原生社区最近倒是在做一个云原生系列的分享 ，大家可以关注下。\n这是我今天分享的大纲：\n第一探讨下服务网格跟云原生的关系 第二是给大家陈述下我观察到的目前社区里关于服务网格有哪些争论 第三是给大家介绍几个服务网格的相关的开源项目 最后是畅想下服务网格未来的发展 服务网格与云原生的关系 首先我们将探讨下服务网格与云原生的关系。\n服务网格——容器编排大战后的产物 Docker Swarm vs Kubernetes vs Mesos 如果你关注云原生领域足够早的话，应该还会对 2015 到 2017 年间的容器编排大战记忆犹新。关于服务网格的起源已经无需多言。2017 年 Kubernetes 获得了容器大战的胜利，微服务的理念已经深入人心，容器化的趋势可谓势不可挡。Kubernetes 架构趋向成熟，慢慢变得无聊，以 Linkerd、Istio 为代表的服务网格技术进入了 CNCF 定义的云原生关键技术视野中。\n服务网格将微服务中的通用的功能给下沉到了基础设施层，让开发者可以更加专注于业务逻辑，从而加快服务交付，这与整个云原生的理念的一致的。你不需要再在应用中集成笨重的 SDK，为不同语言开发和维护 SDK，应用部署完后，使用服务网格进行 Day 2 操作即可。\nKubernetes 设计之初就是按照云原生的理念设计的，云原生中有个重要概念就是微服务的架构设计，当将单体应用拆分微服务后， 随着服务数量的增多，如何微服务进行管理以保证服务的 SLA 呢？为了从架构层面上解决这个问题，解放程序员的创造性，避免繁琐的服务发现、监控、分布式追踪等事务，服务网格应运而生。\n微服务关注点 来源：https://developers.redhat.com/blog/2016/12/09/spring-cloud-for-microservices-compared-to-kubernetes 服务网格被誉为下一代微服务，从右面这幅图里我们可以看到微服务的一些关注点，这些关注点很多与 Kubernetes 的功能是重合的，既然这些作为平台级的功能 Kubernetes 已经提供了，为什么还要使用服务网格呢？其实 Kubernetes 关注的还是应用的生命周期，它管理的对象是资源和部署，对于服务的管控力度很小。而服务网格正好弥补了这个缺陷。服务网格可以连接、控制、观察和保护微服务。\nKubernetes vs xDS vs Istio 这幅图展示的是 Kubernetes 和 Istio 的分层架构图。\nKubernetes vs Service mesh 从图中我们可以看到 kube-proxy 的设置是全局的，无法对每个服务进行细粒度的控制，Kubernetes 可以做的只有拓扑感知路由、将流量就近路由，为 Pod 设置进出站的网络策略。\n而服务网格通过 sidecar proxy 的方式将 Kubernetes 中的流量控制从服务层中抽离出来，为每个 Pod 中注入代理，并通过一个控制平面来操控这些分布式代理。这样可以实现更大的弹性。\nKube-proxy 实现了一个 Kubernetes 服务的多个 pod 实例之间的流量负载均衡，但如何对这些服务之间的流量进行精细化控制–比如将流量按百分比划分给不同的应用版本（这些应用版本都是同一个服务的一部分，但在不同的部署上），或者做金丝雀发布（灰度发布）和蓝绿发布？\nKubernetes 社区给出了一个使用 Deployment 做金丝雀发布的方法，本质上是通过修改 pod 的标签来给部署的服务分配不同的 pod。\nEnvoy 架构图 目前在中国最流行的服务网格开源实现是 Istio，也有很多公司对 Istio 进行了二次开发，比如蚂蚁、网易、腾讯等，其实 Istio 是在 Envoy 的基础上开发的，从它开源的第一天起就默认使用了 Envoy 作为它的分布式代理。Envoy 开创性的创造了 xDS 协议，用于分布式网关配置，大大简化了大规模分布式网络的配置。2019 年蚂蚁开源的 MOSN 同样支持了 xDS。Envoy 还是 CNCF 中最早毕业的项目之一，经过大规模的生产应用考验。可以说 Istio 的诞生已经有了很好的基础。\n下表是 Kubernetes、xDS、Istio 三者之间的资源抽象对比。\nKubernetes xDS Istio 服务网格 Endpoint Endpoint WorkloadEntry Service Route VirtualService kube-proxy Route DestinationRule kube-proxy Listener EnvoyFilter Ingress Listener Gateway Service Cluster ServiceEntry kube-proxy 组件、xDS 和 Istio 对流量管理的抽象后，现在我们仅从流量管理的角度来看看这三个组件 / 协议的比较。请注意，三者并不完全等同。Kubernetes 更加注重的是应用层面的流量管理，xDS 是更加抽象的协议层面的配置下发，而 Istio 是服务层面的配置。\n服务网格——云原生网络基础设施 在列举过以上 Kubernetes 和服务网格的对比后，我们可以看出服务网格在云原生应用架构中的地位。那就是构建一个云原生网络基础设施，具体来说就是：\n流量管理：控制服务间的流量和API调用流，使调用更可靠，增强不同环境下的网络鲁棒性。 可观测性：了解服务之间的依赖关系和它们之间的性质和流量，提供快速识别定位问题的能力。 策略实施：通过配置网格而不是以改变代码的方式来控制服务之间的访问策略。 服务识别与安全：提供在网格里的服务可识别性和安全性保护。 社区里关于 Istio 和服务网格的争论 然而构建基础设施，可谓牵一发而动全身。理想很丰满，现实很骨感。关于服务网格和 Istio，在社区中也不乏争论。我们来看看有这些争论主要有哪些。\n这里列举了我在社区中观察到的关于 Istio 和服务网格最常见的几个问题。\n有人在生产使用 Istio 吗？ 为 pod 注入 sidecar 后带来的大量资源消耗，影响应用性能？ Istio 支持的协议有限，不易扩展？ Istio 太过复杂，老的服务迁移成本太高，业界经验太少，学习曲线陡峭？ 第一个问题，也是很多人刚加入社区和了解这门技术的时候，问的第一个问题，那是有人在生产使用 Istio 吗？\n随着对 Istio 研究的深入，很多人就会抛出第二个问题，为 pod 注入 sidecar 后带来的大量资源消耗，会影响应用性能吗？\n如果能问到第三个问题，说明对 Istio 有比较强的需求了，大多是使用了自定义的 RPC，对 Istio 的协议扩展有需求。 最后一个问题是抱怨 Istio 的概念太过复杂，也没有一个清晰的迁移路径可以使用，学习曲线太过陡峭。\n下面我将一一回答这些问题。\nIstio 架构稳定，生产可用，生态渐起 Istio 发布时间表 首先我们来看下 Istio 的发布时间表，1.12 版本在上周刚刚发布，这里列举了从它开源到 1.8 版本发布的时间表。2018 年可以说是服务网格爆发之年，Tetrate 也在这一年成立。自1.5 版本起 Istio 正式确立了当前的架构。Istio 社区也也举办了丰富多彩的活动，2021 年 3 月首届 IstioCon 召开，7 月 Istio Meetup China 在北京举行，2022 年 1 月，Service Mesh Summit 2022 也将在上海举行。\nIstio 有着庞大的社区以及供应商和用户群体 。目前主流公有云全都支持了 Istio 服务网格，如阿里云、华为云、腾讯云、网易云等，Istio 的官网上也列举了几十个社区用户，云原生社区 Istio SIG 还陆续举办了八场 Istio 大咖说 ，百度、腾讯、网易、小红书、小电科技都来分享过他们的 Istio 实践。\n还有很多企业基于 Istio 做了二次开发或者适配或者为其开发插件，可以说是 Istio 架构已稳定，生产可用，生态正在萌芽中。\n服务网格对应用性能的影响 服务网格为了做到对应用程序透明，默认采用了 iptables 流量劫持的方式，当服务数量大的时候会有大量的 iptables 规则，影响网络性能，你可以使用 eBPF 这样的技术来提高应用性能，但是该技术对操作系统内核的版本要求比较高，很少有企业能够达到。\nIstio 中的智能 DNS 代理 来源：https://cloudnative.to/blog/istio-dns-proxy/ 还有一种方式，也是小红书使用的方式 ，那就是利用 Istio 1.8 中引入的智能 DNS 代理功能。首先使用 ServiceEntry 定义服务，让所有服务属于一个 VIP 范围，再利用 Istio 的智能 DNS 代理功能，让sidecar只拦截 VIP 网段的流量，这样可以减少 iptables 规则，从而提高性能。如果想深入了解这个做法的细节，大家可以去浏览 Istio 大咖说第八期的分享视频 。\nIstio 在初期是将整个网格内的所有服务的路由信息全量下发到所有的 proxy sidecar 中，会导致 sidecar 占用大量资源，后来 Istio 引入了 Sidecar 资源 来精细化控制需要下发的代理配置范围，另外还有企业自己开发了配置懒加载功能，例如腾讯云开源的 Aeraki 、网易开源的 Slime 都可以实现配置懒加载。我们会在 Istio 开源生态中介绍这两个开源项目。\n最后是一个涉及到 Sidecar proxy 运维的问题，如何在保证流量不断的情况下，升级所有 Envoy 代理，这个阿里开源的 OpenKruise 中的 SidecarSet 资源已经给出了解决方案。\n另外 Sidecar 的引入带来的资源消耗以及网络延迟也是在合理的范围内，大家可以参考 Istio 官方博客上的 Service Mesh 基准性能测试 。\n扩展 Istio 服务网格 下一个问题是关于扩展 Istio 服务网格的。目前官方社区给出的方案是使用 WebAssembly，目前这种扩展方式在国内用的还比较少，而且性能也堪忧。我观察到的大部分解决方案都是自定义 CRD，基于 Istio 构建服务网格管理平面。\n另外，让 Istio 支持异构环境，适用于一切工作负载，如虚拟机、容器，这个对于终端用户来说也有很强的需求，因为这可以让用户很方便的从传统负载迁移应用到服务网格中。最后是多集群、多网格的混合云流量管理，这个属于比较高阶的需求了。\n陡峭的学习曲线 以下列举的是 Istio 学习资源：\nIstio 官网中文文档 IstioCon 2021 Istio Meetup China Istio 大咖说/Istio Weekly 云原生社区 Istio SIG Istio 基础教程（中文） Certified Istio Administrator Istio 开源至今已有 4 年时间，2018 年时我和敖小剑一起创建了 ServiceMesher 社区，当时组织过 9 次 Service Mesh Meetup，同其他服务网格爱好者一起翻译了 Istio 的官方文档。我还在今年初参与了 IstioCon 2021 的筹办及首届 Istio Meetup China。可以说是亲眼目睹了国内服务网格技术的应用和发展，在这期间也写过和翻译过大量的文章，加入 Tetrate 后，我还参与发布了 Istio 基础教程，免费提供给大家学习。同时 Tetrate …","relpermalink":"/blog/service-mesh-insight/","summary":"本文探讨了服务网格和云原生的关系，社区发展现状，开源生态，及未来发展。","title":"服务网格现状之我见"},{"content":"业界首个在线的免费中文多媒体的 Istio 教程来了，在 9 月 25 日周六的云原生社区 meetup 第七期深圳站上，Tetrate 布道师、云原生社区创始人宋净超（Jimmy Song）发布了 Istio 中文基础教程。\n该教程由企业级服务网格供应商 Tetrate 出品，该课程已上线，可免费报名学习。报名地址：https://academy.tetrate.io/courses/istio-fundamentals-zh Istio 基础教程页面 课程简介 本课程分为 8 个模块，其中包括理论部分，我们将学习 Istio 的各项功能；实践实验室，我们将在实践中尝试这些功能；以及测试你的这些知识的掌握程度。\n在本课程结束时，您将了解什么是 Istio，使用真实世界的例子，了解服务网格给您的组织带来的价值。你将能够配置流量路由、注入故障、使用弹性功能并保护你的服务。本课程是为了加强你的学习，在课程结束时，如果你能正确回答所有测验的 70%，将提供一份完成证书。\n本课程是为了加强你的学习，在课程结束时，如果你能正确回答所有测验的 70%，将提供一份完成证书。\n本课程有 4 个小时的内容，是自定进度的。你可以按照自己的节奏完成，可以在一天内完成，或者根据你的方便，分散在不同的时间内完成。\n","relpermalink":"/notice/istio-fundamental-courses-zh/","summary":"本教程有企业级服务网格提供商 Tetrate 出品。","title":"首个免费的多媒体 Istio 在线中文教程来了"},{"content":" 时间：2021 年 8 月 22 日（星期日） 地点：大连市甘井子区汇贤街大连腾飞园区腾飞软件园 5 号楼（世达教育） 主持人：马景贺（云原生社区管委会成员，云原生社区大连站长） 报名方式：活动行 关于云原生社区 云原生社区是国内最大的独立第三方云原生终端用户和泛开发者社区，由 CNCF 大使、开源意见领袖共同发起成立于 2020 年 5 月 12 日，提供云原生专业资讯，促进云原生产业发展。\n社区官网：https://cloudnative.to 加入大连站 关注云原生社区微信公众号，微信搜索（CloudNativeCN），在后台回复「大连站」即可加入大连站微信群。\n","relpermalink":"/notice/cloud-native-meetup-dalian/","summary":"8 月 22 日，周日，大连见！","title":"云原生社区 Meetup 第六期大连站报名"},{"content":"近日美国国家安全局（NSA）和网络安全与基础设施安全署（CISA）发布了一份网络安全技术报告 Kubernetes Hardening Guidance（查看英文原版 PDF ）。\nJimmy 翻译的《Kubernetes 加固指南》（或译作《Kubernetes 强化指南》）中文版，点击在线阅读 ，如您发现错误，欢迎在 GitHub 上提交勘误。\n《Kubernetes 加固指南》或（《Kubernetes 强化指南》中文版封面 ","relpermalink":"/notice/kubernetes-hardening-guidance-zh-release/","summary":"《Kubernetes 加固指南》（或译作《Kubernetes 强化指南》中文版发布。","title":"《Kubernetes 加固指南》中文版发布"},{"content":"API 网关作为客户端访问后端的入口，已经存在很长时间了，它主要是用来管理”南北向“的流量；近几年服务网格开始流行，它主要是管理系统内部，即“东西向”流量，而像 Istio 这样的服务网格还内置了网关，从而将系统内外部的流量纳入了统一管控。这经常给初次接触 Istio 的人带来困惑——服务网格与 API 网关之间是什么关系？是不是使用了 Istio 就可以替代了 API 网关？Istio 的 API 网关是如何运作的？有哪些方式暴露 Istio mesh 中的服务？这篇文章给为你解答。\n主要观点 服务网格诞生的初衷是为了解决分布式应用的内部流量的管理问题，而在此之前 API 网关已存在很久了。 虽然 Istio 中内置了Gateway，但是你仍可以使用自定义的 Ingress Controller 来代理外部流量。 API 网关和服务网格正朝着融合的方向发展。 如何暴露 Istio mesh 中的服务？ 下图展示了使用 Istio Gateway、Kubernetes Ingress、API Gateway 及 NodePort/LB 暴露 Istio mesh 中服务的四种方式。\n暴露 Kubernetes 中服务的几种方式 其中阴影表示的是 Istio mesh，mesh 中的的流量属于集群内部（东西向）流量，而客户端访问 Kubernetes 集群内服务的流量属于外部（南北向）流量。不过因为 Ingress、Gateway 也是部署在 Kubernetes 集群内的，这些节点访问集群内其他服务的流量就难以归属了。\n方式 控制器 功能 NodePort/LoadBalancer Kubernetes 负载均衡 Kubernetes Ingress Ingress Controller 负载均衡、TLS、虚拟主机、流量路由 Istio Gateway Istio 负载均衡、TLS、虚拟主机、高级流量路由、其他 Istio 的高级功能 API 网关 API Gateway 负载均衡、TLS、虚拟主机、流量路由、API 生命周期管理、权限认证、数据聚合、账单和速率限制 由于 NodePort/LoadBalancer 是 Kubernetes 内置的基本的暴露服务的方式，本文就不讨论这种方式了。下文将对其他三种方式分别作出说明。\n使用 Kubernetes Ingress 暴露服务 我们都知道 Kubernetes 集群的客户端是无法直接访问 Pod 的 IP 地址的，因为 Pod 是处于 Kubernetes 内置的一个网络平面中。我们可以将 Kubernetes 内的服务使用 NodePort 或者 LoadBlancer 的方式暴露到集群以外。同时为了支持虚拟主机、隐藏和节省 IP 地址，可以使用 Ingress 来暴露 Kubernetes 中的服务。Kubernetes Ingress 原理如下图所示。\n使用 Kubernetes Ingress 暴露服务 简单的说，Ingress 就是从 Kubernetes 集群外访问集群的入口，将用户的 URL 请求转发到不同的服务上。Ingress 相当于 Nginx、Apache 等负载均衡方向代理服务器，其中还包括规则定义，即 URL 的路由信息，路由信息得的刷新由 Ingress controller 来提供。\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: istio name: ingress spec: rules: - host: httpbin.example.com http: paths: - path: /status/* backend: serviceName: httpbin servicePort: 8000 上面的例子中的 kubernetes.io/ingress.class: istio 注解表明该 Ingress 使用的 Istio Ingress Controller。\n使用 Istio Gateway 暴露服务 我们都知道 Istio 是继承 Kubernetes 之后发展出来的一个流行的服务网格实现，它实现了 Kubernetes 没有的一些功能，请参考什么是 Istio？为什么 Kubernetes 需要 Istio？ 简要来说，正是因为 Istio 补足了 Kubernetes 对于云原生应用的流量管理、可观察性和安全方面的短板，使得流量管理变得对应用程序透明，使这部分功能从应用程序中转移到了平台层，成为了云原生基础设施。\nIstio 0.8 以前版本中使用 Kubernetes Ingress 来作为流量入口，其中使用 Envoy 作为 Ingress Controller。在 Istio 0.8 及以后的版本中，Istio 创建了 Gateway 对象。Gateway 和 VirtualService 用于表示 Istio Ingress 的配置模型，Istio Ingress 的缺省实现则采用了和 sidecar 相同的 Envoy 代理。通过该方式，Istio 控制面用一致的配置模型同时控制了入口网关和内部的 sidecar 代理。这些配置包括路由规则，策略检查、遥测收集以及其他服务管控功能。\nIstio Gateway 的功能与 Kubernetes Ingress 类似，它负责进出集群的南北流量。Istio Gateway 描述了一个负载均衡器，用于承载进出服务网格边缘的连接。该规范描述了一组开放端口和这些端口所使用的协议，以及用于负载均衡的 SNI 配置等。\nIstio Gateway 资源本身只能配置L4到L6的功能，例如暴露的端口、TLS 设置等；但 Gateway 可与 VirtualService 绑定，在VirtualService 中可以配置七层路由规则，例如按比例和版本的流量路由，故障注入，HTTP 重定向，HTTP 重写等所有Mesh内部支持的路由规则。\n下面是一个 Gateway 与 VirtualService 绑定的示例。拥有 istio: ingressgateway 标签的 pod 将作为 Ingress Gateway 并路由对 httpbin.example.com 虚拟主机的 80 端口的 HTTP 访问，这相当于给 Kubernetes 敞开了一个外部访问的入口。这与使用 Kubernetes Ingress 最大的区别就是，需要我们手动将VirtualService与Gateway 绑定，并指定 Gateway 所在的 pod。\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: httpbin-gateway spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;httpbin.example.com\u0026#34; 下面这个 VirtualService 通过 gateways 与上面的网关绑定在了一起，以接受来自该网关的流量。\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: httpbin spec: hosts: - \u0026#34;httpbin.example.com\u0026#34; gateways: - httpbin-gateway http: - match: - uri: prefix: /status route: - destination: port: number: 8000 host: httpbin 使用 API 网关暴露服务 API 网关是位于客户端和后端服务之间的 API 管理工具，一种将客户端接口与后端实现分离的方式，在微服务中得到了广泛的应用。当客户端发出请求时，API 网关会将其分解为多个请求，然后将它们路由到正确的位置，生成响应，并跟踪所有内容。\nAPI Gateway 是微服务架构体系中的一类型特殊服务，它是所有微服务的入口，它的职责是执行路由请求、协议转换、聚合数据、认证、限流、熔断等。大多数企业 API 都是通过 API 网关部署的。API 网关通常会处理跨 API 服务系统的常见任务，例如用户身份验证、速率限制和统计信息。\n在网格中可以有一个或多个 API Gateway。API 网关的职责有：\n请求路由和版本控制 方便单体应用到微服务的过渡 权限认证 数据聚合：监控和计费 协议转换 消息和缓存 安全和报警 以上很多基本功能比如路由和权限认证通过 Istio Gateway 也可以实现，只是在功能的丰富度和扩展性方面有些成熟的 API Gateway 可能更占优势，不过在 Istio mesh 中再引入 API Gateway 也可能带来一些弊端。\n引入了 API Gateway，需要考虑 API Gateway 本身的部署、运维、负载均衡等场景，增加了后端服务的复杂度 API Gateway 中承载了大量的接口适配，导致难以维护 对于部分场景，增加了一跳可能导致性能的降低 总结 在 Istio mesh 中你可以使用多种 Kubernetes Ingress Controller 来充当入口网关，当然你还可以直接使用 Istio 内置的 Istio 网关，对于策略控制、流量管理和用量监控可以直接通过 Istio 网关来完成，这样做的好处是通过 Istio 的控制平面来直接管理网关，而不需要再借助其他工具。但是对于 API 生命周期管理、复杂的计费、协议转换和认证等功能，传统的 API 网关可能更适合你。所以，你可以根据自己的需求来选择，也可以组合使用。\n目前有些传统的反向代理也在向 Service Mesh 方向发展，如 Nginx 构建了 Nginx Service Mesh ，Traefik 构建了 Traefik Mesh 。还有的 API 网关产品也向 Service Mesh 方向挺进，比如 Kong 发展出了 Kuma 。在未来，我们会看到更多 API 网关、反向代理和服务网格的融合产品出现。\n参考 利用 Gateway API 发展 Kubernetes 网络 如何为服务网格选择入口网关？ Service Mesh 和 API Gateway 关系深度探讨 在 Istio 服务网格中使用 Traefik Ingress Controller ","relpermalink":"/blog/istio-servicemesh-api-gateway/","summary":"服务网格与 API 网关之间是什么关系？是不是使用了 Istio 就可以替代了 API 网关？Istio 的 API 网关是如何运作的？有哪些方式暴露 Istio mesh 中的服务？这篇文章给为你解答。","title":"如何理解 Istio Ingress， 它与 API Gateway 有什么区别？"},{"content":" ApacheCon Asia 2021 为了更好地服务于亚太快速增长的 Apache 用户和贡献者，ApacheCon 组委会以及 Apache 软件基金会宣布首次针对亚太地区时区的 ApacheCon 在线会议 ApacheCon Asia 大会将于 2021 年 8 月 6 日至 8 日在线举行。周六（8 月 7 日）下午1 点半到 5 点半，因为「可观察性」专题的出品人吴晟临时有事无法主持，我将代替主持，欢迎大家观看直播。\n可观察性专题 时间 话题 时长 13:30 - 14:10 如何使用Apache SkyWalking Log Analysis Language来进行日志分析, Zhenxu Ke (柯振旭)【中】 40分钟 14:10 - 14:50 Satellite如何强化SkyWalking生态, Evan【英】 40分钟 14:50 - 15:30 使用 Skywalking 监控 Dolphinscheduler, 王海林【中】 40分钟 15:30 - 16:10 如何使用剖析来弥补分布式追踪的盲区, Han Liu【中】 40分钟 16:10 - 16:50 SkyWalking UI的低代码模式,Qiuxia Fan【英】 40分钟 16:50 - 17:30 使用 SkyWalking 和 APISIX 来增强 Nginx 的可观测性, Ming Wen【中】 40分钟 出品人 以下是本次大会的分论坛和出品人：\nAPI / 微服务 温铭 Incubator 潘娟 Keynote 谭中意 Web Server/Tomcat 张乎兴 中间件 张亮， Jean-Baptiste Onofre 可观测性 吴晟 大数据 张铎, 堵俊平, 代立冬 工作流和数据治理 郭炜 开源社区 李建盛 数据可视化 羡辙 流处理 李钰 消息系统 翟佳，王殿进 物联网（IoT）和工业物联网（IIoT） 黄向东, Christofer Dutz 集成 姜宁, Zoran Regvart, María Arias de Reyna 详细日程请查看Apache 首次亚洲线上技术峰会 | 议程总览 ，看看有没有你熟悉的开源项目，点击注册参会 。\n","relpermalink":"/notice/apachecon-asia-2021/","summary":"首次针对亚太地区时区的 ApacheCon 在线会议 ApacheCon Asia 大会将在 8 月 6 日至 8 日线上举行。","title":"ApacheCon Asia 2021"},{"content":" 在琼库什台村与哈萨克族村民在一起 摄于新疆维吾尔自治区伊犁哈萨克自治州特克斯县琼库什台村，晓辉和我与哈萨克族村民在一起\n我欲乘风破浪，踏遍黄沙海洋\n与其误会一场，也要不负勇往\n——Jam，《七月上》\n今天是七月的最后的一天，从今天起我的博客将开通旅行 专栏。距离我从新疆回到北京也快一个星期了，这篇博客用来记录我与张晓辉（Addo Zhang 在新疆的七日之旅。\n行程安排 因为晓辉最近处于工作变动空档期，而我又远程工作，今年元旦以来都没有到远游（上一次是元旦到云南大理、丽江），因此我们商议了为其七天的新疆北疆房车自驾之旅。\n我们将分别从北京和广州前往乌鲁木齐汇合然后提车出发，车是出发前就在网上预约好的，现场办手续，进行了 2 个小时的使用培训，然后就上路了。因为晓辉是老司机，而山路崎岖，房车又过于笨重，对于我这个新手难以驾驭，我就作为副驾，同时也会在路上工作。\n行程路线图 我们基本是按照出发前预定的路线走的，整个旅程最精彩的部分应该是：\n赛里木湖牧民家的烧烤晚餐 琼库什台原生态草原 独库公路 寄蜉蝣于天地，渺沧海之一粟。\n——苏轼，《前赤壁赋》\n夜宿在赛里木湖北门附近的牧民家旁边，夜晚可以看到璀璨的星空。\n赛里木湖的星空 如果说有个地方我想再重走一遍的话，我会选择伊犁哈萨克自治州，如果一定要确定一个具体的村庄的话，那我会选择琼库什台。\n琼库什台 我们在草原上工作，到哈萨克族老乡家吃饭一起庆祝古尔邦节，教小朋友玩无人机，一起骑马、打篮球，不亦乐乎。大美新疆，我一定会再来的！\n人生处处知何似，恰似飞鸿踏雪泥。\n——苏轼，《和子由渑池怀旧》\n在琼库什台草原上骑马，这也是我第一次骑马，骑着骑着，马自己就跑起来了。\n琼库什台骑马 视频 下面是我为本次旅程所剪辑的视频 ，希望大家一键三连（点赞、投币、转发）。\nBilibili 点播 贴士 对于去气候适宜或稳定的地域的短途旅行，建议租普通的 SUV 即可，如果想睡在野外可以搭帐篷，没必要租房车，因为在房车体积较大，大部分人没有驾驶过房车的经验，开起来会比较吃力，而且房车对于路况要求也比较高，很多沟沟坎坎、野路就没办法通过了。\n","relpermalink":"/blog/xinjiang-trip/","summary":"记录我与 Addo 一行七天的房车自驾之旅。","title":"新疆北疆房车自驾之旅"},{"content":"这篇文章将带你了解使用 Kubernetes 和 Istio Service Mesh 构建多集群及混合云的过程和需要考虑的问题。\nKubernetes 使用 Kubernetes 可以快速部署一个分布式环境，实现了云的互操作性，统一了云上的控制平面。并提供了 Service、Ingress 和 Gateway 等资源对象来处理应用程序的流量。如下图所示，Kubernetes 中默认使用 Service 做服务注册和发现，服务之间可以使用服务名称来访问。Kubernetes API Server 与集群内的每个节点上的 kube-proxy 组件通信，为节点创建 iptables 规则，并将请求转发到其他 pod 上。\n假定现在客户端要访问 Kubernetes 中的服务，首先请求会发送到 Ingress/Gateway 上，然后根据 Ingress/Gateway 里的路由配置转发到后端服务上（图中是服务 A），接着服务 A 对服务 B 请求的流量转发轮询到服务 B 的实例上。\nKubernetes Kubernetes 多集群管理 多集群管理最常见的使用场景包括服务流量负载均衡、隔离开发和生产环境、解耦数据处理和数据存储、跨云备份和灾难恢复、灵活分配计算资源、跨区域服务的低延迟访问以及避免厂商锁定等。一个企业内部往往有多个 Kubernetes 集群，由 MultiCluster SIG 开发的 KubeFed 实现 Kubernetes 集群联邦可以实现多集群管理的功能，这使得所有 Kubernetes 集群都通过同一个接口来管理。\n在使用集群联邦时需要解决以下几个通用问题：\n配置需要联邦哪些集群 需要在集群中传播的 API 资源 配置 API 资源如何分配到不同的集群 对集群中 DNS 记录注册以实现跨集群的服务发现 下面是 KubeSphere 的多集群架构，也是最常用的一种 Kubernetes 多集群管理架构，其中 Host Cluster 作为控制平面，有两个成员集群，分别是 West 和 East。\nMulticluster Host 集群需要能够访问 Member 集群的 API Server，Member 集群之间的网络连通性没有要求。管理集群 Host Cluster 独立于其所管理的成员集群，Member Cluster 并不知道 Host Cluster 存在，这样做的好处是当控制平面发生故障时不会影响到成员集群，已经部署的负载仍然可以正常运行，不会受到影响。\nHost 集群同时承担着 API 入口的作用，由 Host Cluster 将对 Member 集群的资源请求转发到 Member 集群，这样做的目的是方便聚合，而且也利于做统一的权限认证。我们看到在 Host Cluster 中有联邦控制平面，其中的 Push Reconciler 会将联邦集群中身份、角色及角色绑定传播到所有成员集群中。\nIstio 当我们在 Kubernetes 中运行着多语言、多版本的微服务，并需要更细粒度的金丝雀发布和统一的安全策略管理，实现服务间的可观察性时，可以考虑使用 Istio 服务网格。Istio 通过向应用程序 Pod 中注入 sidecar proxy，缺省使用 IPTables 透明得拦截进出应用程序的所有流量，从而实现了应用层到集群中其他启用服务网格的服务的智能应用感知负载均衡，并绕过了初级的 kube-proxy 负载均衡。Istio 控制平面与 Kubernetes API Server 通信可以获取集群中所有注册的服务信息。\n下图展示了 Istio 的基本原理，其中所有节点属于同一个 Kubernetes 集群。\nIstio Service Mesh 你可能最终会有至少几个Kubernetes集群，每个集群都承载着微服务。Istio 的多集群部署根据网络隔离、主备情况存在多种部署模式 ，可以使用 Istio Operator 部署时通过声明来指定。集群中的这些微服务之间的通信可以通过服务网格来加强。在集群内部，Istio提供通用的通信模式，以提高弹性、安全性和可观察性。\n以上都是关于 Kubernetes 上的应用负载管理，但是对于虚拟机上遗留应用，如何在同一个平面中管理？如何管理多集群中的流量划分、网关和安全性呢？\n管理平面 在 Istio 之上再增加一层抽象，将网关、流量和安全分组管理，并将它们应用到不同的集群和命名空间上。下图展示的是 Tetrate Service Bridge 的多租户模型，利用 NGAC 来管理用户的访问权限，同时也有利于构建零信任网络。\nManagement Plane Istio 提供了工作负载识别，并由强大的 mTLS 加密保护。这种零信任模型比基于源 IP 等拓扑信息来信任工作负载更好。在 Istio 之上构建一个多集群管理的通用控制平面，然后再增加一个管理平面来管理多集群，提供多租户、管理配置、可观察性等功能。\n下图展示的是 Tetrate Service Bridge 的架构图。\nTetrate Service Bridge 总结 使用 Kubernetes 实现了异构集群的互操作性，Istio 将容器化负载和虚拟机负载纳入到一个同一个控制平面内，统一管理集群内的流量、安全和可观察性。但是，随着集群数量、网络环境和用户权限的越发复杂，人们还需要在 Istio 的控制平面至上再构建一层管理平面来进行混合云管理。\n","relpermalink":"/blog/multicluster-management-with-kubernetes-and-istio/","summary":"这篇文章将带你了解使用 Kubernetes 和 Istio Service Mesh 构建多集群及混合云的过程和需要考虑的问题。","title":"服务网格之旅——使用 Kubernetes 和 Istio Service Mesh 构建混合云"},{"content":"Kubernetes 可以说是目前为止用来运行微服务的最佳载体，但是在调试 Kubernetes 环境中的微服务时的体验可能就没那么友好了。本文将带你了解如何调试 Kubernetes 中的微服务，介绍常用的工具，以及 Istio 的引入为微服务的调试带来的变革。\n调试微服务与传统单体应用有巨大的不同 微服务的调试是一直长期困扰软件开发人员的问题，这在传统的单体应用中不存在，因为开发者可以利用 IDE 中的调试器，为应用程序增加断点、修改环境变量，单步执行等，这些都为软件调试提供了巨大帮助。随着 Kubernetes 的流行，微服务的调试就成了一个棘手的问题，其中相比传统单体应用的调试多了以下问题：\n多依赖 一个微服务往往依赖多个其他微服务，在调试某个微服务时，如何部署其他依赖服务以快速搭建一套最新的 stagging 环境？\n从本地机器访问 微服务在开发者的本地电脑上运行时，通常无法直接访问到 Kubernetes 集群中的服务，如何像调试本地服务一样调试部署在 Kubernetes 集群中的微服务？\n开发效率低下 通常情况下，代码从更新到构建成镜像再推送到集群中需要一个漫长的过程，如何加快开发速度？\n我们一起来看下哪些工具能够解决以上问题。\n工具 调试 Kubernetes 中的微服务的主要解决方案有：\nProxy：在 Kubernetes 集群和本地调试终端中部署一个代理，通过构建一个 VPN，使得本地应用可以直接访问到 Kubernetes 中的服务； Sidecar：替换原来应用容器的镜像为开发镜像，可以在这个容器中中对该服务进行调试，同时在要调试的微服务 pod 中注入一个 sidecar 作为辅助工具来同步代码； 服务网格：要想了解应用的整体情况，就需要在所有微服务中注入 sidecar，这样你就可以获得一个监控全局状态的仪表板； 下面是实现以上解决方案的三个典型的开源项目，它们分别从不同的角度可以帮助你调试微服务。\nProxy 模式：Telepresence Telesprence 本质上是一个本地代理，该代理将 Kubernetes 集群中的数据卷、环境变量、网络都代理到了本地。下图展示的是 Teleprence 的主要使用场景。\nProxy 模式：Telepresence 用户需要在本地自主地执行 telepresence 命令，它会自动将代理部署到 Kubernetes 中，有了该代理之后：\n本地的服务就可以完整的访问到 Kubernetes 集群中的其他服务、环境变量、Secret、ConfigMap 等； 集群中的服务还能直接访问到本地暴露出来的端点； 但是这种方式仍然不够连贯，还需要用户在本地调试时运行多次命令，而且在某些网络环境下可能无法与 Kubernetes 集群建立 VPN 连接。\nSidecar 模式：Nocalhost Nocalhost 是一个基于 Kubernetes 的云端开发环境。要想使用它，你只需要在你的 IDE——VS Code 中安装一个插件即可扩展 Kubernetes，并缩短开发反馈周期。通过为不同的用户创建不同的 namespace，并使用 ServiceAccount 绑定到不同用户角身上时，就可以实现开发环境隔离。同时，Nocalhost 还提供了 Web 控制台和 API，方便管理员来管理不同的开发环境。\nSidecar 模式：Nocalhost 测试 参考 Nocalhost 文档 ，我们在 macOS 上安装 Nocalhost，并使用 Minikube 来演示如何调试。\n执行下面的命令安装 Nocalhost 客户端并查看 nhctl 命令行工具的版本。\nbrew install nocalhost/repo/nocalhost nhctl version 我们假设你机的 kubeconfig 文件位于 ~/.kube/config（若不在此位置需要在下面的命令中使用 --kubeconfig 手动指定） 并拥有 Kubernetes 集群的 admin 角色，执行下面的命令使用 Helm3 在 Kubernetes 上安装 Nocalhost 服务端。\nnhctl init demo -n nocalhost 执行下面的命令启动 Minikube 隧道并查看 Nocalhost web 端地址。\nminikube tunnel kubectl get service nocalhost-web 在浏览器中访问 http://\u0026lt;EXTERNAL-IP\u0026gt; 即可，用户名/密码为：admin@admin.com/123456。\n要想在 VS Code 中使用，你还想需要创建一个 ServiceAccount 并绑定 admin 角色，然后将该 ServiceAccount 作为 Kubeconfig 文件导出。\nkubectl create serviceaccount my-service-account kubectl create rolebinding admin --clusterrole=admin --serviceaccount=default:my-service-account 只要你有一个 Kubernetes 集群，并有集群的 admin 权限，就可以参考 Nocalhost 的文档快速开始试用。在 VS Code 中使用 Nocalhost 插件时需要先为插件中配置 Kubernetes 集群。选择你刚导出的 Kubeconfig 文件或者直接复制文件中的内容粘贴到配置里。然后选择你需要测试的服务，并选择对应的 Dev Container，VS Code 会自动打开一个新的代码窗口。\n下面是以 Istio 官方提供的 bookinfo 示例 为例，你可以在本地 IDE 中打开克隆下来的代码，然后点击代码文件旁边的锤子即可进入开发模式。选择对应的 DevContainer，nocalhost 会自动向 pod 中注入一个开发容器 sidecar，并在终端中自动进入该容器，如下图所示。\nNocalhost VS code 界面 在开发模式中，本地修改代码，无需重新构建镜像，远端开发环境实时生效，这样可以极大的加快开发速度。同时，Nocalhost 还提供了服务端，可用于开发环境和用户权限进行管理，如下图所示。\nNocalhost web 端 Service Mesh 模式：Istio 以上使用 proxy 和 sidecar 的方式，一次只能对一个服务进行调试，如果想要掌握服务的全局状况，比如获取的服务的指标，以及通过分布式追踪了解服务的依赖和调用流程，对服务的性能进行调试。这些可观察性 的功能，需要为所有服务统一注入 sidecar 来实现。\n而且，当你的服务正处于从虚拟机迁移到 Kubernetes 的过程中时，使用 Istio 可以将虚拟机与 Kubernetes 纳入一个网络平面中（如下图所示），方便开发者调试和做渐进式的迁移。\nSerivce Mesh 模式：Istio 当然要获得这些好处也不是一点“代价”也不没有的，引入 Istio 后，你的 Kubernetes service 需要遵守 Istio 的命名规范 ，学习使用 Istioctl 命令行和日志的方式来调试微服务。\n使用 istioctl analyze 命令来调试集群中的微服务部署情况，可以使用 YAML 文件来检查某个命名空间或整个集群中的资源部署情况。 使用 istioctl proxy-config secret 来调试 service mesh 中的 pod 的 secret 被正确的加载并有效。 Istio 的配置信息在大型的集群部署中传播将会耗时更长并且可能有几秒钟的延迟时间，sidecar 的引入会给服务间调用带来一定延迟。\n总结 在应用微服务化和从虚拟机迁移到 Kubernetes 的过程中，开发者需要很多观念和习惯上的转变。通过 proxy 在本地跟 Kubernetes 间构建 VPN，可以方便开发者像调试本地服务一样调试 Kubernetes 中的服务。通过向 pod 中注入 sidecar，可以实现实时调试，加快开发进度。最后，Istio service mesh 真正实现了全局的可观察性，你还可以使用像 Tetrate Service Bridge 这样的工具来管理异构平台，帮助你渐渐地从单体应用过度到微服务。\n","relpermalink":"/blog/how-to-debug-microservices-in-kubernetes-with-proxy-sidecar-or-service-mesh/","summary":"本文讲解了调试 Kubernetes 中微服务的三种模式/工具，以及 Istio 的引入为微服务的调试带来的变革。","title":"如何调试 Kubernetes 中的微服务 ——proxy、sidecar 还是 service mesh？"},{"content":" 时间：2021 年 07 月 03 日（星期六） 地点：益州大道中段 1999 号银泰城写字楼 - 4 号楼 19F 亚马逊云科技 主持人：粟伟（云原生社区管委会成员，云原生社区成都站长） 报名方式：活动行 关于云原生社区 云原生社区是一个中立的云原生终端用户社区，致力于为开发者提供云原生领域的专业资讯，推动中国云原生产业发展，目标成为中国云原生领域最具影响力的开源社区。\n社区官网：https://cloudnative.to 加入成都站 关注云原生社区微信公众号，微信搜索（CloudNativeCN），在后台回复「成都站」即可加入成都站微信群。\n","relpermalink":"/notice/cloud-native-meetup-chengdu/","summary":"7 月 3 日，周六，成都见！","title":"云原生社区 Meetup 第五期成都站报名"},{"content":" 时间：6 月 23 日（星期三）晚 8 点 - 9 点 直播间：https://live.bilibili.com/23095515 主持人：宋净超（Tetrate） 嘉宾：陈鹏（百度） 话题：如何让 Istio 在大规模生产中落地 提问地址：腾讯文档 嘉宾简介 陈鹏，百度研发工程师，现就职于百度基础架构部云原生团队，主导和参与了服务网格在百度内部多个核心业务的大规模落地，对云原生、Service Mesh、Isito 等方向有深入的研究和实践经验。\n话题介绍 百度服务治理现状 \u0026amp; Istio 落地挑战 深入解读如何让 Isito 在大规模生产环境落地 实践经验总结 \u0026amp; 思考 通过本次分享你将了解当前 Isito 落地的困境和解决思路。\n","relpermalink":"/notice/istio-big-talk-ep4/","summary":"《Istio 大咖说》第 4 期，6 月 23 日，周三晚 8 点将在 B 站直播，欢迎收看。","title":"《Istio 大咖说》第 4 期节目预告"},{"content":"安全问题从互联网诞生之初就存在了，云原生因为涉及到高度分布式，安全问题更加严峻。云原生计算基金会（CNCF）特别就此发布了Cloud Native Security Whitepaper v1.1 ，云原生社区将其翻译成了中文版，并合并到了官方仓库，见云原生安全白皮书中文版 。\n云原生安全白皮书（CNCF 出品，云原生社区译） 该白皮书旨在为组织以及技术领导者提供对云原生安全的清晰理解，及其如何在参与整个生命周期流程中使用和评估安全相关的最佳实践。云原生安全是一个多目标和多限制的复杂问题范畴，会跨越许多专业技术和实践领域。软件生命周期中 Day 1、Day 2 的绝大多数操作都会涉及到从身份管理到存储解决方案的安全技术或领域。然而，云原生安全所涵盖的内容远不止这些领域；它是个关于人的广义问题范畴，包含个体、团队和组织。它应该成为人和系统在深度使用甚至改造云原生应用技术过程的一种机理、流程和理念基础。\n","relpermalink":"/notice/cloud-native-security-whitepaper-zh/","summary":"由云原生社区组织翻译的 CNCF 出品的《云原生安全白皮书 v1.1》中文版发布。","title":"CNCF 云原生安全白皮书 v1.1 中文版发布"},{"content":" 时间：6 月 9 日晚 8 点-9点 直播间：https://live.bilibili.com/23095515 主持人：宋净超 嘉宾：杨笛航 话题：如何让 Istio 变得更为高效和智能 嘉宾简介\n杨笛航，Istio 社区成员，网易数帆架构师，负责网易轻舟 Service Mesh 配置管理，并主导 Slime 组件设计与研发，参与网易严选和网易传媒的 Service Mesh 建设。具有三年 Istio 控制面功能拓展和性能优化经验。\n话题介绍\nIstio 作为当前最火的 Service Mesh 框架，既有大厂背书，也有优秀的设计，及活跃的社区。但是随着 Mixer 组件的移除，我们无法通过扩展 mixer adapter 的方式实现高阶的流量管理功能，Istio 的接口扩展性成为亟待解决的问题。本次直播将分享本次分享将介绍网易自研的智能网格管理器 Slime，借助它，我们实现了配置懒加载，自适应限流，HTTP 插件管理等扩展功能，从而更为高效的使用 Istio。\n听众收益\n了解在实际业务中驾驭 Istio 框架的挑战 了解 Slime 的设计特点、技术路线及开源进展 了解网易解决 Service Mesh 架构成熟的经验 ","relpermalink":"/notice/istio-big-talk-ep3/","summary":"《Istio 大咖说》第 3 期，6 月 9 日，周三晚 8 点将在 B 站直播，欢迎收看。","title":"《Istio 大咖说》第 3 期节目预告"},{"content":"《Istio 大咖说》第 2 期今晚 8 点将在 B 站开播，欢迎收看。\n时间：2021 年 6 月 2 日晚 8 点 主持人：宋净超（Tetrate） 嘉宾：潘天颖（小电科技） 主题：从微服务架构到 Istio——架构升级实践分享 嘉宾介绍：小电科技工程师，云原生爱好者，Kubernetes contributor，Apache committer 直播间：https://live.bilibili.com/23095515 互动归档：见腾讯文档 详见：第 2 期：从微服务架构到 Istio—— 架构升级实践分享 。\n","relpermalink":"/notice/istio-big-talk-ep2/","summary":"《Istio 大咖说》第 2 期今晚 8 点将在 B 站开播，欢迎收看。","title":"《Istio 大咖说》第 2 期开播提醒"},{"content":" 5 月 30 日云原生社区将在珠海中安广场A栋15楼举办线下会议，欢迎大家踊跃报名（点击报名 ）！因为广州疫情防控要求，有两位讲师无法按计划来到现场参加，他们会通过直播参与。欢迎珠海的朋友们到现场参加活动。\n活动信息 时间：2020 年 5 月 30 日 14:00 - 17:05 地点：珠海市香洲区拱北夏湾中安广场 A 栋 15 楼 主办方：云原生社区 承办方：云原生社区珠海站 赞助商：红帽公司、亚马逊公司 场地赞助商：天承盛世（珠海）房地产代理有限公司、珠海市钧策商贸发展有限公司 合作伙伴：ServiceMesher、CNCF、Linux Foundation 活动流程 14:00 - 14:30 签到入场 14:30 - 14:35 主持人开场 14:35 - 15:15 漫步云端 —— 华发集团云平台建设经验分享 讲师：胡兆鹏\n个人介绍：华发集团高级经理，高级研发工程师；负责集团 PAAS 云平台运营管理，DevOps 流程构建和应用上云架构优化。\n15:15 - 15:55 云原生应用容器化与编排 讲师：关泽发\n个人介绍：红帽资深架构师。\n15:25 - 15:45 合影、茶歇 15:45 - 16:25 云原生时代下的应用原则 + 12-Factor 讲师：李俊杰\n个人介绍：AWS 解决方案架构师，负责基于 AWS 的云计算方案的咨询与架构设计，同时致力于容器方面研究和推广。\n16:25 - 17:05 企业云原生 PaaS 平台建设 讲师：王志德\n个人介绍：格力软件工程师。\n17:05 - 17:30 自由交流 请到活动行 上报名，或者扫描下面的二维码报名。\n加入云原生社区珠海站 在云原生社区 公众号后台回复「珠海站」即可获取站长联系方式，申请微信好友后即可加入云原生社区珠海站微信群。\n关于云原生社区城市站 为了方便云原生社区成员的线下交流以及后期本地站活动开展，云原生社区城市站计划在全球各地创建城市站长，站长负责本地活动组织，详见云原生社区城市站页面 。\n","relpermalink":"/notice/cloud-native-community-zhuhai/","summary":"5 月 30 日（周日）云原生社区珠海站聚会将按计划举行，欢迎到场参加。","title":"云原生社区珠海站聚会通知"},{"content":"今晚我第一次使用 Zoom + OBS 和马若飞在 B 站上进行了《Istio 大咖说》 栏目的第一期分享——「Istio 开源四周年回顾与展望」。考虑到很多社区、主播、调音台会有在 B 站或其他平台上直播的需求，特别将我的个人经验分享给大家，欢迎大家补充，我会不断优化直播体验。\n下图是我直播时桌面的情况，使用的设备有：\nMacBook Pro，这个自不必说 USB 麦克风博雅 BY-500，作为音频输入麦克风 海康威视外接摄像头，用作第二机位 iPad，作为直播监视器，同时回答观众的弹幕 环形补光灯，直播通常是在晚上，光线太暗需要补光 AirPods Pro，用于监听声音的，不作为音频输入 静音蓝牙键盘，防止键盘敲击声音影响直播的声音体验 iPhone，用于和直播讲师私下沟通，这样不会被直播出去 以上这些不是全部都需要的，只要你有一台电脑和一个耳机就可以直播。\n我的桌面 上面是直播时的桌面（请忽略我杂乱的被各种设备占满空间的桌面），下面是我的配置参考。\n硬件准备 电脑：macOS、Windows 都可以，我是用的是Macbook Pro 2016 年产，配置如下： 系统配置 外接麦克风：切勿直接使用电脑机身上自带的麦克风，那样会收录电脑风扇的声音，我使用的是博雅 BY-500（400 多块钱） 麦克风，电容式麦克风，指向性比较好，基本没有噪音。 摄像头：保证电脑上的摄像头可用，因为会议的时候需要开摄像头，或者用外接摄像头也可以。 网络：确保网速至少 100MB/s 的宽带，因为推流还是比较占用带宽的，而且还需要同时查看直播效果，对下行带宽也有要求。 另一台可联网设备：用来监控直播效果，可以是手机、iPad 等 软件准备 Zoom：需要 Pro 版，这样才可以举行超过 45 分钟的线上会议，否则会在超时后打断再重新加入，需要准备好账号，中国大陆用户貌似不能再注册？如果没有 zoom，换成其他任何一个会议软件都可以，比如腾讯会议。 OBS：用来做推流，到官网 下载最新的版本。 音频插件 Sunflower：点击跳转到下载页面 ，如果安装时遇到系统权限问题，请在命令行中执行 sudo spctl --master-disable 并在电脑的 系统首选项 的 安全与隐私 中批准来自任意途径的软件安装，如果看到有详情页面，点击进去批准软件发行商。 Bilibili：需要一个 B 站账号，并开通直播间，经过实名认证。 OBS 配置 下面是在 Macbook 中安装的 OBS 配置截图。\n需要注意的是输出、音频和视频的配置。请参考图中的配置，尤其注意编码控制、比特率的配置。\nOBS输出配置 OBS音频配置 请注意分辨率的配置，同时调整电脑屏幕的分辨率为 1440x900，不要使用太大的分辨率，否则可能导致直播画面黑屏。\nOBS视频配置 Macbook显示配置 会议直播 以上场景是仅限于本机画面的直播，还有中场景就是现场会议直播，你还需要录制现场画面，这时候你最好制作一个直播底板，例如下图。\nOBS 直播底板 该底板用于布局 PPT 和摄像头画面，同时底板上也包括了活动 logo 和主办方、赞助商信息。该图最好是 PNG 透明的格式，把图片放在布局最上层，这样就能很好地展示布局。\n音频配置 安装 sunflower 后，在 Midi 设备中创建一个多输出设备，如图。\nMacBook音频配置 选择 Sunflower（2ch）和你想要用来监听系统声音的设备，我是用的是 AirPods，你也可以选择其他耳机，总之不要让麦克风录到这个系统输出的即可。\n还要在 OBS 的麦克风配置里增加下新创建的这个输出设备，这样直播的时候就可以收录你的系统，也就是你的耳机听到的声音了，比如在视频会议中，所有人讲话的声音都会被直播出去。\n使用独立音频硬件 如果安装 sunflower 有问题的话，你也可以购买一款独立的音频设备，要知道一台电脑是可以安装多块声卡的，这些声卡可以通过 USB 接口转接，而且价格都很便宜（只要几十块钱），一旦有了多个声卡，你就可以为不同的音频源选择不同的输出，而且可以对它们的音量进行单独单独控制。例如下面这款 USB 外置声卡（非利益相关），即插即用，不需要安装任何软件。\nUSB 外置声卡 你也可以用 USB 外置声卡来转接其他的无线麦克风，比如我就转接了 Rode Wireless Go（你还需要买一根转接的音频线，红色的那根，用于连接声卡）。\n多路推流（多渠道同步直播） 因为我们在直播时往往有多个渠道，比如多个 B 站直播间、微信视频号等，如何使用 OBS 同步推流到多个渠道呢？可以使用 sorayuki/obs-multi-rtmp 插件（支持 Windows 和 macOS），注意需要将 OBS 升级到最新版本（至少 27.0.1 版本）。安装完插件，重启 OBS 后就可以看到一个窗口新建多路推流，如下图所示。\nOBS 新建多路推流 如果没有看到该窗口，请点击【视图】-【停靠部件】-【多路推流】即可显示。\nOBS 多路推流选项 直播效果 下面是当晚直播的 zoom 录制的视频直出，已上传到 B 站 ，大家可以感受下画面的清晰度还有声音效果，我还是比较满意的。\nBilibili 其中只有几个小插曲：\n因为我是用的是 AirPods 蓝牙耳机，戴上耳机的时候我无法确定它要连那个设备（我有两个 iPhone、1 个 iPad、1 个 MacBook 都有可能被脸上）活动开始的时候总是连不上 MacBook，一气之下把其他的苹果设备的蓝牙全关掉，只留下 MacBook 的蓝牙开启，这样可以保证连上 MacBook 说话的时候忘记了把麦克风静音了 直播大概进行了 1 个小时的时候，zoom 突然断开了 10 几秒钟后又自动重连，总体来说 zoom 会议还是比较稳定的，1 个小时左右的会议应该不会断连 直播开始前的检查 电脑屏幕分辨率调整为 1440 x 900 关闭与直播无关的 APP，减少系统资源占用 电脑设置为勿扰模式 使用外接麦克风，切勿直接使用电脑内置的麦克风，会收录风扇及键盘杂音，影响音质 使用外接耳机，如 AirPods 音频输出调整为多设备输出，其中包括 Sunflower（2ch）和耳机 Zoom 会议开始前记得点击录像 会议开始后检查 B 站直播间，确保声音和画质没有问题 OBS 推流的时候不用录像，因为 zoom 已经在录了 直播完成后检查 zoom 生成的视频文件并备份 优化项 直接直播屏幕内容也会造成多重布局的问题，以上方案还是有可优化的地方。比如将视频会议中的共享桌面与摄像头画面分开布局到 OBS 上。\n最后 直播是除了在线下面对面交流以外，可以跟社区及开源爱好者交流最直接最友好的方式，我会时常发起，感谢大家的关注我主持的直播间：\n云原生社区 Istio Service Mesh 关注上面的 B 站账号，获取直播推送提醒。关于 B 站直播，如果你有任何问题或者建议请在下面留言。\n","relpermalink":"/blog/zoom-obs-bilibili-broadcast/","summary":"本文将指导你如何配置和使用 OBS + zoom在 Bilibili 上直播。","title":"Zoom + OBS + B 站直播配置手册"},{"content":"Istio 是由 Tetrate 创始人 Varun Talwar 和谷歌首席工程师 Louis Ryan 命名并在 2017 年 5 月 24 日开源。今天是 Istio 开源四周年，让我们一起来回顾一下 Istio 四年来的发展并展望一下它的未来。\nIstio 的开源历史 2017 年是 Kubernetes 结束容器编排之战的一年，Google 为了巩固在云原生领域的优势，并弥补 Kubernetes 在服务间流量管理方面的劣势，趁势开源了 Istio。下面是截止目前 Istio 历史上最重要的几次版本发布。\n日期 版本 说明 2017-05-24 0.1 正式开源，该版本发布时仅一个命令行工具。确立了功能范围和 sidecar 部署模式，确立的 Envoy 作为默认 sidecar proxy 的地位。 2017-10-10 0.2 支持多运行时环境，如虚拟机。 2018-06-01 0.8 API 重构。 2018-07-31 1.0 生产就绪，此后 Istio 团队被大规模重组。 2019-03-19 1.1 企业就绪，支持多 Kubernetes 集群，性能优化。 2020-03-03 1.5 回归单体架构，支持 WebAssembly 扩展，使得 Istio 的生态更加强大。 2020-11-18 1.8 正式放弃 Mixer，进一步完善对虚拟机的支持。 Istio 开源后经过了一年时间的发展，在 1.0 发布的前两个月发布了 0.8 版本，这是对 API 的一次大规模重构。而在 2018 年 7 月底发布 1.0 时， Istio 达到了生产可用的临界点，此后 Google 对 Istio 团队进行了大规模重组，多家以 Istio 为基础的 Service Mesh 创业公司 诞生，可以说 2018 年是服务网格行业诞生的元年。\n2019年 3 月 Istio 1.1 发布，而这距离 1.0 发布已经过去了近 9 个月，这已经远远超出一个开源项目的平均发布周期。我们知道迭代和进化速度是基础软件的核心竞争力，此后 Istio 开始以每个季度一个版本的固定发布节奏 ，并在 2019 年成为了 GitHub 增长最快的十大项目中排名第 4 名 ！\nIstio 社区 Istio 开源四年来，已经在 GitHub 上收获了 2.7 万颗星，获得了大量的社区用户 。下图是 Istio 的 GitHub star 数增长情况。\n2020 年 Istio 的项目管理开始走向成熟，治理方式也到了进化的阶段。2020 年，Istio 社区进行了第一次管委会选举 ，还把商标转让给了 Open Usage Commons 。首届 IstioCon 在 2021 年 2 月份成功举办，几千人参加了线上会议。在中国也有大量的 Istio 社区用户，2021 年也会有线下面对面的 Istio 社区 meetup 在中国举办。\n根据 CNCF 2020 年调查，46% 的组织在生产中使用服务网格或计划在未来 12 个月内使用。Istio 是在生产中使用的最多的网格。\n未来 经过 4 年的发展，围绕 Istio 不仅形成了庞大的用户群，还诞生了多家 Istio 供应商，你可以在最近改版的 Istio 的官网首页 中看到。在最近几个版本中，Istio 已经将发展中心转移到了提升 Day 2 Operation 体验上来了。我们还希望看到更多的 Istio 的采纳路径建议、案例研究、学习资料、培训及认证（例如来自 Tetrate 的业界的第一个 Istio 管理员认证 ），这些都将有利于 Istio 的推广和采用。\n","relpermalink":"/blog/istio-4-year-birthday/","summary":"今天是 Istio 开源四周年，让我们一起来回顾一下 Istio 四年来的发展并展望一下它的未来。","title":"Istio 开源四周年回顾与展望"},{"content":"继 4 月 17 日杭州站 meetup 之后，云原生社区的第四次线下 meetup 来了，不仅有来自大厂的云原生工程师现场交流，更有《Knative 实战》、《Kubernetes 实战》、《Quarkus 实战》、《云原生数据中台》、《Kubernetes 进阶实战》书籍等你来拿，5 月 22 日云原生社区将在 广州举行，欢迎大家踊跃报名，报名方式：活动行 ！\n本次活动的图书由机械工业出版社赞助，如下。\n赞助图书 访问 https://cloudnative.to/city/guangzhou/ 了解关于云原生社区广州站的更多信息，扫码关注社区公众号，回复「广州站」即可加入广州站微信群。\n主办方：云原生社区 赞助商：APISIX 承办方：云原生社区广州站 合作伙伴：CNCF、Linux Foundation、机械工业出版社 13:30 - 14:00 签到入场 14:00 - 14:05 主持人开场 14:05 - 14:25 开场致辞 讲师：宋净\n个人介绍：Tetrate 布道师、CNCF Ambassador、云原生社区 创始人、电子工业出版社优秀译者、出品人。Kubernetes、Istio 等技术的早期使用及推广者。曾就职于科大讯飞、TalkingData 和蚂蚁集团。\n14:25 - 15:05 死生之地不可不察：论 API 标准化对 Dapr 的重要性 讲师：敖小剑\n个人介绍：资深码农，十九年软件开发经验，微服务专家，Service Mesh布道师，Servicemesher社区联合创始人，Dapr Maintainer。专注于基础架构，Cloud Native 拥护者，敏捷实践者，坚守开发一线打磨匠艺的架构师。曾在亚信、爱立信、唯品会、蚂蚁金服等任职，对基础架构和微服务有过深入研究和实践。目前就职阿里云，在云原生应用平台全职从事 Dapr 开发。\n演讲概要\nDapr作为新兴的云原生项目，以\u0026#34;应用运行时\u0026#34;之名致力于围绕云原生应用的各种分布式需求打造一个通用而可移植的抽象能力层。这个愿景有着令人兴奋而向往的美好前景：一个受到普通认可和遵循的云原生业界标准，基于此开发的云原生应用可以在不同的厂家的云上自由的部署和迁移，，恍惚间一派云原生下世界大同的美景。然而事情往往没这么简单，API的标准化之路异常的艰辛而痛苦，Dapr的分布式能力抽象在实践中会遇到各种挑战和困扰。\n听众收益：\n解Dapr的愿景和分布式能力抽象层的重要 了解Dapr API在抽象和实现时遇到的实际问题，尤其是取舍之间的艰难 了解目前Dapr在API抽象上正在进行的努力和新近准备增加的API 15:05 - 15:45 有了 Nginx 和 Kong，为什么还需要 Apache APISIX？ 讲师：温铭\n个人介绍：Apache member， Apache APISIX PMC 主席，Apache SkyWalking committer，支流科技 CEO\n演讲概要\n在云原生时代，k8s 和微服务已经成为主流，在带来巨大生产力提升的同时，也增加了系统的复杂度。如何发布、管理和可视化服务，成为了一个重要的问题。 每次修改配置都要reload的Nginx、依赖postgres才能工作的Kong，都不是云原生时代的理想之选。 这正是我们创造Apache APISIX的原因：没有reload、毫秒内全集群生效、不依赖数据库、极致性能、支持Java和Go开发插件。\n听众收益\n更好的理解API网关、服务网格，以及各个开源项目的优劣势\n15:45 - 16:05 合影、茶歇 16:05 - 16:45 云原生时代的研发效能 讲师：黄国峰\n个人介绍：腾讯 PCG 工程效能专家。10 多年的软件和互联网从业经验；现任腾讯工程效能部，负责持续集成、研发流程和构建系统等平台；曾任职唯品会高级经理，负责架构团队。在云原生平台下的研发效能方向有丰富的理论知识和实践经验。\n演讲概要\n云原生时代，软件研发的逻辑彻底改变了。传统的软件开发在本机编码/调试、部署到测试环境测试、再发布到生产环境；而云原生时代的开发，基于不可变设施，研发流程从编码、构建、持续测试、持续集成到持续部署，整个过程几乎完全代码化。\n听众收益\n了解云原生开发的新挑战和难点 了解腾讯云原生开发实践的流程和思路 了解腾讯云原生开发中的遇到的坑和解决思路 16:45 - 17:25 37 手游 Go 微服务架构演进和云原生实践 讲师：吴凌峰\n个人介绍：任职于三七互娱集团 37 手游技术部基础架构组，负责平台 golang 基础框架以及 DevOps、CI/CD 生态建设，从业以来一直专注于云原生、DevOps 和容器化等技术应用和推广，在 golang 工程化领域有一定的心得。\n演讲概要\nGolang 微服务应用和云原生的概念近年越来越火热，传统技术栈公司随着业务规模增长，在云原生技术应用落地探索和转型的过程中一定会遇到很多共通的问题以及有各自不同的思考，包括如何更好地提升我们的开发效率、提升服务稳定性、降低运维成本？面对不断增长的服务数量和不断变长变复杂的调用关系网，怎样才能更好地观测、管理和保证核心服务高可用，本次演讲分享将会围绕 37 手游转型为 Go 微服务架构以及建设云原生DevOps体系的历程、过程中的领悟和思考展开。\n听众收益\n了解Golang云原生微服务框架的关键技术和优化实践经验 了解云原生观测体系如链路追踪、监控等Golang微服务落地实践经验 了解混合云混合部署DevOps和CI/CD体系的企业实践经验 ","relpermalink":"/notice/cloud-native-meetup-guangzhou/","summary":"5 月 22 日，周六，广州见！","title":"云原生社区 Meetup 第四期广州站报名"},{"content":" Tetrate 的 Istio 认证管理员考试 企业正在增加对数字化转型的投资，并雇用合适的人才来加速这一旅程。根据 Linux 基金会发布的 2020 年开源工作报告 ，52% 的招聘经理更倾向于雇用有证书的人，而两年前只有 47%。不出所料 93% 的招聘经理表示难以找到足够的人才。Tetrate 今天宣布公开提供 Tetrate 认证 Istio 管理员（CIAT） 考试，该考试评估执行 Istio 服务网格安装和配置以及配置流量管理、弹性和故障注入的技能、知识和能力，以及使用 Istio 服务网格的安全功能。这是继 2 月份推出 Istio 基础知识的 免费培训和认证课程之后的又一举措。自那时起，已有 600 多名 IT 专业人士参加了培训。\n关于 Tetrate 的 Istio 认证管理员（CIAT） CIAT 是由 Tetrate 开发的，以帮助认证个人成为认证 Istio 管理员。该考试是一个在线、监考、基于性能的测试，由一组需要在命令行中解决的问题和一组多选题组成。考试目前基于在 Kubernetes 上运行的 Istio 1.9.1，考生有 2 小时时间完成任务。\nTetrate 的 Istio 认证管理员包括以下内容：\nIstio 基础课程 （培训和认证）：注册 CIAT 的用户会自动加入 Istio 基础课程，以帮助他们准备 Istio 管理员认证考试。然而，对于 CIAT 来说，这并不是必须要经历的。 CIAT 模拟考试 ：模拟考试旨在帮助用户在实际考试前熟悉考试环境。这是一个可选的步骤，不计入 CIAT 的最终得分。 CIAT 考试 ：这是用户必须通过的最后一次（或跳过上述步骤的唯一一次）考试，以获得 Tetrate 认证 Istio 管理员的资格。 谁适合参加？ 该认证适用于 Istio 服务网格管理员、运维和其他负责为 Kubernetes 内外运行的云原生工作负载配置流量路由、安全和其他服务网格功能的 IT 专业人士。该考试假定有基本的容器和 Kubernetes 知识，但不对其进行测试。候选人应该能够熟练使用：\nKubernetes CLI Istio CLI Linux 命令行 评价的范围 该认证项目测试用户在命令行环境中展示其能力的能力。考试测试的领域和能力包括：\nIstio 安装、升级和配置 流量管理 弹性和故障注入 确保工作负载的安全 Kubernetes 场景下的高级测试 经过认证的 Istio 管理员可以为运行在 Kubernetes 集群内外的工作负载安装和配置 Istio 资源。他们可以定义和配置 Istio 资源，控制流量路由、服务目的地、注入故障和使用弹性功能，以及确保工作负载的安全，并将外部和非 Kubernetes 工作负载引入现有的服务网格。\n现已上线 CIAT 考试 现在在 Tetrate 学院 上线，任何人都可以参加，费用为 299 美元。如果你想获得折扣，请与我联系 。\n","relpermalink":"/notice/certified-istio-administrator/","summary":"现已上线 Tetrate 学院，考试费 299 美元。","title":"Tetrate 推出业内首个 Istio 认证管理员考试"},{"content":"Istio 是当前最流行的服务网格实现 ，它是在 Kubernetes 的基础上开发的，它跟 Kubernetes 在云原生应用的生态中拥有着不同的定位。本文不是直接为你介绍 Istio 具有哪些功能，而是先向你介绍 Istio 诞生的历史条件，然后带你从 Kubernetes 与 Istio 的分工开始，了解什么是 Istio。\n要想解释什么是 Istio，还得先了解 Istio 是在什么样的情况下出现的——即为什么会有 Istio？\n容器作为云原生应用的交付物，既解决了环境一致性的问题，又可以更细粒度的限制应用资源，但是随着微服务和 DevOps 的流行，容器作为微服务的载体得以广泛应用。2014 年，Google 开源了 Kubernetes，随后几年得到迅猛发展，在 2017 年奠定了容器编排调度标准的地位。Kubernetes 作为一种容器编排调度工具，解决了分布式应用程序的部署和调度问题。因为一台单机的资源有限，而互联网应用可能因为用户规模的急速扩张，或用户属性的不同在不同时间段会出现流量洪峰，因此对计算资源的弹性要求比较高。而一台单机显然无法满足一个如何规模庞大的应用，反之，对于一个规模很小的应用也没必要占用整台主机，那将导致巨大的浪费。\n简而言之，Kubernetes 定义服务的最终状态，并使系统自动地达到和维持在该状态。那么在应用部署完成后，如何管理服务上的流量呢？下面我们将看下 Kubernetes 中如何做服务管理，及在 Istio 中的变化。\nKubernetes 中如何做服务管理？ 下图展示的是 Kubernetes 中的服务模型。\nKubernetes 服务模型 从上图中我们可以看出：\n同一个服务的的不同示例可能被调度到不同的节点上； Kubernetes 通过 Service 对象将一个服务的多个实例组合在了一起，统一对外服务； Kubernetes 在每个 node 中安装了 kube-proxy 组件来转发流量，它拥有的简单的负载均衡功能； Kubernetes 集群外部流量可以通过 Ingress 进入集群中（Kubernetes 还有其他几种暴露服务的方式，如 NodePort、LoadBalancer 等）； Kubernetes 是用于资源集约管理的工具。但在为应用分配好资源后，如何保证应用的健壮性、冗余性，如何实现更细粒度的流量划分（不是根据服务中实例个数来实现），如何保障服务的安全性，如何进行多集群管理等，这些问题 Kubernetes 都不能很好地解决。\n服务具有多个版本，需要迭代和上线，在新版发布的时候需要切分流量，实现金丝雀发布；同时我们应该假定服务是不可靠的，可能因为各种原因导致请求失败，需要面向失败来编程，如何监控应用程序的指标，了解每个请求的耗时和状态？Istio 的发起这们就想到了在每个 pod 中注入一个代理，将代理的配置通过一个控制平面集中分发，然后将从 pod 中应用容器发起的每个请求都劫持到 sidecar 代理中，然后转发，这样不就可以完美的解决以上问题了吗？Kubernetes 优秀的架构和可扩展性，例如 CRD，pod 内的部署模式，可以完美的解决大量 sidecar 的注入和管理问题，使得 Istio 的实现成为可能。\nIstio 的基本原理 下图是 Istio 中的服务模型，它既可以支持 Kubernetes 中的工作负载，又可以支持虚拟机。\nIstio 从图中我们可以看出：\nIstiod 作为控制平面，将配置下发给所有的 sidecar proxy 和 gateway（为了美观，图中没有画 Istiod 及 sidecar 之间的连接） Istio 不再使用 kube-proxy 组件做流量转发，而是依托在每个 pod 中注入的 sidecar proxy，所有的 proxy 组成了 Istio 的数据平面； 应用程序管理员可以和管理 Kubernetes 中的工作负载一样，通过声明式 API 操作 Istio mesh 中流量的行为； Ingress 被 Gateway 资源所替代，Gateway 是一种特殊的 proxy，实际上也是复用的 Sidecar proxy； 可以在虚拟机中安装 sidecar proxy，将虚拟机引入的 Istio mesh 中； 实际上在 Istio 之前，人们可以使用 SpringCloud、Netflix OSS 等，通过在应用程序中集成 SDK，编程的方式来管理应用程序中的流量。但是这通常会有编程语言限制，而且在 SDK 升级的时候，需要修改代码并重新上线应用，会增大人力负担。Istio 使得流量管理变得对应用程序透明，使这部分功能从应用程序中转移到了平台层，成为了云原生基础设施。\n正是因为 Istio 补足了 Kubernetes 对于云原生应用的流量管理、可观察性和安全方面的短板，在 2017 年由 Google、IBM 和 Lyft 共同发起的这个服务网格开源项目，并在三年来取得了长足的发展。关于 Istio 核心功能的介绍可以参考 Istio 文档 。\n总结 Service Mesh 相当于云原生时代的 TCP/IP，解决应用程序网络通信、安全及可见性问题； Istio 是目前最流行的 service mesh 实现，依托于 Kubernetes，但也可以扩展到虚拟机负载； Istio 的核心由控制平面和数据平面组成，Envoy 是默认的数据平面代理； Istio 作为云原生基础设施的网络层，对应用透明。 ","relpermalink":"/blog/what-is-istio-and-why-does-kubernetes-need-it/","summary":"本文将解释Istio是如何产生的，以及它与Kubernetes的关系。","title":"什么是 Istio？为什么 Kubernetes 需要 Istio？"},{"content":"很高兴的通知大家，《Kubernetes Handbook——Kubernetes 中文指南/云原生架构实践手册》四周年纪念版发布了，你可以在线浏览 或者下载 PDF 。\nKubernetes Handbook——Kubernetes 中文指南/云原生架构实践手册 by Jimmy Song 封面 感想 最近有两条新闻在朋友圈里刷屏：\n《Mesos 已死，容器永生》 《Kubernetes 1.21: Power to the Community》 当着两篇看似不相关的新闻同时映入我的眼帘，让我感觉恍如隔世。我曾在 2017 年的分享过一篇文章——《Kubernetes 与云原生 2017 年年终总结及 2018 年展望》 ，其中就已经断定 Kubernetes 的大局已定，最终连 Docker 都有可能黯然收场。蓦然回首，四年时间如白驹过隙，Kubernetes 已经在 PaaS 层占据了不可撼动的地位，社区里甚至高呼其为“云原生操作系统”。2019 年后实质上就已经进入了 Kubernetes 次世代 ，一个迈向全面云原生的新世代开启。\n还记得四年前，2017 年 3 月，我开源了 《Kubernetes Handbook——Kubernetes 中文指南/云原生架构实践手册》 ，从开始的 Kubernetes 安装笔记到理论文档，再到工程实践，不断的丰富完善，其间有上百人 参与了本书的工作中，丰富和完善了本分内容，并形成了一个圈子，在其中不断交流碰撞着思维的火花，如今已形成了一个近万人规模的云原生社区 。\n为什么发布四周年纪念版 本书的上个 PDF 版本发布距今已经有两年多时间了，市面上已经流传了关于本书的无数种 PDF 版本，为了防止个别别有用心的人自篡改和传播未经核实的版本，使学习 Kubernetes 和云原生的同学误解，特此编译该四周年纪念版以正视听。\n本书从诞生之初就一直使用 GitBook 来管理，发布在 GitHub Pages 上，社区里时不时会有人问我为什么编译成 PDF 时总出错？本书使用的 GitBook 版本过旧（CLI version: 2.3.2，GitBook version: 3.2.2），官方已经不再维护，且编译时对电脑的要求以及环境配置要求较高，我还特别制作了一个 Docker 镜像用来编译，但是编译依然是一个十分耗时的事情，在我的个人电脑上耗时近 5 分钟。但是因为旧版 GitBook 比较稳定，且可以在本地管理文章和编译发布到 GitHub，而且我已经配置了 GitHub Action，每次有 PR 合并会自动发布到网站上，因此将会继续沿用下去。\n后续 该版本中删除了一些过时章节和与书籍内容关系不大的附录，并重新组织了章节，包括新增服务网格、社区与生态章节，还修复了书中的一些错误。本书会继续通过开源协作的方式来撰写，未来还会在最佳实践、领域应用等章节，跟社区取长补短，汇聚众家之长，也希望广大读者可以在云原生社区中积极投稿 ，将云原生架构的最佳实践共同分享给读者。\n","relpermalink":"/notice/kubernetes-handbook-4-anniversary-edition/","summary":"该版本中删除了一些过时章节和与书籍内容关系不大的附录，并重新组织了章节，包括新增服务网格、社区与生态章节，还修复了书中的一些错误。","title":"Kubernetes Handbook 四周年纪念版发布"},{"content":"在刚刚过去的清明节，我去了趟苏州（视频 ）和南京（视频 ），还在南京跟南京站的朋友们吃了饭，聊了下南京站的发展。\n南京站 下面是我在南京跟云原生社区南京站 成员的合影。\n云原生社区南京站 这周六（4 月 10 日）云原生社区南京站会有一次线下聚会，欢迎大家报名参加，点击报名南京站活动 ，本次会议主要内容为交流云原生落地经验。\n长沙站 在周日（4 月 11 日）云原生社区长沙站会有第一次线下聚会暨长沙站筹办会议，也欢迎大家报名参加，点击报名长沙站聚会 。\n关于云原生社区城市站 为了方便云原生社区成员的线下交流以及后期本地站活动开展，云原生社区城市站计划在全球各地创建城市站长，站长负责本地活动组织，详见云原生社区城市站页面 。\n","relpermalink":"/notice/cloud-naitve-community-nanjing-changsha/","summary":"周末南京站、长沙站聚会通知。","title":"云原生社区南京站及长沙站活动提醒"},{"content":" Tetrate 我们很高兴地宣布 Tetrate Service Bridge 1.0（General Available） 发布。Tetrate 的使命是解决应用网络的复杂性，使应用开发者和运维的工作变得简单。今天是我们发展道路上的一个重要里程碑。\n初心 Tetrate 成立的初心是应用网络应与计算无关。我们相信，这是任何组织在构建和交付应用时实现敏捷性和速度的关键。这与当今的应用越来越相关，这些应用在多个云和内部基础设施的异构计算环境（如 Kubernetes 和虚拟机）上运行。如果不加以管理，这种蔓延会导致难以承受的运营成本和复杂性。企业需要一种一致的方式来配置、保护和观察他们的应用，并保持系统的弹性以满足 SLO。\nTetrate Service Bridge 使用一流的开源项目：Istio 、Envoy Proxy 和 Apache SkyWalking （Tetrate 积极参与并维护这些项目），我们开始建立原型，以解决应用网络的挑战。我们与《财富》500 强企业密切合作，并得到他们出色的团队支持，以验证我们的想法和产品。他们一直并将继续帮助我们构建产品，他们所提供的在快速变化的环境中的真实用例为我们的产品构建发挥了重要作用。 我们建立了一个平台，以帮助组织在多云和多集群环境的异构计算中管理其应用网络。这个平台能够满足企业的实际需求，使他们能够按照自己的节奏安全地、渐进地进行现代化和 / 或迁移。\n我们将该平台命名为 Tetrate Service Bridge（TSB）。桥梁提供了一种连接方式，而这正是我们所实现的。TSB 是一座桥梁：\n以实现企业应用的现代化。 从任何地方连接任何服务。 将不同的团队聚集在一起，管理和运维应用服务。 从边缘到工作负载的网络 Tetrate Service Bridge 的核心是提供一个单一的管理平面，以便在构成应用网络的三个层面上配置连接性、安全性和可靠性。\n应用程序边缘网关（Application Edge Gateway），作为传入流量的入口点，然后将流量分配到多个集群。 应用程序入口网关（Application Ingress Gateway），作为进入单个集群的流量的入口点，并将流量分配给在该集群中运行的一个或多个工作负载。这还包括 API 网关功能，如认证、授权、速率限制等。 服务网格 Sidecar，作为代理，允许工作负载之间的连接，控制服务访问，并收集指标和追踪，以提供全面的可观察性。 Tetrate Service Brdige 示意图 Tetrate Service Bridge 位于应用边缘、集群入口和集群内，可在 Kubernetes 集群和传统计算之间路由和负载平衡南北流量，并连接集群内的东西向工作负载。\n一个适合多个团队的平台 在任何组织中，多个团队一起构建和交付应用程序，具体来说是：应用程序、平台和安全团队。这些团队中的每一个团队都会有不同的关注点和期望。\n平台团队希望在其基础设施中提供多集群服务网格，并对其进行维护。 应用开发团队希望配置、观察和排除他们的应用程序和 API 的故障。 安全团队希望对用户和服务访问持续应用安全策略和工作流程。 我们建立了 Tetrate Service Bridge，以满足这些团队的要求，加速组织的成功。\n提供和维护多集群服务网格 多集群、多云、混合云 通过 Tetrate Service Bridge，您可以跨集群、云和数据中心连接和管理应用程序。Tetrate Service Bridge 支持来自主要云供应商的任何 Kubernetes 上游合规发行版。这包括但不限于 Google Kubernetes Engine（GKE）、Amazon Elastic Kubernetes Service（EKS）、Azure Kubernetes Service（AKS）、Openshift 和 Mirantis Kubernetes Engine（MKE）。Tetrate Service Bridge 还支持将虚拟机（VM）和裸机工作负载接入网格。\n多集群管理 集群生命周期管理 Tetrate Service Bridge 提供了所有集群中运行的 Istio 和 Envoy 的可视性，包括部署的版本、每个集群的配置状态等信息。此外，您还可以使用 Tetrate Service Bridge 管理网格部署的完整清单，并放心地逐步升级，而不必担心停机。\n配置和观察应用程序和 API 始终如一的安全体验 Tetrate Service Bridge 的管理平面为平台所有者和应用开发者提供了一致的体验，以控制任何环境中任何应用的连接性、安全性和可观察性。\nTetrate Service Bridge 还提供了更安全的配置模型，允许应用团队编写和验证 Istio 配置，通过构造确保正确性。服务级隔离和组织控制保证了只有正确的网格配置才能在运行时到达您的应用。\n单一视图的可观察性 Tetrate Service Bridge 可收集、存储和汇总来自多个集群和环境的指标、追踪和代理日志，并提供单一界面查看服务的拓扑结构及其依赖关系，以便一目了然地了解应用健康状况。\n监控视图 扩展网格功能 Tetrate Service Bridge 为认证和授权、可观察性或通过 WASM 扩展对请求属性进行操作提供了一个可扩展点。这允许应用团队添加可根据组织特定需求定制的功能。\n确保网格的安全 多租户 利用 Tetrate Service Bridge，您可以为您的团队和企业内部的工作空间创建租户，以定义细粒度的访问控制、编辑权限，并应用零信任标准。Tetrate Service Bridge 符合 NIST 微服务安全标准（SP800-204a 和 - 204b，由 Tetrate 工程师与 NIST 共同编写）和下一代访问控制（NGAC）的实现。\n安全策略 Tetrate Service Bridge 允许您在网格中一致地应用安全策略，而不需应用开发人员操心。Tetrate Service Bridge 符合 NIST 微服务安全标准，并实现零信任。\n下一步 要了解有关 Tetrate Service Bridge 的更多信息，请访问我们的页面 ，并注册参加即将举行的网络研讨会，即 TSB 介绍：连接和保护您的应用程序，无论它们在哪里运行 。 联系我们 ，获取 TSB 演示，了解 TSB 如何帮助您管理应用网络的复杂性。通往现代化的道路并不简单，但我们已准备好铺设桥梁，并为您提供每一步支持。\n","relpermalink":"/notice/tetrate-service-bridge-ga/","summary":"Tetrate Service Bridge（TSB）正式可用！","title":"Tetrate Service Bridge 1.0 发布"},{"content":"如果你听说过服务网格，并尝试过 Istio ，你可能有以下问题。\n为什么 Istio 要在 Kubernetes 上运行？ Kubernetes 和服务网格在云原生应用架构中分别扮演什么角色？ Istio 扩展了 Kubernetes 的哪些方面？它解决了哪些问题？ Kubernetes、Envoy 和 Istio 之间是什么关系？ 本文将带大家了解 Kubernetes 和 Istio 的内部工作原理。此外，我会介绍 Kubernetes 中的负载均衡方法，并解释为什么有了 Kubernetes 后还需要 Istio。\nKubernetes 本质上是通过声明式配置来实现应用生命周期管理，而服务网格本质上是提供应用间的流量、安全管理和可观察性。如果你已经使用 Kubernetes 搭建了一个稳定的应用平台，那么如何设置服务间调用的负载均衡和流量控制？是否有这样一个通用的工具或者说平台（非 SDK），可以实现？这就需要用到服务网格了。\nEnvoy 引入了 xDS 协议，这个协议得到了各种开源软件的支持，比如 Istio、MOSN 等。Envoy 将 xDS 贡献给服务网格或云原生基础设施。Envoy 本质上是一个现代版的代理，可以通过 API 进行配置，在此基础上衍生出许多不同的使用场景–比如 API Gateway、服务网格中的 sidecar 代理和边缘代理。\n本文包含以下内容。\nkube-proxy 的作用描述。 Kubernetes 在微服务管理方面的局限性。 Istio 服务网格的功能介绍。 Kubernetes、Envoy 和 Istio 服务网格中一些概念的比较。 Kubernetes vs Service Mesh 下图显示了 Kubernetes 中的服务访问关系和服务网格（每个 pod 模型一个 sidecar）。\nKubernetes vs Service Mesh 流量转发 Kubernetes 集群中的每个节点都部署了一个 kube-proxy 组件，该组件与 Kubernetes API Server 进行通信，获取集群中的服务信息，然后设置 iptables 规则，将服务请求直接发送到对应的 Endpoint（属于同一组服务的 pod）。\n服务发现 Service Discovery Istio 可以跟踪 Kubernetes 中的服务注册，也可以在控制平面中通过平台适配器与其他服务发现系统对接；然后生成数据平面的配置（使用 CRD，这些配置存储在 etcd 中），数据平面的透明代理。数据平面的透明代理以 sidecar 容器的形式部署在每个应用服务的 pod 中，这些代理都需要请求控制平面同步代理配置。代理之所以 “透明”，是因为应用容器完全不知道代理的存在。过程中的 kube-proxy 组件也需要拦截流量，只不过 kube-proxy 拦截的是进出 Kubernetes 节点的流量–而 sidecar 代理拦截的是进出 pod 的流量。\n服务网格的劣势 由于 Kubernetes 的每个节点上都运行着很多 pod，所以在每个 pod 中放入原有的 kube-proxy 路由转发功能，会增加响应延迟–由于 sidecar 拦截流量时跳数更多，消耗更多的资源。为了对流量进行精细化管理，将增加一系列新的抽象功能。这将进一步增加用户的学习成本，但随着技术的普及，这种情况会慢慢得到缓解。\n服务网格的优势 kube-proxy 的设置是全局的，无法对每个服务进行细粒度的控制，而 service mesh 通过 sidecar proxy 的方式将 Kubernetes 中的流量控制从服务层中抽离出来–可以实现更大的弹性。\nKube-proxy 的不足之处 首先，如果转发的 pod 不能正常服务，它不会自动尝试其他 pod。每个 pod 都有一个健康检查机制，当一个 pod 出现健康问题时，kubelet 会重启 pod，kube-proxy 会删除相应的转发规则。另外，节点 Port 类型的服务不能添加 TLS 或更复杂的消息路由机制。\nKube-proxy 实现了一个 Kubernetes 服务的多个 pod 实例之间的流量负载均衡，但如何对这些服务之间的流量进行精细化控制–比如将流量按百分比划分给不同的应用版本（这些应用版本都是同一个服务的一部分，但在不同的部署上），或者做金丝雀发布（灰度发布）和蓝绿发布？\nKubernetes 社区给出了一个使用 Deployment 做金丝雀发布 的方法，本质上是通过修改 pod 的标签来给部署的服务分配不同的 pod。\nKubernetes Ingress vs Istio Gateway 如上所述，kube-proxy 只能在 Kubernetes 集群内路由流量。Kubernetes 集群的 pod 位于 CNI 创建的网络中。Ingress 是在 Kubernetes 中创建的资源对象，用于集群外部的通信。它由位于 Kubernetes 边缘节点上的入口控制器驱动，负责管理南北向流量。Ingress 必须与各种 Ingress 控制器对接，比如 nginx ingress 控制器 和 traefik 。Ingress 只适用于 HTTP 流量，使用简单。它只能通过匹配有限的字段来路由流量——如服务、端口、HTTP 路径等。这使得它无法对 TCP 流量进行路由，如 MySQL、Redis 和各种 RPC。这就是为什么你会看到人们在 ingress 资源注释中写 Nginx 配置语言的原因（注：使用 Nginx Ingress Controller 可以通过 配置 ConfigMap 和 Service 的方式 来变通支持 TCP 和 UDP 流量转发）。直接路由南北流量的唯一通行方法是使用服务的 LoadBalancer 或 NodePort，前者需要云厂商支持，后者需要额外的端口管理。\nIstio Gateway 的功能与 Kubernetes Ingress 类似，它负责进出集群的南北流量。Istio Gateway 描述了一个负载均衡器，用于承载进出服务网格边缘的连接。该规范描述了一组开放端口和这些端口所使用的协议，以及用于负载均衡的 SNI 配置等。Gateway 是一个 CRD 扩展，它也重用了 sidecar 代理的功能；详细配置请参见 Istio 网站 。\nEnvoy Envoy 是 Istio 中默认的 sidecar 代理。Istio 基于 Enovy 的 xDS 协议扩展了其控制平面。在讨论 Envoy 的 xDS 协议之前，我们需要先熟悉 Envoy 的基本术语。下面是 Envoy 的架构图。\nEnvoy 架构图 基础概念 以下是 Enovy 中你应该知道的基本术语。\n下游。下游主机连接到 Envoy，发送请求，并接收响应，即发送请求的主机。 上游：上游主机。上游主机接收来自 Envoy 的连接和请求，并返回响应；即接收请求的主机。 Listener：监听器。监听器是一个命名的网络地址（如端口、UNIX 域套接字等）；下游客户端可以连接到这些监听器。Envoy 将一个或多个监听器暴露给下游主机进行连接。 集群。集群是一组逻辑上相同的上游主机，Envoy 连接到它们。Envoy 通过服务发现来发现集群的成员。可以选择通过主动的健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略来决定集群中哪个成员的请求路由。 在 Envoy 中可以设置多个监听器，每个监听器可以设置一个过滤链（过滤链表），而且过滤链是可扩展的，这样我们可以更方便地操纵流量的行为–比如设置加密、私有 RPC 等。\nxDS 协议是由 Envoy 提出的，是 Istio 中默认的 sidecar 代理，但只要实现了 xDS 协议，理论上也可以作为 Istio 中的 sidecar 代理 —— 比如蚂蚁集团开源的 MOSN 。\nimg Istio 是一个功能非常丰富的服务网格，包括以下功能。\n流量管理。这是 Istio 最基本的功能。 策略控制。实现访问控制系统、遥测采集、配额管理、计费等功能。 可观察性。在 sidecar 代理中实现。 安全认证。由 Citadel 组件进行密钥和证书管理。 Istio 中的流量管理 Istio 中定义了以下 CRD 来帮助用户进行流量管理。\n网关。网关描述了一个运行在网络边缘的负载均衡器，用于接收传入或传出的 HTTP/TCP 连接。 虚拟服务（VirtualService）。VirtualService 实际上是将 Kubernetes 服务连接到 Istio 网关。它还可以执行额外的操作，例如定义一组流量路由规则，以便在主机寻址时应用。 DestinationRule。DestinationRule 定义的策略决定了流量被路由后的访问策略。简单来说，它定义了流量的路由方式。其中，这些策略可以定义为负载均衡配置、连接池大小和外部检测（用于识别和驱逐负载均衡池中不健康的主机）配置。 EnvoyFilter。EnvoyFilter 对象描述了代理服务的过滤器，可以自定义 Istio Pilot 生成的代理配置。这种配置一般很少被主用户使用。 ServiceEntry。默认情况下，Istio 服务 Mesh 中的服务无法发现 Mesh 之外的服务。ServiceEntry 可以在 Istio 内部的服务注册表中添加额外的条目，从而允许 Mesh 中自动发现的服务访问并路由到这些手动添加的服务。 Kubernetes vs xDS vs Istio 在回顾了 Kubernetes 的 kube-proxy 组件、xDS 和 Istio 对流量管理的抽象后，现在我们仅从流量管理的角度来看看这三个组件 / 协议的比较（注意，三者并不完全等同）。\nKubernetes xDS Istio service mesh Endpoint Endpoint WorkloadEntry Service Route VirtualService kube-proxy Route DestinationRule kube-proxy Listener EnvoyFilter Ingress Listener Gateway Service Cluster ServiceEntry 核心观点 Kubernetes 的本质是应用生命周期管理，具体来说就是部署和管理（伸缩、自动恢复、发布）。 Kubernetes 为微服务提供了一个可扩展、高弹性的部署和管理平台。 服务网格是基于透明代理，通过 sidecar 代理拦截服务之间的流量，然后通过控制平面配置管理它们的行为。 服务网格将流量管理与 Kubernetes 解耦，不需要 kube-proxy 组件来支持服务网格内的流量；通过提供更接近微服务应用层的抽象来管理服务间的流量、安全性和可观察性。 xDS 是服务网格的协议标准之一。 服务网格是 Kubernetes 中服务的一个更高层次的抽象。 总结 如果说 Kubernetes 管理的对象是一个 pod，那么服务网格管理的对象就是一个服务，所以用 Kubernetes 管理微服务，然后应用服务网格就可以了。如果你连服务都不想管理，那就用 Knative 这样的无服务器平台，不过这是后话。\n","relpermalink":"/blog/why-do-you-need-istio-when-you-already-have-kubernetes/","summary":"本文将带大家了解 Kubernetes 和 Istio 的内部工作原理。此外，我会介绍 Kubernetes 中的负载均衡方法，并解释为什么有了 Kubernetes 后可能还需要 Istio。","title":"为什么在使用了 Kubernetes 后你可能还需要 Istio？"},{"content":"继去年 12 月 20 日北京站 meetup 之后，云原生社区的第三次线下 meetup 来了，不仅有来自大厂的云原生工程师现场交流，更有《混沌工程：NETFLIX 系统稳定性之道》,《云原生服务网格 istio》书籍等你来拿，4 月 17 日杭州站将在西湖区蒋村街道中节能・西溪首座 A1-213 如期举办，欢迎大家踊跃报名！\n报名方式：活动行 云原生社区 meetup 第三期杭州站 本次活动海报模板设计由 UCloud 赞助。\n开场致辞 讲师：宋净超（Tetrate 布道师、云原生社区创始人）\n讲师介绍：Tetrate 云原生布道师，云原生社区创始人，CNCF Ambassador，作家和摄影师，热衷于开源和分享。\n使用 Chaos Mesh 来保障云原生系统的健壮性 讲师：周强\n公司：PingCAP\n讲师介绍：周强，PingCAP 工程效率负责人，Chaos Mesh 负责人，专注稳定性和性能测试平台。在混沌工程领域有 4 年的从业经验，领导开发云原生混沌测试平台 Chaos Mesh。\n演讲概要:\n在云原生的世界中，错误无处不在，混沌工程在提高系统稳定性方面起着至关重要的作用。通过执行混沌工程实验，我们可以了解系统的弱点并主动解决。我们开发了云原生混沌工程平台 Chaos Mesh，并在内部使用 Chaos Mesh 来提升云原生分布式数据库 TiDB 的健壮性。目前 Chaos Mesh 已加入 CNCF Sandbox 项目，该平台依托于 k8s 基础设施，通过对 pod/container 进行诸如杀节点、IO 错误和延时注入、时间回退、内核分配内存失败等等来进行混沌测试。主题大纲:\n在分布式领域会遇到的质量和稳定性问题 混沌工程在提升系统稳定性方面的作用和意义 Chaos Mesh 项目简介 混沌工程主要的测试方式和使用案例 混沌工程平台的构建实践 听众收益：\n了解在构建分布式系统可能出现的问题和风险 了解混沌工程的使用经验和踩过的坑，观众后续可以通过混沌工程来进行相关实践，提升产品质量 通过 case study 可以帮助大家构建分布式混沌测试平台 Envoy 在轻舟微服务中落地实践 讲师：王佰平\n公司：网易\n讲师介绍：网易数帆资深工程师，负责轻舟 Envoy 网关与轻舟 Service Mesh 数据面开发、功能增强、性能优化等工作。对于 Envoy 数据面开发、增强、落地具有较为丰富的经验。\n演讲概要：\nEnvoy 是由 Lyft 开源的高性能数据和服务代理，以其可观察性和高扩展性著称。如何充分利用 Envoy 的特性，为业务构建灵活易扩展、稳定高性能的基础设施（服务网格、API 网关）是 Envoy 落地生产实践必须考虑的问题。本次分享主要介绍 Envoy 在轻舟微服务网关与轻舟微服务网格中落地实践经验和构建此类基础设施时轻舟关注的核心问题。希望能够给大家带来一些帮助。\n听众收益：\n了解 Envoy 本身架构与关键特性； 在生产实践当中微服务网关与服务网格关注的核心问题以及轻舟 Envoy 的解决之道； 为期望实现集群流量全方位治理和观察的听众提供些许借鉴。 KubeVela：阿里巴巴新一代应用交付管理系统实践 讲师：孙健波\n公司：阿里巴巴\n讲师介绍：孙健波 (花名：天元) 阿里云技术专家，云原生应用模型 OAM (Open Application Model) 核心成员和主要制定者，KubeVela 项目作者，致力于推动云原生应用标准化，负责大规模云原生应用交付与应用管理相关工作。曾参与编写《Docker 容器与容器云》技术书籍。\n演讲概要：\n云原生应用交付面临的问题与挑战 社区中常见的应用交付解决方案对比 基于 KubeVela 的标准化应用交付管理核心原理 阿里巴巴基于 KubeVela 的应用交付实践 听众收益：\n随着 “云原生” 的普及，基础设施逐渐成熟的今天，越来越多的应用开发者们开始追求快速的构建交付应用。然而使用场景的不同往往意味着应用交付的环境会有巨大的差异。就比如，K8s 中不同的工作负载类型需要对接不同的灰度发布、流量管理等实现方案，不同的部署环境（公有云、私有化部署等）也常常需要对接不同的日志监控体系。如何才能将应用管理和交付变得标准化，使得应用研发不再需要花大量精力对接不同的交付平台？这已经逐渐成为云原生应用管理领域的一大痛点。本次分享将针对这些问题为大家介绍如何基于 KubeVela 构建标准化的应用交付管理平台，介绍阿里巴巴在此基础上的实践经验。\nEnvoy 在阿里巴巴内部的落地实践 讲师：张义飞\n公司：阿里巴巴\n讲师介绍：阿里巴巴云原生部门高级工程师，主要负责阿里巴巴内部 ServiceMesh 数据面的落地。Envoy/Istio 社区 Member，给 envoy 社区贡献了 Dubbo Proxy filter，metadata 优化等。\n演讲概要：\n介绍 Envoy 在大规模场景下存在的问题以及如何优化 介绍 Envoy 实现自定义协议的最佳实践 扩展自定义协议 扩展连接池 扩展 Cluster 介绍 Envoy 在阿里巴巴内部落地遇到的一些困难 听众收益：\nEnvoy 在大规模场景下存在的一些问题 机器数量过多导致的内存问题 机器全量下发导致的 CPU 问题 关于云原生社区 云原生社区是一个中立的云原生终端用户社区，致力于推广云原生技术，构建开发者生态。\n社区官网：https://cloudnative.to ","relpermalink":"/notice/cloud-native-meetup-3-hangzhou/","summary":"本次活动特别邀请了云原生社区稳定性 SIG、Envoy SIG、OAM SIG、Istio SIG 的核心贡献者为大家带来精彩演讲！","title":"云原生社区 meetup 第三期杭州站开始报名"},{"content":"致关注 Istio 项目的学习者和网友们：\n近日我们接到社区网友的举报，某教育机构（magedu）在提供 Istio 中文文档的下载，并基于该文档进行商业课程的宣传。以下是相关链接：\n马哥教育网站下载链接 公众号宣传文章 知乎宣传文章（已删除） 下面是相关的网页截图。\n网页截图 首先，ServiceMesher 是 Istio 官方的合作伙伴，是官方指定的唯一的中文文档的翻译组织，Istio 官方有明确的说明，请查看 Istio 官方链接 。\n官方声明 第二，大家在公网上看到的 Istio 官方中文文档是由社区发起，并组织社区成员共同参与完成的，倾注了上百人共同的心血，我们从 Istio1.2 版本就开始翻译，先后组织了 1.2、1.5 和目前正在进行的 1.9 版本的翻译工作，具体可参考 GitHub 。\n另外，Istio 的中文文档是可以直接访问的（具体链接为 https://istio.io/latest/zh/docs/ ），根本不需要下载，这会误导初学者的学习方式，令学习者无法获取到最新的内容。\n我们强烈谴责这种将他人劳动成果据为己有、并用作商业用途（课程宣传）的行为。相信每个参与翻译的成员，以及整个社区都不会答应这种剽窃行为的存在！开源项目、社区都是以非盈利的方式无偿提供产品和交流学习的机会，让学习者共同成长。这种剽窃行为严重危害了所有开发者的利益，彻底违背了开源、开放的技术生态理念。这种行为必须要抵制，必须还技术分享领域一片干净的天空！\n我们要求该机构立即删除其公众号、知乎、百度网盘、教育机构网站上关于所谓「Istio 官方中文文档」的分享，并在「马哥 Linux 运维」公众号上发布声明致歉！\n最后 Istio 官方文档的翻译仍在进行中，欢迎大家参与进来，点击查看详情 。\n—— ServiceMesher，2021 年 3 月 31 日\n更新 在 3 月 31 日，以上公告发布后，马哥教育第一时间联系了我，并承认错误，积极改正，并发布了公告（关于 Istio官方文档使用声明 ）给马哥教育诚恳的态度点个赞，本次事件并非针对马哥教育，而是希望能够唤醒大家的版权保护意识，以正某些“利用”开源的不正之风。\n","relpermalink":"/notice/istio-doc-announcement/","summary":"将社区上百人的成果据为己有，将一个既非最新版本也没有任何版权声明的 PDF 标称官方中文文档，真的是太不厚道了。","title":"关于 Istio 官方中文文档被恶意使用的声明"},{"content":"近日 Tetrate Academy 发布了 Istio Fundamentals Course（Istio 基础教程），现在可免费学习。赶快到 Tetrate Academy 报名学习吧！\n课程目录 课程安排如下：\nService Mesh and Istio Overview Installing Istio Observability: Telemetry and Logs Traffic Management Security Advanced Features Troubleshooting Real World Examples 每个课程最后都会有自测题目，我已经通过了课程，下面是通过课程后的证书。\nTetrate 学院 Istio 基础教程证书 更多 未来 Tetrate 将发布 Certified Istio Administrator（CIA）考试，欢迎各位 Istio 用户和管理员关注和报名参加。\n","relpermalink":"/notice/tetrate-istio-fundamental-courses/","summary":"近日 Tetrate Academy 发布了 Istio Fundamentals Course（Istio 基础教程），现在可免费学习。","title":"Tetrate 学院发布免费 Istio 基础课程"},{"content":"在 IstioCon 2021 上，Istio 社区确定了 2021 年的社区的工作重点是 Day-2 Operation，很多人问我这个词是什么意思。我查了下中文互联网上，没有对这个词的解释，我在网上找到了一些解释，我发现大部分文章的源头都指向了这篇 Defining Day-2 Operations 。因此，在此我将问翻译一下，同时再加上一些我自己的见解。\n下面是笔者对 Day-2 Operation 的理解。\n假如将开发一个系统比作种下一棵树，那么 Day-2 Operation 就是系统开花后结果的过程。我们要不断改进这颗树的基因，以实现效益最大化。Day-2 Operation 就是对这个系统优化改进的过程。\nDay-2 Operation 的定义 Day-2 Operation 不一定是指第 2 天的行动。一旦 “某物 “进入行动，“Day-2 Operation “是指在这个 “某物 “没有被杀死或被 “其他东西 “取代之前的剩余时间段。如下图中展示的软件的生命周期中，从软件被安装之后到被卸载之前的那段时间。\nDay-2 Operation 当我们审视一个业务流程、应用程序或 IT 基础设施生命中的各个阶段时，有些人喜欢把它们描绘成一个循环过程。我相信这是因为人们倾向于使用 “应用程序的生命周期 “这个词，并以某种方式陷于相信图中必须循环回到起点。各个阶段通常是在时间上向前推进的，而不是把你带回起点。\n假定 “X “称为一个组织或实体所需要的东西，可能是一个业务流程，一个应用程序，或者是一些 IT 基础设施。从技术上讲，每当有人设想 X 的时候，总会有一个起点 —— 我们称它为 “零日”（这是高中物理的管理，时间的起点通常是 T0）。 Day-Zero 可能不是一天：它是提出并记录一套完整的 X 需求所需的时间段，这些活动可能包括高层设计、记录并向某人推销利益、撰写商业案例、寻求资金等。\n这个过程的下一步是构建和部署。Day-1 包括所有活动，从详细（或底层）设计开始，到构建、测试、提出任何所需的流程和人员，以支持 X，使组织受益。在许多情况下，这里可能还涉及一些采购活动。一旦它被安装、设置、配置和批准（“好的开始”），X 就被认为是 “上线 “或 “开放业务”。\n从这一点开始，直到 X 退役、死亡或被替换，我们有 Day-2 操作。这包括保持 X 运行的一系列活动，照看和支持 X，使其以最佳状态运行，确保 X 的运行和交付结果符合最初的意图和期望。监控利用率、确保可用性和成本优化是在通常的内务管理活动基础上增加的，以保持 X 以 “最佳 “的方式运行。\n随着我们周围世界的要求发生变化，组织要决定对 X 的调整或升级，这些都是必然需要的，是被称为整个大修还是仅仅是升级。如果是整体大修，我们可以假设 X 已经退役并被新的系统 Y 所取代。如果新的 X 只是比以前的 X 有了更大的改进，那么 Day-2 Operation 将继续进行，并包含了所有的活动，以逐步改进 X。\n一个简短的补充说明：“不可变系统 “的概念，即人们倾向于通过不允许变化但总是部署新系统来提高可用性，这与上述概念并不冲突。管理不可变系统的过程成为 Day-2 Operation 的一部分。\n对于大多数企业来说，Day-2 Operation 是重复性的。但这是系统为组织产生结果的地方。因此，在 Day-2 Operation 中不断寻求改进，一个能带来最大效益的改进应该是很自然的。\n评论 Day-2 Operation 目前在中文中暂无统一翻译，我暂且将其翻译为 “Day-2 运营”，这样可能会看起来更像是个敏捷词汇，跟 “精益运营” 比较像。这个命名方式可能来自物理（T0，T1，T2，这样来划分时间段），也可能是来自军事术语。Day 0/Day 1/Day 2 - the software lifecycle in the cloud age 这篇文章中对云时代的软件生命周期 Day0、Day1、Day2 做了比较完整的解释。\n在 IT 领域，Day0、Day1、Day2 指的是软件生命周期的不同阶段。在军事术语中，Day0 是训练的第一天，新兵进入成长阶段。在软件开发中，它代表着设计阶段，在这个阶段，项目需求被指定，解决方案的架构被决定。\nDay1 涉及开发和部署在 Day0 阶段设计的软件。在这个阶段，我们不仅要创建应用程序本身，还要创建它的基础设施、网络、外部服务，并实现这一切的初始配置。\nDay2 是产品发货或提供给客户的时间。在这里，大部分精力都集中在维护、监控和优化系统上。分析系统的行为并做出正确的反应是至关重要的，因为由此产生的反馈循环会一直应用到应用程序的寿命结束。在云时代这三个阶段跟云之前有很大的不同。\n软件准备好后，就开始上线，客户开始使用。Day2 从这里开始，介绍包括软件维护和客户支持在内的内容。软件本身要不断发展，以适应不断变化的需求和客户的要求。在 Day2，主要关注的是建立一个反馈循环。我们监控应用的运行情况，收集用户的反馈意见，并将其发送给开发团队，开发团队将在产品中实现并发布新版本。军事术语 Observe-Orient-Decid-Act 恰好能体现这一阶段的工作内容。\n观察：从监控系统中获取信息（资源使用和指标、应用性能监控）。 定位：对问题进行根本原因分析。 决定：找到解决出现的问题的方法。 行动：实施解决方案。 如同在作战过程中，这个循环不断重复，正如下图中展示的那样。\nDay 2 Operation 流程 监控程序是基于服务水平协议（SLA）中定义的要求。SLA基于服务水平目标（SLO），它代表了我们的服务水平指标（SLI）的状态。自动化和监控是解决第2天责任的关键。\n有几类工具可以帮助完成 Day2的工作。应用性能监控（APM）类组软件，帮助IT管理员监控应用性能，从而提供高质量的用户体验。在这里我们可以说出Datadog、Dynatrace、SignalFX或Nutanix Xi Epoch。还有一些自动化和编排工具，如Ansible或Kubernetes，它们有助于管理应用环境。这些工具的应用与Day1 的工作相重叠。最后，JIRA 或 GItHub 系统处理客户服务，使用户能够报告与他们正在运行的应用程序有关的问题。\n参考 Defining Day-2 Operations - ozone.com What is “Day-2” - about.gitlab.com Day 0/Day 1/Day 2 - the software lifecycle in the cloud age - codilime.com ","relpermalink":"/blog/what-is-day-2-operation/","summary":"在 IstioCon 2021 上，Istio 社区确定了 2021 年的社区的工作重点是 Day-2 Operation，很多人问我这个词是什么意思。我查了下中文互联网上，没有对这个词的解释，我在网上找到了一些解释，我发现大部分文章的源头都指向了这篇 Defining Day-2 Operations。因此，在此我将问翻译一下，同时再加上一些我自己的见解。","title":"什么是 Day-2 Operation？"},{"content":"各位云原生领域的创业者，你们的机遇来啦！我有幸参与「腾讯云原生加速器 」，担任开源社区及行业导师，共30个成员名额，报名截止到4月20日。云原生社区将与腾讯一起共同推进腾讯云原生加速器。\n近年来，以 “云原生” 为技术路线，构建信息化平台，已成为企业构建面向未来应用架构的首选。云原生凭借敏捷、开放、标准化的特点，将云计算的优势进一步拓宽，轻量化、松耦合、灵活的技术架构特点，有利于各企业在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。\n3 月 10 日，腾讯正式发布国内首个云原生加速器，面向云原生应用、云原生应用编排及管理、云原生底层技术、云原生安全技术等四大方向开启招募，30 个加速席位虚位以待。\n腾讯云原生加速器是腾讯产业加速器的重要组成部分，依托腾讯领先的科技能力、连接内外部资源、发挥技术创新优势，为入选成员提供技术、资金、品牌等层面的加速赋能与生态合作，加速云原生企业成长，促进 “云原生” 技术路线落地产业，共建云原生产业生态。\n迎接云计算拐点，邀请合作伙伴共建云原生产业生态\n云原生是面向云应用设计的思想理念，以容器、微服务、DevOps、持续集成与持续交付等技术为基础建立，能够发挥云效能的最佳实践路径，充分提升企业研发运维效率。近年来，随着新型 5G、工业互联网等 IT 基础设施的发展，云原生以其敏捷、开放、标准化的特点将云计算的优势进一步拓宽，迅速成为企业构建面向未来的应用架构的首选。中国信息通信研究院《中国云原生用户调查报告 2020》显示，2019 年中国云原生市场规模已达 350.2 亿元，云计算拐点已至，云原生成为驱动业务增长的重要引擎。\n以产业生态模式，腾讯一直与生态伙伴 “共创” 跨产业、跨科技边界的融合创新。近年，腾讯不断加速拓展云原生应用场景，推动产业从线下走向云端。目前，腾讯云已打造了完备的云原生产品体系和架构，涵盖软件研发流程、计算资源、架构框架、数据存储和处理、安全等五大领域。截至 2020 年 12 月，腾讯云原生注册用户规模已达 100 + 万，覆盖政府、金融、文体、教育、能源、电商、互联网、游戏、LBS、IM、媒体、交通、影视等主流行业。\n为进一步发挥产业互联网 “生态共创” 优势，全方位推动云原生生态进阶，腾讯发布云原生加速器，持续挖掘并扶持，具备高成长性、高协同价值的云原生生态合作伙伴，链接内外部资源进行赋能，发挥协同力量寻找企业云原生改造最佳实践路线，共建健康发展的云原生产业生态。\n全链路整合腾讯优势，关键节点赋能入选成员\n此次云原生加速器主要招募四大方向，并将为入选成员提供从热点底层技术，到行业创新应用，再到业务核心价值延展的持续赋能。\n腾讯云原生加速器招募方向 腾讯云原生加速器招募方向\n在技术层面，云原生加速器将对接腾讯云原生、腾讯云计算、腾讯开源联盟、全球顶级开源社区，在关键节点与成员进行联合技术研发，提高存储及计算能力，共同推进云原生技术发展。\n在资金层面，云原生加速器对接腾讯产业生态投资，联合一线 VC 机构，为成员提供多渠道资本扶持，提高资本对接效率。\n在行业层面，腾讯云将提供 “商机 + 流量” 导入，为成员提供多元合作模式与商业机会，拓展云原生应用场景，加快更多行业向云原生环境迁移。\n在资源层面，入选成员将成为腾讯云原生、腾讯云安全、腾讯 AI 实验室、边缘计算实验室、优图实验室等先进技术合作伙伴，收获更多合作机会。\n跨行业邀请 19 位顶尖导师，全维度解读核心痛点\n为给学员构建一个学习交流、业务合作的生态链接平台，腾讯云原生加速器共邀请来自开源社区、腾讯内部、VC 领域的 19 位顶尖导师，与学员共同探究云原生行业生态版图与业务应用。\n其中，来自开源社区及行业的 3 位导师 ——Linux 基金会和 CNCF 云原生基金会大中华区总裁 Keith Chan, 腾讯开源联盟主席单致豪，云原生社区及 ServiceMesher 创始人宋净超，将针对开源社区及开源生态进行观点输出，助力成员拥抱开源生态，链接云原生技术社群。\n腾讯云原生加速器开源社区及行业导师 腾讯云原生加速器开源社区及行业导师\n此外，腾讯公司高级执行副总裁、云与智慧产业事业群总裁汤道生，腾讯公司副总裁、腾讯云总裁邱跃鹏，腾讯公司副总裁丁珂，腾讯云副总裁穆亦飞，腾讯研究院院长司晓，腾讯公司副总裁王巨宏，腾讯云副总裁陈广域，腾讯云副总裁刘颖将分别就云原生战略布局、生态共享、技术应用等话题进行分享，与学员共同探索云原生新课题与新机遇。\n腾讯云原生加速器腾讯内部导师 腾讯云原生加速器腾讯内部导师\n腾讯云原生加速器获得了来自 VC 领域的多位导师支持。来自 IDG、红点中国、云启资本、斯道资本、晨晖创投、北极光、五源资本、宽带资本的合伙人与董事，将从投资角度出发，详解云原生产业发展的核心痛点与底层逻辑。\n腾讯云原生加速器VC导师 腾讯云原生加速器 VC 导师\n除导师辅助外，入选成员还将获得一年期立体孵化，通过 4 次闭门交流 + 1 次海外产业探访 + N 次业务及资源对接，完成技术、资源与合作等全方位持续赋能。\n随着数字化转型的加速，作为基础设施的云原生技术，日益成为构筑商业护城河的新入口。Gartner 曾做出预测：在 2020 年前，50% 的企业将业务工作流放到本地需要作为异常事件进行审批，公司 “无云” 的策略会和现在 “无网络” 的策略一样少。\n在云时代，不仅需要夯实云原生技术底座，还需要通过生态合作与社区链接，完成云原生技术共建。腾讯云原生加速器将联结更多生态伙伴，基于云原生实践经验，共创云原生与业务融合的无限可能。\n报名地址：见腾讯问卷 ","relpermalink":"/notice/tencent-cloud-native-accelerator/","summary":"30 个成员席位虚位以待！报名截止到4月20日。","title":"Jimmy 担任「腾讯云原生加速器」开源社区及行业导师"},{"content":"我所任职的公司 Tetrate，今日宣布获得 4000 万美元 B 轮融资。下面的公告来自 Tetrate 的 CEO Varun Talwar，点击阅读原文 。\n我很高兴地宣布，Tetrate 获得 4000 万美元的 B 轮融资，由 Sapphire Ventures 领投，Scale Venture Partners 、NTTVC 以及之前的投资者 Dell Technologies Capital 、Intel Capital 、8VC 和 Samsung NEXT 跟投。 在此，我要感谢我的同事（也就是 Tetrands）、投资人、客户、合作伙伴和朋友们一直以来的支持。\n我们将使用这笔资金扩大我们的全球市场，在工程设计上加倍投入，以进一步支持我们的客户利用 唯一的混合云应用网络平台 Tetrate Service Bridge 构建其连接结构。我们还会利用这笔资金将全新的 SaaS 产品 Tetrate Cloud 推向市场：一个基于 Istio 的完全托管的服务网格平台，以实现在任何云上的一致体验。\n融资理由 当今的现代应用程序是一组相互关联的服务，并以容器的形式部署在多集群，有时甚至是多云环境中。在这种分布式动态应用的组合中，阻碍企业灵活性的是通过硬编码库、定制网关、负载均衡器和单片 API 网关实现应用网络和安全的传统模式。\n这就是 Tetrate Service Bridge 的优势所在。\nTetrate Service Bridge Tetrate Service Bridge 是唯一的边缘到工作负载的服务网格管理平台，它为企业提供了一种统一、一致的方式，在复杂的异构部署环境中管理和保护传统和现代工作负载的服务。它是为多集群、多租户和多云部署而构建的，为客户在任何环境中提供一致的内置可观察性、运行时安全性和流量管理。此外，对于平台所有者来说，它可以管理 Istio 和 Envoy 在内部和云中的生命周期。\n将应用网络和安全集中在一个结构中，便于应用开发者使用，这对企业的生产力，以及应用的可靠性和安全性都是一种变革。\nTetrate 是唯一一家为 Istio 提供商业支持的平台中立和云中立公司，也是唯一一家基于 Istio 和 Envoy 的服务网格平台的公司，其客户包括财富 200 强的金融服务、电信、零售、媒体和美国联邦政府机构。\nTetrate 的发展历程 当我在 2018 年创办 Tetrate 时，我知道行业需要一个与云无关的服务网格平台来使应用具有弹性和安全性。Tetrate 的联合创始人 JJ 之前领导了 Twitter 的云基础设施管理团队，他在这个领域有着深厚的经验，共同的激情促使我们成立了 Tetrate。很快，我们的创始工程师也加入了。 Zack Butcher、周礼赞和吴晟 ——Istio、Envoy 和 Apache SkyWalking 背后的三巨头。\n2019 年，我们成功完成了由 Dell Technologies Capital 领投的 A 轮融资 ，在得到早期客户的强烈验证后，我们打造了核心产品 Tetrate Service Bridge。我们还启动了与 NIST 的合作，共同制定服务网格的安全标准，并在我们的平台上使用 NGAC 以实现 ZTA。在 2020 年，我们迅速扩大了我们的客户群和收入；今年，B 轮融资标志着我们旅程中的一个重要里程碑 —— 为创纪录的财年度画上了句号，我们的团队扩大了一倍，客户群和收入增加了 10 倍以上。要了解更多关于我们迄今为止的历程，请查看我们的 时间表 。\n我们很自豪，也很谦虚，因为我们获得了超额认购的融资，并且有这么多的投资者验证了我们在开源生态系统中的深厚根基、我们在服务网格领域的思想领导力，以及我们为客户解决问题的记录。\nTetrate B 轮融资投资者 在此观看 我们的投资者对他们投资 Tetrate 的原因和我们的愿景的看法。我们相信，我们才刚刚起步，最好的还在后面。我们期待着共同打造 Tetrate。\n作者：Varun Talwar（Tetrate CEO）\n","relpermalink":"/notice/tetrate-series-b/","summary":"这笔钱将用于扩大全球市场，增大工程投入，以及基于 Istio 的完全托管的服务网格平台 SaaS 服务 Tetrate Cloud。","title":"Tetrate 宣布 B 轮融资 4000 万美元"},{"content":"Envoy 中文文档地址：https://cloudnative.to/envoy 先给大家拜个晚年 云原生社区 Envoy SIG 再次给众位云原生技术爱好者拜个晚年（毕竟正月十五还没过，也不算晚），祝愿大家牛年牛起来！！！\n当然，技术也要搞起来，因此我们需要接着我们去年的任务 —— Envoy 1.16 版本的翻译，继续撸起袖子加油干。下面会回顾及发布一下翻译进度以及做出突出贡献的志愿者（证书年前已经发放）。\n翻译进度 由云原生社区 Envoy SIG 发起的 Envoy 1.16 版本翻译，在几十位（登记信息的为 64 人）志愿者的共同努力下，经过了以下几个阶段的努力贡献：\n第一阶段：2020 年 11 月 4 日至 2020 年 11 月 28 日，共开放 84 个 issue。\n第二阶段：2020 年 11 月 28 日至 2020 年 12 月 6 日，共开放 24 个 issue。\n第三阶段：2020 年 12 月 6 日至 2021 年 1 月 10 日，共开放 128 个 issue。\n第四阶段：2021 年 1 月 10 日至 2021 年 1 月 17 日，共开放 10 个 issue。\n第五阶段：2021 年 1 月 17 日至 2021 年 2 月 9 日，共开放 76 个 issue。\n第六阶段：2021 年 2 月 9 日开始，共开放 34 个 issue。\n现在已经到了决胜攻坚时刻，最后的翻译任务已经开放。欢迎大家搭载翻译的末班车，为 Envoy 的翻译贡献自己的力量。\n突出贡献人员 所有登记参与的人员可以腾讯文档 中查看，做出突出贡献（翻译至少 5 个 issue 或长期从事 Review 工作）已经收到了社区的证书：\n云原生社区证书 收到证书的 Envoy 翻译志愿者如下：\n人员 GitHub 账号 人员 GitHub 账号 许振文 helight 张晓辉 Addo.zhang 刘金欣 scilla0531 梁斌 hzliangbin 李云龙 vgbhfive 王泓智 wiswang 张海立 webup 包仁义 baobaoyeye 黄晓芬 kkfinkkfin 孟显超 smarkm 官余鹏 3ks 申红磊 shenhonglei 除了证书，集社区微薄之力，给突出贡献的部分人员送出了大会门票、云原生书籍等福利。名单会持续更新，大家都有机会。\n小小剧透 在 Envoy 1.16 版本翻译结束后，Envoy SIG 将择一黄道吉日在线上（云原生学院直播平台）开一个发布会。具的流程，暂时保密。\n欢迎感兴趣翻译的小伙伴，赶紧加入我们，一起努力来完成 Envoy 1.16 版本的翻译。\n如何加入 关于如何加入 Envoy SIG 并加入翻译团队，可以查看 Envoy 官方文档翻译工作组成员招募中 ；关于翻译要求及相关福利，可以查看 Envoy 官方文档翻译进度更新及成员招募 。有任何疑问可以直接添加微信（jimmysongio 或者 majinghe11）进行咨询。\n关于云原生社区 云原生社区成立于 2020 年 5 月 12 日，作为中立的云原生终端用户社区，致力于推广云原生技术，构建开发者生态。点击了解我们 。\n云原生社区基于成员兴趣创建了多个 SIG（特别兴趣小组），如 Kubernetes、Istio、Envoy、OAM、Dapr、稳定性、可观察性等。请扫描下面的二维码，点击公众号后台的「加入我们」，填写问卷加入社区。\n加入云原生社区 ","relpermalink":"/notice/envoy-doc-translation/","summary":"Envoy 文档翻译由云原生社区 Envoy SIG 发起，即将完成，欢迎大家加入！","title":"Envoy 文档翻译进入攻坚决胜时刻！欢迎大家加入"},{"content":"本文为我跟 Ignasi Barrera 共同创作，本文英文版首发于 TheNewStack 。\n不同的公司或软件供应商已经设计了无数种方法来控制用户对功能或资源的访问，如酌情访问控制（DAC）、强制访问控制（MAC）、基于角色的访问控制（RBAC）和基于属性的访问控制（ABAC）。从本质上讲，无论何种类型的访问控制模型，都可以抽象出三个基本要素：用户、系统 / 应用和策略。\n在本文中，我们将介绍 ABAC、RBAC 以及一种新的访问控制模型 —— 下一代访问控制（NGAC），并比较三者之间的异同，以及为什么你应该考虑 NGAC。\n什么是 RBAC？ RBAC，即基于角色的访问控制，采用的方法是根据用户在组织中的角色授予（或拒绝）对资源的访问。每个角色都被分配了一系列的权限和限制，这很好，因为你不需要跟踪每个系统用户和他们的属性。你只需要更新相应的角色，将角色分配给用户，或者删除分配。但这可能很难管理和扩展。使用 RBAC 静态角色模型的企业经历了角色爆炸：大公司可能有数万个相似但不同的角色或用户，他们的角色会随着时间的推移而改变，因此很难跟踪角色或审计不需要的权限。RBAC 具有固定的访问权限，没有规定短暂的权限，也没有考虑位置、时间或设备等属性。使用 RBAC 的企业很难满足复杂的访问控制要求，以满足其他组织需求的监管要求。\nRBAC 示例 下面是 Kubernetes 中 default 命名空间中的一个 Role，可以用来授予 pod 的读取权限。\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [\u0026#34;v1\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] 什么是 ABAC？ ABAC 是 “基于属性的访问控制 “的缩写。从高层次上讲，NIST 将 ABAC 定义为一种访问控制方法，“在这种方法中，根据分配的主体属性、环境条件以及用这些属性和条件指定的一组策略，批准或拒绝主体对对象进行操作的请求。” ABAC 是一个细粒度的模型，因为你可以给用户分配任何属性，但同时它也成为一种负担，很难管理：\n在定义权限的时候，用户和对象之间的关系无法可视化。 如果规则设计的有点复杂或者混乱，对于管理员来说，维护和跟踪会很麻烦。 当有大量的权限需要处理时，会造成性能问题。\nABAC 示例 Kubernetes 最初使用 ABAC 作为访问控制，并通过 JSON 行配置，例如：\nAlice 可以只读取命名空间 foo 中的 pod。\n{\u0026#34;apiVersion\u0026#34;: \u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: {\u0026#34;user\u0026#34;: \u0026#34;alice\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;pods\u0026#34;, \u0026#34;readonly\u0026#34;: true}} 什么是 NGAC？ NGAC，即下一代访问控制，采用将访问决定数据建模为图的方法。NGAC 可以实现系统化、策略一致的访问控制方法，以高精细度授予或拒绝用户管理能力。NGAC 由 NIST （美国国家标准与技术研究所）开发，目前用于 Tetrate Q 和 Tetrate Service Bridge 。\n有几种类型的实体；它们代表了您要保护的资源、它们之间的关系以及与系统互动的行为者。这些实体是：\n用户 对象 用户属性，如组织单位 对象属性，如文件夹 策略类，如文件系统访问、位置和时间 NIST 的 David Ferraiolo 和 Tetrate 的 Ignasi Barrera 在旧金山举行的 2019 年服务网格日（Service Mesh Day 2019）上发表了关于下一代访问控制的 演讲 ，分享了 NGAC 的工作原理。\nNGAC 是基于这样一个假设：你可以用一个图来表示你要保护的系统，这个图代表了你要保护的资源和你的组织结构，这个图对你有意义，并且符合你的组织语义。在这个对你的组织非常特殊的模型之上，你可以叠加策略。在资源模型和用户模型之间，定义了权限。这样 NGAC 提供了一种优雅的方式来表示你要保护的资源，系统中的不同角色，以及如何用权限把这两个世界联系在一起。\nNGAC 模型中的 DAG 图片来自于 Linear Time Algorithms to Restrict Insider Access using Multi-Policy Access Control Systems NGAC 示例 下面的例子展示了一个简单的 NGAC 图，其中有一个代表组织结构的用户 DAG，一个代表文件系统中的文件和文件夹的对象 DAG，一个文件的分类，以及两个不同的策略 —— 文件系统和范围，可以结合起来做出访问决策。两个 DAG 之间的关联边定义了行为者对目标资源的权限。\nNGAC 示例图 在这张图中，我们可以看到 /hr-docs 文件夹中的两个文件 resume 和 contract 的表示，每个文件都链接到一个类别（public/confidential）。还有两个策略类，File System 和 Scope，图中的对象被连接在这里 —— 需要满足这些条件才能获得对每个文件的访问权。\n在例子中，用户 Allice 对两个文件都有读写访问权限，因为有一个路径将 Allice 链接到每个文件，而且路径授予了两个策略类的权限。但是，用户 Bob 只有对 resume 文件的访问权，因为虽然存在一个从 Bob 到 contract 文件的路径，该路径满足 File System 策略类的 “读 \u0026#34; 权限，但没有授予 Scope 策略类权限的路径。所以，Bob 对 contract 文件的访问被拒绝。\n为什么选择 NGAC？ 在 ABAC 的情况下，需要跟踪所有对象的属性，这造成了可管理性的负担。RBAC 减少了负担，因为我们提取了所有角色的访问信息，但是这种模式存在角色爆炸的问题，也会变得不可管理。有了 NGAC，我们在图中就有了我们所需要的一切 —— 以一种紧凑、集中的方式。\n当访问决策很复杂时，ABAC 的处理时间会成倍上升。RBAC 在规模上变得特别难以管理，而 NGAC 则可以线性扩展。\nNGAC 真正出彩的地方在于灵活性。它可以被配置为允许或不允许访问，不仅基于对象属性，而且基于其他条件 —— 时间、位置、月相等。\nNGAC 的其他关键优势包括能够一致地设置策略（以满足合规性要求）和设置历时性策略的能力。例如，NGAC 可以在中断期间授予开发人员一次性的资源访问权，而不会留下不必要的权限，以免日后导致安全漏洞。NGAC 可以在一个访问决策中评估和组合多个策略，同时保持其线性时间的复杂度。\n总结 下表从几个方面对 ABAC、RBAC 和 NGAC 进行了比较。\n权限模型 优点 缺点 ABAC 灵活 性能和审计问题 RBAC 简单 角色爆炸、固定的访问权限、合规需求挑战 NGAC 细粒度、利于审计、灵活、组合权限策略 复杂 总而言之：\nRBAC 比较简单，性能好，但在规模上会受到影响。 ABAC 很灵活，但性能和可审计性是个问题。 NGAC 通过使用一种新颖、优雅的革命性方法来修复这些差距：在用户提供的现有世界表示之上叠加访问策略。你也可以对 RBAC 和 ABAC 策略进行建模。 参考 Guide to Attribute-Based Access Control (ABAC) Definition and Considerations Deploying ABAC policies using RBAC Systems RBAC vs. ABAC: What’s the Difference? Role Explosion: The Unintended Consequence of RBAC Exploring the Next Generation of Access Control Methodologies ","relpermalink":"/blog/why-you-should-choose-ngac-as-your-access-control-model/","summary":"本文将向你介绍下一代权限控制模型——NGAC，并对比 ABAC、RABC，说明为什么要选择 NGAC。","title":"为什么应该选择使用 NGAC 作为权限控制模型"},{"content":" IstioCon 海报（Jimmy Song） 主题：Service Mesh in China 时间：北京时间 2 月 23 日，上午 10:00 - 10:10 参与方式：IstioCon 2021官网 费用：免费 北京时间 2 月 22 日至 25 日，Istio 社区将在线上举办第一届 IstioCon，免费报名参加，欢迎大家踊跃参加！我将在 2 月 23 日（农历正月十二，星期二）发表闪电演讲，作为一个 Service Mesh 技术在中国的布道者和见证者，我将为大家介绍中国的 Service Mesh 行业及社区在中国的发展。\n我与徐中虎（华为）、丁少君（Intel）都是首届 IstioCon 组委会成员，也是中国区的组织者，考虑到 Istio 在中国有大量的受众，我们特地安排对中国时区友好的中文演讲。本次活动一共有 14 场中文分享，另外还有几十场英文分享。演讲将分为闪电演讲（10 分钟）和 presentation（40 分钟）两种形式。\n加入云原生社区 Istio SIG ，参与本次大会的交流。关于 IstioCon 2021 的时间表，请访问 IstioCon 2021 官网 ，或点击查看详情。\n","relpermalink":"/notice/istiocon-2021/","summary":"IstioCon 2021，我将发表闪电演讲，北京时间 2 月 22 日上午 10 点。","title":"IstioCon 2021 闪电演讲预告"},{"content":"ServiceMesher 网站 因为其代码托管的 GitHub 与网站发布服务器上的 webhook 程序已”失联“，网站托管服务器暂时无法登录，以致于网站无法更新。今天我花了一天的时间，将 ServiceMesher 上的所有博客都迁移到了云原生社区官网 cloudnative.to ，截止今天，云原生社区上一共有 354 篇博客。\n现计划将 ServiceMesher 官网的 GitHub 归档（servicemesher.com 域名下的所有页面），不再接受新的 PR，请大家直接提交到云原生社区 。谢谢大家！\n","relpermalink":"/notice/servicemesher-blog-merged/","summary":"ServiceMesher 网站已停止维护，计划将网站代码归档，博客已迁移到云原生社区，请将新的博客提交到云原生社区。","title":"ServiceMesher 网站停止维护，原有博客已迁移到云原生社区"},{"content":"很荣幸收到 CSDN 的邀请，接受” 云原生人物志 “专栏采访，其实我从 2017 年起就已经在撰写 Kubernetes 和云原生年度总结和新年展望 ，今天在此聊抒己见，欢迎大家讨论和指正。\n云原生在演进 云原生是一种行为方式和设计理念，究其本质，凡是能够提高云上资源利用率和应用交付效率的行为或方式都是云原生的。云计算的发展史就是一部云原生化的历史。Kubernetes 开启了云原生 1.0 的序幕，服务网格 Istio 的出现，引领了后 Kubernetes 时代的微服务，serverless 的再次兴起，使得云原生从基础设施层不断向应用架构层挺进，我们正处于一个云原生 2.0 的新时代。\n业界动向 最近国内的一些云厂商，如阿里云、腾讯云、华为云陆续发布了各自的云原生相关的架构和实践白皮书。\n2020 年 7，中国信通院发布了《云原生产业白皮书（2020）》。 2020 年 12 月 20 日，在腾讯 2020 Techo Park 开发者大会上，腾讯云正式发布了《云原生最佳实践路线图》，同时发布的还有一份 3 万多字的《腾讯云原生路线图手册》。 2020 年 12 月 23 日，阿里云原生实战峰会上发布了《云原生架构白皮书》。 2020 年 12 月 30 日，华为云在深圳的 TechWave 云原生 2.0 技术峰会上联合 Forrester 发布了《云原生白皮书：拥抱云原生优先战略》。 2021 年初，阿里巴巴达摩院发布 2021 十大科技趋势，其中将 “云原生重塑 IT 技术体系” 作为 2021 年技术预测之一。 云原生项目的 “寒武纪大爆发” 云原生已历经” 寒武纪大爆发 “，标志是从 2018 年 Kubernetes 毕业 后走向深耕路线。云原生领域的开源项目层出不穷，令人眼花缭乱，见我收集的 Awesome Cloud Native。\n云原生发展阶段 2020 年 CNCF 共接纳了 35 个项目加入基金会，并且有多个项目毕业或晋级，CNCF 托管的项目总数达到了 80 多个。\n图片来自 CNCF 年度报告 2020 云原生之争实际上是标准之争 PC 端操作系统 Windows 占据上风，移动端是 iOS 和 Android，服务器端是 Linux，而云计算商用分布式操作系统呢？答案是 Kubernetes。\n2020 年 Kubernete 宣布将在 v1.20 版本之后弃用 Docker ，实际上 Docker 本来就不是 Kubernetes 中默认和唯一的的容器运行时了，实际上只要是支持 CRI（Container Runtime Interface）或 OCI（Open Container Initiative）标准的容器运行时都可以在 Kubernetes 中运行。如下图所示，容器，英文是 container，也是集装箱的意思，其实集装箱不止一种型号，根据运送的货物的不同特性可以制定了多种集装箱类型。而这个容器类型是标准只能是由 Kubernetes 来定，否则只能是削足适履。\n各种容器类型 Kubernetes 统一了云上的资源对象制定和调度的标准，只要在其标准之上开发 CRD 和 Operator 即可。但是这也仅限于单个应用的管理，如何管理复杂的多集群和混合云环境，如何管理应用间流量，如何如何保证调用链的安全？以 Istio 为代表的服务网格就是为了解决这个问题。\n云原生趋势：云上应用管理 Kubernetes 奠定了云原生基础设施的基础，随着而来的监控、存储、AI、大数据等技术的迁移，从单个应用层面来说已经日趋成熟，而在使用云原生架构尤其是对云上应用的管理，而在异构环境、多集群、混合云等已成为常态的情况下，如何对云上的应用进行管理，成为棘手的事情。\nKubernetes 以其开创新的声明式 API 和调节器模式，奠定了云原生的基础。我们看到 Google 的项目 Anthos，Azure 的 Arc，AWS 最近开源的 EKS-D，它们都是着重在混合云管理，让云无处不在。另外，服务网格（Service Mesh）经过两年的推广和发酵，将会看到越来越多的应用。\n云原生与开源社区 目前企业云原生化转型最缺乏的东西 —— 套路和组合拳。对于基础软件，企业往往会选择开源项目并根据自身需求进行改造，而云原生的开源项目又有很多，企业不是没有选择，而是选择太多，以致于无从下手。就像下面教你如何画猫头鹰的示例。我们可以将企业的云原生化的愿景想象成是这只猫头鹰，这些开源项目就像步骤一中圆，你可能想当然的认为只要用了 Kubernetes 就是云原生了，这就像画了两个圆，而剩余部分没有人教你如何完成。\n如何画猫头鹰 开源社区的核心是面向开发者，就是向开发者灌输如何来画好这只 “猫头鹰” 的。开源不意味着免费和做慈善，使用开源也是有代价的。开源社区存在的意义是平衡开发者、终端用户及供应商之间的共同利益，而一个中立的开源社区有利于发挥开源的生态优势。\n近年来随着云原生大热，在美国诞生了大量该领域的初创公司，他们基于 AWS、谷歌云、Azure 等提供各种云原生的解决方案，从每次 KubeCon 的赞助商规模上就可以窥知一二。国内该领域的公司目前还不多，而云原生终端用户社区的公司规模上依然跟国外的公司数量有不小的差距。\n云原生社区就是在这样的背景下于 2020 年初由我发起，开始筹备并在 5 月 12 号正式成立，致力于推广云原生技术，构建开发者生态。云原生社区采取 SIG（特别兴趣小组）和 WG（工作组）的组织形式，基于开源项目和不同的专业领域构建研讨组，与厂商合作定期举办线下 meetup，并邀请社区的专家们定期在 B 站的云原生学院进行直播。\n总结 开源应该关注的是终端用户和开发者生态，用 Apache Way 来说就是 “社区大于代码”，没有社区的项目是难以长久的。因此我们可以看到国内一些云厂商开源项目之后也会积极投入运营，举行各种各样的活动。我们看到在云原生的推广过程中，CNCF 起到的相当大的作用，2020 年国内也有类似的基金会成立，我们希望看到更多中立的基金会和社区的成立，更多的厂商参与其中，为终端用户提供更佳的解决方案。\n最后感谢 CSDN 宋慧编辑和 「CSDN 云计算」的邀请。\n往期报道见：\n梁胜：做开源项目的贡献者没有意义 华为云 CTO 张宇昕：云原生已经进入深水区 APISIX 温铭：开源的本质是要拿开发者的杠杆 个人介绍 在我的职业生涯里先后从事过 Java 开发、大数据运维、DevOps、开源管理等工作，个人爱好是研究并推广开源技术及理念，摄影和旅行。目前在企业级服务网格初创公司 Tetrate 担任 Developer Advocate，同时作为中立的云原生终端用户社区 —— 云原生社区（Cloud Native Community）的负责人。\n我的整个职业生涯都是与开源息息相关的，渊源可以追溯到大学时期。大学时我就开始使用 Linux 系统（Ubuntu）学习，刚进入职场的时候面向的也是 Hadoop 的开源生态及各种开源中间件，2015 起开始接触 Docker，2016 年开始进入云原生领域，2017 年开始写 Kubernetes 领域的第一本开源中文电子书《Kubernetes Handbook——Kubernetes 中文指南 / 云原生应用架构实践手册 》，本书直到如今仍在更新，2018 年在蚂蚁集团做开源管理及服务网格社区 ServiceMesher，2020 年加入基于 Istio、Envoy 和 Apache SkyWalking 等开源项目而构建企业级服务网格的初创公司 Tetrate。\n","relpermalink":"/blog/cloud-native-2021/","summary":"本文为应 CSDN《云原生人物志》栏目约稿，知微见著，窥见云原生价值与趋势。","title":"“寒武纪大爆发” 之后的云原生，2021 年走向何处？"},{"content":"AWS 在 2020 年12 月举行的 re:Invent 大会上发布了 EKS-D ，此举旨在联合合作伙伴，开源 AWS 维护大规模 EKS 集群的经验，帮助用户实现混合云场景下 Kubernetes 的一致性的体验。本文将为你解析 EKS-D 的战略意义，说明它是如何与 Istio 共同保证混合云环境一致性的。\n什么是 EKS-D？ EKS-D 是 Amazon EKS 的一个发行版，可以运行在企业内部、云端或自己的系统上。EKS-D 保持与 Kubernetes 新版本同期发布。在不久的将来，将以 EKS Anywhere（EKS-A）为名，提供 EKS-D 的支持、打包产品和安装方法。\n下图展示了 AWS、EKS-D、 Kubernetes 及用户之间的关系。\nEKS-D 对于 AWS、合作伙伴及用户来说具有不同的意义。\nAWS：增加 AWS 的市场拥有率 合作伙伴：整合 AWS 的渠道和客户资源以触达更多用户 用户：保证了异构环境下的 Kubernetes 的一致性，简化运维 如今企业要考虑选择哪个云供应商要考虑很多因素，同时，还有很多企业的 IT 难以跨入云，而是继续依赖究竟考验的传统 IT 架构以开展业务。\n在上云的时候客户希望在企业内部和云端获得一致的体验，以便进行迁移或实现混合云设置。不是所有应用都适合跨云迁移，为了合规、数据安全等种种原因，多集群、混合云的使用场景将很普遍。\n为什么使用多集群和混合云 我们在很多情况下或使用多集群、混合云等部署方式，例如：\n为了避免厂商锁定，便于应用跨集群迁移； 为了实现应用的高可用； 当一个集群的规模过大造成性能瓶颈时； 为了合规和数据安全； 为了就近部署，降低网络延迟，提高用户体验； 为了进行一些测试； 突发业务，需要集群扩容； 以上情况经常发生，对集群的管理造成了挑战。Kubernetes 统一了容器编排的标准，随着其进一步普及，更有望成为云原生应用的底层 API。但是对于如何管理多集群和混合云环境中的 Kubernetes 集群，又为我们带来了新的挑战。\n使用 Istio service mesh 管理混合云 Istio 服务网格作为云原生应用的网络基础设施层，可以同时管理 Kubernetes 及非容器负载，如虚拟机 。Istio 可以在多种平台 中部署，又支持多种部署模式 ，兼具管理多集群和混合云的功能.在部署时需要充分考虑 Region、Zone 的分布、网络隔离、多租户、控制平面的高可用等因素。\n假如我们同时使用 EKS 和部署在私有数据中心中的 EKS-D，那么如何将两个集群使用一个统一的控制平面管理起来呢？如下图所示，cluster1 和 cluster2 分别表示部署在 EKS 和 EKS-D 的 Kubernetes 集群，这两个集群的网络是隔离的，现因为上文所说的适合使用混合云某个场景，现在为了将它们纳入同一个服务网格使用一个控制平面来管理，我们采用了 Primary-Remote 多网络 的部署模式。\n图示：\n图中黑色箭头表示控制平面内获取服务和端点配置的请求； 图中蓝色箭头表示服务 A 访问服务 B 的路由； 图中绿色箭头表示服务 A/B 向控制平面获取服务端点的路由； 使用该模式部署 Istio 时，需要保证控制平面对 Kubernetes 的 API Server 的连接性，具体的安装过程请参考 Istio 文档 。\n总结 EKS-D 保证了在混合云环境下 Kubernetes 集群的一致性，降低了集群的运维成本。Istio 固有的多集群感知能力，进一步从服务层面增强了用户体验的一致性，帮助我们将多集群中的服务纳入统一的控制平面管理。EKS-D 发布的时有众多的合作伙伴的响应，其中 Tetrate 作为 Istio service mesh 的解决方案供应商提供了 Tetrate Service Bridge（TSB） 在 EKS 和 EKS-D 上实现了跨工作负载的统一应用连接和安全性。\n","relpermalink":"/blog/eks-eksd-istio-hybrid-cloud/","summary":"本文将为你解析 EKS-D 的战略意义，说明它是如何与 Istio 共同保证混合云环境一致性的。","title":"使用 EKS-D 和 Istio 保证混合云环境一致性"},{"content":"本文将为你介绍 Istio 历史上对虚拟机负载的支持情况，尤其是 Istio 1.8 中引入的智能 DNS 代理及 WorkloadGroup 使得虚拟机与容器在资源抽象层面可以等同视之。我将为你展现一幅 Istio 支持虚拟机的波澜壮阔的奥德赛。\n前言 在我之前的博客 中谈到 Istio 1.7 如何支持虚拟机，但那时虚拟机仍然无法无缝的集成到 Istio 中，因为还需要做很多手动的操作。现在，Istio 1.8 新增了 WorkloadGroup 及智能 DNS 代理 ，这使得如虚拟机这样的非 Kubernetes 工作负载可以在 Istio 中成为像 Pod 一样的一等公民。\n不论有没有为虚拟机安装 sidecar，虚拟机通常情况下无法直接访问 Kubernetes 集群中的 DNS 服务器以解析 Kubernetes 服务的 Cluster IP 的（虽然你也许可以通过一些黑客的手段做到），这是在 Istio 中集成虚拟的最后一块短板，终于在 Istio 1.8 中完成了突破。\n为什么要支持虚拟机？ 在我们将应用在迁移到云原生架构，不断容器化的过程中，将经历三个阶段，如下图所示。\n云原生应用的三个阶段 阶段一：应用全部部署在虚拟机上 阶段二：应用既部署在虚拟机上也部署在容器里，正在从虚拟机向容器中迁移，并使用 Kubernetes 管理容器 阶段三：所有的应用优先部署在容器里，使用 Kubernetes 管理容器，使用 Istio 管理应用间的通信 上图仅是对以上三个阶段的最简化描述，实际上还会有多混合云、多机房、多集群等情况，且阶段三只是个理想化的阶段，容器和虚拟机将是长期共存的，但是容器化趋势不变。\n在阶段二中，人们通常会将新业务和少量应用率先实现容器化，并部署到 Kubernetes 中，在应用尚未完全实现容器化的时候，处于过度状态时会遇到很多问题，如何让应用与部署在虚拟机中的服务交互？虚拟机如何访问容器中的服务？在服务迁移的过程中如何保证稳定无缝？是否可以将容器和虚拟机纳入一个统一的控制平面来管理？Istio 从开源初期就考虑并着手解决这一问题。\nIstio 支持虚拟机的历史 Istio 对于虚拟机的支持是个漫长的过程，堪称是一部奥德赛。\nIstio mesh 扩张 Istio 从 0.2 版本开始通过 Istio Mesh Expansion 将虚拟机加入的 Mesh 中，但是需要满足以下前提条件：\n虚拟机必须可以通过 IP 地址直接访问到应用的 Pod，这就要求容器与 VM 之间通过 VPC 或者 VPN 建立扁平网络，虚拟机不需要访问 Cluster IP，直接对服务的 Endpoint 端点访问即可。 虚拟机必须可以访问到 Istio 的控制平面服务（Pilot、Mixer、CA，现在已正整合为 Istiod），可以通过在 Istio Mesh 中部署负载均衡器将控制平面端点暴露给虚拟机。 （可选）虚拟机可以访问到 Mesh 内部的（部署在 Kubernetes 中）的 DNS server。 集成虚拟机的步骤如下：\n为 Istio 控制平面服务及 Kubernetes 集群的 DNS 服务创建 Internal 负载均衡器； 生成 Istio Service CIDR、Service Account token、安全证书、Istio 控制平面服务的 IP（通过 Internal 负载均衡器暴露出来的 IP）的配置文件并发送给虚拟机； （可选）在虚拟机中安装、配置并启动 Istio 的组件、dnsmaq（用于DNS 发现），此时虚拟机可以使用 FQDN 访问 mesh 中的服务了，这一步是为了保证虚拟机可以正确解析出 mesh 中服务的 Cluster IP； 若要在虚拟机中运行服务，需要配置 sidecar，新增需要拦截的 inbound 端口，然后重启 istio，还需要运行 istioctl 为服务注册 下图展示的从集成虚拟机到在 mesh 中访问虚拟机中服务的详细流程。\n图一：从集成虚拟机到在 mesh 中访问虚拟机中服务的详细流程 DNS 被虚拟机中部署的 dnsmasq 劫持，这使得它可以正确的获取 Istio 服务、Kubernetes 内置 DNS 的端点 IP； 访问 Kubernetes 的内置 DNS 服务（该服务已通过 Internal 负载均衡器暴露到集群外，可以直接访问）； 返回 productpage.bookinfo.svc.cluster.local 被解析出来的 Cluster IP，注意该 IP 地址无法直接访问，但是如果无法被 DNS 解析的话将导致 VM 对该服务的请求失败； 虚拟机对 mesh 中服务的访问被 sidecar proxy 劫持； 因为 proxy 已连接 Istio 控制平面，可通过 xDS 查询到该服务的端点，因此流量将被转发到其中的一个端点。关于这一步的详细过程请参考 Istio Handbook 中的 sidecar 流量路由机制分析 一节 ； 要想在 mesh 中访问 VM 中的服务，需要使用 istioctl register 命令手动将 VM 中的服务添加到 mesh 中，这本质上是将 VM 中的服务，注册到 Kubernetes 中的 service 和 endpoint； mesh 中的服务可以使用 VM 注册的服务名称（FQDN，例如 mysql.vm.svc.cluster.local）来访问； 以上 Istio 对虚拟机支持的方式一直延续到 Istio 1.0，在 Istio 1.1 的时候引入了新的 API ServiceEntry ，使用它可以在 Istio 的内部服务注册表中添加额外的条目，这样 mesh 中的服务就可以访问/路由到这些手动指定的服务了，不再需要运行 istioctl register 命令，而且该命令在 Istio 1.9 中将被废弃。\nIstio 1.5 中增加了 istioctl experimental add-to-mesh 命令，可以将虚拟机中的服务添加到 mesh 中，其功能与 istioctl register 一样。\n新增资源抽象 Istio 从 1.6 版本 开始在流量管理 中引入了新的资源类型 WorkloadEntry ，用以将虚拟机进行抽象，使得虚拟机在加入 mesh 后可以作为与 Kubernetes 中的 Pod 等同的负载，具备流量管理、安全管理、可视化等能力。通过 WorkloadEntry 可以简化虚拟机的网格化配置过程。WorkloadEntry 对象可以根据服务条目中指定的标签选择器选择多个工作负载条目和 Kubernetes pod。\nIstio 1.8 中增加了 WorkloadGroup 的资源对象，它提供了一个规范，可以同时包括虚拟机和 Kubernetes 工作负载，旨在模仿现有的用于 Kubernetes 工作负载的 sidecar 注入和部署规范模型来引导 Istio 代理。\n下面是虚拟机与 Kubernetes 中负载的资源抽象层级对比。\n对比项 Kubernetes 虚拟机 基础调度单位 Pod WorkloadEntry 编排组合 Deployment WorkloadGroup 服务注册与发现 Service ServiceEntry 从上面的图表中我们可以看到，对于虚拟机工作负载是可以与 Kubernetes 中的负载一一对对应的。\n此时看似一切都比较完美了，但是直接将 Kubernetes 集群中的 DNS server 暴露出来会带来很大的安全风险 ，因此我们一般手动将虚拟机需要访问的服务的域名和 Cluster IP 对写到本机的 /etc/hosts 中，但是对于一个节点数量庞大的分布式集群来说，这种做法又有些不现实。\n通过配置虚拟机本地 /etc/hosts 访问 mesh 内服务的流程，如下图所示。\n图二：通过配置虚拟机本地 /etc/hosts 访问 mesh 内服务的流程 将虚拟机中的服务注册到 mesh 中； 将要访问的服务的域名、Cluster IP 对手动写入虚拟机本地的 /etc/hosts 文件中； 虚拟机获得访问服务的 Cluster IP； 流量被 sidecar proxy 拦截并解析出要访问的服务的端点地址； 访问服务的指定端点； 在 Kubernetes 中我们一般使用 Service 对象来实现服务的注册和发现，每个服务都有一个独立的 DNS 名称，应用程序可以使用服务名称来互相调用。我们可以使用 ServiceEntry 将虚拟机中的服务注册到 Istio 的服务注册表中，但是在 Kubernetes 集群中的 DNS server 无法对 mesh 外部暴露的情况下，虚拟机无法访问 Kubernetes 集群中的 DNS 服务以获取服务的 Cluster IP，从而导致虚拟机访问 mesh 中的服务失败。如果能在虚拟机中增加一个 sidecar 可以透明地拦截 DNS 请求，可获取 mesh 内所有服务的 ClusterIP，类似于图一中的 dnsmasq 的角色，这样不就可以解决问题了吗？\n智能 DNS 代理 Istio 1.8 中引入了智能 DNS 代理 ，虚拟机访问 mesh 内服务无需再配置 /ect/hosts，如下图所示。\n图三：引入了智能 DNS 代理后虚拟机访问 mesh 内服务的流程 DNS proxy 是用 Go 编写的 Istio sidecar 代理。Sidecar 上的 Istio agent 将附带一个由 Istiod 动态编程的缓存 DNS 代理。来自应用程序的 DNS 查询会被 pod 或 VM 中的 Istio 代理透明地拦截和服务，该代理会智能地响应 DNS 查询请求，可以实现虚拟机到服务网格的无缝多集群访问。\n至此，Istio 1.8 中引入的 WordloadGroup 及智能 DNS 代理，补足了 Istio 对虚拟机支持的最后一块短板，使得部署在虚拟机中的遗留应用可以跟 Kubernetes 中的 Pod 一样完全等同看待。\n总结 在这部 Istio 支持虚拟机的奥德赛中，我们可以看到：从最初的将 mesh 中的 DNS server 暴露给外部，在虚拟机中安装配置 dnsmasq，到最后的使用智能 DNS 代理，并使用 WorkloadEntry、WorkloadGroup 和 ServiceEntry 等资源抽象，逐步实现了虚拟机和 pod 的统一管理。本文仅仅是针对单集群的情况，在实际的生产中使用还远远不够，我们还需要处理安全、多集群、多租户等诸多问题，欢迎关注 Tetrate 的旗舰产品 Tetrate Service Bridge 了解更多关于 Istio 应用在生产上的最佳实践。\n","relpermalink":"/blog/istio-vm-odysssey/","summary":"本文将为你介绍 Istio 历史上对虚拟机负载的支持情况，尤其是 Istio 1.8 中引入的智能 DNS 代理及 WorkloadGroup 使得虚拟机与容器在资源抽象层面可以等同视之。我将为你展现一幅 Istio 支持虚拟机的波澜壮阔的奥德赛。","title":"Istio 对虚拟机支持史话"},{"content":"今天 Istio 1.8 发布了，这是 Istio 在 2020 年发布的最后一个版本，按照 Istio 社区在今年初设定的目标 继续推进，该版本主要有以下更新：\n支持使用 Helm 3 进行安装和升级 正式移除了 Mixer 新增了 Istio DNS proxy，透明地拦截应用程序的 DNS 查询，实现智能应答 新增了 WorkloadGroup 以简化对虚拟机的引入 WorkloadGroup 是一个新的 API 对象，旨在与虚拟机等非 Kubernetes 工作负载一起使用，模仿现有的用于 Kubernetes 工作负载的 sidecar 注入和部署规范模型来引导 Istio 代理。\n安装与升级 Istio 从 1.5 版本开始弃用了 Helm，使用 istioctl manifest 方式安装，后来又改成了 istioctl install，现在又重新回归了 Helm，Helm 作为 Kubernetes 环境下最常用的应用安装管理组件，此次回归也是倾听用户声音，优化安装体验的的反应吧，不过 Istio Operator 依然将是 Istio 安装的最终形式，从 1.8 版本开始 Istio 支持使用 Helm 进行 in-place 升级和 canary 升级。\n增强 Istio 的易用性 istioctl 命令行工具新的了 bug reporting 功能（istioctl bug-report），可以用来收集调试信息和获取集群状态。\n安装 add-on 的方式变了，在 1.7 中已经不推荐使用 istioctl 来安装，在 1.8 中直接被移除了，这样有利于解决 add-on 落后于上游及难以维护的问题。\n正式移除了 Mixer，推荐使用 WebAssembly 通过扩展 Envoy 的方式来扩展 Istio，也推荐大家使用 GetEnvoy Toolkit 来进行 Envoy 的扩展开发。\n对虚拟机的支持 在我之前的博客 中谈到 Istio 1.7 如何支持虚拟机，在 Istio 1.8 中新增了智能 DNS 代理 ，它是由 Go 编写的 Istio sidecar 代理，sidecar 上的 Istio agent 将附带一个由 Istiod 动态编程的缓存 DNS 代理。来自应用程序的 DNS 查询会被 pod 或 VM 中的 Istio 代理透明地拦截和服务，该代理会智能地响应 DNS 查询请求，可以实现虚拟机到服务网格的无缝多集群访问。\n新增了 WorkloadGroup ，它描述了工作负载实例的集合。提供了一个规范，工作负载实例可以用来引导它们的代理，包括元数据和身份。它只打算与虚拟机等非 Kubernetes 工作负载一起使用，旨在模仿现有的用于 Kubernetes 工作负载的sidecar注入和部署规范模型来引导 Istio 代理。\n在 Tetrate ，我们在客户的多集群部署中广泛使用这种机制，以使 sidecar 能够为暴露在网格中所有集群的入口网关的主机解析 DNS，并通过 mTLS 访问。\n总结 总而言之，Istio 团队履行了年初的承诺 ，自 2018 年发布 1.1 版本发布起，保持了固定的发布节奏，每 3 个月发布一个版本，在性能、用户体验上持续优化，以满足 brownfiled 应用与 greenfield 应用在 Istio 上的无缝体验。我们期待 Istio 在 2021 年可以给我们带来更多惊喜。\n最后，感谢马若飞 对本文的审阅。\n","relpermalink":"/blog/istio-18-release/","summary":"Istio 信守了年初的承诺，从1.1开始，几乎每三个月一个版本，更能体会用户的需求了。此次是2020年的最后一个版本，引入了 WorkloadGroup 和 DNS proxy，对如虚拟机的非 Kubernetes 负载的支持更进了一步。","title":"Istio 1.8——用户至上的选择"},{"content":"Istio 是目前最流行的服务网格，用于连接、保护、控制和观察服务。当其 2017 年开源时，Kubernetes 已赢得容器编排之战，Istio 为了满足组织转向微服务的需求。虽然 Istio 声称支持异构环境，如 Nomad、Consul、Eureka、Cloud Foundry、Mesos 等，但实际上，它一直与 Kubernetes 合作得最好–它的服务发现就是基于 Kubernetes。\nIstio 在发展初期就因为一些问题而饱受诟病，比如组件数量多、安装和维护复杂、调试困难、由于引入了太多的新概念和对象（多达 50 个 CRD）而导致学习曲线陡峭，以及 Mixer 组件对性能的影响。但这些问题正在被 Istio 团队逐渐克服。从 2020 年初发布的路线图 中可以看出，Istio 已经取得了长足的进步。\n将基于虚拟机的工作负载更好地集成到服务网格中，是 Istio 团队今年的一大重点。Tetrate 还通过其产品 Tetrate Service Bridge 提供了无缝的多云连接、安全性和可观察性，包括针对虚拟机的。本文将带您了解为什么 Istio 需要与虚拟机整合，以及如何整合。\nIstio 为什么要支持虚拟机？ 虽然现在容器和 Kubernetes 已经被广泛使用，但仍然有很多部署在虚拟机上的服务和 Kubernetes 集群之外的 API 需要由 Istio mesh 来管理。如何将棕地环境与绿地环境统一管理，这是一个巨大的挑战。\n将虚拟机引入到网格中需要具备什么条件？ 在介绍如何集成虚拟机之前，我先介绍一下将虚拟机添加到 Mesh 中需要什么条件。在支持虚拟机流量时，Istio 必须知道几件事：哪些虚拟机的服务要添加到 Mesh 中，以及如何访问虚拟机。每个虚拟机还需要一个身份，以便与服务网格的其他部分安全地通信。这些需求可以和 Kubernetes CRD 一起工作，也可以和 Consul 这样的完整的服务注册表一起工作。而基于服务账户的身份引导机制，为没有平台身份的虚拟机分配工作负载身份。对于有平台身份的虚拟机（如 EC2、GCP、Azure 等），Istio 正在进行这方面的工作，将平台身份与 Kubernetes 身份进行交换，方便设置 mTLS 通信。\nIstio 如何支持虚拟机？ Istio 对虚拟机的支持始于其服务注册表机制。Istio mesh 中的服务和实例信息来自 Istio 的服务注册表，到目前为止，Istio 的服务注册表只关注或跟踪 pod。在新的版本中，Istio 现在有资源类型来跟踪和观察虚拟机。网格内的 sidecar 无法观察和控制网格外服务的流量，因为它们没有任何信息。\nIstio 社区和 Tetrate 在 Istio 对虚拟机的支持上做了很多工作 。1.6 版本中增加了 WorkloadEntry，它允许你像描述 Kubernetes 中运行的主机一样描述虚拟机。在 1.7 版本中，该版本开始增加了通过令牌将虚拟机自动引导到 service mesh 中的基础，Istio 做了大量的工作。Istio 1.8 将首次推出另一个名为 WorkloadGroup 的抽象，它类似于 Kubernetes Deployment 对象 —— 但适用于虚拟机。\n下图显示了 Istio 如何在网格中对服务进行建模。最主要的信息来源来自于 Kubernetes 这样的平台服务注册表，或者 Consul 这样的系统。此外，ServiceEntry 作为用户定义的服务注册表，对虚拟机上的服务或组织外部的服务进行建模。\nIstio 中的服务注册发现模型 为什么不直接使用 ServiceEntry 引入虚拟机中的服务，却还要大费周折在虚拟机中安装 Istio？\n使用 ServiceEntry，你可以让网格内部的服务发现和访问外部服务；此外，还可以管理这些外部服务的流量。结合 VirtualService，你还可以为相应的外部服务配置访问规则，比如请求超时、故障注入等，从而实现对指定外部服务的控制访问。 即便如此，它也只能控制客户端的流量，而不能控制引入的外部服务对其他服务的访问。也就是说，它不能控制作为调用发起者的服务的行为。在虚拟机中部署 sidecar，通过工作负载选择器引入虚拟机工作负载，可以像 Kubernetes 中的 pod 一样，对虚拟机进行无差别管理。\nDemo 在下面这个 demo 中我们将使在 GKE 中部署 Istio 并运行 bookinfo 示例，其中 ratings 服务的后端使用的是部署在虚拟机上的 MySQL，该示例可以在 Istio 官方文档 中找到，我作出了部分改动，最终的流量路由如下图所示。\nBookinfo 示例中的流量示意图 安装流程 下面是示例的安装步骤：\n在 Google Cloud 中部署 Kubernetes 集群，Kubernetes 版本是 1.16.13； 在 GKE 中安装 Istio 1.7.1； 在 Google Cloud 中启动一台虚拟机并配置 Istio，将其加入到 Istio Mesh 中，这一步需要很多手动操作，生成证书、创建 token、配置 hosts 等； 在 Istio Mesh 中部署 bookinfo 示例； 在虚拟机中安装 MySQL； 为虚拟机设置 VPC 防火箱规则； 将虚拟机中的 MySQL 服务作为 ServiceEntry 引入到 Mesh 中并作为 rating 服务的后端； 修改 MySQL 表中的数据，验证 bookinfo 中的 rating 相应的行为符合预期； 未来方向 从 bookinfo 的演示中可以看出，在这个过程中涉及到的人工工作太多，很容易出错。在未来，Istio 会改进虚拟机测试的可操作性，根据平台身份自动引导，改进 DNS 支持和 istioctl 调试等。大家可以关注 Istio 环境工作组 ，了解更多关于虚拟机支持的细节。\n参考阅读 Virtual Machine Installation Virtual Machines in Single-Network Meshes Istio: Bringing VMs into the Mesh (with Cynthia Coan) Bridging Traditional and Modern Workloads ","relpermalink":"/blog/how-to-integrate-virtual-machines-into-istio-service-mesh/","summary":"将基于虚拟机的工作负载更好地集成到服务网格中，是 Istio 团队今年的一大重点。Tetrate 还通过其产品 Tetrate Service Bridge 提供了无缝的多云连接、安全性和可观察性，包括针对虚拟机的。本文将带您了解为什么 Istio 需要与虚拟机整合，以及如何整合。","title":"如何在 Istio Service Mesh 中集成虚拟机？"},{"content":"为什么写这篇文章 看到这个标题后，大家可能会问“都已经 2020 年了，Kubernetes 开源有 6 年时间了，为什么还要写一篇 Kubernetes 入门的文章？”我想说的是，Kubernetes 还远远没有达到我们想象的那么普及。众多的开发者，平时忙于各自的业务开发，学习新技术的时间有限；还有大量的学生群体，可能还仅仅停留在“知道有这门技术”的阶段，远远没有入门。这篇文章将助于各位有志于从事云原生领域工作或需要了解该领域背景的人群快速入门 Kubernetes 和云原生。\n因为云原生的知识体系过于庞杂，本文主要讲解容器、Kubernetes 及服务网格的入门概念，关于云原生的更多细节将在后续文章中推出。\n引言 Kubernetes 一词来自希腊语，意思是“飞行员”或“舵手”。这个名字很贴切，Kubernetes 可以帮助你在波涛汹涌的容器海洋中航行。\nKubernetes 是做什么的？什么是 Docker？什么是容器编排？Kubernetes 是如何工作和扩展的？你可能还有很多其他的问题，本文将一一为你解答。\n这篇文章适合初学者，尤其是那些工作忙碌，没有办法抽出太多时间来了解 Kubernetes 和云原生的开发者们，希望本文可以帮助你进入 Kubernetes 的世界。\n简而言之，Kubernetes 提供了一个平台或工具来帮助你快速协调或扩展容器化应用，特别是在 Docker 容器。让我们深入了解一下这些概念。\n容器和容器化 那么什么是容器呢？\n要讨论容器化首先要谈到虚拟机 (VM)，顾名思义，虚拟机就是可以远程连接的虚拟服务器，比如 AWS 的 EC2 或阿里云的 ECS。\n接下来，假如你要在虚拟机上运行一个网络应用——包括一个 MySQL 数据库、一个 Vue 前端和一些 Java 库，在 Ubuntu 操作系统 (OS) 上运行。你不用熟悉其中的每一个技术——你只要记住，一个应用程序由各种组件、服务和库组成，它们运行在操作系统上。\n现在，将应用程序打包成一个虚拟机镜像，这个镜像中包括了 Ubuntu 操作系统。这使得虚拟机变得非常笨重——通常有几个 G 的大小。\n虚拟机镜像包含了整个操作系统及所有的库，对应用程序来说，这个镜像过于臃肿，其中大部分组件并没有被应用程序直接调用。如果你需要重新创建、备份或扩展这个应用程序，就需要复制整个环境（虚拟机镜像），在新环境中启动应用通常需要几十秒甚至几分钟时间。如果你想单独升级应用中的某个组件，比如说 Vue 应用，就需要重建整个虚拟机镜像。另外，如果你的两个应用依赖同一个底层镜像，升级底层镜像会同时影响这两个应用，而有时候，你只需要升级其中一个应用的依赖而已。这就是所谓的“依赖陷阱”。\n解决这个问题的办法就是容器。容器是继虚拟机之后更高层次的抽象，在这层抽象中，整个应用程序的每个组件被单独打包成一个个独立的单元，这个单元就是所谓的容器。通过这种方式，可以将代码和应用服务从底层架构中分离出来，实现了完全的可移植性（在任何操作系统或环境上运行应用的能力）。所以在上面的例子中，Ubuntu 操作系统就是一个单元（容器）。MySQL 数据库是另一个容器，Vue 环境和随之而来的库也是一个容器。\n但是，MySQL 数据库是如何自己“运行”的？数据库本身肯定也要在操作系统上运行吧？没错！\n更高层次的容器，比如 MySQL 容器，实际上会包含必要的库来与底层的操作系统容器通信和集成。所以你可以把容器看成是整个应用堆栈中的一层，每层都依赖于下层的单元。而这就类似于船舶或港口中集装箱的堆叠方式，每个容器的稳定性都依赖于下面的容器的支持。所以应用容器的核心是一个受控的执行环境。它们允许你从头开始定义整个环境，从操作系统开始，到你要使用的各个版本的库，再到你要添加的代码版本。\n与容器相关的一个重要概念是微服务。将应用程序的各个组件拆分并打包成独立的服务，这样每个组件都可以很容易地被替换、升级、调试。上面的例子中，我们会为 Vue 前端创建一个微服务，为 MySQL 数据库创建另一个微服务，为 Java 中间件部分创建另一个微服务，以此类推。很明显，微服务与容器化是相辅相成的。\n从 Docker 开始 现在你已经对容器有一定了解了吧？Docker 是最常用的容器化工具，也是最流行的容器运行时。\nDocker 开源于 2013 年。用于打包和创建容器，管理基于容器的应用。所有 Linux 发行版、Windows 和 macOS 都支持 Docker。\n还有其他的容器化工具，如 CoreOS rkt 、Mesos Containerizer 和 LXC 。但是目前，绝大多数的容器化应用都是在 Docker 上运行的。\n再到 Kubernetes 首先，简单介绍一下历史。Kubernetes 是 Google 基于其内部容器调度平台 Borg 的经验开发的。2014 年开源，并作为 CNCF（云原生计算基金会）的核心发起项目。\n那么 Kubernetes 又跟容器是什么关系呢？让我们再回到上面的例子。假设我们的应用爆火，每天的注册用户越来越多。\n现在，我们需要增加后端资源，使浏览我们网站的用户在浏览页面时加载时间不会过长或者超时。最简单的方式就是增加容器的数量，然后使用负载均衡器将传入的负载（以用户请求的形式）分配给容器。\n这样做虽然行之有效，但也只能在用户规模有限的情况下使用。当用户请求达到几十万或几百万时，这种方法也是不可扩展的。你需要管理几十个也许是几百个负载均衡器，这本身就是另一个令人头疼的问题。如果我们想对网站或应用进行任何升级，也会遇到问题，因为负载均衡不会考虑到应用升级的问题。我们需要单独配置每个负载均衡器，然后升级该均衡器所服务的容器。想象一下，当你有 20 个负载均衡器和每周 5 或 6 个小的更新时，你将不得不进行大量的手工劳动。\n我们需要的是一种可以一次性将变更传递给所有受控容器的方法，同时也需要一种可以轻松地调度可用容器的方法，这个过程还必须要是自动化的，这正是 Kubernetes 所做的事情。\n接下来，我们将探讨 Kubernetes 究竟是如何工作的，它的各种组件和服务，以及更多关于如何使用 Kubernetes 来编排、管理和监控容器化环境。为了简单起见，假设我们使用的是 Docker 容器，尽管如前所述，Kubernetes 除了支持 Docker 之外，还支持其他几种容器平台。\nKubernetes 架构和组件 首先，最重要的是你需要认识到 Kubernetes 利用了“期望状态”原则。就是说，你定义了组件的期望状态，而 Kubernetes 要将它们始终调整到这个状态。\n例如，你想让你的 Web 服务器始终运行在 4 个容器中，以达到负载均衡的目的，你的数据库复制到 3 个不同的容器中，以达到冗余的目的。这就是你想要的状态。如果这 7 个容器中的任何一个出现故障，Kubernetes 引擎会检测到这一点，并自动创建出一个新的容器，以确保维持所需的状态。\n现在我们来定义一些 Kubernetes 的重要组件。\n当你第一次设置 Kubernetes 时，你会创建一个集群。所有其他组件都是集群的一部分。你也可以创建多个虚拟集群，称为命名空间 (namespace)，它们是同一个物理集群的一部分。这与你可以在同一物理服务器上创建多个虚拟机的方式非常相似。如果你不需要，也没有明确定义的命名空间，那么你的集群将在始终存在的默认命名空间中创建。\nKubernetes 运行在节点 (node) 上，节点是集群中的单个机器。如果你有自己的硬件，节点可能对应于物理机器，但更可能对应于在云中运行的虚拟机。节点是部署你的应用或服务的地方，是 Kubernetes 工作的地方。有 2 种类型的节点——master 节点和 worker 节点，所以说 Kubernetes 是主从结构的。\n主节点是一个控制其他所有节点的特殊节点。一方面，它和集群中的任何其他节点一样，这意味着它只是另一台机器或虚拟机。另一方面，它运行着控制集群其他部分的软件。它向集群中的所有其他节点发送消息，将工作分配给它们，工作节点向主节点上的 API Server 汇报。\nMaster 节点本身也包含一个名为 API Server 的组件。这个 API 是节点与控制平面通信的唯一端点。API Server 至关重要，因为这是 worker 节点和 master 节点就 pod、deployment 和所有其他 Kubernetes API 对象的状态进行通信的点。\nWoker 节点是 Kubernetes 中真正干活的节点。当你在应用中部署容器或 pod（稍后定义）时，其实是在将它们部署到 worker 节点上运行。Worker 节点托管和运行一个或多个容器的资源。\nKubernetes 中的逻辑而非物理的工作单位称为 pod。一个 pod 类似于 Docker 中的容器。记得我们在前面讲到，容器可以让你创建独立、隔离的工作单元，可以独立运行。但是要创建复杂的应用程序，比如 Web 服务器，你经常需要结合多个容器，然后在一个 pod 中一起运行和管理。这就是 pod 的设计目的——一个 pod 允许你把多个容器，并指定它们如何组合在一起来创建应用程序。而这也进一步明确了 Docker 和 Kubernetes 之间的关系——一个 Kubernetes pod 通常包含一个或多个 Docker 容器，所有的容器都作为一个单元来管理。\nKubernetes 中的 service 是一组逻辑上的 pod。把一个 service 看成是一个 pod 的逻辑分组，它提供了一个单一的 IP 地址和 DNS 名称，你可以通过它访问服务内的所有 pod。有了服务，就可以非常容易地设置和管理负载均衡，当你需要扩展 Kubernetes pod 时，这对你有很大的帮助，我们很快就会看到。\nReplicationController 或 ReplicaSet 是 Kubernetes 的另一个关键功能。它是负责实际管理 pod 生命周期的组件——当收到指令时或 pod 离线或意外停止时启动 pod，也会在收到指示时杀死 pod，也许是因为用户负载减少。所以换句话说，ReplicationController 有助于实现我们所期望的指定运行的 pod 数量的状态。\n什么是 Kubectl？ kubectl 是一个命令行工具，用于与 Kubernetes 集群和其中的 pod 通信。使用它你可以查看集群的状态，列出集群中的所有 pod，进入 pod 中执行命令等。你还可以使用 YAML 文件定义资源对象，然后使用 kubectl 将其应用到集群中。\nKubernetes 中的自动扩展 请记住，我们使用 Kubernetes 而不是直接使用 Docker 的原因之一，是因为 Kubernetes 能够自动扩展应用实例的数量以满足工作负载的需求。\n自动缩放是通过集群设置来实现的，当服务需求增加时，增加节点数量，当需求减少时，则减少节点数量。但也要记住，节点是 “物理” 结构——我们把“物理”放在引号里，因为要记住，很多时候，它们实际上是虚拟机。\n无论如何，节点是物理机器的事实意味着我们的云平台必须允许 Kubernetes 引擎创建新机器。各种云提供商对 Kubernetes 支持基本都满足这一点。\n我们再继续说一些概念，这次是和网络有关的。\n什么是 kubernetes Ingress 和 Egress？ 外部用户或应用程序与 Kubernetes pod 交互，就像 pod 是一个真正的服务器一样。我们需要设置安全规则允许哪些流量可以进入和离开“服务器”，就像我们为托管应用程序的服务器定义安全规则一样。\n进入 Kubernetes pod 的流量称为 Ingress，而从 pod 到集群外的出站流量称为 egress。我们创建入口策略和出口策略的目的是限制不需要的流量进入和流出服务。而这些策略也是定义 pod 使用的端口来接受传 …","relpermalink":"/blog/must-read-for-cloud-native-beginner/","summary":"这篇文章将助于各位有志于从事云原生领域工作或需要了解该领域背景的人群快速入门 Kubernetes 和云原生。","title":"云原生初学者入门必读"},{"content":"今天是我加入蚂蚁的第 914 天，也是在蚂蚁 的最后一天，明天就是 9 月 1 日了，这一天通常都是学校开学的日子，在阿里巴巴大家都以”同学”相称，明天我将加入 Tetrate ，这也算开始我的新学期吧！\n蚂蚁/阿里巴巴与云原生社区 迄今为止，蚂蚁/阿里巴巴集团对我的职业生涯产生了深远影响，尤其是它的企业文化和价值观，阿里巴巴招聘的理念的“寻找同路人”，在创建云原生社区的过程，不也是寻找同路人的过程吗？云原生社区 就像一个小型社会，我不求它有多大的社会价值，只求它可以对个人、对企业、对社会带来微小而美好的改变。我不断得思考作为个人、员工。尤其是社区的发起人，我的使命到底是什么？我在公司中应该担当什么样的角色？这个社区要走向何方？我在摸索中前进，但是因为有你们的支持，使我更加坚定，致力于云原生技术在中国的普及和应用，以外我一个人可能走得更快，但现在与社区在一起，我们将走得更远！\n2019 年 6 月 24 日，上海，KubeCon China 2019 2019 年 6 月 24 日，上海，KubeCon China 2019\n加入 Tetrate 在过去的两年里，我一直在着力推广 Istio 和 Service Mesh 技术，在蚂蚁集团的资助下，我创办了 ServiceMesher 社区 ，将 Service Mesh 技术带到了中国，接下来我希望将中国实践带到世界。当然还有今年疫情期间成立的云原生社区 ，向开发者和大众普及云原生知识和应用。\n作为 Developer Advocate，最重要的一点是不要停止学习，同时要善于倾听和总结。在过去的两年里，我看到无数人对 Service Mesh 表现出浓厚的兴趣，但因对新技术的风险了解的不足及知识匮乏而无从下手。我十分兴奋加入这家专注于 Service Mesh 的初创公司 Tetrate ，这是一家全球化远程办公的初创公司，公司的产品围绕开源 Istio 、Envoy 和 Apache SkyWalking 等开源项目构建，致力于打造云原生的网络基础设施。这里有这些开源项目的多位 Maintainer，如 吴晟 、Zack Butcher 、周礼赞 等，我相信跟他们一起，可以帮助大家快速、有效的了解和应用 Service Mesh，跨向云原生。\n写在最后 今年年初在筹备云原生社区的时候，我就确定了未来三年内的工作方向——云原生、开源和社区。在追求梦想的道路上充满荆棘，不仅需要勇气和毅力，还需要你们做我坚强的后盾，我一定披荆斩棘，一往无前。开源是世界的，要想让世界更理解我们，我们必须更加主动地融入这个世界。希望中国开源的明天会更好，希望 Service Mesh 技术在中国更好的落地，希望云原生能够普惠大众，希望大家都可以找到自己的使命。\nTetrate 目前也在招聘 中，欢迎投递简历。\n","relpermalink":"/blog/moving-on-from-ant-group/","summary":"今天是我在蚂蚁的最后一天，明天我就要在 Tetrate 开始新的学期了。","title":"新的开始——告别蚂蚁，加入 Tetrate"},{"content":"就在今晚，jimmysong.io 网站迁移到了阿里云香港节点，此举是为了进一步优化用户体验，提升访问速度。我在阿里云香港节点上购买了一台 ECS，现在我拥有一个公网 IP 还可以设置子域名了，之前网站是部署在 GitHub Pages 上，访问速度一般，而且还要承受 GitHub 不稳定带来的影响（近年来 GitHub 的宕机事件时有发生）。\n同时，近日网站博客也做了大量改进，感谢 白俊遥 @baijunyao 的大力支持，为网站改版做了大量工作，包括：\n修改了主题配色，对比加深 使用 aligolia，支持全站搜索 优化了移动端显示 博客中的文章增加了放大功能 在博客文章中增加了目录 本站是基于 educenter 主题构建。\n感谢广大网友几年来对本网站的支持，网站自上线以来已三年有余，拥有几百万的访问量，前后做过两次重大改版，分别是 2020 年 1 月 31 日 和 2017 年 10 月 8 日，更换了网站的主题。将来我会一如既往给大家分享更多云原生内容，欢迎大家收藏，转发，也欢迎加入云原生社区 和广大云原生开发者一起交流。\n","relpermalink":"/notice/migrating-to-alibaba-cloud/","summary":"将网站迁移到阿里云香港节点，提高网站访问速度，并获取公网 IP、子域名的便利。","title":"迁移到阿里云香港节点"},{"content":"Kubernetes 自开源至今已经走过六个年头了，云原生时代 也已到来，我关注云原生领域也四年有余了，最近开始思考云原生的未来走向，特此撰写本文作为《云原生应用白皮书》 的开篇，更多关于云原生应用的介绍请转到白皮书中浏览。\n重点 云原生基础设施已渡过了野蛮生长期，正朝着统一应用标准方向迈进。 Kubernetes 的原语无法完整描述云原生应用体系，且在资源的配置上开发与运维功能耦合严重。 Operator 在扩展了 Kubernetes 生态的同时导致云原生应用碎片化，亟需一个统一的应用定义标准。 OAM 的本质是将云原生应用定义中的研发、运维关注点分离，资源对象进行进一步抽象，化繁为简，包罗万象。 “Kubernetes 次世代”是指在 Kubernetes 成为基础设施层标准之后，云原生生态的关注点正在向应用层过度，近两年来火热的 Service Mesh 正是该过程中的一次有力探索，而基于 Kubernetes 的云原生应用架构的时代即将到来。 Kubernetes 已成为云原生应用的既定运行平台，本文以 Kubernetes 为默认平台展开，包括云原生应用的分层模型。\n云原生的不同发展阶段 Kubernetes 从开源至今已经走过快六个年头 （2014 年 6 月开源）了，可以说是 Kubernetes 的诞生开启了整个云原生的时代。我粗略的将云原生的发展划分为以下几个时期。\n云原生的发展阶段 第一阶段：孵化期（2014 年）\n2014 年，Google 开源 Kubernetes，在此之前的 2013 年，Docker 开源，DevOps、微服务已变得十分流行，云原生的概念已经初出茅庐。在开源了 Kubernetes 之后，Google 联合其他厂商发起成立了 CNCF，并将 Kubernetes 作为初创项目捐献给了 CNCF。CNCF 作为云原生的背后推手，开始推广 Kubernetes。\n第二阶段：高速发展期（2015 年 - 2016 年）\n这几年间，Kubernetes 保持着高速发展，并于 2017 年打败了 Docker Swarm、Mesos，确立了容器编排工具领导者的地位。CRD 和 Operator 模式的诞生，大大增强了 Kubernetes 的扩展性，促进了周边生态的繁荣。\n第三阶段：野蛮生长期（2017 年 - 2018 年）\n2016 年之后的云原生基本都默认运行在 Kubernetes 平台上，2017、2018 年 Google 主导的 Istio、Knative 相继开源，这些开源项目都大量利用了 Kubernetes 的 Operator 进行了扩展，Istio 刚发布时就有 50 多个 CRD 定义。Istio 号称是后 Kubernetes 时代的微服务 ，它的出现第一次使得云原生以服务（应用）为中心。Knative 是 Google 在基于 Kubernetes 之上开源的 Serverless 领域的一次尝试。2018 年 Kubernetes 正式从 CNCF 毕业 ，Prometheus、Envoy 也陆续从 CNCF 毕业。CNCF 也与 2018 年修改了 charter，对云原生进行了重定义，从原来的三要素：”应用容器化；面向微服务架构；应用支持容器的编排调度“，修改为”云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API“。这一年，我曾写过两篇 Kubernetes 及云原生发展的年终总结和展望，见 2017 年 和 2018 年 的预测和总结。\n第四阶段：普及推广期（2019 年至今）\n经过几年的发展，Kubernetes 已经得到的大规模的应用，云原生的概念开始深入人心，Kubernetes 号称是云原生的操作系统，基于 Operator 模式的生态大放异彩。整合 Kubernetes 和云基础设施，研发和运维关注点分离。Kubernetes 到 Service Mesh（后 Kubernetes 时代的微服务），基于 Kubernetes 的 Serverless 都在快速发展，OAM 诞生，旨在定义云原生应用标准。\nKubernetes 开辟了云原生时代 Kubernetes 开源之初就继承了 Google 内部调度系统 Borg 的经验，屏蔽掉了底层物理机、虚拟机之间的差异，经过几年时间的发展成为了容器编排标准，进而统一了 PaaS 平台的基础设施层。\n下图是Kubernetes 原生内置的可以应用到一个 Pod 上的所有控制器、资源对象等。\nKubernetes 概念 图片来自图书 Kubernetes Patterns（O’Reilly） Kubernetes 作为云原生基础设施设计之初遵循了以下原则：\n基础设施即代码（声明式 API） 不可变基础设施 幂等性 调节器模式（Operator 的原理） 其中声明式 API 可谓开创了云原生时代的基调，而调节器模式是 Kubernetes 区别于其他云部署形式 的主要区别之一，这也为后来的 Operator 框架的诞生 打下了基础。\n声明式 API 根据声明式 API 可以做应用编排，定义组件间的依赖，通常使用人类易读的 YAML 文件来表示。但是，YAML 文件声明的字段真的就是最终的状态吗？有没有可能动态改变？\n我们在创建 Deployment 时会指定 Pod 的副本数，但是其实际副本数并不一定是一成不变的。假如集群中还有定义 HPA，那么 Pod 的副本数就可能随着一些外界因素（比如内存、CPU 使用率或者自定义 metric）而改变，而且如果集群中还有运行自定义的控制器话，那么也有可能修改应用的实例数量。在有多个控制器同时控制某个资源对象时，如何确保控制器之间不会发生冲突，资源对象的状态可预期？可以使用动态准入控制 来达到这一点。\nKubernetes 原生应用 我们都知道要想运行一个应用至少需要以下几点：\n应用的业务逻辑（代码）、运行时（可运行的二进制文件、字节码或脚本）。 应用的配置注入（配置文件、环境变量等），身份、路由、服务暴露等满足应用的安全性和可访问性。 应用的生命周期管理（各种 Controller 登场）。 可观察性、可运维、网络和资源及环境依赖、隔离性等。 下图展示了基于 Kubernetes 原语及 PaaS 平台资源的 Kubernetes 原生应用的组成。\nKubernetes 原生应用 我们都知道 Kubernetes 提供了大量的原语 ，用户可以基于这些原语来编排服务，管理应用的生命周期。上图展示的是基于 Kubernetes 原生应用可以使用的 Kubernetes 原语、扩展及平台层资源，从内向外的对象跟应用程序（业务逻辑）的关联度依次降低，到最外层基本只剩下平台资源依赖，已经与 Kubernetes 几乎没有关系了。该图里仅展示了部分资源和对象（包含阿里巴巴开源的 OpenKruise 、Istio），实际上 Operator 资源之丰富，也是 Kubernetes 生态如此繁荣的原因之一。\nKubernetes 本身的原语、资源对象、配置、常用的 CRD 扩展有几十、上百个之多。开发者需要了解这些复杂的概念吗？我只是想部署一个应用而已！不用所对于应用开发者，即使对于基础实施开发和运维人员也需要很陡峭的学习曲线才能完全掌握它。\n我将 Kubernetes 原生应用所需要的定义和资源进行了分层：\n核心层：应用逻辑、服务定义、生命周期控制； 隔离与服务访问层：资源限制与隔离、配置、身份、路由规则等； 调度层：各种调度控制器，这也是 Kubernetes 原生应用的主要扩展层； 资源层：提供网络、存储和其他平台资源； 而这些不同的层，完全可以将其职责分配给相应的人员，比如核心层是由应用程序开发者负责，将其职责分离，可以很大程度上降低开发和运维的复杂度。\n云原生应用落实到 Kubernetes 平台之上，仅仅利用 Kubernetes 的对象原语已很难描述一个复杂的应用程序，所以诞生了各种各样的 Operator，但这也仅仅解决了单个应用的定义，对于应用的打包封装则无能为力。\n同一个资源对象又有多种实现方式，比如 Ingress 就有 10 多种实现 ，PV 就更不用说，对于对于开发者究竟如何选择，平台如何管理，这都是让人很头疼的问题。而且有时候平台所提供的扩展能力还可能会有冲突，这些能力有的可能互不相干，有的可能会有正交，有的可能完全重合。且应用本身与运维特性之间存在太多耦合，不便于复用。\n资源交集动画 上图中不同颜色的方框代表不同的资源类别，红线框代表不能为一个资源同时应用该配置，否则会出现冲突，不同的颜色上面是一个动画，展示的是部分资源组合。图中仅包含了部分 Kubernetes 中的原语和 Istio 中的资源对象组合及自定义扩展，实际上用户可以根据应用的自身特点，基于 Kubernetes 原语和 CRD 创建出千变万化的组合。\n为了管理这些应用诞生出了众多的 Operator 。Kubernetes 1.7 版本以来就引入了自定义控制器 的概念，该功能可以让开发人员扩展添加新功能，更新现有的功能，并且可以自动执行一些管理任务，这些自定义的控制器就像 Kubernetes 原生的组件一样，Operator 直接使用 Kubernetes API进行开发，也就是说它们可以根据这些控制器内部编写的自定义规则来监控集群、更改 Pods/Services、对正在运行的应用进行扩缩容。\nOperator 的本质是一种调节器模式（Reconciler Pattern）的应用，跟 Kubernetes 本身的实现模式是一样的，用于管理云原生应用，协调应用的实际状态达到预期状态。\n调节器模式的四个原则：\n所有的输入和输出都使用数据结构。 确保数据结构是不可变的。 保持资源映射简单。 使实际状态符合预期状态。 云原生应用走向碎片化 利用声明式 API 及调节器模式，理论上可以在 Kubernetes 上部署任何可声明应用，但是在 Operator 出现之前，管理 Kubernetes 上的有状态应用一直是一个难题，随着 Operator 模式的确立，该难题已得以解决，并促进了 Kubernetes 生态的进一步发展。随着该生态的繁荣，有一种碎片化的特征正在显现。\n云原生应用碎片化的体现\nOperator 模式将运维人员的反应式经验转化成基于 Reconcile 模式的代码，统一了有状态应用的管理模式，极大得扩展了 Kubernetes 应用生态。 开发者在引用 Operator 所提供的能力时没有统一的视图，加大了基础设施运维与开发者之间的沟通成本。 Operator 总体上治理松散，没有统一的管控机制，在同时应用时可能导致互相冲突或无法预期的结果发生。 有状态应用管理难题 Kubernetes 对于无状态应用的管理很出色，但是对于有状态应用就不是那么回事了。虽然 StatefulSet 可以帮助管理有状态应用，但是这还远远不够，有状态应用往往有复杂的依赖。声明式的 API 里往往要加载着大量的配置和启动脚本，才能实现一个复杂应用的 Kubernetes 化。\n例如在 2017 年初，Operator Framework 出现之前，需要使用大量的 ConfigMap、复杂的启动脚本才能在 Kubernetes 上定义 Hadoop YARN 和运行 Spark 。虽然 StatefulSet 号称可以解决有状态应用的部署问题，但是它主要是保证了 Pod 的在启动、伸缩时的顺序和使 Pod 具有稳定的标识。但是很多分布式应用来说并不仅依靠启动顺序就可以保证其状态，根据其在分布式应用中的角色不同（master/worker）而需要有大量的自定义配置，在没有 Operator 之前这些配置通常是通过一些自定义脚本来实现，这些脚本可能存在于应用镜像中，也 …","relpermalink":"/blog/post-kubernetes-era/","summary":"Kubernetes 的次世代在于解决 Kubernetes 生态的碎片化问题。","title":"Kubernetes 次世代的云原生应用"},{"content":" 编译一次，到处运行 就在前几天，Java 刚度过了它的 25 周岁生日 ，从它诞生时就在号称”一次编写，到处运行“，但是 20 多年过去了，从程序编写出来到真正交付生产还是有很深的 gap。IT 的世界从来就不缺概念，如果一个概念无法解决问题，那就再来一层概念。Kubernetes 诞生至今也有6 年时间了，已经来到了后 Kubernetes 时代了——云原生应用时代！\n云原生应用的发展阶段 这本白皮书将带您探索后 Kubernetes 时代的云原生应用发展路径。\n其中的重点传达的观点包括：\n云原生已渡过了野蛮生长期，正朝着统一应用标准方向迈进。 Kubernetes 的原语无法完整描述云原生应用体系，且在对资源的配置上开发与运维功能耦合严重。 Operator 在扩展了 Kubernetes 生态的同时导致云原生应用碎片化，亟需一个统一的应用定义标准。 OAM 的本质是将云原生应用的定义中的研发、运维关注点分离，资源对象进行进一步抽象，化繁为简，包罗万象。 “Kubernetes 次世代”是指在 Kubernetes 成为基础设施层标准之后，云原生生态的关注点正在向应用层过度，近两年来火热的 Service Mesh 正式该过程中的一次有力探索，而基于 Kubernetes 之上的云原生应用架构的时代即将到来。 Kubernetes 已成为云原生应用的既定运行平台，本白皮书将以 Kubernetes 为默认平台展开，包括基于 OAM 的云原生应用的分层模型的解释。\n","relpermalink":"/notice/guide-to-cloud-native-app/","summary":"带您探索后 Kubernetes 时代的云原生应用发展路径","title":"推出云原生应用白皮书"},{"content":"云原生领域近几年来群英荟萃，但缺乏一个企业中立的有组织有纪律的团体把大家团结在一起。我和我的朋友们在疫情期间，利用业余时间共同发起了这个云原生社区，希望能服务于全球华人群体，共同致力于云原生事业。社区官网 https://cloudnative.to 。\n2020 年伊始，受新冠疫情影响，全球各地的员工开启了在家办公的模式，因此人与人之间的距离感觉被拉远了。但是云原生圈子里有我们这样一群人，因为一个共同的愿景聚集到了一起，组建了社区管理委员会，并在过去的三个月里利用业余时间，齐心协力完成了社区的筹备工作。今天我们要正式宣布云原生社区正式成立了。\n关于云原生社区 云原生社区是一个有技术、有温度、有情怀的开源社区。由一群开源的狂热爱好者自发成立，秉持“共识、共治、共建、共享”的原则。社区的宗旨是：连接、中立、开源。立足中国，面向世界，企业中立，关注开源，回馈开源。\n关于云原生社区管理委员会介绍请见团队介绍页面 。\n加入云原生社区，你将获得：\n更接近源头的知识资讯 更富有价值的人际网络 更专业个性的咨询解答 更亲近意见领袖的机会 更快速高效的个人成长 更多知识分享曝光机会 更多行业人才挖掘发现 ","relpermalink":"/notice/cloud-native-community-announecement/","summary":"我和我的朋友们发起了有技术、有温度、有情怀的云原生社区。","title":"云原生社区成立"},{"content":"OAM（Open Application Model） 是阿里巴巴和微软共同开源的云原生应用规范模型，同时开源了基于 OAM 的实现 Rudr ，自 2019 年 10 月宣布开源以来截止本文发稿已经有快半年时间了。\n当前可能大部分人才刚刚开始了解 OAM，所以这篇文章将从最基础出发，为大家介绍 OAM 的诞生背景和要解决的问题，以及它在云原生生态中的作用。\nTakeaways 如果你没有兴趣或者时间阅读下面的全文，那么建议阅读下面这些核心观点：\nOAM 的本质是根据软件设计的“兴趣点分离”原则对负责的 DevOps 流程的高度抽象和封装，这背后还是“康威定律”在起作用。 OAM 仅定义云原生应用的规范，目前推出的 Rudr 可以看做是 OAM 规范的 Kubernetes 解释器（实验实现），将云原生应用定义翻译成 Kubernetes 的资源对象。 OAM 与 Crossplane 将展开合作，就 Kubernetes 式以 API 为中心的应用定义发扬光大，并深度参与 CNCF SIG App Delivery ，以共同定义云原生应用标准。 康威定律（Conway’s Law）\n康威定律 是马尔文·康威（Melvin Conway）1967年提出的： “设计系统的架构受制于产生这些设计的组织的沟通结构。”\nOAM 简介 OAM 全称是 Open Application Model，从名称上来看它所定义的就是一种模型，同时也实现了基于 OAM 的我认为这种模型旨在定义了云原生应用的标准。\n开放（Open）：支持异构的平台、容器运行时、调度系统、云供应商、硬件配置等，总之与底层无关 应用（Application）：云原生应用 模型（Model）：定义标准，以使其与底层平台无关 顺便说下 CNCF 中的也有几个定义标准的「开源项目」，其中有的项目都已经毕业。\nSMI（Service Mesh Interface） ：服务网格接口 Cloud Events ：Serverless 中的事件标准 TUF ：更新框架标准 SPIFFE ：身份安全标准 这其中唯独没有应用标准的定义，CNCF SIG App delivery 即是要做这个的。当然既然要制定标准，自然要对不同平台和场景的逻辑做出更高级别的抽象（这也意味着你在掌握了底层逻辑的情况下还要学习更多的概念），这样才能屏蔽底层差异。本文将默认底层平台为 Kubernetes。\n是从管理大量 CRD 中汲取的经验。 业务和研发的沟通成本，比如 YAML 配置中很多字段是开发人员不关心的。 OAM 基本对象 OAM 模型中包含以下基本对象，以本文发稿时的最新 API 版本 core.oam.dev/v1alpha2 为准：\nComponent ：OAM 中最基础的对象，该配置与基础设施无关，定义负载实例的运维特性。例如一个微服务 workload 的定义。 TraitDefinition ：一个组件所需的运维策略与配置，例如环境变量、Ingress、AutoScaler、Volume 等。（注意：该对象在 apiVersion: core.oam.dev/v1alpha1 中的名称为 Trait）。 ScopeDefinition ：多个 Component 的共同边界。可以根据组件的特性或者作用域来划分 Scope，一个 Component 可能同时属于多个 Scope。 ApplicationConfiguration ：将 Component（必须）、Trait（必须）、Scope（非必须）等组合到一起形成一个完整的应用配置。 OAM API 的演变 因为 OAM 还处在发展早起，API 变化较快，以上四个对象在不同的 API 版本中的 kind 名称不同，请大家使用时注意区别。\n名称 core.oam.dev/v1alpha1 core.oam.dev/v1alpha2 Component ComponentSchematic Component Trait Trait TraitDefinition Scope Scope ScopeDefinition Application configuration ApplicationConfiguration ApplicationConfiguration 总的来说，OAM 模型对象的定义格式与 Kubernetes 对象的类型字段 相似。关于 OAM 的基本概念模型的更多信息请访问 Overview and Terminology 。\nOAM 工作原理 下图来自阿里云原生应用平台团队孙健波在 《OAM:云原生时代的应用模型与 下一代 DevOps 技术》 中的分享，OAM 的工作原理如下图所示，OAM Spec 定义了云原生应用的规范（使用一些列 CRD 定义）， Rudr 可以看做是 OAM 规范的解析器，将应用定义翻译为 Kubernetes 中的资源对象。\nOAM 的原理 可以将上图分为三个层次：\n汇编层：即人工或者使用工具来根据 OAM 规范定义汇编出一个云原生应用的定义，其中包含了该应用的工作负载和运维能力配置。 转义层：汇编好的文件将打包为 YAML 文件，由 Rudr 或其他 OAM 的实现将其转义为 Kubernetes 或其他云服务（例如 Istio）上可运行的资源对象。 执行层：执行经过转义好的云平台上的资源对象并执行资源配置。 Rudr Rudr 是对 OAM v1alpha1 在 Kubernetes 环境下的实现，OAM 正在与 Crossplane 合作\nCrossplane\n使用 Kubernetes 社区开创的以 API 为中心的声明式配置和自动化方法，使基础设施和应用管理标准化。官方网站：https://crossplane.io/ 。\n安装 Rudr 请参考 Rudr 文档 安装，主要依赖以下组件：\nkubectl helm 3 Kubernetes 1.15+ 执行下面的命令安装 Rudr 和需要的 trait。\n# 克隆项目 git clone https://github.com/oam-dev/rudr.git cd rudr # 创建一个名为 oam 的 namespace kubectl create namespace oam # 安装 Rudr helm install rudr ./charts/rudr --wait -n oam # 要使用 ingress trait，推荐安装 Nginx ingress helm repo add stable https://kubernetes-charts.storage.googleapis.com/ helm install nginx-ingress stable/nginx-ingress # 要使用 autoscaler trait，安装 HorizontalPodAutoscaler helm repo add kedacore https://kedacore.github.io/charts helm repo update helm install keda kedacore/keda -n oam 查看当前 oam namespace 下的所有 pod，你会发现已创建了以下 pod。\n$ kubectl get pod -n oam NAME READY STATUS RESTARTS AGE keda-operator-b6466c989-pn25n 1/1 Running 0 63m keda-operator-metrics-apiserver-6cf88c468-k5wd8 1/1 Running 0 63m nginx-ingress-controller-787bd69d8-n6v8c 1/1 Running 15 7d nginx-ingress-default-backend-7c868597f4-vvddn 1/1 Running 2 7d rudr-c648c9b7b-knj9b 1/1 Running 7 7d 部署示例 我们使用 OAM 官方提供的教程 Tutorial: Deploy, inspect, and update a Rudr application and its components 中的 Python flask 示例，该示例基于 OAM v1alpha1 API，最新版 API 的示例可以参考 crossplane-oam-sample 。\n# 部署 Component kubectl apply -f examples/helloworld-python-component.yaml 此时 get pod 会发现并没有创建任何新的 pod，因为 examples/helloworld-python-component.yaml 文件中只定义了一个名为 helloworld-python-v1 的 ComponentSchematic，但是 ComponentSchematic 是仅仅是定义了一个组件而已，还无法直接创建 pod 的，还需要创建一个 ApplicationConfiguration 将其与 Trait 绑定才可以创建应用的 pod。\n关于该示例的详细信息请参考 Python flask 示例 的创建步骤。\n创建应用配置 在部署了 ComponentSchematic 之后我们还需要创建一个 ApplicationConfiguration 将其与 Trait 资源绑定才可以创建应用。\n当前已有的 Trait\n在安装 Rudr 时已在 oam namespace 中部署了一些 trait，使用下面的命令查看。\n$ kubectl get trait -n oam NAME AGE auto-scaler 7d1h empty 7d1h ingress 7d1h manual-scaler 7d1h volume-mounter 7d1h 在 examples/first-app-config.yaml 中将 ComponentSchematic 与 ingress Trait 联系起来。一个完整的可部署的应用配置 examples/first-app-config.yaml 的内容如下所示：\napiVersion: core.oam.dev/v1alpha1 kind: ApplicationConfiguration metadata: name: first-app spec: components: - componentName: helloworld-python-v1 # 引用了上文中的 Component instanceName: first-app-helloworld-python-v1 parameterValues: - name: target value: Rudr - name: port value: \u0026#39;9999\u0026#39; traits: - name: ingress # Ingress 引用，Rudr 已默认创建 properties: hostname: example.com path: / servicePort: 9999 执行下面的命令部署应用。\nkubectl apply -f examples/first-app-config.yaml -n oam 若此时查看 oam namespace 下的 pod 将发现有一个新的 pod 创建。\n$ kubectl get pod -o oam NAME READY STATUS RESTARTS AGE first-app-helloworld-python-v1-69945684c7-wfd82 1/1 Running 0 16m ... 测试 执行下面的命令可以测试刚安装的应用。\n# 将 Python flask 应用的 pod …","relpermalink":"/blog/oam-intro/","summary":"本文是对 OAM 及 Rudr 的初探，主要介绍了 OAM 诞生的背景和要解决的问题，同时介绍了它在云原生生态中的作用。","title":"OAM（开放应用模型）——定义云原生应用标准的野望"},{"content":"如果你刚听说 Service Mesh 不久，并试用过 Istio 的话，那么你可能都会有下面几个疑问：\n为什么 Istio 要运行在 Kubernetes 上呢？ Kubernetes 和 Service Mesh 分别在云原生中扮演什么角色？ Istio 扩展了 Kubernetes 的哪些方面？解决了哪些问题？ Kubernetes、xDS 协议（Envoy 、MOSN 等）与 Istio 之间又是什么关系？ 到底该不该上 Service Mesh？ 这一节我们将试图带您梳理清楚 Kubernetes、xDS 协议以及 Istio Service Mesh 之间的内在联系。此外，本节还将介绍 Kubernetes 中的负载均衡方式，xDS 协议对于 Service Mesh 的意义以及为什么说及时有了 Kubernetes 还需要 Istio。\n使用 Service Mesh 并不是说与 Kubernetes 决裂，而是水到渠成的事情。Kubernetes 的本质是通过声明式配置对应用进行生命周期管理，而 Service Mesh 的本质是提供应用间的流量和安全性管理以及可观察性。假如你已经使用 Kubernetes 构建了稳定的应用平台，那么如何设置服务间调用的负载均衡和流量控制？\nEnvoy 创造的 xDS 协议被众多开源软件所支持，如 Istio 、MOSN 等。Envoy 对于 Service Mesh 或云原生来说最大的贡献就是定义了 xDS，Envoy 本质上是一个 proxy，是可通过 API 配置的现代版 proxy，基于它衍生出来很多不同的使用场景，如 API Gateway、Service Mesh 中的 Sidecar proxy 和边缘代理。\n本节包含以下内容\n说明 kube-proxy 的作用。 Kubernetes 在微服务管理上的局限性。 介绍 Istio Service Mesh 的功能。 介绍 xDS 包含哪些内容。 比较 Kubernetes、Envoy 和 Istio Service Mesh 中的一些概念。 重要观点 如果你想要提前了解下文的所有内容，那么可以先阅读下面列出的本文中的一些主要观点：\nKubernetes 的本质是应用的生命周期管理，具体来说就是部署和管理（扩缩容、自动恢复、发布）。 Kubernetes 为微服务提供了可扩展、高弹性的部署和管理平台。 Service Mesh 的基础是透明代理，通过 sidecar proxy 拦截到微服务间流量后再通过控制平面配置管理微服务的行为。 Service Mesh 将流量管理从 Kubernetes 中解耦，Service Mesh 内部的流量无需 kube-proxy 组件的支持，通过为更接近微服务应用层的抽象，管理服务间的流量、安全性和可观察性。 xDS 定义了 Service Mesh 配置的协议标准。 Service Mesh 是对 Kubernetes 中的 service 更上层的抽象，它的下一步是 serverless。 Kubernetes vs Service Mesh 下图展示的是 Kubernetes 与 Service Mesh 中的的服务访问关系（每个 pod 一个 sidecar 的模式）。\nkubernetes 对比 service mesh 流量转发\nKubernetes 集群的每个节点都部署了一个 kube-proxy 组件，该组件会与 Kubernetes API Server 通信，获取集群中的 service 信息，然后设置 iptables 规则，直接将对某个 service 的请求发送到对应的 Endpoint（属于同一组 service 的 pod）上。\n服务发现\nService Mesh 中的服务注册 Istio 可以沿用 Kubernetes 中的 service 做服务注册，还可以通过控制平面的平台适配器对接其他服务发现系统，然后生成数据平面的配置（使用 CRD 声明，保存在 etcd 中），数据平面的透明代理（transparent proxy）以 sidecar 容器的形式部署在每个应用服务的 pod 中，这些 proxy 都需要请求控制平面来同步代理配置。之所以说是透明代理，是因为应用程序容器完全无感知代理的存在，该过程 kube-proxy 组件一样需要拦截流量，只不过 kube-proxy 拦截的是进出 Kubernetes 节点的流量，而 sidecar proxy 拦截的是进出该 Pod 的流量，详见理解 Istio Service Mesh 中 Envoy Sidecar 代理的路由转发 。\nService Mesh 的劣势\n因为 Kubernetes 每个节点上都会运行众多的 Pod，将原先 kube-proxy 方式的路由转发功能置于每个 pod 中，因为有 sidecar 拦截流量会多一次跳转时，增加响应延迟，同时大量的配置分发、配置同步，可能会影响应用性能。为了细粒度地进行流量管理，必将添加一系列新的抽象，从而会进一步增加用户的学习成本，但随着技术的普及，这样的情况会慢慢地得到缓解。\nService Mesh 的优势\nkube-proxy 的设置都是全局生效的，无法对每个服务做细粒度的控制，而 Service Mesh 通过 sidecar proxy 的方式将 Kubernetes 中对流量的控制从 service 一层抽离出来，可以做更多的扩展。\nkube-proxy 组件 在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。kube-proxy 负责为 Service 实现了一种 VIP（虚拟 IP）的形式。 在 Kubernetes v1.0 版本，代理完全在 userspace 实现。Kubernetes v1.1 版本新增了 iptables 代理模式 ，但并不是默认的运行模式。从 Kubernetes v1.2 起，默认使用 iptables 代理。在 Kubernetes v1.8.0-beta.0 中，添加了 ipvs 代理模式 。关于 kube-proxy 组件的更多介绍请参考 kubernetes 简介：service 和 kube-proxy 原理 和 使用 IPVS 实现 Kubernetes 入口流量负载均衡 。\nkube-proxy 的缺陷 kube-proxy 的不足之处 ：\n首先，如果转发的 pod 不能正常提供服务，它不会自动尝试另一个 pod，每个 pod 都有一个健康检查的机制，当有 pod 健康状况有问题时，kubelet 会重启对应的 pod，kube-proxy 会删除对应的转发规则。另外，nodePort 类型的服务也无法添加 TLS 或者更复杂的报文路由机制。\nKube-proxy 实现了流量在 Kubernetes service 多个 pod 实例间的负载均衡，但是如何对这些 service 间的流量做细粒度的控制，比如按照百分比划分流量到不同的应用版本（这些应用都属于同一个 service，但位于不同的 deployment 上），做金丝雀发布（灰度发布）和蓝绿发布？Kubernetes 社区给出了 使用 Deployment 做金丝雀发布的方法 ，该方法本质上就是通过修改 pod 的 label 来将不同的 pod 划归到 Deployment 的 Service 上。\nKubernetes Ingress vs Istio Gateway 上文说到 kube-proxy 只能路由 Kubernetes 集群内部的流量，而我们知道 Kubernetes 集群的 Pod 位于 CNI 创建的网络中，集群外部是无法直接与其通信的，因此 Kubernetes 中创建了 ingress 这个资源对象，它由位于 Kubernetes 边缘节点 （这样的节点可以是很多个也可以是一组）的 Ingress controller 驱动，负责管理南北向流量，Ingress 必须对接各种 Ingress Controller 才能使用，比如 nginx ingress controller 、traefik 。Ingress 只适用于 HTTP 流量，使用方式也很简单，只能对 service、port、HTTP 路径等有限字段匹配来路由流量，这导致它无法路由如 MySQL、Redis 和各种私有 RPC 等 TCP 流量。要想直接路由南北向的流量，只能使用 Service 的 LoadBalancer 或 NodePort，前者需要云厂商支持，后者需要进行额外的端口管理。有些 Ingress controller 支持暴露 TCP 和 UDP 服务，但是只能使用 Service 来暴露，Ingress 本身是不支持的，例如 nginx ingress controller ，服务暴露的端口是通过创建 ConfigMap 的方式来配置的。\nIstio Gateway 的功能与 Kubernetes Ingress 类似，都是负责集群的南北向流量。Istio Gateway 描述的负载均衡器用于承载进出网格边缘的连接。该规范中描述了一系列开放端口和这些端口所使用的协议、负载均衡的 SNI 配置等内容。Gateway 是一种 CRD 扩展 ，它同时复用了 sidecar proxy 的能力，详细配置请参考 Istio 官网 。\nxDS 协议 下面这张图大家在了解 Service Mesh 的时候可能都看到过，每个方块代表一个服务的实例，例如 Kubernetes 中的一个 Pod（其中包含了 sidecar proxy），xDS 协议控制了 Istio Service Mesh 中所有流量的具体行为，即将下图中的方块链接到了一起。\nService Mesh 示意图 xDS 协议是由 Envoy 提出的，在 Envoy v2 版本 API 中最原始的 xDS 协议指的是 CDS（Cluster Discovery Service）、EDS（Endpoint Discovery service）、LDS（Listener Discovery Service） 和 RDS（Route Discovery Service），后来在 v3 版本中又发展出了 Scoped Route Discovery Service（SRDS）、Virtual Host Discovery Service （VHDS）、Secret Discovery Service（SDS）、Runtime Discovery Service（RTDS）等，详见 xDS REST and gRPC protocol 。\n下面我们以各有两个实例的 service，来看下 xDS 协议。\nxDS 协议 上图中的箭头不是流量进入 Proxy 后的路径或路由，也不是实际顺序，而是想象的一种 xDS 接口处理顺序，其实 xDS 之间也是有交叉引用的。\n支持 xDS 协议的代理通过查询文件或管理服务器来动态发现资源。概括地讲，对应的发现服务及其相应的 API 被称作 xDS。Envoy 通过 订阅（subscription） 的方式来获取资源，订阅方式有以下三种：\n文件订阅：监控指定路径下的文件，发现动态资源的最简单方式就是将其保存于文件，并将路径配置在 ConfigSource 中的 path 参数中。 gRPC 流式订阅：每个 xDS API 可以单独配置 ApiConfigSource ，指向对应的上游管理服务器的集群地址。 轮询 REST-JSON 轮询订阅：单个 xDS API 可对 REST 端点进行的同步（长）轮询。 以上的 xDS 订阅方式详情请参考 xDS 协议解析 。Istio 使用 gRPC 流式订阅的方式配置所有的数据平面的 sidecar proxy。\nxDS …","relpermalink":"/blog/service-mesh-the-microservices-in-post-kubernetes-era/","summary":"本文是以前所写内容的重新修订并收录于 ServiceMesher 社区的 Istio Handbook 中，其他章节仍在编纂中。","title":"Service Mesh——后 Kubernetes 时代的微服务"},{"content":" 在这个最孤独的春节里，进行一场最深刻的修行。\n2020年，的确是开局不利。就在短短不到一个月的时间里，我几乎没听到过一条好消息：\n特朗普暗杀伊朗少将苏莱曼尼；武汉爆发的这次肺炎；我当年的篮球偶像科比因直升机坠毁去世的消息着实让我震惊，湖人队24 号永别了。这让我在原本就兴致缺缺的春节中，又多了一次精神打击。\n2020年，必定是要被深重载入全人类记忆的一年。就在这一年的第一个月最后几天，我决定将网站改版，一是因为延长的春节假期又不能出门，在家实在无聊；且网站上很多图片使用的是微博图床保存，因该图床不稳定，导致很多照片已无法挽回；再加上一个每到长假就想整理网站的习惯（网站的上一次改版是 2018 年国庆假期，在家闭关 7 天完成的），因此我决定将网站再次改版，就成了现在的样子。\n特性 这次改版后网站有如下特性：\n重新梳理网站内容，结构更加合理 支持邮件订阅 图片使用 Github 存储 更换 Hugo 主题为 educenter-hugo ，并做了大量修改 响应式静态网站，卡片式设计，用户体验更好 增加了 Google 自定义搜索 增加了 RSS 订阅，见页面底部 ","relpermalink":"/notice/website-revision-notice/","summary":"2020年的第一个月最后几天，我决定将网站改版。","title":"jimmysong.io网站改版通知"},{"content":"在前几天中秋节期间我翻译了一遍 Google 于 Github 上开源的 Google’s Engineering Practices documentation ，原文档 Github 地址： https://github.com/google/eng-practices ，目前为止的主要内容为 Google 总结的如何进行 Code Review（代码审查） 指南，根据原 Github 仓库的标题判断以后会追加更多 Google 工程实践的内容。\nGithub：https://github.com/rootsongjc/eng-practices 在线浏览：https://jimmysong.io/eng-practices 译文采用跟原文档同样的样式及目录结构，其实如果原文做了国际化要求的话可以直接合入的，但是根据之前的几个人提交的翻译建议，此项目作者并不建议进行翻译，一是翻译的文档可能会无人维护，再就是无法保证文档的准确定。\n中文合入相关建议见：\nAdd Chinese translation #12 Add the link of Chinese version of code reviewer’s guide #8 ","relpermalink":"/notice/google-engineering-practices-zh/","summary":"翻译自 Google 于 Github 上开源的文档","title":"谷歌工程实践文档中文版"},{"content":"《云原生服务网格 Istio：原理、实践、架构与源码解析（张超盟、章鑫、徐中虎、徐飞编著）》 是 2019 年国内出版的第四本 Istio 相关图书，前三本分别是：\n深入浅出Istio：Service Mesh快速入门与实践，崔秀龙 著 Service Mesh实战：用Istio软负载实现服务网格，周遥 著 Istio 入门与实战，毛广献 著 在这四本书刚上市时我都获得了作者的赠书，这本书是由四位华为的同学编写，于 2019 年 7 月第一次印刷，全书共 24 章，606 页，售价 139 元。我是在 KubeCon China 2019 的上海大会现场张超盟亲手赠与我的，张超盟也是 2018 年第三届 Service Mesh Meetup 的讲师。\n右侧是云原生服务网格 Istio（华为云原生技术丛书）作者之一张超盟 本书结构 全书共分四个篇章，24 个章节，606 页，每个章节的页数占比统计如下图所示。\n云原生服务网格 Istio：原理、实践、架构与源码解析》图书章节页数占全书百分比-表格 《云原生服务网格 Istio：原理、实践、架构与源码解析》图书章节页数占全书百分比-饼图 从统计结果中可以看出书中第 3 章（非侵入的流量治理）、第 14 章（司令官 Pilot）一共占全书的页数百分比为 24%，几乎占了四分之一的篇幅。\n这本书是目前（2019年08月15日）市面上能买到的最全的一本 Istio 相关的图书了，话说国外还一本 Istio 的书也出来，国内到现在都出了四本了，是不是有种墙外开花墙内香的感觉？\n建议大家结合 Istio 官方文档 一起来看这本书，Istio 版本更新虽然没有 Kubernetes 那么快，但是在本书发行一个多月后也要发布 1.2 版本了，欢迎大家加入 ServiceMesher 社区 来学习 Istio！\n","relpermalink":"/blog/cloud-native-service-mesh-istio-book/","summary":"云原生服务网格 Istio（华为云原生技术丛书）图书读后感，原理、实践、架构与源码解析（张超盟、章鑫、徐中虎、徐飞编著）。","title":"云原生服务网格 Istio 图书"},{"content":"本文介绍如何为 ServiceMesher.com 网站配置自动化部署的详细说明，通过本文你将了解到：\n如何使用 GitHub Webhook 来自动化发布您的网站 如何配置 Nginx 代理根据 URI 请求转发到本地服务器的指定端口 自动发布脚本 使用名为 deploy.sh 的 Shell 脚本编译 Hugo 生成 HTML 文件，并放到 Nginx 配置的目录下。该脚本位于 ServiceMesher 官网 GitHub 仓库 同级目录下，内容如下：\n#!/bin/bash # 网站的代码仓库目录 input=\u0026#34;website\u0026#34; # Nginx 中配置的网站的 HTML 根目录 output=\u0026#34;/home/admin/servicemesher.com\u0026#34; cd $input git pull hugo cd .. cp -r $input/public/* $output 依赖安装 该网站部署在阿里云上，操作系统为 CentOS 7.6.1810，并配置好了 HTTPS 。\n安装后端服务配置所需的组件。\nyum install -y npm 安装 NPM 包。\nnpm i -S github-webhook-handler npm i -g pm2 创建 webhook 服务后端 我们使用 NodeJS 创建 webhook 服务后端，后端代码保存在 webhook.js文件中，调用 deploy.sh 来发布，因此需要与 deploy.sh 文件在同一级目录中，监听 http://127.0.0.1:6666/webhook：\n当前的所有文件的结构如下：\n$ ls -1 deploy.sh node_modules package.json sofastack.tech webhook.js webhook.js 文件内容如下：\nvar http = require(\u0026#39;http\u0026#39;); var spawn = require(\u0026#39;child_process\u0026#39;).spawn; var createHandler = require(\u0026#39;github-webhook-handler\u0026#39;); // 注意将 secret 修改你自己的 var handler = createHandler({ path: \u0026#39;/webhook\u0026#39;, secret: \u0026#39;yourwebhooksecret\u0026#39; }); http.createServer(function (req, res) { handler(req, res, function (err) { res.statusCode = 404; res.end(\u0026#39;no such location\u0026#39;); }) }).listen(6666); handler.on(\u0026#39;error\u0026#39;, function (err) { console.error(\u0026#39;Error:\u0026#39;, err.message) }); handler.on(\u0026#39;push\u0026#39;, function (event) { console.log(\u0026#39;Received a push event for %s to %s\u0026#39;, event.payload.repository.name, event.payload.ref); runCommand(\u0026#39;sh\u0026#39;, [\u0026#39;./deploy.sh\u0026#39;], function( txt ){ console.log(txt); }); }); function runCommand( cmd, args, callback ){ var child = spawn( cmd, args ); var resp = \u0026#39;Deploy OK\u0026#39;; child.stdout.on(\u0026#39;data\u0026#39;, function( buffer ){ resp += buffer.toString(); }); child.stdout.on(\u0026#39;end\u0026#39;, function(){ callback( resp ) }); } 在 webhook.js 所在目录下启动后端服务：\npm2 start webhook.js 查看服务状态：\n$ pm2 status ┌──────────┬────┬─────────┬──────┬───────┬────────┬─────────┬────────┬─────┬───────────┬──────┬──────────┐ │ App name │ id │ version │ mode │ pid │ status │ restart │ uptime │ cpu │ mem │ user │ watching │ ├──────────┼────┼─────────┼──────┼───────┼────────┼─────────┼────────┼─────┼───────────┼──────┼──────────┤ │ webhook │ 0 │ 1.0.0 │ fork │ 30366 │ online │ 0 │ 6h │ 0% │ 30.8 MB │ root │ disabled │ └──────────┴────┴─────────┴──────┴───────┴────────┴─────────┴────────┴─────┴───────────┴──────┴──────────┘ Use `pm2 show \u0026lt;id|name\u0026gt;` to get more details about an app 使用 pm2 logs webhook 可以查看后端服务日志。\nNginx 配置 在 nginx 配置中增加转发设置，将对网站 /webhook URI 的访问转发到服务器本地的 6666 端口，即 webhook 后端服务商。\n# GitHub auto deploy webhook location /webhook { proxy_pass http://127.0.0.1:6666; } GitHub Webhook 配置 在 GitHub 仓库的 Settings - webhooks 设置中创建一个新的 webhook。\nGitHub Webhook 配置 注意选择 Content Type 为 application/json，secret 设置成与 webhook.js 中的相同。\n配置完成后 GitHub 将自动调用 Webhook 以验证有效性。\nGitHub 自动触发 Webhook 如果看到 200 响应表示成功调用 Webhook 后端服务，这样每次我们的仓库合并后就会触发网站自动部署。\n更多 为了加强 GitHub 自动化，还有更多 GitHub App 可以使用，推荐：\nauto-assigin mergify 这些已经在 servicemesher.com 网站上集成了，感兴趣的读者可以访问 ServiceMesher 官网的代码仓库 查看配置。\n参考 使用Github的webhooks进行网站自动化部署 ","relpermalink":"/blog/github-webhook-website-auto-deploy/","summary":"通过本文你将了解到如何使用 GitHub Webhook 来自动化发布您的网站。","title":"使用 GitHub Webhook 实现静态网站自动化部署"},{"content":"我的博客从上线第一天起就使用了 HTTPS，用的是 Cloudflare ，直接在其后台配置即可。如果你是用 nginx、apache、haproxy 等服务器来运行自己的网站，给大家推荐 Certbot ，可以自动化来配置 SSL 证书和定时更新。\n下面记录我自己为 servicemesher.com 网站配置 HTTPS 证书的过程，全程不需要 5 分钟。\n环境 网站的托管环境如下：\nOS：CentOS 7.6 阿里云 网站服务器：Nginx，使用 yum 安装，版本 1.12 提前配置好 Nginx，确保使用 HTTP 先可以访问到网站 注意：请使用 yum 命令安装 nginx，这样可以确保 nginx 安装在默认的位置，因为 certbot 会检测 /etc/nginx/ 目录下的配置文件。\n操作步骤 执行下面的步骤可以直接为你的网站配置 HTTPS 证书。\nyum -y install yum-utils yum-config-manager --enable rhui-REGION-rhel-server-extras rhui-REGION-rhel-server-optional yum install certbot python2-certbot-nginx 下图是在 Certbot 中选择服务器和操作系统的页面。\nCertBot 页面 执行下面的命令，根据提示会自动配置 nginx。\ncertbot --nginx Saving debug log to /var/log/letsencrypt/letsencrypt.log Plugins selected: Authenticator nginx, Installer nginx Starting new HTTPS connection (1): acme-v02.api.letsencrypt.org Which names would you like to activate HTTPS for? 1：servicemesher.com 2: www.servicemsher.com # 这里直接回车选择所有的域名 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Select the appropriate numbers separated by commas and/or spaces, or leave input blank to select all options shown (Enter \u0026#39;c\u0026#39; to cancel): - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - You have an existing certificate that contains a portion of the domains you requested (ref: /etc/letsencrypt/renewal/servicemesher.com.conf) It contains these names: servicemesher.com, www.servicemesher.com You requested these names for the new certificate: servicemesher.com, prow.servicemesher.com, www.servicemesher.com. Do you want to expand and replace this existing certificate with the new certificate? - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - (E)xpand/(C)ancel: E Renewing an existing certificate Performing the following challenges: http-01 challenge for prow.servicemesher.com Waiting for verification... Cleaning up challenges Deploying Certificate to VirtualHost /etc/nginx/nginx.conf Deploying Certificate to VirtualHost /etc/nginx/nginx.conf Deploying Certificate to VirtualHost /etc/nginx/nginx.conf Please choose whether or not to redirect HTTP traffic to HTTPS, removing HTTP access. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 1: No redirect - Make no further changes to the webserver configuration. 2: Redirect - Make all requests redirect to secure HTTPS access. Choose this for new sites, or if you\u0026#39;re confident your site works on HTTPS. You can undo this change by editing your web server\u0026#39;s configuration. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Select the appropriate number [1-2] then [enter] (press \u0026#39;c\u0026#39; to cancel): # 这里是为了扩展证书支持更多的域名，所有输入 2 回车 Traffic on port 80 already redirecting to ssl in /etc/nginx/nginx.conf Redirecting all traffic on port 80 to ssl in /etc/nginx/nginx.conf Traffic on port 80 already redirecting to ssl in /etc/nginx/nginx.conf - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Your existing certificate has been successfully renewed, and the new certificate has been installed. 然后重新加载配置。\nnginx -t;nginx -s reload 设置证书自动更新。\necho \u0026#34;0 0,12 * * * root python -c \u0026#39;import random; import time; time.sleep(random.random() * 3600)\u0026#39; \u0026amp;\u0026amp; certbot renew\u0026#34; | sudo tee -a /etc/crontab \u0026gt; /dev/null 好了现在访问你的网站就可以看到 https 头部加了 HTTPS 锁了。\n参考 让网站永久拥有HTTPS - 申请免费SSL证书并自动续期 certbot - 免费的 https 证书 ","relpermalink":"/blog/free-certificates-with-certbot/","summary":"实测推荐使用 Certbot 为网站设置永久免费的 HTTPS 证书，超简单，全程不用五分钟！","title":"使用 Certbot 为网站设置永久免费的 HTTPS 证书"},{"content":"总体概述 开源软件合规（Compliance）实践，从狭义上讲就是企业使用开源软件许可证（License）的合规。Recommended Open Source Compliance Practices for the Enterprise 电子书（共32 页）由 Ibrahim Haddad 博士撰写，本书从以下几个角度为你的公司的进行开源合规实践以指导：\n创建开源审查委员会（Open Source Review Committee） 代码扫描 软件溯源（Software Sourcing） 开源法务支持（Open Source Legal Support） 流程中的合规检查（Compliance Checkpoints） 开发和部署检查器 合规事项看板 开源合规审查组 企业为了保证自己产品或软件的合规，通常会有一个许可证合规审查组，负责以下几项职责：\n遵守开源许可条款 促进在产品和服务中使用开源 遵守第三方商业软件的许可条款 保护您的产品/服务差异化（知识产权/IP） 开源促进及合规计划 开源合规流程 图 2 是开源合规流程闭环。\n图 2. 开源合规流程闭环 该流程中分为以下三步：确认（Identify）、批准（Approve）和确知（Satisfy）。\n图 3. 开源合规步骤产出 确认（Identify） 此初始步骤的目标是监控软件组合中开源来源，无论该组合是作为独立软件包还是嵌入在第三方或公司开发的软件中。此步骤的输出是详细的软件物料清单（Bill of Materials），用于标识所有开源软件包（Package）和代码片段（Snippet）的来源（Origin）、许可证（License）以及由软件组合分析工具所识别的许可冲突。\n批准（Approve） 这一步的目标是：\n查看上一步的输出，了解管理相关源代码的使用、修改和分发的许可证； 根据其独特的背景（context），确定是否批准使用已识别的开源软件； 确知（Satisfy） 在最后一步中，准备好所有已批准的开源软件（整个组件和片段）的许可证、版权（copyright）和归属声明，并将其交给相关的部门，以包含在产品文档中。同样，已经确知和标记了许可义务的开源软件包，就可以在产品/服务上线时发布了。\n对企业开源合规实践的建议 作者提出了企业可以实施的实践建议，以改进和加强其开源合规性计划：\n成立开源审查委员会（Open Source Review Board，简称 OSRB） 建立自动化系统来识别开源软件 让软件供应商遵守开源许可证 扩展开源法律支持 在业务和开发过程中集成开源合规性检查点 提供各种开源合规性任务的清单（checklist） 开发和部署支持清单 建立开源合规活动基准（benchmark） 参与关键合规性开源合规性计划 成立开源审查委员会 除开源合规官或开源计划办公室的代表外，开源审查委员会（OSRB）由法律和产品/工程团队的代表组成。OSRB的主要职责是审查和批准计划在产品和服务中使用开源软件。\n下面是 OSRB 中每个参与者职责的宏观概述。\n开源法律顾问（Open Source Legal Counsel）\n审查和批准开源软件的使用、修改和分发。 提供与开源许可相关的法律指导。 为创建合规培训做出贡献。 有助于改进合规计划。 审查和批准与许可证合规性相关的 Web 门户的内容。 审核并批准使用的开源软件的义务列表。 从开源合规性角度签署产品发布。 产品/工程代表（Product / Engineering Representative）\n在其组织内实施合规性政策（policy）和流程（process）。 在软件开发过程中集成合规性实践。 从工程角度为改进合规计划做出贡献。 遵循技术开源合规指南。 与 OSRB 成员协作以响应合规性查询。 进行设计、架构和代码审查。 准备用于出版物的开源软件包和通知。 从开源合规性角度签署产品发布。 开源计划办公室和合规官的代表（Representative of the Open Source Program Office or Compliance Officer）\n推动开源合规活动。 协调源代码扫描和审核。 参与工程审查和分发准备评估。 协调开源软件包和通知的发布。 为培训工作和改进合规计划做出贡献。 有助于促进开源软件的自动化和发现。 从开源合规性角度签署产品发布。 除了 OSRB 的成员之外，实现开源合规是一项跨学科活动，涉及到组织内的各个部门和个人，如图 4 所示。\n图 4. 开源项目办公室组成 下面是对支持团队帮助 OSRB 确保开源合规性的核心职责的描述。\nIT\n创建/获取确保合规性所需的新工具 为合规计划使用的工具和自动化基础设施提供支持和维护 企业发展（Cooperate Development）\n请求/监督开源合规性尽职调查在合并或收购之前完成 从外包开发中心接收源代码时确保合规性记录 文档（Documentation）\n在产品文档中包含开源许可证信息和通知（notice） 开源执行委员会（Open Source Executive Committee）\n审查并批准在开源许可下发布专有源代码的提议 软件采购（Software Procurement）\n授权第三方软件提供商在许可或购买的软件组件中披露开源 协助引入包含（或不包含）开源软件的第三方软件 开源代码审查 开源合规性工作的核心是识别开源代码及其各自的许可证，以便组织可以满足适用的许可证义务。开源策略和流程指导此核心活动。合规性政策和流程管理开源软件的使用、贡献、审核和发布的各个方面。如果我们采用下图 5 所示的基本流程并对其进行扩展，我们将考虑端到端的合规流程。下图显示了这样一个流程，它具有源自多个源的源代码输入。源代码经过一系列步骤，流程的最终输出包括书面报价、通知列表（版权、归属、许可证），以及为履行许可义务而发布的源代码包。\n图 5 提供了端到端合规流程的详细示例，其中包括软件组件被 OSRB 批准在构建系统中与软件产品集成之前经历的各个步骤。\n图 5. 端到端开源合规流程示例 图 6 简要描述了每个步骤中发生的情况。\n图 6. 开源合规代码确认步骤详解 开源代码溯源 让您的软件提供商参与开源合规中至关重要。软件提供商必须披露其可交付成果中包含的开源代码，并提供包括适用源代码在内的所有通知（notice）。\n图 7. 多源开发模型 图 7 描绘了多源开发模型和传入源代码的各种源组合。在此模型下，产品或软件堆栈可以包含专有软件、第三方商业和第三方开源软件的任意组合。例如，除了第三方专有源代码之外，软件组件 A 可以包括专有源代码，而软件组件 B 除了可以包含来自开源项目的源代码之外还可以包括专有源代码。\n当今的公司处于必须更新其供应链（软件采购）程序以解决获取和使用开源软件的状态。通常会有供应链人员参与将软件从供应商转移到贵公司。他们可以通过两种主要方式支持开源合规性活动：\n要求第三方软件提供商披露他们在其可交付成果中使用的任何开源，以及 协助许可与开源软件包捆绑在一起或与之集成的第三方软件。 此领域的推荐做法是强制第三方软件提供商披露其产品中使用的所有开源组件，并声明他们计划如何满足适用的开源许可证义务。如果第三方软件包含开源，供应链必须确保在初始入口后满足开源许可证义务——您作为提供开源产品或服务的分销商将承担这些义务和责任。\n提供便捷的法务支持 大多数组织都会创建开源合规性计划并建立核心团队以确保合规性。大多数公司往往会会遇到开源法律支持的瓶颈，因为您公司里可能有成百上千的使用和集成开源代码的开发人员，而很少有法务人员提供所需的法律支持。扩展开源法律支持需要一些开箱即用的思考，但可以借助以下实用方法实现。\n许可证手册（License Playbooks） 提供面向软件开发人员的易于阅读和摘要的开源许可证摘要。提供有关这些许可证的易于理解的信息，例如许可证授予、限制、义务、专利影响等。使用开源软件许可证手册可以大量减少发送给法律顾问的基本问题的数量，并为开发人员提供了对常见查询的即时指导、信息和答案。\n许可证兼容性矩阵（License Compatibility Matrix） 许可证兼容性是指确定某个许可证是否与另一个许可证兼容。GPL 兼容性是指确定某个许可证是否与 GPL 条款兼容。当合并源自不兼容许可下软件组件的源代码时，开发团队经常会遇到许可兼容性问题。当开发团队将不同许可证下的代码组合在一起时，可以参考许可证兼容性矩阵来验证在单个软件组件中是否存在加入源代码的许可冲突。如果开发团队使用的许可证源不在矩阵中，则可以后续获得法律顾问的建议。\n许可证分类 为了减少开源法律顾问收到的问题数量并增加许可和合规流程教育，一些公司选择在几个类别下对其产品中最常用的许可进行分类。图11显示了许可证分类系统的一个简单示例，其中大多数使用的开源许可证分为四类。\n图 8. 开源许可证分类（仅供参考） 上述许可证类别是对许可证进行分类的简单方法，使开发人员在根据这些许可证集成代码时更容易了解操作过程。下面这个例子是开发人员想要使用在以下许可下的开源软件包的：\nLicense A - Action：尽管用，没有什么问题 License E - Action：获得工程经理的批准 License I - Action：获得法律顾问的批准 License M - Action：根据政策禁止适用该 License 其他 - Action：向经理询问行动方案 有关此话题的进一步阅读，我们建议阅读**扩展开源法律支持的实用建议 **。本文探讨了法律顾问在确保开源合规方面的作用，并为法律顾问提供了可以为软件开发团队提供的实用建议。这些实用建议将使软件开发人员能够做出与开源许可相关的日常决策，而无需再去找负责每个问题的法律顾问。\n开源合规流程中的检查点及发布清单 有必要将合规性实践纳入开发流程，以确保开源合规工作的成功。您可以通过多种方式实现这一目标。\n每个内部版本的合规性：更新流程管理，以确保在产品开发周期中尽早包含开源合规性活动，以使组织能够满足其发布时间表。遵循此模型，未来版本的增量合规性也变得简单明了。 更新供应链程序：定制供应链的供应商选择程序，以确保在对供应商及其可交付成果进行尽职调查（Due Diligence）时考虑开源合规性要求。 执行验证：使用验证步骤确保在发生发行外部版本之前满足所有合规性要求。 培训员工：为所有员工提供开源合规培训。 采用 SPDX 报告许可证信息：以 SDPX 格式提供许可证信息，以尽量减少任何可能的错误，并标准化报告信息的方式。 SPDX\nSPDX® （Software Package Data Exchange®）是用于传达软件物料清单信息（包括组件、许可证、版权和安全参考）的开放标准。\nSPDX 通过为公司和社区提供共享格式来共享软件许可、版权和安全参考的重要数据，从而简化和改进合规性，从而减少冗余工作。\nSPDX 规范由 SPDX 工作组开发，该工作组由 Linux 基金会托管。基层工作包括来自 20 多个组织的代表——软件、系统和工具供应商、基金会和系统集成商——都致力于为软件包数据交换格式创建标准。\n开发和部署清单 清单很有用，可确保执行合规性任务的一致性和完整性。强烈建议根据员工职责建立合规里程碑清单和目标清单。\n清单的示例包括：\n批准将传入代码集成到产品的源代码存储库之前的核对表 确保履行义务的清单 开发人员的清单 工程经理的清单 合规人员清单 开源法律人员的清单 软件采购人员清单 为了说明这一点，我们提供了一个示例清单，展示了在组织发布源代码包之前必须检查的各种任务，以履行在交付产品中包含的开源代码的许可义务：\n预发行清单（Pre-Distribution Checklist）\n验证引入开源软件包的修改是否已记录，并作为更改日志的一部分包含在开源发行说明中。 确保每个修改后的源代码文件都包含版权声明，免责声明和通用“更改日志”（Changelog）条目的附加条目。 确认源代码包的所有 …","relpermalink":"/blog/open-source-compliance-practices/","summary":"本文是开源软件合规实践的,介绍开源软件合规的流程及建议。","title":"开源软件合规实践"},{"content":"本书的主旨是：如果没有成熟的 DevOps 实践，云原生是玩转不起来的。DevOps 已经不是什么新鲜的话题，但到底什么是 “Cloud Native DevOps” 及如何实践 Cloud Native DevOps，这正是本书要探讨的内容。\nDevOps 正在经历一次转型，从自动化构建到声明式基础设施、微服务和 Serverless。大部分人对云原生存在误解，以为云原生就是运行在云上，其实云原生更偏向于一种理念，即应用的定义及架构方式，而不是将应用运行在哪里。而云上的 DevOps 与传统的 DevOps 有什么区别，开发者和运维人员在云原生时代如何转型？也许本书会给你答案。\n关于本书 本书是由 TheNewStack 出品的免费电子书，可以在 TheNewStack 网站 上获取本书的电子版，同时推荐 TheNewStack 的电子书系列 ，囊括了容器、微服务、Kubernetes、云原生诸多主题，可以作为企业决策的参考读物。\n本书是 TheNewStack 编辑集结 DevOps 领域的专家在各种大会上的发言、演讲，有很多观点引用，并结合了一些调查问卷数据展示了一幅云原生 DevOps 的趋势与全景图，下文中我会找一些代表性的观点和图表来说明。\n下面是本书目录，一共分为三大部分：构建、部署和管理，其中前两个部分还给出了参考书目、示例研究等。\n云原生 DevOps 目录 谁适合读这本书 IT 经理、CIO、团队领导者，希望规划自己公司或团队的云原生化 DevOps 的实践路径以面对大规模场景。\n云原生化的 DevOps 云原生是对业务价值和团队功能的重构。\n云原生化的 DevOps 在应用的管理上与原始的 DevOps 最大的区别就是——使用 YAML 文件配置的声明式基础设施（Declarative infrastructure）与应用程序的代码本身放在同一个存储库中，这些 代码 将由开发团队来维护，而运维团队的职能将转变为基础设施的构建者，服务安全性、健壮性、可见性及耐用性的守护者。\nAWS 的 Serverless 布道师 Chris Munns 早已甚至预测到 2025 年非云供应商的运维人员将不复存在，虽然听上去有点危言耸听，但这也是为传统 IT 运维人员的职业生涯敲响的警钟。\n云原生 DevOps 高亮部分 开发接手了原来传统运维的一些职责，如配置和发布，减少了每次发布的成本，而运维的职责向管理整个系统的复杂性转变，例如转变为 SRE（Site Reliability Engineer）。\n工作流自动化的价值 DevOps 的原始教义：DevOps 不是一种工具或流程，而是一种重视整个组织的持续沟通、协作、集成和自动化的实践。\n工作流自动化的五个案例 根据自动化的驱动力及持续时间的长短，将 Workflow Automation 划分为五个类别。\n业务流程自动化 分布式系统通信 分布式事务 编排 决策自动化 运维需要做出的转变 Damon Edwards 提出于运维需要面对的四个灾难（圣经启示录中的四骑士 ）：\nSilos（孤岛） Ticket queues（无尽的低效的工单） Toil（干脏活累活的辛勤） Low trust（低信任度） 要向云原生 DevOps 转变就要克服以上几个问题。\nDevOps 领域的扩展 本书第三章中提到 DevOps 的领域扩展到 Security 和 Networking。\n为了维持合规的编程语言 容器镜像扫描 基于策略的网络安全 金丝雀测试 运行时的威胁检测 日志分析 ","relpermalink":"/blog/cloud-native-devops-book/","summary":"本书是 TheNewStack 编辑集结 DevOps 领域的专家在各种大会上的发言、演讲编纂而成。","title":"TheNewStack 云原生 Devops 报告解读"},{"content":"很多从事开源人可能会注意到有些开源项目要求贡献者在提交 PR 前首先签署 CLA，只有签署了 CLA 之后 PR 才可以合并。\n开源贡献协议简介 下面列举了开源贡献协议的一些简介：\n开源贡献协议有 CLA（Contributor License Agreement）和 DCO （Developer Certificate of Origin）两种； DCO 由 Linux Foundation 提出，是固定的简短条文（只有4条），旨在让贡献者保证遵守开源 license； CLA 是对开源 license 的法律性质补充，由法务制定； CLA 可以自定义，不论是个人还是企业级签署的时候都需要提供详细的信息，如姓名、公司、邮箱、地址、电话等； 下表中对比了 CLA 和 DCO 的特性，推荐大型跨公司开源项目使用 CLA，利用项目更加正规和长久发展； 开源社区的贡献者协议一般分为两种 CLA 和 DCO，这两种协议各有优缺点如下。\n特性 CLA DCO 社区属性 弱 强 签署方式 一次性 每次提交时在 commit 信息里追加 Signed-off-by: email 信息 法律责任 明确法律义务 无声明，用来限制提交者遵守开源 license 是否可自定义 公司或组织可自行定义 否 使用案例 Google、Pivotal、CNCF、阿里巴巴、Apache SkyWalking GitLab、Chef、Harbor、TiKV 公司属性 强，可以签署公司级别的 CLA 弱 什么是 CLA CLA 是 Contributor License Agreement 的缩写，CLA 可以看做是对开源软件本身采用的开源协议的补充。一般分为公司级和个人级别的 CLA，所谓公司级即某公司代表签署 CLA 后即可代表该公司所有员工都签署了该 CLA，而个人级别 CLA 只代表个人认可该 CLA。\nCLA 包含哪些内容？ 因为 CLA 是每个公司或组织自定义的，在细节上可能稍有不同，不过总体都包含以下内容：\n关于签署该 CLA 的主体和贡献的定义； 授予著作权给拥有该软件知识产权的公司或组织； 专利许可的授予； 签署者保证依法有权授予上述许可； 签署者确保所有的贡献内容均为原创作品； 签署者为贡献内容支持的免责描述； 说明贡献者提交非原创作品应该采用的方式； 保证在获悉任何方面不准确的事实或情况之时通知签约方； 对于主体在中国的企业，还加入了一些本地化的内容，如 Alibaba Open Source Individual CLA 。\n因为 CLA 分别为个人级和公司级，所以对于不同名义签署时需要提供不同的信息。签署个人级 CLA 的时候需要提供个人信息（姓名、地址、邮箱、电话等），签署公司级 CLA 还需要提供公司信息（名称、地址、联系电话、邮箱、传真等）；\n什么是 DCO DCO 是 Developer Certificate of Origin 的缩写，由 Linux Foundation 于 2004 年制定。DCO 最大的优点是可以减轻开发者贡献的阻碍，不用阅读冗长的 CLA 法律条文，只需要在提交的时候签署邮件地址即可。Chef 和 GitLab 已分别于 2016 年和 2017 年从 CLA 迁移到 DCO。\n如 CNCF 的 Sandbox 项目 harbor 就是使用的 DCO。\nDCO 目前是 1.1 版本，内容很简单，开源项目的贡献者只需要保证以下四点：\n该贡献全部或部分由我创建，我有权根据文件中指明的开源许可提交；要么 该贡献是基于以前的工作，这些工作属于适当的开源许可，无论这些工作全部还是部分由我完成，我有权根据相同的开源许可证（除非我被允许根据不同的许可证提交）提交修改后的工作；要么 该贡献由1、2、或 3 证明的其他人直接提供给我，而我没有对其进行修改。 我理解并同意该项目和贡献是公开的，并且该贡献的记录（包括我随之提交的所有个人信息，包括我的签字）将无限期保留，并且可以与本项目或涉及的开源许可证保持一致或者重新分配。 CLA vs DCO Kubernetes 社区中有过讨论将 Kubernetes 贡献者从 CLA 迁移到 DCO，最后TOC 成员 Tim Hockin觉得签署 CLA 对于贡献者只需要痛苦一次，每次提交都签署DCO是持续的痛苦，因此最后还是坚持使用CLA。参考Move from CLA to DCO #2649 。\n2018年 CNCF 对其托管的项目的 Maintainer 做了调研，从反馈来看，Maintainer 对 DCO 是存在痛点的，并希望 CNCF 投入更多的 PR 和市场力量来对抗具有全职 PR/marketing 的初创公司。\n如果为了更注重个人贡献者，考虑社区属性，可以使用 DCO，这样对于开源项目的管理者来说就不用指定复杂的 CLA 了，但是对于大型项目由众多合作方的项目，建议使用 CLA。\n阿里巴巴 CLA 阿里巴巴只提供个人级别的 CLA 签署：https://cla-assistant.io/alibaba/weex CLA 内容见：https://github.com/aliyun/cla 阿里巴巴的 CLA 是参照 Apache CLA 撰写的，最后加上两条补充，协议受中国杭州的法院监管，同时提供双语版本，如中引文版本有冲突以英文版本为准。\nGoogle CLA Google 的 CLA 也是仿照 Apache CLA 撰写的，Google 开源的一些列项目如 Istio、TensorFlow、Knative 等都是需要签署 Google CLA 。\n要贡献者授予Google以及其他软件用户贡献内容的版权以及内容背后的专利权。贡献者不要因为版权和专利权诉讼Google和其他软件用户。 明确贡献的原创性。不要因为贡献者的不适当抄袭行为，导致Google和其他软件使用者被诉讼。 签署公司级别 CLA 的人要能代表所在公司的所有贡献者。 维护贡献者列表的不一定是跟签署该协议的是同一个人，签名者可以指定一个人来管理。 参考：解读：Google Software Grant and Corporate Contributor License Agreement Pivotal CLA Pivotal 的 CLA 也是仿照 Apache CLA 撰写的，唯一增加了一点是协议受美国加州法律监管。签署个人级协议的时候需要提供姓名、邮箱、邮寄地址（可选）、国家（可选）、电话（可选），签署公司级别的 CLA 的条款了还增加了一条对于签名者必须有权利代表整个公司，要求的信息也更加详细，包括姓名、邮箱、邮寄地址、国家、电话、公司名称、 GitHub 组织、头衔等。参与贡献 Pivotal 主导的 Spring 社区和 CloudFoundry 里的项目需要签署 Pivotal CLA 。\n建议 如果你的开源项目可能会有公司间合作或者要贡献给基金会，为了防范法律风险，请直接使用 CLA；如果更看重社区内的合作，可以使用 DCO。\n参考 Individual Contributor License Agreement (“Agreement”) V2.0 Move from CLA to DCO #2649 - github.com Alphabet CLA Policy and Rationale - opensource.google.com The Apache Software Foundation Software Grant and Corporate Contributor License Agreement (“Agreement”) - apache.org Alibaba Open Source Individual CLA - github.com ","relpermalink":"/blog/open-source-cla/","summary":"很多从事开源人可能会注意到有些开源项目要求贡献者在提交 PR 前首先签署 CLA。","title":"开源社区贡献者协议 CLA 介绍"},{"content":"前段时间看了一篇文章为什么中国没有 Apache 基金会这样的组织？ ，二叉树视频中采访了开源社 的理事长老刘，他的一番话也让我很受启发，在关注和参与 CNCF 基金会这几年来我也有很多收获，有一点就是了解到了一个开源社区（基金会）治理的规则。\n虽然 CNCF 没有 Apache、GNOME、FreeBSD 历史那么悠久，但是它成立的短短几年内就成功的运作了 Kubernetes 这样的超大型开源项目，一定有其可取之处。今天我就来给大家分享下CNCF基金会的开源项目治理规则和组织架构，还有如何将一个开源项目加入到 CNCF。\nCNCF 根据“鸿沟理论 ”将其托管的项目分成三个成熟阶段，并设置了项目晋级到更高阶段的标准。\n“鸿沟理论 ”是由Geoffrey A. Moore提出的高科技产品的市场营销理论。新技术要想跨越鸿沟，必须能够实现一些跨越式的发展，拥有某一些以前不可能实现的功能，具有某种内在价值并能够赢得非技术人员的青睐。\nCNCF 项目的成熟度分类 图片来自 https://www.cncf.io/projects/ 开源项目如何加入 CNCF 开源项目所支持的公司成为 CNCF 会员 开源项目满足 CNCF 的要求（见后文） 在 GitHub 上提交proposal （GitHub Issue）列举项目介绍、现状、目标、license、用户与社区等 由 Chris Aniszczyk 安排该项目在某个TOC双月会议上介绍给 TOC 成员 1.TOC 会将开源项目指定到某个 SIG 中 项目获得两个TOC成员的赞成可进入sandbox （也可以直接获得2/3多数TOC 投票进入Incubating状态） 知识产权转移给 CNCF CNCF 安排博客撰写、PR等 每年一次评审，晋升到 incubating需要2/3的 TOC 成员投票赞成；至少3家用户成功在生产上使用；通过TOC的尽职调查；贡献者数量健康稳定 Sandbox 中的项目没有时效性质，可能永远都无法进入incubating 状态，被CNCF谨慎宣传 CNCF 开源项目成熟度演进 CNCF 的开源项目遵循如下图所示的成熟度演进。\nCNCF项目成熟度级别 关于上图的一些说明：\n加入Sandbox只需要2个TOC成员赞成 成熟一点的项目可以直接进入incubating阶段，但是 CNCF 会控制不同阶段的项目比例 晋级到Incubating或Graduated 需要至少2/3的 TOC成员（6名或以上）投票赞成 每年将评审一次 目前处于沙箱、孵化中、已毕业项目的数量比例为5：16：13，详见 https://cncf.io/projects 。其中沙箱（sandbox）项目因为其处于早期阶段并没有直接在上面的链接页面中列出，而是一个单独的 Sandbox 页面，因为 CNCF 为 sandbox 阶段的项目会谨慎背书。\n纳入CNCF开源版图的项目需要符合其对云原生的定义 CNCF 中托管的开源项目要符合云原生定义：\n云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API。 这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。 云原生计算基金会（CNCF）致力于培育和维护一个厂商中立的开源生态系统，来推广云原生技术。我们通过将最前沿的模式民主化，让这些创新为大众所用。 项目运作流程 下图演示了开源项目加入 CNCF 后的整个运作流程。\nCNCF中的项目运作 开源项目如何加入 CNCF 开源项目所支持的公司成为 CNCF 会员 开源项目满足 CNCF 的要求（见后文） 在 GitHub 上提交proposal （GitHub Issue）列举项目介绍、现状、目标、license、用户与社区等 由 Chris Aniszczyk 安排该项目在某个TOC双月会议上介绍给 TOC 成员 1.TOC 会将开源项目指定到某个 SIG 中 项目获得两个TOC成员的赞成可进入sandbox （也可以直接获得2/3多数TOC 投票进入Incubating状态） 知识产权转移给 CNCF CNCF 安排博客撰写、PR等 每年一次评审，晋升到 incubating需要2/3的 TOC 成员投票赞成；至少3家用户成功在生产上使用；通过TOC的尽职调查；贡献者数量健康稳定 Sandbox 中的项目没有时效性质，可能永远都无法进入incubating 状态，被CNCF谨慎宣传 开源项目加入 CNCF 的最低要求（Sandbox） 一个开源项目要想加入 CNCF 必须满足以下要求：\n项目名称必须在 CNCF 中唯一 项目描述（用途、价值、起源、历史） 与 CNCF 章程一致的声明 来自 TOC 的 sponsor（项目辅导） license（默认为 Apache 2） 源码控制（Github） 网站（英文） 外部依赖（包括 license） 成熟度模型评估（参考 开源项目加入CNCF Sandbox的要求 ） 创始 committer（贡献项目的时长） 基础设施需求（CI/CNCF集群） 沟通渠道（slack、irc、邮件列表） issue 追踪（GitHub） 发布方法和机制 社交媒体账号 社区规模和已有的赞助商 svg 格式的项目 logo 由 Sandbox 升级到 Incubating 的要求 通过 TOC 的尽职调查 至少有 3 个独立的终端用户在在生产上使用该项目：一般在项目的官网列举实际用户 足够健康数量的贡献者：项目的 GitHub 上有明确的 committer 权限划分、职责说明及成员列表，TOC 将根据项目大小来确认多少committer才算健康 展示项目在持续进行、良好的发布节奏、贡献频率十分重要 由Incubating升级到Graduated的要求 满足 Sandbox 和 Incubating 的所有要求 至少有来自两个组织的贡献者 明确定义的项目治理及 committer 身份、权限管理 接受 CNCF 的行为准则 ，参考Prometheus 获得CII 最佳实践徽章 在项目主库或项目官网有公开的采用者的 logo 参考归档的 Review：https://github.com/cncf/toc/tree/master/reviews 参考 鸿沟理论 - jianshu.com CNCF Graduation Criteria v1.2 - github.com 为什么中国没有 Apache 基金会这样的组织？ - infoq.cn 开源社首页 - kaiyuanshe.cn ","relpermalink":"/blog/contribute-project-to-cncf/","summary":" CNCF 基金会的开源项目治理规则和组织架构，还有如何将一个开源项目加入到 CNCF。","title":"如何将一个开源项目加入CNCF？"},{"content":"下面这段是发布说明，来自 Istio 官方博客 https://istio.io/zh/blog/2019/announcing-1.1/ ，是我翻译的。\nIstio 于北京时间今日凌晨4点，太平洋时间下午1点 Istio 1.1 发布。\n自从去年 7 月份 1.0 发布以来，为了帮助人们将 Istio 投入生产我们做了很多工作。我们不出所料得发布了很多补丁（到目前为止已经发布了 6 个补丁！），但我们也在努力为产品添加新功能。\n1.1 版本的主题是”Enterprise Ready“（企业级就绪）。我们很高兴看到越来越多的公司在生产中使用 Istio，但是随着一些大公司加入进来，Istio 也遇到了一些瓶颈。\n我们关注的主要领域包括性能和可扩展性。随着人们将 Istio 逐步投入生产，使用更大的集群以更高的容量运行更多服务，可能会遇到了一些扩展和性能问题。Sidecar 占用了太多资源增加了太多的延迟。控制平面（尤其是 Pilot）过度耗费资源。\n我们投入了很多精力在使数据平面和控制平面更有效率上。在 1.1 的性能测试中，我们观察到 sidecar 处理 1000 rps 通常需要 0.5 个 vCPU。单个 Pilot 实例能够处理 1000 个服务（以及 2000 个 pod），需要消耗 1.5 个 vCPU 和 2GB 内存。Sidecar 在第 50 百分位增加 5 毫秒，在第 99 百分位增加 10 毫秒（执行策略将增加延迟）。\n我们也完成了命名空间隔离的工作。您可以使用 Kubernetes 命名空间来强制控制边界以确保团队之间不会相互干扰。\n我们还改进了多集群功能和可用性。我们听取了社区的意见，改进了流量控制和策略的默认设置。我们引入了一个名为 Galley 的新组件。Galley 验证 YAML 配置，减少了配置错误的可能性。Galley 还用在多集群设置中——从每个 Kubernetes 集群中收集服务发现信息。我们还支持了其他多集群拓扑，包括单控制平面和多个同步控制平面，而无需扁平网络支持。\n更多信息和详情请查看发布说明 。\n该项目还有更多进展。众所周知 Istio 有许多可移动部件，它们承担了太多工作。为了解决这个问题，我们最近成立了 Usability Working Group（可用性工作组） （可随时加入）。社区会议 （周四上午 11 点）和工作组里也发生了很多事情。您可以使用 GitHub 凭据登录 discuss.istio.io 参与讨论！\n感谢在过去几个月里为 Istio 作出贡献的所有人——修补 1.0，为 1.1 增加功能以及最近在 1.1 上进行的大量测试。特别感谢那些与我们合作安装和升级到早期版本，帮助我们在发布之前发现问题的公司和用户。\n最后，去浏览最新文档，安装 1.1 版本吧！Happy meshing！\n官方网站 ServiceMesher 社区从 Istio 0.6 版本起一直在维护 Istio 官方文档的中文页面 ，截止2019年3月19日晚12点已有596个 PR 合并，共维护文档310余篇，感谢大家的努力！部分文档可能稍微滞后于英文版本，同步工作持续进行中，参与进来请访问 https://github.com/servicemesher/istio-official-translation Istio 官网每个页面右侧都有切换语言按钮，大家可以随时切换中英文版本，还可以提交文档修改，报告网站 Bug 等。\nServiceMesher 社区网站\nServiceMesher 社区网站 http://www.servicemesher.com 上涵盖了所有 Service Mesh 领域的技术文章，并适时发布最新活动，是您一站式了解 Service Mesh 和参与社区的入口。\n","relpermalink":"/notice/istio-11/","summary":"北京时间3月20日凌晨4点 Istio 1.1发布了，该版本历时8个月！ServiceMesher 社区同时推出了 Istio 中文文档。","title":"Istio 1.1发布"},{"content":"Istio handbook 原是我创作的一本开源电子书（见 https://jimmysong.io/istio-handbook ）在捐献给 ServiceMesher 社区之前已经撰写了8个月，为了进一步普及 Istio 和 Service Mesh 技术将次书捐献给社区共同撰写。2019年3月10日完成了原书内容的迁移至 https://github.com/servicemesher/istio-handbook ，原书将不再更新。\nGitHub 地址：https://github.com/servicemesher/istio-handbook 在线阅读地址：http://www.servicemesher.com/istio-handbook/ 本书概念图，封面图片上海静安寺夜景 ，Jimmy Song 摄。\n本书发行版权归属于电子工业出版社博文视点，未经授权请勿私自印刷发行。\nIstio 是由 Google、IBM、Lyft 等共同开源的 Service Mesh（服务网格）框架，于2017年初开始进入大众视野，作为云原生时代下承 Kubernetes、上接 Serverless 架构的重要基础设施层，地位至关重要。ServiceMesher 社区 作为中国最早的一批在研究和推广 Service Mesh 技术的开源社区决定整合社区资源，合作撰写一本开源电子书以飨读者。\n关于本书 本书起源于 rootsongjc/istio-handbook 及 ServiceMesher 社区创作的 Istio 知识图谱 ，集结社区力量合作创作而成。\n本书基于 Istio 1.0+ 版本编写，包含但不限于 Istio 知识图谱 中的主题。\n参与本书 请参阅本书写作规范 ，加入 ServiceMesher 社区 后进入 Slack channel 讨论。\n","relpermalink":"/notice/istio-handbook-by-servicemesher/","summary":"为了进一步普及 Istio 和 Service Mesh 技术将此书捐献给社区共同撰写。","title":"捐献Istio Handbook给ServiceMesher社区"},{"content":"2019年2月初，CNCF 发布了2018年的年度报告，这是 CNCF 继2017年度报告之后，第二次发布年度报告，2017年度的报告只有区区14页，今年的报告长度增长了一倍达31页。下面我将带大家一起来深度解读下这份2018年的年度报告，一窥 CNCF 过去一年里在推广云原生的道路上取得的进展。\n注：本文最后附上了2017年和2018年度的报告下载地址。\nCNCF 年度报告涵盖的范围 在解读 CNCF 的2018年度报告之前，我们先简单回顾下2017年度的报告 ，因为2017年度报告是 CNCF 的首份年度报告，这样我们也能更好的了解 CNCF 的来龙去脉。\n2017年度报告已经基本确定了 CNCF 每个年度报告所包含的主题：\n自我定位 会员参与情况 终端用户社区 项目更新 会议和活动 社区 培训和认证 以上为 CNCF 主要的市场活动，2017年时其成立的第二年，经过一年时间的筹备，这一年里各种市场活动都已经开始确立并有声有色的开展了起来，包括 KubeCon、成员单位、终端用户都已经发展起来了，以后历年里只是对其不断的发展和完善。\n2018年度报告中又新增了一些主题，这些主题是从2018年开始开展的，包括：\n项目更新与满意度调查 给 CNCF 项目的维护者发调查问卷询问满意度 CNCF charter 的修订（2018年11月） 项目更新与发布 项目服务与支援 专项活动、文档、网站与博客支持 本地化、IT 支持和培训 社区拓展 社区奖项 CNCF Meetup CNCF Ambassador 计划 卡通吉祥物 Phippy 生态系统工具 devstats CNCF Landscape 和路线图 项目 logo 物料 测试一致性项目 国际化 进入中国 本地化网站 详情请大家从本文最后的链接下载报告原文以查看详情。\nCNCF 的定位 CNCF（云原生计算基金会）成立于2015年12月11日，每届年度报告的开篇都会阐明 CNCF 的定位，CNCF 的自我定位在2018年发生了一次变动，这也说明基金会是跟随市场形势而动，其定位不是一成不变的，其中的变化暗含着 CNCF 战略的转变。\nCNCF 的2017年度定位 2017年度报告 中是这样正式介绍自己的：\nThe Cloud Native Computing Foundation (CNCF) is an open source software foundation dedicated to making cloud-native computing universal and sustainable. Cloud-native computing uses an open source software stack to deploy applications as microservices, packaging each part into its own container, and dynamically orchestrating those containers to optimize resource utilization. Cloud-native technologies enable software developers to build great products faster.\nWe are a community of open source projects, including Kubernetes, Envoy and Prometheus. Kubernetes and other CNCF projects are some of the highest velocity projects in the history of open source.\n可以看到介绍中的重点技术是：微服务、容器、动态编排。而在2018年 CNCF 对自己进行了重新的定位和包装，增加了新的内容。\nCNCF 的2018年度定位 2018年度报告 中 CNCF 对自己的定位是：\nThe Cloud Native Computing Foundation (CNCF) is an open source software foundation dedicated to making cloud native computing universal and sustainable. Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach.\nWe are a community of open source projects, including Kubernetes, Prometheus, Envoy, and many others. Kubernetes and other CNCF projects are some of the highest velocity projects in the history of open source.\n我们可以看到其表述中更加注重多云环境，主要涉及的技术比2017年多了Service Mesh（服务网格）、不可变基础设施和声明式 API。\n数读报告 CNCF 年度报告的原文主要是汇报了 CNCF 一年来的所展开的活动和进展，下表示根据 CNCF 2017和2018年度报告整理了关键数据。\nYear 2016 2017 2018 Members 63 170 365 Contributors - 18687 47358 CNCF Meetup Members - 53925 89112 Projects 4 14 32 End User Community Members - 32 69 Conference and Events Participants - 4085 - Certified Kubernetes Partners - 44 - Certified Kubernetes Service Providers - 28 74 CNCF Ambassador - - 65 Kubernetes Training Partners - - 18 注：其中2016年是 CNCF 正式开始工作的第一年，大部分数据因为活动尚未开展而缺失。\n从上表中我们可以看到 CNCF 诞生三年来基金会成员规模、托管项目的贡献者、参加 CNCF 名义的 Meetup 的人数取得较大范围的增长，尤其是2018年，因为基金会成员的爆发式增长（+130%），CNCF 开始给成员分级，会员级别、费用和权益也在 CNCF 官网 上明码标价。\n2018年 CNCF 组织的 KubeCon\u0026amp;CloudNativeCon 开始固定每年在西欧、北美和中国举行，且2018年是首次进入中国；原来的 Certified Kubernetes Partners 也取消了变成了 Certified Kubernetes Service Providers；CNCF 的 Ambassador 计划拥有了来自15个国家的65位 Ambassador，在世界各地为云原生布道；CNCF 还首次引入了 Kubernetes Training Partner。\n2018 年 CNCF 又推出了一系列新的认证（CKA 为2017年推出），包括：\nCKA （Kubernetes 管理员认证）：这是 CNCF 最早制定的一个证书，顾名思义，通过该认证证明用户具有管理 Kubernetes 集群的技能、知识和能力。虽然该证书在2017年即推出，但2018年对考试做了更细致的指导。KCSP 要求企业必须有至少三人通过 CKA。 CKAD （Kubernetes 应用开发者认证）：该认证证明用户可以为 Kubernetes 设计、构建、配置和发布云原生应用程序。经过认证的 Kubernetes Application Developer 可以定义应用程序资源并使用核心原语来构建、监控 Kubernetes 中可伸缩应用程序和排除故障。 KCSP （Kubernetes 服务提供商认证）：截止本文发稿时共有74家企业通过该认证。该认证的主体是企业或组织，通过 KCSP 的企业意味着可以为其他组织提供 Kubernetes 支持、咨询、专业服务和培训。通过该认证的中国企业有：灵雀云、阿里云、博云、才云、DaoCloud、EasyStack、易建科技、精灵云、谐云科技、华为、时速云、星号科技、睿云智合、沃趣、元鼎科技、ZTE。 Certified Kubernetes Conformance （Kubernetes 一致性认证）：通过该认证的 Kubernetes 提供商所提供的服务，意味着其可以保证 Kubernetes API 的可移植性及跨云的互操作性；及时更新到最新的 Kubernetes 版本；是否一致是可以通过运行开源脚本 验证的。截止本文发稿通过该认证的中国企业的发行版有：灵雀云（ACE、ACP、AKS）、才云 Compass、华为 FusionStage、酷栈科技 CStack MiaoYun、Daocloud Enterprise、新智认知新氦云、浪潮云、京东 TIG、网易云、七牛云、同方有云、睿云智合 WiseCloud；通过认证的中国企业托管平台有：阿里云、百度云、博云、EasyStack、易建科技、谐云科技、华为云 CCE、腾讯云 TKE、时速云、ZTE TECS。 以上是 CNCF 提供的主要证书，一般通过 KCSP 的企业都要先通过 Kubernetes 一致性认证，而通过 Kubernetes 一致性认证不一定要同时通过 KCSP，所以我们看到很多通过 Kubernetes 一致性认证的企业就不一定会通过 KCSP，因为 KCSP 的要求更多，至少要成为 CNCF 会员才可以。\n下面将就 CNCF 会员、托管项目的成熟度等级划分、Kubernetes 服务提供商认证和 Kubernetes 提供商认证做详细说明。\nCNCF 会员 2018年 CNCF 的会员单位经历了爆发式增长，从170家增长到365家。CNCF 制定了如下的会员等级：\nSilver Member Gold Member Platinum Member Academic/Nonprofit Member End User Member 不同等级的会员需要交纳的年费与权益不同，详情请见 https://www.cncf.io/about/join/ 。\n成为 CNCF 会员的好处 成为 CNCF 会员包括但不限于如下好处：\n将可以参与 CNCF 市场委员会、CNCF Webinar、在 CNCF 和 Kubernetes 官网发表博客、博客被 KubeWeekly 收录、 获得 KubeCon + CloudNativeCon 的门票折扣和参与大会的市场活动 对于 Kubernetes 系列认证如 KCSP、入选 TOC 也要求必须成为 CNCF 会员才可以获得 End User Case Study 有机会加入 Ambassador 计划 在社区里具有更多的话语权，例如 CNCF 在全球范围内组织的活动 项目成熟度等级 自2015年底 CNCF 创立之初 Kubernetes 成为其首个托管项目以来，截止到2018年底，CNCF 已经托管了32个开源项目 ， …","relpermalink":"/blog/cncf-annual-report-2018-review/","summary":"本文是对 CNCF（云原生计算基金会）2018年年度报告的解读。","title":"CNCF年度报告解读（2018年）"},{"content":"最近在回顾 Service Mesh 技术在2018年的发展，想再看看 Linkerd，正好杨彰显的这本《Service Mesh 实战——基于 Linkerd 和 Kubernetes 的微服务实践》上市发售了，机械工业出版社的编辑送了我一本，🙏杨福川编辑，我看了下抽空写了点读后感，我看了下抽空写了点读后感，其实也说不上是读后感，就当是自己的一点感悟吧，就当拿此书借题发挥吧，这个知识爆炸的年代，技术发展如此迅速，可以说是 IT 人员的幸运，也是不幸！有多少写开源软件的书推出一版后能撑过三年的？如果软件红得发紫，持续迭代 N 个版本，例如 Kubernetes，最近两年以每三个月一个版本的速度迭代，之前的书早就跟不上节奏，要么就要不断推出新版，直到软件稳定后不再有大的改动。还有种可能就是软件推广和发展的不理想，无人问津，写这样软件的书就不会有再版了。\n拿到本书后我的第一反应就是看看这本书定稿的时候 Istio 是什么版本，Linkerd 又是什么版本。因为在这一年内两款开源软件都有较大的版本变动，如果书籍定稿的时候基于的软件版本太低，软件架构可能会有较大的变化，影响书中示例和部分章节的时效性。这也是大多技术书籍名短的症结所在，技术发展是在太快，传统的书籍出版流程往往过于繁琐和冗长，等到书籍出版后所介绍的软件都出了好几个版本。例如 Kubernetes 这种的软件，每三个月一个版本，而写一般书从策划到发行少说半年，一般也要一年的时间。\n关于书籍定稿时的软件版本 Istio 0.8\n本书第一章「Service Mesh 简介」对 Service Mesh 相关开源产品介绍时提到本书定稿时 Istio 是 0.8 版本，而 Istio 在 2018年7月31日发布了 1.0 版本 。\n这本书定稿时，Istio 的最新版本是 0.8。\nLinkerd 1.3.6\n本书从序言开始一直到第二章结束也没有提及写作时基于的 Linkerd 版本，我在第二章的安装步骤中看到了说明。\n可以看到本书写作时是基于 Linkerd 1.3.6 版本，而 Linkerd 在同年的 9月18日发布了 2.0 GA ，这一版本跟 1.x 版本相比有重大变化——它还将项目从集群范围的service mesh转换为可组合的 service sidecar ，旨在为开发人员和服务所有者提供在云原生环境中成功所需的关键工具。\nLinkerd vs Envoy Linkerd 2.0 的 service sidecar 设计使开发人员和服务所有者能够在他们的服务上运行 Linkerd，提供自动可观察性、可靠性和运行时诊断，而无需更改配置或代码。通过提供轻量级的增量路径来获得平台范围的遥测、安全性和可靠性的传统 service mesh 功能，service sidecar 方法还降低了平台所有者和系统架构师的风险。该版本还用 Rust 重写了代理部分，在延迟，吞吐量和资源消耗方面产生了数量级的改进。\n而 Linkerd 1.x 继承自 Twitter 开源的 Finagle 高性能 RPC，所有想要深度学习 Linkerd 1.x 还需要了解 Finagle，这就跟 Istio 将 Envoy 作为默认的数据平面一样，要想深度学习 Istio 必须了解 Envoy。\n二者几乎使用了完全不同的术语，假如你已经了解了 Envoy 想要再切换到 Linkerd 上，那么就要再费很多心力来学习它的概念和原理，例如如下这些术语或配置（Linkerd 中独有的配置）：\ndtab（委托表）：由一系列路由组成，由一系列路由规则组成，以逻辑路径为输入，然后经过路由规则做一系列转换生成具体名字。这是 Linkerd 路由机制的根本，就像 Envoy 中的 xDS 协议 一样，本书的第四章「深入 Linkerd 数据访问流」专门讲解了 dtab 的实现机制。 dentry（委托表记录）：委托表的每条路由规则称为 dentry，如 /consul =\u0026gt; /#/io.l5d.consul/dc1。 namer：配置 Linkerd 支持的服务发现工具。 namerd：Linkerd 的控制平面，相当于 Istio 中的 Pilot，对接各种服务发现。当然 Linkerd 也可以直接与某个服务发现平台对接如 consul，而不使用 namerd 这个集中路由和配置管理组件。 interpreter：interpreter 决定如何解析服务名字和客户端名字。 虽然 Linkerd 也是 CNCF 中的项目 ，但它目前还处于孵化阶段，而 Envoy 的 xDS 协议 已经被众多开源项目所支持，如 Istio 、SOFAMesh 、NginxMesh 等，且 Envoy 已经从 CNCF 中毕业，以后可能成为 Service Mesh 领域的标准协议，Linkerd 的生存状况堪忧。\n关于本书 本书中所有示例都提供了虚拟机的快速上手环境，只要使用 Vagrant 即可创建虚拟机和应用，所以在本书的示例代码 有大量的 Vagrantfile。\n本书第三部分「实战篇」花了大量篇幅（本书一半的页数）来讲解如何使用 Linkerd 和 Kubernetes 来管理微服务，可以参考我2017年8月1日写的这篇微服务管理框架service mesh——Linkerd安装试用笔记 ，那时候还是基于 Linkerd 1.1.2，还有 Linkerd 官方示例 ，这些示例基本都不怎么更新了。\n因为该书定稿时所基于的 Linkerd 版本距离本书发售时的 Linkerd 已经落后一个大版本（最新版本是 Linkerd 2.1 ），所以读者一定要注意这一点，老实说我只花了两个夜晚快速过了一下本书，无法对本书内容给出具体评论，所以本书是否是你所需要的就要你自己去思考了。\n","relpermalink":"/blog/service-mesh-in-action-by-yangzhangxian-review/","summary":"顺便对比了下 Linkerd 和 Envoy，给读者一些我自己的建议。","title":"《Service Mesh 实战—基于 Linkerd 和 Kubernetes 的微服务实践》读后感"},{"content":"Envoy 是 Istio Service Mesh 中默认的 Sidecar，Istio 在 Enovy 的基础上按照 Envoy 的 xDS 协议扩展了其控制平面，在讲到 Envoy xDS 协议之前还需要我们先熟悉下 Envoy 的基本术语。下面列举了 Envoy 里的基本术语及其数据结构解析，关于 Envoy 的详细介绍请参考 Envoy 官方文档 ，至于 Envoy 在 Service Mesh（不仅限于 Istio） 中是如何作为转发代理工作的请参考网易云刘超的这篇深入解读 Service Mesh 背后的技术细节 以及理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持 ，本文引用其中的一些观点，详细内容不再赘述。\nEnvoy proxy 架构图 基本术语 下面是您应该了解的 Enovy 里的基本术语：\nDownstream（下游）：下游主机连接到 Envoy，发送请求并接收响应，即发送请求的主机。 Upstream（上游）：上游主机接收来自 Envoy 的连接和请求，并返回响应，即接受请求的主机。 Listener（监听器）：监听器是命名网地址（例如，端口、unix domain socket 等)，下游客户端可以连接这些监听器。Envoy 暴露一个或者多个监听器给下游主机连接。 Cluster（集群）：集群是指 Envoy 连接的一组逻辑相同的上游主机。Envoy 通过服务发现 来发现集群的成员。可以选择通过主动健康检查 来确定集群成员的健康状态。Envoy 通过负载均衡策略 决定将请求路由到集群的哪个成员。 我将在本文的后半部分解释以上术语与 Kubernetes、Istio 中概念之间的联系。\n关于 xDS 的版本 有一点需要大家注意，就是 Envoy 的 API 有 v1 和 v2 两个版本，从 Envoy 1.5.0 起 v2 API 就已经生产就绪了，为了能够让用户顺利的向 v2 版本的额 API 过度，Envoy 启动的时候设置了一个 --v2-config-only 的标志，Enovy 不同版本对 v1/v2 API 的支持详情请参考 Envoy v1 配置废弃时间表 。\nEnvoy 的作者 Matt Klein 在 Service Mesh 中的通用数据平面 API 设计 这篇文章中说明了 Envoy API v1 的历史及其缺点，还有 v2 的引入。v2 API 是 v1 的演进，而不是革命，它是 v1 功能的超集。\n在 Istio 1.0 及以上版本中使用的是 Envoy 1.8.0-dev 版本，其支持 v2 的 API，同时在 Envoy 作为 Sidecar proxy 启动的使用使用了例如下面的命令：\n$ /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster ratings --service-node sidecar~172.33.14.2~ratings-v1-8558d4458d-ld8x9.default~default.svc.cluster.local --max-obj-name-len 189 --allow-unknown-fields -l warn --v2-config-only 上面是都 Bookinfo 示例中的 rating pod 中的 sidecar 启动的分析，可以看到其中指定了 --v2-config-only，表明 Istio 1.0+ 只支持 xDS v2 的 API。\nIstio sidecar proxy 配置 假如您使用 kubernetes-vagrant-centos-cluster 部署了 Kubernetes 集群并开启了 Istio Service Mesh ，再部署 bookinfo 示例 ，那么在 default 命名空间下有一个名字类似于 ratings-v1-7c9949d479-dwkr4 的 Pod，使用下面的命令查看该 Pod 的 Envoy sidecar 的全量配置：\nkubectl -n default exec ratings-v1-7c9949d479-dwkr4 -c istio-proxy curl http://localhost:15000/config_dump \u0026gt; dump-rating.json 将 Envoy 的运行时配置 dump 出来之后你将看到一个长 6000 余行的配置文件。关于该配置文件的介绍请参考 Envoy v2 API 概览 。\nIstio 会在为 Service Mesh 中的每个 Pod 注入 Sidecar 的时候同时为 Envoy 注入 Bootstrap 配置，其余的配置是通过 Pilot 下发的，注意整个数据平面即 Service Mesh 中的 Envoy 的动态配置应该是相同的。您也可以使用上面的命令检查其他 sidecar 的 Envoy 配置是否跟最上面的那个相同。\n使用下面的命令检查 Service Mesh 中的所有有 Sidecar 注入的 Pod 中的 proxy 配置是否同步。\n$ istioctl proxy-status PROXY CDS LDS EDS RDS PILOT VERSION details-v1-876bf485f-sx7df.default SYNCED SYNCED SYNCED (100%) SYNCED istio-pilot-5bf6d97f79-6lz4x 1.0.0 ... istioctl 这个命令行工具就像 kubectl 一样有很多神器的魔法，通过它可以高效的管理 Istio 和 debug。\nEnvoy proxy 配置解析 Istio envoy sidecar proxy 配置中包含以下四个部分。\nbootstrap：Envoy proxy 启动时候加载的静态配置。 listeners：监听器配置，使用 LDS 下发。 clusters：集群配置，静态配置中包括 xds-grpc 和 zipkin 地址，动态配置使用 CDS 下发。 routes：路由配置，静态配置中包括了本地监听的服务的集群信息，其中引用了 cluster，动态配置使用 RDS 下发。 每个部分中都包含静态配置与动态配置，其中 bootstrap 配置又是在集群启动的时候通过 sidecar 启动参数注入的，配置文件在 /etc/istio/proxy/envoy-rev0.json。\n由于 bootstrap 中的配置是来自 Envoy 启动时加载的静态文件，主要配置了节点信息、tracing、admin 和统计信息收集等信息，这不是本文的重点，大家可以自行研究。\nBootstrap 是 Envoy 中配置的根本来源，Bootstrap 消息中有一个关键的概念，就是静态和动态资源的之间的区别。例如 Listener 或 Cluster 这些资源既可以从 static_resources 静态的获得也可以从 dynamic_resources 中配置的 LDS 或 CDS 之类的 xDS 服务获取。\nListener Listener 顾名思义，就是监听器，监听 IP 地址和端口，然后根据策略转发。\nListener 的特点\n每个 Envoy 进程中可以有多个 Listener，Envoy 与 Listener 之间是一对多的关系。 每个 Listener 中可以配置一条 filter 链表（filter_chains），Envoy 会根据 filter 顺序执行过滤。 Listener 可以监听下游的端口，也可以接收来自其他 listener 的数据，形成链式处理。 filter 是可扩展的。 可以静态配置，也可以使用 LDS 动态配置。 目前只能监听 TCP，UDP 还未支持。 Listener 的数据结构\nListener 的数据结构如下，除了 name、address 和 filter_chains 为必须配置之外，其他都为可选的。\n{ \u0026#34;name\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;filter_chains\u0026#34;: [], \u0026#34;use_original_dst\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;per_connection_buffer_limit_bytes\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;metadata\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;drain_type\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;listener_filters\u0026#34;: [], \u0026#34;transparent\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;freebind\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;socket_options\u0026#34;: [], \u0026#34;tcp_fast_open_queue_length\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;bugfix_reverse_write_filter_order\u0026#34;: \u0026#34;{...}\u0026#34; } 下面是关于上述数据结构中的常用配置解析。\nname：该 listener 的 UUID，唯一限定名，默认60个字符，例如 10.254.74.159_15011，可以使用命令参数指定长度限制。\naddress：监听的逻辑/物理地址和端口号，例如\n\u0026#34;address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;10.254.74.159\u0026#34;, \u0026#34;port_value\u0026#34;: 15011 } } filter_chains：这是一个列表，Envoy 中内置了一些通用的 filter，每种 filter 都有特定的数据结构，Enovy 会根据该配置顺序执行 filter。Envoy 中内置的 filter 有：envoy.client_ssl_auth 、envoy.echo 、enovy.http_connection_manager 、envoy.mongo_proxy 、envoy.rate_limit 、enovy.redis_proxy 、envoy.tcp_proxy 、http_filters 、thrift_filters 等。这些 filter 可以单独使用也可以组合使用，还可以自定义扩展，例如使用 Istio 中的 EnvoyFilter 配置 。\nuse_original_dst：这是一个布尔值，如果使用 iptables 重定向连接，则代理接收的端口可能与原始目的地址 的端口不一样。当此标志设置为 true 时，Listener 将重定向的连接切换到与原始目的地址关联的 Listener。如果没有与原始目的地址关联的 Listener，则连接由接收它的 Listener 处理。默认为 false。注意：该参数将被废弃，请使用原始目的地址 的 Listener filter 替代。该参数的主要用途是：Envoy 通过监听 15001 端口将应用的流量截取后再由其他 Listener 处理而不是直接转发出去，详情见 Virtual Listener 。\n关于 Listener 的详细介绍请参考 Envoy v2 API reference - listener 。\nRoute 我们在这里所说的路由指的是 HTTP 路由 ，这也使得 Envoy 可以用来处理网格边缘的流量。HTTP 路由转发是通过路由过滤器实现的。该过滤器的主要职能就是执行路由表 中的指令。除了可以做重定向和转发，路由过滤器还需要处理重试、统计之类的任务。\nHTTP 路由的特点\n前缀和精确路径匹配规则。 可跨越多个上游集群进行基于权重/百分比的路由 。 基于优先级 的路由。 基于哈希 策略的路由。 Route 的数据结构\n{ \u0026#34;name\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;virtual_hosts\u0026#34;: [], …","relpermalink":"/blog/envoy-proxy-config-deep-dive/","summary":"本文介绍了 Envoy proxy 的概念，对应的 xDS 的版本以及配置的详细解析。","title":"Istio 的数据平面 Envoy Proxy 配置详解"},{"content":"本视频为本人在2018年最后一天录制，演示使用 rootsongjc/kubernetes-vagrant-centos-cluster 自动部署 Kubernetes 集群和 Istio Service Mesh。\n前几天天我说到 kubernetes-vagrant-centos-cluster发布v1.2.0版本一键部署云原生实验环境，很久前在Kubernetes和 Service Mesh 社区里就有人要求我做一个视频来讲解和演示下如何安装Kubernetes和Istio Service Mesh，因为平时很忙一直某抽出时间，今天我将demo视频献上，别看视频就几分钟，为了做这个视频耗费我半个小时录制，两个小时编辑，还有多年的拍摄、剪辑、容器、虚拟机、Kubernetes、服务网格经验。与其说这是一个告别，不如说是一个新的开始。\n因为视频是首发在YouTube 上的，所以使用了英文讲解（只是一些补充说明而已，听不懂也没关系，直接看GitHub上的中文文档 即可）。\n跳转到bilibli观看 ，如果你对无人机航拍感兴趣，还可以看看Jimmy Song 的航拍作品 ，支持请投币或点赞，谢谢。\n有什么问题反馈可以发弹幕，或者在视频下面评论。\nPS. 有人会问到为什么选择使用bilibli，因为这个平台看视频没有广告，大多是Up主自己上传的，虽然二次元居多，但社区氛围还是比较好的。\n更多精彩视频请访问 Jimmy Song的 bilibli 主页 。\n","relpermalink":"/notice/cloud-native-kubernetes-service-mesh-local-demo-show/","summary":"本视频为本人在2018年最后一天录制，演示使用 rootsongjc/kubernetes-vagrant-centos-cluster 自动部署 Kubernetes 集群和 Istio Service Mesh。","title":"Kubernetes和Istio服务网格（Service Mesh）云原生本地视频Demo演示"},{"content":"本文以 Istio 官方的 bookinfo 示例 来讲解在进入 Pod 的流量被 iptables 转交给 Envoy sidecar 后，Envoy 是如何做路由转发的，详述了 Inbound 和 Outbound 处理过程。关于流量拦截的详细分析请参考理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持 。\n下面是 Istio 官方提供的 bookinfo 的请求流程图，假设 bookinfo 应用的所有服务中没有配置 DestinationRule。\nBookinfo 示例 下面是 Istio 自身组件与 Bookinfo 示例的连接关系图，我们可以看到所有的 HTTP 连接都在 9080 端口监听。\nBookinfo 示例与 Istio 组件连接关系图 可以在 Google Drive 上下载原图。\nSidecar 注入及流量劫持步骤概述 下面是从 Sidecar 注入、Pod 启动到 Sidecar proxy 拦截流量及 Envoy 处理路由的步骤概览。\n1. Kubernetes 通过 Admission Controller 自动注入，或者用户使用 istioctl 命令手动注入 sidecar 容器。\n2. 应用 YAML 配置部署应用，此时 Kubernetes API server 接收到的服务创建配置文件中已经包含了 Init 容器及 sidecar proxy。\n3. 在 sidecar proxy 容器和应用容器启动之前，首先运行 Init 容器，Init 容器用于设置 iptables（Istio 中默认的流量拦截方式，还可以使用 BPF、IPVS 等方式） 将进入 pod 的流量劫持到 Envoy sidecar proxy。所有 TCP 流量（Envoy 目前只支持 TCP 流量）将被 sidecar 劫持，其他协议的流量将按原来的目的地请求。\n4. 启动 Pod 中的 Envoy sidecar proxy 和应用程序容器。这一步的过程请参考通过管理接口获取完整配置 。\nSidecar proxy 与应用容器的启动顺序问题\n启动 sidecar proxy 和应用容器，究竟哪个容器先启动呢？正常情况是 Envoy Sidecar 和应用程序容器全部启动完成后再开始接收流量请求。但是我们无法预料哪个容器会先启动，那么容器启动顺序是否会对 Envoy 劫持流量有影响呢？答案是肯定的，不过分为以下两种情况。\n情况1：应用容器先启动，而 sidecar proxy 仍未就绪\n这种情况下，流量被 iptables 转移到 15001 端口，而 Pod 中没有监听该端口，TCP 链接就无法建立，请求失败。\n情况2：Sidecar 先启动，请求到达而应用程序仍未就绪\n这种情况下请求也肯定会失败，至于是在哪一步开始失败的，留给读者来思考。\n问题：如果为 sidecar proxy 和应用程序容器添加就绪和存活探针 是否可以解决该问题呢？\n5. 不论是进入还是从 Pod 发出的 TCP 请求都会被 iptables 劫持，inbound 流量被劫持后经 Inbound Handler 处理后转交给应用程序容器处理，outbound 流量被 iptables 劫持后转交给 Outbound Handler 处理，并确定转发的 upstream 和 Endpoint。\n6. Sidecar proxy 请求 Pilot 使用 xDS 协议同步 Envoy 配置，其中包括 LDS、EDS、CDS 等，不过为了保证更新的顺序，Envoy 会直接使用 ADS 向 Pilot 请求配置更新。\nEnvoy 如何处理路由转发 下图展示的是 productpage 服务请求访问 http://reviews.default.svc.cluster.local:9080/，当流量进入 reviews 服务内部时，reviews 服务内部的 Envoy Sidecar 是如何做流量拦截和路由转发的。可以在 Google Drive 上下载原图。\nEnvoy sidecar 流量劫持与路由转发示意图 第一步开始时，productpage Pod 中的 Envoy sidecar 已经通过 EDS 选择出了要请求的 reviews 服务的一个 Pod，知晓了其 IP 地址，发送 TCP 连接请求。\nIstio 官网中的 Envoy 配置深度解析 中是以发起 HTTP 请求的一方来详述 Envoy 做流量转发的过程，而本文中考虑的是接受 downstream 的流量的一方，它既要接收 downstream 发来的请求，自己还需要请求其他服务，例如 reviews 服务中的 Pod 还需要请求 ratings 服务。\nreviews 服务有三个版本，每个版本有一个实例，三个版本中的 sidecar 工作步骤类似，下文只以 reviews-v1-cb8655c75-b97zc 这一个 Pod 中的 Sidecar 流量转发步骤来说明。\n理解 Inbound Handler Inbound handler 的作用是将 iptables 拦截到的 downstream 的流量转交给 localhost，与 Pod 内的应用程序容器建立连接。\n查看下 reviews-v1-cb8655c75-b97zc pod 中的 Listener。\n运行 istioctl pc listener reviews-v1-cb8655c75-b97zc 查看该 Pod 中的具有哪些 Listener。\nADDRESS PORT TYPE 172.33.3.3 9080 HTTP \u0026lt;--- 接收所有 Inbound HTTP 流量，该地址即为当前 Pod 的 IP 地址 10.254.0.1 443 TCP \u0026lt;--+ 10.254.4.253 80 TCP | 10.254.4.253 8080 TCP | 10.254.109.182 443 TCP | 10.254.22.50 15011 TCP | 10.254.22.50 853 TCP | 10.254.79.114 443 TCP | 10.254.143.179 15011 TCP | 10.254.0.2 53 TCP | 接收与 0.0.0.0_15001 监听器配对的 Outbound 非 HTTP 流量 10.254.22.50 443 TCP | 10.254.16.64 42422 TCP | 10.254.127.202 16686 TCP | 10.254.22.50 31400 TCP | 10.254.22.50 8060 TCP | 10.254.169.13 14267 TCP | 10.254.169.13 14268 TCP | 10.254.32.134 8443 TCP | 10.254.118.196 443 TCP \u0026lt;--+ 0.0.0.0 15004 HTTP \u0026lt;--+ 0.0.0.0 8080 HTTP | 0.0.0.0 15010 HTTP | 0.0.0.0 8088 HTTP | 0.0.0.0 15031 HTTP | 0.0.0.0 9090 HTTP | 0.0.0.0 9411 HTTP | 接收与 0.0.0.0_15001 配对的 Outbound HTTP 流量 0.0.0.0 80 HTTP | 0.0.0.0 15030 HTTP | 0.0.0.0 9080 HTTP | 0.0.0.0 9093 HTTP | 0.0.0.0 3000 HTTP | 0.0.0.0 8060 HTTP | 0.0.0.0 9091 HTTP \u0026lt;--+ 0.0.0.0 15001 TCP \u0026lt;--- 接收所有经 iptables 拦截的 Inbound 和 Outbound 流量并转交给虚拟监听器处理 当来自 productpage 的流量抵达 reviews Pod 的时候已经，downstream 必须明确知道 Pod 的 IP 地址为 172.33.3.3 所以才会访问该 Pod，所以该请求是 172.33.3.3:9080。\nvirtual Listener\n从该 Pod 的 Listener 列表中可以看到，0.0.0.0:15001/TCP 的 Listener（其实际名字是 virtual）监听所有的 Inbound 流量，下面是该 Listener 的详细配置。\n{ \u0026#34;name\u0026#34;: \u0026#34;virtual\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;socketAddress\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;portValue\u0026#34;: 15001 } }, \u0026#34;filterChains\u0026#34;: [ { \u0026#34;filters\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;envoy.tcp_proxy\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;cluster\u0026#34;: \u0026#34;BlackHoleCluster\u0026#34;, \u0026#34;stat_prefix\u0026#34;: \u0026#34;BlackHoleCluster\u0026#34; } } ] } ], \u0026#34;useOriginalDst\u0026#34;: true } UseOriginalDst：从配置中可以看出 useOriginalDst 配置指定为 true，这是一个布尔值，缺省为 false，使用 iptables 重定向连接时，proxy 接收的端口可能与原始目的地址 的端口不一样，如此处 proxy 接收的端口为 15001，而原始目的地端口为 9080。当此标志设置为 true 时，Listener 将连接重定向到与原始目的地址关联的 Listener，此处为 172.33.3.3:9080。如果没有与原始目的地址关联的 Listener，则连接由接收它的 Listener 处理，即该 virtual Listener，经过 envoy.tcp_proxy 过滤器处理转发给 BlackHoleCluster，这个 Cluster 的作用正如它的名字，当 Envoy 找不到匹配的虚拟监听器时，就会将请求发送给它，并返回 404。这个将于下文提到的 Listener 中设置 bindToPort 相呼应。\n注意：该参数将被废弃，请使用原始目的地址 的 Listener filter 替代。该参数的主要用途是：Envoy 通过监听 15001 端口将 iptables 拦截的流量经由其他 Listener 处理而不是直接转发出去，详情见 Virtual Listener 。\nListener 172.33.3.3_9080\n上文说到进入 Inbound handler 的流量被 virtual Listener 转移到 172.33.3.3_9080 Listener，我们在查看下该 Listener 配置。\n运行 istioctl pc listener reviews-v1-cb8655c75-b97zc --address 172.33.3.3 --port 9080 -o json 查看。\n[{ \u0026#34;name\u0026#34;: \u0026#34;172.33.3.3_9080\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;socketAddress\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;172.33.3.3\u0026#34;, \u0026#34;portValue\u0026#34;: 9080 } }, \u0026#34;filterChains\u0026#34;: [ { \u0026#34;filterChainMatch\u0026#34;: { \u0026#34;transportProtocol\u0026#34;: \u0026#34;raw_buffer\u0026#34; }, \u0026#34;filters\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;envoy.http_connection_manager\u0026#34;, \u0026#34;config\u0026#34;: { ... \u0026#34;route_config\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;inbound|9080||reviews.default.svc.cluster.local\u0026#34;, \u0026#34;validate_clusters\u0026#34;: false, …","relpermalink":"/blog/envoy-sidecar-routing-of-istio-service-mesh-deep-dive/","summary":"Sidecar proxy 中 Indbound/Outbound 流量处理过程详解。","title":"理解 Istio Service Mesh 中 Envoy Sidecar 代理的路由转发"},{"content":"本文介绍了 Istio 和 Kubernetes 中的一些服务和流量的抽象模型。虽然 Istio 一开始确定的抽象模型与对接的底层平台无关，但目前来看基本绑定 Kubernetes，本文仅以 Kubernetes 说明。另外在 ServiceMesher 社区 中最近有很多关于 Istio、Envoy、Kubernetes 之中的服务模型关系的讨论，本文作为一个开篇说明，Kubernetes 和 Isito 之间有哪些共有的服务模型，Istio 在 Kubernetes 的服务模型之上又增加了什么。\n服务具有多个版本。 在 CI/CD 过程中，同一个服务可能同时部署在多个环境中，如开发、生产和测试环境等，这些服务版本不一定具有不同的 API，可能只是一些小的更改导致的迭代版本。在 A/B 测试和灰度发布中经常遇到这种情况。\nKubernetes 与 Istio 中共有的模型 因为 Istio 基本就是绑定在 Kubernetes 上，下面是我们熟知的 Kubernetes 及 Istio 中共有的服务模型。\nKubernetes 中 iptables 代理模式（另外还有 IPVS 模式）下的 service ，管理员可以在 kube-proxy 中配置简单的负载均衡，对整个 node 生效，无法配置到单个服务的负载均衡和其他微服务的高级功能，例如熔断、限流、追踪等，这些功能只能在应用中实现了，而在 Istio 的概念模型中完全去掉了 kube-proxy 这个组件，将其分散到每个应用 Pod 中同时部署的 Envoy 中实现。\n下面列举的是 Kubernetes 和 Istio 中共有的模型。\nService 这实际上跟 Kubernetes 中的 service 概念是一致的，请参考 Kubernetes 中的 service 。Istio 推出了比 service 更复杂的模型 VirtualService，这不单纯是定义一个服务了，而是在服务之上定义了路由规则。\n每个服务都有一个完全限定的域名（FQDN），监听一个或多个端口。服务还可以有与其相关联的单个负载均衡器或虚拟 IP 地址。针对 FQDN 的 DNS 查询将解析为该负载均衡器或者虚拟 IP 的地址。\n例如 Kubernetes 中一个服务为 foo.default.svc.cluster.local ，虚拟 IP /ClusterIP 是 10.0.1.1，监听的端口是 80 和 8080。\nEndpoint 这里指的是 Kubernetes 中的 endpoint，一个 endpoint 是实现了某服务的具体实例，一个服务可能有一个或者多个 Endpoint，表示为 IP 地址加端口，也可以为 DNS 名称加端口。\n其实到底哪些实例属于同一个 service，还是需要 通过 label 匹配来选择。\nLabel 服务的版本、对应的引用名称等是通过 label 来标记的，例如下面 Kubernetes 中一个应用的 YAML 配置。\napiVersion: extensions/v1beta1 kind: Deployment metadata: name: ratings-v1 spec: replicas: 1 template: metadata: labels: app: ratings version: v1 spec: containers: - name: ratings image: istio/examples-bookinfo-ratings-v1:1.8.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9080 version: v1 标记该服务是 v1 版本，version 是一个约定俗称的标签，建议大家的服务上都带上该标签。\n当然服务的 label 可以设置任意多个，这样的好处是在做路由的时候可以根据标签匹配来做细粒度的流量划分。\n数据平面 Envoy Envoy 是 Istio 中默认的 sidecar proxy，负责服务间的流量管控、认证与安全加密、可观察性等。\n下面是 Envoy 的架构图。\nEnvoy架构图 我再给大家介绍 Envoy 中的如下几个重要概念。\nCluster 集群（cluster）是 Envoy 连接到的一组逻辑上相似的上游主机。Envoy 通过服务发现 发现集群中的成员。Envoy 可以通过主动运行状况检查 来确定集群成员的健康状况。Envoy 如何将请求路由到集群成员由负载均衡策略 确定。\n这个与 Kubernetes 中的 Service 概念类似，只不过 Kubernetes 中的服务发现中并不包含健康状况检查，而是通过配置 Pod 的 liveness 和 readiness 探针 来实现，服务发现默认也是通过 DNS 来实现。\nListener 监听器（listener）是可以由下游客户端连接的命名网络位置（例如，端口、unix域套接字等）。Envoy 公开一个或多个下游主机连接的侦听器。一般是每台主机运行一个 Envoy，使用单进程运行，但是每个进程中可以启动任意数量的 Listener（监听器），目前只监听 TCP，每个监听器都独立配置一定数量的（L3/L4）网络过滤器。Listenter 也可以通过 Listener Discovery Service（LDS）动态获取。\nListener filter Listener 使用 listener filter（监听器过滤器）来操作链接的元数据。它的作用是在不更改 Envoy 的核心功能的情况下添加更多的集成功能。Listener filter 的 API 相对简单，因为这些过滤器最终是在新接受的套接字上运行。在链中可以互相衔接以支持更复杂的场景，例如调用速率限制。Envoy 已经包含了多个监听器过滤器。\nIstio 中增加的流量模型 VirtualService、DestinationRule、Gateway、ServiceEntry 和 EnvoyFilter 都是 Istio 中为流量管理所创建的 CRD，这些概念其实是做路由配置和流量管理的，而 Kubernetes 中的 service 只是用来做服务发现。Service Mesh 中真正的服务模型应该是 Envoy 的 xDS 协议 ，其中包括了服务的流量治理，服务的端点是通过 EDS 来配置的。\nIstio pilot 架构图 上图是 Pilot 设计图，来自Istio Pilot design overview 。\nRouting Kubernetes 中的 service 是没有任何路由属性可以配置的，Istio 在设计之初就通过在同一个 Pod 中，在应用容器旁运行一个 sidecar proxy 来透明得实现细粒度的路由控制。\nVirtualService VirtualService 定义针对指定服务流量的路由规则。每个路由规则都针对特定协议的匹配规则。如果流量符合这些特征，就会根据规则发送到服务注册表中的目标服务（或者目标服务的子集或版本）。对于 A/B 测试和灰度发布等场景，通常需要使用划分 subset，VirtualService 中根据 destination 中的 subset 配置来选择路由，但是这些 subset 究竟对应哪些服务示例，这就需要 DestionationRule。\nDestinationRule DestinationRule 所定义的策略，决定了经过路由处理之后的流量的访问策略。这些策略中可以定义负载均衡配置、连接池尺寸以及外部检测（用于在负载均衡池中对不健康主机进行识别和驱逐）配置。\nGateway Gateway 描述了一个负载均衡器，用于承载网格边缘的进入和发出连接。这一规范中描述了一系列开放端口，以及这些端口所使用的协议、负载均衡的 SNI 配置等内容。\n这个实际上就是定义服务网格的边缘路由。\nServiceEntry ServiceEntry 能够在 Istio 内部的服务注册表中加入额外的条目，从而让网格中自动发现的服务能够访问和路由到这些手工加入的服务。ServiceEntry 描述了服务的属性（DNS 名称、VIP、端口、协议以及端点）。这类服务可能是网格外的 API，或者是处于网格内部但却不存在于平台的服务注册表中的条目（例如需要和 Kubernetes 服务沟通的一组虚拟机服务）。\n如果没有配置 ServiceEntry 的话，Istio 实际上是无法发现服务网格外部的服务的。\nEnvoyFilter EnvoyFilter 对象描述了针对代理服务的过滤器，这些过滤器可以定制由 Istio Pilot 生成的代理配置。这一功能一定要谨慎使用。错误的配置内容一旦完成传播，可能会令整个服务网格进入瘫痪状态。\nEnvoy 中的 listener 可以配置多个 filter ，这也是一种通过 Istio 来扩展 Envoy 的机制。\n参考 Kubernetes 中的 service - jimmysong.io Istio services model - github.com Istio 文档 - istio.io ","relpermalink":"/blog/istio-service-and-traffic-model/","summary":"本文介绍了 Istio 和 Kubernetes 中的一些服务和流量的抽象模型。","title":"Istio 中的服务和流量的抽象模型"},{"content":"北美的KubeCon\u0026amp;CloudNative是每年最值得参加的云原生盛会，这次在西雅图举行，为期四天，从12月10号到13号，参考大会官网 。今年 8000 人参加，你应该留意到了Kubernetes已经越来越底层，云原生应用开发者不需要过多关注它，各大公司都在发布自己的云原生技术栈布局，包括IBM、VMware、SAP等，围绕该生态的创业公司还在不断涌现。本地大会的 PPT 分享地址：https://github.com/warmchang/KubeCon-North-America-2018 ，感谢William Zhang 对本次会议幻灯片的整理和分享。\n西雅图现场 来自 Google 的 Janet Kuo 介绍云原生技术的采用路径。\nKubeCon\u0026amp;CloudNativeCon 的同场活动，第一届 EnvoyCon 现场。\nKubeCon\u0026amp;CloudNativeCon Seattle上，IBM、Google、Mastercard、VMware各种总监、主管、VP以及Gartner分析师正在进行Scaling with Service Mesh and Istio话题讨论直播，现场的人可真多，目视也有上百人了，现场有人提问为什么当我们谈起Service Mesh大家都在谈Istio？有什么不适合Istio的使用场景。。。\n这些 PPT 中包含基础介绍、使用入门、Deep Dive 还有实际应用等，一共 200 多个，建议大家根据大会官网 上的日程挑选自己感兴趣的话题来看，不然可能看不过来。\n一点感想 KubeCon\u0026amp;CloudNativeCon 一年在办三场，欧洲、中国和北美，中国今年是第一次举办，11月在上海，据说明年要放在 6 月份举办，虽然大家都说 Kubernetes 变得 boring，但是大会上关于 Kubernetes 的内容还是很多，使用 CRD 来扩展 Kubernetes 的用法也越来越多了。Service Mesh 已经开始变的火起来了，从上面的直播的图片中就可以看到，现场的参与人数众多，相关的话题也越来也多，被誉为是后 Kubernetes 时代的微服务 ,这必将是云原生在 Kubernetes 之后的一个重要发展方向，ServiceMesher 社区 强烈关注。\n","relpermalink":"/notice/kubecon-cloudnativecon-seattle-2018/","summary":"KubeCon\u0026CloudNativeCon 西雅图2018资料分享。","title":"Kubecon\u0026CloudNativeCon Seattle 2018"},{"content":"TL;DR\nCannot ssh into a running pod/container – rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused “process_linux.go:110: decoding init error from pipe caused \u0026#34;read parent: connection reset by peer\u0026#34;” command terminated with exit code 126 #21590 Bug 影响 如果你使用的是 CentOS7，需要用到 kubectl exec 或者为 Pod 配置了基于命令返回值的健康检查（非常用的 HTTP Get 方式）的话，该 Bug 将导致命令返回错误，Pod 无法正常启动，引起大规模故障，而且也无法使用 kubectl exec 或者 docker exec 与容器交互。\n例如下面的健康检查配置：\nlivenessProbe: exec: command: - /usr/local/bin/sidecar-injector - probe - --probe-path=/health - --interval=4s failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 4 successThreshold: 1 timeoutSeconds: 1 readinessProbe: exec: command: - /usr/local/bin/sidecar-injector - probe - --probe-path=/health - --interval=4s failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 4 successThreshold: 1 timeoutSeconds: 1 以上 YAML 配置摘自 Istio 发行版中的 istio-demo.yaml 文件。\nBug 成因 根据 RedHat 的 Bug 报告 ，导致该 Bug 的原因是：\nCentOS7 发行版中的 Docker 使用的 docker-runc 二进制文件使用旧版本的 golang 构建的，这里面一些可能导致 FIPS 模式 崩溃的错误。\n至于该 Bug 是如何触发的官方只是说因为某些镜像导致的。\n发现过程 本周 Kubernetes 1.13 发布，想着更新下我的 kubernetes-vagrant-centos-cluster 使用 Vagrant 和 VirtualBox 在本地搭建分布式 Kubernetes 1.13 集群和 Istio Service Mesh 的最新版本 1.0.4， 可是在安装 Istio 的时候发现 Istio 有两个 Pod 启动不起来，istio-sidecar-injector 和 istio-galley 这两个 Pod，检查其启动过程，发现它们都是因为 Readiness Probe 和 Liveness Probe 失败导致的。再联想到之前安装较老版本的 Istio 的时候也遇到该问题，见 Increase health probe interval #6610 通过增加健康检查的时间间隔可以解决该问题，可是经过反复的测试后发现还是不行。然后我想到先去掉健康检查，然后我手动使用 kubectl exec 来执行健康检查的命令，解决却遇到下面的错误：\n$ kubectl exec -it istio-sidecar-injector-6fc974b6c8-pts4t -- istio-sidecar-injector-b484dfcbb-9x9l9 probe --probe-path=/health --interval=4s Cannot ssh into a running pod/container -- rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused \u0026#34;process_linux.go:110: decoding init error from pipe caused \u0026#34;read parent: connection reset by peer\u0026#34;\u0026#34; command terminated with exit code 126 然后直接到 Pod 所在的主机使用 docker exec 命令执行，依然报上面的错误，我就确定这不是 Kubernetes 的问题了。更何况前之前 kubernetes-vagrant-centos-cluster 屡试不爽，突然出现问题，有点让人摸不着头脑。知道我搜到了这个四天前才有人提出的 issue 。根据网友反馈，现在 kubernetes-vagrant-centos-cluster 中已经通过降级 Docker 的方式临时修复了该问题，并支持 Kubernetes 1.13 和 Istio 1.0.4，欢迎试用。\n解决方法 有两种解决方法，都需要替换 Docker 版本。\n一、降级到旧的 RedHat CentOS 官方源中的 Docker 版本\n将 RedHat 官方源中的 Docker 版本降级，这样做的好处是所有的配置无需改动，参考 https://github.com/openshift/origin/issues/21590 。\n查看 Docker 版本：\n$ rpm -qa | grep -i docker docker-common-1.13.1-84.git07f3374.el7.centos.x86_64 docker-client-1.13.1-84.git07f3374.el7.centos.x86_64 docker-1.13.1-84.git07f3374.el7.centos.x86_64 降级 Docker 版本。\nyum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64 降级之后再查看 Docker 版本：\n$ rpm -qa | grep -i docker docker-common-1.13.1-75.git8633870.el7.centos.x86_64 docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 此为临时解决方法，RedHat 也在着手解决该问题，为了可能会提供补丁，见 Bug 1655214 - docker exec does not work with registry.access.redhat.com/rhel7:7.3。\n二、更新到 Docker-CE\n众所周知，Docker 自1.13版本之后更改了版本的命名方式，也提供了官方的 CentOS 源，替换为 Docker-CE 亦可解决该问题，不过 Docker-CE 的配置可能会与 Docker 1.13 有所不同，所以可能需要修改配置文件。\n参考 配置Pod的liveness和readiness探针 - jimmysong.io Bug 1655214 - docker exec does not work with registry.access.redhat.com/rhel7:7.3 - redhat.com kubernetes-vagrant-centos-cluster - github.com FIPS Mode - an explanation - mozilla.org ","relpermalink":"/blog/docker-exec-bug-on-centos7/","summary":"CentOS7 官方 Docker 1.13 版本 Bug 导致 docker exec 失败，可致 Kubernetes 中的检测探针失败，官方推荐降级 docker 版本解决。","title":"CentOS7 官方 Docker 发行版现重大 Bug"},{"content":"今天给大家分享的是《软件定义交付宣言》，该宣言发出已经有一周多时间了，目前该宣言的官方网站（https://sdd-manifesto.org/ 已停止维护）还在联署签名中。\n云原生通过不可变基础设施与声明式配置，作为了软件定义交付的基础，再假以持续交付工具可以极大的提高软件交付效率，本宣言的起草者中包含众多云原生理念的鉴定拥护者如 Kenny Bastani、Matt Stine 等。\n该宣言通过 GitHub 协作草拟（https://github.com/sdd-manifesto/manifesto ），仍未达到1.0版本。下面是《软件定义交付宣言》的中文版。\n软件定义交付宣言 我们从日常生产和实践中认识到软件塑造了我们的世界。我们认识到代码才是指定精确操作的最佳方式。我们认识到代码仅在被交付时才有用。\n开发的软件被能够被交付出去就软件本身存在的目的。现在是时候将我们的核心技能应用到实际的工作中去了。是时候对交付去做出 设计 了。我们将区别人类和计算机在交付工作中承担的作用：人类做决策，计算机来完成自动化任务。\n每一次交付工作本质上都是独一无二的。应用程序、组织、部署环境和团队组合千差万别。我们认识到每个团队都需要能够理解这种交付的独特性和对交付做自动化。我们认识到，虽然持续交付对满足业务需求至关重要，但自动执行所有重复任务也非常重要。\n我们使用与加速应用程序开发类似的方式来加速软件交付：使用现代架构和编程语言，通用功能的框架、库和服务。\n交付基础设施现在是可编程的，我们将对其进行编程。\n软件定义交付是指 核心：交付是每个软件团队和组织的基础和战略能力。\n优先：交付的代码是生产代码。 战略：决定团队和组织层面的策略；在代码中实现精确控制。 不断发展：不断改进交付。 工程设计：强大，可测试的代码。70年代的脚本语言是不够的。\n现代软件架构：事件驱动和可扩展。 现代编程语言：逻辑最好用代码指定，而不是图片或GUI。脚本不好扩展。 基于模型：由软件领域的模型支持，具有对代码的理解。 可测试：在生产之前启用小规模应用以发现错误。 协作：\n从群众中来：所有人都可以在代码中表述自己的专业知识，这对大家都有利。 到软件中去：使用最好的工具，但将它们结合起来之后就是独一无二的。 在人与软件之间：协同自动化可以增强我们的感知和帮助我们做决策。将信息落实到行动，使我们能够体察软件的自动化行为。通过代码来区分团队的共享交付目标集及其实现。 加速：\n通过自动化：自动执行重复任务，加快了工作速度还可以避免错误发生。 通过重用：在开发人员、团队和组织之间共享通用功能。 可观察性：通常用于观察和排除作为生产系统的交付过程中发生的情况。\n跟踪：观察系统中的活动并跟踪操作之间的关系。 调试：检查和与交付流程交互。 指标：在整个交付流程的活动中获取指标。 作者：(姓氏按字母顺序排列）：本宣言由 Kenny Bastani、Marc Holmes、Rod Johnson、Jessica Kerr、Mik Kersten、Russ Miles、Erin Schnabel、Matt Stine 及其他社区成员草拟。\n©2018，上述作者和本声明可以任何形式自由复制，但需全文复制本声明。\n","relpermalink":"/blog/software-defined-delivery-manifesto/","summary":"今天给大家分享的是《软件定义交付宣言》，该宣言发出已经有一周多时间了，还在联署签名中。","title":"软件定义交付（SDD）宣言"},{"content":"本次大会是一个难得的机会，让我见到了很多朋友，很多都是第一次在线下见面。\nKubeCon\u0026amp;CloudNativeCon China 上海 2018 图中由上自下的大合影是：ServiceMesher 社区上海聚首合影；中美日的Kubernetes、Envoy、Istio、Apache Skywalking、ServiceMesher社区在KubeCon上海；Yahoo Japan 与蚂蚁集团团队在上海中心办公室合影。\n活动 Meet the Ambassadors\n参加了一场 Meet the Ambassadors 采访，第一次参加英文的采访，本来准备的英文回答没用上，现场反而还紧张了。。。工作人员的摄像机又没就位，我还充当了摄影师（本色出演）。一共采访了四位中国的 Ambassador。\nJiayao (Julia) Han, Caicloud Jia Xuan, China Mobile Research Institute Jimmy Song, Ant Group Jessie Qian, Alauda 我们的名字不约而同的都是 J 字头。\nService Mesh Roundtable\n然后参加了一场 Service Mesh Roundtable，参加人员有：\nJimmy Song, Developer Advocate on Cloud Native at Ant Group Yulin Son, Principal Architect at Huawei George Miranda, PagerDuty Nic Jackson, Developer Advocate at HashiCorp 我们就 Service Mesh 的现状，存在的问题后未来进行了广泛的探讨。\nPPT KubeCon China 会议的很多 PPT 在大会的官网 上都可以下载，或者通过百度网盘 下载 zip 包，提取码：5vn0。或者通过GitHub 下载单个 PPT。\n飞行 在会场随便一坐，就能遇到熟人。还有很多我都叫不上名字的人来打招呼，不能一一道谢了，感谢晚餐以及收到的Prometheus飞行袜。\n11月12日晚在上海静安寺上空飞行，航拍的南京西路夜景。\n","relpermalink":"/blog/kubecon-cloudnativecon-china-2018/","summary":"KubeCon\u0026CloudNativeCon China 2018参会回顾。","title":"KubeCon\u0026CloudNativeCon China 2018"},{"content":"本文最新更新于 2022 年 3 月 7 日。\n以往有很多文章讲解 Istio 是如何做 Sidecar 注入的，但是没有讲解注入之后 Sidecar 工作的细节。本文将带大家详细了解 Istio 是如何将 Envoy 作为 Sidecar 的方式注入到应用程序 Pod 中，及 Sidecar 是如何做劫持流量的。\n在讲解 Istio 如何将 Envoy 代理注入到应用程序 Pod 中之前，我们需要先了解以下几个概念：\nSidecar 模式：容器应用模式之一，Service Mesh 架构的一种实现方式。 Init 容器：Pod 中的一种专用的容器，在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。 iptables：流量劫持是通过 iptables 转发实现的。 查看目前 reviews-v1-745ffc55b7-2l2lw Pod 中运行的容器：\n$ kubectl -n default get pod reviews-v1-745ffc55b7-2l2lw -o=jsonpath=\u0026#39;{..spec.containers[*].name}\u0026#39; reviews istio-proxy reviews 即应用容器，istio-proxy 即 Envoy 代理的 sidecar 容器。另外该 Pod 中实际上还运行过一个 Init 容器，因为它执行结束就自动终止了，所以我们看不到该容器的存在。关注 jsonpath 的用法请参考 JSONPath Support 。\nSidecar 模式 在了解 Istio 使用 Sidecar 注入之前，需要先说明下什么是 Sidecar 模式。Sidecar 是容器应用模式的一种，也是在 Service Mesh 中发扬光大的一种模式，详见 Service Mesh 架构解析 ，其中详细描述了节点代理和 Sidecar 模式的服务网格架构。\n使用 Sidecar 模式部署服务网格时，无需在节点上运行代理（因此您不需要基础结构的协作），但是集群中将运行多个相同的 Sidecar 副本。从另一个角度看：我可以为一组微服务部署到一个服务网格中，你也可以部署一个有特定实现的服务网格。在 Sidecar 部署方式中，你会为每个应用的容器部署一个伴生容器。Sidecar 接管进出应用容器的所有流量。在 Kubernetes 的 Pod 中，在原有的应用容器旁边运行一个 Sidecar 容器，可以理解为两个容器共享存储、网络等资源，可以广义的将这个注入了 Sidecar 容器的 Pod 理解为一台主机，两个容器共享主机资源。\n下图展示的是 Service Mesh 的架构图，其中的位于每个 Pod 中的 proxy 组成了数据平面，而这些 proxy 正是以 sidecar 模式运行的。\nIstio 架构 注意：下文中所指的 Sidecar 都是指的 Envoy 代理容器。\nInit 容器 Init 容器是一种专用容器，它在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。\n一个 Pod 中可以指定多个 Init 容器，如果指定了多个，那么 Init 容器将会按顺序依次运行。只有当前面的 Init 容器必须运行成功后，才可以运行下一个 Init 容器。当所有的 Init 容器运行完成后，Kubernetes 才初始化 Pod 和运行应用容器。\nInit 容器使用 Linux Namespace，所以相对应用程序容器来说具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用程序容器则不能。\n在 Pod 启动过程中，Init 容器会按顺序在网络和数据卷初始化之后启动。每个容器必须在下一个容器启动之前成功退出。如果由于运行时或失败退出，将导致容器启动失败，它会根据 Pod 的 restartPolicy 指定的策略进行重试。然而，如果 Pod 的 restartPolicy 设置为 Always，Init 容器失败时会使用 RestartPolicy 策略。\n在所有的 Init 容器没有成功之前，Pod 将不会变成 Ready 状态。Init 容器的端口将不会在 Service 中进行聚集。 正在初始化中的 Pod 处于 Pending 状态，但应该会将 Initializing 状态设置为 true。Init 容器运行完成以后就会自动终止。\n关于 Init 容器的详细信息请参考 Init 容器 - Kubernetes 中文指南/云原生应用架构实践手册 。\nSidecar 注入示例分析 本文我们将以 Istio 官方示例 bookinfo 中 reivews 服务为例，来接讲解 Sidecar 容器注入的额流程，每个注入了 Sidecar 的 Pod 中除了原先应用的应用本身的容器外，都会多出来这样两个容器：\nistio-init：用于给 Sidecar 容器即 Envoy 代理做初始化，设置 iptables 端口转发 istio-proxy：Envoy 代理容器，运行 Envoy 代理 接下来将分别解析下这两个容器。\nInit 容器解析 Istio 在 Pod 中注入的 Init 容器名为 istio-init，如果你查看 reviews Deployment 配置，你将看到其中 initContaienrs 的启动参数：\ninitContainers: - name: istio-init image: docker.io/istio/proxyv2:1.13.1 args: - istio-iptables - \u0026#39;-p\u0026#39; - \u0026#39;15001\u0026#39; - \u0026#39;-z\u0026#39; - \u0026#39;15006\u0026#39; - \u0026#39;-u\u0026#39; - \u0026#39;1337\u0026#39; - \u0026#39;-m\u0026#39; - REDIRECT - \u0026#39;-i\u0026#39; - \u0026#39;*\u0026#39; - \u0026#39;-x\u0026#39; - \u0026#39;\u0026#39; - \u0026#39;-b\u0026#39; - \u0026#39;*\u0026#39; - \u0026#39;-d\u0026#39; - 15090,15021,15020 我们看到 istio-init 容器的入口是 istio-iptables 命令，该命令是用于初始化路由表的。\nInit 容器启动入口 Init 容器的启动入口是 /usr/local/bin/istio-iptable 命令，该命令的用法如下：\n$ istio-iptables -p PORT -u UID -g GID [-m mode] [-b ports] [-d ports] [-i CIDR] [-x CIDR] [-h] -p: 指定重定向所有 TCP 流量的 Envoy 端口（默认为 $ENVOY_PORT = 15001） -u: 指定未应用重定向的用户的 UID。通常，这是代理容器的 UID（默认为 $ENVOY_USER 的 uid，istio_proxy 的 uid 或 1337） -g: 指定未应用重定向的用户的 GID。（与 -u param 相同的默认值） -m: 指定入站连接重定向到 Envoy 的模式，“REDIRECT” 或 “TPROXY”（默认为 $ISTIO_INBOUND_INTERCEPTION_MODE) -b: 逗号分隔的入站端口列表，其流量将重定向到 Envoy（可选）。使用通配符 “*” 表示重定向所有端口。为空时表示禁用所有入站重定向（默认为 $ISTIO_INBOUND_PORTS） -d: 指定要从重定向到 Envoy 中排除（可选）的入站端口列表，以逗号格式分隔。使用通配符“*” 表示重定向所有入站流量（默认为 $ISTIO_LOCAL_EXCLUDE_PORTS） -i: 指定重定向到 Envoy（可选）的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量。空列表将禁用所有出站重定向（默认为 $ISTIO_SERVICE_CIDR） -x: 指定将从重定向中排除的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量（默认为 $ISTIO_SERVICE_EXCLUDE_CIDR）。 -z: 所有入站 TCP 流量重定向端口（默认为 $INBOUND_CAPTURE_PORT 15006） 关于该命令的详细代码请查看 GitHub：tools/istio-iptables/pkg/cmd/root.go 。\n再参考 istio-init 容器的启动参数，完整的启动命令如下：\n$ /usr/local/bin/istio-iptables -p 15001 -z 15006 -u 1337 -m REDIRECT -i \u0026#39;*\u0026#39; -x \u0026#34;\u0026#34; -b * -d \u0026#34;15090,15201,15020\u0026#34; 该容器存在的意义就是让 Envoy 代理可以拦截所有的进出 Pod 的流量，即将入站流量重定向到 Sidecar，再拦截应用容器的出站流量经过 Sidecar 处理后再出站。\n命令解析\n这条启动命令的作用是：\n将应用容器的所有流量都转发到 Envoy 的 15006 端口。 使用 istio-proxy 用户身份运行， UID 为 1337，即 Envoy 所处的用户空间，这也是 istio-proxy 容器默认使用的用户，见 YAML 配置中的 runAsUser 字段。 使用默认的 REDIRECT 模式来重定向流量。 将所有出站流量都重定向到 Envoy 代理。 将除了 15090、15201、15020 端口以外的所有端口的流量重定向到 Envoy 代理。 因为 Init 容器初始化完毕后就会自动终止，因为我们无法登陆到容器中查看 iptables 信息，但是 Init 容器初始化结果会保留到应用容器和 Sidecar 容器中。\nistio-proxy 容器解析 为了查看 iptables 配置，我们需要登陆到 Sidecar 容器中使用 root 用户来查看，因为 kubectl 无法使用特权模式来远程操作 docker 容器，所以我们需要登陆到 reviews Pod 所在的主机上使用 docker 命令登陆容器中查看。\n查看 reviews Pod 所在的主机。\n$ kubectl -n default get pod -l app=reviews -o wide NAME READY STATUS RESTARTS AGE IP NODE reviews-v1-745ffc55b7-2l2lw 2/2 Running 0 1d 172.33.78.10 node3 从输出结果中可以看到该 Pod 运行在 node3 上，使用 vagrant 命令登陆到 node3 主机中并切换为 root 用户。\n$ vagrant ssh node3 $ sudo -i 查看 iptables 配置，列出 NAT（网络地址转换）表的所有规则，因为在 Init 容器启动的时候选择给 istio-iptables.sh 传递的参数中指定将入站流量重定向到 Envoy 的模式为 “REDIRECT”，因此在 iptables 中将只有 NAT 表的规格配置，如果选择 TPROXY 还会有 mangle 表配置。iptables 命令的详细用法请参考 iptables ，规则配置请参考 iptables 规则配置 。\n理解 iptables iptables 是 Linux 内核中的防火墙软件 netfilter 的管理工具，位于用户空间，同时也是 netfilter 的一部分。Netfilter 位于内核空间，不仅有网络地址转换的功能，也具备数据包内容修改、以及数据包过滤等防火墙功能。\n在了解 Init 容器初始化的 iptables 之前，我们先来了解下 iptables 和规则配置。\n下图展示了 iptables 调用链。\niptables 调用链 iptables 中的表 Init 容器中使用的的 iptables 版本是 v1.6.0，共包含 5 张表：\nraw 用于配置数据包，raw 中的数据包不会被系统 …","relpermalink":"/blog/envoy-sidecar-injection-in-istio-service-mesh-deep-dive/","summary":"以往有很多文章讲解 Istio 是如何做 Sidecar 注入的，但是没有讲解注入之后 Sidecar 工作的细节。本文将带大家详细了解 Istio 是如何将 Envoy 作为 Sidecar 的方式注入到应用程序 Pod 中，及 Sidecar 是如何做劫持流量的。","title":"理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持"},{"content":"这是一篇后Kubernetes时代的檄文。就在今天傍晚我看到了一篇 Bilgin Ibryam的文章 Microservices in a Post-Kuberentes Era 有感而发。\n2017年4月9日，Kubernetes Handbook - Kubernetes中文指南/云原生应用架构实践手册 第一次提交。在过去的16个月的时间里，有53位贡献者参与，1088次commit，共写了23,9014个汉字，同时在Kubernetes\u0026amp;Cloud Native实战群里也聚集了几千名爱好者。\n距离上一版本发布已经有4个多月的时间了，在此期间Kubernetes 和Prometheus 分别从CNCF中毕业，已经在商业上成熟，这两个项目基本成型，未来也不会有太大的变动。而当初为容器编排而开发，为了解决微服务部署问题的Kubernetes已经深入人心，目前的微服务已经逐步进入后Kubernetes时代，Service Mesh和云原生重新定义微服务和分布式应用。\n本版本发布时PDF大小为108M，共239,014个汉字，建议在线浏览 ，或克隆本项目安装Gitbook命令后自行编译。\n本版本主要有以下改进：\n增加了Istio Service Mesh教程 增加了 使用Vagrant和VirtualBox在本地搭建分布式Kubernetes集群和Istio Service Mesh 增加了对云原生编程语言Ballerina 和Pulumi 的介绍 增加了快速开始指南 增加了对Kubernetes 1.11的支持 增加了企业级Service Mesh采用路径指南 增加了SOFAMesh章节 增加了对云原生未来的展望 增加了CNCF章程 和参与事项 增加了Docker镜像仓库的注意事项 增加了Envoy章节 增加了KCSP（Kubernetes认证服务提供商） 和CKA（认证Kubernetes管理员） 相关说明 更新了一些配置文件、YAML和参考链接 更新了CRI章节 删除了过时的描述 改进了etcdctl的命令使用教程 修复了一些笔误 浏览与下载 在线浏览：https://jimmysong.io/kubernetes-handbook GitHub 地址：https://github.com/rootsongjc/kubernetes-handbook 为了方便大家下载，我放了一份在微云 上，提供PDF（108MB）、MOBI（42MB）、EPUB（53MB）格式下载。 感谢 Kubernetes 热心用户对本书的支持，感谢各位Contributors ，在该版本发布之前的几个月内又合作成立了ServiceMesher社区 ，作为后Kubernetes时代的一支生力军，欢迎联系我 加入社区，共同开创云原生新时代。\n目前ServiceMesher社区微信群也有几千名成员，Kubernete Handbook仍然会继续下去，但是Service Mesh已然是一颗冉冉上升的新星，在对Kubernetes已了然于胸的情况下，欢迎加入ServiceMesher 社区 。\n","relpermalink":"/notice/new-kubernetes-handbook-released-and-say-hello-to-post-kubernetes-era/","summary":"这是一篇后Kubernetes时代的檄文，Kubernetes handbook by Jimmy Song v1.4发布，云原生的下一个重心是Service Mesh！","title":"Kubernetes Handbook v1.4发布同时后Kubernetes时代大幕拉启"},{"content":"今天，我们很高兴地宣布 Istio 1.0 。这距离最初的 0.1 版本发布以来已经过了一年多时间了。从 0.1 起，Istio 就在蓬勃发展的社区、贡献者和用户的帮助下迅速发展。现在已经有许多公司成功将 Istio 应用于生产，并通过 Istio 提供的洞察力和控制力获得了真正的价值。我们帮助大型企业和快速发展的创业公司，如 eBay 、Auto Trader UK 、Descartes Labs 、HP FitStation 、Namely 、PubNub 和 Trulia 使用 Istio 从头开始连接、管理和保护他们的服务。将此版本作为 1.0 发布是对我们构建了一组核心功能的认可，用户们可以依赖这些功能进行生产。\n生态系统 去年，我们看到了 Istio 生态系统的大幅增长。Envoy 继续其令人印象深刻的增长，并增加了许多对生产级别服务网格至关重要的功能。像 Datadog 、 SolarWinds 、 Sysdig 、Google Stackdriver 和 Amazon CloudWatch 这样的可观察性提供商也编写了插件来将 Istio 与他们的产品集成在一起。Tigera 、Aporeto 、Cilium 和 Styra 为我们的策略实施和网络功能构建了扩展。Red Hat 构建的 Kiali 为网格管理和可观察性提供了良好的用户体验。Cloud Foundry 正在为 Istio 建立下一代流量路由堆栈，最近宣布的 Knative 无服务器项目也正在做同样的事情，Apigee 宣布计划在他们的 API 管理解决方案中使用它。这些只是社区去年增加的项目的一些汇总。\n功能 自 0.8 发布以来，我们添加了一些重要的新功能，更重要的是将许多现有的功能标记为 Beta 表明它们可以用于生产。这在发行说明 中有更详细的介绍，但值得一提是：\n现在可以将多个 Kubernetes 集群添加到单个网格中 ，并启用跨集群通信和一致的策略实施。多集群支持现在是 Beta。 通过网格实现对流量的细粒度控制的网络 API 现在是 Beta。使用网关显式建模 ingress 和 egress 问题，允许运维人员控制网络拓扑 并满足边缘的访问安全要求。 现在可以增量上线 双向 TLS，而无需更新服务的所有客户端。这是一项关键功能，可以解除在现有生产上部署采用 Istio 的障碍。 Mixer 现在支持开发进程外适配器 。这将成为在即将发布的版本中扩展 Mixer 的默认方式，这将使构建适配器更加简单。 现在，Envoy 在本地完全评估了控制服务访问的授权策略 ，从而提高了它们的性能和可靠性。 Helm chart 安装 现在是推荐的安装方法，提供丰富的自定义选项，以便根据您的需求配置 Istio。 我们在性能方面投入了大量精力，包括连续回归测试、大规模环境模拟和目标修复。我们对结果非常满意，并将在未来几周内详细分享。 下一步 虽然这是该项目的一个重要里程碑，但还有很多工作要做。在与采用者合作时，我们已经获得了很多关于下一步要关注的重要反馈。我们已经听到了关于支持混合云、安装模块化、更丰富的网络功能和大规模部署可扩展性的一致主题。我们在 1.0 版本中已经考虑到了一些反馈，在未来几个月内我们将继续积极地处理这些工作。\n快速开始 如果您是 Istio 的新手，并希望将其用于部署，我们很乐意听取您的意见。查看我们的文档 ，访问我们的聊天论坛 或访问邮件列表 。如果您想更深入地为该项目做出贡献，请参加我们的社区会议 并打个招呼。\n最后 Istio 团队非常感谢为项目做出贡献的每个人。没有你们的帮助，它不会有今天的成就。去年的成就非常惊人，我们期待未来与我们社区成员一起实现更伟大的成就。\nServiceMesher 社区 负责了 Istio 官网中文内容的翻译和维护工作，目前中文内容还未完全与英文内容同步，需要手动输入 URL 切换为中文（https://istio.io/zh ），还有很多工作要做，欢迎大家加入和参与进来。\n","relpermalink":"/notice/istio-v1-released/","summary":"中文文档同时释出!","title":"Istio 1.0发布，生态逐步壮大，且可用于生产！"},{"content":" 注意 SOFAMesh 已闭源，本文已过时。 4 月，蚂蚁集团自主研发的分布式中间件（Scalable Open Financial Architecture，以下简称 SOFA ）启动开源计划，并开放多个组件，（相关背景请点击链接阅读《开源 | 蚂蚁集团启动分布式中间件开源计划，用于快速构建金融级云原生架构 》、《开源 | 蚂蚁集团分布式中间件开源第二弹：丰富微服务架构体系 》），这一系列的动作受到大家的关注和支持，SOFA 社区也日益壮大。\n在两轮开源之后，蚂蚁集团自主研发的分布式中间件（Scalable Open Financial Architecture，以下简称 SOFA ）在今天推出了第三轮的开源产品：SOFAMesh。和前两轮开源的历经多年沉淀和打磨的成熟产品不同，本轮的开源主角 SOFAMesh，将探索一条和以往产品有所不同的开源道路。下面我们就来看看到底有哪些不同吧！\nSOFAMesh 的开源探索之路 SOFAMesh 尝试在以下几个方面进行自我突破和勇敢探索：\n全新的技术领域\nService Mesh 是目前技术社区最为炙手可热的新技术方向，有下一代微服务的明显趋势。但是目前 Service Mesh 技术还处于发展早期，暂时还没有成熟的产品，尤其缺乏大规模的落地实践。\n较早的开源时间\n在上述背景下，我们选择了将启动不久的 Service Mesh 产品开源在开发早期，也就是还未成熟之时，就对社区开放，开放源码并寻求社区合作。\n更加开放的态度\n在 SOFAMesh 上，我们愿意以开源共建的方式来和社区一起推进 Service Mesh 技术的更好发展和实现落地实践，共同打造一个技术先进，功能丰富，具备良好的性能和稳定性，可以实实在在的生产落地的优秀产品。欢迎国内技术社区的朋友们和我们开展不同层面的交流与合作。\n务实的产品路线\nSOFAMesh 在产品路线上，选择了跟随社区主流，我们选择了目前 Service Mesh 中最有影响力和前景的 Istio。SOFAMesh 会在 Istio 的基础上，提升性能，增加扩展性，并在落地实践上做探索和补充，以弥补目前 Istio 的不足，同时保持与 Istio 社区的步骤一致和持续跟进。\nSOFAMesh 介绍 SOFAMesh 将在兼容 Istio 整体架构和协议的基础上，做出部分调整：\n使用 Golang 语言开发全新的 Sidecar，替代 Envoy 为了避免 Mixer 带来的性能瓶颈，合并 Mixer 部分功能进入 Sidecar Pilot 和 Citadel 模块进行了大幅的扩展和增强 我们的目标：打造一个更加务实的 Istio 落地版本！\n备注 以上架构调整的细节以及我们做调整的出发点和原因，请浏览 蚂蚁集团大规模微服务架构下的 Service Mesh 探索之路 一文，有非常详尽的解释。 开源内容 在本轮开源中，我们将推出 SOFAMesh 目前正在开发的两大模块：MOSN 和 SOFAPilot。\nMOSN SOFAMesh 中 Golang 版本的 Sidecar，是一个名为 MOSN (Modular Observable Smart Netstub) 的全新开发的模块，实现 Envoy 的功能，兼容 Envoy 的 API，可以和 Istio 集成。\nMOSN 架构图 此外，我们会增加对 SOFARPC、Dubbo 等通讯协议的支持，以便更好的迎合国内用户包括我们自身的实际需求。\n由于 Sidecar 相对独立，而且我们也预期会有单独使用 MOSN 的场景，因此 MOSN 的代码仓库是独立于 SOFAMesh 的，地址为： https://github.com/mosn/mosn 欢迎大家使用，提供需求、反馈问题、贡献代码或者合作开发。\nSOFAPilot 我们将大幅扩展和增强 Istio 中的 Pilot 模块：\n增加 SOFARegistry 的 Adapter，提供超大规模服务注册和发现的解决方案 增加数据同步模块，以实现多个服务注册中心之间的数据交换。 增加 Open Service Registry API，提供标准化的服务注册功能 MOSN 和 SOFAPilot 配合，将可以提供让传统侵入式框架（如 Spring Cloud，Dubbo，SOFA RPC 等）和 Service Mesh 产品可以相互通讯的功能，以便可以平滑的向 Service Mesh 产品演进和过渡。\nPilot 和后面会陆续开放的 Mixer，Citadel 等 Istio 模块，会统一存放在同一个从 Istio Fork 出来的代码仓库中。未来会持续更新 Istio 最新代码，以保持和 Istio 的一致。\n附录 本文中提到的链接地址集合：\nMOSN 蚂蚁集团大规模微服务架构下的 Service Mesh 探索之路 ","relpermalink":"/blog/sofamesh-and-mosn-proxy-sidecar-service-mesh-by-ant-financial/","summary":"蚂蚁集团开源 SOFAMesh—一款基于 Istio 改进和扩展而来的 Service Mesh 大规模落地实践方案。","title":"蚂蚁集团开源 SOFAMesh"},{"content":" 2018年6月18日 Joe Duffy在他的博客 中宣布开源了云原生编程语言Pulumi 。这是继Ballerina 之后我看到的另一款云原生编程语言，他们之间有一些共同的特点，例如都是为了支持多种云环境，基于不可变基础设施和基础设施即代码的理念构建，使云原生应用的集成更加方便，但也有一些不同，Ballerina是直接创建了一个基于JVM的语言，而Pulumi是为不同编程语言构建了SDK。\n下文是对\tHello, Pulumi! 一文的翻译。\n今天我们发布了Pulumi，这是一款开源的云开发平台。有了Pulumi，您可以使用自己最喜欢的编程语言来开发云应用程序，可以直接跨过底层的基础设施即代码直接开发更高效更具生产力的现代容器和serverless应用。一年多前我们发起了Pulumi，我们都为自己在这一年多取得的成绩感到惊讶。这是我们开源的第一步，也是我们做出的一步重大跨越，我们期望与您分享我们的成就。\nPulumi支持多语言、混合云环境、完全可扩展。初期支持JavaScript、TypeScript、Python和Go语言，支持AWS、Azure、GCP云平台，另外还支持所有兼容Kubernetes的公有云、私有云和混合云。Pulumi实现了一种单一、一致的编程模型，一组编程工具，可管理所有以上环境，丰富的生态系统支持大量可复用的包。使用真实的语言来改变一切。\nTL;DR 有了Pulumi，38页的手动操作说明将变成了38行代码。25000行YAML配置变成了使用真实编程语言的500行语句。\nPulumi的整个运行时、CLI、支持的库都可以在GitHub上免费下载。我们的团队正急切的等待您的反馈。与此同时，我需要告诉您一些关于Pulumi的事情，为什么我们会创造它。\n为什么创造Pulumi？ 我的背景是100%在开发者工具上。我是.NET的早期工程师，设计了其中的并发和异构部分，领导的分布式操作系统的变成平台，管理微软语言小组，包括开源和使.NET Core跨平台。因为这些背景，进入云领域我有自己的见解。\n我发现的东西显然无法吸引我。\n2016年年末的时候我就开始跟我的朋友也是共同创始人Eric Rudder开始构思Pulumi，那时候容器和serverless已经甚嚣尘上，但是距离落地还为时尚早。云的能力十分惊人，但是至今将使用它还是十分困难。\n对于每一个serverless函数来说，我都要写几十行的JSON或者YAML配置。要链接到一个API端点，我还要学习晦涩的概念，执行一系列复制-粘贴的低级工作。如果我想在本机上运行一个小的集群的话，那么Docker还是很棒的，但是如果要在生产上使用的话，那么就要手动管理etcd集群，配置网络和iptables路由表，还有一系列与我的应用程序本身不相干的事情。不过Kubernetes的出现至少让我可以配置一次下次就可以跨云平台重用，但这还是会分散开发人员的精力。\n我认为我还算一个经验丰富的工程师，已经在软件行业从业20年了，但是当我想要将自己的代码部署到云中的时候，我感觉自己就像是个傻子。真是太令人悲哀了！如果我掌握了这些能力，那么是世界就会出触手可及。我总是在淌这浑水，处理云的复杂性，而我真正想做的是花时间来创造业务价值。\n关于编程的许多方面都经历了类似的转变过程：\n在80年代初，我们使用汇编语言对微处理器进行了编程。最终，编译器技术进步了，我们可以同时处理多种常见的架构。像FORTRAN和C这样的Low-level的编程语言开始兴起。 在90年代初期，我们直接针对低级别操作系统原语进行编程，无论是POSIX系统调用还是Win32 API，并进行手动内存和资源管理。最终，语言运行时技术和处理器速度提升到了可以使用更高级别语言的状态，如Java。除了动态语言之外，这种趋势已经加速，如JavaScript统治了Web。 在21世纪初期，我们的编程模型中的共享内存并发性最好是原始的（我花了很多时间在这个问题上 ）。现在，我们简单地假设OS具有高级线程共享、调度和异步IO功能，以及编程到更高级别的API，例如任务和承诺。 我相信云软件也在进行类似的转变。从构建单一应用程序到构建真正的云优先分布式系统，我们正处在一场巨变中。然而，当海啸发生之前，人们几乎不知道它正在发生。\n从上面的角度来看，使用“配置”情况是有道理的。在虚拟机的早期，我们利用现有的应用程序并将它们扔在栅栏上，以便有人添加一点INI或XML粘合剂，让它们在虚拟机内部运行，以实现更灵活的管理。随着我们将这些相同的虚拟机“提升并转移到云中”，这种配置方法一直伴随着我们。这将我们带到了大致正确的边界。\n使用这种相同类型的配置表示基于容器的微服务、serverless和细粒度托管服务之间的关系导致了异常的复杂性。将应用程序转变为分布式系统应该是事后的想法。事实证明，云覆盖了您的架构和设计。表达架构和设计的最好的方式是使用代码，使用真正的编程语言编写抽象，重用和优秀的工具。\n早些时候，Eric和我采访了几十个客户。我们发现，开发人员和DevOps工程师都普遍感到幻灭。我们发现了极端的专业化，即使在同一个团队中，工程师也不会使用同一种语言。最近几周我已经听到了这个消息，我期待有一天会出现NoYAML运动。\n专业化是一件好事，我们希望我们最优秀和最聪明的云计算架构师晋升到DevOps和SRE的高级职位，但团队必须能够在合作时使用相同的语言。因为没有共同的通用语言导致了团队之间的物理隔离，而不是根据策略和环境分工。Pulumi的目标是为人们提供解决这个问题所需的工具。\nPulumi是什么？ Pulumi是一个支持多语言和混合云开发平台。它可以让您使用真实语言和真实代码创建云计算的各个方面，从基础设施到应用程序本身。只需编写程序并运行它们，Pulumi就能帮你完成出其余部分。\nPulumi的中心是一个云对象模型，与运行时相结合以了解如何以任何语言编写程序，理解执行它们所需的云资源，然后以强大的方式规划和管理您的云资源。这种云运行时和对象模型本质上是与语言、云中立的，这就是为什么我们能够支持如此多的语言和云平台。更多支持正在路上。\nPulumi采用了基础设施即代码以及不可变基础设施的概念，并可让您从您最喜欢的语言（而不是YAML或DSL）中获得自动化和可重复性优势。在部署它们之前，您可以对变更进行区分，并且我们会对谁更改了什么以及何时更改进行完善的审计追踪。核心模型因此是陈述性的。\n使用真正的语言可以带来巨大的好处：\n熟悉：不需要学习新的定制DSL或基于YAML的模板语言 抽象：正如我们喜爱的编程语言那样，我们可以用更小的东西来构建更大的东西 共享和重用：利用现有的语言包管理器共享和重用这些抽象，无论是与社区、团队内部共享 表现力：充分利用您的编程语言，包括异步、循环和条件 工具：通过使用真正的语言，我们可以即时访问IDE、重构、测试、静态分析和编排等等 生产力：将以上所有好处加在一起，一起将变得更快，我们也会变得更快乐 当提供原始云资源时，这些好处当然最重要，但是我们在团队中发现，您只能使用抽象。这包括在函数中包装事物以消除样板并创建引入更高级别概念的自定义类，通常将它们打包并重复使用。\n例如，此代码在AWS中创建一个DynamoDB数据库：\nimport * as aws from \u0026#34;@pulumi/aws\u0026#34;; let music = new aws.dynamodb.Table(\u0026#34;music\u0026#34;, { attributes: [ { name: \u0026#34;Album\u0026#34;, type: \u0026#34;S\u0026#34; }, { name: \u0026#34;Artist\u0026#34;, type: \u0026#34;S\u0026#34; }, ], hashKey: \u0026#34;Album\u0026#34;, rangeKey: \u0026#34;Artist\u0026#34;, }); 此代码创建一个基于容器的任务和无服务器功能，由一个存储桶触发：\nimport * as cloud from \u0026#34;@pulumi/cloud\u0026#34;; let bucket = new cloud.Bucket(\u0026#34;bucket\u0026#34;); let task = new cloud.Task(\u0026#34;ffmpegThumbTask\u0026#34;, { build: \u0026#34;./path_to_dockerfile/\u0026#34;, }); bucket.onPut(\u0026#34;onNewVideo\u0026#34;, bucketArgs =\u0026gt; { let file = bucketArgs.key; return task.run({ environment: { \u0026#34;S3_BUCKET\u0026#34;: bucket.id.get(), \u0026#34;INPUT_VIDEO\u0026#34;: file, \u0026#34;TIME_OFFSET\u0026#34;: file.substring(file.indexOf(\u0026#39;_\u0026#39;)+1, file.indexOf(\u0026#39;.\u0026#39;)).replace(\u0026#39;-\u0026#39;,\u0026#39;:\u0026#39;), \u0026#34;OUTPUT_FILE\u0026#34;: file.substring(0, file.indexOf(\u0026#39;_\u0026#39;)) + \u0026#39;.jpg\u0026#39;, }, }); }); 更好的是，这些代码可以根据您的需求部署到任何公共或私有云中。\n最后，这个例子创建了一个Redis缓存。我们怎么知道？我们不需要。缓存组件是一个抽象，它封装了我们可以安全忽略的不重要的细节：\nimport {Cache} from \u0026#34;./cache\u0026#34;; let cache = new Cache(\u0026#34;url-cache\u0026#34;); 在使用Pulumi之后，你不会再以同样的方式考虑基础设施。你的大脑将不再是一个独立于应用程序的独特“事物”，而是开始将分布式云系统看作是你的程序架构的核心部分，而不是事后的想法。\n由于抽象，我们已经能够提供一些强大的库。该库是提炼和执行最佳实践的绝佳方式。当然，对于我们自己的库来说没有什么特别的，因为它们只是功能、类和代码，我们期待着看到你为自己、你的团队或者社区建立的那些库。\n我们最复杂的库——Pulumi云框架——提供了一些令人兴奋的正在进行的工作的早期预览，展示如何创建跨越云提供商自己对诸如容器、无服务器功能和存储桶等核心概念的抽象。以同样的方式，您可以用Node.js、Python、Java、.NET等语言编写功能强大的应用程序，利用进程、线程和文件系统，无论是在macOS、Linux还是Windows上，这种方法都可以让您创建针对任何云提供商的现代混合云应用程序。像Kubernetes和其他CNCF产品组合这样的技术正在帮助推动这一不可避免的结果，因为它们在整个云基板上实现了对基本计算抽象的民主化和共识。\nPulumi不是PaaS，尽管它提供类似PaaS的生产力；您的程序总是直接针对您选择的云运行，并且始终可以访问该基础云的全部功能。即使您选择使用更高级别的组件，它也会向下兼容，并且您可以随时直接使用原始资源。它就像任何复杂的现代软件：有时，整个事情必须用C++编写，以便访问底层平台的全部功能，但对于大多数常见情况，70%到100％可以是平台独立代码，而只有不到30%的专业化才能真正需要直接与操作系统交互。\n接下来我还将发布十几篇博客文章来介绍Pulumi所有方面的更多细节。然而，为了保持这篇文章尽量简短，我将首先介绍下Pulumi的一些我最喜欢的方面。\n我最喜欢的东西 这很难选择，但这里有一些关于Pulumi我最喜欢的东西：\n开源。我坚信所有开发人员工具都应该是开源的。当然，Pulumi也是一家公司，但是有充足的机会通过增加便利性以建立商业模式。（可以认为是​​Git与GitHub的关系）我们从以前的工作中受益匪浅，其中包括Docker、Terraform、Kubernetes、TypeScript以及其他许多明确提及的工作。我们期待成为生态系统的一部分。因为我们在开放源代码方面下了很大功夫，所以我很高兴看到社区给我们带来什么，特别是在更高级别的软件包领域。\n多语言。就像使用Java和.NET一样，Pulumi运行时的架构可以支持多种语言，并以目标语言的所有方面（风格、语法、软件包等）的惯用方式来实现。因为我们是开源的，任何人都可以贡献自己的力量。\n混合 …","relpermalink":"/blog/hello-pulumi-from-jeo-duffy/","summary":"2018年6月18日 Joe Duffy在他的博客中宣布开源了云原生编程语言Pulumi。","title":"云原生编程语言Pulumi开源宣言"},{"content":" 如果有一个可视化的，提供基础设施集群给你操作的学习模式和平台，你会为此买单吗？\n两个月前，我在Kubernetes的Slack channel中结识了Jord，后来又在Taiwan Kubernetes User Group的Facebook group里看到了他（也有可能是其他人）发的MagicSandbox.io的链接，我就点击申请试用了，后来又收到了Jord发来的邮件，他告诉了我他想打造一个Kubernetes学习平台的事情。这就是整件事情的缘起，再后来我们有几次Zoom视频聊了很久。\n关于MagicSandbox MagicSandbox是一家创业公司，Jord（荷兰人）也是一个连续创业者，他19岁起曾在中国的四川大学留学了4年，后来回到了德国，他曾在Boston Consulting Group做PM，现就职于Entrepreneur First（欧洲顶级的风险投资/创业孵化器）德国柏林分公司，也是在那里，他结识了Mislav（克罗地亚人），Mislav是一名全栈工程师，也有几次创业经历，他们气味相投，一拍即合，他们决定致力于互联网教育行业，创造世界级的软件工程师教育平台，他们想从Kubernetes入手，首先提供基于互联网的Kubernetes的理论与实践教学，然后将主题扩大到ElasticSearch、GraphQL等等涉及到分布式系统的话题。\nJord在自己的家中创办了MagicSandbox，而我成为了MagicSandbox在中国的代言人。\n现在我们要发布的是MagicSandbox Alpha版本，该版本是一个未成熟的版本，提供给大家免费试用，也欢迎大家积极反馈。\n官方首页：https://magicsandbox.com/ 中文页面：https://cn.magicsandbox.com/ （内容尚未汉化，目前仅提供了中文版首页） 在Twitter上关注我们：https://twitter.com/magicsandbox ","relpermalink":"/notice/magicsandbox-alpha-version-annoucement/","summary":"在线实践式软件工程教育平台。","title":"MagicSandbox Alpha 版本发布"},{"content":"众所周知Kubernetes并不提供代码构建、发布和部署，所有的这些工作都是由CI/CD工作流完成的，最近TheNewStack又出了本小册子（117页）介绍了Kubernetes中CI/CD的现状，下载本书的PDF 。\n关于本书 本书的作者有：\nRob Scott：ReactiveOps公司的SRE Janakiram MSV：Janakiram \u0026amp; Associates 的首席分析师 Craig Martin：Kenzan的高级副总裁 Container Solutions 这本小册子里主要主要介绍了以下几点：\nDevOps模式 云原生应用模式 使用Spinnaker做持续交付 云原生时代的监控 DevOps模式 这一章从一些流行的自动化运维工具讲起，比如Chef、Puppet等，引申出CI/CD流水线，进而引出Docker和DevOps，将容器如何解除开发和运维之间的隔阂，但同时也带来了一些挑战，比如频繁的发布变更如何控制，如何控制容器集群的行为，如何拆分应用到容器之中等。这是一个专门用于容器编排调度的工具呼之欲出，Kubernetes的出现彻底改变了局面，可以说它直接改变了应用的基础架构。\nKubernetes细化的应用程序的分解粒度，同时将服务发现、配置管理、负载均衡和健康检查等作为基础设施的功能，简化了应用程序的开发。\n而Kubernetes这种声明式配置尤其适合CI/CD流程，况且现在还有如Helm、Draft、Spinnaker、Skaffold等开源工具可以帮助我们发布Kuberentes应用。\n有了基于Kubernetes的CI/CD流程后，又诞生了GitOps（WeaveWorks 的博客中有很多相关文章）和SecOps（Security Operation）。\n云原生应用模式 云原生是通过构建团队、文化和技术，利用自动化和架构来管理系统的复杂性和解放生产力。——Joe Beda，Heptio CTO，联合创始人\n这一章的重点是给出了云原生应用的10条关键属性。\n使用轻量级的容器打包 使用最合适的语言和框架开发 以松耦合的微服务方式设计 以API为中心的交互和协作 无状态和有状态服务在架构上界限清晰 不依赖于底层操作系统和服务器 部署在自服务、弹性的云基础设施上 通过敏捷的DevOps流程管理 自动化能力 通过定义和策略驱动的资源分配 作者然后将应用程序架构中的不同组件映射到云原生的工作负载中。\n这也是DevOps需要关注的部分，如何将云原生的组件映射为Kubernetes的原语（即Kubernetes里的各种资源对象和概念组合）呢？\n总结概括为以下10条：\n不要直接部署裸的Pod。 为工作负载选择合适的Controller。 使用Init容器确保应用程序被正确的初始化。 在应用程序工作负载启动之前先启动service。 使用Deployment history来回滚到历史版本。 使用ConfigMap和Secret来存储配置。 在Pod里增加Readiness和Liveness探针。 给Pod这只CPU和内存资源限额。 定义多个namespace来限制默认service范围的可视性。 配置HPA来动态扩展无状态工作负载。 使用Spinnaker进行持续交付 作者首先讲到了Spinnaker的各种特性，比如面向微服务啦，云原生的交付工具啦，可视化的交付和基础设施啦，支持多个region，支持容器和Kubernetes等等，不一而足，感兴趣大家可以自己看下报告或者登陆Spinnaker官网 查看。\n总之作者就是想说Spinnaker很好很强大啦，足以满足您对云原生应用CI/CD的需求。\n云原生时代的监控 监控是为了实现系统的可观察性，不要以为监控就是简单的出个监控页面，监控其实包括以下部分：\n日志收集 监控和指标度量 追踪 告警和可视化 要把其中任何一个方面做好都不容易。作者主要讲述的Prometheus和Grafana的开源监控方案。这一章我不详述，感兴趣大家可以查看报告原文。\n","relpermalink":"/blog/ci-cd-in-kubernetes/","summary":"TheNewStack的报告解读，介绍了Kubernetes中CI/CD的现状。","title":"Kubernetes中的CI/CD"},{"content":"还记得我之前分享的云原生编程语言终于出现！一文带你了解 Ballerina！ 吗？他们准备来参加KubeCon\u0026amp;CloudNativeCon中国大会了！\nKubeCon\u0026amp;CloudNativeCon 中国大会将于2018年11月14-15日（星期三、星期四）在上海举行。见：https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2018/\n经Ballerina公司官方授权，现在我需要帮他们在中国找一位“大使”，负责团队接引、中英文翻译等事务，要求熟悉Cloud Native与微服务，在业界具有影响力，英语沟通无障碍。\n大使职责\n负责团队接引 负责品宣、PPT材料等的中英文翻译 帮助布置展台 对方可以提供\n大会门票 差旅费 大会期间的住宿 其他补偿金 这是他们团队在今年五月份根本哈根的KubeCon\u0026amp;CloudNativeCon时在自己展台前的合影。\nP.S 这是我能找到的他们团队最全和拍的最好的一张照片了。（拍照技术亟待加强）\n简单介绍下这家名叫Ballerina的初创公司，他们的团队主要来自斯里兰卡，就是这里，位于南亚次大陆印度旁的一个岛国，中国古称其为“狮子国“，盛产宝石💎。\n他们国家的首都是斯里兰卡，用他们国家自己的语言念作：Si li jia ya wa de na pu la ke te\n如有感兴趣者欢迎直接联系我。\n","relpermalink":"/notice/a-ballerina-china-ambassador-required/","summary":"经Ballerina公司官方授权，现在我需要帮他们在中国找一位大使。","title":"云原生编程语言Ballerina公司征求中国大使一枚"},{"content":"Envoy 是一款由 Lyft 开源的，使用 C++ 编写的 L7 代理和通信总线，目前是 CNCF 旗下的开源项目，代码托管在 GitHub 上，它也是 Istio service mesh 中默认的 data plane。我们发现它有很好的性能，同时也不断有基于 Envoy 的开源项目出现，如 Ambassador 、Gloo 等，而目前 Envoy 的官方文档还没有得到很好的汉化，因此我们 Service Mesh 爱好者们觉得发动社区的力量共同翻译 Enovy 最新的（1.7 版本）的官方文档，并通过 GitHub 组织。\nService Mesh 爱好者们联合翻译了 Envoy 最新版本的官方文档 ，翻译的代码托管在 https://github.com/servicemesher/envoy ，如果你也是 Service Mesh 爱好者可以加入到 SerivceMesher GitHub 组织 里共同参与。\nEnvoy 官方文档排除 v1 API 参考 和 v2 API 参考的两个目录下的所有文章后一共有 120 余篇文档，文档的长短不一，原英文的官方文档都是使用 RST 格式，我手动将它们转成了 Markdown 格式，并使用 Gitbook 编译。按照文档相对于主目录的路径生成了 GitHub Issue，想参与翻译的朋友可以联系我 加入 ServiceMesher 组织，然后可以在 Issue 中选择你想要翻译的文章，然后回复 “认领”。\n在这里 可以看到所有的贡献者。未来我们也会创建 Service Mesh 爱好者网站，网站使用静态页面，所有的代码都会托管在 Github 上，欢迎大家参与进来。\n","relpermalink":"/notice/enovy-doc-translation-start/","summary":"SerivceMesher 社区共同参与翻译 Envoy 最新版本的官方文档。","title":"Envoy 最新官方文档翻译工作启动"},{"content":"当我第一眼看到 Ballerina 还真有点惊艳的感觉。Ballerina 这个单词的意思是“芭蕾舞女演员”。我想他们之所以给公司和这们语言起这个名字，可能是希望它成为云原生这个大舞台中，Ballerina 能像一个灵活的芭蕾舞者一样轻松自如吧！\nBallerina 是一款开源的编译式的强类型语言，该语言本身的代码可以通过 GitHub 上获取。我们可以通过 Ballerina 官网上的设计哲学 页面来对这门云原生编程语言一探究竟。\n云原生编程语言 Ballerina 未来的应用程序应该是基于 API 的，而众多 API 之间的通讯和集成就成了关键问题。Ballerina 是一款使用文本和图形语法编译的、事务的、静态和强类型编程语言。Ballerina 包含分布式系统集成到语言的基本概念，并提供类型安全，并发环境下实现的分布式事务，可靠的消息传递，流处理和工作流。\n为什么创建 Ballerina？ 与 ESB 集成仍然是瀑布式开发。你必须部署服务器，配置连接器，使用 XML 编程服务逻辑以及使用 XPath 查询和转换数据。这不是开发者友好的。\n带有 Spring 和 Node.js 等框架的编程语言提供了灵活性，但是它没有使适合于序列并行化、并发模型编程的分布式系统结构变得简单。\nESB、EAI、BPM 和 DSL 需要 XML 和配置来中断迭代开发流程：编辑、构建、运行和测试。这与运行实际应用之间是有一条鸿沟的，而云原生编程语言 Ballerina 的出现就是为了解决这条“集成鸿沟”的。\nBallerina 设计理念 序列图 云原生编程语言Ballerina的序列图设计理念 语言灵感\n序列图反映了设计开发人员记录的互联的系统。Ballerina 的语法和高效的编码模式要求开发人员使用强大的交互最佳实践来编码。\n序列图可视化\nBallerina 的语言语义模型旨在定义独立的各方如何通过结构化的交互沟通。接着，每个 Ballerina 程序都可以显示为其流程的序列图。IntelliJ 和 VS Code 的插件中提供了这些可视化。Ballerina Composer 是一款通过序列图创建 Ballerina 服务的工具。\nActor 与 action\n客户端、worker 和远程系统在 Ballerina 的序列图中以不同的 actor 表示。在代码中，远程端点通过连接器进行连接，连接器提供类型安全操作。在图形上，每个连接器在序列图中表示为一个 actor（即一条垂直线），action 表示为与这些actor 的交互。\n并发 云原生编程语言Ballerina的并发理念 序列图和并发\nBallerina 的并发模型是并行优先的，因为与远程方的交互总是涉及多个 worker。Worker 之间的交互作为消息传递进行处理，它们之间没有共享状态。\nWorker 语义\nBallerina 的执行模型由称为 woker 的轻量级并行执行单元组成。Worker 使用非阻塞策略来确保没有函数锁定正在执行的线程，例如等待响应的 HTTP I/O调用。\n编程模型\nWorker 和 fork/join 语义抽象了底层非阻塞方法，以启用更简单的并发编程模型。\n类型系统 下面是 Ballerina 中支持的类型。\nany anything; int integer = 0; float floatingPoint = 0.0; boolean b = true; string hi = \u0026#34;hello\u0026#34;; blob bl = hi.toBlob(\u0026#34;UTF-8\u0026#34;); json jsonNative = { a: \u0026#34;hello\u0026#34;, b: 5 }; xml x = xml `\u0026lt;ballerina\u0026gt; \u0026lt;supports\u0026gt;XML natively\u0026lt;/supports\u0026gt; \u0026lt;/ballerina\u0026gt;`; string[] stringArray = [\u0026#34;hi\u0026#34;, \u0026#34;there\u0026#34;]; int[][] arrayOfArrays = [[1,2],[3,4]]; json | xml | string unionType; (string, int) tuple = (\u0026#34;hello\u0026#34;, 5); () n = (); // the empty tuple acts as \u0026#34;null\u0026#34; string | int stringOrInt = \u0026#34;this is a union type\u0026#34;; int | () intOrNull = 5; var inferred = (\u0026#34;hello\u0026#34;, 5); map\u0026lt;boolean\u0026gt; myMap = {\u0026#34;ballerina\u0026#34;: true}; type myRecord { string a; int b; }; type myObject object { public { string x; } private { string y; } new (string xi, string yi) { x = xi; y = yi; } function getX() returns (string) { return x; } }; 类型安全\nBallerina 有一个结构化的类型系统，包括 primitive、recored、object、tuple 和 union 类型。该类型安全模型在赋值时包含了类型推断，并为连接器、逻辑和网络绑定的有效负载提供了大量的编译时完整性检查。\nUnion 类型和显式 Null\n各个网络端点通常会根据其输入和逻辑返回具有不同有效负载类型消息或 error。Ballerina 的类型系统采用了基于 union 类型的方法。Union 类型明确地采用了这种语义，不需要开发人员创建不必要的“包装”类型。这种方法也增强了对 null 值的处理。默认情况下，类型不支持 null 值。开发人员必须明确创建 union 类型来处理 null 值。结果是 null 的异常不会发生，并且语言语法和编译器会识别是否需要 null 处理逻辑。\n异构数据处理\nBallerina 类型系统内置丰富的对 JSON、XML、流和表格的支持以及对 ProtoBuf 和 gRPC 的直接支持。这样做的结果是可以获得处理网络负载、SQL 编程和流处理的干净可读的代码。数据转换逻辑不受复杂的生成类型、第三方库代码或其他混淆因素的影响——简单明了的可读代码捕捉与异构数据和转换逻辑的交互。\nBallerina 如何工作？ Ballerina 的语法、代码和编译器创建了运行时服务和部署构件，这些工件都是云原生就绪的，您可以选择将其部署在 IaaS、编排系统或 service mesh 中的。开发人员的体验旨在维护流程，包括快速的编辑、构建、调试周期并集成到团队的生命周期工具链中。\n运行时架构 云原生编程语言ballerina运行时架构 Ballerina API 网关\n强制执行身份策略并保证性能。通过代码注解（类似于 Spring 中的注解）进行配置和部署。可以运行嵌入式服务、作为管理多个服务的容器代理或者使用 API 管理解决方案（如 WSO2 API Manager）。\nBallerina service\n表示您的 API 和执行逻辑。服务通过不同的协议运行，内部代码结构被编译为支持 OpenAPI 和 Swagger 的API接口。服务与端点进行通信，无论它们是调用客户端还是其他服务。\nBallerina bridge\n允许传统代码和服务参与分布式事务中的 Ballerina 服务。Bridge 将您现有服务与本地代理包装起来，通过调用 Ballerina 服务参与和代理分布式事务。\n消息代理、事务协调者和身份代理\n为参与事务、事件驱动的通信和为认证流程的 Ballerina 服务提供代理基础设施功能。这些组件可以嵌入到单个服务部署中或者进行单独部署和扩展以管理多个服务。\n部署架构 云原生编程语言ballerina部署架构图 IaaS\n使用代码注解和构建系统，可以打包 Ballerina 服务和其他运行时组件（如 API 网关）以部署到任何云原生环境中。在 IaaS 环境中，Ballerina 服务可以以虚拟机或容器的方式运行，也可以在构建期间将镜像推送到 registry 中。\n编排器\n代码注解会触发编译器扩展，从而为不同的编排器（如 Kubernetes 或 Cloud Foundry）生成 Ballerina 组件的工件包。供应商或 DevOps 可以添加自定义代码注解以生成特定于环境的部署，例如自定义蓝色部署算法。\nService mesh\nBallerina 可以选择断路器和事务流程逻辑委托给像 Istio 或 Envoy 这样的 service mesh（如果有的话）。如果没有 service mesh 的话，Ballerina 服务将嵌入相应的功能。\n生命周期 云原生编程语言ballerina生命周期架构图 Ballerina 工具\n使用我们的语言服务器可以在 VS Code 和 IntelliJ 中获取自动补全和调试等智能感知。Ballerina 的关键字和语法结构可以用序列图的方式来表示。使用 Ballerina Composer 可以可视化的编辑 Ballerina 代码。另外它也可以做可视化得运行时和开发环境追踪。\nBallerina 构建\n将服务编译为经过优化的字节码，以便使用内存调优后的 BVM 运行。提供了使用Testerina 的项目结构、依赖管理、包管理和单元测试。构建锁可以轻松地重新创建服务和部署。生成可执行文件（.balx）或库（.balo）。\nCI/CD\n部署代码注解会触发构建扩展，从而为持续集成、持续交付或编排器环境生成工件。将构建工件推送到您的 CI/CD 系统或完全跳过。\nRegistry\n将端点连接器、自定义注解和代码功能作为可共享软件包组合在一起。可以在全球共享资源库——Ballerina Central 中 pull 或 push 版本化的软件包。\nBallerina 的语言特性 Ballerina 设计为云优先，内置对现代 Web 协议和数据格式的支持，完全支持图灵完备编程语言，以及对微服务架构的原生支持。\nAPI 构造 逻辑语言 异步 Json 和 XML 注解 稳定和强大的类型 stream Ballerina 中集成了哪些内容？ Ballerina 是一种旨在集成简化的语言。基于顺序图的交互，Ballerina 内置了对通用集成模式和连接器的支持，包括分布式事务、补偿和断路器。凭借对 JSON 和 XML 的一流支持，Ballerina 能够简单有效地构建跨网络终端的强大集成。\n类型安全端点集成 类型安全连接器 可靠的消息传递 分布式事务 断路器 注入攻击防护 Docker 和 Kubernetes 关于 Ballerina 中各个功能的示例代码请查阅 ballerina-example 。\n参考 Ballerina官网 Microservices, Docker, Kubernetes, Serverless, Service Mesh, and Beyond ","relpermalink":"/blog/introducing-cloud-native-programming-language-ballerina/","summary":"编译式强类型基于序列图理念的开源编程语言。","title":"云原生编程语言Ballerina介绍"},{"content":"本文是在 Kubernetes 集群中，使用 Envoy 来做 mesh，来为一个简单的使用 Python 编写的 Flask 应用程序做反向代理和负载均衡。\n注：本教程中的示例来自 envoy-steps ，本文中使用的所有的代码和 YAML 配置见 envoy-tutorial 。\nEnvoy Mesh 架构图 前提条件 使用 kubernetes-vagrant-centos-cluster 部署 kubernetes 集群，只要启动集群并安装了 CoreDNS 即可，无须安装其他插件。\n部署应用 我们首先将应用部署到 Kubernetes 中。\n部署 postgres 数据库。\nkubectl apply -f postgres 创建 usersvc 镜像。\ndocker build -t jimmysong/usersvc:step1 . 部署 usersvc。\nkubectl apply -f usersvc 查看 uservc 的 ClusterIP 地址。\n$ kubectl get svc usersvc kubectl get svc usersvc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE usersvc ClusterIP 10.254.176.248 \u0026lt;none\u0026gt; 5000/TCP 11m 进到 node1 中访问该服务，因为我们要访问的是 ClusterIP，在我们自己的电脑上是无法直接访问的，所以进到虚拟机中操作。\n$ vagrant ssh node1 $ curl 10.254.176.248:5000 { \u0026#34;hostname\u0026#34;: \u0026#34;usersvc-7cf5bb9d85-9gx7w\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;user health check OK\u0026#34;, \u0026#34;ok\u0026#34;: true, \u0026#34;resolvedname\u0026#34;: \u0026#34;172.33.10.7\u0026#34; } 尝试添加一个名为 Alice 的用户。\n$ curl -X PUT -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;fullname\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;alicerules\u0026#34; }\u0026#39; \\ 10.254.176.248/user/alice 将会看到类似如下的输出。\n{ \u0026#34;fullname\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;usersvc-7cf5bb9d85-9gx7w\u0026#34;, \u0026#34;ok\u0026#34;: true, \u0026#34;resolvedname\u0026#34;: \u0026#34;172.33.10.7\u0026#34;, \u0026#34;uuid\u0026#34;: \u0026#34;EF43B475F65848C6BE708F436305864B\u0026#34; } 尝试再添加一个名为 Bob 的用户。\n$ curl -X PUT -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;fullname\u0026#34;: \u0026#34;Bob\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;bobrules\u0026#34; }\u0026#39; \\ 10.254.176.248/user/bob 将会看到类似如下的输出。\n{ \u0026#34;fullname\u0026#34;: \u0026#34;Bob\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;usersvc-7cf5bb9d85-9gx7w\u0026#34;, \u0026#34;ok\u0026#34;: true, \u0026#34;resolvedname\u0026#34;: \u0026#34;172.33.10.7\u0026#34;, \u0026#34;uuid\u0026#34;: \u0026#34;6AC944E7D4254D9A811A82C0FDAC3046\u0026#34; } 当应用部署完毕后，我们该部署 edge envoy 了。\n部署 edge envoy 部署 edge envoy 的方式很简单，执行下面的命令。\nkubectl apply -f edge-envoy 现在访问 edge envoy 是就可以路由到 usersvc 上的，当然直接访问 usersvc 也是可以的。\n我们看下 edge-envoy 的 envoy 配置文件定义。\n{ \u0026#34;listeners\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;tcp://0.0.0.0:80\u0026#34;, \u0026#34;filters\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;read\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;http_connection_manager\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;codec_type\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;stat_prefix\u0026#34;: \u0026#34;ingress_http\u0026#34;, \u0026#34;route_config\u0026#34;: { \u0026#34;virtual_hosts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;backend\u0026#34;, \u0026#34;domains\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;routes\u0026#34;: [ { \u0026#34;timeout_ms\u0026#34;: 0, \u0026#34;prefix\u0026#34;: \u0026#34;/user\u0026#34;, \u0026#34;cluster\u0026#34;: \u0026#34;usersvc\u0026#34; } ] } ] }, \u0026#34;filters\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;decoder\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;router\u0026#34;, \u0026#34;config\u0026#34;: {} } ] } } ] } ], \u0026#34;admin\u0026#34;: { \u0026#34;access_log_path\u0026#34;: \u0026#34;/dev/null\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;tcp://127.0.0.1:8001\u0026#34; }, \u0026#34;cluster_manager\u0026#34;: { \u0026#34;clusters\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;usersvc\u0026#34;, \u0026#34;connect_timeout_ms\u0026#34;: 250, \u0026#34;type\u0026#34;: \u0026#34;strict_dns\u0026#34;, \u0026#34;service_name\u0026#34;: \u0026#34;usersvc\u0026#34;, \u0026#34;lb_type\u0026#34;: \u0026#34;round_robin\u0026#34;, \u0026#34;features\u0026#34;: \u0026#34;http2\u0026#34;, \u0026#34;hosts\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;tcp://usersvc:80\u0026#34; } ] } ] } } 客户端访问 edge-envoy 的 ClusterIP:8000/user/health 就可以检查节点的健康状况。\n部署 usersvc2 删除原来的 usersvc，部署第二版 usersvc2，它与原来的 usersvc 唯一不同的地方是在 entrypoint 中集成了 envoy，查看 Dockerfile 中指定的 entrypoint.sh 的内容便可知。\n#!/bin/sh python /application/service.py \u0026amp; /usr/local/bin/envoy -c /application/envoy.json 首先删除老的 usersvc。\nkubectl delete -f usersvc 使用下面的命令部署 usersvc2，它仍然使用 usersvc 这个 service 名称。\nkubectl apply -f usersvc2 Envoy 以 out-of-process 的方式运行，对应用进程没有侵入性，也可以使用 sidecar 的方式运行，让 envoy 与 应用容器运行在同一个 pod 中。\n增加 usersvc2 的实例个数。\nkubectl scale --replicas=3 deployment/usersvc 此时我们有 3 个 usersvc 实例，现在通过 edge-envoy 的 ClusterIP:8000/user/health 检查节点的健康状况时，是不是会轮询的访问到后端的的 usersvc2 的实例呢？\n我们当初在 edge-node 的 envoy.json 中配置过 cluster 的，其中指定了 lb_type 为 round_robin 。\n\u0026#34;cluster_manager\u0026#34;: { \u0026#34;clusters\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;usersvc\u0026#34;, \u0026#34;connect_timeout_ms\u0026#34;: 250, \u0026#34;type\u0026#34;: \u0026#34;strict_dns\u0026#34;, \u0026#34;service_name\u0026#34;: \u0026#34;usersvc\u0026#34;, \u0026#34;lb_type\u0026#34;: \u0026#34;round_robin\u0026#34;, \u0026#34;features\u0026#34;: \u0026#34;http2\u0026#34;, \u0026#34;hosts\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;tcp://usersvc:80\u0026#34; } ] } ] } 而且该 serivce_name 也可以被 DNS 正确解析。\nroot@usersvc-55b6857d44-gcg5c:/application# nslookup usersvc Server: 10.254.0.2 Address: 10.254.0.2#53 Name: usersvc.envoy-tutorial.svc.cluster.local Address: 10.254.123.166 答案是否定的。\n虽然通过 DNS 可以正确的解析出 serivce 的 ClusterIP，但是负载均衡不再通过 kube-proxy 实现，所以不论我们访问多少次 edge-envoy 永远只能访问到一个固定的后端 usersvc。\n服务发现服务 - SDS Kubernetes 中的 DNS 可以发现所有 serivce 的 ClusterIP，但是 DNS 中不包括所有 endpoint 地址，我们需要一个 SDS（服务发现服务）来发现服务的所有的 endpoint，我们将修改 lb_type，使用 sds 替代 strict_dns。\n执行下面的命令部署 SDS。\nkubectl apply -f usersvc-sds 因为在添加了 SDS 之后需要修改 edge-envoy 中的 envoy.josn 配置，在 clusters 字段中增加 sds 信息，我们将所有的配置都写好了，重新打包成了镜像，我们需要先删除之前部署的 edge-envoy。\nkubectl delete -f edge-envoy 部署新的 edge-envoy2。\nkubectl apply -f edge-envoy2 连续访问 usersvc 12 次看看输出结果如何。\nURL=http://172.17.8.101:30800/user/alice for i in `seq 1 12`;do curl -s $URL|grep \u0026#34;resolvedname\u0026#34;|tr -d \u0026#34; \u0026#34;|tr -d \u0026#34;,\u0026#34;|tr -d \u0026#39;\u0026#34;\u0026#39;;done 我们可以看到类似如下的输出：\nresolvedname:172.33.71.2 resolvedname:172.33.88.2 resolvedname:172.33.10.2 resolvedname:172.33.71.2 resolvedname:172.33.88.2 resolvedname:172.33.10.2 resolvedname:172.33.71.2 resolvedname:172.33.88.2 resolvedname:172.33.10.2 resolvedname:172.33.71.2 resolvedname:172.33.88.2 resolvedname:172.33.10.2 再查看下 usersvc 服务的所有 pod 的 IP 地址。\n$ kubectl get pod -l service=usersvc -o wide NAME READY STATUS RESTARTS AGE IP NODE usersvc-55b6857d44-mkfpv 1/1 Running 0 9m 172.33.88.2 node1 usersvc-55b6857d44-q98jg 1/1 Running 0 9m 172.33.71.2 node2 usersvc-55b6857d44-s2znk 1/1 Running 0 9m 172.33.10.2 node3 我们看到 round-robin 负载均衡生效了。\n参考 Part 2: Deploying Envoy with a Python Flask webapp and Kubernetes envoy-steps kubernetes-vagrant-centos-cluster envoy-tutorial ","relpermalink":"/blog/envoy-mesh-in-kubernetes-tutorial/","summary":"本文是在 Kubernetes 集群中，使用 Envoy 来做 mesh，来为一个简单的使用 Python 编写的 Flask 应用程序做反向代理和负载均衡。","title":"在 Kubernetes 中使用 Envoy mesh 教程"},{"content":"在了解一门技术之前一开始就要了解其中的基本概念和术语，只有融入了该语境才能理解这门技术。本文将为大家介绍 Envoy 中的基本术语和重点概念。\n架构 下图是 Envoy proxy 的架构图，显示了 host B 经过 Envoy 访问 host A 的过程。每个 host 上都可能运行多个 service，Envoy 中也可能有多个 Listener，每个 Listener 中可能会有多个 filter 组成了 chain。\nEnvoy proxy 架构图 其中的基本术语将在下面解释。\n基本术语 Host：能够进行网络通信的实体（在手机或服务器等上的应用程序）。在 Envoy 中主机是指逻辑网络应用程序。只要每台主机都可以独立寻址，一块物理硬件上就运行多个主机。\nDownstream：下游（downstream）主机连接到 Envoy，发送请求并或获得响应。\nUpstream：上游（upstream）主机获取来自 Envoy 的链接请求和响应。\nCluster: 集群（cluster）是 Envoy 连接到的一组逻辑上相似的上游主机。Envoy 通过服务发现 发现集群中的成员。Envoy 可以通过主动运行状况检查 来确定集群成员的健康状况。Envoy 如何将请求路由到集群成员由负载均衡策略 确定。\nMesh：一组互相协调以提供一致网络拓扑的主机。Envoy mesh 是指一组 Envoy 代理，它们构成了由多种不同服务和应用程序平台组成的分布式系统的消息传递基础。\n运行时配置：与 Envoy 一起部署的带外实时配置系统。可以在无需重启 Envoy 或 更改 Envoy 主配置的情况下，通过更改设置来影响操作。\nListener: 侦听器（listener）是可以由下游客户端连接的命名网络位置（例如，端口、unix域套接字等）。Envoy 公开一个或多个下游主机连接的侦听器。一般是每台主机运行一个 Envoy，使用单进程运行，但是每个进程中可以启动任意数量的 Listener（监听器），目前只监听 TCP，每个监听器都独立配置一定数量的（L3/L4）网络过滤器。Listenter 也可以通过 Listener Discovery Service（LDS）动态获取。\nListener filter：Listener 使用 listener filter（监听器过滤器）来操作链接的元数据。它的作用是在不更改 Envoy 的核心功能的情况下添加更多的集成功能。Listener filter 的 API 相对简单，因为这些过滤器最终是在新接受的套接字上运行。在链中可以互相衔接以支持更复杂的场景，例如调用速率限制。Envoy 已经包含了多个监听器过滤器。\nHttp Route Table：HTTP 的路由规则，例如请求的域名，Path 符合什么规则，转发给哪个 Cluster。\nHealth checking：健康检查会与SDS服务发现配合使用。但是，即使使用其他服务发现方式，也有相应需要进行主动健康检查的情况。详见 health checking 。\nxDS xDS 是一个关键概念，它是一类发现服务的统称，其包括如下几类：\nCDS：Cluster Discovery Service EDS：Endpoint Discovery Service SDS：Secret Discovery Service RDS：Route Discovery Service LDS：Listener Discovery Service 正是通过对 xDS 的请求来动态更新 Envoy 配置，另外还有个 ADS（Aggregated Discovery Service）通过聚合的方式解决以上 xDS 的更新顺序问题。\nEnvoy Mesh Envoy Mesh 指的是由 envoy 做负载均衡和代理的 mesh。该 Mesh 中会包含两类 envoy：\nEdge envoy：即流量进出 mesh 时候的 envoy，相当于 kubernetes 中的 ingress。 Service envoy：服务 envoy 是跟每个 serivce 实例一起运行的，应用程序无感知的进程外工具，在 kubernetes 中会与应用容器以 sidecar 形式运行在同一个 pod 中。 Envoy 即可以单独作为 edge envoy，也可以仅做 service envoy 使用，也可以两者同时使用。Mesh 中的所有 envoy 会共享路由信息。\nEnvoy 配置 Envoy 中的配置包括两大类：listenner 配置和 cluster 配置。\nListener 配置 我们知道 Envoy 中可以配置一组 listener 以实现复杂的处理逻辑。Listener 中设置监听的 TCP 端口，还有一组 filter 对这些端口上的数据流进行处理。如下所示，该示例来自使用Envoy 作为前端代理 。\nlisteners: - address: socket_address: address: 0.0.0.0 port_value: 80 filter_chains: - filters: - name: envoy.http_connection_manager config: codec_type: auto stat_prefix: ingress_http route_config: name: local_route virtual_hosts: - name: backend domains: - \u0026#34;*\u0026#34; routes: - match: prefix: \u0026#34;/service/1\u0026#34; route: cluster: service1 - match: prefix: \u0026#34;/service/2\u0026#34; route: cluster: service2 这是一个 http_connection_manager 例子，其中必须包含 virtual_hosts 配置，而 virtual_hosts 配置中必须包含以下几项配置：\nname：服务名称 domains：DNS 域名，必须能跟 virtual_host 的 URL 匹配 routes：路由列表 每个路由中还可以包含以下配置：\nprefix：URL 路径前缀 cluster：处理该请求的 envoy cluster timeout_ms：当出错时的超时时间 如上面的例子中，我们还需要定义 service1 cluster 和 service2 cluster。\nCluster 配置 Cluster 是一组逻辑相似的主机配置，定义哪些主机属于一个服务，cluster 的配置中包含了服务发现和负载均衡方式配置。依然是参考使用Envoy 作为前端代理 中的配置：\nclusters: - name: service1 connect_timeout: 0.25s type: strict_dns lb_policy: round_robin http2_protocol_options: {} hosts: - socket_address: address: service1 port_value: 80 - name: service2 connect_timeout: 0.25s type: strict_dns lb_policy: round_robin http2_protocol_options: {} hosts: - socket_address: address: service2 port_value: 80 Cluster 的配置中至少包含以下信息：\nname：cluster 名称，就是服务名称 type：该 cluster 怎么知道主机是否启动？即服务发现类型，有以下方式： static：监听 cluster 中的所有主机 strict_dns：envoy 会监听 DNS，每个匹配的 A 记录都会认定为有效 logical_dns：envoy 将使用 DNS 来增加主机，如果 DNS 不再返回该主机也不会删除这些主机信息 sds：即 Serivce Discovery Serivce，envoy 访问外部的 REST 获取 cluster 成员信息 lb_type：cluster 的负载均衡类型，有以下方式： round_robin：轮询主机 weighted_least_request：最近获得最少请求的主机 random：随机 hosts：能够定义 cluster 中主机的 URL 地址，通常是tcp:// URL 参考 Part 1: Getting started with Envoy Proxy for microservices resilience - getambassador.io ","relpermalink":"/blog/envoy-archiecture-and-terminology/","summary":"本文介绍了 Envoy proxy 中的基本概念、配置与架构解析。","title":"Envoy 的架构与基本配置解析"},{"content":"Envoy 是一款由 Lyft 开源的，使用 C++ 编写的 L7 代理和通信总线，目前是 CNCF 旗下的开源项目，代码托管在 GitHub 上，它也是 Istio service mesh 中默认的 data plane。本文将给出使用 Envoy 作为 service mesh 的数据平面的示例，应用使用 docker-compose 编排。\n特性 Envoy 包括如下特性：\n进程外架构，不侵入应用进程 使用现代版 C++11 代码 L3/L4 filter 架构 HTTP L7 filter 架构 支持 HTTP/2 HTTP L7 routing 支持 gRPC 支持 MongoDB L7 动态配置 最佳可观测性 支持 front/edge proxy 高级负载均衡 健康检查 服务发现 支持 DynamoDB L7 Envoy 本身无法构成一个完整的 Service Mesh，但是它可以作为 service mesh 中的应用间流量的代理，负责 service mesh 中的数据层。\n更多信息请参考 Envoy 官网 。\nEnvoy 作为前端代理 本文是使用 Envoy 作为前端代理的介绍，仅使用 docker 容器和 docker-compose 做编排在单机中运行，帮助我们从更底层了解 Envoy，当我们将 Envoy 作为 Istio Service Mesh 的 data panel 的时候将更加游刃有余。\n快速开始 Envoy 中的所有规则配置跟 Kubernetes 一样都是通过 YAML 文件来完成的。在继续下面的步骤之前，首先克隆 Envoy 的 GitHub repo。\ngit clone https://github.com/envoyproxy/envoy.git 运行 sandbox 测试 Envoy 官方提供了以下打包用例：\nFront Proxy Zipkin Tracing Jaeger Tracing gRPC Bridge 全部可以使用 docker-compose 运行，代码可以在 https://github.com/envoyproxy/envoy/tree/master/examples 找到。\nFront proxy Envoy 在 envoymesh 的边缘做反向代理，详细使用方式见 https://www.envoyproxy.io/docs/envoy/latest/start/sandboxes/front_proxy ，在此我将解说下以下问题：\nEnvoy 是如何作为进程外架构运行的？ 为何说 Envoy 是无侵入式架构？ Envoy 作为边缘反向代理能做什么？ 本示例的架构图如下所示，此时 Envoy 将作为一个反向代理，类似于 Nginx，但与 Nginx 不同的是它还会作为一个进程，伴随每个服务一起运行在同一个容器中（在 Kubernetes 中可以作为 Sidecar 与应用容器一起运行在同一个 Pod 中）。\nFront proxy 部署结构图 在此示例中一共有 3 个服务，我们需要为其创建容器编排的 docker-compose.yml 文件。\nversion: \u0026#39;2\u0026#39; services: front-envoy: build: context: . dockerfile: Dockerfile-frontenvoy volumes: - ./front-envoy.yaml:/etc/front-envoy.yaml networks: - envoymesh expose: - \u0026#34;80\u0026#34; - \u0026#34;8001\u0026#34; ports: - \u0026#34;8000:80\u0026#34; - \u0026#34;8001:8001\u0026#34; service1: build: context: . dockerfile: Dockerfile-service volumes: - ./service-envoy.yaml:/etc/service-envoy.yaml networks: envoymesh: aliases: - service1 environment: - SERVICE_NAME=1 expose: - \u0026#34;80\u0026#34; service2: build: context: . dockerfile: Dockerfile-service volumes: - ./service-envoy.yaml:/etc/service-envoy.yaml networks: envoymesh: aliases: - service2 environment: - SERVICE_NAME=2 expose: - \u0026#34;80\u0026#34; networks: envoymesh: {} 使用 docker-compose 启动可以保证三个服务都在同一个网络内，即 frontproxy_envoymesh 网络中。\n其中 front-envoy 是前端（边缘）Envoy 服务，用来做反向代理，它使用的是 Dockerfile-frontenvoy 文件来构建镜像的，我们来看下该 Dockerfile 的内容。\nFROM envoyproxy/envoy:latest RUN apt-get update \u0026amp;\u0026amp; apt-get -q install -y \\ curl CMD /usr/local/bin/envoy -c /etc/front-envoy.yaml --service-cluster front-proxy 其中 /etc/front-envoy.yaml 是本地的 front-envoy.yaml 挂载进去的。我们看下该文件的内容。\nstatic_resources: listeners: - address: socket_address: address: 0.0.0.0 port_value: 80 filter_chains: - filters: - name: envoy.http_connection_manager config: codec_type: auto stat_prefix: ingress_http route_config: name: local_route virtual_hosts: - name: backend domains: - \u0026#34;*\u0026#34; routes: - match: prefix: \u0026#34;/service/1\u0026#34; route: cluster: service1 - match: prefix: \u0026#34;/service/2\u0026#34; route: cluster: service2 http_filters: - name: envoy.router config: {} clusters: - name: service1 connect_timeout: 0.25s type: strict_dns lb_policy: round_robin http2_protocol_options: {} hosts: - socket_address: address: service1 port_value: 80 - name: service2 connect_timeout: 0.25s type: strict_dns lb_policy: round_robin http2_protocol_options: {} hosts: - socket_address: address: service2 port_value: 80 admin: access_log_path: \u0026#34;/dev/null\u0026#34; address: socket_address: address: 0.0.0.0 port_value: 8001 我们看到其中包括了三大配置项：\nstatic_resources：路由配置信息 cluster：envoymesh 的服务注册信息 admin：管理接口，可以通过访问 8001 端口的，访问 /stats 获取当前 envoymesh 的一些统计信息，访问 /server_info 获取 Envoy 的版本信息 使用 docker-compose 启动三个容器。\n$ pwd envoy/examples/front-proxy $ docker-compose up --build -d $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------- example_service1_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp example_service2_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp example_front-envoy_1 /bin/sh -c /usr/local/bin/ ... Up 0.0.0.0:8000-\u0026gt;80/tcp, 0.0.0.0:8001-\u0026gt;8001/tcp 我们下面将过一遍 Envoy 作为前端代理的所有功能，这些功能是通用功能。\n路由 访问 service1 http://localhost:8000/service/1 将看到如下输出。\n$ curl -v localhost:8000/service/1 * Trying ::1... * TCP_NODELAY set * Connected to localhost (::1) port 8000 (#0) \u0026gt; GET /service/1 HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; content-type: text/html; charset=utf-8 \u0026lt; content-length: 89 \u0026lt; server: envoy \u0026lt; date: Fri, 20 Apr 2018 08:26:33 GMT \u0026lt; x-envoy-upstream-service-time: 14 \u0026lt; Hello from behind Envoy (service 1)! hostname: a3e4185a9a49 resolvedhostname: 172.18.0.4 * Connection #0 to host localhost left intact 访问 service2 http://localhost:8000/service/2 将看到如下输出。\n* Trying ::1... * TCP_NODELAY set * Connected to localhost (::1) port 8000 (#0) \u0026gt; GET /service/2 HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; content-type: text/html; charset=utf-8 \u0026lt; content-length: 89 \u0026lt; server: envoy \u0026lt; date: Fri, 20 Apr 2018 08:27:27 GMT \u0026lt; x-envoy-upstream-service-time: 10 \u0026lt; Hello from behind Envoy (service 2)! hostname: f6650e1911a0 resolvedhostname: 172.18.0.3 * Connection #0 to host localhost …","relpermalink":"/blog/envoy-as-front-proxy/","summary":"本文是使用 Envoy 作为前端代理的介绍，仅使用 docker 容器和 docker-compose 做编排在单机中运行，帮助我们从更底层了解 Envoy，当我们将 Envoy 作为 Istio Service Mesh 的 data panel 的时候将更加游刃有余。","title":"使用 Envoy 作为前端代理"},{"content":"TL;DR 点此下载本书 PDF 。\n近日 Nginx 公司的 Michael Hausenblas 发布了一本关于 docker 和 kubernetes 中的容器网络的小册子。这份资料一共 72 页，是大家由浅入深的了解 Docker 和 Kubernetes 中的网络的很好的入门资料。\n目标读者 容器软件开发者 SRE 网络运维工程师 想要把传统软件进行容器化改造的架构师 ","relpermalink":"/notice/container-networking-from-docker-to-kubernetes-nginx/","summary":"资料来自 Nginx，O'Reilly 出版。","title":"从 Docker 到 Kubernetes 中的容器网络图书资料分享"},{"content":"本文是 Istio 管理 Java 微服务的案例教程，使用的所有工具和软件全部基于开源方案，替换了 redhat-developer-demos/istio-tutorial 中的 minishift 环境，使用 kubernetes-vagrant-centos-cluster 替代，沿用了原有的微服务示例，使用 Zipkin 做分布式追踪而不是 Jaeger。\n本文中的代码和 YAML 文件见 https://github.com/rootsongjc/istio-tutorial 。\n准备环境 在进行本教程前需要先准备以下工具和环境。\n8G 以上内存 Vagrant 2.0+ Virtualbox 5.0 + 提前下载 kubernetes1.9.1 的 release 压缩包 docker 1.12+ kubectl 1.9.1+ maven 3.5.2+ istioctl 0.7.1 git curl、gzip、tar kubetail siege 安装 Kubernetes 请参考 kubernetes-vagrant-centos-cluster 在本地启动拥有三个节点的 kubernetes 集群。\ngit clone https://github.com/rootsongjc/kubernetes-vagrant-centos-cluster.git cd kubernetes-vagrant-centos-cluster vagrant up 安装 Istio 在 kubernetes-vagrant-centos-cluster 中的包含 Istio 0.7.1 的安装 YAML 文件，运行下面的命令安装 Istio。\nkubectl apply -f addon/istio/ 运行示例\nkubectl apply -n default -f \u0026lt;(istioctl kube-inject -f yaml/istio-bookinfo/bookinfo.yaml) 在您自己的本地主机的/etc/hosts文件中增加如下配置项。\n172.17.8.102 grafana.istio.jimmysong.io 172.17.8.102 servicegraph.istio.jimmysong.io 172.17.8.102 zipkin.istio.jimmysong.io 我们可以通过下面的URL地址访问以上的服务。\nService URL grafana http://grafana.istio.jimmysong.io servicegraph http://servicegraph.istio.jimmysong.io/dotviz，http://servicegraph.istio.jimmysong.io/graph zipkin http://zipkin.istio.jimmysong.io 详细信息请参阅 Istio 文档 。\n部署示例应用 在打包成镜像部署到 kubernetes 集群上运行之前，我们先在本地运行所有示例。\n本教程中三个服务之间的依赖关系如下：\ncustomer → preference → recommendation customer 和 preference 微服务是基于 Spring Boot 构建的，recommendation 微服务是基于 vert.x 构建的。\ncustomer 和 preference 微服务的 pom.xml 文件中都引入了 OpenTracing 和 Jeager 的依赖。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.opentracing.contrib\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;opentracing-spring-cloud-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.1.7\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.uber.jaeger\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jaeger-tracerresolver\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.25.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 本地运行 我们首先在本地确定所有的微服务都可以正常运行，然后再打包镜像在 kubernetes 集群上运行。\n启动 Jaeger\n使用 docker 来运行 jagger。\ndocker run -d \\ --rm \\ -p5775:5775/udp \\ -p6831:6831/udp \\ -p6832:6832/udp \\ -p16686:16686 \\ -p14268:14268 \\ jaegertracing/all-in-one:1.3 Jaeger UI 地址 http://localhost:16686\nCustomer\ncd customer/java/springboot JAEGER_SERVICE_NAME=customer mvn \\ spring-boot:run \\ -Drun.arguments=\u0026#34;--spring.config.location=src/main/resources/application-local.properties\u0026#34; 服务访问地址： http://localhost:8280\nPreference\ncd preference/java/springboot JAEGER_SERVICE_NAME=preference mvn \\ spring-boot:run \\ -Drun.arguments=\u0026#34;--spring.config.location=src/main/resources/application-local.properties\u0026#34; 服务访问地址：http://localhost:8180\nRecommendation\ncd recommendation/java/vertx mvn vertx:run 服务访问地址：http://localhost:8080\n所有服务都启动之后，此时访问 http://localhost:8280 将会看到如下输出。\ncustomer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;unknown\u0026#39;: 1 每访问一次最后的数字就会加 1。\nJaeger\n此时访问 http://localhost:16686 将看到 Jaeger query UI，所有应用将 metrics 发送到 Jeager 中。\n可以在 Jaeger UI 中搜索 customer 和 preference service 的 trace 并查看每次请求的 tracing。\nJaeger query UI 构建镜像 在本地运行测试无误之后就可以构建镜像了。本教程中的容器镜像都是在 fabric8/java-jboss-openjdk8-jdk 的基础上构建的。只要将 Java 应用构建出 Jar 包然后放到 /deployments 目录下基础镜像就可以自动帮我们运行，所以我们看到着几个应用的 Dockerfile 文件中都没有执行入口，真正的执行入口是 run-java.sh 。\nCustomer\n构建 Customer 镜像。\ncd customer/java/springboot mvn clean package docker build -t jimmysong/istio-tutorial-customer:v1 . docker push jimmysong/istio-tutorial-customer:v1 第一次构建和上传需要花费一点时间，下一次构建就会很快。\nPreference\n构建 Preference 镜像。\ncd preference/java/springboot mvn clean package docker build -t jimmysong/istio-tutorial-preference:v1 . docker push jimmysong/istio-tutorial-preference:v1 Recommendation\n构建 Recommendation 镜像。\ncd recommendation/java/vertx mvn clean package docker build -t jimmysong/istio-tutorial-recommendation:v1 . docker push jimmysong/istio-tutorial-recommendation:v1 现在三个 docker 镜像都构建完成了，我们检查一下。\n$ docker images | grep istio-tutorial REPOSITORY TAG IMAGE ID CREATED SIZE jimmysong/istio-tutorial-recommendation v1 d31dd858c300 51 seconds ago 443MB jimmysong/istio-tutorial-preference v1 e5f0be361477 6 minutes ago 459MB jimmysong/istio-tutorial-customer v1 d9601692673e 13 minutes ago 459MB 部署到 Kubernetes 使用下面的命令将以上服务部署到 kubernetes。\n# create new namespace kubectl create ns istio-tutorial # deploy recommendation kubectl apply -f \u0026lt;(istioctl kube-inject -f recommendation/kubernetes/Deployment.yml) -n istio-tutorial kubectl apply -f recommendation/kubernetes/Service.yml # deploy preferrence kubectl apply -f \u0026lt;(istioctl kube-inject -f preference/kubernetes/Deployment.yml) -n istio-tutorial kubectl apply -f preference/kubernetes/Service.yml # deploy customer kubectl apply -f \u0026lt;(istioctl kube-inject -f customer/kubernetes/Deployment.yml) -n istio-tutorial kubectl apply -f customer/kubernetes/Service.yml 注意：preference 和 customer 应用启动速度比较慢，我们将 livenessProb 配置中的 initialDelaySeconds 设置为 20 秒。\n查看 Pod 启动状态：\nkubectl get pod -w -n istio-tutorial 增加 Ingress 配置 为了在 kubernetes 集群外部访问 customer 服务，我们需要增加 ingress 配置。\nkubectl apply -f ingress/ingress.yaml 修改本地的 /etc/hosts 文件，增加一条配置。\n172.17.8.102 customer.istio-tutorial.jimmysong.io 现在访问 http://customer.istio-tutorial.jimmysong.io 将看到如下输出：\ncustomer =\u0026gt; preference =\u0026gt; …","relpermalink":"/blog/istio-tutorial/","summary":"本文是 Istio 管理 Java 微服务的案例教程。","title":"Istio Service Mesh 教程"},{"content":"本文讲述了参与 Istio 社区和进行 Istio 开发时需要注意的事项。\n工作组 绝大多数复杂的开源项目都是以工作组的方式组织的，要想为 Istio 社区做贡献可以加入到以下的工作组（Working Group）：\nAPI Management Config Environments Networking Performance \u0026amp; Scalability Policies \u0026amp; Telemetry Security Test \u0026amp; Release 代码规范 Istio 的代码规范沿用 CNCF 社区的代码规范 。\n开发指南 进行 Istio 开发之前需要做下面几件事情：\n配置基础环境，如 Kubernetes 配置代码库、下载依赖和测试 配置 CircleCI 集成环境 编写参考文档 Git workflow 配置 详见 Dev Guide wiki 。\n设计文档 所有的设计文档都保存在 Google Drive 中，其中包括以下资源：\nTechnical Oversight Committee：ToC管理的文档 Misc：一些杂项 Working Groups：最重要的部分，各个工作组相关的设计文档 Presentations：Istio 相关的演讲幻灯片，从这些文稿中可以快速了解 Istio Logo：Istio logo Eng：社区相关的维护文档 社区角色划分 根据对开发者和要求和贡献程度的不同，Istio 社区中包含以下角色：\nCollaborator ：非正式贡献者，偶尔贡献，任何人都可以成为该角色 Member ：正式贡献者，经常贡献，必须有2个已有的 member 提名 Approver ：老手，可以批准 member 的贡献 Lead ：管理功能、项目和提议，必须由 ToC 提名 Administrator ：管理员，管理和控制权限，必须由 ToC 提名 Vendor ：贡献 Istio 项目的扩展 详见 Istio Community Roles 。\n各种功能的状态 Istio 中的所有 feature 根据是否生产可用、API兼容性、性能、维护策略分为三种状态：\nAlpha：仅仅可以作为 demo，无法生产上使用，也没有性能保证，随时都可能不维护 Beta：可以在生产上使用了，也有版本化的 API 但是无法保证性能，保证三个月的维护 Stable：可以上生产而且还能保证性能，API 向后兼容，保证一年的维护 Istio 的 feature 分为四大类：\n流量管理：各种协议的支持、路由规则配置、Ingress TLS 等 可观察性：监控、日志、分布式追踪、服务依赖拓扑 安全性：各种 checker 和安全性配置 Core：核心功能 功能划分与各种功能的状态详情请见：https://istio.io/latest/about/feature-stages/ 云原生社区 Istio 讨论组 云原生社区 专门成立里 Istio SIG（微信讨论群），将原来 ServiceMesher 中关注 Istio 的人群专门集中到一个讨论组中，其中包含了百度、阿里巴巴、腾讯、网易、Tetrate、Intel、字节跳动等公司的服务网格专家及众多的终端用户，欢迎大家申请加入群聊 。\n","relpermalink":"/blog/istio-community-tips/","summary":"本文讲述了参与 Istio 社区和进行 Istio 开发时需要注意的事项。","title":"Istio 社区介绍与社区参与注意事项"},{"content":"本文译自 Istio Why do I need it? 我最近没有多少时间去玩 k8s，并承认 Istio 到底给 k8s 带来了什么方面有点迷失了。这是否会增加更多的运营开销？它是否简化了我们通常需要做的事情？这些问题都浮现在我的脑海里。\n我怀疑在发布了这些内容之后，我的团队中比我更懂 k8s 的人可能会想找我谈谈…… 虽然我讲会跟团队中的成员辩论，但那将是我最喜欢的对话。\n那么 Istio 究竟是什么？ Istio 网站 上说，Istio 带给你：\nHTTP、gRPC、WebSocket 和 TCP 流量的自动负载均衡。 通过丰富的路由规则、重试、故障转移和故障注入对流量行为进行细粒度控制。 支持访问控制、速率限制和配额的可拔插策略层和配置 API。 自动指标、日志和集群内所有流量的跟踪，包括集群入口和出口。 通过集群中的服务之间的强身份断言来实现服务间的身份验证。 通过在整个环境中部署一个特殊的 sidecar 代理（辅助容器），您可以将 Istio 支持添加到服务中（这给我留下了深刻的印象，如果您想做到这一点，请参阅后面的内容）。安装了 sidecar 代理之后，（微）服务之间的所有网络通信都通过这个代理。此外，所有的网络通信都是使用 Istio 的控制平面功能进行配置和管理的。\nIstio 是 Service Mesh（服务网格）。我认为的 service mesh 定义就是 “它是一个专用的基础设施层，使得服务间的通信安全、高效和可靠”\n然而，如果像我一样，你从概念文档 开始看的话，上面有这样的内容：“术语 service mesh 通常用于描述组成这些应用程序的微服务网络以及它们之间的交互。随着服务网格的大小和复杂程度不断增加，可能会变得难以理解和管理。可能出现包括服务发现、负载平衡、故障恢复、度量和监控，以及更复杂的需求，如 A/B 测试、金丝雀发布、速率限制、访问控制和端到端身份验证。Istio 提供了一个完整的解决方案，通过对整个服务网格提供行为分析和操作控制来满足微服务应用程序的各种需求。“\n读完之后你可能会像我一样困惑！最后在网上查了一圈关于什么是服务网格之后，我终于搞明白了。我最后使用的可能是一个在所有搜索到的样本里一个非代表性的共识，但这是一个合理的选择。不过有个细节确实了，就是如何将它与 k8s 等编排工具分开。Istio 需要跟 k8s 一起使用，没有 k8s 或其他容器编排工具的它就不存在了吗？它没有做编排，实际上它的是为解决管理基于微服务的解决方案中网络和操作复杂性而设计的。它涵盖的范围就像 k8s 一样！现在我真的需要继续这个帖子了。。。\n所以我知道 Istio 是什么，给我们带来了什么，但它实际上解决了什么挑战呢？\n从为什么使用 Istio 页面 中可以看出，它在服务网络中统一提供了许多关键功能：\n流量管理 可观察性 强制策略 服务身份标识和安全 对于我来说，要真正理解 Istio 的价值，所以我使用了 codelab 。编写 code lab 的人真是太棒了！\nCode lab 向我介绍了 Istio 控制平面的四个主要组件：\nPilot：处理代理 sidecar 的配置和编程。 Mixer：为您的流量处理决策并收集遥测数据。 Ingress：处理来自群集外部的传入请求。 CA：证书颁发机构。 查看 Istio 架构概念 页面了解这些组件如何协同工作的。\nCode lab 提供了路由规则 —— 流量管理部分\n我还尝试了 Istio.io 中的一些 task，因为我需要了解它如何处理那些领域的工作。\n提示：如果您在完成 codelab 时也决定在四处看看，那么请将您的群集与应用程序一起启动并运行。无论如何，你会再次使用它。\n所以我对它如何解决这些问题有了一个基本的了解，但是如果我使用像 GKE 这样的托管 K8s（好吧，你知道我会选那个不是吗？）使用 Istio 是否合适？\n注意：是的，这里有更多的细节，但我主要想弄明白为什么需要使用 Istio。\n集群最终用户 / 开发人员访问\nGKE 结合使用 IAM 和 RBAC，是的，这里面有很多东西需要你了解。\n要为您的集群用户授予比 Cloud IAM 更细粒度的权限，您可以使用 namespace 和 RBAC 来限制对特定 pod 的访问或排除对 secret 的访问。\nIstio RBAC 介绍了两个侧重于服务的角色\nServiceRole 定义用于访问网格中的服务的角色。 ServiceRoleBinding 将角色授予主题（例如用户、组、服务）。 它们是 k8s 中的 CustomResourceDefinition（CRD）对象。但您仍然需要了解 IAM。\n服务身份标识 GKE 可以使用 service account 来管理 GKE 上运行的应用程序 可以使用哪些 GCP 服务。这些 service accout 的密钥使用 secret 存储。Pod 中运行的进程的身份标识是由 k8s service account 与 RBAC 一起决定的。Istio 使用 istio-auth ，它使用双向 TLS 提供强大的服务间和最终用户身份验证，内置身份和凭证管理。Istio-auth 使用 Kubernetes service account。\nIstio 不提供任何使用 GCP service account 帮助。这还很早，但是它正在制定未来发展计划的路线图。\nIstio-auth 很好，计划中的增强功能将值得等待。我对安全的复杂性感到厌烦，因为这不可避免地导致配置错误，所以我希望它与 service account 类型之间进行更加无缝的对齐！\n网络控制 GKE（用于 k8s 版本 1.7.6 +）使用 k8s 网络策略 来管理哪些 Pod 可以和服务通信。这是相对简单的配置。 Istio 也有网络策略，但他们不是你知道和喜欢的 K8s 策略，为什么会有这样的区别呢？ 这篇文章 很好地解释了这一点，所以我不会在这里重述，但是这个表格总结了不同之处以及为什么会有这样的不同。\n项目 Istio 策略 网络策略 层 Service（7 层） Network（3、4 层） 实现 Userspace Kernel 控制点 Pod Node Istio 使用 envoy 作为 sidecar 代理。Envoy 在 OSI 模型的应用层运行，所以在第 7 层。我的这篇博客将为你详细解释。\n您需要两种策略类型，这是纵深防御的方法。\n多个集群 Istio 有个非常酷的功能是 mixer 适配器 。简而言之，它可以从底层后端进行抽象以提供核心功能，例如日志记录、监控、配额、ACL 检查等。它公开了一个一致的 API，与使用的后端无关。就像 GCS 公开了一个 API，无论您使用什么存储类别！\n我认为 mixer 适配器模型 博客文章中的这张图片解释了 mixer 适配器中的全部要点。\n有一个早期 demo ，我认为它是 istio 最有用的特性之一，它实际上使用虚拟机来承载 codelab 中使用的评分 dbase MySQL 数据库，并将其作为 GKE 集群所属网格的一部分。使用一个网格来管理它们！\n流量管理 如果你使用了 codelab，你会看到使用 istio 来引导使用路由规则的流量是多么容易。使用 K8s，您还可以使用金丝雀部署进行流量管理，并以类似于 istio 的方式将一定比例的流量引导至您的应用的一个版本，但 Istio 在这种情况下更灵活，方法是允许您设置细粒度流量百分比并控制流量使用 code lab 中的其他标准。\n服务发现 服务注册在 k8s 中完成。Istio 抽象出底层的服务发现机制，并将其转换为 envoy sidecar 可消费的标准格式。\n审计记录和监控 如果是超出 GKE 提供的标准日志记录的话，可以将 GKE 与 StackDriver 日志记录 集成来收集，在持久化存储中存储容器日志、系统日志和关于群集中的活动的事件，例如 Pod 的调度。还可以与 StackDriver Monitoring 集成以收集系统度量指标（度量群集的基础设施，例如 CPU 或内存使用情况）和自定义指标（特定于应用程序的指标）。\nIstio 利用 prometheus 与 grafana 一起作为仪表板进行记录和监控。我喜欢 service graph 配置 ，它可以为您提供 service mesh 的图形表示。你也可以用 kibana 和 fluentd 来配合 Elasticsearch 使用。\n那么我需要 Istio 吗？ Istio 的流量管理非常棒，mixer 适配器模型可以轻松管理覆盖多个群集和虚拟机的网格。我喜欢 Istio 是因为它可以让你进中精力思考服务，而不是那么多的 pod 和节点，并不是说你不必担心这些，而是只关注服务就好了！\n如果你需要管理一个分布式集群，那么 Istio 应该在你的选择列表里。如果您需要在流量管理方面有比 k8s 提供的更多的灵活性的化那么 Istio 也很值得关注。\n如果你有足够的资源来处理处于发展早期的事物，那么尽早理解 Istio 是值得的。如果你已经在使用 k8s 的话那么 istio 的学习曲线将很低。\n记住它是一个建立在上层的东西，所以你仍然需要在 k8s 层做些事情，比如配置 k8s 网络策略来补充 istio 网络策略。\nIstio 还处于发展的早期阶段，所以它不会做你期望的所有事情，但我们希望它会。你将无法避免的在提供商 API 和 Istio 之间来回调用才能完成一个完整的工作，所以它不是你希望的那种一站式解决方案。\nDashboard 是可视化网格配置的一种很好的方式，因为编写 YAML 会让人很快疲惫！是的，您可以设置仪表板上的控制面板来可视化度量指标，但我希望看到它与 StackDriver 集成。\n因此，在总体了解 Istio 之后，我实际上很喜欢它所承诺的内容。\n","relpermalink":"/blog/why-do-we-need-istio/","summary":"Istio 为我们带来了什么？","title":"为什么需要 Istio？"},{"content":"CNCF，全称Cloud Native Computing Foundation（云原生计算基金会），口号是坚持和整合开源技术来编排容器作为微服务架构的一部分，其作为致力于云原生应用推广和普及的一支重要力量，不论您是云原生应用的开发者、管理者还是研究人员都有必要了解。\nCNCF作为一个厂商中立的基金会，致力于Github上的快速成长的开源技术的推广，如Kubernetes、Prometheus、Envoy等，帮助开发人员更快更好的构建出色的产品。\nhttp://github.com/cncf/landscape 中维护了一幅CNCF的全景图。\n其中包含了CNCF中托管的项目，还有很多是非CNCF项目，还有个交互式的浏览CNCF涵盖的所有的项目的页面：https://i.cncf.io\n关于CNCF的使命与组织方式请参考CNCF章程 ，概括的讲CNCF的使命宝库以下三点：\n容器化包装。 通过中心编排系统的动态资源管理。 面向微服务。 以上是CNCF最初对云原生特征的定义。\nCNCF这个角色的作用是推广技术，形成社区，开源项目管理与推进生态系统健康发展。\n另外CNCF组织由以下部分组成：\n会员：白金、金牌、银牌、最终用户、学术和非赢利成员，不同级别的会员在治理委员会中的投票权不同。 理事会：负责事务管理 TOC（技术监督委员会）：技术管理 最终用户社区：推动CNCF技术的采纳并选举最终用户技术咨询委员会 最终用户技术咨询委员会：为最终用户会议或向理事会提供咨询 营销委员会：市场推广 CNCF项目成熟度分级与毕业条件 每个CNCF项目都需要有个成熟度等级，申请成为CNCF项目的时候需要确定项目的成熟度级别。\n成熟度级别（Maturity Level）包括以下三种：\nsandbox（初级）一开始是叫inception incubating（孵化中） graduated（毕业） 是否可以成为CNCF项目需要通过Technical Oversight Committee （技术监督委员会）简称TOC ，投票采取fallback策略，即回退策略，先从最高级别（graduated，目前是从incubating）开始，如果2/3多数投票通过的话则确认为该级别，如果没通过的话，则进行下一低级别的投票，如果一直到inception级别都没得到2/3多数投票通过的话，则拒绝其进入CNCF项目。一般一个项目处于孵化阶段不会超过2年。\n当前所有的CNCF项目可以访问 https://www.cncf.io/projects/ 在太平洋时间3月6日，Kubernetes成为了CNCF的第一个毕业项目！\nTOC（技术监督委员会） TOC（Technical Oversight Committee）作为CNCF中的一个重要组织，它的作用是：\n定义和维护技术视野 审批新项目加入组织，为项目设定概念架构 接受最终用户的反馈并映射到项目中 调整组件见的访问接口，协调组件之间兼容性 TOC成员通过选举产生，见选举时间表 。\n参考CNCF TOC：https://github.com/cncf/toc\nCNCF章程 CNCF（云原生计算基金会）是Linux基金会旗下的一个基金会，加入CNCF等于同时加入Linux基金会（也意味着你还要交Linux基金会的份子钱），对于想加入CNCF基金会的企业或者组织首先要做的事情就是要了解CNCF的章程（charter），就像是作为一个国家的公民，必须遵守该国家的宪法一样。CNCF之所以能在短短三年的时间内发展壮大到如此规模，很大程度上是与它出色的社区治理和运作模式有关。了解该章程可以帮助我们理解CNCF是如何运作的，也可以当我们自己进行开源项目治理时派上用场。\n该章程最后更新于2018年5月15日，详见https://www.cncf.io/about/charter/ 。下文中关于CNCF章程的介绍部分引用自CNCF 是如何工作的 ，有改动。\n1. CNCF的使命 CNCF 没有偏离自己的主题，核心是解决技术问题：基金会的使命是创建并推动采用新的计算模式，该模式针对现代分布式系统环境进行了优化，能够扩展至数万个自愈式多租户节点。\n所谓的云原生系统须具备下面这些属性：\n应用容器化：将软件容器中的应用程序和进程作为独立的应用程序部署单元运行，并作为实现高级别资源隔离的机制。从总体上改进开发者的体验、促进代码和组件重用，而且要为云元是国内应用简化运维工作。 动态管理：由中心化的编排来进行活跃的调度和频繁的管理，从根本上提高机器效率和资源利用率，同时降低与运维相关的成本。 面向微服务：与显式描述的依赖性松散耦合（例如通过服务端点），可以提高应用程序的整体敏捷性和可维护性。CNCF 将塑造技术的发展，推动应用管理的先进技术发展，并通过可靠的接口使技术无处不在，并且易于使用。 2. CNCF扮演的角色 CNCF 其实是在开源社区的基础上发挥着作用，应负责：\na) 项目管理\n确保技术可用于社区并且没有杂七杂八的影响 确保技术的品牌（商标和标识）得到社区成员的关注和使用，特别强调统一的用户体验和高水平的应用程序兼容性 b) 促进生态系统的发展和演进\n评估哪些技术可以纳入云原生计算应用的愿景，鼓励社区交付这样的技术，以及集成它们，且要积极的推进总结进度。 提供一种方法来培养各个部分的通用技术标准 c) 推广底层技术和应用定义和管理方法，途径包括：活动和会议、营销（SEM、直接营销）、培训课程和开发人员认证。\nd) 通过使技术可访问和可靠来为社区服务\n旨在通过对参考架构进行明确定义的节奏，为每个组成部分提供完全集成和合格的构建。 3. CNCF的价值观 CNCF 会极力遵循以下一些原则：\n快速胜过磨叽，基金会的初衷之一就是让项目快速的发展，从而支持用户能够积极的使用。 开放！ CNCF 是以开放和高度透明为最高准则的，而且是独立于任何的其它团体进行运作的。CNCF根据贡献的内容和优点接受所有的贡献者，且遵循开源的价值观，CNCF输出的技术是可以让所有人使用和受益的，技术社区及其决策应保持高度透明。 公平：CNCF 会极力避免那些不好的影响、不良行为、以及“按需付费”的决策。 强大的技术身份：CNCF 会实现并保持高度的自身技术认同，并将之同步到所有的共享项目中。 清晰的边界：CNCF 制定明确的目标，并在某些情况下，要确定什么不是基金会的目标，并会帮助整个生态系统的运转，让人们理解新创新的重点所在。 可扩展：能够支持从小型开发人员中心环境到企业和服务提供商规模的所有部署规模。这意味着在某些部署中可能不会部署某些可选组件，但总体设计和体系结构仍应适用。 平台中立：CNCF 所开发的项目并不针对某个特定平台，而是旨在支持各种体系结构和操作系统。 4. 会员制 CNCF中的会员包括白金、金牌、银牌、最终用户、学术和非赢利成员等级别，不同级别的会员在理事会中的投票权不同。\na) 白金会员：在CNCF理事会中任命1名代表，在理事会的每个次级委员会和活动中任命1名有投票权的代表，在网站可以突出显示；如果也是终端用户成员将继承终端用户成员的所有权利\nb) 金牌会员：基金会中每有5个金牌会员，该级别的会员就可以任命1名代表，最多任命3个；如果也是终端用户成员将继承终端用户成员的所有权利\nc) 银牌会员：基金会中每有10个银牌会员，该级别的会员就可以任命1名代表，最多任命3个；如果也是终端用户成员将继承终端用户成员的所有权利\nd) 终端用户：参加终端用户咨询社区；向终端用户技术咨询委员会中提名1名代表\ne) 学术和非赢利会员：学术和非营利会员分别限于学术和非营利机构，需要理事会批准。学术成员和非营利成员有权将其组织认定为支持CNCF使命的成员以及理事会确定的任何其他权利或利益。\n5. 理事会 a) CNCF理事会负责市场营销、业务监督和预算审批，不负责技术方面，除了与TOC配合确定CNCF工作范围、完成时间表a)、更新CNCF网站\nb) 负责日常事务\n与TOC协商CNCF的整体范围 商标和版权保护 市场营销、布道和生态系统建设 创建和执行品牌承诺项目，如果需要的话 监督运营，业务发展； 募资和财务管理 c) 理事会投票成员由会员代表和社区代表组成：\n成员代表包括： 每名白金会员任命1名代表 黄金和银牌成员当选代表 技术社区代表包括： 技术监督委员会主席 根据当时在任的理事会批准的程序从CNCF项目中选出两名提交者。 理事会可能会以白金会员比例的价格扩展白金会员资格，对年收入低于5000万美元的创业公司进行长达5年的逐年审计，这些公司被视为理事会的战略技术贡献者。 只有来自一组关联公司的人员可以担任会员代表。只有来自一组关联公司的人员可以担任技术社区代表。 d) 职责\n批准预算，指导将所有收入来源筹集的资金用于技术、市场或社区投资，以推动 CNCF 基金的使命； 选举理事会主席主持会议，批准预算批准的支出并管理日常运作； 对理事会的决定或事项进行投票； 界定和执行基金会的知识产权（版权，专利或商标）政策； 通过活动、新闻和分析师宣传、网络、社交媒体以及其他营销活动进行直接营销和布道； 监督运营，业务发展； 建立并监督为推动CNCF的使命而创建的任何委员会； 根据CNCF要求（可能包括认证测试）建立并执行品牌合规计划（如有），以使用TOC建立的品牌标志； 采用商标使用准则或政策； 提供整体财务管理。 e) 基金会的收入用途\n市场营销，用户扩展CNCF中的项目的采用 关键设施建设、运行和管理项目的基础设施 促进基于容器的计算使用CNCF中的项目实现 6. 技术监督委员会（TOC） a) 要求\nCNCF 技术监督委员会，为了保持中立，则达成了以下共识：\n定义和维护CNCF的技术愿景。 批准由理事会制定的CNCF范围内的新项目，并为项目创建一个概念架构。 纠正项目的发展方向，决策删除或存档项目。 接受最终用户委员会的反馈并反映在项目中。 在科学管理的情况下调整组件的接口（在代码标准化之前实现参考） 定义在CNCF项目中实施的常用做法（如果有的话）。 b) 技术监督委员会的构成\nTOC最多由9名成员组成。 选出的TOC成员将涵盖关键的技术领域：容器技术、操作系统、技术运维、分布式系统、用户级应用程序设计等。 理事会将选举6名TOC成员，最终用户TAB将选出1名TOC成员，最初的7名TOC成员应另选两名TOC成员。 如果超过两名TOC成员来自同一组关联公司，无论是在选举时还是来自后来的工作变更，他们将共同决定谁应该下台，或如果没有协商的依据，则应抽签决定。 c) 运营模式\nTOC 会选举出TOC的主席来，此角色主要负责 TOC 的议程和召集会议。 TOC 每个季度会面对面讨论重要的热点问题。 TOC 可能会根据需要开会讨论新出现的问题。 TOC审核可能会提出以下问题： 任何的 TOC 成员 任何的理事会成员 建立的CNCF项目的维护者或顶级项目负责人 CNCF 执行董事 最终用户技术咨询委员会获得多数票 保持透明：TOC会议、邮件列表、以及会议记录等均是公开可访问的。 简单的TOC问题可以通过简短的讨论和简单的多数表决来解决。TOC讨论可通过电子邮件或TOC会议进行。 在对意见和可选虚拟讨论/辩论选项进行审查后，寻求共识并在必要时进行投票。 目的是让TOC在TOC和社区内寻找达成共识的途径。满足法定人数要求的会议的TOC决定应以超过TOC成员出席率的50％的方式通过。 TOC会议需要TOC总人数的三分之二法定人数进行表决或作出任何决定。如果TOC会议未能达到法定人数要求，可以进行讨论，但不应有任何投票或决定。 TOC决定可以在没有会议的情况下以电子方式提出，但要通过表决则需要多少票数才能达到会议法定人数。在电子投票中，如果任何两名TOC成员要求召开会议讨论决定，则电子投票结束时无效，并且在会议结束后可以启动新的投票，以讨论决定已经完成。 d) 提名标准\n获得 TOC 提名的开源贡献者应该具备下面条件：\n承诺有足够的可用可用时间参与CNCF TOC的活动，包括 …","relpermalink":"/blog/cncf-introduction/","summary":"CNCF云原生计算基金会简介以及CNCF的运作方式与项目成熟度级别标准介绍。","title":"CNCF - 云原生计算基金会简介"},{"content":"在我们安装Kubernetes集群的时候就已经安装了kube-dns插件，这个插件也是官方推荐安装的。通过将 Service 注册到 DNS 中，Kuberentes 可以为我们提供一种简单的服务注册发现与负载均衡方式。\nCoreDNS 作为CNCF中的托管的一个项目，在Kuberentes1.9版本中，使用kubeadm方式安装的集群可以通过以下命令直接安装CoreDNS。\nkubeadm init --feature-gates=CoreDNS=true 您也可以使用CoreDNS替换Kubernetes插件kube-dns，可以使用 Pod 部署也可以独立部署，请参考Using CoreDNS for Service Discovery ，下文将介绍如何配置kube-dns。\n本文已归档到kubernetes-handbook 中。\nkube-dns kube-dns是Kubernetes中的一个内置插件，目前作为一个独立的开源项目维护，见https://github.com/kubernetes/dns。\n下文中给出了配置 DNS Pod 的提示和定义 DNS 解析过程以及诊断 DNS 问题的指南。\n前提要求 Kubernetes 1.6 及以上版本。 集群必须使用 kube-dns 插件进行配置。 kube-dns 介绍 从 Kubernetes v1.3 版本开始，使用 [cluster add-on 插件管理器回自动启动内置的 DNS。\nKubernetes DNS pod 中包括 3 个容器：\nkubedns：kubedns 进程监视 Kubernetes master 中的 Service 和 Endpoint 的变化，并维护内存查找结构来服务DNS请求。 dnsmasq：dnsmasq 容器添加 DNS 缓存以提高性能。 sidecar：sidecar 容器在执行双重健康检查（针对 dnsmasq 和 kubedns）时提供单个健康检查端点（监听在10054端口）。 DNS pod 具有静态 IP 并作为 Kubernetes 服务暴露出来。该静态 IP 分配后，kubelet 会将使用 --cluster-dns = \u0026lt;dns-service-ip\u0026gt; 标志配置的 DNS 传递给每个容器。\nDNS 名称也需要域名。本地域可以使用标志 --cluster-domain = \u0026lt;default-local-domain\u0026gt; 在 kubelet 中配置。\nKubernetes集群DNS服务器基于 SkyDNS 库。它支持正向查找（A 记录），服务查找（SRV 记录）和反向 IP 地址查找（PTR 记录）\nkube-dns 支持的 DNS 格式 kube-dns 将分别为 service 和 pod 生成不同格式的 DNS 记录。\nService\nA记录：生成my-svc.my-namespace.svc.cluster.local域名，解析成 IP 地址，分为两种情况： 普通 Service：解析成 ClusterIP Headless Service：解析为指定 Pod 的 IP 列表 SRV记录：为命名的端口（普通 Service 或 Headless Service）生成 _my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster.local 的域名 Pod\nA记录：生成域名 pod-ip.my-namespace.pod.cluster.local kube-dns 存根域名 可以在 Pod 中指定 hostname 和 subdomain：hostname.custom-subdomain.default.svc.cluster.local，例如：\napiVersion: v1 kind: Pod metadata: name: busybox labels: name: busybox spec: hostname: busybox-1 subdomain: busybox-subdomain containers: name: busybox - image: busybox command: - sleep - \u0026#34;3600\u0026#34; 该 Pod 的域名是 busybox-1.busybox-subdomain.default.svc.cluster.local。\n继承节点的 DNS 运行 Pod 时，kubelet 将预先配置集群 DNS 服务器到 Pod 中，并搜索节点自己的 DNS 设置路径。如果节点能够解析特定于较大环境的 DNS 名称，那么 Pod 应该也能够解析。请参阅下面的已知问题 以了解警告。\n如果您不想要这个，或者您想要为 Pod 设置不同的 DNS 配置，您可以给 kubelet 指定 --resolv-conf 标志。将该值设置为 \u0026#34;\u0026#34; 意味着 Pod 不继承 DNS。将其设置为有效的文件路径意味着 kubelet 将使用此文件而不是 /etc/resolv.conf 用于 DNS 继承。\n配置存根域和上游 DNS 服务器 通过为 kube-dns （kube-system:kube-dns）提供一个 ConfigMap，集群管理员能够指定自定义存根域和上游 nameserver。\n例如，下面的 ConfigMap 建立了一个 DNS 配置，它具有一个单独的存根域和两个上游 nameserver：\napiVersion: v1 kind: ConfigMap metadata: name: kube-dns namespace: kube-system data: stubDomains: | {“acme.local”: [“1.2.3.4”]} upstreamNameservers: | [“8.8.8.8”, “8.8.4.4”] 如上面指定的那样，带有“.acme.local”后缀的 DNS 请求被转发到 1.2.3.4 处监听的 DNS。Google Public DNS 为上游查询提供服务。\n下表描述了如何将具有特定域名的查询映射到其目标DNS服务器：\n域名 响应查询的服务器 kubernetes.default.svc.cluster.local kube-dns foo.acme.local 自定义 DNS (1.2.3.4) widget.com 上游 DNS (8.8.8.8 或 8.8.4.4) 查看 ConfigMap 选项 获取更多关于配置选项格式的详细信息。\n对 Pod 的影响 自定义的上游名称服务器和存根域不会影响那些将自己的 dnsPolicy 设置为 Default 或者 None 的 Pod。\n如果 Pod 的 dnsPolicy 设置为 “ClusterFirst”，则其名称解析将按其他方式处理，具体取决于存根域和上游 DNS 服务器的配置。\n未进行自定义配置：没有匹配上配置的集群域名后缀的任何请求，例如 “www.kubernetes.io”，将会被转发到继承自节点的上游 nameserver。\n进行自定义配置：如果配置了存根域和上游 DNS 服务器（和在 前面例子 配置的一样），DNS 查询将根据下面的流程进行路由：\n查询首先被发送到 kube-dns 中的 DNS 缓存层。\n从缓存层，检查请求的后缀，并转发到合适的 DNS 上，基于如下的示例：\n具有集群后缀的名字 （例如 “.cluster.local”）：请求被发送到 kube-dns。 具有存根域后缀的名字 （例如 “.acme.local”）：请求被发送到配置的自定义 DNS 解析器（例如：监听在 1.2.3.4）。 不具有能匹配上后缀的名字 （例如 “widget.com”）：请求被转发到上游 DNS（例如：Google 公共 DNS 服务器，8.8.8.8 和 8.8.4.4）。 DNS lookup flow ConfigMap 选项 kube-dns kube-system:kube-dns ConfigMap 的选项如下所示：\n字段 格式 描述 stubDomains（可选） 使用 DNS 后缀 key 的 JSON map（例如 “acme.local”），以及 DNS IP 的 JSON 数组作为 value。 目标 nameserver 可能是一个 Kubernetes Service。例如，可以运行自己的 dnsmasq 副本，将 DNS 名字暴露到 ClusterDNS namespace 中。 upstreamNameservers（可选） DNS IP 的 JSON 数组。 注意：如果指定，则指定的值会替换掉被默认从节点的 /etc/resolv.conf 中获取到的 nameserver。限制：最多可以指定三个上游 nameserver。 示例 示例：存根域 在这个例子中，用户有一个 Consul DNS 服务发现系统，他们希望能够与 kube-dns 集成起来。 Consul 域名服务器地址为 10.150.0.1，所有的 Consul 名字具有后缀 “.consul.local”。 要配置 Kubernetes，集群管理员只需要简单地创建一个 ConfigMap 对象，如下所示：\napiVersion: v1 kind: ConfigMap metadata: name: kube-dns namespace: kube-system data: stubDomains: | {“consul.local”: [“10.150.0.1”]} 注意，集群管理员不希望覆盖节点的上游 nameserver，所以他们不会指定可选的 upstreamNameservers 字段。\n示例：上游 nameserver 在这个示例中，集群管理员不希望显式地强制所有非集群 DNS 查询进入到他们自己的 nameserver 172.16.0.1。 而且这很容易实现：他们只需要创建一个 ConfigMap，upstreamNameservers 字段指定期望的 nameserver 即可。\napiVersion: v1 kind: ConfigMap metadata: name: kube-dns namespace: kube-system data: upstreamNameservers: | [“172.16.0.1”] 调试 DNS 解析 创建一个简单的 Pod 用作测试环境 创建一个名为 busybox.yaml 的文件，其中包括以下内容：\napiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - name: busybox image: busybox command: - sleep - \u0026#34;3600\u0026#34; imagePullPolicy: IfNotPresent restartPolicy: Always 使用该文件创建 Pod 并验证其状态：\n$ kubectl create -f busybox.yaml pod \u0026#34;busybox\u0026#34; created $ kubectl get pods busybox NAME READY STATUS RESTARTS AGE busybox 1/1 Running 0 \u0026lt;some-time\u0026gt; 该 Pod 运行后，您可以在它的环境中执行 nslookup。如果您看到类似如下的输出，表示 DNS 正在正确工作。\n$ kubectl exec -ti busybox -- nslookup kubernetes.default Server: 10.0.0.10 Address 1: 10.0.0.10 Name: kubernetes.default Address 1: 10.0.0.1 如果 nslookup 命令失败， …","relpermalink":"/blog/configuring-kubernetes-kube-dns/","summary":"配置 Kubernetes DNS Pod 的提示和定义 DNS 解析过程以及诊断 DNS 问题的指南。","title":"配置Kubernetes DNS服务kube-dns"},{"content":"2017年，我们面临着架构变革的大时代，例如Kubernetes结束容器编排之争、Kafka发布1.0、Serverless逐渐发力、边缘计算要取代云计算、Service Mesh蓄势待发，另外人工智能为业务赋能，也给架构带来了新的挑战。\n我即将参加InfoQ在12月8-11日北京国际会议中心举办ArchSummit全球架构师峰会，这场大会还邀请了诸如阿里王坚博士等100+顶尖技术人前来分享总结今年的架构变化和思考。希望各位能凭借这场大会承上启下，总结过去的实践，也展望面向未来的架构，将这个变革的时代转变为我们每人共同的幸运。\n我的演讲 我的演讲内容是从Kubernetes到Cloud Native——云原生应用之路，链接：从Kubernetes到Cloud Native——云原生应用之路 。时间是12月9日，星期六，上午9:30，第五会议室。\n云计算经过了十多年的发展，已然进入的云原生的新阶段，企业应用优先考虑部署在云环境，如何顺应云原生的大潮，使用容器和Kubernetes构建云原生平台，践行DevOps理念和敏捷IT，开源软件和社区如何助力IT转型，所有这些问题的解决方案就是PaaS平台，其对于企业的重要性不言而喻。\n我们还为大家准备了赠品：《Cloud Native Go——基于Go和React构建云原生Web应用》 和《智能数据时代——企业大数据战略与实战》，到场的朋友都有机会获得，同时场外还有电子工业出版社博文视点的图书展台，欢迎大家参观。\nArchSummit大会官网链接：http://bj2017.archsummit.com/ 更多详细信息请参考大会官网：http://bj2017.archsummit.com/ ","relpermalink":"/notice/archsummit-beijing-2017-from-kubernetes-to-cloud-native/","summary":"Jimmy Song 将在ArchSummit北京站的演讲，从Kubernetes到Cloud Native，我的云原生应用之路。","title":"ArchSummit Beijing 2017演讲预告"},{"content":"本文主要讲解访问 kubenretes 中的 Pod 和 Serivce 的几种方式，包括如下几种：\nhostNetwork hostPort NodePort LoadBalancer Ingress 说是暴露 Pod 其实跟暴露 Service 是一回事，因为 Pod 就是 Service 的 backend。\nhostNetwork: true 这是一种直接定义 Pod 网络的方式。\n如果在 Pod 中使用 hostNetwork:true 配置的话，在这种 pod 中运行的应用程序可以直接看到 pod 启动的主机的网络接口。在主机的所有网络接口上都可以访问到该应用程序。以下是使用主机网络的 pod 的示例定义：\napiVersion: v1 kind: Pod metadata: name: influxdb spec: hostNetwork: true containers: - name: influxdb image: influxdb 部署该 Pod：\n$ kubectl create -f influxdb-hostnetwork.yml 访问该 pod 所在主机的 8086 端口：\ncurl -v http://$POD_IP:8086/ping 将看到 204 No Content 的 204 返回码，说明可以正常访问。\n注意每次启动这个 Pod 的时候都可能被调度到不同的节点上，所有外部访问 Pod 的 IP 也是变化的，而且调度 Pod 的时候还需要考虑是否与宿主机上的端口冲突，因此一般情况下除非您知道需要某个特定应用占用特定宿主机上的特定端口时才使用 hostNetwork: true 的方式。\n这种 Pod 的网络模式有一个用处就是可以将网络插件包装在 Pod 中然后部署在每个宿主机上，这样该 Pod 就可以控制该宿主机上的所有网络。\nhostPort 这是一种直接定义 Pod 网络的方式。\nhostPort 是直接将容器的端口与所调度的节点上的端口路由，这样用户就可以通过宿主机的 IP 加上来访问 Pod 了，如:。\napiVersion: v1 kind: Pod metadata: name: influxdb spec: containers: - name: influxdb image: influxdb ports: - containerPort: 8086 hostPort: 8086 这样做有个缺点，因为 Pod 重新调度的时候该 Pod 被调度到的宿主机可能会变动，这样就变化了，用户必须自己维护一个 Pod 与所在宿主机的对应关系。\n这种网络方式可以用来做 nginx Ingress controller 。外部流量都需要通过 kubenretes node 节点的 80 和 443 端口。\nNodePort NodePort 在 kubenretes 里是一个广泛应用的服务暴露方式。Kubernetes 中的 service 默认情况下都是使用的 ClusterIP 这种类型，这样的 service 会产生一个 ClusterIP，这个 IP 只能在集群内部访问，要想让外部能够直接访问 service，需要将 service type 修改为 nodePort。\napiVersion: v1 kind: Pod metadata: name: influxdb labels: name: influxdb spec: containers: - name: influxdb image: influxdb ports: - containerPort: 8086 同时还可以给 service 指定一个 nodePort 值，范围是 30000-32767，这个值在 API server 的配置文件中，用 --service-node-port-range 定义。\nkind: Service apiVersion: v1 metadata: name: influxdb spec: type: NodePort ports: - port: 8086 nodePort: 30000 selector: name: influxdb 集群外就可以使用 kubernetes 任意一个节点的 IP 加上 30000 端口访问该服务了。kube-proxy 会自动将流量以 round-robin 的方式转发给该 service 的每一个 pod。\n这种服务暴露方式，无法让你指定自己想要的应用常用端口，不过可以在集群上再部署一个反向代理作为流量入口。\nLoadBalancer LoadBalancer 只能在 service 上定义。这是公有云提供的负载均衡器，如 AWS、Azure、CloudStack、GCE 等。\nkind: Service apiVersion: v1 metadata: name: influxdb spec: type: LoadBalancer ports: - port: 8086 selector: name: influxdb 查看服务：\n$ kubectl get svc influxdb NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE influxdb 10.97.121.42 10.13.242.236 8086:30051/TCP 39s 内部可以使用 ClusterIP 加端口来访问服务，如 19.97.121.42:8086。\n外部可以用以下两种方式访问该服务：\n使用任一节点的 IP 加 30051 端口访问该服务 使用 EXTERNAL-IP 来访问，这是一个 VIP，是云供应商提供的负载均衡器 IP，如 10.13.242.236:8086。 Ingress Ingress 是自 kubernetes1.1 版本后引入的资源类型。必须要部署 Ingress controller 才能创建 Ingress 资源，Ingress controller 是以一种插件的形式提供。Ingress controller 是部署在 Kubernetes 之上的 Docker 容器。它的 Docker 镜像包含一个像 nginx 或 HAProxy 的负载均衡器和一个控制器守护进程。控制器守护程序从 Kubernetes 接收所需的 Ingress 配置。它会生成一个 nginx 或 HAProxy 配置文件，并重新启动负载平衡器进程以使更改生效。换句话说，Ingress controller 是由 Kubernetes 管理的负载均衡器。\nKubernetes Ingress 提供了负载平衡器的典型特性：HTTP 路由，粘性会话，SSL 终止，SSL 直通，TCP 和 UDP 负载平衡等。目前并不是所有的 Ingress controller 都实现了这些功能，需要查看具体的 Ingress controller 文档。\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: influxdb spec: rules: - host: influxdb.kube.example.com http: paths: - backend: serviceName: influxdb servicePort: 8086 外部访问 URL http://influxdb.kube.example.com/ping 访问该服务，入口就是 80 端口，然后 Ingress controller 直接将流量转发给后端 Pod，不需再经过 kube-proxy 的转发，比 LoadBalancer 方式更高效。\n总结 总的来说 Ingress 是一个非常灵活和越来越得到厂商支持的服务暴露方式，包括 Nginx、HAProxy、Traefik，还有各种 Service Mesh，而其它服务暴露方式可以更适用于服务调试、特殊应用的部署。\n","relpermalink":"/blog/accessing-kubernetes-pods-from-outside-of-the-cluster/","summary":"关于在 Kubenretes 中暴露 Pod 及服务的五种方式。","title":"从外部访问 Kubernetes 中的 Pod"},{"content":"Cloudinary-go is a Go client library and CLI tool to upload static assets to the Cloudinary service.\nInstallation Install the CLI tool and the library with:\ngo get github.com/rootsongjc/cloudinary-go/cloudinary Or download the release binary from release .\n","relpermalink":"/notice/cloudinary-go/","summary":"Cloudinary-go is a Go client library and CLI tool to upload static assets to Cloudinary service","title":"Go 语言编写的Cloudinary文件上传工具发布"},{"content":"截止本文发稿时，笔者是以下两本云原生图书的译者：\nCloud Native Go ：已由电子工业出版社出版 Cloud Native Python ：正在翻译中 同时我还参与了 Kubernetes 、Istio 的文档翻译，撰写了开源电子书 kubernetes-handbook ，下面是我本人在翻译过程中的的一些心得。\n说明：本文中使用的方法仅供参考，机器翻译有助您快速了解全书或文章的梗概，请勿直接使用机器翻译结果输出。\n图书引进 1. 联系出版社 假如您看到一本很好的外文书籍想要翻译，首先需要联系出版社，询问该书是否已被引进，因为每年国内引进的外文书籍是有数量控制的，而且有的书也不是你先给引进就可以引进的，每年都有版权引进会议，国内的出版社统一参加确定引进的书籍，哪家引进多少本，哪一本分给哪一家等。可以与出版社编辑沟通，查看该书是否可以引进，是否已经有别的出版社引进且在翻译中，这个过程基本不需要你与原作者沟通。\n2. 取得图书引进的版权 如果很幸运的，这本书可以引进到国内，而且还没有人来翻译，可以跟出版社编辑要求翻译这本书，如果书籍内容适当可以一个人翻译，如果内容较多可以分多个人翻译，建议人数不要超过 4 人。\n环境准备 首先需要准备如下环境：\nGit：用户版本管理，也方便在线查看，我使用 码云 私有代码库管理。 Markdown 编辑器：我推荐使用 typora 。 Gitbook：使用 Gitbook 生成 web 页面便于阅读和查看，注意不要公开发布到 Github 上。 Word：虽然我们使用 markdown 编辑器来编辑，但是 word 还是需要的，因为编辑会在 word 中批注，再返回给你修改。 Translation-shell：命令行翻译工具，见 Github 。 翻译过程 以下是我个人总结的图书翻译流程，仅供参考。\n1. 分析原版压缩包的结构 以 Cloud Native Python 这本书为例，原文的压缩包里包含以下目录：\nCode：书中的代码示例 Cover：本书的封面图片 E-Book：本书的完成 PDF 文档（一个文件） Graphics：书中的图片，按照章节和顺序编号，放在一个目录下，不一定与图片在书中出现的顺序相同，有些后来补充的图片会另外编号 Printers：用于印刷的 PDF 文档，分为封面和正文 2. 初始化翻译项目 我们使用 Git 来管理，使用 Gitbook 来预览，需要先初始化一些目录结构和 gitbook 配置。\n初始化的目录和文件：\nLANGS.md：语言配置文件 README.md：项目说明 book.json：gitbook 配置文件 cn：中文翻译（按章节划分成不同的文件） corrigendum.md：勘误表 cover.jpg：书籍封面 en：英文原文（按章节划分成不同的文件） glossary.md：术语表 images：保存书中的图片 让 Gitbook 支持多语言的 book.json 配置如下：\n{ \u0026#34;title\u0026#34;: \u0026#34;Cloud Native Python\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Cloud Native Python\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;zh-hans\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Jimmy Song\u0026#34;, \u0026#34;links\u0026#34;: { \u0026#34;sidebar\u0026#34;: {\u0026#34;Home\u0026#34;: \u0026#34;https://jimmysong.io\u0026#34;} }, \u0026#34;plugins\u0026#34;: [ \u0026#34;codesnippet\u0026#34;, \u0026#34;splitter\u0026#34;, \u0026#34;page-toc-button\u0026#34;, \u0026#34;back-to-top-button\u0026#34;, \u0026#34;-lunr\u0026#34;, \u0026#34;-search\u0026#34;, \u0026#34;search-plus\u0026#34;, \u0026#34;tbfed-pagefooter@^0.0.1\u0026#34; ], \u0026#34;pluginsConfig\u0026#34;: { \u0026#34;tbfed-pagefooter\u0026#34;: { \u0026#34;copyright\u0026#34;: \u0026#34;Copyright © jimmysong.io 2017\u0026#34;, \u0026#34;modify_label\u0026#34;: \u0026#34;Updated:\u0026#34;, \u0026#34;modify_format\u0026#34;: \u0026#34;YYYY-MM-DD HH:mm:ss\u0026#34; } } } LANG.md 文件中定义不同语言的文件目录：\n# Languages * [中文](cn/) * [English](en/) 3. 原文 Markdown 化 之所以将原文 Markdown 化一是便于我们后续翻译的时候对照英文和引用其中的原文，二是为了生成 gitbook 便于浏览。将每一章的内容都划分成一个 Markdown 文件，按照章节的名字为文档命名，分别在 cn 和 en 目录下都放一份。\n中英文目录 4. 开始正文的翻译 建议从头开始按顺序翻译，如果前后章节联系不大的可以跳跃翻译，翻译的过程中将一些关键的术语，包括翻译不明确的，需要后续参考的数据记录在 glossary.md 文档中。\n格式如下所示：\nEnglish 中文 说明 是否翻译 Cross-Origin Resource Sharing 跨源资源共享 是 HTTP header 否 Observable 观察者 可以不翻译，中文翻译比较模糊 否 cookies 不翻译，保持复数 否 module 模块 是 origin 源 有争议 是 session 会话 否 可以不断向其中追加新的术语。\n翻译的过程中需要用到翻译工具，我使用的是 translation-shell ，一款基于命令行的翻译工具，可以使用 Google、bing 或者 Yandex 翻译，十分方便快捷。也推荐大家使用 DeepL ，翻译效果更好。\n注：使用翻译工具是为了将书籍快速汉化，减少大量的人工输入，但是因为机器翻译比较生硬，而且其中难免有错误，需要译者投入大量心思去优化。\nTranslation-shell 使用 trans :zh -b -shell 进入 translation-shell 交互式界面，拷贝英文段落进去翻译成中文。\nTranslation-shell 注：推荐使用翻译质量更高的工具 DeepL （更新于 2022年02月22日）。\n使用 Typora 编辑中文翻译 同时打开 en 和 cn 目录下的同一章节开始翻译。\n中英文翻译界面 在 Gitbook 中查看 使用 gitbook serve 启用 gitbook 服务，在 http://localhost:4000 页面上查看内容。\n首先会出来语言选择页面，我们可以分别选择中文和英文内容浏览，语言是在 LAGNS.md 文件中定义的。\nGitbook 导出为不同格式 使用 typora 编辑完中文翻译后，可以导出为 pdf、word 等其它格式，我们导出为 word 格式后发送给编辑批阅。\n生成的 word 内容格式是这样的：\nword 文档格式 我们可以看到生产的 word 文档仍然保留了代码的高亮，而且可读性也很好。\n5. 审校 每当翻译完一章内容后就发送给编辑，编辑会使用 word 进行审校批注，根据编辑的批注修改后再发回给编辑。\nword review 界面 6. 二审 当所有的章节分别翻译和审校完成后，需要在通读一遍全书，更正前后不一致和翻译中的谬误，然后交给编辑等待排版。这时候还要准备写译者序，还要找人写推荐序。翻译版的图书封面会沿用原书的封面。\n7. 印刷 当正文、译者序、推荐序都完成后就可以交给出版社印刷了，一般初次会印刷几千本。\n8. 后续事宜 书籍印刷后后续事宜主要包括：\n出版社支付稿费：翻译图书稿费 = 图书销量 x 定价 x4%，著作一般为 8% 配合图书宣传：一些 meetup、大会、线上交流时推荐图书 读者交流：可以开设社区、微信群、网站等交流 贴士 图书翻译耗时费力，倾注了原作者和译者的很多心力，打击盗版，维护正版！\n有用的链接 术语在线 非文学翻译理论与实践 - 王长栓 ","relpermalink":"/blog/how-to-translate-a-book/","summary":"如何翻译一本外文书，从图书引进到翻译出版全攻略。","title":"如何翻译一本外文书"},{"content":"很多人问我 jimmysong.io 这个网站是怎么做出来的，我想有必要写本书给大家普及下静态网站构建的知识还有Hugo这个利器。\n本手册将指导你如何使用Hugo 构建静态网站用于个人博客或者项目展示。\n手把手教你如何从0开始构建一个静态网站，这不需要有太多的编程和开发经验和时间投入，也基本不需要多少成本（除了个性化域名），使用GitHub和Hugo模板即可快速构建和上线一个网站。\nGithub地址：https://github.com/rootsongjc/hugo-handbook Gitbook访问地址：https://jimmysong.io/hugo-handbook 将随着时间的推移，也随着我的网站的改进而不断的完善本书内容，敬请期待。\n","relpermalink":"/notice/building-static-website-with-hugo/","summary":"静态网站构建手册，使用Hugo构建个人博客Gitbook。","title":"Hugo Handbook发布"},{"content":"今天受 k8smeetup 社区邀请来到杭州，参加 Kubernetes 中国用户大会简称 KEUC ，这已经是我第三次来杭州了，算是再续前缘吧！\n其实今年 6 月 19 日 LinuxCon + ContainerCon + CloudOpen 简称 L3 大会 在北京国家会议中心召开，那是我跟 CNCF 首次相会，也获得了我的首批贴纸，该社区的一系列活动吸引了我浓浓的兴趣，自那以后开始持续关注 CNCF 的社区活动。\n借用孙中山先生在黄埔军校的训词，愿 Kubernetes 携手云原生应用，让IT基础设施和软件开发流程进入新的纪元。\n云原生主义歌\n库巴内提，吾辈所宗；携云原生，以进大同。 咨尔多士，为民前锋；夙夜匪懈，主义是从。 创业维艰，矢勤矢勇；同心共德，贯彻始终。\n祝愿明天的大会圆满成功！\n今天不小心获得了 k8smeetup 最佳技术专栏作者和社区最佳译者奖，感谢 CNCF 和 k8smeetup 社区。\nk8smeetup 译者合影 然后跟 CNCF 执行副总裁 Dan Kohn 探讨了下云原生技术在中国的推广，明年在中国会有云原生相关大会。\nJimmy 和 CNCF 执行副总裁 Dan Kohn 在一起 欢迎大家持续关注云原生的发展。\n","relpermalink":"/blog/keuc-china-2017/","summary":"2017 年 10 月 15 日，杭州，一场顶级的 Kubernetes 行业用户落地大会。","title":"记 Kubernetes 中国用户大会 KEUC 2017"},{"content":"Service Mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A Service Mesh? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。\n下面是 Willian Morgan 对 Service Mesh 的解释。\nA Service Mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application. In practice, the Service Mesh is typically implemented as an array of lightweight network proxies that are deployed alongside application code, without the application needing to be aware.\n翻译成中文是：\n服务网格（Service Mesh）是处理服务间通信的基础设施层。它负责构成现代云原生应用程序的复杂服务拓扑来可靠地交付请求。在实践中，Service Mesh 通常以轻量级网络代理阵列的形式实现，这些代理与应用程序代码部署在一起，对应用程序来说无需感知代理的存在。\nService Mesh的特点 Service Mesh 有如下几个特点：\n应用程序间通信的中间层 轻量级网络代理 应用程序无感知 解耦应用程序的重试/超时、监控、追踪和服务发现 目前两款流行的 Service Mesh 开源软件 Istio 和 Linkerd 都可以直接在 Kubernetes 中集成，其中 Linkerd 已经成为 CNCF 中的项目。\n理解 Service Mesh 如果用一句话来解释什么是 Service Mesh，可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用 Service Mesh 也就无须关心服务之间的那些原本通过服务框架实现的事情，比如 Spring Cloud、Netflix OSS 和其他中间件，现在只要交给 Service Mesh 就可以了。\nPhil Calçado 在他的这篇博客 Pattern: Service Mesh 中详细解释了 Service Mesh 的来龙去脉：\n从最原始的主机之间直接使用网线相连 网络层的出现 集成到应用程序内部的控制流 分解到应用程序外部的控制流 应用程序的中集成服务发现和断路器 出现了专门用于服务发现和断路器的软件包/库，如 Twitter 的 Finagle 和 Facebook 的 Proxygen ，这时候还是集成在应用程序内部 出现了专门用于服务发现和断路器的开源软件，如 Netflix OSS 、Airbnb 的 synapse 和 nerve 最后作为微服务的中间层 Service Mesh 出现 Service Mesh 的架构如下图所示：\nService Mesh 架构图 图片来自：Pattern: Service Mesh Service Mesh 作为 sidecar 运行，对应用程序来说是透明，所有应用程序间的流量都会通过它，所以对应用程序流量的控制都可以在 serivce mesh 中实现。\nService Mesh如何工作？ 下面以 Istio 为例讲解 Service Mesh 如何工作，后续文章将会详解 Istio 如何在 Kubernetes 中工作。\nSidecar（Istio 中使用 Envoy 作为 sidecar 代理）将服务请求路由到目的地址，根据请求中的参数判断是到生产环境、测试环境还是 staging 环境中的服务（服务可能同时部署在这三个环境中），是路由到本地环境还是公有云环境？所有的这些路由信息可以动态配置，可以是全局配置也可以为某些服务单独配置。这些配置是由服务网格的控制平面推送给各个 sidecar 的， 当 sidecar 确认了目的地址后，将流量发送到相应服务发现端点，在 Kubernetes 中是 service，然后 service 会将服务转发给后端的实例。 Sidecar 根据它观测到最近请求的延迟时间，选择出所有应用程序的实例中响应最快的实例。 Sidecar 将请求发送给该实例，同时记录响应类型和延迟数据。 如果该实例挂了、不响应了或者进程不工作了，sidecar 会将把请求发送到其他实例上重试。 如果该实例持续返回 error，sidecar 会将该实例从负载均衡池中移除，稍后再周期性得重试。 如果请求的截止时间已过，sidecar 主动标记该请求为失败，而不是再次尝试添加负载。 SIdecar 以 metric 和分布式追踪的形式捕获上述行为的各个方面，这些追踪信息将发送到集中 metric 系统。 为何使用 Service Mesh？ Service Mesh 并没有给我们带来新功能，它是用于解决其他工具已经解决过的问题，只不过这次是在以 Kubernetes 为基础的云原生生态环境下的实现。\n在传统的 MVC 三层 Web 应用程序架构下，服务之间的通讯并不复杂，在应用程序内部自己管理即可，但是在现今的复杂的大型网站情况下，单体应用被分解为众多的微服务，服务之间的依赖和通讯十分复杂，出现了 twitter 开发的 Finagle 、Netflix 开发的 Hystrix 和 Google 的 Stubby 这样的 “胖客户端” 库，这些就是早期的 Service Mesh，但是它们都仅适用于特定的环境和特定的开发语言，并不能作为平台级的 Service Mesh 支持。\n在 Cloud Native 架构下，容器的使用赋予了异构应用程序更多的可能性，Kubernetes 增强了应用的横向扩容能力，用户可以快速的编排出复杂环境、复杂依赖关系的应用程序，同时开发者又无须过分关心应用程序的监控、扩展性、服务发现和分布式追踪这些繁琐的事情，进而专注于程序开发，赋予开发者更多的创造性。\n参考 What’s a Service Mesh? And why do I need one? So what even is a Service Mesh? Hot take on Istio and Linkerd linkerd: A Service Mesh for AWS ECS Introducing Istio: A robust Service Mesh for microservices Application Network Functions With ESBs, API Management, and Now.. Service Mesh? Pattern: Service Mesh Envoy 官方文档 Istio 官方文档 Istio Handbook - Istio 服务网格进阶实战 ","relpermalink":"/blog/what-is-a-service-mesh/","summary":"本文介绍了 Service Mesh 是什么，其工作原理并提供了一些有用的链接。","title":"什么是Service Mesh（服务网格）？"},{"content":"对于没有使用过 kubernetes 的 docker 用户，如何快速掌握 kubectl 命令？kubectl 跟 docker 命令之间有什么区别和联系？\n在本文中，我们将向 docker-cli 用户介绍 Kubernetes 命令行如何与 api 进行交互。该命令行工具——kubectl，被设计成 docker-cli 用户所熟悉的样子，但是它们之间又存在一些必要的差异。该文档将向您展示每个 docker 子命令和 kubectl 与其等效的命令。\n在使用 kubernetes 集群的时候，docker 命令通常情况是不需要用到的，只有在调试程序或者容器的时候用到，我们基本上使用 kubectl 命令即可，所以在操作 kubernetes 的时候我们抛弃原先使用 docker 时的一些观念。\ndocker run 如何运行一个 nginx Deployment 并将其暴露出来？ 查看 kubectl run 。\n使用 docker 命令：\n$ docker run -d --restart=always -e DOMAIN=cluster --name nginx-app -p 80:80 nginx a9ec34d9878748d2f33dc20cb25c714ff21da8d40558b45bfaec9955859075d0 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a9ec34d98787 nginx \u0026#34;nginx -g \u0026#39;daemon of 2 seconds ago Up 2 seconds 0.0.0.0:80-\u0026gt;80/tcp, 443/tcp nginx-app 使用 kubectl 命令：\n# start the pod running nginx $ kubectl run --image=nginx nginx-app --port=80 --env=\u0026#34;DOMAIN=cluster\u0026#34; deployment \u0026#34;nginx-app\u0026#34; created 在大于等于 1.2 版本 Kubernetes 集群中，使用kubectl run 命令将创建一个名为 “nginx-app” 的 Deployment。如果您运行的是老版本，将会创建一个 replication controller。 如果您想沿用旧的行为，使用 --generation=run/v1 参数，这样就会创建 replication controller。查看 kubectl run 获取更多详细信息。\n# expose a port through with a service $ kubectl expose deployment nginx-app --port=80 --name=nginx-http service \u0026#34;nginx-http\u0026#34; exposed 在 kubectl 命令中，我们创建了一个 Deployment ，这将保证有 N 个运行 nginx 的 pod（N 代表 spec 中声明的 replica 数，默认为 1）。我们还创建了一个 service ，使用 selector 匹配具有相应的 selector 的 Deployment。查看 快速开始 获取更多信息。\n默认情况下镜像会在后台运行，与docker run -d ... 类似，如果您想在前台运行，使用：\nkubectl run [-i] [--tty] --attach \u0026lt;name\u0026gt; --image=\u0026lt;image\u0026gt; 与 docker run ... 不同的是，如果指定了 --attach ，我们将连接到 stdin，stdout 和 stderr，而不能控制具体连接到哪个输出流（docker -a ...）。\n因为我们使用 Deployment 启动了容器，如果您终止了连接到的进程（例如 ctrl-c），容器将会重启，这跟 docker run -it不同。 如果想销毁该 Deployment（和它的 pod），您需要运行 kubeclt delete deployment \u0026lt;name\u0026gt;。\ndocker ps 如何列出哪些正在运行？查看 kubectl get 。\n使用 docker 命令：\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a9ec34d98787 nginx \u0026#34;nginx -g \u0026#39;daemon of About an hour ago Up About an hour 0.0.0.0:80-\u0026gt;80/tcp, 443/tcp nginx-app 使用 kubectl 命令：\n$ kubectl get po NAME READY STATUS RESTARTS AGE nginx-app-5jyvm 1/1 Running 0 1h docker attach 如何连接到已经运行在容器中的进程？查看 kubectl attach 。\n使用 docker 命令：\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a9ec34d98787 nginx \u0026#34;nginx -g \u0026#39;daemon of 8 minutes ago Up 8 minutes 0.0.0.0:80-\u0026gt;80/tcp, 443/tcp nginx-app $ docker attach a9ec34d98787 ... 使用 kubectl 命令：\n$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-app-5jyvm 1/1 Running 0 10m $ kubectl attach -it nginx-app-5jyvm ... docker exec 如何在容器中执行命令？查看 kubectl exec 。\n使用 docker 命令：\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a9ec34d98787 nginx \u0026#34;nginx -g \u0026#39;daemon of 8 minutes ago Up 8 minutes 0.0.0.0:80-\u0026gt;80/tcp, 443/tcp nginx-app $ docker exec a9ec34d98787 cat /etc/hostname a9ec34d98787 使用 kubectl 命令：\n$ kubectl get po NAME READY STATUS RESTARTS AGE nginx-app-5jyvm 1/1 Running 0 10m $ kubectl exec nginx-app-5jyvm -- cat /etc/hostname nginx-app-5jyvm 执行交互式命令怎么办？\n使用 docker 命令：\n$ docker exec -ti a9ec34d98787 /bin/sh # exit 使用 kubectl 命令：\n$ kubectl exec -ti nginx-app-5jyvm -- /bin/sh # exit 更多信息请查看 获取运行中容器的 Shell 环境 。\ndocker logs 如何查看运行中进程的 stdout/stderr？查看 kubectl logs 。\n使用 docker 命令：\n$ docker logs -f a9e 192.168.9.1 - - [14/Jul/2015:01:04:02 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.35.0\u0026#34; \u0026#34;-\u0026#34; 192.168.9.1 - - [14/Jul/2015:01:04:03 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.35.0\u0026#34; \u0026#34;-\u0026#34; 使用 kubectl 命令：\n$ kubectl logs -f nginx-app-zibvs 10.240.63.110 - - [14/Jul/2015:01:09:01 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.26.0\u0026#34; \u0026#34;-\u0026#34; 10.240.63.110 - - [14/Jul/2015:01:09:02 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.26.0\u0026#34; \u0026#34;-\u0026#34; 现在是时候提一下 pod 和容器之间的细微差别了；默认情况下如果 pod 中的进程退出 pod 也不会终止，相反它将会重启该进程。这类似于 docker run 时的 --restart=always 选项， 这是主要差别。在 docker 中，进程的每个调用的输出都是被连接起来的，但是对于 kubernetes，每个调用都是分开的。要查看以前在 kubernetes 中执行的输出，请执行以下操作：\n$ kubectl logs --previous nginx-app-zibvs 10.240.63.110 - - [14/Jul/2015:01:09:01 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.26.0\u0026#34; \u0026#34;-\u0026#34; 10.240.63.110 - - [14/Jul/2015:01:09:02 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.26.0\u0026#34; \u0026#34;-\u0026#34; 查看 记录和监控集群活动 获取更多信息。\ndocker stop 和 docker rm 如何停止和删除运行中的进程？查看 kubectl delete 。\n使用 docker 命令：\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a9ec34d98787 nginx \u0026#34;nginx -g \u0026#39;daemon of 22 hours ago Up 22 hours 0.0.0.0:80-\u0026gt;80/tcp, 443/tcp nginx-app $ docker stop a9ec34d98787 a9ec34d98787 $ docker rm a9ec34d98787 a9ec34d98787 使用 kubectl 命令：\n$ kubectl get deployment nginx-app NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-app 1 1 1 1 2m $ kubectl get po -l run=nginx-app NAME READY STATUS RESTARTS AGE nginx-app-2883164633-aklf7 1/1 Running 0 2m $ kubectl delete deployment nginx-app deployment \u0026#34;nginx-app\u0026#34; deleted $ kubectl get po -l run=nginx-app # Return nothing 请注意，我们不直接删除 pod。使用 kubectl 命令，我们要删除拥有该 pod 的 Deployment。如果我们直接删除pod，Deployment 将会重新创建该 pod。\ndocker login 在 kubectl 中没有对 docker login 的直接模拟。如果您有兴趣在私有镜像仓库中使用 Kubernetes，请参阅 使用私有镜像仓库 。\ndocker version 如何查看客户端和服务端的版本？查看 kubectl version 。\n使用 docker 命令：\n$ docker …","relpermalink":"/blog/docker-cli-to-kubectl/","summary":"对于没有使用过 kubernetes 的 docker 用户，如何快速掌握 kubectl 命令？","title":"Docker 用户过渡到 kubectl 命令行指南"},{"content":"Kubernetes 是一个自动发布、扩缩容和管理容器化应用的开源软件。\n尽管kubernetes非常强大，有如此多有用的技术特性，但是工具从来都不会被隔离起来单独使用，这要取决与底层基础架构，使用它的团队等等。\n在将kubernetes应用到生产环境（如果想成功的引入生产的话）之前，每个CTO都应该了解这三件事。\n1. 你需要有坚实的基础设施 大多数组织在运行kubernetes时遇到的第一个问题是在其下运行的平台。无论是基于VMware的私有云还是像AWS这样的公共云，您的平台需要稳定运行一段时间，并且具备以下基础设施基础知识：\n配置：根据需要创建虚拟机，或者直接使用裸机 网络：DNS、负载均衡、VPC/VLAN、防火墙、安全组等 存储：NFS/EFS/EBS/Ceph等，通过API创建 如果这些基础设施建设不到位，那么在尝试部署和运行kubernetes群集时，您将会遇到许多问题。\n根据经验，我们通常向客户推荐从像AWS这样的公共云提供商开始，然后配备了一些Hashicorp工具，如Terraform和Packer，以实现坚实的基础设施。\n2. 您需要打造一支强大的团队 让企业内部实现容器编排的能力很一个很有挑战的事情。\n打造是一个全面的团队，包括一些具有非常强大的Ops背景的成员，可以让他们去调试一些底层的东西，还有一些自动化工程师将负责设置和集群管理的日常工作，更多的研究人员将确保CI/CD流水线顺利运行以保证开发人员有一个很好的体验。\n下面是构建团队的几个建议：\n找到已经在尝试使用容器的团队，也许他们以前用过Docker Swarm或Rancher。他们可能已经渴望使用Kubernetes，并且愿意努力实施。 给您的开发和运维团队做关于容器和容器编排方面的培训。 聘请新人才。有时候，您可能会发现，最好的选择是建立一个全新的团队，这样他们就不会被当前流程所淹没，还可以向其他团队展示未来的样子。 3. 依托社区 Kubernetes能够坐上了容器编排系统的头把交椅的主要原因是社区的支持。\nKubernetes最初是基于Google的borg，borg具有非常丰富的功能集，现在已经是一个成熟的框架——这对kubernetes来说是非常有利的 ，但其成功的主要原因是已经形成了积极支持它的社区。\n下面是一些关于如何参与社区的小贴士：\n加入kubernetes slack channel，现在里面已经有21,000人，http://slack.k8s.io 参与一个SIG（特别兴趣小组），这里面包括从在AWS上运行kubernetes到管理大数据集群。 参加meetup 关注#kubernetes 关注那些主流传道者，有个人要特别关注下那就是Kelsey Hightower 走向成功 在拥有了坚如磐石的平台，熟练和多样化的团队，以及与Kubernetes社区不断增长的关系后，您将有处理通向成功道路上的遇到的任何问题的资本，克服成长过程中的痛苦。\n原文地址 作者: Marcus Maxwell\n","relpermalink":"/blog/3-things-every-cto-should-know-about-kubernetes/","summary":"使用 Kubernetes 前你需要先了解的问题。","title":"每位 CTO 都该知道的关于 Kubernetes 的三件事"},{"content":"即日起我有了自己的独立域名 jimmysong.io ，网站依然托管在 GitHub 上，原来的网址 https://jimmysong.io 依然可以访问。\n为什么使用 .io 作为后缀呢？因为这是 The First Step to Cloud Native！\n为什么选择在今天？因为今天是8月18日，日子好记。\nP.S 域名注册于 namecheap ，费用几十美元/年。\nProudly powered by hugo 🎉🎊🎉\n","relpermalink":"/notice/domain-name-jimmysong-io/","summary":"即日起我有了自己的独立域名jimmysong.io。","title":"即日起更换域名为jimmysong.io"},{"content":"本文已归档在kubernetes-handbook 中的第3章【用户指南】中，一切更新以kubernetes-handbook中为准。\n为了详细说明，我特意写了两个示例程序放在GitHub中，模拟应用开发流程：\nk8s-app-monitor-test ：生成模拟的监控数据，发送http请求，获取json返回值 K8s-app-monitor-agent ：获取监控数据并绘图，访问浏览器获取图表 API文档见k8s-app-monitor-test 中的api.html文件，该文档在API blueprint中定义，使用aglio 生成，打开后如图所示：\nAPI文档 关于服务发现\nK8s-app-monitor-agent服务需要访问k8s-app-monitor-test服务，这就涉及到服务发现的问题，我们在代码中直接写死了要访问的服务的内网DNS地址（kubedns中的地址，即k8s-app-monitor-test.default.svc.cluster.local）。\n我们知道Kubernetes在启动Pod的时候为容器注入环境变量，这些环境变量在所有的 namespace 中共享（环境变量是不断追加的，新启动的Pod中将拥有老的Pod中所有的环境变量，而老的Pod中的环境变量不变）。但是既然使用这些环境变量就已经可以访问到对应的service，那么获取应用的地址信息，究竟是使用变量呢？还是直接使用DNS解析来发现？\n答案是使用DNS，详细说明见Kubernetes中的服务发现与Docker容器间的环境变量传递源码探究 。\n打包镜像\n因为我使用wercker自动构建，构建完成后自动打包成docker镜像并上传到docker hub中（需要提前在docker hub中创建repo），如何使用 wercker 做持续构建与发布，并集成docker hub插件请参考使用Wercker进行持续构建与发布 。\n查看详细构建流程 wercker 生成了如下两个docker镜像：\njimmysong/k8s-app-monitor-test:latest jimmysong/k8s-app-monitor-agent:latest 启动服务\n所有的kubernetes应用启动所用的yaml配置文件都保存在那两个GitHub仓库的manifest.yaml文件中。\n分别在两个GitHub目录下执行kubectl create -f manifest.yaml即可启动服务。\n外部访问\n服务启动后需要更新ingress配置，在ingress.yaml 文件中增加以下几行：\n- host: k8s-app-monitor-agent.jimmysong.io http: paths: - path: / backend: serviceName: k8s-app-monitor-agent servicePort: 8080 保存后，然后执行kubectl replace -f ingress.yaml即可刷新ingress。\n修改本机的/etc/hosts文件，在其中加入以下一行：\n172.20.0.119 k8s-app-monitor-agent.jimmysong.io 当然你也可以加入到DNS中，为了简单起见我使用hosts。\n详见边缘节点配置 。\n在浏览器中访问http://k8s-app-monitor-agent.jimmysong.io\n图表 刷新页面将获得新的图表。\n参考 使用Wercker进行持续构建与发布 示例的项目代码服务器端 示例项目代码前端 kubernetes-handbok 边缘节点配置 ","relpermalink":"/blog/deploy-applications-in-kubernetes/","summary":"本文以在 Kubernetes 中部署两个应用来说明。","title":"适用于 Kubernetes 的应用开发与部署流程详解"},{"content":"下面是这本书的基本信息。\n书名： Kubernetes Management Design Patterns: With Docker, CoreOS Linux, and Other Platforms Amazon购买链接：链接 作者：Deepak Vohra 发行日期：2017年1月20日 出版社：Apress 页数：399 简介 Kubernetes引领容器集群管理进入一个全新的阶段；学习如何在CoreOS上配置和管理kubernetes集群；使用适当的管理模式，如ConfigMaps、Autoscaling、弹性资源使用和高可用性配置。讨论了kubernetes的一些其他特性，如日志、调度、滚动升级、volume、服务类型和跨多个云供应商zone。\nKubernetes中的最小模块化单位是Pod，它是拥有共同的文件系统和网络的系列容器的集合。Pod的抽象层可以对容器使用设计模式，就像面向对象设计模式一样。容器能够提供与软件对象（如模块化或包装，抽象和重用）相同的优势。\n在大多数章节中使用的都是CoreOS Linux，其他讨论的平台有CentOS，OpenShift，Debian 8（jessie），AWS和Debian 7 for Google Container Engine。\n使用CoreOS主要是因为Docker已经在CoreOS上开箱即用。CoreOS：\n支持大多数云提供商（包括Amazon AWS EC2和Google Cloud Platform）和虚拟化平台（如VMWare和VirtualBox） 提供Cloud-Config，用于声明式配置OS，如网络配置（flannel），存储（etcd）和用户帐户 为容器化应用提供生产级基础架构，包括自动化，安全性和可扩展性 引领容器行业标准，并建立了应用程序标准 提供最先进的容器仓库，Quay Docker于2013年3月开源，现已称为最流行的容器平台。kubernetes于2014年6月开源，现在已经成为最流行的容器集群管理平台。第一个稳定版CoreOS Linux于2014年7月发行，现已成为最流行的容器操作系统之一。\n你将学到什么 使用docker和kubernetes 在AWS和CoreOS上创建kubernetes集群 应用集群管理设计模式 使用多个云供应商zone 使用Ansible管理kubernetes 基于kubernetes的PAAS平台OpenShift 创建高可用网站 构建高可用用的kubernetes master集群 使用volume、configmap、serivce、autoscaling和rolling update 管理计算资源 配置日志和调度 谁适合读这本书 Linux管理员、CoreOS管理员、应用程序开发者、容器即服务（CAAS）开发者。阅读这本书需要Linux和Docker的前置知识。介绍Kubernetes的知识，例如创建集群，创建Pod，创建service以及创建和缩放replication controller。还需要一些关于使用Amazon Web Services（AWS）EC2，CloudFormation和VPC的必备知识。\n关于作者 Deepak Vohra is an Oracle Certified Associate and a Sun Certified Java Programmer. Deepak has published in Oracle Magazine, OTN, IBM developerWorks, ONJava, DevSource, WebLogic Developer’s Journal, XML Journal, Java Developer’s Journal, FTPOnline, and devx.\n目录 第一部分：平台 第1章：Kuberentes on AWS 第2章：kubernetes on CoreOS on AWS 第3章：kubernetes on Google Cloud Platform 第二部分：管理和配置 第4章：使用多个可用区 第5章：使用Tectonic Console 第6章：使用volume 第7章：使用service 第8章：使用Rolling updte 第9章：在node上调度pod 第10章：配置计算资源 第11章：使用ConfigMap 第12章：使用资源配额 第13章：使用Autoscaling 第14章：配置logging 第三部分：高可用 第15章：在OpenShift中使用HA master 第16章：开发高可用网站 个人评价 本书更像是一本参考手册，对于想在公有云中（如AWS、Google Cloud Platform）中尝试Kubernetes的人会有所帮助，而对于想使用kubernetes进行自己的私有云建设，或想了解kubernetes的实现原理和技术细节的人来说，就不适合了。对我来说，本书中有个别几个章节可以参考，如高可用，但还是使用OpenShift实现的。总之，如果你使用AWS这样的公有云，对操作系统没有特别要求，可以接受CoreOS的话，那么可以看看这本书。本来本书会对kubernetes中的各种应用模式能够有个详解，但是从书中我并没有找到。\n本书有两个优点，一个是每个章节都给出了问题的起因和kubernetes的解决方案，二是几乎所有的命令和操作都附有截图，说明很详细。\n","relpermalink":"/blog/book-kubernetes-management-design-patterns/","summary":"本书有两个优点，一个是每个章节都给出了问题的起因和kubernetes的解决方案，二是几乎所有的命令和操作都附有截图，说明很详细。","title":"记一本关于kubernetes management design patterns的书"},{"content":"前言 今天创建了两个kubernetes示例应用：\nk8s-app-monitor-test ：启动server用来产生metrics k8s-app-monitor-agent ：获取metrics并绘图，显示在web上 注：相关的kubernetes应用manifest.yaml文件分别见以上两个应用的GitHub 。\n当我查看Pod中的环境变量信息时，例如kubernetes中的service k8s-app-monitor-test注入的环境变量时，包括了以下变量：\nK8S_APP_MONITOR_TEST_PORT_3000_TCP_ADDR=10.254.56.68 K8S_APP_MONITOR_TEST_PORT=tcp://10.254.56.68:3000 K8S_APP_MONITOR_TEST_PORT_3000_TCP_PROTO=tcp K8S_APP_MONITOR_TEST_SERVICE_PORT_HTTP=3000 K8S_APP_MONITOR_TEST_PORT_3000_TCP_PORT=3000 K8S_APP_MONITOR_TEST_PORT_3000_TCP=tcp://10.254.56.68:3000 K8S_APP_MONITOR_TEST_SERVICE_HOST=10.254.56.68 K8S_APP_MONITOR_TEST_SERVICE_PORT=3000 我们知道Kubernetes在启动Pod的时候为容器注入环境变量，这些环境变量将在该Pod所在的namespace中共享。但是既然使用这些环境变量就已经可以访问到对应的service，那么获取应用的地址信息，究竟是使用变量呢？还是直接使用DNS解析来发现？下面我们从代码中来寻求答案。\n如果不想看下面的文字，可以直接看图。\nkubernetes中传递ENV的探索过程 探索 docker的docker/engine-api/types/container/config.go中的Config结构体中有对环境变量的定义：\n// Config contains the configuration data about a container. // It should hold only portable information about the container. // Here, \u0026#34;portable\u0026#34; means \u0026#34;independent from the host we are running on\u0026#34;. // Non-portable information *should* appear in HostConfig. // All fields added to this struct must be marked `omitempty` to keep getting // predictable hashes from the old `v1Compatibility` configuration. type Config struct { Hostname string // Hostname Domainname string // Domainname User string // User that will run the command(s) inside the container ... Env []string // List of environment variable to set in the container Cmd strslice.StrSlice // Command to run when starting the container ... } Kubernetes中在pkg/kubelet/container/runtime.go中的RunContainerOptions结构体中定义：\n// RunContainerOptions specify the options which are necessary for running containers type RunContainerOptions struct { // The environment variables list. Envs []EnvVar // The mounts for the containers. Mounts []Mount // The host devices mapped into the containers. ... } Kubelet向容器中注入环境变量的配置是在下面的方法中定义：\npkg/kubelet/kuberuntime/kuberuntime_container.go // generateContainerConfig generates container config for kubelet runtime v1. func (m *kubeGenericRuntimeManager) generateContainerConfig(container *v1.Container, pod *v1.Pod, restartCount int, podIP, imageRef string) (*runtimeapi.ContainerConfig, error) { opts, _, err := m.runtimeHelper.GenerateRunContainerOptions(pod, container, podIP) ... // set environment variables envs := make([]*runtimeapi.KeyValue, len(opts.Envs)) for idx := range opts.Envs { e := opts.Envs[idx] envs[idx] = \u0026amp;runtimeapi.KeyValue{ Key: e.Name, Value: e.Value, } } config.Envs = envs return config, nil } kubelet的pkg/kubelet/kubelet_pods.go的如下方法中生成了RunContainerOptions：\n// GenerateRunContainerOptions generates the RunContainerOptions, which can be used by // the container runtime to set parameters for launching a container. func (kl *Kubelet) GenerateRunContainerOptions(pod *v1.Pod, container *v1.Container, podIP string) (*kubecontainer.RunContainerOptions, bool, error) { ... opts := \u0026amp;kubecontainer.RunContainerOptions{CgroupParent: cgroupParent} ... opts.Envs, err = kl.makeEnvironmentVariables(pod, container, podIP) return opts, useClusterFirstPolicy, nil } 我们再看下makeEnvironmentVariables(pod, container, podIP)方法中又做了什么（该方法也在pkg/kubelet/kubelet_pods.go文件中）。\n// Make the environment variables for a pod in the given namespace. func (kl *Kubelet) makeEnvironmentVariables(pod *v1.Pod, container *v1.Container, podIP string) ([]kubecontainer.EnvVar, error) { var result []kubecontainer.EnvVar // Note: These are added to the docker Config, but are not included in the checksum computed // by dockertools.BuildDockerName(...). That way, we can still determine whether an // v1.Container is already running by its hash. (We don\u0026#39;t want to restart a container just // because some service changed.) // // Note that there is a race between Kubelet seeing the pod and kubelet seeing the service. // To avoid this users can: (1) wait between starting a service and starting; or (2) detect // missing service env var and exit and be restarted; or (3) use DNS instead of env vars // and keep trying to resolve the DNS name of the service (recommended). ... } 该代码段比较长，kubernetes究竟如何将环境变量注入到docker容器中的奥秘就在这里，按图索骥到了这里，从代码注释中已经可以得出结论，使用DNS解析而不要使用环境变量来做服务发现，究竟为何这样做，改天我们再详细解读。\n","relpermalink":"/blog/exploring-kubernetes-env-with-docker/","summary":"基于实际应用研究。","title":"Kubernetes中的服务发现与docker容器间的环境变量传递源码探究"},{"content":"这是一份记录关于Cloud Native的软件、工具、架构以及参考资料的列表，是我在GitHub上开的一个项目 awesome-cloud-native ，同时也可以通过 Web 页面浏览 。\n初步划分成以下这些领域：\nAwesome Cloud Native\nAI API gateway Big Data Container engine CI-CD Database Data Science Fault tolerant Logging Message broker Monitoring Networking Orchestration and scheduler Portability Proxy and load balancer RPC Security and audit Service broker Service mesh Service registry and discovery Serverless Storage Tracing Tools Tutorial 今后会不断更新和完善该列表，不仅是为了本人平时的研究记录，也作为Cloud Native 业内人士的参考。\n","relpermalink":"/notice/awesome-cloud-native/","summary":"这是一份记录关于Cloud Native的软件、工具、架构以及参考资料的列表，是我在GitHub上开的一个项目。","title":"Awesome Cloud Native列表发布"},{"content":"本书是 Migrating to Cloud Native Application Architectures 的中文版，本书英文版发布于 2015 年 2 月，中文版由 Jimmy Song 翻译，发布于 2017 年 7 月。\n本书GitHub托管地址：https://github.com/rootsongjc/migrating-to-cloud-native-application-architectures Gitbook 阅读地址：https://jimmysong.io/migrating-to-cloud-native-application-architectures 本书中讨论的应用架构包括：\n十二因素应用程序：云原生应用架构模式的集合 微服务：独立部署的服务，每个服务只做一件事情 自助服务的敏捷基础设施：快速，可重复和一致地提供应用环境和后台服务的平台 基于API的协作：发布和版本化的API，允许在云原生应用架构中的服务之间进行交互 抗压性：根据压力变强的系统 ","relpermalink":"/notice/changes-needed-to-cloud-native-archtecture/","summary":"本书译自 Matt Stine 2015 年 2 月发表的电子书。","title":"《迁移到云原生应用架构》中文译本发布"},{"content":"数据落盘问题的由来 这本质上是数据持久化问题，对于有些应用依赖持久化数据，比如应用自身产生的日志需要持久化存储的情况，需要保证容器里的数据不丢失，在Pod挂掉后，其他应用依然可以访问到这些数据，因此我们需要将数据持久化存储起来。\n数据落盘问题解决方案 下面以一个应用的日志收集为例，该日志需要持久化收集到ElasticSearch集群中，如果不考虑数据丢失的情形，可以直接使用kubernetes-handbook 中【应用日志收集】一节中的方法，但考虑到Pod挂掉时logstash（或filebeat）并没有收集完该pod内日志的情形，我们想到了如下这种解决方案，示意图如下：\n日志持久化收集解决方案示意图 首先需要给数据落盘的应用划分node，即这些应用只调用到若干台主机上 给这若干台主机增加label 使用deamonset方式在这若干台主机上启动logstash的Pod（使用nodeSelector来限定在这几台主机上，我们在边缘节点启动的treafik也是这种模式） 将应用的数据通过volume挂载到宿主机上 Logstash（或者filebeat）收集宿主机上的数据，数据持久化不会丢失 Side-effect 首先kubernetes本身就提供了数据持久化的解决方案statefulset，不过需要用到公有云的存储货其他分布式存储，这一点在我们的私有云环境里被否定了。 需要管理主机的label，增加运维复杂度，但是具体问题具体对待 必须保证应用启动顺序，需要先启动logstash 为主机打label使用nodeSelector的方式限制了资源调度的范围 本文已归档到kubernetes-handbook 中的【最佳实践—运维管理】章节中，一切内容以kubernetes-handbook为准。\n","relpermalink":"/blog/data-persistence-problem/","summary":"以日志收集问题为例来讨论和解决方案探究。","title":"Kubernetes中的数据持久化问题"},{"content":"在进行微服务开发的过程中，为了保证最终开发的系统跟最初的设计保持一致，约定RESTful接口之间的调用方法，我们需要将API设计文档化，因此我们引入了API Blueprint。\nAPI Blueprint 是什么 API Blueprint 用来编写API文档的一种标记语言，类似于Markdown，因为是纯文本的，所以方便共享编辑，具体的语法规则可以在 API Blueprint documentation 查看，配合一些开源的工具可以把接口文档渲染成 html 再搭配一个静态服务器，方便于分享。\n另外，配合一些工具，可以直接生成一个 mock data 数据，这样只要和后端的同学约定好接口格式，那么前端再开发的时候可以使用 mock data 数据来做测试，等到后端写好接口之后再做联调就可以了。\n我们以Cloud Native Go 书中的gogo-service 示例里的apiary.apib文件为例。\n该文件实际上是一个Markdown格式的文件，Github中原生支持该文件，使用Typora 打开后是这样子。\napiary.apib文件 在Visual Studio Code中有个API Element extension对于API Blueprint文件的支持也比较好。\n生成静态页面和进行冒烟测试 我们分别使用开源的aglio 和drakov 来生成静态页面和进行冒烟测试。\naglio 是一个可以根据 api-blueprint 的文档生成静态 HTML 页面的工具。\n其生成的工具不是简单的 markdown 到 html 的转换, 而是可以生成类似 rdoc 这样的拥有特定格式风格的查询文档。\n在本地安装有node环境的情况下，使用下面的命令安装和使用aglio。\n$ npm install -g aglio $ aglio -i apiary.apib -o api.html 打开api.html文件后，如图：\n使用aglio生成的API文档 安装和使用drakov。\n$ npm install -g drakov $ drakov -f apiary.apib -p 3000 [INFO] No configuration files found [INFO] Loading configuration from CLI DRAKOV STARTED [LOG] Setup Route: GET /matches List All Matches [LOG] Setup Route: POST /matches Start a New Match [LOG] Setup Route: GET /matches/:match_id Get Match Details [LOG] Setup Route: GET /matches/:match_id Get Current Liberties for Match [LOG] Setup Route: GET /matches/:match_id Get Current Chains for Match [LOG] Setup Route: GET /matches/:match_id/moves Get a Sequential List of All Moves Performed in a Match [LOG] Setup Route: POST /matches/:match_id/moves Make a Move Drakov 1.0.4 Listening on port 3000 通过http://localhost:3000就可以对该应用进行冒烟测试了。\n例如查询有哪些match：\n$ curl http://localhost:3000/matches [ { \u0026#34;id\u0026#34; : \u0026#34;5a003b78-409e-4452-b456-a6f0dcee05bd\u0026#34;, \u0026#34;started_at\u0026#34;: 13231239123391, \u0026#34;turn\u0026#34; : 27, \u0026#34;gridsize\u0026#34; : 19, \u0026#34;playerWhite\u0026#34; : \u0026#34;bob\u0026#34;, \u0026#34;playerBlack\u0026#34; : \u0026#34;alfred\u0026#34; } ] 另外通过Apiary 这个网站，我们可以直接以上的所有功能，还可以同时在页面上进行mock test，生成多种语言的code example。如图：\nApiary页面 ","relpermalink":"/blog/creating-api-document-with-api-blueprint/","summary":"在进行微服务开发的过程中的 API 设计文档化工具。","title":"使用 API blueprint 创建 API 文档"},{"content":"本文介绍了wercker和它的基本用法，并用我GitHub上的magpie 应用作为示例，讲解如何给GitHub项目增加wercker构建流程，并将生成的镜像自动上传到Docker Hub上。\n注：本文参考了Cloud Native Go 书中的”持续交付“章节。\nCI工具 开源项目的构建离不开CI工具，你可能经常会在很多GitHub的开源项目首页上看到这样的东西：\nwercker status badge 这些图标都是CI工具提供的，可以直观的看到当前的构建状态，例如wercker中可以在Application-magpie-options中看到：\nwercker status badge设置 将文本框中的代码复制到你的项目的README文件中，就可以在项目主页上看到这样的标志了。\n现在市面上有很多流行的CI/CD工具和DevOps工具有很多，这些工具提高了软件开发的效率，增加了开发人员的幸福感。这些工具有：\n适用于GitHub上的开源项目，可以直接使用GitHub账户登陆，对于公开项目可以直接使用：Travis-ci 、CircleCI 、Wercker 。从目前GitHub上开源项目的使用情况来看，Travis-ci的使用率更高一些。\n适用于企业级的：Jenkins 不仅包括CI/CD功能的DevOps平台：JFrog 、Spinnaker 、Fabric8 Wercker简介 Wercker是一家为现代云服务提供容器化应用及微服务的快速开发、部署工具的初创企业，成立于2012年，总部位于荷兰阿姆斯特丹。其以容器为中心的平台可以对微服务和应用的开发进行自动化。开发者通过利用其命令行工具能够生成容器到桌面，然后自动生成应用并部署到各种云平台上面。其支持的平台包括Heroku、AWS以及Rackspace等。\nWercker于2016年获得450万美元A轮融资，此轮融资由Inkef Capital领投，Notion Capital跟投，融资所得将用于商业版产品的开发。此轮融资过后其总融资额为750万美元。\nWercker于2017年4月被Oracle甲骨文于收购。\n为什么使用Wercker 所有的CI工具都可以在市面上获取，但为何要建议使用Wercker呢？依据云之道的准则评估了所有工具，发现Wercker正是我们需要的。\n首先，无须在工作站中安装Wecker，仅安装一个命令行客户端即可，构建过程全部在云端进行。\n其次，不用通过信用卡就可使用Wercker。当我们迫切希望简化流程时，这是一件令人赞叹的事。付款承诺这一条件大大增加了开发者的压力，这通常是不必要的。\n最后，Wercker使用起来非常简单。它非常容易配置，不需要经过高级培训或拥有持续集成的博士学位，也不用制定专门的流程。\n通过Wercker搭建CI环境只需经过三个基本步骤。\n在Wercker网站中创建一个应用程序。 将wercker.yml添加到应用程序的代码库中。 选择打包和部署构建的位置。 如何使用 可以使用GitHub帐号直接登录Wercker ，整个创建应用CI的流程一共3步。\n一旦拥有了账户，那么只需简单地点击位于顶部的应用程序菜单，然后选择创建选项即可。如果系统提示是否要创建组织或应用程序，请选择应用程序。Wercker组织允许多个Wercker用户之间进行协作，而无须提供信用卡。下图为设置新应用程序的向导页面。\n向导页面 选择了GitHub中的repo之后，第二步配置访问权限，最后一步Wercker会尝试生成一个wercker.yml文件（后面会讨论）。不过至少对于Go应用程序来说，这个配置很少会满足要求，所以我们总是需要创建自己的Wercker配置文件。\n安装Wercker命令行程序 这一步是可选的，如果你希望在本地进行wercker构建的话才需要在本地安装命令行程序。本地构建和云端构建都依赖于Docker的使用。基本上，代码会被置于所选择的docker镜像中（在wercker.yml中定义），然后再选择执行的内容和方法。\n要在本地运行Wercker构建，需要使用Wercker CLI。有关如何安装和测试CLI的内容，请查看 http://devcenter.wercker.com/docs/cli 。Wercker更新文档的频率要比本书更高，所以请在本书中做个标记，然后根据Wercker网站的文档安装Wercker CLI。\n如果已经正确安装了CLI，应该可以查询到CLI的版本，代码如下所示。\nVersion: 1.0.882 Compiled at: 2017-06-02 06:49:39 +0800 CST Git commit: da8bc056ed99e27b4b7a1b608078ddaf025a9dc4 No new version available 本地构建只要在项目的根目录下输入wercker build命令即可，wercker会自动下载依赖的docker镜像在本地运行所有构建流程。\n创建Wercker配置文件wercker.yml Wercker配置文件是一个YAML文件，该文件必须在GitHub repo的最顶层目录，该文件主要包含三个部分，对应可用的三个主要管道。\nDev：定义了开发管道的步骤列表。与所有管道一样，可以选定一个box用于构建，也可以全局指定一个box应用于所有管道。box可以是Wercker内置的预制Docker镜像之一，也可以是Docker Hub托管的任何Docker镜像。 Build：定义了在Wercker构建期间要执行的步骤和脚本的列表。与许多其他服务（如Jenkins和TeamCity）不同，构建步骤位于代码库的配置文件中，而不是隐藏在服务配置里。 Deploy：在这里可以定义构建的部署方式和位置。 Wercker中还有工作流的概念，通过使用分支、条件构建、多个部署目标和其他高级功能扩展了管道的功能，这些高级功能读着可以自己在wercker的网站中探索。\n示例 我们以我用Go语言开发的管理yarn on docker集群的命令行工具magpie 为例，讲解如何使用wercker自动构建，并产生docker镜像发布到Docker Hub中。\n下面是magpie这个项目中使用的wercker.yml文件。\nbox: golang build: steps: # Sets the go workspace and places you package # at the right place in the workspace tree - setup-go-workspace # Gets the dependencies - script: name: go get code: | go get github.com/rootsongjc/magpie # Build the project - script: name: go build code: | go build -o magpie main.go # Test the project - script: name: go test code: | go test ./... - script: name: copy files to wercker output code: | cp -R ./ ${WERCKER_OUTPUT_DIR} deploy: steps: - internal/docker-push: username: $USERNAME password: $PASSWORD cmd: /pipeline/source/magpie tag: latest repository: jimmysong/magpie 此文件包含两个管道：build和deploy。在开发流程中，我们使用Wercker和Docker创建一个干净的Docker镜像，然后将它push到Docker Hub中。Wercker包含一个叫做Internal/docker-push的deploy plugin，可以将构建好的docker镜像push到镜像仓库中，默认是Docker Hub，也可以配置成私有镜像仓库。\nbox键的值是golang。这意味着我们使用的是一个基础的Docker镜像，它已经安装了Go环境。这一点至关重要，因为执行Wercker构建的基准Docker镜像需要包含应用程序所需的构建工具。\n这部分存在一些难以理解的概念。当使用Wercker进行构建时，其实并没有使用本地工作站的资源（即使在技术层面上，构建也是在本地执行的），相反，使用的是Docker镜像中的可用资源。因此，如果要使用Wercker编译Go应用程序，需要首先运行包含Go的Docker镜像。如果想要构建唯一的工件，无论它是在本地还是在Wercker的云端运行，使用Docker镜像都是完全合理的。\n本次构建中运行的第一个脚本是go get。这一步可以go get可能需要的、但不包含在基础镜像中的任何东西。无论为脚本设置什么名称，构建输出都会有所显示，如下图所示。\n构建流程输出 在build管道中，接下来的两个脚本执行的构建和测试流程，最后一个脚本是将构建后的文件拷贝到wercker的输出目录中，我们将使用该目录构建docker镜像。\n我们注意到deploy中有两个变量：$USERNAME、$PASSWORD，这是我们自定义的变量，当你不希望将隐私内容直接写在代码中的时候，可以在wercker中自定义变量，变量可以只作用于单个pipeline，也可以是所有pipeline共享的。\n在wercker中设置环境变量 可以将变量设置成Protected模式，这样只有设置者本人才知道该变量的值是什么，其他人即使有共享访问权限，也看不到该变量的值，但可以重新设置来覆盖原值。\nDeploy管道中配置的docker镜像的repo、tag和cmd命令，其他容器配置都在代码顶层目录的Dockerfile中定义。当整个构建流程完成后，就可以在docker镜像仓库中看到刚构建的镜像jimmysong/magpie:latest了。\n使用wercker自动构建的docker镜像magpie 总结 当然以上只是一个很简单的示例，还有很多可以优化的流程，比如我们在示例使用latest作为docker镜像的tag，wercker本身提供了很多内置和构建时环境变量 ，我们可以在wercker.yml文件里获取这些变量作为命令中的值。\n当比于其他CI工具，wercker配置简单，更易于使用，同时在wercker的registry中还可以看到很多别人构建的pipline可供参考，还有十分友好的workflows 可用于编排构建流程和依赖。\n当然CI工具的功能不止这些，利用它可以实现很多自动化流程，节约我们的时间，解放生产力，更多玩法就要大家自己去探索了。\n参考 容器化应用开发部署平台Wercker获450万美元A轮融资 甲骨文收购创业公司Wercker 为开发人员自动化代码测试部署 Wercker docs Wercker workflow magpie ","relpermalink":"/blog/continuous-integration-with-wercker/","summary":"本文介绍了wercker和它的基本用法。","title":"使用Wercker进行持续构建与发布"},{"content":"前言 本文档旨在汇总和强调用户指南、快速开始文档和示例中的最佳实践。该文档会很很活跃并持续更新中。如果你觉得很有用的最佳实践但是本文档中没有包含，欢迎给我们提Pull Request。\n本文已上传到kubernetes-handbook 中的第四章最佳实践章节，本文仅作归档，更新以kubernetes-handbook 为准。\n通用配置建议 定义配置文件的时候，指定最新的稳定API版本（目前是V1）。 在配置文件push到集群之前应该保存在版本控制系统中。这样当需要的时候能够快速回滚，必要的时候也可以快速的创建集群。 使用YAML格式而不是JSON格式的配置文件。在大多数场景下它们都可以作为数据交换格式，但是YAML格式比起JSON更易读和配置。 尽量将相关的对象放在同一个配置文件里。这样比分成多个文件更容易管理。参考guestbook-all-in-one.yaml 文件中的配置（注意，尽管你可以在使用kubectl命令时指定配置文件目录，你也可以在配置文件目录下执行kubectl create——查看下面的详细信息）。 为了简化和最小化配置，也为了防止错误发生，不要指定不必要的默认配置。例如，省略掉ReplicationController的selector和label，如果你希望它们跟podTemplate中的label一样的话，因为那些配置默认是podTemplate的label产生的。更多信息请查看 guestbook app 的yaml文件和 examples 。 将资源对象的描述放在一个annotation中可以更好的内省。 裸奔的Pods vs Replication Controllers和 Jobs 如果有其他方式替代“裸奔的pod”（如没有绑定到replication controller 上的pod），那么就使用其他选择。在node节点出现故障时，裸奔的pod不会被重新调度。Replication Controller总是会重新创建pod，除了明确指定了restartPolicy: Never 的场景。Job 也许是比较合适的选择。 Services 通常最好在创建相关的replication controllers 之前先创建service （没有这个必要吧？）你也可以在创建Replication Controller的时候不指定replica数量（默认是1），创建service后，在通过Replication Controller来扩容。这样可以在扩容很多个replica之前先确认pod是正常的。 除非时分必要的情况下（如运行一个node daemon），不要使用hostPort（用来指定暴露在主机上的端口号）。当你给Pod绑定了一个hostPort，该pod可被调度到的主机的受限了，因为端口冲突。如果是为了调试目的来通过端口访问的话，你可以使用 kubectl proxy and apiserver proxy 或者 kubectl port-forward 。你可使用 Service 来对外暴露服务。如果你确实需要将pod的端口暴露到主机上，考虑使用 NodePort service。 跟hostPort一样的原因，避免使用 hostNetwork。 如果你不需要kube-proxy的负载均衡的话，可以考虑使用使用headless services 。 使用Label 定义 labels 来指定应用或Deployment的 semantic attributes 。例如，不是将label附加到一组pod来显式表示某些服务（例如，service:myservice），或者显式地表示管理pod的replication controller（例如，controller:mycontroller），附加label应该是标示语义属性的标签， 例如 {app:myapp,tier:frontend,phase:test,deployment:v3} 这将允许您选择适合上下文的对象组——例如，所有的”tier:frontend“pod的服务或app是“myapp”的所有“测试”阶段组件。 有关此方法的示例，请参阅guestbook 应用程序。可以通过简单地从其service的选择器中省略特定于发行版本的标签，而不是更新服务的选择器来完全匹配replication controller的选择器，来实现跨越多个部署的服务，例如滚动更新。\n为了滚动升级的方便，在Replication Controller的名字中包含版本信息，例如作为名字的后缀。设置一个version标签页是很有用的。滚动更新创建一个新的controller而不是修改现有的controller。因此，version含混不清的controller名字就可能带来问题。查看Rolling Update Replication Controller 文档获取更多关于滚动升级命令的信息。\n注意 Deployment 对象不需要再管理 replication controller 的版本名。Deployment 中描述了对象的期望状态，如果对spec的更改被应用了话，Deployment controller 会以控制的速率来更改实际状态到期望状态。（Deployment目前是 extensions API Group 的一部分）。\n利用label做调试。因为Kubernetes replication controller和service使用label来匹配pods，这允许你通过移除pod中的label的方式将其从一个controller或者service中移除，原来的controller会创建一个新的pod来取代移除的pod。这是一个很有用的方式，帮你在一个隔离的环境中调试之前的“活着的” pod。查看 kubectl label 命令。\n容器镜像 默认容器镜像拉取策略 是 IfNotPresent, 当本地已存在该镜像的时候 Kubelet 不会再从镜像仓库拉取。如果你希望总是从镜像仓库中拉取镜像的话，在yaml文件中指定镜像拉取策略为Always（ imagePullPolicy: Always）或者指定镜像的tag为 :latest 。\n如果你没有将镜像标签指定为:latest，例如指定为myimage:v1，当该标签的镜像进行了更新，kubelet也不会拉取该镜像。你可以在每次镜像更新后都生成一个新的tag（例如myimage:v2），在配置文件中明确指定该版本。\n注意： 在生产环境下部署容器应该尽量避免使用:latest标签，因为这样很难追溯到底运行的是哪个版本的容器和回滚。\n使用kubectl 尽量使用 kubectl create -f \u0026lt;directory\u0026gt; 。kubeclt会自动查找该目录下的所有后缀名为.yaml、.yml和.json文件并将它们传递给create命令。 使用 kubectl delete 而不是 stop. Delete 是 stop的超集，stop 已经被弃用。 使用 kubectl bulk 操作（通过文件或者label）来get和delete。查看label selectors 和 using labels effectively 。 使用 kubectl run 和 expose 命令快速创建直有耽搁容器的Deployment。查看 quick start guide 中的示例。 参考 Configuration Best Practices ","relpermalink":"/blog/configuration-best-practice/","summary":"本文档旨在汇总和强调用户指南、快速开始文档和示例中的最佳实践。","title":"Kubernetes配置最佳实践"},{"content":"前言 本示例来自 GitHub - distributed-load-testing-using-kubernetes 。\n该教程描述如何在Kubernetes 中进行分布式负载均衡测试，包括一个web应用、docker镜像和Kubernetes controllers/services。更多资料请查看Distributed Load Testing Using Kubernetes 。\n注意：该测试是在我自己本地搭建的kubernetes集群上测试的，不需要使用Google Cloud Platform。\n准备 不需要GCE及其他组件，你只需要有一个kubernetes集群即可。\n部署Web应用 sample-webapp 目录下包含一个简单的web测试应用。我们将其构建为docker镜像，在kubernetes中运行。你可以自己构建，也可以直接用这个我构建好的镜像index.tenxcloud.com/jimmy/k8s-sample-webapp:latest。\n在kubernetes上部署sample-webapp。\n$ cd kubernetes-config $ kubectl create -f sample-webapp-controller.yaml $ kubectl create -f kubectl create -f sample-webapp-service.yaml 部署Locust的Controller和Service locust-master和locust-work使用同样的docker镜像，修改cotnroller中spec.template.spec.containers.env字段中的value为你sample-webapp service的名字。\n- name: TARGET_HOST value: http://sample-webapp:8000 创建Controller Docker镜像（可选） locust-master和locust-work controller使用的都是locust-tasks docker镜像。你可以直接下载，也可以自己编译。自己编译大概要花几分钟时间，镜像大小为820M。\n$ docker build -t index.tenxcloud.com/jimmy/locust-tasks:latest . $ docker push index.tenxcloud.com/jimmy/locust-tasks:latest 注意：我使用的是时速云的镜像仓库。\n每个controller的yaml的spec.template.spec.containers.image 字段指定的是我的镜像：\nimage: index.tenxcloud.com/jimmy/locust-tasks:latest 部署locust-master $ kubectl create -f locust-master-controller.yaml $ kubectl create -f locust-master-service.yaml 部署locust-worker Now deploy locust-worker-controller:\n$ kubectl create -f locust-worker-controller.yaml 你可以很轻易的给work扩容，通过命令行方式：\n$ kubectl scale --replicas=20 replicationcontrollers locust-worker 当然你也可以通过WebUI：Dashboard - Workloads - Replication Controllers - ServiceName - Scale来扩容。\nDashboard 配置Traefik 参考kubernetes的traefik ingress安装 ，在ingress.yaml中加入如下配置：\n- host: traefik.locust.io http: paths: - path: / backend: serviceName: locust-master servicePort: 8089 然后执行kubectl replace -f ingress.yaml即可更新traefik。\n通过Traefik的dashboard就可以看到刚增加的traefik.locust.io节点。\nTraefik dashboard 执行测试 打开http://traefik.locust.io页面，点击Edit输入伪造的用户数和用户每秒发送的请求个数，点击Start Swarming就可以开始测试了。\n启动 locust 在测试过程中调整sample-webapp的pod个数（默认设置了1个pod），观察pod的负载变化情况。\n示例 Web 应用 从一段时间的观察中可以看到负载被平均分配给了3个pod。\n在locust的页面中可以实时观察也可以下载测试结果。\nLocust dashboard 参考 Distributed Load Testing Using Kubernetes 运用Kubernetes进行分布式负载测试 ","relpermalink":"/blog/distributed-load-testing-using-kubernetes/","summary":"该教程描述如何在 Kubernetes 中进行分布式负载均衡测试。","title":"使用 Kubernetes 进行分布式负载测试"},{"content":"Cluster IP 即Service的IP，通常在集群内部使用Service Name来访问服务，用户不需要知道该IP地址，kubedns会自动根据service name解析到服务的IP地址，将流量分发给Pod。\nService Name才是对外暴露服务的关键。\n在kubeapi的配置中指定该地址范围。\n默认配置\n--service-cluster-ip-range=10.254.0.0/16 --service-node-port-range=30000-32767 Pod IP 通过配置flannel的network和subnet来实现。\n默认配置\nFLANNEL_NETWORK=172.30.0.0/16 FLANNEL_SUBNET=172.30.46.1/24 Pod的IP地址不固定，当pod重启时IP地址会变化。\n该IP地址也是用户无需关心的。\n但是Flannel会在本地生成相应IP段的虚拟网卡，为了防止和集群中的其他IP地址冲突，需要规划IP段。\n主机/Node IP 物理机的IP地址，即kubernetes管理的物理机的IP地址。\n$ kubectl get nodes NAME STATUS AGE VERSION 172.20.0.113 Ready 12d v1.6.0 172.20.0.114 Ready 12d v1.6.0 172.20.0.115 Ready 12d v1.6.0 服务发现 集群内部的服务发现\n通过DNS即可发现，kubends是kubernetes的一个插件，不同服务之间可以直接使用service name访问。\n通过sericename:port即可调用服务。\n服务外部的服务发现\n通过Ingress来实现，我们是用的Traefik来实现。\n参考 Ingress解析 Kubernetes Traefik Ingress安装试用 ","relpermalink":"/blog/ip-and-service-discovry-in-kubernetes/","summary":"几种 IP 的来历。","title":"Kubernetes中的IP和服务发现体系"},{"content":"无意中发现Fabric8 这个对于Java友好的开源微服务管理平台。\n其实这在这里发现的Achieving CI/CD with Kubernetes （by Ramit Surana,on February 17, 2017），其实是先在slideshare 上看到的。\n大家可能以前听过一个叫做fabric 的工具，那是一个 Python (2.5-2.7) 库和命令行工具，用来流水线化执行 SSH 以部署应用或系统管理任务。所以大家不要把fabric8跟fabric搞混，虽然它们之间有一些共同点，但两者完全不是同一个东西，fabric8不是fabric的一个版本。Fabric是用python开发的，fabric8是java开发的。\n如果你想了解简化Fabric可以看它的中文官方文档 。\nFabric8简介 fabric8是一个开源集成开发平台，为基于Kubernetes 和Jenkins 的微服务提供持续发布 。\n使用fabric可以很方便的通过Continuous Delivery pipelines 创建、编译、部署和测试微服务，然后通过Continuous Improvement和ChatOps 运行和管理他们。\nFabric8微服务平台 提供：\nDeveloper Console ，是一个富web应用 ，提供一个单页面来创建、编辑、编译、部署和测试微服务。 Continuous Integration and Continous Delivery ，使用 Jenkins with a Jenkins Workflow Library 更快和更可靠的交付软件。 Management ，集中式管理Logging 、Metrics , ChatOps 、Chaos Monkey ，使用Hawtio 和Jolokia 管理Java Containers。 Integration Integration Platform As A Service with deep visualisation of your Apache Camel integration services, an API Registry to view of all your RESTful and SOAP APIs and Fabric8 MQ provides Messaging As A Service based on Apache ActiveMQ 。 Java Tools 帮助Java应用使用Kubernetes : Maven Plugin for working with Kubernetes ，这真是极好的 Integration and System Testing of Kubernetes resources easily inside JUnit with Arquillian Java Libraries and support for CDI extensions for working with Kubernetes . Fabric8微服务平台 Fabric8提供了一个完全集成的开源微服务平台，可在任何的Kubernetes 和OpenShift 环境中开箱即用。\n整个平台是基于微服务而且是模块化的，你可以按照微服务的方式来使用它。\n微服务平台提供的服务有：\n开发者控制台，这是一个富Web应用程序，它提供了一个单一的页面来创建、编辑、编译、部署和测试微服务。 持续集成和持续交付，帮助团队以更快更可靠的方式交付软件，可以使用以下开源软件： Jenkins ：CI／CD pipeline Nexus ： 组件库 Gogs ：git代码库 SonarQube ：代码质量维护平台 Jenkins Workflow Library ：在不同的项目中复用Jenkins Workflow scripts Fabric8.yml ：为每个项目、存储库、聊天室、工作流脚本和问题跟踪器提供一个配置文件 ChatOps ：通过使用hubot 来开发和管理，能够让你的团队拥抱DevOps，通过聊天和系统通知的方式来approval of release promotion Chaos Monkey ：通过干掉pods 来测试系统健壮性和可靠性 管理 日志 统一集群日志和可视化查看状态 metris 可查看历史metrics和可视化 参考 fabric8：容器集成平台——伯乐在线 Kubernetes部署微服务速成指南——2017-03-09 徐薛彪 容器时代微信公众号 fabric8官网 fabric8 get started 后记 我在自己笔记本上装了个minikube，试玩感受将在后续发表。\n试玩时需要科学上网。\n$gofabric8 start using the executable /usr/local/bin/minikube minikube already running using the executable /usr/local/bin/kubectl Switched to context \u0026#34;minikube\u0026#34;. Deploying fabric8 to your Kubernetes installation at https://192.168.99.100:8443 for domain in namespace default Loading fabric8 releases from maven repository:https://repo1.maven.org/maven2/ Deploying package: platform version: 2.4.24 Now about to install package https://repo1.maven.org/maven2/io/fabric8/platform/packages/fabric8-platform/2.4.24/fabric8-platform-2.4.24-kubernetes.yml Processing resource kind: Namespace in namespace default name user-secrets-source-admin Found namespace on kind Secret of user-secrets-source-adminProcessing resource kind: Secret in namespace user-secrets-source-admin name default-gogs-git Processing resource kind: Secret in namespace default name jenkins-docker-cfg Processing resource kind: Secret in namespace default name jenkins-git-ssh Processing resource kind: Secret in namespace default name jenkins-hub-api-token Processing resource kind: Secret in namespace default name jenkins-master-ssh Processing resource kind: Secret in namespace default name jenkins-maven-settings Processing resource kind: Secret in namespace default name jenkins-release-gpg Processing resource kind: Secret in namespace default name jenkins-ssh-config Processing resource kind: ServiceAccount in namespace default name configmapcontroller Processing resource kind: ServiceAccount in namespace default name exposecontroller Processing resource kind: ServiceAccount in namespace default name fabric8 Processing resource kind: ServiceAccount in namespace default name gogs Processing resource kind: ServiceAccount in namespace default name jenkins Processing resource kind: Service in namespace default name fabric8 Processing resource kind: Service in namespace default name fabric8-docker-registry Processing resource kind: Service in namespace default name fabric8-forge Processing resource kind: Service in namespace default name gogs ... ------------------------- Default GOGS admin username/password = gogsadmin/RedHat$1 Checking if PersistentVolumeClaims bind to a PersistentVolume .... Downloading images and waiting to open the fabric8 console... ------------------------- ..................................................... 启动了半天一直是这种状态：\nWaiting, endpoint for service is not ready yet... 我一看下载下来的\nhttps://repo1.maven.org/maven2/io/fabric8/platform/packages/fabric8-platform/2.4.24/fabric8-platform-2.4.24-kubernetes.yml 文件，真是蔚为壮观啊，足足有24712行(这里面都是实际配置，没有配置充行数)，使用了如下这些docker镜像，足足有53个docker镜像：\nfabric8/alpine-caddy:2.2.311 fabric8/apiman-gateway:2.2.168 fabric8/apiman:2.2.168 fabric8/chaos-monkey:2.2.311 fabric8/configmapcontroller:2.3.5 fabric8/eclipse-orion:2.2.311 fabric8/elasticsearch-k8s:2.3.4 fabric8/elasticsearch-logstash-template:2.2.311 fabric8/elasticsearch-v1:2.2.168 fabric8/exposecontroller:2.3.2 fabric8/fabric8-console:2.2.199 fabric8/fabric8-forge:2.3.88 fabric8/fabric8-kiwiirc:2.2.311 …","relpermalink":"/blog/fabric8-introduction/","summary":"本文介绍了开源的微服务管理平台 Fabric8。","title":"开源微服务管理平台 fabric8 简介"},{"content":"前言 我已就该话题已在2016年上海Qcon上发表过演讲。另外InfoQ网站上的文字版数据中心的YARN on Docker集群方案 ，即本文。\n项目代码开源在Github上：Magpie 当前数据中心存在的问题 数据中心中的应用一般独立部署，为了保证环境隔离与方便管理，保证应用最大资源 数据中心中普遍存在如下问题：\n主机资源利用率低 部署和扩展复杂 资源隔离无法动态调整 无法快速响应业务 为何使用YARN on Docker 彻底隔离队列\n为了合理利用Hadoop YARN的资源，队列间会互相抢占计算资源，造成重要任务阻塞 根据部门申请的机器数量划分YARN集群方便财务管理 更细粒度的资源分配 统一的资源分配\n每个NodeManager和容器都可以限定CPU、内存资源 YARN资源划分精确到CPU核数和内存大小 弹性伸缩性服务\n每个容器中运行一个NodeManager，增减YARN资源只需增减容器个数 可以指定每个NodeManager拥有的计算资源多少，按需申请资源 给我们带来什么好处？ Swarm统一集群资源调度\n统一资源 增加Docker虚拟化层，降低运维成本 增加Hadoop集群资源利用率\n对于数据中心：避免了静态资源隔离\n对于集群：加强集群内部资源隔离\n系统架构 YARN 在 swarm 上运行的架构 比如数据中心中运行的Hadoop集群，我们将HDFS依然运行在物理机上，即DataNode依然部署在实体机器上，将YARN计算层运行在Docker容器中，整个系统使用二层资源调度，Spark、Flink、MapReduce等应用运行在YARN上。\nSwarm调度最底层的主机硬件资源，CPU和内存封装为Docker容器，容器中运行NodeManager，提供给YARN集群，一个Swarm集群中可以运行多个YARN集群，形成圈地式的YARN计算集群。\nYARN 在 Swarm 上的架构之资源分配 具体流程\nswarm node向swarm master注册主机资源并加入到swarm cluster中 swarm master向cluster申请资源请求启动容器 swarm根据调度策略选择在某个node上启动docker container swarm node的docker daemon根据容器启动参数启动相应资源大小的NodeManager NodeManager自动向YARN的ResourceManager注册资源一个NodeManager资源添加完成。 Swarm为数据中心做容器即主机资源调度，每个swarmnode的节点结构如图：\nYARN 在 swarm 上的架构之单节点资源分配 一个Swarm node就是一台物理机，每台主机上可以起多个同类型的docker container，每个container的资源都有限制包括CPU、内存NodeManager容器只需要考虑本身进程占用的资源和需要给主机预留资源。假如主机是24核64G，我们可以分给一个容器5核12G，NodeManager占用4核10G的资源提供给YARN。\nKubernetes VS Swarm\n关于容器集群管理系统的选型，用Kubernetes还是Swarm？我们结合自己的经验和业务需求，对比如下：\nKubernetes vs Swarm 基于以上四点，我们当时选择了Swarm，它基本满足我们的需求，掌握和开发时常较短。\n镜像制作与发布 镜像制作和发布流程如下图：\nCI 流程 用户从客户端提交代码到Gitlab中，需要包含Dockerfile文件，通过集成了docker插件的Jenkins的自动编译发布机制，自动build镜像后push到docker镜像仓库中，同一个项目每提交一次代码都会重新build一次镜像，生成不同的tag来标识镜像，Swarm集群使用该镜像仓库就可以直接拉取镜像。\nDockerfile的编写技巧 Dockerfile 编写技巧 Dockerfile相当于docker镜像的编译打包流程说明，其中也不乏一些技巧。 很多应用需要配置文件，如果想为每次启动容器的时候使用不同的配置参数，可以通过传递环境变量的方式来修改配置文件，前提是需要写一个bash脚本，脚本中来处理配置文件，再将这个脚本作为entrypoint入口，每当容器启动时就会执行这个脚本从而替换配置文件中的参数，也可以通过CMD传递参数给该脚本。\n启动容器的时候通过传递环境变量的方式修改配置文件：\ndocker run -d --net=mynet -e NAMESERVICE=nameservice -e ACTIVE_NAMENODE_ID=namenode29 \\ -e STANDBY_NAMENODE_ID=namenode63 \\ -e HA_ZOOKEEPER_QUORUM=zk1:2181,zk2:2181,zk3:2181 \\ -e YARN_ZK_DIR=rmstore \\ -e YARN_CLUSTER_ID=yarnRM \\ -e YARN_RM1_IP=rm1 \\ -e YARN_RM2_IP=rm2 \\ -e CPU_CORE_NUM=5 -e NODEMANAGER_MEMORY_MB=12288 \\ -e YARN_JOBHISTORY_IP=jobhistory \\ -e ACTIVE_NAMENODE_IP=active-namenode \\ -e STANDBY_NAMENODE_IP=standby-namenode \\ -e HA=yes \\ docker-registry/library/hadoop-yarn:v0.1 resourcemanager 最后传递Resource Manager或者Node Manager参数指定启动相应的服务。\n集群管理 我开发的命令行工具magpie ，也可以通过其他开源可视化页面来管理集群，比如shipyard。\nShipyard 自定义网络 Docker容器跨主机互访一直是一个问题，Docker官方为了避免网络上带来的诸多麻烦，故将跨主机网络开了比较大的口子，而由用户自己去实现。我们开发并开源了Shrike这个docker网络插件，大家可以在这里下载到：GitHub - docker-ipam-plugin 目前Docker跨主机的网络实现方案也有很多种，主要包括端口映射、ovs、fannel等。但是这些方案都无法满足我们的需求，端口映射服务内的内网IP会映射成外网的IP，这样会给开发带来困惑，因为他们往往在跨网络交互时是不需要内网IP的，而ovs与fannel则是在基础网络协议上又包装了一层自定义协议，这样当网络流量大时，却又无端的增加了网络负载，最后我们采取了自主研发扁平化网络插件，也就是说让所有的容器统统在大二层上互通。架构如下：\nYARN 网络 我们首先需要创建一个br0自定义网桥，这个网桥并不是通过系统命令手动建立的原始Linux网桥，而是通过Docker的cerate network命令来建立的自定义网桥，这样避免了一个很重要的问题就是我们可以通过设置DefaultGatewayIPv4参数来设置容器的默认路由，这个解决了原始Linux自建网桥不能解决的问题. 用Docker创建网络时我们可以通过设置subnet参数来设置子网IP范围，默认我们可以把整个网段给这个子网，后面可以用ipamdriver（地址管理插件）来进行控制。还有一个参数gateway是用来设置br0自定义网桥地址的，其实也就是你这台宿主机的地址。\ndocker network create --opt=com.docker.network.bridge.enable_icc=true --opt=com.docker.network.bridge.enable_ip_masquerade=false --opt=com.docker.network.bridge.host_binding_ipv4=0.0.0.0 --opt=com.docker.network.bridge.name=br0 --opt=com.docker.network.driver.mtu=1500 --ipam-driver=talkingdata --subnet=容器IP的子网范围 --gateway=br0网桥使用的IP,也就是宿主机的地址 --aux-address=DefaultGatewayIPv4=容器使用的网关地址 mynet IPAM 插件 IPAM驱动是专门管理Docker 容器IP的, Docker 每次启停与删除容器都会调用这个驱动提供的IP管理接口，然后IP接口会对存储IP地址的Etcd有一个增删改查的操作。此插件运行时会起一个UnixSocket, 然后会在docker/run/plugins目录下生成一个.sock文件，Dockerdaemon之后会和这个sock 文件进行沟通去调用我们之前实现好的几个接口进行IP管理，以此来达到IP管理的目的，防止IP冲突。 通过Docker命令去创建一个自定义的网络起名为mynet，同时会产生一个网桥br0，之后通过更改网络配置文件（在/etc/sysconfig/network-scripts/下ifcfg-br0、ifcfg-默认网络接口名）将默认网络接口桥接到br0上，重启网络后，桥接网络就会生效。Docker默认在每次启动容器时都会将容器内的默认网卡桥接到br0上，而且宿主机的物理网卡也同样桥接到了br0上了。其实桥接的原理就好像是一台交换机，Docker 容器和宿主机物理网络接口都是服务器，通过vethpair这个网络设备像一根网线插到交换机上。至此，所有的容器网络已经在同一个网络上可以通信了，每一个Docker容器就好比是一台独立的虚拟机，拥有和宿主机同一网段的IP，可以实现跨主机访问了。\n性能瓶颈与优化 大家可能会担心自定义网络的性能问题，为此我们用iperf进行了网络性能测试。我们对比了不同主机容器间的网速，同一主机上的不同容器和不同主机间的网速，结果如下表：\n网络性能对比 从表中我们可以看到，在这一组测试中，容器间的网速与容器是在想通主机还是在不同主机上的差别不大，说明我们的网络插件性能还是很优异的。 Hadoop配置优化 因为使用docker将原来一台机器一个nodemanager给细化为了多个，会造成nodemanager个数的成倍增加，因此hadoop的一些配置需要相应优化。\n- yarn.nodemanager.localizer.fetch.thread-count 随着容器数量增加，需要相应调整该参数 - yarn.resourcemanager.amliveliness-monitor.interval-ms 默认1秒，改为10秒，否则时间太短可能导致有些节点无法注册 - yarn.resourcemanager.resource-tracker.client.thread-count 默认50，改为100，随着容器数量增加，需要相应调整该参数 - yarn.nodemanager.pmem-check-enabled 默认true，改为false，不检查任务正在使用的物理内存量 - 容器中hadoop ulimit值修改，默认4096，改成655350 集群监控 如果使用shipyard管理集群会有一个单独的监控页面，可以看到一定时间段内的CPU、内存、IO、网络使用状况。\n集群监控 关于未来 YARN 的未来 我们未来规划做的是DC/OS，基于Docker的应用自动打包编译分发系统，让开发人员可以很便捷的申请资源，上下线服务，管理应用。要达到这个目标还有很多事情要做：\nService Control Panel：统一的根据服务来管理的web页面 Loadbalance：容器根据机器负载情况自动迁移 Scheduler：swarm调度策略优化 服务配置文件：提供镜像启动参数的配置文件，所有启动参数可通过文件配置 监 …","relpermalink":"/blog/yarn-on-docker/","summary":"基于docker swarm。","title":"容器技术在大数据场景下的应用——YARN on Docker"},{"content":"如果你看过美剧「硅谷」会记得剧中主角们所在的创业公司PiedPipper ，他们就是靠自己发明的视频压缩算法来跟大公司Hooli竞争的，这部剧现在已经发展到第4季，在腾讯视频 上可以免费观看。\n最近关注了两个图像处理的Open Source Projects。\nGoogle Guetzli 图像压缩工具 Luan Fujun’s Deep Photo Style Transfer 图像style转换工具 另外对于图像处理还处于Photoshop、Lightroom这种摄影后期和图像处理命令行工具ImageMagick 的我来说，图像压缩，智能图像风格转换实乃上乘武功，不是我等凡夫俗子驾驭的了，但是乘兴而来，总不能败兴而归吧，下面我们来一探究竟。\nGoogle Guetzli 聊聊架构微信公众号上有一篇介绍Google开源新算法，可将JPEG文件缩小35% 文章。\n我在Mac上试用了一下，安装很简单，只要一条命令：\nbrew install guetzli 但是当我拿一张22M大小的照片使用guetzli压缩的时候，我是绝望的，先后三次kill掉了进程。\n因为实在是太慢了，也能是我软件对内存和CPU的利用率不高，效果你们自己看看。\n原图是这个样子的，拍摄地点在景山上的，俯瞰紫禁城的绝佳位置。\n原图 guetzli --quality 84 --verbose 20160403052.jpg output.jpg 为什么quality要设置成84呢？因为只能设置为84+的quality，如果要设置的更低的话需要自己修改代码。\nprocess 耗时了一个小时，后台进程信息。\n后台进程 这个是使用Squash压缩后的大小效果，压缩每张照片差不多只要3秒钟。\nSquash的logo就是个正在被剥皮的🍊，这是下载地址 。\n压缩比分别为70%和30%。\nImg 压缩比70%后的细节放大图\n70 压缩比30%的细节放大图\n30 你看出什么区别了吗？反正我是没有。\n下面再来看看耗时一个小时，千呼万唤始出来的guetzli压缩后的效果和使用squash压缩比为30%的效果对比。\n对比 左面是使用guetzli压缩后（4.1M），右面使用的squash压缩后（3.1M）的照片。\n似乎还是没有什么区别啊？你看出来了吗？\nGuetzli总结 可能是我使用Guetzli的方式不对，但是命令行里确实没有设置CPU和内存资源的选项啊，为啥压缩照片会这么慢呢？效果也并不出彩，不改代码的话照片质量只能设置成84以上，但是这个是Open Source的，使用的C++开发，可以研究下它的图像压缩算法。\nDeep Photo Style Transfer 来自康奈尔大学的Luan Fujun开源的图像sytle转换工具，看了README 的介绍，上面有很多图像风格转换的例子，真的很惊艳，市面上好像还没有这种能够在给定任意一张照片的情况下，自动将另一张照片转换成该照片的style。\n这个工具使用Matlab和Lua开发，基于Torch 运行的时候需要CUDA ，cudnn ，Matlab ，环境实在太复杂，就没折腾，启动有人发布Docker镜像 ，已经有人提了issue。\n如果它能够被商用，绝对是继Prisma后又一人工智能照片处理应用利器。\n后记 是不是有了照片风格转换这个东西就不需要做照片后期了？只要选几张自己喜欢的风格照片，再鼠标点几下就可以完成照片处理了？摄影师要失业了？非也！照片风格东西本来就是很主观性的，每个人都有自己喜欢的风格，照相机发明后就有人说画家要失业了，其实不然，画画依然是创造性地劳动，只能说很多写实风格的画家要失业了。Deep Photo Style Transfer也许会成为Lightroom或者手机上一款app的功能，是一个不错的工具。也许还会成为像Prisma一样的现象级产品，who knows?🤷‍♂️\n","relpermalink":"/blog/picture-process/","summary":"Google Guetzli和基于AI的Deep Photo Style Transfer。","title":"两款开源图片处理工具对比"},{"content":"前言 最近研究了下Pivotal的Cloud foundry，CF本身是一款开源软件，很多PAAS厂商都加入了CF，我们用的是的PCF Dev（PCF Dev是一款可以在工作站上运行的轻量级PCF安装）来试用的，因为它可以部署在自己的环境里，而Pivotal Web Services只免费两个月，之后就要收费。这里 有官方的详细教程。\n开始 根据官网的示例，我们将运行一个Java程序示例。\n安装命令行终端\n下载 后双击安装即可，然后执行cf help能够看到帮助。\n安装PCF Dev\n先下载 ，如果你没有Pivotal network账号的话，还需要注册个用户，然后用以下命令安装：\n$./pcfdev-VERSION-osx \u0026amp;\u0026amp; \\ cf dev start Less than 4096 MB of free memory detected, continue (y/N): \u0026gt; y Please sign in with your Pivotal Network account. Need an account? Join Pivotal Network: https://network.pivotal.io Email\u0026gt; 849122844@qq.com Password\u0026gt; Downloading VM... Progress: |+++++++++++++=======\u0026gt;| 100% VM downloaded. Allocating 4096 MB out of 16384 MB total system memory (3514 MB free). Importing VM... Starting VM... Provisioning VM... Waiting for services to start... 8 out of 57 running 8 out of 57 running 8 out of 57 running 46 out of 57 running 57 out of 57 running _______ _______ _______ ______ _______ __ __ | || || | | | | || | | | | _ || || ___| | _ || ___|| |_| | | |_| || || |___ | | | || |___ | | | ___|| _|| ___| | |_| || ___|| | | | | |_ | | | || |___ | | |___| |_______||___| |______| |_______| |___| is now running. To begin using PCF Dev, please run: cf login -a https://api.local.pcfdev.io --skip-ssl-validation Apps Manager URL: https://local.pcfdev.io Admin user =\u0026gt; Email: admin / Password: admin Regular user =\u0026gt; Email: user / Password: pass 启动过程中还需要Sign In，所以注册完后要记住用户名（邮箱地址）和密码（必须超过8位要有特殊字符和大写字母）。这个过程中还要下载VM，对内存要求至少4G。而且下载速度比较慢，我下载的了大概3个多小时吧。\n下面部署一个应用到PCF Dev上试一试。\n部署应用 下载代码\n$git clone https://github.com/cloudfoundry-samples/spring-music $cd ./spring-music $cf login -a api.local.pcfdev.io --skip-ssl-validation API endpoint: api.local.pcfdev.io Email\u0026gt; user Password\u0026gt; pass Authenticating... OK Targeted org pcfdev-org Targeted space pcfdev-space API endpoint: https://api.local.pcfdev.io (API version: 2.65.0) User: user Org: pcfdev-org Space: pcfdev-space 编译应用\n使用gradle来编译。\n$./gradlew assemble :compileJava UP-TO-DATE :processResources UP-TO-DATE :classes UP-TO-DATE :findMainClass :jar :bootRepackage Download https://repo1.maven.org/maven2/com/sun/jersey/jersey-client/1.13/jersey-client-1.13.jar Download https://repo1.maven.org/maven2/com/sun/jersey/jersey-json/1.13/jersey-json-1.13.jar Download https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.5/httpcore-4.4.5.jar Download https://repo1.maven.org/maven2/com/nimbusds/oauth2-oidc-sdk/4.5/oauth2-oidc-sdk-4.5.jar Download https://repo1.maven.org/maven2/com/google/code/gson/gson/2.3.1/gson-2.3.1.jar Download https://repo1.maven.org/maven2/com/sun/jersey/jersey-core/1.13/jersey-core-1.13.jar Download https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.2/jackson-core-asl-1.9.2.jar Download https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.2/jackson-mapper-asl-1.9.2.jar Download https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar Download https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar Download https://repo1.maven.org/maven2/net/jcip/jcip-annotations/1.0/jcip-annotations-1.0.jar Download https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.3.1/commons-lang3-3.3.1.jar Download https://repo1.maven.org/maven2/net/minidev/json-smart/1.1.1/json-smart-1.1.1.jar Download https://repo1.maven.org/maven2/com/nimbusds/lang-tag/1.4/lang-tag-1.4.jar Download https://repo1.maven.org/maven2/com/nimbusds/nimbus-jose-jwt/3.1.2/nimbus-jose-jwt-3.1.2.jar Download https://repo1.maven.org/maven2/stax/stax-api/1.0.1/stax-api-1.0.1.jar Download https://repo1.maven.org/maven2/org/bouncycastle/bcprov-jdk15on/1.51/bcprov-jdk15on-1.51.jar Download https://repo1.maven.org/maven2/javax/mail/mail/1.4.7/mail-1.4.7.jar :assemble BUILD SUCCESSFUL Total time: 1 mins 25.649 secs This build could be faster, please consider using the Gradle Daemon: https://docs.gradle.org/2.14/userguide/gradle_daemon.html 上传应用\n设置应用的主机名为spring-music。\n$cf push --hostname spring-music Using manifest file /Users/jimmy/Workspace/github/cloudfoundry-samples/spring-music/manifest.yml Creating app spring-music in org pcfdev-org / space pcfdev-space as user... OK Creating route spring-music.local.pcfdev.io... OK Binding spring-music.local.pcfdev.io to spring-music... OK Uploading spring-music... Uploading app files from: /var/folders/61/f7mqkyjn1nz5mfmfvdztgzjw0000gn/T/unzipped-app139680305 Uploading 38.9M, 234 files Done uploading OK Starting app spring-music in org pcfdev-org / space pcfdev-space as user... Downloading dotnet-core_buildpack... Downloading go_buildpack... Downloading python_buildpack... Downloading php_buildpack... Downloading staticfile_buildpack... Downloaded staticfile_buildpack Downloading binary_buildpack... Downloaded binary_buildpack (9.3K) Downloading java_buildpack... Downloaded java_buildpack (249.1M) Downloaded dotnet-core_buildpack (169.3M) Downloading ruby_buildpack... Downloading nodejs_buildpack... Downloaded python_buildpack (255.3M) …","relpermalink":"/blog/cloud-foundry-tryout/","summary":"最近研究了下Pivotal的Cloud foundry，CF本身是一款开源软件，很多PAAS厂商都加入了CF。","title":"Pivotal Cloud foundry快速开始指南"},{"content":"前言 之前陆陆续续看过一点docker的源码，都未成体系，最近在研究Docker-17.03-CE，趁此机会研究下docker的源码，在网上找到一些相关资料，都比较过时了，发现孙宏亮写过一本书叫《Docker源码分析》，而且之前也在InfoQ上陆续发过一些文章，虽然文章都比较老了，基于老的docker版本，但我认为依然有阅读的价值。起码能有这三方面收获：\n一是培养阅读源码的思维方式，为自己阅读docker源码提供借鉴。 二是可以了解docker版本的来龙去脉。 三还可以作为Go语言项目开发作为借鉴。 下载地址 鉴于截止本文发稿时这本书已经发行一年半了了，基于的docker版本还是1.2.0，而如今都到了1.13.0（docker17.03的老版本号），应该很少有人买了吧，可以说这本书的纸质版本的生命周期也差不多了吧。如果有人感兴趣可以到网上找找看看，Docker源码解析-机械工业出版社-孙宏亮著-2015年8月（完整文字版，大小25.86M），Docker源码解析-看云整理版（文字版，有缩略，大小7.62M）。\nOut-of-date 有一点必须再次强调一下，这本书中的docker源码分析是基于docker1.2.0，而这个版本的docker源码在github上已经无法下载到了，github上available的最低版本的docker源码是1.4.1。\n顺便感叹一句，科技行业发展实在太快了，尤其是互联网，一本书能连续用上三年都不过时，如果这样的话那么这门技术恐怕都就要被淘汰了吧？\n总体架构 Docker总体上是用的是Client/Server模式，所有的命令都可以通过RESTful接口传递。\n整个Docker软件的架构中可以分成三个角色：\nDaemon：常驻后台运行的进程，接收客户端请求，管理docker容器。 Client：命令行终端，包装命令发送API请求。 Engine：真正处理客户端请求的后端程序。 代码结构 Docker的代码结构比较清晰，分成的目录比较多，有以下这些：\napi：定义API，使用了Swagger2.0这个工具来生成API，配置文件在api/swagger.yaml builder：用来build docker镜像的包，看来历史比较悠久了 bundles：这个包是在进行docker源码编译和开发环境搭建 的时候用到的，编译生成的二进制文件都在这里。 cli：使用cobra 工具生成的docker客户端命令行解析器。 client：接收cli的请求，调用RESTful API中的接口，向server端发送http请求。 cmd：其中包括docker和dockerd两个包，他们分别包含了客户端和服务端的main函数入口。 container：容器的配置管理，对不同的platform适配。 contrib：这个目录包括一些有用的脚本、镜像和其他非docker core中的部分。 daemon：这个包中将docker deamon运行时状态expose出来。 distribution：负责docker镜像的pull、push和镜像仓库的维护。 dockerversion：编译的时候自动生成的。 docs：文档。这个目录已经不再维护，文档在另一个仓库里 。 experimental：从docker1.13.0版本起开始增加了实验特性。 hack：创建docker开发环境和编译打包时用到的脚本和配置文件。 image：用于构建docker镜像的。 integration-cli：集成测试 layer：管理 union file system driver上的read-only和read-write mounts。 libcontainerd：访问内核中的容器系统调用。 man：生成man pages。 migrate：将老版本的graph目录转换成新的metadata。 oci：Open Container Interface库 opts：命令行的选项库。 pkg： plugin：docker插件后端实现包。 profiles：里面有apparmor和seccomp两个目录。用于内核访问控制。 project：项目管理的一些说明文档。 reference：处理docker store中镜像的reference。 registry：docker registry的实现。 restartmanager：处理重启后的动作。 runconfig：配置格式解码和校验。 vendor：各种依赖包。 volume：docker volume的实现。 下一篇将讲解docker的各个功能模块和原理。\n","relpermalink":"/blog/docker-source-code-analysis-code-structure/","summary":"之前陆陆续续看过一点docker的源码，都未成体系，最近在研究Docker-17.03-CE，趁此机会研究下docker的源码。","title":"Docker源码分析第一篇——代码结构"},{"content":" 亲，你还在为虚拟主机、域名、空间而发愁吗？你想拥有自己的网站吗？你想拥有一个分享知识、留住感动，为开源事业而奋斗终身吗？那么赶快拿起你手中的📱拨打16899168，不对，是看这篇文章吧，不用998，也不用168，这一切都是免费的，是的你没看错，真的不要钱！\n准备 当然还是需要你有一点电脑基础的，会不会编程不要紧，还要会一点英文，你需要先申请一下几个账号和安装一些软件环境：\nGitHub 这是必需的，因为你需要使用Github Pages 来托管你的网站。而且你还需要安装git工具。创建一个以自己用户名命名的username.github.io的project。 七牛云存储 非必需，为了存储文件方便，建议申请一个，免费10G的存储空间，存储照片和一些小文件是足够的，可以用来做外链，方便存储和管理，这样你就不用把图片也托管到Github上了。流量也是不限的。我没有收七牛的一点好处，以为是我自己用的，所以推荐给大家，七牛还有命令行客户端，方便你上传和同步文件。如上的题图都是存储在七牛云中的。 百度统计 非必需，基本的网站数据分析，免费的，质量还行。还有微信公众号可以查看，这一点我发现腾讯分析居然都没有微信公众号，自家的产品咋都不推出微信客户端呢。顺便提一下，这个统计账号跟你的百度账号不是同一个东西，两者是两套体系，当然你可以和自己的百度账号关联。只需要在Web的Header中植入一段JS代码即可。 Hugo 必需的，静态网站生成工具，用来编译静态网站的。跟Hexo比起来我更喜欢这个工具。 Typro 非必需，但是强烈推荐，我最喜欢的免费的Markdown编辑器，hugo可以编译markdown格式为HTML，所以用它来写博客是最合适不过了。 好了注册好Github后你现在可以尽情的玩耍了！😄\nLet’s rock\u0026amp;roll! 首先介绍下Hugo\nHugo是一种通用的网站框架。严格来说，Hugo应该被称作静态网站生成器。\n静态网站生成器从字面上来理解，就是将你的内容生成静态网站。所谓“静态”的含义其实反映在网站页面的生成的时间。一般的web服务器（WordPress, Ghost, Drupal等等）在收到页面请求时，需要调用数据库生成页面（也就是HTML代码），再返回给用户请求。而静态网站则不需要在收到请求后生成页面，而是在整个网站建立起之前就将所有的页面全部生成完成，页面一经生成便称为静态文件，访问时直接返回现成的静态页面，不需要数据库的参与。\n采用静态网站的维护也相当简单，实际上你根本不需要什么维护，完全不用考虑复杂的运行时间，依赖和数据库的问题。再有也不用担心安全性的问题，没有数据库，网站注入什么的也无从下手。\n静态网站最大好处就是访问快速，不用每次重新生成页面。当然，一旦网站有任何更改，静态网站生成器需要重新生成所有的与更改相关的页面。然而对于小型的个人网站，项目主页等等，网站规模很小，重新生成整个网站也是非常快的。Hugo在速度方面做得非常好，Dan Hersam在他这个Hugo教程 里提到，5000篇文章的博客，Hugo生成整个网站只花了6秒，而很多其他的静态网站生成器则需要几分钟的时间。我的博客目前文章只有几十篇，用Hugo生成整个网站只需要0.1秒。官方文档提供的数据是每篇页面的生成时间不到1ms。\n认为对于个人博客来说，应该将时间花在内容上而不是各种折腾网站。Hugo会将Markdown格式的内容和设置好模版一起，生成漂亮干净的页面。挑选折腾好一个喜爱的模版，在Sublime Text里用Markdown写博客，再敲一行命令生成同步到服务器就OK了。整个体验是不是非常优雅简单还有点geek的味道呢？\n了解Hugo 首先建立自己的网站，mysite是网站的路径\n$ hugo new site mysite 然后进入该路径\n$ cd mysite 在该目录下你可以看到以下几个目录和config.toml文件\n▸ archetypes/ ▸ content/ ▸ layouts/ ▸ static/ config.toml config.toml是网站的配置文件，包括baseurl, title, copyright等等网站参数。\n这几个文件夹的作用分别是：\narchetypes：包括内容类型，在创建新内容时自动生成内容的配置 content：包括网站内容，全部使用markdown格式 layouts：包括了网站的模版，决定内容如何呈现 static：包括了css, js, fonts, media等，决定网站的外观 Hugo提供了一些完整的主题可以使用，下载这些主题：\n$ git clone --recursive https://github.com/spf13/hugoThemes themes 此时现成的主题存放在themes/文件夹中。\n现在我们先熟悉一下Hugo，创建新页面：\n$ hugo new about.md 进入content/文件夹可以看到，此时多了一个markdown格式的文件about.md，打开文件可以看到时间和文件名等信息已经自动加到文件开头，包括创建时间，页面名，是否为草稿等。\n--- date: \u0026#34;2015-02-01T18:19:54+08:00\u0026#34; draft: true title: \u0026#34;about\u0026#34; categories: \u0026#34;github-pages\u0026#34; tag: [\u0026#34;blog\u0026#34;,\u0026#34;post\u0026#34;] --- # About me - Jimmy Song - rootsongjc@gmail.com 我在页面中加入了一些内容，然后运行Hugo:\n$ hugo server -t hyde --buildDrafts -t参数的意思是使用hyde主题渲染我们的页面，注意到about.md目前是作为草稿，即draft参数设置为true，运行Hugo时要加上--buildDrafts参数才会生成被标记为草稿的页面。 在浏览器输入localhost:1313，就可以看到我们刚刚创建的页面。\n注意观察当前目录下多了一个文件夹public/，这里面是Hugo生成的整个静态网站，如果使用Github pages来作为博客的Host，你只需要将public/里的文件上传就可以，这相当于是Hugo的输出。\n详细说明请看这位朋友的文章：Nanshu Wang - Hugo静态网站生成器中文教程 说明\n使用hugo new命令生成的文章前面的加号中包括的那几行，是用来设置文章属性的，这些属性使用的是yaml语法。\ndate 自动增加时间标签，页面上默认显示n篇最新的文章。 draft 设置为false的时候会被编译为HTML，true则不会编译和发表，在本地修改文章时候用true。 title 设置文章标题 tags 数组，可以设置多个标签，都好隔开，hugo会自动在你博客主页下生成标签的子URL，通过这个URL可以看到所有具有该标签的文章。 categories 文章分类，跟Tag功能差不多，只能设置一个字符串。 今天先说到这里，再次声明下，Jimmy Song’s blog 就是用👆的步骤建立的。\nJimmy Song’s blog的页面比较简陋，你可以在这里 找到更多可爱的模版。另外我给自己翻译的书Cloude Native Go 做一个静态页面，点此查看 ，欢迎大家关注。🙏\n以下 2017年8月31日更新\n如果你对GitHub的域名不满意，想要用自己的域名，那么申请域名的地方有很多，比如万网、GoDaddy、Namecheap，我的域名 jimmysong.io 就是在 Namecheap 上申请的，申请完域名后还需要做域名解析，我使用的是 DNSPod，免费的，然后在 GitHub 中配置下 CNAME 即可。\n","relpermalink":"/blog/building-github-pages-with-hugo/","summary":"Hugo 是一种通用的网站框架，本问教你如何使用 Hugo 来构建静态网站。","title":"零基础使用 Hugo 和 GitHub Pages 创建自己的博客"},{"content":"前几天写的几篇关于Contiv的文章 已经把引入坑了😂\n今天这篇文章将带领大家用正确的姿势编译和打包一个contiv netplugin。\n请一定要在Linux环境中编译。docker中编译也会报错，最好还是搞个虚拟🐔吧，最好还有VPN能翻墙。\n环境准备 我使用的是docker17.03-CE、安装了open vSwitch(这个包redhat的源里没有，需要自己的编译安装)。\n编译 这一步是很容易失败的，有人提过issue-779 具体步骤\n创建一个link /go链接到你的GOPATH目录，下面编译的时候要用。 将源码的vender目录下的文件拷贝到$GOPATH/src目录。 执行编译 在netplugin目录下执行以下命令能够编译出二进制文件。\nNET_CONTAINER_BUILD=1 make build 在你的**/$GOPATH/bin**目录下应该会有如下几个文件：\ncontivk8s github-release godep golint misspell modelgen netcontiv netctl netmaster netplugin ⚠️编译过程中可能会遇到 有些包不存在或者需要翻墙下载。\n打包 我们将其打包为docker plugin。\nMakefile里用于创建plugin rootfs的命令是：\nhost-pluginfs-create: @echo dev: creating a docker v2plugin rootfs ... sh scripts/v2plugin_rootfs.sh v2plugin_rootfs.sh这个脚本的内容：\n#!/bin/bash # Script to create the docker v2 plugin # run this script from contiv/netplugin directory echo \u0026#34;Creating rootfs for v2plugin \u0026#34;, ${CONTIV_V2PLUGIN_NAME} cat install/v2plugin/config.template | grep -v \u0026#34;##\u0026#34; \u0026gt; install/v2plugin/config.json sed -i \u0026#34;s%PluginName%${CONTIV_V2PLUGIN_NAME}%\u0026#34; install/v2plugin/config.json cp bin/netplugin bin/netmaster bin/netctl install/v2plugin docker build -t contivrootfs install/v2plugin id=$(docker create contivrootfs true) mkdir -p install/v2plugin/rootfs sudo docker export \u0026#34;${id}\u0026#34; | sudo tar -x -C install/v2plugin/rootfs docker rm -vf \u0026#34;${id}\u0026#34; docker rmi contivrootfs rm install/v2plugin/netplugin install/v2plugin/netmaster install/v2plugin/netctl 先把$GOPATH/bin下生成的\nnetplugin\nnetmaster\nnetctl\nnetplugin\n这几个二进制文件拷贝到netplugin源码的bin目录下。\n这里面用语创建contivrootfs镜像的Dockerfile内容：\n# Docker v2plugin container with OVS / netplugin / netmaster FROM alpine:3.5 MAINTAINER Cisco Contiv (http://contiv.github.io/) RUN mkdir -p /run/docker/plugins /etc/openvswitch /var/run/contiv/log \\ \u0026amp;\u0026amp; echo \u0026#39;http://dl-cdn.alpinelinux.org/alpine/v3.4/main\u0026#39; \u0026gt;\u0026gt; /etc/apk/repositories \\ \u0026amp;\u0026amp; apk update \u0026amp;\u0026amp; apk add openvswitch=2.5.0-r0 iptables COPY netplugin netmaster netctl startcontiv.sh / ENTRYPOINT [\u0026#34;/startcontiv.sh\u0026#34;] 执行make host-pluginfs-create创建rootfs。\n创建出了rootfs后，然后执行\ndocker plugin create localhost:5000/contiv/netplugin . docker push localhost:5000/contiv/netplugin 注：我们将插件push到docker registry的镜像仓库中，当前Harbor 还不支持docker插件的push。\nInstall plugin\n下面是编译和安装我自己生成v2plugin的过程。\n修改config.json文件中的plugin_name字段的值为插件的名称。\n$docker plugin install localhost:5000/contiv/v2plugin Plugin \u0026#34;localhost:5000/contiv/v2plugin\u0026#34; is requesting the following privileges: - network: [host] - mount: [/etc/openvswitch] - mount: [/var/log/openvswitch] - mount: [/var/run] - mount: [/lib/modules] - capabilities: [CAP_SYS_ADMIN CAP_NET_ADMIN CAP_SYS_MODULE] Do you grant the above permissions? [y/N] y latest: Pulling from contiv/v2plugin fd87a71d9090: Download complete Digest: sha256:b13ad7930f771c9602acf562c2ae147482466f4d94e708692a215935663215a6 Status: Downloaded newer image for localhost:5000/contiv/v2plugin:latest Installed plugin localhost:5000/contiv/v2plugin 自己create的插件enable的时候从docker daemon的日志中依然可以看到之前看到找不到socket的错误，实际上也确实是没有生成。如果直接使用docker plugin install store/contiv/v2plugin:1.0.0-beta.3 的方式安装插件是没有问题的。\nDocker17.03-CE中插件机制存在的问题 Docker17.03的插件机制是为了docker公司的商业化策略而实行的，所有的docker插件都运行在自己的namespace和rootfs中，插件接口\nPlugin backend接口\n// Backend for Plugin type Backend interface { Disable(name string, config *enginetypes.PluginDisableConfig) error Enable(name string, config *enginetypes.PluginEnableConfig) error List(filters.Args) ([]enginetypes.Plugin, error) Inspect(name string) (*enginetypes.Plugin, error) Remove(name string, config *enginetypes.PluginRmConfig) error Set(name string, args []string) error Privileges(ctx context.Context, ref reference.Named, metaHeaders http.Header, authConfig *enginetypes.AuthConfig) (enginetypes.PluginPrivileges, error) Pull(ctx context.Context, ref reference.Named, name string, metaHeaders http.Header, authConfig *enginetypes.AuthConfig, privileges enginetypes.PluginPrivileges, outStream io.Writer) error Push(ctx context.Context, name string, metaHeaders http.Header, authConfig *enginetypes.AuthConfig, outStream io.Writer) error Upgrade(ctx context.Context, ref reference.Named, name string, metaHeaders http.Header, authConfig *enginetypes.AuthConfig, privileges enginetypes.PluginPrivileges, outStream io.Writer) error CreateFromContext(ctx context.Context, tarCtx io.ReadCloser, options *enginetypes.PluginCreateOptions) error } 从Plugin的后端接口中可以看到，没有像镜像一样的两个常用方法：\n没有修改plugin名字的方法，因为没有这个方法，就无法push plugin到自己的镜像仓库，另外Harbor还是不支持docker plugin push Issue-1532 。 没有导出plugin的方法，这样就只能在联网的主机上安装docker plugin了，对于无法联网的主机只好束手无策了。 估计docker官方也不会开放这两个接口吧。毕竟这是Docker EE 的一个重要卖点：\nDocker EE’s Certified Plugins provide networking and volume plugins and easy to download and install containers to the Docker EE environment.\n疑问 为什么一定要使用docker plugin install\n因为docker plugin install的时候会申请一些访问权限。\n这一块在上面的步骤中可以看到。\n为什么docker plugin不能改名字？\n我们看下Plugin的结构体（在api/types/plugin.go中定义）：\n// Plugin A plugin for the Engine API // swagger:model Plugin type Plugin struct { // config // Required: true Config PluginConfig `json:\u0026#34;Config\u0026#34;` // True when the plugin is running. …","relpermalink":"/blog/contiv-ultimate/","summary":"本文将带领大家用正确的姿势编译和打包一个 contiv netplugin。","title":"Docker 17.03CE 下思科 Docker 网络插件contiv 趟坑终极版"},{"content":" 当你看到这篇文章时，如果你也正在进行docker1.13+版本下的plugin开发，恭喜你也入坑了，如果你趟出坑，麻烦告诉你的方法，感恩不尽🙏\n看了文章后你可能会觉得，官网上的可能是个假🌰。虽然官网上的文档写的有点不对，不过你使用docker-ssh-volume的开源代码自己去构建plugin的还是可以成功的！\nDocker plugin开发文档 首先docker官方给出了一个docker legacy plugin文档 ，这篇文章基本就是告诉你docker目前支持哪些插件，罗列了一系列连接，不过对不起，这些不是docker官方插件，有问题去找它们的开发者去吧😂\nDocker plugin貌似开始使用了新的v2 plugin了，legacy版本的plugin可以能在后期被废弃。\n从docker的源码plugin/store.go中可以看到：\n/* allowV1PluginsFallback determines daemon\u0026#39;s support for V1 plugins. * When the time comes to remove support for V1 plugins, flipping * this bool is all that will be needed. */ const allowV1PluginsFallback bool = true /* defaultAPIVersion is the version of the plugin API for volume, network, IPAM and authz. This is a very stable API. When we update this API, then pluginType should include a version. e.g. \u0026#34;networkdriver/2.0\u0026#34;. */ const defaultAPIVersion string = \u0026#34;1.0\u0026#34; 随着docker公司是的战略调整，推出了docker-CE和docker-EE之后，未来有些插件就可能要收费了，v2版本的插件都是在docker store中下载了，而这种插件在创建的时候都是打包成docker image，如果不开放源码的话，你即使pull下来插件也无法修改和导出的，docker plugin目前没有导出接口。\n真正要开发一个docker plugin还是得看docker plugin API ，这篇文档告诉我们：\n插件发现 当你开发好一个插件docker engine怎么才能发现它们呢？有三种方式：\n- **.sock**，linux下放在/run/docker/plugins目录下，或该目录下的子目录比如[flocker](https://github.com/ClusterHQ/flocker)插件的`.sock`文件放在`/run/docker/plugins/flocker/flocker.sock`下 - **.spec**，比如**convoy**插件在`/etc/docker/plugins/convoy.spec `定义，内容为`unix:///var/run/convoy/convoy.sock` - **.json**，比如**infinit**插件在`/usr/lib/docker/plugins/infinit.json `定义，内容为`{\u0026#34;Addr\u0026#34;:\u0026#34;https://infinit.sh\u0026#34;,\u0026#34;Name\u0026#34;:\u0026#34;infinit\u0026#34;}` 文章中的其它部分貌似都过时了，新的插件不是作为systemd进程运行的，而是完全通过docker plugin命令来管理的。\n当你使用**docker plugin enable \u0026lt;plugin_name\u0026gt;**来激活了插件后，理应在/run/docker/plugins目录下生成插件的.sock文件，但是现在只有一个以runc ID命名的目录，这个问题下面有详细的叙述过程，你也可以跳过，直接看issue-31723 docker plugin管理 创建sshfs volume plugin 官方示例文档 （这个文档有问题）docker-issue29886 官方以开发一个sshfs的volume plugin为例。\n执行docker plugin create命令的目录下必须包含以下内容：\nconfig.json文件，里面是插件的配置信息，plugin config参考文档 rootfs目录，插件镜像解压后的目录。v2版本的docker plugin都是以docker镜像的方式包装的。 $ git clone https://github.com/vieux/docker-volume-sshfs $ cd docker-volume-sshfs $ go get github.com/docker/go-plugins-helpers/volume $ go build -o docker-volume-sshfs main.go $ docker build -t rootfsimage . $ id=$(docker create rootfsimage true) # id was cd851ce43a403 when the image was created $ sudo mkdir -p myplugin/rootfs $ sudo docker export \u0026#34;$id\u0026#34; | sudo tar -x -C myplugin/rootfs $ docker rm -vf \u0026#34;$id\u0026#34; $ docker rmi rootfsimage 我们可以看到sshfs的Dockerfile是这样的：\nFROM alpine RUN apk update \u0026amp;\u0026amp; apk add sshfs RUN mkdir -p /run/docker/plugins /mnt/state /mnt/volumes COPY docker-volume-sshfs docker-volume-sshfs CMD [\u0026#34;docker-volume-sshfs\u0026#34;] 实际上是编译好的可执行文件复制到alpine linux容器中运行。\n编译rootfsimage镜像的过程。\ndocker build -t rootfsimage . Sending build context to Docker daemon 11.71 MB Step 1/5 : FROM alpine ---\u0026gt; 4a415e366388 Step 2/5 : RUN apk update \u0026amp;\u0026amp; apk add sshfs ---\u0026gt; Running in 1551ecc1c847 fetch http://dl-cdn.alpinelinux.org/alpine/v3.5/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.5/community/x86_64/APKINDEX.tar.gz v3.5.2-2-ge626ce8c3c [http://dl-cdn.alpinelinux.org/alpine/v3.5/main] v3.5.1-71-gc7bb9a04f0 [http://dl-cdn.alpinelinux.org/alpine/v3.5/community] OK: 7959 distinct packages available (1/10) Installing openssh-client (7.4_p1-r0) (2/10) Installing fuse (2.9.7-r0) (3/10) Installing libffi (3.2.1-r2) (4/10) Installing libintl (0.19.8.1-r0) (5/10) Installing libuuid (2.28.2-r1) (6/10) Installing libblkid (2.28.2-r1) (7/10) Installing libmount (2.28.2-r1) (8/10) Installing pcre (8.39-r0) (9/10) Installing glib (2.50.2-r0) (10/10) Installing sshfs (2.8-r0) Executing busybox-1.25.1-r0.trigger Executing glib-2.50.2-r0.trigger OK: 11 MiB in 21 packages ---\u0026gt; 1a73c501f431 Removing intermediate container 1551ecc1c847 Step 3/5 : RUN mkdir -p /run/docker/plugins /mnt/state /mnt/volumes ---\u0026gt; Running in 032af3b2595a ---\u0026gt; 30c7e8463e96 Removing intermediate container 032af3b2595a Step 4/5 : COPY docker-volume-sshfs docker-volume-sshfs ---\u0026gt; a924c6fcc1e4 Removing intermediate container ffc5e3c97707 Step 5/5 : CMD docker-volume-sshfs ---\u0026gt; Running in 0dc938fe4f4e ---\u0026gt; 0fd2e3d94860 Removing intermediate container 0dc938fe4f4e Successfully built 0fd2e3d94860 编写config.json文档\n{ \u0026#34;description\u0026#34;: \u0026#34;sshFS plugin for Docker\u0026#34;, \u0026#34;documentation\u0026#34;: \u0026#34;https://docs.docker.com/engine/extend/plugins/\u0026#34;, \u0026#34;entrypoint\u0026#34;: [ \u0026#34;/docker-volume-sshfs\u0026#34; ], \u0026#34;env\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;DEBUG\u0026#34;, \u0026#34;settable\u0026#34;: [ \u0026#34;value\u0026#34; ], \u0026#34;value\u0026#34;: \u0026#34;0\u0026#34; } ], \u0026#34;interface\u0026#34;: { \u0026#34;socket\u0026#34;: \u0026#34;sshfs.sock\u0026#34;, \u0026#34;types\u0026#34;: [ \u0026#34;docker.volumedriver/1.0\u0026#34; ] }, \u0026#34;linux\u0026#34;: { \u0026#34;capabilities\u0026#34;: [ \u0026#34;CAP_SYS_ADMIN\u0026#34; ], \u0026#34;devices\u0026#34;: [ { \u0026#34;path\u0026#34;: \u0026#34;/dev/fuse\u0026#34; } ] }, \u0026#34;mounts\u0026#34;: [ { \u0026#34;destination\u0026#34;: \u0026#34;/mnt/state\u0026#34;, \u0026#34;options\u0026#34;: [ \u0026#34;rbind\u0026#34; ], \u0026#34;source\u0026#34;: \u0026#34;/var/lib/docker/plugins/\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bind\u0026#34; } ], \u0026#34;network\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host\u0026#34; }, \u0026#34;propagatedmount\u0026#34;: \u0026#34;/mnt/volumes\u0026#34; } 该插件使用host网络类型，使用/run/docker/plugins/sshfs.sock接口与docker engine通信。\n注意官网上的这个文档有问题，config.json与代码里的不符，尤其是Entrypoint的二进制文件的位置不对。\n注意socket配置的地址不要写详细地址，默认会在/run/docker/plugins目录下生成socket文件。\n创建plugin\n使用docker plugin create \u0026lt;plugin_name\u0026gt; /path/to/plugin/data/命令创建 …","relpermalink":"/blog/docker-plugin-develop/","summary":"看了文章后你可能会觉得，官网上的可能是个假例子。","title":"Docker 17.03-CE 插件开发案例"},{"content":"继续上一篇Docker17.03-CE插件开发的🌰 ，今天来看下docker create plugin的源码。\ncli/command/plugin/create.go\nDocker命令行docker plugin create调用的，使用的是cobra ，这个命令行工具开发包很好用，推荐下。\n执行这两个函数\nfunc newCreateCommand(dockerCli *command.DockerCli) *cobra.Command //调用下面的函数，拼装成URL调用RESTful API接口 func runCreate(dockerCli *command.DockerCli, options pluginCreateOptions) error { ... if err = dockerCli.Client().PluginCreate(ctx, createCtx, createOptions); err != nil { return err } ... } 我们再看下下面的这个文件：\napi/server/router/plugin/plugin_routes.go func (pr *pluginRouter) createPlugin(ctx context.Context, w http.ResponseWriter, r *http.Request, vars map[string]string) error { ... if err := pr.backend.CreateFromContext(ctx, r.Body, options); err != nil { return err } ... } createPlugin这个方法定义在api/server/route/plugin/backen.go的Backend接口中。\nPluginCreate这个方法定义在docker/docker/client/Interface.go的PluginAPIClient接口中。\ndocker/client/plugin_create.go\n// PluginCreate creates a plugin func (cli *Client) PluginCreate(ctx context.Context, createContext io.Reader, createOptions types.PluginCreateOptions) error { headers := http.Header(make(map[string][]string)) headers.Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/x-tar\u0026#34;) query := url.Values{} query.Set(\u0026#34;name\u0026#34;, createOptions.RepoName) resp, err := cli.postRaw(ctx, \u0026#34;/plugins/create\u0026#34;, query, createContext, headers) if err != nil { return err } ensureReaderClosed(resp) return err } plugin在后端接收到请求后会执行下面的方法。最终create plugin的实现在plugin/backend_linux.go下：\n// CreateFromContext creates a plugin from the given pluginDir which contains // both the rootfs and the config.json and a repoName with optional tag. func (pm *Manager) CreateFromContext(ctx context.Context, tarCtx io.ReadCloser, options *types.PluginCreateOptions) (err error) {} 至于docker create plugin时docker后台究竟做了什么，就看👆那个文件。\n","relpermalink":"/blog/docker-create-plugin/","summary":"今天来看下docker create plugin的源码。","title":"Docker 17.03-CE create plugin源码解析"},{"content":"最近在看 《微服务设计（Sam Newman著）》 这本书。作者是 ThoughtWorks 的Sam Newman。这本书中包括很多业界是用案例，比如 Netflix 和 亚马逊。有兴趣的话大家一起看看讨论一下。😄\nP.S 这本书比较偏理论，另外还有一本中国人写的书，《微服务架构与实践，王磊著，电子工业出版社》 。这个人同样也是 ThoughtWorks 的，两个人的观点不谋而合，依然是便理论的东西。\nCloud Native Go - 基于Go和React的web云服务构建指南\n这本书是我最近在翻译的，将由 电子工业出版社 出版，本书根据实际案例教你如何构建一个web微服务，是实践为服务架构的很好的参考。查看本书介绍 。\n1.微服务初探 什么是微服务？ 微服务（Microservices）这个词比较新颖，但是其实这种架构设计理念早就有了。微服务是一种分布式架构设计理念，为了推动细粒度服务的使用，这些服务要能协同工作，每个服务都有自己的生命周期。一个微服务就是一个独立的实体，可以独立的部署在PAAS平台上，也可以作为一个独立的进程在主机中运行。服务之间通过API访问，修改一个服务不会影响其它服务。\n微服务的好处 微服务的好处有很多，包括:\n帮助你更快的采用新技术 解决技术异构的问题，因为是用API网络通信，可以使用不同的语言和技术开发不同的服务 增强系统弹性，服务的边界比较清晰，便于故障处理 方便扩展，比如使用容器技术，可以很方便的一次性启动很多个微服务 方便部署，因为微服务之间彼此独立，所以能够独立的部署单个服务而不影响其它服务，如果部署失败的话还可以回滚 别忘了康威定律，微服务可以很好契合解决组织架构问题 可重用，可随意组合 便于维护，可以随时重写服务，不必担心历史遗留问题 与面向服务架构SOA的关系 可以说微服务架构师SOA的一种，但是目前的大多数SOA做的都不好，在通信协议的选择、第三方中间件的选择、服务力度如何划分方面做的都不够好。\n微服务与SOA的共同点\n都使用共享库，比如可重用的代码库 模块化，比如Java中的OSGI(Open Source Gateway Initiative)、Erlang中的模块化 2.架构师的职责 架构师应该关心是什么 架构师（Architect）在英文中和建筑师是同一个词，他们之间也有很多相同之处，架构师构建的是软件，而建筑师构建的是建筑。\n终于看到了我翻译的Cloud Native Go第14章中引用的这本书的原话了。\n原话 软件的需求变更是来的那么快来的那么直接，不像建筑那样可以在设计好后按照设计图纸一步步的去建设。\n架构师应该关心的是什么呢？\n保证系统适合开发人员在上面工作 关注服务之间的交互，不需要过于关注各个服务内部发生的事情，比如服务之间互相调用的接口，是使用protocol buffer呢，还是使用RESTful API，还是使用Java RMI，这个才是架构师需要关注的问题，至于服务内部究竟使用什么，那就看开发人员自己了，架构师更需要关注系统的边界和分区。 架构师应该与团队在一起，结对编程 🤓🤓 了解普通工作，知道普通的工作是什么样子，做一个代码架构师 😂 架构师应该做什么 提供原则指导实践，比如Heroku的12因素法则 用来指导SAAS应用架构一样，微服务架构设计也要有一套原则。 提供要求标准，通过日志功能和监控对服务进行集中式管理，明确接口标准，提供安全性建议。 代码治理。为开发人员提供范例和服务代码模板。 解决技术债务。 集中治理和领导。维持良好的团队关系，当团队跑偏的时候及时纠正。 3.服务建模 以MusicCorp这家公司的服务为例子讲解。\n服务建模的两个指导原则：\n高内聚：关键是找出问题的边界，把相关的问题放在同一个服务中。 松耦合：修改一个服务不需要修改另一个。 使用限定上下文（一个由显示边界限定的特定指责）的方法将服务拆分，比如MusicCorp的服务可以拆分为：\n财务部门 仓库 他们都不需要知道各自的具体实现，只要给它们提供特定的输入就会有你想要的产出。\n过早的将一个系统划分成微服务的代价非常高，尤其是在面对新领域时，将一个已有的代码库划分成微服务会比葱头开始建设微服务要简单的多。\n4.集成 使用共享数据库，为用户创建好接口，可以使用RPC（protocol buffer、thrift）或者REST。服务端和客户端消息格式可以用Json或XML。当然每种技术都有各自的适用场景，结合自己的业务选择。\n微服务的协作方式是什么样的呢？基于事件的异步通信，使用消息中间件来实现事件发布和消费者接收机制。比如用Kafka或RabbitMQ。\n5.分解单块系统 分解巨大无比没人感动的单块系统，首先要做的是理清代码库，找到接缝。\n分解系统带来的好处：\n加快以后系统开发速度 划清了团队结构（又是康威定律） 增加安全审计功能后，保障安全性 利于开展新技术 6. 部署 这一块跟传统服务的部署并没有太大的不同，无非是微服务的短平快，加快了CI（持续集成）的速度。如果将微服务打包为docker镜像，使用Jenkins、ansible、puppet等技术来部署微服务可以实现部署自动和效率的显著提高。\n其它 该书的后面还讲了测试、监控、安全、康威定律、最后还上升到人本，给予广大的软件开发人员强烈的人文关怀，可见提倡架构师要融入团队，最一个代码架构师和结对编程的作者是多么博爱❤️。\n该书的核心部分是第11章规模化微服务，为将在下篇中来探讨一下。\n","relpermalink":"/blog/microservice-reading-notes/","summary":"这本书中包括很多业界是用案例，比如Netflix和亚马逊。有兴趣的话大家一起看看讨论一下。","title":"微服务设计读书笔记"},{"content":"本文是Docker v.s Kubernetes 系列第二篇，续接上文Docker对比Kuberntes第一部分 。\nKubernetes是典型的Master/Slave架构模式，本文简要的介绍kubenetes的架构和组件构成。\nKubernetes核心架构 master节点 apiserver：作为kubernetes系统的入口，封装了核心对象的增删改查操作，以RESTFul接口方式提供给外部客户和内部组件调用。它维护的REST对象将持久化到etcd（一个分布式强一致性的key/value存储）。 scheduler：负责集群的资源调度，为新建的Pod分配机器。这部分工作分出来变成一个组件，意味着可以很方便地替换成其他的调度器。 controller-manager：负责执行各种控制器，目前有两类： endpoint-controller：定期关联service和Pod(关联信息由endpoint对象维护)，保证service到Pod的映射总是最新的。 replication-controller：定期关联replicationController和Pod，保证replicationController定义的复制数量与实际运行Pod的数量总是一致的。 node节点 kubelet：负责管控docker容器，如启动/停止、监控运行状态等。它会定期从etcd获取分配到本机的Pod，并根据Pod信息启动或停止相应的容器。同时，它也会接收apiserver的HTTP请求，汇报Pod的运行状态。 proxy：负责为Pod提供代理。它会定期从etcd获取所有的service，并根据service信息创建代理。当某个客户Pod要访问其他Pod时，访问请求会经过本机proxy做转发。 master slave架构 Kubernetes组件详细介绍 etcd 虽然不是Kubernetes的组件但是有必要提一下，etcd是一个分布式协同数据库，基于Go语言开发，CoreOS公司出品，使用raft一致性算法协同。Kubernetes的主数据库，在安装kubernetes之前就要先安装它，很多开源下项目都用到，老版本的docker swarm也用到了它。目前主要使用的是2.7.x版本，3.0+版本的API变化太大。\nAPIServer APIServer负责对外提供kubernetes API服务，它运行在master节点上。任何对资源的增删改查都要交给APIServer处理后才能提交给etcd。APIServer总体上由两部分组成：HTTP/HTTPS服务和一些功能性插件。这些功能性插件又分为两种：一部分与底层IaaS平台（Cloud Provide）相关；另一部分与资源管理控制（Admission Control）相关。\nScheduler Scheduler的作用是根据特定的调度算法将pod调度到node节点上，这一过程也被称为绑定。Scheduler调度器的输入是待调度的pod和可用的工作节点列表，输出则是一个已经绑定了pod的节点，这个节点是通过调度算法在工作节点列表中选择的最优节点。\n工作节点从哪里来？工作节点并不是由Kubernetes创建，它是由IaaS平台创建，或者就是由用户管理的物理机或者虚拟机。但是Kubernetes会创建一个Node对象，用来描述这个工作节点。描述的具体信息由创建Node对象的配置文件给出。一旦用户创建节点的请求被成功处理，Kubernetes又会立即在内部创建一个node对象，再去检查该节点的健康状况。只有那些当前可用的node才会被认为是一个有效的节点并允许pod调度到上面运行。\n工作节点可以通过资源配置文件或者kubectl命令行工具来创建。Kubernetes主要维护工作节点的两个属性：spec和status来描述一个工作节点的期望状态和当前状态。其中，所谓的当前状态信息由3个信息组成：HostIp、NodePhase和Node Condition。\n工作节点的动态维护过程依靠Node Controller来完成，它是Kubernetes Controller Manager下属的一个控制器。它会一直不断的检查Kubernetes已知的每台node节点是否正常工作，如果一个之前已经失败的节点在这个检查循环中被检查为可以工作的，那么Node Controller会把这个节点添加到工作节点中，Node Controller会从工作节点中删除这个节点。\nController Manager Controller Manager运行在集群的Master节点上，是基于pod API的一个独立服务，它重点实现service Endpoint（服务端点）的动态更新。管理着Kubernetes集群中各种控制节点，包括replication Controller和node Controller。\n与APIServer相比，APIServer负责接受用户请求并创建对应的资源，而Controller Manager在系统中扮演的角色是在一旁旁默默的管控这些资源，确保他们永远保持在预期的状态。它采用各种管理器定时的对pod、节点等资源进行预设的检查，然后判断出于预期的是否一致，若不一致，则通知APIServer采取行动，比如重启、迁移、删除等。\nkubelet kubelet组件工作在Kubernetes的node上，负责管理和维护在这台主机上运行着的所有容器。 kubelet与cAdvisor交互来抓取docker容器和主机的资源信息。 kubelet垃圾回收机制，包括容器垃圾回收和镜像垃圾回收。 kubelet工作节点状态同步。\nkube-proxy kube-proxy提供两种功能:\n提供算法将客服端流量负载均衡到service对应的一组后端pod。 使用etcd的watch机制，实现服务发现功能，维护一张从service到endpoint的映射关系，从而保证后端pod的IP变化不会对访问者的访问造成影响。 ","relpermalink":"/blog/docker-vs-kubernetes-part2/","summary":"这一系列文章是对比kubernetes 和docker两者之间的差异。","title":"Docker对比Kubernetes第二部分"},{"content":"前言 这一系列文章是对比 kubernetes 和 docker 两者之间的差异，鉴于我之前从 docker1.10.3 起开始使用 docker，对原生 docker 的了解比较多，最近又正在看《Kunernetes 权威指南（第二版）》 这本书（P.S 感谢电子工业出版社的编辑朋友赠送此书）。这系列文章不是为了比较孰优孰劣，适合自己的才是最好的。\n此系列文章中所说的 Docker 指的是 17.03-ce 版本。\n概念性的差别 Kubernetes\n了解一样东西首先要高屋建瓴的了解它的概念，kubernetes 包括以下几种资源对象：\nPod Service Volume Namespace ReplicaSet Deployment StatefulSet DaemonSet Job Docker\nDocker 的资源对象相对于 kubernetes 来说就简单多了，只有以下几个：\nService Node Stack Docker 就这么简单，使用一个 docker-compose.yml 即可以启动一系列服务。当然简单的好处是便于理解和管理，但是在功能方面就没有 kubernetes 那么强大了。\n功能性差别 Kubernetes 资源限制 CPU 100m 千分之一核为单位，绝对值，requests 和 limits，超过这个值可能被杀掉，资源限制力度比 docker 更细。 Pod 中有个最底层的 pause 容器，其他业务容器共用他的 IP，docker 因为没有这层概念，所以没法共用 IP，而是使用 overlay 网络同处于一个网络里来通信。 Kubernetes 在 rc 中使用环境变量传递配置（1.3 版本是这样的，后续版本还没有研究过） Kuberentes Label 可以在开始和动态的添加修改，所有的资源对象都有，这一点 docker 也有，但是资源调度因为没有 kubernetes 那么层级，所有还是相对比较弱一些。 Kubernetes 对象选择机制继续通过 label selector，用于对象调度。 Kubernetes 中有一个比较特别的镜像，叫做 google_containers/pause，这个镜像是用来实现 Pod 概念的。 HPA horizontal pod autoscaling 横向移动扩容，也是一种资源对象，根据负载变化情况针对性的调整 pod 目标副本数。 Kubernetes 中有三个 IP，Node,Pod,Cluster IP 的关系比较复杂，docker 中没有 Cluster IP 的概念。 持久化存储，在 Kubernetes 中有 Persistent volume 只能是网络存储，不属于任何 node，独立于 pod 之外，而 docker 只能使用 volume plugin。 多租户管理，kubernetes 中有 `Namespace，docker 暂时没有多租户管理功能。 总体来说 Docker 架构更加简单，使用起来也没有那么多的配置，只需要每个结点都安装 docker 即可，调度和管理功能没 kubernetes 那么复杂。但是 kubernetes 本身就是一个通用的数据中心管理工具，不仅可以用来管理 docker，pod 这个概念里就可以运行不仅是 docker 了。\n以后的文章中将结合 docker 着重讲 Kubernetes，基于 1.3 版本。\n","relpermalink":"/blog/docker-vs-kubernetes-part1/","summary":"这一系列文章是对比kubernetes 和docker两者之间的差异。","title":"Docker对比Kubernetes第一部分"},{"content":"继续趟昨天挖的坑。\n昨天的issue-776 已经得到@gkvijay的回复，原来是因为没有安装contiv/v2plugin的缘故，所以create contiv network失败，我需要自己build一个docker plugin。\n查看下这个commit 里面有build v2plugin的脚本更改，所以直接调用以下命令就可以build自己的v2plugin。\n前提你需要先build出netctl、netmaster、netplugin三个二进制文件并保存到bin目录下，如果你没自己build直接下载release里面的文件保存进去也行。\n编译v2plugin插件 修改config.json插件配置文件\n{ \u0026#34;manifestVersion\u0026#34;: \u0026#34;v0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Contiv network plugin for Docker\u0026#34;, \u0026#34;documentation\u0026#34;: \u0026#34;https://contiv.github.io\u0026#34;, \u0026#34;entrypoint\u0026#34;: [\u0026#34;/startcontiv.sh\u0026#34;], \u0026#34;network\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host\u0026#34; }, \u0026#34;env\u0026#34;: [ { \u0026#34;Description\u0026#34;: \u0026#34;To enable debug mode, set to \u0026#39;-debug\u0026#39;\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;dbg_flag\u0026#34;, \u0026#34;Settable\u0026#34;: [ \u0026#34;value\u0026#34; ], \u0026#34;Value\u0026#34;: \u0026#34;-debug\u0026#34; }, { \u0026#34;Description\u0026#34;: \u0026#34;VLAN uplink interface used by OVS\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;iflist\u0026#34;, \u0026#34;Settable\u0026#34;: [ \u0026#34;value\u0026#34; ], \u0026#34;Value\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;Description\u0026#34;: \u0026#34;Etcd or Consul cluster store url\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;cluster_store\u0026#34;, \u0026#34;Settable\u0026#34;: [ \u0026#34;value\u0026#34; ], \u0026#34;Value\u0026#34;: \u0026#34;etcd://172.20.0.113:2379\u0026#34; }, { \u0026#34;Description\u0026#34;: \u0026#34;Local IP address to be used by netplugin for control communication\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;ctrl_ip\u0026#34;, \u0026#34;Settable\u0026#34;: [ \u0026#34;value\u0026#34; ], \u0026#34;Value\u0026#34;: \u0026#34;none\u0026#34; }, { \u0026#34;Description\u0026#34;: \u0026#34;Local VTEP IP address to be used by netplugin\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;vtep_ip\u0026#34;, \u0026#34;Settable\u0026#34;: [ \u0026#34;value\u0026#34; ], \u0026#34;Value\u0026#34;: \u0026#34;none\u0026#34; }, { \u0026#34;Description\u0026#34;: \u0026#34;In \u0026#39;master\u0026#39; role, plugin runs netmaster and netplugin\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;plugin_role\u0026#34;, \u0026#34;Settable\u0026#34;: [ \u0026#34;value\u0026#34; ], \u0026#34;Value\u0026#34;: \u0026#34;master\u0026#34; }, { \u0026#34;Description\u0026#34;: \u0026#34;Netmaster url to listen http requests on\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;listen_url\u0026#34;, \u0026#34;Settable\u0026#34;: [ \u0026#34;value\u0026#34; ], \u0026#34;Value\u0026#34;: \u0026#34;172.20.0.113:9999\u0026#34; }, { \u0026#34;Description\u0026#34;: \u0026#34;Network Driver name for requests to dockerd. Should be same as name:tag of the plugin\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;plugin_name\u0026#34;, \u0026#34;Settable\u0026#34;: [ \u0026#34;value\u0026#34; ], \u0026#34;Value\u0026#34;: \u0026#34;contiv/v2plugin:latest\u0026#34; } ], \u0026#34;mounts\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;bind\u0026#34;, \u0026#34;options\u0026#34;: [\u0026#34;rbind\u0026#34;], \u0026#34;source\u0026#34;: \u0026#34;/etc/openvswitch\u0026#34;, \u0026#34;destination\u0026#34;: \u0026#34;/etc/openvswitch\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;bind\u0026#34;, \u0026#34;options\u0026#34;: [\u0026#34;rbind\u0026#34;], \u0026#34;source\u0026#34;: \u0026#34;/var/log/openvswitch\u0026#34;, \u0026#34;destination\u0026#34;: \u0026#34;/var/log/openvswitch\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;bind\u0026#34;, \u0026#34;options\u0026#34;: [\u0026#34;rbind\u0026#34;], \u0026#34;source\u0026#34;: \u0026#34;/var/run\u0026#34;, \u0026#34;destination\u0026#34;: \u0026#34;/var/run\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;bind\u0026#34;, \u0026#34;options\u0026#34;: [\u0026#34;rbind\u0026#34;], \u0026#34;source\u0026#34;: \u0026#34;/lib/modules\u0026#34;, \u0026#34;destination\u0026#34;: \u0026#34;/lib/modules\u0026#34; } ], \u0026#34;interface\u0026#34; : { \u0026#34;types\u0026#34;: [\u0026#34;docker.networkdriver/1.0\u0026#34;, \u0026#34;docker.ipamdriver/1.0\u0026#34;], \u0026#34;socket\u0026#34;: \u0026#34;netplugin.sock\u0026#34; }, \u0026#34;Linux\u0026#34;: { \u0026#34;Capabilities\u0026#34;: [\u0026#34;CAP_SYS_ADMIN\u0026#34;, \u0026#34;CAP_NET_ADMIN\u0026#34;, \u0026#34;CAP_SYS_MODULE\u0026#34;] } } 关于docker plugin v2配置文件的说明 方法一\n自动化make\n$make host-pluginfs-create 方法二\n直接调用Makefile里指定的那个shell脚本scripts/v2plugin_rootfs.sh。\n$bash scripts/v2plugin_rootfs Creating rootfs for v2plugin , sed: 1: \u0026#34;install/v2plugin/config ...\u0026#34;: command i expects \\ followed by text Sending build context to Docker daemon 73.94 MB Step 1/5 : FROM alpine:3.5 ---\u0026gt; 4a415e366388 Step 2/5 : MAINTAINER Cisco Contiv (http://contiv.github.io/) ---\u0026gt; Running in fada1677341b ---\u0026gt; f0440792dff6 Removing intermediate container fada1677341b Step 3/5 : RUN mkdir -p /run/docker/plugins /etc/openvswitch /var/run/contiv/log \u0026amp;\u0026amp; echo \u0026#39;http://dl-cdn.alpinelinux.org/alpine/v3.4/main\u0026#39; \u0026gt;\u0026gt; /etc/apk/repositories \u0026amp;\u0026amp; apk update \u0026amp;\u0026amp; apk add openvswitch=2.5.0-r0 iptables ---\u0026gt; Running in 2ae2fbee6834 fetch http://dl-cdn.alpinelinux.org/alpine/v3.5/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.5/community/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.4/main/x86_64/APKINDEX.tar.gz v3.5.2-3-g3649125268 [http://dl-cdn.alpinelinux.org/alpine/v3.5/main] v3.5.1-71-gc7bb9a04f0 [http://dl-cdn.alpinelinux.org/alpine/v3.5/community] v3.4.6-81-g1f1f409 [http://dl-cdn.alpinelinux.org/alpine/v3.4/main] OK: 13194 distinct packages available (1/6) Installing libmnl (1.0.4-r0) (2/6) Installing libnftnl-libs (1.0.7-r0) (3/6) Installing iptables (1.6.0-r0) (4/6) Installing libcrypto1.0 (1.0.2k-r0) (5/6) Installing libssl1.0 (1.0.2k-r0) (6/6) Installing openvswitch (2.5.0-r0) Executing busybox-1.25.1-r0.trigger OK: 19 MiB in 17 packages ---\u0026gt; b130141ad660 Removing intermediate container 2ae2fbee6834 Step 4/5 : COPY netplugin netmaster netctl startcontiv.sh / ---\u0026gt; 2b88b2f8e5e7 Removing intermediate container d7580a394c64 Step 5/5 : ENTRYPOINT /startcontiv.sh ---\u0026gt; Running in e6fc5c887cb3 ---\u0026gt; 1c569e4c633d Removing intermediate container e6fc5c887cb3 Successfully built 1c569e4c633d Password: 03d60dc01488362156f98a062d17af7a34e4b17569c2fe4f5d2048d619860314 Untagged: contivrootfs:latest Deleted: sha256:1c569e4c633d27bd3e79d9d30b2825ce57452d30f90a3452304b932835331b13 Deleted: sha256:2b88b2f8e5e7bae348bf296f6254662c1d444760db5acd1764b9c955b106adad Deleted: sha256:b60594671dc9312bf7ba73bf17abb9704d2b0d0e802c0d990315c5b4a5ca11fe Deleted: sha256:b130141ad660d4ee291d9eb9a1e0704c4bc009fc91a73de28e8fd110aa45c481 Deleted: sha256:ab3c02d5a171681ba00d27f2c456cf8b63eeeaf408161dc84d9d89526d0399de Deleted: sha256:f0440792dff6a89e321cc5d34ecaa21b4cb993f0c4e4df6c2b04eef8878bb471 创建镜像这一步需要输入你的docker hub密码。而且alpine下载软件需要翻墙的。打包v2plugin目录需要使用sudo，不然会报一个错。\n整个插件打包压缩后的大小是91M。现在rootfs和config.json都已经有了，就可以在你自己的系统上create docker plugin了。\n启动contiv plugin 创建docker network plugin并enable。\n$docker plugin create contiv/v2plugin . contiv/v2plugin $docker …","relpermalink":"/blog/contiv-v2plugin/","summary":"继续趟昨天挖的坑。","title":"Contiv入坑指南-v2plugin"},{"content":"Contiv是思科开发的docker网络插件，从2015年就开源了，业界通常拿它和Calico比较。貌似Contiv以前还开发过volume plugin，现在销声匿迹了，只有netplugin仍在活跃开发。\n容器网络插件 Calico 与 Contiv Netplugin深入比较 。\n还有篇文章讲解了docker网络方案的改进 。\nContiv Netplugin 简介 Contiv Netplugin 是来自思科的解决方案。编程语言为 Go。它基于 OpenvSwitch，以插件化的形式支持容器访问网络，支持 VLAN，Vxlan，多租户，主机访问控制策略等。作为思科整体支持容器基础设施contiv项目的网络部分，最大的亮点在于容器被赋予了 SDN 能力，实现对容器更细粒度，更丰富的访问控制功能。另外，对 Docker CNM 网络模型的支持，并内置了 IPAM 接口，不仅仅提供了一容器一 IP，而且容器的网络信息被记录的容器配置中，伴随着容器的整个生命周期，减低因为状态不同步造成网络信息丢失的风险。有别于 CNI，这种内聚化的设计有利于减少对第三方模块的依赖。随着项目的发展，除了 Docker，还提供了对 Kubernetes 以及 Mesos 的支持，即 CNI 接口。\nnetplugin 架构 Netmaster 后台进程负责记录所有节点状态，保存网络信息，分配 IP 地址 Netplugin 后台进程作为每个宿主机上的 Agent 与 Docker 及 OVS 通信，处理来自 Docker 的请求，管理 OVS。Docker 方面接口为 remote driver，包括一系列 Docker 定义的 JSON-RPC(POST) 消息。OVS 方面接口为 remote ovsdb，也是 JSON-RPC 消息。以上消息都在 localhost 上处理。 集群管理依赖 etcd/serf netplugin 代码结构 Netplugin的优势 较早支持CNM模型。与已有的网络基础设施兼容性较高，改造影响小。基于VLAN的平行扩展与现有网络结构地位对等 SDN能力，能够对容器的网络访问做更精细的控制 多租户支持，具备未来向混合云/公有云迁移的潜力 代码规模不大，逻辑结构清晰，并发好，VLAN在公司内部有开发部署运维实践经验，稳定性经过生产环境验证 京东基于相同的技术栈（OVS + VLAN）已支持10w+ 容器的运行。 Next 后续文章会讲解contiv netplugin的环境配置和开发。目前还在1.0-beta版本。Docker store上提供了contiv插件的下载地址 。\n","relpermalink":"/blog/contiv-guide/","summary":"Contiv是思科开发的docker网络插件，从2015年就开源了，业界通常拿它和Calico比较。","title":"思科开源docker网络插件Contiv简介"},{"content":"起源 久闻Vagrant大名，之前经常看到有开源项目使用它作为分布式开发的环境配置。\n因为今天在看contiv 正好里面使用vagrant搭建的开发测试环境，所以顺便了解下。它的Vagrantfile 文件中定义了三台主机。并安装了很多依赖软件，如consul、etcd、docker、go等，整的比较复杂。\n➜ netplugin git:(master) ✗ vagrant status Current machine states: netplugin-node1 running (virtualbox) netplugin-node2 running (virtualbox) netplugin-node3 running (virtualbox) This environment represents multiple VMs. The VMs are all listed above with their current state. For more information about a specific VM, run `vagrant status NAME`. Vagrant是hashicorp 这家公司的产品，这家公司主要做数据中心PAAS和虚拟化，其名下大名鼎鼎的产品有Consul、Vault、Nomad、Terraform。他们的产品都是基于Open Source的Github地址 。\n用途 Vagrant是用来管理虚拟机的，如VirtualBox、VMware、AWS等，主要好处是可以提供一个可配置、可移植和复用的软件环境，可以使用shell、chef、puppet等工具部署。所以vagrant不能单独使用，如果你用它来管理自己的开发环境的话，必须在自己的电脑里安装了虚拟机软件，我使用的是virtualbox。\nVagrant提供一个命令行工具vagrant，通过这个命令行工具可以直接启动一个虚拟机，当然你需要提前定义一个Vagrantfile文件，这有点类似Dockerfile之于docker了。\n跟docker类比这来看vagrant就比较好理解了，vagrant也是用来提供一致性环境的，vagrant本身也提供一个镜像源，使用vagrant init hashicorp/precise64就可以初始化一个Ubuntu 12.04的镜像。\n用法 你可以下载安装文件来安装vagrant，也可以使用RubyGem安装，它是用Ruby开发的。\nVagrantfile\nVagrantfile是用来定义vagrant project的，使用ruby语法，不过你不必了解ruby就可以写一个Vagrantfile。\n看个例子，选自https://github.com/fenbox/Vagrantfile\n# -*- mode: ruby -*- # vi: set ft=ruby : # All Vagrant configuration is done below. The \u0026#34;2\u0026#34; in Vagrant.configure # configures the configuration version (we support older styles for # backwards compatibility). Please don\u0026#39;t change it unless you know what # you\u0026#39;re doing. Vagrant.configure(\u0026#34;2\u0026#34;) do |config| # The most common configuration options are documented and commented below. # For a complete reference, please see the online documentation at # https://docs.vagrantup.com. # Every Vagrant development environment requires a box. You can search for # boxes at https://atlas.hashicorp.com/search. config.vm.box = \u0026#34;ubuntu/trusty64\u0026#34; # Disable automatic box update checking. If you disable this, then # boxes will only be checked for updates when the user runs # `vagrant box outdated`. This is not recommended. # config.vm.box_check_update = false # Create a forwarded port mapping which allows access to a specific port # within the machine from a port on the host machine. In the example below, # accessing \u0026#34;localhost:8080\u0026#34; will access port 80 on the guest machine. # config.vm.network \u0026#34;forwarded_port\u0026#34;, guest: 80, host: 8080 # Create a private network, which allows host-only access to the machine # using a specific IP. config.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.10\u0026#34; # Create a public network, which generally matched to bridged network. # Bridged networks make the machine appear as another physical device on # your network. # config.vm.network \u0026#34;public_network\u0026#34; # Share an additional folder to the guest VM. The first argument is # the path on the host to the actual folder. The second argument is # the path on the guest to mount the folder. And the optional third # argument is a set of non-required options. # config.vm.synced_folder \u0026#34;../data\u0026#34;, \u0026#34;/vagrant_data\u0026#34; # Provider-specific configuration so you can fine-tune various # backing providers for Vagrant. These expose provider-specific options. # Example for VirtualBox: # # config.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| # # Display the VirtualBox GUI when booting the machine # vb.gui = true # # # Customize the amount of memory on the VM: # vb.memory = \u0026#34;1024\u0026#34; # end # # View the documentation for the provider you are using for more # information on available options. # Define a Vagrant Push strategy for pushing to Atlas. Other push strategies # such as FTP and Heroku are also available. See the documentation at # https://docs.vagrantup.com/v2/push/atlas.html for more information. # config.push.define \u0026#34;atlas\u0026#34; do |push| # push.app = \u0026#34;YOUR_ATLAS_USERNAME/YOUR_APPLICATION_NAME\u0026#34; # end # Enable provisioning with a shell script. Additional provisioners such as # Puppet, Chef, Ansible, Salt, and Docker are also available. Please see the # documentation for more information about their specific syntax and use. # config.vm.provision \u0026#34;shell\u0026#34;, inline: \u0026lt;\u0026lt;-SHELL # apt-get update # apt-get install -y apache2 # SHELL config.vm.provision :shell, path: \u0026#34;bootstrap.sh\u0026#34; end Boxes\nVagrant的基础镜像，相当于docker images。可以在这些基础镜像的基础上制作自己的虚拟机镜像。\n添加一个box\n$ vagrant box add hashicorp/precise64 在Vagrantfile中指定box\nVagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.box = \u0026#34;hashicorp/precise64\u0026#34; config.vm.box_version = \u0026#34;1.1.0\u0026#34; end 使用ssh进入vagrant\nvagrant up后就可以用vagrant ssh $name进入虚拟机内，如果主机上就一个vagrant可以不指定名字。默认进入的用户是vagrant。\n文件同步\nvagrant up后在虚拟机中会有一个/vagrant目录，这跟你定义Vagrantfile是同一级目录。\n这个目录跟你宿主机上的目录文件是同步的。\n软件安装\n在Vagrantfile中定义要安装的软件和操作。\n例如安装apache\n在与Vagrantfile同级的目录下创建一个bootstrap.sh文件。\n#!/usr/bin/env bash apt-get update apt-get install -y apache2 if ! [ -L /var/www ]; then rm -rf /var/www ln -fs /vagrant /var/www fi 然后在Vagrantfile中使用它。\nVagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.box = \u0026#34;hashicorp/precise64\u0026#34; config.vm.box_version = \u0026#34;1.1.0\u0026#34; end 网络\n端口转发\nVagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.box = \u0026#34;hashicorp/precise64\u0026#34; config.vm.provision :shell, path: \u0026#34;bootstrap.sh\u0026#34; …","relpermalink":"/blog/vagrant-intro/","summary":"我在一年内使用Vagrant的心路历程。","title":"Vagrant从使用到放弃再到掌握完全指南"},{"content":"回顾历史 多少次我回过头看看走过的路，你还在小村旁。\n去年基于 docker1.11 对 Hadoop yarn 进行了 docker 化改造，我将这个项目命名为 magpie ，因为它就像是喜鹊一样收集着各种各样的资源搭建自己的小窝。magpie 还是有很多事情可以做的，大数据集群的虚拟化也不会止步，它仅仅是对其做了初步的探索，对于资源利用率和管理方面的优化还有很长的路要走，Yarn 本身就是做为大数据集群的资源管理调度角色出现的，一开始是为调度 MapReduce，后来的 spark、hive、tensrflow、slide 等等不一而足陆续出现。但是用它来管理 docker 似乎还是有点过重，还不如用 kubernetes、marathon、nomad、swarm 等。\n但是在微服务方面 docker1.11 的很多弊端或者说缺点就暴露了出来，首先 docker1.11 原生并不带 cluster 管理，需要配合 docker swarm、kubernetes、marathon 等才能管理 docker 集群。之前的对于 docker 的使用方式基本就是按照虚拟机的方式使用的，固定 IP 有悖于微服务的原则。\n我们基于 docker1.11 和 shrike 二层网络模式，还有 shipyard 来做集群管理，shipyard 只是一个简单的 docker 集群管理的 WebUI，基本都是调用 docker API，唯一做了一点 docker 原生没有的功能就是 scale 容器，而且只支持到 docker1.11，早已停止开发。我抛弃了 shipyard，它的页面功能基本可有可无，我自己开发的 magpie 一样可以管理 yarn on docker 集群。\nDocker Swarm 有如下几个缺点\n对于大规模集群的管理效率太低，当管理上百个 node 的时候经常出现有节点状态不同步的问题，比如主机重启后容器已经 Exited 了，但是 master 让然认为是 Running 状态，必须重启所有 master 节点才行。 没有中心化 Node 管理功能，必须登录到每台 node 上手动启停 swarm-agent。 集群管理功能实在太太太简陋，查看所有 node 状态只能用 docker info 而且那个格式就不提了，shipyard 里有处理这个格式的代码，我 copy 到了 magpie 里，彻底抛弃 shipyard 了。 Docker swarm 的集群管理概念缺失，因为 docker 一开始设计的时候就不是用来管理集群的，所以出现了 swarm，但是只能使用 docker-compose 来编排服务，但是无法在 swarm 集群中使用我们自定义的 mynet 网络，compose issue-4233 ，compose 也已经被 docker 官方废弃（最近一年 docker 发展的太快了，原来用 python 写的 compose 已经被用 go 重构为 libcompose 直接集成到 swarm mode 里了），而且 docker1.11 里也没有像 kubernetes 那样 service 的单位，在 docker1.11 所有的管理都是基于 docker 容器的。 Docker Swarm 的问题也是 shipyard 的问题，谁让 shipyard 直接调用 docker 的 API 呢。当然，在后续版本的 docker 里以上问题都已经不是问题，docker 已经越来越像 kubernetes，不论是在设计理念上还是在功能上，甚至还发行了企业版，以后每个月发布一个版本。\n技术选型 主要对比 Docker1.11 和 Docker17.03-ce 版本。\n首先有一点需要了解的是，docker1.12 + 带来的 swarm mode ，你可以使用一个命令直接启动一个复杂的 stack ，其中包括了服务编排和所有的服务配置，这是一个投票应用的例子 。\n下表对比了 docker1.11 和 docker17.03-ce\n版本 docker1.11 docker17.03-ce 基本单位 docker 容器 docker 容器、service、stack 服务编排 compose，不支持 docker swarm 的 mynet 网络 改造后的 compose，支持 stack 中完整的服务编排 网络模型 Host、bridge、overlay、mynet 默认支持跨主机的 overlay 网络，创建单个容器时也可以 attach 到已有的 overla 网络中 插件 没有插件管理命令，但是可以手动创建和管理 有插件管理命令，可以手动创建和从 docker hub 中下载，上传插件到自己的私有镜像仓库 升级 不支持平滑升级，重启 docker 原来的容器也会停掉 可以停止 docker engine 但不影响已启动的容器 弹性伸缩 不支持 service 内置功能 服务发现 监听 docker event 增删 DNS 内置服务发现，根据 DNS 负载均衡 节点管理 手动启停 中心化管理 node 节点 服务升级 手动升级 service 内置功能 负载均衡 本身不支持 Swarm mode 内部 DNS 轮寻 基于以上对比，使用 docker17.03-ce 不仅可以兼容以前的 mynet 网络模式，只需要重构以前的 shrike 为 docker plugin，在创建 service 的时候指定为 mynet 即可。也可以同时使用 docker mode 的 overlay 网络，而且还可以安装其它 docker plugin 首先更高级网络和 volume 功能。\nDocker17.03-ce 借鉴了很多 kubernetes 的设计理念，docker 发力企业级市场，相信新版的才符合微服务的方向，既能兼容以前的虚拟机式的使用模式，也能兼容微服务架构。\n下一步 之前考虑过使用 docker1.11 + compose + shipyard + eureka + nginx 等做微服务架构，但是考虑到最新版 docker 的重大升级，从长远的眼光来看，不能一直限定于之前的那一套，我更倾向于新版本。\n调研 Docker17.03-ce 的新特性，尤其是服务治理方面 结合具体业务试用 重构 shrike 为 docker plugin Don’t speak, I’ll try to save us from ourselves\nIf were going down, we’re going down in flames\n","relpermalink":"/blog/docker-tech-selection/","summary":"本文讲述如何进行 Docker 的版本选择。","title":"如何选择 Docker 版本？"},{"content":"看了下网上其他人写的 docker 开发环境搭建，要么是在 ubuntu 下搭建，要么就是使用官方说明的 build docker-dev 镜像的方式一步步搭建的，甚是繁琐，docker hub 上有一个 docker 官方推出的 dockercore/docker 镜像，其实这就是官网上所说的 docker-dev 镜像，不过以前的那个 deprecated 了，使用目前这个镜像搭建 docker 开发环境是最快捷的了。\n想要修改 docker 源码和做 docker 定制开发的同学可以参考下。\n官方指导文档 设置 docker 开发环境 docker 的编译实质上是在 docker 容器中运行 docker。\n因此在本地编译 docker 的前提是需要安装了 docker，还需要用 git 把代码 pull 下来。\n创建分支 为了方便以后给 docker 提交更改，我们从 docker 官方 fork 一个分支。\ngit clone https://github.com/rootsongjc/docker.git git config --local user.name \u0026#34;Jimmy Song\u0026#34; git config --local user.email \u0026#34;rootsongjc@gmail.com\u0026#34; git remote add upstream https://github.com/docker/docker.git git config --local -l git remote -v git checkout -b dry-run-test touch TEST.md vim TEST.md git status git add TEST.md git commit -am \u0026#34;Making a dry run test.\u0026#34; git push --set-upstream origin dry-run-test 然后就可以在 dry-run-test 这个分支下工作了。\n配置 docker 开发环境 官网 上说需要先清空自己电脑上已有的容器和镜像。\ndocker 开发环境本质上是创建一个 docker 镜像，镜像里包含了 docker 的所有开发运行环境，本地代码通过挂载的方式放到容器中运行，下面这条命令会自动创建这样一个镜像。\n在 dry-run-test 分支下执行\nmake BIND_DIR=. shell 该命令会自动编译一个 docker 镜像，From debian:jessie。这一步会上网下载很多依赖包，速度比较慢。如果翻不了墙的话肯定都会失败。因为需要下载的软件和安装包都是在国外服务器上，不翻墙根本就下载不下来，为了不用这么麻烦，推荐直接使用 docker 官方的 dockercore/docker 镜像，也不用以前的 docker-dev 镜像，那个造就废弃了。这个镜像大小有 2.31G。\ndocker pull dockercore/docker 使用方法见 docker hub 然后就可以进入到容器里\ndocker run --rm -i --privileged -e BUILDFLAGS -e KEEPBUNDLE -e DOCKER_BUILD_GOGC -e DOCKER_BUILD_PKGS -e DOCKER_CLIENTONLY -e DOCKER_DEBUG -e DOCKER_EXPERIMENTAL -e DOCKER_GITCOMMIT -e DOCKER_GRAPHDRIVER=devicemapper -e DOCKER_INCREMENTAL_BINARY -e DOCKER_REMAP_ROOT -e DOCKER_STORAGE_OPTS -e DOCKER_USERLANDPROXY -e TESTDIRS -e TESTFLAGS -e TIMEOUT -v \u0026#34;/Users/jimmy/Workspace/github/rootsongjc/docker/bundles:/go/src/github.com/docker/docker/bundles\u0026#34; -t \u0026#34;dockercore/docker:latest\u0026#34; bash 按照官网的说明 make 会报错\nroot@f2753f78bb6d:/go/src/github.com/docker/docker# ./hack/make.sh binary error: .git directory missing and DOCKER_GITCOMMIT not specified Please either build with the .git directory accessible, or specify the exact (--short) commit hash you are building using DOCKER_GITCOMMIT for future accountability in diagnosing build issues. Thanks! 这是一个 issue-27581 ，解决方式就是在 make 的时候手动指定 DOCKER_GITCOMMIT。\nroot@f2753f78bb6d:/go/src/github.com/docker/docker# DOCKER_GITCOMMIT=3385658 ./hack/make.sh binary ---\u0026gt; Making bundle: binary (in bundles/17.04.0-dev/binary) Building: bundles/17.04.0-dev/binary-client/docker-17.04.0-dev Created binary: bundles/17.04.0-dev/binary-client/docker-17.04.0-dev Building: bundles/17.04.0-dev/binary-daemon/dockerd-17.04.0-dev Created binary: bundles/17.04.0-dev/binary-daemon/dockerd-17.04.0-dev Copying nested executables into bundles/17.04.0-dev/binary-daemon bundles 目录下会生成如下文件结构\n. ├── 17.04.0-dev │ ├── binary-client │ │ ├── docker -\u0026gt; docker-17.04.0-dev │ │ ├── docker-17.04.0-dev │ │ ├── docker-17.04.0-dev.md5 │ │ └── docker-17.04.0-dev.sha256 │ └── binary-daemon │ ├── docker-containerd │ ├── docker-containerd-ctr │ ├── docker-containerd-ctr.md5 │ ├── docker-containerd-ctr.sha256 │ ├── docker-containerd-shim │ ├── docker-containerd-shim.md5 │ ├── docker-containerd-shim.sha256 │ ├── docker-containerd.md5 │ ├── docker-containerd.sha256 │ ├── docker-init │ ├── docker-init.md5 │ ├── docker-init.sha256 │ ├── docker-proxy │ ├── docker-proxy.md5 │ ├── docker-proxy.sha256 │ ├── docker-runc │ ├── docker-runc.md5 │ ├── docker-runc.sha256 │ ├── dockerd -\u0026gt; dockerd-17.04.0-dev │ ├── dockerd-17.04.0-dev │ ├── dockerd-17.04.0-dev.md5 │ └── dockerd-17.04.0-dev.sha256 └── latest -\u0026gt; 17.04.0-dev 4 directories, 26 files 现在可以将 docker-daemon 和 docker-client 目录下的 docker 可以执行文件复制到容器的 /usr/bin/ 目录下了。\n启动 docker deamon\ndocker daemon -D\u0026amp; 检查下 docker 是否可用\nroot@f2753f78bb6d:/go/src/github.com/docker/docker/bundles/17.04.0-dev# docker version DEBU[0048] Calling GET /_ping DEBU[0048] Calling GET /v1.27/version Client: Version: 17.04.0-dev API version: 1.27 Go version: go1.7.5 Git commit: 3385658 Built: Mon Mar 6 08:39:06 2017 OS/Arch: linux/amd64 Server: Version: 17.04.0-dev API version: 1.27 (minimum version 1.12) Go version: go1.7.5 Git commit: 3385658 Built: Mon Mar 6 08:39:06 2017 OS/Arch: linux/amd64 Experimental: false 到此 docker 源码编译和开发环境都已经搭建好了。\n如果想要修改 docker 源码，只要在你的 IDE、容器里或者你本机上修改 docker 代码后，再执行上面的 hack/make.sh binary 命令就可以生成新的 docker 二进制文件，再替换原来的 /usr/bin/ 目录下的 docker 二进制文件即可。\n","relpermalink":"/blog/docker-dev-env/","summary":"本文将讲解如何进行 Docker 源码编译及开发环境搭建。","title":"Docker 源码编译和开发环境搭建"}]