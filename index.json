[{"content":"本文指导你如何配置 Envoy 代理与 SPIFFE 和 SPIRE 配合使用。\nEnvoy 是一种流行的开源服务代理，广泛用于提供抽象、安全、经过身份验证和加密的服务间通信。Envoy 拥有丰富的配置系统，允许灵活地与第三方进行交互。\n该配置系统的一个组成部分是 Secret Discovery Service 协议或 SDS。Envoy 使用 SDS 从 SDS 提供者检索和维护更新的“密钥”。在 TLS 身份验证的上下文中，这些密钥是 TLS 证书、私钥和可信 CA 证书。SPIRE 代理可以配置为 Envoy 的 SDS 提供者，使其能够直接向 Envoy 提供所需的密钥材料以进行 TLS 身份验证。SPIRE 代理还会根据需要重新生成短期密钥和证书。\n有关如何将 SPIRE 与 Envoy 集成的基于 Kubernetes 的示例，请参阅使用 X.509 证书集成 Envoy 和使用 JWT 集成 Envoy 。\n工作原理 当 Envoy 连接到 SPIRE 代理提供的 SDS 服务器时，代理会对 Envoy 进行验证，并确定应向 Envoy 公开哪些服务标识和 CA 证书，以通过 SDS。\n随着服务标识和 CA 证书的轮换，更新会流式传输回 Envoy，使其可以立即将其应用于新连接，无需中断或停机，并且无需私钥接触磁盘。换句话说，SPIRE 丰富的定义和验证服务的方法可以用于定位 Envoy 进程、为其定义标识，并为其提供 Envoy 可用于 TLS 通信的 X.509 证书和信任信息。\n示意图展示了两个 Envoy 代理在使用 SPIRE 代理 SDS，实现获取用于服务间相互认证的 TLS 通信的密钥。 配置 SPIRE 在 SPIRE v0.10 版本中，默认启用了 SDS 支持，因此不需要进行 SPIRE 配置更改。在早期版本的 SPIRE 中，SPIRE 代理配置文件中需要设置 enable_sds = true。该设置现已停用，应在 SPIRE v0.10 及更高版本的 SPIRE 代理配置文件中删除该设置。\n配置 Envoy SPIRE 代理集群 必须配置 Envoy 以与 SPIRE 代理通信，方法是配置一个指向 SPIRE 代理提供的 Unix 域套接字的集群。\n例如：\nclusters: - name: spire_agent connect_timeout: 0.25s http2_protocol_options: {} hosts: - pipe: path: /tmp/spire-agent/public/api.sock connect_timeout 影响当 Envoy 在启动时 SPIRE 代理未运行或 SPIRE 代理重新启动时，Envoy 能够快速响应的速度。\nTLS 证书 要从 SPIRE 获取 TLS 证书和私钥，可以在 TLS 上下文中设置 SDS 配置。\n例如：\ntls_context: common_tls_context: tls_certificate_sds_secret_configs: - name: \u0026#34;spiffe://example.org/backend\u0026#34; sds_config: api_config_source: api_type: GRPC grpc_services: envoy_grpc: cluster_name: spire_agent TLS 证书的名称是 Envoy 作为代理的服务的 SPIFFE ID。\n验证上下文 Envoy 使用可信 CA 证书来验证对等证书。验证上下文提供这些可信 CA 证书。SPIRE 可以为每个信任域提供验证上下文。\n要获取信任域的验证上下文，可以在 TLS 上下文的 SDS 配置中配置验证上下文，将验证上下文的名称设置为信任域的 SPIFFE ID。\n例如：\ntls_context: common_tls_context: validation_context_sds_secret_config: name: \u0026#34;spiffe://example.org\u0026#34; sds_config: api_config_source: api_type: GRPC grpc_services: envoy_grpc: cluster_name: spire_agent SPIFFE 和 SPIRE 的重点是促进安全身份验证作为授权的构建块，而不是授权本身，因此验证上下文中的授权相关字段（例如 match_subject_alt_names）不在其范围之内。相反，我们建议你利用 Envoy 的广泛过滤器框架执行授权。\n此外，你可以配置 Envoy 以将客户端证书详细信息转发到目标服务，使其能够执行自己的授权步骤，例如使用嵌入在客户端 X.509-SVID 的 URI SAN 中的 SPIFFE ID。\n","relpermalink":"/book/spiffe-and-spire/examples/envoy/","summary":"本文指导你如何配置 Envoy 代理与 SPIFFE 和 SPIRE 配合使用。 Envoy 是一种流行的开源服务代理，广泛用于提供抽象、安全、经过身份验证和加密的服务间通信。Envoy 拥有丰富的配置系统，允许灵活地与第三方进行交互。 该配置系统的一个","title":"在 Envoy 中集成 SPIRE"},{"content":"SPIFFE，即普适安全生产身份框架（Secure Production Identity Framework for Everyone），是一套开源标准，用于在动态和异构环境中安全地进行身份识别。采用 SPIFFE 的系统无论在哪里运行，都可以轻松可靠地相互认证。\nSPIFFE 开源规范的核心是——通过简单 API 定义了一个短期的加密身份文件 SVID。然后，工作负载进行认证时可以使用该身份文件，例如建立 TLS 连接或签署和验证 JWT 令牌等。\nSPIFFE 已经在云原生应用中得到了大量的应用，尤其是在 Istio 和 Envoy 中。下面将向你介绍 SPIFFE 的一些基本概念。\n工作负载 工作负载是一个单一的软件实体，通过特定配置部署，用于单一目的；它可能包括多个运行中的软件实例，所有这些实例执行相同的任务。术语“工作负载”可能涵盖软件系统的各种不同定义，包括：\n运行 Python Web 应用程序的 Web 服务器，部署在一组虚拟机上，前面有一个负载均衡器。 一个 MySQL 数据库的实例。 处理队列中条目的工作程序。 一组独立部署的系统共同工作，例如使用数据库服务的 Web 应用程序。Web 应用程序和数据库也可以分别被视为工作负载。 对于 SPIFFE 来说，工作负载往往比物理或虚拟节点更精细 - 通常精细到节点上的单个进程。对于在容器编排器中托管的工作负载而言，这对于多个工作负载可以共存（但在彼此之间隔离）于单个节点的情况非常重要。\n对于 SPIFFE 来说，工作负载也可能跨越多个节点 - 例如，一个可以在多台机器上同时运行的弹性缩放的 Web 服务器。\n尽管在不同的上下文中，将何为工作负载的粒度会有所不同，但对于 SPIFFE 的目的而言，假定工作负载与其他工作负载隔离得足够好，以至于恶意的工作负载在发放证书后无法窃取另一个工作负载的凭据。此隔离的稳固性以及其实现机制超出了 SPIFFE 的范围。\nSPIFFE ID SPIFFE ID 是一个字符串，唯一且具体地标识一个工作负载。SPIFFE ID 也可以分配给工作负载运行在的中间系统（如一组虚拟机）。例如，spiffe://acme.com/billing/payments 是一个有效的 SPIFFE ID。\nSPIFFE ID 是一个统一资源标识符 (URI) ，其格式如下：spiffe://信任域/工作负载标识符\n工作负载标识符唯一地标识信任域 中的特定工作负载。\nSPIFFE 规范 详细描述了 SPIFFE ID 的格式和用途。\n信任域 信任域对应于系统的信任根。信任域可以代表运行其独立 SPIFFE 基础设施的个人、组织、环境或部门。在相同信任域中标识的所有工作负载都会收到可以与信任域的根密钥进行验证的身份文件。\n通常建议将位于不同物理位置（例如不同数据中心或云区域）或应用不同安全实践的环境（例如与生产环境相比的暂存或实验环境）的工作负载保持在不同的信任域中。\nSPIFFE 可验证身份文件（SVID） SVID 是工作负载用于向资源或调用方证明其身份的文档。如果由 SPIFFE ID 信任域内的权威签名，SVID 被认为是有效的。\n一个 SVID 包含一个单一的 SPIFFE ID，代表了呈现它的服务的身份。它将 SPIFFE ID 编码在一个密码学可验证的文档中，支持两种当前支持的格式之一：X.509 证书或 JWT 令牌。\n由于令牌容易受到重放攻击，在传输中获取了令牌后，攻击者可以使用它来冒充一个工作负载，因此建议尽可能使用 X.509-SVIDs。但是，在某些情况下，JWT 令牌格式可能是唯一的选择，例如当你的架构在两个工作负载之间有一个 L7 代理或负载均衡器时。\n有关 SVID 的详细信息，请参阅SVID 规范 。\nSPIFFE 工作负载 API 工作负载 API 提供以下功能：\n对于 X.509 格式的身份文件（X.509-SVID）：\n其身份，以 SPIFFE ID 形式描述。 与该 ID 相关的私钥，可用于代表工作负载对数据进行签名。还创建了相应的短暂的 X.509 证书，即 X509-SVID。这可用于建立 TLS 连接或以其他方式对其他工作负载进行身份验证。 一组证书 - 称为信任捆绑包 - 可用于验证另一个工作负载呈现的 X.509-SVID。 对于 JWT 格式的身份文件（JWT-SVID）：\n其身份，以 SPIFFE ID 形式描述。 JWT 令牌 一组证书 - 称为信任捆绑包 - 可用于验证其他工作负载的身份。 与Amazon EC2 实例元数据 API 和Google GCE 实例元数据 API 类似，工作负载 API 不要求调用工作负载具有自己的身份知识，或在调用 API 时拥有任何身份验证令牌。这意味着你的应用程序无需将任何身份验证密钥与工作负载一起部署。\n然而，与这些其他 API 不同，工作负载 API 是平台无关的，并且可以在进程级别以及内核级别识别运行的服务 - 这使其适用于与容器调度器（如 Kubernetes）一起使用。\n为了最小化由于密钥泄露或被破坏而造成的风险，所有私钥（及相应的证书）都是短暂的，会经常自动轮换。在相应的密钥到期之前，工作负载可以从工作负载 API 请求新的密钥和信任捆绑包。\n信任捆绑包 在使用 X.509-SVID 时，信任捆绑包用于由目标工作负载验证源工作负载的身份。信任捆绑包是一个包含一个或多个证书颁发机构（CA）根证书的集合，工作负载应将其视为可信任的。信任捆绑包包含了验证 X.509 和 JWT SVID 的公钥材料。\n用于验证 X.509 SVID 的公钥材料是一组证书。用于验证 JWT 的公钥材料是一个原始的公钥。信任捆绑包的内容经常会发生变化。在调用工作负载 API 时，工作负载会检索信任捆绑包。\n","relpermalink":"/book/spiffe-and-spire/concept/spiffe/","summary":"SPIFFE，即普适安全生产身份框架（Secure Production Identity Framework for Everyone），是一套开源标准，用于在动态和异构环境中安全地进行身份识别。采用 SPIFFE 的系统无论在哪里运行，都可以轻松可靠地相互认证。 SPIFFE 开源规范","title":"SPIFFE 基本概念"},{"content":"SPIFFE 标准提供了一个规范，用于在异构环境和组织边界中引导和颁发服务的身份。它包括各种规范，每个规定了 SPIFFE 功能的特定子集的操作。\n特别是本文档作为 SPIFFE 标准的核心规范。虽然在 SPIFFE 范围内还有其他规范，但符合本文档就足以实现 SPIFFE 合规性，并获得 SPIFFE 标准本身的互操作性好处。\n引言 本文档提出了正式的 SPIFFE 规范。它定义了 SPIFFE 标准的两个最基本组件：SPIFFE 身份和 SPIFFE 可验证身份文档。\n第 2 节概述了 SPIFFE 身份（SPIFFE ID）及其命名空间。SPIFFE ID 被定义为符合RFC 3986 标准的 URI，包括“信任域名”和相关路径。信任域名作为 URI 的授权组件，用于识别发放给定身份的系统。以下示例演示了如何构造 SPIFFE ID：\nspiffe://trust-domain-name/path\n有效的 SPIFFE ID 必须将方案设置为spiffe，包含非零的信任域名，并且不能包含查询或片段组件。换句话说，SPIFFE ID 由spiffe方案和一个特定站点的hier-part（其中包括授权组件和可选路径）完全定义。\n信任域 信任域对应于系统的信任根。信任域可以代表独立运行其自己的 SPIFFE 基础设施的个人、组织、环境或部门。\n信任域名称通常是自我注册的，与公共 DNS 不同，没有委托权机构来断言并注册基本域名到实际的法律实体，或者断言该法律实体对任何特定信任域名拥有公正和正当的权利。\n信任域名被定义为 URI 的授权组件，并应用以下限制：\n授权组件的host部分不得为空。 授权组件的userinfo和port部分必须为空。 授权组件的host部分必须小写。 授权组件的host部分只能包含字母、数字、点、破折号和下划线（[a-z0-9.-_]）。 授权组件的host部分不能包含百分比编码的字符。 请注意，此定义不排除用点分四段表示法表示的 IPv4 地址，但排除了 IPv6 地址。DNS 名称是有效信任域名的严格子集。实现在处理信任域名时，无论它们是有效的 IP 地址还是有效的 DNS 名称，都不得以不同方式处理它们。\n信任域名称冲突 信任域操作员可以自由选择任何他们认为合适的信任域名称：没有中央权威机构来监管或注册信任域名称。因此，不能保证全局唯一性，也没有技术手段阻止不同的信任域使用相同的信任域名称。\n为防止意外碰撞（两个信任域选择相同的名称），建议操作员选择高度可能全球唯一的信任域名称。即使信任域名称不是 DNS 名称，但如果可用，使用注册的域名作为信任域名的后缀将降低意外碰撞的可能性；例如，如果信任域操作员拥有域名example.com，那么使用类似trust_domain_name.example.com的信任域名可能不会产生冲突。当信任域名在没有操作员输入的情况下自动生成时，强烈建议随机生成一个唯一的名称（例如 UUID）。\n发生冲突时，这些信任域将继续独立运行，但将无法联合（相互连接）。因为每个信任域使用独特的信任根，由一个信任域发放的身份声明将在另一个信任域中验证失败。\n路径 SPIFFE ID 的路径组件允许唯一标识给定的工作负载。路径的含义是开放式的，由管理员负责定义。\n有效的 SPIFFE ID 路径组件必须遵循以下规则：\n路径组件不能包含百分比编码的字符。 路径组件不能包含空段或相对路径修饰符（即.、..）。 路径组件不能以斜杠结尾。 单个路径段只能包含字母、数字、点、破折号和下划线（[a-zA-Z0-9.-_]）。 路径可以是分层的，类似于文件系统路径。路径的具体含义保留给实施者，不属于 SVID 规范的范围之内。以下是一些示例和约定。\n直接标识服务\n通常，直接标识服务是有价值的。例如，管理员可能会决定在特定一组节点上运行的任何进程都应该能够以特定的身份呈现自己。例如：\nspiffe://staging.example.com/payments/mysql 或 spiffe://staging.example.com/payments/web-fe\n上述两个 SPIFFE ID 指代了两个不同的组件 - mysql 数据库服务和一个运行在暂存环境中的支付服务的 web 前端。环境“staging”的含义和“payments”作为高级服务集合的含义由实施者定义。\n标识服务所有者\n通常，更高级别的编排器和平台可能已经内置了它们自己的身份概念（如 Kubernetes 服务账户或 AWS/GCP 服务账户），直接将 SPIFFE 身份映射到这些身份是很有帮助的。例如：\nspiffe://k8s-west.example.com/ns/staging/sa/default\n在这个示例中，example.com 的管理员正在运行一个名为 k8s-west.example.com 的 Kubernetes 集群，该集群有一个“staging”命名空间，在其中有一个名为“default”的服务账户（sa）。这些都是由 SPIFFE 管理员定义的约定，而不是本规范所保证的断言。\n不透明的 SPIFFE 身份\n上述示例是说明性的，在最一般的情况下，SPIFFE 路径可能是不透明的，不包含任何可见的分层信息。例如，地理位置、逻辑系统分区和/或服务名称等元数据可以由注册身份及其属性的次级系统提供。可以查询以检索与 SPIFFE 标识符相关联的任何元数据。例如：\nspiffe://example.com/9eebccd2-12bf-40a6-b262-65fe0487d453\n最大 SPIFFE ID 长度 如RFC 3986 定义的 URI 没有最大长度。出于互操作性考虑，SPIFFE 实现必须支持最长为 2048 字节的 SPIFFE URI，并且不应生成长度大于 2048 字节的 URI。RFC 3986 仅允许 ASCII 字符，因此 SPIFFE ID 的推荐最大长度为 2048 字节。\n所有 URI 组件都会影响 URI 的长度，包括“spiffe”方案、“：//”分隔符、信任域名和路径组件。非 ASCII 字符在将其编码为 ASCII 字符后会影响 URI 的长度。请注意，RFC 3986 为 URI 的“host”组件定义了最大长度为 255 个字符；因此，信任域名的最大长度为 255 字节。\nSPIFFE ID 解析 SPIFFE ID 遵循由RFC 3986 定义的 URI 规范。SPIFFE ID 的方案和信任域名对大小写不敏感，而路径对大小写敏感。\nSPIFFE 可验证身份文档 SPIFFE 可验证身份文档（SVID）是工作负载将其身份通信给资源或调用者的机制。如果 SVID 已由 SPIFFE ID 所在信任域内的授权方签名，则认为 SVID 是有效的。\nSVID 信任 SPIFFE 信任根植于给定 ID 的信任域。每个信任域必须存在一个签名授权机构，该授权机构必须携带自己的 SVID。签名授权机构的 SPIFFE ID 应该驻留在其具有权威性的信任域中，并且不应具有路径组件。授权机构的 SVID 然后形成了给定信任域的信任基础。\n如果需要，可以通过使用外部信任域授权机构的私钥对授权机构的 SVID 进行签名来实现信任链。如果不需要链接信任，那么授权机构的 SVID 将进行自签名。\nSVID 组件 SVID 是一个相当简单的构造，包括三个基本组件：\n一个 SPIFFE ID 一个有效的签名 一个可选的公钥 SPIFFE ID 和公钥（如果存在）必须包含在签名的有效载荷的一部分中。如果包含了公钥，则相应的私钥将由发放 SVID 的实体保留，并用于证明对 SVID 本身的所有权。\n个别的 SVID 规范可能要求或以其他方式允许在 SVID 中包含超出此处描述的内容。所包含信息的性质可能或可能不会严格由相关的 SPIFFE 规范定义 - 例如，JWT-SVID 规范允许用户在 SVID 本身中包含任意信息。在相关 SVID 规范未明确指定此附加信息的情况下，操作者在将此信息用作安全决策的输入时应格外小心，特别是如果要验证的 SVID 属于不同的信任域。有关更多信息，请参阅安全注意事项部分。\nSVID 格式 SVID 本身不是一种文件类型。已经存在许多文件格式可以满足 SPIFFE SVID 的需求，我们不希望重新发明这些格式。相反，我们定义了一组特定于格式的规范，规范化了 SVID 信息的编码。\n为了使 SVID 被视为有效，它必须利用已定义相应规范的文件类型。在撰写本文时，唯一受支持的文件类型是 X.509 和 JWT。请注意，特定于格式的 SVID 规范可能会升级本文中规定的要求。\n安全注意事项 本节包含在使用 SPIFFE ID 和 SVID 时实施者和用户应考虑的安全注意事项。\nSVID 断言 SVID 始终包含一组数据 - 至少是一个 SPIFFE ID。有时，此数据代表了信任域授权机构对 SVID 主体所做的断言。在从此数据中解释含义时，必须小心确保所有涉及方都充分理解所使用信息的含义和重要性。\n在考虑给定断言的相对安全性时，有四个主要问题：\n首先是时间上的准确性 - SVID 在到期之前一段时间内是有效的，SVID 中的断言在 SVID 的整个生命周期内是否为真？ 其次，断言的范围和影响 - 断言最初是在什么上下文下进行的，它的影响有多大？ 第三是解释和含义的问题 - 断言对授权机构和消费者是否具有相同的含义或解释，或者存在着不同的解释可能性？ 最后，断言本身的真实性在某些情况下可能会受到质疑。 本节探讨了这四个关注领域的所有方面，并提供了操作者可以评估任何给定 SVID 断言的相对安全性的指导方针。一般来说，操作者应该以谨慎为原则，只包含那些对所涉及的断言的安全性具有非常高度信心的断言。\n值得注意的是，虽然通常由 SPIFFE 规范直接形式化的断言通常不容易受到与解释和含义相关的问题的影响，但它们仍然可能容易受到与真实性相关的问题的影响。但是，由于 SPIFFE 定义的断言的范围非常有限，因此在这方面的真实性问题表明了与问题相关的信任域的安全姿态的更大问题，此时操作者应该认真考虑是否应该在第一时间与这些系统交换数据。\n时间上的准确性 SVID 在一段有限的时间内有效，主要是为了降低密钥被泄露和相关损害的可能性。虽然通常情况下，SVID 中的断言在签发时是真实的，但并不一定意味着在使用时也是真实的。\n某些类型的断言比其他类型更容易受到此问题的影响。服务所有者的名称、角色或组成员资格以及访问策略都是在 SVID 签发时和验证或使用时之间更有可能发生变化的示例。相反，工作负载及其运行时的自然属性（例如 SPIFFE ID 或工作负载所在的区域）通常与工作负载的生命周期绑定，因此不太可能发生变化，这使得它们不太容易受到时间上的准确性的问题影响。\n在决定是否应该在 SVID 中包含某个特定的断言时，考虑到这一点是很重要的。在 SVID 中作出的断言将被认为在 SVID 的生命周期内都是有效的，并且对于具有旧断言的所有 SVID 来说，将首先过期，因此在活动系统上对此断言进行更改（或撤销）将会很费时。如果对于所考虑的断言的波动性不清楚，操作者应该以谨慎为原则，并将其排除在 SVID 之外。\n范围和影响 SVID 由位于其信任域中的授权机构签名。签名授权机构有责任验证其签署的 SVID 中的所有信息，而包含在 SVID 中的所有断言实际上都是由签名授权机构所做的断言。\n此授权机构的影响和断言所做的范围是自然有限的。一个信任域的授权机构的权限不应该对其他信任域中的实体做出断言（即其断言的范围仅限于其控制下的实体）。同样，在消费 SVID 数据时，消费者应该将其中包含的所有断言视为受到 SVID 所在信任域的限制。\n例如：如果信任域 A 和 B 都使用名为“role”的属性，那么信任域 A 中具有“admin”角色的实体可以使用该角色做出自己信任域中 SVID 的断言，但信 …","relpermalink":"/book/spiffe-and-spire/standard/spiffe-id/","summary":"SPIFFE 标准提供了一个规范，用于在异构环境和组织边界中引导和颁发服务的身份。它包括各种规范，每个规定了 SPIFFE 功能的特定子集的操作。 特别是本文档作为 SPIFFE 标准的核心规范。虽然在 SPIFFE 范围内还有其他规范，但符合本文档就足以","title":"SPIFFE ID 和 SVID"},{"content":"本指南介绍云原生安全与身份解决方案 SPIFFE 和 SPIRE。\n本书大纲 概念\n规范\n架构\n安装\n配置\nSPIRE 集成示例\n开始阅读 ","relpermalink":"/book/spiffe-and-spire/","summary":"本指南介绍云原生安全与身份解决方案 SPIFFE 和 SPIRE。","title":"SPIFFE 和 SPIRE：实现零信任安全身份"},{"content":"本章介绍 SPIFFE 和 SPIRE 中的基本概念。\nSPIFFE\nSPIRE\n","relpermalink":"/book/spiffe-and-spire/concept/","summary":"本章介绍 SPIFFE 和 SPIRE 中的基本概念。 SPIFFE SPIRE","title":"概念"},{"content":"本页面描述了一些开始使用 SPIRE 的选项。\nDocker Compose SPIRE 101 是一个在 Docker Compose 上运行的 SPIRE 入门介绍 spire-tutorials 存储库中提供了其他 Docker Compose 演示 Kubernetes SPIRE 没有官方的 Helm chart、Kustomize 文件或自定义资源操作器，但 Kubernetes 快速入门 包括一套用于测试 SPIRE Server 和 Agent 的基本 Kubernetes YAML 文件 spire-tutorials 存储库中提供了其他 Kubernetes 演示 Linux SPIRE GitHub releases 页面提供了每个 SPIRE 版本的下载链接和变更日志 spiffe.io 的获取 SPIRE 页面提供了其他下载选项和构建 SPIRE 的说明 Linux 和 MacOS X 快速入门 介绍了如何下载和测试 SPIRE Server 和 Agent 的简单单节点安装 MacOS 没有预编译的 MacOS 可执行文件可用，但 Linux 和 MacOS X 快速入门 介绍了如何下载和构建 SPIRE 以测试 SPIRE Server 和 Agent 的简单单节点安装 ","relpermalink":"/book/spiffe-and-spire/installation/getting-spire/","summary":"本页面描述了一些开始使用 SPIRE 的选项。 Docker Compose SPIRE 101 是一个在 Docker Compose 上运行的 SPIRE 入门介绍 spire-tutorials 存储库中提供了其他 Docker Compose 演示 Kubernetes SPIRE 没有官方的 Helm chart、Kustomize 文件或自定义资源操作器，但 Kubernetes 快速入门 包括一套用于测试 SPIRE","title":"获取 SPIRE"},{"content":"扩展 SPIRE 可以通过嵌套拓扑和联合拓扑来实现。嵌套拓扑允许将多个 SPIRE 服务器链接在一起，以发放属于同一信任域的身份。联合拓扑用于在不同信任域之间建立信任，使工作负载能够在不同信任域中进行身份验证。SPIRE 还可以与其他 SPIFFE 兼容系统和 OIDC 提供者系统进行联合，以实现安全的身份验证和通信。在部署规模时，需要考虑 SVID 和根证书的生存时间、工作负载数量和分布、JWT-SVID 的使用等因素，并注意数据存储的设计和规划。\nSPIRE 部署可以根据工作负载的增长来调整大小或规模。一个 SPIRE 部署由一个或多个共享复制数据存储的 SPIRE 服务器组成，或者相反，由在同一信任域中的一组 SPIRE 服务器和至少一个 SPIRE 代理（通常是一个以上）组成。\n部署的大小范围广泛。单个 SPIRE 服务器可以容纳多个代理和工作负载注册条目。一个规模大小的考虑是，由于涉及到管理和发放与这些条目相对应的身份所涉及的操作数量，SPIRE 服务器实例的内存和 CPU 消耗往往与部署中的工作负载注册条目数量成比例增长。单个 SPIRE 服务器实例也代表了一个单点故障。\n为了支持给定部署中更多的代理和工作负载（数以万计或数十万个节点），可以水平扩展 SPIRE 服务器的数量。有了多个服务器，SPIRE 服务器执行的计算工作将在所有 SPIRE 服务器实例之间分布。除了额外的容量之外，使用多个 SPIRE 服务器实例还可以消除单点故障，实现高可用性。\n高可用性模式下的 SPIRE 服务器 水平扩展 SPIRE Server 要水平扩展 SPIRE 服务器，无论是出于高可用性还是负载分配目的，都要配置所有属于同一信任域的服务器以读写相同的共享数据存储。\n数据存储是 SPIRE 服务器持久保存动态配置信息的地方，例如注册条目和身份映射策略。SQLite 已捆绑到 SPIRE 服务器中，它是默认的数据存储。支持一些兼容的 SQL 数据库，以及一个用于使用 Kubernetes CRD 的 Kubernetes 插件。在水平扩展 SPIRE 服务器时，请选择符合你要求的数据存储，并配置所有 SPIRE 服务器以使用所选的数据存储。有关详细信息，请参阅 数据存储插件配置参考 。\n在高可用性模式下，每个服务器都维护自己的证书颁发机构，可以是自签名证书，也可以是从共享根证书颁发机构获取的中间证书（即在配置了上游机构时）。\n选择 SPIRE 部署拓扑 有三种主要的 SPIRE 部署拓扑：\n单一信任域 嵌套 SPIRE 联合 SPIRE 诸如管理域边界、工作负载数量、可用性要求、云供应商数量和身份验证要求等因素将决定你环境中的适当拓扑，如下所述。\n单一信任域 单一信任域 单一信任域最适合个体环境或在管理域内具有相似特征的环境。创建一个单一的主导信任域的主要动机是从单个证书颁发机构中发放身份，因为这会减少在不同部署中管理的 SPIRE 服务器数量。\n然而，当将单个 SPIRE 信任域部署以跨越地理区域、平台和云提供商环境时，在跨越地理位置或跨越云提供商边界的多个地方管理共享数据存储会带来一定的复杂性。在这些情况下，当部署扩展以覆盖多个环境时，解决在单一信任域上使用共享数据存储的问题的方法是在嵌套拓扑中配置 SPIRE 服务器。\n嵌套 SPIRE 嵌套 SPIRE 嵌套 SPIRE 允许 SPIRE 服务器被“链接”在一起，所有服务器仍然发放属于同一信任域的身份，这意味着在同一信任域中识别的所有工作负载都会获得可以与信任域的根密钥进行验证的身份文档。\n嵌套拓扑通过在每个下游 SPIRE 服务器与“链接”的 SPIRE 代理共存来工作。下游 SPIRE 服务器通过 Workload API 获取凭据，然后使用这些凭据直接与上游 SPIRE 服务器进行身份验证以获取中间 CA。\n一个有助于理解嵌套拓扑功能的心理模型是将顶级 SPIRE 服务器想象成是一个全局服务器（或一组用于高可用性的服务器），而下游 SPIRE 服务器是区域或集群级别的服务器。\n在此配置中，顶层 SPIRE 服务器保存根证书/密钥，而下游服务器请求中间签名证书，用作下游服务器的 X.509 签名授权机构。这提供了弹性，因为顶层可能会崩溃，中间服务器将继续运行。\n嵌套拓扑非常适用于多云部署。由于能够混合匹配节点证明者，下游服务器可以驻留在不同云提供商环境中，为不同云提供商环境中的工作负载和代理提供身份。\n作为为了实现高可用性和负载平衡而将 SPIRE 服务器水平扩展的补充，嵌套拓扑可以用作分隔故障域的约束策略。\n联合 SPIRE 联合 SPIRE 部署可能需要多个信任根：也许因为一个组织有不同的组织部门，各自有不同的管理员，或者因为它们有单独的临时和生产环境，需要偶尔进行通信。\n另一个用例是在组织之间实现 SPIFFE 互操作性，例如在云提供商和其客户之间。\n这些多信任域和互操作性用例都需要一种明确定义的、可互操作的方法，使一个信任域中的工作负载能够对另一个信任域中的工作负载进行身份验证。首先通过验证各自的束终点来建立不同信任域之间的信任，然后通过经过身份验证的端点检索外部信任域束。\n有关如何实现这一点的更多细节，请参阅以下 SPIFFE 规范，其中描述了这种机制：SPIFFE 信任域和束 有关配置联合 SPIRE 的教程，请参阅：联合 SPIRE 教程 与外部系统的交互 与 SPIFFE 兼容的系统联合 与 SPIRE 兼容的系统联合 SPIFFE 身份颁发者可以与其他暴露 SPIFFE 联合 API 实现的 SPIFFE 身份颁发者联合，使联合域中的工作负载能够安全地进行身份验证和通信。与在 SPIRE 部署之间建立联合一样，SPIFFE 联合用于在 SPIFFE 兼容系统之间启用联合，比如在一个 Istio 服务网格和另一个 Istio 服务网格之间运行的 Istio 服务网格。\n例如，在当前的 Istio 中，服务网格上的所有应用程序都位于同一个信任域中，因此共享一个共同的信任根。可能会有不止一个服务网格，或者在服务网格中通信到需要进行身份验证的外部服务。使用联合功能可以使得 SPIFFE 兼容的系统，比如多个 Istio 服务网格，能够为安全的跨网格和脱网通信建立信任。\n与 OIDC 提供者系统的联合 与 OIDC 提供者系统联合 SPIRE 具有一个特性，可以代表已识别的工作负载对远程系统进行编程身份验证，例如与支持 OIDC 联合的公共云提供商服务和密钥存储进行交互。例如，在亚马逊网络服务的情况下，一个经过 SPIRE 认证的工作负载可以对接 AWS S3 存储桶、AWS RDS 实例或 AWS CodePipeline 进行身份验证和通信。\nSPIRE OIDC 发现提供者使用 ACME 协议检索 WebPKI 证书，用于保护一个端点，该端点提供 OIDC 兼容的 JWKS 束和标准 OIDC 发现文档。然后需要配置远程 OIDC 认证服务以定位该端点并确定 WebPKI 服务。一旦完成此配置，可以设置远程系统的 IAM 策略和角色以映射到特定的 SPIFFE ID。工作负载随后将通过发送 JWT-SVID 与 OIDC 认证系统通信。然后，目标系统从预定义的 URI 获取 JWKS，该 URI 由 OIDC 发现提供者提供。目标系统使用 JWKS 文件验证 JWT-SVID，如果 JWT-SVID 中包含的 SPIFFE ID 被授权访问所请求的资源，则服务请求。然后，工作负载就能够访问外部远程服务，而无需拥有由其提供的任何凭据。\n有关 OIDC 发现提供者的配置参考，请参阅：OIDC 发现提供者配置参考 有关在 Amazon Web Services 配置 OIDC 联合的详细教程，请参阅：配置 OIDC 到 Amazon Web Services 部署规模考虑 在为 SPIRE 部署调整大小以实现最佳性能时，需要考虑的因素包括但不限于以下内容：\nSVID 和根证书的生存时间 每个节点的工作负载数量和分布 大量 JWT-SVID 的使用（因为 JWT 需要根据需要进行签名，而不像 x509 那样预先存储） 注册更改的频率 在 SPIRE 服务器节点上运行的其他进程 基础架构环境的“形状”和“大小” 特别要注意对数据存储的设计和规划。请注意，数据存储的性能在上述列表中没有得到解决，并且可能会限制 SPIRE 的性能。由于每个代理同步（每 5 秒一次）都会进行授权检查，因此数据存储通常是性能瓶颈。在嵌套拓扑中，由于嵌套拓扑中的每个 SPIRE 服务器集群都有自己的数据存储，因此可以降低此成本。\n下表旨在提供关于在 SPIRE 部署中调整 SPIRE 服务器大小的参考信息。这些参考数字基于测试环境。它们仅作为数量级指南，不代表任何特定用户环境的性能保证。网络带宽和数据库查询信息未包含在内。此外，所显示的工作负载和代理数量不代表在理论上可能的 SPIRE 部署规模。\n工作负载数量 10 代理 100 代理 1000 代理 5000 代理 10 工作负载 2 个服务器单元，1 个 CPU 核心，1GB RAM 2 个服务器单元，2 个 CPU 核心，2GB RAM 2 个服务器单元，4 个 CPU 核心，4GB RAM 2 个服务器单元，8 个 CPU 核心，8GB RAM 100 工作负载 2 个服务器单元，2 个 CPU 核心，2GB RAM 2 个服务器单元，2 个 CPU 核心，2GB RAM 2 个服务器单元，8 个 CPU 核心，8GB RAM 2 个服务器单元，16 个 CPU 核心，16GB RAM 1000 工作负载 2 个服务器单元，16 个 CPU 核心，8GB RAM 2 个服务器单元，16 个 CPU 核心，8GB RAM 2 个服务器单元，16 个 CPU 核心，8GB RAM 4 个服务器单元，16 个 CPU 核心，8GB RAM 10000 工作负载 每个 4 个服务器单元，16 个 CPU 核心，16GB RAM 每个 4 个服务器单元，16 个 CPU 核心，16GB RAM 每个 4 个服务器单元，16 个 CPU 核心，16GB RAM 每个 8 个服务器单元，16 个 CPU 核心，16GB RAM ","relpermalink":"/book/spiffe-and-spire/architecture/scaling-spire/","summary":"扩展 SPIRE 可以通过嵌套拓扑和联合拓扑来实现。嵌套拓扑允许将多个 SPIRE 服务器链接在一起，以发放属于同一信任域的身份。联合拓扑用于在不同信任域之间建立信任，使工作负载能够在不同信任域中进行身份验证。SPIRE 还可","title":"扩展 SPIRE 部署：支持的 SPIRE 拓扑结构、身份联合和规模考虑"},{"content":"要根据你的应用程序需求自定义 SPIRE 服务器和 SPIRE 代理的行为，你需要编辑服务器和代理的配置文件。\n如何配置 SPIRE SPIRE 服务器和代理的配置文件分别为 server.conf 和 agent.conf。\n默认情况下，服务器期望配置文件位于 conf/server/server.conf，但是服务器可以通过 --config 标志配置为使用不同位置的配置文件。有关更多信息，请参阅 SPIRE 服务器参考 。\n同样，代理期望配置文件位于 conf/agent/agent.conf，但是代理可以通过 --config 标志配置为使用不同位置的配置文件。有关更多信息，请参阅 SPIRE 代理参考 。\n配置文件在启动服务器或代理时加载一次。如果更改了服务器或代理的配置文件，则必须重新启动服务器或代理以使配置生效。\n在 Kubernetes 中运行 SPIRE 时，通常将配置文件存储在 ConfigMap 对象中，然后将其作为文件挂载到运行代理或服务器进程的容器中。\nSPIRE 代理支持使用 HCL 或 JSON 作为配置文件结构语法。下面的示例将假定使用 HCL。\n配置信任域 此配置适用于 SPIRE 服务器和 SPIRE 代理\n信任域对应于 SPIFFE 身份提供者的信任根。信任域可以表示运行其自己独立的 SPIFFE 基础设施的个人、组织、环境或部门。在同一信任域中标识的所有工作负载都将获得可以与信任域的根密钥进行验证的身份文件。\n每个 SPIRE 服务器关联一个必须在该组织内唯一的信任域。信任域采用与 DNS 名称相同的形式（例如，prod.acme.com），但不需要与任何 DNS 基础设施对应。\n在首次启动服务器之前，需要在 SPIRE 服务器中配置信任域。通过在配置文件的 server 部分的 trust_domain 参数中配置。例如，如果服务器的信任域应配置为 prod.acme.com，则应设置为：\ntrust_domain = \u0026#34;prod.acme.com\u0026#34; 同样，代理必须通过在代理配置文件的 agent 部分的 trust_domain 参数中配置来为相同的信任域颁发身份。\nSPIRE 服务器和代理只能为单个信任域颁发身份，代理配置的信任域必须与其连接的服务器的信任域匹配。\n配置服务器监听代理的端口 此配置适用于 SPIRE 服务器\n默认情况下，SPIRE 服务器在端口 8081 上监听来自 SPIRE 代理的传入连接；要选择不同的值，请编辑 server.conf 文件中的 bind_port 参数。例如，要将监听端口更改为 9090：\nbind_port = \u0026#34;9090\u0026#34; 如果从服务器的默认配置更改了此配置，则还必须在代理上更改服务端口的配置。\n配置节点认证 此配置适用于 SPIRE 服务器和 SPIRE 代理\nSPIFFE 服务器通过节点认证和解析的过程来识别和验证代理。这是通过节点验证器和节点解析器插件来完成的，你需要在服务器中配置和启用它们。\n你选择的节点认证方法将确定你在 SPIRE 配置文件的服务器插件和代理插件部分中配置 SPIRE 使用哪些节点验证器插件。服务器上必须配置至少一个节点验证器，每个代理上只能配置一个节点验证器。\n对运行在 Kubernetes 上的节点进行认证 为了向在 Kubernetes 集群中运行的工作负载发放身份，需要在每个运行负载的集群节点上部署一个 SPIRE 代理。（在 Kubernetes 上安装 SPIRE 代理 了解如何在 Kubernetes 上安装 SPIRE 代理）。\n可以使用 Kubernetes 的 Token Review API 对服务帐户令牌进行验证。因此，SPIRE 服务器本身不需要在 Kubernetes 上运行，并且单个 SPIRE 服务器可以支持在启用了 PSAT 认证的多个 Kubernetes 集群上运行的代理。\nProjected Service Account Tokens 在撰写本文时，预投影的服务帐户是 Kubernetes 的一个相对较新的功能，不是所有部署都支持它们。你的 Kubernetes 平台文档将告诉你是否支持此功能。如果你的 Kubernetes 部署不支持预投影的服务帐户令牌，则应启用服务帐户令牌。\n使用 Kubernetes 的 Projected Service Account Tokens (PSATs) 对节点进行认证允许 SPIRE 服务器验证在 Kubernetes 集群上运行的 SPIRE 代理的身份。预投影的服务帐户令牌相对于传统的 Kubernetes 服务帐户令牌提供了额外的安全保证，因此，如果 Kubernetes 集群支持，PSAT 是推荐的认证策略。\n要使用 PSAT 节点认证，请在 SPIRE Server 和 SPIRE Agent 上配置启用 PSAT 节点认证器插件。\n服务帐户令牌 在 Kubernetes 上运行工作负载时，如果集群上没有 Projected Service Account Token 功能，则 SPIRE 可以使用 Service Account Tokens 在 Server 和 Agent 之间建立信任。与使用 Projected Service Account Tokens 不同，此方法要求 SPIRE Server 和 SPIRE Agent 都部署在同一个 Kubernetes 集群上。\n由于服务帐户令牌不包含可用于强力识别运行 Agent 的节点/守护程序/Pod 的声明，因此任何在允许的服务帐户下运行的容器都可以冒充 Agent。因此，强烈建议在使用此认证方法时，Agent 应在专用的服务帐户下运行。\n要使用 SAT 节点认证，请在 SPIRE Server 和 SPIRE Agent 上配置和启用 SAT 节点认证器插件。\n对运行 Linux 的节点进行认证 SPIRE 能够对运行 Linux 的物理或虚拟机（节点）上的工作负载的身份进行认证。作为认证过程的一部分，SPIRE Server 需要建立与运行 Linux 节点上的 SPIRE Agent 的信任关系。根据节点运行的位置，SPIRE 支持各种节点认证器，这些节点认证器允许在创建注册项时使用不同的选择器来标识特定的工作负载。\n加入令牌（Join Token） 加入令牌是一种使用单次使用的令牌来对服务器进行认证的简单方法，该令牌在服务器上生成并在启动代理时提供给代理。它适用于在 Linux 上运行的任何节点。\nSPIRE 服务器可以通过在 server.conf 配置文件中启用内置的join-token NodeAttestor 插件来支持加入令牌认证，如下所示：\nNodeAttestor \u0026#34;join_token\u0026#34; { plugin_data { } } 配置了加入令牌节点认证之后，可以使用spire-server token generate命令在服务器上生成加入令牌。可以使用-spiffeID标志将特定的 SPIFFE ID 与加入令牌关联起来。在此处阅读更多 有关使用此命令的更多信息。\n当第一次启动启用加入令牌证明的 SPIRE 代理时，可以使用 spire-agent run 命令启动代理，并使用 -joinToken 标志指定服务器生成的加入令牌。有关此命令的详细信息，请阅读更多 。\n服务器将验证加入令牌并向代理颁发 SVID（SPIFFE 身份验证信息文档）。只要代理与服务器保持连接，SVID 将自动轮换。在以后的启动中，除非 SVID 已过期且未续订，否则代理将使用该 SVID 对服务器进行身份验证。\n要使用加入令牌节点证明，请在 SPIRE 服务器 和 SPIRE 代理 上配置和启用加入令牌节点证明插件。\n要在服务器上禁用加入令牌证明，请在启动之前从配置文件中注释或删除此部分。\nX.509 证书 在许多情况下，特别是在手动配置节点的情况下（例如在数据中心），可以通过验证先前安装在节点上的现有 X.509 叶子证书来识别节点并唯一标识它。\n通常，这些叶子证书是从单个公共密钥和证书（在本指南中称为根证书包）生成的。服务器必须配置根密钥和任何中间证书，以便能够验证特定机器呈现的叶子证书。只有找到可以通过证书链验证到服务器的证书时，节点证明才会成功，并且可以向该节点上的工作负载发布 SPIFFE ID。\n此外，证明者公开了 subject:cn 选择器，该选择器将匹配满足以下条件的证书：（a）有效，如上所述，（b）其通用名称（CN）与选择器中描述的通用名称匹配。\n要使用 X.509 证书节点证明，请在 SPIRE 服务器 和 SPIRE 代理 上配置和启用 x509pop 节点证明插件。\nSSH 证书 在某些环境中，每个节点都会自动配备一个有效且唯一的 SSH 证书，用于标识该节点。SPIRE 可以使用此证书来引导其身份验证。\n通过这种方法进行验证的节点会自动获得形式为的 SPIFFE ID：\nspiffe://\u0026lt;trust-domain\u0026gt;/spire/agent/sshpop/\u0026lt;fingerprint\u0026gt; 其中 \u0026lt;fingerprint\u0026gt; 是证书本身的哈希值。然后，可以使用此 SPIFFE ID 作为其他工作负载注册条目的基础。\n要使用 SSH 证书节点验证，请在 SPIRE 服务器和 SPIRE 代理上配置并启用 sshpop 节点验证插件。\n云供应商上的 Linux 节点验证 许多云供应商提供特权 API，允许在由该供应商托管的特定节点上运行的进程能够证明其所在的节点。SPIRE 可以配置为利用这些 API 进行节点验证。这对于自动化来说特别方便，因为在新实例上首次启动代理时，代理可以自动向 SPIRE 服务器证明其身份，而无需为其发行预先存在的证书或加入令牌。\nGoogle Compute Engine 实例 Google Compute Engine（GCE）节点验证和解析允许 SPIRE 服务器自动识别和验证在 GCP GCE 实例上运行的 SPIRE 代理。简而言之，通过以下步骤完成：\nSPIRE 代理 gcp_iit 节点验证插件检索 GCP 实例的实例标识令牌，并向 SPIRE 服务器 gcp_iit 节点验证插件标识自身。 如果 use_instance_metadata 配置值设置为 true，SPIRE 服务器 gcp_iit 节点验证插件调用 GCP API 验证令牌的有效性。 验证完成后，SPIRE 代理被视为经过验证，并分配其自己的 SPIFFE ID。 最后，如果工作负载与注册条目匹配，SPIRE 会向节点上的工作负载发放 SVID。注册条目可以包括节点验证插件或解析器公开的选择器，或者将 SPIRE 代理的 SPIFFE ID 作为父级。 要使用 GCP IIT 节点验证，请在 SPIRE 服务器和 SPIRE 代理上配置并启用 gcp_iit 节点验证插件。\nAmazon EC2 实例 EC2 节点认证和解析允许 SPIRE 服务器自动识别和验证在 AWS EC2 实例上运行的 SPIRE Agent。简而言之，通过以下方式实现：\nSPIRE Agent 的 aws_iid 节点证明插件检索 AWS 实例的实例身份文档，并向 SPIRE Server 的 aws_iid 节点证明插件进行身份验证。 SPIRE Server 的 aws_iid 节点证明插件使用具有有限权限的 AWS IAM 角色调用 AWS API 来验证文档的有效性。 如果配置了 aws_iid 节点解析器插件，则 SPIRE 将使用节点的已验证身份查找有关节点的其他信息。此元数据可以用作注册条目中的选择器。 验证完成后，SPIRE Agent 被视为经过验证的，并被分配其自己的 SPIFFE ID。 最后，如果工作负载与注册条目匹配，SPIRE 为节点上的工作负载发放 SVID。注册条目可以包含节点证明者或解析器提供的选择器， …","relpermalink":"/book/spiffe-and-spire/configuration/configuring/","summary":"要根据你的应用程序需求自定义 SPIRE 服务器和 SPIRE 代理的行为，你需要编辑服务器和代理的配置文件。 如何配置 SPIRE SPIRE 服务器和代理的配置文件分别为 server.conf 和 agent.conf。 默认情况下，服务器期望配置文件位于 conf/se","title":"配置 SPIRE"},{"content":"请见 TSB 文档：https://docs.tetrate.io/service-bridge/reference/k8s-api/tsb-crds-gen 包：\ntsb.tetrate.io/v2 application.tsb.tetrate.io/v2 extension.tsb.tetrate.io/v2 gateway.tsb.tetrate.io/v2 istiointernal.tsb.tetrate.io/v2 rbac.tsb.tetrate.io/v2 security.tsb.tetrate.io/v2 traffic.tsb.tetrate.io/v2 ","relpermalink":"/book/tsb/reference/k8s-api/tsb-crds-gen/","summary":"请见 TSB 文档：https://docs.tetrate.io/service-bridge/reference/k8s-api/tsb-crds-gen 包： tsb.tetrate.io/v2 application.tsb.tetrate.io/v2 extension.tsb.tetrate.io/v2 gateway.tsb.tetrate.io/v2 istiointernal.tsb.tetrate.io/v2 rbac.tsb.tetrate.io/v2 security.tsb.tetrate.io/v2 traffic.tsb.tetrate.io/v2","title":"API 参考"},{"content":"本指南将帮助你实际开始使用“工作负载载入”。\n作为本指南的一部分，你将：\n在你的 Kubernetes 集群中部署 Istio Bookinfo 示例 在 AWS EC2 实例上部署 ratings 应用程序并将其载入到服务网格 验证 Kubernetes Pod(s) 与 AWS EC2 实例之间的流量 在 AWS Auto Scaling Group 上部署 ratings 应用程序并将其载入到服务网格 本指南旨在演示工作负载载入功能，易于跟随。\n为了保持简单，你无需配置基础设施，就像在生产部署的情况下所需的那样。\n具体来说：\n你无需设置可路由的 DNS 记录 你无需使用受信任的 CA 授权（如 Let’s Encrypt） 你无需将 Kubernetes 集群和 AWS EC2 实例放在同一网络或对等网络上 在继续之前，请确保完成以下先决条件：\n创建一个 Kubernetes 集群，以安装 TSB 和示例应用程序 按照 TSB 演示 安装说明操作 创建一个 AWS 帐户以启动 EC2 实例，在那里部署工作负载，并将其载入到服务网格。 安装 Bookinfo 示例\n启用工作负载载入\n配置 WorkloadGroup 和 Sidecar\n配置虚拟机\n虚拟机工作负载载入\n从 AWS Auto Scaling Group 上加入工作负载\n","relpermalink":"/book/tsb/setup/workload-onboarding/quickstart/aws-ec2/","summary":"本指南将帮助你实际开始使用“工作负载载入”。 作为本指南的一部分，你将： 在你的 Kubernetes 集群中部署 Istio Bookinfo 示例 在 AWS EC2 实例上部署 ratings 应用程序并将其载入到服务网格 验证 Kubernetes Pod(s) 与 AWS EC2 实例之间的流量 在 AWS Auto Scaling Group 上部署 ratings 应用程序并将","title":"在 AWS EC2 上快速载入工作负载"},{"content":"如果你的 Elasticsearch 访问受角色限制，你需要确保为 TSB 组件存在正确的角色。\nOAP 对于 OAP，必要的角色权限在下面的 JSON 中描述如下。\n{ \u0026#34;cluster\u0026#34;: [\u0026#34;manage_index_templates\u0026#34;, \u0026#34;monitor\u0026#34;], \u0026#34;indices\u0026#34;: [ { \u0026#34;names\u0026#34;: [\u0026#34;skywalking_*\u0026#34;], \u0026#34;privileges\u0026#34;: [\u0026#34;manage\u0026#34;, \u0026#34;read\u0026#34;, \u0026#34;write\u0026#34;], \u0026#34;allow_restricted_indices\u0026#34;: false } ], \u0026#34;applications\u0026#34;: [], \u0026#34;run_as\u0026#34;: [], \u0026#34;metadata\u0026#34;: {}, \u0026#34;transient_metadata\u0026#34;: { \u0026#34;enabled\u0026#34;: true } } 你可以使用 cURL、Kibana 控制台或任何其他工具将此信息发布到 Elasticsearch 服务器以创建角色，然后你可以将该角色分配给将使用的 OAP 用户。\n","relpermalink":"/book/tsb/operations/elasticsearch/elasticsearch-role/","summary":"Elasticsearch 所需权限的概述。","title":"Elasticsearch 权限"},{"content":" Protobuf files 要通过 gRPC 连接与 TSB 通信，您需要 protobuf 和 gRPC 服务定义文件。如果您希望获取它们，请联系您的 Tetrate 账户经理。 在本指南中，您将看到如何使用 TSB gRPC API 执行常见操作。本指南中的示例使用 Go 绑定，因为这是我们默认提供的绑定，但为其他语言生成的 gRPC 客户端也可以工作。\n初始化 gRPC 客户端 传输配置 在创建 gRPC 客户端时，首先要配置的是与 TSB 的连接。根据 TSB 的公开方式，您可以将连接配置为纯文本连接或 TLS 连接。\n配置纯文本连接 要配置纯文本连接，请使用以下 gRPC 拨号选项：\nopts := []grpc.DialOption{ grpc.WithBlock(), // 在调用 Dial 时阻塞，直到连接真正建立 grpc.WithInsecure(), } tsbAddress := \u0026#34;\u0026lt;tsb 主机\u0026gt;:\u0026lt;tsb 端口\u0026gt;\u0026#34; cc, err := grpc.DialContext(ctx, tsbAddress, opts...) 第一个选项将指导 gRPC 客户端在 Dial 调用上阻塞，直到建立连接，第二个选项配置纯文本传输凭据。\n配置 TLS 连接 如果 TSB 通过 TLS 公开，则应按如下方式启用 TLS 连接：\ntlsConfig := \u0026amp;tls.Config{} // 使用主机 CA 的默认 TLS 配置 creds := credentials.NewTLS(tlsConfig) opts := []grpc.DialOption{ grpc.WithBlock(), // 在调用 Dial 时阻塞，直到连接真正建立 grpc.WithTransportCredentials(creds), } tsbAddress := \u0026#34;\u0026lt;tsb 主机\u0026gt;:\u0026lt;tsb 端口\u0026gt;\u0026#34; cc, err := grpc.DialContext(ctx, tsbAddress, opts...) 默认 TLS 配置将使用主机 CA 来验证 TSB 服务器呈现的证书。如果证书无法由该 CA 验证，因为它是自签名的，或因为其根 CA 不是公共 CA，则请按如下方式使用自定义 CA 配置 TLS 连接：\n// 从文件中读取自定义 CA 捆绑包 ca, err := ioutil.ReadFile(\u0026#34;custom-ca.crt\u0026#34;) if err != nil { return fmt.Errorf(\u0026#34;加载 CA 文件失败：%w\u0026#34;, err) } // 将 CA 捆绑包加载到 x509 证书池中 certs := x509.NewCertPool() if ok := certs.AppendCertsFromPEM(ca); !ok { return errors.New(\u0026#34;加载 CA 出错\u0026#34;) } // 配置 TLS 选项以使用加载的根 CA tlsConfig := \u0026amp;tls.Config{ RootCAs: certs, } 最后，如果要建立 TLS 连接，但不想验证服务器证书，可以通过以下方式配置 TLS 选项来跳过验证：\ntlsConfig := \u0026amp;tls.Config{ InsecureSkipVerify: true, } 这将指示 gRPC 客户端建立 TLS 连接，而无需验证 TSB 提交的证书。\n身份验证 一旦传输选项已配置，您还必须设置身份验证。身份验证配置为 credentials.PerRPCCredentials 对象。\nTSB 有两种主要的身份验证机制：基本身份验证和 JWT 令牌身份验证。\n基本身份验证 为了配置基本身份验证，您可以使用一个实现所需 gRPC 接口的辅助对象。以下代码显示了如何配置它的示例：\n// BasicAuth 是 gRPC 的 HTTP 基本凭据提供程序 type BasicAuth struct { Username string Password string } // GetRequestMetadata 实现 credentials.PerRPCCredentials func (b BasicAuth) GetRequestMetadata(_ context.Context, _ ...string) (map[string]string, error) { auth := b.Username + \u0026#34;:\u0026#34; + b.Password enc := base64.StdEncoding.EncodeToString([]byte(auth)) return map[string]string{\u0026#34;authorization\u0026#34;: \u0026#34;Basic \u0026#34; + enc}, nil } // RequireTransportSecurity 实现 credentials.PerRPCCredentials func (BasicAuth) RequireTransportSecurity() bool { return false } 有了这个对象，可以如下配置 gRPC 客户端以使用基本身份验证：\nauth := BasicAuth{ Username: \u0026#34;username\u0026#34;, Password: \u0026#34;password\u0026#34;, } opts := []grpc.DialOption{ grpc.WithBlock(), // 在调用 Dial 时阻塞，直到连接真正建立 grpc.WithPerRPCCredentials(auth), } tsbAddress := \u0026#34;\u0026lt;tsb 主机\u0026gt;:\u0026lt;tsb 端口\u0026gt;\u0026#34; cc, err := grpc.DialContext(ctx, tsbAddress, opts...) JWT 令牌身份验证 对于基于 JWT 令牌的身份验证，使用一个支持对象来简化配置 gRPC 选项：\n// TokenAuth 是 gRPC 的基于 JWT 的凭据提供程序 type TokenAuth string // GetRequestMetadata 实现 credentials.PerRPCCredentials func (t TokenAuth) GetRequestMetadata(_ context.Context, _ ...string) (map[string]string, error) { return map[string]string{\u0026#34;x-tetrate-token\u0026#34;: string(t)}, nil } // RequireTransportSecurity 实现 credentials.PerRPCCredentials func (TokenAuth) RequireTransportSecurity() bool { return false } 然后，使用以下 DialOptions 配置 gRPC 客户端：\nauth := TokenAuth(\u0026#34;jwt-token\u0026#34;) opts := []grpc.DialOption{ grpc.WithBlock(), // 在调用 Dial 时阻塞，直到连接真正建立 grpc.WithPerRPCCredentials(auth), } tsbAddress := \u0026#34;\u0026lt;tsb 主机\u0026gt;:\u0026lt;tsb 端口\u0026gt;\u0026#34; cc, err := grpc.DialContext(ctx, tsbAddress, opts...) 示例：创建组织、租户和工作空间 以下示例演示了如何使用配置的连接实例化不同 TSB gRPC 客户端，以访问 TSB 的不同 API。\nvar ( tsbAddress = \u0026#34;\u0026lt;tsb 主机\u0026gt;:\u0026lt;tsb 端口\u0026gt;\u0026#34; username = \u0026#34;username\u0026#34; password = \u0026#34;password\u0026#34; tlsConfig = \u0026amp;tls.Config{ // 添加任何自定义 TLS 选项 } ) // ---------------------------------- // 连接配置 // ---------------------------------- opts := []grpc.DialOption{ grpc.WithBlock(), grpc.WithTransportCredentials(credentials.NewTLS(tlsConfig)), grpc.WithPerRPCCredentials(BasicAuth{ Username: username, Password: password, }), } cc, err := grpc.DialContext(context.Background(), tsbAddress, opts...) if err != nil { panic(err) } // ------------------------------------------------ // 创建 gRPC 客户端并使用 API // ------------------------------------------------ var ( orgsClient = v2.NewOrganizationsClient(cc) tenantsClient = v2.NewTenantsClient(cc) workspacesClient = v2.NewWorkspacesClient(cc) ) ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() org, err := orgsClient.CreateOrganization(ctx, \u0026amp;v2.CreateOrganizationRequest{ Name: \u0026#34;myorg\u0026#34;, Organization: \u0026amp;v2.Organization{ DisplayName: \u0026#34;My Organization\u0026#34;, Description: \u0026#34;Organization created using the TSB gRPC API\u0026#34;, }, }) if err != nil { panic(err) } fmt.Printf(\u0026#34;Created organization %q (%s)\\n\u0026#34;, org.DisplayName, org.Fqn) tenant, err := tenantsClient.CreateTenant(ctx, \u0026amp;v2.CreateTenantRequest{ Parent: org.Fqn, // 在我们刚刚创建的组织中创建租户 Name: \u0026#34;mytenant\u0026#34;, Tenant: \u0026amp;v2.Tenant{ DisplayName: \u0026#34;My Tenant\u0026#34;, Description: \u0026#34;Tenant created using the TSB gRPC API\u0026#34;, }, }) if err != nil { panic(err) } fmt.Printf(\u0026#34;Created tenant %q (%s)\\n\u0026#34;, tenant.DisplayName, tenant.Fqn) ws, err := workspacesClient.CreateWorkspace(ctx, \u0026amp;v2.CreateWorkspaceRequest{ Parent: tenant.Fqn, // 在我们刚刚创建的租户中创建工作空间 Name: \u0026#34;myworkspace\u0026#34;, Workspace: \u0026amp;v2.Workspace{ DisplayName: \u0026#34;My Workspace\u0026#34;, Description: \u0026#34;Workspace created using the TSB gRPC API\u0026#34;, NamespaceSelector: …","relpermalink":"/book/tsb/reference/grpc-api/guide/","summary":"介绍如何使用我们的 gRPC API 与 TSB 进行通信的指南。","title":"gRPC API 指南"},{"content":"在开始之前，你必须具备以下条件：\nVault 1.3.1 或更新版本 Vault 注入器 0.3.0 或更新版本 设置 Vault 安装 Vault（不需要在 Kubernetes 集群中安装，但应该能够从 Kubernetes 集群内部访问）。Vault 注入器（agent-injector）必须安装到集群中，并配置以注入 sidecar。这可以通过自动完成 Helm 图表 v0.5.0+ 来实现，该图表安装了 Vault 0.12+ 和 Vault 注入器 0.3.0+。下面的示例假设 Vault 安装在 tsb 命名空间中。\n有关详细信息，请查看 Vault 文档 。\nhelm install --name=vault --set=\u0026#39;server.dev.enabled=true\u0026#39; ./vault-helm 为 PostgreSQL 设置数据库秘密引擎 在 Vault 中启用数据库秘密引擎。\nvault secrets enable database 预期输出：\nSuccess! Enabled the database secrets engine at: database/ 默认情况下，秘密引擎在与引擎同名的路径上启用。要在不同路径上启用秘密引擎，请使用 -path 参数。\n使用适当的插件和连接信息配置 Vault。在 connection_url 参数中，将 postgres.tsb.svc:5432/tsb 替换为你的 PostgreSQL 集群的完整 host:port/db_name。只需更改 URL 中的小写 username 和 password，不要编辑在 URL 中的 {{ }}，它用作模板：\nvault write database/config/tsb \\ plugin_name=postgresql-database-plugin \\ allowed_roles=\u0026#34;pg-role\u0026#34; \\ connection_url=\u0026#34;postgresql://{{username}}:{{password}}@postgres.tsb.svc:5432/tsb?sslmode=disable\u0026#34; \\ username=\u0026#34;\u0026lt;postgres-username\u0026gt;\u0026#34; \\ password=\u0026#34;\u0026lt;postgres-password\u0026gt;\u0026#34; 你可以使用 read 操作来查看配置：\nvault read database/config/tsb # Key Value # --- ----- # allowed_roles [pg-role] # connection_details map[connection_url:postgresql://{{username}}:{{password}}@postgres.tsb.svc:5432/?sslmode=disable username:postgres] # plugin_name postgresql-database-plugin # root_credentials_rotate_statements [] 配置一个角色，将 Vault 中的名称映射到 Vault 可以执行以创建数据库凭据的模板化 SQL 语句。\nmax_ttl 定义了新凭证的有效时间。\ndefault_ttl 定义了租约时间，Vault 注入器将续订租约，直到达到 max_ttl。\nTTL 值必须与应用程序的数据库连接生命周期配对，以确保在 TTL 到期之前关闭它们。\n运行以下命令，确保不要编辑 {{ }} 之间的参数，因为它们被 Vault 用作模板：\nvault write database/roles/pg-role \\ db_name=tsb \\ creation_statements=\u0026#34;CREATE ROLE \\\u0026#34;{{name}}\\\u0026#34; WITH LOGIN PASSWORD \u0026#39;{{password}}\u0026#39; VALID UNTIL \u0026#39;{{expiration}}\u0026#39;; \\ GRANT ALL ON ALL TABLES IN SCHEMA public TO \\\u0026#34;{{name}}\\\u0026#34;;\u0026#34; \\ default_ttl=\u0026#34;12h\u0026#34; \\ max_ttl=\u0026#34;24h\u0026#34; Success! Data written to: database/roles/pg-role 再次使用 read 操作来验证设置：\nvault read database/roles/pg-role # Key Value # --- ----- # creation_statements [CREATE ROLE \u0026#34;{{name}}\u0026#34; WITH LOGIN PASSWORD \u0026#39;{{password}}\u0026#39; VALID UNTIL \u0026#39;{{expiration}}\u0026#39;; GRANT SELECT ON ALL TABLES IN SCHEMA public TO \u0026#34;{{name}}\u0026#34;;] # db_name tsb # default_ttl 24h # max_ttl 24h # renew_statements [] # revocation_statements [] # rollback_statements [] 现在，通过使用角色名称从 /creds 终端点生成新凭据。这是 Vault 注入器将用于为你的 Kubernetes 应用程序获取凭据的机制：\nvault read database/creds/pg-role Key Value --- ----- lease_id database/creds/pg-role/tUEs8eogkk9KL5erU5rLv7hD lease_duration 24h lease_renewable true password A1a-1ZYMcUHKJIJH6rrc username v-token-pg-role-KQ4ze3GYi5He0D70tEmo-1587973449 设置 Kubernetes 秘密引擎 配置一个名为 “pg-auth” 的策略。这是一个非常不受限制的策略，但在生产环境中，你应该添加更多的限制。\nvault policy write pg-auth - \u0026lt;\u0026lt;EOF path \u0026#34;database/creds/*\u0026#34; { capabilities = [\u0026#34;read\u0026#34;] } EOF Success! Uploaded policy: pg-auth 配置 Vault 以启用对 Kubernetes API 的访问。此示例假设你正在 Vault pod 中使用 kubectl exec 运行命令。如果不是这样，你将需要找到正确的 JWT 令牌、Kubernetes API URL（Vault 将用于连接到 Kubernetes 的 URL）以及 vaultserver 服务帐户的 CA 证书，如 Vault 文档 中所述。\nvault auth enable kubernetes vault write auth/kubernetes/config \\ token_reviewer_jwt=\u0026#34;$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\u0026#34; \\ kubernetes_host=https://${KUBERNETES_PORT_443_TCP_ADDR}:443 \\ kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt 将数据库策略附加到管理命名空间中的服务帐户（在这里是 tsb 命名空间）：\nvault write auth/kubernetes/role/pg \\ bound _service_account_names=* \\ bound_service_account_namespaces=tsb \\ policies=pg-auth \\ ttl=24h 要添加更多限制，为每个 ServiceAccount 创建一个角色。对于 PostgreSQL，你将需要为 tsb-iam、tsb-spm 和 default 服务帐户创建一个角色，因为 TSB API pod 使用 default 服务帐户运行：\nvault write auth/kubernetes/role/pg \\ bound_service_account_names=default,tsb-spm,tsb-iam \\ bound_service_account_namespaces=tsb \\ policies=pg-auth \\ ttl=24h 将凭据注入到 Pod 要在管理平面中使用 Vault Agent 注入器与 PostgreSQL 结合使用，请向 ManagementPlane 自定义资源中的部署 pod 注释和环境变量中添加以下内容。\n使用覆盖层来即时重新配置部署：\nspec: dataStore: postgres: connectionLifetime: 1h # 设置连接生存期 components: apiServer: kubeSpec: deployment: podAnnotations: vault.hashicorp.com/agent-inject: \u0026#39;true\u0026#39; vault.hashicorp.com/agent-init-first: \u0026#39;true\u0026#39; vault.hashicorp.com/agent-inject-secret-config.yaml: \u0026#39;database/creds/pg-role\u0026#39; vault.hashicorp.com/agent-inject-template-config.yaml: | {{- with secret \u0026#34;database/creds/pg-role\u0026#34; -}} data: username: {{ .Data.username }} password: {{ .Data.password }} {{- end -}} vault.hashicorp.com/role: \u0026#39;pg\u0026#39; vault.hashicorp.com/secret-volume-path: /etc/dbvault overlays: - apiVersion: v1 kind: Deployment name: tsb patches: - path: spec.template.spec.containers[name:tsb].args.[:/etc/db/config\\.yaml] value: /etc/dbvault/config.yaml - path: spec.template.spec.initContainers[name:migration].args.[:/etc/db/config\\.yaml] value: /etc/dbvault/config.yaml iamServer: kubeSpec: deployment: podAnnotations: vault.hashicorp.com/agent-inject: \u0026#39;true\u0026#39; vault.hashicorp.com/agent-init-first: \u0026#39;true\u0026#39; vault.hashicorp.com/agent-inject-secret-config.yaml: \u0026#39;database/creds/pg-role\u0026#39; vault.hashicorp.com/agent-inject-template-config.yaml: | {{- with secret \u0026#34;database/creds/pg-role\u0026#34; -}} data: username: {{ .Data.username }} password: {{ .Data.password }} {{- end -}} …","relpermalink":"/book/tsb/operations/vault/postgresql/","summary":"如何将 Vault Agent 注入器与 PostgreSQL 结合使用。","title":"PostgreSQL 凭据"},{"content":"在本指南中，您将学习如何使用 TSB REST API 执行常见操作。本指南中的示例使用 curl，因为它是用于执行 HTTP 请求的常用命令，但任何可以执行 HTTP 请求的工具都可以使用。\n身份验证 TSB 有两种主要的身份验证机制：基本身份验证和 JWT 令牌身份验证。\n基本身份验证 基本 HTTP 身份验证 通过在 HTTP Authorization 头中发送编码在头值中的凭据来完成。头的基本格式如下：\nAuthorization: Basic base64(username:password) 例如：\nAuthorization: Basic dGVzdDoxMjPCow== JWT 令牌身份验证 JWT 令牌身份验证是基于头的，并通过在 x-tetrate-token 头中设置 JWT 令牌来配置。例如：\nx-tetrate-token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c HTTP 动词 TSB REST API 使用常见的 HTTP 动词来建模对不同 TSB 资源进行的所有操作：\nGET 请求用于获取资源列表或获取特定对象的详细信息。 POST 请求用于创建新资源。 PUT 请求用于修改现有资源。 DELETE 请求用于删除资源及其子资源。 示例：常见资源 CRUD 操作 以下示例显示了如何使用 REST API 对 TSB 资源执行常见的 CRUD 操作：\n创建资源 在此示例中，您将在现有组织中创建一个租户。您可以通过向 TSB REST API 发送相应的 POST 请求并使用基本身份验证来执行此操作：\n$ curl -u username:password \\ https://tsbhost:8443/v2/organizations/myorg/tenants \\ -X POST -d@- \u0026lt;\u0026lt;EOF { \u0026#34;name\u0026#34;: \u0026#34;mytenant\u0026#34;, \u0026#34;tenant\u0026#34;: { \u0026#34;displayName\u0026#34;: \u0026#34;My tenant\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Tenant created using the TSB REST API\u0026#34; } } EOF 输出：\n{\u0026#34;fqn\u0026#34;:\u0026#34;organizations/myorg/tenants/mytenant\u0026#34;,\u0026#34;displayName\u0026#34;:\u0026#34;My tenant\u0026#34;,\u0026#34;etag\u0026#34;:\u0026#34;\\\u0026#34;hhO8m7WN3LM=\\\u0026#34;\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;Tenant created using the TSB REST API\u0026#34;} 修改资源 通过在 PUT 请求中发送更新后的对象来修改资源。\n要更新对象，您需要拥有它的最新副本。TSB 具有防止并发更新和避免相同对象的冲突版本的机制。为此，TSB 为每个对象分配一个 etag，并在每次对象更新时更新它。在 PUT 请求中，必须发送对象的最新 etag，以告诉 TSB 您正在修改对象的最新版本。\n在此示例中，您将执行以下操作：\n✓ 首先发送 GET 请求，以获取对象的最新版本（使用最新的 etag 更新）。\n✓ 在返回的 JSON 中进行本地修改。\n✓ 将修改后的 JSON 文档发送回以 PUT 请求。\ncurl -u username:password \\ https://tsbhost:8443/v2/organizations/myorg/tenants/mytenant \\ -X GET | jq . 输出：\n{ \u0026#34;fqn\u0026#34;: \u0026#34;organizations/myorg/tenants/mytenant\u0026#34;, \u0026#34;displayName\u0026#34;: \u0026#34;My tenant\u0026#34;, \u0026#34;etag\u0026#34;: \u0026#34;\\\u0026#34;hhO8m7WN3LM=\\\u0026#34;\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Tenant created using the TSB REST API\u0026#34; } 修改 JSON 文档并将其发送回。重要的是要保留 etag 字段：\ncurl -u username:password \\ https://tsbhost:8443/v2/organizations/myorg/tenants/mytenant \\ -X PUT -d@- \u0026lt;\u0026lt;EOF { \u0026#34;fqn\u0026#34;: \u0026#34;organizations/myorg/tenants/mytenant\u0026#34;, \u0026#34;displayName\u0026#34;: \u0026#34;My Modified tenant\u0026#34;, \u0026#34;etag\u0026#34;: \u0026#34;\\\u0026#34;hhO8m7WN3LM=\\\u0026#34;\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Modified description\u0026#34; } EOF 输出：\n{\u0026#34;fqn\u0026#34;:\u0026#34;organizations/myorg/tenants/mytenant\u0026#34;,\u0026#34;displayName\u0026#34;:\u0026#34;My Modified tenant\u0026#34;,\u0026#34;etag\u0026#34;:\u0026#34;\\\u0026#34;BhsObrdJUWI=\\\u0026#34;\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;Modified description\u0026#34;} 删除资源 通过向资源的 URL 发送相应的 DELETE 请求来删除资源。如果将请求发送到父资源，则还将删除所有子资源。\ncurl -u username:password \\ https://tsbhost:8443/v2/organizations/myorg/tenants/mytenant \\ -X DELETE ","relpermalink":"/book/tsb/reference/rest-api/guide/","summary":"介绍如何使用我们的 REST API 与 TSB 进行通信的指南。","title":"REST API 指南"},{"content":" Access Bindings\nAPI\nAPI Access Bindings\nApplication\nApplication Access Bindings\nApplication Service\nApprovals Service\nAuth\nAuth\nCluster Service\nClusters\nCommon Object Types\nEast/West Gateway\nEgress Gateway\nGateway\nGateway Access Bindings\nGateway Common Configuration Messages\nGateway Group\nGateway Service\nInfo\nIngress Gateway\nIstio Direct Mode Gateway Service\nIstio Direct Mode Security Service\nIstio Direct Mode Traffic Service\nIstio Internal Access Bindings\nIstio Internal Direct Mode Service\nIstio Internal Direct Mode Service\nIstio internal Group\nMetric\nMetric Service\nOpenAPI Extensions\nOrganization\nOrganization Access Bindings\nOrganization Setting\nOrganizations Service\nPermissions\nPermissions Service\nPolicy Bindings\nPolicy Service\nRegistered Service\nRole\nRole Service\nSecurity Access Bindings\nSecurity Group\nSecurity Service\nSecurity Setting\nService Registry Lookup Service\nService Route\nService Security Setting\nServive Registry Registration Service\nSource\nStatus\nStatus Service\nTeams and Users\nTeams Service\nTelemetry Source Service\nTenant\nTenant Access Bindings\nTenant Service\nTenant Setting\nTier1 Gateway\nTraffic Access Bindings\nTraffic Group\nTraffic Service\nTraffic Setting\nWAF Settings\nWASM Extension\nWasmExtension Service\nWorkspace\nWorkspace Access Bindings\nWorkspace Service\nWorkspace Setting\n","relpermalink":"/book/tsb/refs/tsb/","summary":"Access Bindings\nAPI\nAPI Access Bindings\nApplication\nApplication Access Bindings\nApplication Service\nApprovals Service\nAuth\nAuth\nCluster Service\nClusters\nCommon Object Types\nEast/West Gateway\nEgress Gateway\nGateway\nGateway Access Bindings\nGateway Common Configuration Messages\nGateway Group\nGateway Service\nInfo\nIngress Gateway\nIstio Direct Mode Gateway Service\nIstio Direct Mode Security Service\nIstio Direct Mode Traffic Service\nIstio Internal Access Bindings\nIstio Internal Direct Mode Service\nIstio Internal Direct Mode Service\nIstio internal Group\nMetric\nMetric Service\nOpenAPI Extensions\nOrganization\nOrganization Access Bindings\nOrganization Setting\nOrganizations Service\nPermissions\nPermissions Service\nPolicy Bindings\nPolicy Service\nRegistered Service\nRole\nRole Service\nSecurity Access Bindings","title":"tsb"},{"content":"概述 本文介绍如何使用 Helm Charts 来安装 Tetrate Service Bridge (TSB) 的不同组件。假设你的系统上已经安装了 Helm 。\nTSB 为其每一个平面 都提供了 Chart：\n管理平面 ：安装 TSB 管理平面 Operator（可选择安装 MP CR 和/或密钥）。 控制平面 ：安装 TSB 控制平面 Operator（可选择安装 MP CR 和/或密钥）。 数据平面 ：安装 TSB 数据平面 Operator。 每个 Chart 都安装了相应平面的 Operator。管理平面和控制平面都允许创建触发 Operator 的相应资源（使用 spec 属性）以部署所有 TSB 组件和/或必需的密钥（使用 secrets 属性）以使其正常运行。\n这种行为让你选择完全配置 TSB 并与 CD 流水线集成的方式。你可以使用 Helm 来：\n仅安装 Operator 安装/升级平面资源（管理平面或控制平面 CR）以及 Operator 安装/升级Operator和密钥 一次安装/升级它们（Operator、资源、密钥） 关于密钥，要牢记 helm install/upgrade 命令接受可以由不同来源提供的不同文件，使用其中一个源提供规范，另一个源提供密钥。\n还有一个额外的配置 (secrets.keep)，用于保留已安装的密钥并避免删除它们。有了这个功能，密钥只需应用一次，以后的升级不会删除它们。\n默认情况下，Helm Chart 还会安装 TSB CRD。如果你希望跳过 CRD 安装步骤，可以传递 --skip-crds 标志。\n安装过程 先决条件 在开始之前，请确保你已经：\n检查了要求 安装了 Helm 安装了 kubectl 同步 了 Tetrate Service Bridge 镜像 配置 Helm 存储库 添加存储库：\nhelm repo add tetrate-tsb-helm \u0026#39;https://charts.dl.tetrate.io/public/helm/charts/\u0026#39; helm repo update 列出可用版本：\nhelm search repo tetrate-tsb-helm -l 安装 前往 管理平面安装 来安装 TSB 管理平面组件 。\n前往 控制平面安装 以将 TSB 控制平面组件 安装到你的应用程序集群中。这将引入你的应用程序集群到 TSB 中。\n前往 数据平面安装 来安装将管理网关生命周期的 TSB 数据平面组件 到你的应用程序集群中。\n基于版本的控制平面 当你使用基于版本的控制平面时，不再需要 Data Plane Operator 来管理 Istio 网关，你可以跳过数据平面安装。要了解有关基于版本的控制平面的更多信息，请前往 Istio 隔离边界 。 ","relpermalink":"/book/tsb/setup/helm/helm/","summary":"如何使用 Helm Charts 安装 Tetrate Service Bridge (TSB) 的不同组件。","title":"TSB Helm Chart"},{"content":"Tetrate Servcie Bridge（TSB）基于开源的 Istio、Envoy 和 SkyWalking 建立，是 Tetrate 的旗舰产品。本手册将帮助你全面地了解 TSB。无论你是应用程序开发人员、平台运维者，我们都会定制内容来满足你的需求。如果你遇到任何障碍，请放心，我们随时提供支持。\n对于应用程序开发人员 作为使用 TSB 将应用程序部署到环境中的应用程序开发人员，你将体验到简化的过程。首先使用 Sidecar 代理 部署你的应用程序。然后，深入研究高级配置，例如将流量路由到应用程序、实施速率限制或在虚拟机和 Kubernetes 应用程序之间划分流量以实现逐步现代化。\n理解关键概念 掌握服务网格架构 探索 TSB 的架构 高效的交通管理 TSB 的全局可观测性 部署和配置应用程序 使用 Sidecar 部署应用程序 - 如果需要，请熟悉 Istio 的故障排除资源。 为外部流量配置 TSB 使用 OpenAPI 注释 高效的应用管理 监控指标和跟踪 解决常见用例 将流量引导至应用程序 实施速率限制 逐步金丝雀发布 将虚拟机流量迁移到 Kubernetes 跨集群故障转移 参考资料 TSB 常见问题解答 Istio 官方文档 对于平台运维者 对于使用 TSB 将集群转变为统一网格的平台运维者来说，旅程从安装 TSB 的管理平面 开始。你还将加入应用程序集群以实现可观测性和控制，并通过演示应用程序部署 掌握应用程序部署过程。\n掌握基本概念 掌握服务网格架构 深入了解 TSB 的架构 ，包括管理、控制和数据平面 高效的交通管理 TSB 的全局可观测性 了解配置数据流 资源和权限的层次结构 (IAM) 安装、配置和操作 TSB 资源规划 安装 TSB 的管理平面 设置 OIDC 登录 - 使用 OIDC 修改基于 LDAP 的登录。 应用程序板载集群 部署和配置入口代理 了解证书要求 升级 TSB 版本 管理 TSB ImagePullSecret 将 GitOps 与服务网格结合使用 监控配置状态 管理与运维 TSB 访问管理 应用程序和 TSB 的默认日志级别 TSB 组件警报指南 使用 TSB 的调试容器进行故障排除 实施 GitOps 参考资料 TSB 常见问题解答 TSB 安装和 OIDC 参考 TSB 通信的防火墙配置 对于安全管理员 服务网格使安全团队能够集中实施和执行策略，同时保持开发人员的敏捷性。\n掌握关键概念 了解服务网格架构 探索高级 TSB 安全概述 探索管理平面/运行时拆分 管理平面安全 IAM：资源和权限层次结构 深入研究运行时架构 应用程序运行时安全 了解服务身份 实施服务到服务授权 （TSB 上的薄层） 使用网格对最终用户进行身份验证 对应用程序实施控制 在各处强制实施 (m)TLS 应用服务到服务的身份验证和授权 管理到外部服务的出站 实施最终用户身份验证 配置 Envoy 的外部授权 API 确保控制措施得到执行 通过全局可观测性监控服务间流量 审核日志概述和 API TSB 的访问管理 通过灵活的 RBAC 来利用租户、工作区和组 TSB 连接的防火墙要求 参考资料 TSB 常见问题解答 Istio 安全概述 TSB 的 RBAC 访问控制 API 参考 ","relpermalink":"/book/tsb/","summary":"Tetrate Service Bridge（TSB）中文文档：安装、使用和升级。","title":"Tetrate Service Bridge 手册"},{"content":"本文档解释了如何在 TSB 中使用 GitOps 工作流。该文档假设已在管理平面集群 和/或应用程序集群中启用了 GitOps。\nTSB 中 GitOps 支持的主要思想是允许：\n管理员团队可以直接在管理平面集群中创建 TSB 配置资源。 应用程序团队可以直接在应用程序集群中创建 TSB 配置资源。 应用程序团队可以像推送应用程序本身的更改一样推送应用程序配置的更改，并允许将应用程序部署资源和 TSB 配置打包在一起，例如在同一个 Helm 图中。\n为了实现这一点，所有 TSB 配置对象都存在于 Kubernetes 自定义资源定义（CRD）中，以便可以轻松应用于集群。如下图所示，一旦资源应用到集群中，它们将被自动协调并转发到管理平面。\nTSB Kubernetes 自定义资源 用于 TSB 配置的 Kubernetes 自定义资源与任何其他 Kubernetes 资源一样。以下示例显示了一个 Workspace 定义：\napiVersion: tsb.tetrate.io/v2 kind: Workspace metadata: name: bookinfo annotations: tsb.tetrate.io/organization: tetrate tsb.tetrate.io/tenant: engineering spec: namespaceSelector: names: - \u0026#34;*/bookinfo\u0026#34; 它们与你可以使用 tctl 应用 的资源非常相似，不同之处在于：\nspec 的内容在 YAML API 参考 中定义。规格与你在 tctl 中使用的规格相同。 元数据部分不具有 TSB 属性，如 organization、tenant 等。相反，必须使用以下适当的注释提供层次结构信息： tsb.tetrate.io/organization tsb.tetrate.io/tenant tsb.tetrate.io/workspace tsb.tetrate.io/trafficGroup tsb.tetrate.io/securityGroup tsb.tetrate.io/gatewayGroup tsb.tetrate.io/istioInternalGroup tsb.tetrate.io/application 除以下内容之外，apiVersion 和 kind 属性对于所有资源都是相同的： API 组 api.tsb.tetrate.io/v2 改为 tsb.tetrate.io/v2。 请参阅 TSB Kubernetes API 以下载 TSB Kubernetes CRD。\n使用 Istio 直连模式资源 在使用 GitOps 与 Istio 直连模式资源时，需要为资源添加一个附加标签：\nlabels: istio.io/rev: \u0026#34;tsb\u0026#34; 例如，在 Gateway 组中的 Gateway 对象如下所示：\napiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: bookinfo-gateway namespace: bookinfo labels: istio.io/rev: tsb annotations: tsb.tetrate.io/organization: tetrate tsb.tetrate.io/tenant: tetrate tsb.tetrate.io/workspace: bookinfo tsb.tetrate.io/gatewayGroup: bookinfo spec: selector: app: tsb-gateway-bookinfo servers: - hosts: - \u0026#34;bookinfo.tetrate.io\u0026#34; port: number: 80 name: http protocol: HTTP 这是为了防止集群中正在运行的 Istio 立即处理该资源，因为它只应该由 TSB 中继读取，然后推送到管理平面。有一个验证 Webhook 将检查所有需要此标签的资源，如果缺少它，则会拒绝它们。\n应用 TSB 自定义资源 TSB 自定义资源可以使用 kubectl 正常应用。例如，要应用上面示例中的工作区，你只需运行：\nkubectl apply -f workspace.yaml kubectl get workspaces -A NAMESPACE NAME PRIVILEGED TENANT AGE bookinfo bookinfo engineering 4m20s 如果你想要验证对象是否已在管理平面中正确创建，你也可以使用 tctl 在那里查看对象：\n$ tctl get ws bookinfo NAME DISPLAY NAME DESCRIPTION bookinfo 与持续部署解决方案集成 TSB GitOps 功能允许你轻松将 TSB 配置工作流与 CI/CD 解决方案集成。以下页面提供了一些配置示例，你可以按照这些示例来了解它的工作原理：\n配置 Flux CD 以在 TSB 中使用 GitOps ","relpermalink":"/book/tsb/howto/gitops/gitops/","summary":"如何在 TSB 中使用 GitOps 工作流进行应用配置。","title":"TSB 中的 GitOps"},{"content":"本文将描述什么是 WASM 扩展以及其好处。\n什么是 WASM 扩展？ WASM 扩展是WebAssembly 的软件插件，可用于扩展 Istio 代理（Envoy）。 这些 WASM 扩展在一个沙盒环境中执行，对外部系统的访问受到限制，并且可以使用不同的编程语言及其 SDK 创建。 这个沙盒环境提供了隔离，以防止一个插件中的编程错误或崩溃影响其他插件，并提供了安全性，以防止一个插件从系统获取信息。\nWASM 扩展的好处是什么？ Envoy 可以使用过滤器进行扩展，有各种内置的过滤器 用于不同的协议，可以配置为在网络流量的一部分执行。 通过这些过滤器（网络、HTTP）的组合，你可以增强传入请求、转换协议、收集统计信息、修改响应、执行身份验证等等。\n为了拥有自定义过滤器，有几种选择：\n使用 C++ 编写自己的过滤器并将其与 Envoy 打包。 这意味着重新编译 Envoy 并维护不同版本。 使用依赖于 HTTP Lua 过滤器的 Lua 脚本。 适用于简单的脚本和更复杂的部署过程。 使用基于 WASM 的扩展 允许使用不同的编程语言编写复杂的脚本，并自动化部署过程。 一些 WASM 扩展的好处包括：\n使用自定义功能扩展网关 应用有效载荷验证（在 Istio 过滤器上不可能，因为它们只操作元数据） 快速应对 CVE 或 0 天漏洞（例如 Log4Shell） 在 AUTHZ 和 AUTHN 上添加自定义安全验证 改善应用程序的安全性，而不触及其代码库 进一步阅读：\nWASM 模块和 Envoy 的可扩展性解释 为什么 WebAssembly 即使在浏览器之外也具有创新性 WebAssembly 对你的应用程序安全性和可扩展性能做什么 ","relpermalink":"/book/tsb/howto/wasm/wasm-overview/","summary":"展示了什么是 WASM 扩展及其好处。","title":"WASM 扩展概述"},{"content":"在本指南中，你将学习如何使用 TSB CLI（tctl）执行常见操作。你将学习如何配置 CLI 以访问你的 TSB 安装，并如何从命令行管理 TSB 资源。\n入门 要使用 YAML API，你需要安装并配置 TSB CLI。一旦你安装并配置了 CLI 以与你的 TSB 安装进行通信，你将需要使用 tctl login 命令配置对 TSB 平台的访问权限：\ntctl login 你将被要求提供 TSB 组织名称，这在 TSB 安装过程中已设置或提供给你，租户名称以及凭据。\n平台管理员应该已经为你分配了一个租户。如果没有，或者你是执行初始设置的管理员，可以将租户留空。你始终可以稍后编辑已配置的用户并在需要时设置租户。\nOrganization: tetrate Tenant: Username: admin Password: Login Successful! Configured user: demo-admin User \u0026#34;demo-admin\u0026#34; enabled in profile: demo 重要提示 用户设置中预先配置的组织和租户仅用于 tctl get 和 tctl delete 命令。在使用 tctl apply 创建或修改资源时，组织和租户将从每个资源的 metadata 部分中获取，如下所示。 YAML API 基础知识 TSB YAML API 具有声明性语义。所有 TSB 对象共享一组用于唯一标识资源在资源层次结构中位置的属性，以及一个特定的模型，其中包含属于该特定资源的值。例如，以下 TSB 资源配置了给定流量组的流量设置：\n# 块 1 - 资源类型 apiVersion: traffic.tsb.tetrate.io/v2 kind: TrafficSetting # 块 2 - 资源元数据 metadata: name: defaults group: helloworld workspace: helloworld tenant: tetrate organization: tetrate # 块 3 - 资源内容 spec: reachability: mode: GROUP resilience: circuitBreakerSensitivity: MEDIUM 第一个块（apiVersion 和 kind）标识资源的类型。 第二个块定义了资源的 metadata。所有资源都有一个 name 和一组配置资源在资源层次结构中所属位置的元数据属性。 第三个块（spec）包含了资源对象的实际内容。 应用资源 资源是使用 tctl apply 命令应用的。如果应用的资源尚不存在，这将创建它。如果资源已经存在，则该命令将替换其中包含的信息。\n注意 更新操作是完整的对象更新。必须在每次应用操作中发送整个对象，不支持部分更新。 在应用资源时，父资源也必须存在。如果 apply 请求包含多个资源，则必须以正确的顺序提供它们，以确保操作不会因资源缺少其父资源而失败。以下示例显示了如何在现有组织中创建租户和工作空间：\ntctl apply -f - \u0026lt;\u0026lt;EOF apiVersion: api.tsb.tetrate.io/v2 kind: Tenant metadata: organization: tetrate # 此组织必须存在 name: example-tenant # 要创建的租户的名称 spec: displayName: Example Tenant description: An example tenant for the YAML guide --- apiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tetrate tenant: example-tenant # 上面的租户名称 name: first-workspace # 要创建的工作空间的名称 spec: displayName: First Workspace description: An example workspace namespaceSelector: names: - \u0026#34;*/default\u0026#34; --- EOF 列出和获取资源 tctl get 命令检索资源。如果未指定名称，将返回所请求类型的所有资源。如果给定了特定名称，则仅返回请求的资源。\nget 命令的语法为：tctl get \u0026lt;resource type\u0026gt; \u0026lt;parameters\u0026gt;\n其中参数包括可选的资源名称以及配置资源层次结构中资源所属位置的必要标志。\n该命令还接受多个输出参数，以以表格形式（默认）、YAML 或 JSON 检索对象：\n获取配置的租户中的所有工作空间 tctl get workspace 示例输出：\nNAME DISPLAY NAME DESCRIPTION helloworld Helloworld Helloworld application bookinfo Bookinfo Bookinfo application 获取工作空间的详细信息 tctl get workspace helloworld -o yaml 示例输出：\napiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: description: Helloworld application displayName: Helloworld name: helloworld organization: tetrate resourceVersion: \u0026#39;\u0026#34;BePMGaj00FM=\u0026#34;\u0026#39; tenant: tetrate spec: description: Helloworld application displayName: Helloworld etag: \u0026#39;\u0026#34;BePMGaj00FM=\u0026#34;\u0026#39; fqn: organizations/tetrate/tenants/tetrate/workspaces/helloworld namespaceSelector: names: - \u0026#39;*/helloworld\u0026#39; 获取给定流量组中的所有服务路由 注意，我们需要提供标志来指定我们要获取服务路由的工作空间和组：\ntctl get serviceroute \\ --workspace helloworld \\ --trafficgroup helloworld -o yaml 示例输出：\napiVersion: traffic.tsb.tetrate.io/v2 kind: ServiceRoute metadata: group: helloworld name: hello organization: tetrate resourceVersion: \u0026#39;\u0026#34;NWEYABT/fjM=\u0026#34;\u0026#39; tenant: tetrate workspace: helloworld spec: etag: \u0026#39;\u0026#34;NWEYABT/fjM=\u0026#34;\u0026#39; fqn: organizations/tetrate/tenants/tetrate/workspaces/helloworld/trafficgroups/helloworld/serviceroutes/hello service: helloworld/helloworld.helloworld.svc.cluster.local subsets: - labels: version: v1 name: v1 weight: 80 - labels: version: v2 name: v2 weight: 20 删除资源 tctl delete 命令用于删除资源。它遵循与 tctl get 命令相同的语义，只是需要名称参数。\n:::warning 请注意，删除资源将删除资源及其所有子对象，因此请谨慎使用，特别是在删除资源位于资源层次结构较高级别时。 :::\n假设你已经有一个现有的 trafficgroup，你可以使用以下命令查询：\ntctl get trafficgroup --workspace test 示例输出：\nNAME DISPLAY NAME DESCRIPTION test-tg test-tg et-tg 要删除 trafficgroup：\ntctl delete trafficgroup test-tg --workspace test 再次查询它：\ntctl get trafficgroup -w test No resources found ","relpermalink":"/book/tsb/reference/yaml-api/guide/","summary":"介绍如何使用我们的 YAML API 与 TSB 进行通信的指南。","title":"YAML API 指南"},{"content":"为了演示在 Kubernetes 之外部署的工作负载如何与网格的其余部分集成，我们需要有其他应用程序可以与之通信。\n在本指南中，你需要部署 Istio Bookinfo 示例到你的 Kubernetes 集群中。\n部署 Bookinfo 示例 创建命名空间 bookinfo，并添加正确的标签：\nkubectl create namespace bookinfo kubectl label namespace bookinfo istio-injection=enabled 部署 bookinfo 应用程序：\ncat \u0026lt;\u0026lt;EOF | kubectl apply -n bookinfo -f - apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default spec: mtls: mode: STRICT EOF kubectl apply -n bookinfo -f https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/platform/kube/bookinfo.yaml kubectl wait --for=condition=Available -n bookinfo deployments --all 为了从本地环境向 bookinfo 产品页面发送请求，你需要设置端口转发。\n在单独的终端会话中运行以下命令：\nkubectl port-forward -n bookinfo service/productpage 9080 产品页面将在 http://localhost:9080 上可访问。 要在可视上验证产品页面，请在浏览器中打开 http://localhost:9080/productpage。 如果多次刷新页面，你应该在页面上看到 3 次中有 2 次出现评分星级。\n或者，要从命令行验证，请运行：\nfor i in `seq 1 9`; do curl -fsS \u0026#34;http://localhost:9080/productpage?u=normal\u0026#34; | grep \u0026#34;glyphicon-star\u0026#34; | wc -l | awk \u0026#39;{print $1\u0026#34; stars on the page\u0026#34;}\u0026#39; done 3 次中有 2 次应该会得到消息 10 stars on the page：\n10 stars on the page 0 stars on the page 10 stars on the page 缩减 ratings 应用程序 在本指南中，你将通过 VM 通过工作负载载入部署 ratings 应用程序。为了做到这一点，我们必须首先“禁用”与 bookinfo 示例一起部署的默认 ratings 应用程序。\n运行以下命令并将 ratings 应用程序的副本数减少到 0：\nkubectl scale deployment ratings-v1 -n bookinfo --replicas=0 kubectl wait --for=condition=Available -n bookinfo deployment/ratings-v1 要验证 ratings 应用程序已经被缩减，并且不再显示在产品页面上，请按照上一节中的说明访问产品页面。三次中的两次应该会看到消息 Ratings service is currently unavailable。\n","relpermalink":"/book/tsb/setup/workload-onboarding/quickstart/aws-ec2/bookinfo/","summary":"为了演示在 Kubernetes 之外部署的工作负载如何与网格的其余部分集成，我们需要有其他应用程序可以与之通信。 在本指南中，你需要部署 Istio Bookinfo 示例到你的 Kubernetes 集群中。 部署 Bookinfo 示例 创建命名空间 bookinfo，并添加正确的标签： kubectl create","title":"安装 Bookinfo 示例"},{"content":"httpbin 是一个简单的 HTTP 请求和响应服务，用于测试。\n在 TSB 文档中的许多示例中都使用了 httpbin 服务。本文档提供了该服务的基本安装过程。\n请确保参考每个 TSB 文档，以了解特定的注意事项或所需的自定义，以使示例正常工作，因为本文档描述了最通用的安装步骤。\n以下示例假设你已经设置了 TSB，并且已经将 Kubernetes 集群注册到要安装 httpbin 工作负载的 TSB 上。\n除非另有说明，使用 kubectl 命令的示例必须指向相同的集群。在运行这些命令之前，请确保你的 kubeconfig 指向所需的集群。\n命名空间 除非另有说明，假定 httpbin 服务已安装在 httpbin 命名空间中。如果尚未存在，请在目标集群中创建此命名空间。\n运行以下命令以创建命名空间（如果尚不存在）：\nkubectl create namespace httpbin 此命名空间中的 httpbin pod 必须运行 Istio sidecar 代理。要自动启用此 sidecar 对所有 pod 的注入，请执行以下操作：\nkubectl label namespace httpbin istio-injection=enabled --overwrite=true 这将告诉 Istio 需要向稍后创建的 pod 注入 sidecar。\n部署 httpbin Pod 和服务 下载在 Istio 存储库中找到的 httpbin.yaml 清单。\n运行以下命令，在 httpbin 命名空间中部署 httpbin 服务：\nkubectl apply -n httpbin -f httpbin.yaml 暴露 httpbin 服务 下一步可能需要根据使用情况进行或不进行，如果需要 Ingress Gateway，则创建一个名为 httpbin-ingress-gateway.yaml 的文件，其中包含以下内容。\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: httpbin-ingress-gateway namespace: httpbin spec: kubeSpec: service: type: LoadBalancer 然后使用 kubectl 部署它：\nkubectl apply -f httpbin-ingress-gateway.yaml 创建证书 下一步可能需要根据使用情况进行或不进行，如果需要 TLS 证书，可以按照以下步骤准备它们。\n下载脚本 gen-cert.sh 并执行以下操作以生成必要的文件。有关更多详细信息，请参阅此文档 。\nchmod +x ./gen-cert.sh mkdir certs ./gen-cert.sh httpbin httpbin.tetrate.com certs 上述假设你已将 httpbin 服务公开为 httpbin.tetrate.com。如果需要，请相应更改其值。\n一旦你在 certs 目录中生成了必要的文件，请创建 Kubernetes 密钥。\nkubectl -n httpbin create secret tls httpbin-certs \\ --key certs/httpbin.key \\ --cert certs/httpbin.crt 创建 httpbin 工作区 下一步可能需要根据使用情况进行或不进行，如果要创建 TSB 工作区，则按照以下步骤操作。\n在此示例中，我们假设你已经在组织中创建了一个租户。如果尚未创建，请阅读文档中的示例并创建一个 。\n创建名为 httpbin-workspace.yaml 的文件，其中包含类似以下示例的内容。请确保将组织、租户和集群名称替换为适当的值。\n注意 如果你已经安装了 demo 配置文件 ，则已经存在名为 tetrate 的组织和一个名为 demo 的集群。 apiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: \u0026lt;organization\u0026gt; tenant: \u0026lt;tenant\u0026gt; name: httpbin spec: displayName: Httpbin Workspace namespaceSelector: names: - \u0026#34;\u0026lt;cluster\u0026gt;/httpbin\u0026#34; 使用 tctl 应用清单：\ntctl apply -f httpbin-workspace.yaml 创建配置组 下一步可能需要根据使用情况进行或不进行，如果要为此服务创建配置组，则按照以下步骤操作。\n在此示例中，我们假设你已经在组织中创建了一个租户和一个工作区。如果尚未创建，请阅读文档中的示例并创建一个 ，以及创建 httpbin 工作区中的说明。\n创建名为 httpbin-groups.yaml 的文件，其中包含类似以下示例的内容。请确保将组织、租户、工作区和集群名称替换为适当的值。\napiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: organization: \u0026lt;organization\u0026gt; tenant: \u0026lt;tenant\u0026gt; workspace: httpbin name: httpbin-gateway spec: namespaceSelector: names: - \u0026#34;\u0026lt;cluster\u0026gt;/httpbin\u0026#34; configMode: BRIDGED --- apiVersion: traffic.tsb.tetrate.io/v2 kind: Group Metadata: organization: \u0026lt;organization\u0026gt; tenant: \u0026lt;tenant\u0026gt; workspace: httpbin name: httpbin-traffic spec: namespaceSelector: names: - \u0026#34;\u0026lt;cluster\u0026gt;/httpbin\u0026#34; configMode: BRIDGED --- apiVersion: security.tsb.tetrate.io/v2 kind: Group Metadata: organization: \u0026lt;organization\u0026gt; tenant: \u0026lt;tenant\u0026gt; workspace: httpbin name: httpbin-security spec: namespaceSelector: names: - \u0026#34;\u0026lt;cluster\u0026gt;/httpbin\u0026#34; configMode: BRIDGED 使用 tctl 应用清单：\ntctl apply -f httpbin-groups.yaml 完成后，你应该得到 3 个组，一个网关组 (httpbin-gateway)，一个流量组 (httpbin-traffic) 和一个安全组 (httpbin-security)。\n注册 httpbin 应用程序 下一步可能需要根据使用情况进行或不进行，如果要创建 TSB 应用程序，则按照以下步骤操作。\n首先，确保你已经创建了 httpbin 工作区。\n在此工作区中创建一个应用程序。创建名为 httpbin-application.yaml 的文件，其中包含类似以下示例的内容。请确保将组织和租户名称替换为适当的值。\napiVersion: application.tsb.tetrate.io/v2 kind: Application metadata: name: httpbin organization: \u0026lt;organization\u0026gt; tenant: \u0026lt;tenant\u0026gt; spec: displayName: httpbin workspace: organizations/\u0026lt;organization\u0026gt;/tenants/\u0026lt;tenant\u0026gt;/workspaces/httpbin gatewayGroup: organizations/\u0026lt;organization\u0026gt;/tenants/\u0026lt;tenant\u0026gt;/workspaces/httpbin/gatewaygroups/httpbin-gateway 使用 tctl 应用清单：\ntctl apply -f httpbin-application.yaml ","relpermalink":"/book/tsb/reference/samples/httpbin/","summary":"httpbin 是一个简单的 HTTP 请求和响应服务，用于测试。 在 TSB 文档中的许多示例中都使用了 httpbin 服务。本文档提供了该服务的基本安装过程。 请确保参考每个 TSB 文档，以了解特定的注意事项或所需的自定义，以使示例正常工作，因为本文档","title":"安装 httpbin"},{"content":"在继续之前，请确保你熟悉 Istio 隔离边界 功能。\n升级前 从非修订版升级到版本控制平面设置涉及启用 Istio 隔离边界功能。 启用后，可以在隔离边界内配置版本，控制平面必须升级到该版本。 按照 隔离边界安装 中提到的步骤部署具有启用隔离边界功能的控制平面。\n启用 Istio 隔离边界功能后，你需要在添加隔离边界到 ControlPlane CR 之前，将 TSB 数据平面 Operator 的规模缩小。这是为了避免 TSB 数据平面 Operator 和 TSB 控制平面 Operator 在协调相同的 TSB Ingress/Egress/Tier1Gateway 资源时发生竞争条件。\nkubectl scale --replicas=0 deployment tsb-operator-data-plane -n istio-gateway 出于同样的原因，我们还必须将 istio-operator 在 istio-gateway 命名空间中的规模缩小。\nkubectl scale --replicas=0 deployment istio-operator -n istio-gateway 随着这一步，还删除了由 tsb-operator-data-plane 创建和管理的 webhooks。\nkubectl delete validatingwebhookconfiguration tsb-operator-data-plane-egress tsb-operator-data-plane-ingress tsb-operator-data-plane-tier1; \\ kubectl delete mutatingwebhookconfiguration tsb-operator-data-plane-egress tsb-operator-data-plane-ingress tsb-operator-data-plane-tier1; 控制平面升级策略 TSB 仅支持从非修订版到版本的控制平面升级的金丝雀升级。这意味着在任何给定时间点，将部署两个 Istio 控制平面 - 非修订版和版本控制平面。 控制平面 在你的 ControlPlane CR 中配置一个隔离边界。如果使用 Helm，你可以在 Helm 值文件中添加隔离边界配置。\nspec: hub: \u0026lt;registry-location\u0026gt; telemetryStore: elastic: host: \u0026lt;tsb-address\u0026gt; port: \u0026lt;tsb-port\u0026gt; version: \u0026lt;elastic-version\u0026gt; selfSigned: \u0026lt;is-elastic-use-self-signed-certificate\u0026gt; managementPlane: host: \u0026lt;tsb-address\u0026gt; port: \u0026lt;tsb-port\u0026gt; clusterName: \u0026lt;cluster-name-in-tsb\u0026gt; selfSigned: \u0026lt;is-mp-use-self-signed-certificate\u0026gt; components: xcp: isolationBoundaries: - name: global revisions: - name: revisioned centralAuthMode: \u0026#39;JWT\u0026#39; global 隔离边界 尽管我们可以在启用隔离边界支持后部署多个版本控制平面，使用任何 “name” 的边界，但建议创建一个名为 “global” 的隔离边界，以便现有的 Workspace 可以被视为 “global” 隔离边界的一部分。已经在集群中部署的现有工作区将不会绑定到特定的隔离边界，因此 “global” 命名的隔离边界为所有这些未指定其隔离边界的工作区提供了一个后备。 在 ControlPlane CR 中配置隔离边界将在 istio-system 命名空间中设置版本化的控制平面，如下所示\nkubectl get deployment -n istio-system | grep istio-operator # 输出 istio-operator 1/1 1 1 15h istio-operator-revisioned 1/1 1 1 2m kubectl get deployment -n istio-system | grep istiod # 输出 istiod 1/1 1 1 15h istiod-revisioned 1/1 1 1 2m 请注意，仍然部署了一个非修订版的控制平面，负责管理现有的 sidecar 和网关。\n网关升级 要升级网关，请在 Ingress/Egress/Tier1Gateway 资源中 添加 spec.revision 。这将使现有的网关 pod 被调整为连接到版本化的 Istio 控制平面。TSB 默认配置了 Gateway 安装资源，使用 RollingUpdate 策略，确保零停机时间。\n你还可以通过对网关 CR 进行打补丁来添加 spec.revision。\nkubectl patch ingressgateway.install \u0026lt;name\u0026gt; -n \u0026lt;namespace\u0026gt; --type=json --patch \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;,\u0026#34;path\u0026#34;: \u0026#34;/spec/revision\u0026#34;,\u0026#34;value\u0026#34;: \u0026#34;revisioned\u0026#34;}]\u0026#39;; \\ 应用升级 要升级 sidecar，请移除工作负载命名空间标签中的 istio-injection=enabled，并将 istio.io/rev 标签应用于 Istio 版本的工作负载命名空间。\nkubectl label namespace workload-ns istio-injection- istio.io/rev=revisioned 然后重新启动应用工作负载。首选滚动更新以避免流量中断。\nkubectl rollout restart deployment -n workload-ns VM 工作负载升级 要升级 VM 工作负载，请\n使用 版本化链接 从你的入驻平面下载最新的 Istio sidecar，然后在 VM 上重新安装 Istio sidecar。\n使用 revision 值 更新 onboarding-agent 配置，然后重新启动 onboarding-agent。Istio sidecar 将连接到版本化的 Istio 控制平面。\n升级后清理 一旦所有 sidecar 都已移动到版本化代理，所有应用网关都已具备版本化网关，并确保升级正常运行，我们可以继续清理现在已经过时的旧非修订版资源。\n请记住，我们已经将 TSB 数据平面 Operator 和非修订版 istio-operator 从 istio-gateway 命名空间的规模缩小。现在，可以安全地删除 istio-gateway 命名空间，因为不再需要它。\nkubectl delete ns istio-gateway 使用 kubectl 删除位于命名空间 istio-system 中的名为 tsb-istiocontrolplane 的 IstioOperator 资源。\nkubectl delete iop tsb-istiocontrolplane -n istio-system 确保 istio-system 命名空间中的 istiod 部署由 istio-operator 部署删除。然后删除 Istio operator 部署和 Kubernetes RBAC（clusterrole 和 clusterrolebinding）。\nkubectl delete clusterrole,clusterrolebinding istio-operator kubectl delete deployment,sa istio-operator -n istio-system 从版本化回滚到非修订版 在升级后清理之前 将 istio-gateway 命名空间中的 tsb 数据平面 Operator 的规模增加。\nkubectl scale --replicas=1 deployment tsb-operator-data-plane -n istio-gateway 随着此操作，删除由 tsb-operator-control-plane 创建和管理的 webhooks。\nkubectl delete validatingwebhookconfiguration tsb-operator-control-plane-egress tsb-operator-control-plane-ingress tsb-operator-control-plane-tier1; \\ kubectl delete mutatingwebhookconfiguration tsb-operator-control-plane-egress tsb-operator-control-plane-ingress tsb-operator-control-plane-tier1; 要回滚网关，从 TSB 网关安装资源的 Ingress/Egress/Tier1Gateway 中移除 spec.revision。\n对于网关部署，最好配置滚动更新以避免流量中断。这可以在 ingress/Egress/Tier1Gateway 资源中配置。 这将导致网关 pod 启动并连接到仍在运行的较旧的非修订版 Istio 控制平面。\n通过将工作负载命名空间标签中的 istio.io/rev 值更改为 default 来回滚 sidecars。\nkubectl label namespace workload-ns istio.io/rev=default 然后重新启动应用工作负载。\nkubectl rollout restart deployment -n workload-ns 一旦所有数据平面组件都回滚到非修订版的控制平面，我们可以继续从 ControlPlane CR 中删除隔离边界。这将删除在 istio-system 命名空间中部署的版本化控制平面组件。\n在升级后清理之后 网关回滚 在进行升级后的清理之后，将网关从版本化回滚到非修订版控制平面不能保证零停机时间。 首先，我们需要恢复非修订版的控制平面。要获取较旧的非修订版控制平面，请使用禁用了 ISTIO_ISOLATION_BOUNDARIES 的 TSB 集群 Operator 重新安装。\ntctl install manifest cluster-operators --registry $HUB \u0026gt; clusteroperators.yaml kubectl apply -f clusteroperators.yaml 再次部署 Operator 将在 istio-gateway 命名空间中带回 TSB 数据平面 Operator。然后，非修订版的 TSB 控制平面 Operator 将协调更新的 ControlPlane 资源以重新部署非修订版的 Istio 控制平面。 由于已删除隔离边界支持，这还将清理所有版本化的控制平面组件。\n编辑现有的 ControlPlane CR，以删除 spec.components.xcp.isolationBoundaries。\n要回滚网关，请从 TSB 网关安装资源的 Ingress/Egress/Tier1Gateway 中移除 spec.revision。 对于网关部署，最好配置滚动更新以避免流量中断。这可以在 ingress/Egress/Tier1Gateway 资源中配置。 这将导致网关 pod 启动并连接到仍在运行的较旧的非修订版 Istio 控制平面。\n通过将工作负载命名空间标签中的 istio.io/rev 值更改为 default 来回滚 sidecars。 …","relpermalink":"/book/tsb/setup/upgrades/non-revisioned-to-revisioned/","summary":"如何将控制平面集群从非修订版升级到修订版。","title":"非修订版到修订版的升级"},{"content":"Service Mesh 架构已得到广泛采用，Tetrate 的团队由一些最早开发支持该架构的技术的工程师组成。在本节中，我们将介绍该架构、其术语、功能、特性，并重点介绍 Istio，这是为 Tetrate Service Bridge 提供支持的领先网格实现。\n什么是服务网格？ 服务网格是通过代理位于应用程序组件和网络之间的基础设施层。虽然这些组件通常是微服务，但任何工作负载（从无服务器容器到虚拟机或裸机上的传统 n 层应用程序）都可以参与网格。代理不是通过网络在组件之间进行直接通信，而是拦截并管理该通信。\n数据平面 这些代理被称为“sidecar 代理”，因为它们与每个应用程序实例一起部署，构成了服务网格的数据平面。它们在运行时处理应用程序流量。Tetrate Service Bridge 采用 Envoy 作为数据平面实现。Envoy 提供了大量的安全、流量策略和遥测功能，包括：\n服务发现 弹性机制（重试、熔断、异常值检测） 客户端负载均衡 细粒度的 L7 流量控制 根据请求实施安全策略 基于 L7 元数据的身份验证、速率限制、策略 具有强 L7 身份的工作负载身份 服务间授权 使用 WASM 扩展的可扩展性 指标、日志和跟踪 通过将这些功能从应用程序转移到边车代理，可以引入控制平面来动态配置数据平面，从而提供一系列好处。\n控制平面 控制平面负责数据平面代理的运行时配置。它将控制平面的声明性配置转换为 Envoy 的具体运行时配置。控制平面协调多个 Envoy 代理，创建一个有凝聚力的网格。\n通过每个应用程序实例的边车代理和动态控制平面，服务网格提供了集中控制和分布式执行。这种级别的控制无法通过框架和库实现，但具有以下优点：\n集中可见性和控制 整个环境的一致性 通过基于代码的配置进行有效的策略更改 将功能生命周期与应用程序生命周期分开 Tetrate Service Bridge 利用 Istio 作为其控制平面在运行时配置 Envoy 代理。\n服务网格的起源 2010 年代初，服务网格架构在多家公司同时出现，以解决采用面向服务架构的挑战。谷歌的旅程导致了原型服务网格的创建，该网格解决了共同命运中断、成本归因和跨领域功能实现等问题。\n在内部体验到服务网格的好处后，Istio 诞生了，就是为了将这些功能带给世界。Tetrate 的成立是为了满足在现代化和云采用方面面临类似挑战的企业的需求。\nAPI 网关和服务网格 服务网格架构起源于分布式 API 网关，解决跨领域问题。随着微服务架构的盛行，内部流量大大超过外部流量。这种转变以及向零信任安全的转变，推动网格处理跨环境的流量。\n因此，API 网关的功能正在成为应用程序流量平台不可或缺的一部分，在平台中随处可用。传统上被视为“边缘”设备的其他功能也正在合并到应用程序流量平台中。\nIstio：领先的网格实现 服务网格充当基于微服务的应用程序的安全内核，因此网格实现的选择对于应用程序和信息安全至关重要。Istio 是使用最广泛、支持最广泛的服务网格，可作为微服务安全标准的参考实现。它符合 NIST 的指导方针，并拥有活跃的错误赏金、安全审计和 CVE 补丁。\nIstio 与 Kubernetes 生态系统一起发展，提供无缝集成和标准化。Tetrate 的团队由早期 Istio 贡献者组成，选择 Istio 作为为 Tetrate Service Bridge 提供支持的网格。\n继续阅读，了解 TSB 如何利用 Istio 将你的基础设施统一为一个有凝聚力的网格。\n","relpermalink":"/book/tsb/concepts/service-mesh/","summary":"服务网格架构和收益简介。","title":"服务网格简介"},{"content":"当你在 Kubernetes 上部署工作负载时，以下操作会在背后自动进行：\nIstio Sidecar 会部署在你的工作负载旁边。 该 Sidecar 会配置工作负载的位置和其他所需元数据。 然而，当你将工作负载部署在独立的虚拟机之外时， 你必须自己处理这些事情。\n工作负载载入功能为你解决了这个问题。 使用此功能，你只需执行以下步骤，即可将部署在虚拟机上的工作负载引入到网格中：\n在目标虚拟机上安装 Istio Sidecar（通过 DEB/RPM 软件包）。 在目标虚拟机上安装 Workload Onboarding Agent（同样通过 DEB/RPM 软件包）。 提供一个最小的、声明性的配置，描述在哪里引入工作负载，例如： apiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: # 连接至 host: onboarding-endpoint.your-company.corp workloadGroup: # 加入至 namespace: bookinfo name: ratings 组件和工作流程 工作负载载入包括以下组件：\n组件 描述 Workload Onboarding Operator 安装到你的 Kubernetes 集群中作为 TSB 控制平面的一部分 Workload Onboarding Agent 需要安装到你的虚拟机工作负载旁边的组件 Workload Onboarding Endpoint Workload Onboarding Agent 将连接注册工作负载并获取 Istio Sidecar 的引导配置的组件 下图概述了完整的载入流程：\nWorkload Onboarding Agent 根据用户提供的声明性配置执行载入流程。\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: # (1) host: onboarding-endpoint.your-company.corp workloadGroup: # (2) namespace: bookinfo name: ratings 根据上述配置，以下操作将发生：\nWorkload Onboarding Agent 将连接到 Workload Onboarding Endpoint 在 https://onboarding-endpoint.your-company.corp:15443 (1) Workload Onboarding Endpoint 将使用 VM 的云特定凭据对连接的 Agent 进行身份验证 Workload Onboarding Endpoint 将决定是否允许具有此标识（即 VM 的标识）的工作负载加入特定的 WorkloadGroup（2） Workload Onboarding Endpoint 将在 Istio 控制平面上注册一个新的 WorkloadEntry 以表示工作负载 Workload Onboarding Endpoint 将生成启动 Istio Proxy 所需的引导配置，根据相应的 WorkloadGroup 资源 (2) Workload Onboarding Agent 将保存返回的引导配置到磁盘，并启动 Istio Sidecar Istio Sidecar 将连接到 Istio 控制平面并接收其运行时配置 ","relpermalink":"/book/tsb/setup/workload-onboarding/guides/overview/","summary":"工作负载载入概述。","title":"概览"},{"content":"本文描述如何更改 TSB 管理员的密码。\nTSB 管理员在每个 TSB 实例中都是本地配置的，不属于企业身份提供者（IdP）。这允许超级用户在连接到身份提供者出现问题以进行故障排除和平台修复时能够登录 TSB。\n更新密钥 管理员凭据存储在管理平面命名空间中的 admin-credentials Kubernetes 密钥中（默认为 tsb）。它以 SHA-256 哈希的形式安全存储，因此无法被反向解析，可以通过直接更新带有所需密码的密钥来修改。\n以下示例显示了如何生成一个稍后可以应用的更新密钥：\nnew_password=\u0026#34;Tetrate1\u0026#34; new_password_shasum=$(echo -n $new_password | shasum -a 256 | awk \u0026#39;{print $1}\u0026#39;) kubectl -n tsb create secret generic admin-credentials --from-literal=admin=$new_password_shasum --dry-run=client -o yaml 这将输出包含更新密码的密钥的 YAML，并可以使用 kubectl 正常应用。\n一旦密钥已更新，需要重新启动 iam 部署的 pods 以加载更改：\nkubectl -n tsb rollout restart deployment/iam ","relpermalink":"/book/tsb/operations/users/admin-password/","summary":"更改 TSB 管理员的密码。","title":"更改管理员密码"},{"content":"工作负载载入是 TSB 的一个功能，它自动化了在服务网格中将在 Kubernetes 之外部署的工作负载载入的过程。\n例如，你可以使用它来将部署在虚拟机上（或者可能是自动缩放组中的虚拟机）的工作负载载入，这些工作负载并不属于你的 Kubernetes 集群的一部分。\n注意 工作负载载入功能目前是一个 alpha 版本。\n有可能它尚不支持所有可能的部署场景。尤其值得注意的是，它尚不支持使用 Iptables 进行流量重定向。你应该根据需要配置 Istio Sidecar 和你的应用程序。\n目前，此功能支持从以下环境载入工作负载：\n部署在 AWS EC2 实例上的工作负载 部署在 AWS Auto-Scaling Groups 上的工作负载 作为 AWS ECS 任务部署的工作负载 部署在本地环境的工作负载 概览\n设置工作负载载入\n载入虚拟机\n管理已载入的工作负载\n故障排除指南\n在本地载入工作负载\n载入 AWS ECS 工作负载\n","relpermalink":"/book/tsb/setup/workload-onboarding/guides/","summary":"自动化在 Kubernetes 之外部署的工作负载的载入。","title":"工作负载载入指南"},{"content":"本页深入介绍了 TSB Operator 如何配置管理平面组件，并概述了 TSB Operator 管理的各种组件。\nTSB Operator 配置为监督管理平面组件的生命周期，主动监视部署的同一命名空间内的 ManagementPlane 自定义资源 (CR)。默认情况下，管理平面驻留在 tsb 命名空间中。你可以参阅管理平面安装 API 参考文档，了解有关自定义资源 API 的全面详细信息。\n组件 以下是你可以使用管理平面 Operator 配置和管理的各种类型的自定义组件：\n组件 Service Deployment Cronjobs apiServer tsb tsb teamsync iamServer iam iam webUI web web frontEnvoy envoy envoy oap oap oap collector otel-collector otel-collector xcpOperator xcp-operator-central xcp-operator-central xcpCentral xcp-central central mpc mpc mpc Operator 配置并安装以下组件：\napiServer：TSB API 服务器，负责： 管理用户创建的服务网格配置 将服务网格配置推送到控制平面集群 管理从控制平面集群推送的集群信息 加强用户操作授权 存储操作审计日志 frontEnvoy：充当管理平面的入口网关。 iamServer：管理用户和 TSB 代理令牌身份验证。 webUI：TSB UI 服务器。 oap：为 TSB UI 提供 GraphQL 查询并聚合跨集群指标。 收集器：一个开放遥测收集器，从管理和控制平面组件收集指标并通过 Prometheus 指标端点公开它们。 xcpOperator：控制平面 Operator，管理管理平面所需的控制平面组件。 xcpCentral：控制平面的核心组件，管理平面使用它来向每个集群分发配置并接收有关每个集群状态的信息。 mpc：apiServer 和 xcpCentral 之间的配置转换组件。 演示安装 在演示安装过程中，TSB Operator 还设置 PostgreSQL 和 Elasticsearch 组件。但是，这些仅用于演示目的，Tetrate 不支持用于生产环境或深入的系统评估。 ","relpermalink":"/book/tsb/concepts/operators/management-plane/","summary":"TSB Operator 和管理平面生命周期。","title":"管理平面"},{"content":"如果 Tetrate 管理平面失败，您需要恢复管理平面以恢复正常操作状态。本指南提供了一个流程概述，您应该在进行此过程时与 Tetrate 技术支持 协商。\n为了应对管理组件的意外故障，我们建议考虑以下建议：\n要么在可靠的冗余集群中维护 Postgres 数据库，要么（在 TSE 的情况下）利用定期的 Postgres 备份 。 保留 iam-signing-key 的备份。 如果保留指标很重要，请在可靠的冗余集群中维护 ElasticSearch 数据库，或定期备份，以便在必要时进行恢复。 概述 如果管理平面失败 或托管管理平面的集群停止运行 ，您需要恢复管理平面以恢复正常运行状态。恢复是使用 helm 基础安装完成的。 本方案将演示如何在新的管理集群上从失败的管理集群中恢复配置的任务。\n先决条件 本指南做出以下假设：\nPostgreSQL 数据库（配置）可用。要么数据库位于失败的集群之外，要么可以从备份中恢复（仅适用于 TSE） 。 ElasticSearch 数据库（指标）可用。要么数据库位于失败的集群之外，要么可以从备份中恢复，或者可以使用全新的（空的）ElasticSearch 数据库，并容忍指标丢失。 新管理平面集群的所有证书都使用与之前失败的集群相同的根证书颁发机构。 您可以更新用于发现管理平面的任何 DNS 记录。 您有 iam-signing-key 的备份。 流程 请与Tetrate 技术支持 合作，按照以下步骤操作：\n部署新集群 部署新集群，将管理平面恢复到其中。\n安装依赖项 在集群中安装所需的依赖项。这些依赖项可能包括：\nCert-Manager（如果您没有使用捆绑的 cert-manager 实例）及相关发行人/证书。确保使用相同的根 CA。 保存凭据/证书的任何密钥。 来自失败管理平面集群的 iam-signing-key - 可选 使用 kubectl apply 安装 iam-signing-key 密钥。如果无法执行此操作，您需要稍后在此过程中重新配置每个控制平面以使用全新的密钥。\n准备配置 使用与失败集群相同的 mp-values.yaml，更新任何必要的字段，如 hub 或 registry，或者如果需要的话，更新任何其他环境相关字段。\n如果使用外部 IP 端点，则无需更新 Elastic/Postgres 配置，但可能需要调整防火墙规则。\n安装管理平面 使用 mp-values.yaml 执行管理平面的 helm 安装，并使用以下命令监视进度：\nkubectl get pod -n tsb kubectl logs -f -n tse -l name=tsb-operator 对于 Tetrate Service Express（TSE），组件安装在 tse 命名空间中（而不是 tsb）。\n获取管理平面地址 安装完成后，请获取 front envoy 的公共 IP 地址，例如：\nkubectl get svc -n tsb envoy 使用 Envoy IP 地址登录 UI：\n验证您的 Tetrate 配置是否在 Postgres 数据库中得以保留。 如果可用，检查 Elastic 历史数据。 更新 DNS 使用在步骤 5 中获取的新 IP 地址更新用于定位管理平面的 DNS A 记录。远程控制平面集群将使用此 DNS 记录与管理平面进行通信。\n传播可能需要一些时间。一旦更改传播完成，请验证您是否可以使用 FQDN 访问管理平面 UI。\n验证控制平面操作 在管理平面 UI 中，验证工作负载集群控制平面是否连接并与新的管理平面同步。\n刷新控制平面令牌 iam-signing-key 用于生成、验证和旋转令牌，这些令牌提供给控制平面集群，以与管理平面进行通信。\n如果无法恢复和恢复原始的 iam-signing-key，则需要在每个控制平面上手动刷新令牌：\n登录每个控制平面集群。\n删除旧令牌以旋转令牌：\nkubectl delete secret otel-token oap-token ngac-token xcp-edge-central-auth-token -n istio-system 验证控制平面现在是否连接到并与新的管理平面同步。\n成功恢复新管理平面后，您将完全恢复故障，您的工作负载集群将由新的管理平面实例控制。\n故障排除 管理平面和控制平面安装由 Operator 管理。如果进行配置更改，可以监视 Operator 日志以查看进度并识别任何错误。\n控制平面无法同步 检查 ControlPlane Envoy 的日志，\n查找与连接到管理平面或与令牌验证相关的错误：\nkubectl logs deploy/edge -n istio-system -f 按照上述描述的方法删除控制平面上的现有令牌，并验证这些令牌是否在控制平面上重新生成。\nkubectl get secrets otel-token oap-token ngac-token xcp-edge-central-auth-token -n istio-system 如果令牌未重新生成：\n检查控制平面实例与新的管理平面实例之间的防火墙规则，并确保允许连接。 确保管理平面使用相同的根 CA。 无法访问外部组件，如 postgres 验证到 postgres 或任何其他外部组件的防火墙规则。 验证通过 helm 或在 mp-values.yaml 中传递的凭据。 ","relpermalink":"/book/tsb/design-guides/ha-dr-mp/dr-managementplane/","summary":"如果 Tetrate 管理平面失败，您需要恢复管理平面以恢复正常操作状态。本指南提供了一个流程概述，您应该在进行此过程时与 Tetrate 技术支持 协商。 为了应对管理组件的意外故障，我们建议考虑以下建议： 要么在可靠的冗余集群中维护 Postgres","title":"恢复失败的管理平面组件"},{"content":"本文档介绍了在 TSB 中进行基本故障排除的一些可能方法，以便查找特定路由的错误配置问题或 50x 错误的常见原因。\n系统架构 在本文档中，采用了以下具有 Tier1-Tier2 设置的系统架构：\n有两个不同的集群，training-mp 包含管理平面和配置为 tier1 的控制平面，training-cp 配置为 tier2，包含 bookinfo 和 httpbin 应用程序。\nTier1 网关故障排除 当检测到 50x 错误时，重要的是要理解错误消息，因为它会指向不同的信息源。\n例如，假设你使用 curl 发出了一个 HTTP 请求到由 TSB 控制的服务之一，并且观察到类似以下的错误：\nFailed to connect to \u0026lt;hostname\u0026gt; port \u0026lt;port\u0026gt;: Connection refused 这通常意味着没有配置监听器。这又意味着我们要么：\n缺少网关对象 访问了错误的端口 网关没有正确配置，或者 Tier1 网关的 Pod 没有运行。 要检查监听器是否存在，你可以使用 istioctl：\n$ istioctl pc listener \u0026lt;ingressgateway\u0026gt;.\u0026lt;namespace\u0026gt; 如果没有监听器，或者你想检查当前配置，你需要审查你的网关配置。要获取网关对象，使用 kubectl：\nkubectl get gateway 如果网关不存在，你需要排查为什么 XCP 没有创建配置。在这种情况下，请定位管理平面命名空间中的 mpc Pod，并查找可能指向错误配置的 Webhook 错误。\n如果网关和虚拟服务已创建，但仍然在 HTTP 请求中获得 50x 错误，例如以下错误：\nHTTP/2 503 在这种情况下，请查看 ingressgateway 的日志。由于在这种情况下系统配置为 tier1-tier2 设置，因此首先应该检查 tier1gateway。\n查找相应 Pod 的日志。根据问题的性质，你可能需要启用跟踪日志以进行进一步的调查。\n如果你找到以下类似的条目，这意味着无法找到到达 tier2 网关的路由。\nHTTP/2\u0026#34; 503 NR 如果是这种情况，请尝试检查以下内容：\n确保已应用 nodeSelector 注释 如果在 XCP-edge 服务中使用 NodePort，请记住你必须在 tier1 和 tier2 中都添加以下注释：\ntraffic.istio.io/nodeSelector: {\u0026#34;value\u0026#34;:\u0026#34;value\u0026#34;}\u0026#39; 检查 tier1gateway 配置 可以通过将流量路由到特定的集群名称或使用标签来配置 tier1gateway。确保集群或标签名称在 tier1gateway 配置的 spec.externalServers.name[x].clusters 字段中是正确的。\n你可以使用以下命令获取 tier1gateway 对象：\n$ tctl get t1 -w \u0026lt;workspace\u0026gt; -l \u0026lt;gatewaygroup\u0026gt; \u0026lt;name\u0026gt; -o yaml … externalServers: - clusters: - name: training-cp hostname: bookinfo … - clusters: - labels: tier: tier2 hostname: httpbin … 并将其与 cluster 对象进行比较：\n$ tctl get cluster \u0026lt;name\u0026gt; -o yaml … metadata: labels: tier: tier2 name: training-cp … 检查网络之间的通信权限 如果在集群对象中定义了一个 network，并且参与的集群并不都共享相同的 network，请检查是否存在一个允许在不同网络之间进行通信的 组织设置 。\n$ tctl get os 修复此问题后，你应该会在命名空间 xcp-multicluster 中看到创建的服务。该服务条目是为多集群目的而创建的，还会在应用程序命名空间中创建目标规则以设置 mTLS。\n如果此时你仍然注意到从 tier1gateway 获取到 503 错误，请检查 错误代码 以更好地了解可能导致错误的原因。\n在此时使用 istioctl 命令也非常有用，因为很可能在 tier1 - tier2 情况下，你会遇到下游的某些问题。\n首先，请检查你的 tier1gateway 的配置是否已同步，检查状态中是否存在 SYNC：\n$ istioctl ps 验证你要访问的路由是否存在：\n$ istioctl pc route \u0026lt;ingressgateway\u0026gt;.\u0026lt;namespace\u0026gt; 如果路由不存在，那么 tier1gateway 对象中可能存在配置错误。如果存在，请检查服务的 cluster：\n$ istioctl pc cluster \u0026lt;ingressgateway\u0026gt;.\u0026lt;namespace\u0026gt; 你应该能够在上述命令的输出中看到子集和目标规则。检查目标规则的配置是否正确。\n最后，请检查 endpoints。检查配置以查看下游是否正常：\n$ istioctl pc endpoint \u0026lt;ingressgateway\u0026gt;.\u0026lt;namespace\u0026gt; 如果所有上述都正确，那么很可能你需要查看 tier2gateway。\n在 tier1gateway 的日志中检查是否存在类似以下的错误：\nHTTP/2\u0026#34; 503 LR,URX 这很可能意味着从 tier1gateway 到 tier2gateway 的连接超时。尝试使用 netcat 查看是否可以访问 tier2gateway。如果无法成功连接到 tier2gateway，可能存在配置错误，或者中间可能有阻止通信的防火墙。\n你可能还可以在 ingressgateway 的日志中找到一些有用的信息。如果你在日志中找到类似以下的错误消息，这意味着 istio-system 命名空间中的 cacert 密钥并未由两个集群中的相同根（或中间）CA 签名。\n$ HTTP/2\u0026#34; 503 UF,URX \u0026#34;-\u0026#34; \u0026#34;TLS error: 268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED\u0026#34; 如果你对证书进行了更改，你将需要重新启动所有 sidecar 和网关，或者等待 30 分钟，直到组件从 istiod 获取新证书。这些更新之间的间隔可以配置，但默认值为 30 分钟。\nTier2Gateway 故障排除 如果调试 tier1gateway 不足以解决问题，你将不得不执行与你在 tier2gateway 上执行的大部分类似的操作，并了解你的问题是否源自配置错误或配置传播问题（即 XCP）。\n检查是否已在 tier2 命名空间中创建了网关，可以使用 kubectl get gateway 进行检查。如果网关不存在，请在 XCP 方面检查。你可以从管理平面命名空间中的 mpc Pod 中查看是否存在任何 Webhook 问题。\n如果网关已创建，请验证监听器是否正确创建。\n$ istioctl pc listener \u0026lt;ingressgateway\u0026gt;.\u0026lt;namespace\u0026gt; 在 ingressgateway 资源中还必须包含端口 15443 的监听器，因为从 tier1 到 tier2 的流量将需要使用此端口。还重要的是检查端口 15443 是否在监听器列表的第一个条目中指定，因为一些云供应商会将第一个端口用于负载均衡器的健康检查。\n如果在检查了监听器是否正确创建后，问题仍然存在，你需要检查 tier2gateway 的日志。如果在这些日志中看到了 50x 错误，则很可能是应用程序本身存在问题，或者从 istiod 到 tier2gateway 的配置传播存在问题。\n如果需要进一步的故障排除，那么你将需要启用跟踪日志以找出根本原因：\nkubectl exec \u0026lt;pod\u0026gt; -c istio-proxy -- pilot-agent request POST ‘logging?level=trace\u0026#39; 你还可以检查是否从 istiod 接收到配置：\n$ istioctl ps 如果配置未正确同步，请检查 istiod 与 `tier2gateway\n` 之间是否有可能阻止通信的任何网络条件。\n还要验证 istiod 命名空间中的 istiod Pod 是否正常运行。你可能存在资源问题，可能会阻止配置的发送。\n如果要验证特定主机名的 tier2gateway 中的所有配置，可以获取配置转储：\nkubectl exec \u0026lt;pod\u0026gt; -c istio-proxy -- pilot-agent request GET config_dump \u0026gt; config_dump.json XCP 故障排除 如果注意到 XCP 没有创建你期望的配置，请检查管理平面命名空间中 mpc Pod 的日志。\n在这些日志中，你可能会发现验证错误，指示了从 TSB 转换到 XCP API 的配置存在问题。例如，你可能会看到类似以下的条目：\nkubectl logs -n tsb \u0026lt;mpc\u0026gt; 2022-03-02T13:58:26.153872Z error mpc/config failed to convert TSB config into its XCP equivalent: no gateway object found for reference \u0026#34;httpbin/httpbin-gw\u0026#34; in \u0026#34;organizations/\u0026lt;org\u0026gt;/tenants/\u0026lt;tenant\u0026gt;/workspaces/\u0026lt;ws\u0026gt;/gatewaygroups/\u0026lt;gg\u0026gt;/virtualservices/\u0026lt;vs\u0026gt;\u0026#34; 如果在 mpc 中没有 Webhook 错误，然后检查集群应用程序命名空间中 edge Pod 的日志。\n如果一切正常，你应该能够在 istio-system 命名空间中看到应用于所有配置的日志：\nkubectl logs -n istio-system \u0026lt;edge\u0026gt; 2022-03-09T11:17:25.492365Z debug configapply ===BEGIN: Apply request for \u0026lt;n\u0026gt; objects in istio-system namespace 如果你要查找的对象在此列表中不存在，那么可能是 XCP edge 或 XCP central 中的问题。\n要启用 XCP edge 的调试日志，你可以对部署进行如下修改（这将重新启动 Pod）：\nkubectl edit deployment edge -n istio-system 具体取决于你要排查的问题，你可能必须更详细地配置记录器。例如，如果你想为每个记录器配置不同的记录级别，你可以使用以下命令：\n- --log_output_level default:info,transform:info,discovery-server:info,configapply:debug,translator:debug,model:debug,istiod-discovery:error,cluster-gen:error,stream:debug 或者，你可以一次性为所有记录器设置日志级别：\n- --log_output_level - default:debug 如果要永久更改所有未来 XCP edge 组件的日志记录配置，你可以为控制平面运算符创建一个覆盖：\noverlays: - apiVersion: install.xcp.tetrate.io/v1alpha1 kind: EdgeXcp name: edge-xcp patches: - path: spec.logLevels value: …","relpermalink":"/book/tsb/troubleshooting/troubleshooting/","summary":"本文档介绍了在 TSB 中进行基本故障排除的一些可能方法，以便查找特定路由的错误配置问题或 50x 错误的常见原因。 系统架构 在本文档中，采用了以下具有 Tier1-Tier2 设置的系统架构： 有两个不同的集群，training-mp 包含管理","title":"基本故障排除"},{"content":"平台所有者（“平台”）将通过以下步骤准备一个集群：\n部署 TSE/TSB\n首先部署 TSE 或 TSB，并启动预期的工作负载集群。\n启用严格（零信任）安全\n配置平台以遵循 ‘require-mTLS’ 和 ‘deny-all’ 的零信任安全策略。\n创建 Kubernetes 命名空间\n在每个将由应用所有者用于托管服务和应用程序的集群中创建并标记命名空间。\n创建 Tetrate 工作区\n创建将用于管理命名空间内服务行为的 Tetrate 工作区和相关配置。\n部署入口网关\n如有需要，在将托管应该提供给外部访问的服务的工作区中部署入口网关。\n启用 GitOps 集成\n启用 GitOps 集成，以便应用所有者用户可以在不需要 Tetrate 特权访问的情况下与平台交互。\n启用其他集成\n启用其他集成，以便应用所有者用户可以在不需要 Tetrate 特权访问的情况下与平台交互。\n平台：部署 TSE/TSB 按照产品说明部署 TSE 或 TSB 管理平面，然后启动预期的工作负载集群。\n请确保安装所需的附加组件并满足必要的先决条件。\n平台：启用严格安全 你应该使用 TSE/TSB 配置平台以零信任方式运行。具体来说：\n组件与应用所有者服务之间的所有通信都使用 mTLS 进行安全保护。这意味着外部的第三方，例如集群中的其他服务或具有数据路径访问权限的服务，不能读取事务、修改事务或冒充客户或服务。 默认情况下拒绝所有通信。平台所有者必须明确打开所需的通信路径。这意味着只允许明确允许的通信。 严格安全 TSE\n导航到 设置 \u0026gt; 基本设置。确保 Enforce mTLS 和 Deny-All 都已启用：\n你也可以使用 Tetrate API 配置严格安全，方法是遵循 Tetrate Service Bridge 的说明。\n在 Tetrate 产品中，默认设置与顶级组织关联，顶级组织在 TSB 中是可定义的，而在 TSE 中设置为值 tse。\n你将在名为 default 的 OrganizationSetting 中的 OrganizationSetting.spec.defaultSecuritySetting 部分中找到安全设置：\ntctl get os -o yaml 可以在租户或工作区的基础上进一步覆盖这些设置（请注意，TSE 有一个名为 tse 的单一租户，而 TSB 支持多个用户定义的租户）。\n要默认要求使用 mTLS，请将 authenticationSettings.trafficMode 设置为 REQUIRED 要默认声明拒绝所有通信，请将 authorization.rules.denyAll 设置为 true 要防止子资源覆盖这些设置，请将 propagationStrategy 设置为 STRICTER（此步骤是非必需的） 可以在 TSB API 参考 中找到这些设置的描述。\n稍后，你将有选择地覆盖这些设置以允许允许的流量。\n平台：创建 Kubernetes 命名空间 Kubernetes 的核心隔离单元是命名空间 。许多部署使用非常细粒度的命名空间以强制执行高级别的控制并为每个服务提供重复配置的自由。\n一旦将工作负载集群接入到 TSE/TSB 中，然后可以创建每个应用所有者团队将需要的命名空间，并为 Istio 注入进行标记。这就是命名空间中的资源将由 TSE/TSB 管理所需的全部内容：\nkubectl create namespace bookinfo kubectl label namespace bookinfo istio-injection=enabled 平台：创建 Tetrate 工作区 在实践中，细粒度的命名空间并不准确地模拟许多企业遵循的应用程序和团队结构。应用程序由多个命名空间组成，通常跨越多个不同的集群、区域、地区甚至云。\n出于这个原因，Tetrate 引入了一个称为 工作区 的更高级别的结构。工作区是 Tetrate 产品中的主要隔离单元，它只是一个或多个集群中的一组命名空间。\n工作区提供了一个便捷的更高级别抽象，与组织的应用程序保持一致，这些应用程序通常跨越多个命名空间和/或集群。\nTSB 租户 Tetrate Service Express（TSE）提供一个单一的组织（用于全局设置）和多个工作区（用于个别设置）。TSE 旨在由单个团队使用。\nTetrate Service Bridge 添加了一个中间层的 租户 概念，允许在顶级组织内拥有多个独立的团队。租户 可以在团队层面上应用额外的隔离，并可以覆盖全局设置。\n在本文档中，我们假设组织内只有一个团队，因此所有设置将应用于工作区级别。示例将使用名为 tse 的组织和名为 tse 的租户；当使用 TSB 时，你应该将这些更改为反映你选择的层次结构。\n为每个应用程序创建 Tetrate 工作区，覆盖分配给该应用程序的命名空间：\n通过工作区的 namespaceSelector 定义命名空间列表。条目可以限制在单个集群中 cluster-1/bookinfo，也可以跨足所有集群 */bookinfo 注意我们如何使用 WorkspaceSetting 覆盖了每个工作区的 defaultSecuritySetting。 cat \u0026lt;\u0026lt;EOF \u0026gt; bookinfo-ws.yaml apiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tse tenant: tse name: bookinfo-ws spec: namespaceSelector: names: - \u0026#34;*/bookinfo\u0026#34; --- apiVersion: api.tsb.tetrate.io/v2 kind: WorkspaceSetting metadata: organization: tse tenant: tse workspace: bookinfo-ws name: bookinfo-ws-settings spec: defaultSecuritySetting: authenticationSettings: trafficMode: REQUIRED authorization: mode: WORKSPACE EOF tctl apply -f bookinfo-ws.yaml 在打开一个工作区（authorization.mode: WORKSPACE）时，你在零信任环境中创建了一个“泡泡”。该工作区内的所有服务可以相互通信，但必须使用 mTLS。\n平台：部署入口网关 通常，你会希望安排外部流量到达工作区内的特定服务。为此，你首先应在每个集群中的每个工作区部署一个入口网关。应用程序所有者随后可以定义通过此入口网关公开其服务的网关规则。\n创建 Tetrate 网关组 首先，创建一个 Tetrate 网关组，其范围限定在将托管入口网关的每个工作区和集群内。例如，如果 Bookinfo 工作区跨足了 cluster-1 和 cluster-2，你可以为此工作区创建两个网关组，每个集群一个：\ncat \u0026lt;\u0026lt;EOF \u0026gt; bookinfo-gwgroup-cluster-1.yaml apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: name: bookinfo-gwgroup-cluster-1 organization: tse tenant: tse workspace: bookinfo-ws spec: namespaceSelector: names: - \u0026#34;cluster-1/bookinfo\u0026#34; EOF tctl apply -f bookinfo-gw-group-1.yaml 部署入口网关 接下来，在要接收外部流量的每个工作区和集群中部署一个入口网关：\ncat \u0026lt;\u0026lt;EOF \u0026gt; bookinfo-ingress-gw.yaml apiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: bookinfo-ingress-gw namespace: bookinfo spec: kubeSpec: service: type: LoadBalancer EOF kubectl apply -f bookinfo-ingress-gw.yaml 这一步将在相应的命名空间中创建一个 envoy 代理 pod，它将作为入口网关运行（kubectl get pod -n bookinfo -l app=bookinfo-ingress-gw）。请注意，你使用 IngressGateway 在特定集群中创建资源，因此使用 kubectl 部署资源。\n稍后，应用程序所有者将想要创建 Gateway 资源以公开其选择的服务。他们需要知道：\nTetrate 工作区的名称，例如 bookinfo-ws 每个集群上 Tetrate 网关组的名称，例如 bookinfo-gwgroup-cluster-1 每个集群上入口网关的名称，例如 bookinfo-ingress-gw。可以在所有集群上使用相同的名称 入口网关非常轻量级，并且为了安全和容错目的，为每个工作区运行一个单独的入口网关提供了隔离。对于非常大型的部署，你可能希望在多个工作区之间共享入口网关 。\n平台：启用 GitOps 集成 提供 Tetrate 管理的平台配置有两种方式：\n使用 tctl 提供平台范围的配置，调用用户需要对 Tetrate API 服务器进行身份验证 使用 kubectl 提供每个集群的配置，调用用户需要对 Kubernetes API 服务器进行身份验证 对于某些用例，用户（平台所有者或应用程序所有者）需要提供平台范围和每个集群的配置。\nTetrate 的 GitOps 集成允许用户使用 Kubernetes API 提供平台范围的配置。GitOps 应该在一个或多个集群上启用；该过程会安装 Tetrate 平台范围配置的 CRD，任何资源都会自动从集群推送到 Tetrate API 服务器：\nTetrate Service Express： 在 Tetrate Service Express 上默认启用 GitOps 集成。有关集成的概述，请参阅 TSE 中的 GitOps 指南。 Tetrate Service Bridge： 你需要在 Tetrate Service Bridge 上明确启用 GitOps。有关详细信息，请参阅 TSB 文档中的 配置 GitOps 。 总的来说，GitOps 不仅适用于 GitOps 的用例。即使在组织采用 GitOps 姿态来管理配置之前，它也是有用的；GitOps 也可以用于允许选定的 K8s 用户管理 Tetrate 配置。这意味着用户不必拥有 Tetrate 用户/角色，他们可以使用他们已经习惯的 K8s 工具。\n平台：启用额外的集成 你可能希望为你的平台启用其他集成。例如，在使用 AWS 时：\n安装 AWS 负载均衡控制器 以实现更好的负载均衡器集成 启用 AWS Route 53 控制器 以管理由应用程序所有者公开的服务的 DNS 记录条目 ","relpermalink":"/book/tsb/design-guides/app-onboarding/prepare/","summary":"平台所有者（“平台”）将通过以下步骤准备一个集群： 部署 TSE/TSB 首先部署 TSE 或 TSB，并启动预期的工作负载集群。 启用严格（零信任）安全 配置平台以遵循 ‘require-mTLS’ 和 ‘deny-all’ 的零信任安全策略。 创建 Kubernetes 命名空间","title":"集群准备"},{"content":"欢迎使用 TSB 快速入门指南！本指南旨在引导你完成在 TSB 上加入和配置应用程序的过程。通过遵循本快速入门，你将了解如何针对各种基本场景部署应用程序并配置 TSB 及其组件。\n在本快速入门指南中，你将探索以下场景：\n部署 Istio bookinfo 示例应用程序 创建租户并连接集群 创建工作区 建立对工作区的 tctl 访问权限 创建配置组 配置权限 设置入口网关 检查服务拓扑和指标 使用 TSB 进行流量转移 在 TSB 内启用安全设置 创建应用程序并使用 OpenAPI 规范配置 API 在开始使用快速入门指南之前，请确保你：\n熟悉 TSB 概念 安装 TSB 演示环境 本指南中的每个示例将演示如何使用 tctl 命令行工具和 TSB UI 进行更改。\n在这些示例中，你将使用超级管理员权限，授予你访问所有 TSB 功能的权限。但是，请记住，对于生产用途，并非每个人都可以被授予管理员权限。出于安全考虑，不建议为每个人提供管理员访问权限。\n","relpermalink":"/book/tsb/quickstart/introduction/","summary":"快速开始简介。","title":"快速开始简介"},{"content":"本指南描述了 Tetrate 管理平面的灾难恢复方案及其影响。适用于 Tetrate Service Bridge（TSB）和 Tetrate Service Express（TSE）。\nTetrate 管理平面的设计（以及分布式控制平面架构）具有以下特点：\n架构松散耦合：Tetrate 的架构设计松散耦合且具有自我修复能力，这意味着故障的“影响范围”有限，当组件恢复时，平台很快会达到良好的配置状态。 所有 Tetrate 组件都是无状态的，可以从故障中恢复：唯一的例外是 Postgres 数据库（配置和审计日志）和 ElasticSearch 数据库（指标），以及 K8s 集群中的秘密。 应用和服务不受影响：管理或控制平面组件的任何故障都不会影响在工作负载集群中运行的应用程序和服务的正确操作或安全性。 高可用性：我们建议以冗余的高可用方式运行工作负载。冗余的高可用管理平面是可能的，但在 Tetrate 的松散耦合架构中，带来的好处有限，代价是资源使用和额外复杂性。 使用 Tetrate 实现高可用性和灾难恢复 工作负载和数据平面的高可用性 Tetrate 可以帮助你管理和运营跨区域和云的多个生产集群，从而创建一个冗余的数据平面。Tetrate 的 Edge 和东西向网关等功能，以及与 Amazon Route 53 等 GSLB 解决方案的集成，可以在任何原因导致工作负载集群发生故障时，为生产工作负载提供高可用性。\nIstio 多区域灾难恢复配置 Tetrate 的解决方案不依赖于 Istio 数据平面的任何 多区域灾难恢复配置 。当没有更高级别的控制平面时，才需要这些配置，它们会增加显著的复杂性和额外的故障情景。Tetrate 的控制平面架构意味着单个 istio-per-cluster 部署完全足够，并在故障发生时提供更好的隔离。此外，较小的故障域使得渐进式升级更加容易且风险更低。 管理和控制平面的高可用性 Tetrate 管理和控制平面中的大多数组件都可以从故障中恢复，重新同步配置并在没有任何用户干预的情况下恢复正确的操作。\n中央控制平面不能以冗余方式运行，但通过将所有配置缓存到本地 Kubernetes API 服务器（tsb 命名空间）并在需要时从管理平面和远程 Edge 控制平面实例重新同步来实现高可用性。中央控制平面中的任何故障都会迅速恢复，没有任何持久的影响。\n管理平面在一个 PostgreSQL 数据库中维护配置并存储审计日志：\n在部署 TSB 时，客户提供并维护一个合适的 PostgreSQL 实例 在部署 TSE 时，会包含一个简单的 PostgreSQL 实例，并由 TSE 进行管理。可以用客户提供的实例替换它 Tetrate 分享了以下建议：\n为管理平面配置维护一个高可用的 PostgreSQL 数据库 如有必要，定期备份数据库 有关各种故障和恢复方案的详细描述，请阅读 故障场景 解释。\n","relpermalink":"/book/tsb/design-guides/ha-dr-mp/","summary":"本指南描述了 Tetrate 管理平面的灾难恢复方案及其影响。适用于 Tetrate Service Bridge（TSB）和 Tetrate Service Express（TSE）。 Tetrate 管理平面的设计（以及分布式控制平面架构）具有以下特点： 架构松散耦合：Tetrate 的架构","title":"理解高可用性"},{"content":" 在 Kubernetes 和虚拟机之间切分流量\n将虚拟机上的单体应用迁移到你的集群\n客户端负载均衡\n将流量发送到使用 HTTPS 的外部主机\n配置（多端口、多协议）服务的 ServiceRoute\n金丝雀发布\n","relpermalink":"/book/tsb/howto/traffic/","summary":"在 Kubernetes 和虚拟机之间切分流量 将虚拟机上的单体应用迁移到你的集群 客户端负载均衡 将流量发送到使用 HTTPS 的外部主机 配置（多端口、多协议）服务的 ServiceRoute 金丝雀发布","title":"流量管理和迁移"},{"content":"你将在本地虚拟机上部署 ratings 应用程序并将其加入服务网格。\n创建工作负载组 执行以下命令以创建一个 WorkloadGroup：\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: labels: app: ratings class: vm serviceAccount: bookinfo-ratings EOF 字段 spec.template.network 被省略，以指示 Istio 控制平面虚拟机在本地具有直接连接到 Kubernetes Pod 的能力。\n字段 spec.template.serviceAccount 声明工作负载具有 Kubernetes 集群内服务账号 bookinfo-ratings 的身份。此服务账号是在之前的 Istio bookinfo 示例部署期间创建的（../../aws-ec2/bookinfo）。\n创建 Sidecar 配置 执行以下命令以创建新的 Sidecar 配置：\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1beta1 kind: Sidecar metadata: name: bookinfo-ratings-no-iptables namespace: bookinfo spec: workloadSelector: # (1) labels: app: ratings class: vm ingress: - defaultEndpoint: 127.0.0.1:9080 # (2) port: name: http number: 9080 # (3) protocol: HTTP egress: - bind: 127.0.0.2 # (4) port: number: 9080 # (5) hosts: - ./* # (6) EOF 以上 Sidecar 配置仅适用于具有标签 app=ratings 和 class=vm（1）的工作负载。你创建的 WorkloadGroup 具有这些标签。\nIstio 代理将配置为侦听 \u0026lt;host IP\u0026gt;:9080（3），并将 传入 请求转发到侦听 127.0.0.1:9080（2）的应用程序。\n最后，代理将配置为侦听 127.0.0.2:9080（4）（5），以将 传出 请求代理到其他服务的应用程序（6），这些服务使用端口 9080（5）。\n允许工作负载通过 JWT 令牌进行身份验证 在本指南中，你将使用 Sample JWT Credential Plugin 来为你的本地工作负载提供 [JWT 令牌] 凭据。\n在此部分中，你将配置 Workload Onboarding Plane 来信任由 Sample JWT Credential Plugin 颁发的 JWT 令牌。\n执行以下命令将 Sample JWT Credential Plugin 下载到本地：\ncurl -fL \u0026#34;https://dl.cloudsmith.io/public/tetrate/onboarding-examples/raw/files/onboarding-agent-sample-jwt-credential-plugin_0.0.1_$(uname -s)_$(uname -m).tar.gz\u0026#34; \\ | tar -xz onboarding-agent-sample-jwt-credential-plugin 执行以下命令生成供 Sample JWT Credential Plugin 使用的唯一签名密钥：\n./onboarding-agent-sample-jwt-credential-plugin generate key \\ -o ./sample-jwt-issuer 上述命令将生成两个文件：\n./sample-jwt-issuer.jwk - 签名密钥（秘密部分） - 用于配置本地虚拟机上的 Sample JWT Credential Plugin ./sample-jwt-issuer.jwks - JWKS 文档（公共部分） - 用于配置 Workload Onboarding Plane 执行以下命令将配置 Workload Onboarding Plane 以信任由上述生成的密钥签名的 [JWT 令牌]：\ncat \u0026lt;\u0026lt; EOF \u0026gt; controlplane.patch.yaml spec: meshExpansion: onboarding: workloads: authentication: jwt: issuers: - issuer: https://sample-jwt-issuer.example jwks: | $(cat sample-jwt-issuer.jwks | awk \u0026#39;{print \u0026#34; \u0026#34;$0}\u0026#39;) shortName: my-corp tokenFields: attributes: jsonPath: .custom_attributes EOF kubectl patch controlplane controlplane -n istio-system --type merge --patch-file controlplane.patch.yaml 注意：为了使上述命令正常工作，你需要使用 kubectl 的版本 v1.20+。\n允许工作负载加入工作负载组 你需要创建一个 OnboardingPolicy 资源来明确授权在 Kubernetes 之外部署的工作负载加入网格。\n执行以下命令：\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: authorization.onboarding.tetrate.io/v1alpha1 kind: OnboardingPolicy metadata: name: allow-onpremise-vms namespace: bookinfo # (1) spec: allow: - workloads: - jwt: issuer: \u0026#34;https://sample-jwt-issuer.example\u0026#34; # (2) onboardTo: - workloadGroupSelector: {} # (3) EOF 以上策略适用于通过由 ID 为 https://sample-jwt-issuer.example 的发行者颁发的 [JWT 令牌]（2）进行身份验证的任何 本地 工作负载，并允许它们加入 bookinfo 命名空间（1）中的任何 WorkloadGroup（3）。\n","relpermalink":"/book/tsb/setup/workload-onboarding/quickstart/on-premise/configure-workload-onboarding/","summary":"你将在本地虚拟机上部署 ratings 应用程序并将其加入服务网格。 创建工作负载组 执行以下命令以创建一个 WorkloadGroup： cat \u003c\u003cEOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: labels: app: ratings class: vm serviceAccount: bookinfo-ratings EOF 字段 spec.template.network 被省略，以指示 Istio 控制","title":"配置本地 WorkloadGroup 和 Sidecar"},{"content":"你将部署 ratings 应用程序作为 AWS ECS 任务，并将其加入服务网格。\n创建 WorkloadGroup 执行以下命令创建一个 WorkloadGroup：\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: labels: app: ratings class: ecs cloud: aws serviceAccount: bookinfo-ratings EOF 字段 spec.template.serviceAccount 声明了工作负载将具有 Kubernetes 集群内的服务账号 bookinfo-ratings 的身份。服务账号 bookinfo-ratings 是在之前部署 Istio bookinfo 示例 时创建的。\n创建 Sidecar 配置 执行以下命令创建一个新的 Sidecar 配置：\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1beta1 kind: Sidecar metadata: name: bookinfo-ratings-no-iptables namespace: bookinfo spec: workloadSelector: # (1) labels: app: ratings class: ecs ingress: - defaultEndpoint: 127.0.0.1:9080 # (2) port: name: http number: 9080 # (3) protocol: HTTP egress: - bind: 127.0.0.2 # (4) port: number: 9080 # (5) hosts: - ./* # (6) EOF 上述 Sidecar 配置仅适用于具有标签 app=ratings 和 class=ecs（1）的工作负载。你已经创建的 WorkloadGroup 具有这些标签。\nIstio 代理将配置为侦听 \u0026lt;主机 IP\u0026gt;:9080（3），并将 传入 请求转发到侦听 127.0.0.1:9080（2）的应用程序。\n最后，代理将配置为侦听 127.0.0.2:9080（4）（5），以将应用程序的 传出 请求代理到其他服务（6），这些服务使用端口 9080（5）。\n允许工作负载加入 WorkloadGroup 你需要创建一个 OnboardingPolicy 资源，以明确授权在 Kubernetes 外部部署的工作负载加入网格。\n首先，获取你的 AWS 帐户 ID 。如果不知道你的 AWS 帐户 ID，请参阅 AWS 帐户文档 以获取有关如何查找你的 ID 的更多详细信息。\n如果已经设置了你的 aws CLI ，可以执行以下命令：\nAWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) 然后，通过执行以下命令，创建一个 OnboardingPolicy，以允许你 AWS 帐户 ID 拥有的任何 AWS ECS 任务加入 bookinfo 命名空间中的任何 WorkloadGroup。将 \u0026lt;AWS_ACCOUNT_ID\u0026gt; 替换为适当的值。\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: authorization.onboarding.tetrate.io/v1alpha1 kind: OnboardingPolicy metadata: name: allow-ecs namespace: bookinfo # (1) spec: allow: - workloads: - aws: accounts: - \u0026#34;\u0026lt;AWS_ACCOUNT_ID\u0026gt;\u0026#34; # (2) ecs: {} # (3) onboardTo: - workloadGroupSelector: {} # (4) EOF 上述策略适用于由 (2) 中指定的帐户拥有的任何 AWS ECS 任务 (3)，并允许它们加入 bookinfo 命名空间 (1) 中的任何 WorkloadGroup (4)。\n","relpermalink":"/book/tsb/setup/workload-onboarding/quickstart/aws-ecs/configure-workload-onboarding/","summary":"你将部署 ratings 应用程序作为 AWS ECS 任务，并将其加入服务网格。 创建 WorkloadGroup 执行以下命令创建一个 WorkloadGroup： cat \u003c\u003cEOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: labels: app: ratings class: ecs cloud: aws serviceAccount: bookinfo-ratings EOF 字段 spec.template.serviceAccount 声明了工作负载将具有 Kubernetes 集群","title":"配置 AWS ECS 工作负载的 WorkloadGroup 和 Sidecar"},{"content":"TSB 带有每个控制平面集群的速率限制服务器组件。默认情况下，此功能已禁用。\n此部分仅讨论内部模式 的安装过程，不涉及外部服务器 的安装。\n配置 可以通过在 ControlPlane Operator API 或 Helm 值 中明确指定 rateLimitServer 组件的配置并将其应用于相关的控制平面集群来启用速率限制服务器。rateLimitServer 需要一个 Redis 后端来跟踪速率限制属性计数，并且其详细信息需要包含在配置中。\n你的 Control Plane Operator 配置可能如下所示：\nspec: ... components: rateLimitServer: domain: \u0026lt;domain\u0026gt; backend: redis: uri: \u0026lt;redis-uri\u0026gt; 注意在 components 对象中引入了 rateLimitServer。\ndomain 的值用于对速率限制的存储元数据进行分组。对所有 Control Planes 指定相同的 domain 将有效允许你配置跨所有集群的全局速率限制。如果使用不同的值为 domain，那么速率限制效果将仅局限于查看相同 domain 的那些集群。这假定 Control Planes 指定相同的 Redis 服务器。\n我们建议你仅在同一地理区域内的集群中指定相同的域，例如 us-east。\nredis-uri 的值是要使用的 Redis 实例的服务器名称和端口。你需要确保从控制平面集群能够访问此 URI。\nRedis 认证 如果你的 Redis 数据库需要密码，你可以自己创建密钥：\nkubectl -n istio-system create secret generic \\ redis-credentials \\ --from-literal=REDIS_AUTH=\u0026lt;password\u0026gt; 如果运行的 TSB 版本 \u0026gt;= 1.4.0，你可以使用 tctl install manifest control-plane-secrets 命令中的 --redis-password 参数来指定密码以生成适当的密钥。\nTLS 如果你的 Redis 数据库支持传输加密（TLS），则需要通过在 redis-credentials 密钥中将 REDIS_TLS 键设置为 true 来启用 Ratelimit Redis 客户端中的 TLS。示例命令如下：\nkubectl -n istio-system create secret generic \\ redis-credentials \\ --from-literal=REDIS_AUTH=\u0026lt;password\u0026gt; --from-literal=REDIS_TLS=true 如果运行的 TSB 版本 \u0026gt;= 1.5.0，你可以使用 tctl install manifest control-plane-secrets 命令中的 --redis-tls 参数来指定它以生成适当的密钥。你还可以使用 --redis-tls-ca-cert 参数指定自定义 CA 证书以验证 TLS 连接，以及使用 --redis-tls-client-key 和 --redis-tls-client-cert 参数指定 Redis 客户端密钥和证书（如果启用了客户端证书身份验证），这将在 tctl install manifest control-plane-secrets 命令中生成适当的 redis-credentials 密钥。\n部署服务器 创建一个使用上述示例的清单。确保在以前的示例中省略的控制平面中包含所有必要的字段。\n如果要更新现有的控制平面，你可以使用 kubectl get controlplane -n istio-system -o yaml 来获取当前的值。\n将清单保存到文件中，例如 control-plane-with-rate-limiting.yaml，然后使用 kubectl 应用它：\nkubectl apply -f control-plane-with-rate-limiting.yaml 要检查速率限制服务器是否在集群中正常运行，请执行以下命令：\nkubectl get pods -n istio-system | grep ratelimit ratelimit-server-864654b5b5-d77bq 1/1 Running 2 2d1h ","relpermalink":"/book/tsb/howto/rate-limiting/internal-rate-limiting/","summary":"TSB 带有每个控制平面集群的速率限制服务器组件。默认情况下，此功能已禁用。 此部分仅讨论内部模式 的安装过程，不涉及外部服务器 的安装。 配置 可以通过在 ControlPlane Operator API 或 Helm 值 中明确指定 rateLimitServer 组件的配置并将其应用于相关的控制平面","title":"启用内部速率限制服务器"},{"content":"Tetrate Service Bridge 的命令行界面（CLI）允许你与 TSB API 交互，以便以编程或交互的方式轻松操作对象和配置。CLI 通过提交 TSB 或 Istio 对象的 YAML 表示来工作。\n安装 请参阅 TSB 文档 。\n配置 CLI 配置支持多个配置文件，用于轻松管理来自同一 CLI 的不同环境。CLI 中的配置由集群和凭据的一对一配对定义。\n凭据 CLI 中的凭据被称为 user。有关 user 子命令的完整参考信息可以在 CLI 参考 页面中找到。下面是创建名为 admin-user 用户的示例：\ntctl config users set admin-user --username admin --password \u0026#39;MySuperSecret!\u0026#39; --org tetrate --tenant tenant1 每当在配置文件中使用 admin-user 时，CLI 将提交 admin 用户和 MySuperSecret! 密码，以及 tetrate 组织和 tenant1 租户。\n密码中的特殊字符 在终端中使用可能被视为特殊字符的字符时要小心。例如，如果包含 $（美元符号）并使用双引号引用它们，可能会以意想不到的方式解释它们。\n由于每个终端的行为可能略有不同，请始终查阅手册以获取确切的语法，以避免这些特殊字符以意外的方式被解释。在大多数情况下，使用单引号应该是安全的。\n这个警告适用于在终端上键入的几乎所有内容，但密码有更高的风险，因为鼓励使用特殊字符。\n集群 CLI 中的集群映射到给定的 TSB API 终点。有关 clusters 子命令的完整参考信息可以在 CLI 参考 页面中找到。下面是创建名为 my-tsb 集群的示例：\ntctl config clusters set my-tsb --bridge-address my.tsb.corp:8443 每当在配置文件中使用 my-tsb 时，CLI 将发送请求到 https://my.tsb.corp:8443/ 终点。\n配置文件 配置文件是 cluster 和 username 的给定组合。其结果是 CLI 发送请求到由 cluster 指定的终点，并使用 username 凭据进行身份验证。有关 profiles 子命令的完整参考信息可以在 CLI 参考 页面中找到。下面是创建名为 demo-tsb 配置文件的示例：\ntctl config profiles set demo-tsb --cluster my-tsb --username admin-user CLI 可以使用不同的集群和用户组合拥有多个 profiles。在未指定 --profile 选项时，其中一个配置文件将被用作默认配置文件。你可以随时更改当前配置文件，如下所示。\ntctl config profiles list CURRENT NAME CLUSTER ACCOUNT * default demo-tsb my-tsb admin-user tctl config profiles set-current demo-tsb tctl config profiles list CURRENT NAME CLUSTER ACCOUNT default * demo-tsb my-tsb admin-user 命令完成 tctl 为 bash shell 提供了命令完成，允许轻松查找命令及其标志。假设已启用 bash 完成，你可以在 completion 命令的输出上执行源代码，以使 bash 中的 tctl 命令的自动完成工作。\nsource \u0026lt;(tctl completion bash) ","relpermalink":"/book/tsb/reference/cli/guide/index/","summary":"使用 Tetrate Service Bridge CLI 入门。","title":"入门指南"},{"content":"Tetrate Service Bridge (TSB) 提供了授权功能，用于授权来自另一个服务的每个 HTTP 请求（“服务到服务\u0026#34;请求）。\nTSB 支持本地授权，使用 JWT 声明，以及外部授权，后者使用在外部运行的服务来确定是否应允许或拒绝请求。外部授权可以用于网关和工作负载（通过它们的 Sidecar）。\n如果你有一个独立的内部系统，或者希望与第三方授权解决方案（如 Open Policy Agent (OPA) 或 PlainID ）集成，你可以决定使用外部授权系统。\n本文描述了如何使用 OPA 作为示例配置服务到服务的授权。OPA 是一个开源的通用策略引擎，提供高级声明性语言，让你可以将策略规定为代码。\nOPA 支持 Tetrate 不提供对 OPA 的支持。如果你需要针对你的用例支持，请查找其他支持。 在开始之前，请确保你已经完成以下步骤：\n熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成了 TSB 使用快速入门。本文假设你已经创建了一个租户，并熟悉 工作空间 和配置组。还需要将 tctl 配置到你的 TSB 环境。 概述 下图显示了在使用外部授权系统授权服务间请求时的请求和响应流程。\n期望的结果是能够从\u0026#34;Sleep 工作负载\u0026#34;向”httpbin with OPA 工作负载\u0026#34;发送请求，并通过 OPA 执行适当的授权检查。如果从\u0026#34;Sleep 工作负载\u0026#34;发出的请求被视为未经授权，则应返回403 Forbidden。\n请注意，尽管在此示例中，你将 OPA 部署为 Pod 内的 Sidecar，但也可以将 OPA 部署为单独的 Pod。如果将 OPA 部署为单独的 Pod，你将需要自行调查在稍后指定外部系统的 URL 时使用的值。\n设置服务 设置 httpbin 服务 首先设置\u0026#34;服务器端\u0026#34;，即图表中的\u0026#34;httpbin with OPA 工作负载\u0026#34;组件。\nOPA 策略 在启动服务之前，你需要创建包含 OPA 策略的 Kubernetes Secret。\n以下是你将用于授权请求的 OPA 策略示例。当以下条件满足时，它将允许请求：\n存在 JWT 令牌 JWT 令牌未过期 你要访问的 URL 路径在 JWT 令牌中指定 创建一个名为 s2s-policy.rego 的文件，其内容如下：\n然后将策略存储在 Kubernetes 中作为 Secret。\nkubectl create namespace httpbin kubectl create secret generic opa-policy -n httpbin --from-file s2s-policy.rego 创建带有 OPA 和 Envoy Sidecar 的 httpbin 部署 一旦你有了策略，就可以部署引用该策略的 httpbin 服务。 创建一个名为 s2s-httpbin-with-opa.yaml 的文件，其内容如下：\n然后使用 kubectl 应用它：\nkubectl label namespace httpbin istio-injection=enabled --overwrite=true kubectl apply -n httpbin -f s2s-httpbin-with-opa.yaml 设置 sleep 服务 由于你将配置服务到服务授权，因此需要一个服务作为httpbin服务的客户端。\n在本示例中，你将部署一个什么都不做的服务，该服务映射到上图中的\u0026#34;sleep 工作负载\u0026#34;。稍后你将使用 kubectl exec 发出 HTTP 请求到 httpbin 服务。\n创建一个名为 s2s-sleep.yaml 的文件，其内容如下：\n使用 kubectl 部署此 sleep 服务：\nkubectl create namespace sleep kubectl label namespace httpbin istio-injection=enabled --overwrite=true kubectl apply -n sleep -f s2s-sleep.yaml 测试 禁用外部授权进行测试 到目前为止，你已经部署了服务，但尚未启用外部授权。因此，来自\nsleep服务到httpbin服务的请求不会检查授权。\n这可以通过检查是否从sleep服务发送的 HTTP 请求导致200 OK 来看到。\n要从 sleep 服务发送请求，请在sleep服务中确定要发送请求的 Pod：\nexport SLEEP_POD=$(kubectl get pod -n sleep -l app=sleep -o jsonpath={.items..metadata.name}) 然后从此 Pod 发送请求到httpbin服务，应该可以在 http://httpbin-with-opa.httpbin:8000 处到达：\nkubectl exec ${SLEEP_POD} -n sleep -c sleep -- curl http://httpbin-with-opa.httpbin:8000/headers -s -o /dev/null -w \u0026#34;%{http_code}\\n\u0026#34; 禁用外部授权时，上述命令应显示200。\n启用外部授权进行测试 要查看外部授权的工作原理，你需要创建一个工作空间和安全组。\n创建工作空间 创建一个名为 s2s-workspace.yaml 的文件，其内容如下。\n请注意，在以下示例中，我们假设你已经使用 TSB 演示安装创建了名为demo的集群，并在其中部署了你的httpbin服务。如果你使用其他集群，请相应更改示例中的集群名称。\n然后使用 tctl 应用它：\ntctl apply -f s2s-workspace.yaml 创建 SecuritySettings 一旦有了工作空间，你需要为该工作空间创建 SecuritySettings 以启用外部授权。\n创建一个名为 s2s-security-settings.yaml 的文件，其内容如下。\n请注意，uri 指向本地地址 (grpc://127.0.0.1:9191)，因为在此示例中，OPA 服务部署在同一 Pod 中作为 Sidecar。如果你将 OPA 部署在单独的 Pod 中，你需要相应地更改 uri 的值。\n然后使用 tctl 应用它：\ntctl apply -f s2s-security-settings.yaml 测试授权 再次向 httpbin 服务发送请求。\n使用已应用的 SecuritySettings，来自sleep服务到httpbin服务的普通请求应该失败，并显示403 Forbidden。\nkubectl exec ${SLEEP_POD} -n sleep -c sleep -- curl http://httpbin-with-opa.httpbin:8000/headers -s -o /dev/null -w \u0026#34;%{http_code}\\n\u0026#34; 上述命令应显示403。\n为了授权请求，你需要在请求中添加 JWT。对于此示例，我们希望附加到请求的原始 JWT 如下所示：\n{ \u0026#34;path\u0026#34;: \u0026#34;L2hlYWRlcnM=\u0026#34;, \u0026#34;nbf\u0026#34;: 1500000000, \u0026#34;exp\u0026#34;: 1900000000 } 路径声明的值为 L2hlYWRlcnM=，这是字符串 /headers 的 Base64 编码形式。\nJWT 需要通过 Authorization 标头传递，这需要整个 JWT 作为 Base64 编码，如下所示。将其保存到环境变量中：\nexport JWT_TOKEN=\u0026#34;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiTDJobFlXUmxjbk09IiwibmJmIjoxNTAwMDAwMDAwLCJleHAiOjE5MDAwMDAwMDB9.9yl8LcZdq-5UpNLm0Hn0nnoBHXXAnK4e8RSl9vn6l98\u0026#34; 最后，使用上述 JWT 令牌向 httpbin 服务发送请求，确保请求指向与 JWT 中的声明匹配的路径 /headers。这次你应该收到 200 OK。\nkubectl exec ${SLEEP_POD} -n sleep -c sleep -- curl http://httpbin-with-opa.httpbin:8000/headers -H \u0026#34;Authorization: Bearer $JWT_TOKEN\u0026#34; -s -o /dev/null -w \u0026#34;%{http_code}\\n\u0026#34; 要检查其他路径的请求是否未经授权，请尝试发送以下请求，该请求指向路径 /get。以下命令应显示 403 Forbidden。\nkubectl exec ${SLEEP_POD} -n sleep -c sleep -- curl http://httpbin-with-opa.httpbin:8000/get -H \u0026#34;Authorization: Bearer $JWT_TOKEN\u0026#34; -s -o /dev/null -w \u0026#34;%{http_code}\\n\u0026#34; ","relpermalink":"/book/tsb/howto/authorization/sidecar/","summary":"演示如何使用 OPA 授权服务到服务的流量。","title":"使用外部授权进行服务间的授权"},{"content":" 在应用集群中启用 Tier1 网关\n流式服务日志\nIstio CNI\n网关删除保持 Webhook\n配置保护\nEdge 处的 DNS 解析\nGitOps\n配置集群外部地址\n","relpermalink":"/book/tsb/operations/features/","summary":"在应用集群中启用 Tier1 网关 流式服务日志 Istio CNI 网关删除保持 Webhook 配置保护 Edge 处的 DNS 解析 GitOps 配置集群外部地址","title":"特性"},{"content":"本文介绍了 Tetrate Service Bridge（TSB）生态系统中统一网关的概念，解释了其重要性，并提供了详细的使用场景。\n简介 统一网关是在 TSB 1.7.0 中引入的关键功能，它将Tier1Gateway 和IngressGateway 的功能合并到一个称为Gateway 的公共资源中。这种统一简化了网关管理过程，并提供了更一致的体验。\n从 TSB 1.7.0 开始，Tier1Gateway 和 IngressGateway 资源将被弃用，我们强烈建议使用 Gateway 资源满足你的所有网关需求。前 Tier1 Gateway 现在将被统称为Edge Gateway 。\n统一网关选项卡无缝集成到 TSB UI 中，使得任何网关的配置都变得容易，不管它是作为 Tier 1 还是 Tier 2 网关工作。\n为什么需要统一网关？ 在我们的旅程早期，我们认识到我们的客户对集群特定（Tier 2）和跨云供应商（Tier1）网关有不同的需求。因此，我们开发了不同的网关解决方案来满足这些不同的需求。然而，随着我们的 Gateway API 的发展和客户需求变得更加复杂，我们不断增强 Tier1 网关的能力的需求变得明显。\n这种发展带来了挑战——持续的工程努力、客户教育何时选择 Tier1 或 Tier2 以及维护并行代码库。我们已经着手开展一项开创性的工作：统一网关，以简化这些复杂性并提供更一致的体验。\n统一网关的优势 统一网关不仅是 Tier 1 和 Tier 2 网关的融合，它是网关管理的范式转变。以下是你需要了解这个变革性解决方案的内容：\n全面的功能 统一网关结合了 TSB 版本 1.6.X 中 Tier 1 和 Tier 2 网关的强大功能，确保你获得最佳的两个世界。无论是处理重试、故障转移还是任何其他高级功能，统一网关都可以为你提供支持，无论它是作为 Tier 1 还是 Tier 2 网关配置的。\n无缝过渡 对于我们现有的客户，我们了解连续性的重要性。不用担心，你的 Tier 1 和 Tier 2 网关将继续像往常一样使用 1.6.X 版本提供的功能。但我们不会止步于此。我们正在引入一个无缝的过渡过程，将你现有的网关过渡到统一网关模型，增强 Tier 1 功能，如重试等等。\n统一网关的新 API 拥抱创新并不意味着忽略过去。在为新机遇引入新的统一网关 API 的同时，我们致力于支持后续三个 TSB 版本的先前 API。这确保你可以按照自己的节奏切换，而不会受到干扰。\n授权直连模式 统一网关不仅仅是网关，而是赋能。新老客户都可以通过直连模式发挥网关 API 的全部功能，从而对其网格基础设施获得无与伦比的控制和自定义。\n与 Open API 策略相符 我们相信开放标准的力量。统一网关与我们的开放 API 策略完美契合，使你可以使用标准化的 Open API 规范配置统一网关。这种方法促进了一致性，并简化了与现有工具链的集成。\n使用案例 让我们深入了解统一网关的使用场景。\n准备集群 下图显示了我们在本文档中使用的部署架构。我们在 GKE 中创建了 3 个集群，在其中一个集群中部署了 TSB，将另外三个集群加载到了 TSB 中，并在基础设施下的集群中部署了 bookinfo 应用程序。\n下表描述了这些集群的角色和应用程序：\n集群 gke-jimmy-us-central1-1 gke-jimmy-us-west1-1 gke-jimmy-us-west1-2 gke-jimmy-us-west2-3 Region us-central1 us-west1 us-west1 us-west2 TSB 角色 Management Plane Control Plane Control Plane Control Plane Application - bookinfo-frontend bookinfo-backend httpbin Services - productpage productpage, ratings, reviews, details httpbin Network tier1 cp-cluster-1 cp-cluster-2 cp-cluster-3 本节介绍了统一网关的使用场景。\n场景 1：基于集群的路由，使用 HTTP 路径和 Header 匹配 在这种情况下，我们将使用 Gateway 资源来公开bookinfo.tetrate.io和httpbin.tetrate.io。我们将利用基于 Gateway 的集群路由功能，根据 Gateway 上的路径前缀将 bookinfo 前端服务路由到 cp-cluster-1，将其他后端服务路由到 cp-cluster-2。使用 Gateway，用户可以公开多个具有 clusterDestination 的主机，只要主机：端口组合是唯一的即可。\n部署拓扑和流量路由\n我们设置了以下部署拓扑：\nTier 1 集群：该集群用作外部流量的入口点，并将其路由到相应的后端集群。 后端集群：有三个后端集群，每个集群托管不同的服务： cp-cluster-1托管“Bookinfo”应用程序的前端服务。 cp-cluster-2托管“Bookinfo”应用程序的后端服务。 cp-cluster-3托管名为httpbin的 HTTP 服务。 配置\n1. Tier 1 集群网关（边缘网关）：\n在 tier1 集群中，我们部署了一个名为 edge-gateway 的网关。该网关接收传入的流量，并根据主机和路径前缀将其路由到适当的后端集群。\n以下是路由请求到“Bookinfo”前端和后端服务的配置摘录：\napiVersion: gateway.tsb.tetrate.io/v2 kind: Gateway metadata: name: edge-gateway namespace: tier1 annotations: tsb.tetrate.io/organization: tetrate tsb.tetrate.io/tenant: tier1 tsb.tetrate.io/workspace: tier1 tsb.tetrate.io/gatewayGroup: edge-gateway-group spec: workloadSelector: namespace: tier1 labels: app: edge-gateway http: - name: bookinfo hostname: bookinfo.tetrate.io port: 80 routing: rules: - match: - uri: prefix: \u0026#34;/productpage\u0026#34; headers: X-CLUSTER-SELECTOR: exact: gke-jimmy-us-west1-1 route: clusterDestination: clusters: - name: gke-jimmy-us-west1-1 weight: 100 - match: - uri: prefix: \u0026#34;/productpage\u0026#34; headers: X-CLUSTER-SELECTOR: exact: gke-jimmy-us-west1-2 route: clusterDestination: clusters: - name: gke-jimmy-us-west1-2 weight: 100 - match: - uri: prefix: \u0026#34;/productpage\u0026#34; route: clusterDestination: clusters: - name: gke-jimmy-us-west1-1 weight: 100 - match: - uri: prefix: \u0026#34;/api/v1/products\u0026#34; route: clusterDestination: clusters: - name: gke-jimmy-us-west1-2 weight: 100 - name: httpbin hostname: httpbin.tetrate.io port: 80 routing: rules: - route: clusterDestination: clusters: - name: gke-jimmy-us-west2-3 weight: 100 这些规则确保带有不同路径前缀的对 bookinfo.tetrate.io 的请求被路由到适当的后端集群。同样，请求到 httpbin.tetrate.io 的流量被重定向到 cp-cluster-3。\n2. 后端集群中的入口网关\n在每个后端集群（cp-cluster-1、cp-cluster-2 和 cp-cluster-3）中，我们部署了入口网关，以接收来自 tier1 集群的流量，并将其路由到相应的服务。\n以下是 cp-cluster-1 中 Ingress Gateway 的示例配置：\napiVersion: gateway.tsb.tetrate.io/v2 kind: Gateway metadata: name: bookinfo-ingress-gateway spec: # ... (metadata and selectors) http: - hostname: bookinfo.tetrate.io name: bookinfo-tetrate port: 80 routing: rules: - route: serviceDestination: host: bookinfo-frontend/productpage.bookinfo-frontend.svc.cluster.local 这个配置可以确保在 cp-cluster-1 中收到的 bookinfo.tetrate.io 的 Ingress Gateway 的流量被路由到前端服务。\n验证\n我们可以使用像 curl 这样的工具请求公开的服务以验证设置。例如，要测试 /productpage：\nexport GATEWAY_IP=$(kubectl -n tier1 get service edge-gateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) curl -Ss \u0026#34;\u0026lt;http://bookinfo.tetrate.io/productpage\u0026gt;\u0026#34; --resolve \u0026#34;bookinfo.tetrate.io:80:$GATEWAY_IP\u0026#34; -v 同样，你可以根据定义的路由规则测试其他路由和服务。\n场景 2：主机路由与网关标头重写 此场景展示了统一网关的权限重写或标头重写功能。我们在 tier1 集群中部署边缘网关，以在不同集群之间路由流量，并使用 IngressGateways 为每个控制平面集群接收流量。\n部署拓扑和流量路由\n我们已经设置了以下部署拓扑：\nTier 1 Cluster： 该集群作为外部流量的入口点，并将其路由到相应的后端集群。\n后端集群：\n有三个后端集群，每个集群托管不同的服务：\ncp-cluster-1 托管\u0026#34;Bookinfo\u0026#34;应用程序的前端服务。 cp-cluster-2 托管\u0026#34;Bookinfo\u0026#34;应用程序的后端服务。 配置\n1. Tier 1 Cluster Gateway (tier1-gateway)\n在 Tier 1 集群中，我们部署名为 tier1-gateway 的网关。此网关接收传入流量并根据主机和路径前缀将其路由到适当的后端集群。此外，它会为特定路由执行主机标头重写。\n以下是用于使用标头重写路由到\u0026#34;Bookinfo\u0026#34;前端和后端服务的配置片段：\napiVersion: gateway.tsb.tetrate.io/v2 kind: Gateway metadata: name: tier1-gateway namespace: tier1 …","relpermalink":"/book/tsb/howto/gateway/unified-gateway/","summary":"本文介绍了 Tetrate Service Bridge（TSB）生态系统中统一网关的概念，解释了其重要性，并提供了详细的使用场景。","title":"统一网关"},{"content":"本页面提供了开始使用 Tetrate Service Bridge（TSB）安装所需的先决条件和下载说明的全面概述。\n要有效地管理 TSB 服务网格，需要对 Kubernetes 和 Docker 仓库操作有深入的了解。我们建议咨询它们各自的支持文档以获取额外的指导。\n先决条件 你可以安装用于生产的 TSB，也可以安装用于演示配置文件以快速了解 TSB。请查看以下表格中的每个要求：\n项目 生产 TSB 演示/快速入门TSB Kubernetes 集群：\nEKS 1.21 - 1.24\nGKE 1.21 - 1.24\nAKS 1.21 - 1.24（包括 Azure Stack HCI）\nOpenShift 4.7 - 4.11\nDocker UCP 3.2.5 或更高版本 ✓ ✓ Docker UCP 3.2.5 或更高版本 ✓ ✓ 私有 Docker 注册表（HTTPS） ✓ ✓ Tetrate 存储库帐户和 API 密钥（如果你尚未拥有此内容，请联系 Tetrate） ✓ ✓ Docker 引擎 18.03.01 或更高版本，具有对私有 Docker 注册表的推送访问权限 ✓ ✓ PostgreSQL 11.1 或更高版本 ✓ 打包（v14.4） Elasticsearch 6.x 或 7.x ✓ 打包（v7.8.1） Redis 6.2 或更高版本 ✓ 打包（v7.0.5） LDAP 服务器或 OIDC 提供程序 ✓ 打包（OpenLDAP v2.6） Cert-manager：cert-manager v1.7.2 或更高版本 ✓ 打包（cert-manager v1.10.0） cert-manager 用法 cert-manager 用于为 TSB webhook、TSB 内部通信和 Istio 控制平面与外部 CA 集成等颁发和管理证书。 cert-manager 版本 cert-manager 1.4.0 是使用 TSB 1.5 所需的最低版本。它具有特性标志，用于签署 K8S CSR 请求，支持 Kubernetes 1.16-1.21。前往cert-manager 受支持的版本 以获取有关受支持的 Kubernetes 和 OpenShift 版本的更多信息。 生产安装注意事项 你的 Kubernetes 集群的大小取决于平台部署要求。基本的 TSB 安装不会消耗太多额外的资源。存储的大小非常取决于应用程序集群的大小、工作负载的数量（及其请求率）以及可观测性配置（采样率、数据保留期等）。有关更多信息，请参见我们的容量规划指南。 当运行自托管时，你的组织可能会对上述环境和应用程序施加额外的（安全）限制、可用性和灾难恢复要求。有关如何调整 TSB 安装和配置的详细信息，请参阅 Operator 参考指南以及我们的文档中的操作任务 部分，在其中可以找到有关配置选项、常见部署方案和解决方案的描述。\n身份标识提供者 TSB 需要标识提供程序（IdP）作为用户来源。此标识提供程序用于用户身份验证以及定期将现有用户和组的信息同步到平台中。TSB 可以与 LDAP 或任何符合 OIDC 的标识提供程序集成。\n要使用 LDAP，你必须弄清楚如何查询 LDAP，以便 TSB 可以将其用于身份验证和用户和组的同步。有关 LDAP 配置的更多详细信息，请参见 LDAP 作为标识提供程序 。\n要使用 OIDC，请在你的 IdP 中创建 OIDC 客户端。启用授权代码流以使用 UI 登录，并启用设备授权以使用设备代码使用 tctl 登录。有关更多信息和示例，请参见如何设置 Azure AD 作为 TSB 标识提供程序。\nOIDC IdP 同步 TSB 支持 Azure AD 用于同步用户和组。如果你使用其他 IdP，则必须创建同步作业，将从你的 IdP 获取用户和团队并使用同步 API 将它们同步到 TSB 中。有关更多详细信息，请参见用户同步。 数据和遥测存储 TSB 需要外部数据和遥测存储。TSB 使用 PostgreSQL 作为数据存储和 Elasticsearch 作为遥测存储。\nDemo 存储 演示安装将部署 PostgreSQL、Elasticsearch 和 LDAP 服务器作为标识提供程序，其中填充了模拟用户和团队。演示存储不适用于生产使用。请确保为你的生产环境提供适当的 PostgreSQL、Elasticsearch 和标识提供程序。 证书提供者 TSB 1.5 需要证书提供者来支持内部 TSB 组件的证书颁发，例如 Webhook 证书和其他用途。此证书提供者必须在管理平面集群和所有控制平面集群中都可用。\nTSB 支持cert-manager作为其中一个受支持的提供者。它可以为你管理cert-manager安装的生命周期。要在集群中配置cert-manager的安装，请将以下部分作为ManagementPlane或ControlPlane CR 的一部分添加：\ncomponents: internalCertProvider: certManager: managed: INTERNAL 你还可以使用任何支持kube-CSR API 的证书提供者。要使用自定义提供者，请参阅以下部分 Internal Cert Provider\n现有的 cert-manager 安装 如果你已经使用 cert-manager 作为集群的一部分，则可以将ManagementPlane或ControlPlane CR 中的managed字段设置为EXTERNAL，使 TSB 利用现有的 cert-manager 安装。如果将managed字段设置为INTERNAL，则 TSB Operator 会在找到已安装的 cert-manager 时失败，以确保它不覆盖现有的 cert-manager 安装。 cert-manager Kube-CSR TSB 使用 kubernetes CSR 资源为各种 Webhook 颁发证书。如果你的配置使用外部 cert-manager 安装，请确保 cert-manager 可以签署 Kubernetes CSR 请求。例如，在 cert-manager 1.7.2 中，通过设置此特性标志 ExperimentalCertificateSigningRequestControllers=true启用此功能。对于使用内部托管的 cert-manager 的 TSB 管理安装，此配置已作为安装的一部分设置。 下载 tctl 设置 TSB 的初始步骤是安装我们的 TSB CLI 工具，称为tctl。使用tctl，你可以执行 TSB 安装（或升级），使用 YAML 对象与 TSB API 进行交互，并将 TSB 无缝集成到 GitOps 工作流程中。\n请按照 CLI 参考页面中概述的说明下载和安装tctl。\n同步 Tetrate Service Bridge 镜像 安装了tctl之后，你可以检索必要的容器镜像并将它们上传到你的私有 Docker 注册表。tctl工具通过image-sync命令简化了此过程，该命令下载与当前tctl版本对应的 镜像版本，并将其推送到你的 Docker 注册表。使用你的 Tetrate 存储库帐户凭据和指定你的私有 Docker 注册表的registry参数使用username和apikey参数。\ntctl install image-sync --username \u0026lt;user-name\u0026gt; \\ --apikey \u0026lt;api-key\u0026gt; --registry \u0026lt;registry-location\u0026gt; 在初始执行期间，你需要接受最终用户许可协议（EULA）。如果你在没有交互式终端访问权限的环境中运行 TSB 安装，例如 CI / CD 流程，请将--accept-eula标志附加到上述命令中。\n在 Kind 集群中加载演示镜像 对于本地kind 集群中的demo配置文件安装，请使用以下命令直接将镜像加载到 kind 节点中：\n#使用我们的“用户名”和“apikey”登录到Docker注册表 docker login containers.dl.tetrate.io #拉取所有docker镜像 for i in `tctl install image-sync --just-print --raw` ; do docker pull $i ; done #将镜像加载到kind节点中 for i in `tctl install image-sync --just-print --raw` ; do kind load docker-image $i ; done 安装 集群配置文件 在操作多集群 TSB 环境时，与多个 Kubernetes 集群的交互变得普遍。虽然文档没有明确引用kubectl配置上下文和tctl config 配置文件，但这些选择是特定于环境的。确保选择了正确的kubectl上下文和tctl配置文件作为默认值，或在使用这些工具执行命令时使用显式参数。 要使用 Helm Chart 继续进行安装，请参阅 Helm 安装指南。\n要使用tctl进行安装，请继续查看 tctl 安装指南。\n有关演示安装过程的详细说明，请转到演示安装指南。\n","relpermalink":"/book/tsb/setup/requirements-and-download/","summary":"本页面提供了开始使用 Tetrate Service Bridge（TSB）安装所需的先决条件和下载说明的全面概述。 要有效地管理 TSB 服务网格，需要对 Kubernetes 和 Docker 仓库操作有深入的了解。我们建议咨询它们各自的支持文档以获取额外的指导。 先决条件","title":"先决条件和下载"},{"content":"本指南将引导你完成 TSB 演示配置文件的安装，该配置文件旨在快速概述 TSB 的功能。演示配置文件包括 PostgreSQL、Elasticsearch 和 LDAP，所有这些都在 Kubernetes 集群上进行编排。为了确保无缝体验，你的集群应包含 3-6 个节点，每个节点至少配备 4 个 vCPU 和 16 GB 内存。集群还必须建立默认存储类，并能够为 Elasticsearch 和 PostgreSQL 创建最小容量为 100 GB 的持久卷声明。\n在继续之前，请参阅 TSB 支持政策 来验证与你的 Kubernetes 版本的兼容性。\n先决条件 要安装演示配置文件，请确保你已完成以下步骤：\n1. 获取 tctl 并同步镜像 首先按照下载部分 中概述的步骤下载 tctl 。此外，按照同步容器镜像中所述同步所需的容器镜像 。\n2. 设置 Kubernetes 集群 准备一个要安装演示配置文件的 Kubernetes 集群。创建集群的具体步骤取决于你的环境。有关创建 Kubernetes 集群的具体说明，请参阅你的环境手册。\n使用 kind 如果你使用 kind 集群进行安装，请按照以下步骤操作：\n创建类型集群后，安装 MetalLB 以使 TSB 能够使用 LoadBalancer 类型的服务。 配置 L2 网络 ，指定 kind Docker 网络 IP 范围内的 IP 地址范围。 安装 请按照以下步骤安装演示配置文件：\n1.执行 tctl install demo 确保你的 Kubernetes 上下文设置为目标集群。使用 tctl install demo 命令，该命令利用 kubectl 配置中的 current-context 。在继续之前，请确认引用了正确的 Kubernetes 集群。\n运行安装命令，如下所示。你可以使用 --admin-password 选项（自版本 1.4.0 起可用）提供管理员密码。或者，将为你生成一个密码。\ntctl install demo \\ --registry \u0026lt;registry-location\u0026gt; \\ --admin-password \u0026lt;password\u0026gt; 安装注意事项 在某些资源受限或负载较重的环境中，安装时间可能比预期长，并且 tctl 工具可能会退出。 tctl install demo 命令是幂等的，允许你重新运行它，直到安装完成。 成功安装后，你的 Kubernetes 集群将托管管理和控制平面，并将创建一个名为 tetrate 的组织。\n访问网络用户界面 要访问 TSB Web UI，请执行以下步骤：\n从演示安装命令的输出中获取 URL 和凭据。查找类似于以下内容的输出：\nControlplane installed successfully! Management Plane UI accessible at: https://31.224.214.68:8443 Admin credentials: username: admin, password: yGWx1s!Y@\u0026amp;-KBe0V 使用提供的 URL 和管理凭据登录 Web UI。\n提示 即使你跳过快速入门，也请考虑创建租户 ，因为遵循本网站上的示例可能需要它。 进一步配置 有关演示安装的其他自定义（例如载入集群），请参阅载入集群指南 。\n","relpermalink":"/book/tsb/setup/self-managed/demo-installation/","summary":"TSB 演示应用安装。","title":"演示安装"},{"content":"创建一个简单的示例，包括两个工作负载集群和一个边缘网关集群。\n在这个示例中，我们将配置三个 Kubernetes 集群：\n集群 cluster-1 和 cluster-2 将作为工作负载集群，每个集群都有一个 bookinfo 应用程序实例和一个 Ingress Gateway 用于公开应用程序 集群 cluster-edge 将托管前端边缘（“Tier-1”）网关，该网关将接收流量并分发到工作负载集群中的 Ingress Gateway 开始之前 在配置中有一些移动部分，因此在继续之前，识别并命名每个部分会很有帮助：\ncluster-1 cluster-2 cluster-edge AWS 区域： eu-west-1 eu-west-2 eu-west-1 命名空间： bookinfo bookinfo edge 工作区： bookinfo-ws bookinfo-ws edge-ws 网络： app-network app-network edge-network 网关组： bookinfo-gwgroup-1 bookinfo-gwgroup-2 edge-gwgroup Ingress 网关： ingressgw-1 ingressgw-2 edgegw 网关资源： bookinfo-ingress-1 bookinfo-ingress-2 bookinfo-edge Kubectl 上下文别名： k1 k2 k3 确保 cluster-1 和 cluster-edge 位于同一个区域，而 cluster-2 位于另一个区域；在测试集群故障转移时，这将会很有用。\n在这个示例中，我们将使用组织 tse 和租户 tse。如果你使用 Tetrate Service Bridge (TSB)，请修改 Tetrate 配置以匹配你的组织层次结构。\n管理多个集群 在处理多个 Kubernetes 集群时，为每个集群的 kubectl 命令创建一个别名可能很有用。例如，对于 AWS 上下文，你可以执行以下操作：\nalias k1=\u0026#39;kubectl --context arn:aws:eks:eu-west-1:901234567890:cluster/my-cluster-1\u0026#39; 在应用 Tetrate 配置时，不需要执行此操作，Tetrate 配置可以使用 tctl 应用，或者与支持 GitOps 集成的任何 Kubernetes 集群。\n先决条件 我们将假设以下初始配置：\n集群 cluster-1、cluster-2 和 cluster-edge 已经加入 Tetrate 平台，无论是 TSE 还是 TSB 在每个集群上部署了任何必要的集成（例如 AWS 负载均衡控制器） 如果使用 Tetrate Service Express，已在 cluster-edge 上部署了 Route 53 控制器 步骤：\n创建 Tetrate 配置：创建 Tetrate 工作区、网络和网关组 在 cluster-1 中部署 bookinfo：在第一个集群中部署 bookinfo。部署一个 Ingress Gateway 和一个 Gateway 资源。 在 cluster-2 中部署 bookinfo：重复，在第二个集群中部署 bookinfo。部署一个 Ingress Gateway 和一个 Gateway 资源。 配置 Edge Gateway：在 Edge 集群中部署 Edge Gateway 和一个 Gateway 资源。如有必要，配置 DNS 并测试结果。 创建演示环境 创建 Tetrate 配置 我们将：\n为两个工作负载集群创建一个工作区，每个集群都有一个网关组 为边缘集群创建一个工作区和网关组 配置 cluster-edge 为 Tier-1 集群 定义 Tetrate 网络和可达性配置 我们如何做... 创建工作负载集群的配置 创建一个横跨两个工作负载集群的工作区 bookinfo-ws，以及每个集群的网关组。\ncat \u0026lt;\u0026lt;EOF \u0026gt; bookinfo-ws.yaml apiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tse tenant: tse name: bookinfo-ws spec: displayName: Bookinfo description: Test Bookinfo application namespaceSelector: names: - \u0026#34;cluster-1/bookinfo\u0026#34; - \u0026#34;cluster-2/bookinfo\u0026#34; EOF tctl apply -f bookinfo-ws.yaml cat \u0026lt;\u0026lt;EOF \u0026gt; bookinfo-gwgroup-1.yaml apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: organization: tse tenant: tse workspace: bookinfo-ws name: bookinfo-gwgroup-1 spec: namespaceSelector: names: - \u0026#34;cluster-1/bookinfo\u0026#34; EOF tctl apply -f bookinfo-gwgroup-1.yaml cat \u0026lt;\u0026lt;EOF \u0026gt; bookinfo-gwgroup-2.yaml apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: organization: tse tenant: tse workspace: bookinfo-ws name: bookinfo-gwgroup-2 spec: namespaceSelector: names: - \u0026#34;cluster-2/bookinfo\u0026#34; EOF tctl apply -f bookinfo-gwgroup-2.yaml 创建边缘集群的配置 创建一个工作区 edge-ws 和一个边缘集群的网关组：\ncat \u0026lt;\u0026lt;EOF \u0026gt; edge-ws.yaml apiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tse tenant: tse name: edge-ws spec: namespaceSelector: names: - \u0026#34;cluster-edge/edge\u0026#34; EOF tctl apply -f edge-ws.yaml cat \u0026lt;\u0026lt;EOF \u0026gt; edge-gwgroup.yaml apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: tenant: tse organization: tse workspace: edge-ws name: edge-gwgroup spec: namespaceSelector: names: - \u0026#39;cluster-edge/edge\u0026#39; EOF tctl apply -f edge-gwgroup.yaml 配置边缘集群为 Tier-1 集群 设置 Edge 集群的 “Is Tier 1” 标志。\n通常，使用 Tetrate UI 更容易配置集群设置：\n导航到 Clusters。编辑 cluster-edge 并将 ‘Tier 1 Cluster?’ 字段设置为 Yes。保存更改：\n更新 cluster-edge 的 Cluster 配置，添加键 spec: tier1Cluster: 如下所示：\napiVersion: api.tsb.tetrate.io/v2 kind: Cluster metadata: name: cluster-edge organization: tse spec: # highlight-next-line tier1Cluster: true 配置网络和可达性设置 Tetrate 平台使用网络设置来分组一组集群并定义访问控制列表。如果一个集群没有分配到网络，那么任何其他集群都可以访问该集群。在大规模操作时，网络设置提供了一种高级方式来标识一组集群并定义允许的流量。\n我们将：\n将 cluster-edge 分配给网络 Edge-Network 将 cluster-1 和 cluster-2 分配给网络 App-Network 定义可达性设置，以便 Edge-Network 可以向 App-Network 发送流量 通常，使用 Tetrate UI 配置网络设置更容易：\n分配网络 导航到 Clusters。编辑 cluster-edge 并将 Network 字段设置为值 Edge-Network。保存更改：\n对于集群 cluster-1 和 cluster-2，重复此步骤，将它们分配到网络 App-Network。\n定义可达性 导航到 Settings 和 Network Reachability。指定 Edge-Network 允许连接（发送流量到）App-Network：\n保存更改。\n分配网络 更新每个 Cluster 配置，添加键 spec: network: 如下所示：\napiVersion: api.tsb.tetrate.io/v2 kind: Cluster metadata: name: cluster-edge organization: tse spec: # highlight-next-line network: edge-network 定义可达性 更新 OrganizationSettings 配置，添加如下的 networkReachability 部分：\napiVersion: api.tsb.tetrate.io/v2 kind: OrganizationSetting metadata: name: default organization: tse spec: defaultSecuritySetting: authenticationSettings: trafficMode: REQUIRED authorization: mode: RULES rules: {} fqn: organizations/tse/settings/default # highlight-start networkSettings: networkReachability: Edge-Network: App-Network # highlight-end OrganizationSettings 资源是一个内部对象；你可以使用 tctl get organizationsettings -o yaml 获取它。在提交更新之前，删除任何 resourceVersion 或 etag 值。\n检查你的更改 完成更改后，UI 中的集群页面应如下所示：\n请注意每个集群的 Network 和 Is Tier1 列以及其值。\n此外，你将为每个集群创建了工作区和网关组，并定义了可达性设置，以使 Edge-Network 可以访问 App-Network。\n在 cluster-1 中部署 Bookinfo 我们将：\n创建 bookinfo 命名空间并部署 BookInfo 应用程序 在集群中部署一个 Ingress Gateway 发布一个 Gateway 资源以暴露 productpage.bookinfo 服务 验证服务是否正常运行 请记住设置 kubectl 上下文或使用你的上下文别名来指向 cluster-1。\n操作步骤... 创建 bookinfo 命名空间并部署 Bookinfo 应用程序： kubectl create namespace bookinfo kubectl label namespace …","relpermalink":"/book/tsb/design-guides/ha-multicluster/demo-1/","summary":"创建一个简单的示例，包括两个工作负载集群和一个边缘网关集群。 在这个示例中，我们将配置三个 Kubernetes 集群： 集群 cluster-1 和 cluster-2 将作为工作负载集群，每个集群都有一个 bookinfo 应用程序实例和一个 Ingress Gateway 用于公开应用程序 集群 cluster-edge 将托管前端边","title":"演示环境"},{"content":" 注意 本页面详细介绍了如何收集 Tetrate Service Bridge 运营所需的遥测数据，而不是由 Tetrate Service Bridge 管理的应用程序。 Tetrate Service Bridge 使用 Open Telemetry Collector 来简化指标收集。标准部署包括管理平面中的一个 Collector，以及每个已接入的控制平面旁边都有一个 Collector。使用 Collector 使 Tetrate Service Bridge 能够通过只需 Operator 抓取一个组件而不是所有组件，从而简化每个集群的遥测数据收集。\n管理平面 在管理平面中有一个名为 collector 的组件。它是一个聚合器，通过 Prometheus 公开了一个用于抓取所有管理平面组件的端点。\n要查看此端点的输出，可以使用以下方式查询：\nkubectl port-forward -n \u0026lt;managementplane-namespace\u0026gt; svc/otel-collector 9090:9090 \u0026amp; curl localhost:9090/metrics 示例输出：\n... # 来自管理平面中 API 服务器的指标。 persistence_transaction_duration_count{component=\u0026#34;tsb\u0026#34;,plane=\u0026#34;management\u0026#34;} 4605 控制平面 在每个控制平面中，还有一个 collector，它公开了其控制平面中组件的指标端点。你可以以与管理平面 Collector 相同的方式使用 Prometheus 抓取此 Collector。\nOpen Telemetry Collector 尽管 Open Telemetry 收集器可以将指标转发到其他收集器，但 TSB 不依赖于生产安装中转发的指标。相反，我们建议在每个可用的 Collector 上本地抓取指标。 要查看此端点的输出，请使用以下命令：\nkubectl port-forward -n \u0026lt;controlplane-namespace\u0026gt; svc/otel-collector 9090:9090 \u0026amp; curl localhost:9090/metrics ","relpermalink":"/book/tsb/operations/telemetry/telemetry-architecture/","summary":"从 Service Bridge 收集遥测数据。","title":"遥测架构"},{"content":"本指南描述了为应用载入准备 Tetrate 管理的平台的基本建议。适用于 Tetrate Service Express (TSE) 和 Tetrate Service Bridge (TSB)。\n为简单起见，本文档仅考虑在 Kubernetes 集群上部署的服务。\n用户类型 本文档假设存在以下两种用户类型：\n平台所有者：用户可以直接访问 TSE 或 TSB，并希望预先配置平台以接收和托管服务。平台所有者需要定义适当的默认值和防护措施，并准备附加服务，如 DNS 或仪表板。\n应用所有者：用户无法访问 TSE 或 TSB，但可以访问 Kubernetes 集群中的一个或多个命名空间。每个应用所有者希望使用标准的 Kubernetes API 和工具（如 CD 流水线）将生产服务部署到集群中。\nTSB 用户和角色层次结构 TSB 提供了一个非常丰富的 用户和角色层次结构 ，允许平台所有者将有限的 TSB 功能委托给其他用户类型，包括多个应用所有者用户和团队。本文档不涵盖这些更复杂的情况。\n相反，本文档适用于由 TSE 和 TSB 支持的更简单的 “一个平台所有者团队，多个应用所有者，高信任” 情况。它假定你将使用 Kubernetes RBAC 或类似的方法来控制应用所有者如何访问 Kubernetes 命名空间。本文档使用 Tetrate 的“GitOps”集成来授予应用所有者访问某些 Tetrate 特定功能的权限，例如部署入口网关。GitOps 在 TSE 中默认启用 ，并且可以在 TSB 中启用和配置 。\n情景 本文档将涵盖以下情景：\n准备集群 平台所有者将为每个应用所有者团队创建命名空间，并创建相应的 Tetrate 工作区。他们将配置一个仅允许工作区内通信的零信任基础环境。在需要接收外部流量的命名空间中部署入口网关。\n部署服务并配置网关规则 应用所有者将在其命名空间内部署服务，并在必要时配置入口网关规则以允许外部流量。\n监控 Tetrate 指标 平台所有者将配置与第三方度量系统的集成，以便应用所有者可以从其应用程序中观察 Tetrate 指标。或者，平台所有者可以授予应用所有者访问 Tetrate 管理平面的权限，以便他们可以直接访问指标。\n扩展安全规则 平台所有者可以\u0026#34;打开\u0026#34;零信任环境，以允许有限的安全例外。这允许应用所有者拥有的服务使用其他命名空间和位置中的服务。\n管理集群之间的流量 平台所有者可以预先配置暴露和故障转移措施，以公开远程集群中的源服务，并安排在冗余服务实例的本地到远程的故障转移。\n高级主题 共享网关： 了解如何在工作区之间共享入口网关 ，以减少在大规模部署中网关的数量。 Route 53 集成： 了解如何微调 Tetrate 的 Route 53 集成 。 ","relpermalink":"/book/tsb/design-guides/app-onboarding/","summary":"本指南描述了为应用载入准备 Tetrate 管理的平台的基本建议。适用于 Tetrate Service Express (TSE) 和 Tetrate Service Bridge (TSB)。","title":"应用载入最佳实践"},{"content":"本文档描述了如何在 AWS 单一 VPC 中安装 TSB。\n在开始之前，请确保你已经：\n熟悉 TSB 概念 安装 tctl 并 同步你的 tctl 镜像 安装 EKS CLI 安装 AWS CLI 使用单一 VPC 安装 TSB 在这种情况下，你将需要在你的 AWS 帐户中运行 3 个 EKS 集群，以及运行 Elasticsearch 和 Postgres。\n请按照相应的 AWS 指南进行更详细的设置：\n创建 EKS 集群 开始使用 Amazon OpenSearch 服务 （CLI 参考 ） 创建 PostgreSQL DB 实例 （CLI 参考 ） 首先，使用以下命令模板创建管理平面集群。 由于命令中没有明确定义 VPC，将为你创建一个新的 VPC。\n$ eksctl create cluster \\ --name \u0026lt;NAME\u0026gt; \\ --version \u0026lt;VERSION\u0026gt; \\ --region \u0026lt;REGION\u0026gt; \\ --nodegroup-name \u0026lt;POOLNAME\u0026gt; \\ --nodes \u0026lt;NUM\u0026gt; \\ --node-type \u0026lt;TYPE\u0026gt; \\ --managed 一旦管理平面集群、节点和 VPC 准备就绪，请记录下子 VPC 名称，并继续进行 Tier 1 和控制平面集群的设置。\n对于 Tier 1 和控制平面集群，你需要在前面的命令模板之上指定 VPC 网络信息。使用以下命令模板创建两个集群，一个用于 Tier 1，另一个用于控制平面。\n$ eksctl create cluster \\ --name \u0026lt;NAME\u0026gt; \\ --version \u0026lt;VERSION\u0026gt;\\ --region \u0026lt;REGION\u0026gt; \\ --nodegroup-name \u0026lt;POOLNAME\u0026gt; \\ --nodes \u0026lt;NUM\u0026gt; \\ --node-type \u0026lt;TYPE\u0026gt; \\ --managed \\ --vpc-private-subnets \u0026lt;VPCNAMES\u0026gt; \\ --vpc-public-subnets \u0026lt;VPCNAMES\u0026gt; 一旦 EKS 集群准备就绪，请确保根据提供的链接设置 OpenSearch 和 PostgreSQL。\n部署管理平面 指向为管理平面安装创建的集群，并按照 管理平面安装 中的说明操作。\n但是，请确保在创建管理平面密钥时指定 Elasticsearch 和 PostgreSQL 的额外信息：\n$ tctl install manifest management-plane-secrets \\ --elastic-username \u0026lt;USER\u0026gt; \\ --elastic-password \u0026lt;PASS\u0026gt; \\ --postgres-username \u0026lt;USER\u0026gt; \\ --postgres-password \u0026lt;PASS\u0026gt; \\ ... other options ... 此外，ManagementPlane 自定义资源应该指向正确的 PostgreSQL 和 OpenSearch 端点：\n# \u0026lt;snip\u0026gt; dataStore: postgres: address: \u0026lt;postgres-endpoint\u0026gt; name: \u0026lt;database-name\u0026gt; telemetryStore: elastic: host: \u0026lt;elastic-endpoint\u0026gt; port: \u0026lt;elastic-port\u0026gt; version: \u0026lt;elastic-version\u0026gt; # \u0026lt;snip\u0026gt; 安装管理平面后，你应该能够使用以下命令获取外部主机名（确保你的 Kubernetes 上下文指向适当的集群）：\n$ kubectl get svc -n tsb 从上述命令的输出中，你应该能够找到一个主机名，类似于 ab940458d752c4e0c80830e9eb89a99d-1487971349.\u0026lt;Region\u0026gt;.elb.amazonaws.com。这是在配置 Tier 1 和控制平面配置 YAML 文件时要使用的端点。\n部署 Tier 1 和控制平面（Tier2）集群 对于 Tier 1 和 CP 集群，请按照以下说明进行操作：\n查看以下链接以获取有关 Tier1 网关 和 控制平面 的更多信息。\n部署控制平面 Operator 安装控制平面密钥 应用 ControlPlane CR 来 安装 TSB 控制平面组件 设置这些集群后，在 Tier 1 和 Tier 2 中的 Edge XCP 中添加以下注释以启用 多集群路由 并应用这些设置。\n# \u0026lt;snip\u0026gt; components: xcp: kubeSpec: overlays: - apiVersion: install.xcp.tetrate.io/v1alpha1 kind: EdgeXcp name: edge-xcp patches: - path: spec.components.edgeServer.kubeSpec.overlays value: - apiVersion: v1 kind: Service name: xcp-edge patches: - path: spec.type value: NodePort - path: metadata.annotations value: traffic.istio.io/nodeSelector: \u0026#39;{\u0026#34;beta.kubernetes.io/arch\u0026#34;:\u0026#34;amd64\u0026#34;}\u0026#39; 集群设置完成后，你可以按照 部署 bookinfo 应用程序的说明 继续进行演示工作负载。\n使用多个 VPC 安装 TSB 对于此安装，你应该已经在\n单个 VPC 中运行了 TSB。\n此情景中的基础架构与使用单个 VPC 的情况类似，但托管控制平面（Tier2）的集群位于与管理平面和 Tier 1 网关的集群不同的 VPC 中。这些 VPC 需要配置以能够相互通信。请阅读 AWS 中有关 VPC 对等连接 的指南以及 CLI 参考中的相关部分，以获取更多详细信息。\n首先创建一个集群和控制平面的新 VPC。你可以使用与为单个 VPC 案例创建第一个 EKS 集群时相同的命令模板。\n$ eksctl create cluster \\ --name \u0026lt;NAME\u0026gt; \\ --version \u0026lt;VERSION\u0026gt; \\ --region \u0026lt;REGION\u0026gt; \\ --nodegroup-name \u0026lt;POOLNAME\u0026gt; \\ --nodes \u0026lt;NUM \\ --node-type \u0026lt;TYPE\u0026gt; \\ --managed 配置 VPC 你需要检索 VPC 信息以继续配置。使用以下命令获取必要信息：\n$ aws ec2 --output text \\ --query \u0026#39;Vpcs[*].{VpcId:VpcId,Name:Tags[?Key==`Name`].Value|[0],CidrBlock:CidrBlock}\u0026#39; describe-vpcs 找到将要参与的每个 VPC 的 ID，并使用 aws ec2 create-vpc-peering-connection 命令创建 VPC 对等连接，以允许这些 VPC 互相通信：\n$ aws ec2 create-vpc-peering-connection \\ --vpc-id \u0026lt;VPC-ID1\u0026gt; \\ --peer-vpc-id \u0026lt;VPC-ID2\u0026gt; 请注意从上述命令的输出中获取 VpcPeeringConnectionId 字段的值。你将需要此值来接受对等连接请求。\n使用此 ID，使用 aws ec2 accept-vpc-peering-connection 命令接受对等连接：\n$ aws ec2 accept-vpc-peering-connection --vpc-peering-connection-id \u0026lt;PEERID\u0026gt; 当上述命令成功执行时，这些 VPC 应该能够相互通信。\n配置控制平面集群 为了连接到控制平面集群，你需要更新你的 kubeconfig。使用适当的值运行以下命令：\n$ aws eks --region \u0026lt;REGION\u0026gt; update-kubeconfig --name \u0026lt;NAME\u0026gt; 启动控制平面。一般的设置与 Onboarding Clusters 指南 中的相同。\n你的控制平面的集群定义应如下所示。注意 spec 组件中的额外字段。\napiVersion: api.tsb.tetrate.io/v2 kind: Cluster metadata: name: cp-remote organization: tetrate spec: displayName: \u0026#34;Control Plane Remote\u0026#34; network: tier2 当你准备好 安装控制平面自定义资源 时，从指南中修改定义，并使用以下 YAML 作为指南设置适当的值：\napiVersion: install.tetrate.io/v1alpha1 kind: ControlPlane metadata: name: controlplane namespace: istio-system spec: hub: \u0026lt;repository\u0026gt; dataStore: postgres: address: \u0026lt;postgres-endpoint\u0026gt; name: \u0026lt;database-name\u0026gt; telemetryStore: elastic: host: \u0026lt;elastic-endpoint\u0026gt; port: \u0026lt;elastic-port\u0026gt; version: \u0026lt;elastic-version\u0026gt; managementPlane: host: \u0026lt;management-plane-endpoint\u0026gt; port: 8443 clusterName: \u0026lt;management-plane-cluster\u0026gt; components: internalCertProvider: certManager: managed: INTERNAL xcp: kubeSpec: overlays: - apiVersion: install.xcp.tetrate.io/v1alpha1 kind: EdgeXcp name: edge-xcp patches: - path: spec.components.edgeServer.kubeSpec.overlays value: - apiVersion: v1 kind: Service name: xcp-edge patches: - path: spec.type value: NodePort - path: metadata.annotations value: traffic.istio.io/nodeSelector: \u0026#39;{\u0026#34;beta.kubernetes.io/arch\u0026#34;:\u0026#34;amd64\u0026#34;}\u0026#39; 如果一切配置正确，你应该能够在新集群上部署工作负载。\n","relpermalink":"/book/tsb/setup/aws/vpc/","summary":"本文档描述了如何在 AWS 单一 VPC 中安装 TSB。 在开始之前，请确保你已经： 熟悉 TSB 概念 安装 tctl 并 同步你的 tctl 镜像 安装 EKS CLI 安装 AWS CLI 使用单一 VPC 安装 TSB 在这种情况下，你将需要在你的 AWS 帐户中运行 3 个 EKS 集群，以及运行 Elasticsearch 和 Pos","title":"在 AWS 安装 TSB"},{"content":"本文将教你如何设置在虚拟机和 Kubernetes 集群上运行的服务之间的流量路由。\n在本指南中，你将：\n在集群中安装 Istio 演示bookinfo应用程序 在虚拟机上安装bookinfo应用程序的ratings服务 将流量在虚拟机和集群中的ratings应用程序之间进行 80/20 的分流 在开始之前，请确保你已经：\n安装了 TSB 管理平面 对一个集群进行了载入 安装了数据面 Operator 首先，从在你的集群中安装 bookinfo 开始。\nkubectl create ns bookinfo kubectl apply -f \\ https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/platform/kube/bookinfo.yaml \\ -n bookinfo 遵循VM 载入文档 。在载入过程中，将 Istio 演示的ratings应用程序作为你的工作负载运行。\nsudo docker run -d \\ --name ratings \\ -p 127.0.0.1:9080:9080 \\ docker.io/istio/examples-bookinfo-ratings-v1.1:1.16.2 为ratings创建一个工作负载入口，\napiVersion: networking.istio.io/v1beta1 kind: WorkloadEntry metadata: name: ratings-vm namespace: bookinfo annotations: sidecar-bootstrap.istio.io/ssh-host: \u0026lt;ssh-host\u0026gt; sidecar-bootstrap.istio.io/ssh-user: istio-proxy sidecar-bootstrap.istio.io/proxy-config-dir: /etc/istio-proxy sidecar-bootstrap.istio.io/proxy-image-hub: docker.io/tetrate sidecar-bootstrap.istio.io/proxy-instance-ip: \u0026lt;proxy-instance-ip\u0026gt; spec: address: \u0026lt;address\u0026gt; labels: class: vm app: ratings # 用于通过 TSB 进行可观测性的必需标签 version: v3 # 用于通过 TSB 进行可观测性的必需标签 serviceAccount: bookinfo-ratings network: \u0026lt;vm-network-name\u0026gt; 并应用一个 Sidecar。\napiVersion: networking.istio.io/v1beta1 kind: Sidecar metadata: name: bookinfo-ratings-no-iptables namespace: bookinfo spec: egress: - bind: 127.0.0.2 hosts: - ./* ingress: - defaultEndpoint: 127.0.0.1:9080 port: name: http number: 9080 protocol: HTTP workloadSelector: labels: app: ratings class: vm 一旦你载入了虚拟机，你的 Mesh 将在集群中的ratings应用程序和虚拟机之间分发流量，因为ratings服务选择任何带有app: ratings标签的工作负载，而我们的集群Deployment和WorkloadEntry都有这个标签。你可以通过日志或 UI 拓扑仪表板来验证流量正流经这两个应用程序。\n现在，让我们微调流量，使 80% 的流量流向集群中的应用程序，而 20% 流向虚拟机。使用包含以下配置的文件运行tctl apply -f（根据你的安装填写\u0026lt;tenant\u0026gt;和\u0026lt;cluster\u0026gt;）。\n注意 你可能已经设置了一个工作空间（例如用于入口流量）。如果是这样，你可以省略此工作空间并相应地调整其余配置。 apiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: name: bookinfo-ws tenant: \u0026lt;tenant\u0026gt; spec: namespaceSelector: names: - \u0026lt;cluster\u0026gt;/bookinfo --- apiVersion: traffic.tsb.tetrate.io/v2 kind: Group metadata: name: bookinfo-tg workspace: bookinfo-ws tenant: \u0026lt;tenant\u0026gt; spec: namespaceSelector: names: - \u0026#34;\u0026lt;cluster\u0026gt;/bookinfo\u0026#34; configMode: BRIDGED --- apiVersion: traffic.tsb.tetrate.io/v2 kind: ServiceRoute metadata: name: ratings group: bookinfo-tg workspace: bookinfo-ws tenant: \u0026lt;tenant\u0026gt; spec: service: bookinfo/ratings subsets: - name: v1 labels: version: v1 weight: 80 - name: v3 labels: version: v3 weight: 20 在发送一些流量通过应用程序后，我们可以再次查看服务仪表板或日志，以查看流量在v1和v3之间以 80/20 的比例分配。\n","relpermalink":"/book/tsb/howto/traffic/splitting-service-traffic-between-k8s-vms/","summary":"本文将教你如何设置在虚拟机和 Kubernetes 集群上运行的服务之间的流量路由。 在本指南中，你将： 在集群中安装 Istio 演示bookinfo应用程序 在虚拟机上安装bookinfo应用程序的ratings服务 将流量在虚拟机和集群","title":"在 Kubernetes 和虚拟机之间切分流量"},{"content":"Tier1 网关 用于使用 Istio mTLS 在其他集群中跨一个或多个入口网关（或 Tier2 网关）分发流量。在 1.6 版本之前，Tier1 网关需要一个专用集群，并且不能与其他网关（例如入口网关）或应用工作负载一起使用。\n从 TSB 1.6 版本开始，你无需为运行 Tier1 网关而提供一个专用的集群。你可以在任何应用程序集群中部署 Tier1 网关。目前此功能默认处于禁用状态；在将来的版本中将默认启用。\n在应用集群中启用 Tier1 网关 为了在应用集群中部署 Tier1 网关，你首先需要编辑 ControlPlane CR 或 Helm 值中的 xcp 组件，并添加一个名为 DISABLE_TIER1_TIER2_SEPARATION 的环境变量，其值为 true。\nspec: components: xcp: ... kubeSpec: overlays: - apiVersion: install.xcp.tetrate.io/v1alpha1 kind: EdgeXcp name: edge-xcp patches: ... - path: spec.components.edgeServer.kubeSpec.deployment.env[-1] value: name: DISABLE_TIER1_TIER2_SEPARATION value: \u0026#34;true\u0026#34; ... 有关如何部署和配置 Tier1 网关的示例，请参阅使用 Tier-1 网关进行多集群流量转移 。\n","relpermalink":"/book/tsb/operations/features/tier1-in-app-cluster/","summary":"如何在应用集群中启用 Tier1 网关。","title":"在应用集群中启用 Tier1 网关"},{"content":" 注意 自 1.7 版本以来，TSB 支持用于 TSB 管理平面 TLS 证书、内部证书和中间 Istio CA 证书的自动证书管理。详细信息请参阅 自动证书管理 。 有 4 种 TSB 运算符需要了解的证书类型：\nTSB 内部证书：用于 TSB 内部组件相互信任的证书。 应用 TLS 证书：提供给应用程序用户的证书，用于 Web 浏览器或工具。 中间 Istio CA 证书：用于签发 Istio 工作负载叶子证书的中间 CA 证书。 工作负载叶子证书：针对每个代理和网关签发的证书。 下面的图片显示了这些证书及其与 TSB 组件和你的应用程序的关系。\nTSB 内部证书 TSB 的全局控制平面 (XCP) 从管理平面分发配置到控制平面集群。XCP 由 XCP central 和 XCP edge 组成。XCP central 部署在管理平面，TSB 服务器通过名为 MPC 的组件与其交互。TSB 内部证书（图片中突出显示为绿色）用于保护 XCP central、XCP edge、MPC 组件之间的通信。TSB 使用带 TLS 的 JWT 来确保通信的安全性。在部署 TSB 之前，你需要准备这些证书。\n应用 TLS 证书 应用 TLS 证书（图片中突出显示为紫色）由客户端应用程序使用，以便信任访问应用程序。\n你的应用程序提供的每个公开可访问的 HTTPS 服务都应具有作为 Kubernetes 机密挂载的 TLS 证书。在发布应用程序时，必须提供应用程序的 TLS 证书。虽然在技术上不是一个 “应用程序”，但你还需要设置命令行工具的 TLS 证书，以便它们可以访问 TSB 管理平面，以及你可以通过 Web 浏览器访问 TSB UI。TSB TLS 证书必须在部署 TSB 之前可用。\n中间 Istio CA 证书 中间 Istio CA 证书（图片中突出显示为青色）在每个控制平面上以 cacerts 机密的形式挂载，以便可以签发 Istio 工作负载叶子证书。默认情况下，istiod 充当叶子证书发行者，使用中间 CA 证书来签署叶子证书。\n证书应由企业 Root CA 签署（或可验证），以用于服务内部通信。集群特定的中间 CA 应在 TSB 控制平面部署期间可用。\n有关在多集群设置中设置中间 Istio CA 的演示示例，请参阅 Istio 文档 。\n对于生产环境，强烈建议使用生产就绪的 PKI 基础结构，例如以下内容，并遵循行业最佳实践：\n使用 AWS Private CA 作为企业 CA 创建中间 CA（不是自动化过程）。 将现有 CA 集成到 Kubernetes CSR API 中（例如 AWS 证书管理器 、HashiCorp Vault ）。 通常，企业安全团队负责这些类型的证书。\n工作负载叶子证书 工作负载叶子证书（图片中突出显示为黄色）会分发给每个代理和网关（或每个工作负载）。这些证书是短期证书（默认情况下为 24 小时，可以通过在 ControlPlane CR 中设置 defaultWorkloadCertTTL 来更改）。\n重要的是要了解，这些证书会自动轮换，不受 TSB 管理。Istiod 负责使用企业中间证书签发和轮换证书。\n","relpermalink":"/book/tsb/setup/certificate/certificate-setup/","summary":"解释了 TSB 中使用的不同类型的证书。","title":"证书类型"},{"content":"Argo CD 是一种开源的持续交付工具，用于自动化和管理应用程序的部署、更新和回滚。它是一个声明式的工具，专为在 Kubernetes 集群中进行应用程序部署而设计。\n🔔 注意：本文档根据 Argo CD v2.8 Commit 4d2cd06f86 （北京时间 2023 年 6 月 30 日 19 时）翻译。\nArgo CD 的主要功能包括：\n持续交付：Argo CD 允许用户将应用程序的配置和清单文件定义为 Git 存储库中的声明式资源，从而实现持续交付。它能够自动检测 Git 存储库中的更改，并将这些更改应用于目标 Kubernetes 集群。\n健康监测和回滚：Argo CD 能够监测应用程序的健康状态，并在检测到问题时触发回滚操作。这有助于确保应用程序在部署期间和运行时保持稳定和可靠。\n多环境管理：Argo CD 支持多个环境（例如开发、测试、生产）的管理。它可以帮助用户在不同环境中进行应用程序的部署和配置管理，并确保这些环境之间的一致性。\n基于 GitOps 的操作：Argo CD 采用了 GitOps 的操作模式，即将应用程序的状态和配置定义为 Git 存储库中的声明式资源。这使得团队可以使用版本控制和代码审查等软件工程实践来管理应用程序的生命周期。\n大纲 简介\n理解基础\n核心概念\n安装\n入门\n操作手册\n用户手册\n开发手册\nFAQ\n开始阅读 ","relpermalink":"/book/argo-cd/","summary":"Argo CD 中文文档（非官方）","title":"Argo CD 中文文档"},{"content":"生产 Argo CD 支持多种不同的 Kubernetes 清单定义方式：\nKustomize 应用程序 Helm Chart YAML/JSON/Jsonnet 清单的目录，包括Jsonnet 。 任何配置为配置管理插件的自定义配置管理工具 开发 Argo CD 还支持直接上传本地清单。由于这是 GitOps 范式的反模式，因此只能出于开发目的而这样做。override需要具有权限的用户（通常是管理员）才能在本地上传清单。支持上述所有不同的 Kubernetes 部署工具。上传本地应用程序：\n$ argocd app sync APPNAME --local /path/to/dir/ ","relpermalink":"/book/argo-cd/user-guide/application-sources/","summary":"生产 Argo CD 支持多种不同的 Kubernetes 清单定义方式： Kustomize 应用程序 Helm Chart YAML/JSON/Jsonnet 清单的目录，包括Jsonnet 。 任何配置为配置管理插件的自定义配置管理工具 开发 Argo CD 还支持直接上传本地清单。由于这是 GitOps 范式的反模式，因此只能出于开发","title":"工具"},{"content":" Argo CD 架构 组件 API 服务器 API 服务器是一个 gRPC/REST 服务器，用于公开 Web UI、CLI 和 CI/CD 系统使用的 API。它具有以下职责：\n应用程序管理和状态报告 调用应用程序操作（例如同步、回滚、用户定义的操作） 存储为 K8s 机密的存储库和集群凭据管理 身份验证和身份验证委派到外部身份提供者 RBAC 执行 Git webhook 事件的侦听器/转发器 存储库服务器 存储库服务器是一个内部服务，它维护 Git 存储库的本地缓存，其中包含应用程序清单。它负责在提供以下输入时生成并返回 Kubernetes 清单：\n存储库 URL 修订版（提交、标记、分支） 应用程序路径 模板特定设置：参数、helm values.yaml 应用程序控制器 应用程序控制器是一个 Kubernetes 控制器，它不断监视运行中的应用程序，并将当前的实时状态与期望的目标状态（如 repo 中指定的）进行比较。它检测 OutOfSync 应用程序状态，并可选择采取纠正措施。它负责调用任何用户定义的生命周期事件钩子（PreSync、Sync、PostSync）\n","relpermalink":"/book/argo-cd/operator-manual/architecture/","summary":"Argo CD 架构 组件 API 服务器 API 服务器是一个 gRPC/REST 服务器，用于公开 Web UI、CLI 和 CI/CD 系统使用的 API。它具有以下职责： 应用程序管理和状态报告 调用应用程序操作（例如同步、回滚、用户定义的操作） 存储为 K8s 机密的存储库和集","title":"架构概述"},{"content":"什么是 Argo CD？ Argo CD 是一个基于声明式 GitOps 的 Kubernetes 应用程序交付工具。\nArgo CD UI 为什么选择 Argo CD？ 应用程序定义、配置和环境应该是声明式的，并进行版本控制。应用程序部署和生命周期管理应该是自动化的、可审计的和易于理解的。\n入门指南 快速入门 kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml 请参阅我们的入门指南 。我们还为其他功能提供了面向用户的文档。如果你想升级 ArgoCD，请参阅升级指南。我们还为有兴趣构建第三方集成的开发人员提供面向开发者的文档。\n工作原理 Argo CD 遵循使用 Git 存储库作为定义期望应用程序状态的真实来源的 GitOps 模式。Kubernetes 清单可以通过以下几种方式指定：\nkustomize 应用程序 helm chart jsonnet 文件 YAML/json 清单的普通目录 配置为配置管理插件的任何自定义配置管理工具 Argo CD 自动部署指定目标环境中所需的应用程序状态。应用程序部署可以跟踪分支、标签的更新或固定到 Git 提交的特定版本的清单。请参见跟踪策略 以了解有关可用跟踪策略的更多详细信息。\n针对 Sig Apps 社区会议展示的快速 10 分钟的 Argo CD 概述，请查看演示：\n架构 ArgoCD 架构 Argo CD 实现为 Kubernetes 控制器，它持续监视正在运行的应用程序，并将当前的实时状态与所需的目标状态（如 Git 存储库中指定的状态）进行比较。实时状态偏离目标状态的已部署应用程序被视为“OutofSync”。Argo CD 报告并可视化差异，同时提供自动或手动同步实时状态到所需目标状态的设施。对 Git 存储库中所需的目标状态所做的任何修改都可以自动应用并反映在指定的目标环境中。\n有关详细信息，请参见架构概述 。\n功能 自动将应用程序部署到指定的目标环境 支持多个配置管理/模板工具（Kustomize、Helm、Jsonnet、plain-YAML） 能够管理和部署到多个集群 SSO 集成（OIDC、OAuth2、LDAP、SAML 2.0、GitHub、GitLab、Microsoft、LinkedIn） 授权的多租户和 RBAC 策略 回滚/在任何提交到 Git 存储库中的应用程序配置中进行回滚 应用程序资源的健康状态分析 自动配置漂移检测和可视化 应用程序同步到其所需状态的自动或手动同步 Web UI 提供应用程序活动的实时视图 用于自动化和 CI 集成的 CLI Webhook 集成（GitHub、BitBucket、GitLab） 访问令牌用于自动化 为覆盖 Git 中的 Helm 参数而提供的参数重写 用于支持复杂应用程序升级（例如蓝/绿和金丝雀升级）的 PreSync、Sync、PostSync 钩子 应用程序事件和 API 调用的审计跟踪 Prometheus 指标 开发状态 Argo CD 正在由社区积极开发。我们的发布可以在这里 找到。\n采用情况 正式采用 Argo CD 的组织可以在这里 找到。\n","relpermalink":"/book/argo-cd/overview/","summary":"什么是 Argo CD？ Argo CD 是一个基于声明式 GitOps 的 Kubernetes 应用程序交付工具。 Argo CD UI 为什么选择 Argo CD？ 应用程序定义、配置和环境应该是声明式的，并进行版本控制。应用程序部署和生命周期管理应该是自动化的、可审计的和易于理解的","title":"Argo CD 简介"},{"content":"介绍 应用程序集控制器是一个Kubernetes 控制器 ，它添加了对ApplicationSet自定义资源定义 (CRD) 的支持。这个控制器/CRD 使得自动化和更大的灵活性在管理 Argo CD 应用程序跨大量的集群和在 monorepos 内成为可能，同时它使多租户 Kubernetes 集群上的自助式使用成为可能。\n应用程序集控制器与现有的 Argo CD 安装一起工作。Argo CD 是一个声明性的、GitOps 持续交付工具，允许开发人员从他们现有的 Git 工作流程中定义和控制 Kubernetes 应用程序资源的部署。\n从 Argo CD v2.3 开始，应用程序集控制器与 Argo CD 捆绑在一起。\n应用程序集控制器通过添加支持面向集群管理员的附加功能来补充 Argo CD。ApplicationSet控制器提供：\n使用单个 Kubernetes 清单定位多个 Kubernetes 集群的能力，使用 Argo CD 部署多个应用程序的能力 使用单个 Kubernetes 清单从一个或多个 Git 存储库中部署多个应用程序的能力 改进了对 monorepos 的支持：在 Argo CD 的上下文中，monorepo 是一个单个 Git 存储库中定义的多个 Argo CD 应用程序资源 在多租户集群中，提高了各个集群租户使用 Argo CD 部署应用程序的能力（无需涉及特权集群管理员以启用目标集群/命名空间） 🔔 注意：在使用 ApplicationSets 之前，请注意其安全影响。\n应用程序集资源 此示例定义了一个名为guestbook的ApplicationSet资源：\napiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: guestbook spec: generators: - list: elements: - cluster: engineering-dev url: https://1.2.3.4 - cluster: engineering-prod url: https://2.4.6.8 - cluster: finance-preprod url: https://9.8.7.6 template: metadata: name: \u0026#39;{{cluster}}-guestbook\u0026#39; spec: project: my-project source: repoURL: \u0026lt;https://github.com/infra-team/cluster-deployments.git\u0026gt; targetRevision: HEAD path: guestbook/{{cluster}} destination: server: \u0026#39;{{url}}\u0026#39; namespace: guestbook 在此示例中，我们想要将我们的guestbook应用程序（由于这是 GitOps，该应用程序的 Kubernetes 资源来自 Git）部署到一系列 Kubernetes 集群（目标集群的列表在ApplicationSet资源的 List 项元素中定义）。\n虽然ApplicationSet资源可以使用多种类型的生成器，但此示例使用了列表生成器，它只包含一个固定的、字面的集群列表。这个集群列表将是 Argo CD 在处理了ApplicationSet资源后部署guestbook应用程序资源的集群。\n生成器（如 List 生成器）负责生成参数。参数是键值对，在模板渲染期间被替换到ApplicationSet资源的template:部分中。\n当前支持的多个生成器：\nList 生成器：基于固定的集群名称/URL 值列表生成参数，如上面的示例所示。 Cluster 生成器：而不是一个字面的集群列表（如列表生成器），集群生成器根据在 Argo CD 中定义的集群自动生成集群参数。 Git 生成器：Git 生成器基于包含在生成器资源中的文件或文件夹生成参数。 - 包含 JSON 值的文件将被解析并转换为模板参数。 - Git 存储库中的单个目录路径也可以用作参数值。 Matrix 生成器：矩阵生成器结合了两个其他生成器的生成参数。 有关各个生成器的更多信息以及未列出的其他生成器，请参见生成器部分。\n将参数替换为模板 不管使用哪个生成器，由生成器生成的参数都会被替换为{{parameter name}}值，而这些值位于ApplicationSet资源的template:部分中。在此示例中，列表生成器定义了cluster和url参数，这些参数随后分别被替换为模板的{{cluster}}和{{url}}值。\n在将参数替换后，将此guestbook ApplicationSet资源应用于 Kubernetes 集群：\n1.应用程序集控制器处理生成器条目，生成一组模板参数。2.这些参数被替换到模板中，每组参数替换一次。3.每个渲染的模板都被转换为一个 Argo CD Application资源，然后创建（或更新）在 Argo CD 名称空间中。4.最后，Argo CD 控制器会收到这些Application资源的通知，并负责处理它们。\n对于我们的示例中定义的三个不同集群——engineering-dev、engineering-prod和finance-preprod，这将产生三个新的 Argo CDApplication资源：每个集群一个。\n这是将创建的Application资源之一的示例，为engineering-dev集群在1.2.3.4处：\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: engineering-dev-guestbook spec: source: repoURL: https://github.com/infra-team/cluster-deployments.git targetRevision: HEAD path: guestbook/engineering-dev destination: server: https://1.2.3.4 namespace: guestbook 我们可以看到生成的值已被替换到模板的server和path字段中，模板已被渲染成一个完整的 Argo CD 应用程序。\n现在这些 Application 也可以在 Argo CD UI 中看到：\nArgoCD Web UI 中的列表生成器示例 应用程序集控制器将确保将对ApplicationSet资源所做的任何更改、更新或删除自动应用于相应的Application。\n例如，如果向列表生成器添加了新的集群/URL 列表条目，则将为此新集群相应地创建一个新的 Argo CDApplication资源。对guestbook ApplicationSet资源所做的任何编辑都将影响由该资源实例化的所有 Argo CD 应用程序，包括新应用程序。\n虽然列表生成器的字面集群列表相当简单，但 ApplicationSet 控制器支持其他可用生成器的更复杂的场景。\n","relpermalink":"/book/argo-cd/operator-manual/applicationset/overview/","summary":"介绍 应用程序集控制器是一个Kubernetes 控制器 ，它添加了对ApplicationSet自定义资源定义 (CRD) 的支持。这个控制器/CRD 使得自动化和更大的灵活性在管理 Argo CD 应用程序跨大量的集群和在 monorepos 内成为","title":"ApplicationSet 控制器介绍"},{"content":"Argo Rollouts 是一个 Kubernetes 控制器，它提供了在应用程序部署过程中执行渐进式发布和蓝绿部署等高级部署策略的能力。它是基于 Kubernetes 原生的 Deployment 资源构建的，通过引入新的 Rollout 资源来扩展和增强部署控制。\n🔔 注意：本文档根据 Argo Rollouts v1.5 Commit 1d53b25 （北京时间 2023 年 6 月 21 日 3 时）翻译。\nArgo Rollouts 具有以下主要功能：\n渐进式发布（Progressive Delivery）：Argo Rollouts 允许你逐步增加新版本的流量并监控其性能，以确保新版本稳定可靠。你可以通过配置渐进式发布的步骤和条件来控制流量的切换和回滚。\n蓝绿部署（Blue-Green Deployment）：Argo Rollouts 支持蓝绿部署模式，其中在新旧版本之间进行无缝切换。通过在新版本上运行一些或全部流量，并根据用户的反馈和性能指标来验证新版本的稳定性，可以确保零停机时间的部署。\n金丝雀部署（Canary Deployment）：Argo Rollouts 支持金丝雀部署模式，可以将新版本的流量逐渐引入生产环境，并基于预定义的指标和策略来评估新版本的性能。这使你能够在生产中小范围测试新版本，并及时发现和修复潜在问题。\n自动回滚（Automated Rollback）：如果新版本在部署过程中出现问题或未达到预期的性能指标，Argo Rollouts 具备自动回滚功能，可以快速将流量切换回稳定版本，以减少对用户的影响。\nArgo Rollouts 使用自定义资源 Rollout 来描述部署的状态和策略，并通过控制器与 Kubernetes API 交互，实现部署的管理和控制。它提供了灵活且可扩展的部署流程定义，使你能够以可控的方式执行复杂的应用程序部署和更新操作。\n大纲 简介\n安装\n概念\n架构\n入门\nDashboard\nRollout\n流量管理\n分析\n实验\n通知\nKubectl plugin\n迁移到 Rollouts\n最佳实践\nFAQ\n安全\n开始阅读 ","relpermalink":"/book/argo-rollouts/","summary":"Argo Rollouts 中文文档（非官方）","title":"Argo Rollouts 中文文档"},{"content":" 蓝绿部署\n金丝雀部署\n","relpermalink":"/book/argo-rollouts/rollout/deployment-strategies/","summary":"蓝绿部署 金丝雀部署","title":"部署策略"},{"content":"Argo Rollouts 提供了多种形式的分析方法来驱动渐进式交付。本文档描述了如何实现不同形式的渐进式交付，包括分析执行的时间点、频率和发生次数。\n自定义资源定义 CRD 描述 Rollout Rollout 作为 Deployment 资源的替代品，提供了额外的蓝绿和金丝雀更新策略。这些策略可以在更新过程中创建 AnalysisRuns 和 Experiments，这些 AnalysisRuns 和 Experiments 可以推进更新，或者中止更新。 AnalysisTemplate AnalysisTemplate 是一个模板规范，定义了如何执行金丝雀分析，例如应该执行的指标、其频率以及被视为成功或失败的值。AnalysisTemplates 可以使用输入值进行参数化。 ClusterAnalysisTemplate ClusterAnalysisTemplate 类似于 AnalysisTemplate，但它不限于其命名空间。它可以被任何 Rollout 在整个集群中使用。 AnalysisRun AnalysisRun 是 AnalysisTemplate 的一个实例化。AnalysisRuns 类似于 Job，它们最终会完成。完成的运行被认为是成功、失败或不确定的，该运行的结果影响 Rollout 的更新是否继续、中止或暂停。 Experiment Experiment 是用于分析目的的一个或多个 ReplicaSets 的有限运行。实验通常运行一段预定的时间，但也可以一直运行直到停止。实验可以引用一个 AnalysisTemplate，在实验期间或之后运行。实验的典型用例是并行启动基线和金丝雀部署，并比较基线和金丝雀 pod 产生的指标以进行相等的比较。 背景分析 可以在金丝雀通过其滚动更新步骤时运行分析。\n下面的示例每 10 分钟逐渐增加金丝雀权重 20%，直到达到 100%。在后台，基于名为 success-rate 的 AnalysisTemplate 启动了 AnalysisRun。success-rate 模板查询 Prometheus 服务器，在 5 分钟的间隔/采样内测量 HTTP 成功率。它没有结束时间，并且会一直持续直到停止或失败。如果度量小于 95%，并且有三个这样的度量，那么分析将被视为失败。失败的分析会导致 Rollout 中止，将金丝雀权重设置回零，Rollout 将被视为 Degraded。否则，如果滚动更新完成了所有的金丝雀步骤，则滚动更新将被视为成功，控制器会停止分析运行。\n这个例子强调了：\n背景分析风格的渐进式交付 使用 Prometheus 查询执行测量 能够将分析参数化 推迟分析运行的启动时间，直到第三步（设置重量为 40%） Rollout\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: guestbook spec: ... strategy: canary: analysis: templates: - templateName: success-rate startingStep: 2 # delay starting analysis run until setWeight: 40% args: - name: service-name value: guestbook-svc.default.svc.cluster.local steps: - setWeight: 20 - pause: {duration: 10m} - setWeight: 40 - pause: {duration: 10m} - setWeight: 60 - pause: {duration: 10m} - setWeight: 80 - pause: {duration: 10m} AnalysisTemplate\napiVersion: argoproj.io/v1alpha1 kind: AnalysisTemplate metadata: name: success-rate spec: args: - name: service-name metrics: - name: success-rate interval: 5m # 注意：Prometheus 查询以向量形式返回结果。因此，通常访问返回的数组的索引 0 以获取值 successCondition: result[0] \u0026gt;= 0.95 failureLimit: 3 provider: prometheus: address: \u0026lt;http://prometheus.example.com:9090\u0026gt; query: | sum(irate( istio_requests_total{reporter=\u0026#34;source\u0026#34;,destination_service=~\u0026#34;{{args.service-name}}\u0026#34;,response_code!~\u0026#34;5.*\u0026#34;}[5m] )) / sum(irate( istio_requests_total{reporter=\u0026#34;source\u0026#34;,destination_service=~\u0026#34;{{args.service-name}}\u0026#34;}[5m] )) 内联分析 分析也可以作为一个内联的“分析”步骤作为部署步骤来执行。当分析被执行时，“内联”，会在到达该步骤时启动一个 AnalysisRun，并阻塞部署，直到运行完成。分析运行的成功或失败决定了部署是否继续到下一步，或者完全中止。\n该示例将金丝雀权重设置为 20%，暂停 5 分钟，然后运行分析。如果分析成功，则继续进行部署，否则中止。\n这个例子演示了：\n作为步骤的一部分调用分析的能力\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: guestbook spec: ... strategy: canary: steps: - setWeight: 20 - pause: {duration: 5m} - analysis: templates: - templateName: success-rate args: - name: service-name value: guestbook-svc.default.svc.cluster.local 在这个例子中，AnalysisTemplate 与背景分析示例相同，但由于没有指定时间间隔，因此分析将执行一次测量并完成。\napiVersion: argoproj.io/v1alpha1 kind: AnalysisTemplate metadata: name: success-rate spec: args: - name: service-name - name: prometheus-port value: 9090 metrics: - name: success-rate successCondition: result[0] \u0026gt;= 0.95 provider: prometheus: address: \u0026#34;http://prometheus.example.com:{{args.prometheus-port}}\u0026#34; query: | sum(irate( istio_requests_total{reporter=\u0026#34;source\u0026#34;,destination_service=~\u0026#34;{{args.service-name}}\u0026#34;,response_code!~\u0026#34;5.*\u0026#34;}[5m] )) / sum(irate( istio_requests_total{reporter=\u0026#34;source\u0026#34;,destination_service=~\u0026#34;{{args.service-name}}\u0026#34;}[5m] )) 可以通过指定 count 和 interval 字段来执行多个度量，以在较长的持续时间内执行多个度量：\nmetrics: - name: success-rate successCondition: result[0] \u0026gt;= 0.95 interval: 60s count: 5 provider: prometheus: address: http://prometheus.example.com:9090 query: ... ClusterAnalysisTemplates 🔔 重要提示：从 v0.9.0 开始可用\nRollout 可以引用一个名为 ClusterAnalysisTemplate 的集群作用域 AnalysisTemplate。当你希望在多个 Rollout 中共享 AnalysisTemplate 时，这可能非常有用。在不同的命名空间中，避免在每个命名空间中重复相同的模板。使用 clusterScope: true 字段引用 ClusterAnalysisTemplate 而不是 AnalysisTemplate。\nRollout\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: guestbook spec: ... strategy: canary: steps: - setWeight: 20 - pause: {duration: 5m} - analysis: templates: - templateName: success-rate clusterScope: true args: - name: service-name value: guestbook-svc.default.svc.cluster.local ClusterAnalysisTemplate\napiVersion: argoproj.io/v1alpha1 kind: ClusterAnalysisTemplate metadata: name: success-rate spec: args: - name: service-name - name: prometheus-port value: 9090 metrics: - name: success-rate successCondition: result[0] \u0026gt;= 0.95 provider: prometheus: address: \u0026#34;http://prometheus.example.com:{{args.prometheus-port}}\u0026#34; query: | sum(irate( istio_requests_total{reporter=\u0026#34;source\u0026#34;,destination_service=~\u0026#34;{{args.service-name}}\u0026#34;,response_code!~\u0026#34;5.*\u0026#34;}[5m] )) / sum(irate( istio_requests_total{reporter=\u0026#34;source\u0026#34;,destination_service=~\u0026#34;{{args.service-name}}\u0026#34;}[5m] )) 🔔 注意：结果的 AnalysisRun 仍将在 Rollout 的命名空间中运行\n使用多个模板的分析 Rollout 可以在构建 AnalysisRun 时引用多个 AnalysisTemplates。这允许用户从多个 AnalysisTemplate 中组合分析。如果引用了多个模板，则控制器将合并这些模板。控制器组合所有模板的 metrics 和 args 字段。\nRollout\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: guestbook spec: ... strategy: canary: analysis: templates: - templateName: success-rate - …","relpermalink":"/book/argo-rollouts/analysis/overview/","summary":"Argo Rollouts 提供了多种形式的分析方法来驱动渐进式交付。本文档描述了如何实现不同形式的渐进式交付，包括分析执行的时间点、频率和发生次数。 自定义资源定义 CRD 描述 Rollout Rollout 作为 Deployment 资源的替代品，提供了额外的蓝绿和金丝雀更新策","title":"分析和渐进式交付"},{"content":"Kubectl 插件是一种扩展 kubectl 命令提供额外行为的方式。通常，它们用于添加新功能到 kubectl 并自动化可脚本化的工作流程来操作集群。官方文档可在 此处 找到。\nArgo Rollouts 提供了一个 Kubectl 插件来丰富 Rollouts、Experiments 和 Analysis 的体验。它提供了可视化 Argo Rollouts 资源的能力并从命令行上运行常规操作，例如 promote 或 retry。\n安装 请参阅 安装指南 了解安装插件的说明。\n用法 获取有关可用的 Argo Rollouts kubectl 插件命令的信息的最佳方法是运行 kubectl argo rollouts。插件列出了该工具可以执行的所有可用命令以及每个命令的描述。所有插件命令与 Kubernetes API 服务器交互，并使用 KubeConfig 凭据进行身份验证。由于插件利用运行命令的用户的 KubeConfig，因此插件具有这些配置的权限。\n与 kubectl 类似，该插件使用许多与 kubectl 相同的标志。例如，kubectl argo rollouts get rollout canary-demo -w 命令会在canary-demo rollout 对象上启动一个 watch，类似于kubectl get deployment canary-demo -w 命令在部署上启动一个 watch。\n可视化 Rollouts 和 Experiments 除了封装许多常规命令之外，Argo Rollouts kubectl 插件还支持使用 get 命令可视化 rollouts 和 experiments。get 命令提供了一个干净的表示形式，用于表示在集群中运行的 rollouts 或 experiments。它返回关于资源的大量元数据，以及父资源创建的子资源的树状视图。以下是使用 get 命令检索到的一个 rollout 的示例：\nkubectl argo rollouts 命令行示例 下面是一个表格，解释了树视图上的一些图标：\n图标 Kind ⟳ Rollout Σ Experiment α AnalysisRun # Revision ⧉ ReplicaSet □ Pod ⊞ Job 如果 get 命令包括 watch 标志（-w或--watch），则终端会随着 rollouts 或 experiments 的进展而更新，突出显示进度。\n","relpermalink":"/book/argo-rollouts/kubectl-plugin/overview/","summary":"Kubectl 插件是一种扩展 kubectl 命令提供额外行为的方式。通常，它们用于添加新功能到 kubectl 并自动化可脚本化的工作流程来操作集群。官方文档可在 此处 找到。 Argo Rollouts 提供了一个 Kubectl 插件来丰富 Rollouts、Experiments 和 Analysis","title":"Kubectl 插件"},{"content":"🔔 重要提示：自版本 1.1 起可用。\nArgo Rollouts 提供通知功能，由Notifications Engine 支持。控制器管理员可以利用灵活的触发器和模板系统来配置终端用户请求的通知。终端用户可以通过在 Rollout 对象中添加注释来订阅配置的触发器。\n配置 触发器定义了通知应该在何时发送以及通知内容模板。默认情况下，Argo Rollouts 附带了一系列内置触发器，涵盖了 Argo Rollout 生命周期的最重要事件。触发器和模板都在argo-rollouts-notification-configmap ConfigMap 中配置。为了快速入门，你可以使用在notifications-install.yaml 中定义的预配置通知模板。\n如果你正在利用 Kustomize，则建议将notifications-install.yaml 作为远程资源包含在你的kustomization.yaml文件中：\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml - https://github.com/argoproj/argo-rollouts/releases/latest/download/notifications-install.yaml 在包含argo-rollouts-notification-configmap ConfigMap 之后，管理员需要配置与所需通知服务（例如 Slack 或 MS Teams）的集成。下面的示例演示了 Slack 集成：\napiVersion: v1 kind: ConfigMap metadata: name: argo-rollouts-notification-configmap data: # 模板的详细信息被省略 # 触发器的详细信息被省略 service.slack: | token: $slack-token --- apiVersion: v1 kind: Secret metadata: name: argo-rollouts-notification-secret stringData: slack-token: \u0026lt;my-slack-token\u0026gt; 有关支持的服务和配置设置的更多信息，请参见服务文档 。\n默认触发器模板 目前，以下触发器具有内置模板 。\non-rollout-completed当一个 rolling rollout 结束并且所有步骤都已完成时 on-rollout-step-completed当滚动部署定义中的单个步骤已完成时 on-rollout-updated当 Rollout 定义更改时 on-scaling-replica-set当 Rollout 中的副本数更改时 订阅 终端用户可以使用notifications.argoproj.io/subscribe.\u0026lt;trigger\u0026gt;.\u0026lt;service\u0026gt;: \u0026lt;recipient\u0026gt;注释开始使用通知。例如，以下注释订阅了两个 Slack 频道，以通知有关金丝雀滚动步骤完成的信息：\n--- apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollout-canary annotations: notifications.argoproj.io/subscribe.on-rollout-step-completed.slack: my-channel1;my-channel2 注释键由以下部分组成：\non-rollout-step-completed - 触发器名称 slack - 通知服务名称 my-channel1;my-channel2 - 一个由分号分隔的收件人列表 定制 Rollout 管理员可以通过配置通知模板和自定义触发器来自定义通知argo-rollouts-notification-configmap ConfigMap。\n模板 通知模板是生成通知内容的无状态函数。该模板利用html/template golang 包。它旨在可重用，并且可以被多个触发器引用。\n以下示例演示了样本模板：\napiVersion: v1 kind: ConfigMap metadata: name: argo-rollouts-notification-configmap data: template.my-purple-template: | message: | Rollout {{.rollout.metadata.name}} has purple image slack: attachments: | [{ \u0026#34;title\u0026#34;: \u0026#34;{{ .rollout.metadata.name}}\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#800080\u0026#34; }] 每个模板都可以访问以下字段：\nrollout保存 rolling rollout 对象。 recipient保存收件人名称。 模板定义的message字段允许为任何通知服务创建基本通知。你可以利用特定于通知服务的字段来创建复杂的通知。例如，使用特定于服务的你可以为 Slack 添加块和附件，为电子邮件添加主题或 URL 路径，为 Webhook 添加正文。有关更多信息，请参见相应的服务文档。\n自定义触发器 除了自定义通知模板外，管理员还可以配置自定义触发器。自定义触发器定义了发送通知的条件。定义包括名称、条件和通知模板引用。条件是返回 true 如果应发送通知的谓词表达式。触发器条件评估由antonmedv/expr 支持。条件语言语法在Language-Definition.md 中描述。\n触发器在argo-rollouts-notification-configmap ConfigMap 中配置。例如，以下触发器在 rolling rollout pod 规范使用argoproj/rollouts-demo:purple镜像时发送通知：\napiVersion: v1 kind: ConfigMap metadata: name: argo-rollouts-notification-configmap data: trigger.on-purple: | - send: [my-purple-template] when: rollout.spec.template.spec.containers[0].image == \u0026#39;argoproj/rollouts-demo:purple\u0026#39; 每个条件可能使用多个模板。通常，每个模板负责生成特定于服务的通知部分。\n通知度量 在 argo-rollouts 中启用通知时，将发出以下 prometheus 度量标准。\nnotification_send_success 是计算成功发送通知的计数器。 notification_send_error 是计算发送通知失败的计数器。 notification_send 是测量发送通知性能的直方图。 ","relpermalink":"/book/argo-rollouts/notifications/overview/","summary":"🔔 重要提示：自版本 1.1 起可用。 Argo Rollouts 提供通知功能，由Notifications Engine 支持。控制器管理员可以利用灵活的触发器和模板系统来配置终端用户请求的通知。终端用户可以通过在 Rollout 对象中添加注释来订阅配置的触发","title":"通知"},{"content":"Argo Rollouts 是一个 Kubernetes 自定义资源定义 (Custom Resource Definition, CRD)，扩展了 Kubernetes Deployment Controller，它添加了渐进式交付（Progressive Delivery）和蓝绿部署（Blue Green Deployment）等交付策略。Argo Rollouts 可以与 Istio、Linkerd 等服务网格和其他流量管理工具集成。\n流量管理 流量管理是通过控制数据平面，为应用程序创建智能的路由规则。这些路由规则可以操作流量，将其引导到应用程序的不同版本，从而实现渐进式交付。这些控制规则通过确保只有一小部分用户接收新版本，从而限制了新版本的波及范围。\n实现流量管理有各种技术：\n原始百分比（例如，5% 的流量应该流向新版本，而其余的流向稳定版本） 基于头的路由（例如，将带有特定标头的请求发送到新版本） 交叉流量，其中所有流量都被复制并并行发送到新版本（但响应被忽略） Kubernetes 中的流量管理工具 核心 Kubernetes 对象没有细粒度的工具来满足所有流量管理要求。在最多的情况下，Kubernetes 通过服务（Service）对象提供原生负载平衡功能，通过提供将流量路由到根据该服务选择器组合的 Pod 的端点。使用默认的核心服务对象无法实现流量镜像或通过标头路由，并且控制应用程序的不同版本的流量百分比的唯一方法是通过操作这些版本的副本计数。\n服务网格填补了 Kubernetes 中的这些缺失功能。它们通过使用 CRD 和其他核心 Kubernetes 资源引入了新的概念和功能，以控制数据平面。\nArgo Rollouts 如何实现流量管理？ Argo Rollouts 通过操作服务网格资源来匹配 Rollout 的意图，从而实现流量管理。Argo Rollouts 目前支持以下服务网格：\nAWS ALB Ingress Controller Ambassador Edge Stack Apache APISIX Istio Nginx Ingress Controller Service Mesh Interface (SMI)（SMI 已不再维护） Traefik Proxy Multiple Providers 如果需要其他实现，请在此处提交问题 （或给它点赞，如果该问题已存在） 无论使用哪个服务网格，Rollout 对象都必须在其 spec 中设置金丝雀服务和稳定服务。以下是设置了这些字段的示例：\napiVersion: argoproj.io/v1alpha1 kind: Rollout spec: ... strategy: canary: canaryService: canary-service stableService: stable-service trafficRouting: ... 控制器修改这些服务以将流量路由到适当的金丝雀和稳定 ReplicaSet，随着 Rollout 的进展，Service Mesh 使用这些服务来定义应该接收金丝雀和稳定流量的 Pod 组。\n此外，使用流量管理时，Argo Rollouts 控制器需要以不同的方式处理 Rollout 对象。特别是，由 Rollout 拥有的稳定 ReplicaSet 在 Rollout 通过金丝雀步骤时仍然完全扩展。\n由于流量由服务网格资源独立控制，控制器需要尽力确保稳定和新的 ReplicaSet 不会被发送到它们的流量压垮。通过保留稳定的 ReplicaSet 扩展，控制器确保稳定的 ReplicaSet 可以在任何时候处理 100% 的流量1。新的 ReplicaSet 的行为与没有流量管理时相同。新的 ReplicaSet 副本计数等于最新的 SetWeight 步骤百分比乘以 Rollout 的总副本计数。该计算确保金丝雀版本不会接收超出其处理能力的流量。\n基于托管路由和路由优先级的流量路由 流量路由器支持：（Istio） 启用流量路由时，你还可以让 argo rollouts 添加和管理除控制流量权重到金丝雀之外的其他路由。其中两个这样的路由规则是基于标头和镜像的路由。在使用这些路由时，我们还必须设置上游流量路由器的路由优先级。我们使用 spec.strategy.canary.trafficRouting.managedRoutes 字段来完成这项工作，这是一个数组，其中项目的顺序决定了优先级。这组路由也将按照指定的顺序放在手动定义的任何其他路由之上。\n警告\n在托管路由中列出的所有路由都将在 rollout 结束或中止时被删除。不要将任何手动创建的路由放入列表中。\n以下是一个例子：\napiVersion: argoproj.io/v1alpha1 kind: Rollout spec: ... strategy: canary: ... trafficRouting: managedRoutes: - name: priority-route-1 - name: priority-route-2 - name: priority-route-3 基于 Header 值的金丝雀流量路由 流量路由器支持：（Istio） Argo Rollouts 有能力根据 HTTP 请求头值将所有流量发送到金丝雀服务。标头基础流量路由的步骤是 setHeaderRoute，具有标头的多个匹配器。\nname - 标头路由的名称。\nmatch - 标头匹配规则是一组 headerName, headerValue 对。\nheaderName - 要匹配的标头的名称。\nheaderValue- 包含 exact - 指定确切标头值的值、regex - 以正则表达式格式的值、prefix - 可以提供值的前缀。并非所有流量路由器都支持所有匹配类型。\n要禁用基于标头的流量路由，只需指定 setHeaderRoute，并仅包含路由名称。\n例如：\napiVersion: argoproj.io/v1alpha1 kind: Rollout spec: ... strategy: canary: canaryService: canary-service stableService: stable-service trafficRouting: managedRoutes: - name: set-header-1 istio: virtualService: name: rollouts-demo-vsvc steps: - setWeight: 20 - setHeaderRoute: # 允许基于标头的流量路由 name: \u0026#34;set-header-1\u0026#34; match: - headerName: Custom-Header1 # Custom-Header1=Mozilla headerValue: exact: Mozilla - headerName: Custom-Header2 # 或者 Custom-Header2 以 Mozilla 为前缀 headerValue: prefix: Mozilla - headerName: Custom-Header3 # 或者 Custom-Header3 值匹配正则表达式：Mozilla(.*) headerValue: regex: Mozilla(.*) - pause: {} - setHeaderRoute: name: \u0026#34;set-header-1\u0026#34; # 禁用基于标头的流量路由 将流量路由镜像到金丝雀 流量路由器支持：（Istio） Argo Rollouts 有能力根据各种匹配规则将流量镜像到金丝雀服务。将流量镜像到基于镜像的流量路由的步骤是 setMirrorRoute，具有标头的多个匹配器。\nname - 镜像路由的名称。\npercentage - 要镜像的匹配流量的百分比\nmatch - 头路由的匹配规则，如果缺少此项，它将充当路由的删除。单个匹配块内的所有条件都具有 AND 语义，而匹配块列表具有 OR 语义。每个匹配内的每种类型（方法、路径、标头）必须具有一种且仅有一种匹配类型（完整、正则表达式、前缀）。并非所有匹配类型（完整、正则表达式、前缀）都受到所有流量路由器的支持。\n要禁用基于镜像的流量路由，只需指定一个仅包含路由名称的 setMirrorRoute。\n此示例将镜像 HTTP 流量的 35%，该流量匹配 GET 请求并具有 / 的 URL 前缀\napiVersion: argoproj.io/v1alpha1 kind: Rollout spec: ... strategy: canary: canaryService: canary-service stableService: stable-service trafficRouting: managedRoutes: - name: mirror-route istio: virtualService: name: rollouts-demo-vsvc steps: - setCanaryScale: weight: 25 - setMirrorRoute: name: mirror-route percentage: 35 match: - method: exact: GET path: prefix: / - pause: duration: 10m - setMirrorRoute: name: \u0026#34;mirror-route\u0026#34; # 移除基于镜像的流量路由 Rollout 必须假定应用程序在完全扩展时可以处理 100% 的流量。它应该外包给 HPA 来检测如果 100% 不够，Rollout 是否需要更多的副本。 ↩︎\n","relpermalink":"/book/argo-rollouts/traffic-management/overview/","summary":"Argo Rollouts 是一个 Kubernetes 自定义资源定义 (Custom Resource Definition, CRD)，扩展了 Kubernetes Deployment Controller，它添加了渐进式交付（Progressive Delivery）和蓝绿部署（Blue Green Deployment）等交付策略。Argo Rollouts","title":"流量管理概览"},{"content":"本指南通过演示部署、升级、推广和终止 Rollout 来演示 Argo Rollouts 的各种概念和特性。\n要求 安装了 argo-rollouts 控制器的 Kubernetes 集群（请参阅 安装指南 ） 安装了带有 argo-rollouts 插件的 kubectl（请参阅 安装指南 ） 1. 部署 Rollout 首先，我们部署一个 Rollout 资源和一个针对该 Rollout 的 Kubernetes Service。本指南中的示例 Rollout 利用了金丝雀升级策略，该策略将 20％的流量发送到金丝雀，然后进行手动推广，最后对其余升级进行逐渐自动化的流量增加。此行为在 Rollout spec 的以下部分中描述：\nspec: replicas: 5 strategy: canary: steps: - setWeight: 20 - pause: {} - setWeight: 40 - pause: {duration: 10} - setWeight: 60 - pause: {duration: 10} - setWeight: 80 - pause: {duration: 10} 运行以下命令以部署初始 Rollout 和 Service：\nkubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/rollout.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/service.yaml 任何 Rollout 的初始创建都将立即将副本扩展到 100％（跳过任何金丝雀升级步骤、分析等），因为没有进行升级。\nArgo Rollouts kubectl 插件允许你可视化 Rollout 及其相关资源（ReplicaSet、Pod、AnalysisRun），并呈现随着其发生的实时状态更改。要观察部署过程，请从插件运行 get rollout --watch 命令：\nkubectl argo rollouts get rollout rollouts-demo --watch 初始化 Rollout 2. 更新 Rollout 接下来是执行更新。与 Deployment 一样，对 Pod 模板字段（spec.template）的任何更改都会导致部署新版本（即 ReplicaSet）。更新 Rollout 涉及修改 Rollout 规范，通常使用新版本更改容器镜像字段，然后对新清单运行 kubectl apply。作为方便，rollouts 插件提供了一个 set image 命令，它针对现场 Rollout 对象执行这些步骤。运行以下命令，使用容器的“yellow”版本更新 rollouts-demo Rollout：\nkubectl argo rollouts set image rollouts-demo rollouts-demo=argoproj/rollouts-demo:yellow 在升级过程中，控制器将按照 Rollout 的升级策略的定义进行。示例 Rollout 将 20％的流量权重设置为金丝雀，暂停升级，并无限期地保持暂停状态，直到用户操作来取消暂停/推广 Rollout。更新镜像后，请再次观察 Rollout，直到其达到暂停状态：\nkubectl argo rollouts get rollout rollouts-demo --watch Rollout 已暂停 当演示 Rollout 达到第二步时，我们可以从插件中看到 Rollout 处于暂停状态，现在有 5 个副本中的 1 个正在运行 Pod 模板的新版本，而 4 个副本正在运行旧版本。这相当于由 setWeight: 20 步骤定义的 20％金丝雀权重。\n3. 推广 Rollout 现在 Rollout 处于暂停状态。当 Rollout 到达没有持续时间的 pause 步骤时，它将一直保持暂停状态，直到恢复/推广为止。要手动将 Rollout 推广到下一步，请运行插件的 promote 命令：\nkubectl argo rollouts promote rollouts-demo 推广后，Rollout 将继续执行其余步骤。我们示例中的其余升级步骤都是完全自动化的，因此 Rollout 最终将完成步骤，直到完全过渡到新版本为止。再次观察 Rollout，直到完成所有步骤：\nkubectl argo rollouts get rollout rollouts-demo --watch Rollout 已推广 提示\npromote 命令还支持使用 --full 标志跳过所有剩余步骤和分析的能力。\n一旦所有步骤成功完成，新的 ReplicaSet 将被标记为 stable 的 ReplicaSet。每当 Rollout 在更新期间中止（自动通过失败的金丝雀分析或用户手动），Rollout 将回退到 stable 版本。\n4. 终止 Rollout 接下来，我们将学习如何在更新期间手动中止 Rollout。首先，使用 set image 命令部署新的“红色”容器版本，并等待 Rollout 再次达到暂停步骤：\nkubectl argo rollouts set image rollouts-demo rollouts-demo=argoproj/rollouts-demo:red 暂停 Rollout（Revision 3） 这次，不是将 Rollout 推广到下一步，而是中止更新，以使其回退到 stable 版本。插件提供了一个 abort 命令作为手动中止更新期间 Rollout 的一种方式：\nkubectl argo rollouts abort rollouts-demo 当 Rollout 中止时，它将扩展 stable 版本的 ReplicaSet（在本例中为黄色镜像），并缩小任何其他版本。尽管 ReplicaSet 的稳定版本正在运行并且健康，但仍将整体 Rollout 视为 Degraded，因为所需版本（红色镜像）不是实际运行的版本。\n退出 Rollout 为了使 Rollout 再次被认为是 Healthy 而不是 Degraded，有必要将所需状态更改回以前的稳定版本。这通常涉及针对以前的 Rollout spec 运行 kubectl apply。在我们的情况下，我们可以简单地使用先前的“黄色”镜像重新运行 set image 命令。\nkubectl argo rollouts set image rollouts-demo rollouts-demo=argoproj/rollouts-demo:yellow 运行此命令后，你应该注意到 Rollout 立即变为 Healthy，并且没有任何与新 ReplicaSets 创建相关的活动。\n健康的 Rollout（Revision 4） 当 Rollout 尚未达到所需状态（例如，它被中止或正在升级中），并且稳定清单已重新应用时，Rollout 将检测到此为回滚而不是升级，并将通过跳过分析和步骤快速跟踪稳定版本 ReplicaSet 的部署。\n总结 在本指南中，我们学习了 Argo Rollouts 的基本功能，包括：\n部署 Rollout 执行金丝雀升级 手动推广 手动中止 本基本示例中的 Rollout 未使用入口控制器或服务网格提供程序来路由流量。相反，它使用正常的 Kubernetes Service 网络（即 kube-proxy）实现了一个“近似的”金丝雀权重，基于新旧副本计数的最接近比率。因此，此 Rollout 具有一项限制，即只能通过将 5 个 Pod 之一扩展为运行新版本来实现最小的金丝雀权重 20％。为了实现更细粒度的金丝雀，需要使用入口控制器或服务网格。\n请按照流量路由指南之一，查看 Argo Rollouts 如何利用网络提供程序实现更高级的流量整形。\nALB 指南 App Mesh 指南 Ambassador 指南 Istio 指南 NGINX 指南 ","relpermalink":"/book/argo-rollouts/getting-started/basic-usage/index/","summary":"本指南通过演示部署、升级、推广和终止 Rollout 来演示 Argo Rollouts 的各种概念和特性。 要求 安装了 argo-rollouts 控制器的 Kubernetes 集群（请参阅 安装指南 ） 安装了带有 argo-rollouts 插件的 kubectl（请参阅 安装指南 ） 1. 部署 Rollout 首先，我们部署一个 Rollout 资源和一个针","title":"基础使用"},{"content":"什么是 Argo Rollouts？ Argo Rollouts 是一组 Kubernetes 控制器 和自定义资源（CRD） ，为 Kubernetes 提供高级部署功能，例如蓝绿、金丝雀、金丝雀分析、实验和渐进式交付等功能。\nArgo Rollouts 可选地与 Ingress 控制器 和服务网格集成，利用它们的流量整形能力在更新期间逐渐将流量转移到新版本。此外，Rollouts 可以查询和解释来自各种提供商的度量标准，以验证关键 KPI 并在更新期间驱动自动升级或回滚。\n这是一个演示视频（点击在 Youtube 上观看）：\n为什么选择 Argo Rollouts？ Kubernetes Deployment 对象支持滚动更新策略，该策略提供了一组基本的安全性保证（就绪探针）来保证更新期间的安全性。但是，滚动更新策略面临许多限制：\n对滚动更新速度的控制很少 无法控制流量流向新版本 就绪探针不适用于更深入的、压力或一次性检查 没有查询外部度量标准以验证更新的能力 可以停止进程，但无法自动中止并回滚更新 因此，在大型高容量生产环境中，滚动更新往往被认为是过于冒险的更新过程，因为它无法控制爆炸半径，可能过于激进地进行滚动更新，并且在发生故障时无法提供自动回滚。\n控制器特点 蓝绿更新策略 金丝雀更新策略 精细、加权的流量转移 自动回滚和升级 手动判断 可自定义的度量标准查询和业务 KPI 分析 Ingress 控制器集成：NGINX、ALB、Apache APISIX 服务网格集成：Istio、Linkerd、SMI 同时使用多个提供程序：SMI + NGINX、Istio + ALB 等。 度量提供程序集成：Prometheus、Wavefront、Kayenta、Web、Kubernetes Jobs、Datadog、New Relic、Graphite、InfluxDB 快速开始 kubectl create namespace argo-rollouts kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml 请按照完整的 入门指南 ，演示创建并更新一个滚动对象。\n它是如何工作的？ 与 Deployment 对象 类似，Argo Rollouts 控制器将管理 ReplicaSet 的创建、扩展和删除。这些 ReplicaSet 是由 Rollout 资源中的 spec.template 字段定义的，该字段使用与 Deployment 对象相同的 pod 模板。\n当更改 spec.template 时，这表明 Argo Rollouts 控制器将引入一个新的 ReplicaSet。该控制器将使用 spec.strategy 字段中设置的策略来确定如何从旧 ReplicaSet 进行滚动更新到新 ReplicaSet。一旦新 ReplicaSet 缩放（并可选地通过 分析 ），控制器将将其标记为“stable”。\n如果在从稳定的 ReplicaSet 到新的 ReplicaSet 的转换过程中 spec.template 发生其他更改（即，在滚动更新过程中更改应用程序版本），则先前的新 ReplicaSet 将被缩小，控制器将尝试推进反映更新的 spec.template 字段的 ReplicasSet。有关每个策略行为的更多信息，请参见 Rollout 规范 部分。\nArgo Rollouts 的用例 用户想在开始为生产流量服务之前对新版本进行最后一分钟的功能测试。使用 BlueGreen 策略，Argo Rollouts 允许用户指定预览服务和活动服务。Rollout 将配置预览服务以将流量发送到新版本，而活动服务仍将接收生产流量。一旦用户满意，他们可以将预览服务升级为新的活动服务。(示例 ) 在新版本开始接收实时流量之前，需要执行一组通用步骤。使用 BlueGreen 策略，用户可以启动新版本，而不会从活动服务接收流量。一旦这些步骤完成执行，Rollout 可以将流量切换到新版本。 用户希望将生产流量的一小部分分配给他们的应用程序的新版本，持续几个小时。之后，他们想要缩小新版本并查看一些指标，以确定新版本与旧版本相比是否性能良好。然后，他们将决定是否要将新版本滚动到所有生产流量或坚持使用当前版本。使用金丝雀策略，Rollout 可以将新版本的 ReplicaSet 扩展到接收指定百分比的流量，等待指定时间后将百分比设置回 0，然后等待满意后再滚动到服务所有流量。(示例 ) 用户希望慢慢将新版本的生产流量增加。他们首先给它一小部分实时流量，并等待一段时间，然后再给新版本更多的流量。最终，新版本将接收所有生产流量。使用金丝雀策略，用户可以指定他们希望新版本接收的百分比以及百分比之间的等待时间。(示例 ) 用户希望使用 Deployment 的常规滚动更新策略。如果用户使用金丝雀策略且没有步骤，则 Rollout 将使用最大浮动和最大不可用值进行滚动到新版本。(示例 ) 示例 你可以在以下位置查看更多 Rollouts 示例：\n示例目录 Argo Rollouts 演示应用程序 ","relpermalink":"/book/argo-rollouts/overview/","summary":"什么是 Argo Rollouts？ Argo Rollouts 是一组 Kubernetes 控制器 和自定义资源（CRD） ，为 Kubernetes 提供高级部署功能，例如蓝绿、金丝雀、金丝雀分析、实验和渐进式交付等功能。 Argo Rollouts 可选地与 Ingress 控制器 和服务网格集成，利用它们的流量整形能","title":"Argo Rollouts 简介"},{"content":"蓝绿部署允许用户减少同时运行多个版本的时间。\n概述 除了管理 ReplicaSet 外，在 BlueGreenUpdate 策略期间，Rollout 控制器还将修改 Service 资源。Rollout 规范要求用户在同一命名空间中指定对活动服务的引用以及可选的预览服务。活动服务用于将常规应用程序流量发送到旧版本，而预览服务用于将流量漏斗到新版本。Rollout 控制器通过向这些服务的选择器注入 ReplicaSet 的唯一哈希来确保正确的流量路由。这允许 Rollout 定义一个活动和预览堆栈以及从预览到活动的过程。\n当 Rollout 的 .spec.template 字段发生更改时，控制器将创建新的 ReplicaSet。如果活动服务没有将流量发送到 ReplicaSet，则控制器将立即开始将流量发送到 ReplicaSet。否则，活动服务将指向旧 ReplicaSet，而 ReplicaSet 变得可用。一旦新的 ReplicaSet 变得可用，控制器将修改活动服务以指向新的 ReplicaSet。在等待 .spec.strategy.blueGreen.scaleDownDelaySeconds 配置的一些时间之后，控制器将缩小旧 ReplicaSet。\n🔔 重要：当 Rollout 在服务上更改选择器时，所有节点更新其 IP 表以将流量发送到新的 Pod 而不是旧的 Pod 之前存在传播延迟。在此延迟期间，如果节点尚未更新，则流量将被定向到旧 Pod。为了防止将数据包发送到杀死旧 Pod 的节点，Rollout 使用 scaleDownDelaySeconds 字段为节点提供足够的时间来广播 IP 表更改。\n示例 apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollout-bluegreen spec: replicas: 2 revisionHistoryLimit: 2 selector: matchLabels: app: rollout-bluegreen template: metadata: labels: app: rollout-bluegreen spec: containers: - name: rollouts-demo image: argoproj/rollouts-demo:blue imagePullPolicy: Always ports: - containerPort: 8080 strategy: blueGreen: # activeService 指定要在升级时使用新模板哈希更新的服务。 # 对于蓝绿更新策略，此字段为必填字段。 activeService: rollout-bluegreen-active # previewService 指定要在推广之前使用新模板哈希更新的服务。 # 这使得预览栈可以在不提供生产流量的情况下可达。 # 此字段为可选字段。 previewService: rollout-bluegreen-preview # autoPromotionEnabled 通过在推广之前立即暂停 Rollout 来禁用新堆栈的自动推广。 # 如果省略，默认行为是在 ReplicaSet 完全准备/可用后立即推广新堆栈。 # 可以使用以下命令恢复 Rollout：`kubectl argo rollouts promote ROLLOUT` autoPromotionEnabled: false 可配置的特性 以下是将更改 BlueGreen 部署行为的可选字段：\nspec: strategy: blueGreen: autoPromotionEnabled: boolean autoPromotionSeconds: *int32 antiAffinity: object previewService: string prePromotionAnalysis: object postPromotionAnalysis: object previewReplicaCount: *int32 scaleDownDelaySeconds: *int32 scaleDownDelayRevisionLimit: *int32 事件序列 以下描述了蓝绿更新期间发生的事件序列。\n从完全推广的稳定状态开始，使用修订版本 1 的 ReplicaSet 指向 activeService 和 previewService。 用户通过修改 Pod 模板 (spec.template.spec) 启动更新。 创建大小为 0 的修订版本 2 ReplicaSet。 修改预览服务以指向修订版本 2 ReplicaSet。activeService 仍指向修订版本 1。 如果使用了 previewReplicaCount，则将修订版本 2 ReplicaSet 缩放到 spec.replicas 或 previewReplicaCount。 一旦修订版本 2 ReplicaSet Pod 完全可用，prePromotionAnalysis 就会开始。 在 prePromotionAnalysis 成功后，如果 autoPromotionEnabled 为 false 或 autoPromotionSeconds 不为零，则蓝绿色暂停。 通过手动或自动超过 autoPromotionSeconds 来恢复 Rollout。 如果使用了 previewReplicaCount，则将修订版本 2 ReplicaSet 缩放到 spec.replicas。 Rollout 通过将 activeService 更新为指向新的 ReplicaSet 来“推广”修订版本 2 ReplicaSet。此时，没有服务指向修订版本 1 postPromotionAnalysis 分析开始 一旦 postPromotionAnalysis 成功完成，更新成功，修订版本 2 ReplicaSet 被标记为稳定。Rollout 被认为是完全推广。 在等待 scaleDownDelaySeconds（默认为 30 秒）之后，修订版本 1 ReplicaSet 被缩小。 autoPromotionEnabled AutoPromotionEnabled 将使 Rollout 在新的 ReplicaSet 健康后自动将其推广到活动服务。如果未指定，该字段的默认值为 true。\n默认为 true\nautoPromotionSeconds AutoPromotionSeconds 将使 Rollout 在自动暂停状态下进入 AutoPromotionSeconds 时间后自动将新的 ReplicaSet 推广到活动服务。如果 AutoPromotionEnabled 字段设置为 false，则将忽略此字段。\n默认为 nil\nantiAffinity 有关更多信息，请查看 Anti Affinity 文档。\n默认为 nil\nmaxUnavailable 在更新期间可以不可用的 Pod 的最大数量。该值可以是绝对数字（例如：5）或所需 Pod 的百分比（例如：10%）。如果 MaxSurge 为 0，则不能为 0。\n默认为 0\nprePromotionAnalysis 在将流量切换到新版本之前，配置 Analysis。AnalysisRun 可用于在 AnalysisRun 成功完成之前阻止 Service 选择器切换。分析运行的成功或失败决定 Rollout 是否切换流量，或完全中止 Rollout。\n默认为 nil\npostPromotionAnalysis 在将流量切换到新版本后配置 Analysis。如果分析运行失败或出错，则 Rollout 进入中止状态并将流量切换回以前的稳定 Replicaset。如果指定了 scaleDownDelaySeconds，控制器将在 scaleDownDelay 时取消任何 AnalysisRuns 以缩小 ReplicaSet。如果省略它，并且指定了后期分析，则仅在 AnalysisRun 完成后缩小 ReplicaSet（最少为 30 秒）。\n默认为 nil\npreviewService PreviewService 字段引用将被修改以在新 ReplicaSet 之前发送流量的 Service。一旦新 ReplicaSet 开始接收来自活动服务的流量，预览服务也将被修改以将流量发送到新 ReplicaSet。Rollout 始终确保预览服务将流量发送到最新的 ReplicaSet。因此，如果在将旧版本推广到活动服务之前引入新版本，则控制器将立即切换到全新的版本。\n此功能用于提供可以用于测试应用程序的新版本的终点。\n默认为空字符串\npreviewReplicaCount PreviewReplicaCount 字段将指示新版本的应用程序应运行的副本数。一旦应用程序准备好推广到活动服务，控制器将扩展新的 ReplicaSet 到 spec.replicas 的值。在测试阶段期间，此功能的主要用途是节省资源。如果应用程序不需要完全缩放应用程序进行测试，则此功能可以帮助节省一些资源。\n如果省略，则预览 ReplicaSet 堆栈将缩放到 100% 的副本。\nscaleDownDelaySeconds ScaleDownDelaySeconds 用于在将活动服务切换到新 ReplicaSet 后延迟缩小旧 ReplicaSet。\n默认为 30\nscaleDownDelayRevisionLimit ScaleDownDelayRevisionLimit 限制保留在活动服务中的旧 ReplicaSet 数量，直到从活动服务中删除后的 scaleDownDelay。如果省略，则将保留所有 ReplicaSets 以供指定的 scaleDownDelay。\n","relpermalink":"/book/argo-rollouts/rollout/deployment-strategies/bluegreen/","summary":"蓝绿部署允许用户减少同时运行多个版本的时间。 概述 除了管理 ReplicaSet 外，在 BlueGreenUpdate 策略期间，Rollout 控制器还将修改 Service 资源。Rollout 规范要求用户在同一命名空间中指定对活动服务的引用以及可选的预览服务。活动服","title":"蓝绿部署策略"},{"content":"本书介绍了服务身份的 SPIFFE 标准，以及 SPIFFE 的参考实现 SPIRE。这些项目为现代异构基础设施提供了一个统一的身份控制平面。这两个项目都是开源的，隶属于云原生计算基金会（CNCF）。\n随着企业发展他们的应用架构以充分利用新的基础设施技术，他们的安全模式也必须不断发展。软件已经从一个单片机上的单个应用发展到了几十甚至几百个紧密联系的微服务，这些微服务可能分布在公共云或私人数据中心的数千个虚拟机上。在这个新的基础设施世界里，SPIFFE 和 SPIRE 帮助保障系统的安全。\n本书努力提炼了 SPIFFE 和 SPIRE 的最重要的专家经验，以提供对身份问题的深刻理解，并协助你解决这些问题。通过这些项目，开发和运维可以利用新的基础设施技术构建软件，同时让安全团队摆脱昂贵和耗时的人工安全流程。\n关于零号乌龟 访问控制、秘密管理和身份是相互依赖的。大规模地管理秘密需要有效的访问控制；实施访问控制需要身份；证明身份需要拥有一个秘密。保护一个秘密需要想出一些办法来保护另一个秘密，这就需要保护那个秘密，以此类推。\n这让人想起一个著名的轶事：一个女人打断了一位哲学家的讲座，告诉他世界是在乌龟的背上。当哲学家问她乌龟靠的是什么时，她说：“还是乌龟！\u0026#34;。找到底层的乌龟，即所有其他安全所依赖的坚实基础，是 SPIFFE 和 SPIRE 项目的目标。\n本书封面上的 “零号乌龟” 就是这只底层乌龟。零代表了数据中心和云计算的安全基础。零号是值得信赖的，愉快地支持所有其他的乌龟。\nSPIFFE 和 SPIRE 是帮助你为你的组织找到底层乌龟的项目。通过这本书中的工具，我们希望你也能为 “底层乌龟” 找到一个家。\n","relpermalink":"/book/spiffe/preface/","summary":"本书介绍了服务身份的 SPIFFE 标准，以及 SPIFFE 的参考实现 SPIRE。这些项目为现代异构基础设施提供了一个统一的身份控制平面。这两个项目都是开源的，隶属于云原生计算基金会（CNCF）。 随着企业发展他们的应用架构以充","title":"关于本书"},{"content":"本书译自 Solving the Bottom Turtle — a SPIFFE Way to Establish Trust in Your Infrastructure via Universal Identity，译者 Jimmy Song 。\n《零信任的基石》封面 Copyright Solving the Bottom Turtle — a SPIFFE Way to Establish Trust in Your Infrastructure via Universal Identity\nby Daniel Feldman, Emily Fox, Evan Gilman, Ian Haken, Frederick Kautz, Umair Khan, Max Lambrecht, Brandon Lum, Agustín Martínez Fayó, Eli Nesterov, Andres Vega, Michael Wardrop. 2020.\nThis work is licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0).\nISBN: 978-0-578-77737-5 URL: thebottomturtle.io\nThis book was produced using the Book Sprints methodology (www.booksprints.net ). Its content was written by the authors during an intensive collaboration process conducted online over two weeks.\nBook Sprints Facilitation: Barbara Rühling Copy Editors: Raewyn Whyte and Christine Davis HTML Book Design: Manuel Vazquez Illustrations and Cover Design: Henrik Van Leeuwen Fonts: Work Sans designed by Wei Huang, Iosevka by Belleve Invis\nBook Sprint participants Daniel Feldman is Principal Software Engineer at Hewlett Packard Enterprise Emily Fox is the Cloud Native Computing Foundation (CNCF) Special Interest Group for Security (SIG-Security), Co-Chair Evan Gilman is a Staff Engineer at VMware Ian Haken is Senior Security Software Engineer at Netflix Frederick Kautz is Head of Edge Infrastructure at Doc.ai Umair Khan is Sr. Product Marketing Manager at Hewlett Packard Enterprise Max Lambrecht is Senior Software Engineer at Hewlett Packard Enterprise Brandon Lum is Senior Software Engineer at IBM Agustín Martínez Fayó is Principal Software Engineer at Hewlett Packard Enterprise Eli Nesterov is Security Engineering Manager at ByteDance Andres Vega is Product Line Manager at VMware Michael Wardrop is Staff Engineer at Cohesity 本书大纲 关于本书\n1. SPIFFE 的历史和动机\n2. 收益\n3. 身份背后的通用概念\n4. SPIFFE 和 SPIRE 概念介绍\n5. 开始前的准备\n6. 设计一个 SPIRE 部署\n7. 与其他系统集成\n8. 使用 SPIFFE 身份通知授权\n9. SPIFFE 与其他安全技术对比\n10. 业界案例\n开始阅读 ","relpermalink":"/book/spiffe/","summary":"本书系统的讲解了 SPFFE 来解决零信任的身份问题。","title":"零信任的基石：使用 SPIFFE 为基础设施创建通用身份"},{"content":"Linux 内核在网络堆栈中支持一组 BPF 钩子（hook），可用于运行 BPF 程序。Cilium 数据路径使用这些钩子来加载 BPF 程序，这些程序一起使用时会创建更高级别的网络结构。\n以下是 Cilium 使用的钩子列表和简要说明。有关每个钩子细节的更详尽的文档，请参阅 BPF 和 XDP 参考指南 。\nXDP：XDP BPF 钩子位于网络驱动程序中的最早可能点，并在数据包接收时触发 BPF 程序的运行。这实现了可能的最佳数据包处理性能，因为程序在任何其他处理发生之前直接在数据包数据上运行。此钩子非常适合运行丢弃恶意或意外流量的过滤程序以及其他常见的 DDOS 保护机制。\n流量控制入口/出口：附加到流量控制（traffic control，简称 TC）入口钩子的 BPF 程序附加到网络接口，与 XDP 相同，但将在网络堆栈完成数据包的初始处理后运行。该钩子在三层网络之前运行，但可以访问与数据包关联的大部分元数据。这非常适合进行本地节点处理，例如应用三层/四层端点策略并将流量重定向到端点。对于面向网络的设备，TC 入口钩子可以与上面的 XDP 钩子耦合。完成此操作后，可以合理地假设此时的大部分流量是合法的并以主机为目的地。\n容器通常使用称为 veth 对的虚拟设备，它充当将容器连接到主机的虚拟路由。通过附加到这个 veth 对的主机端的 TC 入口钩子，Cilium 可以监控和执行所有离开容器的流量的策略。通过将 BPF 程序附加到与每个容器关联的 veth 对，并将所有网络流量路由到主机端虚拟设备，同时将另一个 BPF 程序附加到 TC 入口钩子，Cilium 可以监控所有进入或离开节点的流量并执行策略。\n套接字操作：套接字操作钩子附加到特定的 cgroup 并在 TCP 事件上运行。Cilium 将 BPF 套接字操作程序附加到根 cgroup 并使用它来监视 TCP 状态转换，特别是 ESTABLISHED 状态转换。如果 TCP 套接字具有节点本地对等节点（可能是本地代理），则当套接字转换为 ESTABLISHED 状态时，附加套接字发送 / 接收程序。\n套接字发送/接收：套接字发送/接收钩子（socket send/recv hook）在 TCP 套接字执行的每个发送操作上运行。此时，钩子可以检查消息并丢弃消息、将消息发送到 TCP 层或将消息重定向到另一个套接字。Cilium 使用它来加速数据路径重定向，如下所述。\n将上述钩子与虚拟接口（cilium_host、cilium_net）、可选的 overlay 接口（cilium_vxlan）、Linux 内核加密支持和用户空间代理（Envoy）相结合，Cilium 创建了以下网络对象。\n前置过滤器（Prefilter）：前置过滤器对象运行一个 XDP 程序并提供一组前置过滤器规则，用于过滤来自网络的流量以获得最佳性能。具体来说，数据包要么被丢弃，例如当目标不是有效的端点时，要么被堆栈处理。这可以根据需要轻松扩展以构建新的前置过滤器标准/功能。\n端点策略：端点策略对象实现 Cilium 端点强制。使用映射来查找与身份和策略相关的数据包，该层可以很好地扩展到许多端点。根据策略，该层可能会丢弃数据包、转发到本地端点、转发到服务对象或转发到七层策略对象以获取进一步的七层规则。这是 Cilium 数据路径中的主要对象，负责将数据包映射到身份并执行三层和四层策略。\n服务：服务对象对对象接收的每个数据包的目标 IP 和可选的目标端口执行映射查找。如果找到匹配条目，数据包将被转发到配置的三层/四层端点之一。Service 块可用于使用 TC 入口钩子在任何接口上实现独立的负载均衡器，或者可以集成到端点策略对象中。\n三层加密：在入口时，三层加密对象标记要解密的数据包，将数据包传递给 Linux xfrm（转换）层进行解密，数据包解密后对象接收数据包，然后将其向上传递到堆栈以供其他人进一步处理对象。根据模式、直接路由或覆盖，这可能是 BPF 尾调用或将数据包传递给下一个对象的 Linux 路由堆栈。解密所需的密钥在 IPsec 标头中编码，因此在入口处我们不需要进行映射查找来查找解密密钥。\n在出口处，首先使用目标 IP 执行映射查找以确定数据包是否应加密，如果加密，则目标节点上可用的密钥。选择两个节点上可用的最新密钥，并将数据包标记为加密。然后将数据包传递到对其进行加密的 Linux xfrm 层。在接收到现在加密的数据包后，通过将其发送到 Linux 堆栈进行路由或在使用覆盖时进行直接尾调用，将其传递到下一层。\n套接字层强制：套接字层强制使用两个钩子，套接字操作钩子和套接字发送/接收钩子来监视和附加到与 Cilium 托管端点关联的所有 TCP 套接字，包括任何 七层代理。套接字操作钩子将识别用于加速的候选套接字。这些包括所有本地节点连接（端点到端点）和任何到 Cilium 代理的连接。然后，这些已识别的连接将由套接字发送/接收钩子处理所有消息，并将使用 sockmap 快速重定向加速。快速重定向确保 Cilium 中实现的所有策略对关联的套接字 / 端点映射有效，并假设它们直接将消息发送到对等套接字。这是允许的，因为 sockmap send/recv 钩子确保消息不需要由上述任何对象处理。\n七层策略：七层策略对象将代理流量重定向到 Cilium 用户空间代理实例。Cilium 使用 Envoy 实例作为其用户空间代理。然后，Envoy 将根据配置的七层策略转发流量或生成适当的拒绝消息。\n这些组件相互连接，以创建 Cilium 使用的灵活高效的数据路径。下面我们展示了连接单个节点上的端点、入口到端点以及端点到出口网络设备的以下可能流程。在每种情况下，都有一个附加图表显示启用套接字层强制时可用的 TCP 加速路径。\n","relpermalink":"/book/cilium-handbook/ebpf/intro/","summary":"Linux 内核在网络堆栈中支持一组 BPF 钩子（hook），可用于运行 BPF 程序。Cilium 数据路径使用这些钩子来加载 BPF 程序，这些程序一起使用时会创建更高级别的网络结构。 以下是 Cilium 使用的钩子列表和简要说明。有关每个钩子","title":"eBPF 数据路径介绍"},{"content":"Cilium 能为 Kubernetes 集群提供什么？ 在 Kubernetes 集群中运行 Cilium 时提供以下功能：\nCNI 插件支持，为 pod 连接 提供 联网 。 NetworkPolicy 资源的基于身份的实现，用于隔离三层和四层网络 pod 的连接。 以 CustomResourceDefinition 形式对 NetworkPolicy 的扩展，扩展策略控制以添加： 针对以下应用协议的入口和出口执行七层策略： HTTP Kafka 对 CIDR 的出口支持以保护对外部服务的访问 强制外部无头服务自动限制为服务配置的 Kubernetes 端点集 ClusterIP 实现为 pod 到 pod 的流量提供分布式负载平衡 完全兼容现有的 kube-proxy 模型 Pod 间连接 在 Kubernetes 中，容器部署在称为 pod 的单元中，其中包括一个或多个可通过单个 IP 地址访问的容器。使用 Cilium，每个 pod 从运行 pod 的 Linux 节点的节点前缀中获取一个 IP 地址。有关其他详细信息，请参阅 IP 地址管理（IPAM） 。在没有任何网络安全策略的情况下，所有的 pod 都可以互相访问。\nPod IP 地址通常位于 Kubernetes 集群本地。如果 pod 需要作为客户端访问集群外部的服务，则网络流量在离开节点时会自动伪装。\n服务负载均衡 Kubernetes 开发了服务抽象，它为用户提供了将网络流量负载平衡到不同 pod 的能力。这种抽象允许 pod 通过单个 IP 地址（一个虚拟 IP 地址）与其他 pod 联系，而无需知道所有运行该特定服务的 pod。\n如果没有 Cilium，kube-proxy 会安装在每个节点上，监视 kube-master 上的端点和服务的添加和删除，这允许它在 iptables 上应用必要的强制策略执行。因此，从 pod 接收和发送到的流量被正确地路由到为该服务服务的节点和端口。有关更多信息，您可以查看服务的 Kubernetes 用户 指南 。\n在实现 ClusterIP 时，Cilium 的行为与 kube-proxy 相同，它监视服务的添加或删除，但不是在 iptables 上执行，而是更新每个节点上的 eBPF 映射条目。有关更多信息，请参阅 GItHub PR 。\n延伸阅读 Kubernetes 文档包含有关 Kubernetes 网络模型 和 Kubernetes 网络插件 的更多背景信息。\n","relpermalink":"/book/cilium-handbook/kubernetes/intro/","summary":"Cilium 能为 Kubernetes 集群提供什么？ 在 Kubernetes 集群中运行 Cilium 时提供以下功能： CNI 插件支持，为 pod 连接 提供 联网 。 NetworkPolicy 资源的基于身份的实现，用于隔离三层和四层网络 pod 的连接。 以 CustomResourceDefinition 形式对 NetworkPolicy 的扩展，扩展策略控制以添加： 针对以下应用协议的入","title":"Kubernetes 集成介绍"},{"content":"Cilium 在多个层面上提供安全性。可以单独使用或组合使用。\n基于身份 ：端点之间的连接策略（三层），例如任何带有标签的端点 role=frontend 都可以连接到任何带有标签的端点 role=backend。 限制传入和传出连接的可访问端口（四层），例如带标签的端点 role=frontend只能在端口 443（https）上进行传出连接，端点role=backend 只能接受端口 443（https）上的连接。 应用程序协议级别的细粒度访问控制，以保护 HTTP 和远程过程调用（RPC）协议，例如带有标签的端点 role=frontend 只能执行 REST API 调用 GET /userdata/[0-9]+，所有其他与 role=backend API 的交互都受到限制。 ","relpermalink":"/book/cilium-handbook/security/intro/","summary":"Cilium 在多个层面上提供安全性。可以单独使用或组合使用。 基于身份 ：端点之间的连接策略（三层），例如任何带有标签的端点 role=frontend 都可以连接到任何带有标签的端点 role=backend。 限制传入和传出连接的可访问端口（","title":"介绍"},{"content":"封装 当没有提供配置时，Cilium 会自动在此模式下运行，因为它是对底层网络基础设施要求最低的模式。\n在这种模式下，所有集群节点使用基于 UDP 的封装协议 VXLAN 或 Geneve 。Cilium 节点之间的所有流量都被封装。\n对网络的要求 封装依赖于正常的节点到节点的连接。这意味着如果 Cilium 节点已经可以互相到达，那么所有的路由要求都已经满足了。\n底层网络和防火墙必须允许封装数据包：\n封装方式 端口范围 / 协议 VXLAN（默认） 8472/UDP Geneve 6081/UDP 封装模式的优点 封装模式具有以下优点：\n简单\n连接集群节点的网络不需要知道 PodCIDR。集群节点可以产生多个路由或链路层域。只要集群节点可以使用 IP/UDP 相互访问，底层网络的拓扑就无关紧要。\n寻址空间\n由于不依赖于任何底层网络限制，如果相应地配置了 PodCIDR 大小，可用的寻址空间可能会更大，并且允许每个节点运行任意数量的 Pod。\n自动配置\n当与 Kubernetes 等编排系统一起运行时，集群中所有节点的列表（包括它们关联的分配前缀节点）会自动提供给每个代理。加入集群的新节点将自动合并到网格中。\n身份上下文\n封装协议允许将元数据与网络数据包一起携带。Cilium 利用这种能力来传输元数据，例如源安全身份。身份传输是一种优化，旨在避免在远程节点上进行一次身份查找。\n封装模式的缺点 封装模式有以下缺点：\nMTU 开销\n由于添加了封装标头，可用于有效负载的有效 MTU 低于本地路由（VXLAN 的每个网络数据包 50 字节）。这会导致特定网络连接的最大吞吐率较低。这可以通过启用巨型帧（每 1500 字节 50 字节的开销对比每 9000 字节的 50 字节开销）在很大程度上得到缓解。\n本地路由 本地路由数据路径使用 tunnel: disabled 启用并启用本机数据包转发模式。本地数据包转发模式利用 Cilium 运行的网络的路由功能，而不是执行封装。\n本地路由模式 在本地路由模式下，Cilium 会将所有未发送到另一个本地端点的数据包委托给 Linux 内核的路由子系统。这意味着数据包将被路由，就好像本地进程会发出数据包一样。因此，连接集群节点的网络必须能够路由 PodCIDR。\n当配置本地路由时，Cilium 会在 Linux 内核中自动启用 IP 转发。\n对网络的要求 为了运行本地路由模式，连接运行 Cilium 的主机的网络必须能够使用分配给 pod 或其他工作负载的地址转发 IP 流量。\n节点上的 Linux 内核必须知道如何转发运行 Cilium 的所有节点的 pod 或其他工作负载的数据包。这可以通过两种方式实现：\n节点本身不知道如何路由所有 pod IP，但网络上存在一个知道如何到达所有其他 pod 的路由器。在这种情况下，Linux 节点配置为包含指向此类路由器的默认路由。该模型用于云提供商网络集成。 每个单独的节点都知道所有其他节点的所有 Pod IP，并且路由被插入到 Linux 内核路由表中来表示这一点。如果所有节点共享一个 L2 网络，则可以通过启用 auto-direct-node-routes: true 选项来解决此问题。否则，必须运行额外的系统组件（例如 BGP 守护程序）来分发路由。请参阅 使用 kube-router 运行 BGP 指南，了解如何使用 kube-router 项目实现此目的。 配置 必须设置以下配置选项以在本地路由模式下运行数据路径：\ntunnel: disabled：启用本机路由模式。 ipv4-native-routing-cidr: x.x.x.x/y：设置可以执行本机路由的 CIDR。 ","relpermalink":"/book/cilium-handbook/networking/routing/","summary":"封装 当没有提供配置时，Cilium 会自动在此模式下运行，因为它是对底层网络基础设施要求最低的模式。 在这种模式下，所有集群节点使用基于 UDP 的封装协议 VXLAN 或 Geneve 。Cilium 节点之间的所有流量都被封装。 对网络的","title":"路由"},{"content":"本文将为你简要介绍 Cilium 和 Hubble。\n什么是 Cilium？ Cilium 是开源软件，用于透明地保护使用 Docker 和 Kubernetes 等 Linux 容器管理平台部署的应用服务之间的网络连接。\nCilium 的基础是一种新的 Linux 内核技术，称为 eBPF，它使强大的安全可视性和控制逻辑动态插入 Linux 本身。由于 eBPF 在 Linux 内核内运行，Cilium 安全策略的应用和更新无需对应用程序代码或容器配置进行任何改动。\n什么是 Hubble？ Hubble 是一个完全分布式的网络和安全可观测性平台。它建立在 Cilium 和 eBPF 之上，以完全透明的方式实现对服务的通信行为以及网络基础设施的深度可视性。\n通过建立在 Cilium 之上，Hubble 可以利用 eBPF 实现可视性。依靠 eBPF，所有的可视性都是可编程的，并允许采用一种动态的方法，最大限度地减少开销，同时按照用户的要求提供深入和详细的可视性。Hubble 的创建和专门设计是为了最好地利用这些新的 eBPF 力量。\nHubble 可以回答诸如以下问题。\n服务依赖和拓扑图 哪些服务在相互通信？频率如何？服务依赖关系图是什么样子的？ 正在进行哪些 HTTP 调用？服务正在生产或消费哪些 Kafka 主题？ 网络监控和警报 是否有任何网络通信失败？为什么会出现通信失败？是 DNS 的问题吗？是应用还是网络问题？通讯是在四层（TCP）还是七层（HTTP）中断的？ 在过去 5 分钟内，哪些服务遇到了 DNS 解析问题？哪些服务最近经历了 TCP 连接中断或连接超时？未回复的 TCP SYN 请求的比率是多少？ 应用监控 某一服务或所有集群中的 5xx 或 4xx HTTP 响应代码的比率是多少？ 在我的集群中，HTTP 请求和响应之间的 95% 和 99% 的延迟是什么？哪些服务的性能最差？两个服务之间的延时是多少？ 安全观察性 哪些服务由于网络策略而被阻止连接？哪些服务被从集群外访问过？哪些服务解析了一个特定的 DNS 名称？ 为什么使用 Cilium 和 Hubble？ 现代数据中心应用程序的开发已经转向面向服务的体系结构（SOA），通常称为微服务，其中大型应用程序被分成小型独立服务，这些服务使用 HTTP 等轻量级协议通过 API 相互通信。微服务应用程序往往是高度动态的，作为持续交付的一部分部署的滚动更新期间单个容器启动或销毁，应用程序扩展 / 缩小以适应负载变化。\n这种向高度动态的微服务的转变过程，给确保微服务之间的连接方面提出了挑战和机遇。传统的 Linux 网络安全方法（例如 iptables）过滤 IP 地址和 TCP/UDP 端口，但 IP 地址经常在动态微服务环境中流失。容器的高度不稳定的生命周期导致这些方法难以与应用程序并排扩展，因为负载均衡表和访问控制列表要不断更新，可能增长成包含数十万条规则。出于安全目的，协议端口（例如，用于 HTTP 流量的 TCP 端口 80）不能再用于区分应用流量，因为该端口用于跨服务的各种消息。\n另一个挑战是提供准确的可视性，因为传统系统使用 IP 地址作为主要识别工具，其在微服务架构中的寿命被大大缩短，可能才仅仅几秒钟。\n利用 Linux eBPF，Cilium 保留了透明地插入安全可视性 + 强制执行的能力，但这种方式基于服务 /pod/ 容器标识（与传统系统中的 IP 地址识别相反），并且可以根据应用层进行过滤（例如 HTTP）。因此，通过将安全性与寻址分离，Cilium 不仅可以在高度动态的环境中应用安全策略，而且除了提供传统的三层和四层分割之外，还可以通过在 HTTP 层运行来提供更强的安全隔离。\neBPF 的使用使得 Cilium 能够以高度可扩展的方式实现以上功能，即使对于大规模环境也不例外。\n功能概述 以下是关于 Cilium 可以提供的功能的概述。\n透明的保护 API 能够保护现代应用程序协议，如 REST/HTTP、gRPC 和 Kafka。传统防火墙在三层和四层运行，在特定端口上运行的协议要么完全受信任，要么完全被阻止。Cilium 提供了过滤各个应用程序协议请求的功能，例如：\n允许所有带有方法 GET 和路径 /public/.* 的 HTTP 请求。拒绝所有其他请求。 允许 service1 在 Kafka topic1 主题上生产，service2 消费 topic1。拒绝所有其他 Kafka 消息。 要求 HTTP 标头 X-Token: [0-9]+ 出现在所有 REST 调用中。 详情请参考 七层协议 。\n基于身份来保护服务间通信 现代分布式应用程序依赖于诸如容器之类的技术来促进敏捷性并按需扩展。这将导致在短时间内启动大量应用容器。典型的容器防火墙通过过滤源 IP 地址和目标端口来保护工作负载。这就要求不论在集群中的哪个位置启动容器时都要操作所有服务器上的防火墙。\n为了避免受到规模限制，Cilium 为共享相同安全策略的应用程序容器组分配安全标识。然后，该标识与应用程序容器发出的所有网络数据包相关联，从而允许验证接收节点处的身份。使用键值存储执行安全身份管理。\n安全访问外部服务 基于标签的安全性是集群内部访问控制的首选工具。为了保护对外部服务的访问，支持入口（ingress）和出口（egress）的传统基于 CIDR 的安全策略。这允许限制对应用程序容器的访问以及对特定 IP 范围的访问。\n简单网络 一个简单的扁平三层网络能够跨越多个集群连接所有应用程序容器。使用主机范围分配器可以简化 IP 分配。这意味着每个主机可以在主机之间没有任何协调的情况下分配 IP。\n支持以下多节点网络模型：\nOverlay：基于封装的虚拟网络产生所有主机。目前 VXLAN 和 Geneve 已经完成，但可以启用 Linux 支持的所有封装格式。\n何时使用此模式：此模式具有最小的基础架构和集成要求。它几乎适用于任何网络基础架构，唯一的要求是主机之间可以通过 IP 连接。\n本机路由：使用 Linux 主机的常规路由表。网络必须能够路由应用程序容器的 IP 地址。\n何时使用此模式：此模式适用于高级用户，需要了解底层网络基础结构。此模式适用于：\n本地 IPv6 网络 与云网络路由器配合使用 如果你已经在运行路由守护进程 负载均衡 Cilium 为应用容器之间的流量和外部服务实现了分布式负载均衡，能够完全取代 kube-proxy 等组件。负载均衡是在 eBPF 中使用高效的 hashtables 实现的，允许几乎无限规模扩展。\n对于南北向的负载均衡，Cilium 的 eBPF 实现了最大的性能优化，可以连接到 XDP（eXpress Data Path），并支持直接服务器返回（DSR）以及 Maglev 一致的散列，如果负载均衡操作不在源主机上执行的话。\n对于东西向类型的负载均衡，Cilium 在 Linux 内核的套接字层（如在 TCP 连接时）执行有效的服务到后端转换，这样就可以避免在较低层的每包 NAT 操作开销。\n带宽管理 Cilium 通过高效的基于 EDT（Earlist Departure Time，最早离开时间）的速率限制来实现带宽管理，eBPF 适用于从节点流出的容器的流量。与传统的方法相比，如在 CNI 插件中使用的 HTB（Hierarchy Token Bucket，分层令牌桶）或 TBF（Token Bucket Filter，令牌桶过滤器），这可以大大减少应用的传输尾部延迟，并避免在多队列网卡下锁定。\n监控和故障排除 可视性和故障排查是任何分布式系统运行的基础。虽然我们喜欢用 tcpdump 和 ping，它们很好用，但我们努力为故障排除提供更好的工具。包括以下工具：\n带有元数据的事件监测：当一个数据包被丢弃时，该工具不只是报告数据包的来源和目的地 IP，该工具还提供发送方和接收方的完整标签信息以及许多其他信息。 通过 Prometheus 输出指标。关键指标通过 Prometheus 导出，以便与你现有的仪表盘整合。 Hubble ：一个专门为 Cilium 编写的可观测性平台。它提供服务依赖拓扑、监控和警报，以及基于流量日志的应用和安全可视性。 下一章 ","relpermalink":"/book/cilium-handbook/intro/","summary":"本文将为你简要介绍 Cilium 和 Hubble。 什么是 Cilium？ Cilium 是开源软件，用于透明地保护使用 Docker 和 Kubernetes 等 Linux 容器管理平台部署的应用服务之间的网络连接。 Cilium 的基础是一种新的 Linux 内核技术，称为 eBPF，它使强大的安全可","title":"Cilium 和 Hubble 简介"},{"content":" 注意 《Cilium 中文指南》除特殊说明，默认基于 Cilium 1.11 稳定版本。 《Cilium 中文指南》当前内容译自 Cilium 官方文档 ，节选了以下章节：\n概念：描述了 Cilium 的组件以及部署 Cilium 的不同模式。提供高层次的 运行一个完整的 Cilium 部署并理解其行为所需的高层次理解。 开始：快速开始使用 Cilium。 策略：详细介绍了策略语言结构和支持的格式。 内部原理：介绍了一些组件的内部细节。 其他未翻译部分主要涉及命令、API 及运维，本指南未来会加入笔者个人观点及其他内容。\n大纲 Cilium 和 Hubble 简介\n多集群（集群网格）\nCilium 概念\n网络\n网络安全\neBPF 数据路径\n网络策略\nKubernetes 集成\n开始阅读 ","relpermalink":"/book/cilium-handbook/","summary":"Cilium 中文指南，译自 Cilium 官方文档及博客。","title":"Cilium 中文指南"},{"content":"Cilium 代理（agent）和 Cilium 网络策略的配置决定了一个端点（Endpoint）是否接受来自某个来源的流量。代理可以进入以下三种策略执行模式：\ndefault\n如果任何规则选择了一个 Endpoint 并且该规则有一个入口部分，那么该端点就会在入口处进入默认拒绝状态。如果任何规则选择了一个 Endpoint 并且该规则有一个出口部分，那么该端点就会在出口处进入默认拒绝状态。这意味着端点开始时没有任何限制，一旦有规则限制其在入口处接收流量或在出口处传输流量的能力，那么端点就会进入白名单模式，所有流量都必须明确允许。\nalways\n在 always 模式下，即使没有规则选择特定的端点，也会在所有端点上启用策略执行。如果你想配置健康实体，在启动 cilium-agent 时用 enable-policy=always 检查整个集群的连接性，你很可能想启用与健康端点的通信。\nnever\n在“never\u0026#34; 模式下，即使规则选择了特定的端点，所有端点上的策略执行也被禁用。换句话说，所有流量都允许来自任何来源（入口处）或目的地（出口处）。\n要在运行时为 Cilium 代理管理的所有端点配置策略执行模式，请使用：\n$ cilium config PolicyEnforcement={default,always,never} 如果你想在启动时为一个特定的代理配置策略执行模式，在启动 Cilium 守护程序时提供以下标志：\n$ cilium-agent --enable-policy={default,always,never} [...] 同样，你可以通过在 Cilium DaemonSet 中加入上述参数来启用整个 Kubernetes 集群的策略执行模式：\n- name: CILIUM_ENABLE_POLICY value: always 规则基础知识 所有策略规则都是基于白名单模式，也就是说，策略中的每条规则都允许与该规则相匹配的流量。如果存在两条规则，其中一条可以匹配更广泛的流量，那么所有匹配更广泛规则的流量都将被允许。如果两个或更多的规则之间有一个交叉点，那么与这些规则的结合点相匹配的流量将被允许。最后，如果流量不匹配任何规则，它将根据网络策略执行模式被丢弃。\n策略规则共享一个共同的基本类型，指定规则适用于哪些端点，并共享元数据以识别规则。每条规则都被分割成一个入口部分和一个出口部分。入口部分包含必须应用于进入端点的流量的规则，出口部分包含应用于来自匹配端点选择器的端点的流量的规则。可以提供入口、出口或两者。如果入口和出口都被省略，规则就没有效果。\ntype Rule struct { // EndpointSelector 选择所有应该受此规则约束的端点。 // EndpointSelector 和 NodeSelector 不能同时为空，并且 // 互相排斥。 // // +optional EndpointSelector EndpointSelector `json: \u0026#34;endpointSelector,omitempty\u0026#34;`。 // NodeSelector 选择所有应该受此规则约束的节点。 // EndpointSelector 和 NodeSelector 不能同时为空，并且相互排斥的。 // 只能在 CiliumClusterwideNetworkPolicies 中使用。 // // +optional NodeSelector EndpointSelector `json: \u0026#34;nodeSelector,omitempty\u0026#34;`。 // Ingress 是一个 IngressRule 的列表，它在 Ingress 时被强制执行。 // 如果省略或为空，则此规则不适用于入口处。 // // +optional Ingress []IngressRule `json: \u0026#34;ingress,omitempty\u0026#34;`。 // Egress 是一个在出口处执行的 EgressRule 的列表。 // 如果省略或为空，该规则不适用于出口处。 // // +optional Egress []EgressRule `json: \u0026#34;egress,omitempty\u0026#34;` // Labels 是一个可选的字符串列表，可以用来 // 重新识别该规则或存储元数据。它可以根据标签来查询 // 或删除基于标签的字符串。标签并不要求是 // 唯一的，多个规则可以有重叠的或相同的标签。 // // +optional Labels labels.LabelArray `json: \u0026#34;labels,omitempty\u0026#34;` // Description 是一个自由格式的字符串，它可以由规则的创建者使用。 // 它可以被规则的创建者用来存储该规则目的的可读解释。 // 规则不能通过注释来识别。 // // +optional Description string `json: \u0026#34;description,omitempty\u0026#34;`. } endpointSelector / nodeSelector\n选择策略规则所适用的端点或节点。策略规则将被应用于所有符合选择器中指定标签的端点。\nIngress\n必须在端点入口处适用的规则列表，即适用于进入端点的所有网络数据包。\nEgress\n必须适用于端点出口的规则列表，即适用于离开端点的所有网络数据包。\nLebels\n标签是用来识别规则的。规则可以通过标签列出和删除。通过以下方式导入的策略规则 kubernetes 自动获得 io.cilium.k8s.policy.name=NAME 的标签，其中 NAME 对应的是在 NetworkPolicy 或 CiliumNetworkPolicy 资源中指定的名称。\nDescription\n描述是一个字符串，不被 Cilium 所解释。它可以用来以人类可读的形式描述规则的意图和范围。\n端点选择器 端点选择器基于 Kubernetes LabelSelector 。它之所以被称为端点选择器，是因为它只适用于 Endpoint.\n节点选择器 节点选择器也是基于端点选择器的，不过它不是与端点相关的标签相匹配，而是适用于与集群中的节点相关的标签。\n节点选择器只能用在 CiliumClusterwideNetworkPolicy。请参阅 Host Policies 以了解关于节点级策略范围的详细信息。\n","relpermalink":"/book/cilium-handbook/policy/intro/","summary":"Cilium 代理（agent）和 Cilium 网络策略的配置决定了一个端点（Endpoint）是否接受来自某个来源的流量。代理可以进入以下三种策略执行模式： default 如果任何规则选择了一个 Endpoint 并且该规则有一个入口部分，那么该端点就会","title":"网络策略模式"},{"content":"本文将为你介绍 Cilium 和 Hubble 部署中包含的组件。\nCilium 包含以下组件：\n代理 客户端 Operator CNI 插件 Hubble 包含以下组件：\n服务器 中继器 客户端 图形用户界面 eBPF 另外你还需要一个数据库来存储代理的状态。\n下图展示的 Cilium 部署的组件。\nCilium 代理\nCilium 代理（cilium-agent）在集群的每个节点上运行。在高层次上，代理接受通过 Kubernetes 或 API 的配置，描述网络、服务负载均衡、网络策略、可视性和监控要求。\nCilium 代理监听来自编排系统（如 Kubernetes）的事件，以了解容器或工作负载的启动和停止时间。它管理 eBPF 程序，Linux 内核用它来控制这些容器的所有网络访问。\n客户端（CLI）\nCilium CLI 客户端（cilium）是一个命令行工具，与 Cilium 代理一起安装。它与运行在同一节点上的 Cilium 代理的 REST API 互动。CLI 允许检查本地代理的状态。它还提供工具，直接访问 eBPF map 以验证其状态。\nOperator\nCilium Operator 负责管理集群中的职责，这些职责在逻辑上应该是为整个集群处理一次，而不是为集群中的每个节点处理一次。Cilium Operator 不在任何转发或网络策略决定的关键路径上。如果 Operator 暂时不可用，集群一般会继续运作。然而，根据配置的不同，Operator 的可用性失败可能导致：\nIP 地址管理（IPAM）的延迟，因此，如果 Operator 需要分配新的 IP 地址，那么新工作负载的调度也会延迟 未能更新 kvstore 的心跳密钥，这将导致代理宣布 kvstore 不健康并重新启动。 CNI 插件\nCNI 插件（cilium-cni）由 Kubernetes 在一个节点上调度或终止 pod 时调用。它与节点的 Cilium API 交互，以触发必要的数据通路配置，为 pod 提供网络、负载均衡和网络策略。\nHubble 服务器\nHubble 服务器在每个节点上运行，从 Cilium 检索基于 eBPF 的可视性。它被嵌入到 Cilium 代理中，以实现高性能和低开销。它提供一个 gRPC 服务来检索流量和 Prometheus 指标。\n中继器\n中继器（hubble-relay）是一个独立的组件，它知道所有正在运行的 Hubble 服务器，并通过连接它们各自的 gRPC API 和提供代表集群中所有服务器的 API，提供集群范围内的可视性。\n客户端（CLI）\nHubble CLI（hubble）是一个命令行工具，能够连接到 hubble-relay 的 gRPC API 或本地服务器来检索流量事件。\n图形用户界面（GUI）\n图形用户界面（hubble-ui）利用基于中继的可视性，提供一个图形化的服务依赖性和连接图。\neBPF\neBPF 是一个 Linux 内核字节码解释器，最初是用来过滤网络数据包的，例如 tcpdump 和 socket 过滤器。此后，它被扩展为额外的数据结构，如 hashtable 和数组，以及额外的动作，以支持数据包的处理、转发、封装等。内核验证器确保 eBPF 程序安全运行，JIT 编译器将字节码转换为 CPU 架构的特定指令，以提高本地执行效率。eBPF 程序可以在内核的各种钩点上运行，如传入和传出数据包。\neBPF 继续发展，并在每个新的 Linux 版本中获得额外的功能。Cilium 利用 eBPF 来执行核心数据通路过滤、处理、监控和重定向，并要求 eBPF 的功能在任何 Linux 内核 4.8.0 或更新的版本中。基于 4.8.x 已经宣布终结，4.9.x 已经被提名为稳定版本，我们建议至少运行内核 4.9.17（截至本文撰写时，当前最新的稳定 Linux 内核是 4.10.x）。\nCilium 能够探测到 Linux 内核的可用功能，并在探测到时自动利用更多的最新功能。\n数据存储 Cilium 需要一个数据存储来传播代理之间的状态。它支持以下数据存储：\nKubernetes CRD（默认）\n存储任何数据和传播状态的默认选择是使用 Kubernetes 自定义资源定义（CRD）。CRD 由 Kubernetes 提供，用于集群组件通过 Kubernetes 资源表示配置和状态。\n键值存储\n在 Cilium 的默认配置中配置的 Kubernetes CRD 可以满足状态存储和传播的所有要求。键值存储可以选择作为一种优化，以提高集群的可扩展性，因为直接使用键值存储的变化通知和存储要求更有效率。\n目前支持的键值存储是：\netcd 提示 可以直接利用 Kubernetes 的 etcd 集群或维护一个专门的 etcd 集群。 ","relpermalink":"/book/cilium-handbook/concepts/overview/","summary":"本文将为你介绍 Cilium 和 Hubble 部署中包含的组件。 Cilium 包含以下组件： 代理 客户端 Operator CNI 插件 Hubble 包含以下组件： 服务器 中继器 客户端 图形用户界面 eBPF 另外你还需要一个数据库来存储代理的状态。 下图展示的 Cilium 部署的组件。 Cilium 代理 Cilium 代理（c","title":"组件概览"},{"content":"在过去的几年里，eBPF 已经从相对默默无闻变成了现代基础设施建设中最热门的技术领域之一。就我个人而言，自从看到 Thomas Graf 在 DockerCon 17 的黑带会议 （Black Blet）1 上谈到 eBPF 时，我就对它的可能性感到兴奋。在云原生计算基金会（CNCF），我在技术监督委员会（TOC）的同事把 eBPF 作为我们预测 2021 年将会起飞的重点技术之一来关注。超过 2500 人报名参加了当年的 eBPF 峰会线上会议，世界上最先进的几家软件工程公司共同创建了 eBPF 基金会 。显然，人们对这项技术有很大的兴趣。\n在这个简短的报告中，我希望能给你一些启示，为什么人们对 eBPF 如此兴奋，以及它在现代计算环境中提供的工具能力。你会了解到 eBPF 是什么以及为什么它如此强大。还有一些代码实例，以使这种感觉更加具象化（但如果你愿意，你可以跳过这些）。\n你将了解到在建立支持 eBPF 的工具时涉及的内容，以及为什么 eBPF 在如此短的时间内变得如普遍。\n在这份简短的报告中，难免无法了解所有的细节，但如果你想更深入地了解，我将给你一些参考信息。\n扩展的伯克利数据包过滤器 让我们把缩写说出来：eBPF 代表扩展的伯克利数据包过滤器（Extended Berkeley Packet Filter）。从这个名字中，你可以看到它的根源在于过滤网络数据包，而且最初的 论文 2 是在伯克利实验室（Lawrence Berkeley National Laboratory）写的。但是（在我看来）这个名字对于传达 eBPF 的真正力量并没有很大的帮助，因为\u0026#34;扩展\u0026#34; 版本可以实现比数据包过滤多得多的功能。最近，eBPF 被用作一个独立的名称，它所包含的内容比它的缩写更多。\n那么，如果它不仅仅是包过滤，什么是 eBPF？eBPF 是一个框架，允许用户在操作系统的内核内加载和运行自定义程序。这意味着它可以扩展甚至修改内核的行为。\n当 eBPF 程序被加载到内核中时，有一个验证器确保它是安全运行的，如果无法确认，则拒绝它。一旦加载，eBPF 程序需要被附加到一个事件上，这样，每当事件发生时，程序就会被触发。\neBPF 最初是为 Linux 开发的，这也是我在本报告中重点讨论的操作系统；但值得注意的是，截至本文写作时，微软正在 为 Windows 开发 eBPF 的实现 。\n现在，广泛使用的 Linux 内核都支持 “扩展” 部分，eBPF 和 BPF 这两个术语现在基本上可以互换使用。\n基于 eBPF 的工具 正如你在本报告中所看到的，动态改变内核行为的能力非常有用。传统上，如果我们想观察应用程序的行为，我们在应用程序中添加代码，以产生日志和追踪。eBPF 允许我们收集关于应用程序行为的定制信息，通过在内核中观察它，而不必修改应用程序。我们可以在这种可观测性的基础上创建 eBPF 安全工具，从内核中检测甚至防止恶意活动。我们可以用 eBPF 创建强大的、高性能的网络功能，在内核内处理网络数据包，避免昂贵的用户空间转换。\n从内核的角度来观测应用程序的概念并不新颖——这建立在较早的 Linux 功能之上，比如 perf 3，它也从内核内部收集行为和性能信息，而不需要修改被测量的应用程序。但是这些工具定义了可以收集的数据种类以及数据的格式。有了 eBPF，我们就有了更大的灵活性，因为我们可以编写完全自定义的程序，允许我们出于不同的目的建立广泛的工具。\neBPF 编程的能力异常强大，但也很复杂。对于我们大多数人来说，eBPF 的效用不是来自于自己写的程序，而是来自于使用别人创造的工具。有越来越多的项目和供应商在 eBPF 平台上创建了新一代的工具，包括可观测性、安全性、网络等。\n我将在本报告后面讨论这些更高级别的工具，但是，如果你对 Linux 命令行很熟悉，迫不及待地想看到 eBPF 的运行，推荐你从 BCC 项目开始。BCC 包含丰富的追踪工具集合；即使只是瞥一眼列表也可以领略到 eBPF 广泛的应用，包括文件操作、内存使用、CPU 统计，甚至观察在系统任意位置输入的 bash 命令。\n在下一章，我们将介绍改变内核行为的作用，为什么使用 eBPF 比直接编写内核代码更容易操作。\n参考 Thomas Graf，《Cilium——使用 BPF 和 XDP 的网络和应用安全》（DockerCon 2017 年 4 月 17-20 日） ↩︎\nSteven McCanne 和 Van Jacobson，《BSD 数据包过滤器：用户级数据包捕获的新架构》（工作文件，劳伦斯伯克利国家实验室，伯克利，1992 年 12 月 19 日）。 ↩︎\nperf 是一个 Linux 子系统，用于收集性能数据。 ↩︎\n","relpermalink":"/book/what-is-ebpf/introduction/","summary":"在过去的几年里，eBPF 已经从相对默默无闻变成了现代基础设施建设中最热门的技术领域之一。就我个人而言，自从看到 Thomas Graf 在 DockerCon 17 的黑带会议 （Black Blet）1 上谈到 eBPF 时，我就对它的可能性感到兴奋。在云原生","title":"第一章：eBPF 简介"},{"content":"《什么是 eBPF —— 新一代网络、安全和可观测性工具介绍》译自 O’Reilly 发布的报告“What is eBPF”，作者是 Liz Rice，由 JImmy Song 翻译，英文原版可以在 O’Reilly 网站 上获取。\n《什么是 eBPF》中文版封面 译者序 最近两年来关于 eBPF 的讨论在云原生社区里越来越多，尤其是当谈到 Cilium 的商业化，使用 eBPF 来优化 Istio 服务网格，甚至扬言干掉 Sidecar 时，eBPF 更是赚足了眼球。\n这本报告是由基于 Cilium 的创业公司 Isovalent 的 Liz Rice 撰写，由 O’Reilly 发布，相信可以为你揭开 eBPF 技术的神秘面纱，带你了解什么是 eBPF 还有它的强大之处。更重要的是它在云原生环境中，在服务网格、可观测性和安全中的应用。\n关于作者 Liz Rice 是云原生网络和安全专家，Isovalent 的首席开源官，是基于 eBPF 的 Cilium 网络项目的创建者。她在 2019-2022 年担任 CNCF 的技术监督委员会（TOC）主席，并在 2018 年担任 KubeCon + CloudNativeCon 的联合主席。她也是 Container Security 一书的作者，由 O\u0026#39;Reilly 出版。她拥有丰富的软件开发、团队和产品管理经验，曾在网络协议和分布式系统以及数字技术领域（如 VOD、音乐和 VoIP）工作。在不写代码的时候，Liz 喜欢在天气比她的家乡伦敦好的地方骑自行车，和在 Zwift 上参加虚拟比赛。\n本书大纲 第一章：eBPF 简介\n第二章：修改内核很困难\n第三章：eBPF 程序\n第四章：eBPF 的复杂性\n第五章：云原生环境中的 eBPF\n第六章：eBPF 工具\n第七章：结论\n开始阅读 ","relpermalink":"/book/what-is-ebpf/","summary":"新一代网络、安全和可观测性工具简介。","title":"什么是 eBPF？"},{"content":"Kubernetes Hardening Guidance（查看英文原版 PDF ）是由美国国家安全局（NSA）于 2021 年 8 月发布的，其中文版《Kubernetes 加固指南》（或译作《Kubernetes 强化指南》），译者 Jimmy Song 。\n《Kubernetes 加固指南》封面 许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区 可观测性讨论组（微信群）参与讨论交流，加入前请先填写入群申请问卷 后联系 Jimmy Song 入群。\n开始阅读 ","relpermalink":"/book/kubernetes-hardening-guidance/","summary":"本指南译自美国国家安全局（NSA）于 2021 年 8 月发布的的 Kubernetes Hardening Guidance。","title":"Kubernetes 加固指南"},{"content":"首先我们来阐述下将应用迁移到云原生架构的动机。\n速度 天下武功，唯快不破，市场竞争亦是如此。想象一下，能够快速创新、实验并交付软件的企业，与使用传统软件交付模式的企业，谁将在市场竞争中胜出呢？\n在传统企业中，为应用提供环境和部署新版本花费的时间通常以天、周或月来计算。这种速度严重限制了每个发行版可以承担的风险，因为修复这些错误往往跟发行一个新版本有差不多的耗时。\n互联网公司经常提到它们每天几百次发布的实践。为什么频繁发布如此重要？如果你可以每天实现几百次发布，你们就可以几乎立即从错误的版本恢复过来。如果你可以立即从错误中恢复过来，你就能够承受更多的风险。如果你可以承受更多的风险，你就可以做更疯狂的试验 —— 这些试验结果可能会成为你接下来的竞争优势。\n基于云基础设置的弹性和自服务的特性天生就适应于这种工作方式。通过调用云服务 API 来提供新的应用程序环境比基于表单的手动过程要快几个数量级。然后通过另一个 API 调用将代码部署到新的环境中。将自服务和 hook 添加到团队的 CI/CD 服务器环境中进一步加快了速度。现在，我们可以回答精益大师 Mary Poppendick 提出的问题了 ——“如果只是改变了应用的一行代码，您的组织需要多长时间才能把应用部署到线上？“答案是几分钟或几秒钟。\n你可以大胆想象 一下，如果你也可以达到这样的速度，你的团队、你的业务可以做哪些事情呢？\n安全 光是速度快还是不够的。如果你开车是一开始就把油门踩到底，你将因此发生事故而付出惨痛的代价（有时甚至是致命的）！不同的交通方式如飞机和特快列车都会兼顾速度和安全性。云原生应用架构在快速变动的需求、稳定性、可用性和耐久性之间寻求平衡。这是可能的而且非常有必要同时实现的。\n我们前面已经提到过，云原生应用架构可以让我们迅速地从错误中恢复。我们没有谈论如何预防错误，而在企业里往往在这一点上花费了大量的时间。在追寻速度的路上，大而全的前端升级，详尽的文档，架构复核委员会和漫长的回归测试周期在一次次成为我们的绊脚石。当然，之所以这样做都是出于好意。不幸的是，所有这些做法都不能提供一致可衡量的生产缺陷改善度量。\n那么我们如何才能做到即安全又快速呢？\n可视化\n我们的架构必须为我们提供必要的工具，以便可以在发生故障时看到它。我们需要观测一切的能力，建立一个“哪些是正常”的概况，检测与标准情况的偏差（包括绝对值和变化率），并确定哪些组件导致了这些偏差。功能丰富的指标、监控、警报、数据可视化框架和工具是所有云原生应用架构的核心。\n故障隔离\n为了限制与故障带来的风险，我们需要限制可能受到故障影响的组件或功能的范围。如果每次亚马逊的推荐引擎挂掉后人们就不能再在亚马逊上买产品，那将是灾难性的。单体架构通常就是这种类型的故障模式。云原生应用架构通常使用微服务。通过将系统拆解为微服务，我们可以将任何一个微服务的故障范围限制在这个微服务上，但还需要结合容错才能实现这一点。\n容错\n仅仅将系统拆解为可以独立部署的微服务还是不够的；还需要防止出现错误的组件将错误传递它所依赖的组件上而造成级联故障。Mike Nygard 在他的《Release It! - Pragmatic Programmers》一书中描述了一些容错模型，最受欢迎的是断路器。软件断路器的工作原理就类似于电子断路器（保险丝）：断开它所保护的组件与故障系统之间的回路以防止级联故障。它还可以提供一个优雅的回退行为，比如回路断开的时候提供一组默认的产品推荐。我们将在“容错”一节详细讨论该模型。\n自动恢复\n凭借可视化、故障隔离和容错能力，我们拥有确定故障所需的工具，从故障中恢复，并在进行错误检测和故障恢复的过程中为客户提供合理的服务水平。一些故障很容易识别：它们在每次发生时呈现出相同的易于检测的模式。以服务健康检查为例，结果只有两个：健康或不健康，up 或 down。很多时候，每次遇到这样的故障时，我们都会采取相同的行动。在健康检查失败的情况下，我们通常只需重新启动或重新部署相关服务。云原生应用架构不要当应用在这些情况下无需手动干预。相反，他们会自动检测和恢复。换句话说，他们给电脑装上了寻呼机而不是人。\n弹性扩展 随着需求的增加，我们必须扩大服务能力。过去我们通过垂直扩展来处理更多的需求：购买了更强悍的服务器。我们最终实现了自己的目标，但是步伐太慢，并且产生了更多的花费。这导致了基于高峰使用预测的容量规划。我们会问”这项服务需要多大的计算能力？”然后购买足够的硬件来满足这个要求。很多时候我们依然会判断错误，会在如黑色星期五这类事件中打破我们的可用容量规划。但是，更多的时候，我们将会遇到数以百计的服务器，它们的 CPU 都是空闲的，这会让资源使用率指标很难看。\n创新型的公司通过以下两个开创性的举措来解决这个问题：\n它们不再继续购买更大型的服务器，取而代之的是用大量的更便宜机器来水平扩展应用实例。这些机器更容易获得，并且能够快速部署。 通过将大型服务器虚拟化成几个较小的服务器，并向其部署多个隔离的工作负载来改善现有大型服务器的资源利用率。 随着像亚马逊 AWS 这样的公有云基础设施的出现，这两个举措融合了起来。虚拟化工作被委托给云提供商，消费者只需要关注在大量的云服务器实例横向扩展它们的应用程序实例。最近，作为应用程序部署的单元，发生了另一个转变，从虚拟机转移到了容器。\n由于公司不再需要大量启动资金来部署软件，所以向云的转变打开了更多创新之门。正在进行的维护还需要较少的资本投入，并且通过 API 进行配置不仅可以提高初始部署的速度，还可以最大限度地提高我们应对需求变化的速度。\n不幸的是，所有这些好处都带有成本。相较于垂直扩展的应用，支持水平扩展的应用程序的架构必须不同。云的弹性要求应用程序的状态短暂性。我们不仅可以快速创建新的应用实例；我们也必须能够快速、安全地处置它们。这种需求是状态管理的问题：一次性与持久性如何互相影响？在大多数垂直架构中采用的诸如聚类会话和共享文件系统的传统方法并不能很好地支持水平扩展。\n云原生应用架构的另一个标志是将状态外部化到内存数据网格、缓存和持久对象存储，同时保持应用程序实例本身基本上是无状态的。无状态应用程序可以快速创建和销毁，以及附加到外部状态管理器和脱离外部状态管理器，增强我们响应需求变化的能力。当然这也需要外部状态管理器自己来扩展。大多数云基础设施提供商已经认识到这一必要性，并提供了这类服务的健康管理。\n移动应用和客户端多样性 2014 年 1 月，美国移动设备占互联网使用量的 55％。专门针对桌面用户而开发的应用程序的时代已经过去。不过，我们必须假设用户装在口袋里到处散步的是超级计算机。这对我们的应用架构有很大的影响，因为指数级用户可以随时随地与我们的系统进行交互。\n以查看银行账户余额为例。这项任务过去是通过拨打银行的呼叫中心，前往 ATM，或者在银行的一个分支机构的向柜员请求完成的。这些客户互动模式在任何时间内，都会对银行为底层软件系统提出新需求产生极大的限制。\n迁移到网上银行导致访问量的上升，但并没有从根本上改变交互模式。您仍然必须在计算机终端上与系统进行交互，这仍然显著限制了需求。正如我的同事 Andrew Clay Shafer 经常说的那样，“我们口袋里正带着超级计算机到处游走”，我们开始对这些系统带来很大负载。现在，成千上万的客户可以随时随地与银行系统进行互动。一位银行行政人员表示，在发薪日，客户会每隔几分钟检查一次余额。遗留的银行系统架构根本无法满足这种需求，而云原生的应用程序体系结构却可以。\n移动平台的巨大差异也对应用架构提出了要求。客户随时都可能与多个不同供应商生产的设备，运行多个不同的操作系统平台，运行多个版本的相同操作平台以及不同类别的设备（例如手机与平板电脑）进行交互。这不仅对移动应用程序开发人员，还对后端服务的开发人员造成了各种限制。\n移动应用程序通常必须与多个传统系统以及云原生应用架构中的多个微服务进行交互。这些服务无法设计成支持客户使用的各种各样移动平台的独特需求。强迫实现这些不同的服务，为移动应用程序开发人员上带来了负担，增加了应用访问延迟和网络访问频率，导致应用响应慢、耗电量高，最终导致用户删除您的应用程序。云原生应用架构还通过诸如 API 网关之类的设计模式来支持移动优先开发的概念，API 网关将服务聚合负担转移回服务器端。\n","relpermalink":"/book/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/why-cloud-native-application-architectures/","summary":"首先我们来阐述下将应用迁移到云原生架构的动机。 速度 天下武功，唯快不破，市场竞争亦是如此。想象一下，能够快速创新、实验并交付软件的企业，与使用传统软件交付模式的企业，谁将在市场竞争中胜出呢？ 在传统企业中","title":"1.1 为何使用云原生应用架构"},{"content":"由于微服务通常是以容器的形式实现的，因此容器编排和资源管理平台被用于服务的部署、运维和维护。\n一个典型的协调和资源管理平台由各种逻辑（形成抽象层）和物理工件组成，用于部署容器。例如，在 Kubernetes 中，容器在最小的部署单元内运行，称为 Pod。一个 Pod 理论上可以承载一组容器，但通常情况下，一个 Pod 内只运行一个容器。一组 Pod 被定义在所谓的节点内，节点可以是物理机或虚拟机（VM）。一组节点构成了一个集群。通常情况下，需要单个微服务的多个实例来分配工作负载，以达到预期的性能水平。集群是一个资源池（节点），用于分配微服务的工作负载。使用的技术之一是横向扩展，即访问频率较高的微服务被分配更多的实例或分配到具有更多资源（如 CPU 和 / 或内存）的节点。\n","relpermalink":"/book/service-mesh-devsecops/reference-platform/container-orchestration-and-resource-management-platform/","summary":"由于微服务通常是以容器的形式实现的，因此容器编排和资源管理平台被用于服务的部署、运维和维护。 一个典型的协调和资源管理平台由各种逻辑（形成抽象层）和物理工件组成，用于部署容器。例如，在 Kubernetes 中，容器在最小的","title":"2.1 容器编排和资源管理平台"},{"content":"企业 IT 采用云原生架构所需的变革根本不是技术性的，而是企业文化和组织的变革，围绕消除造成浪费的结构、流程和活动。在本节中，我们将研究必要的文化转变。\n从信息孤岛到 DevOps 企业 IT 通常被组织成以下许多孤岛：\n软件开发 质量保证 数据库管理 系统管理 IT 运营 发布管理 项目管理 创建这些孤岛是为了让那些了解特定领域的人员来管理和指导那些执行该专业领域工作的人员。这些孤岛通常具有不同的管理层次，工具集、沟通风格、词汇表和激励结构。这些差异启发了企业 IT 目标的不同范式，以及如何实现这一目标。\n但这里面存在很多矛盾，例如开发和运维分别对软件变更持有的观念就是个经常被提起的例子。开发的任务通常被视为通过开发软件功能为组织提供额外的价值。这些功能本身就是向 IT 生态系统引入变更。所以开发的使命可以被描述为“交付变化”，而且经常根据有多少次变更来进行激励。\n相反，IT 运营的使命可以被描述为“防止变更”。IT 运营通常负责维护 IT 系统所需的可用性、弹性、性能和耐用性。因此，他们经常以维持关键绩效指标（KPI）来进行激励，例如平均故障间隔时间（MTBF）和平均恢复时间（MTTR）。与这些措施相关的主要风险因素之一是在系统中引入任何类型的变更。那么，不是设法将开发期望的变更安全地引入 IT 生态系统，而是通过将流程放在一起，使变更变得痛苦，从而降低了变化率。\n这些不同的范式显然导致了许多额外的工作。项目工作中的协作、沟通和简单的交接变得乏味和痛苦，最糟糕的是导致绝对混乱（甚至是危险的）。企业 IT 通常通过创建基于单据的系统和委员会会议驱动的复杂流程来尝试“修复”这种情况。企业 IT 价值流在所有非增值浪费下步履瞒珊。\n像这样的环境与云原生的速度思想背道而驰。专业的信息孤岛和流程往往是由创造安全环境的愿望所驱动。然而，他们通常提供很少的附加安全性，在某些情况下，会使事情变得更糟！\n在其核心上，DevOps 代表着这样一种思想，即将这些信息孤岛构建成共享的工具集、词汇表和沟通结构，以服务于专注于单一目标的文化：快速、安全得交付价值。然后创建激励结构，强制和奖励领导组织朝着这一目标迈进的行为。官僚主义和流程被信任和责任所取代。\n在这个新的世界中，开发和 IT 运营部门向共同的直接领导者汇报，并进行合作，寻找能够持续提供价值并获得期望的可用性、弹性、性能和耐久性水平的实践。今天，这些对背景敏感的做法越来越多地包括采用云原生应用架构，提供完成组织的新的共同目标所需的技术支持。\n从间断均衡到持续交付 企业经常采用敏捷流程，如 Scrum，但是只能作为开发团队内部的本地优化。\n在这个行业中，我们实际上已经成功地将个别开发团队转变为更灵活的工作方式。我们可以这样开始项目，撰写用户故事，并执行敏捷开发的所有例程，如迭代计划会议，日常站会，回顾和客户展示 demo。我们中的冒险者甚至可能会冒险进行工程实践，如结对编程和测试驱动开发。持续集成，这在以前是一个相当激进的概念，现在已经成为企业软件词典的标准组成部分。事实上，我已经是几个企业软件团队中的一部分，并建立了高度优化的“故事到演示”周期，每个开发迭代的结果在客户演示期间被热烈接受。\n但是，这些团队会遇到可怕的问题：我们什么时候可以在生产环境中看到这些功能？\n这个问题我们很难回答，因为它迫使我们考虑自己无法控制的力量：\n我们需要多长时间才能浏览独立的质量保证流程？ 我们什么时候可以加入生产发布的行列中？ 我们可以让 IT 运营及时为我们提供生产环境吗？ 在这一点上，我们意识到自己已经陷入了戴维・韦斯特哈斯（Dave Westhas）所说的 scrum 瀑布中了。我们的团队已经开始接受敏捷原则，但我们的组织却没有。所以，不是每次迭代产生一次生产部署（这是敏捷宣言的原始出发点），代码实际上是批量参与一个更传统的下游发布周期。\n这种操作风格产生直接的后果。我们不是每次迭代都将价值交付给客户，并将有价值的反馈回到开发团队，我们继续保持“间断均衡”的交付方式。间断均衡实际上丧失了敏捷交付的两个主要优点：\n客户可能需要几周的时间才能看到软件带来的新价值。他们认为，这种新的敏捷工作方式只是“像往常一样”，不会增强对开发团队的信任。因为他们没有看到可靠的交付节奏，他们回到了以前的套路将尽可能多的要求尽可能多地堆放到发布版上。为什么？因为他们对软件能够很快发布没有信心，他们希望尽可能多的价值被包括在最终交付时。 开发团队可能会好几周都没有得到真正的反馈。虽然演示很棒，但任何经验丰富的开发人员都知道，只有真实用户参与到软件之中才能获得最佳反馈。这些反馈能够帮助软件修正，使团队去做正确的事情。反馈推迟后，错误的可能性只能增加，并带来昂贵的返工。 获得云原生应用架构的好处需要我们转变为持续交付。我们拥抱端到端拥抱价值的原则，而不是 Water Scrum Fall 组织驱动的间断平衡。设想这样一个生命周期的模型是由 Mary 和 Tom Poppendieck 在《实施精益软件开发（Addison-Wesley）》一书中描述的“概念到现金”的想法中提出来的。这种方法考虑了所有必要的活动，将业务想法从概念传递到创造利润的角度，并构建可以使人们和过程达到最佳目标的价值流。\n我们技术上支持这种使用连续交付的工程实践的方法，每次迭代（实际上是次每个源代码提交！）都被证明可以以自动化的方式部署。我们构建部署流水线，可自动执行每次测试，如果该测试失败，将会阻止生产部署。唯一剩下的决定是商业决策：现在部署可用的新功能有很好的业务意义吗？我们已经知道它已经如广告中的方式工作，但是我们要现在就把它们交给客户吗？因为部署管道是完全自动化的，所以企业能够通过点击按钮来决定是否采取行动。\n从集中治理到分散自治 Waterscrumfall 文化中的一部分已经被特别提及，因为它已经被视为云原生架构采纳的一个关键。\n企业通常采用围绕应用架构和数据管理的集中治理结构，负责维护指导方针和标准的委员会，以及批准个人设计和变更。集中治理旨在帮助解决以下几个问题：\n可以防止技术栈的大范围不一致，降低组织的整体维护负担。 可以防止架构选型中的大范围不一致，从而形成组织的应用程序开发的共同观点。 整个组织可以一致地处理跨部门关切，例如合规性。 数据所有权可由具有全局视野的人来决定。 之所以创造这些结构，是因为我们相信它们将有助于提高质量、降低成本或两者兼而有之。然而，这些结构很少能够帮助我们提高质量节约成本，并且进一步妨碍了云原生应用架构寻求的交付速度。正如单体应用架构导致了限制技术创新速度的瓶颈一样，单一的治理结构同样如此。架构委员会经常只会定期召集，并且经常需要很长的等待时才能发挥工作。即使是很小的数据模型的变化 —— 可能在几分钟或几个小时内完成的更改，即将被委员会批准的变更 —— 将会把时间浪费在一个不断增长的待办事项中。\n采用云原生应用架构时通常都会与分散式治理结合起来。建立云原生应用的团队拥有他们负责交付的能力的所有方面。他们拥有和管理数据、技术栈、应用架构、每个组件设计和 API 协议并将它们交付给组织的其余部分。如果需要对某事作出决策，则由团队自主制定和执行。\n团队个体的分散自治和自主性是通过最小化、轻量级的结构进行平衡的，这些结构在可独立开发和部署的服务之间使用集成模式（例如，他们更喜欢 HTTP REST JSON API 而不是不同风格的 RPC）来实现。这些结构通常会在底层解决交叉问题，如容错。激励团队自己设法解决这些问题，然后自发组织与其他团队一起建立共同的模式和框架。随着整个组织中的最优解决方案出现，该解决方案的所有权通常被转移到云框架 / 工具团队，这可能嵌入到平台运营团队中也可能不会。当组织正在围绕对架构共识进行改革时，云框架 / 工具团队通常也将开创解决方案。\n","relpermalink":"/book/migrating-to-cloud-native-application-architectures/changes-needed/cultural-change/","summary":"企业 IT 采用云原生架构所需的变革根本不是技术性的，而是企业文化和组织的变革，围绕消除造成浪费的结构、流程和活动。在本节中，我们将研究必要的文化转变。 从信息孤岛到 DevOps 企业 IT 通常被组织成以下许多孤岛： 软件开发","title":"2.1 文化变革"},{"content":"在和客户讨论分解数据、服务和团队后，客户经常向我提出这样的问题，“太棒了！但是我们要怎样实现呢？”这是个好问题。如何拆分已有的单体应用并把他们迁移上云呢？\n事实证明，我已经看到了很多成功的例子，使用增量迁移这种相当可复制的模式，我现在向我所有的客户推荐这种模式。SoundCloud 和 Karma 就是公开的例子。\n本节中，我们将讲解如何一步步地将单体服务分解并将它们迁移到云上。\n新功能使用微服务形式 您可能感到很惊奇，第一步不是分解单体应用。我们假设您依然要在单体应用中构建服务。事实上，如果您没有任何新的功能来构建，那么您甚至不应该考虑这个分解。（鉴于我们的主要动机是速度，您如何维持原状还能获取速度呢？）\n团队决定，处理架构变化的最佳方法不是立即分解 Mothership 架构，而是不添加任何新的东西。我们所有的新功能以微服务形式构建…\n——Phil Calcado, SoundCloud\n所以不要继续再向单体应用中增加代码，将所有的新功能以微服务的形式构建。这是第一步就要考虑好的，因为从头开始构架一个服务比分解一个单体应用并提出服务出来容易和快速的多。\n然而有一点不可避免，就是新构建的微服务需要与已有的单体应用通信才能完成工作，这个问题怎么解决？\n隔离层 因为我们大部分的业务逻辑都是基于 Rails 的单体应用，所以我们的微服务基本也要跟它们通信。\n——Phil Calcado, SoundCloud\nEric Evans（Addison-Wesley）的领域驱动设计（DDD）讨论了隔离层的思想。其目的是允许两个系统的集成，而不允许一个系统的领域模型破坏另一个系统的领域模型。当您将新功能集成到微服务中时，不希望这些新服务与整体的紧密结合，让他们深入了解整体的内部结构。隔离层是创建 API 协议的一种方式，使得整体架构看起来像其他微服务。\nEvans 将隔离层的实施划分为三个子模块，前两个代表着经典设计模式。\n（来自 Gamma 等人，Design Patterns：Elements of Reusable Object-Oriented So ware [Addison Wesley]）：\n表现层\n表现层的目的是为了简化与单体应用接口集成的过程。单体应用设计之初很可能没有考虑这个集成，因此我们引入了表现层来解决这个问题。它没有改变单体应用的模型，这很重要，注意不要将转换和集成问题耦合到一起。\n适配器\n我们用适配器来定义 service，用来提供我们需要的新功能。它知道如何获取系统请求并使用协议将请求发送给单体应用的表层。\n转换器\n转换器的职责是在单体应用与新的微服务之间进行请求和响应的领域模型转换。\n这三个松耦合的组件解决了以下三个问题：\n系统集成 协议转换 模型转换 剩下的是通信链路的位置。在 DDD 中，Evans 讨论了两种选择。当您无法访问或更改遗留系统时，第一，将系统的表现层设置为主要功能。我们的重点在于我们控制的整体，所以我们将倾向于 Evans 的第二个建议，适配器到表现层。使用这种替代方法，我们将表现层构筑到单体中，允许在适配器和表现层之间进行通信，因为在为此专门构建的组件之间创建连接更容易。\n最后，要注意隔离层可以促进双向通信。正如我们新的微服务可能需要与整体进行通信以完成工作一样，反之亦然，特别是当我们进入下一阶段时。\n扼杀单体应用 在架构调整后，我们的团队可以在更加灵活的环境中自由构建新功能和增强功能。然而，一个重要的问题仍然存在：我们如何从名为 Mothership 的单体 Rails 应用程序中提取功能？\n——Pilil Calcado，SoundCloud\n我从 Martin Fowler 的题为“扼杀应用”的文章中借用了“扼杀巨石”的想法。在这篇文章中，Fowler 解释了逐渐创造“围绕旧系统边缘的新系统，让它几年来慢慢增长，直到旧系统被扼杀”的想法。这种情况同样适用于我们。通过提取的微服务和其他隔离层的组合，我们将围绕现有单体的边缘构建一个新的云原生系统。\n两个标准帮助我们选择要提取哪些组件：\nSoundCloud 指出了第一个标准：识别单体中的有界上下文。如果您回想起我们之前讨论有限上下文，它需要一个内部一致的领域模型。我们的单体领域模型极有可能不是内部一致的。现在是开始识别子模型的时候了，里面有我们要提取的候选者。 第二个标准是优先考虑的：在众多的候选者中我们应该首先提取哪一个呢？我们可以回顾一下迁移到云原生架构的第一个原因：创新速度。什么候选微服务将最受益于创新速度？我们显然希望选择那些正在改变我们当前业务需求的服务。看看单体应用的积压。确定需要更改的代码的区域，以便提交更改的要求，然后在进行所需更改之前提取适当的有界上下文。 潜在的结束状态 我们怎么知道何时结束？下面有两个基本的结束状态：\n单体架构已经被完全扼杀。所有的有界上下文都被提取为微服务。最后一步是确定消除不再需要隔离层的机会。 单体架构被扼杀到了这样一个点：额外服务提取的成本超过必要开发努力的回报。单体的一些部分可能相当稳定 —— 它们几年来都没有改变，还是一直都在运行得好好的。迁移这些部分可能没有太大的价值，维持必要的隔离层与其集成的成本足够低，我们可以长期负担。 ","relpermalink":"/book/migrating-to-cloud-native-application-architectures/migration-cookbook/decomposition-recipes/","summary":"在和客户讨论分解数据、服务和团队后，客户经常向我提出这样的问题，“太棒了！但是我们要怎样实现呢？”这是个好问题。如何拆分已有的单体应用并把他们迁移上云呢？ 事实证明，我已经看到了很多成功的例子，使用增量","title":"3.1 分解架构"},{"content":"DevSecOps 是一种软件开发、部署和生命周期管理方法，它涉及到从整个应用程序或平台的一次大型发布转变为持续集成、持续交付和持续部署（CI/CD）方法。这种转变又要求公司的 IT 部门的结构和工作流程发生变化。最明显的变化是组织一个 DevSecOps 小组，由软件开发人员、安全专家和 IT 运维专家组成，负责应用程序（即微服务）的每一部分。这个较小的团队不仅能促进最初的敏捷开发和部署的效率和效果，还能促进后续的生命周期管理活动，如监控应用行为、开发补丁、修复错误或扩展应用。这种具有三个领域专业知识的跨职能团队的组成，构成了在组织中引入 DevSecOps 的关键成功因素。\n","relpermalink":"/book/service-mesh-devsecops/devsecops/organizational-preparedness-for-devsecops/","summary":"DevSecOps 是一种软件开发、部署和生命周期管理方法，它涉及到从整个应用程序或平台的一次大型发布转变为持续集成、持续交付和持续部署（CI/CD）方法。这种转变又要求公司的 IT 部门的结构和工作流程发生变化。最明显的变","title":"3.1 组织对 DevSecOps 的准备情况"},{"content":"对上述五类代码（即应用、应用服务、基础设施、策略和监控）的简要描述如下：\n应用程序代码和应用服务代码：前者包含一组特定业务事务的数据和应用逻辑，而后者包含所有服务的代码，如网络连接、负载均衡和网络弹性。 基础设施即代码（IaC）：用于提供和配置基础设施资源的代码，它以可重复和一致的方式承载 应用程序的部署 。这种代码是用一种声明性语言编写的，当执行时，为正在部署的应用程序提供和配置基础设施。这种类型的代码就像在应用程序的微服务中发现的任何其他代码，只是它提供的是基础设施服务（例如，配置服务器）而不是事务服务（例如，在线零售应用程序的支付处理）。 策略即代码：描述了许多策略，包括安全策略，作为 可执行模块 。一个例子是授权策略，它的代码包含了策略（如允许、拒绝等）和适用领域（如 RESTAPI 的方法，GET、PUT 等，路径等动词或工件）这段代码可以用特殊用途的策略语言（如 Rego）或常规应用中使用的语言（如 Go）编写。这段代码可能与 IaC 的配置代码有一些重合。然而，对于实施与特定于应用领域的关键安全服务相关的策略，需要一个单独的策略作为代码，驻留在参考平台的策略执行点（PEP）中。 可观测性即代码：推断系统内部状态的能力，并对系统内何时以及更重要的是为何发生错误提供可操作的洞察力。它是一种全栈式的可观测性，包括监测和分析，并对应用程序和承载它们的系统的整体性能提供关键的洞察力。在参考平台的背景下，可观测性即代码是指在代理中创建机构的那部分代码，并为从微服务应用中收集三种类型的数据（即日志、跟踪和遥测）创建功能 。这类代码还向外部工具提供或传输数据（例如，日志聚合工具，它聚合来自单个微服务的日志数据，为瓶颈服务提供跟踪数据分析，从遥测数据生成反映应用健康状况的指标等）。以下描述了对作为代码的可观测性所实现的三种功能： 日志捕获详细的错误信息，以及用于故障排除的调试日志和堆栈跟踪。 追踪应用程序的请求，因为它们通过多个微服务来完成一项事务，以确定分布式或基于微服务的生态系统中的问题或性能瓶颈。 监测，或称度量，收集遥测从应用程序和服务中收集数据。 每种代码类型都有相关的 CI/CD 管道，并在第 4.2 到 4.5 节中进行了描述。应用服务代码、基础设施即代码、策略即代码和可观测性即代码类型之间可能存在重叠。\n托管这五种代码类型的参考平台的组成成分是：\n业务功能组件（由几个微服务模块组成，每个模块通常作为一个容器实现），体现了应用逻辑（例如，与数据交互，执行事务等），从而形成应用代码。 基础设施组件（包含计算机、网络和存储资源），其成员可以使用基础设施即代码进行配置。 服务网格组件（通过控制面模块和服务代理的组合实现），提供应用服务，执行策略（例如，认证和授权），并包含应用服务代码和策略作为代码。 监测组件（参与确定表明应用程序健康状况的参数的模块），执行功能（例如，日志聚合、生成指标、生成仪表板的显示等）并包含作为代码的可观测性。 策略和可观测性代码类型在服务网格中的分布情况如下：\n代理组件（入口、sidecar 和出口）。这些组件容纳了与会话建立、路由、认证和授权功能有关的编码策略。 服务网格的控制平面。这里面有一些代码，用于转发来自服务的遥测信息，并由代理发送至专门的监控工具，认证证书的生成和维护，更新代理机构中的策略，监控服务协调平台中的整体配置，以生成新的代理，并删除与停用的微服务相关的过时代理。 外部模块。这些内部模块在应用和企业层面执行专门的功能（例如，如集中授权或权利服务器、集中记录器、通过仪表板监测 / 提醒服务器状态等），并建立一个全面的应用状态视图。这些模块由来自代理或控制平面的代码调用。 ","relpermalink":"/book/service-mesh-devsecops/implement/description-of-code-types-and-reference-platform-components/","summary":"对上述五类代码（即应用、应用服务、基础设施、策略和监控）的简要描述如下： 应用程序代码和应用服务代码：前者包含一组特定业务事务的数据和应用逻辑，而后者包含所有服务的代码，如网络连接、负载均衡和网络弹性。","title":"4.1 代码类型和参考平台组件的描述"},{"content":" 软件正在吞噬世界。\n——Mark Andreessen\n近些年来，在一些长期由领导者支配的行业中，这些领导者的领先地位已经岌岌可危，这都是由以这些行业为核心业务的软件公司造成的。像 Square、Uber、Netflix、Airbnb 和特斯拉这样的公司能够持续快速增长，并且拥有傲人的市场估值，成为它们所在行业的新领导者。这些创新公司有什么共同点？\n快速创新\n持续可用的服务\n弹性可扩展的 Web\n以移动为核心的用户体验\n将软件迁移到云上是一种自演化，使用了云原生应用架构是这些公司能够如此具有破坏性的核心原因。对于云，我们指的是一个任何能够按需、自助弹性提供和释放计算、网络和存储资源的计算环境。云的定义包括公有云（例如 Amazon Web Services、Google Cloud 和 Microsoft Azure）和私有云（例如 VMware vSphere 和 OpenStack）。\n本章中我们将探讨云原生应用架构的创新性，然后验证云原生应用架构的主要特性。\n开始阅读 ","relpermalink":"/book/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/","summary":"软件正在吞噬世界。 ——Mark Andreessen 近些年来，在一些长期由领导者支配的行业中，这些领导者的领先地位已经岌岌可危，这都是由以这些行业为核心业务的软件公司造成的。像 Square、Uber、Netflix、Ai","title":"第一章：云原生的崛起"},{"content":"Google 有许多通用工程实践，几乎涵盖所有语言和项目。此文档为长期积累的最佳实践，是集体经验的结晶。我们尽可能地将其公之于众，您的组织和开源项目也会从中受益。\n《谷歌工程实践》封面 当前包含以下文档：\nGoogle 代码审查指南，实则两套指南：\n代码审查者指南 代码开发者指南 译者序 此仓库翻译自 google/eng-practices ，目前为止的主要内容为 Google 总结的如何进行 Code Review（代码审查） 指南，根据原 Github 仓库的标题判断以后会追加更多 Google 工程实践的内容。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区 微信讨论群，加入前请先填写入群申请问卷 后联系 Jimmy Song 入群。\n开始阅读 ","relpermalink":"/book/eng-practices/","summary":"此仓库翻译自 Google 官方仓库，目前为止的主要内容为 Google 总结的如何进行 Code Review（代码审查） 指南。","title":"谷歌工程实践"},{"content":"本书译自美国国家标准标准与技术研究院（NIST）Special Publication 800-204C。\n《利用服务网格为基于微服务的应用程序实施 DevSecOps》封面 本书大纲 声明\n执行摘要\n第一章：简介\n第二章：实施 DevSecOps 原语的参考平台\n第三章：DevSecOps 组织准备、关键基本要素和实施\n第四章：为参考平台实施 DevSecOps 原语\n第五章：摘要和结论\n关于本书 作者：Ramaswamy Chandramouli\n计算机安全司信息技术实验室\n美国商务部\nGina M. Raimondo，秘书\n国家标准和技术研究所\nJames K. Olthoff，履行负责标准和技术的商务部副部长兼国家标准和技术研究所所长的非专属职能和职责\n本出版物可在：https://doi.org/10.6028/NIST.SP.800-204C 免费获取。\n开始阅读 ","relpermalink":"/book/service-mesh-devsecops/","summary":"本书译自美国国家标准标准与技术研究院（NIST）Special Publication 800-204C。","title":"利用服务网格为基于微服务的应用程序实施 DevSecOps"},{"content":"本书是 Migrating to Cloud Native Application Architectures 的中文版，本书英文版发布于 2015 年 2 月，中文版由 Jimmy Song 翻译，发布于 2017 年 7 月。\n《迁移到云原生应用架构》图书封面 译者序 云时代的云原生应用大势已来，将传统的单体架构应用迁移到云原生架构，你准备好了吗？\n俗话说“意识决定行动”，在迁移到云原生应用之前，我们大家需要先对 Cloud Native（云原生）的概念、组织形式并对实现它的技术有一个大概的了解，这样才能指导我们的云原生架构实践。\n英文版作于 2015 年，其中的示例主要针对 Java 应用，实际上也适用于任何应用类型，云原生应用架构适用于异构语言的程序开发，不仅仅是针对 Java 语言的程序开发。截止到本人翻译本书时，云原生应用生态系统已经初具规模，CNCF 成员不断发展壮大，基于 Cloud Native 的创业公司不断涌现，Kubernetes 引领容器编排潮流，和 Service Mesh 技术（如 Linkerd 和 Istio ）的出现，Go 语言的兴起等为我们将应用迁移到云原生架构的提供了更多的方案选择。\n简介 当前很多企业正在采用云原生应用架构，这可以帮助其 IT 转型，成为市场竞争中真正敏捷的力量。O’Reilly 的报告中定义了云原生应用架构的特性，如微服务和十二因素应用程序。\n本书中作者 Matt Stine 还探究了将传统的单体应用和面向服务架构（SOA）应用迁移到云原生架构所需的文化、组织和技术变革。本书中还有一个迁移手册，其中包含将单体应用程序分解为微服务，实施容错模式和执行云原生服务的自动测试的方法。\n本书中讨论的应用架构包括：\n十二因素应用程序：云原生应用架构模式的集合 微服务：独立部署的服务，每个服务只做一件事情 自助服务的敏捷基础设施：快速，可重复和一致地提供应用环境和后台服务的平台 基于 API 的协作：发布和版本化的 API，允许在云原生应用架构中的服务之间进行交互 抗压性：根据压力变强的系统 本章大纲 第一章：云原生的崛起\n第二章：在变革中前行\n第三章：迁移指南\n关于作者 Matt Stine，Pivotal 的技术产品经理，拥有 15 年企业 IT 和众多业务领域的经验。Matt 强调精益/敏捷方法、DevOps、架构模式和编程范例，他正在探究使用技术组合帮助企业IT部门能够像初创公司一样工作。\n开始阅读 ","relpermalink":"/book/migrating-to-cloud-native-application-architectures/","summary":"本书是 Migrating to Cloud Native Application Architectures 的中文版。","title":"迁移到云原生应用架构"},{"content":"还记得有一次告警电话半夜把我吵醒，发现是生产环境离线了。原来是系统瘫痪了，我们不得不要赔钱，这是我的错。\n从那一刻起，我就一直痴迷于构建坚如磐石的基础架构和基础架构管理系统，这样我就不会重蹈覆辙了。在我的职业生涯中，我为 Terraform、Kubernetes，一些编程语言和 Kops 做出过贡献，并创建了 Kubicorn。我不仅见证了系统基础架构的发展，而且我也帮助它完善。随着基础架构行业的发展，我们发现企业基础架构现在正以新的、令人兴奋的方式通过应用层管理。到目前为止，Kubernetes 是这种管理基础架构的新范例的最成熟的例子。\n我与人合著了这本书，部分地介绍了将基础架构作为云原生软件的新范例。此外，我希望鼓励基础架构工程师开始编写云原生应用程序。在这本书中，我们探讨了管理基础架构的丰富历史，并为云原生技术的未来定义了管理基础架构的模式。我们解释了基础架构由软件化 API 驱动的重要性。我们还探索了创建复杂系统的第一个基础架构组件的引导问题，并教授了扩展和测试基础架构的重要性。\n我于 2017 年加入 Heptio，担任资深布道师，并且很高兴能与行业中最聪明的系统工程师密切合作。构建纯粹的开源技术对我来说一直都很重要，Heptio 也拥有这种激情。我很荣幸能在这样一个环境中工作，让我更热爱这个行业。我希望你喜欢这本书，就像 Justin 和我喜欢写这本书一样。\n——Kris Nova\n","relpermalink":"/book/cloud-native-infra/forward/","summary":"还记得有一次告警电话半夜把我吵醒，发现是生产环境离线了。原来是系统瘫痪了，我们不得不要赔钱，这是我的错。 从那一刻起，我就一直痴迷于构建坚如磐石的基础架构和基础架构管理系统，这样我就不会重蹈覆辙了。在我","title":"前言"},{"content":"以下是关于本书的声明。\n许可 本出版物由 NIST 根据 2014 年《联邦信息安全现代化法案》（FISMA）（44 U.S.C. §3551 etseq）规定的法定职责编写，公共法律（P.L.）113-283。NIST 负责制定信息安全标准和准则，包括联邦信息系统的最低要求，但这些标准和准则在未经对国家安全系统行使策略权力的适当联邦官员明确批准的情况下，不得适用于这些系统。本准则与管理和预算办公室（OMB）A-130 号通知的要求一致。\n本出版物中的任何内容都不应被视为与商务部长根据法定授权对联邦机构的强制性和约束性标准和准则相抵触。这些准则也不应被解释为改变或取代商务部长、OMB 主任或任何其他联邦官员的现有权力。本出版物可由非政府组织在自愿的基础上使用，在美国不受版权限制。但是，请注明出处，NIST 将对此表示感谢。\n国家标准和技术研究所特别出版物 800-204C Natl.Inst. Stand.Technol.Spec.800-204C, 45 pages (March 2022) CODEN: NSPUE2\n本出版物可从以下网站免费获取。\nhttps://doi.org/10.6028/NIST.SP.800-204C 关于计算机系统技术的报告 美国国家标准与技术研究所（NIST）的信息技术实验室（ITL）通过为国家的测量和标准基础设施提供技术领导来促进美国经济和公共福利。ITL 开发测试、测试方法、参考数据、概念实施证明和技术分析，以推动信息技术的发展和生产性使用。ITL 的职责包括为联邦信息系统中与国家安全无关的信息制定管理、行政、技术和物理标准和准则，以实现低成本的安全和隐私。\n摘要 云原生应用已经发展成为一个标准化的架构，由多个松散耦合的组件组成，这些组件被称为微服务（通常通常以容器实现），由提供应用服务的基础设施（如服务网格）支持。这两个组件通常都被托管在一个容器调度和资源管理平台上。在这个架构中，应用环境中涉及的整套源代码可以分为五种类型：1）应用代码（体现应用逻辑）；2）应用服务代码（用于会话建立、网络连接等服务）；3）基础设施即代码（用于配置计算、网络和存储资源）；4）策略即代码（用于定义运行时策略，如以声明性代码表达的零信任）；5）可观测性即代码（用于持续监测应用运行时状态）。由于安全、商业竞争力和松散耦合的应用组件的固有结构，这类应用需要一个不同的开发、部署和运行时范式。DevSecOps（分别由开发、安全和运维的首字母缩写组成）已经被发现是这些应用的促进范式，其基本要素包括持续集成、持续交付和持续部署（CI/CD）管道。这些管道是将开发者的源代码通过各个阶段的工作流程，如构建、测试、打包、部署和运维，由带有反馈机制的自动化工具支持。本文的目的是为云原生应用的 DevSecOps 原语的实施提供指导，其架构和代码类型如上所述。本文还讨论了这种方法对高安全保障和实现持续运维授权（C-ATO）的好处。\n鸣谢 作者首先要感谢 NIST 的 David Ferraiolo，他发起了这项工作，为基于微服务的应用中服务网格的开发、部署和监控提供了有针对性的 DevSecOps 原语实施指导。衷心感谢美国空军 CSO Nicolas Chaillan 先生，感谢他详细而有见地的审查和反馈。还要感谢 Tetrate 公司的 Zack Butcher 为本文标题提供的建议。作者还对 NIST 的 Isabel Van Wyk 的详细编辑审查表示感谢。\n专利披露通知 通知：信息技术实验室（ITL）已要求专利权持有人向 ITL 披露其使用可能需要遵守本出版物的指导或要求的专利权。然而，专利持有人没有义务回应 ITL 的专利要求，ITL 也没有进行专利搜索，以确定哪些专利可能适用于本出版物。\n截至本出版物发布之日，以及在呼吁确定可能需要使用其来遵守本出版物的指导或要求的专利权利要求之后，ITL 没有发现任何此类专利权利要求。\nITL 没有作出或暗示在使用本出版物时不需要许可证以避免专利侵权。\n","relpermalink":"/book/service-mesh-devsecops/preface/","summary":"以下是关于本书的声明。 许可 本出版物由 NIST 根据 2014 年《联邦信息安全现代化法案》（FISMA）（44 U.S.C. §3551 etseq）规定的法定职责编写，公共法律（P.L.）113-283。NIST 负责制定信息安全标准","title":"声明"},{"content":"文件变更历史\n英文版\n日期 版本 描述 2021 年 8 月 1.0 首次发布 中文版\n日期 版本 描述 2021 年 8 月 8 日 1.0 首次发布 担保和认可的免责声明\n本文件中的信息和意见是 “按原样” 提供的，没有任何保证或担保。本文件以商品名称、商标、制造商或其他方式提及任何具体的商业产品、程序或服务，并不一定构成或暗示美国政府对其的认可、推荐或青睐，而且本指南不得用于广告或产品代言的目的。\n关于中文版\n中文版为 Jimmy Song 个人翻译，翻译过程中完全遵照原版，未做任何删减。其本人与本书的原作者没有任何组织或利益上的联系，翻译本书仅为交流学习之用。\n商标认可\nKubernetes 是 Linux 基金会的注册商标。 SELinux 是美国国家安全局的注册商标。 AppArmor 是 SUSE LLC 的注册商标。 Windows 和 Hyper-V 是微软公司的注册商标。 ETCD 是 CoreOS, Inc. 的注册商标。 Syslog-ng 是 One Identity Software International Designated Activity 公司的注册商标。 Prometheus 是 Linux 基金会的注册商标。 Grafana 是 Raintank, Inc.dba Grafana Labs 的注册商标。 Elasticsearch 和 ELK Stack 是 Elasticsearch B.V 的注册商标。 版权确认\n本文件中的信息、例子和数字基于 Kubernetes 作者的 Kubernetes 文档 ，以知识共享 署名 4.0 许可方式 发布。\n","relpermalink":"/book/kubernetes-hardening-guidance/notices-and-hitory/","summary":"文件变更历史 英文版 日期 版本 描述 2021 年 8 月 1.0 首次发布 中文版 日期 版本 描述 2021 年 8 月 8 日 1.0 首次发布 担保和认可的免责声明 本文件中的信息和意见是 “按原样” 提供的，没有任何保证或担保。本文件以","title":"通知和历史"},{"content":"本书为 Cloud Native Infrastructure 中文版，作者 Justin Garrison 和 Kris Nova，英文版发行于 2017 年 11 月，已可以在网上免费获得，本书是关于创建和管理基础架构，以适用于云原生应用全生命周期管理的模式和实践。\n《云原生基础架构》封面 阅读完这本书后，您将会有如下收获：\n理解为什么说云原生基础架构是高效运行云原生应用所必须的 根据准则来决定您的业务何时以及是否应该采用云原生 了解部署和管理基础架构和应用程序的模式 设计测试以证明您的基础架构可以按预期工作，即使在各种边缘情况下也是如此 了解如何以策略即代码的方式保护基础架构 本书大纲 前言\n介绍\n第 1 章：什么是云原生基础架构？\n第 2 章：采纳云原生基础架构的时机\n第 3 章：云原生部署的演变\n第 4 章：设计基础架构应用程序\n第 5 章：开发基础架构应用程序\n第 6 章：测试云原生基础架构\n第 7 章：管理云原生应用程序\n第 8 章：保护应用程序\n第 9 章：实施云原生基础架构\n附录 A：网络弹性模式\n附录 B：锁定\n附录 C Box：案例研究\n免责声明 本书英文版版权属于 O’Reilly，中文版版权归属于机械工业出版，基于署名 - 非商业性使用 - 相同方式共享 4.0（CC BY-NC-SA 4.0） 分享，本书为云原生爱好者翻译，仅可用于学习和交流目的，请勿私自印制贩卖，如有需要请购买纸质书 。\n开始阅读 ","relpermalink":"/book/cloud-native-infra/","summary":"《云原生基础架构》，Cloud Native Infrastructure 中文版。","title":"云原生基础架构"},{"content":"本报告译自 O’Reilly 出品的 The Future of Observablity with OpeTelemetry ，作者 Ted Young，译者 Jimmy Song 。\n《OpenTelemetry 可观测性的未来》封面 关于本书 本书内容包括：\nOpenTelemetry 如何满足库作者、应用程序拥有者、运维和响应者的需求 应用程序的不同角色如何围绕 OpenTelemetry 来协同和独立工作 关于在组织中采用和管理 OpenTelemetry 的实用建议 关于作者 Ted Young 是 OpenTelemetry 项目的联合创始人之一。在过去的二十年里，他设计并建立了各种大规模的分布式系统，包括可视化 FX 管道和容器调度系统。他目前在 Lightstep 公司担任开发者教育总监，住在俄勒冈州波特兰的一个小农场里。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区 可观测性讨论组（微信群）参与讨论交流，加入前请先填写入群申请问卷 后联系 Jimmy Song 入群。\n开始阅读 ","relpermalink":"/book/opentelemetry-obervability/","summary":"本报告译自 O'Reilly 出品的 The Future of Observablity with OpenTelemetry，作者 Ted Young，译者 Jimmy Song。","title":"OpenTelemetry 可观测性的未来"},{"content":"软件开发的模式又一次改变了。开源软件和公有云供应商已经从根本上改变了我们构建和部署软件的方式。有了开源软件，我们的应用不再需要从头开始编码。而通过使用公有云供应商，我们不再需要配置服务器或连接网络设备。所有这些都意味着，你可以在短短几天甚至几小时内从头开始构建和部署一个应用程序。\n但是，仅仅因为部署新的应用程序很容易，并不意味着操作和维护它们也变得更容易。随着应用程序变得更加复杂，更加异质，最重要的是，更加分布式，看清大局，以及准确地指出问题发生的地方，变得更加困难。\n但有些事情并没有改变：作为开发者和运维，我们仍然需要能够从用户的角度理解应用程序的性能，我们仍然需要对用户的事务有一个端到端的看法。我们也仍然需要衡量和说明在处理这些事务的过程中资源是如何被消耗的。也就是说，我们仍然需要可观测性。\n但是，尽管对可观测性的需求没有改变，我们实施可观测性解决方案的方式必须改变。本报告详细介绍了关于可观测性的传统思维方式对现代应用的不足：继续将可观测性作为一系列工具（尤其是作为 “三大支柱”）来实施，几乎不可能以可靠的方式运行现代应用。\nOpenTelemetry 通过提供一种综合的方法来收集有关应用程序行为和性能的数据，包括指标、日志和跟踪，来解决这些挑战。正如本报告所解释的，OpenTelemetry 是一种生成和收集这些数据的新方法，这种方法是为使用开源组件构建的云原生应用而设计的。\n事实上，OpenTelemetry 是专门为这些开源组件设计的。与供应商特定的仪表不同，OpenTelemetry 可以直接嵌入到开放源代码中。这意味着开源库的作者可以利用他们的专业知识来添加高质量的仪表，而不需要在他们的项目中增加任何解决方案或供应商特定的代码。\nOpenTelemetry 对应用程序所有者也有好处。在过去，遥测的产生方式与它的评估、存储和分析方式相联系。这意味着，选择使用哪种可观测性解决方案必须在开发过程的早期完成，并且在之后很难改变。OpenTelemetry（就像它的前身 OpenTracing 和 OpenCensus 一样）将你的应用程序产生遥测的方式与遥测的分析方式相分离。\n因此，当我们采用新技术时，我们往往没有考虑到该技术将如何被使用，特别是它将如何被不同角色的人使用。本报告描述了 OpenTelemetry 如何满足库作者、应用程序拥有者、运维和响应者的需求，以及它如何使这些角色中的每个人都能独立工作 —— 即自动做出关键决定并进行有效协作。\n也许最重要的是，OpenTelemetry 为应用程序所有者和操作者提供了灵活性，使他们能够选择最适合其应用程序和组织需求的可观测性解决方案，包括需要多种工具的情况。它还包括这些需求随时间变化的情况，以及需要新的工具来满足这些需求的情况，因为随着技术的成熟和组织的发展，可能会出现这种情况。\n虽然用户不需要致力于可观测性解决方案，但他们确实需要投资于生成遥测数据的一致方式。本报告展示了 OpenTelemetry 是如何被设计成范围狭窄、可扩展，而且最重要的是稳定的：如果你被要求进行这种投资，这正是你所期望的。\n与以往任何时候相比，可观测性都不能成为事后的想法。没有一个有效的可观测性解决方案的风险，也就是说，无法了解你的应用程序中正在发生的事情的风险是非常高的，而对可观测性采取错误的方法的成本会造成你的组织多年来一直在偿还的债务。无论是长时间停机，还是开发团队被无休止的供应商迁移所困扰，你的组织的成功都取决于像 OpenTelemetry 这样的集成和开放的方法。\n本报告提供了关于在你的组织中采用和管理 OpenTelemetry 的实用建议。它将使你开始走向成功的可观测性实践的道路，并释放出云原生和开源技术的许多真正的好处：你不仅能够快速建立和部署应用程序，而且能够可靠和自信地运行它们。\n—— Daniel “Spoons” Spoonhower Lightstep\n","relpermalink":"/book/opentelemetry-obervability/foreword/","summary":"前言","title":"前言"},{"content":"代码审查的主要目的是确保逐步改善 Google 代码库的整体健康状况。代码审查的所有工具和流程都是为此而设计的。\n为了实现此目标，必须做出一系列权衡。\n首先，开发人员必须能够对任务进行改进。如果开发者从未向代码库提交过代码，那么代码库的改进也就无从谈起。此外，如果审核人员对代码吹毛求疵，那么开发人员以后也很难再做出改进。\n另外，审查者有责任确保随着时间的推移，CL 的质量不会使代码库的整体健康状况下降。这可能很棘手，因为通常情况下，代码库健康状况会随着时间的而下降，特别是在对团队有严格的时间要求时，团队往往会采取捷径来达成他们的目标。\n此外，审查者应对正在审核的代码负责并拥有所有权。审查者希望确保代码库保持一致、可维护及 Code Review 要点 中所提及的所有其他内容。\n因此，我们将以下规则作为 Code Review 中期望的标准：\n一般来说，审核人员应该倾向于批准 CL，只要 CL 确实可以提高系统的整体代码健康状态，即使 CL 并不完美。\n这是所有 Code Review 指南中的高级原则。\n当然，也有一些限制。例如，如果 CL 添加了审查者认为系统中不需要的功能，那么即使代码设计良好，审查者依然可以拒绝批准它。\n此处有一个关键点就是没有“完美”的代码，只有更好的代码。审查者不该要求开发者在批准程序前仔细清理、润色 CL 每个角落。相反，审查者应该在变更的重要性与取得进展之间取得平衡。审查者不应该追求完美，而应是追求持续改进。不要因为一个 CL 不是“完美的”，就将可以提高系统的可维护性、可读性和可理解性的 CL 延迟数天或数周才批准。\n审核者应该随时在可以改善的地方留下审核评论，但如果评论不是很重要，请在评论语句前加上“Nit：”之类的内容，让开发者知道这条评论是用来指出可以润色的地方，而他们可以选择是否忽略。\n注意：本文档中没有任何内容证明检查 CL 肯定会使系统的整体代码健康状况恶化。您会做这种事情应该只有在紧急情况 时。\n指导 代码审查具有向开发人员传授语言、框架或通用软件设计原则新内容的重要功能。留下评论可以帮助开发人员学习新东西，这总归是很好的。分享知识是随着长年累月改善系统代码健康状况的一部分。请记住，如果您的评论纯粹是教育性的，且对于本文档中描述的标准并不重要，请在其前面添加“Nit：”或以其他方式表明作者不必在此 CL 中解决它。\n原则 基于技术事实和数据否决意见和个人偏好。 关于代码风格问题，风格指南 是绝对权威。任何不在风格指南中的纯粹风格点（例如空白等）都是个人偏好的问题。代码风格应该与风格指南中的一致。如果没有以前的风格，请接受作者的风格。 软件设计方面几乎不是纯粹的风格或个人偏好问题。软件设计基于基本原则且应该权衡这些原则，而不仅仅是个人意见。有时候会有多种有效的选择。如果作者可以证明（通过数据或基于可靠的工程原理）该方法同样有效，那么审查者应该接受作者的偏好。否则，就要取决于软件设计的标准原则。 如果没有其他适用规则，则审查者可以要求作者与当前代码库中的内容保持一致，只要不恶化系统的整体代码健康状况即可。 解决冲突 如果在代码审查过程中有任何冲突，第一步应该始终是开发人员和审查者根据本文档中的 CL 开发者指南 和审查者指南 达成共识。\n当达成共识变得特别困难时，审阅者和开发者可以进行面对面的会议，或者有 VC 参与调停，而不仅仅是试着通过代码审查评论来解决冲突。 （但是，如果您这样做了，请确保在 CL 的评论中记录讨论结果，以供将来的读者使用。）\n如果这样还不能解决问题，那么解决该问题最常用方法是将问题升级。通常是将问题升级为更广泛的团队讨论，有一个 TL 权衡，要求维护人员对代码作出决定，或要求工程经理的帮助。 不要因为 CL 的开发者和审查者不能达成一致，就让 CL 在那里卡壳。\n","relpermalink":"/book/eng-practices/review/reviewer/standard/","summary":"代码审查的主要目的是确保逐步改善 Google 代码库的整体健康状况。代码审查的所有工具和流程都是为此而设计的。 为了实现此目标，必须做出一系列权衡。 首先，开发人员必须能够对任务进行改进。如果开发者从未向代码库提交过","title":"Code Review 标准"},{"content":"简介 代码审查是除了代码作者之外，其他人检查代码的过程。\nGoogle 通过 Code Review 来维护代码和产品质量。\n此文档是 Google Code Review 流程和政策的规范说明。\n此页面是我们进行 Code Review 流程的概述。本指南还有另外两套文档：\n如何进行 Code Review ：针对代码审查者的详细指南。 代码开发者指南 ：针对 CL 开发者的的详细指南。 代码审查者应该关注哪些方面？ 代码审查时应该关注以下方面：\n设计：代码是否经过精心设计并适合您的系统？ 功能：代码的行为是否与作者的意图相同？代码是否可以正常响应用户的行为？ 复杂度：代码能更简单吗？将来其他开发人员能轻松理解并使用此代码吗？ 测试：代码是否具有正确且设计良好的自动化测试？ 命名：开发人员是否为变量、类、方法等选择了明确的名称？ 注释：注释是否清晰有用？ 风格：代码是否遵守了风格指南 ？ 文档：开发人员是否同时更新了相关文档？ 参阅 如何进行 Code Review 获取更多资料。\n选择最合适审查者 一般而言，您希望找到能在合理的时间内回复您的评论的最合适的审查者。\n最合适的审查者应该是能彻底了解和审查您代码的人。他们通常是代码的所有者，可能是 OWNERS 文件中的人，也可能不是。有时 CL 的不同部分可能需要不同的人审查。\n如果您找到了理想的审查者但他们又没空，那您也至少要抄送他们。\n面对面审查 如果您与有资格做代码审查的人一起结对编程了一段代码，那么该代码将被视为已审查。\n您还可以进行面对面的代码审查，审查者提问，CL 的开发人员作答。\n术语 文档中使用了 Google 内部术语，在此为外部读者澄清：\nCL：代表“变更列表（Change List）”，表示已提交到版本控制或正在进行代码审查的自包含更改。有的组织会将其称为“变更（change）”或“补丁（patch）”。 LGTM：意思是“我觉得不错（Looks Good to Me）”。这是批准 CL 时代码审查者所说的。 参考 如何进行 Code Review ：针对代码审查者的详细指南。 代码开发者指南 ：针对 CL 开发者的的详细指南。 ","relpermalink":"/book/eng-practices/review/","summary":"简介 代码审查是除了代码作者之外，其他人检查代码的过程。 Google 通过 Code Review 来维护代码和产品质量。 此文档是 Google Code Review 流程和政策的规范说明。 此页面是我们进行 Code Review 流程的概述。本指南还有另外两套文档： 如何进行 Code Review ：针对代码","title":"代码审查指南"},{"content":"CL 描述是进行了哪些更改以及为何更改的公开记录。CL 将作为版本控制系统中的永久记录，可能会在长时期内被除审查者之外的数百人阅读。\n开发者将来会根据描述搜索您的 CL。有人可能会仅凭有关联性的微弱印象，但没有更多具体细节的情况下，来查找你的改动。如果所有重要信息都在代码而不是描述中，那么会让他们更加难以找到你的 CL。\n首行 正在做什么的简短摘要。 完整的句子，使用祈使句。 后面跟一个空行。 CL 描述的第一行应该是关于这个 CL 是做什么的简短摘要，后面跟一个空白行。这是将来大多数的代码搜索者在浏览代码的版本控制历史时，最常被看到的内容，因此第一行应该提供足够的信息，以便他们不必阅读 CL 的整个描述就可以获得这个 CL 实际上是做了什么的信息。\n按照传统，CL 描述的第一行应该是一个完整的句子，就好像是一个命令（一个命令句）。例如，“Delete the FizzBuzz RPC and replace it with the new system.”而不是“Deleting the FizzBuzz RPC and replacing it with the new system.“但是，您不必把其余的描述写成祈使句。\nBody 是信息丰富的 其余描述应该是提供信息的。可能包括对正在解决的问题的简要描述，以及为什么这是最好的方法。如果方法有任何缺点，应该提到它们。如果相关，请包括背景信息，例如错误编号，基准测试结果以及设计文档的链接。\n即使是小型 CL 也需要注意细节。在 CL 描述中提供上下文以供参照。\n糟糕的 CL 描述 “Fix bug”是一个不充分的 CL 描述。什么 bug？你做了什么修复？其他类似的不良描述包括：\n“Fix build.” “Add patch.” “Moving code from A to B.” “Phase 1.” “Add convenience functions.” “kill weird URLs.” 其中一些是真正的 CL 描述。他们的作者可能认为自己提供了有用的信息，却没有达到 CL 描述的目的。\n好的 CL 描述 以下是一些很好的描述示例。\n功能更新 rpc：删除 RPC 服务器消息 freelist 上的大小限制。\n像 FIzzBuzz 这样的服务器有非常大的消息，并且可以从重用中受益。增大 freelist，添加一个 goroutine，缓慢释放 freelist 条目，以便空闲服务器最终释放所有 freelist 条目。\n前几个词描述了 CL 实际上做了什么。其余的描述讨论了正在解决的问题，为什么这是一个很好的解决方案，以及有关具体实现的更多信息。\n重构 Construct a Task with a TimeKeeper to use its TimeStr and Now methods.\nAdd a Now method to Task, so the borglet() getter method can be removed (which was only used by OOMCandidate to call borglet’s Now method). This replaces the methods on Borglet that delegate to a TimeKeeper.\nAllowing Tasks to supply Now is a step toward eliminating the dependency on Borglet. Eventually, collaborators that depend on getting Now from the Task should be changed to use a TimeKeeper directly, but this has been an accommodation to refactoring in small steps.\nContinuing the long-range goal of refactoring the Borglet Hierarchy.\n第一行描述了 CL 的作用以及改变。其余的描述讨论了具体的实现，CL 的背景，解决方案并不理想，以及未来的可能方向。它还解释了为什么正在进行此更改。\n需要上下文的 小 CL Create a Python3 build rule for status.py.\nThis allows consumers who are already using this as in Python3 to depend on a rule that is next to the original status build rule instead of somewhere in their own tree. It encourages new consumers to use Python3 if they can, instead of Python2, and significantly simplifies some automated build file refactoring tools being worked on currently.\n第一句话描述实际做了什么。其余的描述解释了为什么正在进行更改并为审查者提供了大量背景信息。\n在提交 CL 前审查描述 CL 在审查期间可能会发生重大变更。在提交 CL 之前检查 CL 描述是必要的，以确保描述仍然反映了 CL 的作用。\n","relpermalink":"/book/eng-practices/review/developer/cl-descriptions/","summary":"CL 描述是进行了哪些更改以及为何更改的公开记录。CL 将作为版本控制系统中的永久记录，可能会在长时期内被除审查者之外的数百人阅读。 开发者将来会根据描述搜索您的 CL。有人可能会仅凭有关联性的微弱印象，但没有","title":"写好 CL 描述"},{"content":"本教程在 Kubernetes 快速入门教程 的基础上，演示了如何配置 SPIRE 以提供动态的 X.509 证书形式的服务身份，并由 Envoy 秘密发现服务（SDS）使用。本教程中展示了实现 X.509 SVID 身份验证所需的更改，因此你应该首先运行或至少阅读 Kubernetes 快速入门教程。\n为了演示 X.509 身份验证，我们创建了一个简单的场景，包含三个服务。其中一个服务是后端服务，是一个简单的 nginx 实例，用于提供静态数据。我们另外运行两个 Symbank 演示银行应用作为前端服务。Symbank 前端服务向 nginx 后端发送 HTTP 请求以获取用户账户详细信息。\n如图所示，前端服务通过 Envoy 实例建立的 mTLS 连接与后端服务连接，并且 Envoy 实例会为每个工作负载执行 X.509 SVID 身份验证。\n在本教程中，你将学习如何：\n配置 SPIRE 以支持 SDS 配置 Envoy SDS 以使用 SPIRE 提供的 X.509 证书 在 SPIRE 服务器上为 Envoy 实例创建注册条目 使用 SPIRE 测试成功的 X.509 身份验证 可选地配置 Envoy RBAC HTTP 过滤器策略 先决条件 在继续之前，请先阅读以下内容：\n你需要访问通过 Kubernetes 快速入门教程 配置的 Kubernetes 环境。可选地，你可以使用下面描述的 pre-set-env.sh 脚本创建 Kubernetes 环境。Kubernetes 环境必须能够将 Ingress 公开到公共互联网上。注意：对于本地 Kubernetes 环境（例如 Minikube），通常不适用此条件。 本教程所需的 YAML 文件可在 https://github.com/spiffe/spire-tutorials 的 k8s/envoy-x509 目录中找到。如果你尚未克隆 Kubernetes 快速入门教程的存储库，请现在进行克隆。 如果 Kubernetes 快速入门教程环境不可用，你可以使用以下脚本创建该环境，并将其用作本教程的起点。从k8s/envoy-x509目录中运行以下命令：\n$ bash scripts/pre-set-env.sh 该脚本将创建所需的 SPIRE 服务器和 SPIRE 代理资源。\n外部 IP 支持 本教程需要一个可以分配外部 IP 的负载均衡器（例如metallb ）。\n$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.7/config/manifests/metallb-native.yaml 等待 metallb 启动\n$ kubectl wait --namespace metallb-system \\ --for=condition=ready pod \\ --selector=app=metallb \\ --timeout=90s 应用 metallb 配置\n$ kubectl apply -f metallb-config.yaml Envoy SDS 支持 SPIRE 代理原生支持 Envoy Secret Discovery Service（SDS）。SDS 通过与工作负载 API 和连接到 SDS 的 Envoy 进程使用相同的 Unix 域套接字提供服务，并对工作负载进行验证。\n第 1 部分：运行工作负载 现在，让我们部署本教程中将使用的工作负载。它由三个工作负载组成：如前所述，两个 Symbank 演示应用程序的实例将充当前端服务，另一个提供静态文件的 nginx 实例将充当后端服务。\n为了区分两个 Symbank 应用程序的实例，让我们将其称为 frontend 和 frontend-2。前者配置为显示与用户 Jacob Marley 相关的数据，而后者将显示用户 Alex Fergus 的帐户详细信息。\n部署所有工作负载 确保当前的工作目录是 .../spire-tutorials/k8s/envoy-x509，然后使用以下命令部署新的资源：\n$ kubectl apply -k k8s/. configmap/backend-balance-json-data created configmap/backend-envoy created configmap/backend-profile-json-data created configmap/backend-transactions-json-data created configmap/frontend-2-envoy created configmap/frontend-envoy created configmap/symbank-webapp-2-config created configmap/symbank-webapp-config created service/backend-envoy created service/frontend-2 created service/frontend created deployment.apps/backend created deployment.apps/frontend-2 created deployment.apps/frontend created kubectl apply 命令将创建以下资源：\n每个工作负载的部署。它包含一个用于我们的服务和 Envoy Sidecar 的容器。 每个工作负载的服务。用于它们之间的通信。 多个 Configmap： json-data 用于向作为后端服务运行的 Nginx 实例提供静态文件。 envoy 包含每个工作负载的 Envoy 配置。 symbank-webapp- 包含供每个前端服务实例使用的配置。 接下来的两个部分将重点介绍配置 Envoy 所需的设置。\nSPIRE Agent 集群 为了让 Envoy SDS 使用 SPIRE Agent 提供的 X.509 证书，我们配置一个集群，指向 SPIRE Agent 提供的 Unix 域套接字。后端服务的 Envoy 配置位于 k8s/backend/config/envoy.yaml。\nclusters: - name: spire_agent connect_timeout: 0.25s http2_protocol_options: {} load_assignment: cluster_name: spire_agent endpoints: - lb_endpoints: - endpoint: address: pipe: path: /run/spire/sockets/agent.sock TLS 证书 要从 SPIRE 获取 TLS 证书和私钥，你需要在 TLS 上下文中设置一个 SDS 配置。TLS 证书的名称是 Envoy 充当代理的服务的 SPIFFE ID。此外，SPIRE 为每个信任域提供一个验证上下文，Envoy 使用它来验证对等证书。\ntransport_socket: name: envoy.transport_sockets.tls typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext common_tls_context: tls_certificate_sds_secret_configs: - name: \u0026#34;spiffe://example.org/ns/default/sa/default/backend\u0026#34; sds_config: resource_api_version: V3 api_config_source: api_type: GRPC transport_api_version: V3 grpc_services: envoy_grpc: cluster_name: spire_agent combined_validation_context: # validate the SPIFFE ID of incoming clients (optionally) default_validation_context: match_typed_subject_alt_names: - san_type: URI matcher: exact: \u0026#34;spiffe://example.org/ns/default/sa/default/frontend\u0026#34; - san_type: URI matcher: exact: \u0026#34;spiffe://example.org/ns/default/sa/default/frontend-2\u0026#34; # obtain the trust bundle from SDS validation_context_sds_secret_config: name: \u0026#34;spiffe://example.org\u0026#34; sds_config: resource_api_version: V3 api_config_source: api_type: GRPC transport_api_version: V3 grpc_services: envoy_grpc: cluster_name: spire_agent 类似的配置也适用于前端服务，以建立一个 mTLS 通信。检查名为 backend 的集群在 k8s/frontend/config/envoy.yaml 和 k8s/frontend-2/config/envoy.yaml 中的配置。\n创建注册条目 为了获得 SPIRE 颁发的 X.509 证书，必须先注册服务。我们通过在 SPIRE Server 上为每个工作负载创建注册条目来实现这一点。让我们使用以下 Bash 脚本：\n$ bash create-registration-entries.sh 运行脚本后，将显示所创建的注册条目列表。输出将显示 Kubernetes Quickstart Tutorial 创建的其他注册条目。这里重要的是每个工作负载的三个新条目：\n... Entry ID : 0d02d63f-712e-47ad-a06e-853c8b062835 SPIFFE ID : spiffe://example.org/ns/default/sa/default/backend Parent ID : spiffe://example.org/ns/spire/sa/spire-agent TTL : 3600 Selector : k8s:container-name:envoy Selector : k8s:ns:default Selector : k8s:pod-label:app:backend Selector : k8s:sa:default Entry ID : 3858ec9b-f924-4f69-b812-5134aa33eaee SPIFFE ID : spiffe://example.org/ns/default/sa/default/frontend Parent ID : spiffe://example.org/ns/spire/sa/spire-agent TTL : 3600 Selector : k8s:container-name:envoy Selector : k8s:ns:default Selector : k8s:pod-label:app:frontend Selector : k8s:sa:default Entry ID : 4e37f863-302a-4b3c-a942-dc2a86459f37 SPIFFE ID …","relpermalink":"/book/spiffe-and-spire/examples/envoy-x509/","summary":"本教程在 Kubernetes 快速入门教程 的基础上，演示了如何配置 SPIRE 以提供动态的 X.509 证书形式的服务身份，并由 Envoy 秘密发现服务（SDS）使用。本教程中展示了实现 X.509 SVID 身份验证所需的更改，因此你应该首先运行或至少阅读 Kubernetes 快速入门教程","title":"使用 Envoy 和 X.509-SVID"},{"content":"SPIRE 是 SPIFFE API 的一个生产就绪的实现，它执行节点和工作负载认证，以便根据一组预先定义的条件，安全地向工作负载发出 SVID，并验证其他工作负载的 SVID。\nSPIRE 架构和组件 SPIRE 部署由一个 SPIRE 服务器和一个或多个 SPIRE 代理组成。服务器充当通过代理向一组工作负载发放身份的签名机构。它还维护一个工作负载身份的注册表，以及为签发这些身份而必须验证的条件。代理在本地向工作负载公开 SPIFFE 工作负载 API，必须安装在工作负载运行的每个节点上。\nSPIRE 架构图 服务器 SPIRE 服务器负责管理和发布其配置的 SPIFFE 信任域中的所有身份。它存储注册条目（指定决定特定 SPIFFE ID 应被签发的条件的选择器）和签名密钥，使用节点证明来自动验证代理的身份，并在被验证的代理请求时为工作负载创建 SVID。\nSPIRE 服务器 服务器的行为是通过一系列的插件决定的。SPIRE 包含几个插件，你可以建立额外的插件来扩展 SPIRE 以满足特定的使用情况。插件的类型包括：\n节点证明器插件：与代理节点证明器一起，验证代理运行的节点的身份。 节点解析器插件：它通过验证关于节点的额外属性来扩展服务器可以用来识别节点的选择器集合。 数据存储插件：服务器用它来存储、查询和更新各种信息，如注册条目、哪些节点已认证、这些节点的选择器是什么。有一个内置的数据存储插件，可以使用 MySQL、SQLite3 或 PostgresSQL 数据库来存储必要的数据。默认情况下，使用 SQLite 3。 密钥管理器插件：控制服务器如何存储用于签署 X.509-SVID 和 JWT-SVID 的私钥。 上游权威机构插件：默认情况下，SPIRE 服务器充当其自身的证书授权机构。但是，你可以使用上游权威机构插件来使用来自不同 PKI 系统的不同 CA。 你可以通过配置插件和其他各种配置变量来定制服务器的行为。详见 SPIRE 服务器配置参考 。\n代理 SPIRE 代理在已识别的工作负载所运行的每个节点上运行。该代理：\n从服务器上请求 SVID，并将其缓存起来，直到工作负载请求其 SVID 为止。 向节点上的工作负载公开 SPIFFE 工作负载 API，并证明调用它的工作负载的身份 为已识别的工作负载提供其 SVID SPIRE 代理 该代理的主要组成部分包括：\n节点证明器插件：与服务器节点证明器一起，验证代理运行的节点的身份。 工作负载证明器插件：通过从节点操作系统中查询有关工作负载进程的信息，并将其与你在使用选择器注册工作负载属性时提供给服务器的信息进行比较，来验证节点上工作负载进程的身份。 密钥管理器插件：代理用来生成和使用颁发给工作负载的 X.509-SVID 的私钥。 你可以通过配置插件和其他配置变量来定制代理的行为。详见《SPIRE 代理配置参考》 。\n自定义服务器和代理插件 你可以为特定的平台和架构创建自定义的服务器和代理插件，而 SPIRE 并不包括这些插件。例如，你可以为一个架构创建服务器和代理节点验证器，而不是在节点验证下总结的那些。或者你可以创建一个自定义密钥管理器插件，以 SPIRE 目前不支持的方式处理私钥。因为 SPIRE 在运行时加载自定义插件，你不需要重新编译 SPIRE 来启用它们。\n工作负载注册 为了让 SPIRE 识别工作负载，你必须通过注册条目向 SPIRE 服务器注册工作负载。工作负载注册告诉 SPIRE 如何识别工作负载以及为其提供哪个 SPIFFE ID。\n注册条目将身份（以 SPIFFE ID 的形式）映射到一组称为选择器的属性，工作负载必须拥有这些属性才能获得特定身份。在工作负载证明期间，代理使用这些选择器值来验证工作负载的身份。\nSPIRE 文档 中详细介绍了工作负载注册。\n证明 SPIRE 上下文中的证明（attestation）是断言工作负载的身份。SPIRE 通过从受信任的第三方收集工作负载进程本身和运行 SPIRE 代理的节点的属性并将它们与工作负载注册时定义的一组选择器进行比较来实现这一点。\n用于执行证明的可信第三方 SPIRE 查询是特定于平台的。\nSPIRE 分两个阶段执行证明：首先是节点证明（其中验证工作负载正在运行的节点的身份），然后是工作负载证明（其中验证节点上的工作负载）。\nSPIRE 有一个灵活的架构，允许它根据工作负载运行的环境，使用许多不同的受信第三方进行节点和工作负载验证。你通过代理和服务器配置文件中的条目告诉 SPIRE 使用哪些受信任的第三方，并通过你在注册工作负载时指定的选择器值告诉 SPIRE 使用哪些类型的信息进行验证。\n节点证明 SPIRE 要求每个代理在首次连接到服务器时进行身份验证和自我验证；这个过程称为节点证明（Node Attestation）。在节点证明期间，代理和服务器一起验证运行代理的节点的身份。他们通过称为节点证明器的插件来做到这一点。所有节点证明器都向节点及其环境询问只有该节点拥有的信息片段，以证明该节点的身份。\n节点证明的成功后，代理收到唯一的 SPIFFE ID。然后，代理的 SPIFFE ID 充当其负责的工作负载的“父级”。\n节点身份证明的示例包括：\n通过云平台交付给节点的身份证明文件（例如 AWS Instance 身份证明文件） 验证存储在连接到节点的硬件安全模块或可信平台模块上的私钥 安装代理时通过加入令牌提供的手动验证 多节点软件系统安装在节点上时提供的标识凭据（例如 Kubernetes 服务账户令牌） 其他机器身份证明（例如部署的服务器证书） 节点证明器向服务器返回一组（可选）节点选择器，用于标识特定机器（例如 Amazon 实例 ID）。由于在定义工作负载的身份时，单个机器的特定身份通常没有用处，因此 SPIRE 会查询节点解析器 （如果有）以查看可以验证被证明节点的哪些附加属性（例如，如果节点是 AWS 安全组的成员）。来自证明器和解析者的选择器集成为与代理节点的 SPIFFE ID 关联的选择器集。\n注意 节点证明不需要节点选择器，除非你将工作负载映射到多个节点 。 下图说明了节点证明中的步骤。在此图中，底层平台是 AWS：\nSPIRE 节点证明步骤 步骤总结：节点证明 代理 AWS 节点证明器插件向 AWS 查询节点身份证明，并将该信息提供给代理。 代理将此身份证明传递给服务器。服务器将此数据传递给其 AWS 节点证明器。 服务器 AWS 节点证明器独立验证身份证明，或者通过调用 AWS API，使用它在步骤 2 中获得的信息。节点证明器还为代理创建一个 SPIFFE ID，并将其传递回服务器进程，以及它发现的任何节点选择器。 服务器发回代理节点的 SVID。 节点证明器 代理和服务器通过它们各自的节点证明器询问底层平台。SPIRE 支持节点证明器在各种环境中证明节点身份，包括：\nAWS 上的 EC2 实例（使用 EC2 实例身份文档） Microsoft Azure 上的 VM（使用 Azure 托管服务标识） Google Cloud Platform 上的 Google Compute Engine 实例（使用 GCE 实例身份令牌） 作为 Kubernetes 集群成员的节点（使用 Kubernetes 服务账户令牌） 对于没有平台可以直接识别节点的情况，SPIRE 包括用于证明的节点证明器：\n使用服务器生成的加入令牌—— 加入令牌（join token）是 SPIRE 服务器和代理之间的预共享密钥。服务器可以在安装后生成加入令牌，该令牌可用于在代理启动时对其进行验证。为帮助防止滥用，加入令牌在使用后立即过期。\n使用现有的 X.509 证书—— 有关配置节点证明器的信息，请参阅 SPIRE 服务器配置参考 和 SPIRE 代理配置参考 。\n节点解析 一旦验证了单个节点的身份，“节点解析器”插件就会扩展一组选择器，这些选择器可用于通过验证节点的其他属性来识别节点（例如，如果节点是特定 AWS 安全组的成员） ，或具有与之关联的特定标签）。只有服务器参与节点解析。SPIRE 在证明之后直接运行一次节点解析器。\n节点解析器 服务器支持以下平台的节点解析器插件：\n亚马逊网络服务（AWS） 微软 Azure 工作负载证明 工作负载证明提出了一个问题：“这是谁的进程？”代理通过询问本地可用的权限（例如节点的操作系统内核，或在同一节点上运行的本地 kubelet）来回答这个问题，以确定调用工作负载 API 的进程的属性。\n然后，当你使用选择器注册工作负载的属性时，将这些属性与提供给服务器的信息进行比较。\n这些类型的信息可能包括：\n底层操作系统如何调度进程。在基于 Unix 的系统上，这可能是用户 ID (uid)、组 ID (gid)、文件系统路径等。） 进程是如何由 Kubernetes 等编排系统调度的。在这种情况下，工作负载可能由运行它的 Kubernetes 服务账户或命名空间来描述。 虽然代理和服务器都在节点证明中发挥作用，但只有代理参与工作负载证明。\n下图说明了工作负载证明的步骤：\n工作负载证明 步骤摘要：工作负载证明 工作负载调用工作负载 API 以请求 SVID。在 Unix 系统上，这被暴露为 Unix 域套接字。 代理询问节点的内核以识别调用者的进程 ID。然后，它调用任何已配置的工作负载证明器插件，为它们提供工作负载的进程 ID。 工作负载证明者使用进程 ID 来发现有关工作负载的其他信息，并根据需要查询相邻平台特定的组件，例如 Kubernetes kubelet。通常，这些组件也与代理驻留在同一节点上。 证明者将发现的信息以选择器的形式返回给代理。 代理通过将发现的选择器与注册条目进行比较来确定工作负载的身份，并将正确的缓存 SVID 返回给工作负载。 工作负载证明者 SPIRE 包括适用于 Unix、Kubernetes 和 Docker 的工作负载证明器插件。\n","relpermalink":"/book/spiffe-and-spire/concept/spire/","summary":"SPIRE 是 SPIFFE API 的一个生产就绪的实现，它执行节点和工作负载认证，以便根据一组预先定义的条件，安全地向工作负载发出 SVID，并验证其他工作负载的 SVID。 SPIRE 架构和组件 SPIRE 部署由一个 SPIRE 服务器和一个或多个 SPIRE 代理组成。","title":"SPIRE 基本概念"},{"content":"嵌套 SPIRE 允许将 SPIRE 服务器“链接”在一起，并且所有 SPIRE 服务器都可以在同一信任域中发放身份，这意味着在同一信任域中标识的所有工作负载都可以使用根密钥验证其身份文档。\n嵌套拓扑结构通过将一个 SPIRE 代理与每个下游 SPIRE 服务器“链接”在一起来实现。下游 SPIRE 服务器通过 Workload API 获得凭证，然后直接与上游 SPIRE 服务器进行身份验证，以获取一个中间 CA。\n为了演示嵌套拓扑中的 SPIRE 部署，我们使用 Docker Compose 创建了一个场景，其中包括一个根 SPIRE 部署和两个嵌套的 SPIRE 部署。\n嵌套拓扑结构非常适合多云部署。由于可以混合匹配节点验证者，下游 SPIRE 服务器可以位于不同的云提供商环境中，并为工作负载和 SPIRE 代理提供身份。\n在本教程中，你将学习以下内容：\n在嵌套拓扑中配置 SPIRE 配置 UpstreamAuthority 插件 为嵌套 SPIRE 服务器创建注册条目 测试在整个信任域中创建的 SVID 是否有效 先决条件 本教程的所需文件可以在 https://github.com/spiffe/spire-tutorials 的 docker-compose/nested-spire 目录中找到。如果尚未克隆存储库，请现在进行克隆。\n在继续之前，请查看以下系统要求：\n64 位 Linux 或 macOS 环境 已安装 Docker 和 Docker Compose （macOS Docker Desktop 包含 Docker Compose） 已安装 Go 1.14.4 或更高版本 第一部分：运行服务 本教程的“nested-spire”主目录包含三个子目录，分别用于存放 SPIRE 部署的配置文件：root、nestedA和nestedB。这些目录包含用于验证 Agents 在 Servers 上的身份的私钥和证书。这些私钥和证书是在场景初始化时使用 Go 应用程序创建的，其详细信息超出了本教程的范围。\n创建共享目录 首先，需要一个本地目录，在服务上进行卷挂载，以在根 SPIRE Agent 和嵌套 SPIRE Servers 之间共享工作负载 API。本教程使用.../spire-tutorials/docker-compose/nested-spire/sharedRootSocket作为共享目录。\n配置根 SPIRE 部署 根 SPIRE 服务器和代理的配置文件与默认的server.conf和agent.conf文件没有改动，但值得注意的是 SPIRE 代理定义绑定工作负载 API socket 的位置：socket_path =\u0026#34;/opt/spire/sockets/workload_api.sock\u0026#34;。稍后将使用此路径来配置卷，以便与嵌套 SPIRE Servers 共享工作负载 API。\n我们在docker-compose.yaml 文件中定义了本教程中的所有服务。在root-agent服务定义中，我们将 SPIRE Agent 容器中的/opt/spire/sockets目录挂载到新的本地目录sharedRootSocket上。在下一节中，当定义嵌套 SPIRE Server 服务时，我们将使用此目录将root-agent套接字挂载到 SPIRE Server 容器上。\nservices: # Root root-server: image: ghcr.io/spiffe/spire-server:1.5.1 hostname: root-server volumes: - ./root/server:/opt/spire/conf/server command: [\u0026#34;-config\u0026#34;, \u0026#34;/opt/spire/conf/server/server.conf\u0026#34;] root-agent: # Share the host pid namespace so this agent can attest the nested servers pid: \u0026#34;host\u0026#34; image: ghcr.io/spiffe/spire-agent:1.5.1 depends_on: [\u0026#34;root-server\u0026#34;] hostname: root-agent volumes: # Share root agent socket to be accessed by nestedA and nestedB servers - ./sharedRootSocket:/opt/spire/sockets - ./root/agent:/opt/spire/conf/agent - /var/run/:/var/run/ command: [\u0026#34;-config\u0026#34;, \u0026#34;/opt/spire/conf/agent/agent.conf\u0026#34;] 配置嵌套 A SPIRE 部署 nestedB SPIRE 部署需要相同的一组配置，但本文不描述这些更改，以避免重复。\nSPIRE Agent 和 Server 可以通过各种插件 进行扩展。UpstreamAuthority 插件 类型允许 SPIRE Server 与现有 PKI 系统集成。UpstreamAuthority 插件可以使用从磁盘加载的 CA 进行证书签名，第三方工具如 AWS 和 Vault 等。嵌套 SPIRE 部署需要使用spire UpstreamAuthority 插件 ，该插件使用同一信任域中的上游 SPIRE Server 获取 SPIRE Server 的中间签名证书。\n*nestedA-server 的配置文件 *包括spire UpstreamAuthority 插件定义，其中root-server被定义为其上游 SPIRE Server。 UpstreamAuthority \u0026#34;spire\u0026#34; { plugin_data = { server_address = \u0026#34;root-server\u0026#34; server_port = 8081 workload_api_socket = \u0026#34;/opt/spire/sockets/workload_api.sock\u0026#34; } } 在docker-compose.yaml 文件中，nestedA-server服务的 Docker Compose 定义将新的本地目录sharedRootSocket作为卷进行挂载。请记住，前一节中将root-agent套接字挂载在该目录上。这样，nestedA-server就可以访问root-agent的工作负载 API 并获取其 SVID。\nnestedA-server: # Share the host pid namespace so this server can be attested by the root agent pid: \u0026#34;host\u0026#34; image: ghcr.io/spiffe/spire-server:1.5.1 hostname: nestedA-server labels: # label to attest nestedA-server against root-agent - org.example.name=nestedA volumes: # Add root agent socket - ./shared/rootSocket:/opt/spire/sockets - ./nestedA/server:/opt/spire/conf/server command: [\u0026#34;-config\u0026#34;, \u0026#34;/opt/spire/conf/server/server.conf\u0026#34;] 创建下游注册项 nestedA-server必须在root-server中注册，以获取其身份，该身份将用于生成 SVID。我们通过在根 SPIRE Server 中创建一个注册项来实现为nestedA-server。\ndocker-compose exec -T root-server \\ /opt/spire/bin/spire-server entry create \\ -parentID \u0026#34;spiffe://example.org/spire/agent/x509pop/$(fingerprint root/agent/agent.crt.pem)\u0026#34; \\ -spiffeID \u0026#34;spiffe://example.org/nestedA\u0026#34; \\ -selector \u0026#34;docker:label:org.example.name:nestedA-server\u0026#34; \\ -downstream parentID标志包含root-agent的 SPIFFE ID。root-agent的 SPIFFE ID 是由x509pop Node Attestor 插件 创建的，该插件将 SPIFFE ID 定义为spiffe://\u0026lt;trust domain\u0026gt;/spire/agent/x509pop/\u0026lt;fingerprint\u0026gt;。shell 脚本中的fingerprint()函数计算证书的 SHA1 指纹。另一个要注意的是downstream选项。设置此选项时，表示该条目描述的是下游 SPIRE Server。 运行场景 使用set-env.sh脚本来运行构成场景的所有服务。该脚本使用之前描述的配置选项启动root、nestedA和nestedB服务。\n确保当前工作目录是.../spire-tutorials/docker-compose/nested-spire，然后运行以下命令：\nbash scripts/set-env.sh 脚本完成后，在另一个终端中运行以下命令以查看所有服务的日志：\ndocker-compose logs -f -t 第二部分：测试部署 现在 SPIRE 部署已准备就绪，让我们测试所配置的场景。\n创建工作负载注册项 为了测试场景，我们创建两个工作负载注册项，一个用于每个嵌套 SPIRE Server（nestedA和nestedB）。测试的目标是演示在嵌套配置中创建的 SVID 在整个信任域中都有效，而不仅仅在生成 SVID 的 SPIRE Server 的范围内。以下命令演示了我们将用于创建这两个工作负载注册项的命令行选项，但你可以使用下面显示的create-workload-registration-entries.sh脚本运行这些命令。\n# nestedA部署的工作负载 docker-compose exec -T nestedA-server \\ /opt/spire/bin/spire-server entry create \\ -parentID \u0026#34;spiffe://example.org/spire/agent/x509pop/$(fingerprint nestedA/agent/agent.crt.pem)\u0026#34; \\ -spiffeID \u0026#34;spiffe://example.org/nestedA/workload\u0026#34; \\ -selector \u0026#34;unix:uid:1001\u0026#34; \\ # nestedB部署的工作负载 docker-compose exec -T nestedB-server \\ /opt/spire/bin/spire-server entry create \\ -parentID \u0026#34;spiffe://example.org/spire/agent/x509pop/$(fingerprint nestedB/agent/agent.crt.pem)\u0026#34; \\ -spiffeID \u0026#34;spiffe://example.org/nestedB/workload\u0026#34; \\ -selector \u0026#34;unix:uid:1001\u0026#34; 示例再次使用fingerprint path/to/nested-agent-cert的形式，以显示-parentID标志指定了嵌套 SPIRE Agent 的 SPIFFE …","relpermalink":"/book/spiffe-and-spire/architecture/nested/","summary":"嵌套 SPIRE 允许将 SPIRE 服务器“链接”在一起，并且所有 SPIRE 服务器都可以在同一信任域中发放身份，这意味着在同一信任域中标识的所有工作负载都可以使用根密钥验证其身份文档。 嵌套拓扑结构通过将一个 SPIRE 代理与每个下游 SPIRE 服务器","title":"SPIRE 嵌套架构：将 SPIRE 服务器链接为同一信任域"},{"content":"SPIFFE 标准提供了一种框架的规范，能够在异构环境和组织边界中引导和发放服务的身份。它定义了一种称为 SPIFFE 可验证身份文档（SVID）的身份文档。\nSVID 本身并不代表一种新的文档类型。相反，我们提出了一个规范，定义了如何将 SVID 信息编码到现有文档类型中。\n本文档定义了一种标准，其中将 X.509 证书用作 SVID。假设读者对 X.509 有基本的了解。关于 X.509 的具体信息，请参考RFC 5280 。\n引言 SPIFFE 的最重要的功能之一是保护进程间通信。核心标准允许进行身份验证，但利用加密身份来构建安全的通信通道也是非常有益的。由于 TLS 被广泛采用，并且使用基于 X.509 的身份验证，将 X.509 用作 SPIFFE SVID 显然是有优势的。\n本规范讨论了将 SVID 信息编码到 X.509 证书中的约束条件，以及如何验证 X.509 SVID。\nSPIFFE ID 在 X.509 SVID 中，对应的 SPIFFE ID 被设置为主题备用名称扩展（SAN 扩展，参见RFC 5280 第 4.2.1.6 节 ）。一个 X.509 SVID 必须恰好包含一个 URI SAN，因此也只包含一个 SPIFFE ID。包含多个 SPIFFE ID 的 SVID 会引入与审计和授权逻辑相关的挑战，包含多个 URI SAN 的 SVID 会引入与 SPIFFE ID 验证相关的挑战。遇到包含多个 URI SAN 的 SVID 的验证器必须拒绝该 SVID。有关更多信息，请参见验证部分。\n一个 X.509 SVID 可以包含任意数量的其他 SAN 字段类型，包括 DNS SAN。\n层级关系 本节讨论了叶证书、根证书和中间证书之间的关系，以及对每个证书的要求。\n叶证书 叶证书是用于标识调用方或资源的 SVID，适用于身份验证过程。叶证书（相对于签名证书，第 3.2 节）是唯一能够用于标识资源或调用方的类型。\n叶证书的 SPIFFE ID 必须具有非根路径组件。如果省略了主题字段，则不需要主题字段，但如果省略了主题字段，则 URI SAN 扩展必须标记为关键扩展，根据RFC 5280 第 4.1.2.6 节 的规定。有关区分叶证书和签名证书的 X.509 特定属性的信息，请参见第 4.1 节。\n签名证书 X.509 SVID 签名证书是具有在密钥用途扩展中设置keyCertSign的证书。它还在基本约束扩展中将CA标志设置为true（参见第 4.1 节）。也就是说，它是一个 CA 证书。\n签名证书应该本身是一个 SVID。如果存在，签名证书的 SPIFFE ID 必须没有路径组件，并且可以位于其发行的任何叶 SVID 的信任域中。签名证书可以用于在相同或不同的信任域中发行进一步的签名证书。\n签名证书不能用于身份验证目的。它们只作为验证材料，并且可以像RFC 5280 中描述的那样以典型的 X.509 方式链接在一起。请参见第 4.3 节和第 4.4 节以获取有关签名证书的 X.509 特定限制的更多信息。\n约束和用途 叶证书和签名证书具有不同的 X.509 属性 - 一些用于安全目的，一些用于支持其特殊的功能。本节描述了两种类型 X.509 SVID 的约束和密钥用法配置。\n基本约束 基本约束 X.509 扩展标识证书是否为签名证书，以及包括该证书在内的有效证书路径的最大深度。它在RFC 5280 第 4.2.1.9 节 中定义。\n有效的 X.509 SVID 签名证书可以设置pathLenConstraint字段。签名证书必须将cA字段设置为true，而叶证书必须将cA字段设置为false。\n名称约束 名称约束指示了一个命名空间，其中后续证书中的所有 SPIFFE ID 必须位于其中。它们用于将受损的签名证书的影响范围限制在命名的信任域内，并在RFC 5280 第 4.2.1.10 节 中定义。本节仅适用于签名证书。\n名称约束的类型与主题备用名称相同。由于 SVID 关心的仅是 SPIFFE ID，并且 SPIFFE ID 被定义为 SAN 类型 URI，因此我们只定义 URI 类型名称约束的语义。\n目前，对 URI 类型名称约束的支持相对较少。不支持它们的库将拒绝此类证书，阻止路径验证成功。虽然名称约束是 SPIFFE 希望使用的 X.509 功能，但作者认识到广泛支持的缺乏可能会给实现和/或部署带来重大痛苦。因此，X.509 SVID 签名证书可以根据实现者的意愿应用 URI 名称约束，但在此领域应谨慎使用。SPIFFE 社区正在努力在各种平台上启用对 URI 名称约束的支持，并且应该预期在未来的版本中，随着广泛支持的实现，本节中定义的要求将变得更加严格。\n密钥用法 密钥用法扩展定义了证书中包含的密钥的用途。当要限制可以用于多个操作的密钥时，可以使用此用法限制。密钥用法扩展在RFC 5280 第 4.2.1.3 节 中定义。\n密钥用法扩展必须在所有 SVID 上设置，并且必须标记为关键扩展。\nSVID 签名证书必须设置keyCertSign。它们可以设置cRLSign。\n叶 SVID 必须设置digitalSignature。它们可以设置keyEncipherment和/或keyAgreement；这些对于 RSA 密钥的证书来说通常只有在需要的情况下才有意义，即使在那种情况下通常也不需要。叶 SVID 不能设置keyCertSign或cRLSign。\n扩展密钥用途 该扩展指示证书中包含的密钥可以用于的一个或多个目的，除了或代替密钥用途扩展中指示的基本目的之外。它在RFC 5280，第 4.2.1.2 节 中定义。\nLeaf SVID 应包括此扩展，并且可以将其标记为关键。当包含时，字段id-kp-serverAuth和id-kp-clientAuth必须设置。\n签名证书可以包括扩展密钥用途。请注意，X.509 证书验证库中间 CA 证书中扩展密钥用途的处理方式因实现而异。有些 X.509 实现会对信任链中其下的所有证书都施加中间 CA 证书中的扩展密钥用途约束，而其他实现则不会。\n验证 本节描述了如何验证 X.509 SVID。该过程使用标准的 X.509 验证，以及一系列 SPIFFE 特定的验证步骤。\n路径验证 对给定 SVID 的信任验证基于标准的 X.509 路径验证，并且必须遵循RFC 5280 的路径验证语义。\n证书路径验证需要提供叶子 SVID 证书和一个或多个 SVID 签名证书。用于验证的签名证书集合称为 CA 捆绑包。实体检索相关 CA 捆绑包的机制不在本文档范围内，而是在 SPIFFE 工作负载 API 规范中定义。\n叶子验证 在验证资源或调用方时，需要进行超出 X.509 标准范围的验证。即，我们必须确保 1）证书是叶子证书，2）签发机构有权签发它。\n在验证用于身份验证目的的 X.509 SVID 时，验证器必须确保基本约束扩展中的CA字段设置为false，并且密钥用途扩展中未设置keyCertSign和cRLSign。验证器还必须确保 SPIFFE ID 的方案设置为spiffe://。包含多个 URI SAN 的 SVID 必须被拒绝。\n随着 URI 名称约束的支持越来越广泛，本文档的未来版本可能会更新本节中设定的要求，以便更好地利用名称约束验证。\n在 SPIFFE 捆绑包中的表示 本节描述了如何将 X509-SVID CA 证书发布到 SPIFFE 捆绑包中，并从中使用。有关 SPIFFE 捆绑包的更多信息，请参阅 SPIFFE 信任域和捆绑包规范。\n发布 SPIFFE 捆绑包元素 给定信任域的 X509-SVID CA 证书在 SPIFFE 捆绑包中表示为RFC 7517 兼容 的 JWK 条目，每个 CA 证书一个条目。\n每个 JWK 条目的use参数必须设置为x509-svid。另外，每个 JWK 条目的kid参数不能设置。\n除了RFC 7517 要求的参数之外，表示 X509-SVID CA 证书的每个条目必须包含具有与该条目表示的基于 Base64 编码的 DER CA 证书相等的值的x5c参数。该值必须包含且仅包含一个 CA 证书，并且该证书应为自签名。\n使用 SPIFFE 捆绑包 从外部信任域使用 SPIFFE 捆绑包时，需要提取 X509-SVID CA 证书以供实际使用。SPIFFE 捆绑包可以包含许多不同类型的 SVID 条目，因此第一步是识别表示 X509-SVID CA 证书的条目。\n对于捆绑包中use参数设置为x509-svid的每个 JWK 条目，请检查x5c参数是否设置并且至少具有一个值。如果x5c未设置或为空，则必须忽略该条目。\nx5c参数的第一个值是该条目表示的 Base64 DER 编码的 CA 证书。如果x5c参数包含多个值，则除第一个值外的所有值都必须被忽略。然后，X509-SVID CA 捆绑包是从x509-svid JWK 条目中提取的 CA 证书的并集。如果捆绑包中不存在x509-svid JWK 条目，则信任域不支持 X509-SVID。\n结论 本文档提出了 X.509 基于 SPIFFE 可验证身份文档的约定和标准。它构成了现实世界 SPIFFE 服务身份验证和 SVID 验证的基础。通过遵守 X.509 SVID 标准，可以构建一个可互操作且与平台无关的身份和身份验证系统。\n附录 A. X.509 字段参考 扩展 字段 描述 主题备用名称 uniformResourceIdentifier 此字段设置为 SPIFFE ID。仅允许一个此字段的实例。 基本约束 CA 如果 SVID 是签名证书，则必须将此字段设置为 true。 基本约束 pathLenConstraint 如果实现者希望执行有限的 CA 层次结构深度约束（例如，从现有私钥基础设施（PKI）继承），则可以设置此字段。 名称约束 permittedSubtrees 如果实现者希望使用 URI 名称约束，则可以设置此字段。这将在本文档的未来版本中要求。 密钥用途 keyCertSign 如果 SVID 是签名证书，则必须设置此字段。 密钥用途 cRLSign 如果 SVID 是签名证书，则可以设置此字段。 密钥用途 keyAgreement 如果 SVID 是叶子证书，则可以设置此字段。 密钥用途 keyEncipherment 如果 SVID 是叶子证书，则可以设置此字段。 密钥用途 digitalSignature 如果 SVID 是叶子证书，则必须设置此字段。 扩展密钥用途 id-kp-serverAuth 此字段可为叶子证书或签名证书设置。 扩展密钥用途 id-kp-clientAuth 此字段可为叶子证书或签名证书设置。 ","relpermalink":"/book/spiffe-and-spire/standard/x509-svid/","summary":"SPIFFE 标准提供了一种框架的规范，能够在异构环境和组织边界中引导和发放服务的身份。它定义了一种称为 SPIFFE 可验证身份文档（SVID）的身份文档。 SVID 本身并不代表一种新的文档类型。相反，我们提出了一个规范，定义了如何","title":"X.509 SPIFFE 可验证身份文档"},{"content":"本文指导你如何在 Linux 和 Kubernetes 上安装 SPIRE 服务器。\n步骤 1：获取 SPIRE 二进制文件 预构建的 SPIRE 发行版可在 SPIRE 下载页面 找到。tarball 包含服务器和代理二进制文件。\n如果需要，你也可以从源代码构建 SPIRE 。\n步骤 2：安装服务器和代理 本入门指南描述了如何在同一节点上安装服务器和代理。在典型的生产部署中，服务器将安装在一个节点上，而一个或多个代理将安装在不同的节点上。\n要安装服务器和代理，请执行以下操作：\n从 SPIRE 下载页面 获取最新的 tarball，然后使用以下命令将其解压缩到 /opt/spire 目录中：\nwget https://github.com/spiffe/spire/releases/download/v1.8.2/spire-1.8.2-linux-amd64-musl.tar.gz tar zvxf spire-1.8.2-linux-amd64-musl.tar.gz sudo cp -r spire-1.8.2/. /opt/spire/ 为了方便起见，将 spire-server 和 spire-agent 添加到你的 $PATH 中：\nsudo ln -s /opt/spire/bin/spire-server /usr/bin/spire-server sudo ln -s /opt/spire/bin/spire-agent /usr/bin/spire-agent 步骤 3：配置服务器 要在 Linux 上配置服务器，你需要：\n配置信任域 配置服务器证书颁发机构（CA），可能包括配置 UpstreamAuthority 插件 配置节点认证插件 配置用于持久化数据的默认 .data 目录 但是，为了简单起见，仅需完成步骤 1、2 和 3 即可快速部署演示目的。\n要配置步骤 1、2 和 4 中的项，请编辑服务器的配置文件，位于 /opt/spire/conf/server/server.conf。\n有关如何配置 SPIRE 的详细信息，请参阅配置 SPIRE ，特别是节点认证和工作负载认证。\n注意，SPIRE 服务器在修改配置后必须重新启动才能生效。\n请参阅安装 SPIRE 代理 ，了解如何安装 SPIRE 代理。\n如何在 Kubernetes 上安装 SPIRE 服务器 本节将逐步向你介绍在 Kubernetes 集群中运行服务器并配置工作负载容器以访问 SPIRE 的方法。\n你必须从包含用于配置的 .yaml 文件的目录中运行所有命令。\n步骤 1：获取所需文件 要获取所需的.yaml 文件，请克隆 https://github.com/spiffe/spire-tutorials 并从 spire-tutorials/k8s/quickstart 子目录复制 .yaml 文件。\n步骤 2：为 SPIRE 组件配置 Kubernetes 命名空间 按照以下步骤配置部署 SPIRE 服务器和 SPIRE 代理的 spire 命名空间。\n创建命名空间：\n$ kubectl apply -f spire-namespace.yaml 运行以下命令，并验证输出中是否列出了spire：\n$ kubectl get namespaces 步骤 3：配置 SPIRE 服务器 要在 Kubernetes 上配置 SPIRE 服务器，你需要：\n创建服务器服务帐户 创建服务器捆绑包 ConfigMap 创建服务器 ConfigMap 创建服务器 StatefulSet 创建服务器服务 有关详细信息，请参阅以下各节。\n创建服务器服务帐户 通过应用 server-account.yaml 配置文件来配置名为 spire-server 的服务帐户：\n$ kubectl apply -f server-account.yaml 通过运行以下命令确认成功创建，并验证该服务帐户是否出现在以下命令的输出中：\n$ kubectl get serviceaccount --namespace spire 创建服务器捆绑包 ConfigMap、角色和 ClusterRoleBinding 为了使服务器能够为代理提供证书以用于在建立连接时验证服务器的身份，服务器需要具备在 spire 命名空间中获取和修补 ConfigMap 对象的功能。\n在这种部署中，代理和服务器共享同一集群，SPIRE 可以配置为定期生成这些证书并将证书内容更新到 ConfigMap 中。为此，服务器需要能够在 Kubernetes RBAC 中获取和修补 ConfigMap 对象。\n通过应用 spire-bundle-configmap.yaml 配置文件来创建名为 spire-bundle 的 ConfigMap：\n$ kubectl apply -f spire-bundle-configmap.yaml 通过运行以下命令确认成功创建，并验证 spire-bundle ConfigMap 是否列在以下命令的输出中：\n$ kubectl get configmaps --namespace spire | grep spire 为了允许服务器读取和写入此 ConfigMap，必须创建一个 ClusterRole，授予 Kubernetes RBAC 相应的特权，并将 ClusterRoleBinding 与前一步创建的服务帐户关联。\n通过应用 server-cluster-role.yaml 配置文件来创建名为 spire-server-trust-role 的 ClusterRole 和相应的 ClusterRoleBinding：\n$ kubectl apply -f server-cluster-role.yaml 通过运行以下命令确认成功创建，并验证 spire-server-trust-role ClusterRole 是否出现在以下命令的输出中：\n$ kubectl get clusterroles --namespace spire | grep spire 创建服务器 ConfigMap 服务器在 Kubernetes ConfigMap 中进行配置，该 ConfigMap 在 server-configmap.yaml 中指定了一些重要的目录，特别是 /run/spire/data和/run/spire/config。这些卷在部署服务器容器时绑定。\n请参阅配置 SPIRE 部分，了解如何配置 SPIRE 服务器的详细信息，特别是节点认证和工作负载认证。\n注意，SPIRE 服务器在修改配置后必须重新启动才能生效。\n使用以下命令将服务器 ConfigMap 应用到你的集群：\n$ kubectl apply -f server-configmap.yaml 创建服务器 StatefulSet 通过应用 server-statefulset.yaml 配置文件来部署服务器：\n$ kubectl apply -f server-statefulset.yaml 这将在 spire 命名空间中创建一个名为 spire-server 的 StatefulSet，并启动一个 spire-server 的 Pod，如以下两个命令的输出所示：\n$ kubectl get statefulset --namespace spire NAME READY AGE spire-server 1/1 86m $ kubectl get pods --namespace spire NAME READY STATUS RESTARTS AGE spire-server-0 1/1 Running 0 86m 当你部署服务器时，它会自动在 SPIRE 服务器的 gRPC 端口上配置 livenessProbe，以确保容器的可用性。\n服务器部署时，绑定到以下表中总结的卷：\n卷 描述 挂载位置 spire-config 引用在前一步中创建的 spire-server ConfigMap /run/spire/config spire-data 服务器的 SQLite 数据库和密钥文件的 hostPath /run/spire/data 创建服务器服务 通过应用 server-service.yaml 配置文件来创建服务器服务：\n$ kubectl apply -f server-service.yaml 通过运行以下命令确认成功创建，并验证 spire 命名空间现在是否有一个名为 spire-server 的服务：\n$ kubectl get services --namespace spire NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE spire-server NodePort 10.107.205.29 \u0026lt;none\u0026gt; 8081:30337/TCP 88m ","relpermalink":"/book/spiffe-and-spire/installation/install-server/","summary":"本文指导你如何在 Linux 和 Kubernetes 上安装 SPIRE 服务器。 步骤 1：获取 SPIRE 二进制文件 预构建的 SPIRE 发行版可在 SPIRE 下载页面 找到。tarball 包含服务器和代理二进制文件。 如果需要，你也可以从源代码构建 SPIRE 。 步骤 2：安装服务器和代理 本","title":"安装 SPIRE 服务器"},{"content":" SPIFFE ID 和 SVID\nX.509 SVID\nJWT SVID\nSPIFFE 工作负载 API\nSPIFFE 工作负载端点\nSPIFFE 信任域和 Bundle\nSPIFFE 联邦\n","relpermalink":"/book/spiffe-and-spire/standard/","summary":"SPIFFE ID 和 SVID X.509 SVID JWT SVID SPIFFE 工作负载 API SPIFFE 工作负载端点 SPIFFE 信任域和 Bundle SPIFFE 联邦","title":"规范"},{"content":"本文将指导你在 SPIRE 服务器中使用 SPIFFE ID 注册工作负载。\n如何创建注册条目 注册条目包含以下内容：\nSPIFFE ID 一个或多个选择器集合 父级 ID 服务器将向代理发送所有有权在该节点上运行的工作负载的注册条目列表。代理缓存这些注册条目并保持其更新。\n在工作负载认证期间，代理会发现选择器并将其与缓存的注册条目中的选择器进行比较，以确定应该为工作负载分配哪些 SVID。\n你可以通过在命令行中发出 spire-server entry create 命令或直接调用 Entry API 来注册工作负载，具体方法请参阅 Entry API 文档 。可以使用 spire-server entry update 命令修改现有条目。\n在 Kubernetes 上运行时，调用 SPIRE 服务器的常见方法是通过在运行 SPIRE 服务器的 Pod 上使用kubectl exec命令。例如：\nkubectl exec -n spire spire-server-0 -- \\ /opt/spire/bin/spire-server entry create \\ -spiffeID spiffe://example.org/ns/default/sa/default \\ -parentID spiffe://example.org/ns/spire/sa/spire-agent \\ -selector k8s:ns:default \\ 有关 spire-server entry create 和 spire-server entry update 命令和选项的更多信息，请参阅 SPIRE 服务器参考指南 。\n如何注册工作负载 通过在 SPIRE 服务器中创建一个或多个注册条目来注册工作负载。要注册工作负载，需要告诉 SPIRE：\n分配给在工作负载有权运行的节点上运行的代理的 SPIFFE ID。 运行在这些机器上的工作负载本身的属性。 1. 定义代理的 SPIFFE ID 分配给代理的 SPIFFE ID 可能是作为节点认证过程的一部分自动分配的 ID。例如，当代理经过 AWS IID 节点认证时，会自动分配形式为 spiffe://example.org/agent/aws_iid/ACCOUNT_ID/REGION/INSTANCE_ID 的 SPIFFE ID。\n或者，可以通过创建一个指定了选择器的注册条目 来为一个或多个代理分配 SPIFFE ID。例如，可以通过创建以下注册条目将 SPIFFE ID spiffe://acme.com/web-cluster 分配给在标记 app 设置为 webserver 的一组 EC2 实例上运行的任何 SPIRE 代理：\nspire-server entry create \\ -node \\ -spiffeID spiffe://acme.com/web-cluster \\ -selector tag:app:webserver 选择器是 SPIRE 可以在发出身份之前验证的节点或工作负载的本机属性。单个注册条目可以包含节点选择器或工作负载选择器，但不能同时包含两者。请注意上述命令中的 -node 标志，它表示此命令正在指定节点选择器。\n根据工作负载应用程序运行的平台或架构，提供了不同的选择器。\n平台 请访问 Kubernetes GitHub AWS GitHub Azure GitHub 2. 定义工作负载的 SPIFFE ID 一旦代理或代理有一个分配的 SPIFFE ID，就可以创建另一个注册条目来标识在调用该代理公开的工作负载 API 时的特定工作负载。\n例如，要创建一个注册条目，以匹配在标识为 spiffe://acme.com/web-cluster 的代理上运行的 Unix 组 ID 1000 下运行的 Linux 进程，可以使用以下命令：\nspire-server entry create \\ -parentID spiffe://acme.com/web-cluster \\ -spiffeID spiffe://acme.com/webapp \\ -selector unix:gid:1000 平台 请访问 Unix GitHub Kubernetes GitHub Docker GitHub 如何列出注册条目 要列出所有现有的注册条目，请使用命令 spire-server entry show。\n要将注册条目筛选为与特定 SPIFFE ID、父级 SPIFFE ID 或注册条目 ID 匹配的条目，请分别使用 -spiffeID、-parentID、-selector 或 -entryID 标志。\n请注意，每个注册条目都有一个唯一的注册条目 ID，但是多个注册条目可以指定相同的 SPIFFE ID。\n例如，要列出与标记 app 设置为 webserver 的一组 EC2 实例匹配的所有注册条目，请运行以下命令：\nspire-server entry show -selector tag:app:webserver 有关 spire-server entry show 命令和选项的更多信息，请参阅SPIRE 服务器参考指南 。\n如何删除注册条目 要永久删除现有的注册条目，请使用 spire-server entry delete 命令，并指定相关的注册条目 ID。\n例如：\nspire-server entry delete -entryID 92f4518e-61c9-420d-b984-074afa7c7002 有关 spire-server entry delete 命令和选项的更多信息，请参阅 SPIRE 服务器参考指南 。\n将工作负载映射到多个节点 工作负载注册条目可以有一个父级 ID。这可以是特定节点的 SPIFFE ID（即通过节点认证获得的代理的 SPIFFE ID），也可以是节点注册条目（有时称为节点别名/集合）的 SPIFFE ID。节点别名（或集合）是具有相似特征的一组节点，它们被赋予了一个共享的身份。节点注册条目具有节点选择器，要求节点至少具有这些选择器才能符合共享的身份。这意味着具有至少与节点注册条目中定义的选择器相同的任何节点都被赋予该别名（或属于该节点集）。当工作负载注册条目使用节点别名的 SPIFFE ID 作为父级时，具有该别名的任何节点都有权为该工作负载获取 SVID\n","relpermalink":"/book/spiffe-and-spire/configuration/registering/","summary":"本文将指导你在 SPIRE 服务器中使用 SPIFFE ID 注册工作负载。 如何创建注册条目 注册条目包含以下内容： SPIFFE ID 一个或多个选择器集合 父级 ID 服务器将向代理发送所有有权在该节点上运行的工作负载的注册条目列表。代理缓存这些注册条目","title":"注册工作负载"},{"content":"本指南将帮助你快速入门实际中的“工作负载载入”。\n在本指南的一部分中，你将会：\n将 Istio Bookinfo 示例部署到 Elastic Kubernetes Service (EKS) 集群中。 将 ratings 应用程序部署为 AWS ECS 任务，并将其加入服务网格。 验证 Kubernetes Pod(s) 与 AWS ECS 任务之间的流量。 本指南旨在演示工作负载加入功能的易于跟随的演示。\n为了保持简单，你无需像在生产部署的情况下那样配置基础设施。\n具体而言：\n你无需设置可路由的 DNS 记录。 你无需使用受信任的 CA 机构（例如 Let’s Encrypt）。 在继续之前，请确保完成以下先决条件：\n创建一个 EKS 集群，以便安装 TSB 和示例应用程序。 按照 TSB 演示 安装说明进行安装。 按照 安装 Bookinfo 示例 中的说明进行操作。 按照 启用工作负载加入 中的说明进行操作。 配置工作负载的 WorkloadGroup 和 Sidecar\n加入 AWS ECS 任务\n","relpermalink":"/book/tsb/setup/workload-onboarding/quickstart/aws-ecs/","summary":"本指南将帮助你快速入门实际中的“工作负载载入”。 在本指南的一部分中，你将会： 将 Istio Bookinfo 示例部署到 Elastic Kubernetes Service (EKS) 集群中。 将 ratings 应用程序部署为 AWS ECS 任务，并将其加入服务网格。 验证 Kubernetes Pod(s) 与 AWS ECS 任务之间的流量。 本指南旨在演示","title":"在 AWS ECS 上使用工作负载快速入门"},{"content":"在开始之前，你必须具备以下条件：\nVault 1.3.1 或更新版本 Vault Injector 0.3.0 或更新版本 Elasticsearch 6.x 或 7.x，带有基本许可证或更高版本 设置 Vault 安装 Vault（不需要在 Kubernetes 集群中安装，但应该能够从 Kubernetes 集群内部访问）。必须将 Vault Injector（agent-injector）安装到集群中，并配置以注入 sidecar。Helm Chart v0.5.0+ 会自动完成此操作，它安装了 Vault 0.12+ 和 Vault-Injector 0.3.0+。下面的示例假设 Vault 安装在 tsb 命名空间中。\n有关详细信息，请查看 Vault 文档 。\nhelm install --name=vault --set=\u0026#39;server.dev.enabled=true\u0026#39; ./vault-helm 启动启用安全性的 Elasticsearch 如果你管理自己的 Elasticsearch 实例，你需要启用安全性并设置超级用户密码。\n为 Vault 创建角色 首先，使用超级用户身份，创建一个角色，该角色将允许 Vault 通过向 Elasticsearch 发送 POST 请求获得最低权限。\ncurl -k \\ -X POST \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;cluster\u0026#34;: [\u0026#34;manage_security\u0026#34;]}\u0026#39; \\ https://\u0026lt;超级用户用户名\u0026gt;:\u0026lt;超级用户密码\u0026gt;@\u0026lt;elastic-ip\u0026gt;:\u0026lt;elastic-port\u0026gt;/_xpack/security/role/vault 接下来，为 Vault 创建与该角色关联的用户。\ncurl -k \\ -X POST \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d @data.json \\ https://\u0026lt;超级用户用户名\u0026gt;:\u0026lt;超级用户密码\u0026gt;@\u0026lt;elastic-ip\u0026gt;:\u0026lt;elastic-port\u0026gt;/_xpack/security/user/vault 在此示例中，data.json 的内容如下：\n{ \u0026#34;password\u0026#34;: \u0026#34;\u0026lt;vault-elastic 密码\u0026gt;\u0026#34;, \u0026#34;roles\u0026#34;: [\u0026#34;vault\u0026#34;], \u0026#34;full_name\u0026#34;: \u0026#34;Hashicorp Vault\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;plugin_name\u0026#34;: \u0026#34;Vault Plugin Database Elasticsearch\u0026#34;, \u0026#34;plugin_url\u0026#34;: \u0026#34;https://github.com/hashicorp/vault-plugin-database-elasticsearch\u0026#34; } } 现在，已配置好 Elasticsearch 用户，可以被 Vault 使用。\n为 Elasticsearch 设置数据库秘密引擎 在 Vault 中启用数据库秘密引擎。\nvault secrets enable database 预期输出：\nSuccess! Enabled the database secrets engine at: database/ 默认情况下，秘密引擎在与引擎同名的路径上启用。要在不同路径上启用秘密引擎，请使用 -path 参数。\n使用插件和连接信息配置 Vault。你需要提供证书、密钥和 CA 捆绑（根 CA 和中间 CA）：\nvault write database/config/tsb-elastic \\ plugin_name=\u0026#34;elasticsearch-database-plugin\u0026#34; \\ allowed_roles=\u0026#34;tsb-elastic-role\u0026#34; \\ username=vault \\ password=\u0026lt;vault-elastic密码\u0026gt; \\ url=https://\u0026lt;elastic-ip\u0026gt;:\u0026lt;elastic-port\u0026gt; \\ ca_cert=es-bundle.ca 配置一个将 Vault 中的名称映射到 Elasticsearch 中的角色定义的角色。\nvault write database/roles/tsb-elastic-role \\ db_name=tsb-elastic \\ creation_statements=\u0026#39;{\u0026#34;elasticsearch_role_definition\u0026#34;: {\u0026#34;cluster\u0026#34;:[\u0026#34;manage_index_templates\u0026#34;,\u0026#34;monitor\u0026#34;],\u0026#34;indices\u0026#34;:[{\u0026#34;names\u0026#34;:[\u0026#34;*\u0026#34;],\u0026#34;privileges\u0026#34;:[\u0026#34;manage\u0026#34;,\u0026#34;read\u0026#34;,\u0026#34;write\u0026#34;]}],\u0026#34;applications\u0026#34;:[],\u0026#34;run_as\u0026#34;:[],\u0026#34;metadata\u0026#34;:{},\u0026#34;transient_metadata\u0026#34;:{\u0026#34;enabled\u0026#34;:true}}}\u0026#39; \\ default_ttl=\u0026#34;1h\u0026#34; \\ max_ttl=\u0026#34;24h\u0026#34; Success! Data written to: database/roles/internally-defined-role 为验证配置，通过使用角色名称从 /creds 终端点生成新凭据：\nvault read database/creds/tsb-elastic-role Key Value --- ----- lease_id database/creds/tsb-elastic-role/jZHuJvZeEOvGJhfFixVcwOyB lease_duration 1h lease_renewable true password A1a-SkZ9KgF7BJGn2FRH username v-root-tsb-elastic-rol-2eRGSeD09gTNzYHf7a2G-1610542455 你可以检查 Elasticsearch 集群，以验证新创建的凭据是否存在：\ncurl -u \u0026#34;vault:\u0026lt;vault-elastic密码\u0026gt;\u0026#34; -ks -XGET https://\u0026lt;超级用户用户名\u0026gt;:\u0026lt;超级用户密码\u0026gt;@\u0026lt;elastic-ip\u0026gt;:\u0026lt;elastic-port\u0026gt;/_xpack/security/user/v-root-tsb-elastic-rol-2eRGSeD09gTNzYHf7a2G-1610542455|jq \u0026#39;.\u0026#39; { \u0026#34;v-root-tsb-elastic-rol-2eRGSeD09gTNzYHf7a2G-1610542455\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;v-root-tsb-elastic-rol-2eRGSeD09gTNzYHf7a2G-1610542455\u0026#34;, \u0026#34;roles\u0026#34;: [ \u0026#34;v-root-tsb-elastic-rol-2eRGSeD09gTNzYHf7a2G-1610542455\u0026#34; ], \u0026#34;full_name\u0026#34;: null, \u0026#34;email\u0026#34;: null, \u0026#34;metadata\u0026#34;: {}, \u0026#34;enabled\u0026#34;: true } } 设置 Kubernetes 秘密引擎 配置一个名为 “database” 的策略，这是一个非常不受限制的策略，在生产环境中，我们应该更加安全。\nvault policy write es-auth - \u0026lt;\u0026lt;EOF path \u0026#34;database/creds/internally-defined-role\u0026#34; { capabilities = [\u0026#34;read\u0026#34;] } EOF Success! Uploaded policy: es-auth 配置 Vault 以启用对 Kubernetes API 的访问。此示例假设你正在 Vault pod 中使用 kubectl exec 运行命令。如果不是这样，你需要找到正确的 JWT 令牌、Kubernetes API URL（Vault 将用于连接到 Kubernetes 的 URL）\n以及 vaultserver 服务帐户的 CA 证书，如 Vault 文档 中所述。\nvault auth enable kubernetes vault write auth/kubernetes/config \\ token_reviewer_jwt=\u0026#34;$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\u0026#34; \\ kubernetes_host=https://${KUBERNETES_PORT_443_TCP_ADDR}:443 \\ kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt 将我们的数据库策略附加到将使用它的服务帐户上，即管理平面和控制平面命名空间的 OAP 服务帐户：\nvault write auth/kubernetes/role/es \\ bound_service_account_names=tsb-oap,istio-system-oap,default \\ bound_service_account_namespaces=tsb,istio-system \\ policies=es-auth \\ ttl=1h 注入凭据到 Pod 管理平面 要在管理平面中与 Elasticsearch 一起使用 Vault Agent Injector，请向 ManagementPlane 自定义资源添加以下部署 pod 注释和环境变量。\nspec: components: oap: kubeSpec: deployment: env: - name: SW_ES_SECRETS_MANAGEMENT_FILE value: /vault/secrets/credentials podAnnotations: vault.hashicorp.com/agent-inject: \u0026#34;true\u0026#34; vault.hashicorp.com/agent-init-first: \u0026#34;true\u0026#34; vault.hashicorp.com/agent-inject-secret-credentials: \u0026#34;database/creds/tsb-elastic-role\u0026#34; vault.hashicorp.com/agent-inject-template-credentials: | {{- with secret \u0026#34;database/creds/tsb-elastic-role\u0026#34; -}} user={{ .Data.username }} password={{ .Data.password }} trustStorePass=tetrate {{- end -}} vault.hashicorp.com/role: \u0026#34;es\u0026#34; 控制平面 控制平面也需要连接到 Elasticsearch 的凭据。你需要将 Vault 注入器的 pod 注释添加到控制平面中的 OAP 组件中。以下是带有所需更改的 YAML 片段。\nspec: components: oap: kubeSpec: deployment: env: - name: SW_ES_SECRETS_MANAGEMENT_FILE value: /vault/secrets/credentials podAnnotations: vault.hashicorp.com/agent-init-first: \u0026#34;true\u0026#34; vault.hashicorp.com/agent-inject: \u0026#34;true\u0026#34; vault.hashicorp.com/agent-inject-secret-credentials: database/creds/tsb-elastic-role vault.hashicorp.com/agent-inject-template-credentials: | {{- with …","relpermalink":"/book/tsb/operations/vault/elasticsearch/","summary":"如何将 Vault Agent Injector 与 Elasticsearch 结合使用。","title":"Elasticsearch 凭据"},{"content":"在某些情况下，由于 Elasticsearch 索引中的数据模型更改，需要清除现有的索引和模板，以便新版本的 OAP 可以正常运行。\n下面的步骤描述了如何从 Elasticsearch 中清除此类数据并确保 OAP 组件将正确启动。\n减少副本数 确保在继续之前按照步骤 1 和步骤 2 进行操作 1. 将控制平面命名空间中的 oap-deployment 部署的副本数减少为 0。\n注意 这需要在 TSB 中登记的所有 CP 集群中执行。 kubectl -n ${CONTROL_NAMESPACE} scale deployment oap-deployment --replicas=0 2. 将管理命名空间中的 oap 部署的副本数减少为 0。\nkubectl -n ${MANAGEMENT_NAMESPACE} scale deployment oap --replicas=0 3. 执行以下命令以删除 Elasticsearch 中的模板和索引。\nes_host=localhost es_port=9200 es_user=\u0026lt;USER\u0026gt; es_pass=\u0026lt;PASS\u0026gt; for tmpl in $(curl -u \u0026#34;$es_user:$es_pass\u0026#34; -sS https://$es_host:$es_port/_cat/templates | \\ egrep \u0026#34;skywalking\u0026#34; | \\ awk \u0026#39;{print $1}\u0026#39;); do echo \u0026#34;$tmpl: \u0026#34;; curl -u \u0026#34;$es_user:$es_pass\u0026#34; -sS https://$es_host:$es_port/_index_template/$tmpl -XDELETE; echo \u0026#34;\\n\u0026#34;; done for idx in $(curl -u \u0026#34;$es_user:$es_pass\u0026#34; https://$es_host:$es_port/_cat/indices | \\ egrep \u0026#34;skywalking\u0026#34; | \\ awk \u0026#39;{print $3}\u0026#39;); do echo \u0026#34;$idx: \u0026#34;; curl -u \u0026#34;$es_user:$es_pass\u0026#34; https://$es_host:$es_port/$idx -XDELETE; echo \u0026#34;\\n\u0026#34;; done 选择日期 如果你只想删除特定日期的索引，而不是删除所有内容，你只需将上面的命令添加到 grep 中，例如 ...curl -u \u0026#34;$es_user:$es_pass\u0026#34; https://$es_host:$es_port/_cat/indices | grep \u0026#34;20221006\u0026#34;| ... 以此示例，你将删除在 2022 年 10 月 6 日创建的所有索引。 Elasticsearch 选项 上面的命令假定纯 HTTP Elasticsearch 实例没有身份验证。除了适当设置 \u0026lt;es_host\u0026gt; 和 \u0026lt;es_port\u0026gt; 外，如果需要，你还需要通过向上面的 curl 命令提供 -u \u0026lt;es_user\u0026gt;:\u0026lt;es_pass\u0026gt; 来添加基本认证，或者如果需要，将模式设置为 https。 4. 增加管理命名空间中的 oap 部署的副本数。\nkubectl -n ${MANAGEMENT_NAMESPACE} scale deployment oap --replicas=1 密切关注管理平面命名空间中新的 OAP 容器的日志，如果没有错误并且容器正常运行，可以继续执行下一步。\nOAP 可用性 确保管理平面中的 OAP 在继续执行此过程之前能够正确启动。该组件的管理平面 pod 会创建系统所需的索引模板和索引，因此在继续扩展控制平面组件之前，你需要确保 OAP 正常运行。 5. 增加控制平面命名空间中的 oap-deployment 部署的副本数。\n这需要在 TSB 中登记的所有 CP 集群中执行。 kubectl -n ${CONTROL_NAMESPACE} scale deployment oap-deployment \\ --replicas=1 ","relpermalink":"/book/tsb/operations/elasticsearch/wipe-elastic/","summary":"按照描述的步骤清理 Elasticsearch 数据。","title":"Elasticsearch 清理流程"},{"content":"无论我们是使用 TSB 的IngressGateway还是 Istio 的Gateway和VirtualService资源将外部流量路由到我们的服务，都可能会遇到我们暴露的路由问题。在本文档中，我们将展示一些最常见的故障场景以及如何进行故障排查。\n缺少配置 首先要检查的一件事是我们在 TSB 中创建的配置是否存在于目标集群中。例如，在这种情况下：\n$ curl -vk http://helloworld.tetrate.io/hello [ ... ] \u0026gt; GET /hello HTTP/1.1 \u0026gt; Host: helloworld.tetrate.io \u0026gt; User-Agent: curl/7.81.0 \u0026gt; Accept: */* \u0026gt; * Mark bundle as not supporting multiuse \u0026lt; HTTP/1.1 404 Not Found \u0026lt; date: Wed, 27 Apr 2022 14:46:41 GMT \u0026lt; server: istio-envoy \u0026lt; content-length: 0 \u0026lt; 我们得到了一个 404 的 HTTP 响应（未找到），对于刚刚配置的入口路由，这是一个问题。要检查的第一件事是资源状态 。确保你的入口资源的状态为ACCEPTED。\n注意 404 Envoy 返回的 404 响应不包含正文，如上所示。如果你看到 404 和一些“未找到”的消息，通常表示路由配置正确，但你正在访问错误的 URL。例如：\n$ curl -vk https://httpbin.tetrate.io/foobar [ ... ] \u0026lt; HTTP/2 404 \u0026lt; server: istio-envoy \u0026lt; date: Wed, 27 Apr 2022 14:53:32 GMT \u0026lt; content-type: text/html \u0026lt; content-length: 233 \u0026lt; access-control-allow-origin: * \u0026lt; access-control-allow-credentials: true \u0026lt; x-envoy-upstream-service-time: 47 \u0026lt; \u0026lt;!DOCTYPE HTML PUBLIC \u0026#34;-//W3C//DTD HTML 3.2 Final//EN\u0026#34;\u0026gt; \u0026lt;title\u0026gt;404 Not Found\u0026lt;/title\u0026gt; \u0026lt;h1\u0026gt;Not Found\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.\u0026lt;/p\u0026gt; 你在那里看到的 HTML 代码是应用程序本身发送的，这意味着路由正常工作，但你正在访问错误的路径。\n$ tctl experimental status workspace hello --tenant tetrate NAME STATUS LAST EVENT MESSAGE hello FAILED The following children resources have issues: organizations/tetrate/tenants/tetrate/workspaces/hello/gatewaygroups/hello 例如，上面的输出表明hello工作区中的某些内容有问题，具体来说是名为hello的网关组中。\n$ tctl experimental status gatewaygroup hello --workspace hello --tenant tetrate NAME STATUS LAST EVENT MESSAGE hello FAILED The following children resources have issues: organizations/tetrate/tenants/tetrate/workspaces/hello/gatewaygroups/hello/virtualservices/hello 并且在为此路由部署的VirtualService中似乎存在问题。\n$ tctl experimental status virtualservice hello --gatewaygroup hello --workspace hello --tenant tetrate NAME STATUS LAST EVENT MESSAGE hello FAILED MPC_FAILED no gateway object found for reference \u0026#34;helloworld/hellogw\u0026#34; in \u0026#34;organizations/tetrate/tenants/tetrate/workspaces/hello/gatewaygroups/hello/virtualservices/hello\u0026#34; 此时我们可以确定缺少配置的原因实际上是与配置本身有关的问题，因此可以进行修复以使其部署，从而修复我们之前看到的 404 错误。状态对象中的错误消息将引导你找到错误所在。\nEnvoy 访问日志 X-REQUEST-ID HEADER 你可以发送 X-REQUEST-ID 头以关联日志中的请求。你可以使用任意随机字符串作为请求 ID。Envoy 代理将在其创建的每个日志语句中包含该 ID。以下是示例：\n$ curl -vk -H \u0026#34;X-REQUEST-ID:4e3e3e04-6509-43d4-9a97-52b7b2cea0e8\u0026#34; http://helloworld.tetrate.io/hello TSB 配置了 Istio 以便 Envoy 在stdout中打印访问日志，并仅对特定模块的错误进行记录。如果从istiod接收到的配置无效，你将会看到一条消息，但对于失败的请求，你将会看到一个带有一些标志的503响应，这些标志在Envoy 文档 中有说明（请参阅%RESPONSE_FLAGS%部分）。让我们看看以下示例。\n$ curl -vk https://httpbin.tetrate.io/foobar [ ... ] \u0026lt; HTTP/2 503 \u0026lt; content-length: 19 \u0026lt; content-type: text/plain \u0026lt; date: Wed, 27 Apr 2022 15:02:19 GMT \u0026lt; server: istio-envoy \u0026lt; no healthy upstream 如果我们查看这个请求的访问日志，我们可以看到：\n[2022-04-27T15:02:20.472Z] \u0026#34;GET /foobar HTTP/2\u0026#34; 503 UH no_healthy_upstream - \u0026#34;-\u0026#34; 0 19 0 - \u0026#34;X.X.X.X\u0026#34; \u0026#34;curl/7.81.0\u0026#34; \u0026#34;55fef75a-70e5-449f-ad01-cd34960f465c\u0026#34; \u0026#34;httpbin.tetrate.io\u0026#34; \u0026#34;-\u0026#34; outbound|8000||httpbin.httpbin.svc.cluster.local - 10.16.0.20:8443 X.X.X.X:36009 httpbin.tetrate.io httpbin 好的，我们可以看到日志的时间戳，请求的一些 HTTP 信息（方法、路径、协议），然后可以看到响应代码503，后跟标志UH，与我们在响应中得到的消息相匹配，说明没有可用的上游服务。当前用于此入口路由的VirtualService是：\nkind: VirtualService metadata: annotations: tsb.tetrate.io/organization: tetrate tsb.tetrate.io/tenant: tetrate xcp.tetrate.io/workspace: httpbin xcp.tetrate.io/gatewayGroup: httpbin name: httpbin namespace: httpbin spec: gateways: - httpbin/httpbingw hosts: - httpbin.tetrate.io http: - name: httpbin route: - destination: host: httpbin.httpbin.svc.cluster.local 如果我们检查目标服务httpbin的端点：\nkubectl get ep NAME ENDPOINTS AGE httpbin \u0026lt;none\u0026gt; 48m httpbin-gateway 10.16.0.20:8080,10.16.0.20:8443,10.16.0.20:15443 48m 我们没有有效的端点，这是导致问题的原因。接下来的步骤是检查为什么我们的服务没有端点（选择器错误、计算问题等）。\n调试或跟踪日志 还有其他情况可能需要提高 Envoy 日志的详细程度，以找出问题所在。假设我们创建了一个应用程序，它执行：\n$ curl localhost:8090/ super funny stuff... 然后我们将其部署到我们的服务网格。一旦所有配置就绪，我们发现它实际上无法正常工作…\n$ curl -v http://fun.tetrate.io/ [ ... ] \u0026gt; GET / HTTP/1.1 \u0026gt; Host: fun.tetrate.io \u0026gt; User-Agent: curl/7.81.0 \u0026gt; Accept: */* \u0026gt; * Mark bundle as not supporting multiuse \u0026lt; HTTP/1.1 502 Bad Gateway \u0026lt; content-length: 87 \u0026lt; content-type: text/plain \u0026lt; date: Thu, 28 Apr 2022 11:03:48 GMT \u0026lt; server: istio-envoy \u0026lt; x-envoy-upstream-service-time: 3 \u0026lt; upstream connect error or disconnect/reset before headers. reset reason: protocol error HTTP 状态代码502表示“网关错误”，因此问题不应出现在我们的应用程序中。\n检查网关的日志显示：\n[2022-04-28T10:58:59.087Z] \u0026#34;GET / HTTP/1.1\u0026#34; 502 - via_upstream - \u0026#34;-\u0026#34; 0 87 3 3 \u0026#34;X.X.X.X\u0026#34; \u0026#34;curl/7.81.0\u0026#34; \u0026#34;3d1f5c5c-e788-4f55-ba0f-00f15b749767\u0026#34; \u0026#34;fun.tetrate.io\u0026#34; \u0026#34;10.16.0.40:8090\u0026#34; outbound|8090||faulty-http.faulty-http.svc.cluster.local 10.16.2.34:50440 10.16.2.34:8080 X.X.X.X:20985 - faulty-http 以及 Sidecar 显示：\n[2022-04-28T10:58:59.089Z] \u0026#34;GET / HTTP/1.1\u0026#34; 502 UPE upstream_reset_before_response_started{protocol_error} - \u0026#34;-\u0026#34; 0 87 1 - \u0026#34;X.X.X.X\u0026#34; \u0026#34;curl/7.81.0\u0026#34; \u0026#34;3d1f5c5c-e788-4f55-ba0f-00f15b749767\u0026#34; \u0026#34;fun.tetrate.io\u0026#34; \u0026#34;10.16.0.40:8090\u0026#34; inbound|8090|| 127.0.0.6:36281 10.16.0.40:8090 X.X.X.X:0 …","relpermalink":"/book/tsb/troubleshooting/gateway-troubleshooting/","summary":"在 TSB 中排查入口路由问题。","title":"Ingress Gateway 故障排除"},{"content":" Agent Configuration\nAWS Identity\nAWS Identity Matcher\nCondition\nCore types\nHost Info\nJWT Identity\nJWT Identity Matcher\nJWT Issuer\nOnboarding Configuration\nOnboarding Policy\nTransport layer security config\nWorkload Auto Registration\nWorkload Configuration\nWorkload Identity\nWorkload Registration\n","relpermalink":"/book/tsb/refs/onboarding/","summary":"Agent Configuration\nAWS Identity\nAWS Identity Matcher\nCondition\nCore types\nHost Info\nJWT Identity\nJWT Identity Matcher\nJWT Issuer\nOnboarding Configuration\nOnboarding Policy\nTransport layer security config\nWorkload Auto Registration\nWorkload Configuration\nWorkload Identity\nWorkload Registration","title":"onboard"},{"content":" 注意 默认情况下，控制平面中的 OAP 不会公开 RED 指标。 要公开 RED 遥测数据，请在启动 OAP 时设置环境变量 SW_EXPORTER_ENABLE_OC=true。 TSB 提供一个与 Prometheus 兼容的单一端点，通过 OAP 服务公开来自 sidecar 的 RED 应用程序指标。每个控制平面集群都公开一个供 Prometheus 抓取的端点，可以使用以下命令查询：\nkubectl port-forward -n \u0026lt;controlplane-namespace\u0026gt; svc/oap 1234:1234 \u0026amp; curl localhost:1234/metrics 导出的 RED 指标包括：\n请求状态码 # HELP tsb_oap_service_status_code 状态码的数量 # TYPE tsb_oap_service_status_code 计数器 tsb_oap_service_status_code{status=\u0026#34;\u0026lt;STATUS|ALL\u0026gt;\u0026#34;,svc=\u0026#34;SERVICE_NAME\u0026#34;,} COUNT 请求延迟 # HELP tsb_oap_service_latency_sum 延迟的总和 # TYPE tsb_oap_service_latency_sum 计数器 tsb_oap_service_latency_sum{svc=\u0026#34;SERVICE_NAME\u0026#34;,} SUM # HELP tsb_oap_service_latency_count 请求的数量 # TYPE tsb_oap_service_latency_count 计数器 tsb_oap_service_latency_count{svc=\u0026#34;SERVICE_NAME\u0026#34;,} COUNT ","relpermalink":"/book/tsb/operations/telemetry/red-metrics/","summary":"从 Dataplane 的 Envoy Sidecar 收集 RED 指标。","title":"Sidecar RED 指标"},{"content":"TSB 主控制平面提供了 Kubernetes CRD（自定义资源定义），您可以使用它们来配置您的应用程序。请参阅以下链接以获取更多详细信息：\n如何启用 GitOps 功能 如何使用 GitOps 功能 完整的 TSB Kubernetes CRD 可以在此处 下载。\n","relpermalink":"/book/tsb/reference/k8s-api/guide/","summary":"TSB 主控制平面提供了 Kubernetes CRD（自定义资源定义），您可以使用它们来配置您的应用程序。请参阅以下链接以获取更多详细信息： 如何启用 GitOps 功能 如何使用 GitOps 功能 完整的 TSB Kubernetes CRD 可以在此处 下载。","title":"TSB CRD 参考"},{"content":"本节重点介绍组成 TSB 的架构及其含义。你将获得以下方面的知识：\n我们的可靠部署理念是 TSB 架构背后的驱动力 数据平面，由 Envoy 提供支持 由 Istio 提供支持的本地控制平面 Tetrate Service Bridge 的全局控制平面 Tetrate Service Bridge 的管理平面 拥有管理平面的重要性 Tetrate Service Bridge 中的 Envoy 扩展 读完本节后，你应该清楚地了解 TSB 架构的每个元素以及它们如何协同工作以帮助你管理环境。\n部署理念 TSB 的架构基于以故障域为中心的强大部署理念。此方法涉及识别和隔离关键系统发生故障时受影响的基础设施部分。这些故障域分为三类：\n物理故障：包括主机故障、机架故障、硬盘驱动器故障和资源短缺。 逻辑故障：逻辑故障包括错误配置、安全漏洞以及与依赖项和数据相关的问题。 数据故障：这些涉及数据库问题、错误更新、复制失败和备份问题。 为了创建可靠的系统，这些故障域被分组到孤岛中并作为独立实例进行复制。所得系统的可靠性取决于最小化副本之间的相互依赖性。\n物理故障域 在现代云环境中，需要考虑的一组物理故障域已被浓缩为一个简单的概念，称为可用区（Zone）。然而，我们必须记住，可用性区域并不总是彼此完全隔离，正如云提供商中断所证明的那样。\n为了解决这个问题，云提供商将多个可用区域组织到一个称为区域的更高级别的故障域中。虽然同一区域中的多个可用区发生故障的情况并不罕见，但多个区域发生故障的可能性很小。\n因此，在考虑可用区和区域（Region）作为物理故障域的同时，还必须考虑逻辑故障域。\n逻辑故障 逻辑故障很大程度上取决于我们自己的应用程序架构，并且往往更难以推理。作为应用程序开发人员，要考虑的关键要素是应用程序的部署、配置、数据、依赖项以及它的公开方式（负载平衡、DNS 等）\n在典型的微服务架构中，我们设计应用程序来使用云提供商数据库，使用单个区域中跨可用区的云提供商原语运行它（使用 Kubernetes、VM 自动缩放组、容器 aaS 产品等），并尽可能与部署在同一区域的依赖进行通信。当这不可能时，我们依赖于 DNS 等全局负载平衡层。这是我们的故障域孤岛，我们将其复制到其他区域以实现可用性，以某种方式处理数据复制（这是一个难题，也是影响所有孤岛的常见故障域）。\n保持本地化 创建没有耦合故障域的隔离孤岛的最简单方法之一是在每个孤岛中运行关键服务的独立副本。我们可以说这些副本对于筒仓来说是“本地”的，因为它们共享相同的故障域，包括物理故障域，这意味着它们位于附近。\n在 Tetrate Service Bridge，我们遵循这种“保持本地化”模式，在运行应用程序的每个计算集群中运行 Istio 控制平面的实例。换句话说，我们部署 Istio，使其物理故障域与你的应用程序保持一致。此外，我们确保这些实例是松散耦合的，不需要彼此直接通信，从而最大限度地减少它们需要在孤岛之外进行的任何通信。\n这意味着你运行的每个集群都是一座孤岛，其中一个集群的故障不会导致其他集群的故障。此外，由于控制平面位于集群本地，因此它了解正在发生的情况。当发生故障时，它可以继续保持其网格部分在其本地上下文中尽可能表现最佳。\n有了这个基本原语，我们就可以通过简单地跨孤岛进行整体故障转移来开始构建更可靠的系统。如果孤岛中出现任何问题，我们可以将所有流量发送到不同的孤岛，直到解决问题为止。\n但为了将其提升到一个新的水平，我们想要做的是在部分发生故障时促进跨孤岛的通信，而不是对整个事情进行故障转移。为此，我们需要跨部门进行沟通。\n促进跨孤岛通信 保持本地化为我们提供了一组可用但不互连的孤岛。这会导致浪费并使故障转移操作变得痛苦。\n我们经常想要找到一个健康的依赖实例，即使它不在我们的本地孤岛中，并将请求路由到它。例如，如果我们在一个集群中的后端服务部署不当，我们希望前端能够故障转移到第二个集群中的现有部署。TSB 促进了应用程序的跨孤岛通信，同时最大限度地减少了 Istio 控制平面实例与其全局控制平面之间所需的配置同步。它将网格中每个集群的入口点地址以及该集群中可用的服务集发布到 TSB 管理的每个 Istio 实例。当服务在本地发生故障，或者本地服务需要与远程依赖项进行通信时，我们可以将该流量定向到具有我们所需服务的远程集群，而应用程序本身无需担心依赖项所在的位置。\n与互联网比较 我们经常使用互联网作为例子来解释我们的方法。你完全了解本地网络，并且要连接到其他网络上的主机，你可以使用 BGP 发布的路由。这些路由提供有关使用哪个本地网关来转发流量以到达远程地址的信息。在我们的设置中，Istio 控制平面的每个实例都完全了解其自己的本地集群。全局控制平面的功能类似于 BGP，发布“第 7 层路由”，指示 Istio 使用哪些远程网关（网格中的其他 Istio 集群）来转发流量以到达远程服务。 架构概述 Tetrate Service Bridge 的架构由四层组成：数据平面、本地控制平面、全局控制平面和管理平面。\n数据平面：数据平面负责入站和出站流量，确保使用 Envoy 代理强制执行流量和安全策略。 本地控制平面：本地控制平面部署在集群内并管理服务网格功能，例如提供服务身份、加密和流量路由。 全局控制平面：全局控制平面链接本地控制平面，方便跨集群的配置传播、服务发现和灾难恢复。 管理平面：管理平面是可以管理工作区、组和服务的用户的主要访问点。这提供了对环境的有效管理和控制。 数据平面 Istio 使用 Envoy 代理的扩展版本作为数据平面。Envoy 是一种高性能边缘/中间/服务代理，旨在调解服务网格中所有服务的所有入站和出站流量。当作为 sidecar 部署到应用程序时，Envoy 代理可以通过流量管理、安全性和可观测性功能来增强这些应用程序。\n将 Envoy 部署为 sidecar 会自动配置所有入站和出站流量通过 Envoy。这允许在不需要重新架构或重写应用程序的情况下增加服务。\n现代 API 网关、Web 应用程序防火墙和其他“边缘”功能 利用 Envoy 作为一致的数据平面，我们可以提供传统上仅限于应用程序流量平台中任何位置的边缘或 DMZ 的功能。TSB 将 Envoy 的一系列功能组合到一个易于使用的包中，支持 API 网关功能，例如令牌验证、速率限制和基于 OpenAPI 规范的配置。它还为 sidecar、入口网关和边缘负载均衡器带来了 WAF 功能。最重要的是，TSB 允许你编写单个策略并将其应用于任何地方的流量：外部客户端和你的服务之间、网络中的集群或数据中心之间、甚至同一集群上运行的服务之间。\n扩展 TSB 在数据平面的扩展点是 WebAssembly。Envoy 有多个扩展点，但正常的 Envoy 扩展需要重建和链接 Envoy 二进制文件。WebAssembly 是一种沙盒技术，从 Istio 1.6 开始可用于动态扩展 Envoy。\nIstio 文档提供了 WebAssembly 扩展的概述。在 TSB 中，通过 WASM 扩展和 Istio 的 WasmPlugin 资源提供了对 WebAssembly 扩展的更好支持。这有助于开发人员构建和测试 Envoy 扩展，并与 TSB 集成以促进扩展部署。\n本地控制平面 TSB 使用 Istio 作为每个集群内的本地控制平面，通过多个 Istio 实例提供隔离的故障域，并从 TSB 管理平面进行简单、标准化的管理。\n作为用户，你可以从管理平面访问和控制它们，这意味着你不会与本地控制平面进行直接交互。此外，你只需推送一个配置即可更新所有配置。\n本地控制平面负责：\n智能本地负载均衡 在集群内实施零信任 在本地级别实施身份验证和授权 控制平面是 TSB 推送配置、挖掘数据并根据集群活动做出智能决策的本地接入点。\n全局控制平面 全局控制平面 (XCP) 是管理平面的一部分，作为用户，你无法直接访问全局控制平面的 API。全局控制平面负责：\n集群间的服务发现 从本地控制平面和数据平面收集的遥测数据 网关中断或集群故障时的灾难恢复和故障转移 用户以及应用程序之间的身份验证和授权 出口控制以确定什么可以离开网络。 全局控制平面使集群能够相互通信并通告可用服务。\n全局控制平面由两个应用程序组成：XCP Central 和 XCP Edge。XCP Central 部署在 TSB 管理平面中，负责将配置传播到 XCP Edge 应用程序。XCP Edge 应用程序部署在每个运行用户应用程序的载入集群中，以便将 TSB 配置本地转换为 Istio API。\n多集群方法比较 与使用 Istio 跨多个集群构建网格的其他方法（例如将每个集群的每个服务的 Pod 或 VM IP 地址更改发布到所有其他 Istio 实例）相比，TSB 的方法需要的数据更改率非常低传播，数据本身非常小，并且不需要跨集群进行 n 方通信，这意味着保持最新和准确要容易得多，并且整体系统更简单。越简单总是更容易运行并且更可靠。TSB 促进跨孤岛通信的方法可实现非常稳健且可靠的运行时部署。 Tetrate Service Bridge 管理平面 TSB 管理平面是网格管理环境中所有内容的主要访问点。\n管理平面将你的基础架构划分为称为工作区、组和服务的逻辑分组，从而简化管理环境的过程，从而轻松管理你的环境。\n影响网格管理环境的所有更改均由管理平面处理，包括流量管理、服务发现、服务间通信和入口/出口控制等运行时操作，以及管理用户访问等管理操作（IAM）、安全控制和审计。\n了解管理平面 在上一篇服务网格架构中，我们介绍了数据平面和控制平面的概念。上面，我们介绍了故障域的概念以及为什么我们要部署本地控制平面的许多实例。拥有许多本地控制平面自然意味着我们需要一些东西来帮助它们作为一个整体运行，因此全局控制平面会出现问题。但为什么要在管理平面之上添加另一层呢？\n与控制平面不同，控制平面的主要工作是可用、低延迟并尽快服务数据平面配置（它以机器的速度变化），管理平面的主要工作是促进用户与系统的交互，他们之间的工作流程。\n管理平面是将运行时系统连接到组织中的用户和团队的层。它允许你在具有许多用户和团队、具有许多不同策略和兴趣的复杂组织中安全地管理同一物理基础设施上的分布式系统。它使用 Envoy、Istio 和 SkyWalking 创建一个工具，可在企业中自信地实施监管要求的控制，在同一基础设施上维护许多不相关的团队，而不会造成共同的命运中断，并让团队按照他们想要的速度快速行动它将是安全可靠的。\n我们将在概念部分的其余部分讨论你可以使用 Tetrate Service Bridge 管理平面做什么，从授权应用程序开发人员管理流量到应用全局策略和使用 TSB 的 IAM 系统管理用户访问，再到了解你的整个基础设施单块玻璃。但乍一看，它可以让你：\n无论应用程序或其依赖项部署在何处，都可以在一处控制流量 使用高级权限系统管理谁可以更改什么（防止应用程序开发人员更改安全设置；防止安全团队导致应用程序中断） 审核系统中的每一个变更 了解和控制系统中的流量：内部流量、入口和出口 管理整个基础设施的控制平面生命周期 详细的数据流 Tetrate Service Bridge 组件和数据流的详细、幕后视图可以帮助你了解构成运行时系统的各个部分以及数据在它们之间的流动方式。\n有关每个组件的详细说明，请参阅 TSB 组件 。有关每个组件使用的端口的完整列表，请转到组件端口 。\n","relpermalink":"/book/tsb/concepts/architecture/","summary":"本节重点介绍组成 TSB 的架构及其含义。你将获得以下方面的知识： 我们的可靠部署理念是 TSB 架构背后的驱动力 数据平面，由 Envoy 提供支持 由 Istio 提供支持的本地控制平面 Tetrate Service Bridge 的全局控制平面 Tetrate Service Bridge 的管理平面 拥有管理平面的重要性","title":"TSB 架构"},{"content":"本文将描述 WASM 扩展在 TSB 中是如何定义的，以及它们如何分配给层次结构中的组件。\nTSB 中的 WASM 为了控制网格中允许的扩展，避免安全泄漏并简化扩展升级过程，TSB 拥有一个WASM 扩展 目录， 管理员将在其中注册所有可用于不同组件中使用的扩展。 此目录将包含每个扩展的描述、镜像和执行属性。 当扩展的新版本可用时，更改 WASM 扩展目录记录的内容将将更新传播到该扩展的所有分配。\n这些扩展被打包为 OCI 镜像，包含 WASM 文件，并部署在容器镜像仓库中，Istio 将从中拉取并提取内容。 使用 OCI 镜像交付 WASM 扩展的好处在于，安全性已经实现并标准化，与其他工作负载镜像一样。\n扩展可以允许全局使用，也可以在一组租户中受到限制，这将影响扩展可以附加的位置。\n在扩展在目录中创建后，它们将启用并可用于与同一组织层次结构中的 TSB 组件的附件中使用。它们的属性将成为附件的配置。 可以配置 WASM 扩展的组件包括：组织 、租户 、工作区 、安全组 、入口网关 、出口网关 和第一层网关 。\n在 TSB 资源中使用 WASM 扩展 WASM 扩展可以在组织设置 、租户设置 、工作区设置 的defaultSecuritySettings属性中指定，并将影响层次结构中属于这些资源的所有工作负载。 此外，这些附件可以在 IngressGateway、EgressGateway 和 Tier1Gateway 的extension 属性中指定，只有与这些网关链接的工作负载才会受到 WASM 扩展的影响。TSB 将使用工作负载选择器来指定工作负载。\nextension: - fqn: \u0026#34;organizations/tetrate/extensions/wasm-add-header\u0026#34; config: header: x-wasm-header value: igw-tsb 在 TSB 中使用 WASM 扩展的另一种方式是使用 Istio 直接模式，创建一个IstioInternalGroup 和一个WasmPlugin ，并引用该组。 例如：\napiVersion: istiointernal.tsb.tetrate.io/v2 kind: Group metadata: organization: tetrate tenant: mytenant workspace: myworkspace name: internal-group spec: namespaceSelector: names: - \u0026#34;*/httpbin\u0026#34; 然后直接创建 Istio WasmPlugin：\napiVersion: extensions.istio.io/v1alpha1 kind: WasmPlugin metadata: name: demo-wasm-add-header namespace: app-namespace annotations: tsb.tetrate.io/organization: tetrate tsb.tetrate.io/tenant: mytenant tsb.tetrate.io/workspace: myworkspace tsb.tetrate.io/istioInternalGroup: internal-group spec: selector: matchLabels: app: httpbin url: oci://docker.io/tetrate/xcp-wasm-e2e:0.3 imagePullPolicy: IfNotPresent pluginConfig: header: x-wasm-header value: xcp ","relpermalink":"/book/tsb/howto/wasm/wasm-extension/","summary":"展示了如何在 TSB 中创建 WASM 扩展并将它们分配到层次结构中的组件。","title":"TSB 配置"},{"content":"在本文档中，我们将启用入口网关中的速率限制，并展示如何基于 HTTP 请求中的 user-agent 字符串进行速率限制。\n在开始之前，请确保你已经完成以下步骤：\n熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成了 TSB 使用快速入门 。本文档假定你已经创建了租户，熟悉工作区和配置组，并配置了 tctl 到你的 TSB 环境。 启用速率限制服务器 请阅读并按照 启用速率限制服务器文档 中的说明操作。\n演示安装 如果你使用 TSB 演示 安装，你已经有一个正在运行并且可以使用的速率限制服务，可以跳过这一部分。 部署 httpbin 服务 请按照 本文档中的说明 创建 httpbin 服务。你可以跳过 “创建证书” 和 “载入 httpbin 应用程序” 部分。\n基于 User-agent 进行速率限制 创建一个名为 rate-limiting-ingress-config.yaml 的文件，用于编辑现有的入口网关，以便对每个 User-agent 头的值进行速率限制，限制为每分钟 5 次请求。将 organization 和 tenant 替换为适当的值。\n其他速率限制选项的详细信息可以在 此文档 中找到。\napiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: name: httpbin-gateway # 不需要与 spec.labels.app 相同 organization: \u0026lt;organization\u0026gt; tenant: \u0026lt;tenant\u0026gt; group: httpbin-gateway workspace: httpbin spec: workloadSelector: namespace: httpbin labels: app: httpbin-ingress-gateway # 为 httpbin 创建的入口网关的名称 http: - name: httpbin hostname: \u0026#34;httpbin.tetrate.com\u0026#34; port: 80 routing: rules: - route: host: \u0026#34;httpbin/httpbin.httpbin.svc.cluster.local\u0026#34; port: 8000 rateLimiting: settings: rules: - dimensions: - header: name: user-agent limit: requestsPerUnit: 5 unit: MINUTE 使用 tctl 配置入口网关：\ntctl apply -f rate-limiting-ingress-config.yaml 测试 你可以通过从外部计算机或本地环境发送 HTTP 请求到 httpbin 入口网关来测试速率限制，并观察在一定数量的请求后速率限制生效。\n在以下示例中，由于你无法控制 httpbin.tetrate.com，你需要欺骗 curl，让它认为 httpbin.tetrate.com 解析为 Tier-1 网关的 IP 地址。\n使用以下命令获取之前创建的 Tier-1 网关的 IP 地址。\nkubectl -n httpbin get service httpbin-ingress-gateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39; 然后执行以下命令，将 HTTP 请求发送到入口网关的 httpbin 服务。将 \u0026lt;gateway-ip\u0026gt; 替换为你在上一步中获取的值。\ncurl -k -v \u0026#34;http://httpbin.tetrate.com/get\u0026#34; \\ --resolve \u0026#34;httpbin.tetrate.com:80:\u0026lt;gateway-ip\u0026gt;\u0026#34; \\ -s \\ -o /dev/null \\ -w \u0026#34;%{http_code}\\n\u0026#34; 前面的 5 个请求应该会在屏幕上显示 “200”。之后，你应该开始看到 “429”。\n你可以将 user-agent 头更改为另一个唯一的值以获得成功的响应。\ncurl -k -v -A \u0026#34;another-agent\u0026#34; \\ \u0026#34;http://httpbin.tetrate.com/get\u0026#34; \\ --resolve \u0026#34;httpbin.tetrate.com:80:\u0026lt;gateway-ip\u0026gt;\u0026#34; \\ -s \\ -o /dev/null \\ -w \u0026#34;%{http_code}\\n\u0026#34; 在 5 次请求之后，你应该再次开始看到 “429”，直到你再次更改头部。\n","relpermalink":"/book/tsb/howto/rate-limiting/ingress-gateway/","summary":"在本文档中，我们将启用入口网关中的速率限制，并展示如何基于 HTTP 请求中的 user-agent 字符串进行速率限制。 在开始之前，请确保你已经完成以下步骤： 熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成了 TSB 使用快速入门 。","title":"TSB 入口网关中的速率限制"},{"content":"本文档提供有关 TSB 的发布节奏、不同版本的支持期以及组件版本矩阵的信息。\nTSB 发布说明 TSB 遵循基于 semver.org 的语义版本控制模型。每个版本号代表的含义如下：\nMAJOR ：因不兼容的 API 更改而增加。 MINOR ：针对新功能增加。 PATCH ：针对错误和安全修复而增加。 TSB 发布节奏和支持 TSB 定期且至少每季度发布 PATCH 更新 (1.x.y)。 每个新的 MINOR 版本都属于长期支持 (LTS) 政策。 Tetrate 为 LTS 版本提供从一般可用性 (GA) 到一般支持终止 (EoGS) 的支持，通常在 GA 后 12 个月设置。 在支持窗口期间通过 PATCH 版本提供错误和安全修复。 新功能不会向后移植到 LTS 版本。 TSB 候选版本 候选版本提供对新功能的早期访问以进行测试，但不建议用于生产使用。\n发布候选版本 候选版本可能包含已知或未知的错误，并且不适合生产使用。 支持的版本 Tetrate 根据以下时间表提供支持和补丁：\nTSB 版本 一般可用性 一般支持结束 Kubernetes 版本 OpenShift 版本 TSB v1.6.x 2023 年 1 月 1 日 2023 年 12 月 31 日 1.22 - 1.26 [1] 4.7 - 4.12 [1] TSB v1.5.x 2022 年 7 月 15 日 2023 年 7 月 15 日 1.19 - 1.24 4.6 - 4.11 TSB v1.4.x 2021 年 11 月 1 日 2022 年 10 月 31 日 1.18 - 1.21 [2] 4.6 - 4.8 TSB v1.3.x 2021 年 6 月 1 日 2022 年 5 月 31 日 1.18 - 1.20 4.6 - 4.8 TSB v1.2.x 2021 年 5 月 1 日 2022 年 4 月 30 日 1.18 - 1.20 [3] 4.6 - 4.8 TSB v1.1.x 2021 年 4 月 1 日 2022 年 3 月 31 日 TSB v1.0.x 2021 年 3 月 1 日 2022 年 2 月 28 日 注意 [1] Kubernetes 1.25、1.26 和 OpenShift 4.12 需要 TSB 1.6.1 或更高版本。 [2] Kubernetes 1.21 和 TSB 1.4.x - 支持但有警告，请参阅 Tetrate 支持。 [3] Kubernetes 1.19 和 1.20 以及 TSB 1.2.x - 支持但有警告，请参阅 Tetrate 支持。 TSB 组件版本矩阵 Tetrate Service Bridge 包括以下开源组件：\nTSB Istio Envoy SkyWalking Zipkin OpenTelemetry Collector 1.6.3 1.15.7 1.23.11 9.4.0-20230726-0846 0.81.0 1.6.2 1.15.7 1.23.7 9.4.0-20230407-0339 0.77.0 1.6.1 1.15.7 1.23.7 9.4.0-20230331-1055 2.24.0 0.70.0 1.6.0 1.15.2 1.23.2 9.4.0-20221215-0956 2.24.0 0.62.1 ","relpermalink":"/book/tsb/release-notes-announcements/support-policy/","summary":"TSB 支持策略，发布计划和组件版本矩阵。","title":"TSB 支持策略"},{"content":"Open Policy Agent (OPA) 是一个开源的通用策略引擎，提供了一种高级声明性语言，允许您将策略定义为代码。OPA 还提供了简单的 API，用于从您的软件中卸载策略决策。\n本文档描述了在 TSB 中配置 OPA 的简化版本，以配合使用它作为外部授权 (ext-authz) 服务的部分，您的实际应用程序可能存在需要进行调整的差异。\nOPA 支持 Tetrate 不提供对 OPA 的支持。如果您需要支持，请在其他地方寻找。 有关下面所述的配置的更详细解释，请参考官方文档 。\n准备策略 OPA 需要使用OPA 的策略语言 编写策略文件以决定是否应授权请求。由于实际策略将因示例而异，因此本文档不会涵盖如何编写此文件的详细信息。请参考OPA 网站上的文档 以获取详细信息。\n需要注意的一点是策略文件中指定的包名称。如果您的策略文件具有以下包声明，您将在稍后的容器配置中使用值 helloworld.authz。\npackage helloworld.authz 示例：具有基本身份验证的策略 此示例显示了一个策略，仅允许用户 alice 和 bob 通过基本身份验证进行身份验证。如果用户被授权，用户名称将存储在名为 x-user 的 HTTP 标头中。\npackage example.basicauth default allow = false # 用户名和密码数据库 user_passwords = { \u0026#34;alice\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;bob\u0026#34;: \u0026#34;password\u0026#34; } allow = response { # 检查标头中的密码是否与特定用户的数据库中的密码相同 basic_auth.password == user_passwords[basic_auth.user_name] response := { \u0026#34;allowed\u0026#34;: true, \u0026#34;headers\u0026#34;: {\u0026#34;x-user\u0026#34;: basic_auth.user_name} } } basic_auth := {\u0026#34;user_name\u0026#34;: user_name, \u0026#34;password\u0026#34;: password} { v := input.attributes.request.http.headers.authorization startswith(v, \u0026#34;Basic \u0026#34;) s := substring(v, count(\u0026#34;Basic \u0026#34;), -1) [user_name, password] := split(base64url.decode(s), \u0026#34;:\u0026#34;) } 将策略存储在 Kubernetes 中 假设您的策略存储在名为 policy.rego 的文件中，您需要将文件存储在 Kubernetes 的 Secret 或 ConfigMap 中。\n要创建一个 Secret，请执行以下命令，将 namespace 替换为适当的值：\nkubectl create secret generic opa-policy -n \u0026lt;namespace\u0026gt; \\ --from-file policy.rego 如果使用 ConfigMap，请以相同的方式执行以下命令：\nkubectl create configmap opa-policy -n \u0026lt;namespace\u0026gt; \\ --from-file policy.rego 资源的名称（opa-policy）可以根据需要更改。\n基本部署 以下清单显示了一个示例，可用于部署一个 OPA 服务和大部分默认设置的 OPA 代理。请记住将配置中的 package 和 namespace 替换为正确的值。\napiVersion: v1 kind: Service metadata: name: opa namespace: \u0026lt;namespace\u0026gt; spec: selector: app: opa ports: - name: grpc protocol: TCP port: 9191 targetPort: 9191 --- kind: Deployment apiVersion: apps/v1 metadata: name: opa namespace: \u0026lt;namespace\u0026gt; spec: replicas: 1 selector: matchLabels: app: opa template: metadata: labels: app: opa name: opa spec: volumes: - name: opa-policy secrets: secretName: opa-policy containers: image: openpolicyagent/opa:latest-envoy name: opa securityContext: runAsUser: 1111 args: - \u0026#34;run\u0026#34; - \u0026#34;--server\u0026#34; - \u0026#34;--addr=localhost:8181\u0026#34; - \u0026#34;--diagnostic-addr=0.0.0.0:8282\u0026#34; - \u0026#34;--set=plugins.envoy_ext_authz_grpc.addr =:9191\u0026#34; - \u0026#34;--set=plugins.envoy_ext_authz_grpc.query=data.\u0026lt;package\u0026gt;.allow\u0026#34; - \u0026#34;--set=decision_logs.console=true\u0026#34; - \u0026#34;--ignore=.*\u0026#34; - \u0026#34;/policy/policy.rego\u0026#34; livenessProbe: httpGet: path: /health?plugins scheme: HTTP port: 8282 initialDelaySeconds: 5 periodSeconds: 5 readinessProbe: httpGet: path: /health?plugins scheme: HTTP port: 8282 initialDelaySeconds: 5 periodSeconds: 5 volumeMounts: - readOnly: true mountPath: /policy name: opa-policy 假设您已将上述清单保存在名为 opa.yaml 的文件中，请执行以下命令进行部署：\nkubectl apply -f opa.yaml 终止 TLS 为了保障 ext-authz 服务（我们在这里使用 OPA 作为示例）与其客户端（网关和 sidecar）之间的通信，您可以启用 TLS 验证。作为示例，在这里我们将使用 Envoy sidecar 代理来终止 TLS 并验证来自客户端的 TLS 证书。\n注意 以下示例中的设置仅用于测试目的。请根据您的生产用例的安全需求进行不同配置。 准备证书 可以使用管理员提供的证书，也可以使用自签名证书进行测试。您可以利用快速入门指南中的说明 创建自签名证书。\n如果您尚未这样做，请创建一个包含证书的 Secret。Secret 将命名为 opa-certs，稍后将使用它。假设您已生成了文件 opa.key 和 opa.crt，请执行以下命令创建 Secret。将 namespace 的值替换为适当的值。\nkubectl -n \u0026lt;namespace\u0026gt; create secret tls opa-certs \\ --key opa.key \\ --cert opa.crt 创建 Envoy 配置文件 创建一个名为 config.yaml 的文件，具有以下内容。将 namespace 的值替换为适当的值。此配置假定管理员端口位于端口 10250，端口 18080 上有一个 “不安全” 的 grpc，端口 18443 上有一个带 TLS 终止的 grpc。\nadmin: address: socket_address: address: 127.0.0.1 port_value: 10250 static_resources: listeners: # 不安全的 GRPC 监听器 - name: grpc-insecure address: socket_address: address: 0.0.0.0 port_value: 18080 access_log: - name: envoy.access_loggers.file typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog path: /dev/stdout filter_chains: - filters: - name: envoy.filters.network.tcp_proxy typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy cluster: grpc_rlserver stat_prefix: grpc_insecure # 通过 TLS 进行安全保护 - name: grpc-simple-tls address: socket_address: address: 0.0.0.0 port_value: 18443 access_log: - name: envoy.access_loggers.file typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog path: /dev/stdout filter_chains: - filters: - name: envoy.filters.network.tcp_proxy typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy cluster: grpc_rlserver stat_prefix: grpc_simple_tls transport_socket: name: envoy.transport_sockets.tls typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext common_tls_context: tls_certificates: - certificate_chain: { filename: /certs/tls.crt } private_key: { filename: /certs/tls.key } 创建一个 ConfigMap，将配置存储在 Kubernetes 中。将 namespace 的值替换为适当的值。\nkubectl create configmap -n \u0026lt;namespace\u0026gt; opa-proxy \\ --from-file=config.yaml 部署服务 创建一个名为 opa-tls.yaml 的文件，具有以下内容。将 namespace 的值替换为适当的值。\napiVersion: v1 kind: Service metadata: name: opa-tls namespace: \u0026lt;namespace\u0026gt; spec: selector: app: opa-tls ports: - name: http port: 8080 targetPort: 8080 # Doesn\u0026#39;t go through Envoy - name: grpc-insecure port: 18080 targetPort: …","relpermalink":"/book/tsb/reference/samples/opa/","summary":"Open Policy Agent (OPA) 是一个开源的通用策略引擎，提供了一种高级声明性语言，允许您将策略定义为代码。OPA 还提供了简单的 API，用于从您的软件中卸载策略决策。 本文档描述了在 TSB 中配置 OPA 的简化版本，以配合使用它作为外部授","title":"安装 Open Policy Agent"},{"content":"在本部分中，你将在演示 TSB 环境中部署示例应用程序 (bookinfo)。将使用 TSB UI 和 tctl 命令验证部署。\n先决条件 在继续阅读本指南之前，请确保你已完成以下步骤：\n熟悉 TSB 概念，包括工作区和组 安装 TSB 演示 TSB 演示安装负责载入集群、安装所需的 Operator 并为你提供必要的访问凭据。\n部署 Bookinfo 应用程序 你将使用经典的 Istio bookinfo 应用程序 来测试 TSB 的功能。\n创建命名空间并部署应用程序 # Create namespace and label it for Istio injection kubectl create namespace bookinfo kubectl label namespace bookinfo istio-injection=enabled # Deploy the bookinfo application kubectl apply -n bookinfo -f https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/platform/kube/bookinfo.yaml 确认服务 要确认所有服务和 pod 都在运行，请执行以下命令：\nkubectl get pods -n bookinfo 预期输出：\nNAME READY STATUS RESTARTS AGE details-v1-5bc5dccd95-2qx8b 2/2 Running 0 38m productpage-v1-f56bc8d5c-42kcg 2/2 Running 0 38m ratings-v1-68f58946ff-vcrdh 2/2 Running 0 38m reviews-v1-5976d456d4-nltg2 2/2 Running 0 38m reviews-v2-57cf5b5488-rgq8l 2/2 Running 0 38m reviews-v3-7745dbf976-4gnl9 2/2 Running 0 38m 访问 Bookinfo 应用程序 确认你可以访问 bookinfo 应用程序：\nkubectl exec \u0026#34;$(kubectl get pod -n bookinfo -l app=ratings -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; \\ -n bookinfo -c ratings -- curl -s productpage:9080/productpage | \\ grep -o \u0026#34;\u0026lt;title\u0026gt;.*\u0026lt;/title\u0026gt;\u0026#34; 你应该看到类似的输出：\n\u0026lt;title\u0026gt;Simple Bookstore App\u0026lt;/title\u0026gt; 这些步骤成功地将 bookinfo 应用程序部署到你的 TSB 环境中，确保它按预期启动并运行。\n","relpermalink":"/book/tsb/quickstart/deploy-sample-app/","summary":"在 TSB 中部署示例应用程序。","title":"部署应用程序"},{"content":"本文档介绍了如何通过 AWS 容器市场在你的 Amazon Kubernetes (EKS) 集群中安装 Tetrate Service Bridge (TSB)。\n注意 本文档适用于已经购买了 Tetrate 的 AWS 容器市场提供的用户。 如果你没有订阅 Tetrate 容器市场提供的服务，本文档将不起作用。 如果你对 AWS 市场的私有报价感兴趣，请联系 Tetrate。 Tetrate Operator 概述 Tetrate Operator 是 Tetrate 提供的 Kubernetes Operator，它使安装、部署和升级 TSB 更加简单。Tetrate Service Bridge 的 AWS 容器市场提供安装了 Tetrate Operator 的一个版本到 EKS 集群中。之后，TSB 可以安装在你 EKS 集群中的任何命名空间中。 在本文档中，假定 TSB 将安装在 tsb 命名空间中。\n使用 Tetrate Operator 的先决条件 要使用市场上的 Tetrate 提供，确保满足以下要求：\n你可以访问配置了服务帐户的 EKS 集群（Kubernetes 版本 1.16 或更高）。 你在 EKS 集群上具有集群管理员访问权限。 你已经设置了 EKS 集群，并且已经设置了 kubectl。 你已经下载了 tctl 。 安装步骤 创建和配置 Kubernetes 集群的 AWS IAM 角色 AWS IAM 权限是通过 AWS 的 Kubernetes 服务帐户 IAM 角色授予 Tetrate 的。此功能必须在集群级别启用。 创建一个名为 eks-tsb-operator 的 IAM 角色，用于 Tetrate Operator pod，并根据 AWS 指南为 EC2 配置它 。稍后将替换信任关系。 然后授予 AWS 管理策略 AWSMarketplaceMeteringRegisterUsage 给 eks-tsb-operator。\n创建 IAM 角色的信任关系。使用以下模板，并将 AWS_ACCOUNT_ID 和 OIDC_PROVIDER 替换为适当的值。\nAWS_ACCOUNT_ID 应替换为你的 AWS 帐户 ID。\nOIDC_PROVIDER 应替换为你的 Kubernetes 集群的 OpenID Connect 提供程序 URL。在替换之前，你必须删除 URL 中的 https:// 前缀。\n有关 EKS 集群的 IAM OIDC 提供程序的详细信息，请参阅官方文档 。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::AWS_ACCOUNT_ID:oidc-provider/OIDC_PROVIDER\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;OIDC_PROVIDER:sub\u0026#34;: \u0026#34;system:serviceaccount::tsb:tsb-operator-management-plane\u0026#34; } } } ] } 安装 Tetrate Operator 和 TSB 管理平面 使用 Tetrate CLI (tctl)，生成 Tetrate Operator 的 Kubernetes 清单，并将其安装到你的 Kubernetes 集群中。\n使用以下命令生成 TSB 管理平面的 CRDs：\ntctl install manifest management-plane-operator \\ --registry 709825985650.dkr.ecr.us-east-1.amazonaws.com/tetrate-io \u0026gt; managementplaneoperator.yaml 打开上面创建的 managementplaneoperator.yaml 文件，找到 tsb-operator-management-plane 的 ServiceAccount 定义。在 ServiceAccount 的 YAML 定义内部，添加 annotation 部分，包含 IAM 角色信息，以便 ServiceAccount 可以访问它。将注释中的 AWS_ACCOUNT_ID 替换为你的 AWS 帐户 ID：\napiVersion: v1 kind: ServiceAccount metadata: labels: platform.tsb.tetrate.io/application: tsb-operator-managementplane platform.tsb.tetrate.io/component: tsb-operator platform.tsb.tetrate.io/plane: management name: tsb-operator-management-plane namespace: \u0026#39;tsb\u0026#39; annotations: eks.amazonaws.com/role-arn: arn:aws:iam::AWS_ACCOUNT_ID:role/eks-tsb-operator 使用 kubectl 部署 Operator，确保你的 Kubernetes 上下文指向正确的集群：\nkubectl apply -f managementplaneoperator.yaml 部署 Tetrate Operator 可能需要一些时间。你可以通过运行以下命令来监视其状态：\nkubectl -n tsb get pod -owide 你应该会看到类似以下示例的文本。当 READY 和 STATUS 列的值分别为 1/1 和 Running 时，Operator 已准备就绪。\nkubectl -n tsb get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES tsb-operator-management-plane-68c98756d5-n44d7 1/1 Running 0 71s 192.168.17.234 ip-192-168-24-207.ca-central-1.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 请按照 管理平面安装 中的说明完成安装管理平面的步骤。\n访问 TSB 用户界面 执行以下命令获取分配给管理平面的 ELB 地址：\nkubectl -n tsb get svc -l=app=envoy 为你的 EL\nB 分配一个 DNS 记录。有关详细信息，请参阅官方文档 。\n设置 DNS 记录后，你可以使用 URL https://\u0026lt;DNS Name\u0026gt;:8443 访问 Web 用户界面。\n下一步 如果你有进一步的问题，请联系我们 。\n","relpermalink":"/book/tsb/setup/aws/container-marketplace/","summary":"本文档介绍了如何通过 AWS 容器市场在你的 Amazon Kubernetes (EKS) 集群中安装 Tetrate Service Bridge (TSB)。 注意 本文档适用于已经购买了 Tetrate 的 AWS 容器市场提供的用户。 如果你没有订阅 Tetrate 容器市场提供的服务，本文档将不起作用。 如果你对 AWS 市场的私有报价","title":"从 AWS 容器市场安装 TSB"},{"content":"GitOps 是一种实践，它使用 Git 存储库作为应用程序/系统状态的真实来源。对状态的更改通过拉取请求（PR）和批准工作流程执行，并将由持续交付（CD）流程自动应用于系统。以下图像对此进行了说明。\nGitOps 有三个核心实践：\n基础设施即代码 这描述了将所有基础设施和应用程序配置存储为 Git 中的代码的实践。\n使用拉取请求进行更改 更改是在分支上提出的，然后创建 PR 将更改合并到主分支中。使用 PR 允许运维工程师与开发团队、安全团队和其他利益相关者进行协作进行对等审查。\nCI（持续集成）和 CD（持续部署）自动化 在理想情况下，不会对 GitOps 管理的环境进行手动更改。相反，CI 和 CD 充当一种协调循环。每次进行更改时，自动化工具会将环境的状态与 Git 存储库中定义的真实来源进行比较。\nGitOps 好处 通过使用 GitOps 实践，你可以获得以下几个好处：\n带有历史记录的版本\n由于每个更改都存储在 Git 中，因此很容易发现进行了哪些更改。在大多数情况下，更改可以追溯到特定的事件或更改请求。\n所有权\n由于更改可以在 Git 历史记录中查找，因此可以找出谁拥有相关文件（在本例中主要是清单/YAML 文件）。这也意味着可以推断出生成内容的所有者，并在运维过程中使用。\n不变性\n任何构建或部署都是可重复的和不可变的。\n确定性\n即使容器/集群存在手动更改，负责应用配置的运维者也在观察并将根据存储在 Git 中的信息进行修复。\n注意 请记住，要生成机密后，重新启动 tsb-operator-control-plane Pod，并一旦生成后，重新启动控制平面 Pod。 构建（CI）与部署（CD）的分离 虽然你可以为应用程序代码和配置使用单一流水线，但出于以下原因，你可能希望将它们分开：\n责任分离\n应用程序的 开发人员 提交代码并创建发布，这可以在 CI 中完成。应用程序的 运维人员 将构件部署到集群中，这可以在 CD 中完成。\n多次部署\n可以使用 CD 将单一应用程序代码部署到多个环境中，无需一次又一次地进行 CI。\n重复部署\n重新创建部署不应该需要新的构建。可以在 CD 中管理构件，而无需一次又一次地进行 CI。\n可以通过使用协调循环将 CI 与 CD 连接起来，该协调循环将检查新的构件版本并创建 PR 到应用程序配置存储库以更新应用程序镜像标签。\n推送与拉取 GitOps 有两种方法：推送 和 拉取。\n在 推送 方法中，一旦在 Git 存储库上提交了更改或成功执行了 CI 流水线，外部系统（通常是 CD 流水线）将触发部署构件到集群。在这种方法中，流水线系统需要访问集群的相关权限。\n推送解决方案的示例包括：Github Actions 、Azure Pipelines 、GitlabCI 和 CircleCI 。\n在 拉取 方法中，目标集群内部的代理定期扫描相关的 Git 存储库。如果检测到更改，将从集群内部更新集群状态。如下图所示，CD 组件移至集群旁边。\n拉取解决方案的示例包括：Argo CD 和 Flux 。\n这两种方法都有各自的优点和缺点。如果你有兴趣进一步阅读，请查看以下文章：\nGitOps 中的推送与拉取：真的有区别吗？ GitOps —— 比较拉取和推送 不会有一种单一的解决方案适用于所有可能的用例，因为每种情况都有很大的变化。因此，始终根据每种方法的利弊权衡你的团队或组织的需求，有助于你决定使用哪种方法（甚至同时使用两种方法）。\n集群部署流水线 在使用 GitOps 流水线规划如何提供集群时，你还需要考虑以下几个因素：\n这包括创建集群、在集群中安装常见的基础设施组件：日志记录、监控、秘密、证书、访问控制、服务网格组件（如 Tetrate Service Bridge，TSB）等。\nGitOps 的挑战 像任何技术一样，成功采用的关键在于管理期望并充分了解技术的优势和劣势。了解它们是准确确定哪种解决方案适合你的文化、环境和/或流程的重要因素，如果适用的话。\n以下是采用 GitOps 面临的一些挑战：\nGitOps 无法处理自动缩放和动态资源。 由于 GitOps 期望状态存储在 Git 中，因此动态方面，如自动缩放，可能会在尝试同步状态时引发问题。工具如 ArgoCD 具有自定义差异 来处理此类问题。\nGitOps 不解决在不同环境之间推广版本的问题。 每个环境的配置可能存储在不同的 Git 分支中。通常使用模板化解决方案，例如 Helm 和 Kustomize ，以提供一个可以在每个环境上进行定制的基本模板。\n尽管所有信息都在 Git 中，但审计仍然具有问题。 尽管 Git 具有更改历史记录，但要回答某些问题而不使用附加工具来分析多个 Git 存储库中的数据很困难。例如，环境 X 的部署中有多少成功，需要回滚多少？环境 X 中存在多少功能，但尚未在环境 Y 中存在？\n大规模部署，具有大量集群和服务，将面临挑战。 影响大量资源的操作，例如在所有部署上添加新的全公司标签，可能会有问题，因为你可能要处理多个服务，其配置存储在多个不同的 Git 存储库中。\n缺乏标准实践。 一个很好的例子是仍然没有关于如何管理配置的单一接受的实践，例如秘密。如果秘密存储在 Git 中，它们需要加密，因此在部署期间需要有自己的工作流程来处理它们。如果它们不存储在 Git 中，那么你将无法在 Git 中存储集群的状态。实际上，组织往往使用外部秘密管理工具，如 Vault。\n缺乏可见性和运行时验证。 Git 在运行时不提供关于发生了什么的可见性。例如，如果单个更新导致其他依赖服务出现问题，那么很难找出问题。\nGitOps 用于服务网格 服务网格通过独立的网络层或“网格”来控制服务之间的通信，从而将运维与开发分开，解决了应用程序的网络、安全和可观测性问题。该网格由充当代理的连接边车形成，这些边车控制参与网格的服务之间的通信。服务网格提供了在不更改应用程序本身的情况下控制和保护应用程序网络的功能。\n服务网格引入了控制平面组件以管理代理并将运行时配置分发给代理。你需要在 GitOps 流水线中部署和管理网格控制平面的生命周期。这可以添加到你已经用于提供集群的现有流水线中，或者可以使用不同的流水线。\n由于服务网格运行在应用程序之外，因此除了应用程序部署配置之外，你还需要在 GitOps 流水线中添加另一种配置。对于 Istio ，这将是 Istio 资源，例如 Gateway 、VirtualService 、DestinationRule 等。\nTSB 的 GitOps TSB 在多集群控制平面之上添加了一个管理平面（MP），并提供了一种统一的方式来连接和保护整个网格管理的环境中的服务。在将 TSB 纳入你的 GitOps 流水线时，你需要注意几个因素。\nTSB API 结构 TSB 有两个 API：安装和配置。安装 API 通常用于配置管理平面和控制平面集群。安装 API 是 Kubernetes YAML 文件，可以添加到集群配置流水线中。\n要使用 TSB 配置来配置应用程序网格行为，你可以以两种方式进行：\n基于 TSB 你需要使用 TSB API 或使用 TSB CLI (tctl) 来应用 TSB 配置。TSB 配置必须应用于 TSB 管理平面，而不是直接应用于集群。管理平面将根据指定的 namespaceSelector 将配置分发到 Kubernetes 集群。\n由于 TSB 需要将应用程序网络配置应用于 TSB 管理平面，因此你需要将 tctl 添加到你的 CD 流水线中。\n基于 Kubernetes 这需要应用程序集群启用 GitOps 功能 。\n一旦完成，你可以将 TSB Kubernetes YAML 文件添加到每个集群的配置流水线中。\n你可以在 GitOps howtos 中详细了解如何操作。\n回滚选项 不同的系统和应用程序根据其依赖关系采用不同的回滚方法。\n对于 TSB 和所有接入的集群，配置的来源存储在 TSB 内部。 由于 TSB 配置格式在一定程度上与所使用的 TSB 版本相关联，因此确保已部署的 TSB 版本、TSB 中的配置以及存储在 Git 中的配置保持同步非常重要，特别是在尝试执行回滚时。\n你需要根据回滚情况的具体情况进行调查，这超出了本文档的范围，因为它们在不同环境中有很大的变化。示例可能包括在与 Git 配置更改一起回滚应用程序或一组应用程序的 TSB 配置，与特定控制平面版本相关的回滚或回滚 TSB 管理平面等。\n灾难恢复设置 企业可以设置镜像环境用于灾难恢复（DR），包括 TSB DR。Kubernetes 清单和 TSB 配置与主要集群同步应用于 DR 集群。\n另一种情况是使用单个 TSB 管理主要和 DR 用户集群。在这种情况下，可以创建并应用与主要集群相同的配置的重复集，除了资源名称、集群和命名空间名称不同。\n踏上 GitOps 之路 对于踏上 GitOps 之路的组织，可以开始评估现有解决方案，并根据组织目标验证每个解决方案，同时牢记当前 GitOps 解决方案的局限性和挑战。\n模板化将在抽象和扩展用于大型集群和服务的 GitOps 中发挥重要作用。你需要进行实验，并查看哪种方法对你的组织最合适。\n与任何要引入到组织中的变化一样，从一个单一应用程序团队开始，建立成功案例，激励其他人开始采用 GitOps。GitOps 的采用需要相关利益相关者的支持。他们需要看到 GitOps 将帮助他们解决问题，使他们的工作更轻松和有效。\nGitOps 是一种相对较新的实践，仍在不断发展。随着这项技术的发展，开源社区和供应商将努力解决一些限制。如果你对进一步探讨这个主题感兴趣，可以查看 CNCF 的 GitOps Working Group。\n","relpermalink":"/book/tsb/knowledge-base/gitops/","summary":"GitOps 是一种实践，它使用 Git 存储库作为应用程序/系统状态的真实来源。对状态的更改通过拉取请求（PR）和批准工作流程执行，并将由持续交付（CD）流程自动应用于系统。以下图像对此进行了说明。 GitOps 有三个核心实践： 基","title":"服务网格与 GitOps"},{"content":"欢迎来到 Tetrate Service Bridge (TSB) 的概念部分。本节向你介绍 TSB 背后的基本思想、其架构以及它如何在你的环境中工作。\nTSB 如何运作？ Tetrate Service Bridge 是一个在基础设施之上运行的服务网格管理平面，提供一个集中平台来管理和配置整个网格管理环境的网络、安全性和可观测性。\n以下是 TSB 工作原理的概述：\n逻辑视图：TSB 通过将资源分组为 services 、 workspaces 和 groups 来组织你的环境，使它们更易于管理。 基于租户的方法：TSB 鼓励你在组织内创建 tenants 。它将用户帐户和团队与你的公司目录同步，从而简化访问管理。 访问控制：TSB 允许你定义细粒度的访问控制，提供编辑权限并实施零信任方法。这增强了安全性并使你能够监控环境中的活动。 审计跟踪：TSB 跟踪服务和共享资源的更改，确保对所有操作（无论是批准还是拒绝）进行审计跟踪。 配置管理：TSB 允许你编写配置更改并将其分组到 services 中，从而使你能够高效地进行更改并集中管理它们。 隔离故障域：TSB 通过为每个集群提供自己的 Istio 控制平面和网关来创建隔离故障域。这可以防止一个集群中的问题影响其他集群并增强应用程序可靠性。 可观测性和遥测：TSB 提供标准化的可观测性和遥测工具，使你可以更轻松地识别问题并近乎实时地监控应用程序。 要点 Tetrate Service Bridge 的设计考虑了企业用户和体验。它提供了一个用于管理不同环境、确保安全性、可靠性和可观测性的综合平台。要深入研究 TSB 的架构，请参阅文档的下一部分，或考虑在演示环境中尝试 TSB。\n","relpermalink":"/book/tsb/concepts/","summary":"TSB 的基础介绍。","title":"概念"},{"content":"本指南使用在 演示环境 中描述的环境，即：\n一个位于 region-1 的 Edge 集群 两个工作负载集群，分别位于 region-1 和 region-2 在工作负载集群中运行的 BookInfo 应用程序 一个 Edge Gateway 用于负载均衡流量到工作负载集群 一个简单的 HTTP 请求被路由到 Edge Gateway，然后转发到其中一个工作负载集群，并生成成功的响应：\ncurl http://bookinfo.tse.tetratelabs.io/productpage 在本指南中，我们将查看系统在工作负载集群发生故障时的故障转移行为。\n生成和观察测试流量 生成一定数量的测试流量对于系统非常有帮助。适用于基准测试的工具，如 wrk，非常适合这个任务：\nwhile sleep 1; do \\ wrk -c 1 -t 1 -d 30 http://bookinfo.tse.tetratelabs.io/productpage ; \\ done 观察 Edge Gateway 上的流量，方法如下：\nkubectl logs -f -n edge -l=app=edgegw | cut -c -60 使用另外两个终端窗口，观察每个 Ingress Gateway 上的流量，方法如下：\nkubectl logs -f -n bookinfo -l=app=ingressgw-1 | cut -c -60 kubectl logs -f -n bookinfo -l=app=ingressgw-2 | cut -c -60 配置 Edge Gateway Edge Gateway 使用 Gateway 资源进行配置，具有以下选项：\nspec: workloadSelector: namespace: edge labels: app: edgegw http: - name: bookinfo port: 80 hostname: bookinfo.tse.tetratelabs.io routing: rules: - route: clusterDestination: {} spec: workloadSelector: namespace: edge labels: app: edgegw http: - name: bookinfo port: 80 hostname: bookinfo.tse.tetratelabs.io routing: rules: - route: clusterDestination: clusters: - name: cluster-1 - name: cluster-2 spec: workloadSelector: namespace: edge labels: app: edgegw http: - name: bookinfo port: 80 hostname: bookinfo.tse.tetratelabs.io routing: rules: - route: # highlight-start clusterDestination: clusters: - name: cluster-1 weight: 50 - name: cluster-2 weight: 50 # highlight-end 自动集群列表 自动集群列表配置是一种简单有效的配置，需要最少的管理工作\nrouting: rules: - route: clusterDestination: {} 使用自动集群列表，Tetrate 平台将确定适合的目标集群。它通过比较 Edge Gateway 资源中的主机名与工作负载集群 Gateway 资源中的匹配主机名来实现。如果向环境中添加或删除了工作负载集群和 Gateway 资源，则 Edge Gateway 将自动重新配置。\nEdge Gateway 仅将流量引导到位于相同地区的工作负载集群上的 Ingress Gateway。这种基于地理位置的选择旨在最小化延迟并避免昂贵的跨地区流量。\n如果位于同一地区的所有 Ingress Gateway 失败，那么 Edge Gateway 将在远程地区的其余工作负载集群之间共享流量。健康检查基于对 Ingress Gateway pods 的异常检测。\n命名集 群列表\n命名集群列表的功能类似于自动集群列表，但只考虑列表中命名的工作负载集群\nrouting: rules: - route: clusterDestination: clusters: - name: cluster-1 - name: cluster-2 使用命名集群列表，Tetrate 平台将流量引导到指定的工作负载集群上的 Ingress Gateway。平台会验证这些集群是否具有 Ingress Gateway，并具有包含匹配主机名的 Gateway 资源。\nEdge Gateway 将流量引导到位于相同地区的工作负载集群上的 Ingress Gateway。这种基于地理位置的选择旨在最小化延迟并避免昂贵的跨地区流量。\n如果位于同一地区的所有 Ingress Gateway 失败，那么 Edge Gateway 将在远程地区的其余工作负载集群之间共享流量。健康检查基于对 Ingress Gateway pods 的异常检测。\n权重集群列表 使用权重来协调流量从一个集群逐渐转移到另一个集群\nrouting: rules: - route: clusterDestination: clusters: - name: cluster-1 weight: 50 - name: cluster-2 weight: 50 使用权重列表，Tetrate 平台将根据配置的权重严格分配流量到集群中。不执行健康检查，因此如果集群没有正常工作的 Ingress Gateway，则定向到该集群的请求将生成 ‘503 no healthy upstream’ 或类似的错误响应。\n权重旨在在受控情况下使用，用于金丝雀测试新集群。\n测试故障处理 你可以按照以下方式测试故障处理：\n运行负载生成器，并按上述方式查看 Gateway 日志 应用所需的 Edge Gateway 配置：tctl apply -f bookinfo-edge.yaml 现在，你可以引发故障并执行以下操作：\n观察每个 Ingress Gateway 上的流量分布 观察成功的请求（状态码 200）和错误（503 或其他状态码） 请注意，在使用权重与集群时，故障转移不会发生。\n移除 Gateway 资源 引发故障的最简单方法是删除工作负载集群上的 Gateway 资源。Tetrate 平台将确定哪些 Ingress Gateway 管理着所需主机名的流量，并相应地配置 Edge Gateway：\n要删除 Gateway 资源：tctl delete -f bookinfo-ingress-1.yaml 要恢复 Gateway 资源：tctl apply -f bookinfo-ingress-1.yaml 此方法模拟了操作性故障，其中 Gateway 资源未应用到工作正常的集群，或者删除了服务及其相关的 Gateway 资源。\n缩放 Ingress Gateway 测试故障处理的另一种方法是将 Ingress Gateway 服务缩放为 0 个副本：\n通过将工作负载集群 Ingress Gateway 缩放为 0 个副本来引发故障：kubectl scale deployment -n bookinfo ingressgw-2 --replicas=0 通过将其缩放为 1 个副本来恢复 Ingress Gateway 故障转移的速度取决于异常检测的速度以及 Tetrate 控制平面的响应速度。要识别故障并重新配置 Edge Gateway 可能需要最多 60 秒。\n此方法模拟了基础架构故障，其中工作负载集群上的 Ingress Gateway 失败。\n缩放上游服务 在每个工作负载集群上，Gateway 资源将流量转发到命名的上游服务，例如 bookinfo/productpage.bookinfo.svc.cluster.local。Tetrate 平台不会明确检查上游服务是否存在和运行，因此如果服务失败，平台将继续将流量引导到工作负载集群。\n然而，作为 Edge Gateway 运行的 Envoy 代理会验证响应代码，并在可能的情况下重试请求，如果收到故障响应。在这种情况下，你会观察到针对失败的工作负载集群的请求生成 503 UH no_healthy_upstream 或类似的错误，然后 Envoy 会重试该请求以针对其他集群。Envoy 会在发送请求到失败的集群时进行退避，但会偶尔尝试以检测其恢复情况。\n你可以通过以下方式调查故障行为，方法如下：\n通过将工作负载集群上游服务缩放为 0 个副本来引发服务故障：kubectl scale deployment -n bookinfo productpage-v1 --replicas=0 通过将其缩放为 1 个副本来恢复服务 这种方法模拟了工作负载集群上的上游服务发生操作性故障的情况。\n内部服务故障转移 此外，你还可以使用 东西向网关 来管理集群内部服务的故障转移。 我们取得了什么成就？ 我们观察了 Tetrate 平台在各种场景中检测故障并进行恢复的行为，这些场景包括 工作负载集群 或其托管的服务发生故障的情况。接下来，我们将考虑如何扩展 Edge Gateways 并处理一个或多个 Edge 集群 / Edge Gateways 的故障。\n","relpermalink":"/book/tsb/design-guides/ha-multicluster/cluster-failover/","summary":"本指南使用在 演示环境 中描述的环境，即： 一个位于 region-1 的 Edge 集群 两个工作负载集群，分别位于 region-1 和 region-2 在工作负载集群中运行的 BookInfo 应用程序 一个 Edge Gateway 用于负载均衡流量到工作负载集群 一个简单的 HTTP 请求被路由到 Edge Gateway，","title":"工作负载集群故障转移"},{"content":"下表显示了现有的 TSB 功能及其当前阶段。该表将在每个主要或次要版本中更新。\n特征阶段定义 下表定义了 TSB 功能的成熟阶段。\n阶段 Alpha（技术预览） Beta Stable 特征 可能不包含最终版本计划的所有功能。 功能完整，但可能包含许多已知或未知的错误。 功能完整，没有已知错误。 生产用途 不应该在生产中使用。 可用于生产。 可靠，生产经过强化。 API 不保证向后兼容性。 API 是有版本的。 可靠，值得生产。API 具有版本控制功能，并具有自动版本转换功能以实现向后兼容性。 性能 未量化或保证。 未量化或保证。 性能（延迟/规模）被量化、记录，并保证不会出现回归。 文档 缺乏文档。 Documented. 记录用例。 环境 在单一环境（仅限 EKS 或 GKE）上进行测试。 至少在两个环境上进行了测试。 （EKS、GKE、OpenShift） 在多种环境下经过良好测试。 （AKS、EKS、GKE、MKE、OpenShift） 监控 并非所有重要指标都可用。 大多数重要指标都可用。 所有重要指标均可用。 功能状态表 领域 描述 状态 API tctl UI 安装 tctl 安装 Stable N Y N helm 安装 Stable N N N Istio 隔离边界 Alpha N N N 用户和访问 从 LDAP 自动同步用户和团队 Stable Y Y Y 从 Azure AD 自动同步用户和团队 Stable Y Y Y 角色 Stable Y Y Y 权限 Stable Y Y Y 与 OIDC 的单点登录 Stable Y Y Y 配置 工作空间 Stable Y Y Y 配置组 Stable Y Y Y 配置桥接模式 - 流量 Stable Y Y Y 配置桥接模式 - 安全 Stable Y Y Y 配置桥接模式 - 网关 Stable Y Y Y 配置直接模式 - 流量 Stable Y Y Y 配置直接模式 - 安全 Stable Y Y Y 配置直连模式 - 网关 Stable Y Y Y 配置直接模式 - IstioInternal Stable Y Y Y 1 级网关 Stable Y Y Y 入口网关（第 2 层） Stable Y Y Y 东西方门户 Beta Y Y Y 虚拟机网关 Stable Y Y Y 出口网关 Beta Y Y Y TCP 流量 Beta Y Y Y 配置状态传播 Beta Y Y Y GitOps/Kubernetes CRD Beta N N N 分级政策 Beta Y Y Y 受限的层级策略 Beta Y Y Y 允许/拒绝规则 Beta Y Y Y 安全域 Alpha Y Y Y 服务安全设置 Alpha Y Y Y 身份传播 Alpha Y Y Y 应用 应用 Beta Y Y Y 使用 OpenAPI 规范配置 API Beta Y Y Y API 网关 速率限制 Beta Y Y Y 外部授权 Beta Y Y Y WASM 扩展 Beta Y Y Y WAF Alpha Y Y Y 可观测性 服务指标 Stable N N Y 服务拓扑 Stable N N Y 服务追踪 Stable N N Y 服务注册中心 库伯内特服务 Stable N Y Y Istio 服务条目 Beta N Y Y 虚拟机工作负载 基于 tctl 的虚拟机载入 Beta N Y N EC2 的工作负载入门 Beta N Y N 本地工作负载的工作负载入门 Beta N Y N ECS 的工作负载载入 Beta N Y N ","relpermalink":"/book/tsb/release-notes-announcements/feature-status/","summary":"内置功能状态。","title":"功能状态"},{"content":"我们将考虑以下故障场景：\n工作负载集群 边缘控制平面 从管理到工作负载的连接丢失 中央控制平面 管理平面 管理集群 并评估故障对以下操作的影响：\n运行生产负载 - 生产负载的可用性、安全性和正确运行 本地集群操作 - 包括直接修改集群配置（如 kubectl 操作）和间接修改（即由本地边缘控制平面进行的更改以应用 TSB 策略或更新服务发现终端点） 指标收集 - 来自远程工作负载集群的指标的集中收集和存储 管理操作 - 由 GitOps、API 或管理 UI 执行的 TSB 配置更改 我们将研究典型的恢复情况，当失败组件恢复或被恢复时。\n架构和术语 在本指南中，我们将使用以下架构描述：\n工作负载集群：工作负载集群是托管生产工作负载的 Kubernetes 集群。 生产工作负载：生产工作负载是在工作负载集群中运行的应用程序或服务。为了避免疑虑，‘生产工作负载’ 还包括非生产工作负载。 数据平面：数据平面是部署在工作负载集群中的本地 Istio 实例。 边缘控制平面：边缘控制平面是部署在工作负载集群中的 Tetrate 软件组件（位于 istio-system 和其他命名空间中，如 cert-manager、xcp-multicluster）。它配置本地 Istio 数据平面，并向中央控制平面报告状态。 管理集群：管理集群是托管 Tetrate 管理平面组件（管理平面、中央控制平面）的 Kubernetes 集群。 中央控制平面：中央控制平面是 Tetrate 软件组件，它接受来自管理平面的配置以及来自边缘控制平面的状态信息。它评估整个配置，然后将必要的配置更新分发给每个边缘控制平面。 管理平面：管理平面是 Tetrate 软件组件，实体（GitOps、API 客户端、UI 客户端）与之交互。它提供 RBAC 访问控制，以控制哪些实体可以对哪些配置进行 CRUD 操作。配置存储在本地，并同步到中央控制平面。 有关更多信息，请参阅 Tetrate 架构文档。\n术语 故障（Failure）意味着相关组件不可用 恢复（Recovery）意味着重新获得相关组件的可用性，通常使用过时的配置或状态 修复（Restoration）意味着重新安装失败的组件，无法恢复组件 ✅ 组件或服务不受影响 ⚠️ 发生了有限的服务中断。 ❌ 发生了受影响组件的完全服务中断 工作负载集群故障 场景：单个工作负载集群发生灾难性故障。\n影响 操作 影响 运行工作负载 本地工作负载不可用。其他集群上的工作负载不受影响。工作负载 HA（Tier1 和 EW 网关）确保不中断服务。 ⚠️ 本地集群操作 无法进行本地集群更改。其他集群不受影响。 ⚠️ 指标收集 无法从本地集群收集指标。其他集群的指标收集不受影响。 ⚠️ 管理操作 针对受影响的工作负载集群的更改将排队，并在集群恢复时应用。所有其他管理操作不受影响。 ✅ 恢复 如果本地集群恢复，配置将迅速更新，并且指标收集将恢复。\n修复 如果需要，可以重新安装 Tetrate 边缘控制平面。当集群重新引入管理平面时，它将同步到正确的配置。\n边缘控制平面故障 场景：单个工作负载集群中的边缘控制平面发生灾难性故障。\n影响 操作 影响 运行工作负载 本地或远程集群中的运行工作负载不受影响。 ✅ 本地集群操作 本地集群更改（kubectl）不受影响。您可以继续向集群推送更新。根据故障的性质：新工作负载可能部分配置运行，直到边缘控制平面恢复。它们可能获取已存在于集群中但可能缺少命名空间定向和细粒度策略的全局集群策略。远程服务的本地服务发现终端点可能不会更新（本地工作负载集群不知道远程集群上的服务更改）。GitOps 集成可能会中断。 ⚠️ 指标收集 指标在本地进行收集，然后转发到管理平面的 ElasticSearch。如果收集器服务不可用，可能无法收集指标。 ⚠️ 管理操作 针对受影响的工作负载集群的更改将排队，并在边缘控制平面恢复时应用。所有其他管理操作不受影响。 ✅ 恢复 如果本地集群恢复，配置将迅速更新，并且指标收集将恢复。\n修复 如果需要，可以重新安装 Tetrate 边缘控制平面。当集群重新引入管理平面时，它将同步到正确的配置。\n连接丢失 - 工作负载到管理集群 场景：工作负载集群与中央管理集群之间的连接丢失。\n影响 操作 影响 运行工作负载 运行本地或远程集群中的工作负载不受影响。 ✅ 本地集群操作 本地集群更改（kubectl）不受影响。新工作负载可能部分配置运行，直到恢复连接。它们可能获取已存在于集群中但缺少命名空间定向和细粒度策略的全局集群策略。远程服务的本地服务发现终端点不会更新。GitOps 操作可能会中断。 ⚠️ 指标收集 指标在受影响的工作负载集群中进行收集并排队。长时间失去连接将导致一些指标丢失。 ⚠️ 管理操作 针对受影响的工作负载集群的更改将排队，并在恢复连接时应用。所有其他管理操作不受影响。 ✅ 恢复 恢复连接后，配置将迅速更新，并且指标收集将恢复。\n中央控制平面故障 场景：中央控制平面组件发生故障。\n影响 操作 影响 运行工作负载 运行本地或远程集群中的工作负载不受影响。 ✅ 本地集群操作 本地集群更改（kubectl）不受影响。新工作负载可能部分配置运行，直到中央控制平面恢复。它们可能获取已存在于集群中但缺少命名空间定向和细粒度策略的全局集群策略。远程服务的本地服务发现终端点不会更新。 ⚠️ 指标收集 指标收集不受影响。 ✅ 管理操作 配置读取、仪表板（指标）不受影响。配置更新排队。 ✅ 恢复 当中央控制平面恢复时，其本地配置缓存会自动恢复，通常在 1-2 分钟内完成。然后会更新远程集群配置。不会丢失任何配置更改或数据。\n修复 如果需要，可以在Tetrate 技术支持 的帮助下重新安装中央控制平面组件。不需要备份。\n管理平面故障 场景：管理平面组件故障。\n影响 操作 影响 运行工作负载 本地或远程集群中运行的工作负载不受影响。 ✅ 本地集群操作 本地集群更改（kubectl）不受影响。新的工作负载可能会部分配置，直到管理平面恢复。它们可能会获取全局集群策略，但会缺少针对命名空间和细粒度策略的配置。中央和本地控制平面维护服务发现端点。 ✅ 指标收集 如果 ElasticSearch 数据库对工作负载集群不可用，将影响指标收集。 ⚠️ 管理操作 无法进行 TSB 配置更改。UI、API 和 GitOps 操作不可用。 ❌ TSB 配置存储在客户提供的 PostgreSQL 数据库中，还存储在一些辅助服务中 - 用于 PKI 管理的 cert-manager，tsb 命名空间中的其他密钥。\n恢复 如果管理平面在不丢失数据的情况下恢复，操作将如之前一样继续进行，不应发生配置丢失。\n恢复 可以从 PostgreSQL 数据库的备份和 iam-signing-key 重新构建管理控制平面组件，需要Tetrate 技术支持 的协助。有关该过程的概述，请参阅管理平面灾难恢复 文档。\n管理集群故障 场景：Kubernetes 管理集群故障。\n影响 操作 影响 运行工作负载 本地或远程集群中运行的工作负载不受影响。 ✅ 本地集群操作 本地集群更改（kubectl）不受影响。中央 TSB 策略（例如新命名空间的默认设置）适用于新配置。远程服务的本地服务发现端点不会更新。 ⚠️ 指标收集 无法从任何工作负载集群中收集指标。 ❌ 管理操作 无法进行 TSB 配置更改。UI、API 和 GitOps 操作不可用。 ❌ 恢复 如果管理平面在不丢失数据的情况下恢复，操作将如之前一样继续进行，不应发生配置丢失。\n修复 可以从 PostgreSQL 数据库的备份和 iam-signing-key 重新构建管理控制平面组件，需要Tetrate 技术支持 的协助。有关该过程的概述，请参阅管理平面灾难恢复 文档。\n建议 为了应对管理组件的意外故障，我们建议考虑以下建议：\n要么在可靠的冗余集群中维护 Postgres 数据库，要么（在 TSE 的情况下）利用定期的 Postgres 备份 。 保留 iam-signing-key 的备份 如果保留指标很重要，请在可靠的冗余集群中维护 ElasticSearch 数据库，或定期备份，以便在必要时进行恢复。 有关恢复故障管理平面组件的过程概述，请参阅管理平面灾难恢复 文档。\n","relpermalink":"/book/tsb/design-guides/ha-dr-mp/scenarios/","summary":"我们将考虑以下故障场景： 工作负载集群 边缘控制平面 从管理到工作负载的连接丢失 中央控制平面 管理平面 管理集群 并评估故障对以下操作的影响： 运行生产负载 - 生产负载的可用性、安全性和正确运行 本地集群操作 - 包括直接","title":"故障场景"},{"content":"此 Chart 安装 TSB 管理平面 Operator，还允许你使用 TSB ManagementPlane CR 安装 TSB 管理平面组件以及使其完全运行所需的所有秘密。\n在开始之前，请确保你已经查看了 Helm 安装过程 。\n安装概述 创建一个 values.yaml 文件并使用所需的配置进行编辑。你可以在下面的配置部分中找到有关可用 Helm 配置的更多详细信息。有关 spec 部分的完整参考，请参阅 TSB ManagementPlane CR 。\n使用 helm install 命令安装 TSB 管理平面。确保将 image.registry 和 version 设置为正确的注册表位置和 TSB 版本。\n等待所有 TSB 管理平面组件成功部署。你可以尝试登录 TSB UI 或使用 tctl 来验证你的安装。\n安装 要安装 TSB 管理平面，请创建一个 values.yaml 文件，包含以下内容，并根据你的需求进行编辑。\nspec: # 设置组织名称。组织名称必须小写以符合 RFC 标准。 organization: \u0026lt;organization-name\u0026gt; dataStore: postgres: host: \u0026lt;postgres-hostname-or-ip\u0026gt; port: \u0026lt;postgres-port\u0026gt; name: \u0026lt;database-name\u0026gt; telemetryStore: elastic: host: \u0026lt;elastic-hostname-or-ip\u0026gt; port: \u0026lt;elastic-port\u0026gt; version: \u0026lt;elastic-version\u0026gt; selfSigned: \u0026lt;is-elastic-use-self-signed-certificate\u0026gt; # TSB 支持 OIDC 和 LDAP 作为身份提供者。 # 根据你的环境进行设置。 identityProvider: ... # 启用自动证书管理。 # 如果要使用其他方法管理证书，可以删除此字段。 # 请注意，在这种情况下，你将需要提供证书作为机密。 certIssuer: selfSigned: {} tsbCerts: {} clusterIntermediateCAs: {} # TSB 管理平面的默认端口是 8443。你可以在此处更改它。 components: frontEnvoy: port: 443 # 启用 oap 流式日志功能 oap: streamingLogEnabled: true secrets: tsb: adminPassword: \u0026lt;tsb-admin-password\u0026gt; postgres: username: \u0026lt;postgres-username\u0026gt; password: \u0026lt;postgres-password\u0026gt; # 根据你的 IdP，你需要在此处设置所需的秘密。 ... 然后，使用以下 helm install 命令安装 TSB 管理平面。此安装可能需要最多 10 分钟才能完成。确保用正确的值替换 \u0026lt;tsb-version\u0026gt; 和 \u0026lt;registry-location\u0026gt;。\nhelm install mp tetrate-tsb-helm/managementplane \\ --version \u0026lt;tsb-version\u0026gt; \\ --namespace tsb --create-namespace \\ --values values.yaml \\ --timeout 10m \\ --set image.registry=\u0026lt;registry-location\u0026gt; 非生产外部依赖 如果在 values.yaml 文件中省略了 dataStore、telemetryStore 和 identityProvider 字段，TSB 将安装非生产 Postgres、Elasticsearch 和 LDAP。请注意，你仍然需要设置正确的秘密和凭据以使用存储。\n注意 请勿在生产环境中使用非生产存储和身份提供者。 以下是演示安装的 values.yaml 文件的示例完成内容：\nspec: organization: \u0026lt;organization-name\u0026gt; # 启用自动证书管理。 certIssuer: selfSigned: {} tsbCerts: {} clusterIntermediateCAs: {} secrets: tsb: adminPassword: \u0026lt;tsb-admin-password\u0026gt; postgres: username: tsb password: tsb-postgres-password ldap: binddn: cn=admin,dc=tetrate,dc=io bindpassword: admin 访问 TSB 管理平面 完成安装后，你可以通过登录到 TSB UI 或使用 tctl 来访问 TSB 管理平面。\n故障排除 如果在安装过程中遇到任何问题，请检查以下几点：\n确保你在 values.yaml 文件中输入了正确的值。 验证你在 helm install 命令中使用的注册表位置和 TSB 版本是否正确。 如果你使用自定义身份提供者，请确保你在 values.yaml 文件的 secrets 部分设置了所有所需的 secrets。 如果无法连接到 TSB，请确保所有 TSB 组件都已成功部署，日志中没有错误。 如果你使用私有注册表来托管 TSB 控制平面 Operator 镜像，请确保已对注册表进行身份验证，并且 image.registry 值正确。 配置 镜像配置 这是一个 必填 字段。将 image.registry 设置为你的私有注册表位置，你已经同步了 TSB 镜像，将 image.tag 设置为要部署的 TSB 版本。仅指定此字段将安装 TSB 控制平面 Operator，而不安装其他 TSB 组件。\n名称 描述 默认值 image.registry 用于下载 Operator 镜像的注册表。必填 containers.dl.tetrate.io image.tag Operator 镜像的标签。必填 与 Chart 版本相同 管理平面资源配置 这是一个 可选 字段。你可以在 Helm 值文件中设置 TSB ManagementPlane CR spec ，以使 TSB 管理平面完全运行。\n名称 描述 默认值 spec 包含 ManagementPlane CR 的 spec 部分。可选 秘密配置 这是一个 可选 字段。你可以在安装 TSB 管理平面之前将秘密应用到你的集群中，或者你可以使用 Helm 值来指定所需的秘密。请注意，如果要将秘密与管理平面规范分开，则可以使用不同的 Helm 值文件。\n注意 请记住，这些选项只是帮助创建秘密的选项，它们必须遵守 TSB ManagementPlane CR 中提供的配置，否则安装将配置不正确。 名称 描述 默认值 secrets.keep 启用此选项将使生成的秘密在卸载 Chart 后持续存在于集群中，如果它们在将来的更新中未提供，则不会删除它们。 (参见 Helm 文档 ) false secrets.tsb.adminPassword 为 admin 用户配置的密码。 secrets.tsb.cert 管理平面 (front envoy) 暴露的 TLS 证书。 secrets.tsb.key 管理平面 (front envoy) 暴露的 TLS 证书的密钥。 secrets.postgres.username 访问 Postgres 数据库的用户名。 secrets.postgres.password 访问 Postgres 数据库的密码。 secrets.postgres.cacert 用于验证 Postgres 数据库提供的 TLS 证书的 CA 证书。 secrets.postgres.clientcert 访问 Postgres 数据库所需的客户端证书。 secrets.postgres.clientkey 访问 Postgres 数据库所需的客户端证书的密钥。 secrets.elasticsearch.username 访问 Elasticsearch 所需的用户名。 secrets.elasticsearch.password 访问 Elasticsearch 所需的密码。 secrets.elasticsearch.cacert 用于验证 Elasticsearch 提供的 TLS 证书的 CA 证书。 secrets.ldap.binddn 用于从 LDAP IDP 中读取的绑定 DN。 secrets.ldap.bindpassword 用于从 LDAP IDP 中读取的绑定 DN 的提供密码。 secrets.ldap.cacert 用于验证 LDAP IDP 提供的 TLS 证书的 CA 证书。 secrets.oidc.clientSecret 用于连接到配置的 OIDC 的客户端密钥。 secrets.oidc.deviceClientSecret 用于连接到配置的 OIDC 的设备客户端密钥。 secrets.azure.clientSecret 用于连接到 Azure OIDC 的客户端密钥。 XCP 秘密配置 XCP 使用 TLS 和 JWT 在 Edge 和 Central 之间进行身份验证。\n如果 secrets.xcp.autoGenerateCerts 已 禁用，则用户必须提供 XCP Central 的证书和密钥，使用 secrets.xcp.central.cert 和 secrets.xcp.central.key。\n此外，用户可以选择提供 CA，使用 secrets.xcp.rootca 允许 MPC 组件使用它来验证 XCP Central 提供的证书。\n如果 secrets.xcp.autoGenerateCerts 已 启用，则需要 Cert Manager 来提供 XCP Central 证书。\n然后，secrets.xcp.rootca 和 secrets.xcp.rootcakey 将用于创建正确的 Issuer 并生成 XCP Central 的证书，并与 MPC 共享 CA，以允许其验证 XCP Central 生成的证书。\n以下属性允许用于配置 XCP 身份验证模式：\n名称 描述 默认值 secrets.xcp.autoGenerateCerts 启用此选项将自动生成 XCP Central 的 TLS 证书。需要 cert-manager false secrets.xcp.rootca XCP 组件的 CA 证书。 ","relpermalink":"/book/tsb/setup/helm/managementplane/","summary":"如何使用 Helm 安装管理平面元素。","title":"管理平面安装"},{"content":"本页面将向你展示如何在生产环境中安装 Tetrate Service Bridge 管理平面。\n在开始之前，请确保你已经：\n检查了要求 检查了TSB 管理平面组件 检查了证书类型 和内部证书要求 检查了防火墙信息 如果你正在升级以前的版本，请还要检查PostgreSQL 备份和还原 下载 了 Tetrate Service Bridge CLI（tctl）\n同步 了 Tetrate Service Bridge 镜像\n管理平面 Operator 为了保持安装简单，但仍允许许多自定义配置选项，我们创建了一个管理平面 Operator。该 Operator 将在集群中运行，并根据 ManagementPlane 自定义资源中描述的内容引导管理平面的启动。它会监视更改并执行它们。为了帮助创建正确的自定义资源文档（CRD），我们已经添加了能力到我们的tctl客户端，用于创建基本清单，然后你可以根据你的要求进行修改。之后，你可以将清单直接应用于适当的集群，或在你的源控制操作的集群中使用。\n关于 Operator 如果你想了解有关 Operator 的内部工作原理以及 Operator 模式的更多信息，请查看Kubernetes 文档 。 创建清单，以允许你从私有 Docker 注册表安装管理平面 Operator：\ntctl install manifest management-plane-operator \\ --registry \u0026lt;registry-location\u0026gt; \u0026gt; managementplaneoperator.yaml OpenShift\n使用安装清单命令创建的managementplaneoperator.yaml文件可以通过使用 kubectl 客户端直接应用于适当的集群：\nkubectl apply -f managementplaneoperator.yaml 应用清单后，你将在tsb命名空间中看到 Operator 正在运行：\nkubectl get pod -n tsb RedHat 生态系统目录 TSB 已在 RedHat 生态系统目录上获得了认证并列出。可以按照以下说明或通过此处 在 OpenShift 平台上安装 TSB。 使用安装清单命令创建的managementplaneoperator.yaml文件可以通过使用oc客户端直接应用于适当的集群：\noc apply -f managementplaneoperator.yaml 应用清单后，你将在tsb命名空间中看到 Operator 正在运行：\noc get pod -n tsb 示例输出：\nNAME READY STATUS RESTARTS AGE tsb-operator-management-plane-d4c86f5c8-b2zb5 1/1 Running 0 8s 配置机密 管理平面组件需要一些机密，用于内部和外部通信目的。以下是你需要创建的机密列表。\n机密名称 描述 admin-credentials TSB 将创建一个默认的管理员用户，用户名为：admin，这是该特殊帐户的密码的单向哈希。这些凭据保存在你的 IdP（身份提供者）之外，而其他任何凭据必须存储在你的 IdP 中。 tsb-certs TLS 证书，类型为kubernetes.io/tls。必须具有tls.key和tls.cert值。TLS 证书可以是自签名的，也可以由公共 CA 颁发。 postgres-credentials 包含：\n1. PostgreSQL 用户名和密码。\n2. 用于在 PostgreSQL 配置为呈现自签名证书时验证 PostgreSQL 连接的 CA 证书。仅当在 Postgres 设置中将sslMode设置为verify-ca或verify-full时，才会进行 TLS 验证。有关详细信息，请参见PostgresSettings 。 3. 如果 Postgres 配置了互通 TLS，则包含客户端证书和私钥。 elastic-credentials Elasticsearch 用户名和密码。 es-certs Elasticsearch 配置为呈现自签名证书时，用于验证 Elasticsearch 连接的 CA 证书。 ldap-credentials 仅在使用 LDAP 作为身份提供者（IdP）时设置。包含 LDAP binddn和bindpassword。 custom-host-ca 仅在使用 LDAP 作为 IdP 时设置。用于验证 LDAP 连接的 CA 证书，当 LDAP 配置为呈现自签名证书时。 iam-oidc-client-secret 仅在使用 OIDC 与任何 IdP 时设置。包含 OIDC 客户端密钥和设备客户端密钥。 azure-credentials 仅在使用 Azure AD 作为 IdP 时设置。用于连接到 Azure AD 进行团队和用户同步的客户端密钥。 xcp-central-cert XCP 中央 TLS 证书。请转到内部证书要求 以获取更多详细信息。 使用 tctl 生成机密 注意 自 1.7 以来，TSB 支持自动管理 TSB 管理平面 TLS 证书、内部证书和中间 Istio CA 证书。有关详细信息，请转到自动证书管理 。这意味着你不再需要创建tsb-certs和xcp-central-cert机密。以下示例将假定你正在使用自动证书管理。 可以通过将它们作为命令行标志传递给tctl管理平面机密命令，以正确的格式生成这些机密。\nOIDC 作为 IdP\n以下命令将生成包含 Elasticsearch、Postgres、OIDC 和管理员凭据以及 TSB TLS 证书的managementplane-secrets.yaml：\ntctl install manifest management-plane-secrets \\ --elastic-password \u0026lt;elastic-password\u0026gt; \\ --elastic-username \u0026lt;elastic-username\u0026gt; \\ --oidc-client-secret \u0026#34;\u0026lt;oidc-client-secret\u0026gt;\u0026#34; \\ --postgres-password \u0026lt;postgres-password\u0026gt; \\ --postgres-username \u0026lt;postgres-username\u0026gt; \\ --tsb-admin-password \u0026lt;tsb-admin-password\u0026gt; \u0026gt; managementplane-secrets.yaml LDAP 作为 IdP\n以下命令将生成包含 Elasticsearch、Postgres、LDAP 和管理员凭据以及 TSB TLS 证书的managementplane-secrets.yaml：\ntctl install manifest management-plane-secrets \\ --elastic-password \u0026lt;elastic-password\u0026gt; \\ --elastic-username \u0026lt;elastic-username\u0026gt; \\ --ldap-bind-dn \u0026lt;ldap-bind-dn\u0026gt; \\ --ldap-bind-password \u0026lt;ldap-bind-password\u0026gt; \\ --postgres-password \u0026lt;postgres-password\u0026gt; \\ --postgres-username \u0026lt;postgres-username\u0026gt; \\ --tsb-admin-password \u0026lt;tsb-admin-password\u0026gt; \u0026gt; managementplane-secrets.yaml 查看CLI 参考 文档以获取所有可用选项，例如为Elasticsearch、PostgreSQL和LDAP提供 CA 证书。你还可以通过运行以下帮助命令，从tctl中检查绑定的解释：\ntctl install manifest management-plane-secrets --help 应用机密 一旦你创建了机密清单，就可以将其添加到源代码控制或应用于你的集群。\nVault 注入 如果你正在为某些组件使用Vault注入，请在将其应用到集群之前，从你创建的清单中删除适用的机密。 OpenShift\nkubectl apply -f managementplane-secrets.yaml 在应用它之前，请记住，你必须允许不同管理平面组件的服务帐户访问你的 OpenShift 授权策略。\noc adm policy add-scc-to-user anyuid -n tsb -z tsb-iam oc adm policy add-scc-to-user anyuid -n tsb -z tsb-oap 现在可以应用它：\noc apply -f managementplane-secrets.yaml 注意：TSB 将每小时自动执行此操作，因此此命令只需在初始安装后运行一次。\n验证安装 为验证你的安装成功，请使用管理员用户登录。尝试连接到 TSB UI 或使用tctl CLI 工具登录。\nTSB UI 可通过以下命令返回的外部 IP 的端口 8443 访问：\n标准\nkubectl get svc -n tsb envoy OpenShift\noc get svc -n tsb envoy 要将tctl的默认配置文件设置为指向你的新 TSB 集群，请执行以下操作：\n标准\ntctl config clusters set default --bridge-address $(kubectl get svc -n tsb envoy --output jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;):8443 AWS\ntctl config clusters set default --bridge-address $(kubectl get svc -n tsb envoy --output jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;):8443 现在，你可以使用tctl登录，并提供组织名称和管理员帐户凭据。租户字段是可选的，可以在稍后配置，当添加租户到平台时。\ntctl login Organization: tetrate Tenant: Username: admin Password: ***** Login Successful! 查看使用 tctl 连接到 TSB 以获取有关如何配置 tctl 的更多详细信息。\n","relpermalink":"/book/tsb/setup/self-managed/management-plane-installation/","summary":"安装和设置 Tetrate Service Bridge 管理平面。","title":"管理平面安装"},{"content":"概述 要将 AWS 弹性容器服务（ECS）任务加入，你需要按照以下步骤操作：\n创建 AWS ECS 集群 创建任务的 IAM 角色 创建任务执行 IAM 角色 创建一个包含 Workload Onboarding Agent 作为 sidecar 容器的 AWS ECS 任务定义 创建任务的子网 创建安全组 使用此任务定义创建 AWS ECS 服务 创建 AWS ECS 集群 使用 FARGATE 作为容量提供程序创建名为 bookinfo 的 AWS ECS 集群。\naws ecs create-cluster --cluster-name bookinfo --capacity-providers FARGATE 为任务创建 IAM 角色 创建任务的 IAM 角色，并使用以下信任策略。\ncat \u0026lt;\u0026lt; EOF \u0026gt; task-role-trust-policy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ecs-tasks.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } EOF aws iam create-role \\ --role-name bookinfoECSTaskRole \\ --assume-role-policy-document file://task-role-trust-policy.json 使用以下策略配置此角色，以允许 ECS Exec 。 这不是任务加入网格所必需的，但在后续的指南中用于验证任务到 Kubernetes 服务的流量。\ncat \u0026lt;\u0026lt; EOF \u0026gt; ecs-exec-policy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ssmmessages:CreateControlChannel\u0026#34;, \u0026#34;ssmmessages:CreateDataChannel\u0026#34;, \u0026#34;ssmmessages:OpenControlChannel\u0026#34;, \u0026#34;ssmmessages:OpenDataChannel\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } EOF aws iam put-role-policy \\ --role-name bookinfoECSTaskRole \\ --policy-name bookinfoECSExecPolicy \\ --policy-document file://ecs-exec-policy.json 创建任务执行 IAM 角色 创建任务执行 IAM 角色，并使用以下信任策略，并配置使用 AWS 托管的 AmazonECSTaskExecutionRolePolicy 策略。此策略授予任务访问 Elastic Container Registry（ECR）中的镜像和写日志的权限。\ncat \u0026lt;\u0026lt; EOF \u0026gt; task-exec-role-trust-policy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ecs-tasks.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } EOF aws iam create-role \\ --role-name bookinfoECSTaskExecRole \\ --assume-role-policy-document file://task-exec-role-trust-policy.json aws iam attach-role-policy \\ --role-name bookinfoECSTaskExecRole \\ --policy-arn arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy 创建 AWS ECS 任务定义 将 Onboarding 配置设置为 JSON 形式的 shell 变量，去除空格并转义引号，以便可以编码在 ECS 任务容器定义中。将 \u0026lt;ONBOARDING_ENDPOINT_ADDRESS\u0026gt; 替换为你之前获取的值。\nONBOARDING_CONFIG=$(jq --compact-output . \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; | sed \u0026#39;s/\u0026#34;/\\\\\u0026#34;/g\u0026#39; { \u0026#34;apiVersion\u0026#34;: \u0026#34;config.agent.onboarding.tetrate.io/v1alpha1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;OnboardingConfiguration\u0026#34;, \u0026#34;onboardingEndpoint\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;\u0026lt;ONBOARDING_ENDPOINT_ADDRESS\u0026gt;\u0026#34;, \u0026#34;transportSecurity\u0026#34;: { \u0026#34;tls\u0026#34;: { \u0026#34;sni\u0026#34;: \u0026#34;onboarding-endpoint.example\u0026#34; } } }, \u0026#34;workload\u0026#34;: { \u0026#34;labels\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;v5\u0026#34; } }, \u0026#34;workloadGroup\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;ratings\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;bookinfo\u0026#34; } } EOF ) 将用于签名 Onboarding 平面 TLS 证书的自签名根证书设置为 shell 变量，并转义换行符，以便可以编码在 ECS 任务容器定义中。example-ca.crt.pem 是在启用工作负载加入 时创建的自签名证书。\nONBOARDING_AGENT_ROOT_CERTS=$(awk \u0026#39;{printf \u0026#34;%s\\\\n\u0026#34;, $0}\u0026#39; example-ca.crt.pem) 现在，使用以下命令创建 ECS 任务定义：\nAWS_REGION=$(aws configure get region) TASK_ROLE_ARN=$(aws iam get-role --role-name bookinfoECSTaskRole --query \u0026#39;Role.Arn\u0026#39; --output text) TASK_EXECUTION_ROLE_ARN=$(aws iam get-role --role-name bookinfoECSTaskExecRole --query \u0026#39;Role.Arn\u0026#39; --output text) ONBOARDING_AGENT_IMAGE=$(kubectl get deploy onboarding-operator -n istio-system -ojsonpath=\u0026#39;{.spec.template.spec.containers[?(@.name==\u0026#34;onboarding-operator\u0026#34;)].image}\u0026#39; | sed \u0026#39;s|/onboarding-operator-server:|/onboarding-agent:|\u0026#39;) aws ecs register-task-definition \\ --task-role-arn=\u0026#34;${TASK_ROLE_ARN}\u0026#34; \\ --execution-role-arn=\u0026#34;${TASK_EXECUTION_ROLE_ARN}\u0026#34; \\ --family=\u0026#34;bookinfo_ratings\u0026#34; \\ --network-mode=\u0026#34;awsvpc\u0026#34; \\ --cpu=256 \\ --memory=512 \\ --requires-compatibilities FARGATE \\ --container-definitions=\u0026#39;[ { \u0026#34;name\u0026#34;: \u0026#34;onboarding-agent\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;\u0026#39;\u0026#34;${ONBOARDING_AGENT_IMAGE}\u0026#34;\u0026#39;\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;environment\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ONBOARDING_CONFIG\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\u0026#39;\u0026#34;${ONBOARDING_CONFIG}\u0026#34;\u0026#39;\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;ONBOARDING_AGENT_ROOT_CERTS\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\u0026#39;\u0026#34;${ONBOARDING_AGENT_ROOT_CERTS}\u0026#34;\u0026#39;\u0026#34; } ], \u0026#34;logConfiguration\u0026#34;: { \u0026#34;logDriver\u0026#34;: \u0026#34;awslogs\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;awslogs-group\u0026#34;: \u0026#34;/ecs/bookinfo_ratings\u0026#34;, \u0026#34;awslogs-region\u0026#34;: \u0026#34;\u0026#39;\u0026#34;${AWS_REGION}\u0026#34;\u0026#39;\u0026#34;, \u0026#34;awslogs-stream-prefix\u0026#34;: \u0026#34;ecs\u0026#34; } } }, { \u0026#34;name\u0026#34;: \u0026#34;ratings\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;docker.io/tetrate/tetrate-examples-bookinfo-ratings-localhost-v1:1.16.4\u0026#34;, \u0026#34;essential\u0026#34;: true, \u0026#34;logConfiguration\u0026#34;: { \u0026#34;logDriver\u0026#34;: \u0026#34;awslogs\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;awslogs-group\u0026#34;: \u0026#34;/ecs/bookinfo_ratings\u0026#34;, \u0026#34;awslogs-region\u0026#34;: \u0026#34;\u0026#39;\u0026#34;${AWS_REGION}\u0026#34;\u0026#39;\u0026#34;, \u0026#34;awslogs-stream-prefix\u0026#34;: \u0026#34;ecs\u0026#34; } } } ]\u0026#39; 这将配置任务使用 awslogs 驱动程序 将日志写入 /ecs/bookinfo_ratings 日志组。使用以下命令创建此日志组：\naws logs create-log-group --log-group-name \u0026#34;/ecs/bookinfo_ratings\u0026#34; 创建子网 确保 VPC 中存在 NAT 网关 通过运行以下命令，确保 VPC 中存在 NAT 网关，将 EKS_CLUSTER_NAME 替换为你的 EKS 集群名称。\nVPC_ID=$(aws eks describe-cluster --name \u0026lt;EKS_CLUSTER_NAME\u0026gt; --query \u0026#39;cluster.resourcesVpcConfig.vpcId\u0026#39; --output text) aws ec2 describe-nat-gateways --filter Name=vpc-id,Values=${VPC_ID} 如果返回的列表为空，请使用以下命令创建具有 NAT 网关的公共子网，并将 NAT 网关替换为上面找到或创建的 NAT 网关，使用以下命令，将 CIDR 块替换为用于子网的所需 CIDR 块，例如 10.0.3.0/24。\nINTERNET_GATEWAY_ID=$(aws ec2 describe-internet-gateways --filters Name=attachment.vpc-id,Values=${VPC_ID} --query \u0026#39;InternetGateways[0].InternetGatewayId\u0026#39; --output text) aws ec2 create-subnet \\ --vpc-id \u0026#34;${VPC_ID}\u0026#34; \\ --cidr-block \u0026lt;CIDR_BLOCK\u0026gt; \\ --tag-specifications \u0026#39;ResourceType=subnet,Tags=[{Key=Name,Value=bookinfo-ecs-nat-gw-subnet}]\u0026#39; NAT_GW_SUBNET_ID=$(aws ec2 describe-subnets --filters Name=tag:Name,Values=bookinfo-ecs-nat-gw-subnet …","relpermalink":"/book/tsb/setup/workload-onboarding/quickstart/aws-ecs/onboard-ecs/","summary":"概述 要将 AWS 弹性容器服务（ECS）任务加入，你需要按照以下步骤操作： 创建 AWS ECS 集群 创建任务的 IAM 角色 创建任务执行 IAM 角色 创建一个包含 Workload Onboarding Agent 作为 sidecar 容器的 AWS ECS 任务定义 创建任务的子网 创建安全组 使用此任务定义创建 AWS ECS","title":"加入 AWS ECS 任务"},{"content":"本文描述了如何配置 Tetrate Service Bridge（TSB）的 LDAP 集成。在 TSB 中，LDAP 集成允许你将 LDAP 用作用户登录 TSB 的身份提供者，并自动将用户和组从 LDAP 同步到 TSB。\n本文假定你已经具备配置 LDAP 服务以及如何使用它进行身份验证的工作知识。\n配置 LDAP 可以通过ManagementPlane CR 或 Helm 值进行配置。以下是一个使用 LDAP 作为 TSB 身份提供者的自定义资源 YAML 的示例。你需要编辑ManagementPlane CR 或 Helm 值，并配置相关部分。请参阅LDAPSettings 了解更多详细信息。\n以下各节将更详细地解释 YAML 文件的每个部分的含义。\nspec: hub: \u0026lt;registry-location\u0026gt; organization: \u0026lt;organization-name\u0026gt; ... identityProvider: ldap: host: \u0026lt;ldap-hostname-or-ip\u0026gt; port: \u0026lt;ldap-port\u0026gt; search: baseDN: dc=tetrate,dc=io iam: matchDN: \u0026#34;cn=%s,ou=People,dc=tetrate,dc=io\u0026#34; matchFilter: \u0026#34;(\u0026amp;(objectClass=person)(uid=%s))\u0026#34; sync: usersFilter: \u0026#34;(objectClass=person)\u0026#34; groupsFilter: \u0026#34;(objectClass=groupOfUniqueNames)\u0026#34; membershipAttribute: uniqueMember 身份提供者配置 使用 LDAP 作为身份提供者有两种方法：\n使用直接绑定身份验证 使用基于搜索的身份验证 使用直接绑定身份验证是首选的，因为性能更好，但它要求整个 LDAP 树中的用户 Distinguished Names（“DN\u0026#34;s）是统一的。如果情况不是这样，可以配置更灵活的基于搜索的身份验证，以根据预先配置的查询对用户进行身份验证。\n这些方法不是互斥的。如果都配置了，将首先尝试直接绑定身份验证，如果无法验证用户，TSB 将回退到使用基于搜索的身份验证。\n直接绑定身份验证 在 LDAP 中，身份验证是通过执行与 DN 的绑定操作来完成的。DN 预期是一个已配置密码的用户记录。绑定操作尝试将给定的 DN 和密码与现有记录匹配。如果绑定操作成功，则身份验证成功。\n然而，DNs 通常以以下形式出现，例如uid=nacx,ou=People,dc=tetrate,dc=io。这种格式对于常规登录来说不太方便，因为不应该要求用户在登录表单中键入完整的 DN。直接绑定身份验证允许你配置一个用于匹配登录用户的模式，并将其用作 DN。\n以下示例配置了直接绑定身份验证模式：\niam: matchdn: \u0026#39;uid=%s,ou=People,dc=tetrate,dc=io\u0026#39; 在此示例中，登录时，模式中的%s将被提供的登录用户替换，生成的 DN 将用于绑定身份验证。\n基于搜索的身份验证 如前所述，如果所有现有用户都可以与相同的 DN 模式匹配，直接绑定身份验证效果很好。然而，在某些情况下，用户可能会创建在 LDAP 树的不同部分（例如，每个用户可以在组织内特定部门的组内创建），这样就不可能有一个单一的模式来匹配所有用户。\n在这种情况下，你可以在 LDAP 树上执行搜索，查找与给定用户名匹配的记录，然后尝试使用记录的 DN 进行绑定身份验证。\n为了执行搜索，必须建立到 LDAP 服务器的连接。如果服务器未配置为匿名访问，可能需要凭据。有关更多详细信息，请参阅“凭据和证书”部分。\n以下示例显示了如何配置基于搜索的身份验证：\nsearch: baseDN: dc=tetrate,dc=io iam: matchfilter: \u0026#39;(\u0026amp;(objectClass=person)(uid=%s))\u0026#39; 在此示例中，配置了一个搜索，以查找从dc=tetrate,dc=io开始的树（iam.matchFilter使用在search.baseDN中定义的查询）。将尝试匹配所有类型为person并且具有uid属性等于给定用户名的记录。与直接绑定身份验证类似，搜索模式期望有一个%s占位符，该占位符将由给定的用户名替换。\n结合直接和搜索身份验证方法 可以结合两种身份验证方法，以配置更灵活的身份验证配置。当同时配置了这两种方法时，直接绑定身份验证将具有优先权，因为它不需要遍历 LDAP 树，因此更有效率。\n同时使用两种身份验证策略的示例可能如下所示：\niam: matchdn: \u0026#39;uid=%s,ou=People,dc=tetrate,dc=io\u0026#39; matchfilter: \u0026#39;(\u0026amp;(objectClass=person)(uid=%s))\u0026#39; 使用 Microsoft Active Directory Microsoft Active Directory 以不同的方式实现了 LDAP 绑定身份验证。它不使用 LDAP 绑定操作的完整 DN，而是使用用户（应该是形式为：user@domain）。\n由于这是在登录表单中可能配置的用户名，因此直接身份验证可以简单地配置如下：\niam: matchdn: \u0026#39;%s\u0026#39; 如果直接身份验证无法满足所有身份验证需求，可以使用以下筛选器配置 Active Directory 中的基于搜索的身份验证，该筛选器匹配标准的 AD 用户帐户标识方式：\niam: matchfilter: \u0026#39;(\u0026amp;(objectClass=user)(samAccountName =%s))\u0026#39; 凭据和证书 某些操作需要对 LDAP 服务器运行特权查询，例如获取整个组和用户列表，或使用搜索对用户进行身份验证。在这些情况下，如果需要凭据，则必须在 Kubernetes Secret 中进行配置。\n你可以使用 tctl install manifest management-plane-secrets 命令创建所需的凭据和证书，以连接到 LDAP 服务器。\ntctl install manifest management-plane-secrets \\ … --ldap-bind-dn \u0026lt;ldap-bind-dn\u0026gt; \\ --ldap-bind-password \u0026lt;ldap-bind-password\u0026gt; \\ --ldap-ca-certificate \u0026#34;$(cat ldap-ca.cert)\u0026#34; \\ --tsb-admin-password \u0026lt;tsb-admin-password\u0026gt; \\ --tsb-server-certificate \u0026#34;$(cat foo.cert)\u0026#34; \\ --tsb-server-key \u0026#34;$(cat foo.key)\u0026#34; \u0026gt; managementplane-secrets.yaml 如果无法使用上述命令并需要手动执行此操作，请按以下方式创建 ldap-credentials 密钥：\napiVersion: v1 kind: Secret metadata: name: ldap-credentials namespace: tsb data: binddn: \u0026#39;base64-encoded full DN of the user to use to authenticate\u0026#39; bindpassword: \u0026#39;base64-encoded password\u0026#39; 还需要创建 custom-host-ca 密钥，如果你的 LDAP 配置为使用自签名证书。\nkubectl create secret generic custom-host-ca \\ --from-file=ca-certificates.crt=\u0026lt;path to custom CA file\u0026gt; \\ --namespace tsb 用户和组同步 用户和组的同步是通过运行上述 LDAP 配置中的同步查询来完成的。以下示例显示了可用于从标准 LDAP 服务器获取用户和组的两个示例查询。\nmembershipattribute 用于将用户与他们所属的组进行匹配。对于每个找到的组，将读取此属性以提取组的成员信息。\n请注意，这些查询高度依赖于 LDAP 树结构，每个人都必须更改它们以进行匹配。\nsync: usersfilter: \u0026#39;(objectClass=person)\u0026#39; groupsfilter: \u0026#39;(objectClass=groupOfUniqueNames)\u0026#39; membershipattribute: uniqueMember ","relpermalink":"/book/tsb/operations/users/configuring-ldap/","summary":"本文描述了如何配置 Tetrate Service Bridge（TSB）的 LDAP 集成。在 TSB 中，LDAP 集成允许你将 LDAP 用作用户登录 TSB 的身份提供者，并自动将用户和组从 LDAP 同步到 TSB。 本文假定你已经具备配置 LDAP 服务以及如何使用它进行身份验证","title":"将 LDAP 配置为身份提供者"},{"content":"本文将教你如何将虚拟机上的部分\u0026#34;单体\u0026#34;工作负载迁移到集群，并在虚拟机和集群之间分流流量。\n在这个示例中，在虚拟机上运行的服务器将被视为\u0026#34;单体\u0026#34;应用程序。如果虚拟机正在调用其他虚拟机，可以按照相同的步骤进行操作。只需确保可以从你的集群解析并访问被调用的虚拟机。\n在开始之前：\n安装 TSB 管理平面 载入了一个集群 安装数据面 Operator 配置一个虚拟机以运行在 TSB 工作负载中（本指南假定使用 Ubuntu 20.04）。 第一步是安装 Docker。\nsudo apt-get update sudo apt-get -y install docker.io 然后，运行 httpbin 服务器并测试其是否正常工作。\nsudo docker run -d \\ --name httpbin \\ -p 127.0.0.1:80:80 \\ kennethreitz/httpbin curl localhost/headers { \u0026#34;headers\u0026#34;: { \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, \u0026#34;Host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;curl/7.68.0\u0026#34; } } 接下来，将虚拟机载入到你的集群中，按照VM 载入文档 的指示进行操作。 在你的集群中为你的虚拟机创建以下服务账户。\nkubectl create namespace httpbin kubectl label namespace httpbin istio-injection=enabled cat \u0026lt;\u0026lt;EOF | kubectl apply -f- apiVersion: v1 kind: ServiceAccount metadata: name: httpbin labels: account: httpbin namespace: httpbin EOF 调整你在虚拟机上载入的 WorkloadEntry，以适应你的工作负载。以下是 httpbin 的示例。\napiVersion: networking.istio.io/v1beta1 kind: WorkloadEntry metadata: name: httpbin-vm namespace: httpbin annotations: sidecar-bootstrap.istio.io/ssh-host: \u0026lt;ssh-host\u0026gt; sidecar-bootstrap.istio.io/ssh-user: istio-proxy sidecar-bootstrap.istio.io/proxy-config-dir: /etc/istio-proxy sidecar-bootstrap.istio.io/proxy-image-hub: docker.io/tetrate sidecar-bootstrap.istio.io/proxy-instance-ip: \u0026lt;proxy-instance-ip\u0026gt; spec: address: \u0026lt;vm-address\u0026gt; labels: class: vm app: httpbin version: v1 serviceAccount: httpbin network: \u0026lt;vm-network-name\u0026gt; 修改 Sidecar 资源，并确保已设置任何必要的防火墙规则，以允许流量流向你的虚拟机。\napiVersion: networking.istio.io/v1beta1 kind: Sidecar metadata: name: httpbin-no-iptables namespace: httpbin spec: egress: - bind: 127.0.0.2 hosts: - ./* ingress: - defaultEndpoint: 127.0.0.1:80 port: name: http number: 80 protocol: HTTP workloadSelector: labels: app: httpbin class: vm 在你的集群中，添加以下内容以配置从你的集群到虚拟机的流量流向。\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f- apiVersion: v1 kind: Service metadata: name: httpbin labels: app: httpbin namespace: httpbin spec: ports: - port: 80 targetPort: 80 name: http selector: app: httpbin EOF 使用 tctl 应用入口网关配置，配置一个网关来接受流量并将其转发到虚拟机。\n注意 在生产中使用时，应根据需要更新此配置以匹配你的设置。例如，如果你有一个工作空间或网关组，可以使用。 apiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: name: foo-ws tenant: \u0026lt;tenant\u0026gt; spec: namespaceSelector: names: - \u0026#34;*/httpbin\u0026#34; --- apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: name: httpbin workspace: foo-ws tenant: \u0026lt;tenant\u0026gt; spec: namespaceSelector: names: - \u0026#34;*/httpbin\u0026#34; configMode: BRIDGED --- apiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: name: httpbin group: httpbin workspace: foo-ws tenant: \u0026lt;tenant\u0026gt; spec: workloadSelector: # 根据你正在配置的网关进行调整 namespace: httpbin labels: app: httpbin-gateway http: - name: httpbin port: 8080 hostname: \u0026#34;httpbin.tetrate.io\u0026#34; routing: rules: - route: host: \u0026#34;httpbin/httpbin.httpbin.svc.cluster.local\u0026#34; port: 80 在这一点上，你应该能够将流量发送到你的网关，主机为httpbin.tetrate.io，并且它应该被转发到你的虚拟机。\n你可以通过手动设置主机标头并访问你的网关的 IP（例如curl -v -H \u0026#34;Host: httpbin.tetrate.io\u0026#34; 34.122.114.216/headers，其中34.122.114.216是你的网关的地址）来验证这一点。\n现在，你可以将指向你的虚拟机的任何内容（DNS 或 L4 LB 等）指向你的集群网关。流量将流向你的集群，然后流向你的虚拟机。\n现在，将在虚拟机上运行的 httpbin 工作负载添加到你的集群，并将流量的一部分发送到集群版本。首先，使用 tctl 应用以下配置（根据需要进行调整）。\napiVersion: traffic.tsb.tetrate.io/v2 kind: Group metadata: name: httpbin workspace: foo-ws tenant: \u0026lt;tenant\u0026gt; spec: namespaceSelector: names: - \u0026#34;\u0026lt;cluster\u0026gt;/httpbin\u0026#34; configMode: BRIDGED --- apiVersion: traffic.tsb.tetrate.io/v2 kind: ServiceRoute metadata: name: httpbin group: httpbin workspace: foo-ws tenant: \u0026lt;tenant\u0026gt; spec: service: httpbin/httpbin subsets: - name: v1 labels: version: v1 weight: 100 这将会将 100% 的流量发送到虚拟机，因为你在将应用程序部署到集群之前设置了这个。要开始流量分流，请在你的集群中运行以下命令。\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f- apiVersion: apps/v1 kind: Deployment metadata: name: httpbin labels: app: httpbin version: v2 namespace: httpbin spec: replicas: 1 selector: matchLabels: app: httpbin version: v2 template: metadata: labels: app: httpbin version: v2 spec: serviceAccountName: httpbin containers: - name: httpbin image: kennethreitz/httpbin imagePullPolicy: IfNotPresent ports: - containerPort: 8080 EOF 验证应用程序是否在你的集群中运行。\n现在编辑 TSB ServiceRoute 配置，包括在集群中部署的新 v2 版本，并使用tctl应用它（根据需要进行调整）。\napiVersion: traffic.tsb.tetrate.io/v2 kind: ServiceRoute metadata: name: httpbin group: httpbin workspace: foo-ws tenant: \u0026lt;tenant\u0026gt; spec: service: httpbin/httpbin subsets: - name: v1 labels: version: v1 weight: 80 - name: v2 labels: version: v2 weight: 20 现在，向你的应用程序发送一些请求通过你的网关。\n你可以通过日志或 TSB UI 来验证流量在你的虚拟机和集群应用程序之间分流。\n一旦你满意新版本的性能，你可以逐渐增加流量百分比，直到所有流量都被发送到你的集群，而不再流向虚拟机。\n","relpermalink":"/book/tsb/howto/traffic/migrating-vm-monoliths/","summary":"本文将教你如何将虚拟机上的部分\"单体\"工作负载迁移到集群，并在虚拟机和集群之间分流流量。 在这个示例中，在虚拟机上运行的服务器将被视为\"单体\"应用程序。如果虚拟机","title":"将虚拟机上的单体应用迁移到你的集群"},{"content":"本页面深入介绍了 TSB Operator 如何管理控制平面组件的生命周期，并概述了你可以通过 TSB Operator 配置和管理的自定义资源。\nTSB Operator 配置为监督控制平面组件的生命周期，主动监控部署的同一命名空间内的 ControlPlane 自定义资源 (CR)。默认情况下，控制平面位于 istio-system 命名空间中。有关自定义资源 API 的详细信息，你可以参考控制平面安装 API 参考文档 。\n组件 以下是你可以使用控制平面 Operator 配置和管理的各种类型的自定义组件：\n组件 Service Deployment istio Istio-operator-metrics (istiod, vmgateway) Istio-operator (istiod vmgateway) (istio-cni-node daemonset in kube-system namespace) oap oap oap-deployment collector otel-collector otel-collector xcpOperator xcp-operator-edge xcp-operator-edge xcpEdge xcp-edge edge 由 Operator 编排和安装的组件包括：\nistio：开源 Istio Operator，TSB Operator 利用它来管理开源 Istio 组件。 oap：负责收集和聚合网格和 Envoy 网关 RED 指标和跟踪数据。 收集器：开放遥测收集器，它从控制平面组件收集指标并通过 Prometheus 指标端点公开它们。 xcpOperator：控制平面 Operator，将控制平面组件的任务委托给 TSB Operator。 xcpEdge：负责将配置从 xcpCentral 转换为 Istio CRD，存储在本地，并将集群信息传输到 xcpCentral。 Istio 作为 TSB 组件 在控制平面的上下文中，TSB Operator 安装开源 Istio Operator。Istio 及其 Operator 被认为是 TSB 控制平面组件的组成部分，在 TSB Operator 的直接管理下运行。需要注意的是，Istio 不是由用户直接配置的。相反，与 Istio 的交互始终通过 TSB Operator 的 ControlPlane CR 进行。\n负责控制平面管理的 TSB Operator 在控制平面的命名空间内创建一个名为 tsb-istiocontrolplane 的 IstioOperator CR。该 CR 指导 Istio Operator 监督必要的 Istio（子）组件的部署。对于 TSB 控制平面，启用以下（子）组件： pilot 、 cni 和 ingressGateway 。\nTSB ingressGateway （子）组件体现了 Envoy 的自定义配置，部署为 vmgateway 。它的主要作用是将来自服务网格装载虚拟机的流量路由到部署在 Kubernetes 集群内的服务。当虚拟机和 Kubernetes Pod 之间的直接通信不可行时，这特别有用。\nSidecar 代理版本 尽管 Sidecar 代理在技术上是数据平面的一部分，但它们的版本与控制平面 Operator 版本相关。 ","relpermalink":"/book/tsb/concepts/operators/control-plane/","summary":"TSB Operator 和控制平面生命周期。","title":"控制平面"},{"content":" AWS EC2\nAWS ECS\n本地工作负载\n","relpermalink":"/book/tsb/setup/workload-onboarding/quickstart/","summary":"AWS EC2 AWS ECS 本地工作负载","title":"工作负载快速载入"},{"content":" Alpha 功能 流式服务日志是一个 Alpha 功能，不建议在生产环境中使用。 TSB 具有直接从 TSB UI 查看服务日志的功能。使用此功能，你将能够查看应用程序和 sidecar 的几乎实时日志，以进行故障排除。\n日志存储 TSB 不会将任何日志存储在存储系统中。日志直接从集群流式传输到管理平面。 管理平面 要在管理平面中启用服务日志流式传输，请在 ManagementPlane CR 或 Helm 值中的 oap 组件下添加 streamingLogEnabled: true ，然后应用。\nspec: hub: \u0026lt;registry_location\u0026gt; organization: \u0026lt;organization\u0026gt; ... components: ... oap: streamingLogEnabled: true 控制平面 对于每个注册的集群，请在 ControlPlane CR 或 Helm 值中的 oap 组件下添加 streamingLogEnabled: true ，然后应用。\nspec: hub: \u0026lt;registry_location\u0026gt; managementPlane: ... telemetryStore: elastic: ... components: ... oap: streamingLogEnabled: true 流式服务日志 UI 要在 TSB UI 中查看服务日志，请转到服务并选择受控服务。受控服务是网格的一部分，并且具有我们可以配置代理的服务。\n你将看到 Logs 选项卡，并且可以选择要查看其日志的容器，然后单击 Start 按钮开始流式传输日志。\n下图显示了具有 sidecar 的服务的服务日志。你可以选择最多两个容器，因此将能够同时查看服务和 sidecar 日志。\n下图显示了 TSB 网关的服务日志。\n","relpermalink":"/book/tsb/operations/features/streaming-log/","summary":"启用服务流式日志以显示工作负载日志。","title":"流式服务日志"},{"content":"在继续之前，请确保你了解 TSB 中的 4 种证书类型 ，特别是内部证书。\n注意 请注意，此处描述的证书仅用于 TSB 组件之间的通信，因此不属于通常由 Istio 或应用程序 TLS 证书管理的工作负载证书。 提醒 如果你在管理平面集群中安装了 cert-manager，你可以使用 tctl 自动在管理平面中安装所需的发行者和证书，并创建控制平面证书。有关更多详细信息，请参阅 管理平面安装 和 载入集群 文档。 要使用常规（非相互）TLS 进行 JWT 身份验证，XCP central 证书必须在其主体备用名称（SANs）中包含其地址。这将是 DNS 名称或 IP 地址。\n与上述 mTLS 类似，管理平面中的 XCP central 使用存储在名为 xcp-central-cert 的管理平面命名空间（默认为 tsb）中的密钥中的证书。密钥必须包含标准的 tls.crt、tls.key 和 ca.crt 字段的数据。\n以下是如果你使用 IP 地址作为 XCP central 证书的 cert-manager 资源示例。\napiVersion: cert-manager.io/v1 kind: Certificate metadata: name: xcp-central-cert namespace: tsb spec: secretName: xcp-central-cert ipAddresses: - a.b.c.d ## \u0026lt;--- 在此处输入 IP 地址 issuerRef: name: xcp-identity-issuer kind: Issuer duration: 30000h 或者，如果你使用域名，编辑字段 spec.dnsNames。\napiVersion: cert-manager.io/v1 kind: Certificate metadata: name: xcp-central-cert namespace: tsb spec: secretName: xcp-central-cert dnsNames: - example-tsb.tetrate.io ## \u0026lt;-- 在此处输入 DNS 名称 issuerRef: name: xcp-identity-issuer kind: Issuer duration: 30000h 使用 tctl 创建证书时的 DNS 名称 如果你使用 tctl 自动安装所需的发行者和证书，XCP central 证书的 DNS 名称将为 central.xcp.tetrate.io。 ","relpermalink":"/book/tsb/setup/certificate/certificate-requirements/","summary":"用于 TSB 内部通信的证书要求。","title":"内部证书要求"},{"content":"本文档解释了如何配置 Flux CD 与 Helm 和 GitHub 集成，以将 TSB 应用程序部署到目标集群。\n注意 本文档假设以下情况：\n已安装 Flux 版本 2 的 CLI。 已安装 Helm 的 CLI。 TSB 正在运行，并且已为目标集群启用了 GitOps 配置 。 集群设置 首先，在目标集群上使用 GitHub 集成 安装 Flux。要执行此操作，请在目标集群 Kubernetes 上下文下使用以下命令。\n注意 你将需要一个 GitHub 个人访问令牌 （PAT）输入以下命令。 $ flux bootstrap github \\ --owner=your-org \\ --repository=git-ops \\ --path=./clusters/cluster-01 注意 如果你使用个人 GitHub 帐户进行测试，可以添加 --personal --private 标志。 这将为名为 cluster-01 的集群在名为 git-ops 的 GitHub 存储库的 clusters/cluster-01/ 目录下设置 Flux 所需的配置。\n注意 为了调试目的，在不同的 shell 中运行 flux logs -A --follow 命令。 你可以运行此命令进行一般状态检查：\n$ flux check ► checking prerequisites ✔ Kubernetes 1.20.15-gke.2500 \u0026gt;=1.20.6-0 ► checking controllers ✔ helm-controller: deployment ready ► ghcr.io/fluxcd/helm-controller:v0.20.1 ✔ kustomize-controller: deployment ready ► ghcr.io/fluxcd/kustomize-controller:v0.24.2 ✔ notification-controller: deployment ready ► ghcr.io/fluxcd/notification-controller:v0.23.4 ✔ source-controller: deployment ready ► ghcr.io/fluxcd/source-controller:v0.24.0 ✔ all checks passed 在内部，Flux 使用 Kustomization 与 GitRepository 源来存储自己的资源。\n你可以查询其状态：\n$ flux get all 名称 修订版本 暂停 已准备好 消息 gitrepository/flux-system\tmain/36dff73\tFalse True 已存储的修订版本 \u0026#39;main/36dff739b5ae411a7b4a64010d42937bd3ae4d25\u0026#39; 名称 修订版本 暂停 已准备好 消息 kustomization/flux-system\tmain/36dff73\tFalse True 应用的修订版本：main/36dff73 同时，在日志中你将看到类似以下的信息：\n2022-04-24T20:42:06.921Z info Kustomization/flux-system.flux-system - 服务器端应用完成 2022-04-24T22:51:30.431Z info GitRepository/flux-system.flux-system - artifact up-to-date with remote revision: \u0026#39;main/36dff739b5ae411a7b4a64010d42937bd3ae4d25\u0026#39; 由于 Flux 现在正在运行，下一步是将新配置推送到 git-ops 存储库，以用于 cluster-01 集群。你可以克隆存储库并 cd 到 clusters/cluster-01 以执行下一步。\n应用程序设置 有几种方法 可以组织你的 GitOps 存储库。在此示例中，出于简单起见，同一个存储库用于集群和应用程序配置。\n本节的目标是部署Bookinfo 应用程序的 Helm 图表及其 TSB 资源。\n首先，创建带有 Sidecar 注入的 bookinfo 命名空间：\nkubectl create namespace bookinfo kubectl label namespace bookinfo istio-injection=enabled 然后，在 clusters/cluster-01/bookinfo.yaml 中创建 HelmRelease Flux 资源，该资源使用 GitRepository 来定义 Bookinfo。\n注意 GitRepository 的替代方法是 HelmRepository ，本文档未涵盖此项内容。 如果 bookinfo TSB helm 图表定义存储在 apps/bookinfo 目录中，则在 clusters/cluster-01/bookinfo.yaml 中创建 HelmRelease 资源。\napiVersion: helm.toolkit.fluxcd.io/v2beta1 kind: HelmRelease metadata: name: bookinfo namespace: flux-system spec: chart: spec: chart: ./apps/bookinfo sourceRef: kind: GitRepository name: flux-system interval: 1m0s install: createNamespace: true targetNamespace: bookinfo 注意：\nHelmRelease 将在 flux-system 命名空间中创建，而 release 的 Helm 图表定义的资源将在 bookinfo 目标命名空间中部署。 由于未指定 spec.chart.spec.version，Flux 将使用图表的 latest 版本。 GitRepository.name 为 flux-system，因为 Flux 在内部使用此名称进行 引导。\n接下来，将文件添加并推送到 git，并监视 flux 日志。你将看到类似以下的信息：\n2022-04-25T08:02:37.233Z info HelmRelease/bookinfo.flux-system - 协调完成，耗时 49.382555ms，下一次运行在 1m0s 内 2022-04-25T08:02:37.980Z info HelmChart/flux-system-bookinfo.flux-system - 丢弃事件，未找到涉及对象的警报 2022-04-25T08:02:45.784Z error HelmChart/flux-system-bookinfo.flux-system - 协调停滞，无效的图表引用：stat /tmp/helmchart-flux-system-flux-system-bookinfo-4167124062/source/apps/bookinfo: 文件或目录不存在 这是因为 helm 图表尚未推送到 apps/bookinfo 目录中。\n请注意，你可以使用 kubectl 查询资源来代替解析 flux 日志：\nkubectl get helmreleases -A kubectl get helmcharts -A 接下来，创建 helm 图表。创建 apps/ 目录，进入该目录并运行：\n$ helm create bookinfo 这将创建以下文件树：\n$ tree . +-- bookinfo +-- Chart.yaml +-- charts +-- templates | +-- NOTES.txt | +-- _helpers.tpl | +-- deployment.yaml | +-- hpa.yaml | +-- ingress.yaml | +-- service.yaml | +-- serviceaccount.yaml | \\-- tests | \\-- test-connection.yaml \\-- values.yaml 4 directories, 10 files 然后，进入 bookinfo/。\n为了简化起见，删除不需要的内容：\n$ rm -rf values.yaml charts templates/NOTES.txt templates/*.yaml templates/tests/ 接下来，编辑 Chart.yaml。最小内容如下：\napiVersion: v2 name: bookinfo description: TSB bookinfo Helm Chart. type: application version: 0.1.0 appVersion: \u0026#34;0.1.0\u0026#34; 接下来，在 templates/ 目录中添加 Bookinfo 定义，从 Istio 的存储库中获取它们：\ncurl https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/platform/kube/bookinfo.yaml -o bookinfo.yaml 一旦我们有了 bookinfo 部署，我们将在 templates/tsb.yaml 文件中添加 TSB 配置资源。创建 TSB 配置时，最佳实践是将它们全部放在 List 资源内。这将强制在将它们应用到集群时遵循严格的顺序，你将能够保证高层级 TSB 资源首先应用，不会因 Helm 资源排序限制 而遇到问题。\n注意 在此示例中，应用程序使用入口网关，该网关将由下面的第一个资源配置进行配置。你可以在 此处 详细了解。 此外，请确保将 your-org 和 your-tenant 更改为实际值。 apiVersion: v1 kind: List items: # 创建一个作为 bookinfo 应用程序入口点的入口网关部署 - apiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: namespace: bookinfo name: tsb-gateway-bookinfo spec: {} # 创建工作空间和网关组，捕获 bookinfo 应用程序将运行的命名空间 - apiVersion: tsb.tetrate.io/v2 kind: Workspace metadata: name: bookinfo annotations: tsb.tetrate.io/organization: your-org tsb.tetrate.io/tenant: your-tenant spec: namespaceSelector: names: - \u0026#34;*/bookinfo\u0026#34; - apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: name: bookinfo-gg annotations: tsb.tetrate.io/organization: your-org tsb.tetrate.io/tenant: your-tenant tsb.tetrate.io/workspace: bookinfo spec: namespaceSelector: names: - \u0026#34;*/*\u0026#34; configMode: BRIDGED # 在应用程序入口中公开 productpage 服务 - apiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: …","relpermalink":"/book/tsb/howto/gitops/flux/","summary":"如何配置 Flux CD 与 Helm 和 GitHub 集成，以将 TSB 应用程序部署到目标集群。","title":"配置 Flux CD 进行 GitOps"},{"content":"安装 Bookinfo Ratings 应用程序 SSH 进入本地虚拟机并安装 ratings 应用程序。执行以下命令：\n# 安装最新版本的可信 CA 证书 sudo apt-get update -y sudo apt-get install -y ca-certificates # 添加带有 Node.js 的 DEB 仓库 curl --fail --silent --location https://deb.nodesource.com/setup_14.x | sudo bash - # 安装 Node.js sudo apt-get install -y nodejs # 下载 Bookinfo Ratings 应用程序的 DEB 包 curl -fLO https://dl.cloudsmith.io/public/tetrate/onboarding-examples/raw/files/bookinfo-ratings.deb # 安装 DEB 包 sudo apt-get install -y ./bookinfo-ratings.deb # 删除下载的文件 rm bookinfo-ratings.deb # 启用 SystemD 单元 sudo systemctl enable bookinfo-ratings # 启动 Bookinfo Ratings 应用程序 sudo systemctl start bookinfo-ratings 验证 ratings 应用程序 执行以下命令以验证 ratings 应用程序是否能够提供本地请求：\ncurl -fsS http://localhost:9080/ratings/1 你应该会得到类似于以下内容的输出：\n{\u0026#34;id\u0026#34;:1,\u0026#34;ratings\u0026#34;:{\u0026#34;Reviewer1\u0026#34;:5,\u0026#34;Reviewer2\u0026#34;:4}} 配置信任示例 CA 请记住，你之前已使用由自定义 CA 签名的 TLS 证书配置了 Workload Onboarding 终端点。因此，运行在本地虚拟机上并尝试连接到 Workload Onboarding 终端点的任何软件默认不会信任其证书。\n在继续之前，你必须配置本地虚拟机以信任你的自定义 CA。\n首先，更新 apt 软件包列表：\nsudo apt-get update -y 然后安装 ca-certificates 软件包：\nsudo apt-get install -y ca-certificates 将你在设置证书时创建的文件 example-ca.crt.pem 的内容复制并放置在本地虚拟机的位置 /usr/local/share/ca-certificates/example-ca.crt。\n使用你喜欢的工具来执行此操作。如果你没有安装任何编辑器或工具，你可以使用以下 cat 和 dd 的组合：\n执行 cat \u0026lt;\u0026lt;EOF | sudo dd of=/usr/local/share/ca-certificates/example-ca.crt 复制 example-ca.crt.pem 的内容并粘贴到你执行上一步的终端中 输入 EOF 并按 Enter 键完成第一个命令 在将自定义 CA 放置在正确位置后，执行以下命令：\nsudo update-ca-certificates 这将重新加载受信任的 CA 列表，并包括你的自定义 CA。\n安装 Istio Sidecar 通过执行以下命令安装 Istio sidecar。将 ONBOARDING_ENDPOINT_ADDRESS 替换为之前获取的值 。\n# 下载 DEB 包 curl -fLO \\ --connect-to \u0026#34;onboarding-endpoint.example:443:${ONBOARDING_ENDPOINT_ADDRESS}:443\u0026#34; \\ \u0026#34;https://onboarding-endpoint.example/install/deb/amd64/istio-sidecar.deb\u0026#34; # 下载校验和 curl -fLO \\ --connect-to \u0026#34;onboarding-endpoint.example:443:${ONBOARDING_ENDPOINT_ADDRESS}:443\u0026#34; \\ \u0026#34;https://onboarding-endpoint.example/install/deb/amd64/istio-sidecar.deb.sha256\u0026#34; # 验证校验和 sha256sum --check istio-sidecar.deb.sha256 # 安装 DEB 包 sudo apt-get install -y ./istio-sidecar.deb # 删除下载的文件 rm istio-sidecar.deb istio-sidecar.deb.sha256 安装 Workload Onboarding Agent 通过执行以下命令安装 Workload Onboarding Agent。将 ONBOARDING_ENDPOINT_ADDRESS 替换为之前获取的值 。\n# 下载 DEB 包 curl -fLO \\ --connect-to \u0026#34;onboarding-endpoint.example:443:${ONBOARDING_ENDPOINT_ADDRESS}:443\u0026#34; \\ \u0026#34;https://onboarding-endpoint.example/install/deb/amd64/onboarding-agent.deb\u0026#34; # 下载校验和 curl -fLO \\ --connect-to \u0026#34;onboarding-endpoint.example:443:${ONBOARDING_ENDPOINT_ADDRESS}:443\u0026#34; \\ \u0026#34;https://onboarding-endpoint.example/install/deb/amd64/onboarding-agent.deb.sha256\u0026#34; # 验证校验和 sha256sum --check onboarding-agent.deb.sha256 # 安装 DEB 包 sudo apt-get install -y ./onboarding-agent.deb # 删除下载的文件 rm onboarding-agent.deb onboarding-agent.deb.sha256 安装示例 JWT 凭据插件 为了本指南的目的，你将使用 Sample JWT Credential Plugin 为本地虚拟机上的工作负载提供 [JWT 令牌] 凭据。\n执行以下命令以安装 Sample JWT Credential Plugin：\ncurl -fL \u0026#34;https://dl.cloudsmith.io/public/tetrate/onboarding-examples/raw/files/onboarding-agent-sample-jwt-credential-plugin_0.0.1_$(uname -s)_$(uname -m).tar.gz\u0026#34; \\ | tar -xz onboarding-agent-sample-jwt-credential-plugin sudo mv onboarding-agent-sample-jwt-credential-plugin /usr/local/bin/ 复制你之前创建的文件 sample-jwt-issuer.jwk 的内容，并将其放置在本地虚拟机上的位置 /var/run/secrets/onboarding-agent-sample-jwt-credential-plugin/jwt-issuer.jwk。\n使用你喜欢的工具来执行此操作。如果你没有安装任何编辑器或工具，你可以使用以下 cat 和 dd 的组合：\n执行 sudo mkdir -p /var/run/secrets/onboarding-agent-sample-jwt-credential-plugin/ cat \u0026lt;\u0026lt;EOF | sudo dd of=/ var/run/secrets/onboarding-agent-sample-jwt-credential-plugin/jwt-issuer.jwk 复制 sample-jwt-issuer.jwk 的内容并粘贴到你执行上一步的终端中 输入 EOF 并按 Enter 键完成第一个命令 执行 sudo chmod 400 /var/run/secrets/onboarding-agent-sample-jwt-credential-plugin/jwt-issuer.jwk sudo chown onboarding-agent: -R /var/run/secrets/onboarding-agent-sample-jwt-credential-plugin/ 配置 Workload Onboarding Agent 执行以下命令将 Agent Configuration 保存到文件 /etc/onboarding-agent/agent.config.yaml 中：\ncat \u0026lt;\u0026lt; EOF | sudo tee /etc/onboarding-agent/agent.config.yaml apiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: AgentConfiguration host: custom: credential: - plugin: name: sample-jwt-credential path: /usr/local/bin/onboarding-agent-sample-jwt-credential-plugin env: - name: SAMPLE_JWT_ISSUER value: \u0026#34;https://sample-jwt-issuer.example\u0026#34; - name: SAMPLE_JWT_ISSUER_KEY value: \u0026#34;/var/run/secrets/onboarding-agent-sample-jwt-credential-plugin/jwt-issuer.jwk\u0026#34; - name: SAMPLE_JWT_SUBJECT value: \u0026#34;vm007-datacenter1-us-east.internal.corp\u0026#34; - name: SAMPLE_JWT_ATTRIBUTES_FIELD value: \u0026#34;custom_attributes\u0026#34; - name: SAMPLE_JWT_ATTRIBUTES value: \u0026#34;instance_name=vm007-datacenter1-us-east,instance_role=app-ratings,region=us-east\u0026#34; EOF 通过支持的 Sample JWT Credential Plugin 环境变量，你已经配置了所需内容的 JWT 令牌。\n","relpermalink":"/book/tsb/setup/workload-onboarding/quickstart/on-premise/configure-vm/","summary":"安装 Bookinfo Ratings 应用程序 SSH 进入本地虚拟机并安装 ratings 应用程序。执行以下命令： # 安装最新版本的可信 CA 证书 sudo apt-get update -y sudo apt-get install -y ca-certificates # 添加带有 Node.js 的 DEB 仓库 curl --fail --silent --location https://deb.nodesource.com/setup_14.x | sudo bash - # 安装 Node.js sudo apt-get install -y nodejs # 下载 Bookinfo Ratings 应用程序的 DEB 包 curl -fLO https://dl.cloudsmith.io/public/tetrate/onboarding-examples/raw/files/bookinfo-ratings.deb # 安装","title":"配置本地虚拟机"},{"content":" 统一网关\n使用 IngressGateway 和 ServiceRoute 基于子集的流量路由\n共享入口网关\n多集群访问控制和身份传播\n使用东西网关实现多集群流量故障转移\n使用 Tier-2 网关进行多集群流量路由\n使用 Tier-1 网关进行多集群流量切换\n配置 Authz 以支持代理协议\n使用 Keycloak 进行终端用户身份验证\n控制对外部服务的访问\n分布式入口网关\n配置和路由 TSB 中的 HTTP、非 HTTP（多协议）和多端口服务流量\n使用 OpenAPI 注解配置应用程序网关\n应用程序入口\n","relpermalink":"/book/tsb/howto/gateway/","summary":"统一网关 使用 IngressGateway 和 ServiceRoute 基于子集的流量路由 共享入口网关 多集群访问控制和身份传播 使用东西网关实现多集群流量故障转移 使用 Tier-2 网关进行多集群流量路由 使用 Tier-1 网关进行多集群流量切换 配置 Authz 以支持代理协议 使用 Keycloak 进行终端用户","title":"配置网关"},{"content":"为了启用工作负载载入，你需要以下信息：\n用于分配工作负载载入端点的 DNS 名称 该 DNS 名称的 TLS 证书 在本示例中，你将使用 DNS 名称 onboarding-endpoint.example，因为我们不希望你使用可路由的 DNS 名称。\n准备证书 出于生产目的，你需要使用由可信任的证书颁发机构（CA）签名的 TLS 证书，例如 Let’s Encrypt 或内部 CA（如 Vault ）。\n在本示例中，你将设置一个示例 CA，它将在本指南的其余部分中使用。\n通过执行以下命令创建一个自签名证书（example-ca.crt.pem）和 CA 私钥（example-ca.key.pem）：\nopenssl req \\ -x509 \\ -subj \u0026#39;/CN=Example CA\u0026#39; \\ -days 3650 \\ -sha256 \\ -newkey rsa:2048 \\ -nodes \\ -keyout example-ca.key.pem \\ -out example-ca.crt.pem \\ -config \u0026lt;(cat \u0026lt;\u0026lt;EOF # \u0026#34;openssl req\u0026#34; 命令的配置部分 [ req ] distinguished_name = req # 包含要提示输入的显著名称字段的部分的名称 x509_extensions = v3_ca # 包含要添加到自签名证书的扩展列表的部分的名称 # 包含要添加到自签名证书的扩展列表的部分的名称 [ v3_ca ] basicConstraints = CA:TRUE # 为了与破损的软件兼容性而不标记为关键 subjectKeyIdentifier = hash # PKIX 建议 authorityKeyIdentifier = keyid:always,issuer # PKIX 建议 EOF ) 然后，通过执行以下命令创建证书签名请求（onboarding-endpoint.example.csr.pem）和 工作负载载入端点的私钥（onboarding-endpoint.example.key.pem）：\nopenssl req \\ -subj \u0026#39;/CN=onboarding-endpoint.example\u0026#39; \\ -sha256 \\ -newkey rsa:2048 \\ -nodes \\ -keyout onboarding-endpoint.example.key.pem \\ -out onboarding-endpoint.example.csr.pem 最后，通过执行以下命令创建 DNS 名称 onboarding-endpoint.example 的证书 （onboarding-endpoint.example.crt.pem），该证书由你在前面步骤中创建的 CA 签名：\nopenssl x509 \\ -req \\ -days 3650 \\ -sha256 \\ -in onboarding-endpoint.example.csr.pem \\ -out onboarding-endpoint.example.crt.pem \\ -CA example-ca.crt.pem \\ -CAkey example-ca.key.pem \\ -CAcreateserial \\ -extfile \u0026lt;(cat \u0026lt;\u0026lt;EOF # 包含要添加到证书的扩展列表的名称部分 extensions = usr_cert # 包含要添加到证书的扩展列表的名称部分 [ usr_cert ] basicConstraints = CA:FALSE # 为了与破损的软件兼容性而不标记为关键 subjectKeyIdentifier = hash # PKIX 建议 authorityKeyIdentifier = keyid:always,issuer # PKIX 建议 keyUsage = digitalSignature, keyEncipherment extendedKeyUsage = serverAuth subjectAltName = DNS:onboarding-endpoint.example EOF ) 然后，通过执行以下命令将证书部署到 Kubernetes 集群：\nkubectl create secret tls onboarding-endpoint-tls-cert \\ -n istio-system \\ --cert=onboarding-endpoint.example.crt.pem \\ --key=onboarding-endpoint.example.key.pem 启用工作负载载入 一旦 TLS 证书准备好，你可以通过执行以下命令启用工作负载载入：\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: install.tetrate.io/v1alpha1 kind: ControlPlane metadata: name: controlplane namespace: istio-system spec: meshExpansion: onboarding: endpoint: hosts: - onboarding-endpoint.example secretName: onboarding-endpoint-tls-cert localRepository: {} EOF 上述命令指定了应使用 DNS 名称 onboarding-endpoint.example 设置工作负载载入端点，使用在 secret onboarding-endpoint-tls-cert 中可用的证书。\n它还指定应部署一个本地存储库，其中包含用于 Workload Onboarding 代理和 Istio Sidecar 的 DEB/RPM 包。\n执行上述命令后，请等待直到各个 Workload Onboarding 组件可用：\nkubectl wait --for=condition=Available -n istio-system \\ deployment/vmgateway \\ deployment/onboarding-plane \\ deployment/onboarding-repository 验证工作负载载入端点 由于你未使用可路由的 DNS 名称，因此需要找出已公开的工作负载载入端点的地址。\n执行以下命令以获取地址（DNS 名称或 IP 地址）：\nONBOARDING_ENDPOINT_ADDRESS=$(kubectl get svc vmgateway \\ -n istio-system \\ -ojsonpath=\u0026#34;{.status.loadBalancer.ingress[0][\u0026#39;hostname\u0026#39;, \u0026#39;ip\u0026#39;]}\u0026#34;) 在本指南的其余部分中，你将使用存储在 ONBOARDING_ENDPOINT_ADDRESS 环境变量中的地址。\n最后，执行以下命令以验证端点是否可用于外部流量。\ncurl -f -i \\ --cacert example-ca.crt.pem \\ --connect-to \u0026#34;onboarding-endpoint.example:443:${ONBOARDING_ENDPOINT_ADDRESS}:443\u0026#34; \\ \u0026#34;https://onboarding-endpoint.example/install/\u0026#34; 你应该会看到类似以下内容的输出：\nHTTP/2 200 content-type: text/html; charset=utf-8 server: istio-envoy \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026#34;deb/\u0026#34;\u0026gt;deb/\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;rpm/\u0026#34;\u0026gt;rpm/\u0026lt;/a\u0026gt; \u0026lt;/pre\u0026gt; ","relpermalink":"/book/tsb/setup/workload-onboarding/quickstart/aws-ec2/enable-workload-onboarding/","summary":"为了启用工作负载载入，你需要以下信息： 用于分配工作负载载入端点的 DNS 名称 该 DNS 名称的 TLS 证书 在本示例中，你将使用 DNS 名称 onboarding-endpoint.example，因为我们不希望你使用可路由的 DNS 名","title":"启用工作负载载入"},{"content":"步骤 启用工作负载载入 创建 WorkloadGroup 允许工作负载加入 WorkloadGroup 创建 Sidecar 配置 在 VM 上安装工作负载载入代理 启用工作负载载入 要在给定的 Kubernetes 集群中启用工作负载载入，你需要编辑 TSB ControlPlane 资源或 Helm 配置，如下所示：\nspec: ... meshExpansion: onboarding: # (1) REQUIRED endpoint: hosts: - \u0026lt;onboarding-endpoint-dns-name\u0026gt; # (2) REQUIRED secretName: \u0026lt;onboarding-endpoint-tls-cert\u0026gt; # (3) REQUIRED tokenIssuer: jwt: expiration: \u0026lt;onboarding-token-expiration-time\u0026gt; # (4) OPTIONAL localRepository: {} # (5) OPTIONAL 然后：\n要在给定的 Kubernetes 集群中启用工作负载载入，你需要编辑 spec.meshExpansion.onboarding 部分，并为所有强制性字段提供值 你必须为 Workload Onboarding Endpoint 提供一个 DNS 名称，例如 onboarding-endpoint.your-company.corp 你必须提供保存 Workload Onboarding Endpoint 的 TLS 证书的 Kubernetes Secret 的名称 你可以选择自定义载入令牌的过期时间，默认为 1 小时 你可以选择部署 Workload Onboarding Agent 和 Istio sidecar 的 DEB/RPM 软件包的本地副本 Workload Onboarding Endpoint Workload Onboarding Endpoint 是各个 Workload Onboarding Agent 连接加入网格的组件。\n在生产场景中，Workload Onboarding Endpoint 必须具有高可用性、稳定的地址，并对传入连接进行 TLS 强制执行。\n因此，DNS 名称和 TLS 证书是启用 Workload Onboarding 的强制性参数。\nDNS 名称 你可以为 Workload Onboarding Endpoint 选择任何 DNS 名称。\n该名称必须与 istio-system 命名空间中的 Kubernetes 服务 vmgateway 的地址关联。\n在生产场景中，你可以使用 external-dns 来实现这一点。\nTLS 证书 为 Workload Onboarding Endpoint 提供证书，你需要在 istio-system 命名空间中创建一个 TLS 类型的 Kubernetes Secret。\n你有几个选项：\n从 X509 证书和私钥创建 Kubernetes Secret（手动获取） 或者你可以使用 cert-manager 自动提供 TLS 证书 从外部获取的 TLS 证书 为 Workload Onboarding Endpoint 提供从外部获取的 TLS 证书，请使用以下命令：\nkubectl create secret tls \u0026lt;onboarding-endpoint-tls-cert\u0026gt; \\ -n istio-system \\ --cert=\u0026lt;path/to/cert/file\u0026gt; \\ --key=\u0026lt;path/to/key/file\u0026gt; 由 cert-manager 提供的 TLS 证书 要自动提供 TLS 证书，你可以使用 cert-manager 。\n例如，你可以获取由受信任的 CA 签名的免费 TLS 证书，如 Let’s Encrypt 。\n在这种情况下，你的配置将如下所示：\napiVersion: cert-manager.io/v1 kind: Certificate metadata: name: \u0026lt;onboarding-endpoint-tls-cert\u0026gt; namespace: istio-system spec: secretName: \u0026lt;onboarding-endpoint-tls-cert\u0026gt; duration: 2160h # 90d renewBefore: 360h # 15d usages: - server auth dnsNames: - \u0026lt;onboarding-endpoint-dns-name\u0026gt; issuerRef: name: \u0026lt;your-issuer\u0026gt; kind: ClusterIssuer 有关更多详细信息，请参阅 cert-manager 文档。\nWorkload Onboarding 令牌 Workload Onboarding 令牌代表将工作负载引入服务网格的临时授权。\n在验证由 Workload Onboarding Agent 提交的平\n台特定凭据后（例如工作负载所在的 VM 的凭据），Workload Onboarding Endpoint 会发放 Workload Onboarding 令牌。\nWorkload Onboarding 令牌用作 Workload Onboarding Agent 向 Workload Onboarding Endpoint 发送的后续请求的会话令牌，以提高身份验证和授权的效率。\n默认情况下，Workload Onboarding 令牌的有效期为 1 小时。\n用户可能会选择为 Workload Onboarding 令牌选择自定义过期时间，出于多种原因，例如：\n缩短过期时间以满足组织内部制定的更严格的安全策略 延长过期时间以降低频繁令牌续订导致的负载 本地仓库 为了方便起见，可以在其网络内托管 Workload Onboarding Agent 和 Istio sidecar 的 DEB/RPM 软件包的本地副本。\n一旦启用本地仓库，用户将能够从 https://\u0026lt;onboarding-endpoint-dns-name\u0026gt; 的 HTTP 服务器下载 DEB/RPM 软件包。\n本地仓库允许下载以下工件：\nURI 描述 /install/deb/amd64/onboarding-agent.deb Workload Onboarding Agent 的 DEB 软件包 /install/deb/amd64/onboarding-agent.deb.sha256 DEB 软件包的 SHA-256 校验和 /install/deb/amd64/istio-sidecar.deb Istio sidecar 的 DEB 软件包 /install/deb/amd64/istio-sidecar.deb.sha256 DEB 软件包的 SHA-256 校验和 /install/rpm/amd64/onboarding-agent.rpm Workload Onboarding Agent 的 RPM 软件包 /install/rpm/amd64/onboarding-agent.rpm.sha256 RPM 软件包的 SHA-256 校验和 /install/rpm/amd64/istio-sidecar.rpm Istio sidecar 的 RPM 软件包 /install/rpm/amd64/istio-sidecar.rpm.sha256 RPM 软件包的 SHA-256 校验和 创建 WorkloadGroup 当将运行在 Kubernetes 集群之外的工作负载引入到网格时，它被视为加入某个 WorkloadGroup 。\nIstio WorkloadGroup 资源保存所有加入它的工作负载共享的配置。\n从某种意义上说，Istio WorkloadGroup 资源对于单个工作负载就像 Kubernetes Deployment 资源对于单个 Pod 一样。\n为了能够将单个工作负载引入给定的 Kubernetes 集群，你必须首先在其中创建相应的 Istio WorkloadGroup。\n例如，\napiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: # (1) labels: app: ratings class: vm serviceAccount: ratings-vm # (2) network: virtual-machines # (3) 其中\n所有加入该组的工作负载都继承 spec.template 中指定的配置 在网格内，加入网格的工作负载将具有 spec.template.serviceAccount 中指定的 Kubernetes 服务帐户的标识。如果未设置 spec.template.serviceAccount，则假定为 \u0026#34;default\u0026#34;（此帐户在每个 Kubernetes 命名空间中都保证存在）。 如果该组中的工作负载与 Kubernetes Pods 没有直接的连接性，必须将 spec.template.network 字段设置为非空值。 允许工作负载加入 WorkloadGroup 在 Kubernetes 集群之外运行的工作负载除非经过明确授权，否则不能加入到 mesh 中。\n为了进行载入，工作负载被视为运行在其上的主机的身份。例如，如果一个工作负载运行在 AWS EC2 实例上，它被认为具有该 AWS EC2 实例的身份。\n为了允许工作负载加入特定的集群，用户必须在该集群中创建一个载入策略 。\n一个 OnboardingPolicy 是 Kubernetes 资源，授权具有特定身份的工作负载加入特定的 WorkloadGroup(s)。OnboardingPolicy 必须在适用于它的 WorkloadGroup(s) 相同的命名空间中创建。\n示例 下面的示例允许运行在任何与给定 AWS 帐户关联的 AWS EC2 实例上的工作负载加入给定 Kubernetes 命名空间中的任何可用 WorkloadGroup：\napiVersion: authorization.onboarding.tetrate.io/v1alpha1 kind: OnboardingPolicy metadata: name: 允许任何 AWS EC2 实例加入给定帐户 namespace: bookinfo spec: allow: - workloads: - aws: accounts: - \u0026#39;123456789012\u0026#39; - \u0026#39;234567890123\u0026#39; ec2: {} # 上述帐户中的任何 AWS EC2 实例 onboardTo: - workloadGroupSelector: {} # 该命名空间中的任何 WorkloadGroup 出于安全原因，AWS 帐户必须始终明确列出。由于这从不是一个良好的实践，你将无法指定与任何帐户关联的工作负载自由加入 mesh。\n尽管前面的示例可能是一个相当“宽松”的策略，但更严格的载入策略可能只允许从特定 AWS 区域和/或区域中的 AWS EC2 实例加入，带有特定 AWS IAM 角色等。它还可能只允许工作负载加入特定的 WorkloadGroups 子集。\n以下是一个更严格策略的示例：\napiVersion: authorization.onboarding.tetrate.io/v1alpha1 kind: OnboardingPolicy metadata: name: 允许 AWS EC2 实例的狭窄子集加入 namespace: bookinfo spec: …","relpermalink":"/book/tsb/setup/workload-onboarding/guides/setup/","summary":"如何设置工作负载载入。","title":"设置工作负载载入"},{"content":"在本操作指南中，你将了解如何通过基于 URI 端点、标头和端口匹配流量并将其路由到目标服务的主机:端口来设置基于子集的流量路由。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念 。 安装 TSB 演示环境 。 创建一个租户 。 创建工作区和配置组 首先，使用以下 YAML 配置创建工作区和配置组：\nhelloworld-ws-groups.yaml apiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tetrate tenant: tetrate name: helloworld-ws spec: namespaceSelector: names: - \u0026#39;*/helloworld\u0026#39; --- apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: organization: tetrate tenant: tetrate workspace: helloworld-ws name: helloworld-gw spec: namespaceSelector: names: - \u0026#39;*/helloworld\u0026#39; configMode: BRIDGED --- apiVersion: traffic.tsb.tetrate.io/v2 kind: Group metadata: organization: tetrate tenant: tetrate workspace: helloworld-ws name: helloworld-trf spec: namespaceSelector: names: - \u0026#39;*/helloworld\u0026#39; configMode: BRIDGED 将其保存为 helloworld-ws-groups.yaml ，并将其与 tctl 一起应用：\ntctl apply -f helloworld-ws-groups.yaml 部署你的应用程序 首先创建命名空间并启用 Istio sidecar 注入：\nkubectl create namespace helloworld kubectl label namespace helloworld istio-injection=enabled 接下来，使用以下 YAML 配置部署你的应用程序：\nhelloworld-2-subsets.yaml apiVersion: v1 kind: Service metadata: name: helloworld labels: app: helloworld service: helloworld spec: ports: - port: 5000 name: http selector: app: helloworld --- apiVersion: apps/v1 kind: Deployment metadata: name: helloworld-v1 labels: app: helloworld version: v1 spec: replicas: 1 selector: matchLabels: app: helloworld version: v1 template: metadata: labels: app: helloworld version: v1 spec: containers: - name: helloworld image: docker.io/istio/examples-helloworld-v1 resources: requests: cpu: \u0026#39;100m\u0026#39; imagePullPolicy: IfNotPresent #Always ports: - containerPort: 5000 --- apiVersion: apps/v1 kind: Deployment metadata: name: helloworld-v2 labels: app: helloworld version: v2 spec: replicas: 1 selector: matchLabels: app: helloworld version: v2 template: metadata: labels: app: helloworld version: v2 spec: containers: - name: helloworld image: docker.io/istio/examples-helloworld-v2 resources: requests: cpu: \u0026#39;100m\u0026#39; imagePullPolicy: IfNotPresent #Always ports: - containerPort: 5000 将其保存为 helloworld-2-subsets.yaml ，并将其与 kubectl 一起应用：\nkubectl apply -f helloworld-2-subsets.yaml -n helloworld 部署应用程序 IngressGateway 使用以下 YAML 配置部署应用程序 IngressGateway：\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: tsb-helloworld-gateway namespace: helloworld spec: kubeSpec: service: type: LoadBalancer 将其保存为 helloworld-ingress.yaml ，并将其与 kubectl 一起应用：\nkubectl apply -f helloworld-ingress.yaml 获取网关 IP：\nexport GATEWAY_IP=$(kubectl -n helloworld get service tsb-helloworld-gateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) echo $GATEWAY_IP 网关证书 在本例中，你将在网关处使用简单的 TLS 公开应用程序。你需要为它提供一个存储在 Kubernetes 密钥中的 TLS 证书：\nkubectl create secret tls -n helloworld helloworld-cert \\ --cert /path/to/some/helloworld-cert.pem \\ --key /path/to/some/helloworld-key.pem 部署 IngressGateway 和 ServiceRoute 使用以下 YAML 配置创建 IngressGateway：\nhelloworld-gw.yaml apiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: name: helloworld-gateway group: helloworld-gw workspace: helloworld-ws tenant: tetrate organization: tetrate spec: workloadSelector: namespace: helloworld labels: app: tsb-helloworld-gateway http: - name: helloworld port: 443 hostname: helloworld.tetrate.com tls: mode: SIMPLE secretName: helloworld-cert routing: rules: - route: host: helloworld/helloworld.helloworld.svc.cluster.local port: 5000 将其保存为 helloworld-gw.yaml ，并将其与 tctl 一起应用：\ntctl apply -f helloworld-gw.yaml 创建一个 ServiceRoute，根据标头匹配流量并将其路由到不同的子集：\nhelloworld-header-based-routing-service-route.yaml apiVersion: traffic.tsb.tetrate.io/v2 kind: ServiceRoute metadata: name: helloworld-service-route group: helloworld-trf workspace: helloworld-ws tenant: tetrate organization: tetrate spec: service: helloworld/helloworld.helloworld.svc.cluster.local portLevelSettings: - port: 5000 trafficType: HTTP subsets: - name: v1 labels: version: v1 weight: 50 - name: v2 labels: version: v2 weight: 50 httpRoutes: - name: http-route-match-header-and-port match: - name: match-header-and-port headers: end-user: exact: jason port: 5000 destination: - subset: v1 weight: 80 port: 5000 - subset: v2 weight: 20 port: 5000 - name: http-route-match-port match: - name: match-port port: 5000 destination: - subset: v1 weight: 100 port: 5000 将其保存为 helloworld-header-based-routing-service-route.yaml ，并将其与 tctl 一起应用：\ntctl apply -f helloworld-header-based-routing-service-route.yaml 验证 带标头的请求 发送带有标头 end-user: jason 的连续 curl 请求。流量将以 80:20 的比例在 v1 和 v2 之间路由。\nfor i in {1..20}; do curl -k \u0026#34;https://helloworld.tetrate.com/hello\u0026#34; \\ --resolve \u0026#34;helloworld.tetrate.com:443:$GATEWAY_IP\u0026#34; \\ -H \u0026#34;end-user: jason\u0026#34; 2\u0026gt;\u0026amp;1; done 无标头请求 发送不带任何标头的连续卷曲请求。所有流量都将路由到 v1 。\nfor i in {1..20}; do curl -k \u0026#34;https://helloworld.tetrate.com/hello\u0026#34; \\ --resolve \u0026#34;helloworld.tetrate.com:443:$GATEWAY_IP\u0026#34; 2\u0026gt;\u0026amp;1; done 通过执行这些步骤，你已使用 IngressGateway 和 ServiceRoute 成功设置基于子集的流量路由。\n","relpermalink":"/book/tsb/howto/gateway/subset-based-routing-using-igw-and-service-route/","summary":"在本操作指南中，你将了解如何通过基于 URI 端点、标头和端口匹配流量并将其路由到目标服务的主机:端口来设置基于子集的流量路由。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念 。 安装 TSB 演示环境 。 创建一个","title":"使用 IngressGateway 和 ServiceRoute 基于子集的流量路由"},{"content":"在整个安装过程中，你可能会发现参考“安装与升级 ”部分中介绍的主题很有用。请务必查看下载 tctl 和使用 tctl 连接到 TSB 等主题，以增强你的理解。\n如需快速概览，你还可以浏览我们的演示安装 指南。\n通过遵循这些指南，你将能够使用 tctl 无缝安装 TSB，无论是在本地基础设施上还是在云服务器上。\n演示安装\n管理平面安装\n载入集群\nTSB 卸载\nTSB 升级\n","relpermalink":"/book/tsb/setup/self-managed/","summary":"在整个安装过程中，你可能会发现参考“安装与升级 ”部分中介绍的主题很有用。请务必查看下载 tctl 和使用 tctl 连接到 TSB 等主题，以增强你的理解。 如需快速概览，你还可以浏览我们的演示安装 指南。 通过遵循这些指南，你将能够","title":"使用 tctl 安装"},{"content":"在继续之前，请确保你熟悉Istio 隔离边界 功能。\n注意 可以在单个隔离边界内执行已修订到已修订的控制平面升级。 升级之前 一旦启用了 Istio 隔离边界功能，边界可以用于保持服务发现隔离，并在隔离边界内升级 Istio 控制平面。对于包含单个隔离边界的ControlPlane CR 或 Helm 值：\nspec: ... components: xcp: isolationBoundaries: - name: global revisions: - name: stable istio: tsbVersion: 1.6.0 你将升级stable修订版本中的所有工作负载以使用tsbVersion: 1.6.1。\n控制平面升级策略 TSB 支持修订到修订升级的原地和金丝雀控制平面升级。 控制平面原地升级 对于原地升级，你可以直接更新tsbVersion字段 - 保留修订name不变。\nspec: ... components: xcp: isolationBoundaries: - name: global revisions: - name: stable istio: tsbVersion: 1.6.1 这将重新部署 Istio 控制平面组件，与tsbVersion: 1.6.1对应的 TSB Istio 版本。这个状态将由 istio-system 命名空间中的 xcp-operator-edge Operator 协调。\n网关升级 默认情况下，网关将自动升级以使用最新的tsbVersion。有关网关升级行为的更多详细信息，请参阅网关升级 。\n应用程序升级 由于修订名称未更改，因此在工作负载命名空间（本示例中的workload-ns）中不需要进行任何更新。但是，你仍然需要重新启动应用程序工作负载。为避免流量中断，推荐使用滚动更新。\nkubectl rollout restart deployment -n workload-ns VM 工作负载升级 要升级 VM 工作负载，请使用修订链接 从你的上机平面下载最新的 Istio sidecar，然后在 VM 上重新安装 Istio sidecar。\n然后重新启动在 VM 上运行的onboarding-agent。\n控制平面金丝雀升级 对于金丝雀升级，你可以添加另一个修订版本，其名称为1-6-1，其具有升级的tsbVersion值。\nspec: ... components: xcp: isolationBoundaries: - name: global revisions: - name: stable istio: tsbVersion: 1.6.0 - name: 1-6-1 istio: tsbVersion: 1.6.1 这将部署另一个 Istio 控制平面（修订1-6-1）与tsbVersion: 1.6.1对应的 TSB Istio 版本。这个状态将由 istio-system 命名空间中的 xcp-operator-edge 操作符协调。你可以检查istio-operator和istiod部署以进行验证。\nkubectl get deployment -n istio-system | grep istio-operator # 输出 istio-operator-stable 1/1 1 1 15小时 istio-operator-1-6-1 1/1 1 1 2分钟 kubectl get deployment -n istio-system | grep istiod # 输出 istiod-stable 1/1 1 1 15小时 istiod-1-6-1 1/1 1 1 2分钟 请注意，仍然部署了一个旧的修订控制平面（stable），它管理着现有的 sidecars 和网关。\n网关升级 要升级网关，请更新spec.revision 在Ingress/Egress/Tier1Gateway资源中。这将协调现有的网关 Pod 以连接到新的修订 Istio 控制平面。TSB 默认配置了带有RollingUpdate策略的网关安装资源，以确保零停机时间。\n你还可以通过修补网关 CR 来更新spec.revision。\nkubectl patch ingressgateway.install \u0026lt;name\u0026gt; -n \u0026lt;namespace\u0026gt; --type=json --patch \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;,\u0026#34;path\u0026#34;: \u0026#34;/spec/revision\u0026#34;,\u0026#34;value\u0026#34;: \u0026#34;1-6-1\u0026#34;}]\u0026#39;; \\ 应用程序升级 要升级 sidecars，请替换工作负载命名空间标签istio.io/rev=stable并应用新的修订。\nkubectl label namespace workload-ns istio.io/rev=1-6-1 --overwrite=true 然后重新启动应用程序工作负载。为避免流量中断，推荐使用滚动更新。\nkubectl rollout restart deployment -n workload-ns VM 工作负载升级 要升级 VM 工作负载，请使用修订链接 从你的上机平面下载最新的 Istio sidecar，然后在 VM 上重新安装 Istio sidecar。\n在onboarding-agent配置中更新revision值 ，然后重新启动onboarding-agent。\n升级后清理 不再使用的修订版本可以从ControlPlane CR 中删除或标记为“禁用”。将其标记为禁用有助于随时启用修订版本。\n选择 1：禁用修订版本\nspec: ... components: xcp: isolationBoundaries: - name: global revisions: - name: stable istio: tsbVersion: 1.6.0 disable: true - name: 1-6-1 istio: tsbVersion: 1.6.1 选择 2：删除修订版本\nspec: ... components: xcp: isolationBoundaries: - name: global revisions: - name: 1-6-1 istio: tsbVersion: 1.6.1 旧的 Istio 控制平面组件 在禁用/删除隔离边界下的修订版本后，可能会保留一些旧的组件。例如，IstioOperator资源、istio-operator（修订版）部署或 istiod（修订版）部署。这是由于删除IstioOperator资源和 istio-operator 部署的竞争条件造成的。 在这种情况下，可以像普通的 Kubernetes 对象一样删除这些 Istio 组件。\nkubectl delete iop xcp-iop-stable -n istio-system kubectl delete deployment istio-operator-stable -n istio-system kubectl delete configmap istio-sidecar-injector-stable -n istio-system kubectl delete deployment istiod-stable -n istio-system 从已修订到已修订的回滚 这个工作流程与从已修订到已修订的控制平面升级类似。你需要更新你的工作负载以使用旧的修订版本。\n","relpermalink":"/book/tsb/setup/upgrades/revisioned-to-revisioned/","summary":"如何从已修订升级控制平面集群到已修订。","title":"已修订版本间的升级"},{"content":" 更改管理员密码\n将 LDAP 配置为身份提供者\nAzure AD 作为身份提供者\n用户同步\n角色和权限\n","relpermalink":"/book/tsb/operations/users/","summary":"更改管理员密码 将 LDAP 配置为身份提供者 Azure AD 作为身份提供者 用户同步 角色和权限","title":"用户权限"},{"content":"在本文中，将使用 httpbin 作为工作负载。传入 Ingress GW 的请求将由 OPA 检查。如果请求被视为未经授权，那么将以 403（Forbidden）响应拒绝请求。\n下图显示了在使用外部授权系统时的请求和响应流程，你将部署 OPA 作为独立服务。\n部署 httpbin 服务 按照此文档中的所有说明 创建httpbin服务。\n部署 OPA 服务 参考\u0026#34;安装 Open Policy Agent “文档，创建具有基本身份验证的策略 并部署 OPA 作为独立服务 。\n配置 Ingress Gateway 你需要为httpbin再次配置 Ingress Gateway 以使用 OPA。创建名为 httpbin-ingress.yaml 的文件，其中包含以下内容：\napiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: organization: tetrate name: httpbin-ingress-gateway group: httpbin workspace: httpbin tenant: tetrate spec: workloadSelector: namespace: httpbin labels: app: httpbin-ingress-gateway http: - name: httpbin port: 443 hostname: \u0026#34;httpbin.tetrate.com\u0026#34; tls: mode: SIMPLE secretName: httpbin-certs routing: rules: - route: host: \u0026#34;httpbin/httpbin.httpbin.svc.cluster.local\u0026#34; port: 8000 authorization: external: uri: grpc://opa.opa.svc.cluster.local:9191 使用 tctl apply 命令应用配置：\ntctl apply -f httpbin-ingress.yaml 测试 你可以通过从外部机器或本地环境向httpbin Ingress Gateway 发送 HTTP 请求来测试外部授权。\n在以下示例中，由于你无法控制 httpbin.tetrate.com，你必须欺骗 curl 以使其认为 httpbin.tetrate.com 解析为 Ingress Gateway 的 IP 地址。\n使用以下命令获取之前创建的 Ingress Gateway 的 IP 地址。\nkubectl -n httpbin get service httpbin-ingress-gateway \\ -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39; 然后执行以下命令，通过 Ingress Gateway 发送 HTTP 请求到 httpbin 服务。将 \u0026lt;gateway-ip\u0026gt; 替换为你在前一步中获取的值。\n请记住，示例 OPA 策略包含两个用户 alice 和 bob，可以使用基本身份验证进行授权。\n以下命令应显示 200。同样，将用户名更改为 bob 也应显示 200。\ncurl -u alice:password \u0026#34;https://httpbin.tetrate.com/get\u0026#34; \\ --resolve \u0026#34;httpbin.tetrate.com:443:\u0026lt;gateway-ip\u0026gt;\u0026#34; \\ --cacert certs/httpbin-ca.crt \\ -s \\ -o \\ -w \u0026#34;%{http_code}\\n\u0026#34; 以下命令向用户 alice 提供错误的密码。这应该显示 403。\ncurl -u alice:wrongpassword \u0026#34;https://httpbin.tetrate.com/get\u0026#34; \\ --resolve \u0026#34;httpbin.tetrate.com:443:\u0026lt;gateway-ip\u0026gt;\u0026#34; \\ --cacert certs/httpbin-ca.crt \\ -s \\ -o \\ -w \u0026#34;%{http_code}\\n\u0026#34; 最后，如果提供的用户不是 alice 或 bob，则应显示 403。\ncurl -u charlie:password \u0026#34;https://httpbin.tetrate.com/get\u0026#34; \\ --resolve \u0026#34;httpbin.tetrate.com:443:\u0026lt;gateway-ip\u0026gt;\u0026#34; \\ --cacert certs/httpbin-ca.crt \\ -s \\ -o \\ -w \u0026#34;%{http_code}\\n\u0026#34; ","relpermalink":"/book/tsb/howto/authorization/ingress-gateway/","summary":"如何使用 Open Policy Agent（OPA）示例配置 Ingress Gateway 外部授权。","title":"在 Ingress Gateways 中配置外部授权"},{"content":"在有效使用 Argo CD 之前，有必要了解该平台构建的底层技术。还有必要了解向你提供的功能以及如何使用它们。以下部分提供了一些有用的链接来建立这种理解。\n学习基础知识 浏览在线 Docker 和 Kubernetes 教程： 适合初学者的容器、虚拟机和 Docker 简介 Kubernetes 简介 教程 根据你计划如何模板化你的应用程序： Kustomize Helm 如果你要与 CI 工具集成： GitHub Actions 文档 Jenkins 用户指南 ","relpermalink":"/book/argo-cd/understand-the-basics/","summary":"在有效使用 Argo CD 之前，有必要了解该平台构建的底层技术。还有必要了解向你提供的功能以及如何使用它们。以下部分提供了一些有用的链接来建立这种理解。 学习基础知识 浏览在线 Docker 和 Kubernetes 教程： 适合初学者的容器、虚拟机和 Docker","title":"理解基础"},{"content":"本指南假定你已经熟悉 Argo CD 及其基本概念。有关更多信息，请参阅 Argo CD 文档。\n要求 安装 kubectl 命令行工具 有一个 kubeconfig 文件（默认位置为 ~/.kube/config）。 安装 有几种安装 ApplicationSet 控制器的选项。\nA) 将 ApplicationSet 作为 Argo CD 的一部分安装 从 Argo CD v2.3 开始，ApplicationSet 控制器已捆绑在 Argo CD 中。无需从 Argo CD 单独安装 ApplicationSet 控制器。\n有关更多信息，请参阅 Argo CD 入门指南 。\nB) 将 ApplicationSet 安装到现有的 Argo CD 安装中（Argeo CD v2.3 之前） 注意: 以下说明仅适用于 Argo CD 版本 v2.3.0 之前。\nApplicationSet 控制器 必须 安装到与其所针对的 Argo CD 相同的命名空间中。\n假设 Argo CD 安装在 argocd 命名空间中，请运行以下命令：\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/applicationset/v0.4.0/manifests/install.yaml 安装完成后，ApplicationSet 控制器不需要进行其他设置。\nmanifests/install.yaml 文件包含安装 ApplicationSet 控制器所需的 Kubernetes 清单：\nApplicationSet 资源的自定义资源定义 argocd-applicationset-controller 的 Deployment 供 ApplicationSet 控制器使用的 ServiceAccount，用于访问 Argo CD 资源 授予 ServiceAccount 所需资源的 RBAC 访问权限的 Role 将 ServiceAccount 和 Role 绑定的 RoleBinding 启用高可用性模式 要启用高可用性，必须在 argocd-applicationset-controller 容器中设置命令 --enable-leader-election=true 并增加副本数。\n在 manifests/install.yaml 中执行以下更改：\nspec: containers: - command: - entrypoint.sh - argocd-applicationset-controller - --enable-leader-election=true 可选：升级后的额外安全保障 请参阅 控制资源修改 页面，了解你可能希望在 install.yaml 中的 ApplicationSet Resource 中添加的其他参数，以提供额外的安全性，以防止任何初始意外的后升级行为。\n例如，为了暂时防止升级后的 ApplicationSet 控制器进行任何更改，你可以：\n启用干运行 使用仅创建策略 在 ApplicationSets 上启用 preserveResourcesOnDeletion 在你的 ApplicationSets 模板中暂时禁用自动同步 这些参数将允许你观察/控制新版本 ApplicationSet 控制器在你的环境中的行为，以确保你对结果感到满意（请参阅 ApplicationSet 日志文件以获取详细信息）。只需不要忘记在完成测试后删除任何临时更改！\n但是，如上所述，这些步骤并不是必需的：升级 ApplicationSet 控制器应该是一项最小侵入性的过程，并且这些步骤仅建议作为额外安全措施。\n下一步 一旦你的 ApplicationSet 控制器正常运行，请继续阅读 用例 ，了解更多支持的场景，或直接转到 生成器 查看示例 ApplicationSet 资源。\n","relpermalink":"/book/argo-cd/operator-manual/applicationset/getting-started/","summary":"本指南假定你已经熟悉 Argo CD 及其基本概念。有关更多信息，请参阅 Argo CD 文档。 要求 安装 kubectl 命令行工具 有一个 kubeconfig 文件（默认位置为 ~/.kube/config）。 安装 有几种安装 ApplicationSet 控制器的选项。 A) 将 ApplicationSet 作为 Argo CD 的一部分安装","title":"ApplicationSet 入门"},{"content":"本教程将指导你如何配置 Argo Rollouts 与 Ambassador 配合以实现金丝雀发布。本指南中使用的所有文件都可在此存储库的 examples 目录中找到。\n要求 Kubernetes 集群 在集群中安装 Argo-Rollouts 注意\n如果使用 Ambassador Edge Stack 或 Emissary-ingress 2.0+，则需要安装 Argo-Rollouts 版本 v1.1+，并需要向 argo-rollouts 部署提供 --ambassador-api-version getambassador.io/v3alpha1。\n1. 安装和配置 Ambassador Edge Stack 如果你的集群中没有 Ambassador，可以按照 Edge Stack 文档 进行安装。\n默认情况下，Edge Stack 通过 Kubernetes 服务路由。为了获得更好的金丝雀性能，我们建议你使用端点路由。通过将以下配置保存在名为 resolver.yaml 的文件中，启用集群上的端点路由：\napiVersion: getambassador.io/v2 kind: KubernetesEndpointResolver metadata: name: endpoint 将此配置应用于你的集群：kubectl apply -f resolver.yaml。\n2. 创建 Kubernetes 服务 我们将创建两个 Kubernetes 服务，分别命名为 echo-stable 和 echo-canary。将此配置保存到名为 echo-service.yaml 的文件中。\napiVersion: v1 kind: Service metadata: labels: app: echo name: echo-stable spec: type: ClusterIP ports: - name: http port: 80 protocol: TCP targetPort: 8080 selector: app: echo --- apiVersion: v1 kind: Service metadata: labels: app: echo name: echo-canary spec: type: ClusterIP ports: - name: http port: 80 protocol: TCP targetPort: 8080 selector: app: echo 我们还将为服务创建 Edge Stack 路由。将以下配置保存到名为 echo-mapping.yaml 的文件中。\napiVersion: getambassador.io/v2 kind: Mapping metadata: name: echo spec: prefix: /echo rewrite: /echo service: echo-stable:80 resolver: endpoint 将这两个配置都应用于 Kubernetes 集群：\nkubectl apply -f echo-service.yaml kubectl apply -f echo-mapping.yaml 3. 部署 Echo 服务 创建一个 Rollout 资源并将其保存到名为 rollout.yaml 的文件中。注意 trafficRouting 属性，它告诉 Argo 使用 Ambassador Edge Stack 进行路由。\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: echo-rollout spec: selector: matchLabels: app: echo template: metadata: labels: app: echo spec: containers: - image: hashicorp/http-echo args: - \u0026#34;-text=VERSION 1\u0026#34; - -listen=:8080 imagePullPolicy: Always name: echo-v1 ports: - containerPort: 8080 strategy: canary: stableService: echo-stable canaryService: echo-canary trafficRouting: ambassador: mappings: - echo steps: - setWeight: 30 - pause: {duration: 30s} - setWeight: 60 - pause: {duration: 30s} - setWeight: 100 - pause: {duration: 10} 将 Rollout 应用于你的集群 kubectl apply -f rollout.yaml。注意，由于这是部署的服务的第一个版本，因此不会进行金丝雀部署。\n4. 测试服务 现在，我们将测试此部署是否按预期工作。打开一个新的终端窗口。我们将使用它来发送请求到集群。获取 Edge Stack 的外部 IP 地址：\nexport AMBASSADOR_LB_ENDPOINT=$(kubectl -n ambassador get svc ambassador -o \u0026#34;go-template={{range .status.loadBalancer.ingress}}{{or .ip .hostname}}{{end}}\u0026#34;) 发送请求到 echo 服务：\ncurl -Lk \u0026#34;https://$AMBASSADOR_LB_ENDPOINT/echo/\u0026#34; 你应该会得到一个 “VERSION 1” 的响应。\n5. 部署新版本 现在是时候部署服务的新版本了。将 rollout.yaml 中的 echo 容器更新为显示 “VERSION 2”：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: echo-rollout spec: selector: matchLabels: app: echo template: metadata: labels: app: echo spec: containers: - image: hashicorp/http-echo args: - \u0026#34;-text=VERSION 2\u0026#34; - -listen=:8080 imagePullPolicy: Always name: echo-v1 ports: - containerPort: 8080 strategy: canary: stableService: echo-stable canaryService: echo-canary trafficRouting: ambassador: mappings: - echo steps: - setWeight: 30 - pause: {duration: 30s} - setWeight: 60 - pause: {duration: 30s} - setWeight: 100 - pause: {duration: 10} 通过键入 kubectl apply -f rollout.yaml 将 Rollout 应用于集群。这将通过在 30 秒内将 30% 的流量路由到服务，然后在另外 30 秒内将 60% 的流量路由到服务来部署服务的第 2 个版本。\n你可以在命令行上监视 Rollout 的状态：\nkubectl argo rollouts get rollout echo-rollout --watch 将显示类似于以下内容的输出：\nName: echo-rollout Namespace: default Status: ॥ Paused Message: CanaryPauseStep Strategy: Canary Step: 1/6 SetWeight: 30 ActualWeight: 30 Images: hashicorp/http-echo (canary, stable) Replicas: Desired: 1 Current: 2 Updated: 1 Ready: 2 Available: 2 NAME KIND STATUS AGE INFO ⟳ echo-rollout Rollout ॥ Paused 2d21h ├──# revision:3 │ └──⧉ echo-rollout-64fb847897 ReplicaSet ✔ Healthy 2s canary │ └──□ echo-rollout-64fb847897-49sg6 Pod ✔ Running 2s ready:1/1 ├──# revision:2 │ └──⧉ echo-rollout-578bfdb4b8 ReplicaSet ✔ Healthy 3h5m stable │ └──□ echo-rollout-578bfdb4b8-86z6n Pod ✔ Running 3h5m ready:1/1 └──# revision:1 └──⧉ echo-rollout-948d9c9f9 ReplicaSet • ScaledDown 2d21h 在其他终端窗口中，你可以通过循环发送请求来验证金丝雀是否正在适当地进行：\nwhile true; do curl -k https://$AMBASSADOR_LB_ENDPOINT/echo/; sleep 0.2; done 这将显示一个来自服务的响应的运行列表，这些响应将逐渐从 VERSION 1 字符串转换为 VERSION 2 字符串。\n有关 Ambassador 和 Argo-Rollouts 集成的更多详细信息，请参见 Ambassador Argo 文档 。\n","relpermalink":"/book/argo-rollouts/getting-started/ambassador/index/","summary":"本教程将指导你如何配置 Argo Rollouts 与 Ambassador 配合以实现金丝雀发布。本指南中使用的所有文件都可在此存储库的 examples 目录中找到。 要求 Kubernetes 集群 在集群中安装 Argo-Rollouts 注意 如果使用 Ambassador Edge Stack 或 Emissary-ingress 2.0+，则需要安装 Argo-Rollouts 版本 v1.1+，并需要向 argo-rollouts 部","title":"Argo Rollouts 和 Ambassador 快速开始"},{"content":"Ambassador Edge Stack 提供了你在 Kubernetes 集群边缘所需的功能（因此称为“边缘堆栈”）。这包括 API 网关、入口控制器、负载均衡器、开发人员门户、金丝雀流量路由等。它提供了一组 CRD，用户可以配置以启用不同的功能。\nArgo-Rollouts 提供了一个集成，利用了 Ambassador 的 金丝雀路由功能 。这允许你的应用程序的流量在部署新版本时逐步增加。\n工作原理 Ambassador Edge Stack 提供了一个名为“Mapping”的资源，用于配置如何将流量路由到服务。通过创建具有相同 URL 前缀并指向不同服务的 2 个映射，可以实现 Ambassador 金丝雀部署。考虑以下示例：\napiVersion: getambassador.io/v2 kind: Mapping metadata: name: stable-mapping spec: prefix: /someapp rewrite: / service: someapp-stable:80 --- apiVersion: getambassador.io/v2 kind: Mapping metadata: name: canary-mapping spec: prefix: /someapp rewrite: / service: someapp-canary:80 weight: 30 在上面的示例中，我们正在配置 Ambassador 以将来自\u0026lt;public ingress\u0026gt;/someapp 的 30% 流量路由到服务 someapp-canary，其余流量将进入服务 someapp-stable。如果用户想逐步增加到金丝雀服务的流量，则必须手动或自动化地更新 canary-mapping 权重的值。\n使用 Argo-Rollouts 无需创建 canary-mapping。Argo-Rollouts 控制器完全自动化创建它并逐步更新其权重的过程。以下示例说明如何配置 Rollout 资源以使用 Ambassador 作为金丝雀部署的流量路由器：\napiVersion: argoproj.io/v1alpha1 kind: Rollout ... spec: strategy: canary: stableService: someapp-stable canaryService: someapp-canary trafficRouting: ambassador: mappings: - stable-mapping steps: - setWeight: 30 - pause: {duration: 60s} - setWeight: 60 - pause: {duration: 60s} 在 spec.strategy.canary.trafficRouting.ambassador 下有 2 个可能的属性：\nmappings：必需。Argo-Rollouts 必须提供至少一个 Ambassador 映射才能管理金丝雀部署。如果有多个路由到服务的路由（例如，你的服务具有多个端口或可以通过不同的 URL 访问），则还支持多个映射。如果未提供映射，则 Argo-Rollouts 将发送错误事件，并且回滚将中止。 当在清单的 trafficRouting 中配置了 Ambassador 时，Rollout 控制器将：\n为 Rollout manifest 中提供的每个 stable mapping 创建一个金丝雀映射 根据配置继续执行步骤，更新金丝雀映射权重 在流程结束时，Argo-Rollout 将删除所有创建的金丝雀映射 端点解析器 默认情况下，Ambassador 使用 kube-proxy 将流量路由到 Pod。但是，我们应该将其配置为绕过 kube-proxy 并直接将流量路由到 pod。这将提供真正的 L7 负载平衡，在金丝雀工作流中是可取的。这种方法称为 endpoint routing ，可以通过配置 endpoint resolvers 实现。\n要将 Ambassador 配置为使用端点解析器，必须在集群中应用以下资源：\napiVersion: getambassador.io/v2 kind: KubernetesEndpointResolver metadata: name: endpoint 然后配置映射以使用它设置 resolver 属性：\napiVersion: getambassador.io/v2 kind: Mapping metadata: name: stable-mapping spec: resolver: endpoint prefix: /someapp rewrite: / service: someapp-stable:80 有关 Ambassador 和 Argo-Rollouts 集成的更多详细信息，请参见 Ambassador Argo 文档 。\n","relpermalink":"/book/argo-rollouts/traffic-management/ambassador/","summary":"Ambassador Edge Stack 提供了你在 Kubernetes 集群边缘所需的功能（因此称为“边缘堆栈”）。这包括 API 网关、入口控制器、负载均衡器、开发人员门户、金丝雀流量路由等。它提供了一组 CRD，用户可以配置以启用不同的功能。 Argo-Rollouts 提供了一个集成，","title":"Ambassador Edge Stack"},{"content":"以下描述了 Rollout 的所有可用字段：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: example-rollout-canary spec: # 期望的 Pod 数量。 # 默认为 1。 replicas: 5 analysis: # 限制存储历史上成功的分析运行和实验的数量 # 默认为 5。 successfulRunHistoryLimit: 10 # 限制存储历史上不成功的分析运行和实验的数量。 # 不成功的阶段有：\u0026#34;Error\u0026#34;，\u0026#34;Failed\u0026#34;，\u0026#34;Inconclusive\u0026#34; # 默认为 5。 unsuccessfulRunHistoryLimit: 10 # Pod 的标签选择器。被选择的 Pod 的现有副本集将受到此 Rollout 的影响。它必须与 Pod 模板的标签匹配。 selector: matchLabels: app: guestbook # WorkloadRef 包含对提供 Pod 模板（例如 Deployment）的工作负载的引用。如果使用，则不使用 Rollout 模板属性。 workloadRef: apiVersion: apps/v1 kind: Deployment name: rollout-ref-deployment # 模板描述将被创建的 Pod。与 deployment 相同。 # 如果使用，则不使用 Rollout workloadRef 属性。 template: spec: containers: - name: guestbook image: argoproj/rollouts-demo:blue # 新创建的 Pod 必须准备好而没有任何容器崩溃的最小秒数， # 才能被视为可用。默认为 0（Pod 将在准备就绪后立即被视为可用） minReadySeconds: 30 # 要保留的旧 ReplicaSet 的数量。 # 默认为 10。 revisionHistoryLimit: 3 # Pause 允许用户在任何时候手动暂停 Rollout。 # 在手动暂停期间，Rollout 将不会通过其步骤前进，但是 HPA 自动扩展将仍然发生。 # 通常不在清单中明确设置，而是通过工具（例如 kubectl argo rollouts pause）进行控制。 # 如果在 Rollout 的初始创建时为 true，则不会从零自动扩展副本，除非手动推广。 paused: true # 在更新期间，Rollout 必须取得进展的最大时间（以秒为单位）， # 否则将被视为失败。Argo Rollouts 将继续处理失败的 Rollout， # 并在 Rollout 状态中显示具有 ProgressDeadlineExceeded 原因的条件。 # 请注意，进度不会在 Rollout 暂停期间估计。 # 默认为 600 秒 progressDeadlineSeconds: 600 # 当超过 ProgressDeadlineSeconds 时，是否中止更新。 # 可选，默认值为 false。 progressDeadlineAbort: false # UTC 时间戳，Rollout 应该按顺序重新启动所有的 Pod。 # 由“kubectl argo rollouts restart ROLLOUT”命令使用。 # 控制器将确保所有 Pod 的 creationTimestamp 大于或等于此值。 restartAt: \u0026#34;2020-03-30T21:19:35Z\u0026#34; # 回滚窗口提供了一种快速跟踪到以前部署的版本的方法。 # 可选，默认未设置。 rollbackWindow: revisions: 3 strategy: # 蓝绿更新策略 blueGreen: # Rollout 修改的服务的引用作为活动服务。 # 必填项。 activeService: active-service # 促销之前执行分析的运行，以在服务切换之前执行分析。+可选 prePromotionAnalysis: templates: - templateName: success-rate args: - name: service-name value: guestbook-svc.default.svc.cluster.local # 促销后执行分析的运行，以在服务切换之后执行分析。+可选 postPromotionAnalysis: templates: - templateName: success-rate args: - name: service-name value: guestbook-svc.default.svc.cluster.local # Rollout 修改的服务的名称作为预览服务的名称。+可选 previewService: preview-service # 在切换之前，在预览服务下运行的副本数。 # 一旦 Rollout 恢复，新的 ReplicaSet 将完全扩展， # 然后才会发生切换 +可选 previewReplicaCount: 1 # 指示 Rollout 是否应自动将新的 ReplicaSet 提升为活动服务， # 还是进入暂停状态。如果未指定，则默认值为 true。+可选 autoPromotionEnabled: false # 在新的 ReplicaSet 准备就绪后，自动将当前 ReplicaSet 提升为活动状态 # 经过指定的暂停延迟时间（以秒为单位）。如果省略，则 Rollout 将进入暂停状态， # 直到通过将 spec.Paused 重置为 false 手动恢复。 autoPromotionSeconds: 30 # 在缩放之前添加延迟以缩小先前的 ReplicaSet。如果省略， # Rollout 将在缩小先前的 ReplicaSet 之前等待 30 秒。建议至少等待 30 秒， # 以确保在集群中的节点之间进行 IP 表传播。 scaleDownDelaySeconds: 30 # 在被缩放之前可运行的旧 RS 的数量限制。 # 默认为 nil。 scaleDownDelayRevisionLimit: 2 # 如果更新被中止，则在缩小预览副本集之前添加延迟。 # 0 表示不缩小。默认为 30 秒。 abortScaleDownDelaySeconds: 30 # 所需和先前 ReplicaSet 之间的反亲和力配置。 # 只能指定一个 antiAffinity: requiredDuringSchedulingIgnoredDuringExecution: {} preferredDuringSchedulingIgnoredDuringExecution: weight: 1 # 在 1-100 之间 # activeMetadata 将合并并即时更新到活动 Pod 的 ReplicaSet 的 spec.template.metadata 中。+可选 activeMetadata: labels: role: active # 只在预览阶段将分配给预览 Pod 的元数据。 # +可选 previewMetadata: labels: role: preview # 金丝雀更新策略 canary: # 控制器将更新以选择金丝雀 Pod 的服务的引用。用于流量路由。 # 必需的。 canaryService: canary-service # 控制器将更新以选择稳定 Pod 的服务的引用。用于流量路由。 # 必需的。 stableService: stable-service # 将附加到金丝雀 Pod 的元数据。此元数据仅在更新期间存在，因为在完全推广的 Rollout 中没有金丝雀 Pod。 canaryMetadata: annotations: role: canary labels: role: canary # 将附加到稳定 Pod 的元数据。 stableMetadata: annotations: role: stable labels: role: stable # 在更新期间可以不可用的最大 Pod 数。 # 值可以是绝对数（例如 5），也可以是开始更新时总 Pod 数的百分比（例如 10％）。 # 绝对数由百分比四舍五入计算。如果 MaxSurge 为 0，则不能为 0。默认情况下，使用固定值 1。 # 例如：将此设置为 30％时，可以在滚动更新开始时立即将旧 RC 缩减 30％。 # 一旦新的 Pod 准备就绪，旧的 RC 可以进一步缩减，然后才能扩展新的 RC，从而确保 # 在更新期间始终至少有 70％的原始 Pod 数可用。 # +可选 maxUnavailable: 1 # 可以调度的 Pod 的最大数量，超过原始 Pod 数量。 # 值可以是绝对数（例如 5）或开始更新时总 Pod 数量的百分比（例如 10％）。 # 如果 MaxUnavailable 为 0，则不能为 0。绝对数从百分比计算，四舍五入。默认情况下，使用值 1。 # 例如：将此设置为 30％时，可以在滚动更新开始时立即将新的 RC 扩展 30％。 # 一旦旧 Pod 被杀死，新的 RC 可以进一步扩展，以确保在更新期间运行的 Pod 的总数最多为原始 Pod 的 130％。 # +可选 maxSurge: \u0026#34;20%\u0026#34; # 当使用流量路由的金丝雀策略时，添加在缩小先前的 ReplicaSet 之前的延迟（默认为 30 秒）。 # 在将稳定服务选择器切换到指向新的 ReplicaSet 之后，需要在缩小先前的 ReplicaSet 之前延迟，以便为流量提供程序提供时间 # 重新定位新的 Pod。在基本的，基于副本权重的金丝雀策略中使用此值时，将忽略它。 scaleDownDelaySeconds: 30 # 在使用流量路由的金丝雀时，每个 ReplicaSet 将请求的最小 Pod 数量。 # 这是为了确保每个 ReplicaSet 的高可用性。默认为 1。+可选 minPodsPerReplicaSet: 2 # 在被缩放之前可运行的旧 RS 的数量限制。 # 默认为 nil。 scaleDownDelayRevisionLimit: 2 # 在滚动更新期间运行的后台分析。在滚动更新的初始部署时跳过。+可选 analysis: templates: - templateName: success-rate args: - name: service-name value: guestbook-svc.default.svc.cluster.local # valueFrom.podTemplateHashValue 是一种方便的方法，用于提供稳定 ReplicaSet 或最新 ReplicaSet 的 rollouts-pod-template-hash 值 - name: stable-hash valueFrom: podTemplateHashValue: Stable - name: latest-hash valueFrom: podTemplateHashValue: Latest # valueFrom.fieldRef 允许提供有关 Rollout 的元数据作为分析的参数。 - name: region valueFrom: fieldRef: fieldPath: metadata.labels[\u0026#39;region\u0026#39;] # 步骤定义了在更新金丝雀时要执行的步骤序列。 # 在 Rollout 的初始部署时跳过。+可选 steps: # 将金丝雀 ReplicaSet 的比例设置为 20％ - setWeight: 20 # 暂停 Rollout 一小时。支持的单位：s，m，h - pause: duration: 1h # …","relpermalink":"/book/argo-rollouts/rollout/specification/","summary":"以下描述了 Rollout 的所有可用字段： apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: example-rollout-canary spec: # 期望的 Pod 数量。 # 默认为 1。 replicas: 5 analysis: # 限制存储历史上成功的分析运行和实验的数量 # 默认为 5。 successfulRunHistoryLimit: 10 # 限制存储历史上不成功的分析运行和实验的数量。 # 不成功的阶段有：","title":"Rollout 规范"},{"content":"控制器安装 两种安装方式：\ninstall.yaml - 标准安装方法。 kubectl create namespace argo-rollouts kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml 这将创建一个新的命名空间 argo-rollouts，在其中运行 Argo Rollouts 控制器。\n🔔 提示：如果你使用的是其他命名空间名称，请更新 install.yaml 集群角色绑定的服务账户命名空间名称。\n🔔 提示：在 Kubernetes v1.14 或更低版本上安装 Argo Rollouts 时，CRD 清单必须使用 --validate = false 选项进行 kubectl apply。这是由于在 v1.15 中引入的新 CRD 字段的使用，在较低的 API 服务器中默认被拒绝。\n🔔 提示：在 GKE 上，你需要授予你的账户创建新集群角色的权限：\nkubectl create clusterrolebinding YOURNAME-cluster-admin-binding \\ --clusterrole=cluster-admin --user=YOUREMAIL@gmail.com namespace-install.yaml - 安装 Argo Rollouts，仅需要命名空间级别的特权。使用此安装方法的示例用途是在同一集群上的不同命名空间中运行多个 Argo Rollouts 控制器实例。\n注意：Argo Rollouts CRD 未包含在 namespace-install.yaml 中。必须单独安装 CRD 清单。CRD 清单位于 manifests/crds 目录中。使用以下命令安装它们：\nkubectl apply -k https://github.com/argoproj/argo-rollouts/manifests/crds\\?ref\\=stable 你可以在 Quay.io 找到控制器的发布容器镜像。还有旧版本在 Dockerhub 上，但由于引入了速率限制，Argo 项目已经转移到了 Quay。\nKubectl 插件安装 kubectl 插件是可选的，但方便从命令行管理和可视化升级。\nBrew brew install argoproj/tap/kubectl-argo-rollouts 手动 使用 curl 安装 Argo Rollouts Kubectl 插件 。\ncurl -LO https://github.com/argoproj/argo-rollouts/releases/latest/download/kubectl-argo-rollouts-darwin-amd64 🔔 提示：对于 Linux 发行版，请将 darwin 替换为 linux\n将 kubectl-argo-rollouts 二进制文件设置为可执行。\nchmod +x ./kubectl-argo-rollouts-darwin-amd64 将二进制文件移动到你的 PATH 中。\nsudo mv ./kubectl-argo-rollouts-darwin-amd64 /usr/local/bin/kubectl-argo-rollouts 测试以确保你安装的版本是最新的：\nkubectl argo rollouts version Shell 自动完成 CLI 可以为多个 shell 导出 shell 完成代码。\n对于 bash，请确保安装并启用 bash 完成。要在当前 shell 中访问完成，请运行 source \u0026lt;(kubectl-argo-rollouts completion bash)。或者，将其写入文件并在.bash_profile 中进行 source。\n完成命令支持 bash、zsh、fish 和 powershell。\n有关更多详细信息，请参见完成命令文档 。\n使用 Docker CLI CLI 也可以作为容器镜像在 https://quay.io/repository/argoproj/kubectl-argo-rollouts 中提供。\n你可以像任何其他 Docker 镜像一样运行它，或在支持 Docker 镜像的任何 CI 平台中使用它。\ndocker run quay.io/argoproj/kubectl-argo-rollouts:master version 支持的版本 在任何时候，Argo Rollouts 的官方支持版本是最新发布的版本，在 Kubernetes 版本 N 和 N-1（由 Kubernetes 项目本身支持）上支持。\n例如，如果 Argo Rollouts 的最新次要版本是 1.2.1 并支持 Kubernetes 版本为 1.24、1.23 和 1.22，则支持以下组合：\n在 Kubernetes 1.24 上的 Argo Rollouts 1.2.1 在 Kubernetes 1.23 上的 Argo Rollouts 1.2.1 升级 Argo Rollouts Argo Rollouts 是一个不持有任何外部状态的 Kubernetes 控制器。只有在实际发生部署时，它才是活动的。\n要升级 Argo Rollouts：\n尝试找到没有部署发生的时间段； 删除控制器的先前版本并应用/安装新版本； 发生新的 Rollout 时，新控制器将被激活。 如果你在升级控制器时进行部署，则不应有任何停机时间。当前的 Rollouts 将被暂停，一旦新控制器变为活动状态，它将恢复所有正在进行的部署。\n","relpermalink":"/book/argo-rollouts/installation/","summary":"控制器安装 两种安装方式： install.yaml - 标准安装方法。 kubectl create namespace argo-rollouts kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml 这将创建一个新的命名空间 argo-rollouts，在其中运行 Argo Rollouts 控制器。 🔔 提示：如果你使用的是其他命名空间名称，请更新 install.yaml 集群角色绑定的服","title":"安装 Argo Rollouts"},{"content":"🔔 重要提醒：从 v1.5 开始可用 - 状态：Alpha\nArgo Rollouts 通过第三方插件系统支持获取分析指标。这使得用户可以扩展 Rollouts 的功能，以支持不受本地支持的度量提供者。Rollouts 使用一个名为 go-plugin 的插件库来实现。你可以在此处找到示例插件：rollouts-plugin-metric-sample-prometheus 使用指标插件 安装和使用 argo rollouts 插件有两种方法。第一种方法是将插件可执行文件挂载到 rollouts 控制器容器中。第二种方法是使用 HTTP(S) 服务器托管插件可执行文件。\n将插件可执行文件挂载到 rollouts 控制器容器中 有几种方法可以将插件可执行文件挂载到 rollouts 控制器容器中。其中一些方法将取决于你的特定基础架构。这里有几种方法：\n使用 init 容器下载插件可执行文件 使用 Kubernetes 卷挂载共享卷，例如 NFS、EBS 等。 将插件构建到 rollouts 控制器容器中 然后，你可以使用 configmap 将插件可执行文件位置指向插件。示例：\napiVersion: v1 kind: ConfigMap metadata: name: argo-rollouts-config data: metricProviderPlugins: |- - name: \u0026#34;argoproj-labs/sample-prometheus\u0026#34; # 插件名称，它必须与插件所需的名称匹配，以便它可以找到其配置 location: \u0026#34;file://./my-custom-plugin\u0026#34; # 支持 http(s):// url 和 file:// 使用 HTTP(S) 服务器托管插件可执行文件 Argo Rollouts 支持从 HTTP(S) 服务器下载插件可执行文件。要使用此方法，你需要通过 argo-rollouts-config configmap 配置控制器，并将 pluginLocation 设置为 http(s) url。示例：\napiVersion: v1 kind: ConfigMap metadata: name: argo-rollouts-config data: metricProviderPlugins: |- - name: \u0026#34;argoproj-labs/sample-prometheus\u0026#34; # 插件名称，它必须与插件所需的名称匹配，以便它可以找到其配置 location: \u0026#34;https://github.com/argoproj-labs/rollouts-plugin-metric-sample-prometheus/releases/download/v0.0.4/metric-plugin-linux-amd64\u0026#34; # 支持 http(s):// 和 file:// sha256: \u0026#34;dac10cbf57633c9832a17f8c27d2ca34aa97dd3d\u0026#34; # 可选的插件可执行文件的 sha256 校验和 一些注意事项 根据你用于安装和插件的方法，有一些需要注意的事项。如果无法下载或找到插件可执行文件，则控制器将不会启动。这意味着如果你正在使用需要下载插件的安装方法，并且由于某些原因服务器不可用，而且 rollouts 控制器 pod 在服务器宕机期间被删除或第一次启动时，它将无法启动，直到服务器再次可用。\nArgo Rollouts 仅在启动时下载插件一次，但如果删除了 pod，则需要在下一次启动时再次下载插件。在 HA 模式下运行 Argo Rollouts 可以在一定程度上帮助解决此问题，因为每个 pod 都将在启动时下载插件。因此，如果在服务器故障期间删除了单个 pod，则其他 pod 仍将能够接管，因为它已经有可用的插件可执行文件。Argo Rollouts 管理员的责任是定义插件安装方法并考虑每种方法的风险。\n可用插件列表（按字母顺序） 在此处添加你的插件 如果你已创建插件，请提交 PR 将其添加到此列表中。 rollouts-plugin-metric-sample-prometheus 这只是一个示例插件，可用作创建自己的插件的起点。它不适用于生产。它基于内置的 prometheus 提供程序。 ","relpermalink":"/book/argo-rollouts/analysis/plugins/","summary":"🔔 重要提醒：从 v1.5 开始可用 - 状态：Alpha Argo Rollouts 通过第三方插件系统支持获取分析指标。这使得用户可以扩展 Rollouts 的功能，以支持不受本地支持的度量提供者。Rollouts 使用一个名为 go-plugin 的插件库来实现。你可以在此处","title":"指标插件"},{"content":"Argo Rollouts 支持以下通知服务：\nAlertmanager Email GitHub Google Chat Grafana Mattermost NewRelic Opsgenie Overview Pagerduty Pushover Rocket.Chat Slack Teams Telegram Webex Teams Webhook ","relpermalink":"/book/argo-rollouts/notifications/services/","summary":"Argo Rollouts 支持以下通知服务： Alertmanager Email GitHub Google Chat Grafana Mattermost NewRelic Opsgenie Overview Pagerduty Pushover Rocket.Chat Slack Teams Telegram Webex Teams Webhook","title":"通知服务列表"},{"content":"金丝雀发布是一种部署策略，操作员将新版本的应用程序释放到生产流量的一小部分中。\n概述 由于没有关于金丝雀部署的共识标准，因此 Rollouts Controller 允许用户概述他们想要运行其金丝雀部署的方式。用户可以定义一个控制器使用的步骤列表，以在.spec.template发生更改时操作 ReplicaSets。在新的 ReplicaSet 被提升为稳定版本并且旧版本被完全缩减之前，每个步骤将被评估。\n每个步骤可以有两个字段。setWeight字段指定应发送到金丝雀的流量百分比，pause结构指示 Rollout 暂停。当控制器到达 Rollout 的pause步骤时，它将向.status.PauseConditions字段添加一个PauseCondition结构。如果pause结构中的duration字段设置，Rollout 将在等待duration字段的值之前不会进入下一个步骤。否则，Rollout 将无限期等待该 Pause 条件被删除。通过使用setWeight和pause字段，用户可以描述他们想要如何进入新版本。以下是金丝雀策略的示例。\n🔔 重要：如果金丝雀 Rollout 未使用流量管理 ，则 Rollout 将尽最大努力在新版本和旧版本之间实现最后一个setWeight步骤中列出的百分比。例如，如果 Rollout 有 10 个副本和 10％ 的第一个setWeight步骤，则控制器将将新的期望 ReplicaSet 缩放为 1 个副本，将旧的稳定 ReplicaSet 缩放为 9 个。在 setWeight 为 15％的情况下，Rollout 尝试通过向上舍入计算（即，新的 ReplicaSet 具有 2 个 Pod，因为 10 的 15％向上舍入为 2，旧的 ReplicaSet 具有 9 个 Pod，因为 10 的 85％向上舍入为 9）获得更精细的控制百分比而不使用大量副本。那个用户应该使用流量管理功能。\n示例 apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: example-rollout spec: replicas: 10 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.15.4 ports: - containerPort: 80 minReadySeconds: 30 revisionHistoryLimit: 3 strategy: canary: #表明 Rollout 应使用金丝雀策略 maxSurge: \u0026#34;25%\u0026#34; maxUnavailable: 0 steps: - setWeight: 10 - pause: duration: 1h # 1 小时 - setWeight: 20 - pause: {} # 無限期暂停 暂停持续时间 暂停持续时间可以用可选的时间单位后缀指定。有效的时间单位为“s”、“m”、“h”。如果未指定，则默认为“s”。\nspec: strategy: canary: steps: - pause: { duration: 10 } # 10 秒 - pause: { duration: 10s } # 10 秒 - pause: { duration: 10m } # 10 分钟 - pause: { duration: 10h } # 10 小时 - pause: {} # 无限期暂停 如果暂停步骤没有指定duration，则 Rollout 将无限期暂停。要取消暂停，请使用 argo kubectl 插件promote命令。\n# 提升到下一步 kubectl argo rollouts promote \u0026lt;rollout\u0026gt; 动态金丝雀规模（带流量路由） 默认情况下，Rollout 控制器将根据当前步骤的trafficWeight来扩展金丝雀，以匹配流量权重。例如，如果当前权重为 25％，并且有四个副本，则为了匹配流量权重，金丝雀将被缩放为 1。\n可以控制金丝雀副本在步骤期间的缩放比例，以便它不必必须匹配流量权重。以下是此类用例：\n新版本不应公开（setWeight: 0），但你希望将金丝雀扩展到进行测试。 你希望最小化金丝雀堆栈的规模，并使用一些基于标头的流量整形到金丝雀，而setWeight仍然设置为 0。 你希望将金丝雀扩展到 100％，以便进行流量阴影处理。 🔔 重要：仅当使用带有流量路由的金丝雀策略时，才可以设置金丝雀比例。\n要在步骤期间控制金丝雀比例和权重，请使用setCanaryScale步骤，并指示金丝雀应使用哪个比例：\n显式副本计数而不更改流量权重（replicas） spec.replicas 中的明确的权重百分比，而不更改流量权重（weight） 是否匹配当前金丝雀的setWeight步骤（matchTrafficWeight：true或false） spec: strategy: canary: steps: # 显式计数 - setCanaryScale: replicas: 3 # spec.replicas 的百分比 - setCanaryScale: weight: 25 # matchTrafficWeight 返回与 canary 流量权重匹配的默认行为 - setCanaryScale: matchTrafficWeight: true 在使用具有显式值的setCanaryScale的情况下，如果与setWeight步骤一起使用时不正确，则必须小心。如果不正确地完成，则会将不平衡的流量比例定向到金丝雀（与 Rollout 的比例成比例）。例如，以下一组步骤将导致 90％的流量仅由 10％的 Pod 提供服务：\nspec: replicas: 10 strategy: canary: steps: # 1 金丝雀 Pod（spec.replicas 的 10％） - setCanaryScale: weight: 10 # 90％的流量到 1 个金丝雀 Pod - setWeight: 90 - pause: {} 上述情况是由于setWeight在setCanaryScale之后的更改行为引起的。要重置，请设置matchTrafficWeight：true，并将恢复setWeight行为，即，后续setWeight将创建与流量权重匹配的金丝雀副本。\n动态稳定规模（带流量路由） 🔔 重要：从 v1.1 开始可用\n在使用流量路由时，默认情况下稳定的 ReplicaSet 在更新期间保持缩放为 100％。这具有一个优点，即如果发生中止，则可以立即将流量转移到稳定的 ReplicaSet 而无需延迟。但是，它的缺点是在更新期间，将最终存在双倍数量的副本 Pod（类似于蓝绿色部署），因为稳定的 ReplicaSet 在整个更新期间都被缩放。\n可以通过动态减少稳定的 ReplicaSet 规模来实现，以使其在流量权重增加到金丝雀时缩小。这对于 Rollout 具有高副本计数和资源成本是一个问题，或者在裸机情况下不可能创建额外的节点容量以容纳双倍副本的情况下是有用的。\n可以通过将canary.dynamicStableScale标志设置为 true 来启用动态缩放稳定的 ReplicaSet：\nspec: strategy: canary: dynamicStableScale: true 请注意，如果设置了dynamicStableScale并且 Rollout 中止了，则金丝雀 ReplicaSet 将动态缩小，因为流量转移到了稳定的 ReplicaSet。如果你希望在中止时保留金丝雀 ReplicaSet 的缩放比例，则可以设置abortScaleDownDelaySeconds的显式值：\nspec: strategy: canary: dynamicStableScale: true abortScaleDownDelaySeconds: 600 模仿滚动更新 如果省略steps字段，则金丝雀策略将模仿滚动更新行为。与部署类似，金丝雀策略具有maxSurge和maxUnavailable字段，以配置 Rollout 应如何向新版本推进。\n其他可配置特性 以下是将修改金丝雀策略行为的可选字段：\nspec: strategy: canary: analysis: object antiAffinity: object canaryService: string stableService: string maxSurge: stringOrInt maxUnavailable: stringOrInt trafficRouting: object analysis 配置在 Rollout 期间执行的后台分析。如果分析不成功，则 Rollout 将中止。\n默认为 nil\nantiAffinity 有关更多信息，请查看 Anti Affinity 文档。\n默认为 nil\ncanaryService canaryService引用将被修改为仅将流量发送到金丝雀 ReplicaSet 的 Service。这使用户只能击中金丝雀 ReplicaSet。\n默认为空字符串\nstableService stableService是一个选择具有稳定版本的 Pod，并且不会选择任何具有金丝雀版本的 Pod 的 Service 的名称。这使用户只能击中稳定的 ReplicaSet。\n默认为空字符串\nmaxSurge maxSurge 定义了升级过程中可以创建的最大副本数，以达到最后一次 setWeight 设置的正确比例。Max Surge 可以是整数或百分比字符串（例如 “20%\u0026#34;）。\n默认为 “25%\u0026#34;。\nmaxUnavailable 升级期间可以不可用的 Pod 的最大数量。值可以是绝对数（例如：5）或所需 Pod 的百分比（例如：10%）。如果 MaxSurge 为 0，则不能为 0。\n默认为 “25%\u0026#34;。\ntrafficRouting 流量管理 规则，用于控制活动和金丝雀版本之间的流量。如果未设置，则将使用默认的基于加权 Pod 副本的路由。\n默认为 nil。\n","relpermalink":"/book/argo-rollouts/rollout/deployment-strategies/canary/","summary":"金丝雀发布是一种部署策略，操作员将新版本的应用程序释放到生产流量的一小部分中。 概述 由于没有关于金丝雀部署的共识标准，因此 Rollouts Controller 允许用户概述他们想要运行其金丝雀部署的方式。用户可以定义一个控制器使用的步骤","title":"金丝雀部署"},{"content":" rollouts rollouts abort rollouts completion rollouts create rollouts create analysisrun rollouts dashboard rollouts get rollouts get experiment rollouts get rollout rollouts lint rollouts list rollouts list experiments rollouts list rollouts rollouts notifications rollouts notifications template rollouts notifications template get rollouts notifications template notify rollouts notifications trigger rollouts notifications trigger get rollouts notifications trigger run rollouts pause rollouts promote rollouts restart rollouts retry rollouts retry experiment rollouts retry rollout rollouts set rollouts set image rollouts status rollouts terminate rollouts terminate analysisrun rollouts terminate experiment rollouts undo rollouts version ","relpermalink":"/book/argo-rollouts/kubectl-plugin/command/","summary":"rollouts rollouts abort rollouts completion rollouts create rollouts create analysisrun rollouts dashboard rollouts get rollouts get experiment rollouts get rollout rollouts lint rollouts list rollouts list experiments rollouts list rollouts rollouts notifications rollouts notifications template rollouts notifications template get rollouts notifications template notify rollouts notifications trigger rollouts notifications trigger get rollouts notifications trigger run rollouts pause rollouts promote rollouts restart rollouts retry rollouts retry experiment rollouts retry rollout rollouts set rollouts set image rollouts status rollouts terminate rollouts terminate analysisrun rollouts terminate experiment rollouts undo rollouts version","title":"kubectl argo rollouts 命令用法"},{"content":"本章介绍了 SPIFFE 的动机和它是如何诞生的。\n压倒性的动机和需要 我们到达今天的位置，首先要经历一些成长的痛苦。\n当互联网在 1981 年首次广泛使用时，它只有 213 个不同的服务器，而安全问题几乎没有被考虑到 。随着互联计算机数量的增加，安全问题仍然是一个弱点：容易被利用的漏洞导致了大规模的攻击，如莫里斯蠕虫病毒 ，它在 1988 年占领了互联网上的大多数 Unix 服务器，或 Slammer 蠕虫病毒 ，它在 2003 年在数十万台 Windows 服务器上传播。\n随着时间的推移，过去的传统周边防御模式已经不能很好地适应不断发展的计算架构和现代技术的边界。各种解决方案和技术层出不穷，以掩盖基础网络安全概念未能跟上现代化趋势而出现的越来越大的裂缝。\n那么，为什么周边模式如此普遍，怎样才能解决这些缺陷？\n多年来，我们观察到三大的趋势，突出了传统的周边模式对网络未来的限制：\n软件不再在组织控制的单个服务器上运行。自 2015 年以来，新的软件通常被构建为微服务的集合，可以单独扩展或转移到云主机供应商。如果你不能在需要安全的服务周围画出一条精确的线，就不可能在它们周围筑起一道墙。 你不能相信一切，即使是公司内的软件。曾经，我们认为软件漏洞就像苍蝇，我们可以单独拍打；现在，它们似乎更像一群蜜蜂。平均而言，国家漏洞数据库每年报告超过 15000 个新的软件漏洞 。如果你编写或购买了一个软件，它在某些时候可能会有一些漏洞。 你也不能相信人，他们会犯错，会心烦意乱，再加上他们可以完全接触到内部服务。首先，每年有数以万计的攻击是基于网络钓鱼 或窃取有效的员工凭证。其次，随着云计算应用和移动工作队伍的出现，员工可以合法地从许多不同的网络访问资源。当人们为了工作而不得不不断地来回穿越这堵墙时，建造一堵墙就不再有意义了。正如你所看到的，对于当今的组织来说，周界安全不再是一个现实的解决方案。当周界安全被严格执行时，阻碍了组织使用微服务和云；当周界安全松懈时，它给了入侵者可乘之机。2004 年，Jericho 论坛认识到需要一个周界安全的继承者。十年后的 2014 年，谷歌发布了一个关于 BeyondCorp 安全架构的案例研究 。然而，两者都没有被广泛的采用。 网络曾经是友好的，只要我们保持自律 最初的互联网使用案例集中在学术界，目的是分享信息，而不是组织意外访问。当其他组织开始将联网的计算机系统用于商业敏感的场景时，他们在很大程度上依赖于物理周界和物理证明，以保证访问网络的个人得到授权。当时还不存在受信任的内部威胁的概念。随着网络从学术用途发展到商业用途，软件从单体发展到微服务，安全成为增长的障碍。\n最初，通过防火墙、网络分段、私有地址空间和 ACL 来模拟墙和警卫来保护计算机的物理访问的传统方法。这在当时是有意义的，特别是考虑到需要控制的点的数量有限。\n随着网络的扩散，以及用户和商业伙伴访问点的增加，通过安全地交换和管理密钥、凭证和令牌，物理身份验证（常见于墙和警卫）变得虚拟化，随着技术和需求的发展，所有这些都变得越来越有问题。\n采用公共云 从传统的内部部署和数据中心运作迁移到公共云，放大了现有问题。\n随着人们获得了在云中创建计算资源的自由，企业内的开发团队和运维团队开始更紧密地合作，并围绕着专注于软件自动化部署和管理的 DevOps 概念形成新的团队。公共云快速发展的动态环境使团队能够更频繁地进行部署 —— 从每几个月部署一次，到每天部署多次。按需配置和部署资源的能力使人们能够高速创建专门的、有针对性的、可独立部署的服务套件，其责任范围较小，俗称为微服务。这反过来又增加了跨部署集群识别和访问服务的需要。\n这种高度动态和弹性的环境打破了公认的边界安全概念，需要更好的服务水平互动，与基础网络无关。传统的边界执行使用 IP 和端口进行认证、授权和审计，在云计算范式下，这不再是干净地映射到工作负载。\n公共云的参与所激发的模式，如 API 网关或用于多服务工作负载的管理的负载均衡器，强调了对不依赖网络拓扑或路径的身份的需求。保护这些服务间的通信的完整性变得更加重要，特别是对于需要跨工作负载统一性的团队。\n休斯顿，我们有一个问题 随着企业采用新技术，如容器、微服务、云计算和无服务器功能，有一个趋势是明确的：更多更小的软件。这既增加了攻击者可以利用的潜在漏洞的数量，也使得管理周边防御越来越不现实。\n这种趋势正在加快，这意味着越来越多的组件被部署在自动化基础设施上，往往牺牲了安全和保障。绕过手动流程，如防火墙规则票或安全组的变化，并非闻所未闻。在这个新的现代世界里，无论部署环境如何，面向网络的访问控制都会迅速过时，需要不断维护。\n这些规则和例外情况的管理可以自动化，然而，它们需要迅速发生，这在较大的基础设施中可能是一个挑战。此外，网络拓扑结构，如网络地址转换（NAT），会使其变得困难。随着基础设施变得越来越大，越来越动态，依靠人力的系统根本无法扩展。毕竟，没有人愿意花钱请一个整天玩弄防火墙规则的团队，而他们仍然无法跟上需求的进度。\n依靠特定地点的细节，如服务器名称、DNS 名称、网络接口细节，在一个动态调度和弹性扩展的应用世界里有几个缺点。虽然网络结构的使用很普遍，但该模型是软件身份的一个无效的模拟。在应用层，使用传统的用户名和密码组合或其他硬编码的凭证，可以赋予一定程度的身份，但更多的是处理授权而不是认证。\n在软件开发生命周期的早期整合安全并引入反馈，使开发人员能够对其工作负载的识别和信息交换机制进行更多的操作控制。有了这种变化，授权政策的决定可以委托给个别服务或产品所有者，他们最适合做出与有关组件相关的决定。\n重新认识访问控制 在采用公有云之前，企业所经历的问题不断增加，而采用公有云后，企业又痛苦地出现了问题，这就推动了传统的周界是不够的，需要有更好的解决方案的概念。最终，去边界化意味着企业需要弄清楚如何识别他们的软件并实现服务对服务的访问控制。\n解决方案之一：秘密\n像密码和 API 密钥这样的共享秘密为分布式系统的访问控制提供了一个简单的选择。但这种解决方案也带来了许多问题。密码和 API 密钥很容易被泄露（试着在 GitHub 上搜索 “client_secret” 这样的短语，看看会发生什么）。在大型组织中，秘密可能很难因泄露而轮换，因为每个服务都需要以一种协调的方式意识到变化（而错过一个服务可能会导致停工）。\n诸如 HashiCorp Vault 这样的工具和秘密存储库已经被开发出来，以帮助缓解秘密管理和生命周期的困难。虽然也有许多其他工具试图解决这个问题，但它们往往提供了一个更加有限的解决方案，效果平平（见 Secrets at Scale: Automated Bootstrapping of Secrets \u0026amp; Identity in the Cloud ）。有了所有这些选择，我们最终还是回到了同一个问题上：工作负载应该如何获得对这个秘密库的访问？仍然需要一些 API 密钥、密码或其他秘密。\n所有这些解决方案最终都会出现“乌龟下面还有一只乌龟\u0026#34; 的问题。\n启用对资源的访问控制，如数据库或其他资源。 服务需要一个秘密，如 API 密钥或密码。 该钥匙或密码需要被保护，所以你可以保护它，比如说用加密。但是，你仍然担心密码的解密密钥。 该解密密钥可以被放入秘密库，但这样你仍然需要一些凭证，如密码或 API 密钥来访问秘密库。 最终，为了保护对秘密的访问会产生需要保护的一个新秘密。 为了打破这个循环，我们需要找到一个底层乌龟，也就是说，一些秘密提供了对我们所需的认证和访问控制的其他秘密的访问。一种选择是在服务部署时手动提供秘密。然而，这在高度动态的生态系统中是无法扩展的。随着企业转向具有快速部署管道和自动扩展资源的云计算，随着新计算单元的创建，手动配置秘密变得不可行。而当一个秘密被破坏时，使旧的凭证失效会带来使整个系统崩溃的风险。\n在应用程序中嵌入一个秘密，这样它就不需要手动配置了，其安全属性甚至更差。嵌入到源代码中的秘密有一个习惯，那就是出现在公共存储库中（你试过我建议的 GitHub 搜索吗）。虽然秘密可以在构建时被嵌入到机器镜像中，但这些镜像最终还是会被意外地推送到公共镜像库中，或者作为杀毒链的第二步从内部镜像库中提取。\n我们希望有一种解决方案，不包括长期存在的秘密（这些秘密很容易被破坏，也很难轮换），也不需要人工向工作负载提供秘密。为了实现这一点，无论是在硬件还是在云提供商，都必须有一个信任的根，在此基础上建立一个以软件（工作负载）身份为中心的自动化解决方案。然后，这种身份构成了所有需要认证和授权的互动的基础。为了避免产生另一个底层乌龟，工作负载需要能够在没有秘密或其他凭证的情况下获得这一身份。\n迈向未来\n自 2010 年以来，业界为解决软件身份问题进行了多种努力。谷歌的低开销认证服务（LOAS），后来被命名为应用层传输安全（ALTS） ，建立了一个新的身份格式和有线协议，用于从运行时环境接收软件身份，并将其应用于所有网络通信。它被称为拨号音安全（dial tone security）。\n在另一个例子中，Netflix 的内部开发的解决方案（代号为 Metatron ）通过利用云 API 来证明实例上运行的机器镜像，并通过 CI/CD 集成来产生机器镜像和代码身份之间的加密绑定，从而在每个实例基础上建立软件身份。这种软件身份采取 X.509 证书的形式，对服务间的通信进行相互认证，包括访问作为该解决方案一部分开发的秘密服务，在此基础上实现秘密管理。\n业界的其他一些努力，包括来自 Facebook 等公司的努力 ，证明了对这样一个系统的需要，并强调了实施的难度。\n普适安全生产身份框架（SPIFFE）的愿景\n在成立通用的解决方案之前，需要建立一个框架编码原则。Kubernetes 的创始工程师 Joe Beda 在过去的工作中接触过各种技术，这些技术使工程团队的生活更加轻松，他在 2016 年开始呼吁为生产身份创建一个解决方案，作为一个专门的解决方案，旨在以一种通用的方式解决问题，可以在许多不同类型的系统中利用，而不是以艰难的方式进行 PKI 的中间解决方案。这种公司之间的大规模合作努力，为基于 PKI 的服务身份开发了一个新的标准，这就是 SPIFFE 的开始。\n2016 年 Beda 在 GlueCon 上发表的论文 展示了一个具有这些参数的难题：\n通过利用基于内核的自省来获得关于调用者的信息，而无需调用者出示凭证，从而解决零号秘密的问题。 使用 X.509，因为大多数软件已经兼容，而且 有效地将身份的概念从网络定位器中剥离出来。 在引入 SPIFFE 概念之后，Netflix 服务认证方面的专家举行了一次会议，讨论原始 SPIFFE 提案的最终形态和可行性。许多成员已经实施了、继续改进并重新解决了工作负载识别问题，突出了社区合作的机会。参加会议的成员希望获得彼此之间的互操作性。这些专家意识到他们已经实施了类似的解决方案来解决同样的问题，可以建立一个共同的标准。\n解决工作负载身份问题的最初目标是建立一个开放的规范和相应的生产实现。该框架需要在不同的实现和现成的软件之间提供互操作性，其核心是在一个不信任的环境中建立信任的根基，驱除隐性信任。最后，摆脱以网络为中心的身份，以实现灵活性和更好的扩展特性。\n","relpermalink":"/book/spiffe/history-and-motivation-for-spiffe/","summary":"本章介绍了 SPIFFE 的动机和它是如何诞生的。","title":"SPIFFE 的历史和动机"},{"content":"标签 标签（Label）是一种通用的、灵活的和高度可扩展的方式，可以用来处理大量资源，因为我们可以用它来对事物任意分组和创建集合。每当需要描述、解决或选择某物时，它都是基于标签完成的：\n端点 被分配了从容器运行时、编排系统或其他来源派生的标签。 网络策略 根据标签选择允许通信的 端点对 ，策略本身也由标签标识。 什么是标签？ 标签是一对由 key 和 value 组成的字符串。可以将标签格式化为具有 key=value 的单个字符串。key 部分是强制性的，并且必须是唯一的。这通常是通过使用反向域名概念来实现的，例如 io.cilium.mykey=myvalue。value 部分是可选的，可以省略，例如 io.cilium.mykey.\n键名通常应由字符集组成 [a-z0-9-.]。\n当使用标签选择资源时，键和值都必须匹配，例如，当一个策略应该应用于所有带有标签 my.corp.foo 的端点时，标签 my.corp.foo=bar 不会与该选择器匹配。\n标签来源 标签可以来自各种来源。例如，端点 将通过本地容器运行时派生与容器关联的标签，以及与 Kubernetes 提供的 pod 关联的标签。由于这两个标签命名空间彼此不知道，这可能会导致标签键冲突。\n为了解决这种潜在的冲突，Cilium 在导入标签时为所有标签键添加前缀 source: 以指示标签的来源，例如 k8s:role=frontend、container:user=joe、k8s:role=backend。这意味着当您使用 docker run [...] -l foo=bar 运行 Docker 容器时，标签 container:foo=bar 将出现在代表容器的 Cilium 端点上。类似地，以 foo: bar 标签启动的 Kubernetes pod 将由与标签关联的 Cilium 端点表示。为每个潜在来源分配一个唯一名称。当前支持以下标签源：\ncontainer: 对于从本地容器运行时派生的标签 k8s: 对于从 Kubernetes 派生的标签 reserved: 有关特殊保留标签，请参阅 特殊标识 。 unspec: 对于未指定来源的标签 当使用标签来识别其他资源时，可以包含源以将标签匹配限制为特定类型。如果未提供源，则标签源默认为 any:，将匹配所有标签，无论其来源如何。如果提供了来源，则选择和匹配标签的来源需要匹配。\n端点 Cilium 通过分配 IP 地址使应用程序容器在网络上可用。多个应用容器可以共享同一个 IP 地址；此模型的一个典型示例是 Kubernetes Pod。所有共享公共地址的应用程序容器都在 Cilium 所指的端点中分组在一起。\n分配单独的 IP 地址允许每个端点使用整个四层端口范围。这实质上允许在同一个集群节点上运行的多个应用程序容器都绑定到众所周知的端口，例如 80 不会引起任何冲突。\nCilium 的默认行为是为每个端点分配 IPv6 和 IPv4 地址。但是，可以将此行为配置为仅使用该 --enable-ipv4=false 选项分配 IPv6 地址。如果同时分配了 IPv6 和 IPv4 地址，则任一地址都可用于到达端点。相同的行为将适用于策略规则、负载均衡等。\n身份识别 出于识别目的，Cilium 为集群节点上的所有端点分配一个内部端点 ID。端点 ID 在单个集群节点的上下文中是唯一的。\n端点元数据 端点自动从与端点关联的应用程序容器中派生元数据。然后可以使用元数据来识别端点，以实现安全 / 策略、负载均衡和路由目的。\n元数据的来源取决于使用的编排系统和容器运行时。当前支持以下元数据检索机制：\n系统 描述 Kubernetes Pod 标签（通过 Kubernetes API） containerd（Docker） 容器标签（通过 Docker API） 元数据以 标签 的形式附加到端点。\n以下示例启动一个带有标签的容器，该标签 app=benchmark 随后与端点相关联。标签带有前缀， container: 表示标签是从容器运行时派生的。\n$ docker run --net cilium -d -l app=benchmark tgraf/netperf aaff7190f47d071325e7af06577f672beff64ccc91d2b53c42262635c063cf1c $ cilium endpoint list ENDPOINT POLICY IDENTITY LABELS (source:key [=value]) IPv6 IPv4 STATUS ENFORCEMENT 62006 Disabled 257 container:app=benchmark f00d::a00:20f:0:f236 10.15.116.202 ready 一个端点可以有来自多个源的元数据。例如使用 containerd 作为容器运行时的 Kubernetes 集群。端点将派生 Kubernetes pod 标签（以k8s:源前缀为前缀）和容器标签（以container: 源前缀为前缀）。\n身份 所有 端点 都分配了一个身份。身份用于端点之间的基本连接。在传统的网络术语中，这运行在三层。\n身份由 标签 标识，并被赋予一个集群范围的唯一标识符。端点被分配与端点的 安全相关标签 匹配的身份，即共享同一组 安全相关标签 的所有端点将共享相同的身份。此概念允许将策略实施扩展到大量端点，因为随着应用程序的扩展，许多单独的端点通常会共享同一组安全 标签 。\n什么是身份？ 端点的身份是基于与派生到 端点 的 pod 或容器关联的 标签 派生的。当一个 pod 或容器启动时，Cilium 会根据容器运行时收到的事件创建一个 端点 来代表网络上的 pod 或容器。下一步，Cilium 将解析 端点 创建的身份。每当 Pod 或容器的 标签 发生变化时，都会重新确认身份并根据需要自动修改。\n安全相关标签 在派生 身份 时，并非所有与容器或 pod 关联的 标签 都有意义。标签可用于存储元数据，例如容器启动时的时间戳。Cilium 需要知道哪些标签是有意义的，知道在推导身份时需要考虑哪些标签。为此，用户需要指定有意义标签的字符串前缀列表。标准行为是包含所有以 id 为前缀开头的标签，例如，id.service1、id.service2、id.groupA.service44。启动代理时可以指定有意义的标签前缀列表。\n特殊身份 Cilium 管理的所有端点都将被分配一个身份。为了允许与不由 Cilium 管理的网络端点进行通信，存在特殊的身份来表示它们。特殊保留标识以 reserved: 字符串为前缀。\n身份 数字 ID 描述 reserved:unknown 0 无法推导出身份。 reserved:host 1 本地主机。源自或指定到本地主机 IP 之一的任何流量。 reserved:world 2 集群外的任何网络端点。 reserved:unmanaged 3 不受 Cilium 管理的端点，例如在安装 Cilium 之前启动的 Kubernetes pod。 reserved:health 4 这是 Cilium 代理生成的健康检查流量。 reserved:init 5 尚未解析身份的端点被分配了初始身份。这代表了一个端点的阶段，在该阶段中，派生安全身份所需的一些元数据仍然缺失。这通常是引导阶段的情况。仅当端点的标签在创建时未知时才分配初始化标识。Docker 插件可能就是这种情况。 reserved:remote-node 6 所有远程集群主机的集合。源自或指定到任何连接集群中任何主机的 IP 之一的任何流量，而不是本地节点。 reserved:kube-apiserver 7 具有为 kube-apiserver 运行的后端的远程节点。 提示 Cilium 曾经在 reserved:host 身份中同时包含本地和所有远程主机。除非使用最近的默认 ConfigMap，否则这仍然是默认选项。可以通过 enable-remote-node-identity 选项启用远程节点身份。 知名身份 以下是 Cilium 自动识别的知名身份列表，Cilium 将分发安全身份，而无需联系任何外部依赖项，例如 kvstore。这样做的目的是允许引导 Cilium 并通过集群中的策略强制实现网络连接，以实现基本服务，而无需任何依赖项。\n部署 命名空间 服务账户 集群名称 数字 ID 标签 kube-dns kube-system kube-dns cilium-cluster 102 k8s-app=kube-dns kube-dns（EKS） kube-system kube-dns cilium-cluster 103 k8s-app=kube-dns,eks.amazonaws.com/component=kube-dns core-dns kube-system coredns cilium-cluster 104 k8s-app=kube-dns core-dns（EKS） kube-system coredns cilium-cluster 106 k8s-app=kube-dns,eks.amazonaws.com/component=coredns cilium-operator cilium-namspace cilium-operator cilium-cluster 105 name=cilium-operator,io.cilium/app=operator 如果 cilium-cluster 未定义该 cluster-name 选项，则默认值将设置为 default。 集群中的身份管理 身份在整个集群中都是有效的，这意味着如果在多个集群节点上启动了多个 pod 或容器，如果它们共享身份相关标签，那么它们都将解析并共享同一个身份。这需要集群节点之间的协调。\n集群中的身份管理示意图 解析端点身份的操作是在分布式键值存储的帮助下执行的，如果之前没有看到以下值，则允许以生成新的唯一标识符的形式执行原子操作。这允许每个集群节点创建与身份相关的标签子集，然后查询键值存储以派生身份。根据之前是否查询过这组标签，要么创建一个新的身份，要么返回初始查询的身份。\n节点 Cilium 将节点称为集群的单个成员。每个节点都必须运行 cilium-agent 并且将以自主的方式运行。为了简单和规模化，在不同节点上运行的 Cilium 代理之间的状态同步保持在最低限度。它仅通过键值存储或数据包元数据发生。\n节点地址 Cilium 会自动检测节点的 IPv4 和 IPv6 地址。当 cilium-agent 启动时打印出检测到的节点地址：\nLocal node-name: worker0 Node-IPv6: f00d::ac10:14:0:1 External-Node IPv4: 172.16.0.20 Internal-Node IPv4: 10.200.28.238 ","relpermalink":"/book/cilium-handbook/concepts/terminology/","summary":"标签 标签（Label）是一种通用的、灵活的和高度可扩展的方式，可以用来处理大量资源，因为我们可以用它来对事物任意分组和创建集合。每当需要描述、解决或选择某物时，它都是基于标签完成的： 端点 被分配了从容器","title":"Cilium 术语说明"},{"content":"IP 地址管理（IPAM）负责分配和管理由 Cilium 管理的网络端点（容器和其他）使用的 IP 地址。Cilium 支持以下各种 IPAM 模式，以满足不同用户的需求。\n集群范围（默认） Kubernetes 主机范围 Azure IPAM AWS ENI Google Kubernetes Engine CRD 支持 集群范围 集群范围 IPAM 模式将 PodCIDR 分配给每个节点，并使用每个节点上的主机范围分配器分配 IP。因此它类似于 Kubernetes 主机范围 模式。区别在于 Kubernetes 不是通过 Kubernetes v1.Node资源分配每个节点的 PodCIDR，而是 Cilium Operator 通过 v2.CiliumNode 资源管理每个节点的 PodCIDR。这种模式的优点是它不依赖于 Kubernetes 被配置为分发每个节点的 PodCIDR。\n架构 如果无法将 Kubernetes 配置为分发 PodCIDR 或需要更多控制，这将非常有用。\n在这种模式下，Cilium 代理将在启动时等待，直到 PodCIDRs 范围通过 Cilium 节点 v2.CiliumNode 对象，通过 v2.CiliumNode 中设置的资源字段为所有启用的地址族提供：\n字段 描述 Spec.IPAM.PodCIDRs IPv4 和/或 IPv6 PodCIDR 范围 集群范围配置 有关如何在 Cilium 中启用此模式的实用教程，请参阅 由 Cilium 集群池 IPAM 支持的 CRD 。\n故障排除 查找分配错误 检查 Error 字段中的 Status.Operator 字段：\nkubectl get ciliumnodes -o jsonpath=\u0026#39;{range .items[*]}{.metadata.name}{\u0026#34;\\t\u0026#34;}{.status.operator.error}{\u0026#34;\\n\u0026#34;}{end Kubernetes 主机范围 Kubernetes 主机范围 IPAM 模式启用 ipam: kubernetes 并将地址分配委托给集群中的每个单独节点。Kubernetes 分配的 IP 超出了与每个节点关联的 PodCIDR 范围。\nKubernetes 主机范围模式 IPAM 架构图 在这种模式下，Cilium 代理将在启动时等待，直到 PodCIDR 范围通过 Kubernetes v1.Node 对象通过以下方法之一为所有启用的地址家族提供：\n通过 v1.Node 资源字段\n字段 描述 spec.podCIDRs IPv4 和 / 或 IPv6 PodCIDR 范围 spec.podCIDR IPv4 或 IPv6 PodCIDR 范围 提示 kube-controller-manager 使用 --allocate-node-cidrs 标志运行 kube-controller-manager 以指示 Kubernetes 应该分配的 PodCIDR 范围。 通过 v1.Node 注释\n注解 描述 io.cilium.network.ipv4-pod-cidr IPv4 PodCIDR 范围 io.cilium.network.ipv6-pod-cidr IPv6 PodCIDR 范围 io.cilium.network.ipv4-cilium-host cilium 主机接口的 IPv4 地址 io.cilium.network.ipv6-cilium-host cilium 主机接口的 IPv6 地址 io.cilium.network.ipv4-health-ip cilium-health 端点的 IPv4 地址 io.cilium.network.ipv6-health-ip cilium-health 端点的 IPv6 地址 提示 基于注解的机制主要与旧的 Kubernetes 版本结合使用，这些版本尚不支持spec.podCIDRs但同时支持 IPv4 和 IPv6。 主机范围配置 存在以下 ConfigMap 选项来配置 Kubernetes 主机范围：\nipam: kubernetes：启用 Kubernetes IPAM 模式。启用此选项将自动启用k8s-require-ipv4-pod-cidr 如何 enable-ipv4 是 true 和 k8s-require-ipv6-pod-cidr 如何 enable-ipv6 是 true。 k8s-require-ipv4-pod-cidr: true：指示 Cilium 代理等待，直到 IPv4 PodCIDR 通过 Kubernetes 节点资源可用。 k8s-require-ipv6-pod-cidr: true：指示 Cilium 代理等待，直到 IPv6 PodCIDR 通过 Kubernetes 节点资源可用。 使用 helm 之前的选项可以定义为：\nipam: kubernetes：--set ipam.mode=kubernetes k8s-require-ipv4-pod-cidr: true：--set k8s.requireIPv4PodCIDR=true，仅适用于 --set ipam.mode=kubernetes k8s-require-ipv6-pod-cidr: true：--set k8s.requireIPv6PodCIDR=true，仅适用于 --set ipam.mode=kubernetes CRD 支持 CRD 支持的 IPAM 模式提供了一个可扩展的接口，以通过 Kubernetes 自定义资源定义 (CRD) 控制 IP 地址管理。这允许将 IPAM 委托给外部运营商或使其用户可配置每个节点。\n架构 启用此模式后，每个 Cilium 代理将开始监视 ciliumnodes.cilium.io名称与运行代理的 Kubernetes 节点匹配的 Kubernetes 自定义资源。\n每当更新自定义资源时，每个节点的分配池都会更新为 spec.ipam.available 字段中列出的所有地址。移除当前分配的 IP 后，该 IP 将继续使用，但在释放后无法重新分配。\n在分配池中分配 IP 后，将 IP 添加到 status.ipam.inuse 字段中。\n提示 节点状态更新被限制为最多每 15 秒运行一次。因此，如果同时调度多个 Pod，状态部分的更新可能会滞后。 CRD 支持配置 通过在 cilium-config ConfigMap 中设置 ipam: crd 或指定选项 --ipam=crd，可以启用 CRD 支持的 IPAM 模式。启用后，代理将等待与 Kubernetes 节点名称相匹配的 CiliumNode 自定义资源变得可用，并且至少有一个 IP 地址被列为可用。当连接性健康检查被启用时，必须有至少两个 IP 地址可用。\n在等待期间，代理将打印以下日志消息：\nWaiting for initial IP to become available in \u0026#39;\u0026lt;node-name\u0026gt;\u0026#39; custom resource 有关如何使用 Cilium 启用 CRD IPAM 模式的实用教程，请参阅 CRD 支持的 IPAM 部分。\n权限 为了使自定义资源发挥作用，需要以下额外权限。使用标准 Cilium 部署工件时会自动授予这些权限：\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cilium rules: - apiGroups: - cilium.io resources: - ciliumnodes - ciliumnodes/status verbs: - \u0026#39;*\u0026#39; CRD 定义 CilumNode 自定义资源以标准 Kubernetes 资源为模型，分为一个 spec 和 status 部分：\ntype CiliumNode struct { [...] // Spec is the specification of the node Spec NodeSpec `json:\u0026#34;spec\u0026#34;` // Status it the status of the node Status NodeStatus `json:\u0026#34;status\u0026#34;` } IPAM 规范 该 spec 部分嵌入了一个 IPAM 特定字段，该字段允许定义节点可用于分配的所有 IP 的列表：\n// AllocationMap is a map of allocated IPs indexed by IP type AllocationMap map[string]AllocationIP // NodeSpec is the configuration specific to a node type NodeSpec struct { // [...] // IPAM is the address management specification. This section can be // populated by a user or it can be automatically populated by an IPAM // operator // // +optional IPAM IPAMSpec `json:\u0026#34;ipam,omitempty\u0026#34;` } // IPAMSpec is the IPAM specification of the node type IPAMSpec struct { // Pool is the list of IPs available to the node for allocation. When // an IP is used, the IP will remain on this list but will be added to // Status.IPAM.InUse // // +optional Pool AllocationMap `json:\u0026#34;pool,omitempty\u0026#34;` } // AllocationIP is an IP available for allocation or already allocated type AllocationIP struct { // Owner is the owner of the IP, this field is set if the IP has been // allocated. It will be set to the pod name or another identifier // representing the usage of the IP // // The owner field is left blank for an entry in Spec.IPAM.Pool // and filled out as the IP is used and also added to // Status.IPAM.InUse. // // +optional Owner string `json:\u0026#34;owner,omitempty\u0026#34;` // Resource is set for both available and allocated IPs, it represents // what resource the IP is associated with, e.g. in combination with // AWS ENI, this will refer to the ID of the ENI // // +optional Resource string `json:\u0026#34;resource,omitempty\u0026#34;` } IPAM 状态 该 status 部分包 …","relpermalink":"/book/cilium-handbook/networking/ipam/","summary":"IP 地址管理（IPAM）负责分配和管理由 Cilium 管理的网络端点（容器和其他）使用的 IP 地址。Cilium 支持以下各种 IPAM 模式，以满足不同用户的需求。 集群范围（默认） Kubernetes 主机范围 Azure IPAM AWS ENI Google Kubernetes Engine CRD 支持 集群范围 集群范围 IPAM","title":"IP 地址管理（IPAM）"},{"content":"部署 标准 Cilium Kubernetes 部署的配置包括几个 Kubernetes 资源：\nDaemonSet 资源：描述部署到每个 Kubernetes 节点的 Cilium pod。这个 pod 运行 cilium-agent 和相关的守护进程。这个 DaemonSet 的配置包括指示 Cilium docker 容器的确切版本（例如 v1.0.0）的镜像标签和传递给 cilium-agent 的命令行选项。 资源：描述传递给 cilium-agent 的ConfigMap 常用配置值，例如 kvstore 端点和凭据、启用/禁用调试模式等。 ServiceAccount、ClusterRole 和 ClusterRoleBindings 资源：当启用 Kubernetes RBAC 时，cilium-agent` 用于访问 Kubernetes API 服务器的身份和权限。 资源：如果 Secret 需要，描述用于访问 etcd kvstore 的凭据。 现有 Pod 的联网 如果在部署 Cilium 之前 pod 已经在运行 DaemonSet ，这些 pod 仍将根据 CNI 配置使用以前的网络插件连接。一个典型的例子是默认运行在 kube-system 命名空间中的 kube-dns 服务。\n改变这种现有 pod 的网络的一个简单方法是依靠 Kubernetes 在 Deployment 中的 pod 被删除时自动重新启动的事实，所以我们可以简单地删除原来的 kube-dns pod，紧接着启动的替换 pod 将由 Cilium 管理网络。在生产部署中，这个步骤可以作为 kube-dns pod 的滚动更新来执行，以避免 DNS 服务的停机。\n$ kubectl --namespace kube-system delete pods -l k8s-app=kube-dns pod \u0026#34;kube-dns-268032401-t57r2\u0026#34; deleted 运行 kubectl get pods 将显示 Kubernetes 启动了一组新的 kube-dns pod，同时终止了旧的 pod：\n$ kubectl --namespace kube-system get pods NAME READY STATUS RESTARTS AGE cilium-5074s 1/1 Running 0 58m kube-addon-manager-minikube 1/1 Running 0 59m kube-dns-268032401-j0vml 3/3 Running 0 9s kube-dns-268032401-t57r2 3/3 Terminating 0 57m 默认允许本地主机的入口流量 Kubernetes 具有通过存活探针和就绪探针 向用户指示其应用程序当前运行状况的功能。为了让 kubelet 对每个 pod 运行这些健康检查，默认情况下，Cilium 将始终允许从本地主机到每个 pod 的所有入口流量。\n","relpermalink":"/book/cilium-handbook/kubernetes/concepts/","summary":"部署 标准 Cilium Kubernetes 部署的配置包括几个 Kubernetes 资源： DaemonSet 资源：描述部署到每个 Kubernetes 节点的 Cilium pod。这个 pod 运行 cilium-agent 和相关的守护进程。这个 DaemonSet 的配置包括指示 Cilium docker 容器的确切版本（例如 v1.0.0）的镜像标签和传递给 cilium-agent 的命令行选项。","title":"概念"},{"content":"Kubernetes 等容器管理系统部署了一个网络模型，该模型为每个 pod（容器组）分配一个单独的 IP 地址。这确保了架构的简单性，避免了不必要的网络地址转换（NAT），并为每个单独的容器提供了全范围的端口号以供使用。这种模型的逻辑结果是，根据集群的大小和 pod 的总数，网络层必须管理大量的 IP 地址。\n在传统上，安全策略基于 IP 地址过滤器。下面是一个简单的例子。如果所有具有标签 role=frontend 的 pod 应该被允许发起与所有具有标签 role=backend 的 pod 的连接，那么每个运行至少一个具有标签 role=backend 的 pod 的集群节点必须安装一个相应的过滤器，允许所有 role=frontend pod 的所有 IP 地址发起与所有本地 role=backend pod 的 IP 地址的连接。所有其他的连接请求都应该被拒绝。这可能看起来像这样。如果目标地址是 10.1.1.2，那么只有当源地址是下列之一时才允许连接 [10.1.2.2,10.1.2.3,20.4.9.1]。\n每次启动或停止带有 role=frontend 或 role=backend 标签的新 pod 时，必须更新运行任何此类 pod 的每个集群节点的规则，从允许的 IP 地址列表中添加或删除相应的 IP 地址。在大型分布式应用中，这可能意味着每秒多次更新数以千计的集群节点，这取决于部署的 pod 的流失率。更糟糕的是，新的 role=frontend pod 的启动必须推迟到所有运行 role=backend pod 的服务器都被更新了新的安全规则之后，否则来自新 pod 的连接尝试可能会被错误地放弃。这使得它难以有效地扩展。\n为了避免这些可能限制扩展性和灵活性的复杂情况，Cilium 将安全与网络寻址分开。相反，安全是基于 pod 的身份，它是通过标签得出的。这个身份可以在 pod 之间共享。这意味着，当第一个 role=frontend pod 启动时，Cilium 会给该 pod 分配一个身份，然后允许它与 role=backend pod 的身份发起连接。随后启动额外的 role=frontend pod 只需要通过键值存储来解决这个身份，不需要在任何承载 role=backend pod 的集群节点上执行任何操作。一个新的 pod 的启动必须只延迟到 pod 的身份被解决，这比更新所有其他集群节点上的安全规则要简单得多。\n基于身份的安全示意图 ","relpermalink":"/book/cilium-handbook/security/identity/","summary":"Kubernetes 等容器管理系统部署了一个网络模型，该模型为每个 pod（容器组）分配一个单独的 IP 地址。这确保了架构的简单性，避免了不必要的网络地址转换（NAT），并为每个单独的容器提供了全范围的端口号以供使用。这种模","title":"基于身份"},{"content":"虽然监控 数据路径状态提供对数据路径状态的自省，但默认情况下它只会提供对三层/四层数据包事件的可视性。如果配置了 七层示例 ，则可以查看七层协议，但这需要编写每个选定端点的完整策略。为了在不配置完整策略的情况下获得对应用程序的更多可视性，Cilium 提供了一种在与 Kubernetes 一起运行时通过注解 来规定可视性的方法。\n可视性信息由注解中以逗号分隔的元组列表表示：\n\u0026lt;{Traffic Direction}/{L4 Port}/{L4 Protocol}/{L7 Protocol}\u0026gt;\n例如：\n\u0026lt;Egress/53/UDP/DNS\u0026gt;,\u0026lt;Egress/80/TCP/HTTP\u0026gt; 为此，你可以在 Kubernetes YAML 中或通过命令行提供注释，例如：\nkubectl annotate pod foo -n bar io.cilium.proxy-visibility=\u0026#34;\u0026lt;Egress/53/UDP/DNS\u0026gt;,\u0026lt;Egress/80/TCP/HTTP\u0026gt;\u0026#34; Cilium 将拾取 pod 已收到这些注释，并将透明地将流量重定向到代理，以便显示 cilium monitor 流量的输出被重定向到代理，例如：\n-\u0026gt; Request http from 1474 ([k8s:id=app2 k8s:io.kubernetes.pod.namespace=default k8s:appSecond=true k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=app2-account k8s:zgroup=testapp]) to 244 ([k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=app1-account k8s:io.kubernetes.pod.namespace=default k8s:zgroup=testapp k8s:id=app1]), identity 30162-\u0026gt;42462, verdict Forwarded GET http://app1-service/ =\u0026gt; 0 -\u0026gt; Response http to 1474 ([k8s:zgroup=testapp k8s:id=app2 k8s:io.kubernetes.pod.namespace=default k8s:appSecond=true k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=app2-account]) from 244 ([k8s:io.cilium.k8s.policy.serviceaccount=app1-account k8s:io.kubernetes.pod.namespace=default k8s:zgroup=testapp k8s:id=app1 k8s:io.cilium.k8s.policy.cluster=default]), identity 30162-\u0026gt;42462, verdict Forwarded GET http://app1-service/ =\u0026gt; 200 您可以通过检查该 pod 的 Cilium 端点来检查可视性策略的状态，例如：\n$ kubectl get cep -n kube-system NAME ENDPOINT ID IDENTITY ID INGRESS ENFORCEMENT EGRESS ENFORCEMENT VISIBILITY POLICY ENDPOINT STATE IPV4 IPV6 coredns-7d7f5b7685-wvzwb 1959 104 false false ready 10.16.75.193 f00d::a10:0:0:2c77 $ $ kubectl annotate pod -n kube-system coredns-7d7f5b7685-wvzwb io.cilium.proxy-visibility=\u0026#34;\u0026lt;Egress/53/UDP/DNS\u0026gt;,\u0026lt;Egress/80/TCP/HTTP\u0026gt;\u0026#34; --overwrite pod/coredns-7d7f5b7685-wvzwb annotated $ $ kubectl get cep -n kube-system NAME ENDPOINT ID IDENTITY ID INGRESS ENFORCEMENT EGRESS ENFORCEMENT VISIBILITY POLICY ENDPOINT STATE IPV4 IPV6 coredns-7d7f5b7685-wvzwb 1959 104 false false OK ready 10.16.75.193 f00d::a10:0:0:2c7 故障排查 如果七层可视性未出现在 cilium monitor 或 Hubble 组件中，则值得仔细检查：\n没有在注解中指定的方向应用强制策略 CiliumEndpoint 中的“可视性策略”列显示 OK。如果为空，则未配置注解；如果显示错误，则可视性注解存在问题。 以下示例故意错误配置注解，以证明当可视性注解无法实现时，pod 的 CiliumEndpoint 会出现错误：\n$ kubectl annotate pod -n kube-system coredns-7d7f5b7685-wvzwb io.cilium.proxy-visibility=\u0026#34;\u0026lt;Ingress/53/UDP/DNS\u0026gt;,\u0026lt;Egress/80/TCP/HTTP\u0026gt;\u0026#34; pod/coredns-7d7f5b7685-wvzwb annotated $ $ kubectl get cep -n kube-system NAME ENDPOINT ID IDENTITY ID INGRESS ENFORCEMENT EGRESS ENFORCEMENT VISIBILITY POLICY ENDPOINT STATE IPV4 IPV6 coredns-7d7f5b7685-wvzwb 1959 104 false false dns not allowed with direction Ingress ready 10.16.75.193 f00d::a10:0:0:2c77 限制 如果导入的规则选择了被注解的 pod，则可视性注解将不适用。 DNS 可视性仅在 egress 上可用。 不支持 Proxylib 解析器，包括 Kafka。要获得对这些协议的可视性，你必须创建一个允许所有七层流量的网络策略，方法是遵循 七层示例 （Kafka ）或 Envoy proxylib 扩展指南。此限制见 GitHub Issue 14072 。 ","relpermalink":"/book/cilium-handbook/policy/visibility/","summary":"虽然监控 数据路径状态提供对数据路径状态的自省，但默认情况下它只会提供对三层/四层数据包事件的可视性。如果配置了 七层示例 ，则可以查看七层协议，但这需要编写每个选定端点的完整策略。为了在不配置完整策略的情","title":"七层可视性"},{"content":"端点到端点 首先，我们使用可选的七层出口和入口策略显示本地端点到端点的流程。随后是启用了套接字层强制的同一端点到端点流。为 TCP 流量启用套接字层实施后，启动连接的握手将遍历端点策略对象，直到 TCP 状态为 ESTABLISHED。然后在建立连接后，只需要七层策略对象。\n端点到出口 接下来，我们使用可选的 overlay 网络显示本地端点到出口。在可选的覆盖网络中，网络流量被转发到与 overlay 网络对应的 Linux 网络接口。在默认情况下，overlay 接口名为 cilium_vxlan。与上面类似，当启用套接字层强制并使用七层代理时，我们可以避免在端点和 TCP 流量的七层策略之间运行端点策略块。如果启用，可选的 L3 加密块将加密数据包。\n端点到出口的流程 入口到端点 最后，我们还使用可选的 overlay 网络显示到本地端点的入口。与上述套接字层强制类似，可用于避免代理和端点套接字之间的一组策略遍历。如果数据包在接收时被加密，则首先将其解密，然后通过正常流程进行处理。\n入口到端点流程 这样就完成了数据路径概述。更多 BPF 细节可以在 BPF 和 XDP 参考指南 中找到。\n","relpermalink":"/book/cilium-handbook/ebpf/lifeofapacket/","summary":"端点到端点 首先，我们使用可选的七层出口和入口策略显示本地端点到端点的流程。随后是启用了套接字层强制的同一端点到端点流。为 TCP 流量启用套接字层实施后，启动连接的握手将遍历端点策略对象，直到 TCP 状态为 ESTA","title":"数据包流程"},{"content":"现在我们将探索云原生应用架构的几个主要特征，和这些特征是如何解决我们前面提到的使用云原生应用架构的动机。\n12 因素应用 12 因素应用是一系列云原生应用架构的模式集合，最初由 Heroku 提出。这些模式可以用来说明什么样的应用才是云原生应用。它们关注速度、安全、通过声明式配置扩展、可横向扩展的无状态 / 无共享进程以及部署环境的整体松耦合。如 Cloud Foundry、Heroku 和 Amazon ElasticBeanstalk 都对部署 12 因素应用进行了专门的优化。\n在 12 因素的背景下，应用（或者叫 app）指的是独立可部署单元。组织中经常把一些互相协作的可部署单元称作一个应用。\n12 因素应用遵循以下模式：\n代码库\n每个可部署 app 在版本控制系统中都有一个独立的代码库，可以在不同的环境中部署多个实例。\n依赖\nApp 应该使用适当的工具（如 Maven、Bundler、NPM）来对依赖进行显式的声明，而不该在部署环境中隐式的实现依赖。\n配置\n配置或其他随发布环境（如部署、staging、生产）而变更的部分应当作为操作系统级的环境变量注入。\n后端服务\n后端服务，例如数据库、消息代理应视为附加资源，并在所有环境中同等看待。\n编译、发布、运行\n构建一个可部署的 app 组件并将它与配置绑定，根据这个组件 / 配置的组合来启动一个或者多个进程，这两个阶段是严格分离的。\n进程\n该 app 执行一个或者多个无状态进程（例如 master/work），它们之间不需要共享任何东西。任何需要的状态都置于后端服务（例如 cache、对象存储等）。\n端口绑定\n该应用程序是独立的，并通过端口绑定（包括 HTTP）导出任何 / 所有服务。\n并发\n并发通常通过水平扩展应用程序进程来实现（尽管如果需要的话进程也可以通过内部管理的线程多路复用来实现）。\n可任意处置性\n通过快速迅速启动和优雅的终止进程，可以最大程度上的实现鲁棒性。这些方面允许快速弹性缩放、部署更改和从崩溃中恢复。\n开发 / 生产平等\n通过保持开发、staging 和生产环境尽可能的相同来实现持续交付和部署。\n日志\n不管理日志文件，将日志视为事件流，允许执行环境通过集中式服务收集、聚合、索引和分析事件。\n管理进程\n行政或管理类任务（如数据库迁移），应该在与 app 长期运行的相同的环境中一次性完成。\n这些特性很适合快速部署应用程序，因为它们不需要对将要部署的环境做任何假定。不对环境假设能够允许底层云平台使用简单而一致的机制，轻松实现自动化，快速配置新环境，并部署应用。以这种方式，十二因素应用模式能够帮我们优化应用的部署速度。\n这些特性也很好地适用于突发需求，或者低成本地“丢弃”应用程序。应用程序环境本身是 100％一次性的，因为任何应用程序状态，无论是内存还是持久性，都被提取到后端服务。这允许应用程序以易于自动化的非常简单和弹性的方式进行伸缩。在大多数情况下，底层平台只需将现有环境复制到所需的数目并启动进程。缩容是通过暂停正在运行的进程和删除环境来完成，无需设法地实现备份或以其他方式保存这些环境的状态。就这样，12 因素应用模式帮助我们实现规模优化。\n最后，应用程序的可处理性使得底层平台能够非常快速地从故障事件中恢复。\n此外，将日志作为事件流处理能够极大程度上的增强应用程序运行时底层行为的可视性。\n强制环境之间的等同、配置机制的一致性和后端服务管理使云平台能够为应用程序运行时架构的各个方面提供丰富的可视性。以这种方式，十二因素应用模式能够优化安全性。\n微服务 微服务将单体业务系统分解为多个“仅做好一件事”的可独立部署的服务。这件事通常代表某项业务能力，或者最小可提供业务价值的“原子“服务单元。\n微服务架构通过以下几种方式为速度、安全、可扩展性赋能：\n当我们将业务领域分解为可独立部署的有限能力的环境的同时，也将相关的变更周期解耦。只要变更限于单一有限的环境，并且服务继续履行其现有合约，那么这些更改可以独立于与其他业务来进行开展和部署。结果是实现了更频繁和快速的部署，从而实现了持续的价值流动。 通过扩展部署组织本身可以加快部署。由于沟通和协调的开销，添加更多的人，往往会使软件构建变得更加苦难。弗雷德・布鲁克斯（Fred Brooks，人月神话作者）很多年前就教导我们，在软件项目的晚期增加更多的人力将会时软件项目更加延期。然而，我们可以通过在有限的环境中构建更多的沙箱，而不是将所有的开发者都放在同一个沙箱中。 由于学习业务领域和现有代码的认知负担减少，并建立了与较小团队的关系，因此我们添加到每个沙箱的新开发人员可以更快速地提高并变得更高效。 可以加快采用新技术的步伐。大型单体应用架构通常与对技术堆栈的长期保证有关。这些保证的存在是为了减轻采用新技术的风险。采用了错误的技术在单体架构中的代价会更高，因为这些错误可能会影响整个企业架构。如果我们可以在单个整体的范围内采用新技术，将隔离并最大限度地降低风险，就像隔离和最小运行时故障的风险一样。 微服务提供独立、高效的服务扩展。单体架构也可以扩展，但要求我们扩展所有组件，而不仅仅是那些负载较重的组件。当且仅当相关联的负载需要它时，微服务才会被缩放。 自服务敏捷架构 使用云原生应用架构的团队通常负责其应用的部署和持续运营。云原生应用的成功采纳者已经为团队提供了自服务平台。\n正如我们创建业务能力团队为每个有界的环境构建微服务一样，我们还创建了一个能力小组，负责提供一个部署和运行这些微服务的平台。\n这些平台中最大好处是为消费者提供主要的抽象层。通过基础架构即服务（IAAS），我们要求 API 创建虚拟服务器实例、网络和存储，然后应用各种形式的配置管理和自动化，以使我们的应用程序和支持服务能够运行。现在这种允许我们自定义应用和支持服务的平台正在不断涌现。\n应用程序代码简单地以预构建的工件（可能是作为持续交付管道的一部分生成的）或 Git 远程的原始源代码的形式“推送”。然后，平台构建应用程序工件，构建应用程序环境，部署应用程序，并启动必要的进程。团队不必考虑他们的代码在哪里运行或如何到达那里，这些对用户都是透明得，因为平台会关注这些。\n这样的模型同样适合于后端服务。需要数据库？消息队列或邮件服务器？只需要求平台来配合您的需求。平台现在支持各种 SQL/NoSQL 数据存储、消息队列、搜索引擎、缓存和其他重要的后端服务。这些服务实例然后可以“绑定”到您的应用程序，必要的凭据会自动注入到应用程序的环境中以供其使用。从而消除了大量凌乱而易出错的定制自动化。\n这些平台还经常提供广泛的额外操作能力：\n应用程序实例的自动化和按需扩展 应用健康管理 请求到或跨应用程序实例间的动态路由和负载均衡 日志和指标的聚合 这种工具的组合确保了能力团队能够根据敏捷原则开发和运行服务，从而实现速度，安全性和规模化。\n基于 API 的协作 在云原生应用架构中，服务之间的唯一互动模式是通过已发布和版本化的 API。这些 API 通常是具有 JSON 序列化的 HTTP REST 风格，但也可以是其他协议和序列化格式。\n只要有需要，在不会破坏任何现有的 API 协议的前提下，团队就可以部署新的功能，而不需要与其他团队进行同步。自助服务基础设施平台的主要交互模式也是通过 API，就像其他业务服务一样。供给、缩放和维护应用程序基础设施的方式不是通过提交单据，而是将这些请求提交给提供该服务的 API。\n通过消费者驱动的协议，可以在服务间交互的双方验证协议的合规性。服务消费者不能访问其依赖关系的私有实现细节，或者直接访问其依赖关系的数据存储。实际上，只允许有一个服务能够直接访问任何数据存储。这种强制解耦直接支持云原生的速度目标。\n抗脆弱性 Nassim Taleb 在他的 Antifragile（Random House）一书中介绍了抗脆弱性的概念。如果脆弱性是受到压力源的弱化或破坏的质量系统，那么与之相反呢？许多人会以稳健性或弹性作出回应 —— 在遭受压力时不会被破坏或变弱。然而，Taleb 引入了与脆弱性相反的抗脆弱性概念，或者在受到压力源时变得更强的质量系统。什么系统会这样工作？联想下人体免疫系统，当接触病原体时，其免疫力变强，隔离时较弱。我们可以像这样建立架构吗？云原生架构的采用者们已经设法构建它们了。Netflix Simian Army 项目就是个例子，其中著名的子模块“混沌猴”，它将随机故障注入到生产组件中，目的是识别和消除架构中的缺陷。通过明确地寻求应用架构中的弱点，注入故障并强制进行修复，架构自然会随着时间的推移而更大程度地收敛。\n","relpermalink":"/book/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/defining-cloud-native-architectures/","summary":"现在我们将探索云原生应用架构的几个主要特征，和这些特征是如何解决我们前面提到的使用云原生应用架构的动机。 12 因素应用 12 因素应用是一系列云原生应用架构的模式集合，最初由 Heroku 提出。这些模式可以用来说明什么样的","title":"1.2 云原生架构的定义"},{"content":"在看了基于微服务的应用所需的各种应用服务后，考虑一下提供这些服务的服务网格的架构。服务网格由两个主要部分组成：控制平面和数据平面。\n2.2.1 控制平面 控制平面有几个组件。虽然服务网格的数据面主要由作为容器运行在与应用容器相同的 Pod 中的代理组成，但控制面组件在它们自己的 Pod、节点和相关集群中运行。以下是控制平面的 各种功能 ：\nEnvoy sidecar 代理的服务发现和配置 自动化的密钥和证书管理 用于策略定义和收集遥测数据的 API 服务网格组件的配置摄取 管理一个到服务网格的入站连接（入站网关） 管理来自服务网格的出站连接（出口网关） 将 sidecar 代理注入那些托管应用程序微服务容器的 Pod、节点或命名空间中 总的来说，控制平面帮助管理员用配置数据填充数据平面组件，这些数据是由控制平面的策略产生的。上述功能 3 的策略可能包括网络路由策略、负载均衡策略、蓝绿部署的策略、金丝雀部署、超时、重试和断路能力。这后三项被统称为网络基础设施服务的弹性能力的特殊名称。最后要说的是与安全相关的策略（例如，认证和授权策略、TLS 建立策略等）。这些策略规则由一个模块解析，该模块将其转换为配置参数，供执行这些策略的数据平面代理中的可执行程序使用。\n2.2.2 数据平面 数据平面组件执行三种不同的功能：\n安全的网络功能 策略执行功能 可观测性功能 执行上述三种功能的数据平面的主要组件被称为 sidecar 代理。这个七层代理运行在与它执行代理功能的微服务相同的网络命名空间（在这个平台上，就是同一个 Pod）。每个微服务都有一个代理，以确保来自微服务的请求不会绕过其相关的代理，每个代理都作为容器运行在与应用微服务相同的 Pod 中。两个容器都有相同的 IP 地址，并共享相同的 IP 表规则。这使得代理完全控制了 Pod，并 处理所有通过它的流量 。\n第一类功能（安全网络）包括与微服务之间的实际路由或消息通信有关的所有功能。属于这一类的功能是服务发现、建立安全（TLS）会话、为每个微服务及其相关请求建立网络路径和路由规则、验证每个请求（来自服务或用户）以及授权请求。\n以建立双向 TLS 会话为例，发起通信会话的代理将与服务网格的控制平面中的模块进行交互，以检查是否需要通过链加密流量并与后端或目标 Pod 建立双向 TLS。使用双向 TLS 启用这个功能需要每个 Pod 有一个证书（即有效的凭证）。由于一个规模较大的微服务应用程序（由许多微服务组成）可能需要数百个 Pod（即使没有通过多个实例对单个微服务进行横向扩展），这可能涉及到管理数百个生命周期短暂的证书。这反过来又要求每个微服务有一个强大的身份，服务网格要有一个访问管理器、一个证书存储和一个证书验证能力。此外，为了支持认证策略，还需要识别和认证两个通信的 Pod 的机制。\n其他类型的代理包括拦截客户端调用到应用程序的第一个入口点（第一个被调用的微服务）的 入口代理 和处理微服务对驻扎在平台集群外的应用模块的请求的出口代理。\n数据平面执行的第二类功能是通过代理中的配置参数执行控制平面中定义的策略（策略执行服务）。一个例子是使用作为微服务请求一部分的 JWT 令牌中的信息来验证调用服务。另一个例子是使用驻留在代理本身的代码或通过连接到外部授权服务，为每个请求执行访问控制策略。\n服务代理与应用服务容器联合执行的第三类功能是收集遥测数据，这有助于监测服务的健康和状态，将与服务相关的日志传输到控制平面中的日志聚合模块，并将必要的数据附加到应用请求头，以方便追踪与特定应用事务相关的所有请求。应用响应由代理机构以返回代码、响应描述或检索数据的形式传达给其相关的调用服务。\n服务网格是容器编排平台感知的，与 API 服务器互动，该服务器为安装在各种平台工件（如 Pod、节点、命名空间）中的应用服务提供一个窗口，监测它是否有新的微服务，并自动将 sidecar 容器注入包含这些新微服务的 Pod。一旦服务网格插入 sidecar 代理容器，运维和安全团队就可以对流量执行策略，并帮助保护和运维应用程序。这些团队还可以配置对微服务应用的监控，而不干扰应用的运作。\n基础设施、策略执行和可观测性服务的配置可以使用作为 DevSecOps 管道一部分的声明性代码来自动化。虽然开发团队应该全面了解其代码部署的安全和管理细节，但上述服务的自动化为他们提供了更多的时间来集中精力进行高效的开发范式，如代码模块化和结构化。\n下一章 ","relpermalink":"/book/service-mesh-devsecops/reference-platform/service-mesh-software-architecture/","summary":"在看了基于微服务的应用所需的各种应用服务后，考虑一下提供这些服务的服务网格的架构。服务网格由两个主要部分组成：控制平面和数据平面。 2.2.1 控制平面 控制平面有几个组件。虽然服务网格的数据面主要由作为容器运行在","title":"2.2 服务网格架构"},{"content":"在本节中，我们将探讨采用云原生应用架构的组织在创建团队时需要进行的变革。这个重组背后的理论是著名的康威定律。我们的解决方案是在长周期的产品开发中，创建一个包含了各方面专业员工的团队，而不是将他们分离在单一的团队中，例如测试人员。\n业务能力团队 设计系统的组织，最终产生的设计等同于组织之内、之间的沟通结构。\n——Melvyn Conway\n我们已经讨论了“从孤岛到 DevOps”将 IT 组织成专门的信息孤岛的做法。我们很自然地创造了这些孤岛，并把个体放到与这些孤岛一致的团队中。但是当我们需要构建一个新的软件的时候会怎么样？\n一个很常见的做法是成立一个项目团队。该团队向项目经理汇报，然后项目经理与各种孤岛合作，为项目所需的各个专业领域寻求“资源”。正如康威定律所言，这些团队将很自然地在系统中构筑起各种孤岛，我们最终得到的是联合各种孤岛相对应的孤立模块的架构：\n数据访问层 服务层 Web MVC 层 消息层 等等 这些层次中的每一层都跨越了多个业务能力领域，使得在其之上的创新和部署独立于其他业务能力的新功能变的非常困难。\n寻求迁移到将业务能力分离的微服务等云原生架构的公司经常采用 Thoughtworks 称之为的“逆康威定律”。他们没有建立一个与其组织结构图相匹配的架构，而是决定了他们想要的架构，并重组组织以匹配该架构。如果你这样做的话，根据康威定律，您所期望的架构终将出现。\n因此，作为转向 DevOps 文化的一部分，我们组织了跨职能、业务能力的团队，开发的是产品而不再是项目。开发产品需要长期的付出，直到它们不再为企业提供价值为止。（直到你的代码不再运行在生产上为止！）构建、测试、交付和运营提供业务能力的服务所需的所有角色都存在于一个团队中，该团队不会向组织的其他部分交接代码。这些团队通常被组织为“双比萨队”，意思是如果不能用两个比萨饼喂饱，那就意味着团队规模太大了。\n那么剩下的就是确定要创建的团队。如果我们遵循逆康威定律，我们将从组织的领域模型开始，并寻求可以封装在有限环境中的业务能力。一旦我们确定了这些能力，我们就可以创建为这些业务能力的整个生命周期负责的团队。业务能力团队掌握其应用程序从开发到运营的整个生命周期。\n平台运营团队 业务能力团队需要依赖于我们前面提到的”自助敏捷基础架构“。\n事实上，我们可以这样来描述一种特殊的业务能力 —— 开发、部署和运营业务的能力。这种能力应该是平台运营团队所具有的。\n平台运营团队运营自助敏捷基础架构平台，并交付给业务能力团队使用。该团队通常包括传统的系统、网络和存储管理员角色。如果公司正在运营云平台，该团队也将拥有管理数据中心的团队或与他们紧密合作，并了解提供基础架构平台所需的硬件能力。\nIT 运营传统上通过各种基于单据的系统与客户进行互动。由于基于平台操作流来运行自助服务平台，因此必须提供不同形式的交互方式。正如业务能力团队之间通过定义好的 API 协议相互协作一样，平台运营团队也为该平台提供了 API 协议。业务能力团队不再需要排队申请应用环境和数据服务，而是采用更精简的方式构建按需申请环境和服务的自动化发布管道。\n","relpermalink":"/book/migrating-to-cloud-native-application-architectures/changes-needed/organizational-change/","summary":"在本节中，我们将探讨采用云原生应用架构的组织在创建团队时需要进行的变革。这个重组背后的理论是著名的康威定律。我们的解决方案是在长周期的产品开发中，创建一个包含了各方面专业员工的团队，而不是将他们分离在","title":"2.2 组织变革"},{"content":"DevSecOps 是一个敏捷的、自动化的开发和部署过程，它使用称为 CI/CD 管道的原语，在自动化工具的帮助下，将软件从构建阶段带到部署阶段，最后到运行时间 / 操作阶段。这些管道是将开发者的源代码带过各个阶段的工作流程，如构建、测试、打包、交付，以及在各个阶段由测试工具支持的部署。\nDevSecOps 平台是指各种 CI/CD 管道（针对每种代码类型）运行的资源集合。至少，这个平台由以下部分组成。\n(a) 管道软件\nCI 软件 —— 从代码库中提取代码，调用构建软件，调用测试工具，并将测试后的工件存储到图像注册表中。 CD 软件 —— 拉出工件、软件包，并根据 IaC 中的计算、网络和存储资源描述，部署软件包。 (b) SDLC 软件\n构建工具（例如，IDE） 测试工具（SAST、DAST、SCA） (c) 存储库\n源代码库（如 GitHub） 容器镜像存储库或注册表 (d) 可观测性或监测工具\n日志和日志聚合工具 产生指标的工具 追踪工具（应用程序的调用顺序） 可视化工具（结合上述数据生成仪表盘 / 警报）。 在 DevSecOps 平台中，通过内置的设计功能（如零信任）和使用一套全面的安全测试工具，如静态应用安全工具（SAST）、动态安全测试工具（DAST）和软件组成分析（SCA）工具进行测试，在构建和部署阶段提供安全保证。此外，在运行时 / 操作阶段，还通过持续的行为检测 / 预防工具提供安全保证，其中一些工具甚至可能使用人工智能（AI）和机器学习（ML）等复杂技术。因此，DevSecOps 平台不仅在构建和部署阶段运行，而且在运行时 / 操作阶段也运行。\n在一些 DevSecOps 平台中，执行应用程序安全分析的安全工具（例如 SAST、DAST 和 SCA），例如通过在后台的有效扫描来识别漏洞和错误，可以与集成开发环境（IDE）和其他 DevOps 工具紧密集成。这一功能的存在，使得这些工具对开发者来说是透明的，避免了他们为运行这些工具而 调用单独的 API 。根据集成开发环境、所执行的任务或工具所消耗的资源的不同，工具也可以与集成开发环境分开执行。\n3.2.1 DevSecOps 平台的可交付成果 SAST、DAST 和 SCA 工具的使用可能不仅限于测试应用程序代码。DevSecOps 可能包括将这些工具用于其他代码类型，如 IaC，因为 IaC 定义了应用程序的部署架构，因此是自动评估和补救安全设计差距的关键途径。\n总之，DevSecOps 平台可以提供以下内容：\n通过在与应用环境中所有代码类型相关的管道内纳入充分的测试 / 检查，提供安全保证。安全性不是被归入一个单独的任务或阶段。 DevSecOps 平台也在运行时（生产中）运行，通过协助执行零信任原则，并通过持续监控，随后的警报和纠正机制，提供实时的安全保证，从而实现持续授权操作（C-ATO）的认证。 ","relpermalink":"/book/service-mesh-devsecops/devsecops/devsecops-platform/","summary":"DevSecOps 是一个敏捷的、自动化的开发和部署过程，它使用称为 CI/CD 管道的原语，在自动化工具的帮助下，将软件从构建阶段带到部署阶段，最后到运行时间 / 操作阶段。这些管道是将开发者的源代码带过各个阶段的工作流程，如构建、","title":"3.2 DevSecOps 平台"},{"content":"当我们开始构建由微服务组成的分布式系统时，我们还会遇到在开发单体应用时通常不会遇到的非功能性要求。有时，使用物理定律就可以解决这些问题，例如一致性、延迟和网络分区问题。然而，脆弱性和易控性的问题通常可以使用相当通用的模式来解决。在本节中，我们将介绍帮助我们解决这些问题的方法。\n这些方法来自于 Spring Cloud 项目和 Netflix OSS 系列项目的组合。\n版本化和分布式配置 在“12 因素应用“中我们讨论过通过操作系统级环境变量为应用注入对应的配置，强调了这种配置管理方式的重要性。这种方式特别适合简单的系统，但是，当系统扩大后，有时我们还需要附加的配置能力：\n为调试一个生产上的问题而变更运行的应用程序日志级别 更改 message broker 中接收消息的线程数 报告所有对生产系统配置所做的更改以支持审计监管 运行中的应用切换功能开关 保护配置中的机密信息（如密码） 为了支持这些特性，我们需要配置具有以下特性的配置管理方法：\n版本控制 可审计 加密 在线刷新 Spring Cloud 项目中包含的一个可提供这些功能的配置服务器。此配置服务器通过 Git 本地仓库支持的 REST API 呈现了应用程序及应用程序配置文件（例如，可用开 / 关切换的一组配置作为一组，如“deployment”和“staging”配置）（图 3 -1）。\nSpring Cloud Config Server 例 3-1 是示例配置服务器的默认配置文件：\nExample 3-1 该配置中指定了后端 Git 仓库中的 application.yml 文件。\ngreeting 当前被设置为 ohai。\n例 3-1 中的配置是自动生成的，无需手动编码。我们可以看到，通过检查它的 /env 端点（例 3-2），greeting 的值被分发到 Spring 应用中。\nExample 3-2 该应用接收到来自配置服务器的 greeting 的值：ohai。 现在我们就可以无需重启客户端应用就可以更新 greeting 的值。该功能由 Spring Cloud 项目中的一个名为 Spring Cloud Bus 的组件提供。该项目将分布式系统的节点与轻量级消息代理进行链接，然后可以用于广播状态更改，如我们所需的配置更改（图 3-2）。该项目将分布式系统的节点与轻量级消息代理进行链接，然后可以用于广播状态更改，如我们所需的配置更改（图 3-2）。\n只需通过对参与总线的任何应用程序的 /bus/refresh 端点执行 HTTP POST（这显然应该进行适当的安全性保护），指示总线上的所有应用程序使用配置服务器中的最新的可用值刷新其配置。\nFigure 3-2 服务注册发现 当我们创建分布式系统时，代码的依赖不再是一个方法调用。相反，消费它们必须通过网络调用。我们该如何布线，才能使组合系统中的所有微服务彼此通信？\n云中的（图 3-3）的同样架构模式是有一个前端（应用程序）和后端（业务）服务。后端服务往往不能直接从互联网访问，而是通过前端服务访问。服务注册提供的所有服务的列表，使它们可以通过一个客户端库到达前端服务（路由和负载均衡），客户端库执行负载均衡和路由到后端服务。\nFigure 3-3 在使用服务定位器和依赖注入模式的各种形式之前，我们已经解决了这个问题，面向服务的架构长期以来一直使用各种形式的服务注册表。我们将采用类似的解决方案，利用 Eureka，这是一个 Netflix OSS 项目，可用于定位服务，以实现中间层服务的负载平衡和故障转移。为了使用 Netflix OSS 服务，Spring Cloud Netflix 项目提供了基于注释的配置模型，这大大简化了开发人员在开发 Spring 应用程序时对 Eureka 的心力耗费。\n在例 3-3 中，只需简单得在代码中添加 @EnableDiscoveryClient 注释，应用程序就可以进行服务注册和发现。\nExample 3-3 @EnableDiscoveryClient 开启应用程序的服务注册发现。 该应用程序就能够通过利用 DiscoveryClient 与它的依赖组件通信。例 3-4 是应用程序查找名为 PRODUCER 的注册服务的一个实例，获得其 URL，然后利用 Spring 的 RestTemplate 与之通信。\nExample 3-4 开启的 DiscoveryClient 通过 Spring 注入。 getNextServerFromEureka 方法使用 round-robin 算法提供服务实例的位置。 路由和负载均衡 基本的 round-robin 负载平衡在许多情况下是有效的，但云环境中的分布式系统通常需要更高级的路由和负载均衡行为。这些通常由各种外部集中式负载均衡解决方案提供。然而，这种解决方案通常不具有足够的信息或上下文，以便在给定的应用程序尝试与其依赖进行通信时做出最佳选择。此外，如果这种外部解决方案故障，这些故障可以跨越整个架构。\n云原生的解决方案通常将路由和负载均衡的职责放在客户端。Ribbon Netflix OSS 项目就是其中的一种。（图 3-4）\nFigure 3-4 Ribbon 提供一组丰富的功能集：\n多种内建的负载均衡规则：\nRound-robin 轮询负载均衡 平均加权响应时间负载均衡 随机负载均衡 可用性过滤负载均衡（避免跳闸线路和高并发链接数） 自定义负载均衡插件系统 与服务发现解决方案的可拔插集成（包括 Eureka）\n云原生智能，例如可用区亲和性和不健康区规避\n内建的故障恢复能力\n跟 Eureka 一样，Spring Cloud Netflix 项目也大大简化了 Spring 应用程序开发人员使用 Ribbon 的心力耗费。开发人员可以注入一个 LoadBalancerClient 的实例，然后使用它来解析应用程序依赖关系的一个实例（例 3-5），而不是注入 DiscoveryClient 的实例（用于直接从 Eureka 中消费）。\nExample 3-5-1 Example 3-5-2 由 Spring 注入的 LoadBalancerClient。 choose 方法使用当前负载均衡算法提供了服务的一个示例地址。 Spring Cloud Netflix 通过创建可以注入到 Bean 中的 Ribbon-enabled 的 RestTemplate bean 来进一步简化 Ribbon 的配置。RestTemplate 的这个实例被配置为使用 Ribbon（示例 3-6）自动将实例的逻辑服务名称解析为 instanceURI。\nExample 3-6 注入的是 RestTemplate 而不是 LoadBalancerClient。 注入的 RestTemplate 自动将 http://producer 解析为实际的服务实例的 URI。 容错 分布式系统比起单体架构来说有更多潜在的故障模式。由于传入系统中的每一个请求都可能触及几十甚至上百个不同的微服务，因此这些依赖中的某些故障实质上是不可避免的。\n如果不进行容错，30 个依赖，每个都是 99.99% 的正常运行时间，每个月将导致 2 个小时的停机时间（99.99%^30=99.7% 的正常运行时间 = 2 小时以上的停机时间）。\n——Ben Christensen，Netflix 工程师\n如何避免这类故障导致级联故障，给我们的系统可用性数据带来负面影响？Mike Nygard 在他的 Pragmatic Programmers 中提出了几个可以觉得该问题的几个模式，包括：\n熔断器\n当服务的依赖被确定为不健康时，使用熔断器来阻绝该服务与其依赖的远程调用，就像电路熔断器可以防止电力使用过度，防止房子被烧毁一样。熔断器实现为状态机（图 3-5）。当其处于关闭状态时，服务调用将直接传递给依赖关系。如果任何一个调用失败，则计入这次失败。当故障计数在指定时间内达到指定的阈值时，熔断器进入打开状态。在熔断器为打开状态时，所有调用都会失败。在预定时间段之后，线路转变为“半开”状态。在这种状态下，调用再次尝试远程依赖组件。成功的调用将熔断器转换回关闭状态，而失败的调用将熔断器返回到打开状态。\nFigure 3-5 隔板\n隔板将服务分区，以便限制错误影响的区域，并防止整个服务由于某个区域中的故障而失败。这些分区就像将船舶划分成多个水密舱室一样，使用隔板将不同的舱室分区。这可以防止当船只受损时造成整艘船沉没（例如，当被鱼雷击中时）。软件系统中可以用许多方式利用隔板。简单地将系统分为微服务是我们的第一道防线。将应用程序进程分区为 Linux 容器，以便使用单个进程无法接管整个计算机。另一个例子是将并行工作划分为不同的线程池。\nNetflix 的 Hystrix 应用了这些和更多的模式，并提供了强大的容错功能。为了包含熔断器的代码，Hystrix 允许代码被包含到 HystrixCommand 对象中。\nExample 3-7 run 方法中封装了熔断器 Spring Cloud Netflix 通过在 Spring Boot 应用程序中添加 @EnableCircuitBreaker 注解来启用 Hystrix 运行时组件。然后通过另一组注解，使得基于 Spring 和 Hystrix 的编程与我们先前描述的集成一样简单（例 3-8）。\nExample 3-8 使用 @HystrixCommand 注解的方法封装了一个熔断器。 当线路处于打开或者半开状态时，注解中引用的 getProducerFallback 方法，提供了一个优雅的回调操作。 Hystrix 相较于其他熔断器来说是独一无二的，因为它还通过在其自己的线程池中操作每个熔断器来提供隔板。它还收集了许多关于熔断器状态的有用指标，其中包括：\n流量\n请求率\n错误百分比\n主机报告\n延迟百分点\n成功、失败和拒绝\n这些 metric 会被发送到事件流中，然后被 Netflix OSS 项目中的另一个叫做 Turbine 的组聚合。每个单独的和聚合后的 metric 流都可以在强大的 Hystrix Dashboard（图 3-6）中以可视化的方式呈现，该页面提供了很好的分布式系统总体健康状态的可视化效果。\nFigure 3-6 API 网关 / 边缘服务 在“移动应用和客户端多样性”中我们探讨过服务器端聚合与微服务生态系统。为什么有这个必要？\n延迟\n移动设备通常运行在比我们家用设备更低速的网络上。即使是在家用或企业网络上，为了满足单个应用屏幕的需求，需要连接数十（或者上百）个微服务，这样的延迟也将变得不可接受。很明显，应用程序需要使用并发的方式来访问这些服务。在服务端一次性捕获和实行这些并发模式，会比在每一个设备平台上做相同的事情，来得更廉价、更不容易出错。\n延迟的另一个来源是响应数据的大小。在 Web 服务开发领域，近年来一直趋向于“返回一切可能有用的数据”的做法，这将导致响应的返回数据越来愈大，远远超出了单一的移动设备屏幕的需求。移动设备开发者更倾向于通过仅检索必要的信息而忽略其他不重要的信息，来减少等待时间。\n往返通信\n即使网速不成问题，与大量的微服务通信依然会给移动应用开发者造成困扰。移动设备的电池消耗主要是因为网络开销造成的。移动应用开发者尽可能通过最少的服务端调用来减少网络的开销，并提供预期的用户体验。\n设备多样性\n移动设备生态系统中设备多样性是十分巨大的。企业必须应对不断增长的客户群体差异，包括如下这些：\n制造商\n设备类型\n形式因素\n设备尺寸\n编程语言\n操作系统\n运行时环境\n并发模型\n支持的网络协议\n这种多样性甚至扩大到超出了移动设备生态系统，开发者目前可能还会关注家用消费电子设备不断增长的生态系统，包括智能电视和机顶盒。\nAPI 网关模式（图 3-7）旨在将客户端的这些需求负担从设备开发者转移到服务器端。API 网关仅仅是一类特殊的满足单个客户端应用程序的微服务（如特定的 iPhone App），并为其提供一个到 …","relpermalink":"/book/migrating-to-cloud-native-application-architectures/migration-cookbook/distributed-systems-recipes/","summary":"当我们开始构建由微服务组成的分布式系统时，我们还会遇到在开发单体应用时通常不会遇到的非功能性要求。有时，使用物理定律就可以解决这些问题，例如一致性、延迟和网络分区问题。然而，脆弱性和易控性的问题通常可","title":"3.2 使用分布式系统"},{"content":"应用程序代码和应用服务代码驻留在容器编排和资源管理平台中，而实现与之相关的工作流程的 CI/CD 软件通常驻留在同一平台中。应使用第 4.6 节所述的步骤对该管道进行保护，该管道控制下的应用程序代码应接受第 4.8 节所述的安全测试。此外，应用程序所在的调度平台本身应使用运行时安全工具（如 Falco ）进行保护，该工具可以实时读取操作系统内核日志、容器日志和平台日志，并根据威胁检测规则引擎对其进行处理，以提醒用户注意恶意行为（例如，创建有特权的容器、未经授权的用户读取敏感文件等）。它们通常有一套默认（预定义）的规则，可以在上面添加自定义规则。在平台上安装它们，可以为集群中的每个节点启动代理，这些代理可以监控在该节点的各个 Pod 中运行的容器。这种类型的工具的优点是，它补充了现有平台的本地安全措施，如访问控制模型和 Pod 安全策略，通过实际检测它们的发生来 防止漏洞 。\n","relpermalink":"/book/service-mesh-devsecops/implement/ci-cd-pipeline-for-application-code-and-application-services-code/","summary":"应用程序代码和应用服务代码驻留在容器编排和资源管理平台中，而实现与之相关的工作流程的 CI/CD 软件通常驻留在同一平台中。应使用第 4.6 节所述的步骤对该管道进行保护，该管道控制下的应用程序代码应接受第 4.8 节所述的安全","title":"4.2 应用程序代码和应用服务代码的 CI/CD 管道"},{"content":"作者\nCybersecurity and Infrastructure Security Agency (CISA)\nNational Security Agency (NSA) Cybersecurity Directorate Endpoint Security\n联系信息\n客户要求 / 一般网络安全问题。\n网络安全需求中心，410-854-4200，Cybersecurity_Requests@nsa.gov 。\n媒体咨询 / 新闻台\n媒体关系，443-634-0721，MediaRelations@nsa.gov 。\n关于事件响应资源，请联系 CISA：CISAServiceDesk@cisa.dhs.gov 。\n中文版\n关于本书中文版的信息请联系 Jimmy Song：jimmysong@jimmysong.io 。\n宗旨\n国家安全局和 CISA 制定本文件是为了促进其各自的网络安全，包括其制定和发布网络安全规范和缓解措施的责任。这一信息可以被广泛分享，以触达所有适当的利益相关者。\n","relpermalink":"/book/kubernetes-hardening-guidance/publication-information/","summary":"作者 Cybersecurity and Infrastructure Security Agency (CISA) National Security Agency (NSA) Cybersecurity Directorate Endpoint Security 联系信息 客户要求 / 一般网络安全问题。 网络安全需求中心，410-854-4200，Cybersecurity_Requests@nsa.gov 。 媒体咨询 / 新闻台 媒体关系，","title":"出版信息"},{"content":"基础设施技术的历史向来引人入胜。由于基础设施的规模巨大，它已经经历了一次快速的颠覆性变革。除了计算机和互联网的早期，基础设施可谓日新月异。这些创新使基础架构更快，更可靠，更有价值。\n有些公司的人将基础设施推到了极限，他们已经找到了自动化和抽象的方法，提取基础设施更多商业价值。通过提供灵活的可用资源，他们将曾经是昂贵的成本中心转变为所需的商业公用事业。\n然而，公共事业公司很少为企业提供财务价值，这意味着基础设施往往被忽略并被视为不必要的成本。这使得投入创新或改进的时间和金钱很少。\n这样一个如此简单且令人着迷的业务栈怎么可以被轻易忽略？当基础设施出现故障时，业务显然会受到重视，那么为什么基础设施很难改善呢？\n基础设施已经达到了使消费者都感到无聊的成熟度。然而，它的潜力和新挑战又激发了实施者和工程师们新的激情。\n扩展基础设施并使用新的业务方式让来自不同行业的工程师都能找到解决方案。开源软件（OSS）和社区间的互相协作，这股力量又促使了创新的激增。\n如果管理得当，今天基础设施和应用方面的挑战将会不一样。这使得基础设施建设者和维护人员可以取得进展并开展新的有意义的工作。\n有些公司克服了诸如可扩展性、可靠性和灵活性等挑战。他们创建并封装了一些项目，封装了可供他人遵循的模式。这些模式有时很容易被实现者发现，但在其他情况下，它们不太明显。\n在本书中，我们将分享来自云原生技术前沿的公司的经验教训，使您能够有效解决可靠运行可伸缩应用程序的问题。现代商业发展非常迅速。本书中的模式将使您的基础设施能够跟上业务的速度和敏捷性需求。更重要的是，我们会让您自行决定何时采用这些模式。\n这些模式中有很多都已经在开源项目中得到了体现。其中一些项目由云原生计算基金会（CNCF）维护。这些项目和基金会并不是模式的唯一体现，但忽视它们会让你失去理智。以它们为例，但要自己进行尽职调查，以审核您所采用的每个解决方案。\n我们将向您展示云原生基础设施的益处以及可扩展系统和应用程序的基本模式。我们将向您展示如何测试您的基础设施，以及如何创建一个可以适应您需求的灵活的基础设施。您将了一些重要方面和未来发展。\n这本书可以激励你继续前进并自由分享你在社区中学到的知识。\n谁应该读这本书 如果您是开发基础设施或基础设施管理工具的工程师，那么本书就是为您准备的。本书将帮助您了解创建旨在在云环境中运行的基础设施的模式、流程和实践。通过了解应该怎么做，您可以更好地了解应用程序的作用，以及应该何时构建基础设施或使用云服务。\n应用程序工程师从本书中还可以发现哪些服务应该是其应用程序的一部分，哪些服务应该由基础设施提供。通过本书了解应用开发者和管理基础设施的工程师应该共同承担的责任。\n希望提升技能并系统地在设计基础设施和维护云网关基础设施方面发挥更大作用的系统管理员也可以从本书中学到很多。\n你是否在公有云中运行所有的基础设施？本书将帮助您了解何时使用云服务以及何时构建自己的抽象或服务。\n运行在数据中心还是本地云？我们将概述现代应用对基础设施的期望，并将帮助您了解利用当前投资的必要服务。\n这本书不是一本教程，除了给出实现示例之外，我们没有指出特定的产品。对于经理、董事和高管来说，这可能太过技术性，但可能会有所帮助，具体取决于该角色的参与和技术专长。\n最重要的是，如果您想了解基础设施如何影响业务，请阅读本书，以及如何创建经证实可为具备全球互联网运营规模的企业服务的基础设施。即使您的应用程序不需要扩展到这种规模，如果您的基础设施是使用此处描述的模式构建的，并且考虑到灵活性和可操作性，这本书仍然值得一读。\n为什么我们写了这本书 我们希望通过专注于模式和实践而不是特定产品和供应商来帮助您了解。当前存在太多的解决方案而人们不了解它们本身到底要解决什么问题。\n我们相信通过云原生应用程序管理云原生基础设施的好处，并且我们希望所有人都具有这种意识。\n我们希望回馈社区，推动行业向前发展。我们发现这样做的最好方式是解释业务和基础设施之间的关系，阐明问题并解释发现它们的工程师和组织所做的解决方案。\n以不涉及产品的方式解释模式并不总是很容易，但了解产品存在的原因很重要。我们经常使用产品作为模式的例子，但只有当它们会帮助您提供解决方案的实施示例时才会提到。\n如果没有无数人数万小时的自愿编写代码，帮助他人以及投资社区，我们就不会走到这里。我们非常感谢那些帮助我们了解这些模式的人们，我们希望能够回馈并帮助下一代工程师。这本书就是我们表达谢意的方式。\n浏览本书 本书的组织结构如下：\n第 1 章介绍云原生基础设施是什么以及我们如何走到当前这一步。 第 2 章可以帮助您决定是否以及何时采用后面章节中预先描述的模式。 第 3 章和第 4 章展示了应该如何部署基础设施以及如何编写应用程序来管理它。 第 5 章将教你如何从测试开始就设计可靠的基础设施。 第 6 章和第 7 章展示了如何管理基础设施和应用程序。 第 8 章总结并提供了一些有关未来发展的见解。 如果你像我们一样，不会从前到后完整看完本书。以下是关于本书主题的一些建议：\n如果您是一位专注于创建和维护基础设施的工程师，您应该至少阅读第 3 章至第 6 章。 应用程序开发人员可以专注于第 4、5 和 7 章关于将基础架构工具开发为云原生应用程序。 所有不构建云原生基础设施的人都将从第 1、2、8 章中受益匪浅。 在线资源 您应该通过访问 CNCF 网站熟悉云原生计算基金会（CNCF）及其托管项目。本书中的许多项目都被用作示例。\n您还可以通过查看 CNCF 景观项目（参见图 P-1），了解项目这些项目的全局视图。\n云原生应用程序是从 Heroku 的 12 因素的定义开始的。我们会解释它们之间的相似之处，但你应该熟悉下 12 因素是什么（参见 http://12factor.net ）。\n还有许多关于 DevOps 的书籍、文章和演讲。尽管本书不关注 DevOps 实践，但是在实现云原生基础设施时，如果没有 DevOps 规定的工具、实践和文化，将很难实现。\n致谢 Justin Garrison\n感谢 Beth、Logan、我的朋友、家人以及在此过程中支持我们的同事。感谢那些帮助我们的社区和社区领袖以及给予宝贵反馈的评论者。感谢 Kris 让这本书变得更好，感谢读者花点时间阅读本书并提高你的技能。\nKris Nova\n感谢 Allison、Bryan、Charlie、Justin、Kjersti、Meghann 和 Patrick 为我写这本书所作出的帮助。我爱你们，永远感激你们为我所做的一切。\n","relpermalink":"/book/cloud-native-infra/introduction/","summary":"基础设施技术的历史向来引人入胜。由于基础设施的规模巨大，它已经经历了一次快速的颠覆性变革。除了计算机和互联网的早期，基础设施可谓日新月异。这些创新使基础架构更快，更可靠，更有价值。 有些公司的人将基础设","title":"介绍"},{"content":"云原生应用已经发展成为一个标准化的架构，由以下部分组成。\n多个松散耦合的组件被称为微服务（通常或典型地以容器形式实现）。 一个应用服务基础设施，为用户、服务和设备提供安全通信、认证和授权等服务（例如，服务网格）。 由于安全、商业竞争力和其固有的结构（松散耦合的应用组件），这类应用需要一个不同的应用、部署和运行时监控范式 —— 统称为软件生命周期范式。DevSecOps（分别由开发、安全和运维的首字母缩写组成）是这些应用的开发、部署和运维的促进范式之一，其基本要素包括持续集成、持续交付和持续部署（CI/CD）管道。\nCI/CD 管道是将开发人员的源代码通过各个阶段的工作流程，如构建、功能测试、安全扫描漏洞、打包和部署，由带有反馈机制的自动化工具支持。在本文中，应用环境中涉及的整个源代码集被分为五种代码类型：\n应用代码，它体现了执行一个或多个业务功能的应用逻辑。 应用服务代码，用于服务，如会话建立、网络连接等。 基础设施即代码，它是以声明性代码的形式存在的计算、网络和存储资源。 策略即代码，这是运行时策略（例如，零信任），以声明性代码的形式表达。 可观测性即代码，用于持续监测应用程序的健康状况，其中监测功能被表述为声明性代码。 因此，可以为所有五个代码类型创建单独的 CI/CD 管道。还描述了这些代码类型中的每一种所执行的功能，以强调它们在整个应用程序的执行中所发挥的作用。\n虽然云原生应用有一个共同的架构堆栈，但堆栈组件运行的平台可能有所不同。该平台是物理（裸机）或虚拟化（如 Kubernetes）上的一个抽象层。为了在本文中明确提及该平台或应用环境，它被称为 DevSecOps 原语参考平台，或简称为 参考平台。\n本文件的目的是为参考平台的 DevSecOps 原语的实施提供指导。本文还介绍了这种实施对高安全保障的好处，以及在管道内使用风险管理工具和仪表盘指标提供持续授权操作（C-ATO）的工件。\n下一章 ","relpermalink":"/book/service-mesh-devsecops/executive-summary/","summary":"云原生应用已经发展成为一个标准化的架构，由以下部分组成。 多个松散耦合的组件被称为微服务（通常或典型地以容器形式实现）。 一个应用服务基础设施，为用户、服务和设备提供安全通信、认证和授权等服务（例如，服务","title":"执行摘要"},{"content":"可观测性行业正处在巨变中。\n传统上，我们观察系统时使用的是一套筒仓式的、独立的工具，其中大部分包含结构不良（或完全非结构化）的数据。这些独立的工具也是垂直整合的。工具、协议和数据格式都属于一个特定的后端或服务，不能互换。这意味着更换或采用新的工具需要耗费时间来更换整个工具链，而不仅仅是更换后端。\n这种孤立的技术格局通常被称为可观测性的 “三大支柱”：日志、度量和（几乎没有）追踪（见图 1-1）。\n日志\n记录构成事务的各个事件。\n度量\n记录构成一个事务的事件的集合。\n追踪\n测量操作的延迟和识别事务中的性能瓶颈，或者类似的东西。传统上，许多组织并不使用分布式追踪，许多开发人员也不熟悉它。\n图 1-1：可观测性的 \u0026#34;三大支柱\u0026#34;。 我们用这种方法工作了很久，以致于我们不常质疑它。但正如我们将看到的，“三大支柱” 并不是一种正确的结构化的可观测性方法。事实上，这个术语只是描述了某些技术碰巧被实现的方式，它掩盖了关于如何实际使用我们的工具的几个基本事实。\n什么是事务和资源？ 在我们深入探讨不同可观测性范式的利弊之前，重要的是要定义我们所观察的是什么。我们最感兴趣的分布式系统是基于互联网的服务。这些系统可以被分解成两个基本组成部分：事务和资源。\n事务（transaction） 代表了分布式系统需要执行的所有动作，以便服务能够做一些有用的事情。加载一个网页，购买一个装满物品的购物车，订购一个共享汽车：这些都是事务的例子。重要的是要理解，事务不仅仅是数据库事务。对每个服务的每个请求都是事务的一部分。\n例如，一个事务可能从一个浏览器客户端向一个网络代理发出 HTTP 请求开始。代理首先向认证系统发出请求以验证用户，然后将请求转发给前端应用服务器。应用服务器向各种数据库和后端系统发出若干请求。例如，一个消息系统：Kafka 或 AMQP，被用来排队等待异步处理的额外工作。所有这些工作都必须正确完成，以便向急切等待的用户提供结果。如果任何部分失败或耗时过长，其结果就是糟糕的体验，所以我们需要全面理解事务。\n一路下来，所有这些事务都在消耗资源（resource）。网络服务只能处理这么多的并发请求，然后它们的性能就会下降并开始失效。这些服务可以扩大规模，但它们与可能调用锁的数据库互动，造成瓶颈。数据库读取记录的请求可能会阻碍更新该记录的请求，反之亦然。此外，所有这些资源都是要花钱的，在每个月的月底，你的基础设施供应商会给你发账单。你消耗的越多，你的账单就越长。\n如何解决某个问题或提高服务质量？要么是开发人员修改事务，要么是运维人员修改可用资源。就这样了。这就是它的全部内容。当然，魔鬼就在细节中。\n第四章将详细介绍可观测性工具表示事务和资源的理想方式。但是，让我们回到描述迄今为止我们一直在做的非理想的方式。\n不是所有事情都是事务\n请注意，除了跨事务之外，还有其他的计算模型。例如，桌面和移动应用程序通常是基于反应器（reactor） 模型，用户在很长一段时间内连续互动。照片编辑和视频游戏就是很好的例子，这些应用有很长的用户会话，很难描述为离散的事务。在这些情况下，基于事件的可观测性工具，如真实用户监控（RUM），可以增强分布式追踪，以更好地描述这些长期运行的用户会话。\n也就是说，几乎所有基于互联网的服务都是建立在事务模式上的。由于这些是我们关注的服务，观察事务是我们在本书中描述的内容。附录 B 中对 RUM 进行了更详细的描述。\n可观测性的三大支柱 考虑到事务和资源，让我们来看看三大支柱模式。将这种关注点的分离标记为 “三大支柱”，听起来是有意的，甚至是明智的。支柱听起来很严肃，就像古代雅典的帕特农神庙。但这种安排实际上是一个意外。计算机的可观测性由多个孤立的、垂直整合的工具链组成，其真正的原因只是一个平庸的故事。\n这里有一个例子。假设有一个人想了解他们的程序正在执行的事务。所以他建立了一个日志工具：一个用于记录包含时间戳的消息的接口，一个用于将这些消息发送到某个地方的协议，以及一个用于存储和检索这些消息的数据系统。这足够简单。\n另一个人想监测在任何特定时刻使用的所有资源；他想捕捉指标。那么，他不会为此使用日志系统，对吗？他想追踪一个数值是如何随时间变化的，并跨越一组有限的维度。很明显，一大堆非结构化的日志信息与这个问题没有什么关系。因此，一个新的、完全独立的系统被创造出来，解决了生成、传输和存储度量的具体问题。\n另一个人想要识别性能瓶颈。同样，日志系统的非结构化性质使它变得无关紧要。识别性能瓶颈，比如一连串可以并行运行的操作，需要我们知道事务中每个操作的持续时间以及这些操作是如何联系在一起的。因此，我们建立了一个完全独立的追踪系统。由于新的追踪系统并不打算取代现有的日志系统，所以追踪系统被大量抽样，以限制在生产中运行第三个可观测系统的成本。\n这种零敲碎打的可观测性方法是人类工程的一个完全自然和可理解的过程。然而，它有其局限性，不幸的是，这些局限性往往与我们在现实世界中使用（和管理）这些系统的方式相悖。\n实际上我们如何观察系统？ 让我们来看看运维人员在调查问题时所经历的实际的、不加修饰的过程。\n调查包括两个步骤：注意到有事情发生，然后确定是什么原因导致了它的发生。\n当我们执行这些任务时，我们使用可观测性工具。但是，重要的是，我们并不是孤立地使用每一个工具。我们把它们全部放在一起使用。而在整个过程中，这些工具的孤立性给操作者带来了巨大的认知负担。\n通常情况下，当有人注意到一个重要的指标变得歪七扭八时，调查就开始了。在这种情况下，“毛刺” 是一个重要的术语，因为运维人员在这一点上所掌握的唯一信息是仪表盘上一条小线的形状，以及他们自己对该线的形状是否看起来 “正确” 的内部评估（图 1-2）。\n图 1-2：传统上，寻找不同数据集之间的关联性是一种可怕的经历。 在确定了线的形状开始看起来 “不对劲” 的那一点后，运维人员将眯起眼睛，试图找到仪表板上同时出现 “毛刺” 的其他线。然而，由于这些指标是完全相互独立的，运维人员必须在他们的大脑中进行比较，而不需要计算机的帮助。\n不用说，盯着图表，希望找到一个有用的关联，需要时间和脑力，更不用说会导致眼睛疲劳。\n就个人而言，我会用一把尺子或一张纸，只看什么东西排成一排。在 “现代” 仪表盘中，标尺现在是作为用户界面的一部分而绘制的线。但这只是一个粗略的解决方案。识别相关性的真正工作仍然必须发生在操作者的头脑中，同样没有计算机的帮助。\n在对问题有了初步的、粗略的猜测后，运维人员通常开始调查他们认为可能与问题有关的事务（日志）和资源（机器、进程、配置文件）（图 1-3）。\n图 1-3：找到与异常情况相关的日志也是一种可怕的经历。 在这里，计算机也没有真正的帮助。日志存储在一个完全独立的系统中，不能与任何指标仪表板自动关联。配置文件和其他服务的具体信息通常不在任何系统中，运维人员必须通过 SSH 或其他方式访问运行中的机器来查看它们。\n因此，运维人员再次被留下寻找相关性的工作，这次是在指标和相关日志之间。识别这些日志可能很困难；通常必须查阅源代码才能了解可能存在的日志。\n当找到一个（可能是，希望是）相关的日志，下一步通常是确定导致这个日志产生的事件链。这意味着要找到同一事务中的其他日志。\n缺乏关联性给操作者带来了巨大的负担。非结构化和半结构化的日志系统没有自动索引和按事务过滤日志的机制。尽管这是迄今为止运维人员最常见的日志工作流程，但他们不得不执行一系列特别的查询和过滤，将可用的日志筛选成一个子集，希望能代表事务的近似情况。为了成功，他们必须依靠应用程序开发人员来添加各种请求 ID 和记录，以便日后找到并拼接起来。\n在一个小系统中，这种重建事务的过程是乏味的，但却是可能的。但是一旦系统发展到包括许多横向扩展的服务，重建事务所需的时间就开始严重限制了调查的范围。图 1-4 显示了一个涉及许多服务的复杂事务。你将如何收集所有的日志？\n图 1-4：在传统的日志记录中，找到构成一个特定事务的确切日志需要花费大量的精力。如果系统变得足够大，这几乎成为不可能。 分布式追踪是一个好的答案。它实际上拥有自动重建一个事务所需的所有 ID 和索引工具。不幸的是，追踪系统经常被看作是用于进行延迟分析的利基工具。因此，发送给它们的日志数据相对较少。而且，由于它们专注于延迟分析，追踪系统经常被大量采样，使得它们与这类调查无关。\n不是三根柱子，而是一股绳子 不用说，这是一个无奈之举。上述工作流程确实代表了一种可怕的状态。但是，由于我们已经在这种技术体制下生活了这么久，我们往往没有认识到，与它可以做到的相比，它实际上是多么低效。\n今天，为了了解系统是如何变化的，运维人员必须首先收集大量的数据。然后，他们必须根据仪表盘显示和日志扫描等视觉反馈，用他们的头脑来识别这些数据的相关性。这是一种紧张的脑力劳动。如果一个计算机程序能够自动扫描和关联这些数据，那么这些脑力劳动就是不必要的。如果运维人员能够专注于调查他们的系统是如何变化的，而不需要首先确定什么在变化，那么他们将节省大量宝贵的时间。\n在编写一个能够准确执行这种变化分析的计算机程序之前，所有这些数据点都需要被连接起来。日志需要被连接在一起，以便识别事务。衡量标准需要与日志联系在一起，这样产生的统计数据就可以与它们所测量的事务联系起来。每个数据点都需要与底层系统资源 —— 软件、基础设施和配置细节相关联，以便所有事件都能与整个系统的拓扑结构相关联。\n最终的结果是一个单一的、可遍历的图，包含了描述分布式系统状态所需的所有数据，这种类型的数据结构将给分析工具一个完整的系统视图。与其说是不相连的数据的 “三根支柱”，不如说是相互连接的数据的一股绳子。\nOpenTelemetry 因此诞生。如图 1-5 所示，OpenTelemetry 是一个新的遥测系统，它以一种综合的方式生成追踪、日志和指标。所有这些连接的数据点以相同的协议一起传输，然后可以输入计算机程序，以确定整个数据集的相关性。\n图 1-5：OpenTelemetry 将所有这些孤立的信息，作为一个单一的、高度结构化的数据流连接起来。 这种统一的数据是什么样子的？在下一章，我们将把这三个支柱放在一边，从头开始建立一个新的模型。\n","relpermalink":"/book/opentelemetry-obervability/history/","summary":"第 1 章：可观测性的历史","title":"第 1 章：可观测性的历史"},{"content":"注意：在考虑这些要点时，请谨记“Code Review 标准 ”。\n设计 审查中最重要的是 CL 的整体设计。CL 中各种代码的交互是否有意义？此变更是属于您的代码库（codebase）还是属于库（library）？它是否与您系统的其他部分很好地集成？现在是添加此功能的好时机吗？\n功能 这个 CL 是否符合开发者的意图？开发者的意图对代码的用户是否是好的？ “用户”通常都是最终用户（当他们受到变更影响时）和开发者（将来必须“使用”此代码）。\n大多数情况下，我们希望开发者能够很好地测试 CL，以便在审查时代码能够正常工作。但是，作为审查者，仍然应该考虑边缘情况，寻找并发问题，尝试像用户一样思考，并确保您单纯透过阅读方式审查时，代码没有包含任何 bug。\n当要检查 CL 的行为会对用户有重大影响时，验证 CL 的变化将变得十分重要。例如 UI 变更。当您只是阅读代码时，很难理解某些变更会如何影响用户。如果在 CL 中打 patch 或自行尝试这样的变更太不方便，您可以让开发人员为您提供功能演示。\n另一个在代码审查期间特别需要考虑功能的时机，就是如果 CL 中存在某种并行编程，理论上可能导致死锁或竞争条件。通过运行代码很难检测到这些类型的问题，并且通常需要某人（开发者和审查者）仔细思考它们以确保不会引入问题。 （请注意，这也是在可能出现竞争条件或死锁的情况下，不使用并发模型的一个很好的理由——它会使代码审查或理解代码变得非常复杂。）\n复杂度 CL 是否已经超过它原本所必须的复杂度？针对任何层级的 CL 请务必确认这点——每行程序是否过于复杂？功能太复杂了吗？类太复杂了吗？ “太复杂”通常意味着阅读代码的人无法快速理解。也可能意味着“开发者在尝试调用或修改此代码时可能会引入错误。”\n其中一种复杂性就是过度工程（over-engineering），如开发人员使代码过度通用，超过它原本所需的，或者添加系统当前不需要的功能。审查者应特别警惕过度工程。未来的问题应该在它实际到达后解决，且届时才能更清晰的看到其真实样貌及在现实环境里的需求，鼓励开发人员解决他们现在需要解决的问题，而不是开发人员推测可能需要在未来解决的问题。\n测试 将要求单元、集成或端到端测试视为应该做的适当变更。通常，除非 CL 处理紧急情况 ，否则应在与生产代码相同的 CL 中添加测试。\n确保 CL 中的测试正确，合理且有用。测试并非用来测试自己本身，且我们很少为测试编写测试——人类必须确保测试有效。\n当代码被破坏时，测试是否真的会失败？如果代码发生变化时，它们会开始产生误报吗？每个测试都会做出简单而有用的断言吗？不同测试方法的测试是否适当分开？\n请记住，测试也是必须维护的代码。不要仅仅因为它们不是主二进制文件的一部分而接受测试中的复杂性。\n命名 开发人员是否为所有内容选择了好名字？一个好名字应该足够长，可以完全传达项目的内容或作用，但又不会太长，以至于难以阅读。\n注释 开发者是否用可理解的英语撰写了清晰的注释？所有注释都是必要的吗？通常，注释解释为什么某些代码存在时很有用，且不应该用来解释某些代码正在做什么。如果代码无法清楚到去解释自己时，那么代码应该变得更简单。有一些例外（正则表达式和复杂算法通常会从解释他们正在做什么事情的注释中获益很多），但大多数注释都是针对代码本身可能无法包含的信息，例如决策背后的推理。\n查看此 CL 之前的注释也很有帮助。也许有一个 TODO 现在可以删除，一个注释建议不要进行这种改变，等等。\n请注意，注释与类、模块或函数的文档不同，它们应该代表一段代码的目的，如何使用它，以及使用时它的行为方式。\n风格 Google 提供了所有主要语言的风格指南 ，甚至包括大多数小众语言。确保 CL 遵循适当的风格指南。\n如果您想改进风格指南中没有的一些样式点，请在评论前加上“Nit：”，让开发人员知道这是您认为可改善代码的小瑕疵，但不是强制性的。不要仅根据个人风格偏好阻止提交 CL。\nCL 的作者不应在主要风格变更中，包括与其他种类的变更。它会使得很难看到 CL 中的变更了什么，使合并和回滚更复杂，并导致其他问题。例如，如果作者想要重新格式化整个文件，让他们只将重新格式化变为一个 CL，其后再发送另一个包含功能变更的 CL。\n文档 如果 CL 变更了用户构建、测试、交互或发布代码的方式，请检查相关文档是否有更新，包括 README、g3doc 页面和任何生成的参考文档。如果 CL 删除或弃用代码，请考虑是否也应删除文档。如果缺少文档，请询问。\n每一行 查看分配给您审查的每行代码。有时如数据文件、生成的代码或大型数据结构等东西，您可以快速扫过。但不要快速扫过人类编写的类、函数或代码块，并假设其中的内容是 OK 的。显然，某些代码需要比其他代码更仔细的审查——这是您必须做出的判断——但您至少应该确定您理解所有代码正在做什么。\n如果您觉得这些代码太难以阅读了并减慢您审查的速度，您应该在您尝试继续审核前要让开发者知道这件事，并等待他们为程序做出解释、澄清。在 Google，我们聘请了优秀的软件工程师，您就是其中之一。如果您无法理解代码，那么很可能其他开发人员也不会。因此，当您要求开发人员澄清此代码时，您也会帮助未来的开发人员理解这些代码。\n如果您了解代码但觉得没有资格做某些部分的审查，请确保 CL 上有一个合格的审查人，特别是对于安全性、并发性、可访问性、国际化等复杂问题。\n上下文 在广泛的上下文下查看 CL 通常很有帮助。通常，代码审查工具只会显示变更的部分的周围的几行。有时您必须查看整个文件以确保变更确实有意义。例如，您可能只看到添加了四行新代码，但是当您查看整个文件时，您会看到这四行是添加在一个 50 行的方法里，现在确实需要将它们分解为更小的方法。\n在整个系统的上下文中考虑 CL 也很有用。这个 CL 是否改善了系统的代码健康状况，还是使整个系统更复杂，测试更少等等？不要接受降低系统代码运行状况的 CL。大多数系统通过许多小的变化而变得复杂，因此防止新变更引入即便很小的复杂性也非常重要。\n好的事情 如果您在 CL 中看到一些不错的东西，请告诉开发者，特别是当他们以一种很好的方式解决了您的的一个评论时。代码审查通常只关注错误，但也应该为良好实践提供鼓励。在指导方面，比起告诉他们他们做错了什么，有时更有价值的是告诉开发人员他们做对了什么。\n总结 在进行代码审查时，您应该确保：\n代码设计精良。 该功能对代码用户是有好处的。 任何 UI 变更都是合理的且看起来是好的。 其中任何并行编程都是安全的。 代码并不比它需要的复杂。 开发人员没有实现他们将来可能需要，但不知道他们现在是否需要的东西。 代码有适当的单元测试。 测试精心设计。 开发人员使用了清晰的名称。 评论清晰有用，且大多用来解释为什么而不是做什么。 代码有适当记录成文件（通常在 g3doc 中）。 代码符合我们的风格指南。 确保查看您被要求查看的每一行代码，查看上下文，确保您提高代码健康状况，并赞扬开发人员所做的好事。\n","relpermalink":"/book/eng-practices/review/reviewer/looking-for/","summary":"注意：在考虑这些要点时，请谨记“Code Review 标准 ”。 设计 审查中最重要的是 CL 的整体设计。CL 中各种代码的交互是否有意义？此变更是属于您的代码库（codebase）还是属于库（library）？它是否与您系","title":"Code Review 要点"},{"content":"由于 eBPF 允许在 Linux 内核中运行自定义代码，在解释 eBPF 之前我需要确保你对内核的作用有所了解。然后我们将讨论为什么在修改内核行为这件事情上，eBPF 改变了游戏规则。\nLinux 内核 Linux 内核是应用程序和它们所运行的硬件之间的软件层。应用程序运行在被称为用户空间的非特权层，它不能直接访问硬件。相反，应用程序使用系统调用（syscall）接口发出请求，要求内核代表它行事。这种硬件访问可能涉及到文件的读写，发送或接收网络流量，或者只是访问内存。内核还负责协调并发进程，使许多应用程序可以同时运行。\n应用程序开发者通常不直接使用系统调用接口，因为编程语言给了我们更高级别的抽象和标准库，开发者更容易掌握这些接口。因此，很多人都不知道在程序运行时内核做了什么。如果你想了解内核调用频率，你可以使用 strace 工具来显示程序所做的所有系统调用。这里有一个例子，用 cat 从文件中读取 hello 这个词并将其写到屏幕上涉及到 100 多个系统调用：\nliz@liz-ebpf-demo-1:~$ strace -c cat liz.txt hello % time seconds usecs/call calls errors syscall ------ ----------- ----------- --------- --------- ------------- 0.00 0.000000\t0 5\tread 0.00 0.000000\t0 1\twrite 0.00 0.000000\t0 21\tclose 0.00 0.000000\t0 20\tfstat 0.00 0.000000\t0 23\tmmap 0.00 0.000000\t0 4\tmprotect 0.00 0.000000\t0 2\tmunmap 0.00 0.000000\t0 3\tbrk 0.00 0.000000\t0 4\tpread64 0.00 0.000000\t0 1\t1 access 0.00 0.000000\t0 1\texecve 0.00 0.000000\t0 2\t1 arch_prctl 0.00 0.000000\t0 1\tfadvise64 0.00 0.000000\t0 19\topenat ------ ----------- ----------- --------- --------- ------------- 100.00 0.000000 107 2 total 由于应用程序在很大程度上依赖于内核，这意味着如果我们能够观测到应用程序与内核的交互，我们就可以了解到很多关于它的行为方式。例如，如果你能够截获打开文件的系统调用，你就可以准确地看到任何应用程序访问了哪些文件。但是，怎么才能做到这种拦截呢？让我们考虑一下，如果我们想修改内核，添加新的代码，在系统调用时创建某种输出，会涉及到什么问题。\n向内核添加新功能 Linux 内核很复杂，在写这篇文章的时候有大约 3000 万行代码 1。对任何代码库进行修改都需要对现有的代码有一定的熟悉，所以除非你已经是一个内核开发者，否则这很可能是一个挑战。\n但你将面临的挑战并不是纯粹的技术问题。Linux 是一个通用的操作系统，在不同的环境和情况下使用。这意味着，如果你想对内核进行修改，这并不是简单地写出能用的代码。它必须被社区（更确切地说，是被 Linux 的创造者和主要开发者 Linus Torvalds）接受，你的改变将是为了大家的更大利益。而这并不是必然的——提交的内核补丁只有三分之一被接受 2。\n假如，你已经想出了一个好方法来拦截打开文件的系统调用。经过几个月的讨论和一些艰苦的开发工作，让我们想象一下，这个变化被接受到内核中。很好！但是，要到什么时候它才会出现在每个人的机器上呢？\n每隔两三个月就会有一个新的 Linux 内核版本，但是即使一个变化已经进入了其中一个版本，它仍然需要一段时间才能在大多数人的生产环境中使用。这是因为我们大多数人并不直接使用 Linux 内核——我们使用像 Debian、Red Hat、Alpine、Ubuntu 等 Linux 发行版，它们将 Linux 内核的一个版本与其他各种组件打包在一起。你可能会发现，你最喜欢的发行版使用的是几年前的内核版本。\n例如，很多企业用户都采用红帽 ® Enterprise Linux®（RHEL）。在撰写本文时，目前的版本是 RHEL8.5，发行日期为 2021 年 11 月。这使用的是基于 4.18 版本的内核。这个内核是在 2018 年 8 月发布的。\n如 图 2-1 中的漫画所示，将新功能从想法阶段转化为生产环境中的 Linux 内核，需要数年时间 3。\n图 2-1. 向内核添加功能（Isovalent 公司的 Vadim Shchekoldin 绘制的漫画） 内核模块 如果你不想等上好几年才把你的改动写进内核，还有一个选择。Linux 内核可以接受内核模块（module），这些模块可以根据需要加载和卸载。如果你想改变或扩展内核行为，编写一个模块是理所当然的。在我们打开文件的系统调用的例子中，你可以写一个内核模块来实现。\n这里最大的挑战是，这仍然是全面的内核编程。用户在使用内核模块时历来非常谨慎，原因很简单：如果内核代码崩溃了，就会导致机器和上面运行的所有东西瘫痪。用户如何确保内核模块可以安全运行呢？\n“安全运行”并不仅仅意味着不崩溃——用户想知道内核模块从安全角度来看是否安全。是否包括攻击者可以利用的漏洞？我们是否相信模块的作者不会在其中加入恶意代码？因为内核是特权代码，它可以访问机器上的一切，包括所有的数据，所以内核中的恶意代码将是一个令人担忧的严重问题。这也适用于内核模块。\n考虑到内核的安全性，这就是为什么 Linux 发行商需要这么长时间来发布新版本的一个重要原因。如果其他人已经在各种情况下运行了数月或数年的内核版本，那些漏洞可能已经被修复。发行版的维护者可以有一些信心，他们提供给用户 / 客户的内核是经过加固的，也就是说，可以安全运行。\neBPF 提供了一个非常不同的安全方法：eBPF 验证器（verifier），它确保一个 eBPF 程序只有在安全运行的情况下才被加载。\neBPF 验证和安全 由于 eBPF 允许我们在内核中运行任意代码，需要有一种机制来确保它的安全运行，不会使用户的机器崩溃，也不会损害他们的数据。这个机制就是 eBPF 验证器。\n验证器对 eBPF 程序进行分析，以确保无论输入什么，它都会在一定数量的指令内安全地终止。例如，如果一个程序解除对一个指针的定义，验证器要求该程序首先检查指针，以确保它不是空的（null）。解除对指针的引用意味着 “查找这个地址的值”，而空值或零值不是一个有效的查找地址。如果你在一个应用程序中解引用一个空指针，该应用程序就会崩溃；而在内核中解引用一个空指针则会使整个机器崩溃，所以避免这种情况至关重要。\n验证也确保了 eBPF 程序只能访问其应该访问的内存。例如，有一个 eBPF 程序在网络堆栈中触发，并通过内核的 套接字缓冲区（socket buffer），其中包括正在传输的数据。有一些特殊的辅助函数，如 bpf_skb_load_bytes()，这个 eBPF 程序可以调用，从套接字缓冲区读取字节数据。另一个由系统调用触发的 eBPF 程序，没有可用的套接字缓冲区，将不允许使用这个辅助函数。验证器还确保程序只读取套接字缓冲区内的数据字节——它不允许访问任意的内存。这里的目的是确保 eBPF 程序是安全的。\n当然，仍然有可能编写一个恶意的 eBPF 程序。如果你可以出于合法的原因观测数据，你也可以出于非法的原因观测它。要注意只从可验证的来源加载可信的 eBPF 程序，并且只将管理 eBPF 工具的权限授予你信任的拥有 root 权限的人。\neBPF 程序的动态加载 eBPF 程序可以动态地加载到内核中和从内核中删除。不管是什么原因导致该事件的发生，一旦它们被附加到一个事件上就会被该事件所触发。例如，如果你将一个程序附加到打开文件的系统调用，那么只要任何进程试图打开一个文件，它就会被触发。当程序被加载时，该进程是否已经在运行，这并不重要。\n这也是使用 eBPF 的可观测性或安全工具的巨大优势之一——即刻获得了对机器上发生的一切事件的可视性。\n此外，如 图 2-2 所示，人们可以通过 eBPF 非常快速地创建新的内核功能，而不要求其他 Linux 用户都接受同样的变更。\n图 2-2. 用 eBPF 添加内核功能（漫画：Vadim Shchekoldin Isovalent） 现在你已经看到了 eBPF 是如何允许对内核进行动态的、自定义的修改的，让我们来看看如果你想写一个 eBPF 程序会涉及哪些内容。\n参考 Linux 5.12 有大约 2880 万行代码，Phoronix（2021 年 3 月）。 ↩︎\nYujuan Jiang 等人，《我的补丁能用了吗？要多久？》（论文，2013 年）。根据这篇研究论文，33% 的补丁将在 3 - 6 个月后被接受。 ↩︎\n值得庆幸的是，现有功能的安全补丁会更快地被提供。 ↩︎\n","relpermalink":"/book/what-is-ebpf/changing-the-kernel-is-hard/","summary":"由于 eBPF 允许在 Linux 内核中运行自定义代码，在解释 eBPF 之前我需要确保你对内核的作用有所了解。然后我们将讨论为什么在修改内核行为这件事情上，eBPF 改变了游戏规则。 Linux 内核 Linux 内核是应用程序和它们所运行的硬件之间的软","title":"第二章：修改内核很困难"},{"content":"为什么提交小型 CL? 小且简单的 CL 是指：\n审查更快。审查者更容易抽多次五分钟时间来审查小型 CL，而不是留出 30 分钟来审查一个大型 CL。 审查得更彻底。如果是大的变更，审查者和提交者往往会因为大量细节的讨论翻来覆去而感到沮丧——有时甚至到了重要点被遗漏或丢失的程度。 不太可能引入错误。由于您进行的变更较少，您和您的审查者可以更轻松有效地推断 CL 的影响，并查看是否已引入错误。 如果被拒绝，减少浪费的工作。如果您写了一个巨大的 CL，您的评论者说整个 CL 的方向都错误了，你就浪费了很多精力和时间。 更容易合并。处理大型 CL 需要很长时间，在合并时会出现很多冲突，并且必须经常合并。 更容易设计好。打磨一个小变更的设计和代码健康状况比完善一个大变更的所有细节要容易得多。 减少对审查的阻碍。发送整体变更的自包含部分可让您在等待当前 CL 审核时继续编码。 更简单的回滚。大型 CL 更有可能触及在初始 CL 提交和回滚 CL 之间更新的文件，从而使回滚变得复杂（中间的 CL 也可能需要回滚）。 请注意，审查者可以仅凭 CL 过大而自行决定完全拒绝您的变更。通常他们会感谢您的贡献，但要求您以某种方式将其 CL 改成一系列较小的变更。在您编写完变更后，或者需要花费大量时间来讨论为什么审查者应该接受您的大变更，这可能需要做很多工作。首先编写小型 CL 更容易。\n什么是小型 CL？ 一般来说，CL 的正确大小是自包含的变更。这意味着：\nCL 进行了一项最小的变更，只解决了一件事。通常只是功能的一部分，而不是一个完整的功能。一般来说，因为编写过小的 CL 而犯错也比过大的 CL 犯错要好。与您的审查者讨论以确定可接受的大小。 审查者需要了解的关于 CL 的所有内容（除了未来的开发）都在 CL 的描述、现有的代码库或已经审查过的 CL 中。 对其用户和开发者来说，在签入 CL 后系统能继续良好的工作。 CL 不会过小以致于其含义难以理解。如果您添加新 API，则应在同一 CL 中包含 API 的用法，以便审查者可以更好地了解 API 的使用方式。这也可以防止签入未使用的 API。 关于多大算“太大”没有严格的规则。对于 CL 来说，100 行通常是合理的大小，1000 行通常太大，但这取决于您的审查者的判断。变更中包含的文件数也会影响其“大小”。一个文件中的 200 行变更可能没问题，但是分布在 50 个文件中通常会太大。\n请记住，尽管从开始编写代码开始就您就已经密切参与了代码，但审查者通常不清楚背景信息。对您来说，看起来像是一个可接受的大小的 CL 对您的审查者来说可能是压倒性的。如有疑问，请编写比您认为需要编写的要小的 CL。审查者很少抱怨收到过小的 CL 提交。\n什么时候大 CL 是可以的？ 在某些情况下，大变更也是可以接受的：\n您通常可以将整个文件的删除视为一行变更，因为审核人员不需要很长时间审核。 有时一个大的 CL 是由您完全信任的自动重构工具生成的，而审查者的工作只是检查并确定想要这样的变更。但这些 CL 可以更大，尽管上面的一些警告（例如合并和测试）仍然适用。 按文件拆分 拆分 CL 的另一种方法是对文件进行分组，这些文件需要不同的审查者，否则就是自包含的变更。\n例如：您发送一个 CL 以修改协议缓冲区，另一个 CL 发送变更使用该原型的代码。您必须在代码 CL 之前提交 proto CL，但它们都可以同时进行审查。如果这样做，您可能希望通知两组审查者审查您编写的其他 CL，以便他们对您的变更具有更充足的上下文。\n另一个例子：你发送一个 CL 用于代码更改，另一个用于使用该代码的配置或实验；如果需要，这也更容易回滚，因为配置/实验文件有时会比代码变更更快地推向生产。\n分离出重构 通常最好在功能变更或错误修复的单独 CL 中进行重构。例如，移动和重命名类应该与修复该类中的错误的 CL 不同。审查者更容易理解每个 CL 在单独时引入的更改。\n但是，修复本地变量名称等小清理可以包含在功能变更或错误修复 CL 中。如果重构大到包含在您当前的 CL 中，会使审查更加困难的话，需要开发者和审查者一起判断是否将其拆开。\n将相关的测试代码保存在同一个 CL 中 避免将测试代码拆分为单独的 CL。验证代码修改的测试应该进入相同的 CL，即使它增加了代码行数。\n但是，独立的测试修改可以首先进入单独的 CL，类似于重构指南 。包括：\n使用新测试验证预先存在的已提交代码。 重构测试代码（例如引入辅助函数）。 引入更大的测试框架代码（例如集成测试）。 不要破坏构建 如果您有几个相互依赖的 CL，您需要找到一种方法来确保在每次提交 CL 后整个系统能够继续运作。否则可能会在您的 CL 提交的几分钟内打破所有开发人员的构建（如果您之后的 CL 提交意外出错，时间可能会甚至更长）。\n如果不能让它足够小 有时你会遇到看起来您的 CL 必须如此庞大，但这通常很少是正确的。习惯于编写小型 CL 的提交者几乎总能找到将功能分解为一系列小变更的方法。\n在编写大型 CL 之前，请考虑在重构 CL 之前是否可以为更清晰的实现铺平道路。与你的同伴聊聊，看看是否有人想过如何在小型 CL 中实现这些功能。\n如果以上的努力都失败了（这应该是非常罕见的），那么请在事先征得审查者的同意后提交大型 CL，以便他们收到有关即将发生的事情的警告。在这种情况下，做好完成审查过程需要很长一段时间的准备，对不引入错误保持警惕，并且在编写测试时要更下功夫。\n","relpermalink":"/book/eng-practices/review/developer/small-cls/","summary":"为什么提交小型 CL? 小且简单的 CL 是指： 审查更快。审查者更容易抽多次五分钟时间来审查小型 CL，而不是留出 30 分钟来审查一个大型 CL。 审查得更彻底。如果是大的变更，审查者和提交者往往会因为大量细节的讨论翻来覆去","title":"小型 CL"},{"content":"本教程在SPIRE Envoy-X.509 教程 的基础上构建，演示如何使用 SPIRE 代替 X.509 SVID 进行工作负载的 JWT SVID 身份验证。在这个教程中展示了实现 JWT SVID 身份验证所需的更改，因此你应该首先运行或至少阅读 X.509 教程。\n为了说明 JWT 身份验证，我们在 Envoy X.509 教程中使用的每个服务中添加了 sidecar。每个 sidecar 都充当 Envoy 的外部授权过滤器 。\n如图所示，前端服务通过 Envoy 实例连接到后端服务，这些服务之间通过 Envoy 建立的 mTLS 连接进行通信。Envoy 通过携带的 JWT-SVID 进行身份验证的 HTTP 请求通过 mTLS 连接发送，并由 SPIRE Agent 提供和验证。\n在本教程中，你将学习如何：\n将 Envoy JWT Auth Helper gRPC 服务添加到 Envoy X.509 教程中现有的前端和后端服务中 将外部授权过滤器添加到 Envoy 配置中，将 Envoy 连接到 Envoy JWT Auth Helper 在 SPIRE Server 上为 Envoy JWT Auth Helper 实例创建注册条目 使用 SPIRE 测试成功的 JWT 身份验证 先决条件 支持外部 IP 此教程需要一个可以分配外部 IP（例如metallb ）的负载均衡器。\n$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.7/config/manifests/metallb-native.yaml 等待 metallb 启动：\n$ kubectl wait --namespace metallb-system \\ --for=condition=ready pod \\ --selector=app=metallb \\ --timeout=90s 应用 metallb 配置：\n$ kubectl apply -f ../envoy-x509/metallb-config.yaml Auth helper 镜像 使用 Envoy-jwt-auth-helper 实现了一个外部授权过滤器，提供了一个脚本来方便地使用 kind 或 minikube 构建和导入。\n$ bash ./scripts/build-helper.sh kind 之前的 SPIRE 安装 在继续之前，请查看以下内容：\n你需要访问通过 SPIRE Envoy-X.509 教程*配置的 Kubernetes 环境。你也可以使用下面描述的 pre-set-env.sh 脚本创建 Kubernetes 环境。 本教程所需的 YAML 文件可以在 https://github.com/spiffe/spire-tutorials 的 k8s/envoy-jwt 目录中找到。如果你尚未克隆SPIRE Envoy-X.509 教程的存储库，请立即克隆它。 如果 Kubernetes 的SPIRE Envoy-X.509 教程环境不可用，你可以使用以下脚本创建它，并将其作为本教程的起点。从 k8s/envoy-jwt 目录中运行以下命令：\n$ bash scripts/pre-set-env.sh 该脚本将创建集群中 SPIRE Server 和 SPIRE Agent 所需的所有资源，然后将为 SPIRE Envoy X.509 教程创建所有资源，这是本 SPIRE Envoy JWT 教程的基本场景。\n第 1 部分：部署更新和新资源 假设 SPIRE Envoy X.509 教程是一个起点，需要更新一些资源并创建其他资源。目标是通过 JWT SVID 对工作负载进行身份验证。Envoy 实例之间已经建立了 mTLS 连接，可以在请求头中传输 JWT SVID。因此，缺失的部分是如何获取 JWT 并将其插入请求中，以及在另一侧进行验证。本教程中应用的解决方案包括在 Envoy 上配置外部授权过滤器，该过滤器根据配置模式注入或验证 JWT SVID。关于此示例服务器的详细信息，请参见关于 Envoy JWT Auth Helper 。\n关于 Envoy JWT Auth Helper Envoy JWT Auth Helper（auth-helper 服务）是一个简单的 gRPC 服务，实现了 Envoy 的 External Authorization Filter。它是为本教程开发的，以演示如何注入或验证 JWT SVID。\n对于发送到 Envoy 转发代理的每个 HTTP 请求，Envoy JWT Auth Helper 从 SPIRE Agent 获取 JWT-SVID，并将其作为新的请求头注入，然后发送给 Envoy。另一方面，当 HTTP 请求到达反向代理时，Envoy External Authorization 模块将请求发送到 Envoy JWT Auth Helper，后者从标头中提取 JWT-SVID，然后连接到 SPIRE Agent 执行验证。验证成功后，请求将返回给 Envoy。如果验证失败，则拒绝请求。\n在内部，Envoy JWT Auth Helper 利用go-spiffe 库，该库公开了获取和验证 JWT SVID 所需的所有功能。以下是代码的主要部分：\n// 使用 SPIRE 提供的 Unix 域套接字创建配置源的选项。 clientOptions := workloadapi.WithClientOptions(workloadapi.WithAddr(c.SocketPath)) ... // 创建 workloadapi.JWTSource 实例以从工作负载 API 中获取最新的 JWT 批。 jwtSource, err := workloadapi.NewJWTSource(context.Background(), clientOptions) if err != nil { log.Fatalf(\u0026#34;无法创建JWTSource：%v\u0026#34;, err) } defer jwtSource.Close() ... // 获取将添加到请求头中的 JWT-SVID。 jwtSVID, err := a.config.jwtSource.FetchJWTSVID(ctx, jwtsvid.Params{ Audience: a.config.audience, }) if err != nil { return forbiddenResponse(\u0026#34;PERMISSION_DENIED\u0026#34;), nil } ... // 解析并验证令牌与 jwtSource 获取的批对比。 _, err := jwtsvid.ParseAndValidate(token, a.config.jwtSource, []string{a.config.audience}) if err != nil { return forbiddenResponse(\u0026#34;PERMISSION_DENIED\u0026#34;), nil } 注意：workloadapi 和 jwtsvid 是从 go-spiffe 库导入的。\n更新部署 auth-helper 服务使得 Envoy 能够注入或验证携带 JWT-SVID 的身份验证头，如上所述。在这些部分中，k8s/backend/config/envoy.yaml 中的 YAML 文件片段说明了将 JWT 身份验证添加到在SPIRE Envoy-X.509 教程 中定义的 backend 服务所需的更改。其他 YAML 文件也对其他两个服务（frontend 和 frontend-2）应用了相同的更改，但是本文档中不会详细描述这些更改，以避免不必要的重复。你无需手动对 YAML 文件进行这些更改。新文件已包含在 k8s/envoy-jwt/k8s 目录中。必须将此新的 auth-helper 服务作为 sidecar 添加，并且必须配置它与 SPIRE Agent 通信。通过挂载卷来共享 SPIRE Agent 提供的 Unix 域套接字来实现这一目标。通过新的第二个卷，可以访问使用服务配置定义的 configmap。下面是来自 containers 部分的代码片段，描述了这些更改：\n- name: auth-helper image: envoy-jwt-auth-helper:latest imagePullPolicy: IfNotPresent args: [\u0026#34;-config\u0026#34;, \u0026#34;/run/envoy-jwt-auth-helper/config/envoy-jwt-auth-helper.conf\u0026#34;] ports: - containerPort: 9010 volumeMounts: - name: envoy-jwt-auth-helper-config mountPath: \u0026#34;/run/envoy-jwt-auth-helper/config\u0026#34; readOnly: true - name: spire-agent-socket mountPath: /run/spire/sockets readOnly: true spire-agent-socket 卷已在部署中定义，无需再次添加。要将 configmap envoy-jwt-auth-helper-config 添加到 volumes 部分，可以使用以下代码：\n- name: envoy-jwt-auth-helper-config configMap: name: be-envoy-jwt-auth-helper-config 添加外部授权过滤器 接下来，在 Envoy 配置中需要一个外部授权过滤器，该过滤器连接到新的服务。这个新的 HTTP 过滤器调用了刚刚添加到部署中的 auth-helper 服务：\nhttp_filters: - name: envoy.filters.http.ext_authz typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthz transport_api_version: V3 grpc_service: envoy_grpc: cluster_name: ext-authz timeout: 0.5s 这是外部授权过滤器的相应集群配置：\n- name: ext-authz connect_timeout: 1s type: strict_dns http2_protocol_options: {} load_assignment: cluster_name: ext-authz endpoints: - lb_endpoints: - endpoint: address: socket_address: address: 127.0.0.1 port_value: 9010 应用新资源 为了使新的配置生效，需要重新部署服务。让我们删除 backend 和 frontend 部署以便更新它们：\n$ kubectl delete deployment backend $ kubectl delete deployment frontend 确保当前工作目录是 .../spire-tutorials/k8s/envoy-jwt，然后使用以下命令部署新资源：\n$ kubectl apply -k k8s/. configmap/backend-envoy configured configmap/be-envoy-jwt-auth-helper-config created configmap/fe-envoy-jwt-auth-helper-config created …","relpermalink":"/book/spiffe-and-spire/examples/envoy-jwt/","summary":"本教程在SPIRE Envoy-X.509 教程 的基础上构建，演示如何使用 SPIRE 代替 X.509 SVID 进行工作负载的 JWT SVID 身份验证。在这个教程中展示了实现 JWT SVID 身份验证所需的更改，因此你应该首先运行或至少阅读 X.509 教程。 为了说明 JWT 身份验证，我们在 Envoy X.509","title":"使用 Envoy 和 JWT-SVID 进行安全的微服务通信"},{"content":"JWT-SVID 是 SPIFFE 规范集中的第一个基于令牌的 SVID。旨在在解决跨第 7 层边界断言身份时提供即时价值，与现有应用程序和库的兼容性是核心要求。\nJWT-SVID 是一种带有一些限制的标准 JWT 令牌。JOSE 在安全实现上一直存在困难，在安全社区中被认为是一项可能在部署和实现中引入漏洞的技术。JWT-SVID 采取措施尽量减轻这些问题，同时不破坏与现有应用程序和库的兼容性。\nJWT-SVID 是使用 JWS 紧凑序列化的 JSON Web Signature (JWS) 数据结构。不得使用 JWS JSON 序列化。\nJOSE 头 历史上，JOSE 头的密码灵活性引入了一系列流行的 JWT 实现中的漏洞。为了避免这样的陷阱，本规范限制了一些最初允许的内容。本节描述了允许的注册头以及其值。JWT-SVID JOSE 头中未描述的任何头部，无论是注册的还是私有的，都不得包含在其中。\n只支持 JWS。\n算法 alg 头必须设置为 RFC 7518 第 3.3 、3.4 或 3.5 节定义的值之一。接收到 alg 参数设置为其他值的令牌的验证器必须拒绝该令牌。\n支持的 alg 值为：\nalg 参数值 数字签名算法 RS256 使用 SHA-256 的 RSASSA-PKCS1-v1_5 RS384 使用 SHA-384 的 RSASSA-PKCS1-v1_5 RS512 使用 SHA-512 的 RSASSA-PKCS1-v1_5 ES256 使用 P-256 和 SHA-256 的 ECDSA ES384 使用 P-384 和 SHA-384 的 ECDSA ES512 使用 P-521 和 SHA-512 的 ECDSA PS256 使用 SHA-256 和 SHA-256 MGF1 的 RSASSA-PSS PS384 使用 SHA-384 和 SHA-384 MGF1 的 RSASSA-PSS PS512 使用 SHA-512 和 SHA-512 MGF1 的 RSASSA-PSS 密钥 ID kid 头是可选的。\n类型 typ 头是可选的。如果设置，其值必须是 JWT 或 JOSE。\nJWT 声明 JWT-SVID 规范没有引入任何新的声明，但它对 RFC 7519 定义的注册声明设置了一些限制。未在本文档中描述的注册声明，以及私有声明，可以根据实现者的需求使用。但应注意，在未定义的声明上依赖可能会影响互操作性，因为生成和使用令牌的应用程序必须独立协商。在引入其他声明时，实现者应谨慎行事，并仔细考虑其对 SVID 互操作性的影响，特别是在实现者无法控制生产者和消费者的环境中。如果绝对有必要使用其他声明，建议按照 RFC 7519 的建议使其抗冲突。\n本节概述了 JWT-SVID 规范对现有注册声明所施加的要求和限制。\n主题 sub 声明必须设置为其所属工作负载的 SPIFFE ID。这是对工作负载身份进行断言的主要声明。\n受众 aud 声明必须存在，包含一个或多个值。验证器必须拒绝没有设置 aud 声明的令牌，或者验证器所识别的值不存在作为 aud 元素。强烈建议在正常情况下将值的数量限制为一个。有关更多信息，请参见安全注意事项部分。\n所选择的值是特定于站点的，并且应该限定在要呈现给的服务范围内。例如，reports 或 spiffe://example.org/reports 是适用于向报告服务呈现的令牌的合适值。不建议使用 production 或 spiffe://example.org/ 等值，因为它们的范围很广，如果 production 中的单个服务受到损害，则可能导致冒充。\n过期时间 exp 声明必须设置，验证器必须拒绝没有此声明的令牌。鼓励实施者将有效期保持尽可能小，但本规范对其值没有设置任何硬上限。\n令牌签名和验证 JWT-SVID 的签名和验证语义与常规 JWT/JWS 相同。验证器在处理之前必须确保 alg 头设置为支持的值。\nJWT-SVID 的签名是根据 RFC 7519 第 7 节 中概述的步骤进行计算和验证的。aud 和 exp 声明必须存在，并根据 RFC 7519 第 4.1.3 和 4.1.4 节进行处理。接收到没有设置 aud 和 exp 声明的令牌的验证器必须拒绝该令牌。\n令牌传输 本节描述了 JWT-SVID 可以从一个工作负载传输到另一个工作负载的方式。\n序列化 JWT-SVID 必须使用 RFC 7515 第 3.1 节 中描述的紧凑序列化方法进行序列化，正如 RFC 7519 第 1 节 所要求的那样。请注意，这排除了使用 JWS 未保护的头部，正如 JOSE 头 部分所规定的那样。\nHTTP 通过 HTTP 传输的 JWT-SVID 应该在“Authorization”头部（HTTP/2 的“authorization”）中使用“Bearer”身份验证方案进行传输，该方案在 RFC 6750 第 2.1 节 中定义。例如，在 HTTP/1.1 中使用 Authorization: Bearer \u0026lt;serialized_token\u0026gt;，在 HTTP/2 中使用 authorization: Bearer \u0026lt;serialized_token\u0026gt;。\ngRPC gRPC 协议使用 HTTP/2。因此，HTTP 部分 中的 HTTP 传输指南同样适用。具体而言，gRPC 实现应该使用值为 Bearer \u0026lt;serialized_token\u0026gt; 的元数据键 authorization。\n在 SPIFFE Bundle 中的表示 本节描述了 JWT-SVID 签名密钥如何发布到和从 SPIFFE Bundle 中消费。有关 SPIFFE Bundle 的更多信息，请参见 SPIFFE 信任域和 Bundle 规范。\n发布 SPIFFE Bundle 元素 给定信任域的 JWT-SVID 签名密钥在 SPIFFE Bundle 中表示为符合 RFC 7517 的 JWK 条目，每个签名密钥一个条目。\n每个 JWK 条目的 use 参数必须设置为 jwt-svid。此外，每个 JWK 条目的 kid 参数必须设置。\n使用 SPIFFE Bundle SPIFFE Bundle 可能包含许多不同类型的 JWK 条目。在使用这些条目进行验证之前，实现必须提取 JWT-SVID 特定的密钥。可以通过其 use 参数的值来识别表示 JWT-SVID 签名密钥的条目，该值必须为 jwt-svid。如果没有具有 jwt-svid 使用值的条目，则表示 Bundle 的信任域不支持 JWT-SVID。\n提取 JWK 条目后，可以根据 RFC 7517 描述的方式直接用于 JWT-SVID 验证。\n安全注意事项 本节概述了在使用 JWT-SVID 时实施者和用户应考虑的安全注意事项。\n重放保护 作为承载令牌，JWT-SVID 容易受到重放攻击的影响。通过要求设置 aud 和 exp 声明，本规范已经采取措施改善了这种情况，但在保留与 RFC 7515 的验证兼容性的同时无法完全解决。理解这个风险非常重要。建议设置较短的 exp 声明值。某些用户可能希望利用 jti 声明，尽管增加了额外的开销。虽然本规范允许使用 jti 声明，但应注意，JWT-SVID 验证器不必跟踪 jti 的唯一性。\n受众 赋予 JWT-SVID 接收方隐式信任。发送到一个受众的令牌可以被重播到另一个受众，如果存在多个受众。例如，如果 Alice 有一个包含 Bob 和 Chuck 作为受众的令牌，并将该令牌传输给 Chuck，那么 Chuck 可以通过将相同的令牌发送给 Bob 来冒充 Alice。因此，在发行具有多个受众的 JWT-SVID 时应格外小心。强烈建议使用单个受众的 JWT-SVID 令牌，以限制重放的范围。\n传输安全 JWT-SVID 与其他承载令牌方案存在相同的风险，即令牌被拦截后，攻击者可以利用其可重放性获得 JWT-SVID 所授予的完全权限。虽然通过 exp 声明强制令牌过期可以减轻风险，但总会存在一个漏洞窗口。因此，在传输 JWT-SVID 的通信渠道上的每个跳跃/链接都应提供机密性（例如，从工作负载到负载均衡器，从负载均衡器到另一个工作负载）。值得注意的例外是非网络链接，其具有合理的安全假设，例如在同一主机上的两个进程之间的 Unix 域套接字。\n附录 A. 验证参考 以下表格为正在实施 JWT-SVID 验证器的任何人提供快速参考。如果使用现成的库，则实施者有责任确保采取了以下验证步骤。\n此外，请参阅 JWT-SVID Schema 获取更正式的参考。\n字段 类型 要求 alg Header 设置为算法表中的一个值。否则拒绝。 aud Claim 至少有一个值存在。用户应提前配置至少一个可接受的值。否则拒绝。 exp Claim 必须设置。不能过期（允许有一小段宽限期）。否则拒绝。 ","relpermalink":"/book/spiffe-and-spire/standard/jwt-svid/","summary":"JWT-SVID 是 SPIFFE 规范集中的第一个基于令牌的 SVID。旨在在解决跨第 7 层边界断言身份时提供即时价值，与现有应用程序和库的兼容性是核心要求。 JWT-SVID 是一种带有一些限制的标准 JWT 令牌。JOSE 在安全实现上一直存在困难，在安全","title":"JWT SPIFFE 可验证身份文档"},{"content":"本教程展示了如何对由两个不同 SPIRE 服务器识别的两个 SPIFFE 标识的工作负载进行身份验证。\n本文的第一部分演示了如何通过显示 SPIRE 配置文件更改和 spire-server 命令来配置 SPIFFE 联邦，以设置股票报价 web 应用的前端和服务后端为例。本文的第二部分列出了你可以在此教程目录中包含的 Docker Compose 文件中运行的步骤，以显示场景的实际操作。\n在本教程中，你将学到如何：\n配置每个 SPIRE 服务器以使用 SPIFFE 身份验证和 Web PKI 身份验证公开其 SPIFFE 联邦捆绑点。 配置 SPIRE 服务器以从彼此检索信任捆绑点。 使用不同的信任域引导两个 SPIRE 服务器之间的联合。 为工作负载创建注册条目，以便它们可以与其他信任域进行联合。 先决条件 SPIFFE 联邦的基线组件包括：\n运行版本为 1.5.1 的两个 SPIRE 服务器实例。 运行版本为 1.5.1 的两个 SPIRE 代理。一个连接到一个 SPIRE 服务器，另一个连接到另一个 SPIRE 服务器。 两个需要通过 mTLS 进行通信的工作负载，并使用工作负载 API 获取 SVID 和信任捆绑点。 场景 假设我们有一个股票 broker（经纪人）的 web 应用程序，它希望从股票 market web 服务提供商那里获取股票报价并显示。情景如下：\n用户在浏览器中输入 broker web 应用的股票报价 URL。 Web 应用的工作负载接收到请求并使用 mTLS 向股票 market 服务发出获取报价的 HTTP 请求。 股票 market 服务收到请求并在响应中发送报价。 Web 应用呈现使用返回的报价的股票报价页面并将其发送到浏览器。 浏览器向用户显示报价。Web 应用包括一些 JavaScript 以便每隔 1 秒刷新页面，因此每秒都会执行这些步骤。 下图详细描绘了本场景。\n联邦场景 除了上述内容，本教程的其余部分中，我们将假设以下 信任域 名称用于这些示例 SPIRE 安装：broker.example 和 stockmarket.example。请注意，信任域不需要对应实际的 DNS 域名。此外，应用程序直接访问 WorkloadAPI 以获取 SVID 和信任捆绑点，这意味着在所描述的情景中没有代理。\n配置 SPIFFE 联邦捆绑点 为了使联邦工作，并且因为 web 应用程序和报价服务将使用 mTLS，两个 SPIRE 服务器都需要彼此的信任捆绑点。在某种程度上，这是通过在每个 SPIRE 服务器上配置所谓的联邦捆绑点来完成的，该捆绑点提供了由其他信任域中的 SPIRE 服务器使用的 API，以获取他们要与之联合的信任域的信任捆绑点。\n由 SPIRE 服务器公开的联邦捆绑点可以配置为使用两种身份验证方法之一：SPIFFE 身份验证或 Web PKI 身份验证。\n使用 SPIFFE 身份验证配置联邦捆绑点 要配置 broker 的 SPIRE 服务器捆绑点端点，我们在 broker 的 SPIRE 服务器配置文件中使用了 federation 部分（默认为 server.conf）：\nserver { . . trust_domain = \u0026#34;broker.example\u0026#34; . . federation { bundle_endpoint { address = \u0026#34;0.0.0.0\u0026#34; port = 8443 } } } 这将在运行 SPIRE 服务器的主机中的任何 IP 地址上的端口 8443 上发布联邦捆绑点。\n另一方面，股票 market 服务提供商的 SPIRE 服务器配置类似：\nserver { . . trust_domain = \u0026#34;stockmarket.example\u0026#34; . . federation { bundle_endpoint { address = \u0026#34;0.0.0.0\u0026#34; port = 8443 } } } 此时，两个 SPIRE 服务器都暴露了它们的联邦捆绑点以提供它们的信任捆绑点，但它们都不知道如何到达彼此的联邦捆绑点。\n使用 Web PKI 身份验证配置联邦捆绑点 我们将假设仅 broker 的 SPIRE 服务器将使用 Web PKI 身份验证来配置其联邦捆绑点。股票 market SPIRE 服务器仍将使用 SPIFFE 身份验证。因此，股票 market SPIRE 服务器配置与前一节中所见相同。\n然后，要配置 broker 的 SPIRE 服务器捆绑点端点，我们将 federation 部分配置如下：\nserver { . . trust_domain = \u0026#34;broker.example\u0026#34; . . federation { bundle_endpoint { address = \u0026#34;0.0.0.0\u0026#34; port = 443 acme { domain_name = \u0026#34;broker.example\u0026#34; email = \u0026#34;some@email.com\u0026#34; tos_accepted = true } } } } 这将在任何 IP 地址上的端口 443 上发布联邦捆绑点。我们使用端口 443，因为我们演示了使用 Let’s Encrypt 作为我们的 ACME 提供商（如果你要使用其他提供商，则必须设置 directory_url 可配置）。请注意，tos_accepted 设置为 true，这意味着我们接受了我们的 ACME 提供商的服务条款，这在使用 Let’s Encrypt 时是必要的。\n要使使用 Web PKI 的 SPIFFE 联邦正常工作，你必须拥有为 domain_name（在我们的示例中为 broker.example）指定的 DNS 域名，并且该域名必须解析到公开联邦捆绑点的 SPIRE 服务器 IP 地址。\n配置 SPIRE 服务器以从彼此检索信任捆绑点 在配置联邦端点后，启用 SPIFFE 联邦的下一步是配置 SPIRE 服务器以查找其他信任域的信任捆绑点。在 server.conf 中的 federates_with 配置选项是你指定另一个信任域的端点的地方。在使用不同的身份验证方法时，该部分的配置有一些细微的差异，根据每个端点配置文件的要求。\n使用 SPIFFE 身份验证配置信任捆绑点位置（https_spiffe） 如前所述，股票 market 服务提供商的 SPIRE 服务器将其联邦端点监听在任何 IP 地址的端口 8443 上。我们还假设 spire-server-stock 是一个解析为股票 market 服务的 SPIRE 服务器 IP 地址的 DNS 名称。 （这里的 Docker Compose 演示使用主机名 spire-server-stock，但在典型的使用中，你会指定一个 FQDN。）然后，broker 的 SPIRE 服务器必须配置以下 federates_with 部分：\nserver { . . trust_domain = \u0026#34;broker.example\u0026#34; . . federation { bundle_endpoint { address = \u0026#34;0.0.0.0\u0026#34; port = 8443 } federates_with \u0026#34;stockmarket.example\u0026#34; { bundle_endpoint_url = \u0026#34;https://spire-server-stock:8443\u0026#34; bundle_endpoint_profile \u0026#34;https_spiffe\u0026#34; { endpoint_spiffe_id = \u0026#34;spiffe://stockmarket.example/spire/server\u0026#34; } } } } 现在，broker 的 SPIRE 服务器知道在哪里找到可以用于验证包含来自 stockmarket.example 信任域的身份的信任捆绑点。\n另一方面，股票 market 服务提供商的 SPIRE 服务器必须以类似的方式进行配置：\nserver { . . trust_domain = \u0026#34;stockmarket.example\u0026#34; . . federation { bundle_endpoint { address = \u0026#34;0.0.0.0\u0026#34; port = 8443 } federates_with \u0026#34;broker.example\u0026#34; { bundle_endpoint_url = \u0026#34;https://spire-server-broker:8443\u0026#34; bundle_endpoint_profile \u0026#34;https_spiffe\u0026#34; { endpoint_spiffe_id = \u0026#34;spiffe://broker.example/spire/server\u0026#34; } } } } 请注意，指定了 “https_spiffe” 配置文件，指示了联邦捆绑点的预期 SPIFFE ID。指定 server.conf 的 federation 部分和 federates_with 子部分是配置 SPIFFE 联邦所需的全部内容。要完成启用 SPIFFE 联邦，我们需要使用下面描述的 spire-server 命令来引导信任捆绑点和注册工作负载。\n使用 Web PKI 身份验证配置信任捆绑点位置（https_web） 如前所述，在这种备选方案中，我们假设只有 broker 的 SPIRE 服务器将使用 Web PKI 身份验证来配置其联邦端点，因此 broker 服务器的 federates_with 配置与前一节中所见相同。然而，股票 market 服务提供商的 SPIRE 服务器需要一个不同的配置，它使用 “https_web” 配置文件而不是 “https_spiffe”：\nserver { . . trust_domain = \u0026#34;stockmarket.example\u0026#34; . . federation { bundle_endpoint { address = \u0026#34;0.0.0.0\u0026#34; port = 8443 } federates_with \u0026#34;broker.example\u0026#34; { bundle_endpoint_url = \u0026#34;https://spire-server-broker:8443\u0026#34; bundle_endpoint_profile \u0026#34;https_web\u0026#34; {} } } } 可以注意到 “https_web” 配置文件不需要额外的配置设置。端点使用安装在操作系统中的相同公共 CA 证书进行身份验证。\n引导联邦 我们已经配置了 SPIRE 服务器的联邦端点地址，但这并不足以使联邦正常工作。为了使 SPIRE 服务器能够从彼此获取信任捆绑点，它们首先需要彼此的信任捆绑点，因为它们必须对试图访问联邦端点的联合服务器的 SPIFFE 身份进行身份验证。一旦联邦被引导，就可以使用当前信任捆绑点通过联邦端点 API 获取信任捆绑点更新。\n引导工作是通过使用 SPIRE Server 命令 bundle show 和 bundle set 来完成的。\n获取引导信任捆绑点 假设我们想要获取 broker 的 SPIRE 服务器信任捆绑点。在运行 broker 的 SPIRE 服务器的节点上运行：\nbroker\u0026gt; spire-server bundle show -format spiffe \u0026gt; broker.example.bundle 这会将信任捆绑点保存在 broker.example.bundle 文件中。然后，broker 必须将此文件的副本提供给股票 market 服务人员，以便他们可以将此信任捆绑点存储在他们的 SPIRE 服务器上，并将其与 broker.example 信任域关联起来。要做到这一点，股票 market 服务人员必须在他们运行 SPIRE 服务器的节点上运行以下命令：\nstock-market\u0026gt; spire-server bundle set -format spiffe -id …","relpermalink":"/book/spiffe-and-spire/architecture/federation/","summary":"本教程展示了如何对由两个不同 SPIRE 服务器识别的两个 SPIFFE 标识的工作负载进行身份验证。 本文的第一部分演示了如何通过显示 SPIRE 配置文件更改和 spire-server 命令来配置 SPIFFE 联邦，以设置股票报价 web 应用的前端和服务后端为例。本文的第二部分","title":"SPIRE 联邦：验证来自不同 SPIRE 服务器的工作负载"},{"content":"本文指导你如何在 Linux 和 Kubernetes 上安装 SPIRE Agent。\n步骤 1：获取 SPIRE 二进制文件 可以在 SPIRE 下载页面 找到预构建的 SPIRE 发行版。tarball 包含服务器和 Agent 二进制文件。\n如果愿意，也可以从源代码 构建 SPIRE 。\n步骤 2：安装服务器和 Agent 本入门指南描述了如何在同一节点上安装服务器和 Agent。在典型的生产部署中，服务器安装在一个节点上，而一个或多个 Agent 安装在不同的节点上。\n要安装服务器和 Agent：\n从 SPIRE 下载页面 获取最新的 tarball，然后使用以下命令将其解压到 /opt/spire 目录中：\nwget https://github.com/spiffe/spire/releases/download/v1.8.2/spire-1.8.2-linux-amd64-musl.tar.gz tar zvxf spire-1.8.2-linux-amd64-musl.tar.gz sudo cp -r spire-1.8.2/. /opt/spire/ 为了方便起见，将 spire-server 和 spire-agent 添加到 $PATH 中：\nsudo ln -s /opt/spire/bin/spire-server /usr/bin/spire-server sudo ln -s /opt/spire/bin/spire-agent /usr/bin/spire-agent 步骤 3：配置 Agent 安装 SPIRE Agent 后，需要根据你的环境进行配置。有关如何配置 SPIRE 的详细信息，请参阅 配置 SPIRE ，特别是节点验证和工作负载验证。\n请注意，SPIRE Agent 在修改其配置后必须重新启动，以使更改生效。\n如果尚未安装 SPIRE Server，请参阅 安装 SPIRE Server 了解如何安装 SPIRE Server。\n在 Kubernetes 上安装 SPIRE Agents 必须从包含用于配置的 .yaml 文件的目录中运行所有命令。有关详细信息，请参阅 SPIRE Server 安装指南的 Obtain the Required Files 部分。\n要在 Kubernetes 上安装 SPIRE Agents，你需要执行以下操作：\n创建 Agent 服务账号 创建 Agent ConfigMap 创建 Agent DaemonSet 有关详细信息，请参阅以下各节。\n步骤 1：创建 Agent 服务账号 将 agent-account.yaml 配置文件应用于在 spire 命名空间中创建名为 spire-agent 的服务账号：\n$ kubectl apply -f agent-account.yaml 为了允许代理读取 kubelet API 以执行工作负载验证，必须创建一个 ClusterRole，授予 Kubernetes RBAC 适当的权限，并将 ClusterRoleBinding 关联到上一步创建的服务账号。\n通过应用 agent-cluster-role.yaml 配置文件来创建名为 spire-agent-cluster-role 的 ClusterRole 和相应的 ClusterRoleBinding：\n$ kubectl apply -f agent-cluster-role.yaml 为了确认成功创建，请验证 ClusterRole 是否出现在以下命令的输出中：\n$ kubectl get clusterroles --namespace spire | grep spire 步骤 2：创建 Agent ConfigMap 将 agent-configmap.yaml 配置文件应用于创建代理 ConfigMap。这将作为 agent.conf 文件挂载，用于确定 SPIRE Agent 的配置。\n$ kubectl apply -f agent-configmap.yaml agent-configmap.yaml 文件指定了许多重要的目录，特别是 /run/spire/sockets 和 /run/spire/config。这些目录在部署代理容器时绑定。\n请参阅 配置 SPIRE 部分，详细了解如何配置 SPIRE Agent，特别是节点验证和工作负载验证。\n请注意，一旦修改了 SPIRE Agent 的配置，必须重新启动该 Agent 才能使更改生效。\n步骤 3：创建 Agent DaemonSet 代理以 DaemonSet 形式部署，每个 Kubernetes 工作节点上运行一个代理。\n通过应用 agent-daemonset.yaml 配置来部署 SPIRE 代理。\n$ kubectl apply -f agent-daemonset.yaml 这将在 spire 命名空间中创建一个名为 spire-agent 的 DaemonSet，并在 spire-server 旁边启动一个 spire-agent pod，如以下两个命令的输出所示：\n$ kubectl get daemonset --namespace spire NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE spire-agent 1 1 1 1 1 \u0026lt;none\u0026gt; 6m45s $ kubectl get pods --namespace spire NAME READY STATUS RESTARTS AGE spire-agent-88cpl 1/1 Running 0 6m45s spire-server-0 1/1 Running 0 103m 当代理部署时，绑定以下表格中总结的卷：\n卷 描述 挂载位置 spire-config 在步骤 2 中创建的 spire-agent configmap。 /run/spire/config spire-sockets hostPath，将与在同一工作节点上运行的所有其他 pod 共享。它包含一个 UNIX 域套接字，用于工作负载与代理 API 通信。 /run/spire/sockets ","relpermalink":"/book/spiffe-and-spire/installation/install-agent/","summary":"本文指导你如何在 Linux 和 Kubernetes 上安装 SPIRE Agent。 步骤 1：获取 SPIRE 二进制文件 可以在 SPIRE 下载页面 找到预构建的 SPIRE 发行版。tarball 包含服务器和 Agent 二进制文件。 如果愿意，也可以从源代码 构建 SPIRE 。 步骤 2：安装服务器和 Agent 本","title":"安装 SPIRE 代理"},{"content":" 扩展 SPIRE 部署\nSPIRE 嵌套架构\nSPIRE 联邦架构\n","relpermalink":"/book/spiffe-and-spire/architecture/","summary":"扩展 SPIRE 部署 SPIRE 嵌套架构 SPIRE 联邦架构","title":"架构"},{"content":"本文指导你如何编写与 SPIFFE SVID 相关的代码。\nSPIRE 等符合 SPIFFE 的身份提供者将通过 SPIFFE Workload API 公开 SPIFFE 可验证身份文档（SVID）。工作负载可以使用从此 API 检索到的 SVID 来验证消息的来源或在两个工作负载之间建立相互 TLS 安全通道。\n与 Workload API 交互 开发需要与 SPIFFE 进行交互的新工作负载的开发人员可以直接与 SPIFFE Workload API 进行交互，以便：\n检索工作负载的身份，描述为 SPIFFE ID，例如 spiffe://prod.acme.com/billing/api 代表工作负载生成短期密钥和证书，具体包括： 与该 SPIFFE ID 相关联的私钥，可用于代表工作负载签署数据。 对应的短期 X.509 证书 - 一种称为 X509-SVID 的证书。该证书可用于建立 TLS 或以其他方式对其他工作负载进行身份验证。 一组证书 - 称为信任捆绑包（trust bundle） - 工作负载可以使用它来验证同一信任域或联合信任域中的另一个工作负载呈现的 X.509-SVID。 生成或验证代表工作负载或同一信任域或联合信任域中另一个工作负载的 JSON Web Token（JWT-SVID）。 Workload API 不需要任何显式的身份验证（如密钥）。相反，SPIFFE 规范将身份验证工作留给 SPIFFE Workload API 的实现来确定。在 SPIRE 的情况下，这是通过检查 SPIRE 代理在工作负载调用 API 时收集的 Unix 内核元数据来实现的。\n该 API 是基于 gRPC 的 API，派生自 protobuf 。gRPC 项目 提供了从 protobuf 生成各种语言的客户端库的工具。\n在 Go 中使用 SVID 如果你在使用 Go 进行开发，SPIFFE 项目维护了一个 Go 客户端库，提供以下功能：\n一个命令行实用程序，用于解析和验证 X.509 证书中编码的 SPIFFE 身份，如 SPIFFE 标准中所述。 一个客户端库，提供与 SPIFFE Workload API 的交互界面。 你可以在 GitHub 上找到该库以及 GoDoc 的链接。\n使用 SPIFFE Helper 实用程序 SPIFFE Helper 实用程序是一个通用实用程序，用于构建或与无法直接写入 Workload API 的应用程序集成时非常有用。大体上，该实用程序能够：\n获取用于验证 X.509-SVID 的 X.509-SVID、密钥和信任捆绑包（证书链），并将它们写入磁盘上的特定位置。 启动一个子进程，该子进程可以使用这些密钥和证书。 主动监视其过期时间，并根据需要从 Workload API 请求刷新的证书和密钥。 一旦获取到更换的证书，向任何已启动的子进程发送信号。 使用 SPIRE Agent SPIRE Agent 二进制文件可用作作为 SPIFFE Workload API 的实现时的 SPIRE 部署的一部分，但它也可以作为 Workload API 的客户端运行，并提供一些简单的实用程序与其进行交互以检索 SPIFFE 凭据。\n例如，运行以下命令：\nsudo -u webapp ./spire-agent api fetch x509 -socketPath /run/spire/sockets/agent.sock -write /tmp/ 将会：\n连接到 Unix 域套接字 /run/spire/sockets/agent.sock 上的 Workload API（即使 SPIRE 不提供 API）。 检索与该进程所运行的用户相关联的任何身份（在此示例中为 webapp）。 将每个身份关联的 X.509-SVID、私钥写入 /tmp/。 将用于验证在该信任域下颁发的 X.509-SVID 的信任捆绑包（证书链）写入 /tmp/。 有关相关命令的完整列表，请参阅 SPIRE Agent 文档 。\n","relpermalink":"/book/spiffe-and-spire/configuration/svids/","summary":"本文指导你如何编写与 SPIFFE SVID 相关的代码。 SPIRE 等符合 SPIFFE 的身份提供者将通过 SPIFFE Workload API 公开 SPIFFE 可验证身份文档（SVID）。工作负载可以使用从此 API 检索到的 SVID 来验证消息的来源或在两个工作负载之间建立相互 TLS 安全通道。 与 Workload API 交","title":"使用 SVID"},{"content":"本文描述了创建 Azure 中的应用程序以允许 TSB 使用云帐户进行 OIDC 以及从 Azure AD 同步用户和组的步骤。\n创建应用程序 登录到 Azure 门户，然后转到Active Directory \u0026gt; 应用程序注册 \u0026gt; 新应用程序注册。\n将应用程序类型设置为 Web，并将重定向 URI 配置为指向 TSB 地址以及**/iam/v2/oidc/callback**端点。\n配置应用程序秘密 一旦应用程序创建完毕，转到证书和密码以创建一个用于在 TSB 中使用的客户端秘密：\n配置名称和到期时间，然后单击添加。\n一旦添加完成，你将能够看到秘密的Value。复制它，因为稍后将使用它，并且将不再显示。\n配置 OIDC 令牌 一旦创建了秘密，转到身份验证菜单项，然后单击添加平台以配置 OIDC 令牌。\n选择Web\n在接下来的屏幕中，将重定向 URI 配置为指向 TSB 地址以及**/iam/v2/oidc/callback**端点，选择两个令牌（访问令牌和 ID 令牌），可以选择启用 tctl 设备代码流的移动和桌面流程：\n配置应用程序权限 一旦为 OIDC 配置了应用程序，就需要授予应用程序权限，以允许 TSB 从 Azure AD 同步有关用户和组的信息。为此，请转到API 权限，然后单击添加权限：\n在接下来的屏幕上，向下滚动到底部，从常用的 Microsoft API中选择Microsoft Graph：\n在接下来的屏幕上，将权限类型设置为应用程序权限，从权限列表中选择**Directory.Read.All 权限**，然后单击添加权限按钮。\n一旦添加了权限，请单击授予管理员同意按钮以授予应用程序请求的权限。\n一旦授予了权限，状态应该反映出来：\n启用公共工作流 你需要启用公共工作流以允许tctl命令登录。 登录到 Azure 门户，然后转到前面步骤中创建的平台的“身份验证”部分。\n在“高级设置”部分，启用选项“是”，然后单击保存。\n配置 TSB 此时，你已完成 Azure 端的配置。现在，你需要创建 Kubernetes 秘密以存储应用程序的客户端秘密，并配置 ManagementPlane。\n需要从 Azure 应用程序获取以下数据：\n客户端 ID 客户端秘密 租户 ID 配置 URL 客户端 ID和租户 ID可以在 Azure 应用程序的概述中获取。\n客户端秘密是你在前面步骤中配置应用程序秘密时复制的“密码”。\n配置 URI可以从应用程序端点中复制：\n创建 Kubernetes 秘密 使用以下命令创建名为secret.yaml的文件中的秘密。适当替换TSB_Admin_Pass和Client_Secret。\ntctl install manifest management-plane-secrets --allow-defaults --tsb-admin-password \u0026lt;TSB_Admin_Pass\u0026gt; \\ --oidc-client-secret=\u0026lt;Client_Secret\u0026gt; \\ --teamsync-azure-client-secret=\u0026lt;Client_Secret\u0026gt; \u0026gt; secret.yaml 在生成的secret.yaml文件中，我们只关心iam-oidc-client-secret和azure-credentials的值。\n编辑secret.yaml文件，删除所有其他秘密，然后使用kubectl应用 YAML 文件。重要的是要删除所有其他秘密，因为你不希望为此过程覆盖它们。\nkubectl apply -f secret.yaml 配置 ManagementPlane CR 一旦创建了秘密，使用以下命令来配置ManagementPlane CR 的 identityProvider 部分，以开始编辑 CR：\nkubectl edit managementplane managementplane -n tsb 以与下面的示例类似的方式编辑 CR 的内容（仅显示示例中的相关部分）。你需要在ManagementPlane CR 清单中的适当位置插入identityProvider子句。\napiVersion: install.tetrate.io/v1alpha1 kind: ManagementPlane metadata: name: managementplane namespace: tsb spec: ( … ) identityProvider: oidc: clientId: \u0026lt;应用程序客户端 ID\u0026gt; providerConfig: dynamic: configurationUri: \u0026lt;应用程序配置 URI\u0026gt; redirectUri: https://\u0026lt;tsb地址\u0026gt;:8443/iam/v2/oidc/callback scopes: - email - profile - offline_access sync: azure: clientId: \u0026lt;应用程序客户端 ID\u0026gt; tenantId: \u0026lt;应用程序租户 ID\u0026gt; Helm 安装 本文档提供的所有有关更新ManagementPlane CR 的示例也适用于 Helm 安装。你可以编辑管理平面 Helm 值文件中的identityProvider.oidc。 最终用户 TCTL 配置 使用 Azure OIDC 有两种配置 tctl 的方法\n使用服务主体的基于用户的设备代码身份验证。 无服务主体的基于用户的设备代码身份验证。 使用服务主体的基于用户的设备代码身份验证 使用以下命令kubectl edit managementplane managementplane -n tsb配置ManagementPlane CR 的 identityProvider 部分，并将离线部分更新如下：\napiVersion: install.tetrate.io/v1alpha1 kind: ManagementPlane metadata: name: managementplane namespace: tsb spec: ... identityProvider: oidc: ... offlineAccessConfig: deviceCodeAuth: clientId: \u0026lt;应用程序客户端 ID\u0026gt; 使用当前配置文件登录：\ntctl login --use-device-code 无服务主体的基于用户的设备代码身份验证 一旦 OIDC 集成到集群中，最终用户就可以通过使用带有--use-device-code参数的登录命令配置 tctl 以与 OIDC 一起使用。该命令将要求组织和租户，并提供一个可以用于验证提供的 URL 上的代码。一旦验证了该代码，tctl 将准备好使用。\ntctl login --use-device-code 组织：tetrate 租户：mp 代码：CXKF-TDKP 打开浏览器页面 https://aka.ms/devicelogin 并输入代码 或者，tctl 配置可以从 UI 中下载。要这样做，请登录到 TSB，单击右上角的用户信息图标。然后单击“显示令牌信息”，按照 UI 显示的步骤下载并使用文件。\n","relpermalink":"/book/tsb/operations/users/oidc-azure/","summary":"如何配置 Azure AD 作为 TSB 的身份提供者。","title":"Azure AD 作为身份提供者"},{"content":" 入门指南\ntctl\ntctl apply\ntctl collect\ntctl completion\ntctl config\ntctl delete\ntctl edit\ntctl experimental\ntctl get\ntctl install\ntctl login\ntctl ui\ntctl validate\ntctl version\ntctl whoami\nWorkloadEntry Annotations\n","relpermalink":"/book/tsb/reference/cli/","summary":"入门指南\ntctl\ntctl apply\ntctl collect\ntctl completion\ntctl config\ntctl delete\ntctl edit\ntctl experimental\ntctl get\ntctl install\ntctl login\ntctl ui\ntctl validate\ntctl version\ntctl whoami\nWorkloadEntry Annotations","title":"CLI"},{"content":" TSB 中的 GitOps\n配置 Flux CD 进行 GitOps\n使用 Argo Rollout 和 SkyWalking 进行金丝雀分析和渐进式交付\n","relpermalink":"/book/tsb/howto/gitops/","summary":"TSB 中的 GitOps\n配置 Flux CD 进行 GitOps\n使用 Argo Rollout 和 SkyWalking 进行金丝雀分析和渐进式交付","title":"GitOps"},{"content":"在开始之前，你必须拥有：\nVault 1.3.1 或更新版本 Vault 注入器 0.3.0 或更新版本 设置 Vault 安装 Vault（它不必安装在 Kubernetes 集群中，但应该可以从 Kubernetes 集群内部访问）。必须将 Vault 注入器（agent-injector）安装到集群中，并配置以注入 sidecar。Helm chart v0.5.0+ 会自动完成这些工作，该 Helm chart 安装了 Vault 0.12+ 和 Vault-Injector 0.3.0+。下面的示例假定 Vault 安装在 tsb 命名空间中。\n有关更多详细信息，请参阅 Vault 文档 。\nhelm install --name=vault --set=\u0026#39;server.dev.enabled=true\u0026#39; 将 Vault 服务端口转发到本地，并设置环境变量以对 API 进行身份验证\nkubectl port-forward svc/vault 8200:8200 \u0026amp; # 这将在后台运行 export VAULT_ADDR=\u0026#39;http://[::]:8200\u0026#39; export VAULT_TOKEN=\u0026#34;root\u0026#34; 创建 CA 证书 注意 如果你已经有自己的 CA 证书和/或用于 Istio 的中间 CA 证书，请跳到 将中间 CA 证书添加到 Vault 。 Vault 有一个支持创建或管理 CA 证书的 PKI Secret 后端。建议用户使用 Vault 创建一个用于 Istio 的中间 CA 证书，并将根 CA 保留在 Vault 之外。请参考 Vault 文档 了解更多细节。\n使用 Vault PKI 后端在 Vault 中生成一个自签名的根 CA 证书。\n# 在 Vault 中添加审计跟踪 vault audit enable file file_path=/vault/vault-audit.log # 启用 PKI Secret 后端 vault secrets enable pki # 更新 PKI 租约为 1 年 vault secrets tune -max-lease-ttl=8760h pki # 创建自签名 CA vault write pki/root/generate/internal common_name=tetrate.io ttl=8760h 为 Istio 创建中间 CA 现在我们在 Vault 中有了一个 CA，我们将其用于创建 Istio 的中间 CA。我们重复使用 pki Secret 后端，但使用一个新路径 (istioca)。这次我们需要获取 CA 密钥，并调用 exported 端点而不是 internal。\n# Enable PKI in a new path for intermediate CA vault secrets enable --path istioca pki # Update lease time to 5 years vault secrets tune -max-lease-ttl=43800h istioca # Create Intermediate CA cert and Key vault write istioca/intermediate/generate/exported common_name=\u0026#34;tetrate.io Intermediate Authority\u0026#34; ttl=43800h Key Value --- ----- csr -----BEGIN CERTIFICATE REQUEST----- MIICcTCCAVkCAQAwLDEqMCgGA1UEAxMhdGV0cmF0ZS5pbyBJbnRlcm1lZGlhdGUg ... ... 7HJEy22yCFvVcR+Gtf++iZG+w04E2ah99xrzb+NdWDgw6asBg7oJg/bJQoA4/Wb5 OX2jl0E= -----END CERTIFICATE REQUEST----- private_key -----BEGIN RSA PRIVATE KEY----- MIIEpQIBAAKCAQEA8Vmm2urwUHdAp1j3vs8aOqYGrDz3NwJbm6du+3WmgGHQ+sEC ... ... yri2DiQWzwk3zIvzNSSbQdACPPeF90BLFW9L4xvN8D6gBxFL0wBa7GY= -----END RSA PRIVATE KEY----- private_key_type rsa 将证书签名请求（CSR）复制到本地文件 istioca.csr 中（包括 -----BEGIN CERTIFICATE REQUEST----- 和 -----END CERTIFICATE REQUEST-----）。\n将 CA 密钥复制到本地文件 istioca.key 中。建议在安全的地方备份此密钥。\n现在我们可以使用 Vault CA 对 CSR 进行签名：\nvault write pki/root/sign-intermediate csr=@istioca.csr format=pem_bundle ttl=43800h Key Value --- ----- certificate -----BEGIN CERTIFICATE----- MIIDMDCCAhigAwIBAgIUGJgs6yFbK/eDW31RpdSiNeVQYvUwDQYJKoZIhvcNAQEL ... ... PcPHltvhXDjckPK1jt9gpLMTaBhe9uu7Ve7b2OFv+mtAGiCEvALv+ddLa0GHrl3s f2vq6A== -----END CERTIFICATE----- expiration 1618951159 issuing_ca -----BEGIN CERTIFICATE----- MIIDLDCCAhSgAwIBAgIUFVRq5X/cAOlDKl/h34VudUdSiMwwDQYJKoZIhvcNAQEL ... ... d4SklUyQdxnW96IdkHSPWf46jh31tDWqco6LzmrxD4OmjSeLaf0zeErU1i41xAnW -----END CERTIFICATE----- serial_number 18:98:2...:f5 将 certificate 保存到名为 istioca.crt 的文件中。\n将中间 CA 证书添加到 Vault 现在我们需要将已签名的中间 CA 证书放入 Vault 中。\nvault write istioca/intermediate/set-signed certificate=@istioca.crt Istio 还将基于中间 CA 证书生成证书，并需要证书及其密钥。虽然 Vault 可以通过其 API 发送证书，但我们必须注意密钥。为此，我们将密钥的副本放在 Vault Secret 中以备后用。\n如果尚未启用，请启用 kv Secrets 引擎。\nvault secrets enable kv Success! Enabled the kv secrets engine at: kv/ 现在将 CA 密钥保存在其中。\nvault kv put kv/istioca ca-key.pem=@istioca.key 配置 Vault-Injector 配置一个与 Istio 的 Kubernetes Service Account 匹配的 Vault 角色。这个角色将被 Vault-Injector 使用。在以下示例中，角色名为 istiod，与 istiod 服务帐户 istiod-service-account 绑定。\n启用 Kubernetes 认证方法：\nvault auth enable kubernetes 创建名为 istiod 的角色：\nvault write auth/kubernetes/role/istiod \\ bound_service_account_names=istiod-service-account \\ bound_service_account_namespaces=istio-system \\ policies=istioca \\ period=600s 启用 istiod-service-account 以通过 vault-inject 容器与 Vault 进行通信：\nexport VAULT_SA_NAME=$(kubectl get sa -n istio-system istiod-service-account \\ --output jsonpath=\u0026#34;{.secrets[*][\u0026#39;name\u0026#39;]}\u0026#34;) export SA_JWT_TOKEN=$(kubectl get secret -n istio-system $VAULT_SA_NAME \\ --output \u0026#39;go-template={{ .data.token }}\u0026#39; | base64 --decode) export SA_CA_CRT=$(kubectl config view --raw --minify --flatten \\ --output \u0026#39;jsonpath={.clusters[].cluster.certificate-authority-data}\u0026#39; | base64 --decode) export K8S_HOST=$(kubectl config view --raw --minify --flatten \\ --output \u0026#39;jsonpath={.clusters[].cluster.server}\u0026#39;) vault write auth/kubernetes/config \\ token_reviewer_jwt=\u0026#34;$SA_JWT_TOKEN\u0026#34; \\ kubernetes_host=\u0026#34;$K8S_HOST\u0026#34; \\ kubernetes_ca_cert=\u0026#34;$SA_CA_CRT\u0026#34; 创建一个允许上述角色访问 PKI Secret 后端的策略。此策略使 Istio 能够读取存储了中间 CA 密钥的 secret/data/istioca 中的键/值。它还允许 Istio 访问中间 CA 和 CA 链（通过使用 *）以及根 CA。\ncat \u0026gt; policy.hcl \u0026lt;\u0026lt;EOF path \u0026#34;kv/istioca\u0026#34; { capabilities = [\u0026#34;read\u0026#34;, \u0026#34;list\u0026#34;] } path \u0026#34;istioca/cert/*\u0026#34; { capabilities = [\u0026#34;read\u0026#34;, \u0026#34;list\u0026#34;] } path \u0026#34;pki/cert/ca\u0026#34; { capabilities = [\u0026#34;read\u0026#34;, \u0026#34;list\u0026#34;] } EOF vault policy write istioca policy.hcl 配置 TSB ControlPlane CRD 更新你的 ControlPlane CR 或 Helm 值，并添加特定的 istiod 组件配置以使用 Vault：\nspec: components: istio: kubeSpec: overlays: - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator name: tsb-istiocontrolplane patches: - path: spec.meshConfig.defaultConfig.holdApplicationUntilProxyStarts value: true - path: spec.components.pilot.k8s.env value: - name: ROOT_CA_DIR value: /vault/secrets - path: …","relpermalink":"/book/tsb/operations/vault/istiod-ca/","summary":"如何将 Vault Agent 注入器与 Istiod CA 证书结合使用。","title":"Istio CA"},{"content":"默认情况下，Istio 会将 sidecar 代理注入到应用程序的 pod 中，以便处理该 pod 的流量。这些 sidecar 需要成为特权容器，因为它们需要在 pod 网络命名空间中操作 iptables 规则，以便拦截进出该 pod 的流量。\n从安全性的角度来看，这种默认行为并不理想，因为它实际上授予了应用程序 pod 使用这些高级权限的权限。Istio 提供的替代方案是使用 CNI 插件 ，它在 pod 创建时处理 pod 网络命名空间的修改。\n在控制平面中启用 Istio CNI 为了在你的控制平面中启用 Istio CNI 插件，你需要编辑 ControlPlane CR 或 Helm 值，以包括 CNI 配置。\nspec: components: istio: kubeSpec: CNI: chained: true binaryDirectory: /opt/cni/bin configurationDirectory: /etc/cni/net.d traceSamplingRate: 100 hub: \u0026lt;registry-location\u0026gt; managementPlane: host: \u0026lt;tsb-address\u0026gt; port: \u0026lt;tsb-port\u0026gt; clusterName: \u0026lt;cluster-name\u0026gt; telemetryStore: elastic: host: \u0026lt;elastic-hostname-or-ip\u0026gt; port: \u0026lt;elastic-port\u0026gt; version: \u0026lt;elastic-version\u0026gt; 上述片段显示了默认的 ControlPlane CR，其中包含了 spec.components.istio.kubeSpec.CNI 的附加部分。这将配置 Istio 控制平面以部署遵循所提供配置的 CNI 插件。\n注意 配置值可能会根据你使用的 Kubernetes 发行版而发生变化，请参考Istio 文档 了解更多信息。 注意 Istio CNI 也可以绑定到特定的 Istio 修订版，然后可以从一个 Istio 修订版升级到另一个。请参考 Istio CNI 升级 了解更多信息。 ","relpermalink":"/book/tsb/operations/features/istio-cni/","summary":"如何配置 Istio 控制平面以使用 Istio CNI 插件","title":"Istio CNI"},{"content":" IstioOperator Options\n","relpermalink":"/book/tsb/refs/istio.io/","summary":"IstioOperator Options","title":"istio.io"},{"content":"让我们开始 在开始之前，请确保你已经做到了以下几点：\n熟悉 TSB 概念 安装了 TSB 环境。你可以使用 TSB 演示 进行快速安装。 完成了 TSB 的快速入门 。本文假定你已经创建了租户并熟悉工作区和配置组，并且需要将 tctl 配置到你的 TSB 环境。 在这个示例中，将使用httpbin作为工作负载。通过 wasm 扩展执行，发送到 Ingress GW 的请求将在 HTTP 响应中添加一个头部。\n部署httpbin服务 请按照此文档中的所有说明 创建httpbin服务。\n接下来的命令将假定你已经有一个组织=tetrate，租户=tetrate，工作区=httpbin，网关组=httpbin-gateway\n构建和部署 WASM 扩展 让我们使用一个已经存在的 WASM 扩展代码 ，该扩展将在 HTTP 响应中添加头部。 为了构建 WASM 扩展，下载存储库并按照这些 说明进行操作：\nmake build.example name=http_headers 然后需要将其打包为 OCI 镜像：\ndocker build . -t docker.io/\u0026lt;your repo\u0026gt;/demo-wasm:0.1 -f examples/wasm-image.Dockerfile --build-arg WASM_BINARY_PATH=examples/http_headers/main.wasm 之后，使用适当的命令将镜像推送到你的镜像仓库：\ndocker push docker.io/\u0026lt;your repo\u0026gt;/demo-wasm:0.1 创建 Wasm 扩展 第一步是填写 WASM 扩展目录，添加可用于 TSB 资源的扩展。\n必须指定包含扩展的 OCI 镜像（需要使用前缀oci://）。\n还有其他可选字段，如指向扩展源代码的source字段，priority字段将定义 WASM 扩展的执行顺序，allowedIn用于限制此 WASM 扩展仅分配给特定租户的资源。\n最后，字段config将设置 WASM 扩展的默认可选配置。每个 WASM 扩展都可以定义此 JSON 格式配置的特定分类。在将 WASM 扩展附加到 TSB 资源时，可以重新定义 config 字段，以设置与默认值不同的值。\n要创建扩展，可以使用UI、tctl命令行或kubernetes资源（如果启用了GitOps ）。\n使用tctl 创建一个名为wasm-extension.yaml的 yaml 文件，其中包含 WasmExtension 的定义：\napiVersion: extension.tsb.tetrate.io/v2 kind: WasmExtension metadata: name: wasm-add-header organization: tetrate spec: description: Extension to modify the headers image: oci://docker.io/\u0026lt;your repo\u0026gt;/demo-wasm:0.1 source: https://github.com/tetratelabs/proxy-wasm-go-sdk/tree/main/examples/http_headers priority: 1 config: header: x-wasm-header value: tsb-header 在 TSB 上应用定义：\ntctl apply -f wasm-extension.yaml 使用 UI 单击“Wasm 扩展”菜单以打开 WasmExtension 目录，然后单击右上角的“创建”按钮。填写扩展字段，然后单击底部的“创建”按钮。注意，配置必须采用 JSON 格式。\n可以有多个扩展，每个扩展都有不同的名称，并且可以分配给多个资源。\n下一步是将此 WASM 扩展分配给资源，以便影响所需的工作负载。在我们的示例中，选择 IngressGateway 作为资源，以便在网关接收到的每个请求上执行 WASM 扩展。\n在 IngressGateway 上创建附件 使用tctl 创建一个名为ingress-gateway.yaml的文件，其中包含将包含WASM 附件 的IngressGateway 的定义：\napiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: name: ingress-gw group: httpbin-gateway workspace: httpbin tenant: tetrate organization: tetrate spec: workloadSelector: namespace: httpbin labels: app: httpbin-ingress-gateway http: - name: httpbin port: 443 hostname: \u0026#34;httpbin.tetrate.io\u0026#34; routing: rules: - route: host: \u0026#34;httpbin/httpbin.httpbin.svc.cluster.local\u0026#34; extension: - fqn: \u0026#34;organizations/tetrate/extensions/wasm-add-header\u0026#34; config: header: x-wasm-header value: igw-tsb 在 TSB 上应用它：\ntctl apply -f ingress-gateway.yaml 使用 UI WasmExtension 的权限 你需要授予团队或用户具有READ WasmExtension权限的角色，以便他们可以使用 TSB UI 来附加 Wasm 扩展。 你可以使用 UI 将 WASM 扩展附加到 IngressGateway。转到 IngressGateway 配置 UI，然后单击“添加新的 WASM 扩展”。选择要使用的扩展\n并指定配置。注意，配置必须采用 JSON 格式。\n测试 export GATEWAY_IP=$(kubectl -n httpbin get service httpbin-ingress-gateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) curl http://httpbin.tetrate.io:443 -kv --connect-to httpbin.tetrate.io:443:$GATEWAY_IP:443 你应该会看到类似于以下输出：\n* Connecting to hostname: 35.230.60.29 * Connecting to port: 443 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0* Trying 35.230.60.29:443... * Connected to 35.230.60.29 (35.230.60.29) port 443 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: httpbin.tetrate.io:443 \u0026gt; User-Agent: curl/7.79.1 \u0026gt; Accept: */* \u0026gt; * Mark bundle as not supporting multiuse \u0026lt; HTTP/1.1 200 OK \u0026lt; server: istio-envoy \u0026lt; date: Wed, 09 Nov 2022 14:35:13 GMT \u0026lt; content-type: text/html; charset=utf-8 \u0026lt; content-length: 9593 \u0026lt; access-control-allow-origin: * \u0026lt; access-control-allow-credentials: true \u0026lt; x-envoy-upstream-service-time: 54 \u0026lt; x-proxy-wasm-go-sdk-example: http_headers \u0026lt; x-wasm-header: igw-tsb \u0026lt; { [9593 bytes data] 100 9593 100 9593 0 0 22866 0 --:--:-- --:--:-- --:--:-- 23171 * Connection #0 to host 35.230.60.29 left intact 在响应中，你可以看到x-wasm-header已根据 WASM 扩展的配置添加。这是通过执行 WASM 扩展的连接到网关工作负载来完成的。\n它在 Istio / Envoy 中是如何结束的？ 这些 WASM 分配将影响由 TSB 组件处理的工作负载，并最终转化为Istio WasmPlugins ，这些插件由 Istio 处理并转化为 Envoy 过滤器配置在 envoy 代理中执行，其执行顺序取决于插件的阶段和优先级。 一旦配置到达 Envoy 代理，WASM 扩展将成为过滤器链的一部分，它们的位置将取决于它们的阶段，而优先级将确定它们在同一阶段中与其他 WASM 扩展的位置。\n","relpermalink":"/book/tsb/howto/wasm/wasm-try/","summary":"展示了创建 WASM 扩展并将其分配到层次结构的命令和脚本示例。","title":"Wasm 扩展示例"},{"content":"安全性已融入 Tetrate Service Bridge（TSB）架构的方方面面。TSB 的方法建立在零信任原则之上，其中安全性是网格管理环境中每个特性和功能的首要考虑因素。本节深入探讨 TSB 如何将安全视为一等公民，并提供全面的措施来保护应用程序、支持合规性工作并防止中断。\n在本节中，你将深入了解以下关键方面：\n租赁和抽象基础设施 访问控制策略 可审计性和日志记录 安全通信的服务身份 多租户 TSB 构建了一个逻辑资源层次结构，摆脱了物理基础设施的束缚。这意味着你不再受限于单个虚拟机或 Pod 的安全考虑和操作。\n相反，你在 TSB 的管理平台上所做的任何更改都会影响到你环境中的资源集合。TSB 通过将物理基础设施抽象化，使服务配置更加安全和易于使用。\n通过将的基础设施分组成易于查看和管理的块，TSB 为你提供了清晰的资源归属视图，帮助你更好地了解资源的共享情况。共享资源可能导致更多问题，如共享故障或嘈杂的邻居，根据我们的经验，这些是复杂系统最容易出现问题的地方。TSB 简化了你对系统中共享所有权位置的理解，并协助你安全地管理它。\n资源层次结构 TSB 层级结构的核心是组织。\n组织（Organization） 每个 TSB 安装都包含一个组织，充当 TSB 范围设置的容器。该组织还包括团队、用户和集群。在组织内，资源被分类为租户。\n租户（Tenant） TSB 中的租户代表一组共享资源并在指定工作空间内运行的个人或团队。租赁边界提供访问控制和配置分离。租户可以容纳多个团队，例如安全或开发团队。租户拥有的资源进一步划分为工作区，这些工作区通常与特定团队相关联。在租户级别设置的团队级别权限是继承的，允许团队修改其分配的工作区中的资源。\n工作空间（Workspace） TSB 中的工作空间提供了分区区域，团队可以在其中专门管理其资源和网格配置。工作区下面是组，这些组进一步细分为物理服务，最终细分为单个服务实例。\n工作区提供配置流量和安全访问策略所需的粒度。TSB 允许为整个工作区设置默认配置，这些配置会自动应用于这些工作区中的所有服务。但是，可以在组或配置级别创建覆盖。\n访问控制策略 TSB 强制执行两个主要类别的访问控制策略：运行时访问策略和用户管理。\n运行时访问策略 TSB 的逻辑模型允许灵活配置运行时安全策略，例如传输中加密（双向 TLS）和最终用户身份验证（例如基于 JWT 的身份验证）。这些运行时访问策略利用服务身份，这是 TSB 零信任架构 (ZTA) 的基本组件。\n用户管理访问策略有助于配置不同用户角色的权限，确保对 TSB 功能进行清晰且精细的访问控制。\n用户访问策略 TSB 内的每个资源都与定义特定条件下授权访问的访问策略相关联。TSB 通过集成来自企业身份提供商（例如 LDAP）的数据并使用下一代访问控制 (NGAC) 在内部进行映射来与组织结构保持一致。这种一致性导致访问控制策略尊重组织边界并防止用户影响其指定范围之外的资源。\nTSB 的用户和团队资源可以通过团队和用户 API 进行管理。然而，TSB 与 LDAP 和 OIDC 等身份提供商的集成可确保大多数用户数据自动同步，从而减少与这些 API 直接交互的需要。\n用户访问和资源层次结构 TSB 的 RBAC API 允许向 TSB 资源层次结构中的用户和团队分配权限。这些权限在层次结构中向下继承。结合支持自定义角色和细粒度权限的 TSB 复杂的 IAM 系统，用户可以广泛控制谁可以在托管基础设施上执行特定操作。\n例如，在受监管的行业中，TSB 授权中央安全团队为整个基础设施配置传输中的加密。他们可以审核整个系统并在组织级别控制加密设置。业务线安全团队可以进一步为其基础设施的特定部分配置安全控制，而应用程序团队可以在不影响安全设置的情况下专注于开发。\n可审计性和日志记录 TSB 维护对受控资源所做的所有更改的审计跟踪，提供强大的审计功能。TSB 内的每个资源都可以进行审核，提供全面的详细信息，例如谁进行了更改、何时进行更改以及更改的确切性质。此信息可通过 TSB 审核日志 API 访问。\nTSB 审计日志 API 与权限系统紧密结合，确保审计日志仅显示与用户有权访问的资源相关的条目。\n安全通信的服务身份 TSB 依靠服务身份而不是 IP 或网络位置来增强网络安全性和访问控制。Istio 支持 TSB 内的服务身份，为每个工作负载提供可验证的身份，以便在运行时进行身份验证和授权。这些身份由 Istio 发布，在运行时轮换，为工作负载之间的安全通信和传输中的加密奠定基础。\nTSB 利用这些服务身份使应用程序开发人员和安全团队能够定义精细的访问控制策略。这些策略规定了哪些服务可以通信以及如何通信，同时还限制了潜在攻击的范围。通过使用服务身份，TSB 将访问控制策略与底层网络和基础设施组件解耦，确保其在云、Kubernetes、本地和虚拟机等各种环境中的可移植性。\n结论 安全性是 TSB 架构的核心原则，深入融入其设计原则和每一层功能中。从通过资源层次结构提供细粒度的访问控制到通过服务身份实现安全通信，TSB 可确保为网格管理环境提供强大的安全态势。通过使安全性与组织结构保持一致并提供全面的审计功能，TSB 使组织能够自信地管理资源、遵守合规性要求并维护高度可用且安全的应用程序环境。\n","relpermalink":"/book/tsb/concepts/security/","summary":"TSB 架构中的安全考虑。","title":"安全"},{"content":"有时候拥有一个什么都不做的工作负载是很方便的。在这个示例中，使用已安装 curl 的容器作为 sleep 服务的基础，以便更容易进行测试。\nsleep 服务在 TSB 文档中的多个示例中使用。本文档提供了该服务的基本安装步骤。\n请确保查阅每个 TSB 文档，以获取示例正常运行所需的特定注意事项或自定义设置，因为本文档描述了最通用的安装步骤。\n以下示例假设您已经设置好了 TSB，并且已经注册了要安装 sleep 工作负载的 Kubernetes 集群。\n除非另有说明，使用 kubectl 命令的示例必须指向同一集群。在运行这些命令之前，请确保您的 kubeconfig 指向所需的集群。\n命名空间 除非另有说明，假定 sleep 服务安装在 sleep 命名空间中。如果不存在，请在目标集群中创建此命名空间。\n运行以下命令以创建命名空间（如果尚未存在）：\nkubectl create namespace sleep 此命名空间中的 sleep Pod 必须运行 Istio sidecar 代理。要自动启用对所有 Pod 的 sidecar 注入，请执行以下操作：\nkubectl label namespace sleep istio-injection=enabled --overwrite=true 这将让 Istio 知道它需要将 sidecar 注入到稍后将创建的 Pod 中。\n部署 sleep Pod 和服务 下载在 Istio 存储库中找到的 sleep.yaml 清单。\n运行以下命令以在 sleep 命名空间中部署 sleep 服务：\nkubectl apply -n sleep -f sleep.yaml 创建 sleep 工作空间 根据使用情况，下一步可能需要也可能不需要。如果您要创建 TSB 工作空间，请按照以下步骤创建。\n在此示例中，我们假设您已经在组织中创建了一个租户。如果尚未创建，请阅读文档中的示例并创建一个 。\n如果您尚未执行此操作，请创建一个用于 sleep 的工作空间，并声明命名空间 sleep。创建一个名为 sleep-workspace.yaml 的文件，其内容类似于下面的示例。确保将组织、租户和集群名称替换为适当的值。\n注意 如果您已经安装了 demo 配置文件 ，则已经存在一个名为 tetrate 的组织和一个名为 demo 的集群。 apiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: \u0026lt;organization\u0026gt; tenant: \u0026lt;tenant\u0026gt; name: sleep spec: displayName: Sleep Workspace namespaceSelector: names: - \u0026#34;\u0026lt;cluster\u0026gt;/sleep\u0026#34; 使用 tctl 应用清单：\ntctl apply -f sleep-workspace.yaml ","relpermalink":"/book/tsb/reference/samples/sleep-service/","summary":"如何安装用于各种示例的 `sleep` 工作负载的指南。","title":"安装 sleep"},{"content":" 先决条件和下载\n使用 tctl 安装\n使用 Helm 安装\n载入工作负载\nAWS\n证书安装\n使用 tctl 连接到 TSB\n资源消耗与容量规划\nTSB 组件\n防火墙信息\nIstio 隔离边界\n仓库机密\n控制平面升级\n从 tctl 迁移到 Helm\n","relpermalink":"/book/tsb/setup/","summary":"先决条件和下载 使用 tctl 安装 使用 Helm 安装 载入工作负载 AWS 证书安装 使用 tctl 连接到 TSB 资源消耗与容量规划 TSB 组件 防火墙信息 Istio 隔离边界 仓库机密 控制平面升级 从 tctl 迁移到 Helm","title":"安装与升级"},{"content":"本指南将帮助你在实践中开始使用“工作负载载入”。\n作为这个指南的一部分，你将会：\n将 Istio Bookinfo 示例部署到你的 Kubernetes 集群中。 在本地虚拟机上部署 ratings 应用程序，并将其加入到服务网格中。 验证 Kubernetes Pod(s) 和本地虚拟机之间的流量。 本指南旨在演示工作负载载入功能的易于跟随的示例。\n为了保持简单，你无需像在生产部署的情况下那样配置基础设施。\n具体来说：\n你无需设置可路由的 DNS 记录。 你无需使用受信任的 CA 机构（例如 Let’s Encrypt）。 在继续之前，请确保完成以下先决条件：\n创建一个 Kubernetes 集群，以安装 TSB 和示例应用程序。 按照 TSB 演示 安装的说明进行操作。 按照 安装示例 Bookinfo 的说明进行操作。 按照 启用工作负载载入 的说明进行操作。 确保本地虚拟机和 Kubernetes 集群位于相同的网络或对等网络上。 配置本地 WorkloadGroup 和 Sidecar\n配置本地虚拟机\n从本地虚拟机上进行工作负载载入\n","relpermalink":"/book/tsb/setup/workload-onboarding/quickstart/on-premise/","summary":"本指南将帮助你在实践中开始使用“工作负载载入”。 作为这个指南的一部分，你将会： 将 Istio Bookinfo 示例部署到你的 Kubernetes 集群中。 在本地虚拟机上部署 ratings 应用程序，并将其加入到服务网格中。 验证 Kubernetes Pod(s) 和本地虚拟机之间的流量。 本指南","title":"在本地快速入门工作负载"},{"content":"应用程序所有者的用户体验在他们直接使用原生 Kubernetes API 与 Kubernetes 平台交互，或者依赖于间接方法（如 CD 流水线）来部署和配置平台时几乎没有变化。\n应用程序所有者 (“Apps”) 将按以下步骤部署和公开服务：\n部署服务\n在由平台所有者提供的 Kubernetes 命名空间中部署目标服务。\n公开服务\n配置一个 Gateway 资源，通过本地 Ingress 网关公开服务。\nApps: 开始之前 在开始之前，你需要从你的管理员（平台所有者）那里获得以下信息：\nKubernetes 集群和命名空间：在所选择的命名空间中部署和管理服务所需的 API 访问权限 Tetrate 拓扑：Kubernetes 命名空间被分组为 Tetrate 工作空间，一个工作空间可以包含来自多个集群的命名空间。你需要了解拓扑，包括： Tetrate 工作空间的名称 该工作空间中的命名空间和集群 Tetrate Gateway Groups 的名称 - 通常每个工作空间每个集群有一个 Gateway Group 请注意，Tetrate 部署的最常见安全姿态是允许 Workspace 内的流量，在工作空间之间以及不在工作空间中的 mesh 服务之间拒绝所有流量。如果需要，你的管理员可以在工作空间之间开放额外的流量。\nApps: 部署服务 将你的服务部署到由平台所有者提供的 Kubernetes 命名空间中，例如：\nkubectl apply -n bookinfo -f https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/platform/kube/bookinfo.yaml 识别 Tetrate 管理的命名空间 由 Tetrate 管理的命名空间将启用 Istio 注入。你可以使用 kubectl describe ns \u0026lt;namespace\u0026gt; 来验证此信息，并查找标签 istio-injection=enabled。你部署的 Pod 将在运行时具有一个额外的 istio-proxy 容器，以及几个瞬态的 init 容器。 Apps: 公开服务 使用 Gateway 资源公开一个服务。这将配置 Ingress 网关以将流量负载均衡到你的服务：\nGateway 资源引用了以下资源，这些资源应由你的管理员（平台所有者）提供：\nTetrate 组织 tse，租户 tse，工作空间 bookinfo-ws 和 Gateway 组 bookinfo-gwgroup-1 位于 bookinfo/bookinfo-ingress-gw 中的 Ingress Gateway 突出显示的行包含了这些资源的名称：\ncat \u0026lt;\u0026lt;EOF \u0026gt; bookinfo-gateway.yaml apiVersion: gateway.tsb.tetrate.io/v2 kind: Gateway metadata: name: bookinfo-gateway annotations: # highlight-start tsb.tetrate.io/organization: tse tsb.tetrate.io/tenant: tse tsb.tetrate.io/workspace: bookinfo-ws tsb.tetrate.io/gatewayGroup: bookinfo-gwgroup-1 # highlight-end spec: workloadSelector: # highlight-start namespace: bookinfo # highlight-end labels: # highlight-start app: bookinfo-ingress-gw # highlight-end http: - name: bookinfo port: 80 hostname: bookinfo.tse.tetratelabs.io routing: rules: - route: serviceDestination: host: bookinfo/productpage.bookinfo.svc.cluster.local port: 9080 EOF kubectl apply -f bookinfo-gateway.yaml 此配置在所选的 Ingress 网关上实例化。Ingress 网关是一个基于 Envoy 的代理，用于侦听传入流量（在本例中是端口 80 上的 http 流量）。然后，将具有主机头 bookinfo.tse.tetratelabs.io 的流量路由到你的服务 bookinfo/productpage.bookinfo.svc.cluster.local，端口为 9080。\n了解 Tetrate Gateway 在幕后，你应用的 Tetrate Gateway 对象将由 Tetrate 管理平面在 Tetrate 的配置层次结构中处理，然后用于创建一个或多个 Istio gateway 对象：\nkubectl get gateway -n bookinfo bookinfo-gateway -o yaml gateway 对象包含一个选择器，该选择器标识了应该实例化此配置的 Ingress 网关（一个 Envoy 代理 Pod）。\nTetrate 参数 如果你的 Tetrate 参数有任何错误，apply 操作将被拒绝。你将收到类似于以下错误消息：\nError from server: error when creating \u0026#34;bookinfo-gateway.yaml\u0026#34;: admission webhook \u0026#34;gitops.tsb.tetrate.io\u0026#34; denied the request: computing an access decision: checking permissions [organizations/tse/serviceaccounts/auto-cluster-cluster-1#exHFFwuGQSCPQP941C4ilMZFWwmO4lJoTyQXKjAPHXk ([CreateGateway]) organizations/tse/tenants/tse/workspaces/bookinfo-ws/gatewaygroups/bookinfo-gwgroup]: target \u0026#34;organizations/tse/tenants/tse/workspaces/bookinfo-ws/gatewaygroups/bookinfo-gwgroup\u0026#34; does not exist: node not found 这四个参数（organization、tenant、workspace 和 gatewayGroup）对应于由管理员（平台所有者）创建的 Tetrate 配置，它们定义了你的 Ingress 资源应位于 Tetrate 配置层次结构中的位置。\n工作负载选择器 workloadSelector 段标识了应将此配置应用于哪个 Ingress 网关（代理）。如果这些参数中的任何一个不正确（它们与 Ingress 控制器不匹配），则不会应用配置。\n在上述示例中，你可以按以下方式找到 Ingress 网关：\nkubectl get pods -n bookinfo -l=app=bookinfo-ingress-gw 检查 Ingress 网关 Ingress Gateway pod 部署在由你分配的 Tetrate 工作空间管理的一个命名空间中：\nkubectl get ingressgateway -A 检查 Ingress Gateway 配置，并查看托管 Envoy 代理的 Pod：\nkubectl describe ingressgateway -n bookinfo bookinfo-ingress-gw kubectl describe pod -n bookinfo -l app=bookinfo-ingress-gw 跟踪日志，包括实时访问日志：\nkubectl logs -n bookinfo -l app=bookinfo-ingress-gw Route 53 同步 如果管理员配置了 AWS Route 53 同步，那么 Tetrate 集成还将根据你的 Gateway 资源中的 hostname 自动为你配置一个基于 Route 53 DNS 的名称。\n如果你具有访问 istio-system\n命名空间的权限，你可以从 Tetrate 的 Route 53 控制器中查看日志：\nkubectl logs -n istio-system -l app=route53-controller 通过 Ingress 网关访问服务 如果为你配置了 DNS，你应该能够直接访问服务：\ncurl http://bookinfo.tse.tetratelabs.io 你还可以确定 Ingress Gateway 的公共地址（IP 或 FQDN）并直接发送流量：\nexport GATEWAY_IP=$(kubectl -n bookinfo get service bookinfo-ingress-gw -o jsonpath=\u0026#34;{.status.loadBalancer.ingress[0][\u0026#39;hostname\u0026#39;,\u0026#39;ip\u0026#39;]}\u0026#34;) curl -s --connect-to bookinfo.tse.tetratelabs.io:80:$GATEWAY_IP \\ \u0026#34;http://bookinfo.tse.tetratelabs.io/productpage\u0026#34; 进一步阅读 有关配置 Ingress Gateway 资源以控制如何公开服务的更多信息，请参阅以下文档：\n附加的 Gateway 示例 (TSB 文档) Gateway 参考文档：TSB / TSE ","relpermalink":"/book/tsb/design-guides/app-onboarding/deploy-service/","summary":"应用程序所有者的用户体验在他们直接使用原生 Kubernetes API 与 Kubernetes 平台交互，或者依赖于间接方法（如 CD 流水线）来部署和配置平台时几乎没有变化。 应用程序所有者 (“Apps”) 将按以下步骤部署和公开服务： 部署服务 在由平台所有者提供的 Kubernetes 命","title":"部署服务和配置网关规则"},{"content":"在本部分中，你将了解如何使用 TSB UI 或 tctl 创建 TSB 租户。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 使用用户界面 在左侧面板的组织下，选择租户。 单击该卡以添加新租户。 输入租户 ID tetrate 。 向你的租户提供显示名称和描述。 单击添加。 使用 tctl 创建以下 tenant.yaml 文件：\napiVersion: api.tsb.tetrate.io/v2 kind: Tenant metadata: organization: tetrate name: tetrate spec: displayName: Tetrate 使用 tctl 应用配置：\ntctl apply -f tenant.yaml 通过执行这些步骤，你将成功创建一个名为 tetrate 的 TSB 租户。该租户可用于组织和管理你的 TSB 环境。\n","relpermalink":"/book/tsb/quickstart/tenant/","summary":"使用 UI 或 tctl 创建 TSB 租户。","title":"创建租户"},{"content":"启动工作负载载入代理 创建文件 /etc/onboarding-agent/onboarding.config.yaml，并填入以下内容。将 \u0026lt;ONBOARDING_ENDPOINT_ADDRESS\u0026gt; 替换为之前获取的值 。\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: \u0026#34;\u0026lt;ONBOARDING_ENDPOINT_ADDRESS\u0026gt;\u0026#34; transportSecurity: tls: sni: onboarding-endpoint.example # (1) workloadGroup: # (2) namespace: bookinfo name: ratings workload: labels: version: v5 # (3) 此配置指示工作负载载入代理使用一个地址连接到工作负载载入终端点，但对 DNS 名称 onboarding-endpoint.example 验证 TLS 证书 (1)。\n代理将尝试加入你之前创建的 WorkloadGroup (2)。\n在 (3) 中指定的额外标签将与工作负载关联，但不会影响工作负载与 WorkloadGroup 的匹配。\n此配置暗示 Kubernetes 集群和本地虚拟机位于同一网络或已连接的网络中。\n将上述配置文件放置在正确的位置后，执行以下命令以启动工作负载载入代理：\n# 启用 sudo systemctl enable onboarding-agent # 启动 sudo systemctl start onboarding-agent 通过执行以下命令验证 Istio Sidecar 是否已启动：\ncurl -f -i http://localhost:15000/ready 你应该会得到类似于以下内容的输出：\nHTTP/1.1 200 OK content-type: text/plain; charset=UTF-8 server: envoy LIVE 验证工作负载 从你的本地机器上，验证工作负载是否已正确载入。\n执行以下命令：\nkubectl get war -n bookinfo 如果工作负载已正确载入，你应该会得到类似于以下内容的输出：\nNAMESPACE NAME AGENT CONNECTED AGE bookinfo ratings-jwt-my-corp--vm007-datacenter1-us-east.internal.corp True 1m 验证从 Kubernetes 到本地虚拟机的流量 要验证从 Kubernetes Pod 到本地虚拟机的流量，请在 Kubernetes 上部署的 Bookinfo 应用程序上创建一些负载，并确认请求被路由到本地虚拟机上部署的 ratings 应用程序。\n如果尚未设置端口转发，请在你的本地机器上设置端口转发 。\n然后运行以下命令：\nfor i in `seq 1 9`; do curl -fsS \u0026#34;http://localhost:9080/productpage?u=normal\u0026#34; | grep \u0026#34;glyphicon-star\u0026#34; | wc -l | awk \u0026#39;{print $1\u0026#34; stars on the page\u0026#34;}\u0026#39; done 其中两次中的一次应该会显示消息 10 stars on the page。\n此外，你可以通过检查由 Istio sidecar 代理的传入 HTTP 请求的访问日志 来验证虚拟机是否接收到流量。\n执行以下命令：\njournalctl -u onboarding-agent -o cat 你应该会看到类似以下内容的输出：\n[2021-10-25T11:06:13.553Z] \u0026#34;GET /ratings/0 HTTP/1.1\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 0 48 3 2 \u0026#34;-\u0026#34; \u0026#34;curl/7.68.0\u0026#34; \u0026#34;1928e798-dfe7-45a6-9020-d0f3a8641d03\u0026#34; \u0026#34;172.31.7.211:9080\u0026#34; \u0026#34;127.0.0.1:9080\u0026#34; inbound|9080|| 127.0.0.1:40992 172.31.7.211:9080 172.31.7.211:35470 - default 验证从本地虚拟机到 Kubernetes 的流量 SSH 进入本地虚拟机并执行以下命令：\nfor i in `seq 1 5`; do curl -i \\ --resolve details.bookinfo:9080:127.0.0.2 \\ details.bookinfo:9080/details/0 done 上述命令将向 Bookinfo details 应用程序发出 5 个 HTTP 请求。curl 将解析 Kubernetes 集群本地的 DNS 名称 details.bookinfo 为 Istio 代理的 egress 监听器的 IP 地址（根据你之前创建的 sidecar 配置 为 127.0.0.2）。\n你应该会得到类似以下内容的输出：\nHTTP/1.1 200 OK content-type: application/json server: envoy {\u0026#34;id\u0026#34;:0,\u0026#34;author\u0026#34;:\u0026#34;William Shakespeare\u0026#34;,\u0026#34;year\u0026#34;:1595,\u0026#34;type\u0026#34;:\u0026#34;paperback\u0026#34;, \u0026#34;pages\u0026#34;:200,\u0026#34;publisher\u0026#34;:\u0026#34;PublisherA\u0026#34;,\u0026#34;language\u0026#34;:\u0026#34; English\u0026#34;, \u0026#34;ISBN-10\u0026#34;:\u0026#34;1234567890\u0026#34;,\u0026#34;ISBN-13\u0026#34;:\u0026#34;123-1234567890\u0026#34;} ","relpermalink":"/book/tsb/setup/workload-onboarding/quickstart/on-premise/onboard-vm/","summary":"启动工作负载载入代理 创建文件 /etc/onboarding-agent/onboarding.config.yaml，并填入以下内容。将 \u003cONBOARDING_ENDPOINT_ADDRESS\u003e 替换为之前获取的值 。 apiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: \"\u003cONBOARDING_ENDPOINT_ADDRESS\u003e\" transportSecurity: tls: sni: onboarding-endpoint.example # (1) workloadGroup: # (2) namespace: bookinfo name: ratings workload:","title":"从本地虚拟机上进行工作负载载入"},{"content":"TSB 能够为网关和 sidecar 都应用速率限制。在本文档中，我们将启用 sidecar 的速率限制，以控制服务之间的流量配额。\n在开始之前，请确保你已经完成以下步骤：\n熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成了 TSB 使用快速入门 。本文档假定你已经创建了租户，熟悉工作区和配置组，并配置了 tctl 到你的 TSB 环境。 启用速率限制服务器 请阅读并按照 启用速率限制服务器文档 中的说明操作。\n演示安装 如果你使用 TSB 演示 安装，你已经有一个正在运行并且可以使用的速率限制服务，可以跳过这一部分。 如果你打算在多集群设置中使用相同的速率限制服务器，所有集群都必须指向相同的 Redis 后端和域 。\n部署 httpbin 服务 请按照 本文档中的说明 创建 httpbin 服务。你可以跳过 “暴露 httpbin 服务”、“创建证书” 和 “载入 httpbin 应用程序” 部分。\n创建 TrafficSetting 创建一个 TrafficSetting 对象，保存在名为 service-to-service-rate-limiting-traffic-setting.yaml 的文件中。在此示例中，速率限制设置为每个路径每分钟最多 4 次请求。将 organization 和 tenant 替换为适当的值。\napiVersion: traffic.tsb.tetrate.io/v2 kind: TrafficSetting metadata: organization: \u0026lt;organization\u0026gt; tenant: \u0026lt;tenant\u0026gt; workspace: httpbin group: httpbin-traffic name: httpbin-traffic-settings-ratelimit spec: rateLimiting: settings: rules: - dimensions: - header: name: \u0026#34;:path\u0026#34; value: prefix: \u0026#34;/\u0026#34; limit: requestsPerUnit: 4 unit: MINUTE 使用 tctl 应用此清单：\ntctl apply -f service-to-service-rate-limiting-traffic-setting.yaml 部署 sleep 服务 由于你将配置服务之间的速率限制，因此需要另一个服务作为 httpbin 服务的客户端。\n请按照 本文档中的说明 创建 sleep 服务。你可以跳过 “创建 sleep 工作区” 部分。\n测试 你可以通过从 sleep 服务发送 HTTP 请求到 httpbin 服务来测试速率限制，并观察在一定数量的请求后速率限制生效。\n要从 sleep 服务发送请求，你需要确定 sleep 服务内的 Pod。 执行以下命令以查找 Pod 名称：\nkubectl get pod -n sleep -l app=sleep -o jsonpath={.items..metadata.name} 然后从此 Pod 发送请求到 httpbin 服务，该服务应该可以在 http://httpbin.httpbin:8000 上访问。确保将 \u0026lt;sleep-pod\u0026gt; 的值替换为适当的值：\nkubectl exec \u0026lt;sleep-pod\u0026gt; -n sleep -c sleep -- \\ curl http://httpbin.httpbin:8000/get \\ -s \\ -o /dev/null \\ -w \u0026#34;%{http_code}\\n\u0026#34; 重复执行上述命令超过 4 次。在 4 次请求之后，你应该会看到响应代码从 200 变为 429。\n由于速率限制规则基于请求路径，访问 httpbin 上的另一个路径，你应该会再次看到 200 响应：\nkubectl exec \u0026lt;sleep-pod\u0026gt; -n sleep -c sleep -- \\ curl http://httpbin.httpbin:8000/headers \\ -s \\ -o /dev/null \\ -w \u0026#34;%{http_code}\\n\u0026#34; 类似于前面的示例，重复执行上述命令超过 4 次应该会导致速率限制生效，你应该开始看到 429 而不是 200。\n","relpermalink":"/book/tsb/howto/rate-limiting/service-to-service/","summary":"TSB 能够为网关和 sidecar 都应用速率限制。在本文档中，我们将启用 sidecar 的速率限制，以控制服务之间的流量配额。 在开始之前，请确保你已经完成以下步骤： 熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成了 TSB 使用快速入","title":"服务之间的速率限制"},{"content":"Tetrate Service Bridge 默认情况下会为每个工作区部署一个入口网关。我们这样做是为了保持团队隔离，防止共享故障和以信心实现高速度。然而，在大规模部署中，由于诸如负载均衡器地址过多或网关 Pod 在低流量应用中利用率低等原因，这可能是不可取的。因此，TSB 支持配置共享入口网关部署：换句话说，个别应用团队仍然可以发布自己的配置，但它们在运行时都会配置相同的 Envoy 实例。\n什么是网关？ 在 Istio 中，“网关” 这个词有点令人困惑，因为它指的是几个不同的事物：\n作为 Kubernetes 入口网关运行的一组 Envoy，我们将其称为 “网关部署\u0026#34;。 Istio 配置资源，即 Istio 网关 API — 用于在运行时配置 网关部署 的端口、协议和证书。我们将其称为 “Istio 网关 API 资源\u0026#34;。 用于配置 Kubernetes 入口的 Kubernetes 网关 API — 它与 Istio 的 网关 API 资源 做相同的事情，但是是一个原生的 Kubernetes 构造。我们将其称为 “Kubernetes 网关 API 资源\u0026#34;。 网关部署 是一组真正运行的 Envoy，而 Istio 网关 API 资源 和 Kubernetes 网关 API 资源 都是运行 Envoy 的配置。\n在本文中，我们只会关注 TSB 应用入口网关，而不是应用边缘网关（有关 “网关术语 ” 和 “TSB 中的网关 ” 的更多信息）。\n在 TSB 中创建共享网关 当我们配置共享网关时，我们需要做一个基本的决定：谁来管理共享网关，以及应用程序的每个配置存放在哪里？\n通常情况下，一个中心团队，如平台或负载均衡组拥有（共享）网关部署，而个别应用团队希望配置它们。我们将共享网关部署称为 “共享网关”，并建议将它们放在自己专用的 Kubernetes 命名空间和 TSB 工作区中。我们将分别称之为 “共享网关命名空间” 和 “共享网关工作区”。\n然后我们需要决定应用程序的每个配置存放在哪里：是在共享网关命名空间中与共享网关一起，还是在应用程序的命名空间中与应用程序一起。将配置放在共享网关命名空间中意味着共享网关所有者参与配置更改，并可以帮助防止共享故障中断，但这可能会导致共享网关所有者成为所有网关更改的瓶颈，可能会影响灵活性。将配置放在应用程序的命名空间中意味着它可以像应用程序本身一样快速更改，但由于没有中央所有者审查对共享网关的更改，可能会增加由于配置错误导致的共享故障的风险。\nTSB 的桥接模式——网关组 ，对于使用共享网关更安全（如阻止同一主机名的多个所有者）并且使得在网格中的大多数应用程序使用共享网关变得可行。你还可以使用直连模式—— VirtualServices，使用原始的 Istio 配置来配置共享网关，但你需要执行规则来防止共享故障（通常通过代码审查来实现）。\n最终，大多数组织将稳定性优先于功能速度，因此我们建议将应用程序配置放在共享网关命名空间中，以便由拥有共享网关的团队进行审查。\n部署 httpbin 服务 请按照此文档中的说明 创建 httpbin 服务。完成该文档中的所有步骤。\n部署共享网关 要部署共享网关，我们需要在 TSB 中创建一个工作区来托管我们的共享网关，以及为使用共享网关的应用程序创建工作区。\nTSB 设置 首先，我们将创建一个 TSB 租户来保存我们的共享入口示例；在实际部署中，你可以使用现有的租户或为这类用途创建一个共享基础设施租户：\napiversion: api.tsb.tetrate.io/v2 kind: Tenant metadata: organization: tetrate name: shared-ingress-example spec: displayName: Shared Ingress Example description: Tenant for the Shared Ingress example 然后我们将为我们的共享入口创建一个工作区和组：\napiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tetrate tenant: shared-ingress-example name: ingress spec: displayName: Shared Ingress description: Workspace for the shared ingress namespaceSelector: names: - \u0026#34;*/shared-ingress-ns\u0026#34; --- apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: organization: tetrate tenant: shared-ingress-example workspace: ingress name: shared-app-gateway spec: configMode: BRIDGED namespaceSelector: names: - \u0026#34;*/shared-ingress-ns\u0026#34; tctl apply 所有这些文件。\n每个共享网关实例 最后我们部署共享网关本身：\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: shared-gateway namespace: shared-ingress-ns spec: kubeSpec: service: type: LoadBalancer 你需要在要托管共享入口网关部署的每个集群中运行 kubectl apply 命令来应用此文件。\n配置共享网关 现在我们可以继续配置共享网关部署，可以通过以下方式之一完成：\n在共享网关工作区中发布配置 或者\n在应用程序工作区中发布配置 在这两种情况下，我们都会使用 workloadSelector 来定位网关。在应用程序工作区的情况下，我们还需要在网关部署上进行一些额外的配置。\n为获得最佳效果，请勿在同一个共享网关上混合使用桥接模式和直连模式 TSB 的桥接模式有助于确保来自不同团队的配置被隔离，从而减轻了常见的共享命运故障。但 TSB 无法为直连模式配置提供相同的保证。TSB 支持在针对同一个共享网关时同时使用桥接模式和直连模式配置，但无法在这样做时保证桥接模式的所有安全保证。因此，我们建议使用桥接模式的团队使用单独的共享网关部署，而使用直连模式的团队则与直连模式的 Istio 配置隔离开来，从而充分受益于 TSB 桥接模式的安全保证。 在共享网关工作区中发布配置 对于任何给定应用程序，其共享网关配置将应用于 共享网关工作区。\n存储 TLS 证书的位置 对于启用了 TLS 的应用程序，共享网关将需要在与共享网关相同的命名空间中应用证书。 以下是创建我们将在接下来的示例中使用的 secret 的示例：\nkubectl -n shared-ingress-ns create secret tls httpbin-certs \\ --key certs/httpbin.key \\ --cert certs/httpbin.crt 选择 一种 方法来通过共享网关配置应用程序入口：\n通过 IngressGateway 进行桥接模式配置 通过 Istio Gateway 和 VirtualService 进行直连模式配置 通过 IngressGateway 进行桥接模式配置 我们可以在 TSB 中配置一个 IngressGateway，以将流量从共享网关路由到我们的应用程序：\napiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: organization: tetrate tenant: shared-ingress-example workspace: ingress group: shared-app-gateway name: ingress-httpbin spec: # Use the namespace from our IngressGateway with an app label that matches the `name`. workloadSelector: namespace: shared-ingress-ns labels: app: shared-gateway # `name` from our IngressGateway http: - name: httpbin port: 443 hostname: httpbin.tetrate.com tls: mode: SIMPLE secretName: httpbin-certs routing: rules: - match: - headers: \u0026#34;:method\u0026#34;: exact: \u0026#34;GET\u0026#34; route: host: httpbin/httpbin.httpbin.svc.cluster.local 请运行 tctl apply 命令来应用上述配置，从而通过共享网关实现对 httpbin.tetrate.com 到 httpbin 服务的路由。\n你可以使用以下命令发送一些流量到我们的 httpbin 以验证 TSB 配置。\nexport GATEWAY_IP=$(kubectl -n shared-ingress-ns get service shared-gateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) curl -k --resolve httpbin.tetrate.com:443:$GATEWAY_IP https://httpbin.tetrate.com/ 直接模式：使用 VirtualServices 配置 我们也可以通过 Istio 配置直接配置共享网关，方法是创建一个 Gateway 和一个 VirtualService。在许多环境中，Gateway 将由中央团队管理，你只需要发布 VirtualService – 你可以通过运行 kubectl get gateway --namespace shared-ingress-ns 来进行检查：\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: httpbin-shared-gateway namespace: shared-ingress-ns annotations: tsb.tetrate.io/organization: tetrate tsb.tetrate.io/tenant: shared-ingress-example tsb.tetrate.io/workspace: ingress tsb.tetrate.io/gatewayGroup: shared-app-gateway spec: selector: app: shared-gateway # `name` from our IngressGateway servers: - port: number: 443 name: https protocol: HTTPS hosts: - \u0026#34;httpbin.tetrate.com\u0026#34; tls: mode: SIMPLE credentialName: httpbin-certs --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: httpbin namespace: shared-ingress-ns …","relpermalink":"/book/tsb/howto/gateway/shared-ingress/","summary":"如何使用 TSB 配置单个入口网关部署，而不是默认情况下的每个工作区部署。","title":"共享入口网关"},{"content":"Tetrate Service Bridge 收集了大量的指标。此页面是由 Tetrate 内部运行的仪表板生成的，将根据 Tetrate 的操作经验和用户部署中学到的最佳实践定期更新。每个标题代表一个不同的仪表板，每个子标题是该仪表板上的一个面板。因此，你可能会看到多次出现相同的指标。\n本文档中描述的指标构建了一系列 Grafana 仪表板，可以从此处 下载，以便将它们导入到你的 Grafana 设置中。\n指标过多，不在此文中列出，详细指标请见 Tetrate Service Bridge 文档 。\n","relpermalink":"/book/tsb/operations/telemetry/key-metrics/","summary":"由 Service Bridge 生成的关键指标","title":"关键指标"},{"content":"下面的 YAML 文件包含三个对象：\n用于应用程序的 Workspace 用于配置应用程序入口的 GatewayGroup 以及一个允许你配置金丝雀发布流程的 TrafficGroup 将文件存储为helloworld-ws-groups.yaml ，并使用tctl应用：\nhelloworld-ws-group.yaml apiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tetrate tenant: tetrate name: helloworld-ws spec: namespaceSelector: names: - \u0026#39;*/helloworld\u0026#39; --- apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: organization: tetrate tenant: tetrate workspace: helloworld-ws name: helloworld-gw spec: namespaceSelector: names: - \u0026#39;*/helloworld\u0026#39; configMode: BRIDGED --- apiVersion: traffic.tsb.tetrate.io/v2 kind: Group metadata: organization: tetrate tenant: tetrate workspace: helloworld-ws name: helloworld-trf spec: namespaceSelector: names: - \u0026#39;*/helloworld\u0026#39; configMode: BRIDGED tctl apply -f helloworld-ws-groups.yaml 要部署你的应用程序，首先创建命名空间并启用 Istio sidecar 注入。\nkubectl create namespace helloworld kubectl label namespace helloworld istio-injection=enabled 然后部署你的应用程序。\n存储为helloworld-1.yaml ，并使用kubectl应用：\nhelloworld-1.yaml apiVersion: apps/v1 kind: Deployment metadata: name: helloworld-v1 namespace: helloworld spec: replicas: 3 selector: matchLabels: app: helloworld version: v1 template: metadata: labels: app: helloworld version: v1 spec: containers: - name: hello image: \u0026#39;gcr.io/google-samples/hello-app:1.0\u0026#39; env: - name: \u0026#39;PORT\u0026#39; value: \u0026#39;8080\u0026#39; --- apiVersion: v1 kind: Service metadata: name: helloworld namespace: helloworld spec: selector: app: helloworld ports: - protocol: TCP port: 443 targetPort: 8080 kubectl apply -f helloworld-1.yaml 请注意，此部署将使用 3 个副本。\n在这个示例中，你将使用网关以简单的 TLS 方式公开应用程序。你需要提供一个 TLS 证书，将其存储在 Kubernetes 的一个密钥保管库中。\nkubectl create secret tls -n helloworld helloworld-cert \\ --cert /path/to/some/helloworld-cert.pem \\ --key /path/to/some/helloworld-key.pem 现在你可以部署你的入口网关。\n另存为helloworld-ingress.yaml ，并使用kubectl应用：\nhelloworld-ingress.yaml apiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: tsb-helloworld-gateway namespace: helloworld spec: kubeSpec: service: type: LoadBalancer kubectl apply -f helloworld-ingress.yaml 集群中的 TSB 数据面 Operator 将获取此配置并在你的应用程序命名空间中部署网关的资源。最后，配置网关以将流量路由到你的应用程序。\n存储为helloworld-gw.yaml ，并使用tctl应用：\nhelloworld-gw.yaml apiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: name: helloworld-gateway group: helloworld-gw workspace: helloworld-ws tenant: tetrate organization: tetrate spec: workloadSelector: namespace: helloworld labels: app: tsb-helloworld-gateway http: - name: helloworld port: 443 hostname: helloworld.tetrate.com tls: mode: SIMPLE secretName: helloworld-cert routing: rules: - route: host: helloworld/helloworld.helloworld.svc.cluster.local port: 5000 tctl apply -f helloworld-gw.yaml 你可以通过打开你的网络浏览器并将其指向网关服务的 IP 或域名（根据你的配置而定）来检查你的应用程序是否可访问。\n在这一点上，你的应用程序将默认使用轮询进行负载均衡。现在，配置客户端负载均衡并使用源 IP。\n另存为helloworld-client-lb.yaml ，并使用tctl应用：\nhelloworld-client-lb.yaml apiVersion: traffic.tsb.tetrate.io/v2 kind: ServiceRoute metadata: name: helloworld-client-lb group: helloworld-trf workspace: helloworld-ws tenant: tetrate organization: tetrate spec: service: helloworld/helloworld subsets: - name: v1 labels: version: v1 stickySession: useSourceIp: true tctl apply -f helloworld-client-lb.yaml 现在，同一 Pod 将被用作所有来自同一 IP 的请求的后端。\n在这个示例中，你使用了源 IP，但还有其他允许的方法; 使用 HTTP 请求的头部，或配置 HTTP Cookie。\n","relpermalink":"/book/tsb/howto/traffic/load-balance/","summary":"如何设置多个副本并在它们之间进行负载均衡。","title":"客户端负载均衡"},{"content":"此 Chart 安装 TSB 控制平面 Operator 以将集群引入。与管理平面 Helm Chart 类似，它还允许你使用TSB ControlPlane CR 安装 TSB 控制平面组件，以及使其正常运行所需的所有密钥。\n在开始之前，请确保你已完成以下操作：\n检查 Helm 安装过程 已安装 TSB 管理平面 使用 tctl 登录到管理平面 安装 yq 。这将用于从创建集群响应中获取 Helm 值。 隔离边界 TSB 1.6 引入了隔离边界，允许你在 Kubernetes 集群内或跨多个集群中拥有多个 TSB 管理的 Istio 环境。隔离边界的好处之一是你可以执行控制平面的金丝雀升级。\n要启用隔离边界，你必须使用环境变量 ISTIO_ISOLATION_BOUNDARIES=true 更新 Operator 部署，并在控制平面 CR 中包括 isolationBoundaries 字段。 有关更多信息，请参见隔离边界 。\n先决条件 在开始之前，你需要创建一个 集群对象 在 TSB 中表示你将安装 TSB 控制平面的集群。将 \u0026lt;cluster-name-in-tsb\u0026gt; 和 \u0026lt;organization-name\u0026gt; 替换为你的环境中的正确值：\napiVersion: api.tsb.tetrate.io/v2 kind: Cluster metadata: name: \u0026lt;cluster-name-in-tsb\u0026gt; organization: \u0026lt;organization-name\u0026gt; spec: displayName: \u0026#34;App Cluster\u0026#34; 要创建集群对象，请运行以下命令：\ntctl apply -f cluster.yaml -o yaml | yq .spec.installTemplate.helm \u0026gt; cluster-cp-values.yaml 文件 cluster-cp-values.yaml 包含了 TSB 控制平面 Operator 的默认配置，包括与 TSB 管理平面进行身份验证所需的任何必要密钥。要自定义安装，你可以通过在继续下一步之前向此文件添加所需的额外配置值来修改此文件。\n安装 使用以下 helm install 命令安装 TSB 控制平面。确保替换 \u0026lt;tsb-version\u0026gt; 和 \u0026lt;registry-location\u0026gt; 为正确的值。\nhelm install cp tetrate-tsb-helm/controlplane \\ --version \u0026lt;tsb-version\u0026gt; \\ --namespace istio-system --create-namespace \\ --timeout 5m \\ --values cluster-cp-values.yaml \\ --set image.registry=\u0026lt;registry-location\u0026gt; 等待 TSB 控制平面组件成功部署。要验证安装是否成功，你可以尝试登录到 TSB UI 或使用 tctl 连接到 TSB，然后检查集群列表，看看是否已经加入了集群。\n故障排除 如果在安装过程中遇到任何问题，请尝试以下几种故障排除方法：\n确保按照正确的顺序执行了所有步骤。 仔细检查 cluster-cp-values.yaml 文件中的配置值，确保它们是正确的。 检查 TSB 控制平面 Operator 的日志，查看是否有任何错误消息或堆栈跟踪，以帮助诊断问题。 如果你正在使用私有注册表来托管 TSB 控制平面 Operator 镜像，请确保已在 注册表进行了身份验证，并且 image.registry 值是正确的。\n检查集群引入故障排除 指南 。 配置 镜像配置 这是一个 必填 字段。将 registry 设置为你同步了 TSB 镜像的私有注册表，将 tag 设置为要部署的 TSB 版本。仅指定此字段将安装 TSB 控制平面 Operator，而不安装其他 TSB 组件。\n名称 描述 默认值 image.registry 用于下载 Operator 镜像的注册表 containers.dl.tetrate.io image.tag Operator 镜像的标签 与 Chart 版本相同 控制平面资源配置 这是一个 可选 字段。你可以在 Helm 值文件中设置 TSB ControlPlane CR ，以使 TSB 控制平面正常运行。\n名称 描述 默认值 spec 包含 ControlPlane CR 的 spec 部分 密钥配置 这是一个 可选 字段。你可以在安装 TSB 控制平面之前将密钥应用到你的集群，或者你可以使用 Helm 值来指定所需的密钥。请注意，如果要将密钥与控制平面规范分开，可以使用不同的 Helm 值文件。\n注意 请牢记这些选项只有助于创建密钥，并且必须遵守 TSB ManagementPlane CR 中提供的配置，否则安装将配置不正确。 名称 描述 默认值 secrets.keep 启用此选项会在卸载 Chart 后使生成的密钥持久存在于集群中，如果它们在未来的更新中没有提供的话。（请参阅 Helm 文档 ） false secrets.tsb.cacert 用于验证公开的管理平面（前端 envoy）TLS 证书的 CA 证书 secrets.elasticsearch.username 访问 Elasticsearch 的用户名 secrets.elasticsearch.password 访问 Elasticsearch 的密码 secrets.elasticsearch.cacert 控制平面用于验证 TLS 连接的 Elasticsearch CA 证书 secrets.oapToken 用于对接口进行身份验证的 JWT 令牌，该接口与管理平面（OAP）交互 secrets.otelToken 用于对接口进行身份验证的 JWT 令牌，该接口与管理平面（Otel Collector）交互 secrets.clusterServiceAccount.clusterFQN 集群资源的 TSB FQN。这将为所有控制平面代理生成令牌。 secrets.clusterServiceAccount.JWK 用于生成和签名所有控制平面代理令牌的文字 JWK secrets.clusterServiceAccount.encodedJWK 用于生成和签名所有控制平面代理令牌的 Base64 编码 JWK XCP 密钥配置 XCP 使用 JWT 进行 Edge 和 Central 之间的身份验证。\n如果提供了 XCP 根 CA (secrets.xcp.rootca)，它将用于验证 XCP Central 提供的 TLS 证书。\n此外，需要 secrets.xcp.edge.token 或 secrets.clusterServiceAccount 以对接 XCP Central 进行身份验证。\n以下是允许用于配置 XCP 身份验证模式的配置属性：\n名称 描述 默认值 secrets.xcp.rootca XCP 组件的 CA 证书 secrets.xcp.edge.token 用于对接 XCP Edge 和 XCP Central 进行身份验证的 JWT 令牌 secrets.clusterServiceAccount 用于生成和签名所有控制平面代理令牌的 Base64 编码 JWK Operator 扩展配置 这是一个 可选 字段。你可以使用以下可选属性自定义 TSB Operator 相关资源，如部署、服务或服务帐户：\n名称 描述 默认值 operator.deployment.affinity Pod 的亲和性配置 operator.deployment.annotations 要添加到部署的自定义注释集 operator.deployment.env 要添加到容器的自定义环境变量集 operator.deployment.podAnnotations 要添加到 Pod 的自定义注释集 operator.deployment.replicaCount 部署管理的副本数 operator.deployment.strategy 要使用的部署策略 operator.deployment.tolerations 适用于 Pod 调度的耐受性集合 operator.deployment.podSecurityContext 应用于 Pod 的 SecurityContext 属性 operator.deployment.containerSecurityContext 应用于 Pod 的容器的 SecurityContext 属性 operator.service.annotations 要添加到服务的自定义注释集 operator.serviceAccount.annotations 要添加到服务帐户的自定义注释集 operator.serviceAccount.imagePullSecrets 从注册表拉取镜像所需的密钥名称集合 operator.pullSecret JSON 编码的 Docker 配置，将存储为镜像拉取密钥 ","relpermalink":"/book/tsb/setup/helm/controlplane/","summary":"如何使用 Helm 安装控制平面组件。","title":"控制平面安装"},{"content":"添加第二个 Edge Gateway 以提供 Edge 高可用性。\n我们将扩展在演示环境 说明中描述的演示环境。我们将在第二个区域中添加一个额外的 Edge 集群，并部署一个 Edge Gateway：\n在 region-2 上登记一个额外的 Edge 集群 扩展 Tetrate 配置以涵盖 edge-ws，并创建第二个 edge-gwgroup-2 组 创建一个 edge 命名空间并在该命名空间中部署一个 Edge Gateway 部署一个为该 Edge Gateway 暴露服务的 Gateway 资源 开始之前 在配置中有许多组件，因此在继续之前，标识并命名每个组件将非常有帮助：\ncluster-1 cluster-2 cluster-edge cluster-edge-2 AWS 区域： eu-west-1 eu-west-2 eu-west-1 eu-west-2 命名空间： bookinfo bookinfo edge edge 工作空间： bookinfo-ws bookinfo-ws edge-ws edge-ws 网络： app-network app-network edge-network edge-network 网关组： bookinfo-gwgroup-1 bookinfo-gwgroup-2 edge-gwgroup edge-gwgroup-2 入口网关： ingressgw-1 ingressgw-2 edgegw edgegw-2 网关资源： bookinfo-ingress-1 bookinfo-ingress-2 bookinfo-edge bookinfo-edge-2 Kubectl 上下文别名： k1 k2 k3 k4 先决条件 这些说明从创建演示环境 中描述的部署中继续。\n扩展演示环境 添加第二个 Edge Gateway 我们将执行以下步骤：\n登记一个额外的 Edge 集群 扩展 Tetrate 配置（工作空间、新的网关组、集群设置） 配置集群以添加 edge 命名空间和 Tier1Gateway 部署 Gateway 资源以公开服务 操作步骤... 登记额外的 Edge 集群 按照 Tetrate Service Express 或 Tetrate Service Bridge 的说明，登记新的 cluster-edge-2 集群。确保安装任何所需的依赖项，例如 AWS 负载均衡控制器或 Tetrate Route 53 控制器。\n扩展 Tetrate 配置 我们将执行以下操作：\n扩展 edge-ws 工作空间，以包括新的集群和命名空间（稍后创建） 添加一个新的 网关组 用于新的集群 编辑集群设置，设置 tier1Cluster 和 network 值 cat \u0026lt;\u0026lt;EOF \u0026gt; edge-ws.yaml apiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tse tenant: tse name: edge-ws spec: namespaceSelector: names: - \u0026#34;cluster-edge/edge\u0026#34; - \u0026#34;cluster-edge-2/edge\u0026#34; EOF tctl apply -f edge-ws.yaml cat \u0026lt;\u0026lt;EOF \u0026gt; edge-gwgroup-2.yaml apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: tenant: tse organization: tse workspace: edge-ws name: edge-gwgroup-2 spec: namespaceSelector: names: - \u0026#39;cluster-edge-2/edge\u0026#39; EOF tctl apply -f edge-gwgroup-2.yaml 使用界面编辑 edge-cluster-2 集群，将 “Is Tier 1?” 设置为 true 并分配 Edge-Network 网络。\n配置集群 创建 edge 命名空间并部署 Edge Gateway。记得设置 kubectl 上下文或使用上下文别名来指向 cluster-edge-2。\nkubectl create namespace edge kubectl label namespace edge istio-injection=enabled cat \u0026lt;\u0026lt;EOF \u0026gt; edgegw-2.yaml apiVersion: install.tetrate.io/v1alpha1 kind: Tier1Gateway metadata: name: edgegw-2 namespace: edge spec: kubeSpec: service: type: LoadBalancer EOF kubectl apply -f edgegw-2.yaml 部署 Gateway 资源 cat \u0026lt;\u0026lt;EOF \u0026gt; bookinfo-edge-2.yaml apiVersion: gateway.tsb.tetrate.io/v2 kind: Gateway metadata: organization: tse tenant: tse workspace: edge-ws group: edge-gwgroup-2 name: bookinfo-edge-2 spec: workloadSelector: namespace: edge labels: app: edgegw-2 http: - name: bookinfo port: 80 hostname: bookinfo.tse.tetratelabs.io routing: rules: - route: clusterDestination: {} EOF tctl apply -f bookinfo-edge-2.yaml 一旦配置完成，你的集群摘要应如下所示：\n验证服务是否正常运行 无需 DNS 我们在这些测试中不会使用 DNS，因为我们希望精确控制请求我们服务（bookinfo.tse.tetratelabs.io）将路由到哪个 Edge Gateway。 请务必设置正确的 Kubernetes 上下文，获取每个 Edge Gateway 的地址：\nexport GATEWAY_IP_1=$(kubectl -n edge get service edgegw -o jsonpath=\u0026#34;{.status.loadBalancer.ingress[0][\u0026#39;hostname\u0026#39;,\u0026#39;ip\u0026#39;]}\u0026#34;) echo $GATEWAY_IP_1 export GATEWAY_IP_2=$(kubectl -n edge get service edgegw-2 -o jsonpath=\u0026#34;{.status.loadBalancer.ingress[0][\u0026#39;hostname\u0026#39;,\u0026#39;ip\u0026#39;]}\u0026#34;) echo $GATEWAY_IP_2 验证你是否可以通过每个 Edge Gateway 访问 productpage.bookinfo 服务：\ncurl -s --connect-to bookinfo.tse.tetratelabs.io:80:$GATEWAY_IP_1 \u0026#34;http://bookinfo.tse.tetratelabs.io/productpage\u0026#34; curl -s --connect-to bookinfo.tse.tetratelabs.io:80:$GATEWAY_IP_2 \u0026#34;http://bookinfo.tse.tetratelabs.io/productpage\u0026#34; 下一步 你现在可以尝试Edge 集群故障转移 行为。\n","relpermalink":"/book/tsb/design-guides/ha-multicluster/demo-2/","summary":"添加第二个 Edge Gateway 以提供 Edge 高可用性。 我们将扩展在演示环境 说明中描述的演示环境。我们将在第二个区域中添加一个额外的 Edge 集群，并部署一个 Edge Gateway： 在 region-2 上登记一个额外的 Edge 集群 扩展 Tetrate 配置以涵盖 edge-ws","title":"扩展演示环境"},{"content":"你将在 AWS EC2 实例上部署 ratings 应用程序，并将其加入到服务网格中。\n创建 WorkloadGroup 执行以下命令创建一个 WorkloadGroup：\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: labels: app: ratings class: vm cloud: aws network: aws # (1) serviceAccount: bookinfo-ratings # (2) EOF 字段 spec.template.network 设置为非空值，以指示 Istio 控制平面，你稍后将创建的 VM 没有直接连接到 Kubernetes Pods。\n字段 spec.template.serviceAccount 声明工作负载具有 Kubernetes 集群中的服务账号 bookinfo-ratings 的身份。服务账号 bookinfo-ratings 是在之前部署 Istio bookinfo 示例时 创建的。\n创建 Sidecar 配置 执行以下命令创建一个新的 sidecar 配置：\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1beta1 kind: Sidecar metadata: name: bookinfo-ratings-no-iptables namespace: bookinfo spec: workloadSelector: # (1) labels: app: ratings class: vm ingress: - defaultEndpoint: 127.0.0.1:9080 # (2) port: name: http number: 9080 # (3) protocol: HTTP egress: - bind: 127.0.0.2 # (4) port: name: http number: 9080 # (5) protocol: HTTP hosts: - ./* # (6) EOF 上述 sidecar 配置将仅应用于具有标签 app=ratings 和 class=vm（1）的工作负载。你已经创建的 WorkloadGroup 具有这些标签。\nIstio 代理将配置为侦听 \u0026lt;主机 IP\u0026gt;:9080（3），并将 传入 请求转发到侦听 127.0.0.1:9080（2）的应用程序。\n最后，代理将配置为侦听 127.0.0.2:9080（4）（5）以代理 传出 请求，将其发送到其他服务（6），这些服务的端口为 9080（5）。\n允许工作负载加入 WorkloadGroup 你需要创建一个 OnboardingPolicy 资源，以明确授权部署在 Kubernetes 之外的工作负载加入网格。\n首先，获取你的 AWS 账户 ID 。 如果不知道你的 AWS 账户 ID，请参阅 AWS 账户文档 以获取有关如何查找你的 ID 的更多详细信息。\n如果你已经设置了 aws CLI ，可以执行以下命令：\nAWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) 然后，通过执行以下命令创建一个 OnboardingPolicy，以允许你的 AWS EC2 实例的任何实例 通过执行以下命令加入 bookinfo 命名空间中的任何 WorkloadGroup。将 \u0026lt;AWS_ACCOUNT_ID\u0026gt; 替换为适当的值。\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: authorization.onboarding.tetrate.io/v1alpha1 kind: OnboardingPolicy metadata: name: allow-aws-vms namespace: bookinfo # (1) spec: allow: - workloads: - aws: accounts: - \u0026lt;AWS_ACCOUNT_ID\u0026gt; # (2) ec2: {} # (3) onboardTo: - workloadGroupSelector: {} # (4) EOF 上述策略适用于任何 AWS EC2 实例（3），由 (2) 中指定的账户拥有，允许它们加入命名空间 bookinfo（1）中的任何 WorkloadGroup（4）。\n","relpermalink":"/book/tsb/setup/workload-onboarding/quickstart/aws-ec2/configure-workload-onboarding/","summary":"你将在 AWS EC2 实例上部署 ratings 应用程序，并将其加入到服务网格中。 创建 WorkloadGroup 执行以下命令创建一个 WorkloadGroup： cat \u003c\u003cEOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: labels: app: ratings class: vm cloud: aws network: aws # (1) serviceAccount: bookinfo-ratings # (2) EOF 字段 spec.template.network 设置为非空","title":"配置 WorkloadGroup 和 Sidecar"},{"content":"Tetrate Service Bridge 的 tctl CLI 允许你与 TSB API 交互以应用对象的配置。本文档介绍如何使用 tctl 了解系统中资源配置的部署状态。\n资源状态 TSB 通过 ResourceStatus 跟踪配置更改的生命周期。你可以使用 tctl x status 获取它们。运行 tctl x status --help 可查看所有可能的选项。\n根据资源的配置状态计算方式，有不同类型的资源。\n资源类型 配置状态 示例 父资源 聚合其子资源的状态。 workspace, trafficgroup, gatewaygroup, securitygroup 子资源 不依赖于其他资源。 ingressgateway, egressgateway, trafficsettings 等 不可配置资源 不会直接在目标集群中实体化为配置。 organizations, tenants, users 具有依赖关系的资源 高级别资源。 applications 和 apis 资源状态可以具有多个值，这取决于其配置在TSB 组件 中的传播程度。\n类型 状态 条件 子资源和不可配置资源 ACCEPTED 它们的配置已经通过验证并持久化。这是对有效配置的初始值。 READY 它们的配置已传播到所有目标集群。这也是不可配置资源的默认状态。 PARTIAL 其中一些配置在某些目标集群中是就绪的，但在其中一些目标集群中不是。 FAILED 它们的配置在一些或所有目标集群中触发了一些内部错误。 FAILED 目标集群中的某个问题资源会影响配置的正确行为。 父资源 ACCEPTED 它们的所有子资源都是 ACCEPTED 或 READY。 READY 它们的所有子资源都是 READY。 FAILED 它们的任何子资源都是 FAILED。 具有依赖关系的资源 ACCEPTED 其所有依赖配置都是 ACCEPTED。 READY 其所有依赖配置都是 READY。 DIRTY 其所有依赖配置都是 DIRTY。 FAILED 其任何依赖配置都是 FAILED。 PARTIAL 其依赖配置处于 READY, ACCEPTED 和/或 DIRTY 的混合状态。 你可以在 状态 API 规范 中了解更多关于状态类型的信息。\n使用 tctl 了解配置对象的状态 让我们在部署了 bookinfo 应用程序的情况下，看看一些示例。\n提示 我们假设 Bookinfo 应用程序已在其自己的工作区中部署，就像在我们的 快速入门 教程中一样，并已配置相应的组。 你可以使用 tctl x status 检查 bookinfo 入口网关的状态：\n$ tctl x status ig --tenant tetrate --workspace bookinfo --gatewaygroup bookinfo bookinfo NAME STATUS LAST EVENT MESSAGE bookinfo ACCEPTED XCP_ACCEPTED 这显示其配置已被验证并持久化。\n如果你想获取更多信息，它的 YAML 版本将显示此资源状态的事件历史记录。这些信息对于排查资源配置的生命周期非常有用。\n$ tctl x status ig --tenant tetrate --workspace bookinfo --gatewaygroup bookinfo bookinfo apiVersion: api.tsb.tetrate.io/v2 kind: ResourceStatus metadata: group: bookinfo name: bookinfo organization: tetrate tenant: tetrate workspace: bookinfo spec: configEvents: events: - etag: \u0026#39;\u0026#34;sMlEWPbvm6M=\u0026#34;\u0026#39; timestamp: \u0026#34;2022-02-10T16:54:14.710165091Z\u0026#34; type: XCP_ACCEPTED - etag: \u0026#39;\u0026#34;sMlEWPbvm6M=\u0026#34;\u0026#39; timestamp: \u0026#34;2022-02-10T16:54:14.649002805Z\u0026#34; type: MPC_ACCEPTED - etag: \u0026#39;\u0026#34;sMlEWPbvm6M=\u0026#34;\u0026#39; timestamp: \u0026#34;2022-02-10T16:54:10.453242255Z\u0026#34; type: TSB_ACCEPTED status: ACCEPTED 在这里，你可以看到更改了最后一个版本 sMlEWPbvm6M= 的此 ingressgateway 资源的状态的事件历史记录，最近的事件排在最前面。\n在此示例中，资源首先被 TSB 服务器接受，然后是 MPC，最后是 XCP 组件。\n请注意，只有最新资源版本的历史记录会被保留。在接下来的部分中，你将学会如何使用审计日志来显示所有版本的历史记录。\n使用 TSB 审计日志了解配置对象的生命周期 TSB 有一个称为审计日志的概念，显示了发生在 TSB 资源上的所有事件。谁在何时对每个资源进行了什么操作，它还可以提供有关其配置的不同阶段的见解。\n例如，你可以使用以下命令获取在 bookinfo 工作区中发生的所有事件的列表以及其中包含的所有资源。\n$ tctl x audit ws bookinfo --recursive --text bookinfo TIME SEVERITY TYPE OPERATION USER MESSAGE 2022/02/10 17:02:53 INFO api.tsb.tetrate.io/v2/ResourceStatus XCP_CENTRAL_ACCEPTED mpc New ACCEPTED status due to XCP_CENTRAL_ACCEPTED event for trafficgroup \u0026#34;bookinfo\u0026#34; version \u0026#34;oxil15u6bfw=\u0026#34; 2022/02/10 17:02:53 INFO api.tsb.tetrate.io/v2/ResourceStatus XCP_CENTRAL_ACCEPTED mpc New ACCEPTED status due to XCP_CENTRAL_ACCEPTED event for securitygroup \u0026#34;bookinfo\u0026#34; version \u0026#34;gEUA3cK7+YI=\u0026#34; 2022/02/10 17:02:52 INFO api.tsb.tetrate.io/v2/ResourceStatus MPC_ACCEPTED mpc New ACCEPTED status due to MPC_ACCEPTED event for ingressgateway \u0026#34;bookinfo\u0026#34; version \u0026#34;sMlEWPbvm6M=\u0026#34; 2022/02/10 17:02:52 INFO api.tsb.tetrate.io/v2/ResourceStatus MPC_ACCEPTED mpc New ACCEPTED status due to MPC_ACCEPTED event for trafficgroup \u0026#34;bookinfo\u0026#34; version \u0026#34;oxil15u6bfw=\u0026#34; 2022/02/10 17:02:52 INFO api.tsb.tetrate.io/v2/ResourceStatus XCP_CENTRAL_ACCEPTED mpc New ACCEPTED status due to XCP_CENTRAL_ACCEPTED event for workspace \u0026#34;bookinfo\u0026#34; version \u0026#34;GBcgtWe3R80=\u0026#34; 2022/02/10 17:02:52 INFO api.tsb.tetrate.io/v2/ResourceStatus XCP_CENTRAL_REJECTED mpc New ACCEPTED status due to XCP_CENTRAL_ACCEPTED event for gatewaygroup \u0026#34;bookinfo\u0026#34; version \u0026#34;y6q054gFZCQ=\u0026#34; 2022/02/10 17:02:52 INFO api.tsb.tetrate.io/v2/ResourceStatus MPC_ACCEPTED mpc New ACCEPTED status due to MPC_ACCEPTED event for securitygroup \u0026#34;bookinfo\u0026#34; version \u0026#34;gEUA3cK7+YI=\u0026#34; 2022/02/10 17:02:52 INFO api.tsb.tetrate.io/v2/ResourceStatus MPC_ACCEPTED mpc New ACCEPTED status due to MPC_ACCEPTED event for workspace \u0026#34;bookinfo\u0026#34; version \u0026#34;GBcgtWe3R80=\u0026#34; 2022/02/10 17:02:52 INFO api.tsb.tetrate.io/v2/ResourceStatus MPC_ACCEPTED mpc New ACCEPTED status due to MPC_ACCEPTED event for gatewaygroup \u0026#34;bookinfo\u0026#34; version \u0026#34;y6q054gFZCQ=\u0026#34; 2022/02/10 17:02:48 INFO gateway.tsb.tetrate.io/v2/IngressGateway create admin Create IngressGateway \u0026#34;bookinfo\u0026#34; by \u0026#34;admin\u0026#34; 审核日志中标识了一些错误，你可以通过检索那些对象的配置状态的详细信息来进一步检查这些错误：\n$ tctl x status ig --workspace bookinfo --gatewaygroup bookinfo bookinfo NAME STATUS LAST EVENT MESSAGE bookinfo FAILED XCP_CENTRAL_REJECTED admission webhook \u0026#34;central-validation.xcp.tetrate.io\u0026#34; denied the request: configuration is invalid: domain name \u0026#34;tetrate.io---\u0026#34; invalid (label \u0026#34;io---\u0026#34; invalid) 正如你在命令输出中所看到的，配置已被 XCP 组件拒绝，并标记为无效，它将不会传播到目标集群。\n你还可以通过查询工作区的状态来获取见解。它将显示其子资源中的任何错误。通过这种方式，从任何工作区或顶级元素轻松导航到配置对象可能存在的特定错误非常容易。\n$ tctl x status ws bookinfo NAME STATUS LAST EVENT MESSAGE bookinfo FAILED The following children are failing: organizations/tetrate/tenants/tetrate/workspaces/bookinfo/gatewaygroups/bookinfo 或其扩展的 YAML 版本：\n$ tctl x status ws bookinfo -o yaml apiVersion: api.tsb.tetrate.io/v2 kind: ResourceStatus metadata: name: bookinfo organization: tetrate tenant: tetrate spec: aggregatedStatus: children: …","relpermalink":"/book/tsb/troubleshooting/configuration-status/","summary":"使用 tctl 了解 TSB 配置的部署状态。","title":"配置状态故障排除"},{"content":"本文档描述了如何配置 Argo CD 并将 Argo Rollout 与 TSB GitOps 支持集成，以及如何使用 SkyWalking 作为金丝雀部署分析和渐进式交付自动化的指标提供者。\n在开始之前，请确保以下事项：\nArgo CD 已安装在你的集群中，并且已配置 Argo CD CLI 以连接到你的 Argo CD 服务器 Argo Rollout 已安装在你的集群中 TSB 已启动并运行，并且已为目标集群启用了 GitOps 配置 从 Git 仓库创建应用程序 使用以下命令创建一个示例应用程序。一个包含 Istio 的示例仓库，其中包含 Istio 的 bookinfo 应用程序和 TSB 配置，可以在 https://github.com/tetrateio/tsb-gitops-demo 上找到。 你可以使用 Argo CD CLI 或其 Web UI 直接从 Git 导入应用程序配置。\nargocd app create bookinfo-app --repo https://github.com/tetrateio/tsb-gitops-demo.git --path application --dest-server https://kubernetes.default.svc --dest-namespace bookinfo --sync-policy automated --self-heal 检查应用程序的状态\nargocd app get bookinfo-app Name: bookinfo-app Project: default Server: https://kubernetes.default.svc Namespace: bookinfo URL: https://localhost:8080/applications/bookinfo-app Repo: https://github.com/tetrateio/tsb-gitops-demo.git Target: Path: argo/app SyncWindow: Sync Allowed Sync Policy: Automated Sync Status: Synced to (1ba8e2d) Health Status: Healthy GROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE Namespace bookinfo bookinfo Running Synced namespace/bookinfo created ServiceAccount bookinfo bookinfo-details Synced serviceaccount/bookinfo-details created ServiceAccount bookinfo bookinfo-productpage Synced serviceaccount/bookinfo-productpage created ServiceAccount bookinfo bookinfo-ratings Synced serviceaccount/bookinfo-ratings created ServiceAccount bookinfo bookinfo-reviews Synced serviceaccount/bookinfo-reviews created Service bookinfo productpage Synced Healthy service/productpage created Service bookinfo details Synced Healthy service/details created Service bookinfo ratings Synced Healthy service/ratings created Service bookinfo reviews Synced Healthy service/reviews created apps Deployment bookinfo ratings-v1 Synced Healthy deployment.apps/ratings-v1 created apps Deployment bookinfo productpage-v1 Synced Healthy deployment.apps/productpage-v1 created apps Deployment bookinfo reviews OutOfSync Healthy deployment.apps/reviews created apps Deployment bookinfo details-v1 Synced Healthy deployment.apps/details-v1 created Namespace bookinfo Synced 应用程序设置 如果你已经为部署和服务资源创建了 Kubernetes 清单，你可以选择保留相同的对象以及 Argo Rollout 对象，以便实现金丝雀部署。 你可以对 Rollout 对象和 Istio VirtualService/DestinationRule 的 TSB 网格配置进行必要的更改，以实现所需的结果。\nTSB 配置设置 由于 Argo Rollout 要求你根据其 Istio 的金丝雀部署策略约定对 Istio 的 VirtualService 和 DestinatrionRule 对象进行一些修改，你可以使用 TSB 的 DIRECT 模式配置来实现所需的结果。\n根据 Argo Rollout 的约定，需要在 TSB 直接模式资源（如 VirtualService 和 DestinationRule）中配置 2 个子集，分别命名为 stable 和 canary，并添加必要的标签，以标识 canary 和 stable 的 Pod。 请确保版本标签（例如：version: canary/stable）已根据 Istio 的约定进行配置，以便 TSB 可识别子集并在服务仪表板中绘制指标。 在使用 TSB 直接模式资源与 GitOps 时，需要为资源添加一个额外的标签 istio.io/rev: \u0026#34;tsb\u0026#34;。有关更多详细信息，请参阅此处 。 通过从 tsb-gitops-demo/argo/tsb/conf.yaml 导入 TSB 配置来创建一个名为 bookinfo-tsb-conf 的应用。你也可以选择将其保存在同一存储库中。\nargocd app create bookinfo-tsb-conf --repo https://github.com/tetrateio/tsb-gitops-demo.git --path argo/tsb --dest-server https://kubernetes.default.svc --dest-namespace bookinfo --sync-policy automated --self-heal 检查 TSB 资源的状态\nargocd app get bookinfo-tsb-conf 结果：\nName: bookinfo-tsb-conf Project: default Server: https://kubernetes.default.svc Namespace: bookinfo URL: https://localhost:8080/applications/bookinfo-tsb-conf Repo: https://github.com/tetrateio/tsb-gitops-demo.git Target: Path: argo/tsb SyncWindow: Sync Allowed Sync Policy: Automated Sync Status: Synced to (1ba8e2d) Health Status: Healthy GROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE networking.istio.io VirtualService bookinfo bookinfo Synced virtualservice.networking.istio.io/bookinfo created tsb.tetrate.io Tenant bookinfo bookinfo Synced tenant.tsb.tetrate.io/bookinfo unchanged networking.istio.io Gateway bookinfo bookinfo-gateway Synced gateway.networking.istio.io/bookinfo-gateway unchanged traffic.tsb.tetrate.io Group bookinfo bookinfo-traffic Synced group.traffic.tsb.tetrate.io/bookinfo-traffic unchanged security.tsb.tetrate.io Group bookinfo bookinfo-security Synced group.security.tsb.tetrate.io/bookinfo-security unchanged gateway.tsb.tetrate.io Group bookinfo bookinfo-gateway Synced group.gateway.tsb.tetrate.io/bookinfo-gateway unchanged tsb.tetrate.io Workspace bookinfo bookinfo-ws Synced workspace.tsb.tetrate.io/bookinfo-ws unchanged networking.istio.io VirtualService bookinfo details Synced virtualservice.networking.istio.io/details unchanged networking.istio.io DestinationRule bookinfo productpage Synced destinationrule.networking.istio.io/productpage unchanged networking.istio.io DestinationRule bookinfo details Synced destinationrule.networking.istio.io/details unchanged networking.istio.io VirtualService bookinfo ratings Synced virtualservice.networking.istio.io/ratings unchanged networking.istio.io DestinationRule bookinfo reviews Synced destinationrule.networking.istio.io/reviews unchanged networking.istio.io DestinationRule bookinfo ratings Synced destinationrule.networking.istio.io/ratings unchanged networking.istio.io VirtualService bookinfo reviews Synced …","relpermalink":"/book/tsb/howto/gitops/argo-rollouts/","summary":"如何使用 TSB GitOps 支持进行金丝雀部署分析和渐进式交付工作流，使用 Argo CD、Argo Rollout 和 SkyWalking 作为金丝雀部署分析和渐进式交付自动化的指标提供者。","title":"使用 Argo Rollout 和 SkyWalking 进行金丝雀分析和渐进式交付"},{"content":"本设计指南解释了如何配置一个跨多个云区域的多个工作负载集群的高可用部署。\n对于关键业务和大规模服务，可能需要在两个或多个区域部署，以实现可伸缩性和高可用性。本设计指南解释了如何使用 Tetrate 的 Edge Gateway 解决方案来实现这一目标。Edge Gateway 提供了一个高可用负载均衡代理的前端层，接收流量并将其转发到工作负载集群。\n介绍 Tetrate Edge Gateway 解决方案 管理员可以通过多种方式配置其生产环境，以在两个或多个工作负载集群之间分发流量。\n常见的高可用模式 对于简单的小规模部署，通常会看到几种模式：\n全局服务器负载均衡（GSLB）。使用 GSLB 系统，每个工作负载集群都会被分配一个公共端点（IP 地址），并使用 DNS 控制将用户定向到哪个端点。GSLB 系统对每个端点执行健康检查，并将失败的集群排除在轮换之外，它还可以执行其他负载均衡措施，如接近路由（将客户端发送到最近的集群）或基于负载的路由（将客户端发送到性能最佳的集群）。 GSLB 解决方案通常使用 DNS 控制客户端分配到数据中心的方式。基于 DNS 的控制的缺点是客户端和中间服务器会缓存 DNS 条目一段时间，因此故障转移或重新分配不是立即发生的，客户端可能会因此而停机。\n内容交付网络（CDN）。许多 CDN 能够在两个或多个“源”服务器之间平衡负载流量，并为基于 HTTP 的服务提供可靠的全球分布的前端。基于 CDN 的配置可以通过配置源服务器仅接受来自 CDN PoP（存在点）的流量来保护。 CDN 最适合可以缓存和全球分发的 Web 内容，其中管理员希望将负载降到最低并最小化页面加载时间。许多 CDN 的优化功能不适用于 API 或其他动态内容。\n基于云的负载均衡器。在使用单一供应商的云解决方案时，你可能希望考虑你的云提供商的解决方案，以实现多区域高可用性。这类解决方案通常基于 DNS 操作（GSLB）和类似 CDN 的代理（例如 AWS CloudFront），它们可能提供了你对业务关键服务所需的可靠性、安全性和操作便利性。\nTetrate Edge Gateway Tetrate 的 Edge Gateway 是部署在前端的代理，用于管理流量并将其分发到多个后端工作负载集群。它与 Tetrate 管理平面紧密集成，提供了一个有效且简单的用户体验，并可扩展到非常大量的集群、区域和流量级别。\n特别是 Edge Gateway 部署模式：\n支持公共和私有 IP 地址，必要时从公共到私有进行桥接。Edge Gateway 部署可以减少公共 IP 端点的数量，从而减少攻击面和可能的成本 整合了工作负载集群入口网关的功能，将其前置并接近客户端。在 Edge 上执行速率限制或授权可以减轻工作负载集群的负载 保护整个数据路径，从第一个 Edge Gateway 到目标服务，使用 Tetrate 的 mTLS 和隧道功能 可以进行优化，以减少故障转移时间并在故障转移情况下消除不必要的跳数 使 Edge Gateway 具有高可用性 Edge Gateway 模式是一个双层模式，其中 Edge Gateway 提供第一层负载均衡，位于工作负载集群中的入口网关的第二层。\nEdge Gateway 管理工作负载集群中的故障转移，你可以使用上述任何解决方案，例如基于 DNS 的 GSLB 解决方案，来管理（较不频繁的）边缘故障转移。\n在本设计指南中 本设计指南解释了如何从多个区域提供关键服务，配置优化了高可用性和可伸缩性。初始设计的解决方案简要如下：\n两个单独的云区域，可能跨越多个不同的云提供商 一个接收互联网流量的“Edge Gateway” 两个“工作负载集群”，一个位于每个区域中，托管命名服务 一个单一的命名服务，通过单一的 DNS 名称访问 一个第三方 DNS 服务，将流量分发到 Edge Gateway 请从入门文档 开始创建此示例部署。\n演示环境\n工作负载集群故障转移\n扩展演示环境\n边缘网关故障转移\n运维和测试高可用性与故障转移\n","relpermalink":"/book/tsb/design-guides/ha-multicluster/","summary":"本设计指南解释了如何配置一个跨多个云区域的多个工作负载集群的高可用部署。","title":"使用 Edge Gateway 实现高可用集群"},{"content":"本文档解释了如何利用 Helm Charts 来安装 TSB 的不同组件。\nTSB Helm Chart\n管理平面安装\n控制平面安装\n数据平面安装\nTSB Helm 升级\nHelm TSB 卸载\n","relpermalink":"/book/tsb/setup/helm/","summary":"本文档解释了如何利用 Helm Charts 来安装 TSB 的不同组件。 TSB Helm Chart 管理平面安装 控制平面安装 数据平面安装 TSB Helm 升级 Helm TSB 卸载","title":"使用 Helm 安装"},{"content":" 裸机服务器 在本指南中，我们仅提及虚拟机（VM）。如果你想将运行在裸机服务器上的工作负载接入到 TSB 服务网格中，只需将 “VM” 替换为 “裸机” 即可。在处理它们时没有任何区别。 问题定义 Istio 和底层的 Kubernetes 平台共同构建了一个封闭的生态系统，控制平面和数据平面组件紧密集成。例如，运行在每个节点上的控制平面组件创建了相互信任的关系。当新的 Pod 被调度在一个节点上运行时，该节点是一个受信任的实体，其关键资源，如 iptables，会被修改。\n当一个虚拟机（VM）被引入这个生态系统时，它是一个外部实体。要成功地将一个 Istio/Kubernetes 集群扩展到一个 VM 上，必须执行以下步骤：\n认证。VM 必须与控制平面建立一个经过认证的加密会话，证明它被允许载入集群。 路由。VM 必须知道在 Kubernetes 集群中定义的服务，反之亦然。如果 VM 运行一个服务，它必须对在集群内运行的 Pod 可见。 概述 将虚拟机（VM）接入 TSB 管理的 Istio 服务网格可以分为以下步骤：\n使用 Istio 控制平面注册 VM 工作负载（WorkloadEntry） 获取用于在 VM 上运行的 Istio 代理的引导安全令牌和种子配置 将引导安全令牌和种子配置传输到 VM 在 VM 上启动 Istio 代理 为了改善 VM 接入的用户体验，TSB 提供了 tctl CLI，它可以自动化大部分这些任务。\n在高层次上，tctl 旨在将 VM 接入流程简化为一个单一的命令：\ntctl x sidecar-bootstrap tctl Sidecar 引导逻辑以及将 VM 工作负载注册到服务网格的操作由 WorkloadEntry 资源内部的配置驱动。tctl Sidecar 引导允许你将 VM 接入到 Kubernetes 上的服务网格中，从而实现了对各种网络和部署方案的支持。tctl Sidecar 引导还允许在任何情况下、从任何上下文中（开发者机器或 CI/CD 流水线）重现 VM 接入。\n要求 在开始之前，请确保你具备以下条件：\nTSB 版本 0.9 或以上 一个已接入 TSB 的 Kubernetes 集群 在适当的命名空间中部署了相关应用程序 一个已启动并准备好的虚拟机 最新的 kubectl 已准备就绪 最新的 Tetrate Service Bridge CLI (tctl) 已准备就绪 不同环境之间的差异 此设置指南提供了将 VM 接入的通用步骤。由于你必须处理特定的云提供商、网络、防火墙、工作负载和操作系统的组合，你需要调整这些步骤以使其适用于你的情况。在本指南中，我们将以在 Ubuntu VM 上接入 Istio Bookinfo 示例中的 Ratings 服务为例。 操作步骤 集群网格扩展 为了允许来自 Kubernetes 环境之外的工作负载成为服务网格的一部分，你需要在集群中启用 mesh expansion。编辑 ControlPlane CR 或 Helm 值以包含 meshExpansion 属性，如下所示。\nspec: meshExpansion: {} 要编辑资源，请运行以下命令：\nkubectl patch ControlPlane controlplane -n istio-system \\ --patch \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;meshExpansion\u0026#34;:{}}}\u0026#39; \\ --type merge 完成此步骤后，你可以将尽可能多的 VM 工作负载接入到该集群中。如果你有多个集群，请为需要接入 VM 的每个集群重复此步骤。\nVM 准备工作 要为接入做好 VM 的准备工作，你需要对 VM 具有 SSH 访问权限，因为你必须添加一个用户帐户并安装额外的软件。\n示例环境 本指南将以准备一个 Ubuntu 18.04 LTS 虚拟机为例。 首先，请确保 VM 上已安装 Docker。稍后你将安装 Istio 代理，它将在 Docker 容器中运行。使用 Docker 可以使你将代理的依赖项与操作系统安装隔离开来，同时为代理提供一个均匀的运行环境。\n要在 VM 上安装 Docker，请运行：\nsudo apt-get update sudo apt-get -y install docker.io 要允许 tctl 接入你的 VM 工作负载，请创建并配置一个专用用户帐户。此用户帐户将需要权限与 Docker 守护程序进行交互，并具有 SSH 访问权限。为了启动\n接入流程，tctl 工具将使用 SSH 连接到你的 VM。\n要设置和配置用户帐户，请运行以下命令：\n# 为 VM 接入创建专用用户帐户 \u0026#34;istio-proxy\u0026#34; sudo useradd --create-home istio-proxy # 切换到专用用户 sudo su - istio-proxy # 为新用户帐户配置 SSH 访问 mkdir -p $HOME/.ssh chmod 700 $HOME/.ssh touch $HOME/.ssh/authorized_keys chmod 600 $HOME/.ssh/authorized_keys # # 将你的 SSH 公钥添加到 $HOME/.ssh/authorized_keys 中 # # 返回特权用户 exit 要使新用户帐户具有与 Docker 守护程序交互的权限，你必须将该帐户添加到 docker 用户组中：\nsudo usermod -aG docker istio-proxy 为了存储接入配置，你必须设置一个目录。如果你希望使用不同的路径，请确保它反映在稍后将配置的 WorkloadEntry 资源中。\nsudo mkdir -p /etc/istio-proxy sudo chmod 775 /etc/istio-proxy sudo chown istio-proxy:istio-proxy /etc/istio-proxy 如果你的工作负载尚未运行，请立即启动它。在我们的示例中，我们将运行 Istio Bookinfo 示例中的 Ratings 服务。此示例将在 Docker 中运行，但并非必须如此。你的工作负载可以作为操作系统的常规进程运行。\nsudo docker run -d \\ --name ratings \\ -p 127.0.0.1:9080:9080 \\ docker.io/istio/examples-bookinfo-ratings-v1:1.16.2 配置防火墙 要允许 VM 加入服务网格，必须在 VM 和 Kubernetes 集群网络端之间建立 IP（L3）连接。你可能需要在 Kubernetes 和 VM 网络端配置防火墙，以允许在各种用于流量的 TCP 端口之间进行通信。\nKubernetes 和 VM 在同一网络上（或互通的网络） 由于所有工作负载具有直接的 IP 连通性，因此 VM IP 与 Pod IP 之间的流量不会使用 VM 网关。\n在此情况下，你必须：\n允许从 VM IP 到 Pod IPs 的整个 TCP 端口范围的入站流量 允许从 Pod IPs 到 VM IP 的一组相关的 TCP 端口的入站流量 Kubernetes 和 VM 在不同的网络中 当跨越 Kubernetes 和 VM 的工作负载没有直接的 IP 连通性时，流量必须通过 VM 网关传递。\n在这种网络隔离的情况下，Kubernetes 中 VM 网关的以下 TCP 端口必须对 VM 可用：\n15012（控制平面 xDS 流量） 15443（数据平面入站流量） 9411（Sidecar 跟踪数据入站） 11800（Sidecar 访问日志入站） GKE 和 EKS 在 GKE 和 EKS 上，这些端口将自动允许传入流量。 你需要打开用于从 Kubernetes 集群到 VM 工作负载的流量的端口取决于你将在 Sidecar 资源中配置的代理侦听端口。在本例中，我们使用端口 9080。在这种情况下，我们需要允许从 Kubernetes 到 VM 的 TCP 流量使用端口 9080。\n创建 WorkloadEntry WorkloadEntry 资源记录了运行在 VM 上的工作负载的信息，这将允许你使用可验证的身份正确地将 VM 接入到 TSB 服务网格中。\nWorkloadEntry 的配置涉及以下信息：\n服务网格需要注册 VM 工作负载所需的详细信息 为 tctl 提供了正确信息以启动 VM 接入的注释 用于 TSB 可观测性的标签，它们持有工作负载的逻辑服务身份 下面是一个 WorkloadEntry 资源的模板示例，突出显示了根据你的环境的具体情况必须配置的属性。\napiVersion: networking.istio.io/v1beta1 kind: WorkloadEntry metadata: name: ratings-vm namespace: bookinfo annotations: sidecar-bootstrap.istio.io/ssh-host: \u0026lt;ssh-host\u0026gt; sidecar-bootstrap.istio.io/ssh-user: istio-proxy sidecar-bootstrap.istio.io/proxy-config-dir: /etc/istio-proxy sidecar-bootstrap.istio.io/proxy-image-hub: docker.io/tetrate sidecar-bootstrap.istio.io/proxy-instance-ip: \u0026lt;proxy-instance-ip\u0026gt; spec: address: \u0026lt;address\u0026gt; labels: class: vm app: ratings # 可观测性标签，通过 TSB 可观测性可见 version: v3 # 可观测性标签，通过 TSB 可观测性可见 serviceAccount: bookinfo-ratings network: \u0026lt;vm-network-name\u0026gt; network: \u0026lt;vm-network-name\u0026gt; 在你的 Kubernetes 集群中的服务网格需要知道你的 VM 是否位于可以直接到达 Pod IPs 的网络中。正如防火墙部分所述，这将决定流量是否应该通过 VM 网\n关路由。通过添加 network 属性并提供 VM 网络的名称，服务网格将启用 VM 网关路由。如果你省略网络属性，则服务网格将假定 VM 在具有直接 IP 连通性的网络上运行。\naddress: \u0026lt;address\u0026gt; Address 必须保存可以被 Pod 直接连接到的 VM 工作负载的目标 IP。在相同网络场景中，这是一个 pod 可以直接连接到的 VM IP 地址。在网络隔离的情况下，这是 pod 可以到达的 VM IP 地址。例如，如果你对 Kubernetes 和 VM 使用了不同的 VPC，则这可以是一个私有 IP 地址，只要 VPC 路由/对等设置正确即可。在不同云提供商的情况下，这通常是 VM 可以访问的公共 IP 地址。\nproxy-instance-ip: \u0026lt;proxy-instance-ip\u0026gt; 如果提供了此注释，它必须保存 Istio Proxy sidecar 在 VM 上可以将其侦听器绑定到的 IP 地址。通常，这是从外部接收传入流量的接口的 IP 地址。如果 VM 有一个配置了公共 IP 地址的接口，并且这与 address 属性相同，那么可以省略此注释。大多数云提供商的 VM 不会在直接侦听公共 IP 地址的接口上，而是在私有 IP 上。在这种情况下，你必须配置 VM 的内部 IP 地址，以便将外部传入流量路由到该地址。\nssh-host: \u0026lt;ssh-host\u0026gt; 当你执行 tctl bootstrap 命令时，tctl 尝试连接到需要接入的 VM。默认行为 …","relpermalink":"/book/tsb/setup/workload-onboarding/onboarding-vms/","summary":"将虚拟机或裸机接入 TSB 服务网格的指南。","title":"使用 tctl 将虚拟机（VM）接入 TSB 服务网格"},{"content":"TSB 支持指定用于保护与外部授权服务器通信的TLS 或 mTLS 参数。本文将向你展示如何通过将 CA 证书添加到授权配置来为外部授权服务器配置 TLS 验证。\n在开始之前，请确保你：\n熟悉TSB 概念 安装了 TSB 环境。你可以使用TSB 演示 进行快速安装 完成了TSB 用法快速入门 。本文假设你已经创建了租户并熟悉工作空间和配置组。还需要将 tctl 配置到你的 TSB 环境中。 本文中的示例将建立在“在 Ingress Gateways 中配置外部授权” 之上。在继续之前，请确保已完成该文档，并注意你将在命名空间 httpbin 上工作。\n创建 TLS 证书 为了使 Ingress Gateway 到授权服务的流量启用 TLS，你必须拥有 TLS 证书。本文假设你已经有 TLS 证书，通常包括服务器证书和私钥，以及用于客户端的根证书作为 CA。本文使用以下文件：\nauthz.crt 作为服务器证书 authz.key 作为证书私钥 authz-ca.crt 作为 CA 证书 如果你决定使用其他文件名，请在下面的示例中相应地替换它们。\n自签名证书 出于示例目的，你可以使用此脚本 创建自签名证书。 拥有文件后，使用服务器证书和私钥创建 Kubernetes secret。\nkubectl create secret tls -n httpbin opa-certs \\ --cert=authz.crt \\ --key=authz.key 你还需要 CA 证书来验证 TLS 连接。 创建一个名为 authz-ca 的 ConfigMap，其中包含 CA 证书：\nkubectl create configmap -n httpbin authz-ca \\ --from-file=authz-ca.crt 使用 TLS 证书部署授权服务 按照“在 TSB 中安装 Open Policy Agent” 中的说明设置带有终止 TLS 的 Sidecar 代理的 OPA 实例。\n修改 Ingress Gateway 你需要向 Ingress Gateway 添加 CA 证书以验证 TLS 连接。 创建一个名为 httpbing-ingress-gateway.yaml 的文件，其中包含以下内容。此清单添加了覆盖以读取包含 CA 证书的 authz-ca 的 ConfigMap 到 Ingress Gateway 部署。\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: httpbin-ingress-gateway namespace: httpbin spec: kubeSpec: service: type: LoadBalancer overlays: - apiVersion: apps/v1 kind: Deployment name: httpbin-ingress-gateway patches: - path: spec.template.spec.volumes[-1] value: name: authz-ca configMap: name: authz-ca - path: spec.template.spec.containers.[name:istio-proxy].volumeMounts[-1] value: name: authz-ca mountPath: /etc/certs readOnly: true 使用 kubectl 应用：\nkubectl apply -f httpbin-ingress-gateway.yaml 然后更新 Ingress Gateway 配置以启用 TLS 验证：\napiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: organization: tetrate name: httpbin-ingress-gateway group: httpbin workspace: httpbin tenant: tetrate spec: workloadSelector: namespace: httpbin labels: app: httpbin-ingress-gateway http: - name: httpbin port: 443 hostname: \u0026#34;httpbin.tetrate.com\u0026#34; tls: mode: SIMPLE secretName: httpbin-certs routing: rules: - route: host: \u0026#34;httpbin/httpbin.httpbin.svc.cluster.local\u0026#34; port: 8080 authorization: external: tls: mode: SIMPLE files: caCertificates: /etc/certs/authz-ca.crt uri: grpcs://opa.opa.svc.cluster.local:18443 使用 tctl 应用：\ntctl apply -f ext-authz-ingress-gateway-tls.yaml 测试 你可以使用与“在 Ingress Gateways 中配置外部授权” 中显示的相同测试步骤。\n","relpermalink":"/book/tsb/howto/authorization/tls-verification/","summary":"安全地在 TSB 和外部授权服务之间保护流量。","title":"使用 TLS 验证的外部授权"},{"content":"本页介绍如何利用 TSB Operator 来管理数据平面的网关配置。\nTSB Operator 配置为监督数据平面网关组件的生命周期，主动监控所有命名空间中的 IngressGateway 、 Tier1Gateway 和 EgressGateway 自定义资源 (CR) 集群。默认情况下，数据平面网关组件驻留在 istio-gateway 命名空间中。你可以在数据平面安装 API 参考文档中找到有关自定义资源 API 的全面详细信息。\n数据平面 Operator 监视其创建的 Kubernetes 资源。每当它检测到监视事件（例如删除部署）时，它都会启动协调以将系统恢复到所需状态，从而有效地重新创建任何已删除的部署。\n控制平面要求 为了让 TSB Operator 管理数据平面网关组件，同一集群中必须存在功能齐全的控制平面。这就需要有一个有效的 TSB Operator 来管理控制平面，以及有效的 ControlPlane 自定义资源 (CR)。 组件 以下是你可以使用数据平面 Operator 配置和管理的自定义组件类型：\n组件 Service Deployment istio istio-operator-metrics（用户在应用程序命名空间中配置的 istio 代理服务） istio-operator（由用户在应用程序命名空间中配置的 istio 代理部署） 在其专用命名空间中，TSB Operator 生成名为 tsb-gateways 的 IstioOperator 自定义资源 (CR)，并继续部署 Istio Operator。\n默认情况下，生成的 IstioOperator CR 启用了 ingressGateway 和 egressGateway 组件。所有其他 Istio 组件在 CR 中都被明确禁用。这种特殊的配置将网关升级的生命周期与控制平面升级分离。\n当用户在集群内的各个命名空间中创建和部署 IngressGateway 、 Tier1Gateway 和 EgressGateway 自定义资源 (CR) 时，TSB Operator 将转换这些资源并更新数据平面网关组件的命名空间中名为 tsb-gateways 的 IstioOperator CR。然后，部署在此命名空间中的 Istio Operator 将代表 TSB Operator 管理入口和出口 Envoy 网关的生命周期。这些 Envoy 网关对于处理 TSB 服务网格中托管的服务的入口和出口至关重要。\n","relpermalink":"/book/tsb/concepts/operators/data-plane/","summary":"TSB Operator 和数据平面网关生命周期。","title":"数据平面"},{"content":"在继续之前，请确保你熟悉 Istio 隔离边界 功能。\n升级方法 尽管默认情况下首选并假定使用原地网关升级，但你可以通过在 ControlPlane CR 或 Helm 值文件中设置 ENABLE_INPLACE_GATEWAY_UPGRADE 变量来控制以版本为基础的网关升级的两种方式。\nENABLE_INPLACE_GATEWAY_UPGRADE=true 是默认行为。在使用原地网关升级时，现有的网关部署将使用新的代理镜像进行修补，并将继续使用相同的网关服务。这意味着你无需进行任何更改以配置网关的外部 IP。 ENABLE_INPLACE_GATEWAY_UPGRADE=false 意味着将创建一个新的网关服务和部署以进行金丝雀版本的升级，因此现在可能会有两个服务： \u0026lt;网关名称\u0026gt;/\u0026lt;网关名称\u0026gt;-old，负责处理非修订版/旧版本控制平面工作负载流量。 \u0026lt;网关名称\u0026gt;-1-6-0，负责处理版本控制平面工作负载流量，将为此新创建的 \u0026lt;网关名称\u0026gt;-canary 服务分配新的外部 IP。 你可以通过使用外部负载均衡器或更新 DNS 条目来控制两个版本之间的流量。\n由于原地网关升级是默认行为，你无需更改现有的 ControlPlane CR。要使用金丝雀网关升级，你需要在以下 xcp 组件中将 ENABLE_INPLACE_GATEWAY_UPGRADE 设置为 false：\nspec: ... components: xcp: kubeSpec: deployment: env: - name: ENABLE_INPLACE_GATEWAY_UPGRADE value: \u0026#34;false\u0026#34; # 禁用原地升级以创建网关的金丝雀部署和服务 isolationBoundaries: - name: global revisions: - name: 1-6-0 网关升级是通过更新 Ingress/Egress/Tier1Gateway 资源中的 spec.revision 字段来触发的。 如果 ENABLE_INPLACE_GATEWAY_UPGRADE=false，请注意，将会有另一组新版本的 Service/Deployment/其他对象，我们正在对其进行升级。 网关升级是通过更新 Ingress/Egress/Tier1Gateway 资源中的 spec.revision 字段来触发的。\nkubectl get deployments -n bookinfo # 输出 tsb-gateway-bookinfo 1/1 1 1 8m12s tsb-gateway-bookinfo-1-6-0 1/1 1 1 4m19s kubectl get svc -n bookinfo # 输出 tsb-gateway-bookinfo LoadBalancer 10.255.10.81 172.29.255.151 15443:31159/TCP,8080:31789/TCP,... tsb-gateway-bookinfo-1-6-0 LoadBalancer 10.255.10.85 172.29.255.152 15443:31159/TCP,8080:31789/TCP,... ","relpermalink":"/book/tsb/setup/upgrades/gateway-upgrade/","summary":"如何使用多个版本升级网关。","title":"网关升级"},{"content":" 遥测架构\nSidecar RED 指标\n关键指标\n警报指南\n分布式跟踪集成\nNew Relic 集成\n","relpermalink":"/book/tsb/operations/telemetry/","summary":"遥测架构 Sidecar RED 指标 关键指标 警报指南 分布式跟踪集成 New Relic 集成","title":"遥测和审计"},{"content":"本页介绍如何将 Kubernetes 集群加入现有的 Tetrate Service Bridge（TSB）管理平面。\n在开始之前，请确保你已经完成以下操作：\n检查 要求 安装 TSB 管理平面 或 演示安装 使用 tctl 登录管理平面（tctl 连接 ） 检查 TSB 控制平面组件 隔离边界 TSB 1.6 引入了隔离边界，允许你在 Kubernetes 集群内或跨多个集群中拥有多个 TSB 管理的 Istio 环境。隔离边界的一个好处是你可以执行控制平面的金丝雀升级。\n要启用隔离边界，你必须使用环境变量 ISTIO_ISOLATION_BOUNDARIES=true 更新 Operator 部署，并在控制平面 CR 中包含 isolationBoundaries 字段。 有关更多信息，请参阅 隔离边界 。\n创建集群对象 要为集群创建正确的凭据，以便与管理平面通信，你需要使用管理平面 API 创建一个集群对象。\n根据你的需求调整以下 yaml 对象，并保存到名为 new-cluster.yaml 的文件中。\napiVersion: api.tsb.tetrate.io/v2 kind: Cluster metadata: name: \u0026lt;cluster-name-in-tsb\u0026gt; organization: \u0026lt;organization-name\u0026gt; spec: {} TSB 中的集群名称 \u0026lt;cluster-name-in-tsb\u0026gt; 是 TSB 中你集群的指定名称。你将在 TSB API 中使用此名称，例如在工作区和配置组中的命名空间选择器以及创建以下的 ControlPlane CR 时使用。此名称必须唯一。 有关可配置字段的详细信息，请参阅 Cluster 对象的参考文档。\n要在管理平面上创建集群对象，请使用 tctl 来应用包含集群详细信息的 yaml 文件。\ntctl apply -f new-cluster.yaml 部署 Operator 接下来，你需要在集群中安装必要的组件，以载入集群并将其连接到管理平面。\n你必须部署两个 Operator。首先是控制平面 Operator，负责管理 Istio、SkyWalking 和其他各种组件。其次是数据平面 Operator，负责管理网关。\ntctl install manifest cluster-operators \\ --registry \u0026lt;registry-location\u0026gt; \u0026gt; clusteroperators.yaml 标准\ninstall manifest cluster-operators 命令将输出所需 Operator 的 Kubernetes 清单。然后，我们可以将其添加到源代码控制或应用到集群中：\nkubectl apply -f clusteroperators.yaml OpenShift\ninstall manifest cluster-operators 命令将输出所需 Operator 的 Kubernetes 清单。然后，我们可以将其添加到源代码控制或应用到集群中：\noc apply -f clusteroperators.yaml 注意 对于下面的配置密钥和控制平面安装步骤，你必须为每个集群单独创建密钥和自定义资源的 YAML 文件。 换句话说，对每个集群重复这些步骤，并确保传递上面设置的 \u0026lt;cluster-name-in-tsb\u0026gt; 值，然后将两个 YAML 文件应用到正确的集群。 配置密钥 控制平面需要密钥以便与管理平面进行身份验证。这些包括服务帐户密钥、Elasticsearch 凭据，以及如果使用自签名证书进行管理平面、XCP 或 Elasticsearch 的 CA 捆绑包。以下是你需要创建的一些密钥的列表。\n密钥名称 描述 elastic-credentials Elasticsearch 用户名和密码。 es-certs 当 Elasticsearch 配置为呈现自签名证书时，用于验证 Elasticsearch 连接的 CA 证书。 redis-credentials 包含：\n1. Redis 密码。\n2. 使用 TLS 的标志。\n3. 当 Postgres 配置为呈现自签名证书时，用于验证 Redis 连接的 CA 证书。\n4. 如果 Redis 配置了互联 TLS，则包含客户端证书和私钥。 xcp-central-ca-bundle 用于验证由 XCP Central 呈现的证书的 CA 捆绑包。 mp-certs 用于验证管理平面 API 的 TSB 管理平面证书，如果管理平面配置为呈现自签名证书，则为 CA 证书。这是用于签署前端 envoy TLS 证书的 CA。 cluster-service-account 用于集群到控制平面之间的身份验证的服务帐户密钥。 central-cert 用于签署前端 envoy TLS 证书的 CA 证书。 edgectl-cluster 包含：\n1. 前端 envoy 的 TLS 证书和密钥。\n2. 用于验证和管理各种组件连接的 CA 证书。 请按照以下步骤为集群创建密钥：\n使用以下命令创建密钥文件，如 cluster-secrets.yaml： tctl install secrets cluster-secrets \u0026gt; cluster-secrets.yaml 使用文本编辑器打开 cluster-secrets.yaml 文件，将以下字段替换为集群的实际值： cluster_name: 替换为集群名称。 tsb_operator_namespace: 替换为 Istio Operator 的命名空间。 tsb_control_plane_namespace: 替换为 TSB 控制平面的命名空间。 elastic_username 和 elastic_password: 替换为 Elasticsearch 的用户名和密码。 redis_password: 替换为 Redis 的密码。 xcp_central_ca_bundle: 如果使用自签名证书，将其替换为 XCP Central 的 CA 捆绑包。 mp_certs_ca_bundle: 如果管理平面使用自签名证书，将其替换为管理平面的 CA 捆绑包。 以下是示例 cluster-secrets.yaml 文件的一部分，其中包含需要替换的字段示例：\napiVersion: api.tsb.tetrate.io/v2 kind: TSB metadata: name: cluster-secrets organization: \u0026lt;organization-name\u0026gt; spec: cluster_name: \u0026lt;cluster-name-in-tsb\u0026gt; tsb_operator_namespace: tsb-operator tsb_control_plane_namespace: tetrate-system secrets: elastic_username: \u0026lt;elasticsearch-username\u0026gt; elastic_password: \u0026lt;elasticsearch-password\u0026gt; redis_password: \u0026lt;redis-password\u0026gt; xcp_central_ca_bundle: \u0026lt;xcp-central-ca-bundle\u0026gt; mp_certs_ca_bundle: \u0026lt;mp-certs-ca-bundle\u0026gt; 使用以下命令应用密钥文件： kubectl apply -f cluster-secrets.yaml 在完成上述步骤后，集群应配置好密钥并准备好连接到管理平面。 安装控制平面 现在，你可以使用 tctl 来安装控制平面。请将以下命令中的 \u0026lt;cluster-name-in-tsb\u0026gt; 替换为集群的实际名称：\ntctl install control-plane \\ --cluster-name \u0026lt;cluster-name-in-tsb\u0026gt; \\ --registry \u0026lt;registry-location\u0026gt; \u0026gt; controlplane.yaml 接下来，使用以下命令将控制平面清单应用到集群中：\nkubectl apply -f controlplane.yaml 这将在集群中安装 Istio 和相关组件，并将其连接到管理平面。完成后，你的 Kubernetes 集群将成功加入 Tetrate Service Bridge 环境。\n","relpermalink":"/book/tsb/setup/self-managed/onboarding-clusters/","summary":"在 Kubernetes 上部署 Istio 控制平面并将其连接到 TSB。","title":"载入集群"},{"content":"在继续之前，请确保你已经完成了 设置工作负载载入文档 中描述的步骤。\n载入 VM 创建工作负载载入代理配置 默认情况下，工作负载载入代理期望将其配置指定在一个名为 /etc/onboarding-agent/onboarding.config.yaml 的文件中。\n创建文件 /etc/onboarding-agent/onboarding.config.yaml，并使用以下内容替换。 将 onboarding-endpoint-dns-name 替换为要连接的工作负载载入端点，以及将 workload-group-namespace 和 workload-group-name 替换为要加入的 Istio WorkloadGroup 的命名空间和名称。\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: \u0026lt;onboarding-endpoint-dns-name\u0026gt; workloadGroup: namespace: \u0026lt;workload-group-namespace\u0026gt; name: \u0026lt;workload-group-name\u0026gt; 工作负载载入端点假定在 https://\u0026lt;onboarding-endpoint-dns-name\u0026gt;:15443 处可用，并且它使用为适当的 DNS 名称颁发的 TLS 证书。证书应由 VM 信任的 CA 签名。有关更多配置选项，请参阅 OnboardingConfiguration 文档。\n启动工作负载载入代理 要启动 工作负载载入代理，运行：\nsudo systemctl enable onboarding-agent sudo systemctl start onboarding-agent 如果一切配置正确，你的 VM 现在应该已经成功加入到 mesh 中。\n从 VM 自动扩展组载入工作负载 一旦在自动扩展组的 VM 上安装了 Workload 载入代理，将以下 用户数据 传递到 VM 实例。 将 onboarding-endpoint-dns-name 替换为要连接的工作负载载入端点，以及将 workload-group-namespace 和 workload-group-name 替换为要加入的 Istio WorkloadGroup 的命名空间和名称。\n#cloud-config # 为 `Workload 载入代理` 提供 `OnboardingConfiguration` write_files: - content: | apiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: \u0026lt;onboarding-endpoint-dns-name\u0026gt; workloadGroup: namespace: \u0026lt;workload-group-namespace\u0026gt; name: \u0026lt;workload-group-name\u0026gt; path: /etc/onboarding-agent/onboarding.config.yaml owner: root:root permissions: \u0026#39;0644\u0026#39; # 启动 `Workload 载入代理` runcmd: - sudo systemctl enable onboarding-agent - sudo systemctl start onboarding-agent 上述 cloud-init 配置提供了 Workload 载入代理的配置文件，并在 VM 启动流程的一部分启动了 Workload 载入代理。\n如果一切配置正确，你的 VM 应该会自动加入到 mesh 中。\n","relpermalink":"/book/tsb/setup/workload-onboarding/guides/onboarding/","summary":"本文档描述了使用工作负载载入功能将 VM 载入到 TSB 的步骤。","title":"载入虚拟机"},{"content":"TSB 支持为 TSB 组件进行自动证书管理。你可以启用 TSB 以创建自签名根 CA，用于签发证书，例如 TSB 管理平面的 TLS 证书，用于控制平面与管理平面之间的通信的 内部证书 ，以及应用程序集群的中间 CA 证书，Istio 在集群中将使用它们来签发应用程序工作负载的证书。\n外部根 CA 目前，TSB 的自动证书管理不支持使用外部根 CA。将来的版本将添加对外部根 CA 的支持。 启用自动证书管理 要启用自动证书管理，你需要在 TSB 管理平面 CR 或 helm values 中设置 certIssuer 字段：\nspec: certIssuer: selfSigned: {} tsbCerts: {} clusterIntermediateCAs: {} certIssuer 字段是一个你要启用的证书颁发者的映射。目前，TSB 支持以下颁发者：\nselfSigned：这将创建一个自签名的根 CA，用于签发 TSB 组件的证书。 tsbCerts：这将为 TSB 端点提供 TSB TLS 证书，还将提供 TSB 内部证书。 clusterIntermediateCAs：这将为应用程序集群提供中间 CA 证书，Istio 将在集群中使用它们来签发应用程序工作负载的证书。 要启用自动集群中间 CA 证书管理，还需要在 TSB 控制平面 CR 或 helm values 中设置 centralProvidedCaCert 字段：\nspec: ... components: xcp: ... centralProvidedCaCert: true 使用外部证书管理 如果要使用外部证书提供程序，你需要从 TSB 管理平面 CR 或 helm values 中的 certIssuer 字段中删除相关的颁发者以避免冲突。例如：\n要使用 Let’s Encrypt 提供 TSB TLS 证书，请从 certIssuer 字段中删除 tsbCerts。请注意，如果禁用此选项，还需要提供 TSB 内部证书 。 要使用 AWS PCA 提供集群中间 CA，请从 certIssuer 字段中删除 clusterIntermediateCAs，并在 TSB 控制平面 CR 或 helm values 中将 centralProvidedCaCert 设置为 false。 如果计划同时为 tsbCerts 和 clusterIntermediateCAs 使用外部证书管理，则可以从 TSB 管理平面 CR 或 helm values 中删除 certIssuer 字段。\n证书轮换 TSB 将自动轮换 TSB 组件和应用程序集群的证书。集群中间 CA 证书每年轮换一次。TSB TLS 和内部证书每 90 天轮换一次。目前，TSB 不提供配置轮换周期的方法。\n","relpermalink":"/book/tsb/setup/certificate/automated-certificate-management/","summary":"描述如何为 TSB 使用自动证书管理","title":"自动证书管理"},{"content":"假设你熟悉 Git、Docker、Kubernetes、持续交付和 GitOps 的核心概念。以下是 Argo CD 特有的一些概念：\n应用程序由清单定义的一组 Kubernetes 资源。这是自定义资源定义 (CRD)。 应用程序源类型使用哪个工具来构建应用程序。 目标状态应用程序的所需状态，由 Git 存储库中的文件表示。 实时状态该应用程序的实时状态。部署了哪些 pod 等。 同步状态实时状态是否与目标状态匹配。部署的应用程序是否与 Git 所说的一样？ 同步使应用程序移动到其目标状态的过程。例如，通过将更改应用到 Kubernetes 集群。 同步操作状态同步是否成功。 刷新将 Git 中的最新代码与实时状态进行比较。弄清楚有什么不同。 健康应用程序的健康状况，是否正常运行？它可以满足请求吗？ 工具从文件目录创建清单的工具。例如定制。请参阅应用程序源类型。 配置管理工具请参阅工具。 配置管理插件自定义工具。 ","relpermalink":"/book/argo-cd/core-concepts/","summary":"假设你熟悉 Git、Docker、Kubernetes、持续交付和 GitOps 的核心概念。以下是 Argo CD 特有的一些概念： 应用程序由清单定义的一组 Kubernetes 资源。这是自定义资源定义 (CRD)。 应用程序源类型使用哪个工具来构建应","title":"核心概念"},{"content":"使用生成器的概念，应用集控制器提供了一组强大的工具，用于自动化模板化和修改 Argo CD 应用程序。生成器从各种来源（包括 Argo CD 集群和 Git 存储库）生成模板参数数据，支持和启用新的用例。\n虽然可以将这些工具用于任何目的，但这里是应用集控制器旨在支持的一些特定用例。\n用例：集群附加组件 应用集控制器的初始设计重点是允许基础架构团队的 Kubernetes 集群管理员自动创建大量不同的 Argo CD 应用程序，跨多个集群，并将这些应用程序作为单个单元进行管理。 集群附加组件用例 就是其中一个例子。\n在 集群附加组件用例 中，管理员负责为一个或多个 Kubernetes 集群配置集群附加组件：集群附加组件是 Operator，例如 Prometheus Operator 或控制器，例如 argo-workflows 控制器 （Argo 生态系统 的一部分）。\n通常，这些附加组件是开发团队的应用程序所需的（例如作为多租户集群的租户，他们可能希望向 Prometheus 提供度量数据或通过 Argo Workflows 编排工作流程）。\n由于安装这些插件需要集群级别的权限，而这些权限不被单个开发团队持有，因此安装是组织的基础架构/运营团队的责任，在大型组织内，这个团队可能负责数十、数百或数千个 Kubernetes 集群（新集群定期添加/修改/删除）。\n需要在大量集群上扩展，并自动响应新集群的生命周期，这必然需要某种形式的自动化。进一步的要求是允许使用特定标准（例如，staging vs production）将附加组件针对子集群进行定位。\n集群附加组件图 在这个例子中，基础架构团队维护一个包含 Argo Workflows 控制器和 Prometheus Operator 应用程序清单的 Git 存储库。\n基础架构团队希望使用 Argo CD 将这两个插件部署到大量集群，并希望轻松管理新集群的创建/删除。\n在这个用例中，我们可以使用应用集控制器的 List、Cluster 或 Git 生成器中的任一个来提供所需的行为：\nList 生成器：管理员维护两个 ApplicationSet 资源，每个应用程序（工作流和 Prometheus）都包含它们希望在 List 生成器元素中定位的集群列表。 在该生成器中，添加/删除集群需要手动更新 ApplicationSet 资源的列表元素。 Cluster 生成器：管理员维护两个 ApplicationSet 资源，每个应用程序（工作流和 Prometheus）都确保在 Argo CD 中定义所有新集群。 由于 Cluster 生成器自动检测并针对 Argo CD 中定义的集群，添加/删除 Argo CD 中的集群 将自动导致应用集控制器创建 Application 资源（每个应用程序）。 Git 生成器：Git 生成器是生成器中最灵活/最强大的，因此有多种不同的方法来处理此用例。以下是其中的几个： 使用 Git 生成器的 files 字段：将集群列表作为 JSON 文件保存在 Git 存储库中。通过 Git 提交更新 JSON 文件，会导致添加/删除新集群。 使用 Git 生成器的 directories 字段：对于每个目标集群，都在 Git 存储库中存在一个对应的同名目录。通过 Git 提交添加/修改目录，将触发共享目录名称的集群的更新。 有关每个生成器的详细信息，请参见 生成器部分 。\n用例：单体库 在 单体库用例 中，Kubernetes 集群管理员从单个 Git 存储库管理单个 Kubernetes 集群的整个状态。\n合并到 Git 存储库的清单更改应自动部署到集群。\n在这个例子中，基础架构团队维护一个包含 Argo Workflows 控制器和 Prometheus Operator 应用程序清单的 Git 存储库。独立的开发团队还添加了他们希望部署到集群的其他服务。\n对 Git 生成器可能用于支持此用例：\nGit 生成器的 directories 字段可用于指定包含要部署的各个应用程序的特定子目录（使用通配符）。 Git 生成器的 files 字段可以引用包含 JSON 元数据的 Git 存储库文件，该元数据描述要部署的各个应用程序。 更多详情请参见 Git 生成器文档。 用例：多租户集群上的 Argo CD 应用程序自助服务 自助服务用例 旨在允许开发人员（作为多租户 Kubernetes 集群的最终用户）以自动化方式更灵活地执行以下操作：\n使用 Argo CD 将多个应用程序部署到单个集群 使用 Argo CD 自动化地部署到多个集群 但在这两种情况下，使开发人员能够在不需要涉及集群管理员的情况下执行此操作（以代表他们创建所需的 Argo CD 应用程序/AppProject 资源） 这个用例的一个潜在解决方案是，开发团队在 Git 存储库中定义 Argo CD Application 资源（包含他们希望部署的清单），在 app-of-apps 模式 中，并且集群管理员通过合并请求审查/接受对此存储库的更改。\n虽然这听起来像是一种有效的解决方案，但一个主要的劣势是需要高度的信任/审查来接受包含 Argo CD Application 规范更改的提交。这是因为 Application 规范中包含许多敏感字段，包括 project、cluster 和 namespace。意外合并可能会允许应用程序访问其不属于的命名空间/集群。\n因此，在自助服务用例中，管理员希望仅允许开发人员控制 Application 规范的某些字段（例如 Git 源存储库），但不允许控制其他字段（例如目标命名空间或目标集群应受到限制）。\n幸运的是，应用集控制器提供了另一种解决方案：集群管理员可以安全地创建包含 Git 生成器的 ApplicationSet 资源，该生成器使用 template 字段将应用程序资源的部署限制为固定值，同时允许开发人员随意自定义 ‘safe’ 字段。\nkind: ApplicationSet # (...) spec: generators: - git: repoURL: \u0026lt;https://github.com/argoproj/argo-cd.git\u0026gt; files: - path: \u0026#34;apps/**/config.json\u0026#34; template: spec: project: dev-team-one # 项目受到限制 source: # 开发人员可以使用上述 repo URL 中的 JSON 文件自定义应用程序详细信息 repoURL: {{app.source}} targetRevision: {{app.revision}} path: {{app.path}} destination: name: production-cluster # 集群受到限制 namespace: dev-team-one # 命名空间受到限制 有关更多详细信息，请参见 Git 生成器 。\n","relpermalink":"/book/argo-cd/operator-manual/applicationset/use-cases/","summary":"使用生成器的概念，应用集控制器提供了一组强大的工具，用于自动化模板化和修改 Argo CD 应用程序。生成器从各种来源（包括 Argo CD 集群和 Git 存储库）生成模板参数数据，支持和启用新的用例。 虽然可以将这些工具用于任何目的，","title":"ApplicationSet 控制器用例"},{"content":"你可以使用 Apache APISIX 和 Apache APISIX Ingress Controller 来进行 Argo Rollouts 的流量管理。\n当使用 Apache APISIX Ingress Controller 作为 Ingress 时，ApisixRoute 是支持 基于权重的流量分流 的对象。\n本指南展示了如何将 ApisixRoute 与 Argo Rollouts 集成，以将其用作加权轮询负载均衡器。\n先决条件 Argo Rollouts 需要 Apache APISIX v2.15 或更新版本以及 Apache APISIX Ingress Controller v1.5.0 或更新版本。\n使用 Helm v3 安装 Apache APISIX 和 Apache APISIX Ingress Controller：\nhelm repo add apisix https://charts.apiseven.com kubectl create ns apisix helm upgrade -i apisix apisix/apisix --version=0.11.3 \\ --namespace apisix \\ --set ingress-controller.enabled=true \\ --set ingress-controller.config.apisix.serviceNamespace=apisix 引导 首先，我们需要使用其加权轮询负载平衡功能创建 ApisixRoute 对象。\napiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: rollouts-apisix-route spec: http: - name: rollouts-apisix match: paths: - /* hosts: - rollouts-demo.apisix.local backends: - serviceName: rollout-apisix-canary-stable servicePort: 80 - serviceName: rollout-apisix-canary-canary servicePort: 80 kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/examples/apisix/route.yaml 请注意，我们不指定 weight 字段。它需要与 ArgoCD 同步。如果我们指定此字段并且 Argo Rollouts 控制器更改它，则 ArgoCD 控制器将注意到它并显示此资源不同步（如果你正在使用 Argo CD 管理你的 Rollout）。\n其次，我们需要创建 Argo Rollouts 对象。\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollout-apisix-canary spec: replicas: 5 strategy: canary: canaryService: rollout-apisix-canary-canary stableService: rollout-apisix-canary-stable trafficRouting: managedRoutes: - name: set-header apisix: route: name: rollouts-apisix-route rules: - rollouts-apisix steps: - setCanaryScale: replicas: 1 setHeaderRoute: match: - headerName: trace headerValue: exact: debug name: set-header - setWeight: 20 - pause: {} - setWeight: 40 - pause: duration: 15 - setWeight: 60 - pause: duration: 15 - setWeight: 80 - pause: duration: 15 revisionHistoryLimit: 2 selector: matchLabels: app: rollout-apisix-canary template: metadata: labels: app: rollout-apisix-canary spec: containers: - name: rollout-apisix-canary image: argoproj/rollouts-demo:blue ports: - name: http containerPort: 8080 protocol: TCP resources: requests: memory: 32Mi cpu: 5m kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/examples/apisix/rollout.yaml 最后，我们需要为 Argo Rollouts 对象创建服务。\napiVersion: v1 kind: Service metadata: name: rollout-apisix-canary-canary spec: ports: - port: 80 targetPort: http protocol: TCP name: http selector: app: rollout-apisix-canary # 此选择器将更新为 canary ReplicaSet 的 pod-template-hash，例如： # rollouts-pod-template-hash: 7bf84f9696 --- apiVersion: v1 kind: Service metadata: name: rollout-apisix-canary-stable spec: ports: - port: 80 targetPort: http protocol: TCP name: http selector: app: rollout-apisix-canary # 此选择器将更新为 stable ReplicaSet 的 pod-template-hash，例如： # rollouts-pod-template-hash: 789746c88d kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/examples/apisix/services.yaml 任何 Rollout 的初始创建将立即将副本扩展到 100%（跳过任何金丝雀升级步骤、分析等等），因为没有发生任何升级。\nArgo Rollouts kubectl 插件允许你可视化 Rollout、其相关资源（ReplicaSets、Pods、AnalysisRuns）并呈现状态更改。要在部署过程中观察 Rollout，请从插件中运行 get rollout --watch 命令：\nkubectl argo rollouts get rollout rollout-apisix-canary --watch 更新 Rollout 接下来是执行更新的时候。与 Deployments 一样，对 Pod 模板字段（spec.template）的任何更改都会导致部署新版本（即 ReplicaSet）。更新 Rollout 包括修改 rollout spec，通常是使用新版本更改容器镜像字段，然后针对新清单运行 kubectl apply。作为方便起见，rollouts 插件提供了一个 set image 命令，它针对现场的 rollout 对象执行这些步骤。运行以下命令，将 rollout-apisix-canary Rollout 更新为容器的 “yellow” 版本：\nkubectl argo rollouts set image rollout-apisix-canary rollouts-demo=argoproj/rollouts-demo:yellow 在升级期间，控制器将按照 Rollout 的更新策略进行进展。示例 Rollout 将将 20% 的流量权重分配给 canary，并无限期地暂停 Rollout，直到执行用户操作以取消暂停/升级 Rollout。\n你可以通过以下命令检查 ApisixRoute 的后端权重：\nkubectl describe apisixroute rollouts-apisix-route ...... Spec: Http: Backends: Service Name: rollout-apisix-canary-stable Service Port: 80 Weight: 80 Service Name: rollout-apisix-canary-canary Service Port: 80 Weight: 20 ...... rollout-apisix-canary-canary 服务通过 Apache APISIX 获得 20% 的流量。\n你可以通过以下命令检查 SetHeader ApisixRoute 的匹配：\nkubectl describe apisixroute set-header ...... Spec: Http: Backends: Service Name: rollout-apisix-canary-canary Service Port: 80 Weight: 100 Match: Exprs: Op: Equal Subject: Name: trace Scope: Header Value: debug ...... ","relpermalink":"/book/argo-rollouts/traffic-management/apisix/","summary":"你可以使用 Apache APISIX 和 Apache APISIX Ingress Controller 来进行 Argo Rollouts 的流量管理。 当使用 Apache APISIX Ingress Controller 作为 Ingress 时，ApisixRoute 是支持 基于权重的流量分流 的对象。 本指南展示了如何将 ApisixRoute 与 Argo Rollouts 集成，以将其用作加权轮询负载均衡器。 先决条件 Argo Rollouts 需","title":"Apache APISIX"},{"content":"本指南介绍了 Argo Rollouts 如何与 AWS 负载均衡器控制器 集成以进行流量调整。本指南以基本入门指南 的概念为基础。\n要求 安装了 AWS ALB Ingress Controller 的 Kubernetes 集群 🔔 提示：请参阅负载均衡器控制器安装说明 ，了解如何安装 AWS 负载均衡器控制器。\n1. 部署 Rollout、Services 和 Ingress 当 AWS ALB Ingress 用作流量路由器时，Rollout canary 策略必须定义以下字段：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollouts-demo spec: strategy: canary: # canaryService 和 stableService 是指向 Rollout 将要修改的 Service 的引用，以便将其定向到金丝雀 ReplicaSet 和稳定 ReplicaSet（必填）。 canaryService: rollouts-demo-canary stableService: rollouts-demo-stable trafficRouting: alb: # 引用的 Ingress 将通过注释操作被注入，以便将 AWS 负载均衡器控制器中的流量分配到金丝雀和稳定 Service 之间，根据所需的流量权重（必填）。 ingress: rollouts-demo-ingress # Ingress 必须在规则中定位到的 Service 的引用（可选）。 # 如果省略，使用 canary.stableService。 rootService: rollouts-demo-root # Service 端口是 Service 监听的端口（必填）。 servicePort: 443 ... Rollout 引用的 Ingress 必须具有与其中一项 Rollout 服务相匹配的规则。这应该是 canary.trafficRouting.alb.rootService （如果指定），否则 Rollout 将使用 canary.stableService 。\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: rollouts-demo-ingress annotations: kubernetes.io/ingress.class: alb spec: rules: - http: paths: - path: /* backend: # serviceName 必须匹配以下之一：canary.trafficRouting.alb.rootService（如果指定）， # 或 canary.stableService（如果省略 rootService） serviceName: rollouts-demo-root # servicePort 必须是值：use-annotation # 这样可以指示 AWS 负载均衡器控制器查看注释以了解如何定向流量 servicePort: use-annotation 在更新期间，Ingress 将被注入一个自定义动作注释 ，该注释指示 ALB 在稳定版和 Rollout 引用的金丝雀服务之间分割流量。在这个例子中，这些服务的名称分别是：rollouts-demo-stable和 rollouts-demo-canary。\n运行以下命令来部署：\n一个 Rollout 三个服务（root、stable、canary） 一个 Ingress kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/alb/rollout.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/alb/services.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/alb/ingress.yaml 应用清单后，您应该在集群中看到以下 Rollout、服务和 Ingress 资源：\n$ kubectl get ro NAME DESIRED CURRENT UP-TO-DATE AVAILABLE rollouts-demo 1 1 1 1 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE rollouts-demo-root NodePort 10.100.16.123 \u0026lt;none\u0026gt; 80:30225/TCP 2m43s rollouts-demo-canary NodePort 10.100.16.64 \u0026lt;none\u0026gt; 80:30224/TCP 2m43s rollouts-demo-stable NodePort 10.100.146.232 \u0026lt;none\u0026gt; 80:31135/TCP 2m43s $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE rollouts-demo-ingress * b0548428-default-rolloutsd-6951-1972570952.ap-northeast-1.elb.amazonaws.com 80 6m36s kubectl argo rollouts get rollout rollouts-demo Rollout ALB 2. 执行更新 通过更改镜像来更新 Rollout，并等待其达到暂停状态。\nkubectl argo rollouts set image rollouts-demo rollouts-demo=argoproj/rollouts-demo:yellow kubectl argo rollouts get rollout rollouts-demo Rollout ALB Paused 此时，Rollout 的金丝雀和稳定版本都在运行，将 5% 的流量指向金丝雀。为了理解这是如何工作的，请检查 ALB 的侦听器规则。查看侦听器规则时，我们可以看到控制器已修改转发操作权重，以反映金丝雀的当前权重。\nALB Listener_Rules 控制器已将 rollouts-pod-template-hash 选择器添加到服务中，并将相同的标签附加到 Pod 上。因此，您可以通过简单地将请求转发到服务来分割流量并按权重分配请求。\n随着 Rollout 在步骤中的进展，转发操作权重将根据步骤的当前 setWeight 调整。\n","relpermalink":"/book/argo-rollouts/getting-started/alb/index/","summary":"本指南介绍了 Argo Rollouts 如何与 AWS 负载均衡器控制器 集成以进行流量调整。本指南以基本入门指南 的概念为基础。 要求 安装了 AWS ALB Ingress Controller 的 Kubernetes 集群 🔔 提示：请参阅负载均衡器控制器安装说明 ，了解如何安装 AWS 负载均衡器控制器。 1. 部署 R","title":"AWS Load Balancer Controller 快速开始"},{"content":"水平 Pod 自动缩放（HPA）根据观察到的 CPU 利用率或用户配置的指标自动调整 Kubernetes 资源拥有的 Pod 数量。为了实现这种行为，HPA 仅支持启用了 scale 端点的资源，该端点具有几个必需字段。scale 端点允许 HPA 了解资源的当前状态并修改资源以适当地进行扩展。Argo Rollouts 在 0.3.0 版本中添加了对 scale 端点的支持。在 HPA 修改资源后，Argo Rollouts 控制器负责在副本中协调该变化。由于 Rollout 中的策略非常不同，因此 Argo Rollouts 控制器会针对各种策略以不同的方式处理 scale 端点。下面是不同策略的行为。\n蓝绿部署 HPA 将使用从接收来自活动服务的流量的 ReplicaSet 中获取的指标来缩放 BlueGreen 策略的 Rollouts。当 HPA 更改副本计数时，Argo Rollouts 控制器将首先缩放接收来自活动服务的 ReplicaSet，然后是接收来自预览服务的 ReplicaSet。控制器将缩放接收来自预览服务的 ReplicaSet，以准备在 Rollout 将预览切换为活动时使用。如果没有接收来自活动服务的 ReplicaSets，则控制器将使用与基本选择器匹配的所有 Pod 来确定缩放事件。在这种情况下，控制器将将最新的 ReplicaSet 缩放到新计数，并将较旧的 ReplicaSets 缩小。\n金丝雀（基于 ReplicaSet） HPA 将使用所有 Rollout 中的 ReplicaSets 的指标来缩放 Canary 策略的 Rollouts。由于 Argo Rollouts 控制器不控制发送流量到这些 ReplicaSets 的服务，因此它假设 Rollout 中的所有 ReplicaSets 都正在接收流量。\n例子 下面是基于 CPU 指标缩放 Rollout 的 Horizontal Pod Autoscaler 的示例：\napiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: hpa-rollout-example spec: maxReplicas: 6 minReplicas: 2 scaleTargetRef: apiVersion: argoproj.io/v1alpha1 kind: Rollout name: example-rollout targetCPUUtilizationPercentage: 80 要求 为了使 HPA 能够操作 Rollout，托管 Rollout CRD 的 Kubernetes 集群需要 CRD 的子资源支持。该功能在 Kubernetes 版本 1.10 中作为 alpha 引入，并在 Kubernetes 版本 1.11 中过渡为 beta。如果用户想在 v1.10 上使用 HPA，则 Kubernetes 集群运营商将需要向 API 服务器添加自定义功能标志。在 1.10 之后，默认情况下会打开该标志。请查看以下链接 ，了解有关设置自定义功能标志的更多信息。\n","relpermalink":"/book/argo-rollouts/rollout/hpa-support/","summary":"水平 Pod 自动缩放（HPA）根据观察到的 CPU 利用率或用户配置的指标自动调整 Kubernetes 资源拥有的 Pod 数量。为了实现这种行为，HPA 仅支持启用了 scale 端点的资源，该端点具有几个必需字段。scale 端点允许 HPA 了解资源的当前状态","title":"水平 Pod 自动缩放"},{"content":"Rollout Rollout 是 Kubernetes 工作负载资源，相当于 Kubernetes Deployment 对象。它旨在在需要更高级的部署或渐进式交付功能的场景中替换 Deployment 对象。Rollout 提供以下功能，而 Kubernetes Deployment 无法提供：\n蓝绿部署 金丝雀部署 与入口控制器和服务网格集成，用于高级流量路由 与度量提供程序集成，用于蓝绿和金丝雀分析 基于成功或失败的度量自动升级或回滚 渐进式交付 渐进式交付是以受控和逐步的方式发布产品更新的过程，从而降低发布的风险，通常通过耦合自动化和度量分析来驱动更新的自动升级或回滚。\n渐进式交付通常被描述为持续交付的演进，将 CI/CD 中实现的速度优势扩展到部署过程中。这是通过将新版本的曝光限制为子集用户，并观察和分析其正确行为，然后逐步增加曝光范围以涵盖更广泛的受众并持续验证正确性来实现的。\n部署策略 虽然行业已经使用一致的术语来描述各种部署策略，但这些策略的实现在各种工具之间存在差异。为了清楚地了解 Argo Rollouts 的行为，以下是 Argo Rollouts 提供的各种部署策略实现的描述。\n滚动更新（Rolling Update） RollingUpdate 逐步用新版本替换旧版本。随着新版本的启动，旧版本会缩小以维护应用程序的总数。这是 Deployment 对象的默认策略。\n重建（Recreate） 重建部署在启动新版本之前删除应用程序的旧版本。因此，这确保了两个版本的应用程序永远不会同时运行，但在部署期间会有停机时间。\n蓝绿（Blue-Green） 蓝绿部署（有时称为红黑）同时部署新旧版本的应用程序。在此期间，只有旧版本的应用程序将接收生产流量。这使开发人员可以在将实时流量切换到新版本之前对新版本运行测试。\n蓝绿部署示意图 金丝雀（Canary） 金丝雀部署将一部分用户暴露给应用程序的新版本，同时为其余流量提供旧版本。一旦验证了新版本的正确性，新版本就可以逐渐替换旧版本。入口控制器和服务网格（如 NGINX 和 Istio）可实现比本地提供的更复杂的金丝雀流量整形模式（例如实现非常细粒度的流量分割或基于 HTTP 头分割）。\n金丝雀部署示意图 上图显示了具有两个阶段（10％和 33％的流量流向新版本）的金丝雀，但这仅是一个示例。使用 Argo Rollouts，可以根据你的用例定义确切数量的阶段和流量百分比。\n","relpermalink":"/book/argo-rollouts/concepts/","summary":"Rollout Rollout 是 Kubernetes 工作负载资源，相当于 Kubernetes Deployment 对象。它旨在在需要更高级的部署或渐进式交付功能的场景中替换 Deployment 对象。Rollout 提供以下功能，而 Kubernetes Deployment 无法提供： 蓝绿部署 金丝雀部署 与入口控制器和服务网格集成，用于高级流量","title":"概念"},{"content":"Argo Rollouts 支持以下指标度量：\nPrometheus DataDog NewRelic Wavefront Job Web Kayenta CloudWatch Graphite InfluxDB Apache SkyWalking ","relpermalink":"/book/argo-rollouts/analysis/metrics/","summary":"Argo Rollouts 支持以下指标度量： Prometheus DataDog NewRelic Wavefront Job Web Kayenta CloudWatch Graphite InfluxDB Apache SkyWalking","title":"指标度量"},{"content":"本章从业务和技术的角度解释了在基础设施中部署 SPIFFE 和 SPIRE 的好处。\n任何人任何地方都适用 SPIFFE 和 SPIRE 旨在加强对软件组件的识别，以一种通用的方式，任何人在任何地方都可以在分布式系统中加以利用。现代基础设施的技术环境是错综复杂的。环境在硬件和软件投资的混合下变得越来越不一样。通过对系统定义、证明和维护软件身份标准化的方式来维护软件安全，无论系统部署在哪里，也无论谁来部署这些系统，都会带来许多好处。\n对于专注于提高业务便利性和回报的企业领导人来说，SPIFFE 和 SPIRE 可以大大降低管理和签发加密身份文件（如 X.509 证书）的开销，开发人员无需了解服务间通信所需的身份和认证技术，从而加速开发和部署。\n对于专注于提供强大、安全和可互操作产品的服务提供商和软件供应商来说，SPIFFE 和 SPIRE 解决了在将许多解决方案互连到最终产品时普遍存在的关键身份问题。例如，SPIFFE 可以作为产品的 TLS 功能和用户管理/认证功能的基础，一举两得。还有，SPIFFE 可以取代管理和发行平台访问的 API 令牌的需要，免费带来令牌轮换，并消除存储和管理访问所述令牌的客户负担。\n对于希望不仅加强传输中的数据安全，而且实现监管合规并解决其信任根源问题的安全从业人员来说，SPIFFE 和 SPIRE 致力于在不信任的环境中提供相互认证，而不需要交换秘密。安全和管理边界可以很容易地划定，并且在策略允许的情况下，可以跨越这些边界进行通信。\n对于需要身份管理抽象的开发人员、运维和 DevOps 从业人员，以及需要与现代云原生服务和解决方案互操作的工作负载和应用程序，SPIFFE 和 SPIRE 在整个软件开发生命周期中与许多其他工具兼容，以提供可靠的产品。开发人员可以继续他们的工作，直接进入业务逻辑，而不必担心证书、私钥和 JavaScript Web Token（JWT）等烦人的问题。\n对于企业领导人 现代的组织有现代的需求 在今天的商业环境中，通过差异化的应用和服务快速提供创新的客户体验是保持竞争优势的必要条件。因此，企业见证了应用程序和服务的架构、构建和部署方式的变化。诸如云和容器等新技术帮助企业更快、更大规模地发布。服务需要高速构建并部署在大量的平台上。随着开发速度的加快，这些系统变得越来越相互依赖和相互联系，以提供一致的客户体验。\n组织在实现高速发展和获得市场份额或任务保证方面可能会受到抑制，主要原因是合规性、专业知识的储备以及团队 / 组织之间和现有解决方案内的互操作性挑战。\n互操作性的影响\n随着系统的发展，对互操作性的需求也在无限地增长。脱节的团队建立的服务是孤立的，互不相识，尽管他们最终需要意识到彼此的存在。收购发生时，新的或从未见过的系统需要被整合到现有的系统中。商业关系的建立，需要与可能存在于堆栈深处的服务建立新的沟通渠道。所有这些挑战都围绕着“我如何以安全的方式将所有这些服务连接在一起，每个服务都有自己独特的属性和历史？”\n当不同的技术栈必须结合在一起进行互操作时，由于组织融合而产生的技术整合可能是一个挑战。为系统与系统之间的通信与身份和认证制定一个共同的、行业认可的标准，可以简化跨多个堆栈的完全互操作性和整合的技术问题。\nSPIFFE 带来了对构成软件身份的共识。通过进一步利用 SPIFFE Federation，不同组织或团队的不同系统中的组件可以建立信任，安全地进行通信，而不需要增加诸如 VPN 隧道、one-off 证书或用于这些系统之间的共享凭证等结构的开销。\n合规性和可审计性\nSPIRE 实施中的可审计性保证了执行行动的身份不会因为在环境中执行相互认证而被否定。此外，SPIFFE/SPIRE 发布的身份文件使相互认证的 TLS 得到广泛使用，有效地解决了与这种性质的项目相关的最困难的挑战之一。相互认证的 TLS 的其他好处包括对服务之间传输的数据进行本地加密，不仅保护了通信的完整性，还保证了敏感或专有数据的保密性。\n另一个常见的合规要求是由《通用数据保护条例》（GDPR）带来的 —— 特别是要求欧盟（EU）的数据完全停留在欧盟内部，而不是在其管辖范围之外的实体中转或被处理。有了多个信任根基，全球组织可以确保欧盟实体只与其他欧盟实体沟通。\n专业知识库\n确保开发、安全和运营团队具备正确的知识和经验，以适当地处理安全敏感系统，仍然是一项重大挑战。企业需要雇用具有基于标准的技能组合的开发人员，以减少入职时间，并在减少风险的情况下改善产品效率。\n解决以自动方式向每个软件实例提供加密身份的问题，并从根本上实现凭证轮换，是一项重大挑战。对于安全和运维团队来说，具有的实施此类系统所需的专业知识少之又少。在不依靠社区或行业知识的情况下维持日常运营会使问题恶化，导致中断和指责。\n不能合理地期望开发人员了解或获得安全的实际问题的专业知识，特别是在组织环境中适用于服务身份。此外，在开发、运维和工作负载执行方面具有深度知识的安全从业人员的储备是非常少的。利用一个开放的标准和开放的规范来解决关键的身份问题，允许没有个人经验的人通过一个得到良好支持的、不断增长的 SPIFFE/SPIRE 终端用户和从业人员社区来扩展知识。\n节约\n采用 SPIFFE/SPIRE 可以在许多方面节省成本，包括减少云 / 平台锁定，提高开发人员的效率，以及减少对专业知识的依赖，等等。\n通过将云提供商的身份接口抽象为一套建立在开放标准上的定义明确的通用 API，SPIFFE 大大减轻了开发和维护云感知应用的负担。由于 SPIFFE 是平台无关的，它几乎可以在任何地方部署。当需要改变平台技术时，这种差异化可以节省时间和金钱，甚至可以加强与现有平台供应商的谈判地位。从历史上看，身份和访问管理服务是由每个组织自己的部署指挥和控制平台 —— 云服务提供商知道这一点，并利用这一制约因素作为主要的锁定机制，与他们的平台完全整合。\n在提高开发人员的效率方面也有很大的节省。SPIFFE/SPIRE 有两个重要方面可以节省开支：加密身份及其相关生命周期的完全自动化发布和管理，以及认证和服务间通信加密的统一性和加载性。通过消除与前者相关的手动流程，以及在后者中花费的研究和试验 / 错误时间，开发人员可以更好地专注于对他们重要的事情：业务逻辑。\n提高开发人员的生产力 值 开发人员在获取证书和配置每个应用组件的认证 / 保密协议方面花费的平均时间（小时）。 2 减少开发人员在每个应用组件上对应的证书所花费的时间。 95% 开发人员在学习和实施特定 API 网关、秘密存储等控制方面花费的平均时间（小时） 1 减少了开发人员学习和实施特定 API 网关、秘密存储等控制的时间。 75% 本年度开发的新应用组件的数量 200 预计因提高开发人员生产力而节省的时间 530 正如我们在历史上看到的那样，财富 50 强的技术组织雇用了高度熟练和专业的工程师，花了几十年时间来解决这个身份问题。将 SPIFFE/SPIRE 添加到企业的云原生解决方案目录中，可以让你在多年的超专业安全和开发人才的基础上构建，而不需要相应的成本。\n凭借强大的社区支持几十个到几十万个节点的部署，SPIFFE/SPIRE 在复杂、大规模环境中的运作经验可以满足组织的需求。\n对于服务提供商和软件供应商 减少客户在使用产品过程中的负担，始终是所有优秀产品经理的首要目标。了解那些表面上看起来无害的功能的实际意义是很重要的。例如，如果一个数据库产品需要支持 TLS，因为这是客户的要求，很简单，在产品中添加一些配置就可以了。\n不幸的是，这给客户带来了一些重大的挑战。即使是看似简单的用户管理也面临类似的挑战。考虑一下这两个常见的功能在默认情况下引入的以下客户痛点：\n谁生成证书和密码，以及如何生成？ 它们如何被安全地分配给需要的应用程序？ 如何限制对私钥和密码的访问？ 这些秘密是如何存储的，才能让它们不会泄漏到备份中？ 当证书过期，或必须改变密码时，会发生什么？这个过程是否具有破坏性？ 在这些任务中，有多少是必须要有人类操作的？ 在这些功能从客户的角度来看是可行的之前，所有这些问题都需要得到回答。通常，客户发明或安装的解决方案在操作上是很痛苦的。\n这些客户的负担是非常真实的。有些组织有整个团队专门负责管理这些负担。通过简单地支持 SPIFFE，上述所有的担忧都会得到缓解。该产品可以集成进现有的基础设施，并免费增加 TLS 支持。此外，由 SPIFFE 赋予的客户（用户）身份可以直接取代管理用户凭证（如密码）的需要。\n平台访问管理 访问一个服务或平台（如 SaaS 服务）也涉及类似的挑战。归根结底，这些挑战为凭证管理所带来的固有困难，尤其是当凭证是一个共享的秘密时。\n考虑一下 API 令牌 —— 在 SaaS 提供商中，使用 API 令牌来验证非人类的 API 调用者是很普遍的。它们实际上是密码，而且每一个都必须由客户仔细管理。上面列出的所有挑战都适用于此。支持 SPIFFE 认证的平台大大减轻了与访问平台有关的客户负担，一次性解决了存储、发行和生命周期问题。利用 SPIFFE，问题被简化为简单地授予给定工作负载所需的权限。\n对于安全从业人员 技术创新不能成为安全产品的抑制因素。开发、分发和部署工具需要与安全产品和方法无缝集成，不影响软件开发的自主性或为组织带来负担。组织需要易于使用的软件产品，并为现有工具增加额外的安全性。\nSPIRE 不是所有安全问题的最终解决方案。它并不否定对强大的安全实践和深度防御或分层安全的需要。然而，利用 SPIFFE/SPIRE 提供跨不信任网络的信任根，使组织能够在通往零信任架构 的道路上迈出有意义的一步，作为全面安全战略的一部分。\n默认安全 SPIRE 可以帮助减轻 OWASP 的几个主要威胁 。为了减少通过凭证泄露的可能性，SPIRE 为整个基础设施的认证提供了一个强有力的证明身份。保持证明的自动化使平台默认安全，消除了开发团队的额外配置负担。\n对于希望从根本上解决其产品或服务中的信任和身份问题的组织来说，SPIFFE/SPIRE 还通过实现普遍的相互 TLS 认证来解决客户的安全需求，以便在工作负载之间安全地进行通信，无论它们部署在何处。此外，与每个开源产品一样，代码库背后的社区和贡献者提供了更多双眼睛来审查合并前和合并后的代码。这个 莱纳斯法则（Linus Law） 的实施超越了四只眼睛的原则，以确保任何潜在的错误或已知的安全问题在进入发布阶段之前被发现。\n策略执行 SPIRE 的 API 为安全团队提供了一种机制，以便以易于使用的方式在各平台和业务部门执行一致的认证策略。当与定义明确的策略相结合时，服务之间的互动可以保持在最低限度，确保只有授权的工作负载可以相互通信。这限制了恶意实体的潜在攻击面，并可以在策略引擎的默认拒绝规则中触发警报。\nSPIRE 利用一个强大的多因素证明引擎，该引擎实时运行，可以肯定地确定加密身份的发放。它还自动发放、分配和更新短期凭证，以确保组织的身份架构准确反映工作负载的运行状态。\n零信任 在架构中采用零信任模式，可以减少漏洞发生时的爆炸半径。相互认证和信任撤销可以阻止被破坏的前端应用服务器从网络上或集群内可能存在的不相关数据库中渗出数据。虽然不可能发生在网络安全严密的组织中，但 SPIFFE/SPIRE 肯定会增加额外的防御层，以减轻错误配置的防火墙或不变的默认登录带来的漏洞和暴露点。它还将安全决策从 IP 地址和端口号（可以用不可察觉的方式进行操纵）转移到享有完整性保护的加密标识符上。\n记录和监控 SPIRE 可以帮助改善基础设施的可观测性。关键的 SPIRE 事件，如身份请求和发放，是可记录的事件，有助于提供一个更完整的基础设施视图。SPIRE 还将生成各种行动的事件，包括身份注册、取消注册、验证尝试、身份发放和轮换。然后，这些事件可以被汇总并发送到组织的安全信息和事件管理（SIEM）解决方案，以便进行统一监控。\n对于开发、运维和 DevOps 来说 即使你可以通过采用和支持 SPIFFE/SPIRE 而不考虑环 …","relpermalink":"/book/spiffe/benefits/","summary":"本章从业务和技术的角度解释了在基础设施中部署 SPIFFE 和 SPIRE 的好处。","title":"收益"},{"content":"所有 BPF Map 都是有使用容量上限的。超出限制的插入将失败，从而限制了数据路径的可扩展性。下表显示了映射的默认值。每个限制都可以在源代码中更改。如果需要，将根据要求添加配置选项。\nMap 名称 范围 默认限制 规模影响 连接跟踪 节点或端点 1M TCP/256k UDP 最大 1M 并发 TCP 连接，最大 256k 预期 UDP 应答 NAT 节点 512k 最大 512k NAT 条目 邻居表 节点 512k 最大 512k 邻居条目 端点 节点 64k 每个节点最多 64k 个本地端点 + 主机 IP IP 缓存 节点 512k 最大 256k 端点（IPv4+IPv6），最大 512k 端点（IPv4 或 IPv6）跨所有集群 负载均衡器 节点 64k 跨所有集群的所有服务的最大 64k 累积后端 策略 端点 16k 特定端点的最大允许身份 + 端口 + 协议对 16k 代理 Map 节点 512k 最大 512k 并发重定向 TCP 连接到代理 隧道 节点 64k 跨所有集群最多 32k 节点（IPv4+IPv6）或 64k 节点（IPv4 或 IPv6） IPv4 分片 节点 8k 节点上同时传输的最大 8k 个分段数据报 会话亲和性 节点 64k 来自不同客户端的最大 64k 关联 IP 掩码 节点 16k 基于 BPF 的 ip-masq-agent 使用的最大 16k IPv4 cidrs 服务源范围 节点 64k 跨所有服务的最大 64k 累积 LB 源范围 Egess 策略 端点 16k 跨所有集群的所有目标 CIDR 的最大 16k 端点 对于某些 BPF 映射，可以使用命令行选项 cilium-agent 覆盖容量上限。可以使用 --bpf-lb-map-max、 --bpf-ct-global-tcp-max、--bpf-ct-global-any-max、 --bpf-nat-global-max、--bpf-neigh-global-max、--bpf-policy-map-max和 --bpf-fragments-map-max 来设置给定容量。\n提示 如果指定了--bpf-ct-global-tcp-max和 / 或--bpf-ct-global-any-max ，则 NAT 表大小 ( --bpf-nat-global-max) 不得超过组合 CT 表大小（TCP + UDP）的 2/3。--bpf-nat-global-max 如果未显式设置或使用动态 BPF Map 大小（见下文），这将自动设置。 使用 --bpf-map-dynamic-size-ratio 标志，几个大型 BPF Map 的容量上限在代理启动时根据给定的总系统内存比率确定。例如，给定的 0.0025 比率导致 0.25% 的总系统内存用于这些映射。\n此标志会影响以下消耗系统中大部分内存的 BPF Map： cilium_ct_{4,6}_global、cilium_ct_{4,6}_any、 cilium_nodeport_neigh{4,6}、cilium_snat_v{4,6}_external 和 cilium_lb{4,6}_reverse_sk\nkube-proxy根据机器拥有的内核数设置为 linux 连接跟踪表中的最大条目数。无论机器有多少内核，kube-proxy 默认每个内核的最大条目数是 32768 和最小条目数是 131072。\nCilium 有自己的连接跟踪表作为 BPF Map，并且此类映射的条目数是根据节点中的总内存量计算的，无论机器有多少内存，条目最少数是 131072。\n下表介绍了当 Cilium 配置为 -bpf-map-dynamic-size-ratio: 0.0025 时，kube-proxy 和 Cilium 为自己的连接跟踪表设置的数值：\n虚拟 CPU 内存 (GiB) Kube-proxy CT 条目 Cilium CT 条目 1 3.75 131072 131072 2 7.5 131072 131072 4 15 131072 131072 8 30 262144 284560 16 60 524288 569120 32 120 1048576 1138240 64 240 2097152 2276480 96 360 3145728 4552960 ","relpermalink":"/book/cilium-handbook/ebpf/maps/","summary":"所有 BPF Map 都是有使用容量上限的。超出限制的插入将失败，从而限制了数据路径的可扩展性。下表显示了映射的默认值。每个限制都可以在源代码中更改。如果需要，将根据要求添加配置选项。 Map 名称 范围 默认限制 规模影响 连接","title":"eBPF Map"},{"content":"用于 Pod 的 IPv4 地址通常是从 RFC1918 私有地址块分配的，因此不可公开路由。Cilium 会自动将离开集群的所有流量的源 IP 地址伪装成节点的 IPv4 地址，因为节点的 IP 地址已经可以在网络上路由。\nIP 地址伪装示意图 对于 IPv6 地址，只有在使用 iptables 实现模式时才会执行伪装。\n可以使用 IPv4 选项和离开主机的 IPv6 流量禁用此行为。enable-ipv4-masquerade: false``enable-ipv6-masquerade: false\n配置 设置可路由 CIDR\n默认行为是排除本地节点的 IP 分配 CIDR 内的任何目标。如果 pod IP 可在更广泛的网络中路由，则可以使用以下选项指定该网络：ipv4-native-routing-cidr: 10.0.0.0/8 ，在这种情况下，该 CIDR 内的所有目的地都 不会 被伪装。\n设置伪装接口\n请参阅实现模式 以配置伪装接口。\n实现模式 基于 eBPF 基于 eBPF 的实现是最有效的实现。它需要 Linux 内核 4.19，并且可以使用 bpf.masquerade=true helm 选项启用。\n当前的实现依赖于 BPF NodePort 特性 。将来会删除该依赖项（GitHub Issue 13732 ）。\n伪装只能在那些运行 eBPF 伪装程序的设备上进行。这意味着如果输出设备运行程序，从 pod 发送到外部的数据包将被伪装（到输出设备 IPv4 地址）。如果未指定，程序将自动附加到 BPF NodePort 设备检测机制 选择的设备上。要手动更改此设置，请使用 devices helm 选项。使用 cilium status 确定程序在哪些设备上运行：\n$ kubectl exec -it -n kube-system cilium-xxxxx -- cilium status | grep Masquerading Masquerading: BPF (ip-masq-agent) [eth0, eth1] 10.0.0.0/16 从上面的输出可以看出，程序正在 eth0 和 eth1 设备上运行。\n基于 eBPF 的伪装可以伪装以下 IPv4 四层协议的数据包：\nTCP UDP ICMP（仅 Echo 请求和 Echo 回复） 默认情况下，来自 pod 的所有发往 ipv4-native-routing-cidr 范围外 IP 地址的数据包都会被伪装，但发往其他集群节点的数据包除外。排除 CIDR 显示在（cilium status ) 的上述输出中（10.0.0.0/16）。\n提示 启用 eBPF 伪装后，从 Pod 到集群节点外部 IP 的流量也不会被伪装。eBPF 实现在这方面不同于基于 iptables 的伪装。此限制在 GitHub Issue 17177 中进行了跟踪。 为了实现更细粒度的控制，Cilium 在 eBPF 中实现了ip-masq-agent ，可以通过ipMasqAgent.enabled=true helm 选项启用。\n基于 eBPF 的 ip-masq-agent 支持配置文件中设置的 nonMasqueradeCIDRs 和 masqLinkLocal 选项。从 pod 发送到属于任何 CIDR 的目的地的数据包 nonMasqueradeCIDRs 不会被伪装。如果配置文件为空，代理将配置以下非伪装 CIDR：\n10.0.0.0/8 172.16.0.0/12 192.168.0.0/16 100.64.0.0/10 192.0.0.0/24 192.0.2.0/24 192.88.99.0/24 198.18.0.0/15 198.51.100.0/24 203.0.113.0/24 240.0.0.0/4 此外，如果 masqLinkLocal 未设置或设置为 false，则 169.254.0.0/16 附加到非伪装 CIDR 列表中。\n代理使用 Fsnotify 跟踪配置文件的更新，因此 resyncInterval 不需要原始选项。\n下面的示例显示了如何通过配置代理 ConfigMap 并对其进行验证：\n$ cat agent-config/config nonMasqueradeCIDRs: - 10.0.0.0/8 - 172.16.0.0/12 - 192.168.0.0/16 masqLinkLocal: false $ kubectl create configmap ip-masq-agent --from-file=agent-config --namespace=kube-system $ # Wait ~60s until the ConfigMap is mounted into a cilium pod $ kubectl -n kube-system exec -ti cilium-xxxxx -- cilium bpf ipmasq list IP PREFIX/ADDRESS 10.0.0.0/8 169.254.0.0/16 172.16.0.0/12 192.168.0.0/16 注意 IPv6 流量当前不支持基于 eBPF 的伪装。 基于 iptables 这是适用于所有内核版本的遗留实现。\n默认行为将伪装所有离开非 Cilium 网络设备的流量。这通常会导致正确的行为。为了限制应在其上执行伪装的网络接口，可以使用egress-masquerade-interfaces: eth0 选项。\n提示 也可以指定接口前缀 eth+，匹配前缀的所有接口 eth 都将用于伪装。 ","relpermalink":"/book/cilium-handbook/networking/masquerading/","summary":"用于 Pod 的 IPv4 地址通常是从 RFC1918 私有地址块分配的，因此不可公开路由。Cilium 会自动将离开集群的所有流量的源 IP 地址伪装成节点的 IPv4 地址，因为节点的 IP 地址已经可以在网络上路由。 IP 地址伪装示意图 对于 IPv6 地址，只有在","title":"IP 地址伪装"},{"content":"所有安全策略的描述都是假设基于会话协议的有状态策略执行。这意味着策略的意图是描述允许的连接建立方向。如果策略允许A =\u0026gt; B，那么从 B 到 A 的回复数据包也会被自动允许。但是，并不自动允许 B 向 A 发起连接。如果希望得到这种结果，那么必须明确允许这两个方向。\n安全策略可以在 ingress 或 egress 处执行。对于 ingress，这意味着每个集群节点验证所有进入的数据包，并确定数据包是否被允许传输到预定的终端。相应地，对于 egress，每个集群节点验证出站数据包，并确定是否允许将数据包传输到预定目的地。\n为了在多主机集群中执行基于身份的安全，发送端点的身份被嵌入到集群节点之间传输的每个网络数据包中。然后，接收集群节点可以提取该身份，并验证一个特定的身份是否被允许与任何本地端点进行通信。\n默认安全策略 如果没有加载任何策略，默认行为是允许所有通信，除非明确启用了策略执行。一旦加载了第一条策略规则，就会自动启用策略执行，然后任何通信必须是白名单，否则相关数据包将被丢弃。\n同样，如果一个端点不受制于四层策略，则允许与所有端口进行通信。将至少一个 四层策略与一个端点相关联，将阻止与端口的所有连接，除非明确允许。\n","relpermalink":"/book/cilium-handbook/security/policyenforcement/","summary":"所有安全策略的描述都是假设基于会话协议的有状态策略执行。这意味着策略的意图是描述允许的连接建立方向。如果策略允许A =\u003e B，那么从 B 到 A 的回复数据包也会被自动允许。但是，并不自动允许 B 向 A 发起连接。如果希","title":"策略执行"},{"content":"本节指定 Cilium 端点的生命周期。\nCilium 中的端点状态包括：\nrestoring：端点在 Cilium 启动之前启动，Cilium 正在恢复其网络配置。 waiting-for-identity：Cilium 正在为端点分配一个唯一的身份。 waiting-to-regenerate：端点接收到一个身份并等待（重新）生成其网络配置。 regenerating：正在（重新）生成端点的网络配置。这包括为该端点编程 eBPF。 ready：端点的网络配置已成功（重新）生成。 disconnecting：正在删除端点。 disconnected：端点已被删除。 端点状态生命周期 可以使用 cilium endpoint list 和 cilium endpoint get CLI 命令查询端点的状态。\n当端点运行时，它会在 waiting-for-identity、waiting-to-regenerate、regenerating 和 ready 状态之间转换。进入 waiting-for-identity 状态的转换表明端点改变了它的身份。转换到 waiting-to-regenerate 或者 regenerating 状态表示要在端点上实施的策略由于身份、策略或配置的更改而发生了更改。\n端点在被删除时转换为 disconnecting 状态，无论其当前状态如何。\n初始化标识 在某些情况下，Cilium 无法在端点创建时立即确定端点的标签，因此无法在此时为端点分配身份。在知道端点的标签之前，Cilium 会暂时将一个特殊的单一标签 reserved:init 与端点相关联。当端点的标签变得已知时，Cilium 然后用端点的标签替换那个特殊的标签，并为端点分配一个适当的身份。\n这可能在以下情况下在端点创建期间发生：\n通过 libnetwork 使用 docker 运行 Cilium 当 Kubernetes API 服务器不可用时使用 Kubernetes 在对应的 kvstore 不可用时处于 etcd 模式 要在初始化时允许进出端点的流量，您可以创建选择 reserved:init 标签的策略规则和/或允许进出特殊 init 实体的流量的规则。\n例如，编写一个规则，允许所有初始化端点从主机接收连接并执行 DNS 查询，可以如下完成：\napiVersion: \u0026#34;cilium.io/v2\u0026#34; kind: CiliumNetworkPolicy metadata: name: init specs: - endpointSelector: matchLabels: \u0026#34;reserved:init\u0026#34;: \u0026#34;\u0026#34; ingress: - fromEntities: - host egress: - toEntities: - all toPorts: - ports: - port: \u0026#34;53\u0026#34; protocol: UDP 同样，编写允许端点接收来自初始化端点的 DNS 查询的规则可以如下完成：\napiVersion: \u0026#34;cilium.io/v2\u0026#34; kind: CiliumNetworkPolicy metadata: name: \u0026#34;from-init\u0026#34; spec: endpointSelector: matchLabels: app: myService ingress: - fromEntities: - init - toPorts: - ports: - port: \u0026#34;53\u0026#34; protocol: UDP 如果任何入口（resp.egress）策略规则选择了 reserved:init 标签，那么所有到（resp.from）初始化端点（这些规则未明确允许）的入口（resp.egress）流量都将被丢弃。否则，如果策略执行模式是 never 或 default，则允许所有入口（或出口）流量（或来自）初始化端点。否则，所有入口（或出口）流量都会被丢弃。\n","relpermalink":"/book/cilium-handbook/policy/lifecycle/","summary":"本节指定 Cilium 端点的生命周期。 Cilium 中的端点状态包括： restoring：端点在 Cilium 启动之前启动，Cilium 正在恢复其网络配置。 waiting-for-identity：Cilium 正在为端点分配一个唯一的身","title":"端点生命周期"},{"content":"集群网格将网络数据路径扩展到多个集群。它允许所有连接集群中的端点进行通信，同时提供完整的策略执行。负载均衡可通过 Kubernetes 注解获得。\n请参阅如何设置集群网格 的说明。\n","relpermalink":"/book/cilium-handbook/clustermesh/","summary":"集群网格将网络数据路径扩展到多个集群。它允许所有连接集群中的端点进行通信，同时提供完整的策略执行。负载均衡可通过 Kubernetes 注解获得。 请参阅如何设置集群网格 的说明。","title":"多集群（集群网格）"},{"content":"可观测性由 Hubble 提供，它可以以完全透明的方式深入了解服务的通信和行为以及网络基础设施。Hubble 能够在多集群（集群网格） 场景中提供节点级别、集群级别甚至跨集群的可视性。有关 Hubble 的介绍以及它与 Cilium 的关系，请阅读 Cilium 和 Hubble 简介 部分。\n默认情况下，Hubble API 的范围仅限于 Cilium 代理运行的每个单独节点。换句话说，网络可视性仅提供给本地 Cilium 代理观察到的流量。在这种情况下，与 Hubble API 交互的唯一方法是使用 Hubble CLI（hubble）查询通过本地 Unix Domain Socket 提供的 Hubble API。Hubble CLI 二进制文件默认安装在 Cilium 代理 pod 上。\n部署 Hubble Relay 后，Hubble 提供完整的网络可视性。在这种情况下，Hubble Relay 服务提供了一个 Hubble API，它在 ClusterMesh 场景中涵盖整个集群甚至多个集群。可以通过将 Hubble CLI（hubble）指向 Hubble Relay 服务或通过 Hubble UI 访问 Hubble 数据。Hubble UI 是一个 Web 界面，可以自动发现三层/四层甚至七层的服务依赖图，允许用户友好的可视化和过滤数据流作为服务图。\n下一章 ","relpermalink":"/book/cilium-handbook/concepts/observability/","summary":"可观测性由 Hubble 提供，它可以以完全透明的方式深入了解服务的通信和行为以及网络基础设施。Hubble 能够在多集群（集群网格） 场景中提供节点级别、集群级别甚至跨集群的可视性。有关 Hubble 的介绍以及它与 Cilium 的关系，请阅","title":"可观测性"},{"content":"Kubernetes 版本 以下列出的所有 Kubernetes 版本都经过 e2e 测试，并保证与此 Cilium 版本兼容。此处未列出的旧 Kubernetes 版本不支持 Cilium。较新的 Kubernetes 版本未列出，这取决于新版本的的向后兼容性。\n1.16 1.17 1.18 1.19 1.20 1.21 1.22 1.23 系统要求 Cilium 需要 Linux 内核 \u0026gt;= 4.9。有关所有系统要求的完整详细信息，请参阅系统要求 。\n在 Kubernetes 中启用 CNI CNI（容器网络接口）是 Kubernetes 用来委托网络配置的插件层。必须在 Kubernetes 集群中启用 CNI 才能安装 Cilium。这是通过将 --network-plugin=cni 参数在所有节点上传递给 kubelet 来完成的。有关更多信息，请参阅Kubernetes CNI 网络插件文档 。\n启用自动节点 CIDR 分配（推荐） Kubernetes 具有自动分配每个节点 IP CIDR 的能力。如果启用，Cilium 会自动使用此功能。这是在 Kubernetes 集群中处理 IP 分配的最简单方法。要启用此功能，只需在启动时添加以下标志 kube-controller-manager：\n--allocate-node-cidrs 此选项不是必需的，但强烈推荐。\n","relpermalink":"/book/cilium-handbook/kubernetes/requirements/","summary":"Kubernetes 版本 以下列出的所有 Kubernetes 版本都经过 e2e 测试，并保证与此 Cilium 版本兼容。此处未列出的旧 Kubernetes 版本不支持 Cilium。较新的 Kubernetes 版本未列出，这取决于新版本的的向后兼容性。 1.16 1.17 1.18 1.19 1.20 1.21 1.22 1.23 系统要求 Cilium 需要 Linux 内核 \u003e= 4.9。有","title":"要求"},{"content":"概念一章对 Cilium 和 Hubble 的所有方面进行了更深入的介绍。如果你想了解 Cilium 和 Hubble 的概要介绍，请参阅 Cilium 和 Hubble 简介 。\n本章大纲 组件概览\nCilium 术语说明\n可观测性\n阅读本章 ","relpermalink":"/book/cilium-handbook/concepts/","summary":"概念一章对 Cilium 和 Hubble 的所有方面进行了更深入的介绍。如果你想了解 Cilium 和 Hubble 的概要介绍，请参阅 Cilium 和 Hubble 简介 。 本章大纲 组件概览 Cilium 术语说明 可观测性 阅读本章","title":"Cilium 概念"},{"content":"在这一章中，让我们来谈谈编写 eBPF 代码。我们需要考虑在内核中运行的 eBPF 程序本身，以及与之交互的用户空间代码。\n内核和用户空间代码 首先，你可以用什么编程语言来编写 eBPF 程序？\n内核接受字节码形式的 eBPF 程序 1。人工编写这种字节码是可能的，就像用汇编语言写应用程序代码一样——但对人类来说，使用一种可以被编译（即自动翻译）为字节码的高级语言通常更实用。\n由于一些原因，eBPF 程序不能用任意的高级语言编写。首先，语言编译器需要支持发出内核所期望的 eBPF 字节码格式。其次，许多编译语言都有运行时特性——例如 Go 的内存管理和垃圾回收，使它们不适合。在撰写本文时，编写 eBPF 程序的唯一选择是 C（用 clang/llvm 编译）和最新的 Rust。迄今为止，绝大多数的 eBPF 代码都是用 C 语言发布的，考虑到它是 Linux 内核的语言，这是有道理的。\n至少，用户空间的程序需要加载到内核中，并将其附加到正确的事件中。有一些实用程序，如 bpftool，可以帮助我们解决这个问题，但这些都是低级别的工具，假定你有丰富的 eBPF 知识，它们是为 eBPF 专家设计的，而不是普通用户。在大多数基于 eBPF 的工具中，有一个用户空间的应用程序，负责将 eBPF 程序加载到内核中，传入任何配置参数，并以用户友好的方式显示 eBPF 程序收集的信息。\n至少在理论上，eBPF 工具的用户空间部分可以用任何语言编写，但在实践中，有一些库只支持相当少的语言。其中包括 C、Go、Rust 和 Python。这种语言的选择更加复杂，因为并不是所有的语言都有支持 libbpf 的库，libbpf 已经成为使 eBPF 程序在不同版本的内核中可移植的流行选择。(我们将在 第四章 中讨论 libbpf）。\n附属于事件的自定义程序 eBPF 程序本身通常是用 C 或 Rust 编写的，并编入一个对象文件 2。这是一个标准的 ELF（可执行和可链接格式，Executable and Linkable Format）文件，可以用像 readelf 这样的工具来检查，它包含程序字节码和任何映射的定义（我们很快就会讨论）。如 图 3-1 所示，如果在前一章中提到的验证器允许的话，用户空间程序会读取这个文件并将其加载到内核中。\n图 3-1. 用户空间应用程序使用 bpf() 系统调用从 ELF 文件中加载 eBPF 程序到内核中 eBPF 程序加载到内核中时必须被附加到事件上。每当事件发生，相关的 eBPF 程序就会运行。有一个非常广泛的事件，你可以将程序附加到其中；我不会涵盖所有的事件，但以下是一些更常用的选项。\n从函数中进入或退出 你可以附加一个 eBPF 程序，在内核函数进入或退出时被触发。当前的许多 eBPF 例子都使用了 kprobes（附加到一个内核函数入口点）和 kretprobes（函数退出）的机制。在最新的内核版本中，有一个更有效的替代方法，叫做 fentry/fexit 3。\n请注意，你不能保证在一个内核版本中定义的所有函数一定会在未来的版本中可用，除非它们是稳定 API 的一部分，如 syscall 接口。\n你也可以用 uprobes 和 uretprobes 将 eBPF 程序附加到用户空间函数上。\nTracepoints 你也可以将 eBPF 程序附加到内核内定义的 tracepoints 4。通过在 /sys/kernel/debug/tracing/events 下查找机器上的事件。\nPerf 事件 Perf 5 是一个收集性能数据的子系统。你可以将 eBPF 程序挂到所有收集 perf 数据的地方，这可以通过在你的机器上运行 perf list 来确定。\nLinux 安全模块接口 LSM 接口在内核允许某些操作之前检查安全策略。你可能见过 AppArmor 或 SELinux，利用了这个接口。通过 eBPF，你可以将自定义程序附加到相同的检查点上，从而实现灵活、动态的安全策略和一些运行时安全工具的新方法。\n网络接口——eXpress Data Path eXpress Data Path（XDP）允许将 eBPF 程序附加到网络接口上，这样一来，每当收到一个数据包就会触发 eBPF 程序。它可以检查甚至修改数据包，程序的退出代码可以告诉内核如何处理该数据包：传递、放弃或重定向。这可以构成一些非常有效的网络功能 6 的基础。\n套接字和其他网络钩子 当应用程序在网络套接字上打开或执行其他操作时，以及当消息被发送或接收时，你可以附加运行 eBPF 程序。在内核的网络堆栈中也有称为 流量控制（traffic control） 或 tc 的 钩子，eBPF 程序可以在初始数据包处理后运行。\n一些功能可以单独用 eBPF 程序实现，但在许多情况下，我们希望 eBPF 代码能从用户空间的应用程序接收信息，或将数据传递给用户空间的应用程序。允许数据在 eBPF 程序和用户空间之间，或在不同的 eBPF 程序之间传递的机制被称为 map。\neBPF Map map 的开发是 eBPF 缩略语中的 e 代表 extended 重要区别之一。\nmap 是与 eBPF 程序一起定义的数据结构体。有各种不同类型的 map，但它们本质上都是键值存储。eBPF 程序可以读取和写入 map，用户空间代码也可以。map 的常见用途包括：\neBPF 程序写入关于事件的指标和其他数据，供用户空间代码以后检索。 用户空间代码编写配置信息，供 eBPF 程序读取并作出相应的行为。 eBPF 程序将数据写入 map，供另一个 eBPF 程序以后检索，允许跨多个内核事件的信息协调。 如果内核和用户空间的代码都要访问同一个映射，它们需要对存储在该映射中的数据结构体有一个共同的理解。这可以通过在用户空间和内核代码中加入定义这些数据结构体的头文件来实现，但是如果这些代码不是用相同的语言编写的，作者将需要仔细创建逐个字节兼容的结构体定义。\n我们已经讨论了 eBPF 工具的主要组成部分：在内核中运行的 eBPF 程序，加载和与这些程序交互的用户空间代码，以及允许程序共享数据的 map。为了更具体化，让我们看一个例子。\nOpensnoop 示例 在 eBPF 程序的例子中，我选择了 opesnnoop，一个可以显示任何进程所打开的文件的工具。这个工具的原始版本是 Brendan Gregg 最初在 BCC 项目 中编写的许多 BPF 工具之一，你可以在 GitHub 上找到。它后来被重写为 libbpf（你将在下一章见到它），在这个例子中，我使用 libbpf-tools 目录下的较新版本。\n当你运行 opensnoop 时，你将看到的输出在很大程度上取决于当时在虚拟机上发生了什么，但它应该看起来像这样。\nPID COMM\tFD\tERR\tPATH 93965 cat\t3\t0\t/etc/ld.so.cache 93965 cat\t3\t0\t/lib/x86_64-linux-gnu/libc.so.6 93965 cat\t3\t0\t/usr/lib/locale/locale-archive 93965 cat\t3\t0\t/usr/share/locale/locale.alias ... 每一行输出表示一个进程打开（或试图打开）一个文件。这些列显示了进程的 ID，运行的命令，文件描述符，错误代码的指示，以及被打开的文件的路径。\nOpensnoop 的工作方式是将 eBPF 程序附加到 open() 和 openat() 系统调用上，所有应用程序都必须通过这些调用来要求内核打开文件。让我们深入了解一下这是如何实现的。为了简洁起见，我们将不看每一行代码，但我希望这足以让你了解它是如何工作的（如果你对这么深的内容不感兴趣的话，请跳到下一章！）。\nOpensnoop eBPF 代码 eBPF 代码是用 C 语言编写的，在 opensnoop.bpf.c 文件中。在这个文件的开头，你可以看到两个 eBPF map 的定义 —— start 和 events：\nstruct { __uint(type, BPF_MAP_TYPE_HASH); __uint(max_entries, 10240); __type(key, u32); __type(value, struct args_t); } start SEC(\u0026#34;.maps\u0026#34;); struct { __uint(type, BPF_MAP_TYPE_PERF_EVENT_ARRAY); __uint(key_size, sizeof(u32)); __uint(value_size, sizeof(u32)); } events SEC(\u0026#34;.maps\u0026#34;); 当 ELF 对象文件被创建时，它包含了每个 map 和每个要加载到内核的程序的部分，SEC() 宏定义了这些部分。\n当我们研究这个程序的时候，你会看到，在系统调用被处理的时候，start map 被用来临时存储系统调用的参数 —— 包括被打开的文件的名称。events map 7 用于将事件信息从内核中的 eBPF 代码传递给用户空间的可执行程序。如 图 3-2 所示。\n图 3-2. 调用 open() 触发 eBPF 程序，将数据存储在 opensnoop 的 eBPF map 中 在 opensnoop.bpf.c 文件的后面，你会发现两个极其相似的函数：\nSEC(\u0026#34;tracepoint/syscalls/sys_enter_open\u0026#34;) int tracepoint__syscalls__sys_enter_open(struct trace_event_raw_sys_enter* ctx) 和\nSEC(\u0026#34;tracepoint/syscalls/sys_enter_openat\u0026#34;) int tracepoint__syscalls__sys_enter_openat(struct trace_event_raw_sys_enter* ctx) 有两个不同的系统调用用于打开文件 8：openat() 和 open()。它们是相同的，除了 openat() 有一个额外的参数是目录文件描述符，而且要打开的文件的路径名是相对于该目录而言的。同样，除了处理参数上的差异，opensnoop 中的两个函数也是相同的。\n正如你所看到的，它们都需要一个参数，即一个指向名为 trace_event_raw_sys_enter 结构体的指针。你可以在你运行的特定内核生成的 vmlinux 头文件中找到这个结构体的定义。编写 eBPF 程序之道包括找出每个程序接收的结构体作为其上下文，以及如何访问其中的信息。\n这两个函数使用一个 BPF 辅助函数来检索调用这个 syscall 的进程 ID：\nu64 id = bpf_get_current_pid_tgid(); 这段代码得到了文件名和传递给系统调用的标志，并把它们放在一个叫做 args 的结构体中：\nargs.fname = (const char *)ctx-\u0026gt;args[0]; args.flags = (int)ctx-\u0026gt;args[1]; 这个结构体被写入 start map 中，使用当前程序 ID 作为键。\n这就是 eBPF 程序在进入 syscall 时所做的一切。但在 opensnoop.bpf.c 中定义了另一对 eBPF 程序，当系统调用退出时被触发。\n这个程序和它的双胞胎 openat() 在函数 trace_exit() 中共享代码。你有没有注意到，所有被 eBPF 程序调用的函数的前缀都是 static __always_inline？这迫使编译器将这些函数的指令放在内联中，因为在旧的内核中，BPF 程序不允许跳转到一个单独的函数。新的内核和 LLVM 版本可以支持非内联的函数调用，但这是一种安全的方式，可以确保 BPF 验证器满意（现在还有一个 BPF 尾部调用的概念，即执行从一个 BPF …","relpermalink":"/book/what-is-ebpf/ebpf-programs/","summary":"在这一章中，让我们来谈谈编写 eBPF 代码。我们需要考虑在内核中运行的 eBPF 程序本身，以及与之交互的用户空间代码。 内核和用户空间代码 首先，你可以用什么编程语言来编写 eBPF 程序？ 内核接受字节码形式的 eBPF 程序 1。人工编写这","title":"第三章：eBPF 程序"},{"content":"现在我们将一些问题转移到了云中的 DevOps 平台。\n分解单体应用 传统的 n 层单体式应用部署到云中后很难维护，因为它们经常对云基础设施提供的部署环境做出不可靠的假设，这些假设云很难提供。例如以下要求：\n可访问已挂载的共享文件系统 P2P 应用服务器集群 共享库 配置文件位于常用的配置文件目录 大多数这些假设都出于这样的事实：单体应用通常都部署在长期运行的基础设施中，并与其紧密结合。不幸的是，单体应用并不太适合弹性和短暂（非长期支持）生命周期的基础设施。\n但是即使我们可以构建一个不需要这些假设的单体应用，我们依然有一些问题需要解决：\n单体式应用的变更周期耦合，使独立业务能力无法按需部署，阻碍创新速度。 嵌入到单体应用中的服务不能独立于其他服务进行扩展，因此负载更难于优化。 新加入组织的开发人员必须适应新的团队，经常学习新的业务领域，并且一次就熟悉一个非常大的代码库。这样会增加 3-6 个月的适应时间，才能实现真正的生产力。 尝试通过堆积开发人员来扩大开发组织，增加了昂贵的沟通和协调成本。 技术栈需要长期承诺。引进新技术太过冒险，可能会对整体产生不利影响。 细心的读者会注意到，该列表正好与“微服务”的列表相反。将组织分解为业务能力团队还要求我们将应用程序分解成微服务。只有这样，我们才能从云计算基础架构中获得最大的收益。\n分解数据 仅仅将单体应用分解为微服务还是远远不够的。数据模型必须要解耦。如果业务能力团队被认为是自主的，却被迫通过单一的数据存储进行协作，那么单体应用对创新的阻碍将依然存在。\n事实上，产品架构必须从数据开始的说法是有争议的。由 Eric Evans（Addison-Wesley）在领域驱动设计（DDD）中提出的原理认为，我们的成功在很大程度上取决于领域模型的质量（以及支持它的普遍存在的语言）。要使领域模型有效，还必须在内部一致 —— 我们不应该在同一模型内的一致定义中找到重复定义的术语或概念。\n创建不具有这种不一致的联合领域模型是非常困难和昂贵的（可以说是不可能的）。Evans 将业务的整体领域模型的内部一致性子集称为有界上下文。\n最近与航空公司客户合作时，我们讨论了他们业务的核心概念，自然是“航空公司预订”的话题。该集团可以在其预定业务中划分十七种不同的逻辑定义，几乎不能将它们调和为一个。相反，每个定义的所有细微差别都被仔细地描绘成一个个单一的概念，这将成为组织的巨大瓶颈。\n有界上下文允许你在整个组织中保持单一概念的不一致定义，只要它们在有界上下文中一致地定义。\n因此，我们首先需要确定可以在内部保持一致的领域模型的细分。我们在这些细分上画出固定的边界，划分出有界上下文。然后，我们可以将业务能力团队与这些环境相匹配，这些团队将构建提供这些功能的微服务。\n微服务提供了一个有用的定义，用于定义 12 因素应用程序应该是什么。12 因素主要是技术规范，而微服务主要是业务规范。通过定义有界上下文，为它们分配一组业务能力，委托业务能力团队对这些业务能力负责，并建立 12 因素应用程序。在这些应用程序可以独立部署的情况下，为业务能力团队的运维提供了一组有用的技术工具。\n我们将有界上下文与每个服务模式的数据库结合，每个微服务封装、管理和保护自己的领域模型和持久存储。在每个服务模式的数据库中，只允许一个应用程序服务访问逻辑数据存储，逻辑数据存储可能是以多租户集群中的单个 schema 或专用物理数据库中存在。对这些概念的任何外部访问都是通过一个明确定义的业务协议来实现的，该协议的实现方式为 API（通常是 REST，但可能是任何协议）。\n这种分解允许应用拥有多语言支持的持久性，或者基于数据形态和读写访问模式选择不同的数据存储。然而，数据必须经常通过事件驱动技术重新组合，以便请求交叉上下文。诸如命令查询责任隔离（CQRS）和事件溯源（Event Sourcing）之类的技术通常在跨上下文同步类似概念时很有帮助，这超出了本文的范围。\n容器化 容器镜像（例如通过 LXC、Docker 或 Rocket 项目准备的镜像）正在迅速成为云原生应用架构的部署单元。然后通过诸如 Kubernetes、Marathon 或 Lattice 等各种调度解决方案实例化这样的容器镜像。亚马逊和 Google 等公有云供应商也提供一流的解决方案，用于容器化调度和部署。容器利用现代的 Linux 内核原语，如控制组（cgroups）和命名空间来提供类似的资源分配和隔离功能，这些功能与虚拟机提供的功能相比，具有更少的开销和更强的可移植性。应用程序开发人员将需要将应用程序包装成容器镜像，以充分利用现代云基础架构的功能。\n从管弦乐编排到舞蹈编舞 不仅仅服务交付、数据建模和治理必须分散化，服务集成也是如此。企业服务集成传统上是通过企业服务总线（ESB）实现的。ESB 成为管理服务之间交互的所有路由、转换、策略、安全性和其他决策的所有者。我们将其称之为编排，类似于导演，它决定了乐团演出期间演奏音乐的流程。ESB 和编排可以产生非常简单和令人愉快的架构图，但它们的简单性仅仅是表面性的。在 ESB 中隐藏的是复杂的网络。管理这种复杂性成为全职工作，这成为应用开发团队的持续瓶颈。正如我们在联合数据模型所看到的，像 ESB 这样的联合集成解决方案成为阻碍幅度的巨大难题。\n诸如微服务这样的云原生架构更倾向于舞蹈，它们类似于芭蕾舞中的舞者。它们将心智放置在端点上，类似于 Unix 架构中的虚拟管道和智能过滤器，而不是放在集成机制中。当舞台上的情况与原来的计划有所不同时，没有导演告诉舞者该怎么做。相反，他们会自适应。同样，服务通过客户端负载均衡和断路器等模式，适应环境中不断变化的各种情况。\n虽然架构图看起来像一个庞杂的网络，但它们的复杂性并不比传统的 SOA 大。编排简单地承认并暴露了系统原有的复杂性。再次，这种转变是为了支持从云原生架构中寻求速度所需的自治。团队能够适应不断变化的环境，而无需承担与其他团队协调的开销，并避免了在集中管理的 ESB 中协调变更所造成的开销。\n","relpermalink":"/book/migrating-to-cloud-native-application-architectures/changes-needed/technical-change/","summary":"现在我们将一些问题转移到了云中的 DevOps 平台。 分解单体应用 传统的 n 层单体式应用部署到云中后很难维护，因为它们经常对云基础设施提供的部署环境做出不可靠的假设，这些假设云很难提供。例如以下要求： 可访问已挂载的共","title":"2.3 技术变革"},{"content":"所涉及的关键原语和实施任务是。\n管道和 CI/CD 管道的概念 CI/CD 管道的构建块 设计和执行 CI/CD 管道 自动化的战略 CI/CD 管道中对安全自动化工具的要求 3.3.1 管道的概念和 CI/CD 管道 DevSecOps 作为一种敏捷应用开发、部署和运维的方法论或框架，与其他方法论一样，是由 各个阶段 组成的。信息在各阶段中的顺序和流动被称为工作流，其中一些阶段可以平行执行，而其他阶段则必须遵循一个顺序。每个阶段可能需要调用一个独特的工作来执行该阶段的活动。\nDevSecOps 在流程工作流中引入的一个独特概念是 管道 的概念。有了管道，就不需要为启动 / 执行流程的每个阶段单独编写作业。相反，只有一个作业从初始阶段开始，自动触发与其他阶段有关的活动 / 任务（包括顺序的和并行的），并创建一个无错误的智能工作流程。\nDevSecOps 中的管道被称为 CI/CD 管道，这是基于它所完成的总体任务和它所包含的两个单独阶段。CD 可以表示持续交付或持续部署阶段。根据这后一个阶段，CI/CD 可以涉及以下任务：\n构建、测试、安全和交付：经过测试的修改后的代码被交付到暂存区。 构建、测试、安全、交付和部署：暂存区的代码会自动部署。 在前者中，自动化在交付阶段就结束了，接下来在托管平台基础设施中部署修改后的应用程序的任务是手动执行的。在后者中，部署也是自动化的。管道中任何阶段的自动化都可以通过将管道阶段表达为代码的工具来实现。\nCI/CD 管道的工作流程如下图 1 所示。\n图 1：CI/CD 管道工作流程 图中所示的单元和集成测试使用了第 3.2 节中描述的 SAST、DAST 和 SCA 工具。应该注意的是，当测试失败时，组织可以选择继续构建过程。根据组织如何平衡风险容忍度和业务需求，当一个特定的测试失败时，它可以选择 fail-open（记录并继续）或 fail-closed（停止 / 中断）。在失败关闭的情况下，开发人员得到测试结果报告，必须修复问题，并重新启动 CI 过程。\n持续集成涉及到开发人员经常将代码变化合并到一个中央存储库中，在那里运行自动化的构建和测试。构建是将源代码转换为可执行代码的过程，以便在其上运行的平台。在 CI/CD 管道软件中，开发者的修改是通过创建构建和运行针对构建的自动测试来验证的。这个过程避免了在等待发布日将修改合并到发布分支时可能发生的 集成挑战 。\n持续交付是持续集成之后的一个阶段，在这个阶段，代码的变化在构建阶段之后被部署到测试和 / 或暂存环境。持续交付到生产环境包括指定一个发布频率 —— 每天、每周、每两周或其他时期 —— 基于软件的性质或组织运营的市场。这意味着在自动化测试的基础上，有一个预定的发布过程，尽管应用程序可以通过点击一个按钮在任何时候被部署。持续交付中的部署过程的特点是手动，但诸如代码迁移到生产服务器、建立网络参数和指定运行时配置数据等任务可以由自动化脚本来完成。\n持续部署类似于持续交付，只是发布是 自动进行 的，代码的修改在完成后立即提供给客户。自动发布过程在很多情况下可能包括 A/B 测试，以促进新功能的缓慢上线，以便在出现错误 / 误差时减轻失败的影响。\n持续交付和持续部署之间的区别如图 2 中所示。\n图 2：持续交付和持续部署之间的区别 3.3.2 CI/CD 管道的构建块 定义 CI/CD 管道资源、构建管道和执行这些管道的主要软件是 CI/CD 管道软件。这类软件的架构可能有轻微的变化，取决于特定的产品。以下是对 CI/CD 工具（管道软件）运行情况的概述：\n一些 CI/CD 工具在应用程序和相关资源托管的平台上自然运行（即容器编排和资源管理平台），而其他工具需要通过其 API 集成到应用程序托管平台。使用应用托管平台原生的 CI/CD 工具的一些优势是： 它使部署、维护和管理 CI/CD 工具本身更加容易。 CI/CD 工具定义的每个管道都成为另一个平台原生资源，并以同样的方式管理。事实上，执行管道所需的所有实体，如任务和管道（然后分别作为其他实体的蓝图，如任务运行和管道运行），可以作为建立在平台原生资源之上的 自定义资源定义 （CRD）来创建。具有这种架构的软件可以被其他 CI/CD 管道软件产品所使用，以促进管道的快速定义。 一些 CI/CD 工具与代码库集成，以扫描 / 检查应用程序代码。这些类型的工具与每个应用程序和每个环境的代码库有关联。当应用程序模块、基础设施或配置发生变化时，它们被存储在这些代码库中。通过 webhooks 或其他方式连接到代码库的 CI/CD 管道软件在提交时被激活（推送工作流模型）或通过来自这些存储库的拉取请求。 一些 CI/CD 工具单独为本地平台执行 CD 功能（例如，Kubernetes 平台的 Jenkins X）或为多个技术栈执行 CD 功能（例如，多云部署的 Spinnaker）。这类工具的困难在于，它们可能缺乏完成 CI 功能的本地工具（例如，测试代码的工具，构建应用镜像，或将其推送到注册中心）。 3.3.3 准备和执行 CI/CD 管道 创建 CI/CD 管道的目的是为了实现源代码的频繁更新、重建，以及将更新的模块自动部署到生产环境中。\n涉及的 关键任务 是：\n确保 DevSecOps 平台中的所有单个组件（管道软件、SDLC 软件、代码库、可观测性工具等）都可用。 通过认证、验证或定制测试，确保这些组件的安全。 将 CI 和 CD 工具与 SDLC 工具整合起来 —— 访问令牌、调用脚本、管道定义。 根据部署环境（即内部或云端的应用程序托管平台），在 IaC 工具（与 GitOps）中设置配置细节 将运行时工具与部署环境相结合。 设计仪表盘并定义要监控的事件、要生成的警报和要监控的应用程序状态变量（如内存利用率等），通过与日志聚合器、指标生成器和跟踪生成器等工具的连接。 执行任务包括：\n设置源代码库。建立一个存储库（如 GitHub 或 GitLab），用于存储具有适当版本控制的应用程序源代码。 构建过程。使用自动代码构建工具，配置并执行生成可执行文件的构建过程（对于那些需要更新的代码部分）。 保证过程的安全。通过使用 SAST 和 DAST 工具进行单元测试，确保构建时没有静态和动态的漏洞。这和上面的任务是由 CI 工具激活的。 描述部署环境。这可能涉及描述（使用 IaC）物理 / 虚拟资源，以便在云或企业数据中心部署应用程序。 创建交付管道。创建一个将自动部署应用程序的管道。这个任务和前面的任务是由 CD 工具启用的。 测试代码并执行管道。在适当的测试之后，只要有新的代码出现在资源库中，就执行 CI 工具。当构建过程成功后，执行 CD 工具，将应用程序部署到暂存 / 生产环境中。 激活运行时工具和仪表板，启动运行时监控。 重申一下，CI/CD 流程的三个主要阶段是构建 / 测试、发布 / 打包和部署。以下功能将其转化为一个管道：\n当一个服务的源代码被更新时，推送到源代码库的代码变化会触发代码构建工具。\n代码开发环境或代码构建工具（如 IDE），通常与安全测试工具（如静态漏洞分析工具）集成，以促进安全编译代码工件的生成，从而将安全纳入 CI 管道。\n在代码构建工具中生成的编译代码工件会触发交付 / 打包工具，该工具可能与它自己的一套工具（例如，动态漏洞分析、动态渗透测试工具、用于识别所附库中的漏洞的软件组成分析工具）集成在一起，并且还创建与部署环境有关的配置参数。\n然后，发布 / 打包工具的输出被自动送入 CD 工具，该工具将软件包部署到 所需的环境 中（例如，中转、生产）。\nCI/CD 管道的工作流程不应产生无人参与的印象。以下团队 / 角色对 CI/CD 管道做出 贡献 ：\n开发团队：这个团队的成员为他们的应用程序申报第三方现成软件（OSS）的依赖性，审查 DevSecOps 系统围绕脆弱依赖性提出的建议，按照建议进行更新，并编写足够的测试案例以确保所有功能验证（消除运行时的错误）。 首席信息安全官（CISO）：在与安全团队协商后，CISO 定义了 DevSecOps 系统的整体范围（深度和广度），从而可以适当地配置它，以满足应用程序的关键任务需求。 安全团队：这个团队的成员按照最佳实践创建管道，包括执行所有必要的安全功能的任务（例如，SBOM 生成、漏洞扫描、代码构建、代码签名、引入新的测试工具、进行审计等）。具体来说，在某些情况下，安全团队的成员可能会负责设计、构建和维护策略即代码和相关管道。 基础设施团队：这个团队的成员创建、维护和升级基础设施。 QA 团队：这个团队的成员开发集成测试案例。 部署团队 / 发布团队：这个团队的成员为各种环境（UAT/PreProd/Prod）创建管道和包，并为这些环境进行适当的配置和供应。 这些团队进行的许多活动中，有一些包括 CI/CD 管道中采用的工具的定制、更新和增强（例如，用最新的已知漏洞数据库更新静态漏洞分析工具）。在手工操作过程中，应谨慎行事，以免阻塞管道。在设定平均生产时间目标的同时，还应通过使用 “合并（GitLab）或拉取（GitHub）请求” 和这些请求的多个批准者来减少风险，如内部威胁。这个管道由发布后团队设计、维护和执行，该团队 —— 除了监控功能 —— 还执行 其他流程 ，如合规管理、备份流程和资产跟踪。\n3.3.4 自动化的战略 与其他涉及从编码到发布的线性流程的软件开发模式相比，DevOps 使用了一个具有交付管道的前向过程（即构建 / 安全、交付 / 打包和发布）和一个具有反馈回路的反向过程（即计划和监控），形成一个递归的工作流程。自动化在这些活动中的作用是改善工作流程。持续集成强调测试自动化，以确保每当新的提交被集成到主分支时，应用程序不会被破坏。自动化带来了以下好处：\n产生有关软件静态和运行时流程的数据。 减少开发和部署时间。 架构的内置安全、隐私和合规性。 以下是推荐的 自动化策略 ，以便于更好地利用组织资源，并在高效、安全的应用环境方面获得最大利益。\n选择要自动化的活动。例如，以下是测试活动自动化的有效候选项目：\n测试其功能需符合法规要求的模块（如 PCI-DSS、HIPAA、Sarbanes-Oxley）。 具有中度至高度重复性的任务。 测试执行时间序列操作的模块，如消息发布者和消息订阅者。 测试涉及跨越多个服务的事务的工作流程（例如，请求跟踪）。 测试那些资源密集型和可能成为性能瓶颈的服务。 在根据上述标准选择自动化的候选者后，必须应用通常的风险分析来选择一个子集，以提供最佳的回报，并使理想的安全指标（如深度防御）最大化。一些推荐的策略包括：\n使用每年节省的小时数的成本效益比来确定哪些 流程 需要自动化的优先次序。 使用 关键绩效指标（KPI） （例如，识别故障或问题、纠正或恢复的平均时间）作为标记来完善 DevSecOps 流程。 根据应用，对基础设施服务应用不同的权重（例如，授权和其他策略的执行，监测系统状态以确保安全运行状态，在系统可用性、延迟、从中断中恢复的平均时间等方面的网络弹性），以确定对 DevSecOps 流程的资源分配。 3.3.5 CI/CD 管道中对安全自动化工具的要求 在 CI/CD 管道中使用的各种功能（如静态漏洞分析、动态漏洞分析、软件构成分析）的安全自动化工具需要有不同的接口和警报 / 报告要求，因为它们必须根据使用管道阶段（如构建、打包、发布）而无缝运行。这些要求是：\n安全自动化工具应与集成开发环境（IDE）工具一起工作，并帮助开发人员优先处理和补救静态漏洞。需要这些功能来促进开发人员的采用和提高生产力。 安全自动化工具应该是灵活的，以支持特定的工作流程，并为安全服务提供扩展能力。 在构建阶段进行静态漏洞检查的工具确保了数据流的安全，而那些进行动态漏洞检查的工具则确保了运行时应用程序状态的安全。 必须提到的是，安全自动化工具是有成本的，因此，这些工具的使用程度是基于风险因素分析。\n下一章 ","relpermalink":"/book/service-mesh-devsecops/devsecops/key-primitives-and-implementation-tasks/","summary":"所涉及的关键原语和实施任务是。 管道和 CI/CD 管道的概念 CI/CD 管道的构建块 设计和执行 CI/CD 管道 自动化的战略 CI/CD 管道中对安全自动化工具的要求 3.3.1 管道的概念和 CI/CD 管道 DevSecOps 作为一种敏捷应用开发、部署和运维的方法论或框架，与其他方法","title":"3.3 DevSecOps 关键原语和实施任务"},{"content":"本章中我们讨论了两种帮助我们迁移到云原生应用架构的方法：\n分解原架构\n我们使用以下方式分解单体应用：\n所有新功能都使用微服务形式构建。 通过隔离层将微服务与单体应用集成。 通过定义有界上下文来分解服务，逐步扼杀单体架构。 使用分布式系统\n分布式系统由以下部分组成：\n版本化，分布式，通过配置服务器和管理总线刷新配置。 动态发现远端依赖。 去中心化的负载均衡策略 通过熔断器和隔板阻止级联故障 通过 API 网关集成到特定的客户端上 还有很多其他的模式，包括自动化测试、持续构建与发布管道等。欲了解更多信息，请阅读 Toby Clemson 的《Testing Strategies in a Microservice Architecture》，以及 Jez Humbl 和 David Farley（AddisonWesley）的《Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation》。\n","relpermalink":"/book/migrating-to-cloud-native-application-architectures/migration-cookbook/summary/","summary":"本章中我们讨论了两种帮助我们迁移到云原生应用架构的方法： 分解原架构 我们使用以下方式分解单体应用： 所有新功能都使用微服务形式构建。 通过隔离层将微服务与单体应用集成。 通过定义有界上下文来分解服务，逐步扼杀","title":"3.3 本章小结"},{"content":"为应用程序分配基础设施的传统方法包括最初用配置参数和持续的任务配置计算和网络资源，如补丁管理（如操作系统和库），建立符合合规法规（如数据隐私），并进行漂移（当前配置不再提供预期的操作状态）纠正。\n基础设施即代码（IaC）是一种声明式的代码，它对计算机指令进行编码，这些指令封装了通过服务的管理 API 在公共云服务或私有数据中心部署虚拟基础设施所需的 参数 。换句话说，基础设施是以声明式的方式定义的，并使用用于应用程序代码的相同的源代码控制工具（如 GitOps）进行版本控制。根据特定的 IaC 工具，这种语言可以是脚本语言（如 JavaScript、Python、TypeScript 等）或专有配置语言（如 HCL），可能与标准化语言（如 JSON）兼容也可能不兼容。基本指令包括告诉系统如何配置和管理 基础设施 （无论是单个计算实例还是完整的服务器，如物理服务器或虚拟机）、容器、存储、网络连接、连接拓扑和负载均衡器。在某些情况下，基础设施可能是短暂的，基础设施的寿命（无论是不可变的还是可变的）不需要继续配置管理。配置可以与应用程序代码的单个提交相联系，使用的工具可以将应用程序代码和基础设施代码以一种合乎逻辑、富有表现力、为开发和运维团队所熟悉的方式连接起来，其中应用程序代码越来越多地定义了云应用的基础设施资源 要求 。\n因此，IaC 涉及编码所有的软件部署任务（分配服务器的类型，如裸机、虚拟机或容器，服务器的资源内容）和这些服务器及其网络的配置。包含这种代码类型的软件也被称为资源管理器或部署管理器。换句话说，IaC 软件可以自动管理整个 IT 基础设施的生命周期（资源的配置和取消配置），并实现一个可编程的基础设施。将这种软件作为 CI/CD 管道的一部分进行整合，不仅可以实现敏捷的部署和维护，还可以实现安全和满足性能需求的强大应用平台。\n4.3.1 对 IaC 的保护 当基础设施是 IaC 中的代码时，它可能包括有可能成为漏洞的 bug 和疏忽，因此，就像在应用程序代码中一样被利用。因此，保护 IaC 就是保护基础设施的定义和最终的部署环境。任何一段 IaC 在进入 GitOps 并被合并之前，都必须进行潜在漏洞的扫描。\n此外，只有当有一个有条不紊的漂移管理过程时，才能获得安全应用平台的保证。只有当 IaC 中定义的架构是部署环境中实际存在的架构时，才能获得这种保证，因为这种等同性可能会被通过控制台或 CLI 进行的无意或有意的更改所改变，从而绕过 IaC。确保这种对等性必须在部署后立即进行，并在运行期间定期进行，因为对架构的任何改变都可能导致引入安全设计缺陷，并可能需要对 IaC 进行修改。\n4.3.2 配置和基础设施之间的区别 基础设施经常与 配置 相混淆，后者将计算机系统、软件、依赖关系和设置维持在一个理想的、一致的状态。例如，将一台新购买的服务器放到机架上，并将其连接到交换机上，使其与现有网络相连（或启动一个新的虚拟机并为其分配网络接口），属于基础设施的定义。相反，在服务器启动后，安装 HTTPS 服务器并对其进行配置属于配置管理。\n","relpermalink":"/book/service-mesh-devsecops/implement/ci-cd-pipeline-for-infrastructure-as-code/","summary":"为应用程序分配基础设施的传统方法包括最初用配置参数和持续的任务配置计算和网络资源，如补丁管理（如操作系统和库），建立符合合规法规（如数据隐私），并进行漂移（当前配置不再提供预期的操作状态）纠正。 基础设","title":"4.3 基础设施即代码的 CI/CD 管道"},{"content":"基础架构是指支持应用程序的所有软件和硬件，包括数据中心、操作系统、部署流水线、配置管理以及支持应用程序生命周期所需的任何系统或软件。\n已经有无数的时间和金钱花在了基础架构上。通过多年来不断的技术演化和实践提炼，有些公司已经能够运行大规模的基础架构和应用程序，并且拥有卓越的敏捷性。高效运行的基础架构可以使得迭代更快，缩短投向市场的时间，从而加速业务发展。\n使用云原生基础架构是有效运行云原生应用程序的要求。如果没有正确的设计和实践来管理基础架构，即使是最好的云原生应用程序也会浪费。本书中的实践并不一定需要有巨大的基础架构规模，但如果您想从云计算中获取回报，您应该听从开创了这些模式的人的经验。\n在我们探索如何构建云中运行的应用程序的基础架构之前，我们需要了解我们是如何走到这一步。首先，我们将讨论采用云原生实践的好处。接下来，我们将看一下基础架构的简历，然后讨论下一阶段的功能，称为“云原生”，以及它与您的应用程序、运行的平台及业务之间的关系。\n在明白了这一点后，我们将向您展示解决方案及实现。\n云原生的优势 采用本书中的模式有很多好处。它们仿照谷歌、Netflix 和亚马逊这些成功的公司 —— 不是单靠模式保证它们的成功，而是它们提供了这些公司成功所需的可扩展性和敏捷性。\n通过选择在公有云中运行基础架构，您可以更快地创造价值并专注于业务目标。只需构建您的产品所需的内容，并从其他提供商那里获得服务，就可以缩短交付时间，提高灵活性。有些人可能因为“供应商锁定”而犹豫不决，但最糟糕的锁定是您自己建立的锁定。有关不同类型的锁定，以及您应该如何处理的更多信息，请参阅附录 B。\n消费服务还可让您使用所需服务构建定制平台（有时称为服务即平台 [SaaP]）。当您使用云托管的服务时无需精专于管理应用程序所需要的每项服务。这极大地加强了业务变更和业务增值的能力。\n当您无法使用服务时，您应该构建应用程序来管理基础架构。当您这样做时，规模瓶颈不再取决于每个运维工程师可以管理多少台服务器。相反，您可以像扩展应用程序一样来扩展您的基础架构。换句话说，如果您能够运行可扩展的应用程序，则可以使用应用程序扩展您的基础架构。\n同样的好处适用于构建灵活且易于调试的基础架构。您可以使用与管理业务应用程序相同的工具来洞察您的基础架构。\n云原生实践还可以缩小传统工程角色之间的差距（DevOps 的共同目标）。系统工程师将能够从应用程序中学习最佳实践，开发工程师可以拥有应用程序运行所在的基础架构的所有权。\n云原生基础架构的解决方案不一定适用于所有问题，您有责任了解它是否适合您的环境（参见第 2 章）。然而，在创造了这些实践的公司以及采用以该模式创建的工具的公司中，云原生基础架构的成功可以说显而易见。请参见附录 C 的一个例子。\n在深入了解解决方案之前，先让我是探究一下是什么问题导致这些模式的出现。\n服务器 在互联网早期，Web 基础架构始于物理服务器。服务器庞大，吵闹且昂贵，需要大量的电力和人员投入以保持它们的运行。需要细心照料，尽可能保持长时间运行。与云基础架构相比，购买这些设备让应用程序运行在上面会更困难。\n一旦您买了服务器，它就是您的了，无论好坏，都要维护。使用物理服务器适合已确定成本的业务。持有物理服务器并运行的时间越长，您花费的钱越多。做适当的产能规划并确保您获得最佳的投资回报是很重要的。\n物理服务器非常棒，因为它们功能强大，可以根据需要进行配置。故障率相对较低，使用冗余电源供应，风扇和 RAID 控制器来避免出现故障。也可以持续运行很长时间。企业可以通过延长保修和更换零部件，从购买的硬件中挤出额外的价值。\n但是，物理服务器会导致浪费。服务器不仅没有被充分利用，而且还带来了很多开销。在同一台服务器上运行多个应用程序是很困难的。当在同一台服务器上最大限度得部署多个应用程序时，软件冲突，网络路由和用户访问都变得更加复杂。\n硬件虚拟化承诺可以解决其中的一些问题。\n虚拟化 虚拟化使用软件来模拟物理服务器的硬件。虚拟服务器可以按需创建，完全可以通过软件编程，只要您可以模拟硬件，就永远不会出现损耗。\n使用 hypervisor 可以增加这些优势，因为您可以在物理服务器上运行多个虚拟机（VM）。它还使得应用程序可移植，因为您可以将虚拟机从一台物理服务器移动到另一台物理服务器。\n然而，运行自己的虚拟化平台的一个问题是虚拟机仍然需要硬件来运行。公司仍然需要拥有运行物理服务器所需的所有人员和流程，但是现在容量规划变得更加困难，因为他们还必须考虑到虚拟机的开销。至少，公有云出现之前就是如此。\n基础架构即服务 基础架构即服务（IaaS）是云提供商的众多产品之一。它提供了原始的网络、存储和计算能力，客户可以根据需要使用它们。它还包括一些支持服务，如身份和访问管理（IAM）、供应和库存系统。\nIaaS 允许公司摆脱他们的所有硬件，并从别人那里租用虚拟机或物理服务器。这释放了大量人力资源，摆脱了购买、维护以及在某些情况下容量规划所需的流程。\nIaaS 从根本上改变了基础架构与业务的关系。不是随着时间的推移受益的资本支出，而是运营业务的运营支出。企业可以像支付电力和人们的时间一样支付基础架构。通过基于消费的计费，您越早摆脱基础架构，运营成本就越低。\n托管的基础架构还为客户提供了可消费的 HTTP 应用编程接口（API），以便按需创建和管理基础架构。工程师不需要购买订单并等待物品出货，就可以进行 API 调用，并创建服务器。服务器可以轻松删除和丢弃。\n在云中运行基础架构不会使您的基础架构成为云原生。IaaS 仍然需要基础架构管理。在购买和管理物理资源之外，您可以（也有许多公司）认为 IaaS 与过去购买服务器在自己的数据中心架设的传统基础架构一模一样。\n即使没有“货架和堆叠”，仍然有大量的操作系统、监控软件和支持工具。自动化工具帮助减少了运行应用程序所需的时间，但通常根深蒂固的流程会削弱 IaaS 的优势。\n平台即服务 就像 IaaS 对 VM 消费者隐藏了物理服务器一样，平台即服务（PaaS）也对应用程序隐藏了操作系统。开发人员编写应用程序代码并定义应用程序的依赖关系，平台负责创建运行，管理和暴露它所必要的基础架构。与需要基础架构管理的 IaaS 不同，PaaS 中的基础架构由平台提供商管理。\n事实证明，PaaS 限制要求开发人员以不同的方式编写应用程序，以便平台可以有效管理。应用程序必须包含允许由平台管理而不访问底层操作系统的功能。工程师不能再依赖 SSH 登入到服务器来读取磁盘上的日志文件。现在应用程序的生命周期和管理由 PaaS 控制，工程师和应用程序需要适应这个流程。\n这些限制带来了很大的好处。应用程序开发周期变短了，因为工程师不需要花时间管理基础架构。在平台上运行的应用程序是我们现在称为“云原生应用程序”的开始。利用代码中的平台限制，已经一定程度上改变了当今编写应用程序的方式。\n12 因素应用程序\nHeroku 是提供公有 PaaS 的早期先驱之一。通过自己平台的多年扩展，该公司能够确定帮助应用程序在其环境中更好运行的模式。Heroku 定义了应用程序时应实现的 12 个主要因素。\n这 12 个因素是通过将代码逻辑与数据分离来使开发人员更高效，尽可能自动化，独立的构建、传输和运行阶段过程；并声明所有的应用程序的依赖关系。\n如果您使用 PaaS 提供商所提供的基础架构，恭喜您已拥有云原生基础架构的诸多优势。这包括 Google App Engine、AWS Lambda 和 Azure Cloud Services 等平台。任何成功的云原生基础架构都将向应用工程师展示自助服务平台，以部署和管理代码。\n但是，许多 PaaS 平台不足以满足业务需求。它们通常会限制平台运行的语言、库和功能以实现从应用程序中抽离基础架构的承诺。公有 PaaS 提供商还将限制哪些服务可以与应用程序集成以及这些应用程序可以在哪里运行。\n公有平台牺牲了应用程序的灵活性，使基础架构成为别人的问题。图 1-1 是如果您运行自己的数据中心，在 IaaS 中创建基础架构，在 PaaS 上运行应用程序或通过软件即服务（SaaS）运行应用程序时需要管理的组件的直观表示。\n您需要运行的基础架构组件越少越好；但是在公有 PaaS 提供商中运行所有应用程序可能不是一种选择。\n图 1-1. 基础架构层 云原生基础架构 “云原生”是一个过度使用的术语。尽管它已被市场所劫持，但仍具有工程和管理上的意义。对我们来说，它意味着在这个公有云提供商的世界中正发生的技术变革。\n云原生基础架构是隐藏在有用的抽象背后的基础架构，由 API 控制，由软件管理并具有运行应用程序的目的。利用这些特征运行基础架构，能以可扩展高效的方式管理该基础架构。\n当它们成功地向消费者隐藏复杂性时，抽象是有用的。它们可以实现技术的更复杂的使用，但是它们也限制了技术的使用方式。它们适用于底层技术，例如 TCP 如何提取 IP 或更高级别的技术，如虚拟机如何抽象物理服务器。抽象应该总是允许消费者“向上移动堆栈”而不是重新实现底层。\n云原生基础架构需要抽象基础 IaaS 产品以提供自己的抽象。新层负责控制它下面的 IaaS，并将自己的 API 暴露给消费者控制。\n由软件管理的基础架构是云中的一个关键区别点。软件控制的基础架构使基础架构能够扩展，并且在弹性、供应和可维护性方面也发挥着重要作用。软件需要了解基础架构的抽象概念，并知道如何获取抽象资源并相应地在可消费的 IaaS 组件中实现它。\n这些模式不仅影响基础架构的运行方式，而且在云原生基础架构上运行的应用程序类型、在其上工作的人员类型与传统基础架构中是不同的。\n如果云原生基础架构看起来很像 PaaS 产品，那么我们如何才能知道构建自己的产品时需要注意什么？我将快速描述一些领域，它们可能看起来像是云原生解决方案，但不提供云原生基础架构的所有方面。\n什么不是云原生基础架构？ 云原生基础架构不等于在公有云上运行基础架构。仅租用服务器并不会使您的基础架构云原生化。管理 IaaS 的流程与运行物理数据中心通常没有什么不同，许多将现有基础架构迁移到云的公司都未能获得回报。\n云原生不等于在容器中运行应用程序。Netflix 最先推出云原生基础架构时，几乎所有应用程序都部署在虚拟机中，而不是在容器中。使用容器的方式打包应用程序并不能意味着拥有了自治系统的可扩展性和优势。即使应用程序是通过持续集成和持续交付渠道自动构建和部署的，也不等于就可以从 API 驱动部署的基础架构中受益。\n也不是说只要您运行了容器编排器（例如 Kubernetes 和 Mesos）就是云原生架构。容器编排器提供了云原生基础架构所需的平台功能，但如果未按预期方式使用这些功能的话，那也只是将应用程序会动态调度到一组服务器上而已。这是一个非常好的起步，但仍有很多工作要做。\n调度器与编排器\n“调度器”和“编排器”这两个术语通常可以互换使用。\n在大多数情况下，编排器负责集群中的所有资源使用（例如：存储，网络和 CPU）。该术语通常用于描述执行许多任务的产品，如健康检查和云自动化。\n调度器是编排平台的一个子集，仅负责为进程和服务选择所运行的服务器。\n云原生不等于微服务或基础架构即代码。微服务意味着更快的开发周期和更小的独立功能，但是单体应用程序可以具有相同的功能，使其能够通过软件有效管理，并且还可以从云原生基础架构中受益。\n基础架构即代码以机器可解析的语言或领域特定语言（DSL）定义、使基础架构自动化。使用代码管理基础架构的传统工具包括配置管理工具，例如 Chef 和 Puppet。这些工具在自动执行任务和提供一致性方面很有用，但是对于为超出单个服务器的基础架构提供必要的抽象描述方面存在缺陷。\n配置管理工具一次只自动化一台服务器，并靠人将服务器提供的功能绑定在一起。人成了管理大规模基础架构的潜在瓶颈。这些工具也不会使构建完整系统所需的云基础架构（例如存储和网络）的额外部分自动化。\n尽管配置管理工具为操作系统的资源（例如软件包管理器）提供了一些抽象，但它们对底层操作系统的抽象 …","relpermalink":"/book/cloud-native-infra/what-is-cloud-native-infrastructure/","summary":"基础架构是指支持应用程序的所有软件和硬件，包括数据中心、操作系统、部署流水线、配置管理以及支持应用程序生命周期所需的任何系统或软件。 已经有无数的时间和金钱花在了基础架构上。通过多年来不断的技术演化和实","title":"第 1 章：什么是云原生基础架构？"},{"content":" 从客户给我们下达订单开始，一直到我们收到现金为止，我们一直都关注时间线。而且我们正在通过删除非附加值的废物来减少这个时间表。\n—— Taichi Ohno\nTaichi Ohno 被公认为精益制造之父。虽然精益制造的实践无法完全适用于软件开发领域，但它们的原则是一致的。这些原则可以指导我们很好地寻求典型的企业 IT 组织采用云原生应用架构所需的变革，并且接受作为这一转变所带来的部分的文化和组织转型。\n开始阅读 ","relpermalink":"/book/migrating-to-cloud-native-application-architectures/changes-needed/","summary":"从客户给我们下达订单开始，一直到我们收到现金为止，我们一直都关注时间线。而且我们正在通过删除非附加值的废物来减少这个时间表。 —— Taichi Ohno Taichi Ohno 被公认为精益制造之父。虽然精益制造的实践无法完全适用于软件开发领","title":"第二章：在变革中前行"},{"content":"现在我们已经定义了云原生应用架构，并简要介绍了企业在采用它们时必须考虑做出的变化，现在是深入研究技术细节的时候了。对每个技术细节的深入讲解已经超出了本报告的范围。本章中仅是对采用云原生应用架构后，需要做的特定工作和采用的模式的一系列简短的介绍，文中还给出了一些进一步深入了解这些方法的链接。\n开始阅读 ","relpermalink":"/book/migrating-to-cloud-native-application-architectures/migration-cookbook/","summary":"现在我们已经定义了云原生应用架构，并简要介绍了企业在采用它们时必须考虑做出的变化，现在是深入研究技术细节的时候了。对每个技术细节的深入讲解已经超出了本报告的范围。本章中仅是对采用云原生应用架构后，需要","title":"第三章：迁移指南"},{"content":"云原生应用由多个松散耦合的组件（称为微服务，通常以容器形式实现）组成，在需要零信任概念的无边界网络环境中运行（企业内部或云），并由来自不同地点的用户访问（例如，校园、家庭办公室等）。云原生应用不只是指在云中运行的应用。它们还指具有设计和运行时架构的一类应用，如微服务，以及用于提供所有应用服务（包括安全）的专用基础设施。将 零信任原则 纳入这类应用提供了一些技术，其中对所有受保护资源的访问是通过基于身份的保护和基于网络的保护（如微分）来强制执行的。\n由于业务原因，云原生应用程序需要敏捷和安全的更新和部署技术，以及应对网络安全事件的必要弹性。因此，它们需要一种与传统的单层或多层应用不同的应用开发、部署和运行时监控范式（统称为软件生命周期范式）。DevSecOps（开发、安全和运维）是这类应用的促进范式，因为它通过（a）持续集成、持续交付 / 持续部署（CI/CD）管道（在第 3 节中解释）等基本要素促进了敏捷和安全的开发、交付、部署和运维；（b）整个生命周期的安全测试；以及（c）运行时的持续监控，所有这些都由自动化工具支持。事实上，满足上述目标的范式最初被赋予了 DevOps 这个术语，以表明它试图消除开发和运维之间的隔阂，并促进（或推动）加强合作。后来，DevSecOps 这个词是由社区的一部分人创造的，以强调安全团队在整个过程中的作用。因此，DevSecOps 这个术语表示一种文化和一套带有自动化工具的实践，以推动负责交付软件的关键利益相关者（包括开发、运维和安全组织）之间加强协作、信任、分担责任、透明度、自主性、敏捷性和自动化。DevSecOps 拥有必要的基本要素和其他构建模块，以满足云原生应用的设计目标。\n应该注意的是，整个社区对 DevSecOps 一词并无共识。如前所述，该术语主要是为了强调一个事实，即必须在软件开发生命周期的所有阶段（即构建、测试、打包、部署和运行）对安全进行测试和整合。社区中的一部分人继续使用 DevOps 这个术语，理由是没有必要定义一个新的术语，因为安全必须是任何软件生命周期过程的一个组成部分。\n本章大纲 1.1 范围\n1.2 相关的 DevSecOps 倡议\n1.3 目标受众\n1.4 与其他 NIST 指导文件的关系\n1.5 本文件的组织\n开始阅读 ","relpermalink":"/book/service-mesh-devsecops/intro/","summary":"云原生应用由多个松散耦合的组件（称为微服务，通常以容器形式实现）组成，在需要零信任概念的无边界网络环境中运行（企业内部或云），并由来自不同地点的用户访问（例如，校园、家庭办公室等）。云原生应用不只是指","title":"第一章：简介"},{"content":"Kubernetes® 是一个开源系统，可以自动部署、扩展和管理在容器中运行的应用程序，并且通常托管在云环境中。与传统的单体软件平台相比，使用这种类型的虚拟化基础设施可以提供一些灵活性和安全性的好处。然而，安全地管理从微服务到底层基础设施的所有方面，会引入其他的复杂性。本报告中详述的加固指导旨在帮助企业处理相关风险并享受使用这种技术的好处。\nKubernetes 中三个常见的破坏源是供应链风险、恶意威胁者和内部威胁。\n供应链风险往往是具有挑战性的，可以在容器构建周期或基础设施收购中出现。恶意威胁者可以利用 Kubernetes 架构的组件中的漏洞和错误配置，如控制平面、工作节点或容器化应用程序。内部威胁可以是管理员、用户或云服务提供商。对组织的 Kubernetes 基础设施有特殊访问权的内部人员可能会滥用这些特权。\n本指南描述了与设置和保护 Kubernetes 集群有关的安全挑战。包括避免常见错误配置的加固策略，并指导国家安全系统的系统管理员和开发人员如何部署 Kubernetes，并提供了建议的加固措施和缓解措施的配置示例。本指南详细介绍了以下缓解措施：\n扫描容器和 Pod 的漏洞或错误配置。 以尽可能少的权限运行容器和 Pod。 使用网络隔离来控制漏洞可能造成的损害程度。 使用防火墙来限制不需要的网络连接，并使用加密技术来保护机密。 使用强大的认证和授权来限制用户和管理员的访问，以及限制攻击面。 使用日志审计，以便管理员可以监控活动，并对潜在的恶意活动发出警告。\n定期审查所有 Kubernetes 设置，并使用漏洞扫描，以帮助确保风险得到适当考虑并应用安全补丁。\n有关其他安全加固指导，请参见互联网安全中心 Kubernetes 基准、Docker 和 Kubernetes 安全技术实施指南、网络安全和基础设施安全局（CISA）分析报告以及 Kubernetes 文档。\n","relpermalink":"/book/kubernetes-hardening-guidance/executive-summary/","summary":"Kubernetes® 是一个开源系统，可以自动部署、扩展和管理在容器中运行的应用程序，并且通常托管在云环境中。与传统的单体软件平台相比，使用这种类型的虚拟化基础设施可以提供一些灵活性和安全性的好处。然","title":"执行摘要"},{"content":"眯着眼睛看图表并不是寻找相关性的最佳方式。目前在运维人员头脑中进行的大量工作实际上是可以自动化的。这使运维人员可以在识别问题、提出假设和验证根本原因之间迅速行动。\n为了建立更好的工具，我们需要更好的数据。遥测必须具备以下两个要求以支持高质量的自动分析：\n所有的数据点都必须用适当的索引连接在一个图上。 所有代表常见操作的数据点必须有明确的键和值。 在这一章中，我们将从一个基本的构件开始浏览现代遥测数据模型：属性。\n属性：定义键和值 最基本的数据结构是属性（attribute），定义为一个键和一个值。OpenTelemetry 的每个数据结构都包含一个属性列表。分布式系统的每个组件（HTTP 请求、SQL 客户端、无服务器函数、Kubernetes Pod）在 OpenTelemetry 规范中都被定义为一组特定的属性。这些定义被称为 OpenTelemetry 语义约定。表 2-1 显示了 HTTP 约定的部分列表。\n属性 类型 描述 示例 http.method string HTTP 请求类型 GET; POST; HEAD http.target string 在 HTTP 请求行中传递的完整的请求目标或等价物 /path/12314/ http.host string HTTP host header 的值。当标头为空或不存在时，这个属性应该是相同的。 www.example.org http.scheme string 识别所使用协议的 URI 方案 http; https http.status_code int HTTP 请求状态码 200 表 2-1：HTTP 规范的部分列表\n有了这样一个标准模式，分析工具就可以对它们所监测的系统进行详细的表述，同时进行细微的分析，而在使用定义不明确或不一致的数据时，是不可能做到的。\n事件：一切的基础 OpenTelemetry 中最基本的对象是事件（event）。事件只是一个时间戳和一组属性。使用一组属性而不是简单的消息 / 报文，可以使分析工具正确地索引事件，并使它们可以被搜索到。\n有些属性对事件来说是独一无二的。时间戳、消息和异常细节都是特定事件的属性的例子。\n然而，大多数属性对单个事件来说并不独特。相反，它们是一组事件所共有的。例如，http.target 属性与作为 HTTP 请求的一部分而记录的每个事件有关。如果在每个事件上反复记录这些属性，效率会很低。相反，我们把这些属性拉出到围绕事件的封装中，在那里它们可以被写入一次。我们把这些封装称为上下文（context）。\n有两种类型的上下文：静态和动态（如图 2-1 所示）。静态上下文定义了一个事件发生的物理位置。在 OpenTelemetry 中，这些静态属性被称为资源。一旦程序启动，这些资源属性的值通常不会改变。\n图 2-1：虽然事件有一些特定的事件属性，但大多数属性属于事件发生的上下文之一。 动态上下文定义了事件所参与的活动操作。这个操作层面的上下文被称为跨度（span）。每次操作执行时，这些属性的值都会改变。\n不是所有的事件都有两种类型的上下文。只有资源的自由浮动事件，如程序启动时发出的事件，被称为日志（log）。作为分布式事务的一部分而发生的事件被称为跨度事件（span event）。\n资源：观察服务和机器 资源（静态上下文）描述了一个程序正在消费的物理和虚拟信息结构。服务、容器、部署和区域都是资源。图 2-2 显示了一个典型的购物车结账事务中所涉及的资源。\n图 2-2：一个事务，被视为一组资源。 系统运行中的大多数问题都源于资源争夺，许多并发的事务试图在同一时间利用相同的资源。通过将事件放在它们所使用的资源的上下文中，就有可能自动检测出许多类型的资源争夺。\n像事件一样，资源可以被定义为一组属性。表 2-2 显示了一个服务资源的例子。\n表 2-2：服务资源的例子\n属性 类型 描述 示例 service.name string 服务的逻辑名称 shopping cart service.instance.id string 服务实例的 ID 627cc493- f310-47de-96bd-71410b7dec09 service.version string 服务 API 或者实现的版本号 2.0.0 除了识别机器所需的基本信息，配置设置也可以作为资源被记录下来。要访问一台正在运行的机器来了解它是如何配置的，这个负担太让人害怕了。相反，在配置文件中发现的任何重要信息也应该表示为一种资源。\n跨度：观察事务 跨度（动态上下文）描述计算机操作。跨度有一个操作名称，一个开始时间，一个持续时间，以及一组属性。\n标准操作是使用语义约定来描述的，比如上面描述的 HTTP 约定。但也有一些特定的应用属性，如 ProjectID 和 AccountID，可以由应用开发者添加。\n跨度也是我们描述因果关系的方式。为了正确记录整个事务，我们需要知道哪些操作是由其他哪些操作触发的。为了做到这一点，我们需要给跨度增加三个属性：TraceID、SpanID 和 ParentID，如表 2-3 所示。\n表 2-3：跨度的三个额外属性\n属性 类型 描述 示例 traceid 16 字节数组 识别整个事务 4bf92f3577b34da6a3ce929d0e0e4736 spanid 8 字节数组 识别当前操作 00f067aa0ba902b7 parentid 8 字节数组 识别父操作 53ce929d0e0e4736 这三个属性是 OpenTelemetry 的基础。通过添加这些属性，我们所有的事件现在可以被组织成一个图，代表它们的因果关系。这个图现在可以以各种方式进行索引，我们稍后会讨论这个问题。\n追踪：看似日志，胜过日志 我们现在已经从简单的事件变成了组织成与资源相关的操作图的事件。这种类型的图被称为追踪（trace）。图 2-3 显示了一种常见的可视化追踪方式，重点是识别操作的延迟。\n图 2-3：当日志被组织成一个图形时，它们就变成了追踪。 从本质上讲，追踪只是用更好的索引来记录日志。当你把适当的上下文添加到适当的结构化的日志中时，可以得到追踪的定义。\n想想你花了多少时间和精力通过搜索和过滤来收集这些日志；那是收集数据的时间，而不是分析数据的时间。而且，你要翻阅的日志越多，执行并发事务数量不断增加的机器堆积，就越难收集到真正相关的那一小部分日志。\n然而，如果你有一个 TraceID，收集这些日志只是一个简单的查询。通过 TraceID 索引，你的存储工具可以自动为你做这项工作；找到一个日志，你就有了该事务中的所有日志，不需要额外的工作。\n既然如此，为什么你还会要那些没有 “追踪\u0026#34;ID 的 “日志”？我们已经习惯了传统的日志管理迫使我们做大量的工作来连接这些点。但这些工作实际上是不必要的；它是我们数据中缺乏结构的副产品。\n分布式追踪不仅仅是一个测量延迟的工具；它是一个定义上下文和因果关系的数据结构。它是把所有东西联系在一起的胶水。正如我们将看到的，这种胶水包括最后一个支柱 —— 指标。\n指标：观察事件的总体情况 现在我们已经确定了什么是事件，让我们来谈谈事件的聚合。在一个活跃的系统中，同样的事件会不断发生，我们以聚合的方式查看它们的属性来寻找模式。属性的值可能出现得太频繁，或者不够频繁，在这种情况下，我们要计算这些值出现的频率。或者该值可能超过某个阈值，在这种情况下，我们想衡量该值是如何随时间变化的。或者我们可能想以直方图的形式来观察数值的分布。\n这些聚合事件被称为度量。就像普通的事件一样，度量有一组属性和一组语义上的便利条件来描述普通概念。表 2-4 显示了一些系统内存的例子属性。\n表 2-4：系统内存的属性\n属性 值类型 属性值 system.memory.usage int64 used, free, cached, other system.memory.utilization double used, free, cached, other 与事件相关的指标：统一的系统 传统上，我们认为指标是与日志完全分开的。但实际上它们是紧密相连的。例如，假设一个 API 有一个衡量每分钟错误数量的指标。那是一个统计数字。然而，每一个错误都是由一个特定的事务行为产生的，使用特定的资源。这些细节在我们每次递增该计数器时都会出现，我们想知道这些细节。\n当运维人员被提醒发现错误突然激增时，他们会想到的第一个问题自然是：“是什么导致了这个激增？” 看一下例子的追踪可以回答这个问题。在失败的事务中较早发生的事件（或未能发生的事件）可能是错误的来源。\n在 OpenTelemetry 中，当指标事件在跨度的范围内发生时，这些追踪的样本会自动与指标相关联，作为追踪的范例（trace examplar）。这意味着不需要猜测或寻找日志。OpenTelemetry 明确地将追踪和度量联系在一起。一个建立在 OpenTelemetry 上的分析工具可以让你从仪表盘上直接看到追踪，只需一次点击。如果有一个模式 —— 例如，一个特定的属性值与导致一个特定错误的追踪密切相关 —— 这个模式可以被自动识别。\n自动分析和编织 事件、资源、跨度、指标和追踪：这些都被 OpenTelemetry 连接在一个图中，并且它们都被发送到同一个数据库，作为一个整体进行分析。这就是下一代的可观测性工具。\n现代可观测性将建立在使用结构的数据上，这些结构允许分析工具在所有类型的事件和总量之间进行关联，这些关联将对我们如何实践可观测性产生深远影响。\n向全面观察我们的系统过渡将有许多好处。但我相信，这些新工具提供的主要省时功能将是各种形式的自动关联检测。在寻找根本原因时，注意到相关关系可以产生大量的洞察力。如图 2-4 所示，相关性往往是产生根本原因假设的关键因素，然后可以进一步调查。\n图 2-4：可能提供关键洞察力、指向根本原因的关联实例。 平均表现是什么样子的？异常值是什么样子的？哪些趋势在一起变化，它们的共同点是什么？相关性可能发生在许多地方：跨度中的属性之间、追踪中的跨度之间、追踪和资源之间、指标内部，以及所有这些地方都有。当所有这些数据被连接到一个图中时，这些相关性就可以被发现了。\n这就是为什么统一的数据编织是如此关键。任何自动匹配的分析的价值完全取决于被分析的数据的结构和质量。机器遍历数据的图表；它们不会进行逻辑的飞跃。准确的统计分析需要一个有意设计的遥测系统来支持它。\n重点：自动分析为您节省时间 为什么我们关心相关性分析的自动化？因为时间和复杂性对我们不利。随着系统规模的扩大，它们最终变得太复杂了，任何操作者都无法完全掌握系统的情况，而且在建立一个假设时，永远没有足够的时间来调查每一个可能的联系。\n问题是，选择调查什么需要直觉，而直觉往往需要对组成分布式系统的每个组件有深刻的了解。随着企业系统的增长和工程人员的相应增加，任何一个工程师对每个系统深入了解的部分自然会缩减到整个系统的一小部分。直觉并不能很好地扩展。\n直觉也极易被误导；问题经常出现在意想不到的地方。根据定义，可以预见的问题几乎不会经常发生。剩下的就是所有未曾预料到的问题了，这些问题已经超出了我们的直觉。\n这就是自动关联检测的作用。有了正确的数据，机器可以更有效地检测出相关的关联。这使得运维人员能够快速行动，反复测试各种假设，直到他们知道足够的信息来制定解决方案。\n","relpermalink":"/book/opentelemetry-obervability/the-value-of-structured-data/","summary":"第 2 章：结构化数据的价值","title":"第 2 章：结构化数据的价值"},{"content":"总结 现在您已经知道了 Code Review 要点 ，那么管理分布在多个文件中的评论的最有效方法是什么？\n变更是否有意义？它有很好的描述吗？ 首先看一下变更中最重要的部分。整体设计得好吗？ 以适当的顺序查看 CL 的其余部分。 第一步：全面了解变更 查看 CL 描述 和 CL 大致上用来做什么事情。这种变更是否有意义？如果在最初不应该发生这样的变更，请立即回复，说明为什么不应该进行变更。当您拒绝这样的变更时，向开发人员建议应该做什么也是一个好主意。\n例如，您可能会说“看起来你已经完成一些不错的工作，谢谢！但实际上，我们正朝着删除您在这里修改的 FooWidget 系统的方向演进，所以我们不想对它进行任何新的修改。不过，您来重构下新的 BarWidget 类怎么样？“\n请注意，审查者不仅拒绝了当前的 CL 并提供了替代建议，而且他们保持礼貌地这样做。这种礼貌很重要，因为我们希望表明，即使不同意，我们也会相互尊重。\n如果您获得了多个您不想变更的 CL，您应该考虑重整开发团队的开发过程或外部贡献者的发布过程，以便在编写 CL 之前有更多的沟通。最好在他们完成大量工作之前说“不”，避免已经投入心血的工作现在必须被抛弃或彻底重写。\n第二步：检查 CL 的主要部分 查找作为此 CL“主要”部分的文件。通常，包含大量的逻辑变更的文件就是 CL 的主要部分。先看看这些主要部分。这有助于为 CL 的所有较小部分提供上下文，并且通常可以加速代码审查。如果 CL 太大而无法确定哪些部分是主要部分，请向开发人员询问您应该首先查看的内容，或者要求他们将 CL 拆分为多个 CL 。\n如果在该部分发现存在一些主要的设计问题时，即使没有时间立即查看 CL 的其余部分，也应立即留下评论告知此问题。因为事实上，因为该设计问题足够严重的话，继续审查其余部分很可能只是浪费宝贵的时间，因为其他正在审查的程序可能都将无关或消失。\n立即发送这些主要设计评论非常重要，有两个主要原因：\n通常开发者在发出 CL 后，在等待审查时立即开始基于该 CL 的新工作。如果您正在审查的 CL 中存在重大设计问题，那么他们以后的 CL 也必须要返工。您应该赶在他们在有问题的设计上做了太多无用功之前通知他们。 主要的设计变更比起小的变更来说需要更长的时间才能完成。开发人员基本都有截止日期；为了完成这些截止日期并且在代码库中仍然保有高质量代码，开发人员需要尽快开始 CL 的任何重大工作。 第三步：以适当的顺序查看 CL 的其余部分 一旦您确认整个 CL 没有重大的设计问题，试着找出一个逻辑顺序来查看文件，同时确保您不会错过查看任何文件。通常在查看主要文件之后，最简单的方法是按照代码审查工具向您提供的顺序浏览每个文件。有时在阅读主代码之前先阅读测试也很有帮助，因为这样您就可以了解该变更应当做些什么。\n","relpermalink":"/book/eng-practices/review/reviewer/navigate/","summary":"总结 现在您已经知道了 Code Review 要点 ，那么管理分布在多个文件中的评论的最有效方法是什么？ 变更是否有意义？它有很好的描述吗？ 首先看一下变更中最重要的部分。整体设计得好吗？ 以适当的顺序查看 CL 的其余部分。 第一步：全","title":"查看 CL 的步骤"},{"content":"本节页面的内容为开发人员进行代码审查的最佳实践。这些指南可帮助您更快地完成审核并获得更高质量的结果。您不必全部阅读它们，但它们适用于每个 Google 开发人员，并且许阅读全文通常会很有帮助。\n写好 CL 描述 小型 CL 如何处理审查者的评论 另请参阅代码审查者指南 ，它为代码审阅者提供了详细的指导。\n","relpermalink":"/book/eng-practices/review/developer/","summary":"本节页面的内容为开发人员进行代码审查的最佳实践。这些指南可帮助您更快地完成审核并获得更高质量的结果。您不必全部阅读它们，但它们适用于每个 Google 开发人员，并且许阅读全文通常会很有帮助。 写好 CL 描述 小型 CL 如何处","title":"开发者指南"},{"content":"当您发送 CL 进行审查时，您的审查者可能会对您的 CL 发表一些评论。以下是处理审查者评论的一些有用信息。\n不是针对您 审查的目标是保持代码库和产品的质量。当审查者对您的代码提出批评时，请将其视为在帮助您、代码库和 Google，而不是对您或您的能力的个人攻击。\n有时，审查者会感到沮丧并在评论中表达他们的挫折感。对于审查者来说，这不是一个好习惯，但作为开发人员，您应该为此做好准备。问问自己，“审查者试图与我沟通的建设性意见是什么？”然后像他们实际说的那样操作。\n永远不要愤怒地回应代码审查评论。这严重违反了专业礼仪且将永远存在于代码审查工具中。如果您太生气或恼火而无法好好的回应，那么请离开电脑一段时间，或者做一些别的事情，直到您感到平静，可以礼貌地回答。\n一般来说，如果审查者没有以建设性和礼貌的方式提供反馈，请亲自向他们解释。如果您无法亲自或通过视频通话与他们交谈，请向他们发送私人电子邮件。以友善的方式向他们解释您不喜欢的东西以及您希望他们以怎样不同的方式来做些什么。如果他们也以非建设性的方式回复此私人讨论，或者没有预期的效果，那么请酌情上报给您的经理。\n修复代码 如果审查者说他们不了解您的代码中的某些内容，那么您的第一反应应该是澄清代码本身。如果无法澄清代码，请添加代码注释，以解释代码存在的原因。只有在想增加的注释看起来毫无意义时，您才能在代码审查工具中进行回复与解释。\n如果审查者不理解您的某些代码，那么代码的未来读者可能也不会理解。在代码审查工具中回复对未来的代码读者没有帮助，但澄清代码或添加代码注释确可以实实在在得帮助他们。\n自我反思 编写 CL 可能需要做很多工作。在终于发送一个 CL 用于审查后，我们通常会感到满足的，认为它已经完成，并且非常确定不需要进一步的工作。这通常是令人满意的。因此，当审查者回复对可以改进的事情的评论时，很容易本能地认为评论是错误的，审查者正在不必要地阻止您，或者他们应该让您提交 CL。但是，无论您目前多么确定，请花一点时间退一步，考虑审查者是否提供有助于对代码库和对 Google 的有价值的反馈。您首先应该想到的应该是，“审查者是否正确？”\n如果您无法回答这个问题，那么审查者可能需要澄清他们的意见。\n如果您已经考虑过并且仍然认为自己是正确的，请随时回答一下为什么您的方法对代码库、用户和/或 Google 更好。通常，审查者实际上是在提供建议，他们希望您自己思考什么是最好的。您可能实际上对审阅者不知道的用户、代码库或 CL 有所了解。所以提供并告诉他们更多的上下文。通常，您可以根据技术事实在自己和审查者之间达成一些共识。\n解决冲突 解决冲突的第一步应该是尝试与审查者达成共识。如果您无法达成共识，请参阅“代码审查标准 ”，该标准提供了在这种情况下遵循的原则。\n","relpermalink":"/book/eng-practices/review/developer/handling-comments/","summary":"当您发送 CL 进行审查时，您的审查者可能会对您的 CL 发表一些评论。以下是处理审查者评论的一些有用信息。 不是针对您 审查的目标是保持代码库和产品的质量。当审查者对您的代码提出批评时，请将其视为在帮助您、代码库和","title":"如何处理审查者的评论"},{"content":"本节是基于过往经验编写的 Code Review 最佳方式建议。其中分为了很多独立的部分，共同组成完整的文档。虽然您不必阅读文档，但通读一遍会对您自己和团队很有帮助。\nCode Review 标准 Code Review 要点 查看 CL 的步骤 Code Review 速度 如何撰写 Code Review 评论 处理 Code Review 中的抵触 另请参阅代码开发者指南 ，该指南为正在进行 Code Review 的开发开发者提供详细指导。\n","relpermalink":"/book/eng-practices/review/reviewer/","summary":"本节是基于过往经验编写的 Code Review 最佳方式建议。其中分为了很多独立的部分，共同组成完整的文档。虽然您不必阅读文档，但通读一遍会对您自己和团队很有帮助。 Code Review 标准 Code Review 要点 查看 CL 的步骤 Code Review 速度 如何撰写 Code Review 评论 处理","title":"审查者指南"},{"content":"对于互联网工作负载而言，可移植和互操作的网络工作负载的加密身份可能是 SPIFFE 的核心用例。为了完全满足这个需求，社区必须达成一致，采用一种标准化的方式来检索、验证和与 SPIFFE 身份进行交互。本规范概述了要支持基于 SPIFFE 的身份验证系统所需的 API 签名和客户端/服务器行为。\n引言 SPIFFE 工作负载 API 是一个 API，它提供了信息和服务，使工作负载或计算进程能够利用 SPIFFE 身份和基于 SPIFFE 的身份验证系统。它由 SPIFFE 工作负载端点提供，并由一些服务或“概要”组成。\n目前，有两个概要：\nX.509-SVID 概要 JWT-SVID 概要 这两个概要是强制性的，并且 SPIFFE 实现必须支持它们。但是，运营商可以在部署中禁用特定的概要。\n本规范的未来版本可能会引入其他概要或使一个或多个概要成为可选项。\n可扩展性 SPIFFE 工作负载 API 不能超出本规范进行扩展。希望提供扩展功能的实现者可以通过引入新的 gRPC 服务来实现，这是根据 SPIFFE 工作负载端点规范中概述的可扩展性方法来实现的。\n服务定义 SPIFFE 工作负载 API 由一份协议缓冲区（版本 3）服务定义来定义。完整的定义可以在 workloadapi.proto 中找到。\n概要作为单个WorkloadAPI服务中的一组相关的 RPC 实现。\n客户端和服务器行为 身份标识调用者 SPIFFE 工作负载 API 支持任意数量的本地客户端，使其能够引导任何能够访问它的进程的身份标识。通常，希望为每个进程分配身份标识，其中某些进程被授予特定的身份标识。为了做到这一点，SPIFFE 工作负载 API 实现必须能够确定调用者的身份标识。\nSPIFFE 工作负载端点规范规定了不直接进行客户端身份验证的要求，而是依赖于带外真实性检查。因此，SPIFFE 工作负载端点实现有责任识别调用者。然后，SPIFFE 工作负载 API 可以利用有关调用者的信息来确定要提供的适当内容。有关详细信息，请参阅 SPIFFE 工作负载端点规范中的身份验证部分。\n连接生命周期 SPIFFE 工作负载 API 的客户端应尽可能保持打开连接的状态，等待流上接收服务器的响应消息。连接可以随时由服务器或客户端终止。在这种情况下，客户端应立即建立新连接。这有助于确保工作负载保持最新的身份相关材料。SPIFFE 工作负载 API 服务器实现者可以假设此属性，如果未能及时接收到消息，则工作负载可能过时，可能会影响其可用性。\n流式响应 SPIFFE 工作负载 API 包括使用 gRPC 服务器端流式传输的 RPC，以促进快速传播更新，例如吊销和 CA 证书引入。这使得客户端可以循环遍历服务器响应，接受发生的更新。\n服务器发送的每个流式响应消息都必须包含完整的信息集，而不仅仅是发生更改的信息。这避免了在客户端和服务器实现上进行状态跟踪的复杂性，包括对反熵机制的需求。\n服务器响应消息的确切定时是特定于实现的，并且应由更改响应的事件（例如 SVID 旋转、CRL 更新等）来决定。从客户端接收到请求消息被视为生成响应的事件。换句话说，服务器响应流的第一个响应消息（基于连接的基础上）应尽快发送，不延迟。\n最后，SPIFFE 工作负载 API 服务器的实现者应小心地推送更新的响应消息。一些软件可能会在接收到新信息后自动重新加载，如果所有实例同时重新加载，可能会导致一段时间的不可用。\n默认值和删除的信息 SPIFFE Workload API 响应消息是对先前发送的响应消息的完整更新。当响应消息包含设置为默认值或空值的字段时，客户端必须将这些字段的值解释为已设置为它们的默认值或空值；在接收到字段的默认值或空值之后，先前接收到的非默认值或非空值不应由客户端保留。例如，如果客户端在federated_bundles字段中接收到默认值，则应舍弃先前接收到的federated_bundles值。\n由于每个消息必须包含完整的信息集（请参阅 Stream Responses 部分），客户端应将数据的缺失解释为删除。例如，如果客户端加载了spiffe://foo.bar的 bundle，并接收到不包含spiffe://foo.bar的 bundle 的消息，则应卸载该 bundle。\n强制字段 为了执行 profile RPC，交换的消息由强制和可选字段组成。服务器在接收到具有默认值的强制字段的消息时，应使用“InvalidArgument”gRPC 状态代码进行响应（有关更多信息，请参阅 SPIFFE Workload Endpoint 规范中的错误代码部分）。当客户端接收到具有默认值的强制字段的消息时，应报告错误并丢弃该消息。\n联邦 Bundle 在此规范中定义的各种 RPC 可以返回来自外部信任域的信任 Bundle。包含外部 Bundle 可以使工作负载在信任域之间进行通信，并且是启用联邦的主要机制。代表外部信任域的 Bundle 称为联邦 Bundle。\n在验证客户端时，验证器会选择代表客户端所呈现的信任域的 Bundle 进行验证。同样，在验证服务器时，客户端会使用代表服务器所在的信任域的 Bundle。如果在使用的 SVID 的 SVID 中不存在匹配的 Bundle，则对等方是不受信任的。这种方法是必需的，以解决常见 X.509 库中对 SAN URI Name Constraints 的广泛支持的缺乏。\nX.509-SVID Profile SPIFFE Workload API 的 X.509-SVID 配置文件提供了一组 gRPC 方法，工作负载可以使用这些方法来检索 X.509-SVIDs 及其相关的信任捆绑包。该配置文件概述了这些方法的签名，以及相关的客户端和服务器行为。\n配置文件定义 下面定义了 X.509-SVID 配置文件中的 RPC 和相关的消息。有关完整的 Workload API 服务定义，请参见 workloadapi.proto。\nservice SpiffeWorkloadAPI { ///////////////////////////////////////////////////////////////////////// // X509-SVID配置文件 ///////////////////////////////////////////////////////////////////////// // 获取工作负载有权访问的所有SPIFFE标识的X.509-SVID，以及与之相关的信任捆绑包和CRL。随着信息的更改，后续的消息将从服务器流式传输。 rpc FetchX509SVID(X509SVIDRequest) returns (stream X509SVIDResponse); // 获取信任捆绑包和CRL。对于仅需要验证SVID而不获取SVID自身的客户端非常有用。随着信息的更改，后续的消息将从服务器流式传输。 rpc FetchX509Bundles(X509BundlesRequest) returns (stream X509BundlesResponse); // ... 其他配置文件的RPC ... } // X509SVIDRequest消息传递请求X.509-SVID的参数。目前没有此类参数。 message X509SVIDRequest { } // X509SVIDResponse消息携带X.509-SVID和相关信息，包括用于与外部信任域联合的全局CRL集合和捆绑列表。 message X509SVIDResponse { // 必需。X509SVID消息列表，每个消息包括单个X.509-SVID、其私钥和信任域的捆绑。 repeated X509SVID svids = 1; // 可选。ASN.1 DER编码的证书吊销列表。 repeated bytes crl = 2; // 可选。工作负载应该信任的外部信任域的CA证书捆绑，按照外部信任域的SPIFFE ID进行索引。捆绑包是ASN.1 DER编码的。 map\u0026lt;string, bytes\u0026gt; federated_bundles = 3; } // X509SVID消息携带单个SVID和所有相关信息，包括信任域的X.509捆绑包。 message X509SVID { // 必需。此条目中的SVID的SPIFFE ID string spiffe_id = 1; // 必需。ASN.1 DER编码的证书链。可以包括中间证书，但必须首先是叶子证书（或SVID本身）。 bytes x509_svid = 2; // 必需。ASN.1 DER编码的PKCS#8私钥。必须是未加密的。 bytes x509_svid_key = 3; // 必需。信任域的ASN.1 DER编码的X.509捆绑包。 bytes bundle = 4; // 可选。操作员指定的字符串，用于在返回多个SVID时为工作负载提供其使用方式的指导。例如，`internal`和`external`分别表示内部或外部使用的SVID。 string hint = 5; } // X509BundlesRequest消息传递请求X.509捆绑包的参数。目前没有这样的参数。 message X509BundlesRequest { } // X509BundlesResponse消息携带一组全局CRL和工作负载应该信任的信任域的映射的CA证书捆绑包。由SPIFFE ID的信任域键控。 message X509BundlesResponse { // 可选。ASN.1 DER编码的证书吊销列表。 repeated bytes crl = 1; // 必需。工作负载应该信任的信任域的CA证书捆绑包，由SPIFFE ID的信任域键控。捆绑包是ASN.1 DER编码的。 map\u0026lt;string, bytes\u0026gt; bundles = 2; } Profile RPCs FetchX509SVID FetchX509SVID RPC 流式返回 X509-SVID 和信任域以及外部信任域的 X.509 捆绑包。这些捆绑包只能用于验证 X509-SVID。\nX509SVIDRequest请求消息当前为空，是将来扩展的占位符。\nX509SVIDResponse响应由一个必需的svids字段组成，该字段必须包含一个或多个X509SVID消息（每个授予客户端的标识一个）。\nX509SVID消息中的所有字段都是必需的，除了hint字段。当设置hint字段时（即非空），SPIFFE Workload API 服务器必须确保其值在任何给定的X509SVIDResponse消息中是唯一的。如果 SPIFFE Workload API 客户端遇到具有相同设置的hint值的多个X509SVID消息，则应选择列表中的第一个消息。\n如果客户端没有权限接收任何 X509-SVID，则服务器应以“PermissionDenied”gRPC 状态代码响应（有关更多信息，请参见 SPIFFE Workload Endpoint 规范中的“错误代码”部分）。在这种情况下，客户端可以在退避后尝试重新连接到FetchX509SVID RPC 的另一个调用。\n如流式响应所述，每个 FetchX509SVID 流返回的 X509SVIDResponse 消息都包含客户端在那个时间点上的授权 SVID 和 bundle 的完整集合。因此，如果服务器从后续响应中删除了 SVID（或全部 SVID，即返回“PermissionDenied”gRPC 状态代码），客户端应停止使用已删除的 SVID。\nFetchX509Bundles FetchX509Bundles RPC 流返回服务器所在的信任域和外部信任域的 X.509 bundles。这些 bundles 只用于验证 X509-SVID。\nX509BundlesRequest 请求 …","relpermalink":"/book/spiffe-and-spire/standard/spiffe-workload-api/","summary":"对于互联网工作负载而言，可移植和互操作的网络工作负载的加密身份可能是 SPIFFE 的核心用例。为了完全满足这个需求，社区必须达成一致，采用一种标准化的方式来检索、验证和与 SPIFFE 身份进行交互。本规范概述了要支持基于 SPIFFE 的","title":"SPIFFE 工作负载 API"},{"content":"本文描述 SPIRE Agent 的命令行选项、agent.conf 设置和内置插件。\n本文档是 SPIRE Agent 的配置参考。它包括有关插件类型、内置插件、代理配置文件、插件配置和 spire-agent 命令的命令行选项的信息。\n插件类型 类型 描述 KeyManager 生成并存储代理的私钥。对于将密钥绑定到硬件等很有用。 NodeAttestor 收集用于向服务器证明代理身份的信息。一般与同类型的服务器插件搭配使用。 WorkloadAttestor 内省工作负载以确定其属性，生成一组与其关联的选择器。 SVIDStore 将 X509-SVID（私钥、叶证书和中间体（如果有））、捆绑包和联合捆绑包存储到信任存储中。 内置插件 类型 名称 描述 KeyManager disk 将私钥写入磁盘的密钥管理器 KeyManager memory 不保留私钥的内存密钥管理器（必须在重新启动后重新证明） NodeAttestor aws_iid 使用 AWS 实例身份文档证明代理身份的节点证明者 NodeAttestor azure_msi 使用 Azure MSI 令牌证明代理身份的节点证明者 NodeAttestor gcp_iit 使用 GCP 实例身份令牌证明代理身份的节点证明者 NodeAttestor join_token 使用服务器生成的加入令牌的节点证明者 NodeAttestor k8s_sat 使用 Kubernetes 服务帐户令牌证明代理身份的节点证明者 NodeAttestor k8s_psat 使用 Kubernetes 投影服务帐户令牌证明代理身份的节点证明者 NodeAttestor sshpop 使用现有 ssh 证书证明代理身份的节点证明者 NodeAttestor x509pop 使用现有 X.509 证书证明代理身份的节点证明者 WorkloadAttestor docker 工作负载证明器允许基于 docker 构造的选择器，例如 label 和 image_id WorkloadAttestor k8s 工作负载证明器允许基于 Kubernetes 的选择器构造 ns （命名空间）和 sa （服务帐户） WorkloadAttestor unix 一个工作负载证明器，可生成基于 Unix 的选择器，例如 uid 和 gid WorkloadAttestor systemd 工作负载证明器，根据 systemd 单元属性（例如 Id 和 FragmentPath 生成选择器） SVIDStore aws_secretsmanager SVIDstore 将机密存储在 AWS 机密管理器中，以及代理有权访问的条目的生成 X509-SVID。 SVIDStore gcp_secretmanager SVIDStore 将机密存储在 Google Cloud Secret Manager 中，并包含代理有权访问的条目的结果 X509-SVID。 代理配置文件 下表概述了 SPIRE 代理的配置选项。这些可以在配置文件的顶级 agent { ... } 部分中设置。大多数选项都有一个相应的 CLI 标志，如果设置了该标志，则该标志优先于文件中定义的值。\nSPIRE 配置文件可以用 HCL 或 JSON 表示。请参阅示例配置文件部分以获取完整示例。\n如果 -expandEnv 标志传递给 SPIRE，则在解析之前扩展 $VARIABLE 或 ${VARIABLE} 样式环境变量。这对于模板化配置文件可能很有用，例如跨不同的信任域，或者插入诸如加入令牌之类的秘密。\n配置 描述 默认 admin_socket_path 绑定管理 API 套接字的位置（默认禁用） allow_unauthenticated_verifiers 允许代理向未经身份验证的验证者发布信任包 错误的 allowed_foreign_jwt_claims 验证外部 JWTSVID 时要返回的可信声明列表 authorized_delegates 授权代表的 SPIFFE ID 列表。请参阅委托身份 API 了解更多信息 data_dir 代理可用于其运行时数据的目录 $PWD experimental 可能会更改或删除的实验选项（见下文） insecure_bootstrap 如果为 true，代理将在不验证服务器身份的情况下进行引导 错误的 join_token 由 SPIRE 服务器生成的可选令牌 log_file 将日志写入的文件 log_level 设置日志记录级别 信息 log_format 日志格式， 文本 log_source_location 如果为 true，日志将包含源文件、行号和方法名称字段（增加一点运行时成本） 错误的 profiling_enabled 如果为 true，则启用 net/http/pprof 端点 错误的 profiling_freq 将分析数据转储到磁盘的频率。仅当 profiling_enabled 为 true 且 profiling_freq \u0026gt; 0 时启用。 profiling_names 将在每个分析标记上转储到磁盘的配置文件名称列表，请参阅分析名称 profiling_port net/http/pprof 端点的端口号。仅当 profiling_enabled 为 true 时使用。 server_address SPIRE 服务器的 DNS 名称或 IP 地址 server_port SPIRE 服务器的端口号 socket_path 绑定 SPIRE Agent API 套接字的位置（仅限 Unix） /tmp/spire-agent/public/api.sock sds 可选的 SDS 配置部分 trust_bundle_path SPIRE 服务器 CA 捆绑包的路径 trust_bundle_url 下载初始 SPIRE 服务器信任包的 URL trust_bundle_format 初始信任包的格式，pem 或 spiffe pem trust_domain 该代理所属的信任域（不得超过 255 个字符） workload_x509_svid_key_type 工作负载 X509 SVID 密钥类型 ec-p256 实验性的 描述 默认 named_pipe_name 用于绑定 SPIRE Agent API 命名管道的管道名称（仅限 Windows） \\spire-agent\\public\\api sync_interval 与指数退避的 SPIRE 服务器同步间隔 5 秒 初始信任捆绑配置 代理需要初始信任捆绑才能安全连接到 SPIRE 服务器。有以下三种选择：\n如果使用 trust_bundle_path 选项，代理将从该路径的文件中读取初始信任包。你需要在启动 SPIRE 代理之前复制或共享该文件。 如果使用 trust_bundle_url 选项，代理将从指定的 URL 读取初始信任包。为了安全起见，URL 必须以 https:// 开头，并且服务器必须具有有效的证书（通过系统信任存储区进行验证）。这可用于快速部署 SPIRE 代理，而无需手动共享文件。请记住，URL 的内容需要保持最新。 如果 insecure_bootstrap 选项设置为 true ，则代理将不会使用初始信任捆绑包。它将连接到 SPIRE 服务器而不进行身份验证。这不是一个安全配置，因为中间人攻击者可以控制 SPIRE 基础设施。包含它是因为它对于测试和开发来说是一个有用的选项。 一次只能设置这三个选项之一。\nSDS 配置 配置 描述 默认 default_svid_name 用于 Envoy SDS 的默认 X509-SVID 的 TLS 证书资源名称 default default_bundle_name 用于 Envoy SDS 的默认 X.509 捆绑包的验证上下文资源名称 ROOTCA default_all_bundles_name 用于 Envoy SDS 的所有捆绑包（包括联合捆绑包）的验证上下文资源名称 ALL disable_spiffe_cert_validation 禁用 Envoy SDS 自定义验证 false 分析名称 这些是可以在 profiling_freq 配置值中设置的可用配置文件：\ngoroutine threadcreate heap block mutex trace cpu 插件配置 代理配置文件还包含代理插件的配置。插件配置位于 plugins { ... } 部分，其格式如下：\nplugins { pluginType \u0026#34;pluginName\u0026#34; { ... plugin configuration options here ... } } 以下配置选项可用于配置插件：\n配置 描述 plugin_cmd 插件实现二进制文件的路径（可选，内置插件不需要） plugin_checksum 插件二进制文件的可选 sha256（可选，内置不需要） enabled 启用或禁用插件（默认启用） plugin_data 插件特定数据 请参阅内置插件部分，了解有关开箱即用的插件的信息。\n遥测配置 请参阅遥测配置指南，了解有关配置 SPIRE Agent 以发出遥测数据的更多信息。\n健康检查配置 代理可以公开可用于健康检查的其他端点。它可以通过设置 listener_enabled = true 来启用。目前它公开了 2 条路径：一条用于活动（代理启动），一条用于准备（代理准备好服务请求）。默认情况下，健康检查端点将侦听 localhost:80，除非另有配置。\nhealth_checks { listener_enabled = true bind_address = \u0026#34;localhost\u0026#34; bind_port = \u0026#34;8080\u0026#34; live_path = \u0026#34;/live\u0026#34; ready_path = \u0026#34;/ready\u0026#34; } 命令行选项 spire-agent run 上述所有配置文件选项都有相同的命令行对应项。此外，还可以使用以下标志：\n命令 行动 默认 -allowUnauthenticatedVerifiers 允许代理向未经身份验证的验证者发布信任包 -config SPIRE 配置文件的路径 conf/代理/agent.conf -dataDir 代理可用于其运行时数据的目录 -expandEnv 展开配置文件中的环境 $VARIABLES -joinToken 由 SPIRE 服务器生成的可选令牌 -logFile 将日志写入的文件 -logFormat 日志格式， -logLevel 调试、信息、警告或错误 -serverAddress SPIRE 服务器的 IP 地址或 DNS 名称 -serverPort SPIRE 服务器的端口号 -socketPath 绑定工作负载 API 套接字的位置 -trustBundle SPIRE 服务器 CA 捆绑包的路径 -trustBundleUrl 下载 SPIRE 服务器 CA 捆绑包的 URL -trustDomain 该代理所属的信任域（不得超过 255 个字符） 将 SPIRE Agent 作为 Windows 服务运行 在 Windows 平台上，SPIRE Agent 可以选择作为 Windows 服务运行。作为 Windows 服务运行时，唯一支持的命令是 run 命令。\n注意：SPIRE 不会自动在系统中创建该服务，必须由用户创建。启动服务时，使用 run 命令执行 SPIRE Agent 的所有参数都必须作为服务参数传递。\n创建 SPIRE Agent Windows 服务的示例 \u0026gt; sc.exe create spire-agent binpath=c:\\spire\\bin\\spire-agent.exe 运行 SPIRE Agent Windows 服 …","relpermalink":"/book/spiffe-and-spire/configuration/agent/","summary":"本文描述 SPIRE Agent 的命令行选项、agent.conf 设置和内置插件。 本文档是 SPIRE Agent 的配置参考。它包括有关插件类型、内置插件、代理配置文件、插件配置和 spire-agent 命令的命令行选项的信息。 插件类型 类型 描述 KeyManager 生成并存储代理的","title":"SPIRE Agent 配置参考"},{"content":" 获取 SPIRE\n安装 SPIRE 服务器\n安装 SPIRE 代理\n","relpermalink":"/book/spiffe-and-spire/installation/","summary":"获取 SPIRE 安装 SPIRE 服务器 安装 SPIRE 代理","title":"安装"},{"content":"通过 Envoy 与 X.509-SVIDs 实现安全通信并结合 Open Policy Agent（OPA）进行授权。\nOpen Policy Agent （OPA）是一个开源通用策略引擎，其提供的授权（AuthZ）是对 SPIRE 提供的认证（AuthN）的很好补充。\n本教程将在 SPIRE Envoy-X.509 教程 的基础上添加 Open Policy Agent （OPA）以演示如何将 SPIRE、Envoy 和 OPA 结合使用，实现 X.509 SVID 认证和请求授权。本教程将演示如何在现有教程的基础上实现使用 OPA 进行请求授权。\n为了便于说明，让我们通过将 OPA 代理实例作为后端服务的新侧车来扩展 Envoy X.509 教程中创建的场景。借助 Envoy 的外部授权过滤器功能，结合 OPA 作为授权服务，可以实现对传入后端服务的每个请求执行安全策略。\nSPIRE Envoy OPA 集成图 如图所示，前端服务通过 Envoy 实例连接到后端服务，Envoy 实例使用 SPIRE 代理提供的 SDS 模块进行身份验证，从而建立了 mTLS 连接。Envoy 通过 mTLS 连接将 HTTP 请求发送到后端，后端通过 OPA 代理实例根据安全策略对 HTTP 请求进行授权或拒绝。\n在本教程中，你将学到：\n将 OPA 代理添加到现有的 Envoy X.509 教程的后端服务中 将外部授权过滤器添加到将 Envoy 连接到 OPA 的 Envoy 配置中 使用 SPIRE 与 Envoy 进行 OPA 授权的测试 先决条件 在继续之前，请查看以下内容：\n当通过 SPIRE Envoy-X.509 教程 进行配置时，你将需要访问 Kubernetes 环境。可选择使用 pre-set-env.sh 脚本创建 Kubernetes 环境。 本教程所需的 YAML 文件可以在 https://github.com/spiffe/spire-tutorials 的 k8s/envoy-opa 目录中找到。如果尚未克隆 spire-tutorials 存储库，请立即执行。 如果 Kubernetes 中的 配置 Envoy 进行 X.509 SVID 认证 教程环境不可用，你可以使用以下脚本创建该环境，并将其用作本教程的起点。从 k8s/envoy-opa 目录运行以下 Bash 脚本：\n$ bash scripts/pre-set-env.sh 该脚本将创建集群中所需的所有 SPIRE 服务器和 SPIRE 代理资源，然后将为 SPIRE Envoy X.509 教程创建所有资源，该教程是 SPIRE Envoy 和 OPA 教程的基本场景。\n注意： 本教程中所需的配置更改已显示为教程中的代码段。但是，所有这些设置已经配置好了。你无需编辑任何配置文件。\n外部 IP 支持 本教程需要一个能够分配外部 IP（例如 metallb ）的负载均衡器。\n$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.7/config/manifests/metallb-native.yaml 等待 metallb 启动：\n$ kubectl wait --namespace metallb-system \\ --for=condition=ready pod \\ --selector=app=metallb \\ --timeout=90s 应用 metallb 配置：\n$ kubectl apply -f ../envoy-x509/metallb-config.yaml 第一部分：部署更新和新资源 假设以 SPIRE Envoy X.509 教程为起点，需要更新一些资源并创建其他资源。目标是在请求到达 backend 服务之前，通过 OPA 让其进行授权。Envoy 实例之间已经建立了 mTLS 连接，因此唯一缺\n失的部分是将 OPA 作为 sidecar 添加到部署中。可以通过以下方式将新容器添加到 backend-deployment.yaml 中：\n- name: opa image: openpolicyagent/opa:0.50.2-envoy imagePullPolicy: IfNotPresent ports: - name: opa-envoy containerPort: 8182 protocol: TCP - name: opa-api-port containerPort: 8181 protocol: TCP args: - \u0026#34;run\u0026#34; - \u0026#34;--server\u0026#34; - \u0026#34;--config-file=/run/opa/opa-config.yaml\u0026#34; - \u0026#34;/run/opa/opa-policy.rego\u0026#34; volumeMounts: - name: backend-opa-policy mountPath: /run/opa readOnly: true 请注意使用了 openpolicyagent/opa:0.50.2-envoy 镜像。该镜像通过在 OPA 中扩展了一个实现 Envoy 外部授权 API 的 gRPC 服务器，以便 OPA 可以与 Envoy 通信以做出策略决策。\n必须在 volumes 部分中添加 ConfigMap backend-opa-policy，如下所示：\n- name: backend-opa-policy configMap: name: backend-opa-policy-config ConfigMap backend-opa-policy 提供了两个资源，分别是在 OPA Configuration 中描述的 opa-config.yaml 和在 Rego Policy 部分中解释的 opa-policy.rego。\nOPA 配置 在本教程中，我们将在 opa-config.yaml 中创建以下 OPA 配置文件：\ndecision_logs: bash: true plugins: envoy_ext_authz_grpc: addr: :8182 query: data.envoy.authz.allow 在这里，decision_logs.bash: true 强制 OPA 在本地以 info 级别记录决策。稍后在教程中，我们将使用这些日志来检查不同请求的结果。\n接下来，让我们来查看 envoy_ext_authz_grpc 插件的配置。首先，addr 键设置了 Envoy 外部授权 gRPC 服务器的监听地址。这必须与 Envoy 过滤器资源中配置的值相匹配，后面的章节将详细介绍。query 键定义了要查询的策略的名称。接下来的部分将重点介绍针对 query 键指定的 envoy.authz.allow 策略的详细信息。\nOPA 策略 OPA 策略以一种称为 Rego 的高级声明性语言表达。在本教程中，我们创建了一个名为 allow 的示例规则，其中包含三个表达式（参见 opa-policy.rego ）。为了使规则成立，所有表达式都必须为 true。\ndefault allow = false allow { valid_path http_request.method == \u0026#34;GET\u0026#34; svc_spiffe_id == \u0026#34;spiffe://example.org/ns/default/sa/default/frontend\u0026#34; } 让我们逐个查看每个表达式。valid_path 是一个用户定义的函数，用于确保仅允许发送到允许资源的请求。\nimport input.attributes.request.http as http_request valid_path { glob.match(\u0026#34;/balances/*\u0026#34;, [], http_request.path) } valid_path { glob.match(\u0026#34;/profiles/*\u0026#34;, [], http_request.path) } valid_path { glob.match(\u0026#34;/transactions/*\u0026#34;, [], http_request.path) } 函数 valid_path 利用了内置函数 glob.match(pattern, delimiters, match)，其输出为 true，如果 match 可以在以 delimiters 分隔的 pattern 中找到，然后在 Rego 中为了表示逻辑或，你定义具有相同名称的多个规则。这就是为什么有三个定义 valid_path 的规则，每个规则对应一个有效资源。\n接下来的表达式定义了请求的 HTTP 方法必须等于 GET：\nhttp_request.method == \u0026#34;GET\u0026#34; 最后一个表达式也是一个用户定义的函数：\nsvc_spiffe_id == \u0026#34;spiffe://example.org/ns/default/sa/default/frontend\u0026#34; svc_spiffe_id 函数从请求中的 x-forwarded-client-cert（XFCC）头中提取服务的 SPIFFE ID。XFCC 头是一个代理头，指示请求已通过的某些或所有客户端或代理的证书信息。svc_spiffe_id 函数利用了来自 envoy.yaml 的两个 Envoy 设置，这些设置修改了 HTTP 头：\nforward_client_cert_details: sanitize_set set_current_client_cert_details: uri: true 当客户端连接为 mTLS 时，例如在此场景中，forward_client_cert_details: sanitize_set 会将 XFCC 头重置为客户端证书信息，set_current_client_cert_details 指定要转发的客户端证书中的字段。\nXFCC 头值是一个以逗号（“,”）分隔的字符串。每个子字符串都是一个 XFCC 元素，每个 XFCC 元素都是一个以分号（“;”）分隔的字符串。每个子字符串都是一个键值对，由等号（“=”）组合在一起。Envoy 支持以下键：\nBy 当前代理证书的主题可选名称（URI 类型）。 Hash 当前客户端证书的 SHA 256 摘要。 Cert 整个客户端证书的 URL 编码 PEM 格式。 Subject 当前客户端证书的 Subject 字段。值总是被双引号引起来。 URI 当前客户端证书的 URI 类型主题可选名称字段。 DNS 当前客户端证书的 DNS 类型主题可选名称字段。客户端证书可能包含多个 DNS 类型的主题可选名称，每个都将是一个单独的键值对。 以下是带有示例值的 XFCC 头，为了便于阅读，该值分为两行：\nx-forwarded-client-cert: By=spiffe://example.org/ns/default/sa/default/backend;Hash=a9317919875e178ce6d6 1eaa023490a2091299753ca5cd01d5323e40696d690b;URI=spiffe://example.org/ns/default/sa/default/frontend 在 x-forwarded-client-cert 头中，Hash 总是设置的，当客户端证书呈现 URI 类型的主题可选名称值时，By 也总是设置的，这在使用 X.509 SVIDs 时是真的。然后 set_current_client_cert_details: uri: true 确保了 URI 类型的主题可选名称（SAN）字段被转发。\n了解了 XFCC 头的这些细节，并知道 X.509 SVID 必须 包含一个 URI SAN，SPIFFE ID 设置为 SAN …","relpermalink":"/book/spiffe-and-spire/examples/envoy-opa/","summary":"通过 Envoy 与 X.509-SVIDs 实现安全通信并结合 Open Policy Agent（OPA）进行授权。 Open Policy Agent （OPA）是一个开源通用策略引擎，其提供的授权（AuthZ）是对 SPIRE 提供的认证（AuthN）的很好补充。 本教程将在 SPIRE Envoy-X.509 教程 的基础上添加","title":"使用 Envoy 和 X.509-SVID 进行 OPA 授权"},{"content":" Elasticsearch 权限\nElasticsearch 清理流程\n","relpermalink":"/book/tsb/operations/elasticsearch/","summary":"Elasticsearch 权限\nElasticsearch 清理流程","title":"ElasticSearch"},{"content":" Common Configuration Objects\nControl Plane\nData Plane\ninstall/helm/common/v1alpha1/common.proto\ninstall/helm/controlplane/v1alpha1/values.proto\nKubernetes\nManagement Plane\n","relpermalink":"/book/tsb/refs/install/","summary":"Common Configuration Objects\nControl Plane\nData Plane\ninstall/helm/common/v1alpha1/common.proto\ninstall/helm/controlplane/v1alpha1/values.proto\nKubernetes\nManagement Plane","title":"install"},{"content":" REST API 指南\n","relpermalink":"/book/tsb/reference/rest-api/","summary":"REST API 指南","title":"REST API"},{"content":"在本文档中，我们将启用 Tier-1 网关中的速率限制，并展示如何根据客户端 IP 地址进行速率限制。\n在开始之前，请确保你已经完成以下准备工作：\n熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成 TSB 快速入门 。本文档假定你已经创建了租户并熟悉工作区和配置组。还需要将 tctl 配置到你的 TSB 环境中。 部署 Tier-1 网关和 Ingress 网关 在应用任何速率限制之前，请阅读 使用 Tier-1 网关进行多集群流量转移 并熟悉使用 Tier-1 网关设置多集群配置的方法。\n其余文档假定你已完成上述步骤。\n启用速率限制服务器 阅读并按照 启用速率限制服务器文档 中的说明进行操作。\n演示安装 如果你使用 TSB 演示 安装，你已经有一个正在运行并且可以使用的速率限制服务，可以跳过本节。 部署 httpbin 服务 按照此文档中的说明 创建 httpbin 服务，并确保该服务在 httpbin.tetrate.com 处暴露。\n创建 Tier-1 网关 创建一个名为 rate-limiting-tier1-config.yaml 的文件，该文件编辑现有的 Tier-1 网关，以在每个唯一的客户端（源）IP 地址上限制 10 个请求/分钟。将集群名称替换为部署 httpbin 服务的集群。\n有关其他速率限制选项的详细信息，请参见此文档 。\napiVersion: gateway.tsb.tetrate.io/v2 kind: Tier1Gateway metadata: name: tier1-gateway group: tier1-gateway-group workspace: tier1-workspace tenant: tetrate organization: tetrate spec: workloadSelector: namespace: tier1 labels: app: tier1-gateway externalServers: - hostname: httpbin.tetrate.com name: httpbin port: 443 rateLimiting: settings: rules: - dimensions: - remoteAddress: value: \u0026#39;*\u0026#39; limit: requestsPerUnit: 10 unit: MINUTE tls: mode: SIMPLE # 确保使用之前创建的正确的密钥名称 secretName: httpbin-certs clusters: - name: \u0026lt;cluster\u0026gt; weight: 100 使用 tctl 配置 Tier-1 网关：\ntctl apply -f rate-limiting-tier1-config.yaml 测试 你可以通过从外部计算机或本地环境向 httpbin 服务发送 HTTP 请求来测试速率限制，并在一定数量的请求之后观察速率限制生效。\n在以下示例中，由于你不能控制 httpbin.tetrate.com，你需要欺骗 curl 认为 httpbin.tetrate.com 解析为 Tier-1 网关的 IP 地址。\n使用以下命令获取之前创建的 Tier-1 网关的 IP 地址。\nkubectl -n tier1 get service tier1-gateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39; 然后执行以下命令，通过 Tier-1 网关向 httpbin 服务发送 HTTP 请求。将 gateway-ip 替换为你在前一步骤中获取的值。还需要传递 CA 证书，你应该在部署 httpbin 服务的步骤中创建了它。\ncurl -I \u0026#34;https://httpbin.tetrate.com/get\u0026#34; \\ --resolve \u0026#34;httpbin.tetrate.com:443:\u0026lt;gateway-ip\u0026gt;\u0026#34; \\ --cacert httpbin.crt \\ -s \\ -o /dev/null \\ -w \u0026#34;%{http_code}\\n\u0026#34; 在一分钟内多次执行上述命令。在 10 次请求后，你将看到响应代码从 200 更改为 429。\n","relpermalink":"/book/tsb/howto/rate-limiting/tier1-gateway/","summary":"在本文档中，我们将启用 Tier-1 网关中的速率限制，并展示如何根据客户端 IP 地址进行速率限制。 在开始之前，请确保你已经完成以下准备工作： 熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成 TSB 快速入门 。本文档假定","title":"Tier-1 网关中的速率限制"},{"content":"本页描述了从集群中卸载 TSB 的步骤。\n注意 此过程对于所有平面（管理、控制和数据平面）都是相似的，因为你需要删除平面的任何自定义资源，然后删除 Operator 本身。 数据平面 由于数据平面负责在你的集群中部署入口网关，因此第一步是从你的集群中删除所有 IngressGateway 自定义资源。\nkubectl delete ingressgateways.install.tetrate.io --all --all-namespaces 这将删除集群中每个命名空间中部署的所有 IngressGateway。运行此命令后，数据平面 Operator 将删除每个网关的部署和相关资源。这可能需要一些时间来确保它们都成功删除。\n为确保你正常删除 istio-operator 部署，你必须按以下顺序缩放并删除数据平面 Operator 命名空间中的剩余对象：\nkubectl -n istio-gateway scale deployment tsb-operator-data-plane --replicas=0 kubectl -n istio-gateway delete istiooperators.install.istio.io --all kubectl -n istio-gateway delete deployment --all 这将删除存储在数据平面 Operator 命名空间中的 TSB 和 Istio Operator 部署。现在你可以清理验证和变异 Web 钩子。\nkubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io \\ tsb-operator-data-plane-egress \\ tsb-operator-data-plane-ingress \\ tsb-operator-data-plane-tier1 kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io \\ tsb-operator-data-plane-egress \\ tsb-operator-data-plane-ingress \\ tsb-operator-data-plane-tier1 删除控制平面将删除所有 TSB 组件以及 TSB 使用的 Istio 控制平面，并使集群中的任何 Sidecar 失去功能。\n控制平面 要删除 TSB 控制平面，首先删除与其关联的 IstioOperator。\nkubectl delete controlplanes.install.tetrate.io --all --all-namespaces TSB Operator 在 istio-system 命名空间中清理 Istio 组件可能需要一些时间，但一旦完成，删除验证和变异 Web 钩子。\nkubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io tsb-operator-control-plane kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io tsb-operator-control-plane kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io xcp-edge-istio-system 然后删除 istio-system 和 xcp-multicluster 命名空间。\nkubectl delete namespace istio-system xcp-multicluster 为了清理集群范围的资源，请使用 tctl install manifest 命令来呈现 cluster-operators 清单并删除生成的清单。\ntctl install manifest cluster-operators --registry=dummy | \\ kubectl delete -f - --ignore-not-found kubectl delete clusterrole xcp-operator-edge kubectl delete clusterrolebinding xcp-operator-edge 现在清理集群中与控制平面相关的自定义资源定义。\nkubectl delete crd \\ clusters.xcp.tetrate.io \\ controlplanes.install.tetrate.io \\ edgexcps.install.xcp.tetrate.io \\ egressgateways.gateway.xcp.tetrate.io \\ egressgateways.install.tetrate.io \\ gatewaygroups.gateway.xcp.tetrate.io \\ globalsettings.xcp.tetrate.io \\ ingressgateways.gateway.xcp.tetrate.io \\ ingressgateways.install.tetrate.io \\ securitygroups.security.xcp.tetrate.io \\ securitysettings.security.xcp.tetrate.io \\ servicedefinitions.registry.tetrate.io \\ serviceroutes.traffic.xcp.tetrate.io \\ tier1gateways.gateway.xcp.tetrate.io \\ tier1gateways.install.tetrate.io \\ trafficgroups.traffic.xcp.tetrate.io \\ trafficsettings.traffic.xcp.tetrate.io \\ workspaces.xcp.tetrate.io \\ workspacesettings.xcp.tetrate.io \\ --ignore-not-found 此时，Kubernetes 集群中的所有 TSB 资源都已删除，但为了从 TSB 配置中删除此集群，你必须运行以下命令：\ntctl delete cluster \u0026lt;cluster\u0026gt; 管理平面 要卸载管理平面，你需要删除描述管理平面配置的 ManagementPlane CR，这将强制管理平面 Operator 删除与之相关的任何组件。\nkubectl -n tsb delete managementplanes.install.tetrate.io --all 然后，删除管理平面 Operator。\nkubectl -n tsb delete deployment tsb-operator-management-plane 最后，删除验证和变异 Web 钩子。\nkubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io tsb-operator-management-plane kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io tsb-operator-management-plane kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io xcp-central-tsb kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io xcp-central-tsb 为了清理集群范围的资源，请使用 tctl install manifest 命令来呈现 management-plane-operator 清单并删除它。\ntctl install manifest management-plane-operator --registry=dummy | kubectl delete -f - --ignore-not-found kubectl delete clusterrole xcp-operator-central kubectl delete clusterrolebinding xcp-operator-central 现在清理集群中与管理平面相关的自定义资源定义。\nkubectl delete crd \\ centralxcps.install.xcp.tetrate.io \\ clusters.xcp.tetrate.io \\ egressgateways.gateway.xcp.tetrate.io \\ egressgateways.install.tetrate.io \\ gatewaygroups.gateway.xcp.tetrate.io \\ globalsettings.xcp.tetrate.io \\ ingressgateways.gateway.xcp.tetrate.io \\ ingressgateways.install.tetrate.io \\ managementplanes.install.tetrate.io \\ securitygroups.security.xcp.tetrate.io \\ securitysettings.security.xcp.tetrate.io \\ servicedefinitions.registry.tetrate.io \\ serviceroutes.traffic.xcp.tetrate.io \\ tier1gateways.gateway.xcp.tetrate.io \\ tier1gateways.install.tetrate.io \\ trafficgroups.traffic.xcp.tetrate.io \\ trafficsettings.traffic.xcp.tetrate.io \\ workspaces.xcp.tetrate.io \\ workspacesettings.xcp.tetrate.io ","relpermalink":"/book/tsb/setup/self-managed/uninstallation/","summary":"卸载你的集群中的 TSB 的步骤。","title":"TSB 卸载"},{"content":"本指南使用了扩展演示环境 中描述的演示环境，包括：\n两个 Edge 集群，位于 region-1 和 region-2 两个工作负载集群，位于 region-1 和 region-2 在工作负载集群中运行的 BookInfo 应用程序 两个 Edge Gateway 负载均衡流量到工作负载集群 在本指南中，我们将探讨当工作负载和 Edge 集群发生故障以及如何检测这些故障时会发生什么。然后，平台 Operator 可以使用这些信息配置一种分发流量到 Edge 集群的方式，例如基于 DNS 的 GSLB 解决方案。目标是保持尽可能高的可用性，并在可能的情况下优化流量路由。\n生成和观察测试流量 请按以下方式测试演示环境：\n验证所有组件 验证所有组件 为了测试所有流程（Edge Gateway 到两个工作负载集群的流程），我们将使用加权流量分布。将 bookinfo-edge 和 bookinfo-edge-2 编辑如下，并使用 tctl 应用更改：\nrouting: rules: - route: clusterDestination: clusters: - name: cluster-1 weight: 50 - name: cluster-2 weight: 50 请注意设置正确的 Kubernetes 上下文，获取每个 Edge Gateway 的地址：\nexport GATEWAY_IP_1=$(kubectl -n edge get service edgegw -o jsonpath=\u0026#34;{.status.loadBalancer.ingress[0][\u0026#39;hostname\u0026#39;,\u0026#39;ip\u0026#39;]}\u0026#34;) echo $GATEWAY_IP_1 export GATEWAY_IP_2=$(kubectl -n edge get service edgegw-2 -o jsonpath=\u0026#34;{.status.loadBalancer.ingress[0][\u0026#39;hostname\u0026#39;,\u0026#39;ip\u0026#39;]}\u0026#34;) echo $GATEWAY_IP_2 在两个不同的终端窗口中，通过每个 Edge Gateway 发送对 productpage.bookinfo 服务的请求。注意使用 ?e1 和 ?e2 查询字符串来识别源 Edge Gateway：\nwhile sleep 1; do \\ curl -s --connect-to bookinfo.tse.tetratelabs.io:80:$GATEWAY_IP_1 \\ \u0026#34;http://bookinfo.tse.tetratelabs.io/productpage?e1\u0026#34; ; \\ done while sleep 1; do \\ curl -s --connect-to bookinfo.tse.tetratelabs.io:80:$GATEWAY_IP_2 \\ \u0026#34;http://bookinfo.tse.tetratelabs.io/productpage?e2\u0026#34; ; \\ done 如果你使用了在 集群故障转移 指南中描述的查看网关日志的技术，你应该观察到以下情况：\nEdge Gateway 1（edgegw）接收到请求 GET /productpage?e1 Edge Gateway 2（edgegw-2）接收到请求 GET /productpage?e2 Ingress Gateway 1 和 2（ingressgw-1 和 ingressgw-2）接收到请求 GET /productpage?e1 和 GET /productpage?e2 Tetrate UI 中的拓扑图将显示来自两个 Edge Gateway 到两个工作负载集群的流量。\n测试工作负载集群故障转移 测试工作负载集群故障转移 将 bookinfo-edge 和 bookinfo-edge-2 配置更新为使用 自动集群列表 配置：\nrouting: rules: - route: clusterDestination: {} 回想一下，这个配置会考虑所有工作正常的工作负载集群，并优先考虑如果它们可用的话，会优选位于同一地区的集群。\n你的基准测试将显示，工作负载集群 1 仅从 Edge Gateway 1（对于 “2” 也是如此）接收请求，并且一旦数据同步，拓扑图也将反映这些数据。\n引发工作负载集群故障 可以通过删除 cluster-1 上的 Gateway 资源或将 Ingress Gateway 缩放到 0 个副本来引发 cluster-1 上的工作负载集群故障，如 工作负载集群故障转移 中所述。\n例如，要删除 cluster-1 上的 Gateway 资源，可以使用 tctl delete -f bookinfo-ingress-1.yaml。\n注意观察，现在两个 Edge Gateway 都会将流量发送到 cluster-2。故障转移已成功。\nEdge Gateway 故障转移 Edge Gateway 模式是一种两层模式，其中 Edge Gateway 在 Workload 集群的 Ingress Gateway 层之前提供第一层负载均衡。\n与工作负载集群相比，Edge Gateway 的故障相对较少。Edge Gateway 组件具有简单且稳定的配置，由 Tetrate 管理平台完全管理，因此操作错误极不可能发生，并且 Edge Gateway 上的负载通常明显低于等效的工作负载集群。\n你需要实施一种故障转移方法，以处理两种情况下的 Edge Gateway 停用：\nEdge Gateway 完全故障或整个区域失败（情景 2） 与 Edge Gateway 在同一区域的本地工作负载集群完全失败（情景 3，可选） 故障转移配置的目标是在这些故障情况下保持正常运行时间并最小化效率低下。在下面的解释中，我们将解释如何检测这些情况。\n每个区域一个 Edge Gateway 为简单起见，这些情景考虑每个区域只有一个 Edge Gateway 的情况。可以扩展情景实施以涵盖在一个区域中有多个独立的 Edge Gateway 的情况。 情景 0：正常的 Edge 和工作负载负载均衡 在正常运行期间，Edge 集群配置的目标是获得以下行为：\n使用 GSLB 解决方案来分发流量，可能还包括额外的近距离或加权负载均衡。每个 Edge Gateway 将流量分发到其本地区域的工作负载集群。\n情景 1：本地工作负载集群部分故障 某个区域的一些工作负载集群故障\n每个 Edge Gateway 分发请求到所有正常工作的本地工作负载集群。GSLB 解决方案将继续将请求分发到所有 Edge Gateway。\n客户端不受故障影响。\n不需要进行故障转移操作，因为所有 Edge Gateway 都可以继续提供受影响的服务。我们假设每个区域都有足够的容量，可能已经启用了自动扩展以弥补活动集群的损失。\n情景 2：Edge Gateway 或整个区域完全故障 区域失败，要么是因为 Edge Gateway 故障，要么是因为整个基础设施失败\nGSLB 解决方案不应将流量发送到受影响区域的 Edge Gateway。\n在基于 DNS 的 GSLB 中，任何已缓存到受影响区域的客户端都将遇到故障，直到刷新为止。\n受影响区域的 Edge Gateway 必须在 GSLB DNS 解决方案中停用。\nGSLB 解决方案会向其目标 IP 地址发送“合成事务”（即“测试请求”），以确定是否可以从该 IP 地址访问服务，并使用此信息来确定客户端提交 DNS 请求时哪些 IP 地址是候选的。\n可以使用简单的 TCP、TLS 或 HTTPS 健康检查来检测故障，连接应该失败（超时）。也可以通过 情景 3 中描述的 HTTP(S) 健康检查的超时来检测故障。\n情景 3：本地工作负载集群完全故障 一个区域中的所有工作负载集群都故障\n受影响区域的 Edge Gateway 分发请求到远程、正常工作的工作负载集群。此外，GSLB 解决方案不应将流量发送到受影响区域的 Edge Gateway。\n在基于 DNS 的 GSLB 中，任何已缓存到受影响区域的客户端可能会因内部跨区域跳跃而遇到性能下降（延迟）。\n正如我们在工作负载集群故障转移 解释中观察到的，受影响区域的 Edge Gateway 将继续运行，并将请求转发到远程集群。\n尽管如此，受影响区域的 Edge Gateway 应该在 GSLB DNS 解决方案中停用。这可以减少区域内流量，从而产生延迟惩罚和可能的传输成本惩罚。\n每个服务的健康检查 我们希望为每个服务实施一个健康检查（HC），具有以下行为：\n对区域中的 Edge Gateway 发出的 HC 请求应路由到同一区域的正常工作工作负载集群 如果区域中的所有工作负载集群都失败，HC 请求应失败，即使客户端请求将被转发到远程区域并成功执行 这种行为足以检测情景 2和情景 3中的故障。\nX-HealthCheck: true Edge Gateway 需要能够区分常规请求（在所有集群之间进行负载均衡的请求）和只能在本地集群之间进行负载均衡的 HC 请求。我们可以通过在 HC 请求中添加特定标头，例如 X-HealthCheck: true，来实现这一点。你需要在 GSLB 解决方案中配置健康检查以添加此标头。\n然后，只需配置 Edge Gateway，使其将 X-HealthCheck: true 请求路由到本地集群。\n在 Gateway 资源中使用规则 在 bookinfo-edge Gateway 资源中添加以下附加规则：\napiVersion: gateway.tsb.tetrate.io/v2 kind: Gateway metadata: organization: tse tenant: tse workspace: edge-ws group: edge-gwgroup name: bookinfo-edge spec: workloadSelector: namespace: edge labels: app: edgegw http: - hostname: \u0026#34;bookinfo.tse.tetratelabs.io\u0026#34; name: bookinfo port: 80 routing: rules: # highlight-start - match: - headers: x-healthcheck: exact: \u0026#34;true\u0026#34; route: clusterDestination: clusters: - name: cluster-1 # highlight-end - route: clusterDestination: {} 测试健康检查 针对 cluster-edge 上的 Edge Gateway 提交一个 HC 请求。注意使用查询字符串 ?HC 以便我们可以识别 HC 请求：\ncurl -s --connect-to bookinfo.tse.tetratelabs.io:80:$GATEWAY_IP_1 \\ -H \u0026#34;X-HealthCheck: true\u0026#34; \\ \u0026#34;http://bookinfo.tse.tetratelabs.io/productpage?HC\u0026#34; 在正常运行中，健康检查将成功。\n模拟故障 通过删除 cluster-1 上的 Gateway 资源来模拟故障。如果你正在运行上面描述的测试，你将观察到常规流量不受影响，并且 Edge Gateway 开始将这些请求转发到工作负载集群 cluster-2。\n重新播放 HC 请求。HC 请求将失败，返回 503 状态码和响应体 no healthy upstreams。\n结论 你可以使用每个服务的健康检查来触发故障转移（和恢复），适用于情景 2和情景 3。\n我们取得了什么成就？ 我们观察到 Tetrate 平台在Edge 集群或工作负载集群出现故障或其托管服务出现故障的一系列情景中的故障检测和 …","relpermalink":"/book/tsb/design-guides/ha-multicluster/edge-failover/","summary":"本指南使用了扩展演示环境 中描述的演示环境，包括： 两个 Edge 集群，位于 region-1 和 region-2 两个工作负载集群，位于 region-1 和 region-2 在工作负载集群中运行的 BookInfo 应用程序 两个 Edge Gateway 负载均衡流量到工作负载集群 在本指南中，我们将探讨当工作负载和 Edge","title":"边缘网关故障转移"},{"content":"在本节中，你将了解如何创建一个名为 bookinfo-ws 并绑定到 bookinfo 命名空间的 TSB 工作区。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建一个租户。 使用用户界面 在左侧面板的“租户”下，选择“工作区”。 单击该卡可添加新的工作区。 输入工作区 ID 作为 bookinfo-ws 。 为你的工作区提供显示名称和描述。 输入 demo/bookinfo 作为初始命名空间选择器。 单击添加。 如果你之前已成功启动演示应用程序，你应该会看到类似以下内容的内容：\n1 个集群 1 命名空间 4 服务 1 个工作区 使用 tctl 创建以下 workspace.yaml 文件：\napiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tetrate tenant: tetrate name: bookinfo-ws spec: namespaceSelector: names: - \u0026#34;*/bookinfo\u0026#34; 使用 tctl 应用配置：\ntctl apply -f workspace.yaml 如果你之前已成功登录演示应用程序并转到 UI 显示租户，你应该会看到类似于以下内容的内容：\nTSB 租户 UI：创建的对象\n1 个集群 1 命名空间 4 服务 1 个工作区 通过执行这些步骤，你已成功创建了一个名为 bookinfo-ws 并绑定到 bookinfo 命名空间的 TSB 工作区。\n","relpermalink":"/book/tsb/quickstart/workspace/","summary":"在本节中，你将了解如何创建一个名为 bookinfo-ws 并绑定到 bookinfo 命名空间的 TSB 工作区。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建一个租户。 使用用户界面 在左侧面板的“","title":"创建工作区"},{"content":"当流量通过网关转发时，通常会假定流量的身份为该网关的身份。这种默认行为简化了外部流量的访问控制配置。然而，在多集群环境中，通常需要更精细的访问控制。Tetrate Service Bridge（TSB）提供了通过网关跃点保留请求的原始身份的能力，从而实现跨集群身份验证和细粒度的访问控制。\n本文档解释了如何在 TSB 中启用和利用身份传播，从而实现将消费者身份传播到远程服务、在不同集群之间实施详细的访问控制以及将访问控制规则应用于故障转移目标等场景。\n在继续之前，假定你熟悉 TSB 的概念 和术语，如入口网关、Tier-1 网关和东西网关 。\nGitOps 本文档中的示例使用了 TSB 的 GitOps 功能，允许你使用 kubectl 应用 TSB 配置。要在你的 TSB 环境中启用 GitOps，请参见启用 GitOps ，并了解如何在 TSB 中使用 GitOps 工作流程的详细信息GitOps 工作原理 。 启用身份传播 默认情况下，由于网关处的 TLS 终止，服务身份不会通过网关跃点传播。TSB 使用每个网关跃点上的内部 WebAssembly（WASM）扩展实现身份传播。该扩展验证客户端身份并将其附加到请求的 XFCC 标头，然后将其转发。\n要启用身份传播：\n在ControlPlane CR 或 Helm 值的xcp组件中添加enableHttpMeshInternalIdentityPropagation键： spec: ... components: xcp: centralAuthMode: JWT configProtection: {} enableHttpMeshInternalIdentityPropagation: true kubeSpec: ... ... 在ControlPlane CR 或 Helm 值中配置 TSB 镜像注册表的imagePullSecret，这是必需的，以便拉取 WASM 扩展： spec: ... imagePullSecrets: - name: gcr-secret components: xcp: ... ... 验证身份传播 启用身份传播后，你可以通过检查 XCP 边缘中是否启用了ENABLE_HTTP_MESH_INTERNAL_IDENTITY_PROPAGATION来验证其状态：\nkubectl get deployment edge -n istio-system -o yaml | grep ENABLE_HTTP_MESH_INTERNAL_IDENTITY_PROPAGATION -A 1 用例 1：通过 Tier 1 和 Tier 2 网关传播服务身份 在此用例中，我们演示了如何使用 Tier 1 和 Tier 2 网关在集群之间传播服务身份。\n配置两个集群，cluster-1 和 cluster-2，共享相同的信任根。按照此处 的指南和使用此存储库 设置 Istio 根和中间证书。\n在tier-1 集群中：\n创建一个专用集群，用于部署 tier-1 网关。 配置networkReachability 以建立cluster-1和tier-1，以及tier-1和cluster-2之间的可达性。 在cluster-1 中：\n创建一个租户tenant-1，以及其命名空间、工作空间和组。 在tenant-1-ns中部署sleep pod 或类似的文本客户端。 在cluster-2 中：\n创建一个租户tenant-2，以及其命名空间、工作空间和组。 在tenant-2-ns中部署bookinfo 应用程序，以及一个入口网关 。 验证来自cluster-1中sleep pod 的请求是否可以通过 Tier 1 网关到达cluster-2中bookinfo应用程序的服务。\n通过使用适当的规则和设置，在不同级别的工作空间或租户之间拒绝通信，来实施访问控制。\n用例 2：在东西网关故障转移中传播服务身份 在此用例中，我们关注在东西网关设置中的服务故障转移期间传播源身份。\n在cluster-1 中，为Client和Bookinfo租户创建命名空间。在bookinfo-ns中部署bookinfo和bookinfo-gateway服务。 在cluster-2 中，为Bookinfo租户创建bookinfo-ns。在bookinfo-ns中部署bookinfo和bookinfo-gateway服务。 使用defaultEastWestGatewaySettings配置bookinfo-ns/bookinfo-gateway以进行东西故障转移。 实施允许和拒绝规则以控制不同租户和服务之间的通信。 验证Client租户中的客户端是否可以访问适当的服务，同时强制执行访问控制。 在服务故障转移场景中观察身份传播的行为。 故障排除 确保ControlPlane CR 的xcp组件中正确设置了enableHttpMeshInternalIdentityPropagation。 验证ControlPlane CR 中是否配置了imagePullSecret，以允许拉取必要的 WASM 扩展。 确认已成功安装所需的 WASM 扩展在 Istio 环境中。 确保 XFCC 标头传播正常工作。 如果遇到问题，请参阅本页面末尾的故障排除部分以获取进一步的指导。 ","relpermalink":"/book/tsb/howto/gateway/service-identity-propagation/","summary":"当流量通过网关转发时，通常会假定流量的身份为该网关的身份。这种默认行为简化了外部流量的访问控制配置。然而，在多集群环境中，通常需要更精细的访问控制。Tetrate Service Bridge（TSB）提供了通过网关","title":"多集群访问控制和身份传播"},{"content":"工作负载命名 加入到 mesh 中的工作负载由 Kubernetes 资源 WorkloadAutoRegistration 表示。\n当新的工作负载加入到 mesh 中并加入到给定的 WorkloadGroup 时，工作负载载入端点会在该 WorkloadGroup 的命名空间中创建一个 WorkloadAutoRegistration 资源。\n每个 WorkloadAutoRegistration 资源都被分配一个唯一的名称，格式为：\n\u0026lt;workload-group-name\u0026gt;-\u0026lt;workload-identity\u0026gt; 其中 workload-identity 是由 TSB 生成的唯一名称。对于在 AWS EC2 实例上运行的工作负载，它的 workload-identity 将采用以下格式：\naws-\u0026lt;aws-partition\u0026gt;-\u0026lt;aws-account\u0026gt;-\u0026lt;aws-zone\u0026gt;-ec2-\u0026lt;aws-ec2-instance-id\u0026gt; 综合起来，工作负载的唯一名称可能看起来像下面的例子：\nratings-aws-aws-123456789012-us-east-2b-ec2-i-1234567890abcdef0 列出已载入的工作负载 要列出已载入的工作负载，请对 war 资源发出 kubectl get 命令。war 是 WorkloadAutoRegistration 的别名。\n以下命令将列出在所有 Kubernetes 命名空间中注册的工作负载：\nkubectl get war -A 你将看到类似于以下输出：\nNAMESPACE NAME AGENT CONNECTED AGE bookinfo ratings-aws-aws-123456789012-us-east-2b-ec2-i-1234567890abcdef0 True 1m AGENT CONNECTED 列显示了工作负载载入代理的状态。如果值为 True，则代理当前已连接到工作负载载入端点，并且工作负载被视为健康。如果值为 False，则代理不再连接。工作负载本身可能健康也可能不健康。\n描述已载入的工作负载 要查看已载入工作负载的详细信息，请运行 kubectl describe war 命令：\nkubectl describe war \u0026lt;war-name\u0026gt; 你将看到类似于以下输出：\nName: ratings-aws-aws-123456789012-us-east-2b-ec2-i-1234567890abcdef0 Namespace: bookinfo API Version: runtime.onboarding.tetrate.io/v1alpha1 Kind: WorkloadAutoRegistration Spec: Identity: # (1) Aws: Account: 123456789012 ec2: Instance Id: i-1234567890abcdef0 Partition: aws Region: us-east-2 Zone: us-east-2b Registration: Agent: Version: v1.4.0 Host: Addresses: Ip: 172.31.5.254 Type: VPC Settings: Connected Over: VPC Sidecar: Istio: Version: 1.9.8-15bc6e5e32 Workload: Labels: Version: v5 Status: Conditions: Last Transition Time: 2021-10-09T10:56:41.380102645Z Reason: AgentEstablishedConnection Status: True Type: AgentConnected Spec.Identity 部分（1）描述了工作负载的已验证身份，在这种情况下是工作负载正在运行的 VM 的身份。这些信息可能对于验证已载入的工作负载的来源很有用，而不是信任工作负载本身报告的信息。\n检查 Istio Sidecar 的状态 你可以使用 istioctl proxy-status 命令来检查已载入工作负载的 Istio sidecar 的状态。\n运行：\nistioctl proxy-status 你应该会得到类似于以下的输出：\nNAME CDS LDS EDS RDS ISTIOD VERSION ratings-aws-aws-123456789012-us-east-2b-ec2-i-1234567890abcdef0.bookinfo SYNCED SYNCED SYNCED SYNCED istiod-6449df9b98-prvqd 1.9.8-15bc6e5e32 ... istioctl proxy-status 命令显示了当前连接到 Istio 控制平面的所有 Istio 代理（包括 sidecar 和网关）的状态。\nSidecar 的名称将与工作负载的名称相同。\n自动删除已载入的工作负载 工作负载载入端点组件不知道通过工作负载载入代理注册的工作负载的生命周期。例如，如果运行工作负载的 AWS EC2 实例被终止，工作负载载入端点只知道工作负载载入代理不再连接\n到它。\n为了避免无限期保留悬挂的工作负载载入，工作负载载入端点在工作负载载入代理断开连接并且在预先配置的宽限期内没有重新连接时，将工作负载视为不再活动。此宽限期的默认值为 5 分钟。\n更新 WorkloadGroups 在工作负载载入功能中使用的 Istio WorkloadGroup 扮演着类似于 Kubernetes Deployment 的角色。WorkloadGroup 用于定义在组中的每个单独实例中使用的配置模板。\nKubernetes Deployments 和 WorkloadGroup 之间有一个重要的区别。前者由一个控制逻辑支持，该逻辑知道如何逐渐使用新配置替换 Pod 并推出对 Deployment 资源所做的更改，而后者则没有这样的功能。\n这意味着，尽管你对 WorkloadGroup 进行的任何更改都将影响将来加入该组的工作负载，但之前已经加入的工作负载将保留其旧配置。\n应用 WorkloadGroup 更新 WorkloadGroup 定义了 Istio sidecar 的核心配置集，该配置集无法在不重新启动 sidecar 的情况下更新。\n因此，如果我们要将新配置应用于所有工作负载，所有工作负载都必须同时终止。这对于生产环境来说是不安全的。\n同样，在 Istio sidecar 重新连接到控制平面时应用配置也是不安全的，因为由于网络故障，可能会出现多个 sidecar 同时重新连接的可能性。\n另一方面，加入到 mesh 中的每个个体工作负载都由 WorkloadAutoRegistration 资源表示。\n为了确保工作负载 Istio sidecar 的核心配置始终保持稳定，WorkloadAutoRegistration 携带在工作负载加入 mesh 时采取的 WorkloadGroup 的快照。\nWorkloadAutoRegistration 捕获有关工作负载的主要信息，例如 IP 地址、流量重定向的使用等，这些都影响该工作负载的 Istio sidecar 的核心配置。\n因此，在创建 WorkloadAutoRegistration 后，如果要使工作负载观察到 WorkloadGroup 的新更改，就需要删除并重新创建 WorkloadAutoRegistration。\n总之，你应该在对 WorkloadGroup 进行更改后删除 WorkloadAutoRegistration 资源。在对 WorkloadGroup 进行更改后运行以下命令：\nkubectl delete war \u0026lt;war-name\u0026gt; 删除 WorkloadAutoRegistration 资源将导致 Workload 载入代理再次执行 Workload 载入流程，进而重新创建 WorkloadAutoRegistration。这将捕获 WorkloadGroup 配置的最新版本。\n","relpermalink":"/book/tsb/setup/workload-onboarding/guides/managing/","summary":"工作负载命名 加入到 mesh 中的工作负载由 Kubernetes 资源 WorkloadAutoRegistration 表示。 当新的工作负载加入到 mesh 中并加入到给定的 WorkloadGroup 时，工作负载载入端点会在该 WorkloadGroup 的命名空间中创建一个 WorkloadAutoRegistration 资源。 每个 WorkloadAutoRegistration 资源都被分配一个唯一的名称，格式为： \u003cworkload-group-name\u003e-\u003cworkload-identity\u003e 其中 workload-identity 是由 TSB","title":"管理已载入的工作负载"},{"content":"Tetrate 管理平面从每个接入的工作负载平台以及启用了适当 Skywalking 客户端的其他工作负载中收集指标和跟踪信息。这些指标存储在 ElasticSearch 数据库中，并通过 Tetrate UI 中的仪表板提供：\n查看 TSB 或 TSE 示例以获取更多详细信息。\n用户需要访问 Tetrate UI 才能查看这些指标。在简单的单团队部署中可能是可行的，并且在 TSB 中，可以通过为应用程序所有者提供基于角色的访问权限，在非常大规模的情况下也可能是可行的，但在许多情况下，为你的应用程序所有者团队提供访问权限可能不是可行的或不适当的做法。\n另一种方法是将 Tetrate 收集的指标导出到第三方仪表板，如 Grafana。这通常是提供大量用户访问 Tetrate 指标的最合适方式，特别是如果你已经有一个成熟的企业仪表板解决方案。\n在第三方仪表板中公开 Tetrate 指标 你需要公开你的 TSE 指标并安排一个仪表板收集器来抓取并标记它们。以下资源可能会对你有所帮助：\n了解 Tetrate 指标架构 TSE AWS 托管 Grafana 集成指南 TSB Prometheus PromQL 指南 TSB New Relic 集成 获取 Tetrate 跟踪信息 Tetrate 管理平台从工作负载集群的交易中对跟踪信息进行采样，并将其存储在管理集群上。Tetrate 提供了 tctl 命令行工具中的功能来（a）离线复制集群和跟踪数据，并（b）检查此数据。\n可以使用此功能，以便平台所有者可以对活动数据进行特权转储，并将其提供给应用程序所有者团队进行分析。\n有关更多详细信息，请参阅有关 如何识别和排查性能不佳的服务 的文档。\n","relpermalink":"/book/tsb/design-guides/app-onboarding/monitor/","summary":"Tetrate 管理平面从每个接入的工作负载平台以及启用了适当 Skywalking 客户端的其他工作负载中收集指标和跟踪信息。这些指标存储在 ElasticSearch 数据库中，并通过 Tetrate UI 中的仪表板提供： 查看 TSB 或 TSE 示例以获取更多详细信息。 用户需要访问 Tetrate UI 才能查","title":"监控指标"},{"content":"本文将介绍如何使用 HTTPS 重试和超时将流量发送到外部主机。\n在开始之前，请确保你已经：\n熟悉TSB 概念 安装了 TSB 环境。你可以使用TSB 演示 进行快速安装 完成了TSB 使用快速入门 。 理解问题 考虑一个通过ServiceEntry 添加到网格中的外部应用程序。该应用程序监听 HTTPS，因此你将发送的流量预期使用简单的 TLS。\n网格内的应用程序客户端将发起 HTTP 请求，并在 Sidecar 到外部应用程序主机的过程中将其转换为 HTTPS，例如www.tetrate.io。这是由于在 DestinationRule 中定义的出站流量策略实现的。\n以下是你需要设置的内容，以实现客户端与外部主机之间的通信：\n直接模式 这仅在使用 TSB 直接模式配置时有效。 首先，为你的 Istio 对象创建一个命名空间：\nkubectl create ns tetrate 创建一个名为tetrate.yaml的文件，其中包含以下 ServiceEntry、VirtualService 和 DestinationRule。\nkind: ServiceEntry apiVersion: networking.istio.io/v1alpha3 metadata: annotations: tsb.tetrate.io/organization: tetrate tsb.tetrate.io/tenant: tetrate tsb.tetrate.io/workspace: w1 name: tetrate namespace: tetrate spec: endpoints: - address: www.tetrate.io ports: http: 443 hosts: - www.tetrate.io ports: - name: http number: 80 protocol: http location: MESH_EXTERNAL resolution: DNS --- kind: VirtualService apiVersion: networking.istio.io/v1alpha3 metadata: annotations: tsb.tetrate.io/organization: tetrate tsb.tetrate.io/tenant: tetrate tsb.tetrate.io/workspace: w1 tsb.tetrate.io/trafficGroup: t1 name: tetrate namespace: tetrate spec: hosts: - www.tetrate.io http: - retries: attempts: 3 perTryTimeout: 0.001s retryOn: \u0026#34;gateway-error,5xx\u0026#34; route: - destination: host: www.tetrate.io weight: 100 timeout: 0.001s --- apiVersion: networking.istio.io/v1beta1 kind: DestinationRule metadata: annotations: tsb.tetrate.io/organization: tetrate tsb.tetrate.io/tenant: tetrate tsb.tetrate.io/workspace: w1 tsb.tetrate.io/trafficGroup: t1 name: tetrate namespace: tetrate spec: exportTo: - \u0026#34;.\u0026#34; host: www.tetrate.io trafficPolicy: tls: mode: SIMPLE sni: tetrate.io 使用 kubectl 应用：\nkubectl apply -f tetrate.yaml 重要的是要注意如何将外部主机添加到服务注册表。在上面的 YAML 中，你可以看到单个 ServiceEntry 具有端口 80 作为匹配端口，但你的外部应用程序监听 HTTPS，大多数情况下将是 443（如果你的应用程序监听 8443 或其他端口，你可以更改此端口）。\n换句话说，流量被发送到匹配的相同端口，即端口 80，这对于出站 HTTPS 连接是不正确的。为了转发到上游的 443 端口，你需要使 ServiceEntry 中的 endpoints 部分如下所示：\nendpoints: - address: www.tetrate.io ports: http: 443 测试 用于测试的客户端可以执行来自具有注入了 Sidecar 的客户端的请求，此处将使用netshoot 或sleep pod。\n首先，发送一个使用 HTTPS 的请求：\ncurl -I https://www.tetrate.io HTTP/2 200 date: Tue, 13 Sep 2022 16:21:37 GMT content-type: text/html; charset=UTF-8 content-length: 148878 server: Apache link: \u0026lt;https://www.tetrate.io/wp-json/\u0026gt;; rel=\u0026#34;https://api.w.org/\u0026#34;, \u0026lt;https://www.tetrate.io/wp-json/wp/v2/pages/29256\u0026gt;; rel=\u0026#34;alternate\u0026#34;; type=\u0026#34;application/json\u0026#34;, \u0026lt;https://www.tetrate.io/\u0026gt;; rel=shortlink content-security-policy: upgrade-insecure-requests; x-frame-options: SAMEORIGIN strict-transport-security: max-age=31536000;includeSubDomains; x-xss-protection: 1; mode=block x-content-type-options: nosniff referrer-policy: no-referrer x-cacheable: YES:Forced cache-control: must-revalidate, public, max-age=300, stale-while-revalidate=360, stale-if-error=43200 vary: Accept-Encoding x-varnish: 107840197 105743030 age: 1441 via: 1.1 varnish (Varnish/6.5) x-cache: HIT x-powered-by: DreamPress accept-ranges: bytes strict-transport-security: max-age=31536000 你可以看到第一个 curl 命令成功了，因为它通过了直通代理（TCP 代理）。这意味着没有从 DestinationRule 或 VirtualService 应用规则。\n现在，执行一个请求，而不是发送 HTTPS，这将是一个普通的 HTTP 请求。请记住，Sidecar 将按照我们在 DestinationRule 中指示的方式发起 HTTPS 请求。\ncurl -I http://www.tetrate.io HTTP/1.1 504 Gateway Timeout content-length: 24 content-type: text/plain date: Tue, 13 Sep 2022 16:24:32 GMT server: envoy 由于在虚拟服务中定义了一个激进的超时，这将返回一个明显的响应，因此它按预期工作。\n清理 使用相同的 yaml 文件销毁所有资源：\nkubectl delete -f tetrate.yaml 最后，删除命名空间。\nkubectl delete ns tetrate ","relpermalink":"/book/tsb/howto/traffic/external-site-https/","summary":"本文将介绍如何使用 HTTPS 重试和超时将流量发送到外部主机。 在开始之前，请确保你已经： 熟悉TSB 概念 安装了 TSB 环境。你可以使用TSB 演示 进行快速安装 完成了TSB 使用快速入门 。 理解问题 考虑一个通过ServiceE","title":"将流量发送到使用 HTTPS 的外部主机"},{"content":" 注意 Tetrate Service Bridge 收集了大量的指标，而你设置的指标和阈值限制将因环境而异。本文档概述了通用的警报指南，而不是提供详尽的警报配置和阈值列表，因为这些将因不同环境和不同工作负载配置而异。 TSB 运维状态 TSB 可用性 TSB API 的成功请求率。这是一个非常容易被用户看到的信号，应该作为这样对待。\n根据在你的环境中捕获的历史度量数据作为基线来确定 THRESHOLD 值。首次迭代的合理值可能为 0.99。\n示例 PromQL 表达式：\nsum( rate( grpc_server_handled_total{ component=\u0026#34;tsb\u0026#34;, grpc_code=\u0026#34;OK\u0026#34;, grpc_type=\u0026#34;unary\u0026#34;, grpc_method!=\u0026#34;SendAuditLog\u0026#34; }[1m] ) ) BY (grpc_method) / sum( rate( grpc_server_handled_total{ component=\u0026#34;tsb\u0026#34;, grpc_type=\u0026#34;unary\u0026#34;, grpc_method!=\u0026#34;SendAuditLog\u0026#34; }[1m] ) ) BY (grpc_method) \u0026lt; THRESHOLD TSB 请求延迟 由于度量数据基数高，TSB gRPC API 请求延迟度量意图不会被发出。\nTSB 请求流量 对 TSB API 的请求速率。监控值主要来自于检测异常值和意外行为，例如意外高或低的请求速率。要建立合理的阈值，有历史度量数据的记录是至关重要的，以便衡量基线。\n示例 PromQL 表达式：\nsum( rate( grpc_server_handled_total{ component=\u0026#34;tsb\u0026#34;, grpc_type=\u0026#34;unary\u0026#34;, grpc_method!=\u0026#34;SendAuditLog\u0026#34;}[1m] ) ) BY (grpc_method) \u0026lt; THRESHOLD # 或 \u0026gt; THRESHOLD TSB 缺失指标 TSB 即使没有持续的外部负载也会与其持久性后端通信。请求的缺失可靠地指示了 TSB 指标收集存在问题，并应视为高优先级事件，因为缺少指标意味着无法查看 TSB 的状态。\n示例 PromQL 表达式：\nsum(rate(persistence_operation[10m])) == 0 持久性后端可用性 来自 TSB 的持久性后端可用性，没有内部 Postgres 操作的洞察。\nTSB 将其所有状态存储在持久性后端中，因此其运营状态（可用性、延迟、吞吐量等）与持久性后端的状态密切相关。TSB 记录了可能用作警报信号的持久性后端操作的度量标准。\n重要的是要注意，持久性后端操作的任何降级都必然会导致 TSB 整体降级，无论是可用性、延迟还是吞吐量。这意味着警报持久性后端状态可能是多余的，当需要关注需要注意的 Postgres 问题时，值班人员将收到两个页面而不是一个。然而，这样的信号仍然具有显著的价值，可以提供重要的上下文，以减少解决问题和解决根本原因/升级所需的时间。\n注意 “未找到资源” 错误的处理：少量的 “未找到” 响应是正常的，因为为了优化，TSB 通常使用 Get 查询而不是 Exists 查询来确定资源是否存在。然而，大量 “未找到”（类似 404 的）响应很可能表示持久性后端设置存在问题。 示例 PromQL 表达式：\n查询： 1 - ( sum( rate( persistence_operation{ error!=\u0026#34;\u0026#34;, error!=\u0026#34;resource not found\u0026#34; }[1m] ) ) / sum( rate(persistence_operation[1m]) ) OR on() vector(0) ) \u0026lt; THRESHOLD 过多的 “未找到资源” 查询： ( sum( rate(persistence_operation{error=\u0026#34;resource not found\u0026#34;}[1m]) ) OR on() vector(0) / sum( rate(persistence_operation[1m]) ) ) \u0026gt; THRESHOLD # 例如 0.50 事务： sum( rate(persistence_transaction{error=\u0026#34;\u0026#34;}[1m]) ) / sum( rate(persistence_transaction[1m]) ) \u0026lt; THRESHOLD 持久性后端延迟 持久性后端操作的延迟，由持久性后端客户端（TSB）记录。这个延迟实际上转化为用户看到的延迟，因此是一个重要的信号。\nTHRESHOLD 值应该从历史度量数据中建立，作为基线使用。首次迭代的合理值可能是 300ms 的第 99 百分位延迟。\n示例 PromQL 表达式：\n查询： histogram_quantile( 0.99, sum(rate(persistence_operation_duration_bucket[1m])) by (le, method) ) \u0026gt; THRESHOLD 事务： histogram_quantile( 0.99, sum(rate(persistence_transaction_duration_bucket[1m])) by (le) ) \u0026gt; THRESHOLD XCP 运维状态 最后一次管理平面同步 XCP Edge 最后一次与管理平面（XCP 中央）同步的最长时间间隔，对于每个已注册的集群。这表示从管理平面接收的配置在给定集群中的陈旧程度。首次迭代的合理阈值为 30（秒）。\n示例 PromQL 表达式：\ntime() - min( xcp_central_last_config_propagation_event_timestamp_ms{edge!=\u0026#34;\u0026#34;} / 1000 ) by (edge, status) \u0026gt; THRESHOLD XCP Edge 饱和度 TSB 控制平面组件主要受限于 CPU。因此，CPU 利用率作为重要信号应该进行警报。在选择警报 THRESHOLD 时，请记住不仅云提供商 tend to tend to 超额提供 CPU，而且即使在 \u0026lt;~80% CPU 利用率下，超线程也可能对 Linux 调度器效率产生负面影响，导致延迟/错误增加。\n","relpermalink":"/book/tsb/operations/telemetry/alerting-guidelines/","summary":"设置 Tetrate Service Bridge 监控警报的通用指南。","title":"警报指南"},{"content":" 简介\n部署应用程序\n创建租户\n创建工作区\n创建配置组\n配置权限\n入口网关\n拓扑和指标\n流量转移\n安全\n创建应用程序和 API\n","relpermalink":"/book/tsb/quickstart/","summary":"简介 部署应用程序 创建租户 创建工作区 创建配置组 配置权限 入口网关 拓扑和指标 流量转移 安全 创建应用程序和 API","title":"快速入门"},{"content":"Tetrate Service Bridge (TSB) 提供强大的流量管理功能，允许有效控制其域内服务之间的流量。TSB 简化了流量路由、分阶段部署和迁移等复杂任务，从而增强了应用程序交付的整体方法。\nTSB 中的网关 TSB 使用一系列网关来管理流量路由。当流量进入你的 TSB 环境时，它会在到达其预期应用程序之前遍历各个网关。该过程涉及：\n应用程序边缘网关：也称为“边缘网关”，这种共享多租户网关有助于跨集群负载平衡。它将传入流量定向到适当的应用程序入口网关。 应用程序入口网关：称为“应用程序网关”，该网关可以在多个应用程序之间共享，也可以专用于特定应用程序。它控制流量的流动方式以及与应用程序的交互方式。应用程序入口网关归工作区所有，提供对流量的控制。 建议部署多个入口网关以隔离和控制流量。随着时间的推移，随着对网格使用的信心不断增强，可以考虑整合到共享入口网关上。\n智能流量路由 TSB 通过利用每个集群内本地控制平面的信息来确保智能流量路由。它优先考虑本地流量以获得最佳性能和可用性。Envoy 的按请求功能可以对流量路由进行细粒度控制，支持请求粘性、金丝雀部署以及跨后端均匀分配流量等场景。\nTSB 通过本地控制平面跟踪服务可用性、位置和运行状况。这使得系统能够尽可能将流量引导到本地实例。全局控制平面跨集群维护服务信息，实现无缝流量故障转移并增强可用性。\n多集群路由 TSB 擅长管理多集群环境，无论是主动 - 主动还是主动 - 被动配置。多集群管理轻松自如，适合多个集群服务不同团队和应用程序的场景。TSB 使用主机名实现跨集群的无缝应用程序访问，从而促进服务之间的私有通信，无论其集群位置如何。\nAPI 网关能力无处不在 TSB 将 API 网关功能扩展至整个应用程序流量平台。通过注释 OpenAPI 规范，开发人员可以根据自己的意图配置流量。TSB 跨网关和网格内部实施这些配置，从而提供了规则执行的灵活性。这支持身份验证、授权、速率限制、WAF 策略和通过 WebAssembly (WASM) 进行请求转换等功能。\n流量分流和迁移 TSB 简化了迁移、流量分割和金丝雀部署的流量管理。该平台使应用程序开发人员能够通过简单的配置过程来改变服务处理流量的方式。TSB 确保安全，使迁移和变更可扩展且易于管理。借助 TSB 的可观测性功能，可以促进可靠的监控和回滚，从而在基础设施转换和更新期间保持应用程序的可用性。\n结论 Tetrate Service Bridge 的流量管理功能提供对服务如何通信和处理传入请求的精细控制。从网关层次结构到智能流量路由和多集群支持，TSB 简化了复杂环境中的流量管理，增强了应用程序可用性和性能，同时简化了部署流程。\n","relpermalink":"/book/tsb/concepts/traffic-management/","summary":"TSB 中的流量管理和多集群路由。","title":"流量管理"},{"content":"启动 AWS EC2 实例 使用以下配置启动 AWS EC2 实例：\n选择带有 Ubuntu Server（DEB）的 64 位 (x86) AMI 镜像。 选择最小的 实例类型，例如 t2.micro（1 个 vCPU，1 GiB RAM） 或 t2.nano（1 个 vCPU，0.5 GiB RAM） 选择默认 VPC（以使你的实例具有公共 IP） 将 自动分配公共 IP 设置为 启用 配置 安全组，允许从 0.0.0.0/0 的端口 9080 接收流量 为了本指南的目的，你将创建一个具有公共 IP 的 EC2 实例，以便进行配置。\n注意 这不建议用于生产场景。对于生产场景，你应该做相反的操作，将 Kubernetes 集群和 EC2 实例放在同一网络上，或进行网络对等连接，并不为你的虚拟机分配公共 IP。 安装 Bookinfo Ratings 应用程序 SSH 进入你创建的 AWS EC2 实例，并安装 ratings 应用程序。执行以下命令：\n# 安装最新版本的受信任 CA 证书 sudo apt-get update -y sudo apt-get install -y ca-certificates # 添加具有 Node.js 的 DEB 仓库 curl --fail --silent --location https://deb.nodesource.com/setup_14.x | sudo bash - # 安装 Node.js sudo apt-get install -y nodejs # 下载 Bookinfo Ratings 应用程序的 DEB 包 curl -fLO https://dl.cloudsmith.io/public/tetrate/onboarding-examples/raw/files/bookinfo-ratings.deb # 安装 DEB 包 sudo apt-get install -y ./bookinfo-ratings.deb # 删除已下载的文件 rm bookinfo-ratings.deb # 启用 SystemD 单元 sudo systemctl enable bookinfo-ratings # 启动 Bookinfo Ratings 应用程序 sudo systemctl start bookinfo-ratings 验证 ratings 应用程序 执行以下命令验证 ratings 应用程序现在可以提供本地请求：\ncurl -fsS http://localhost:9080/ratings/1 你应该会得到类似以下的输出：\n{\u0026#34;id\u0026#34;:1,\u0026#34;ratings\u0026#34;:{\u0026#34;Reviewer1\u0026#34;:5,\u0026#34;Reviewer2\u0026#34;:4}} 配置信任示例 CA 请记住，你之前使用自定义 CA 签名的 TLS 证书配置了 Workload Onboarding Endpoint。因此，运行在 AWS EC2 实例上并尝试连接到 Workload Onboarding Endpoint 的任何软件默认不信任其证书。\n在继续之前，你必须配置 EC2 实例以信任你的自定义 CA。\n首先，更新 apt 包列表：\nsudo apt-get update -y 然后安装 ca-certificates 包：\nsudo apt-get install -y ca-certificates 将你在设置证书时创建的文件 中的 example-ca.crt.pem 文件的内容复制并放置在 EC2 实例上的路径 /usr/local/share/ca-certificates/example-ca.crt 下。\n可以使用你喜欢的工具来执行此操作。如果你尚未安装任何编辑器或工具，你可以使用以下步骤组合 cat 和 dd：\n执行 cat \u0026lt;\u0026lt;EOF | sudo dd of=/usr/local/share/ca-certificates/example-ca.crt 复制 example-ca.crt.pem 的内容，并粘贴到执行上一步的终端中 输入 EOF 并按 Enter 完成第一个命令 将自定义 CA 放在正确位置后，执行以下命令：\nsudo update-ca-certificates 这将重新加载可信 CA 的列表，并包括你的自定义 CA。\n安装 Istio Sidecar 通过执行以下命令安装 Istio sidecar。将 ONBOARDING_ENDPOINT_ADDRESS 替换为 [你之前获取\n的值](../enable-workload-onboarding#verify-the-workload-onboarding-endpoint)。\n# 下载 DEB 包 curl -fLO \\ --connect-to \u0026#34;onboarding-endpoint.example:443:${ONBOARDING_ENDPOINT_ADDRESS}:443\u0026#34; \\ \u0026#34;https://onboarding-endpoint.example/install/deb/amd64/istio-sidecar.deb\u0026#34; # 下载校验和 curl -fLO \\ --connect-to \u0026#34;onboarding-endpoint.example:443:${ONBOARDING_ENDPOINT_ADDRESS}:443\u0026#34; \\ \u0026#34;https://onboarding-endpoint.example/install/deb/amd64/istio-sidecar.deb.sha256\u0026#34; # 验证校验和 sha256sum --check istio-sidecar.deb.sha256 # 安装 DEB 包 sudo apt-get install -y ./istio-sidecar.deb # 删除已下载的文件 rm istio-sidecar.deb istio-sidecar.deb.sha256 安装 Workload Onboarding Agent 通过执行以下命令安装 Workload Onboarding Agent。将 ONBOARDING_ENDPOINT_ADDRESS 替换为 你之前获取的值 。\n# 下载 DEB 包 curl -fLO \\ --connect-to \u0026#34;onboarding-endpoint.example:443:${ONBOARDING_ENDPOINT_ADDRESS}:443\u0026#34; \\ \u0026#34;https://onboarding-endpoint.example/install/deb/amd64/onboarding-agent.deb\u0026#34; # 下载校验和 curl -fLO \\ --connect-to \u0026#34;onboarding-endpoint.example:443:${ONBOARDING_ENDPOINT_ADDRESS}:443\u0026#34; \\ \u0026#34;https://onboarding-endpoint.example/install/deb/amd64/onboarding-agent.deb.sha256\u0026#34; # 验证校验和 sha256sum --check onboarding-agent.deb.sha256 # 安装 DEB 包 sudo apt-get install -y ./onboarding-agent.deb # 删除已下载的文件 rm onboarding-agent.deb onboarding-agent.deb.sha256 ","relpermalink":"/book/tsb/setup/workload-onboarding/quickstart/aws-ec2/configure-vm/","summary":"启动 AWS EC2 实例 使用以下配置启动 AWS EC2 实例： 选择带有 Ubuntu Server（DEB）的 64 位 (x86) AMI 镜像。 选择最小的 实例类型，例如 t2.micro（1 个 vCPU，1 GiB RAM） 或 t2.nano（1 个 vCPU，0.5 GiB RAM","title":"配置虚拟机"},{"content":"Tetrate Service Bridge (TSB) 是一个由各种协议相互连接的复杂组件集合。对于部署在 TSB 提供的服务网格上的应用程序来说，情况可能也是如此。在许多情况下，你需要检查、测试和验证各种 TSB 组件之间的网络连通性，以确保系统按预期工作。\n为了为你节省在 Kubernetes 集群中创建调试环境的时间，Tetrate 提供了一个调试容器，其中已经安装了大多数用于验证网络状态的工具集。例如，诸如 ping、curl、gpcurl、dig 等工具已经在此容器中安装。\n使用调试容器 只要可以访问适当的镜像仓库来下载容器镜像，这个调试容器就可以部署到任何集群中。\n容器镜像包含在 TSB 发行版中，并且将与运行 tctl install image-sync 命令 时的其余镜像一起同步到你的仓库中。\n要部署调试容器，请运行以下命令。将 \u0026lt;registry-location\u0026gt; 替换为你同步了 TSB 镜像的仓库 URL。\nkubectl run debug-container --image \u0026lt;registry-location\u0026gt;/tetrate-troubleshoot:${vars.versionNumber} -it -- ash 一旦创建了 Pod，你将被置于调试容器内的 shell 中，并且你可以运行必要的故障排除命令。\n检查网络连通性 如果你想要检查 TSB 集群到你使用的数据存储（我们假设在此示例中是 PostgreSQL）的网络连通性，你可以运行以下命令：\ncurl -v telnet://\u0026lt;postgres_IP\u0026gt;:5432 或者使用 PostgreSQL 客户端命令 psql 来验证凭据。\npsql -h my.postgres.local -P 5432 -U myUser ","relpermalink":"/book/tsb/troubleshooting/debug-container/","summary":"如何运行和使用调试容器。","title":"使用调试容器"},{"content":"此 Chart 安装 TSB 数据平面 Operator，用于管理 网关 ，如 Ingress 网关、Tier-1 网关和 Egress 网关的生命周期。\n注意 如果你正在使用基于版本的控制平面，则不再需要数据平面 Operator 来管理 Istio 网关。要了解有关基于版本的控制平面的更多信息，请参阅 Istio 隔离边界 文档。 安装 要安装数据平面 Operator，请运行以下 Helm 命令。确保将 \u0026lt;tsb-version\u0026gt; 和 \u0026lt;registry-location\u0026gt; 替换为正确的值。\nhelm install dp tetrate-tsb-helm/dataplane \\ --version \u0026lt;tsb-version\u0026gt; \\ --namespace istio-gateway --create-namespace \\ --set image.registry=\u0026lt;registry-location\u0026gt; 配置 镜像配置 这是一个 必填 字段。将 image.registry 设置为你的私有注册表位置，你已经同步了 TSB 镜像，将 image.tag 设置为要部署的 TSB 版本。\n名称 描述 默认值 image.registry 用于下载 Operator 镜像的注册表 containers.dl.tetrate.io image.tag Operator 镜像的标签 与图表版本相同 Operator 扩展配置 这是一个 可选 字段。你可以使用以下可选属性自定义与 TSB Operator 相关的资源，如部署、服务或服务帐户：\n名称 描述 默认值 operator.deployment.affinity 用于 pod 的亲和性配置 operator.deployment.annotations 自定义的注释集，用于添加到部署中 operator.deployment.env 自定义的环境变量集，用于添加到容器中 operator.deployment.podAnnotations 自定义的注释集，用于添加到 pod 中 operator.deployment.replicaCount 部署管理的副本数量 operator.deployment.strategy 要使用的部署策略 operator.deployment.tolerations 适用于 pod 调度的容忍集合 operator.deployment.podSecurityContext SecurityContext 用于应用于 pod 的属性 operator.deployment.containerSecurityContext SecurityContext 用于应用于 pod 的容器的属性 operator.service.annotations 自定义的注释集，用于添加到服务中 operator.serviceAccount.annotations 自定义的注释集，用于添加到服务帐户中 operator.serviceAccount.imagePullSecrets 需要能够从注册表中拉取镜像的密钥名称集合 operator.pullSecret 将存储为镜像拉取密钥的 Docker JSON 配置字符串 ","relpermalink":"/book/tsb/setup/helm/dataplane/","summary":"如何使用 Helm 安装数据平面元素。","title":"数据平面安装"},{"content":"如果删除网关（例如在缩容事件期间），远程集群将继续尝试将流量发送到网关 IP 地址，直到它们收到网关的 IP 地址已被移除的更新为止。这可能会导致 HTTP 流量的 503 错误或直通跨集群流量的 000 错误。\n自 TSB 1.6 版以来，你可以通过可配置的周期来延迟网关删除，以便提供足够的时间使网关的 IP 地址移除传播到其他集群，以避免 503 或 000 错误。目前，此功能默认处于禁用状态。\n启用网关删除保持 Webhook 为了在控制平面中启用网关删除保持 Webhook，你需要编辑 ControlPlane CR 或 Helm 值中的 xcp 组件，并添加以下环境变量：\nENABLE_GATEWAY_DELETE_HOLD，将其值设置为 true GATEWAY_DELETE_HOLD_SECONDS。这是可选的，默认值为 10 秒 spec: components: xcp: ... kubeSpec: deployment: env: - name: ENABLE_GATEWAY_DELETE_HOLD value: \u0026#34;true\u0026#34; - name: GATEWAY_DELETE_HOLD_SECONDS value: \u0026#34;20\u0026#34; ... 这将在删除的网关 IP 从远程集群中移除时延迟网关删除 20 秒。\n","relpermalink":"/book/tsb/operations/features/gateway-deletion-webhook/","summary":"如何启用网关删除保持 Webhook 以保持删除操作并允许配置更改在所有集群间传播。","title":"网关删除保持 Webhook"},{"content":"流量限制允许你根据诸如源 IP 地址和 HTTP 头部等流量属性，将通过 TSB 的流量限制在预定限额内。\n如果你担心以下任何一点，你可能需要考虑流量限制：\n防止恶意活动，如 DDoS 攻击。 防止你的应用程序及其资源（如数据库）过载。 实现某种业务逻辑，如为不同用户组设置不同的 API 限制。 TSB 支持两种流量限制模式：内部和外部。\n内部流量限制 此模式在一个集群内或跨多个集群实现全局流量限制。 在此模式中，你可以使用 API 根据各种流量属性配置限制。\n在幕后，TSB 的全局控制平面在每个集群部署一个速率限制服务器，该服务器作为一个全局服务，从多个 Envoy 代理接收元数据，并根据配置作出流量限制决策。\n此模式要求用户设置一个 Redis 服务器，用作存储后端，以持久化速率限制元数据计数。\n如果你希望利用流量限制功能而无需实现自己的流量限制服务，我们建议使用此模式。\n有关如何启用内部流量限制的详细信息，请阅读此文档 。\n外部流量限制 在此模式中，你将部署一个实现 Envoy 速率限制服务接口 的速率限制服务器，并配置 API 根据指定的标准将速率限制元数据发送到你的服务器。\n外部速率限制服务器做出的决策将在 TSB 内的 Envoy 代理中强制执行。\n如果你希望实现自己的流量限制服务，或者希望将流量限制决策逻辑从 TSB 分离出来成为其自己的服务，我们建议使用此模式。\n流量限制上下文 流量限制可以在不同的上下文中配置。虽然你可以根据自己的意愿在任何这些上下文中自定义其行为，但某些类型的流量限制在特定上下文中处理效果更佳。\nTier1Gateway (YAML ) 根据源 IP 地址限制恶意流量 IngressGateway / Tier2 Gateway / Application Gateway (YAML ) 基于业务逻辑实现流量限制，或保护你的应用程序免于过载 TrafficSettings (YAML ) 对与 TrafficSettings 关联的命名空间中的所有代理应用流量限制。用于保护应用程序免于过载 启用内部速率限制服务器\nTSB 入口网关中的速率限制\n服务之间的速率限制\nTier-1 网关中的速率限制\n配置外部速率限制服务器\n带 TLS 验证的外部速率限制\n","relpermalink":"/book/tsb/howto/rate-limiting/","summary":"如何在你的网络上应用流量限制。","title":"限制流量速率"},{"content":"在继续之前，请确保你熟悉Istio 隔离边界 功能。\n修订的 Istio CNI 在修订的 Istio 环境中，Istio CNI 可以绑定到特定的 Istio 修订版本。考虑以下隔离边界配置，允许管理修订的 Istio 环境：\nspec: ... components: xcp: isolationBoundaries: - name: global revisions: - name: stable istio: tsbVersion: 1.6.1 一旦有了修订版本，可以按照以下所示的隔离边界配置启用 Istio CNI，并指定修订版本：\n非 OpenShift\napiVersion: install.tetrate.io/v1alpha1 kind: ControlPlane metadata: name: \u0026lt;cluster-name\u0026gt; namespace: istio-system spec: components: istio: kubeSpec: CNI: chained: true binaryDirectory: /opt/cni/bin configurationDirectory: /etc/cni/net.d revision: stable ... xcp: isolationBoundaries: - name: global revisions: - name: stable istio: tsbVersion: 1.6.1 ... hub: \u0026lt;registry-location\u0026gt; managementPlane: host: \u0026lt;tsb-address\u0026gt; port: \u0026lt;tsb-port\u0026gt; clusterName: \u0026lt;cluster-name\u0026gt; telemetryStore: elastic: host: \u0026lt;elastic-hostname-or-ip\u0026gt; port: \u0026lt;elastic-port\u0026gt; version: \u0026lt;elastic-version\u0026gt; OpenShift\napiVersion: install.tetrate.io/v1alpha1 kind: ControlPlane metadata: name: \u0026lt;cluster-name\u0026gt; namespace: istio-system spec: components: istio: kubeSpec: CNI: revision: stable ... xcp: isolationBoundaries: - name: global revisions: - name: stable istio: tsbVersion: 1.6.1 ... hub: \u0026lt;registry-location\u0026gt; managementPlane: host: \u0026lt;tsb-address\u0026gt; port: \u0026lt;tsb-port\u0026gt; clusterName: \u0026lt;cluster-name\u0026gt; telemetryStore: elastic: host: \u0026lt;elastic-hostname-or-ip\u0026gt; port: \u0026lt;elastic-port\u0026gt; version: \u0026lt;elastic-version\u0026gt; Brownfield 设置 在棕地设置中，已启用隔离边界和 Istio CNI - revision值默认为具有最新的revisions[].istio.tsbVersion的修订版本。 如果存在多个这样的tsbVersion，则会根据revisions[].name的字母顺序优先考虑。 Istio CNI 升级 一旦 Istio CNI 与修订版本绑定，升级到不同的修订版本就非常简单。\n首先，在隔离边界配置中添加一个canary Istio 控制平面。\nspec: ... components: xcp: isolationBoundaries: - name: global revisions: - name: stable istio: tsbVersion: 1.6.1 - name: canary istio: tsbVersion: 1.6.1-rc1 然后，更新 Istio CNI 设置下的revision值，指向canary修订版本，如下所示。\nOpenshift 对于 Openshift 环境，默认情况下启用 Istio CNI，不需要特定的配置。因此，在 Openshift 中管理修订的 Istio CNI 只支持revision字段。 非 Openshift\napiVersion: install.tetrate.io/v1alpha1 kind: ControlPlane metadata: name: \u0026lt;cluster-name\u0026gt; namespace: istio-system spec: components: istio: kubeSpec: CNI: chained: true binaryDirectory: /opt/cni/bin configurationDirectory: /etc/cni/net.d revision: canary ... xcp: isolationBoundaries: - name: global revisions: - name: stable istio: tsbVersion: 1.6.1 - name: canary istio: tsbVersion: 1.6.1-rc1 ... Openshift\napiVersion: install.tetrate.io/v1alpha1 kind: ControlPlane metadata: name: \u0026lt;cluster-name\u0026gt; namespace: istio-system spec: components: istio: kubeSpec: CNI: revision: canary ... xcp: isolationBoundaries: - name: global revisions: - name: stable istio: tsbVersion: 1.6.1 - name: canary istio: tsbVersion: 1.6.1-rc1 ... ","relpermalink":"/book/tsb/setup/upgrades/cni-upgrade/","summary":"如何管理修订的 Istio CNI 并将 Istio CNI 从一个修订版本升级到另一个版本。","title":"修订的 Istio CNI 和升级"},{"content":"TSB 拥有一个 teamsync 组件，它会定期连接到你的身份提供者（IdP），并将用户和团队信息同步到 TSB 中。\n目前，teamsync 支持LDAP 和Azure AD ，并会自动为你执行正确的操作。但是，如果你使用其他 IdP，你将需要手动执行这些任务。本文将描述如何执行这些任务。\n在开始之前，请确保你已经：\n安装了 TSB 管理平面 使用管理员帐户登录到 TSB 。 获取你的 TSB 的组织名称 - 确保使用在 TSB ManagementPlane CR 中配置的安装时的组织名称。 创建组织 Teamsync 不仅同步你的用户和团队，还会在首次运行 TSB 管理平面组件安装后创建一个组织。\n因此，如果你使用的 IdP 不受 teamsync 支持，你还需要手动执行此步骤。\n要创建一个组织，请创建以下organization.yaml文件，然后使用 tctl 应用它\napiVersion: api.tsb.tetrate.io/v2 kind: Organization metadata: name: \u0026lt;organization-name\u0026gt; tctl apply -f organization.yaml 手动同步用户和团队 同步涉及从 IdP 获取用户和团队信息，然后将它们转换为 TSB 同步 API 负载的结构，然后将同步请求发送到 TSB API 服务器。一旦它们被同步，你可以为用户和团队分配角色，以赋予它们访问 TSB 资源的权限。\n从 IdP 获取用户和团队 此步骤的详细信息将取决于你的 IdP。你应该查阅你的 IdP 文档，了解如何获取用户和团队。例如，如果你使用 Okta，你可以使用List users 和List groups API。类似地，如果你使用 Keycloak，你可以使用List users 和List groups API。\n将数据转换为 TSB 同步 API 负载 一旦你从 IdP 获取用户和团队列表，你需要将它们转换为 TSB 同步 API 负载格式。如何执行此转换的确切细节取决于你的 IdP API 的负载格式。\n以下是同步 API 负载的示例。有关更多详细信息，请参阅同步组织 API。\n{ \u0026#34;sourceType\u0026#34;: \u0026#34;MANUAL\u0026#34;, \u0026#34;users\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;user_1_id\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;user_1@email.com\u0026#34;, \u0026#34;loginName\u0026#34;: \u0026#34;user1\u0026#34;, \u0026#34;displayName\u0026#34;: \u0026#34;用户 1\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;user_2_id\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;user_2@email.com\u0026#34;, \u0026#34;loginName\u0026#34;: \u0026#34;user2\u0026#34;, \u0026#34;displayName\u0026#34;: \u0026#34;用户 2\u0026#34; }, ], \u0026#34;teams\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;team_1_id\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;团队 1 描述\u0026#34;, \u0026#34;displayName\u0026#34;: \u0026#34;团队 1\u0026#34;, \u0026#34;memberUserIds\u0026#34;: [ \u0026#34;user_1_id\u0026#34; ] }, { \u0026#34;id\u0026#34;: \u0026#34;team_2_id\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;团队 2 描述\u0026#34;, \u0026#34;displayName\u0026#34;: \u0026#34;团队 2\u0026#34;, \u0026#34;memberUserIds\u0026#34;: [ \u0026#34;user_2_id\u0026#34; ] }, ] } 发送同步 API 请求 在将 IdP 负载转换为 TSB 同步 API 负载后，你可以向 TSB API 服务器发送请求以同步数据。\n以下示例使用curl向运行在\u0026lt;tsb-host\u0026gt;:8443上的 TSB API 服务器发送请求，使用 TSB 管理员用户凭据。假定 TSB 同步 API 负载存储在文件/path/to/data.json中\ncurl --request POST \\ --url https://\u0026lt;tsb-host\u0026gt;:8443/v2/organizations/tetrate/sync \\ --header \u0026#39;Authorization: Basic base64(\u0026lt;admin\u0026gt;:\u0026lt;admin-password\u0026gt;) \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --data-binary \u0026#39;@/path/to/data.json\u0026#39; 自动化流程 现在你知道 teamsync 如何工作，你可以创建一个定期运行的服务（例如作为cron作业），使用你喜欢的编程语言来自动化同步过程。\n这是关于 TSB 中用户同步的文档。TSB 有一个 teamsync 组件，用于定期连接到你的身份提供者（IdP），将用户和团队信息同步到 TSB 中。目前，teamsync 支持 LDAP 和 Azure AD，但如果你使用其他 IdP，需要手动执行同步任务。\n在开始之前，确保你已经完成了以下步骤：\n安装了 TSB 管理平面 使用管理员帐户登录到 TSB 获取了 TSB 的组织名称，这应该是在安装 TSB 时配置的组织名称。 首先，文档介绍了如何手动创建一个组织，因为 teamsync 在首次运行时会自动创建组织。然后，它详细描述了如何手动从 IdP 获取用户和团队信息，将其转换为 TSB 同步 API 负载格式，然后将其同步到 TSB。最后，文档提到可以自动化这个过程，例如使用定期运行的服务来自动同步用户和团队信息。\n","relpermalink":"/book/tsb/operations/users/user-synchronization/","summary":"TSB 拥有一个 teamsync 组件，它会定期连接到你的身份提供者（IdP），并将用户和团队信息同步到 TSB 中。 目前，teamsync 支持LDAP 和Azure AD ，并会自动为你执行正确的操作。但是，如果你使用其他 IdP，你将需","title":"用户同步"},{"content":"TSB 提供了授权功能，用于授权来自公共网络的请求。本文将描述如何使用 Open Policy Agent (OPA) 配置 Tier-1 网关授权。\n在开始之前，请确保你：\n熟悉TSB 概念 。 已完成了 Tier-1 网关路由到 Tier-2 网关，并在 TSB 中已配置了httpbin 。 创建了一个租户，并了解工作空间和配置组。 针对你的 TSB 环境配置了 tctl。 以下图示展示了在 Tier-1 网关中使用 OPA 的请求/响应流程。来到 Tier-1 网关的请求将由 OPA 检查。如果请求被视为未经授权，则请求将被拒绝并返回 403（禁止）响应，否则它们将被发送到 Tier-2 网关。\n部署 httpbin 服务 按照本文中的说明 创建 httpbin 服务，并确保该服务在 httpbin.tetrate.com 上公开。\n配置 OPA 在此示例中，你将部署 OPA 作为其自己独立的服务。如果尚未这样做，请为 OPA 服务创建一个命名空间：\nkubectl create namespace opa 按照OPA 文档 中的说明创建使用基本身份验证的 OPA 策略 ，并在 opa 命名空间中部署 OPA 服务和代理。\nkubectl apply -f opa.yaml 然后更新你的 Tier-1 网关配置以使用 OpenAPI 规范，将以下部分添加到 Tier-1 网关，并使用 tctl 应用它们：\napiVersion: gateway.tsb.tetrate.io/v2 kind: Tier1Gateway metadata: organization: tetrate tenant: tetrate workspace: tier1 group: tier1 name: tier1gw spec: workloadSelector: namespace: tier1 labels: app: tier1gw istio: ingressgateway externalServers: - name: httpbin hostname: httpbin.tetrate.com port: 443 tls: mode: SIMPLE secretName: tier1-cert clusters: - labels: network: tier2 authorization: external: uri: grpc://opa.opa.svc.cluster.local:9191 测试 你可以按照“在 Ingress Gateways 中配置外部授权” 中的说明进行外部授权测试，但需要获取 Tier-1 网关地址而不是 Ingress 网关地址。\n要获取 Tier-1 网关地址，请执行以下命令：\nkubectl -n tier1 get service tier1-gateway \\ -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39; 然后按照说明 操作，但请将 gateway-ip 的值替换为 Tier-1 网关的地址。\n","relpermalink":"/book/tsb/howto/authorization/tier1-gateway/","summary":"如何使用 Open Policy Agent (OPA) 来授权来自公共网络的请求。","title":"在 Tier-1 网关中使用外部授权"},{"content":" 工作负载载入指南\n快速载入\n使用 tctl 将虚拟机（VM）接入 TSB 服务网格\n","relpermalink":"/book/tsb/setup/workload-onboarding/","summary":"工作负载载入指南 快速载入 使用 tctl 将虚拟机（VM）接入 TSB 服务网格","title":"载入工作负载"},{"content":"你可以从此存储库的最新发布页面 下载最新的 Argo CD 版本，其中将包含argocd CLI。\nLinux 和 WSL ArchLinux pacman -S argocd Homebrew brew install argocd 使用 curl 下载 下载最新版本 curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64 sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd rm argocd-linux-amd64 下载具体版本 VERSION将以下命令中的替换设置\u0026lt;TAG\u0026gt;为你要下载的 Argo CD 版本：\nVERSION=\u0026lt;TAG\u0026gt; # Select desired TAG from https://github.com/argoproj/argo-cd/releases curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/download/$VERSION/argocd-linux-amd64 sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd rm argocd-linux-amd64 你现在应该能够运行argocd命令。\nMac (M1) 使用 curl 下载 你可以在上面的链接查看最新版本的 Argo CD 或运行以下命令来获取版本：\nVERSION=$(curl --silent \u0026#34;https://api.github.com/repos/argoproj/argo-cd/releases/latest\u0026#34; | grep \u0026#39;\u0026#34;tag_name\u0026#34;\u0026#39; | sed -E \u0026#39;s/.*\u0026#34;([^\u0026#34;]+)\u0026#34;.*/\\1/\u0026#39;) 将以下命令替换VERSION为你要下载的 Argo CD 版本：\ncurl -sSL -o argocd-darwin-arm64 https://github.com/argoproj/argo-cd/releases/download/$VERSION/argocd-darwin-arm64 安装 Argo CD CLI 二进制文件：\nsudo install -m 555 argocd-darwin-arm64 /usr/local/bin/argocd rm argocd-darwin-arm64 Mac Homebrew brew install argocd 使用 curl 下载 你可以在上面的链接查看最新版本的 Argo CD 或运行以下命令来获取版本：\nVERSION=$(curl --silent \u0026#34;https://api.github.com/repos/argoproj/argo-cd/releases/latest\u0026#34; | grep \u0026#39;\u0026#34;tag_name\u0026#34;\u0026#39; | sed -E \u0026#39;s/.*\u0026#34;([^\u0026#34;]+)\u0026#34;.*/\\1/\u0026#39;) 将以下命令替换VERSION为你要下载的 Argo CD 版本：\ncurl -sSL -o argocd-darwin-amd64 https://github.com/argoproj/argo-cd/releases/download/$VERSION/argocd-darwin-amd64 安装 Argo CD CLI 二进制文件：\nsudo install -m 555 argocd-darwin-amd64 /usr/local/bin/argocd rm argocd-darwin-amd64 完成上述任一说明后，你现在应该能够运行argocd命令。\nWindows 使用 PowerShell 下载：Invoke-WebRequest 你可以在上面的链接查看最新版本的 Argo CD 或运行以下命令来获取版本：\n$version = (Invoke-RestMethod https://api.github.com/repos/argoproj/argo-cd/releases/latest).tag_name 将以下命令替换$version为你要下载的 Argo CD 版本：\n$url = \u0026#34;https://github.com/argoproj/argo-cd/releases/download/\u0026#34; + $version + \u0026#34;/argocd-windows-amd64.exe\u0026#34; $output = \u0026#34;argocd.exe\u0026#34; Invoke-WebRequest -Uri $url -OutFile $output 另请注意，你可能需要将该文件移至你的 PATH 中。\n完成上述说明后，你现在应该能够运行argocd命令。\n","relpermalink":"/book/argo-cd/cli-installation/","summary":"你可以从此存储库的最新发布页面 下载最新的 Argo CD 版本，其中将包含argocd CLI。 Linux 和 WSL ArchLinux pacman -S argocd Homebrew brew install argocd 使用 curl 下载 下载最新版本 curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64 sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd rm argocd-linux-amd64 下载具体版本 VERSION将以下命令中的替换设置\u0026","title":"安装"},{"content":"要求 AWS 负载均衡器控制器 v1.1.5 或更高版本 概述 AWS 负载均衡器控制器 （也称为 AWS ALB Ingress Controller）通过配置 AWS 应用程序负载均衡器（ALB）以将流量路由到一个或多个 Kubernetes 服务的 Ingress 对象，实现流量管理。ALB 通过加权目标组 的概念提供了高级流量分割功能。AWS 负载均衡器控制器通过对 Ingress 对象的注解进行配置“操作”来支持此功能。\n工作原理 ALB 通过侦听器和包含操作的规则进行配置。侦听器定义客户端的流量如何进入，规则定义如何使用各种操作处理这些请求。一种操作类型允许用户将流量转发到多个目标组（每个目标组都定义为 Kubernetes 服务）。你可以在此处 阅读有关 ALB 概念的更多信息。\n由 AWS 负载均衡器控制器管理的 Ingress 通过注解和规范控制 ALB 的侦听器和规则。为了在多个目标组（例如不同的 Kubernetes 服务）之间分割流量，AWS 负载均衡器控制器查看 Ingress 上的特定“操作”注解，alb.ingress.kubernetes.io/actions.\u0026lt;service-name\u0026gt; 。这个注解是通过 Rollout 自动注入和更新的，根据所需的流量权重进行更新。\n用法 要配置 Rollout 使用 ALB 集成并在更新期间在金丝雀和稳定服务之间分割流量，请使用以下字段配置 Rollout：\napiVersion: argoproj.io/v1alpha1 kind: Rollout ... spec: strategy: canary: # canaryService 和 stableService 是对 Service 的引用，Rollout 将使用它们来指定 # canary ReplicaSet 和 stable ReplicaSet (必填)。 canaryService: canary-service stableService: stable-service trafficRouting: alb: # 引用的 Ingress 将被注入自定义 action annotation，指示 AWS Load Balancer Controller # 按照所需的流量权重在 canary 和 stable Service 之间分配流量 (必填)。 ingress: ingress # Ingress 必须在其中一个规则中指向的 Service 的引用 (可选)。 # 如果省略，使用 canary.stableService。 rootService: root-service # Service 端口是 Service 监听的端口 (必填)。 servicePort: 443 所引用的 Ingress 应部署具有匹配 Rollout 服务的 Ingress 规则：\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress annotations: kubernetes.io/ingress.class: alb spec: rules: - http: paths: - path: /* backend: # serviceName 必须匹配 canary.trafficrouting.alb.rootservice(如果指定了)，或者 canary.rootservice.stableService(如果忽略了 rootService) serviceName: root-service # servicePort 必须是 use-annotation 的值 # 这将指示 AWS 负载平衡器控制器查看有关如何引导流量的注解 servicePort: use-annotation 在更新期间，Rollout 控制器注入alb.ingress.kubernetes.io/actions.\u0026lt;SERVICE-NAME\u0026gt;注解，其中包含 AWS Load Balancer 控制器理解的 JSON 有效负载，指示它根据当前金丝雀权重在canaryService和stableService之间分割流量。\n以下是我们的示例 Ingress 在 Rollout 注入将流量分割为金丝雀服务和稳定服务，流量权重分别为 10 和 90 的自定义操作注解后的示例：\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/actions.root-service: | { \u0026#34;Type\u0026#34;:\u0026#34;forward\u0026#34;, \u0026#34;ForwardConfig\u0026#34;:{ \u0026#34;TargetGroups\u0026#34;:[ { \u0026#34;Weight\u0026#34;:10, \u0026#34;ServiceName\u0026#34;:\u0026#34;canary-service\u0026#34;, \u0026#34;ServicePort\u0026#34;:\u0026#34;80\u0026#34; }, { \u0026#34;Weight\u0026#34;:90, \u0026#34;ServiceName\u0026#34;:\u0026#34;stable-service\u0026#34;, \u0026#34;ServicePort\u0026#34;:\u0026#34;80\u0026#34; } ] } } spec: rules: - http: paths: - path: /* backend: serviceName: root-service servicePort: use-annotation 🔔 注意：Argo rollouts 另外注入一个注解rollouts.argoproj.io/managed-alb-actions，用于记账目的。注解指示 Rollout 对象正在管理哪些操作（因为多个 Rollout 可以引用一个 Ingress）。在回滚删除时，回滚控制器查找此注解以了解此操作不再受管理，并将其重置为仅指向带有 100 权重的稳定服务。\nrootService 默认情况下，Rollout 将使用在spec.strategy.canary.stableService下指定的服务/操作名称在服务/操作名称下注入alb.ingress.kubernetes.io/actions.\u0026lt;SERVICE-NAME\u0026gt;注解。但是，可能需要指定与stableService不同的显式服务/操作名称。例如，one pattern 是使用包含三个不同规则以单独到达金丝雀，稳定和根服务的单个 Ingress（例如，用于测试目的）。在这种情况下，你可能希望将“根”服务指定为服务/操作名称，而不是稳定服务。要这样做，请在 alb 规范下引用rootService下的服务：\napiVersion: argoproj.io/v1alpha1 kind: Rollout spec: strategy: canary: canaryService: guestbook-canary stableService: guestbook-stable trafficRouting: alb: rootService: guestbook-root ... 粘性会话 因为使用至少两个目标组（金丝雀和稳定），所以目标组粘性需要额外的配置：\n必须通过目标组激活粘性会话\napiVersion: argoproj.io/v1alpha1 kind: Rollout spec: strategy: canary: ... trafficRouting: alb: stickinessConfig: enabled: true durationSeconds: 3600 ... 有关更多信息，请参见AWS ALB API 使用 AWS TargetGroup 验证进行零停机更新 当与 AWS LoadBalancer 控制器一起使用时，Argo Rollouts 包含两个功能可帮助确保零停机更新：TargetGroup IP 验证和 TargetGroup 权重验证。这两个功能都涉及 Rollout 控制器向 AWS 执行附加的安全性检查，以验证对 Ingress 对象所做的更改是否反映在基础 AWS TargetGroup 中。\nTargetGroup IP 验证 🔔 注意：Target Group IP 验证自 Argo Rollouts v1.1 起提供\nAWS 负载均衡器控制器可以运行在以下两种模式之一：\nInstance mode IP mode 当使用 AWS 负载均衡器控制器的 IP 模式（例如使用 AWS CNI）时，只有在 AWS 负载均衡器控制器处于 IP 模式时，才适用于 TargetGroup IP 验证。在 IP 模式下使用 AWS 负载均衡器控制器时，ALB 负载均衡器将目标组定位到单个 Pod IP，而不是 K8s 节点实例。针对 Pod IP 进行定位在更新期间存在更高的风险，因为来自底层 AWS TargetGroup 的 Pod IP 可以更容易地从实际可用性和 Pod 状态过时，从而导致当 TargetGroup 指向已经缩小的 Pod 时，发生 HTTP 502 错误。\n为了减轻这种风险，AWS 建议在 IP 模式下运行 AWS 负载均衡器控制器时使用pod readiness gate injection 。Readiness gates 允许 AWS 负载均衡器控制器在将新创建的 Pod 标记为“ready”之前验证 TargetGroups 是否准确，从而防止较旧的 ReplicaSet 的过早缩小。\nPod readiness gate injection 使用一个变异的 webhook，在创建 Pod 时根据以下条件决定是否注入准备就绪的门：\n在同一命名空间中存在与 Pod 标签匹配的服务 存在至少一个引用匹配的服务的目标组绑定 另一种描述这种方式的方法是：AWS 负载均衡器控制器仅在从（ALB）Ingress 到达 Pod 的情况下将就绪门注入到 Pod 中。如果（ALB）Ingress 引用与 Pod 标签匹配的服务，则将 Pod 视为可达。它忽略所有其他 Pod。\n使用这种方式的一个挑战是，Service 选择器标签（spec.selector）的修改不允许 AWS 负载均衡器控制器注入就绪门，因为在那时 Pod 已经创建（就绪门是不可变的）。请注意，这是更改任何ALB 服务的服务选择器的问题，而不仅仅是 Argo Rollouts。\n由于 Argo Rollout 的蓝绿策略通过在推广期间修改 activeService 选择器以指向新的 ReplicaSet 标签来工作，因此它存在一个问题，即无法注入 spec.strategy.blueGreen.activeService 的可读性门。这意味着在从 V1 更新到 V2 的以下问题场景中存在可能的停机时间：\n触发更新并增加 V2 ReplicaSet 堆栈 V2 ReplicaSet pods 变得完全可用并准备好进行推广 Rollout 通过将活动服务的标签选择器更新为指向 V2 堆栈（从 V1）来推广 V2 由于未知问题（例如，AWS 负载均衡器控制器停机，AWS 速率限制），V2 Pod IP 的注册未发生或延迟。 V1 ReplicaSet 被缩小以完成更新 在第 5 步之后，当 V1 ReplicaSet 被缩小时，过时的 TargetGroup 仍将指向不再存在的 V1 Pods IPs，从而导致停机时间。\n为了允许零停机更新，Argo Rollouts 具有执行 TargetGroup IP 验证作为更新的附加安全措施的能力。当启用此功能时，每当进行服务选择器修改时，Rollout 控制器都会阻止更新的进展，直到它可以验证 TargetGroup 正确地针对 bluegreen.activeService 的新 Pod …","relpermalink":"/book/argo-rollouts/traffic-management/alb/","summary":"要求 AWS 负载均衡器控制器 v1.1.5 或更高版本 概述 AWS 负载均衡器控制器 （也称为 AWS ALB Ingress Controller）通过配置 AWS 应用程序负载均衡器（ALB）以将流量路由到一个或多个 Kubernetes 服务的 Ingress 对象，实现流量管理。ALB 通过加权目","title":"AWS Load Balancer Controller (ALB)"},{"content":"本指南介绍了如何让 Argo Rollouts 与由 AWS App Mesh 管理的服务网格集成。本指南基于 基本入门指南 的概念构建。\n要求\n安装了 AWS App Mesh Controller for K8s 的 Kubernetes 集群 🔔 提示：请参阅 App Mesh Controler Installation instructions 以了解如何开始使用 Kubernetes 的 App Mesh。\n1. 部署 Rollout、服务和 App Mesh CRD 当 App Mesh 用作流量路由器时，Rollout canary 策略必须定义以下强制字段：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: my-rollout spec: strategy: canary: # canaryService 和 stableService 是指向 Rollout 将要修改的服务的引用，以便针对金丝雀 ReplicaSet 和稳定 ReplicaSet（均为必需）。 canaryService: my-svc-canary stableService: my-svc-stable trafficRouting: appMesh: # 引用的虚拟服务将用于确定虚拟路由器，后者将被操纵以更新金丝雀权重。 virtualService: # 虚拟服务 App Mesh CR 的名称 name: my-svc # 要更新的路由的可选集。如果为空，则更新与虚拟服务关联的所有路由。 routes: - http-primary # virtualNodeGroup 是一个结构，用于引用 App Mesh 虚拟节点 CR，该节点对应于金丝雀和稳定版本 virtualNodeGroup: # canaryVirtualNodeRef 指的是对应于金丝雀版本的虚拟节点的引用。Rollouts 控制器将会 # 更新这个虚拟节点的 podSelector 以匹配控制器生成的最新的 canary pod-hash。 canaryVirtualNodeRef: name: my-vn-canary # stableVirtualNodeRef 指的是对应于稳定版本的虚拟节点的引用。Rollouts 控制器将会 # 更新这个虚拟节点的 podSelector 以匹配控制器生成的最新的 stable pod-hash。 stableVirtualNodeRef: name: my-vn-stable steps: - setWeight: 25 - pause: {} ... 在本指南中，这两个服务分别是：my-svc-canary 和 my-svc-stable。这两个服务对应的是两个名为 my-vn-canary 和 my-vn-stable 的虚拟节点 CR。此外，还有一个名为 rollout-demo-vsvc 的虚拟服务，它由一个名为 rollout-demo-vrouter 的虚拟路由器 CR 提供。这个虚拟路由器需要至少有一个路由，用于将流量转发到 canary 和 stable 虚拟节点。最初，canary 的权重设置为 0%，而 stable 的权重设置为 100%。在部署期间，控制器将根据 steps[N].setWeight 中定义的配置修改路由的权重。\ncanary 和 stable 服务的配置为无头服务。这是必要的，以便 App Mesh 正确处理由 canary 到 stable 重新分配的 pod 的连接池。\n总之，运行以下命令以部署服务：\n两个服务（stable 和 canary） 一个服务（用于 VIP 和 DNS 查询） 两个 App Mesh 虚拟节点（stable 和 canary） 一个具有指向虚拟节点的路由的 App Mesh 虚拟路由器 一个对应 VIP 服务的 App Mesh 虚拟服务 一个 rollout kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/examples/appmesh/canary-service.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/examples/appmesh/canary-rollout.yaml 2. 验证服务 首先确保 rollout 稳定。\nkubectl argo rollouts get rollout my-rollout -n argo-examples -w 然后确保服务正常运行。\nkubectl -n argo-examples port-forward svc/my-svc 8181:80 3. 部署新版本 现在是部署新版本的时候了。使用新镜像更新 rollout。\nkubectl argo rollouts set image my-rollout demo=argoproj/rollouts-demo:green -n argo-examples Rollout 应该会部署一个新的 canary 修订版本，并在虚拟路由器下更新权重。\nkubectl get -n argo-examples virtualrouter my-vrouter -o json | jq \u0026#34;.spec.routes[0].httpRoute.action.weightedTargets\u0026#34; [ { \u0026#34;virtualNodeRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;my-vn-canary\u0026#34; }, \u0026#34;weight\u0026#34;: 25 }, { \u0026#34;virtualNodeRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;my-vn-stable\u0026#34; }, \u0026#34;weight\u0026#34;: 75 } ] 现在手动批准无限期暂停的 rollout，并继续观察路由更新\nkubectl argo rollouts promote my-rollout -n argo-examples watch -d \u0026#39;kubectl get -n argo-examples virtualrouter my-vrouter -o json | jq \u0026#34;.spec.routes[0].httpRoute.action.weightedTargets\u0026#34;\u0026#39; ","relpermalink":"/book/argo-rollouts/getting-started/appmesh/index/","summary":"本指南介绍了如何让 Argo Rollouts 与由 AWS App Mesh 管理的服务网格集成。本指南基于 基本入门指南 的概念构建。 要求 安装了 AWS App Mesh Controller for K8s 的 Kubernetes 集群 🔔 提示：请参阅 App Mesh Controler Installation instructions 以了解如何开始使用 Kubernetes 的 App Mesh。 1. 部署 Rollout、服务","title":"AppMesh 快速开始"},{"content":"垂直 Pod 自动缩放（Vertical Pod Autoscaling，VPA）通过自动配置资源需求降低维护成本并提高集群资源利用率。\nVPA 模式 VPAs 有四种操作模式：\nAuto：VPA 在创建 pod 时分配资源请求，使用首选的更新机制更新现有 pod 的资源请求。目前，这相当于“Recreate”（见下文）。一旦在不重启（“in-place”）更新 pod 请求方面可用，它可能会成为“Auto”模式的首选更新机制。注意：此 VPA 功能是实验性的，可能会导致你的应用停机。 Recreate：VPA 在创建 pod 时分配资源请求，并在请求的资源与新建议显著不同时将其从现有 pod 中驱逐出来（如果定义了 Pod Disruption Budget，则会尊重它）。只有在需要确保每当资源请求更改时都重启 pod 时才应使用此模式。否则，请优先考虑“Auto”模式，一旦可用，该模式可以利用不重启更新。注意：此 VPA 功能是实验性的，可能会导致你的应用停机。 Initial：VPA 仅在创建 pod 时分配资源请求，从不更改它们。 Off：VPA 不会自动更改 pod 的资源要求。建议计算并可以在 VPA 对象中进行检查。 示例 以下是使用 Argo-Rollouts 的垂直 Pod 自动缩放器的示例。\nRollout 示例应用程序：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: vpa-demo-rollout namespace: test-vpa spec: replicas: 5 strategy: canary: steps: - setWeight: 20 - pause: {duration: 10} - setWeight: 40 - pause: {duration: 10} - setWeight: 60 - pause: {duration: 10} - setWeight: 80 - pause: {duration: 10} revisionHistoryLimit: 10 selector: matchLabels: app: vpa-demo-rollout template: metadata: labels: app: vpa-demo-rollout spec: containers: - name: vpa-demo-rollout image: ravihari/nginx:v1 ports: - containerPort: 80 resources: requests: cpu: \u0026#34;5m\u0026#34; memory: \u0026#34;5Mi\u0026#34; Rollout 示例应用程序的 VPA 配置：\napiVersion: \u0026#34;autoscaling.k8s.io/v1beta2\u0026#34; kind: VerticalPodAutoscaler metadata: name: vpa-rollout-example namespace: test-vpa spec: targetRef: apiVersion: \u0026#34;argoproj.io/v1alpha1\u0026#34; kind: Rollout name: vpa-demo-rollout updatePolicy: updateMode: \u0026#34;Auto\u0026#34; resourcePolicy: containerPolicies: - containerName: \u0026#39;*\u0026#39; minAllowed: cpu: 5m memory: 5Mi maxAllowed: cpu: 1 memory: 500Mi controlledResources: [\u0026#34;cpu\u0026#34;, \u0026#34;memory\u0026#34;] 最初部署时描述 VPA 时，我们不会看到推荐，因为它需要几分钟时间才能完成。\nName: kubengix-vpa Namespace: test-vpa Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: autoscaling.k8s.io/v1 Kind: VerticalPodAutoscaler Metadata: Creation Timestamp: 2022-03-14T12:54:06Z Generation: 1 Managed Fields: API Version: autoscaling.k8s.io/v1beta2 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:spec: .: f:resourcePolicy: .: f:containerPolicies: f:targetRef: .: f:apiVersion: f:kind: f:name: f:updatePolicy: .: f:updateMode: Manager: kubectl-client-side-apply Operation: Update Time: 2022-03-14T12:54:06Z Resource Version: 3886 UID: 4ac64e4c-c84b-478e-92e4-5f072f985971 Spec: Resource Policy: Container Policies: Container Name: * Controlled Resources: cpu memory Max Allowed: Cpu: 1 Memory: 500Mi Min Allowed: Cpu: 5m Memory: 5Mi Target Ref: API Version: argoproj.io/v1alpha1 Kind: Rollout Name: vpa-demo-rollout Update Policy: Update Mode: Auto Events: \u0026lt;none\u0026gt; 几分钟后，VPA 开始处理并提供建议：\nName: kubengix-vpa Namespace: test-vpa Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: autoscaling.k8s.io/v1 Kind: VerticalPodAutoscaler Metadata: Creation Timestamp: 2022-03-14T12:54:06Z Generation: 2 Managed Fields: API Version: autoscaling.k8s.io/v1beta2 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:spec: .: f:resourcePolicy: .: f:containerPolicies: f:targetRef: .: f:apiVersion: f:kind: f:name: f:updatePolicy: .: f:updateMode: Manager: kubectl-client-side-apply Operation: Update Time: 2022-03-14T12:54:06Z API Version: autoscaling.k8s.io/v1 Fields Type: FieldsV1 fieldsV1: f:status: .: f:conditions: f:recommendation: .: f:containerRecommendations: Manager: recommender Operation: Update Time: 2022-03-14T12:54:52Z Resource Version: 3950 UID: 4ac64e4c-c84b-478e-92e4-5f072f985971 Spec: Resource Policy: Container Policies: Container Name: * Controlled Resources: cpu memory Max Allowed: Cpu: 1 Memory: 500Mi Min Allowed: Cpu: 5m Memory: 5Mi Target Ref: API Version: argoproj.io/v1alpha1 Kind: Rollout Name: vpa-demo-rollout Update Policy: Update Mode: Auto Status: Conditions: Last Transition Time: 2022-03-14T12:54:52Z Status: True Type: RecommendationProvided Recommendation: Container Recommendations: Container Name: vpa-demo-rollout Lower Bound: Cpu: 25m Memory: 262144k Target: Cpu: 25m Memory: 262144k Uncapped Target: Cpu: 25m Memory: 262144k Upper Bound: Cpu: 1 Memory: 500Mi Events: \u0026lt;none\u0026gt; 在这里，我们可以看到 CPU、内存的建议，以及较低的界限、较高的界限、目标等等。如果我们检查 Pod 的状态，旧的 Pod 会被终止，新的 Pod 会被创建。\n# kubectl get po -n test-vpa -w NAME READY STATUS RESTARTS AGE vpa-demo-rollout-f5df6d577-65f26 1/1 Running 0 17m vpa-demo-rollout-f5df6d577-d55cx 1/1 Running 0 17m vpa-demo-rollout-f5df6d577-fdpn2 1/1 Running 0 17m vpa-demo-rollout-f5df6d577-jg2pw 1/1 Running 0 17m vpa-demo-rollout-f5df6d577-vlx5x 1/1 Running 0 17m ... vpa-demo-rollout-f5df6d577-jg2pw 1/1 Terminating 0 17m vpa-demo-rollout-f5df6d577-vlx5x 1/1 Terminating 0 17m vpa-demo-rollout-f5df6d577-jg2pw 0/1 Terminating 0 18m vpa-demo-rollout-f5df6d577-vlx5x 0/1 Terminating 0 18m vpa-demo-rollout-f5df6d577-w7tx4 0/1 Pending 0 0s vpa-demo-rollout-f5df6d577-w7tx4 0/1 Pending 0 0s vpa-demo-rollout-f5df6d577-w7tx4 0/1 ContainerCreating 0 0s vpa-demo-rollout-f5df6d577-vdlqq 0/1 Pending 0 0s vpa-demo-rollout-f5df6d577-vdlqq 0/1 Pending 0 1s vpa-demo-rollout-f5df6d577-jg2pw 0/1 Terminating 0 18m …","relpermalink":"/book/argo-rollouts/rollout/vpa-support/","summary":"垂直 Pod 自动缩放（Vertical Pod Autoscaling，VPA）通过自动配置资源需求降低维护成本并提高集群资源利用率。 VPA 模式 VPAs 有四种操作模式： Auto：VPA 在创建 pod 时分配资源请求，使用首选的更新机","title":"垂直 Pod 自动缩放"},{"content":"这里是 Argo Rollouts 管理的部署中所有参与的组件的概述。\nArgo Rollouts 架构 Argo Rollouts 控制器 这是主要的控制器，它监视集群中的事件并在更改 Rollout 类型的资源时做出反应。控制器将读取所有 Rollout 的详细信息，并将集群带到与 Rollout 定义中描述的相同状态。\n请注意，Argo Rollouts 不会干涉或响应在普通的 Deployment 资源 上发生的任何更改。这意味着你可以在以其他方法部署应用程序的集群中安装 Argo Rollouts。\n要在你的集群中安装控制器并开始进行渐进式交付，请参见安装页面 。\nRollout 资源 Rollout 资源是由 Argo Rollouts 引入和管理的自定义 Kubernetes 资源。它与原生 Kubernetes Deployment 资源大多兼容，但具有控制高级部署方法（如金丝雀和蓝/绿部署）的阶段、阈值和方法的额外字段。\n请注意，Argo Rollouts 控制器仅响应在 Rollout 源中发生的那些更改。它不会对普通的 Deployment 资源做任何事情。这意味着，如果你想使用 Argo Rollouts 管理它们，需要将你的 Deployments 迁移到 Rollouts，请参考 迁移页面 。\n你可以在 完整规范页面 中查看 Rollout 的所有可能选项。\n旧版本和新版本的副本集 这些是 标准 Kubernetes ReplicaSet 资源 的实例。Argo Rollouts 在其上放置了一些额外的元数据，以跟踪作为应用程序一部分的不同版本。\n请注意，在 Rollout 中参与的副本集是由控制器以自动方式完全管理的。你不应使用外部工具干扰它们。\nIngress/Service 这是流量从实时用户进入你的集群并重定向到适当版本的机制。Argo Rollouts 使用 标准 Kubernetes service 资源 ，但需要一些额外的元数据来进行管理。\nArgo Rollouts 在网络选项方面非常灵活。首先，你可以在 Rollout 期间拥有不同的服务，这些服务仅适用于新版本、旧版本或两者。针对金丝雀部署，Argo Rollouts 支持几种 服务网格和 Ingress 解决方案 ，以根据特定百分比拆分流量，而不是基于 pod 计数的简单负载均衡，并且可以同时使用多个路由提供程序。\nAnalysisTemplate 和 AnalysisRun 分析（Analysis）是将 Rollout 连接到你的指标提供程序并定义确定更新是否成功的特定指标阈值的功能。对于每个分析，你可以定义一个或多个度量查询以及它们的预期结果。如果度量查询良好，Rollout 将自动继续，如果度量显示失败，则自动回滚，如果度量无法提供成功/失败答案，则暂停 Rollout。\n为执行分析，Argo Rollouts 包括两个自定义 Kubernetes 资源：AnalysisTemplate 和 AnalysisRun。\nAnalysisTemplate 包含有关要查询的度量的说明。附加到 Rollout 的实际结果是 AnalysisRun 自定义资源。你可以在特定的 Rollout 上定义 AnalysisTemplate，也可以在集群上全局定义为可由多个 Rollout 共享的 ClusterAnalysisTemplate。AnalysisRun 资源在特定的 Rollout 上进行作用域限制。\n请注意，在 Rollout 中使用分析和度量完全是可选的。你可以手动暂停和推广（promote）Rollout，或通过 API 或 CLI 使用其他外部方法（例如烟雾测试）。你不需要度量解决方案就能使用 Argo Rollouts。你还可以在 Rollout 中混合自动化（即基于分析的）和手动步骤。\n除了度量之外，你还可以通过运行 Kubernetes Job 或运行 Webhook 来决定 Rollout 的成功。\n度量提供程序 Argo Rollouts 包括对几个流行的度量提供程序的本地集成 ，你可以在分析资源中使用它们来自动推广或回滚 Rollout。有关特定设置选项，请参阅每个提供程序的文档。\nCLI 和 UI 你可以使用 Argo Rollouts CLI 或 集成 UI 查看和管理 Rollout，两者都是可选的。\n","relpermalink":"/book/argo-rollouts/architecture/","summary":"这里是 Argo Rollouts 管理的部署中所有参与的组件的概述。 Argo Rollouts 架构 Argo Rollouts 控制器 这是主要的控制器，它监视集群中的事件并在更改 Rollout 类型的资源时做出反应。控制器将读取所有 Rollout 的详细信息，并将集群带到与 Rollout 定义中描述的相同状态。 请","title":"架构"},{"content":"本章解释了什么是身份，以及分配、管理和使用身份的基本知识。这些是你需要知道的概念，以便了解 SPIFFE 和 SPIRE 的工作方式。\n什么是身份？ 对于人类来说，身份是复杂的。人类是独特的个体，不能被克隆，也不能用代码取代他们的思想，而且一生中可能会有多种社会身份。软件服务也同样复杂。\n一个单一的程序可能会扩展到成千上万的节点，或者在构建系统推送新的更新时，一天内多次改变其代码。在这样一个快速变化的环境中，一个身份必须代表服务的特定逻辑目的（例如，客户计费数据库）和与已建立的权威或信任根（例如，my-company.example.org 或生产工作负载的发行机构）的关联。\n一旦为一个组织中的所有服务发布了身份，它们就可以被用于认证：证明一个服务是它所声称的那样。一旦服务经过相互认证，它们就可以使用身份进行授权，或控制谁可以访问这些服务，以及保密性，或保持它们相互传输的数据的秘密。虽然 SPIFFE 本身并不包括认证、授权或保密性，但它发出的身份可用于所有这些。\n为一个组织指定服务身份与设计该组织基础设施的任何其他部分类似：它密切依赖于该组织的需求。当一个服务扩大规模、改变代码或移动位置时，它保持相同的身份可能是合乎逻辑的。\n值得信赖的身份 现在我们已经定义了身份，那么我们如何表示这种身份？我们如何知道，当一个软件（或工作负载）声称自己的身份时，这个声称是值得信赖的？为了开始探索这些问题，我们必须首先讨论身份是如何建立的。\n人类的身份\n请允许我们用大家共同的东西来解释这些概念：现实世界中的身份。\n身份证件\n如果一个名字是一个人的身份，那么这个身份的证明就是一个身份文件。护照是允许一个人证明其身份的文件，所以它是一个身份文件。像不同国家的护照一样，不同类型的软件身份文件可能看起来不同，而且不总是包含相同的信息。但为了有用，它们通常至少都包含一些共同的信息，如姓名。\n护照和写有你名字的餐巾纸之间的区别是什么？\n最重要的区别是来源。对于护照，我们相信签发机构已经核实了你的身份，而且我们有能力核实护照是由该受信任的机构签发的（验证）。对于那张餐巾纸，我们不知道它来自哪里，也没有办法验证它是否来自你说的那家餐馆。我们也无法相信餐厅在餐巾纸上写了正确的名字，或者在你传达你的名字时验证了它的准确性。\n信任一个发行机构\n我们相信护照，因为我们隐含地相信签发护照的机构。我们信任他们签发这些身份文件的过程：他们有记录和控制，以确保他们只向正确的个人签发身份。我们信任这个过程的管理，所以我们知道该机构签发的护照是某人身份的忠实代表。\n核实身份文件\n鉴于此，我们如何区分真护照和假护照？这就是验证的意义所在。组织需要一种方法来确定身份文件是否是由我们信任的权威机构签发的。这通常是通过难以复制但容易验证的水印来实现的。\n对出示身份证件的人进行认证\n护照记录了关于身份所代表的人的几项信息。首先，它们包括一个人的照片，可以用来验证出示者是护照上的同一个人。它们还可能包括此人的其他身体特征 —— 例如，他们的身高、体重和眼睛颜色。\n所有这些属性都可以用来验证出示护照的人。\n简而言之，护照是我们的身份文件，我们用它来确认彼此的身份，因为我们信任签发机构，并且有办法验证该文件来自该机构。最后，我们可以通过交叉参考护照的内容和持有护照的人，来验证出示护照的人。\n数字世界中的身份：加密身份 绕回工作负载身份，上述概念如何映射到计算机系统？计算机使用的是数字身份文件，而不是护照。X.509 证书、签名的 JSON 网络令牌（JWT）和 Kerberos 票据，都是数字身份文件的例子。数字身份文件可以使用加密技术进行验证。然后，计算机系统可以被验证，就像一个拥有护照的人一样。\n做到这一点的最有用和最普遍的技术之一是公钥基础设施（PKI）。PKI 被定义为一套创建、管理、分发、使用、存储和撤销数字证书以及管理公钥加密所需的角色、策略、硬件、软件和程序。有了 PKI，数字身份文件可以在本地进行验证，即根据一组小的、静态的根信任包进行验证。\nX.509 的简要概述\n当国际电信联盟电信标准化部门（ITU-T） 于 1988 年首次发布 X.509 标准的 PKI 时，这在当时是非常雄心勃勃的，现在仍然被认为是如此。该标准最初设想为人类、服务器和其他设备提供证书，形成一个巨大的全球一体化安全通信系统。虽然 X.509 从未达到其最初的预期，但它是几乎所有安全通信协议的基础。\nX.509 是如何在单一机构中工作的\nBob 的计算机需要一个证书。他生成了一个随机的私钥，还有一个证书签名请求（CSR），其中包括他的计算机的基本信息，比如它的名字；我们称之为 bobsbox。CSR 有点像护照申请。 Bob 将他的 CSR 发送给一个证书颁发机构（CA）。CA 验证 Bob 是否真的是 Bob。这种验证的具体方式可能有所不同 —— 它可能涉及一个人检查 Bob 的文件，或自动检查。 然后，CA 通过对 CSR 中提出的信息进行编码来创建证书，并添加数字签名，以断言 CA 已经验证了其中包含的信息是真实和正确的。它把证书送回给 Bob。 当 Bob 想与 Alice 建立安全通信时，他的计算机可以出示他的证书，并以密码学方式证明它拥有 Bob 的私钥（而不需要实际与任何人分享该私钥的内容）。 Alice 的计算机可以通过检查证书颁发机构是否签署了 Bob 的证书来检查 Bob 的证书是否真的是 Bob 的证书。她相信证书颁发机构在签署证书之前正确检查了 Bob 的身份。 带有中间证书机构的 X.509\n在许多情况下，签署了某一特定证书的 CA 并不广为人知。相反，该 CA 有自己的密钥和证书，而该证书是由另一个 CA 签署的。通过签署该 CA 证书，上级 CA 证明了下级 CA 被授权签发数字身份。这种由高阶 CA 对低阶 CA 的授权被称为委托（delegation）。\n委托可以重复发生，低阶 CA 进一步委托他们的权力，形成一个任意高的证书授权树。最高等级的 CA 被称为根 CA，必须有一个知名的证书。链中的每一个其他 CA 都被称为 中间 CA。这种方法的好处是，需要知名的密钥较少，允许列表的变化不那么频繁。\n这导致了 X.509 的一个关键弱点：任何 CA 可以签署任何证书，没有任何限制。如果一个黑客决定建立自己的中间 CA，并能得到任何一个现有中间 CA 的批准，那么他就可以有效地签发他想要的任何身份。至关重要的是，每个知名的 CA 都是完全值得信赖的，而且他们委托的每个中间 CA 也是完全值得信赖的。\n证书和身份的生命周期\n在 PKI 中有几个额外的功能，使数字身份的管理和认证更容易和更安全。权限委托、身份撤销和有限的身份文件寿命是其中的几个例子。\n身份发放\n首先，必须能够发布一个新的身份。人类的诞生，新软件服务的编写，在每一种情况下，我们都必须在以前没有身份的地方颁发一个身份。\n首先，一项服务需要申请一个新的身份。对于人来说，这可能是一个纸质表格。对于软件来说，它是一个 X.509 文件，称为证书签名请求（CSR），它是用一个相应的私钥创建的。CSR 类似于证书，但由于它没有被任何证书颁发机构签署，没有人会承认它是有效的。然后，该服务将 CSR 安全地发送给证书颁发机构。\n接下来，证书颁发机构根据申请证书的服务检查 CSR 的每个细节。最初，这本来是一个手工过程：人类检查文书工作，并在个人基础上作出决定。今天，检查和签署过程通常是完全自动化的。如果你使用过流行的 LetsEncrypt 证书颁发机构，那么你就会熟悉完全自动化的证书颁发机构签署过程。\n一旦满意，证书颁发机构将其数字签名附加到 CSR 上，将其变成一个完全成熟的证书。它把证书返回给服务。与它先前生成的私钥一起，该服务可以让他人安全地识别自己。\n证书撤销\n那么，如果一项服务被破坏了，会发生什么？如果 Bob 的笔记本电脑被黑了，或者 Bob 离开了公司，不应该再有访问权，怎么办？\n这种取消信任的过程被称为证书撤销（Certificate Revocation）。证书颁发机构维护一个名为 **“证书撤销列表”（CRL）**的文件，其中包含被撤销的证书的唯一 ID，并将该文件的签名副本分发给任何要求的人。\n撤销是很棘手的，有几个原因。首先，CRL 必须由某个端点托管和提供，这就给确保端点的正常运行和可达带来了挑战。当该端点不可用时，PKI 是否停止工作？在实践中，大多数软件将无法打开，在 CRL 不可用时继续信任证书，使它们实际上没有用处。\n第二，CRL 可能会变得庞大而不方便。被撤销的证书必须保留在 CRL 中，直到它过期为止，而证书的寿命一般都很长（在几年的时间内）。这可能导致服务、下载和处理清单本身的性能问题。\n已经开发了几种不同的技术，试图使证书撤销更简单、更可靠，如在线证书状态协议（OCSP）。各种各样的方法使证书撤销成为一个持续的挑战。\n证书过期\n每个证书都有一个内置的过期日期。过期日期是 X.509 安全的一个重要部分，有几个不同的原因：管理过时，限制证书显示的身份变化的可能性，限制 CRL 的大小，以及减少秘钥被盗的可能性。\n证书已经存在了很长时间了。当它们刚被开发出来时，许多 CA 使用 1989 年的 MD2 散列算法，这种算法很快被发现是不安全的。如果这些证书仍然有效，攻击者可以伪造它们。\n有限的证书寿命的另一个重要方面是，CA 只有一次机会来验证申请者的身份，但这些信息不能保证长期保持正确。例如，域名经常改变所有权，是证书中一般包括的比较关键的信息之一。\n如果使用了证书撤销列表，那么每个仍然有效的证书都有可能被撤销。如果证书永远持续下去，那么证书撤销列表就会无止境地增长。为了保持证书撤销列表的小规模，证书需要过期。\n最后，证书的有效期越长，其证书的私钥或任何通往根的证书被盗的风险就越大。\n频繁的证书更新\n解决撤销所带来的挑战的一个折中办法是更多地依靠证书过期。如果证书的有效期很短（也许只有几个小时），那么 CA 就可以经常重新执行它最初做的所有检查。如果证书的更新足够频繁，那么 CRL 可能甚至没有必要，因为等待证书过期可能会更快。\n身份寿命的权衡\n更短的寿命 更长的寿命 如果文件被盗，它的有效期会缩短 减少了证书颁发机构的负担（人和程序） CRL 比较短，也许没有必要 降低网络的负荷 一次性减少未结清的身份文件（更容易跟踪） 在一个节点因网络中断而无法更新其证书时，具有更好的弹性 另一种加密身份：JSON 网络令牌（JWT）\n另一个公钥身份文件，JSON 网络令牌（RFC7519），也表现为一个类似 PKI 的系统。它不使用证书，而是使用 JSON 令牌，并有一个称为 JSON Web Key Set 的结构，作为 CA 绑定来验证 JSON 令牌。证书和 JWT 之间有一些重要的区别，超出了本书的范围，但就像 X.509 证书一样，JWT 的真实性可以通过 PKI 来验证。\n外部身份的可信度 无论你使用哪种身份，都必须由一些受信任的机构来签发。在许多情况下，并不是每个人都信任相同的当局或其颁发的过程。Alice 的纽约州驾照在纽约是有效的身份证明，但它在伦敦是无效的，因为伦敦当局不信任纽约州的政府。然而，Bob 的美国护照在伦敦是有效的，因为英国当局信任美国政府，而伦敦当局信任英国当局。\n在数字身份文件领域，情况是相同的。Alice 和 Bob 可能拥有由完全不相关的 CA 签署的证书，但只要他们都信任这些 CA，他们就可以相互认证。这并不意味着 Alice 必须信任 Bob，只是她可以安全地识别他。\n如何使用软件身份 一旦一个软件有了数字身份文件，它就可以被用于许多不同的目的。我们已经讨论了使用身份文件进行认证。它们还可以用于相互 TLS、授权、可观测性和计量。\n认证\n身份文件最常见的用途是作为认证的基础。对于软件身份，存在几种不同的认证协议，使用 X.509 证书或 JWT 来证明一个服务对另一个服务的身份。\n保密性和完整性\n保密性意味着攻击者不能看到信息的内容，而完整 …","relpermalink":"/book/spiffe/general-concepts-behind-identity/","summary":"本章解释了什么是身份，以及分配、管理和使用身份的基本知识。这些是你需要知道的概念，以便了解 SPIFFE 和 SPIRE 的工作方式。","title":"身份背后的通用概念"},{"content":"根据所使用的 Linux 内核版本，eBPF 数据路径可以完全在 eBPF 中实现不同的功能集。如果某些所需功能不可用，则使用旧版 iptables 实现提供该功能。有关详细信息，请参阅 IPsec 要求。 kube-proxy 互操作性 下图显示了 kube-proxy 安装的 iptables 规则和 Cilium 安装的 iptables 规则的集成。\nkube-proxy 与 Cilium 的 iptables 规则集成 下一章 ","relpermalink":"/book/cilium-handbook/ebpf/iptables/","summary":"根据所使用的 Linux 内核版本，eBPF 数据路径可以完全在 eBPF 中实现不同的功能集。如果某些所需功能不可用，则使用旧版 iptables 实现提供该功能。有关详细信息，请参阅 IPsec 要求。 kube-proxy 互操作性 下图显示了 kube-proxy 安装的 iptables 规则和 Cilium 安装的 iptables 规","title":"Iptables 用法"},{"content":"默认情况下，Cilium 将 eBPF 数据路径配置为执行 IP 分片跟踪，以允许不支持分片的协议（例如 UDP）通过网络透明地传输大型消息。IP 分片跟踪在 eBPF 中使用 LRU（最近最少使用）映射实现，需要 Linux 4.10 或更高版本。可以使用以下选项配置此功能：\n--enable-ipv4-fragment-tracking：启用或禁用 IPv4 分片跟踪。默认启用。 --bpf-fragments-map-max：控制使用 IP 分片的最大活动并发连接数。对于默认值，请参阅eBPF Maps 。 注意\n当使用 kube-proxy 运行 Cilium 时，碎片化的 NodePort 流量可能会由于内核错误而中断，其中路由 MTU 不受转发数据包的影响。Cilium 碎片跟踪需要第一个逻辑碎片首先到达。由于内核错误，可能会在外部封装层上发生额外的碎片，从而导致数据包重新排序并导致无法跟踪碎片。\n内核错误已被 修复 并向后移植到所有维护的内核版本。如果您发现连接问题，请确保您的节点上的内核包最近已升级，然后再报告问题。\n提示 这是一个测试版功能。如果你遇到任何问题，请提供反馈并提交 GitHub Issue。 下一章 ","relpermalink":"/book/cilium-handbook/networking/fragmentation/","summary":"默认情况下，Cilium 将 eBPF 数据路径配置为执行 IP 分片跟踪，以允许不支持分片的协议（例如 UDP）通过网络透明地传输大型消息。IP 分片跟踪在 eBPF 中使用 LRU（最近最少使用）映射实现，需要 Linux 4.10 或更高版本。可以","title":"IPv4 分片处理"},{"content":"Cilium 能够透明地将四层代理注入任何网络连接中。这被用作执行更高级别网络策略的基础（请参阅基于 DNS 和 七层示例 ）。\n可以注入以下代理：\nEnvoy 注入 Envoy 代理 注意 此功能目前正处于测试阶段。 如果你有兴趣编写 Envoy 代理的 Go 扩展，请参考开发者指南 。\nEnvoy 代理注入示意图 如上所述，该框架允许开发人员编写少量 Go 代码（绿色框），专注于解析新的 API 协议，并且该 Go 代码能够充分利用 Cilium 功能，包括高性能重定向 Envoy、丰富的七层感知策略语言和访问日志记录，以及通过 kTLS 对加密流量的可视性（即将推出）。总而言之，作为开发者的你只需要关心解析协议的逻辑，Cilium + Envoy + eBPF 就完成了繁重的工作。\n本指南基于假设的 r2d2 协议（参见 proxylib/r2d2/r2d2parser.go ）的简单示例，该协议可用于很久以前在遥远的星系中与简单的协议机器人对话。但它也指向了 cilium/proxylib 目录中已经存在的其他真实协议，例如 Memcached 和 Cassandra。\n下一章 ","relpermalink":"/book/cilium-handbook/security/proxy/","summary":"Cilium 能够透明地将四层代理注入任何网络连接中。这被用作执行更高级别网络策略的基础（请参阅基于 DNS 和 七层示例 ）。 可以注入以下代理： Envoy 注入 Envoy 代理 注意 此功能目前正处于测试阶段。 如果你有兴趣编写 Envoy 代理的 Go 扩展，请参","title":"代理注入"},{"content":"策略规则到端点映射 确定哪些策略规则当前对端点有效，并且可以将来自 cilium endpoint list 和 cilium endpoint get 的数据与 cilium policy get 配对。cilium endpoint get 将列出适用于端点的每个规则的标签。可以传递标签列表给 cilium policy get 以显示确切的源策略。请注意，不能单独获取没有标签的规则（无标签会返回节点上的完整策略）。具有相同标签的规则将一起返回。\n在下面的示例中，其中一个 deathstar pod 的端点 id 是 568。我们可以打印应用于它的所有策略：\n$ # Get a shell on the Cilium pod $ kubectl exec -ti cilium-88k78 -n kube-system -- /bin/bash $ # print out the ingress labels $ # clean up the data $ # fetch each policy via each set of labels $ # (Note that while the structure is \u0026#34;...l4.ingress...\u0026#34;, it reflects all L3, L4 and L7 policy. $ cilium endpoint get 568 -o jsonpath=\u0026#39;{range ..status.policy.realized.l4.ingress[*].derived-from-rules}{@}{\u0026#34;\\n\u0026#34;}{end}\u0026#39;|tr -d \u0026#39;][\u0026#39; | xargs -I{} bash -c \u0026#39;echo \u0026#34;Labels: {}\u0026#34;; cilium policy get {}\u0026#39; Labels: k8s:io.cilium.k8s.policy.name=rule1 k8s:io.cilium.k8s.policy.namespace=default [ { \u0026#34;endpointSelector\u0026#34;: { \u0026#34;matchLabels\u0026#34;: { \u0026#34;any:class\u0026#34;: \u0026#34;deathstar\u0026#34;, \u0026#34;any:org\u0026#34;: \u0026#34;empire\u0026#34;, \u0026#34;k8s:io.kubernetes.pod.namespace\u0026#34;: \u0026#34;default\u0026#34; } }, \u0026#34;ingress\u0026#34;: [ { \u0026#34;fromEndpoints\u0026#34;: [ { \u0026#34;matchLabels\u0026#34;: { \u0026#34;any:org\u0026#34;: \u0026#34;empire\u0026#34;, \u0026#34;k8s:io.kubernetes.pod.namespace\u0026#34;: \u0026#34;default\u0026#34; } } ], \u0026#34;toPorts\u0026#34;: [ { \u0026#34;ports\u0026#34;: [ { \u0026#34;port\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;TCP\u0026#34; } ], \u0026#34;rules\u0026#34;: { \u0026#34;http\u0026#34;: [ { \u0026#34;path\u0026#34;: \u0026#34;/v1/request-landing\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;POST\u0026#34; } ] } } ] } ], \u0026#34;labels\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;io.cilium.k8s.policy.name\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;rule1\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;k8s\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;io.cilium.k8s.policy.namespace\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;k8s\u0026#34; } ] } ] Revision: 217 $ # repeat for egress $ cilium endpoint get 568 -o jsonpath=\u0026#39;{range ..status.policy.realized.l4.egress[*].derived-from-rules}{@}{\u0026#34;\\n\u0026#34;}{end}\u0026#39; | tr -d \u0026#39;][\u0026#39; | xargs -I{} bash -c \u0026#39;echo \u0026#34;Labels: {}\u0026#34;; cilium policy get {}\u0026#39; toFQDNs 规则故障排除 随着 DNS 数据的变化，在应用策略很长时间后，效果 toFQDNs 可能会发生变化。这会使调试意外阻塞的连接或瞬时故障变得困难。Cilium 提供 CLI 工具来内省在多个守护进程层中应用 FQDN 策略的状态：\ncilium policy get 应显示导入的 FQDN 策略：\n{ \u0026#34;endpointSelector\u0026#34;: { \u0026#34;matchLabels\u0026#34;: { \u0026#34;any:class\u0026#34;: \u0026#34;mediabot\u0026#34;, \u0026#34;any:org\u0026#34;: \u0026#34;empire\u0026#34;, \u0026#34;k8s:io.kubernetes.pod.namespace\u0026#34;: \u0026#34;default\u0026#34; } }, \u0026#34;egress\u0026#34;: [ { \u0026#34;toFQDNs\u0026#34;: [ { \u0026#34;matchName\u0026#34;: \u0026#34;api.twitter.com\u0026#34; } ] }, { \u0026#34;toEndpoints\u0026#34;: [ { \u0026#34;matchLabels\u0026#34;: { \u0026#34;k8s:io.kubernetes.pod.namespace\u0026#34;: \u0026#34;kube-system\u0026#34;, \u0026#34;k8s:k8s-app\u0026#34;: \u0026#34;kube-dns\u0026#34; } } ], \u0026#34;toPorts\u0026#34;: [ { \u0026#34;ports\u0026#34;: [ { \u0026#34;port\u0026#34;: \u0026#34;53\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;ANY\u0026#34; } ], \u0026#34;rules\u0026#34;: { \u0026#34;dns\u0026#34;: [ { \u0026#34;matchPattern\u0026#34;: \u0026#34;*\u0026#34; } ] } } ] } ], \u0026#34;labels\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;io.cilium.k8s.policy.derived-from\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;CiliumNetworkPolicy\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;k8s\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;io.cilium.k8s.policy.name\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;fqdn\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;k8s\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;io.cilium.k8s.policy.namespace\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;k8s\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;io.cilium.k8s.policy.uid\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;fc9d6022-2ffa-4f72-b59e-b9067c3cfecf\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;k8s\u0026#34; } ] } 发出 DNS 请求后，应通过以下方式获得 FQDN 到 IP 的映射：cilium fqdn cache list\n# cilium fqdn cache list Endpoint FQDN TTL ExpirationTime IPs 2761 help.twitter.com. 604800 2019-07-16T17:57:38.179Z 104.244.42.67,104.244.42.195,104.244.42.3,104.244.42.131 2761 api.twitter.com. 604800 2019-07-16T18:11:38.627Z 104.244.42.194,104.244.42.130,104.244.42.66,104.244.42.2 如果允许流量，则这些 IP 应通过以下方式具有相应的本地身份：cilium identity list | grep \u0026lt;IP\u0026gt;\n# cilium identity list | grep -A 1 104.244.42.194 16777220 cidr:104.244.42.194/32 reserved:world ","relpermalink":"/book/cilium-handbook/policy/troubleshooting/","summary":"策略规则到端点映射 确定哪些策略规则当前对端点有效，并且可以将来自 cilium endpoint list 和 cilium endpoint get 的数据与 cilium policy get 配对。cilium endpoint get 将列出适用于端点的每个规则的标签。可以传递标签列表给 cilium policy get 以显示确切的源策略。请注意，","title":"故障排除"},{"content":"本章大纲 路由\nIP 地址管理（IPAM）\nIP 地址伪装\nIPv4 分片处理\n阅读本章 ","relpermalink":"/book/cilium-handbook/networking/","summary":"本章大纲 路由 IP 地址管理（IPAM） IP 地址伪装 IPv4 分片处理 阅读本章","title":"网络"},{"content":"本章大纲 介绍\n基于身份\n策略执行\n代理注入\n阅读本章 ","relpermalink":"/book/cilium-handbook/security/","summary":"本章大纲 介绍 基于身份 策略执行 代理注入 阅读本章","title":"网络安全"},{"content":"现在你已经看到了 eBPF 编程的例子，了解到它是如何工作的。虽然基础示例使得 eBPF 看起来相对简单，但也有一些复杂的地方使得 eBPF 编程充满挑战。\n长期以来，有个问题使得编写和发布 eBPF 程序相对困难，那就是内核兼容性。\n跨内核的可移植性 eBPF 程序可以访问内核数据结构，而这些结构可能在不同的内核版本中发生变化。这些结构本身被定义在头文件中，构成了 Linux 源代码的一部分。在过去编译 eBPF 程序时，必须基于你想运行这些程序的内核兼容的头文件集。\nBCC 对可移植性的处理方法 为了解决跨内核的可移植性问题，BCC 1（BPF 编译器集合，BPF Compiler Collection）项目采取了在运行时编译 eBPF 代码的方法，在目标机器上就地进行。这意味着编译工具链需要安装到每个你想让代码运行 2 的目标机器上，而且你必须在工具启动之前等待编译完成，而且文件系统上必须有内核头文件（实际上并不总是这样）。这就引出了 BPF CO-RE。\nCO-RE CO-RE（Compile Once, Run Everyone，编译一次，到处运行）方法由以下元素组成。\nBTF (BPF Type Format)\n这是一种用于表达数据结构和函数签名布局的格式。现代 Linux 内核支持 BTF，因此你可以从运行中的系统中生成一个名为 vmlinux.h 的头文件，其中包含一个 BPF 程序可能需要的关于内核的所有数据结构信息。\nlibbpf，BPF 库\nlibbpf 一方面提供了加载 eBPF 程序和映射到内核的功能，另一方面也在可移植性方面也起着重要的作用：它依靠 BTF 信息来调整 eBPF 代码，以弥补其编译时的数据结构与目标机器上的数据结构之间的差异。\n编译器支持\nclang 编译器得到了增强，因此当它编译 eBPF 程序时，它包括所谓的 BTF 重定位（relocation），这使得 libbpf 在加载 BPF 程序和映射到内核时知道要调整什么。\n可选的 BPF 骨架\n使用 bpftool gen skeleton 可以从编译的 BPF 对象文件中自动生成一个骨架，其中包含用户空间代码可以方便调用的函数，以管理 BPF 程序的生命周期 —— 将它们加载到内核，附加到事件等等。这些函数是更高层次的抽象，对开发者来说比直接使用 libbpf 更方便。\n关于 CO-RE 的更详细的解释，请阅读 Andrii Nakryiko 的出色 描述 。\n自 5.4 版本 3 以来，vmlinux 文件形式的 BTF 信息已经包含在 Linux 内核中，但 libbpf 可以利用的原始 BTF 数据也可以为旧内核生成。在 BTF Hub 上有关于如何生成 BTF 文件的信息，以及用于各种 Linux 发行版的文件档案。\nBPF CO-RE 方法使得 eBPF 程序更易于在任意 Linux 发行版上运行 —— 或者至少在新 Linux 发行版上支持任意 eBPF 能力。但这并不能使 eBPF 更优雅：它本质上仍然是内核编程。\nLinux 内核知识 很快就会发现，为了编写更高级的工具，你需要一些关于 Linux 内核的领域知识。你需要了解你可以访问的数据结构，取决于你的 eBPF 代码被调用的环境。不是所有应用程序的开发者都有解析网络数据包、访问套接字缓冲区或处理系统调用参数的经验。\n内核将如何对你 eBPF 代码的行为做出反应？正如你在 第二章 中了解到的，内核由数百万行代码组成。它的文档可能是稀少的，所以你可能会发现自己不得不阅读内核的源代码来弄清楚某些东西是如何工作的。\n你还需要弄清楚你的 eBPF 代码应该附加到哪些事件。由于可以将 kprobe 附加到整个内核的任何函数入口点，这可能不是一个简单的决定。在某些情况下，这可能很明确 —— 例如，如果你想访问一个传入的网络数据包，那么适当的网络接口上的 XDP 钩子是一个明显的选择。如果你想提供对特定内核事件的可观测性，在内核代码中找到合适的点可能并不难。\n但在其他情况下，选择可能不那么明显。例如，简单地使用 kprobes 来钩住构成内核系统调用接口的函数的工具，可能会被名为 time-of-check 到 time-of-use（TOCTTOU）的安全漏洞所影响。攻击者有一个小的机会窗口，他们可以在 eBPF 代码读取参数后，但在参数被复制到内核内存之前，改变系统调用的参数。在 DEF CON 29 4 上，Rex Guo 和 Junyuan Zeng 做了一个关于这个问题的 出色演讲 。一些被最广泛使用的 eBPF 工具是以相当天真的方式编写的，极易受到这种攻击。这不是一个简单的漏洞，而且有办法减轻这些攻击，但如果你正在保护高度敏感的数据，对抗复杂的、有动机的对手，请深入了解你使用的工具是否可能受到影响。\n你已经看到了 BPF CO-RE 是如何使 eBPF 程序在不同的内核版本上工作的，但它只考虑到了数据结构布局的变化，而没有考虑到内核行为的更大变化。例如，如果你想把一个 eBPF 程序附加到内核中的一个特定的函数或 tracepoint 上，你可能需要一个 B 计划，如果该函数或 tracepoint 在不同的内核版本中不存在，该怎么做。\n编排多个 eBPF 程序 当前有很多基于 eBPF 的工具提供了一套可观测能力，通过将 eBPF 程序与一组内核事件挂钩来实现。其中大部分是由 Brendan Gregg 和其他人在 BCC 和 bpftrace 工具中所做的工作而开创的。很多工具（通常是商业的）可能会提供更漂亮的图形和用户界面，但他们还是在这些 eBPF 程序的基础上实现的。\n当你想写代码来编排不同类型的事件之间的交互时，事情就变得相当复杂了。举个例子，Cilium 通过内核的网络堆栈 5 在不同的点上观察到网络数据包，基于来自 Kubernetes CNI（容器网络接口）关于 Kubernetes pod 的信息，对流量进行操作。构建这个系统需要 Cilium 开发人员深入了解内核如何处理网络流量，以及用户空间的 pod 和 容器 概念如何映射到内核概念，如 cgroups 和命 namespace。在实践中，一些 Cilium 的维护者也是内核的开发者，他们致力于增强 eBPF 和网络支持；因此，他们拥有这些知识。\n底线是，尽管 eBPF 提供了一个极其有效和强大的平台来连接到内核，但对于没有大量内核经验的普通开发者来说，这并不容易。如果你对 eBPF 编程感兴趣，我非常鼓励你把它作为练习来学习；在这个领域积累经验可能是非常有价值的，因为它在未来几年内一定会成为受欢迎的专业技能。但实际上，大多数组织不太可能在内部建立许多定制的 eBPF 工具，而是利用专业 eBPF 社区的项目和产品。\n让我们继续思考为什么这些基于 eBPF 的项目和产品在云原生环境中如此强大。\n参考 你可以在 GitHub 页面 上找到 BCC。 ↩︎\n一些项目采取了将 eBPF 源和所需工具链打包成一个容器镜像的方法。这避免了安装工具链的复杂性和任何随之而来的依赖管理，但这仍意味着编译步骤在目标机器上运行。 ↩︎\n更多信息见 Andrii Nakryiko 的 IO Visor 帖子 。 ↩︎\nRex Guo and Junyuan Zeng, “Phantom Attack: 逃离系统调用监控，\u0026#34;（DEF CON，2021 年 8 月 5-8 日）。 ↩︎\nCilium 文档 描述了 eBPF 程序如何附加到不同的网络能力钩子，组合起来以实现复杂的网络能力。 ↩︎\n","relpermalink":"/book/what-is-ebpf/ebpf-complexity/","summary":"现在你已经看到了 eBPF 编程的例子，了解到它是如何工作的。虽然基础示例使得 eBPF 看起来相对简单，但也有一些复杂的地方使得 eBPF 编程充满挑战。 长期以来，有个问题使得编写和发布 eBPF 程序相对困难，那就是内核兼容性。 跨内核的","title":"第四章：eBPF 的复杂性"},{"content":"从理论上讲，DevSecOps 原语可以应用于许多应用架构，但最适合于基于微服务的架构，由于应用是由相对较小的、松散耦合的模块组成的，被称为微服务，因此允许采用敏捷开发模式。即使在基于微服务的架构中，DevSecOps 原语的实现也可以采取不同的形式，这取决于平台。在本文中，所选择的平台是一个容器编排和资源管理平台（如 Kubernetes）。该平台是物理（裸机）或虚拟化（如虚拟机、容器）基础设施上的一个抽象层。为了在本文中明确提及该平台或应用环境，它被称为 DevSecOps 原语参考平台，或简称为 参考平台。\n在描述参考平台的 DevSecOps 原语的实现之前，我们假设在部署服务网格组件方面采用了以下 尽职调查 ：\n用于部署和管理基于服务网格的基础设施（如网络路由）、策略执行和监控组件的安全设计模式 测试证明这些服务网格组件在应用的各个方面（如入口、出口和内部服务）的各种情况下都能按预期工作。 为参考平台实施 DevSecOps 原语所提供的指导与 (a) DevSecOps 管道中使用的工具和 (b) 提供应用服务的服务网格软件无关，尽管来自 Istio 等服务网格产品的例子被用来将它们与现实世界的应用工件（如容器、策略执行模块等）联系起来。\n以下是对参考平台所呈现的整个应用环境中的代码类型（在执行摘要中提到）的稍微详细的描述。请注意，这些代码类型包括那些支持实施 DevSecOps 原语的代码。\n应用代码：体现了执行一个或多个业务功能的应用逻辑，由描述业务事务和数据库访问的代码组成。 应用服务代码（如服务网格代码）：为应用提供各种服务，如服务发现、建立网络路由、网络弹性服务（如负载均衡、重试），以及安全服务（如根据策略强制执行认证、授权等，见第 4 章）。 基础设施即代码：以声明性代码的形式表达运行应用程序所需的计算、网络和存储资源。 策略即代码：包含声明性代码，用于生成实现安全目标的规则和配置参数，例如在运行期间通过安全控制（如认证、授权）实现零信任。 可观测性即代码：触发与日志（记录所有事务）和追踪（执行应用程序请求所涉及的通信途径）以及监控（在运行期间跟踪应用程序状态）有关的软件。 代码类型 3、4 和 5 可能与代码类型 2 有重叠。\n本文件涵盖了与上述所有五种代码类型相关的管道或工作流程的实施。因此，整个应用环境（不仅仅是应用代码）受益于应用代码的所有最佳实践（例如，敏捷迭代开发、版本控制、治理等）。基础设施即代码、策略即代码和可观测性即代码属于一个特殊的类别，称为声明性代码。当使用“xx 即代码”的技术时，编写的代码（例如，用于配置资源的代码）被管理，类似于应用源代码。这意味着它是有版本的，有文件的，并且有类似于应用源代码库的访问控制定义。通常，使用特定领域的声明性语言：声明需求，并由相关工具将其转换为构成运行时实例的工件。例如，在基础设施即代码（IaC）的情况下，声明性语言将基础设施建模为一系列的资源。相关的配置管理工具将这些资源集中起来，并生成所谓的 清单，定义与所定义的资源相关的平台（运行时实例）的最终状态。这些清单存储在与配置管理工具相关的服务器中，并由该工具用于为指定平台上的运行时实例创建编译的配置指令。清单通常以平台中立的表示方式（如 JSON）进行编码，并通过 REST API 反馈给平台资源配置代理。\n","relpermalink":"/book/service-mesh-devsecops/intro/scope/","summary":"从理论上讲，DevSecOps 原语可以应用于许多应用架构，但最适合于基于微服务的架构，由于应用是由相对较小的、松散耦合的模块组成的，被称为微服务，因此允许采用敏捷开发模式。即使在基于微服务的架构中，D","title":"1.1 范围"},{"content":"本章中，我们讨论了希望通过软件赋予我们业务的能力并迁移到云原生应用架构的动机：\n速度\n比我们的竞争对手更快速得创新、试验并传递价值。\n安全\n在保持稳定性、可用性和持久性的同时，具有快速行动的能力。\n扩展\n根据需求变化弹性扩展。\n移动性\n客户可以随时随地通过任何设备无缝的跟我们交互。\n我们还研究了云原生应用架构的独特特征，以及如何赋予我们这些能力：\n12 因素应用\n一套优化应用设计速度，安全性和规模的模式。\n微服务\n一种架构模式，可帮助我们将部署单位与业务能力保持一致，使每个业务能够独立和自主地移动，这样一来也更快更安全。\n自服务敏捷基础设施\n云平台使开发团队能够在应用和服务抽象层面上运行，提供基础架构级速度，安全性和扩展性。\n基于 API 的协作\n将服务间交互定义为可自动验证协议的架构模式，通过简化的集成工作实现速度和安全性。\n抗脆弱性\n随着速度和规模扩大，系统的压力随之增加，系统的响应能力和安全性也随之提高。\n在下一章中，我们将探讨大多数企业为采用云原生应用架构而需要做出哪些改变。\n下一章 ","relpermalink":"/book/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/summary/","summary":"本章中，我们讨论了希望通过软件赋予我们业务的能力并迁移到云原生应用架构的动机： 速度 比我们的竞争对手更快速得创新、试验并传递价值。 安全 在保持稳定性、可用性和持久性的同时，具有快速行动的能力。 扩展 根据需求","title":"1.3：本章小结"},{"content":"本章中，我们探讨了大多数企业采用云原生应用架构所需要做出的变革。从宏观总体上看是权力下放和自治：\nDevOps\n技能集中化转变为跨职能团队。\n持续交付\n发行时间表和流程的权力下放。\n自治\n决策权力下放。\n我们将这种权力下放编成两个主要的团队结构：\n业务能力团队\n自主决定设计、流程和发布时间表的跨职能团队。\n平台运营团队\n为跨职能团队提供他们所需要运行平台。\n而在技术上，我们也分散自治：\n单体应用到微服务\n将个人业务能力的控制分配给单个自主服务。\n有界上下文\n将业务领域模型的内部一致子集的控制分配到微服务。\n容器化\n将对应用包装的控制分配给业务能力团队。\n编排\n将服务集成控制分配给服务端点。\n所有这些变化造就了无数的自治单元，辅助我们以期望的创新速度安全前行。\n在最后一章中，我们将通过一组操作手册，深入研究迁移到云原生应用架构的技术细节。\n下一章 ","relpermalink":"/book/migrating-to-cloud-native-application-architectures/changes-needed/summary/","summary":"本章中，我们探讨了大多数企业采用云原生应用架构所需要做出的变革。从宏观总体上看是权力下放和自治： DevOps 技能集中化转变为跨职能团队。 持续交付 发行时间表和流程的权力下放。 自治 决策权力下放。 我们将这种权力下放编","title":"2.4 本章小结"},{"content":"策略即代码涉及编纂所有策略，并作为 CI/CD 管道的一部分运行，使其成为应用程序运行时的一个组成部分。策略类别的例子包括授权策略、网络策略和实现工件策略（例如，容器策略）。典型的 策略即代码软件的策略管理能力可能带有一套预定义的策略类别和策略，也支持通过提供策略模板定义新的策略类别和 相关策略 。策略即代码所要求的尽职调查是，它应该提供保护，防止与应用环境（包括基础设施）相关的所有已知威胁，只有当该代码被定期扫描和更新，以应对与应用类别（如网络应用）和托管基础设施相关的威胁，才能确保这一点。下面的表 1 中给出了一些策略类别和相关策略的例子。\n表 1：策略类别和策略实例\n策略类别 策略示例 网络策略和零信任策略 - 封锁指定端口 - 指定入口主机名称 - 一般来说，所有的网络访问控制策略 实施工件策略（例如，容器策略） - 对服务器进行加固，对基础镜像进行漏洞扫描 - 确保容器不以 root 身份运行 - 阻止容器的权限升级 存储策略 - 设置持久性卷大小 - 设置持久性卷回收策略 访问控制策略 - 确保策略涵盖所有数据对象 - 确保策略涵盖管理和应用访问的所有角色 - 确保数据保护策略涵盖静态数据、传输中数据和使用中数据 - 确保所有类型的策略不存在冲突 供应链策略 - 只允许经批准的容器注册表 - 只允许经认证的库 审计和问责策略 - 确保有与审计和问责职能相关的策略 在策略即代码软件中定义的策略可以转化为应用基础设施运行时配置参数中的以下内容：\n强化策略的可执行性（例如，服务代理中的 WASM）。 用于调用外部策略决策模块的触发器（例如，调用外部授权服务器，根据对与当前访问请求相关的访问控制策略的评估，做出允许 / 拒绝的决定）。 它还可能影响 IaC，以确保在部署环境中提供适当的资源，以执行安全、隐私和合规要求。 ","relpermalink":"/book/service-mesh-devsecops/implement/ci-cd-pipeline-for-policy-as-code/","summary":"策略即代码涉及编纂所有策略，并作为 CI/CD 管道的一部分运行，使其成为应用程序运行时的一个组成部分。策略类别的例子包括授权策略、网络策略和实现工件策略（例如，容器策略）。典型的 策略即代码软件的策略管理能力可能","title":"4.4 策略即代码的 CI/CD 管道"},{"content":"云原生基础架构并不适合所有人。任何架构设计都经过了一系列的权衡。您只有熟悉自己的需求才能决定哪些权衡是有益的，哪些是有害的。\n不要在不了解架构的影响和限制的情况下采用工具或设计。我们相信云原生基础架构有很多好处，但需要意识到不应该盲目的采用。我们不愿意引导大家通过错误的方式来满足需求。\n怎么知道是否应该使用云原生基础架构设计？确定云原生基础架构是否适合您，下面是一些需要了解的问题：\n您有云原生应用程序吗？(有关可从云原生基础架构中受益的应用程序功能，请参阅第 1 章) 您的工程团队是否愿意且能够编写出体现其作业功能的生产质量代码？ 您在本地或公有云是否拥有基于 API 驱动的基础架构（IaaS）？ 您的业务是否需要更快的开发迭代或非线性人员 / 系统缩放比例？ 如果您对所有这些问题都回答“yes”，那么您可能会从本书其余部分介绍的基础架构中受益。如果您对这些问题中的某个问题回答是“no”，这并不意味着您无法从某些云原生实践中受益，但是您可能需要做更多工作，然后才能从此类基础架构中充分受益。\n在业务准备好之前武断地采用云原生基础架构效果会很糟糕，因为这会强制使用一个不正确的解决方案。在没有充分调查的情况下可能会失败，您可以能会把云原生架构看做是有缺陷或毫无用处的。鉴于之前尝试的云原生方案的失败，今后该方案也可能很难再次采用，无论它是否是正确的解决方案。\n在您准备将组织和技术转变为云原生时，我们将讨论一些需要关注的领域。要考虑的事情有很多，关键领域是您的应用程序、组织中的人员、基础架构系统和您的业务。\n应用程序 应用程序是准备工作中最简单的部分。设计模式已经很完善，自公共云出现以来，工具性能得到了显着提升。如果您无法构建云原生应用程序并通过自动部署管道来验证它们的话，则不应继续采用云原生基础架构。\n构建云原生应用程序并不一定需要微服务。这并不意味着您必须用最流行的语言开发所有软件。您必须编写可以由软件管理的软件。\n在开发过程中，人只会与云原生应用程序进行交互。其他一切都应该由基础架构或其他应用程序来管理。\n应用程序应该可以动态地扩展出多个实例。扩展通常意味着负载均衡器后运行着同一个应用程序有多个副本。假定应用程序将状态存储在存储服务（即数据库）中，并且不需要运行实例之间的复杂协调。\n动态应用程序管理意味着不需要人参与这项工作。应用程序度量触发了基础架构操作扩展应用程序。这是大多数云环境的基本特征。运行动态伸缩的资源组并不意味着您拥有云原生基础架构；但如果您的应用程序可以自动伸缩，它可能表明您的应用程序已准备就绪了。\n为了使应用程序受益，编写应用程序和配置基础架构的人员需要支持这种工作方法。如果没有人愿意放弃对软件的控制，您将永远无法实现它的好处。\n人 人是云原生基础架构中最难的部分。\n如果您想建立一个能够用软件取代人们职能和决策的架构，那么您需要确保人们了解您有最大的诉求。不仅需要人们接受变化，还需要他们自己主动寻求改变。\n开发应用程序很困难，运维基础架构很难。应用程序开发人员经常相信他们可以用工具和自动化取代基础架构运维，运维人员希望应用程序开发人员能够编写更可靠的代码，并提供自动调试和恢复。这些紧张关系是 DevOps 的基础，DevOps 有许多其他书籍，包括由 Jennifer Davis 和 Katherine Daniels 撰写的 Effective DevOps（O’Reilly，2016）。\n人们不会扩大规模，也不擅长重复无聊的工作。\n应用程序和系统工程师的目标应该是消除无聊和重复的任务，以便他们可以专注于更有趣的问题。他们需要具备开发可以包含业务逻辑和决策的软件的技能。需要有足够的工程师来编写所需的软件，更重要的是维护它。\n最关键的方面是他们需要一起工作。如果没有其他方面的支持，工程的一方无法迁移到运行和管理应用程序的新方式。团队组织和沟通结构非常重要。\n我们会尽快将团队准备好，但首先，我们必须确定基础架构迁移到云原生的时机。\n系统 云原生应用程序需要系统抽象。应用程序不应该关注单个硬编码主机名。如果您的应用程序无法在个别主机上运行，那就说明您的系统尚未准备好使用于云原生基础架构。\n使用单个服务器（虚拟机或物理机）运行操作系统，并将其转换为访问资源的方法，这就是我们所说的“抽象”。单个系统不应该是应用程序部署的目标。资源（CPU、内存和磁盘）应该集中在所有可用的机器上，然后由平台根据应用程序的请求进行分配。\n在云原生基础架构中，您必须隐藏底层系统以提高可靠性。云计算基础架构（如应用程序）会预期基础组件故障，并且可以优雅地处理此类故障。这是必要的，因为基础架构工程师不再控制堆栈中的所有内容。\nKubernetes 云原生基础架构\nKubernetes 是一个框架，它使云的方式管理应用程序变得更加容易。但是，您也可以用一种非云原生的方式使用 Kubernetes。\nKubernetes 公开了基于其核心功能的扩展，但这不是您的基础架构的最终目标。其他项目 (例如，OpenShift) 建立在它之上，将 Kubernetes 从开发人员和应用程序中抽象出来。\n应用程序应该运行在平台上。云原生基础架构并且鼓励这样运行基础架构的方式。\n如果您的应用程序是动态的，但基础架构是静态的，那么您很快就会陷入单靠 Kubernetes 无法解决的僵局。\n当这不再是一个挑战时，基础架构已经准备好成为云原生的了。一旦基础架构变得简单，自动化、自助服务和动态，就有可能被忽略。当系统可以被忽略，并且技术变得单调时，是时候向上移动堆栈了。\n如果您的系统管理依赖于硬件定制或在“混合云”中运行，则您的系统可能还没有准备好。可能需要管理一个数据中心，并且私有化。您需要保持警惕，将建立数据中心的责任与管理基础架构的责任分开。\n谷歌、Facebook、亚马逊和微软都发现通过开放的计算项目从头开始创建硬件是有好处的。之所以创建自己的硬件是因为有性能和成本的限制。因为硬件设计和基础架构构建者之间存在明确的责任分离，这些公司能够在创建定制硬件的同时运行云原生基础架构。它们不会受到“内部部署”的阻碍。相反，他们可以共同优化其硬件和软件，以获得更高的效率和性能。\n管理自己的数据中心需要大量时间和金钱的投入。创建私有云也是如此。两者都需要建立和管理数据中心团队、创建和维护 API 的团队以及在 IaaS API 之上创建抽象的团队。\n所有这些都可以完成，决定管理整个堆栈是否有价值取决于您的业务。\n现在，我们看看业务领域需要做哪些准备才能迁移到云原生。\n业务 如果系统的架构和组织的架构不一致，则组织的架构会胜出。\n—— 鲁斯马兰，“康威定律”\n企业变革速度非常缓慢。当通过扩展人员来管理扩展系统不再有效时，以及产品开发需要更多灵活性时，他们可能已经准备好采用云原生实践了。\n人无法无限扩展。对于增加管理更多服务器或开发更多代码的每个人来说，支持他们的人力基础架构（例如办公室空间）都有一定的压力。因为需要更多的沟通和协调，还会有更多额外的开销。\n正如我们在第 1 章中讨论的那样，通过使用公有云，您可以通过租用服务器来减少一些流程和人员开销。即使使用公有云，您仍然会需要管理基础架构详细信息的人员（例如服务器，服务和用户帐户）。\n当沟通结构反映业务需要创建的基础架构和应用程序时，业务已准备好采用云原生实践。这包括反映像微服务这样架构的沟通结构。他们可能是小型的独立团队，无需通过层层管理与其他团队交流或合作。\nDevOps 和 Cloud Native\nDevOps 可以补充团队合作的方式，并影响使用的工具类型。公司采用后有很多好处，包括快速原型化和提高部署速度。它也非常注重组织的文化。\n云原生需要高性能组织，但更注重于设计、架构和健康度，而不是团队工作流程和文化。如果您原以为必须解决应用程序开发人员、基础架构运维以及技术部门中任何人员之间的交互问题，才可以成功地实现云原生模式的话，那么您可能会对此感到意外。\n迫使业务变化的另一个限制因素是应用程序对敏捷性的要求更高了。企业不仅需要快速部署，还需要彻底改变部署的内容。\n部署的原始数量无关紧要。重要的是尽可能快地提供客户价值。相信部署的软件将第一次，甚至是第 100 次，满足所有客户的需求，这是一个谬论。\n当业务意识到需要频繁迭代和更改时，它可能已经准备好采用云原生应用程序了。只要在人员效率和流程方面遇到限制，且可以随时更改它，就可以准备迁移到云原生基础架构了。\n所有那些表明何时采用云原生的因素都不能说明全部情况。任何设计都需要权衡折衷。因此，在某些情况下，云原生基础架构不是正确的选择。\n什么情况下不需要云原生基础架构 只有了解了系统有哪些限制，再清楚系统可以带来的好处才有用。也就是说，是否采用一个系统的决定性因素是它的限制而使用它可以带来的利益。\n记住需求随时间变化也很重要。现在的关键功能可能在未来并不是。同样，如果下面的情况目前并不理想，那么您可以不采用云原生。\n技术限制 就像应用程序一样，在基础架构中，最简单的是技术性限制。如果您知道什么时候应该采用有技术优势的云原生基础架构，那么您可以思考下何时不应该采用云原生基础架构。\n第一个限制是没有云原生应用程序。正如在第一章中讨论的那样，如果您的应用程序需要人工交互，无论是调度、重新启动还是搜索日志，云原生基础架构都没有多大好处。\n即使您有一个可以动态调度的应用程序，也不会使其成为云原生。如果您的应用程序在 Kubernetes 上运行，但仍需要人工设置监控、日志收集和负载均衡，则它不是云原生。只是将应用程序部署在 Kubernetes 运行并不意味着云原生。\n如果您有一个编排调度器，重要的是看看它是如何运行的。您是否需要下订单、创建工单或发送电子邮件以获取服务器？\n这些是您没有自助服务基础架构的指标，这是云计算的一项要求。\n在云中，您只需提供帐单信息并调用 API。即使您在内部运行服务器，您也应该有一个可以构建 IaaS 的团队，然后将云原生基础架构分层布局。\n如果您要在自己的数据中心中构建云环境，图 2-1 显示了您的基础架构组件适合的示例。所有原始组件（例如，计算、存储、网络）都应该可以从自助式 IaaS API 中获得。\n图 2-1. 云原生基础架构的示例图层 在公有云中，您拥有 IaaS 和托管服务，但这并不意味着您的业务已准备好使用公有云。\n当您构建运行应用程序的平台时，了解您正在进行的操作非常重要。最初的开发只是构建和维护平台所需花费的一小部分，特别是对业务至关重要的平台。\n维护通常会消耗大约 40％到 80％（平均 60％）的软件成本。因此，这可能是最重要的生命周期阶段，发现业务需求和建立开发所需的技能可能对于一个小团队来说太过分了。一旦您掌握了开发所需平台的技能，您仍然需要投入时间来改进和维护系统。这需要比初始开发更长的时间。\n公有云提供商的产品为企业提供绝佳的运行环境。如果您不能或者不愿意让您的平台成为业务，那么您不应该自己创建一个平台。\n请记住，您不必自己构建一切。您可以使用可以组装到所需平台的服务或产品。\n可靠性仍然是基础架构的关键特性。如果您还没有准备好放弃对底层基础架构堆栈的控制，并且仍然通过接受故障来制造可靠的产品，那么云原生基础架构并不是正确的选择。\n非技术限制同样重要，可能超出您的控制范围。\n业务限制 如果现有流程不支持更改基础架构，则需要首先克服该障碍。幸运的是，您不必一个人做。\n本书希望有助于向需要说服力的人清楚地解释云原生的好处和流程。还有许多案例研究和公司分享他们采用这些做法的经验。本书附录 C 中将提供一个案例研究，您还可以找到相关示例并与同行和管理层分享。\n如果企业还没有实验的途径和支持尝试新事物的文化（以及伴随失败而来的后果），那么改变流程可能是不可能的。在这种情况下，您的需要等待达到必须改变的临界点，或者说服管理层认为改变是必要的。\n以一个外部的视角准确辨别一个企业是否准备好采用云原生是不可能的。不过，下面这些流程可以明确指示一个公司未准备接纳云原生：\n需要人工干预的资源请求 定期安排需要人工操作的维护窗口 手动 …","relpermalink":"/book/cloud-native-infra/when-to-adopt-cloud-native/","summary":"云原生基础架构并不适合所有人。任何架构设计都经过了一系列的权衡。您只有熟悉自己的需求才能决定哪些权衡是有益的，哪些是有害的。 不要在不了解架构的影响和限制的情况下采用工具或设计。我们相信云原生基础架构有","title":"第 2 章：采纳云原生基础架构的时机"},{"content":"如第 1.1 节所述，参考平台是一个容器编排和管理平台。在现代应用环境中，平台是物理（裸机）或虚拟化（如虚拟机、容器）基础设施上的一个抽象层。在实施 DevSecOps 原语之前，平台只是包含了应用代码，其中包含了应用逻辑和服务网状代码，而服务网状代码又提供应用服务。本节将考虑以下内容：\n一个容器编排和资源管理平台，容纳了应用程序代码和大部分的服务网格代码 服务网格的软件架构 本章大纲 2.1 容器编排和资源管理平台\n2.2 服务网格架构\n开始阅读 ","relpermalink":"/book/service-mesh-devsecops/reference-platform/","summary":"如第 1.1 节所述，参考平台是一个容器编排和管理平台。在现代应用环境中，平台是物理（裸机）或虚拟化（如虚拟机、容器）基础设施上的一个抽象层。在实施 DevSecOps 原语之前，平台只是包含了应用代码，其中包含了应用逻辑和服务","title":"第二章：实施 DevSecOps 原语的参考平台"},{"content":"勘误 1\nPDF 原文第 4 页，kubelet 端口，默认应为 10250，而不是 10251。\n","relpermalink":"/book/kubernetes-hardening-guidance/corrigendum/","summary":"勘误 1 PDF 原文第 4 页，kubelet 端口，默认应为 10250，而不是 10251。","title":"勘误"},{"content":"自动化开始听起来很神奇，但我们要面对现实：计算机分析不能每天告诉你系统有什么问题或为你修复它。它只能为你节省时间。\n至此，我想停止夸赞，我必须承认自动化的一些局限性。我这样做是因为围绕结合人工智能和可观测性会有相当多的炒作。这种炒作导致了惊人的论断，大意是：“人工智能异常检测和根源分析可以完全诊断问题！” 和 “人工智能操作将完全管理你的系统！”\n谨防炒作 为了摆脱此类炒作，我想明确的是，这类梦幻般的营销主张并不是我所宣称的现代可观测性将提供的。事实上，我预测许多与人工智能问题解决有关的主张大多是夸大其词。\n为什么人工智能不能解决我们的问题？一般来说，机器无法识别软件中的 “问题”，因为定义什么是 “问题” 需要一种主观的分析方式。而我们所讨论的那种现实世界的人工智能不能以任何程度的准确性进行主观决策。机器将始终缺乏足够的背景。\n例如，我们说该版本降低了性能，这里面也隐含了一个期望，就是这个版本包含了每个用户都期望的新功能。对于这个版本，性能退步是一个特征，而不是一个错误。\n虽然它们可能看起来像类似的活动，但在确定相关关系和确定根本原因之间存在着巨大的鸿沟。当你有正确的数据结构时，相关性是一种客观的分析 —— 你只需要计算数字。哪些相关关系是相关的，并表明真正问题的来源，总是需要主观的分析，对数据的解释。所有这些相关关系意味着什么？正如杰弗里・李波斯基（Jeffrey Lebowski）所说：“嗯，你知道，这只是你的观点，伙计。”\n神奇的 AIOps 在调查一个系统时，有两种类型的分析起作用：\n客观的分析，基于事实的、可衡量的、可观测的。 主观分析，基于解释、观点和判断的。 这种二分法 —— 客观与主观，与可计算性理论中一个重要的问题有关，即 停机问题（halting problem） 。停机问题的定义是，在给定任意计算机程序及其输入的描述的情况下，是否可以编写一个计算机程序来确定任意程序是否会结束运行或永远继续运行。简而言之，在 1936 年，艾伦・图灵（Alan Turning）证明了解决停机问题的一般算法是不存在的，这个证明的延伸可以应用于计算机软件中许多形式的识别 “问题”。\n阿兰・图灵的意思是，我们没有办法拥有神奇的 AIOps（IT 运维的人工智能）。寻找那些承诺将繁琐的客观分析自动化的工具 —— 计算数字是机器的强项！但要小心那些声称能找到问题根源并自动修复的工具。它们很可能会让你失望。\n这就是为什么我们可以确定相关关系，但不能确定因果关系：想象一下，有一台机器可以确定任意计算机程序中的问题行为是什么，并确定该行为的根本原因。如果我们真的造出了这样一台机器，那就是开香槟的时候了，因为这意味着我们终于解决了停机问题！但是，目前还没有迹象表明，机器可以解决这个问题。然而，没有迹象表明机器学习已经超越了艾伦・图灵的统一计算模型；你可以相信，这不会发生。\n做出正确的决定和修复的工作还是要靠你自己。\n时间是最宝贵的资源 然而，我们不需要神奇的 AIOps 来看到我们工作流程的巨大改善。识别相关性，同时获取相关信息，以便你能有效地浏览这些信息，这是计算机绝对可以做到的事情！这将为你节省时间。大量的时间。这么多的时间，它将从根本上改变你调查系统的方式。\n减少浪费的时间是实践现代可观测性的核心。即使是在简单的系统中，通过分析数字来识别相关性也是很困难的，而在大规模的系统中，这几乎是不可能的。通过将认知负担转移到机器上，运维人员能够有效地管理那些已经超出人类头脑所能容纳的系统。\n但是，我们分析遥测方式的这种转变并不是可观测性世界中即将发生的唯一重大变化。我们需要的大部分遥测数据来自于我们没有编写的软件：我们所依赖的开源库、数据库和管理服务。这些系统在传统上一直在为产生遥测数据而奋斗。我们可以获得哪些数据，以及这些数据来自哪里，也将发生根本性的变化。\n","relpermalink":"/book/opentelemetry-obervability/the-limitations-of-automated-analysis/","summary":"第 3 章：自动分析的局限性","title":"第 3 章：自动分析的局限性"},{"content":"为什么尽快进行 Code Review？ 在 Google，我们优化了开发团队共同开发产品的速度。，而不是优化单个开发者编写代码的速度。个人开发的速度很重要，它并不如整个团队的速度那么重要。\n当代码审查很慢时，会发生以下几件事：\n整个团队的速度降低了。是的，对审查没有快速响应的个人的确完成了其他工作。但是，对于团队其他人来说重要的新功能与缺陷修復将会被延迟数天、数周甚至数月，只因为每个 CL 正在等待审查和重新审查。 开发者开始抗议代码审查流程。如果审查者每隔几天只响应一次，但每次都要求对 CL 进行重大更改，那么开发者可能会变得沮丧。通常，开发者将表达对审查者过于“严格”的抱怨。如果审查者请求相同实质性更改（确实可以改善代码健康状况），但每次开发者进行更新时都会快速响应，则抱怨会逐渐消失。大多数关于代码审查流程的投诉实际上是通过加快流程来解决的。 代码健康状况可能会受到影响。如果审查速度很慢，则造成开发者提交不尽如人意的 CL 的压力会越来越大。审查太慢还会阻止代码清理、重构以及对现有 CL 的进一步改进。 Code Review 应该有多快？ 如果您没有处于重点任务的中，那么您应该在收到代码审查后尽快开始。\n一个工作日。是应该响应代码审查请求所需的最长时间（即第二天早上的第一件事）。\n遵循这些指导意味着典型的 CL 应该在一天内进行多轮审查（如果需要）。\n速度 vs. 中断 有一种情况下个人速度胜过团队速度。如果您正处于重点任务中，例如编写代码，请不要打断自己进行代码审查。研究表明，开发人员在被打断后需要很长时间才能恢复到顺畅的开发流程中。因此，编写代码时打断自己实际上比让另一位开发人员等待代码审查的代价更加昂贵。\n相反，在回复审查请求之前，请等待工作中断点。可能是当你的当前编码任务完成，午餐后，从会议返回，从厨房回来等等。\n快速响应 当我们谈论代码审查的速度时，我们关注的是响应时间，而不是 CL 需要多长时间才能完成整个审查并提交。理想情况下，整个过程也应该是快速的，快速的个人响应比整个过程快速发生更为重要。\n即使有时需要很久才能完成整个审查流程，但在整个过程中获得审查者的快速响应可以显着减轻开发人员对“慢速”代码审查感到的挫败感。\n如果您太忙而无法对 CL 进行全面审查，您仍然可以发送快速回复，让开发人员知道您什么时候可以开始，或推荐其他能够更快回复的审查人员，或者提供一些大体的初步评论 。 （注意：这并不意味着您应该中断编码，即使发送这样的响应，也要在工作中的合理断点处发出响应。）\n重要的是，审查人员要花足够的时间进行审查，确信他们的“LGTM”意味着“此代码符合我们的标准 。”但是，理想情况下，个人反应仍然应该很快 。\n跨时区审查 在处理时区差异时，尝试在他们还在办公室时回复作者。如果他们已经下班回家了，那么请确保在第二天回到办公室之前完成审查。\n带评论的 LGTM 为了加快代码审查，在某些情况下，即使他们也在 CL 上留下未解决的评论，审查者也应该给予 LGTM/Approval，这可以是以下任何一种情况：\n审查者确信开发人员将适当地处理所有审查者的剩余评论。 其余的更改很小，不必由开发者完成。 如果不清楚的话，审查者应该指定他们想要哪些选项。\n当开发者和审查者处于不同的时区时，带评论的 LGTM 尤其值得考虑，否则开发者将等待一整天才能获得“LGTM，Approval”。\n大型 CL 如果有人向您发送了代码审查太大，您不确定何时有时间查看，那么您应该要求开发者将 CL 拆分为几个较小的 CL 而不是一次审查的一个巨大的 CL。这通常可行，对审查者非常有帮助，即使需要开发人员的额外工作。\n如果 CL 无法分解为较小的 CL，并且您没有时间快速查看整个内容，那么至少要对 CL 的整体设计写一些评论并将其发送回开发人员以进行改进。作为审查者，您的目标之一应该在不牺牲代码健康状况的前提下，始终减少开发者能够快速采取某种进一步的操作的阻力。\n代码审查随时间推移而改进 如果您遵循这些准则，并且您对代码审查非常严格，那么您应该会发现整个代码审核流程会随着时间的推移而变得越来越快。开发者可以了解健康代码所需的内容，并向您发送从一开始就很棒的 CL，且需要的审查时间越来越短。审查者学会快速响应，而不是在审查过程中添加不必要的延迟。但是，从长远来看，不要为了提高想象中的代码审查速度，而在代码审查标准 或质量方面妥协，实际上这样做对于长期来说不会有任何帮助。\n紧急情况 还有一些紧急情况 ，CL 必须非常快速地通过整个审查流程，并且质量准则将放宽。请查看什么是紧急情况？ 中描述的哪些情况属于紧急情况，哪些情况不属于紧急情况。\n","relpermalink":"/book/eng-practices/review/reviewer/speed/","summary":"为什么尽快进行 Code Review？ 在 Google，我们优化了开发团队共同开发产品的速度。，而不是优化单个开发者编写代码的速度。个人开发的速度很重要，它并不如整个团队的速度那么重要。 当代码审查很慢时，会发生","title":"Code Review 速度"},{"content":"对于网络工作负载而言，可移植且互操作的加密身份可能是 SPIFFE 的核心用例之一。为了完全满足这一要求，社区必须达成一致，以标准化检索身份和在运行时使用身份相关服务的方式。\nSPIFFE 工作负载终端点规范通过定义一个终端点来提供 SPIFFE 可验证身份文档（SVIDs）和相关服务。具体而言，它概述了如何定位终端点以及如何服务或使用它。这个终端点所暴露的服务集合超出了本文档的范围，但有一个例外，即 SPIFFE 工作负载 API。\n引言 SPIFFE 工作负载终端点是一个 API 终端点，工作负载或正在运行的计算进程可以通过它在运行时访问与身份相关的服务（如身份签发或身份验证）。这个终端点可以暴露任意数量的与身份相关的服务，但至少，符合规范的环境中运行的工作负载可以期望 SPIFFE 工作负载 API 可用。\n本文档详细介绍了 SPIFFE 工作负载终端点的可访问性和范围、传输协议、身份验证过程以及可扩展性/发现机制。\n可访问性 SPIFFE 工作负载终端点通常用作初始身份引导的机制，包括传递和管理信任根的过程。由于在早期阶段，工作负载可能对自己的身份或应该信任的对象没有任何先验知识，因此很难确保对终端点的访问安全。因此，SPIFFE 工作负载终端点应通过本地终端点公开，并且实现者不应将同一终端点实例公开给多个主机。将终端点和相关流量限制在单个主机上可以减轻与初始身份验证和签发安全相关的引导问题。更多详细信息，请参见 传输 和 身份验证 部分。\n传输 SPIFFE 工作负载终端点必须通过 gRPC 进行提供，并且符合规范的客户端必须支持 gRPC。它可以作为 Unix 域套接字（Unix Domain Socket，UDS）或 TCP 监听套接字公开。实现者应优先选择 Unix 域套接字传输，但如果 Unix 域套接字不可行或不可能，也可以支持 TCP 传输。除非底层网络允许工作负载终端点服务器根据源 IP 地址（例如通过本地主机或链路本地网络）或其他强网络级断言（例如通过 SDN 策略）对工作负载进行强身份验证，否则不得使用 TCP 传输。\n为了防止 服务器端请求伪造 （SSRF）攻击，每个客户端请求 SPIFFE 工作负载终端点时，都必须包含静态的 gRPC 元数据键 workload.spiffe.io，其值为 true（区分大小写）。未包含此元数据键/值的请求必须被 SPIFFE 工作负载终端点拒绝（有关详细信息，请参见 错误代码 部分）。这样可以防止攻击者利用 SSRF 漏洞访问 SPIFFE 工作负载终端点，除非该漏洞还使攻击者能够控制出站 gRPC 元数据。\n传输安全 尽管 gRPC 强烈推荐使用传输层安全（Transport Layer Security，TLS），但不得要求 SPIFFE 工作负载终端点。由于 SPIFFE 工作负载终端点通常传递和管理信任根，我们不能指望工作负载具有对活跃根的先进知识。因此，在早期阶段，工作负载可能无法验证所呈现身份的真实性，除非通过 Workload API 实现的特权位置。这是 SPIFFE 工作负载终端点实例不应公开给多个主机的另一个原因。有关更多信息，请参见 身份验证 部分。\n定位终端点 客户端可以显式配置套接字位置，也可以使用名为 SPIFFE_ENDPOINT_SOCKET 的众所周知的环境变量。如果没有显式配置，符合规范的客户端必须回退到环境变量。\nSPIFFE_ENDPOINT_SOCKET 环境变量的值结构化为 RFC 3986 URI。方案（scheme）必须设置为 unix 或 tcp，分别表示终端点通过 Unix 域套接字或 TCP 监听套接字提供服务。\n如果方案设置为 unix，则授权组件不得设置，路径组件必须设置为 SPIFFE 工作负载终端点 Unix 域套接字的绝对路径（例如 unix:///path/to/endpoint.sock）。方案和路径组件是强制的，不得设置其他组件。\n如果方案设置为 tcp，则授权的主机组件必须设置为 IP 地址，授权的端口组件必须设置为 SPIFFE 工作负载终端点 TCP 监听套接字的 TCP 端口号。方案、主机和端口组件是强制的，不得设置其他组件。例如，tcp://127.0.0.1:8000 是有效的，而 tcp://127.0.0.1:8000/foo 是无效的。\n身份验证 SPIFFE 工作负载终端点通常用作初始身份引导的机制。因此，预期工作负载没有任何可用于自身身份验证的“秘密”材料。为了适应这一非常重要的用例，SPIFFE 工作负载终端点不得要求直接对其客户端进行身份验证。\n实现者应该执行带外真实性检查，而不是直接的客户端身份验证。这可以包括内核检查或编排工具询问等技术。例如，可以通过检查内核套接字状态来了解调用 API 的进程是哪个。另一种方法是允许编排工具将 Unix 域套接字放入特定容器中，向 SPIFFE 工作负载终端点实现传递容器的属性/身份信息。然后可以将此信息用作身份验证机制。\n应注意，虽然如何实现这一点的方法是特定于实现的，但所选择的方法不得要求工作负载积极参与其中。\n错误代码 在与 SPIFFE Workload 端点交互时，客户端可能会遇到多种错误条件。例如，客户端请求可能省略了必需的安全头部（请参阅传输部分获取更多信息），或者 SPIFFE Workload 端点实现可能仍在初始化或无法使用。\n如果收到不包含必需安全头部的客户端请求，实现必须使用 gRPC 状态码 “InvalidArgument” 进行响应。如果客户端收到 “InvalidArgument” 状态码，不应重试，因为这表示客户端实现有误，不可恢复。\n如果 SPIFFE Workload 端点实现正在运行但不可用，例如仍在初始化或执行负载均衡，客户端将收到 gRPC 状态码 “Unavailable”。如果客户端收到这个状态码，或者无法到达 SPIFFE Workload 端点，可以使用指数退避重试。\n最后，如果给定调用者/客户端没有为 SPIFFE Workload 端点服务定义身份，服务应使用 gRPC 状态码 “PermissionDenied” 进行响应。如果客户端收到这个状态码，可以使用指数退避重试，因为在实现最终一致性的情况下可能会遇到此类响应。\n请参阅 附录 A 获取错误条件和代码的摘要。\n可扩展性和提供的服务 SPIFFE Workload 端点可以提供多种与身份相关的服务，例如身份发放或身份验证。通过使用 gRPC/Protobuf 服务原语来公开单个服务。为了扩展 SPIFFE Workload 端点，必须引入一个新的（唯一命名的）服务。\n由于本规范承诺提供强大的可移植性，作者认为允许扩展现有逻辑服务与 SPIFFE 的精神相悖。如果通过向现有逻辑服务添加端点来提供附加功能，那么在从一个符合 SPIFFE 的环境移动到另一个环境时，无法保证可移植性。因此，不能直接扩展现有的 gRPC 逻辑服务，如 SPIFFE Workload API。相反，可以通过添加 SPIFFE 规范集中未描述的独立逻辑服务来增强端点。\n虽然所有 SPIFFE Workload 端点实现都必须公开 SPIFFE Workload API，但有时很难知道给定环境中支持哪些附加服务。因此，端点实现者应该包含对 gRPC Server Reflection 的支持。如果客户端遇到不支持 gRPC Server Reflection 的端点，应假设唯一可用的服务是 SPIFFE Workload API 中定义的那些。\n附录 A. 错误代码列表 本节列出了 SPIFFE Workload 端点实现可能返回的各种错误代码、返回条件以及如何处理它们。请参阅 错误代码 部分和 gRPC Code package 文档 以获取有关这些代码的更多信息。\n代码 条件 客户端行为 InvalidArgument 客户端请求中未包含 gRPC 安全头部。请参阅 传输部分 获取更多信息。 报告错误，不要重试。 Unavailable SPIFFE Workload 端点实现无法处理请求。 使用指数退避重试。 PermissionDenied 客户端无权执行请求的操作。根据实现的情况，这可能表示工作负载在身份或信任域被配置之前就已启动。 使用指数退避重试。 ","relpermalink":"/book/spiffe-and-spire/standard/spiffe-workload-endpoint/","summary":"对于网络工作负载而言，可移植且互操作的加密身份可能是 SPIFFE 的核心用例之一。为了完全满足这一要求，社区必须达成一致，以标准化检索身份和在运行时使用身份相关服务的方式。 SPIFFE 工作负载终端点规范通过定义一个终端点来","title":"SPIFFE 工作负载端点"},{"content":"本文描述了 SPIRE Server 的命令行选项、server.conf 设置和内置插件。\n本文档是 SPIRE Server 的配置参考。它包括有关插件类型、内置插件、服务器配置文件、插件配置和 spire-server 命令的命令行选项的信息。\n插件类型 类型 描述 DataStore 提供持久存储和 HA 功能。注意：不再支持数据存储的可插入性。只能使用内置的 SQL 插件。 KeyManager 为服务器的签名操作实现签名和密钥存储逻辑。对于利用基于硬件的关键操作很有用。 NodeAttestor 为尝试断言其身份的节点实现验证逻辑。一般与同类型的代理插件搭配使用。 UpstreamAuthority 允许 SPIRE 服务器与现有 PKI 系统集成。 Notifier 由 SPIRE 服务器通知正在发生或已经发生的某些事件。对于正在发生的事件，通知者可以将结果告知 SPIRE 服务器。 内置插件 类型 名称 描述 DataStore sql 用于 SPIRE 数据存储的 SQLite、PostgreSQL 和 MySQL 数据库的 SQL 数据库存储 KeyManager aws_kms 管理 AWS KMS 中密钥的密钥管理器 KeyManager disk 管理保存在磁盘上的密钥的密钥管理器 KeyManager memory 管理内存中非持久密钥的密钥管理器 NodeAttestor aws_iid 使用 AWS 实例身份文档证明代理身份的节点证明者 NodeAttestor azure_msi 使用 Azure MSI 令牌证明代理身份的节点证明者 NodeAttestor gcp_iit 使用 GCP 实例身份令牌证明代理身份的节点证明者 NodeAttestor join_token 节点证明器，用于验证使用服务器生成的加入令牌进行证明的代理 NodeAttestor k8s_sat 使用 Kubernetes 服务帐户令牌证明代理身份的节点证明者 NodeAttestor k8s_psat 使用 Kubernetes 投影服务帐户令牌证明代理身份的节点证明者 NodeAttestor sshpop 使用现有 ssh 证书证明代理身份的节点证明者 NodeAttestor tpm_devid 节点证明者，使用已配置 DevID 证书的 TPM 证明代理身份 NodeAttestor x509pop 使用现有 X.509 证书证明代理身份的节点证明者 Notifier gcs_bundle 将最新信任包内容推送到 Google Cloud Storage 中的对象的通知程序。 Notifier k8sbundle 将最新信任包内容推送到 Kubernetes ConfigMap 的通知程序。 UpstreamAuthority disk 使用从磁盘加载的 CA 来签署 SPIRE 服务器中间证书。 UpstreamAuthority aws_pca 使用 AWS Certificate Manager 中的私有证书颁发机构来签署 SPIRE 服务器中间证书。 UpstreamAuthority awssecret 使用从 AWS SecretsManager 加载的 CA 来签署 SPIRE 服务器中间证书。 UpstreamAuthority gcp_cas 使用 GCP 证书颁发机构服务中的私有证书颁发机构来签署 SPIRE 服务器中间证书。 UpstreamAuthority vault 使用 HashiCorp Vault 中的 PKI 秘密引擎来签署 SPIRE 服务器中间证书。 UpstreamAuthority spire 使用同一信任域中的上游 SPIRE 服务器来获取 SPIRE 服务器的中间签名证书。 UpstreamAuthority cert-manager 使用引用的证书管理器颁发者来请求中间签名证书。 服务器配置文件 下表概述了 SPIRE 服务器的配置选项。这些可以在配置文件的顶级 server { ... } 部分中设置。大多数选项都有一个相应的 CLI 标志，如果设置了该标志，则该标志优先于文件中定义的值。\nSPIRE 配置文件可以用 HCL 或 JSON 表示。请参阅示例配置文件部分以获取完整示例。\n如果 -expandEnv 标志传递给 SPIRE，则在解析之前扩展 $VARIABLE 或 ${VARIABLE} 样式环境变量。这对于模板化配置文件（例如跨不同信任域）或插入数据库连接密码等机密可能很有用。\n配置 描述 默认 admin_ids SPIFFE ID，当出现在呼叫者的 X509-SVID 中时，会授予该呼叫者管理员权限。管理 ID 必须驻留在服务器信任域或联合域中，并且不需要在服务器上有相应的管理注册条目。 agent_ttl 用于代理 SVID 的 TTL default_x509_svid_ttl 的值 audit_log_enabled 如果为 true，则启用审核日志记录 错误的 bind_address SPIRE 服务器的 IP 地址或 DNS 名称 0.0.0.0 bind_port SPIRE 服务器的 HTTP 端口号 8081 ca_key_type 用于服务器 CA 的密钥类型（X509 和 JWT），\u0026lt;rsa-2048|rsa-4096|ec-p256|ec-p384\u0026gt; ec-p256（JWT 密钥类型可以被 jwt_key_type 覆盖） ca_subject CA 证书应使用的主题（见下文） ca_ttl 默认 CA/签名密钥 TTL 24h data_dir 服务器运行时可以使用的目录 default_x509_svid_ttl 默认 X509-SVID TTL 1h default_jwt_svid_ttl 默认 JWT-SVID TTL 5m experimental 可能会更改或删除的实验选项（见下文） federation 用于联合的捆绑端点配置部分 jwt_key_type 用于服务器 CA (JWT) 的密钥类型， 如果未定义，则为 ca_key_type 或 ec-p256 的值 jwt_issuer 发行 JWT-SVID 时使用的发行者声明 log_file 将日志写入的文件 log_level 设置日志记录级别 信息 log_format 日志格式，text 或 json 文本 log_source_location 如果为 true，日志将包含源文件、行号和方法名称字段（增加一点运行时成本） 错误的 profiling_enabled 如果为 true，则启用 net/http/pprof 端点 错误的 profiling_freq 将分析数据转储到磁盘的频率。仅当 profiling_enabled 为 true 且 profiling_freq \u0026gt; 0 时启用。 profiling_names 将在每个分析标记上转储到磁盘的配置文件名称列表，请参阅分析名称 profiling_port net/http/pprof 端点的端口号。仅当 profiling_enabled 为 true 时使用。 ratelimit 速率限制配置，通常在服务器位于负载均衡器后面时使用（见下文） socket_path 将 SPIRE 服务器 API 套接字绑定到的路径（仅限 Unix） /tmp/spire-server/private/api.sock trust_domain 该服务器所属的信任域（不应超过 255 个字符） ca_subject 描述 默认 country Country 值数组 organization Organization 值数组 common_name CommonName 值 实验性的 描述 默认 cache_reload_interval 两次重新加载内存条目缓存之间的时间量。增加此值将减轻超大型部署的高数据库负载，但也会减慢新条目或更新条目向代理的传播速度。 5s auth_opa_policy_engine 用于授权决策的 auth opa_policy 引擎 默认 SPIRE 授权策略 named_pipe_name SPIRE Server API 命名管道的管道名称（仅限 Windows） \\spire-server\\private\\api 速率限制 描述 默认 attestation 是否对节点证明进行速率限制。如果为 true，则节点证明的速率限制为每个 IP 地址每秒一次尝试。 真的 signing 是否对 JWT 和 X509 签名进行速率限制。如果为 true，JWT 和 X509 签名的速率限制为每个 IP 地址每秒 500 个请求（单独）。 真的 auth_opa_policy_engine 描述 默认 local 授权策略的本地 OPA 配置。 auth_opa_policy_engine.local 描述 默认 rego_path 用于检索 OPA rego 策略以进行授权的文件。 policy_data_path 用于检索数据绑定以进行策略评估的文件。 分析名称 这些是可以在 profiling_freq 配置值中设置的可用配置文件：\ngoroutine threadcreate heap block mutex trace cpu 插件配置 服务器配置文件还包含各种 SPIRE 服务器插件的配置部分。插件配置位于顶级 plugins { ... } 部分，其格式如下：\nplugins { pluginType \u0026#34;pluginName\u0026#34; { ... plugin configuration options here ... } } 以下配置选项可用于配置插件：\n配置 描述 插件命令 插件实现二进制文件的路径（可选，内置插件不需要） 插件校验和 插件二进制文件的可选 sha256（可选，内置不需要） 已启用 启用或禁用插件（默认启用） 插件数据 插件特定数据 请参阅下面的内置插件部分，了解有关开箱即用的插件的信息。\n联邦配置 SPIRE 服务器可以配置为与位于不同信任域中的其他 SPIRE 服务器联合。SPIRE 支持在 SPIRE 服务器配置文件中（静态关系）和通过信任域 API（动态关系）配置联合关系。本节介绍如何在配置文件中配置静态定义的关系。\n注意：静态关系优先于动态关系。如果需要配置动态关系，请参见 federation 命令。静态关系不会反映在 federation 命令中。\n配置联合信任域允许信任域对其他 SPIFFE 机构颁发的身份进行身份验证，从而允许一个信任域中的工作负载安全地对外部信任域中的工作负载进行身份验证。实现联合的一个关键要素是使用 SPIFFE 捆绑端点，这些资源（由 URL 表示）为信任域提供信任捆绑的副本。使用 federation 部分，你将能够将 SPIRE 设置为 SPIFFE 捆绑包端点服务器，并配置此 SPIRE 服务器将从中获取捆绑包的联合信任域。\nserver { . . . federation { bundle_endpoint { address = \u0026#34;0.0.0.0\u0026#34; port = 8443 refresh_hint = \u0026#34;10m\u0026#34; profile \u0026#34;https_web\u0026#34; { acme { domain_name = \u0026#34;example.org\u0026#34; email = \u0026#34;mail@example.org\u0026#34; } } } federates_with \u0026#34;domain1.test\u0026#34; { bundle_endpoint_url = \u0026#34;https://1.2.3.4:8443\u0026#34; bundle_endpoint_profile \u0026#34;https_web\u0026#34; {} } federates_with \u0026#34;domain2.test\u0026#34; { bundle_endpoint_url = …","relpermalink":"/book/spiffe-and-spire/configuration/server/","summary":"本文描述了 SPIRE Server 的命令行选项、server.conf 设置和内置插件。 本文档是 SPIRE Server 的配置参考。它包括有关插件类型、内置插件、服务器配置文件、插件配置和 spire-server 命令的命令行选项的信息。 插件类型 类型 描述 DataStore 提供持久存","title":"SPIRE Server 配置参考"},{"content":" 配置 SPIRE\n注册工作负载\n使用 SVID\nSPIRE Agent\nSPIRE Server\n","relpermalink":"/book/spiffe-and-spire/configuration/","summary":"配置 SPIRE 注册工作负载 使用 SVID SPIRE Agent SPIRE Server","title":"配置"},{"content":"本文指导你如何使用 Envoy 和 JWT-SVIDs 以及开放策略代理进行安全通信。\n开放策略代理 （OPA）是一个开源的、通用的策略引擎。OPA 提供的授权（AuthZ）可以很好地补充 SPIRE 提供的认证（AuthN）。\n本教程基于 SPIRE Envoy-JWT 教程，演示如何结合 SPIRE、Envoy 和 OPA 进行 JWT SVID 认证和请求授权。实现 OPA 请求授权所需的更改在本教程中以增量形式展示，因此你应先运行或至少阅读 SPIRE Envoy-JWT 教程。\n架构图 为了说明如何使用 OPA 进行请求授权，我们在 SPIRE Envoy JWT 教程中使用的后端服务中添加了一个新的 sidecar。新的 sidecar 充当 Envoy 的新外部授权过滤器 。\n如图所示，前端服务通过由 Envoy 实例建立的 mTLS 连接连接到后端服务。Envoy 通过 mTLS 连接发送 HTTP 请求，其中携带了用于认证的 JWT-SVID。JWT-SVID 由 SPIRE Agent 提供并验证，然后，请求会根据安全策略由 OPA Agent 实例授权或拒绝。\n在本教程中，你将学习如何：\n将 OPA Agent 添加到 SPIRE Envoy JWT 教程中现有的后端服务 在连接 Envoy 到 OPA 的 Envoy 配置中添加一个外部授权过滤器 测试成功的使用 SPIRE 和 OPA 授权的 JWT 认证 先决条件 外部 IP 支持 本教程需要一个负载均衡器，该负载均衡器能够分配外部 IP（例如，metallb ）\n$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.7/config/manifests/metallb-native.yaml 等到 metallb 启动\n$ kubectl wait --namespace metallb-system \\ --for=condition=ready pod \\ --selector=app=metallb \\ --timeout=90s 应用 metallb 配置：\n$ kubectl apply -f ../envoy-x509/metallb-config.yaml 授权助手镜像 使用 Envoy-jwt-auth-helper 实现了一个外部授权过滤器，提供了一个脚本来简化使用kind或minikube的构建和导入\n$ bash ./scripts/build-helper.sh kind 之前的 SPIRE 安装 在开始之前，回顾以下内容：\n你需要访问在完成 SPIRE Envoy-JWT 教程时配置的 Kubernetes 环境。或者，你可以使用下面描述的pre-set-env.sh脚本创建 Kubernetes 环境。 此教程所需的 YAML 文件可以在 https://github.com/spiffe/spire-tutorials 的 k8s/envoy-jwt-opa 目录中找到。如果你还没有克隆 spire-tutorials 存储库，请立即这样做。 如果 Kubernetes 配置 Envoy 以执行 JWT SVID 身份验证教程环境不可用，你可以使用以下脚本创建它，并将它用作本教程的起点。从k8s/envoy-jwt-opa目录中，运行以下 Bash 脚本：\n$ bash scripts/pre-set-env.sh 该脚本将创建 SPIRE 服务器和 SPIRE 代理在集群中可用所需的所有资源，然后将为 SPIRE Envoy JWT 教程创建所有资源，这是此 SPIRE Envoy JWT 与 OPA 教程的基础场景。\n注意：本教程中显示的配置更改需要使 Envoy 和 OPA 与 SPIRE 一起工作。但是，所有这些设置已经配置好了。你不需要编辑任何配置文件。\n第一部分：部署更新和新资源 假定 SPIRE Envoy JWT 教程为起点，需要创建一些资源。目标是在请求到达backend服务之前，由 OPA 代理对其进行授权。在 Envoy 实例之间建立了 mTLS 连接，其中 JWT SVID 在请求中作为authorization头部传输。因此，缺少的部分是添加一个 OPA 代理以根据策略对请求进行授权。在本教程中应用的解决方案包括向运行在backend服务前的 Envoy 实例添加新的外部授权过滤器。新的过滤器在请求通过 Envoy JWT Auth Helper（第一个过滤器）之后调用 OPA 代理，其作用是检查是否应授权或拒绝请求。\n更新部署 为了让 OPA 授权或拒绝发送到backend服务的请求，我们需要将 OPA 添加为部署的 sidecar。我们使用openpolicyagent/opa:0.50.2-envoy镜像，该镜像扩展了 OPA 并添加了一个实现 Envoy 外部授权 API 的 gRPC 服务器，因此 OPA 可以与 Envoy 通信策略决策。在 backend-deployment.yaml 中，添加并配置新的容器，如下所示：\n- name: opa image: openpolicyagent/opa:0.50.2-envoy imagePullPolicy: IfNotPresent ports: - name: opa-envoy containerPort: 8182 protocol: TCP - name: opa-api-port containerPort: 8181 protocol: TCP args: - \u0026#34;run\u0026#34; - \u0026#34;--server\u0026#34; - \u0026#34;--config-file=/run/opa/opa-config.yaml\u0026#34; - \u0026#34;/run/opa/opa-policy.rego\u0026#34; volumeMounts: - name: backend-opa-policy mountPath: /run/opa readOnly: true 需要将backend-opa-policy ConfigMap 添加到volumes部分，如下所示：\n- name: backend-opa-policy configMap: name: backend-opa-policy-config backend-opa-policy ConfigMap 提供了两个资源，opa-config.yaml在OPA 配置 中描述，而opa-policy.rego策略在OPA 策略 部分解释。\nOPA 配置 对于本教程，我们在 opa-config.yaml 中创建了以下 OPA 配置文件：\ndecision_logs: console: true plugins: envoy_ext_authz_grpc: addr: :8182 query: data.envoy.authz.allow 选项decision_logs.console: true强制 OPA 将决策在控制台上以信息级别本地记录。稍后在教程中，我们将使用这些日志来检查不同请求的结果。\n接下来，让我们回顾一下envoy_ext_authz_grpc插件的配置。addr键设置实现 Envoy 外部授权 API 的 gRPC 服务器的监听地址。这必须与接下来的部分中详细描述的 Envoy 过滤器资源中配置的值匹配。query键定义要查询的策略决策的名称。下一部分将关注为query键指定的envoy.authz.allow策略的细节。\nOPA 策略 OPA 政策使用高级声明性语言 Rego 表达。对于本教程，我们创建了一个名为allow的样本规则，该规则包含三个表达式（请参见 opa-policy.rego ）。所有表达式必须为真，该规则才为真。\npackage envoy.authz default allow = false allow { valid_path http_request.method == \u0026#34;GET\u0026#34; svc_spiffe_id == \u0026#34;spiffe://example.org/ns/default/sa/default/frontend\u0026#34; } 让我们逐一查看每个表达式。valid_path是一个用户定义的函数，用于确保只允许发送给允许的资源的请求。\nimport input.attributes.request.http as http_request valid_path { glob.match(\u0026#34;/balances/*\u0026#34;, [], http_request.path) } valid_path { glob.match(\u0026#34;/profiles/*\u0026#34;, [], http_request.path) } valid_path { glob.match(\u0026#34;/transactions/*\u0026#34;, [], http_request.path) } 函数valid_path利用内置函数glob.match( pattern, delimiters, match)的输出，如果在由delimiters分隔的pattern中可以找到match，则其输出为真。然后，要在 Rego 中表示逻辑 OR，你需要定义具有相同名称的多个规则。这就是为什么valid_path有三个定义，每个有效资源一个。\n以下表达式定义了请求的 HTTP 方法必须等于GET:\nhttp_request.method == \u0026#34;GET\u0026#34; 最后一个表达式也对应于一个用户定义的函数，只有当 JWT-SVID 中编码的 SPIFFE ID 等于分配给frontend服务的 SPIFFE ID 时，该函数才会为真。\nsvc_spiffe_id == \u0026#34;spiffe://example.org/ns/default/sa/default/frontend\u0026#34; 函数svc_spiffe_id从请求中的authorization头中提取服务的 SPIFFE ID。因为请求已经通过了第一个 Envoy 筛选器（以验证模式运行的 Envoy JWT Auth Helper），我们知道它有一个有效的 JWT，我们可以解码来提取调用服务的 SPIFFE ID。OPA 提供了一个处理 JWT 的特殊代码，我们可以利用它来解码 JWT 并提取 SPIFFE ID：\nsvc_spiffe_id = payload.sub { [_, encoded_token] := split(http_request.headers.authorization, \u0026#34; \u0026#34;) [_, payload, _] := io.jwt.decode(encoded_token) } 因此，只有当请求被发送到一个有效的资源（/balances/，/profiles/或/transactions/）并且请求的方法为GET，且请求来自一个用等于 spiffe://example.org/ns/default/sa/default/frontend 的 SPIFFE ID 认证的工作负载时，策略才会评估为真。在所有其他情况下，请求都不会被 OPA 授权，因此会被 Envoy 拒绝。\n在 Envoy 中添加一个新的外部授权过滤器 Envoy 需要知道如何联系刚刚配置的 OPA Agent，以执行每个请求的授权。为了完成设置，我们在Envoy 配置 的http_filters部分添加一个类型为 External Authorization Filter 的新过滤器，如下所示：\n- name: envoy.filters.http.ext_authz typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthz with_request_body: max_request_bytes: 8192 …","relpermalink":"/book/spiffe-and-spire/examples/envoy-jwt-opa/","summary":"本文指导你如何使用 Envoy 和 JWT-SVIDs 以及开放策略代理进行安全通信。 开放策略代理 （OPA）是一个开源的、通用的策略引擎。OPA 提供的授权（AuthZ）可以很好地补充 SPIRE 提供的认证（AuthN）。 本教程基于 SPIRE Envoy-JWT 教程，演","title":"使用 Envoy 和 JWT-SVIDs 进行 OPA 授权"},{"content":" 在 AWS 安装 TSB\n从 AWS 容器市场安装 TSB\n","relpermalink":"/book/tsb/setup/aws/","summary":"在 AWS 安装 TSB\n从 AWS 容器市场安装 TSB","title":"AWS"},{"content":" gRPC API 指南\n","relpermalink":"/book/tsb/reference/grpc-api/","summary":"gRPC API 指南","title":"gRPC API"},{"content":" IAM (OAuth)\nIAM (OIDC)\n","relpermalink":"/book/tsb/refs/iam/","summary":"IAM (OAuth)\nIAM (OIDC)","title":"iam"},{"content":"本文档解释了如何利用 Helm Chart 来升级 TSB 的不同元素。本文假定 Helm 已经安装 在系统中。\n本文档仅适用于使用 Helm 创建的 TSB 实例，不适用于从基于 TCTL 的安装升级。\n在开始之前，请确保你已经：\n检查新版本的 要求 先决条件 已经 安装 Helm 已经 安装 TSB cli tctl 已经 安装 kubectl Tetrate 的镜像仓库的凭据 配置 Helm 仓库 添加仓库：\nhelm repo add tetrate-tsb-helm \u0026#39;https://charts.dl.tetrate.io/public/helm/charts/\u0026#39; helm repo update 列出可用版本：\nhelm search repo tetrate-tsb-helm -l 备份 PostgreSQL 数据库 创建 PostgreSQL 数据库的备份 。\n根据你的环境，连接到数据库的确切过程可能会有所不同，请参考你环境的文档。\n升级过程 管理平面 升级管理平面 Chart：\nhelm upgrade mp tetrate-tsb-helm/managementplane --namespace tsb -f values-mp.yaml 控制平面 升级控制平面 Chart：\nhelm upgrade cp tetrate-tsb-helm/controlplane --namespace istio-system -f values-cp.yaml --set-file secrets.clusterServiceAccount.JWK=/tmp/\u0026lt;cluster\u0026gt;.jwk 数据平面 升级数据平面 Chart：\nhelm upgrade dp tetrate-tsb-helm/dataplane --namespace istio-gateway -f values-dp.yaml 回滚 如果发生问题，你希望将 TSB 回滚到以前的版本，你需要回滚管理平面、控制平面和数据平面 Chart。\n回滚控制平面 你可以使用 helm rollback 回滚到当前版本。要查看当前版本，可以运行：\nhelm history cp -n istio-system 然后，你可以回滚到以前的版本：\nhelm rollback cp \u0026lt;REVISION\u0026gt; -n istio-system 回滚管理平面 缩减管理平面中的 Pod 数量 缩减管理平面中连接到 Postgres 的所有 Pod，以使其处于非活动状态。\nkubectl scale deployment tsb iam -n tsb --replicas=0 恢复 PostgreSQL 从备份中恢复你的 PostgreSQL 数据库 。 根据你的环境，连接到数据库的确切过程可能会有所不同，请参考你环境的文档。\n恢复管理平面 helm rollback mp \u0026lt;REVISION\u0026gt; -n tsb ","relpermalink":"/book/tsb/setup/helm/upgrade/","summary":"使用 Helm 升级 TSB。","title":"TSB Helm 升级"},{"content":"本页将指导你如何使用 tctl CLI 升级 TSB，呈现不同 Operator 的 Kubernetes 清单，并使用 kubectl 将它们应用到集群中以进行升级。\n在开始之前，请确保你已完成以下操作：\n检查新版本的 要求 基于 Operator 的版本之间的升级过程相对简单。一旦 Operator 的 Pod 使用新的发布镜像进行了更新，新生成的 Operator Pod 将升级所有必要的组件以适应新版本。\n创建备份 为了确保在出现问题时可以恢复一切，请为管理平面和每个集群的本地控制平面创建备份。\n备份管理平面 备份 tctl 二进制文件 由于每个新的 tctl 二进制文件可能都附带了新的 Operator 和配置来部署和配置 TSB，因此你应该备份当前正在使用的 tctl 二进制文件。请在同步新镜像之前执行此操作。\n将带有版本后缀的 tctl 二进制文件（例如 -1.3.0）复制到一个位置，以便在需要时快速还原旧版本。\ncp ~/.tctl/bin/tctl ~/.tctl/bin/tctl-{version} 如果你不小心丢失了二进制文件，你可以从 此网址 找到正确的版本。但强烈建议你备份当前的副本以确保安全。\n备份 ManagementPlane CR 通过执行以下命令创建 ManagementPlane CR 的备份：\nkubectl get managementplane -n tsb -o yaml \u0026gt; mp-backup.yaml 备份 PostgreSQL 数据库 创建 PostgreSQL 数据库的备份 。\n连接到数据库的确切过程可能因你的环境而异，请参考你的环境的文档。\n备份控制平面自定义资源 通过在每个已载入集群上执行以下命令，创建所有 ControlPlane CR 的备份：\nkubectl get controlplane -n istio-system -o yaml \u0026gt; cp-backup.yaml 升级过程 下载 tctl 和同步镜像 现在你已经创建了备份，请 下载新版本的 tctl 二进制文件 ，然后获取新的 TSB 容器镜像。\n有关如何执行此操作的详细信息在 要求和下载页面 中有描述。\n创建管理平面 Operator 创建基本清单，它将允许你从私有 Docker 注册表中更新管理平面 Operator：\ntctl install manifest management-plane-operator \\ --registry \u0026lt;your-docker-registry\u0026gt; \\ \u0026gt; managementplaneoperator.yaml 管理命名空间名称 从 TSB 0.9.0 开始，默认的管理平面命名空间名称是 tsb，而不是较早版本中使用的 tcc。如果你使用早于 0.9.0 的版本安装了 TSB，则你的管理平面可能位于 tcc 命名空间中。你需要添加 --management-namespace tcc 标志以反映这一点。 自定义 由 install 命令创建的 managementplaneoperator.yaml 文件现在可以用作管理平面升级的基本模板。如果你的现有 TSB 配置包含在标准配置之上的特定调整，你应该将它们复制到新模板中。 现在，将清单添加到源代码控制或直接使用 kubectl 客户端将其应用到管理平面集群：\nkubectl apply -f managementplaneoperator.yaml 应用清单后，你将在 tsb 命名空间中看到新的 Operator 运行：\nkubectl get pod -n tsb 名称 准备就绪 状态 重启 年龄 tsb-operator-management-plane-d4c86f5c8-b2zb5 1/1 运行中 0 8s 有关清单以及如何配置它的更多信息，请查看 ManagementPlane CR 参考 。\n创建控制平面和数据平面 Operator 要在你的应用程序集群中部署新的控制平面和数据平面 Operator，必须运行 tctl install manifest cluster-operators 来检索新版本的控制平面和数据平面 Operator 清单。\ntctl install manifest cluster-operators \\ --registry \u0026lt;your-docker-registry\u0026gt; \\ \u0026gt; clusteroperators.yaml 自定义 clusteroperators.yaml 文件现在可用于你的集群升级。如果你的现有控制平面和数据平面具有标准配置之上的特定调整，你应该将它们复制到模板中。 查看 tier1gateways 和 ingressgateways 由于 Istio 1.14 中引入的修复，当同时设置 replicaCount 和 autoscaleEnaled 时，将会忽略 replicaCount，只会应用自动缩放配置。这可能导致 tier1gateways 和 ingressgateways 在升级过程中暂时缩减到 1 个副本，直到应用自动缩放配置\n。为了避免此问题，你可以编辑 tier1gateway 或 ingressgateway 规范，删除 replicas 字段，由于当前部署已由 HPA 控制器管理，因此这将允许你使用所需的配置升级 Pod。\n你可以通过运行以下命令获取所有 tier1gateways 或 ingressgateways：\nkubectl get tier1gateway.install -A kubectl get ingressgateway.install -A 应用清单 现在，将清单添加到源代码控制或直接使用 kubectl 客户端将其应用到适当的集群中：\nkubectl apply -f clusteroperators.yaml 有关每个清单以及如何配置它们的更多信息，请查看以下指南：\nControlPlane 资源参考 Data Plane 资源参考 Kubernetes 资源参考 回滚 如果出现问题并且你想回滚 TSB 到之前的版本，你需要回滚管理平面和控制平面。\n回滚控制平面 缩减 istio-operator 和 tsb-operator kubectl scale deployment \\ -l \u0026#34;platform.tsb.tetrate.io/component in (tsb-operator,istio)\u0026#34; \\ -n istio-system \\ --replicas=0 删除 IstioOperator 资源 删除 Operator 将需要删除保护 istio 对象的 finalizer，使用以下命令：\nkubectl patch iop tsb-istiocontrolplane -n istio-system --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/metadata/finalizers\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;\u0026#34;}]\u0026#39; kubectl delete istiooperator -n istio-system --all 缩减 istio-operator 和 tsb-operator，用于数据平面 Operator kubectl scale deployment \\ -l \u0026#34;platform.tsb.tetrate.io/component in (tsb-operator,istio)\u0026#34; \\ -n istio-gateway \\ --replicas=0 删除数据平面的 IstioOperator 资源 从 1.5.11 开始，包含 ingressgateways 的 IOP 被拆分为每个 ingressgateway 都有一个 IOP。为了回滚到旧的 Istio 版本，我们需要删除保护 istio 对象的 finalizer，并使用以下命令删除所有 Operator：\nfor iop in $(kubectl get iop -n istio-gateway --no-headers | grep -i \u0026#34;tsb-ingress\u0026#34; | awk \u0026#39;{print $1}\u0026#39;); do kubectl patch iop $iop -n istio-gateway --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/metadata/finalizers\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;\u0026#34;}]\u0026#39;; done kubectl delete istiooperator -n istio-gateway --all 创建集群 Operator，回滚 ControlPlane CR 使用之前版本的 tctl 二进制文件，按照创建集群 Operator 的说明进行操作。\n然后应用 ControlPlane CR 的备份：\nkubectl apply -f cp-backup.yaml 回滚管理平面 缩减管理平面中的 Pod 缩减管理平面中的所有 Pod，使其处于非活动状态。\nkubectl scale deployment tsb iam -n tsb --replicas=0 恢复 PostgreSQL 从备份中 恢复 PostgreSQL 数据库 。连接到数据库的确切过程可能因你的环境而异，请参考你的环境的文档。\n恢复 tctl 并创建管理平面 Operator 从你创建的备份副本中恢复 tctl，或者 下载你想要使用的特定版本的二进制文件 。\nmv ~/.tctl/bin/tctl-{version} ~/.tctl/bin/tctl 按照创建管理平面 Operator 的说明创建管理平面 Operator。然后应用 ManagementPlane CR 的备份：\nkubectl apply -f mp-backup.yaml 恢复部署 最后，恢复部署。\nkubectl scale deployment tsb iam -n tsb --replicas 1 ","relpermalink":"/book/tsb/setup/self-managed/upgrade/","summary":"使用 TSB Operator 升级 TSB。","title":"TSB 升级"},{"content":"TSB 的用户界面显示了你服务的指标和健康状态。然而，如果没有显示任何指标或跟踪信息，那么你可能面临的问题可能是与你的服务或 TSB 中的某个指标组件之一有关。\n本指南将引导你了解问题是与服务还是与 TSB 中的某个指标组件有关的方法。\n指标 如果你看不到指标，请使用本指南的本节进行故障排除。\n首先，请确保你的应用程序中有流量流动。你需要流量来生成指标。\n检查你在 TSB 中设置的时间范围窗口是否正确，并且在该期间是否有流量。\n检查在浏览器中运行 UI 查询是否返回状态。使用浏览器的 inspect 命令并检查请求/响应详细信息。\n从检查器中选择 Network 选项卡，然后从 TSB 用户界面中打开你的应用程序。你应该看到浏览器和 TSB 后端之间的所有请求列表。\n搜索最后一个 graphql 请求。\n如果你看不到查询，这可能表明你的应用程序没有处理任何流量，或者你在 OAP 部署方面存在问题。\n要检查 OAP，请执行以下步骤：\n通过确认tsb命名空间中的OAP Pod 是否正在运行来检查OAP Pod 是否正常运行，并检查 Pod 的日志中是否有任何错误：\nkubectl -n tsb logs -l app=oap 来自日志的错误将帮助你排查问题。\n如果问题与 Elasticsearch 有关，请检查控制平面命名空间（istio-system）中的 OAP 是否通过将 OAP Pod 的监视端口转发到你的本地计算机来接收来自各种 Envoy 的访问日志服务（ALS）数据，并使用以下步骤查询一些指标：\n在一个 shell 中启动到 OAP 的端口转发：\nkubectl -n istio-system port-forward deployment/oap-deployment 1234 如果没有问题，你应该会看到：\nForwarding from 127.0.0.1:1234 -\u0026gt; 1234 Forwarding from [::1]:1234 -\u0026gt; 1234 在另一个 shell 中，使用以下命令获取指标：\ncurl -s http://localhost:1234/ | grep \u0026#34;envoy_als_in_count\u0026#34; 你应该会看到类似于以下示例输出：\nenvoy_als_in_count{id=\u0026#34;router~10.28.0.25~tsb-gateway-7b7fbcdfb7-726bf.bookinfo~bookinfo.svc.cluster.local\u0026#34;,cluster=\u0026#34;tsb-gateway\u0026#34;,} 67492.0 envoy_als_in_count{id=\u0026#34;sidecar~10.28.0.19~details-v1-94d5d794-kt76x.bookinfo~bookinfo.svc.cluster.local\u0026#34;,cluster=\u0026#34;details.bookinfo\u0026#34;,} 33747.0 envoy_als_in_count{id=\u0026#34;sidecar~10.28.0.23~reviews-v3-5556b6949-pvqfn.bookinfo~bookinfo.svc.cluster.local\u0026#34;,cluster=\u0026#34;reviews.bookinfo\u0026#34;,} 22500.0 envoy_als_in_count{id=\u0026#34;sidecar~10.28.0.24~productpage-v1-665ddb5664-ts6pz.bookinfo~bookinfo.svc.cluster.local\u0026#34;,cluster=\u0026#34;productpage.bookinfo\u0026#34;,} 101240.0 envoy_als_in_count{id=\u0026#34;sidecar~10.28.0.22~reviews-v2-6cb744f8ff-mf8s6.bookinfo~bookinfo.svc.cluster.local\u0026#34;,cluster=\u0026#34;reviews.bookinfo\u0026#34;,} 22498.0 envoy_als_in_count{id=\u0026#34;sidecar~10.28.0.20~ratings-v1-744894fbdb-ctvpd.bookinfo~bookinfo.svc.cluster.local\u0026#34;,cluster=\u0026#34;ratings.bookinfo\u0026#34;,} 22499.0 envoy_als_in_count{id=\u0026#34;sidecar~10.28.0.21~reviews-v1-f7c7c7b45-8v2sf.bookinfo~bookinfo.svc.cluster.local\u0026#34;,cluster=\u0026#34;reviews.bookinfo\u0026#34;,} 11249.0 如果应用程序正在使用，右侧的数字应该会增加。\n如果你看不到任何指标，或者指标随时间不变化，请检查你的应用程序 Sidecar（Envoy）是否通过执行 Istio Sidecar 的port-forward在端口 15000 上将 ALS 指标发送到控制平面 OAP，然后查询envoy_accesslog_service指标。 cx_active指标（即当前连接数）的标准数量是两个。\n下面的示例使用bookinfo应用程序的productpage服务：\n# 在一个shell中启动端口转发 kubectl -n bookinfo port-forward deployment/productpage-v1 15000 Forwarding from 127.0.0.1:15000 -\u0026gt; 15000 Forwarding from [::1]:15000 -\u0026gt; 15000 # 在另一个shell中使用curl获取配置 curl -s http://localhost:15000/clusters | grep \u0026#34;envoy_accesslog_service\u0026#34; | grep cx_active envoy_accesslog_service::10.31.243.206:11800::cx_active::2 如果计数器不符合你的预期，请通过编辑 OAP 的config.yml文件，使用以下命令将debug日志级别添加到 OAP 中：\nkubectl -n istio-system edit cm oap-config 搜索以下行并去掉注释：\n\u0026lt;!-- uncomment following line when need to debug ALS raw data \u0026lt;logger name=\u0026#34;io.tetrate.spm.user.receiver .envoy\u0026#34; level=\u0026#34;DEBUG\u0026#34;/\u0026gt; --\u0026gt; 以使其变成：\n\u0026lt;logger name=\u0026#34;io.tetrate.spm.user.receiver.envoy\u0026#34; level=\u0026#34;DEBUG\u0026#34;/\u0026gt; 然后，重新启动 OAP 以使配置更改生效：\nkubectl -n istio-system delete pod -l app=oap 现在，你可以搜索downstream_remote_address的日志。如果你有可搜索的日志，这意味着指标已经到达了 OAP 服务。\n在 Elasticsearch 后端中搜索 指标存储在 Elasticsearch（ES）索引中。你可以通过发送一些查询来检查 ES 的状态和健康状况。 由于 ES 服务器不受 TSB 管理，请参考你的文档以获取正确的连接字符串。\n在示例中，我们将端口转发到tsb命名空间中的 ES Pod：\n# 端口转发到ES服务器 kubectl -n tsb port-forward statefulset/elasticsearch 9200 # 检查集群健康状况 curl -s \u0026#39;http://localhost:9200/_cluster/health?pretty=true\u0026#39; { \u0026#34;cluster_name\u0026#34; : \u0026#34;elasticsearch\u0026#34;, \u0026#34;status\u0026#34; : \u0026#34;yellow\u0026#34;, \u0026#34;timed_out\u0026#34; : false, \u0026#34;number_of_nodes\u0026#34; : 1, \u0026#34;number_of_data_nodes\u0026#34; : 1, \u0026#34;active_primary_shards\u0026#34; : 64, \u0026#34;active_shards\u0026#34; : 64, \u0026#34;relocating_shards\u0026#34; : 0, \u0026#34;initializing_shards\u0026#34; : 0, \u0026#34;unassigned_shards\u0026#34; : 5, \u0026#34;delayed_unassigned_shards\u0026#34; : 0, \u0026#34;number_of_pending_tasks\u0026#34; : 0, \u0026#34;number_of_in_flight_fetch\u0026#34; : 0, \u0026#34;task_max_waiting_in_queue_millis\u0026#34; : 0, \u0026#34;active_shards_percent_as_number\u0026#34; : 92.7536231884058 } status 行应该是绿色或黄色的。如果是红色的，那么问题就出在 ES 集群上。你可以使用以下命令检查索引的状态：\n# 查询2020年3月26日的索引状态 curl -H\u0026#39;Content-Type: application/json\u0026#39; -s -XGET \u0026#39;http://localhost:9200/_cat/shards/*20200326 你应该会看到所有索引的列表。它们都应该处于STARTED状态。下一列包含文档数和索引大小。通过在不同时间运行该命令，你应该会看到这些数字增加。\nservice_5xx-20200326 0 p STARTED 31236 1.4mb 10.28.1.12 elasticsearch-0 service_instance_relation_client_call_sla-20200326 0 p STARTED 53791 5.1mb 10.28.1.12 elasticsearch-0 endpoint_percentile-20200326 0 p STARTED 128707 12.7mb 10.28.1.12 elasticsearch-0 endpoint_2xx-20200326 0 p STARTED 123131 7.4mb 10.28.1.12 elasticsearch-0 ... ","relpermalink":"/book/tsb/troubleshooting/tsb-ui-metrics/","summary":"在 TSB 中当指标不可见时进行故障排除。","title":"UI 指标故障排除"},{"content":" PostgreSQL 凭据\nElasticsearch 凭据\nIstio CA\n","relpermalink":"/book/tsb/operations/vault/","summary":"PostgreSQL 凭据 Elasticsearch 凭据 Istio CA","title":"Vault 客户端集成"},{"content":" 流量管理和迁移\n配置网关\nGitOps\n限制流量速率\n外部授权\n使用 WASM 扩展\n使用 WAF 功能\n使用 SkyWalking 进行 HPA\n使用 TSB 服务账号\n创建安全域\n网络策略\n","relpermalink":"/book/tsb/howto/","summary":"流量管理和迁移 配置网关 GitOps 限制流量速率 外部授权 使用 WASM 扩展 使用 WAF 功能 使用 SkyWalking 进行 HPA 使用 TSB 服务账号 创建安全域 网络策略","title":"操作任务"},{"content":"为了配置 bookinfo 应用程序，你需要创建一个网关组、一个流量组和一个安全组。每个组都提供特定的 API 来配置服务的各个方面。\n先决条件 在继续阅读本指南之前，请确保你已完成以下步骤：\n熟悉 TSB 概念 安装 TSB 演示环境 部署 Istio Bookinfo 示例应用程序 创建租户 创建工作区 使用用户界面 在左侧面板的“租户”下，选择“工作区”。 单击 bookinfo-ws 工作区卡。 单击网关组按钮。 单击带有 + 图标的卡以添加新的网关组。 输入组 ID 作为 bookinfo-gw 。 为你的网关组提供显示名称和描述。 输入 */bookinfo 作为初始命名空间选择器。 将配置模式设置为 BRIDGED 。 单击添加。 从左侧面板中选择“工作区”返回到“工作区”。 对使用组 ID bookinfo-traffic 的流量组和使用组 ID bookinfo-security 的安全组重复相同的步骤。\n使用 tctl 创建 groups.yaml 文件：\ngroups.yaml apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: organization: tetrate tenant: tetrate workspace: bookinfo-ws name: bookinfo-gw spec: namespaceSelector: names: - \u0026#34;*/bookinfo\u0026#34; configMode: BRIDGED --- apiVersion: traffic.tsb.tetrate.io/v2 kind: Group metadata: organization: tetrate tenant: tetrate workspace: bookinfo-ws name: bookinfo-traffic spec: namespaceSelector: names: - \u0026#34;*/bookinfo\u0026#34; configMode: BRIDGED --- apiVersion: security.tsb.tetrate.io/v2 kind: Group metadata: organization: tetrate tenant: tetrate workspace: bookinfo-ws name: bookinfo-security spec: namespaceSelector: names: - \u0026#34;*/bookinfo\u0026#34; configMode: BRIDGED 使用 tctl 应用配置：\ntctl apply -f groups.yaml 这些步骤将创建必要的网关、流量和安全组，用于配置 bookinfo 应用程序中服务的各个方面。\n","relpermalink":"/book/tsb/quickstart/config-groups/","summary":"为了配置 bookinfo 应用程序，你需要创建一个网关组、一个流量组和一个安全组。每个组都提供特定的 API 来配置服务的各个方面。 先决条件 在继续阅读本指南之前，请确保你已完成以下步骤： 熟悉 TSB 概念 安装 TSB 演示环境 部署 Istio Bookinfo 示例应","title":"创建配置组"},{"content":" 需要基本的分布式跟踪知识 本文假设读者具有基本的分布式跟踪概念和名词的知识。如果对分布式跟踪不熟悉，建议在阅读本文和调整 TSB 的分布式跟踪配置之前，首先了解分布式跟踪。关于分布式跟踪概念的很好的介绍，请阅读 Nic Munroe 的优秀博客 。 所需的服务行为 分布式跟踪不会自动工作，因为重要的是你的部署服务传播跟踪上下文。如果不在服务中启用上下文传播，你将遇到跟踪中断问题，并且在跟踪中看到大大降低的价值。我们建议至少支持传播 B3 和 W3C 跟踪上下文头，以及x-request-id以进行请求关联。另请参阅 Istio 文档中的跟踪上下文传播解释 。除了上下文传播，将x-request-id（以及分布式跟踪的trace id，如果有的话）包含在服务的所有请求绑定日志行中是一个很好的主意。这样可以在请求跟踪和服务日志之间实现几乎无需努力的关联，并加速故障排除。 默认情况下，TSB 提供了一个基于SkyWalking 的分布式跟踪后端，与Zipkin 兼容。在 TSB 控制下的所有Envoy 入口网关和 sidecar 都具有其内部 Zipkin 跟踪仪器，用于将跨度数据直接发送到 TSB 的 SkyWalking 收集器。还可以通过 TSB 的ControlPlane 资源对象 配置固定的全局采样率。\n如果需要更灵活地设置更精细的采样率、使用不同的跟踪仪器或将跨度数据发送到不同的后端，本文将为你提供所需的上下文信息以进行必要的更改。\nIstio Telemetry API Istio Telemetry API 通过使用作用域限定的Telemetry对象在运行时提供了调整可观测性信号的精细和灵活的方法。在 Istio Telemetry API 之前，需要调整 TSB 控制平面和数据平面运算符配置对象来配置具有固定采样率的单个分布式跟踪器。\n通过 TSB 控制平面运算符配置对象启用 Istio Telemetry API 的跟踪扩展提供程序 后，可以使用 Istio Telemetry 对象为不同的命名空间设置具有不同采样率的特定跟踪器。\nTelemetry API 功能状态 虽然 Telemetry API 自 Istio 1.12 以来一直存在，但它仍被标记为 alpha 状态。这主要是因为具有许多跟踪、度量和日志的潜在边缘配置的能力。我们已经测试和验证了针对 Zipkin、OpenCensus 和 OpenTelemetry 跟踪提供程序的集群级别跟踪配置在功能上是有效的，但不保证超出这一范围的配置用例的成功使用。Istio 不会在没有使用 Telemetry API 的情况下提供原生 OpenTelemetry 支持。 有关 Istio Telemetry API 的更多信息，请参阅Istio Telemetry API 。\nW3C 跟踪上下文传播 默认情况下，通过 Envoy 的本地 Zipkin 跟踪仪器，TSB 使用了众所周知的B3跟踪上下文传播方法。B3是一种针对各种分布式跟踪生态系统都非常好支持的传播方法，因为它一直是许多早期采用分布式跟踪的站点所有者的事实标准（例如 Netflix）。\n为什么叫 B3？ Zipkin 生态系统起源于 Twitter，那里的大多数服务都有鸟的名称。Zipkin 的后端内部项目名称是 Big Brother Bird。当 Zipkin 生态系统开源时，B3跟踪上下文头保持不变。 在 2019 年的一个分布式跟踪研讨会上，由 Zipkin 开源社区主持，邀请了来自不同组织的几位工程师汇聚在一起，以找出一种新的上下文传播方法，使跟踪系统能够互操作，即使其中一些系统具有不同的（可选）元数据要求（特别是来自 Microsoft、AWS 和 DynaTrace 的项目）。这个想法是在一个\n标题为traceparent的标题中具有所有系统都理解的公共基本上下文，另一个标题（tracestate）可以包含来自不同跟踪供应商的多个元数据块。如果理解特定的元数据块，它可以进行交互。跟踪器可以添加自己的元数据，但需要传播其他跟踪供应商的元数据，直到标题值的最大大小。然后以 FIFO 方式清除元数据块。\n为了更有力地支持新提出的解决方案，该工作被提交给了 W3C，因此这个传播格式被称为W3C跟踪上下文。当 OpenTelemetry 通过 CNCF 指定合并 Google 的 OpenCensus 项目（当时使用B3进行传播）和供应商联盟支持的 OpenTracing（对于跟踪上下文传播根本没有任何保证，每个供应商都使用自己的传播方法）而出现时，决定将默认从B3切换到W3C跟踪上下文。然而，大多数 OpenTelemetry 仪器在进行小的配置更改后仍支持B3。\n从B3上下文传播切换到 TSB 环境中的W3C跟踪上下文可以通过更改活动的 Envoy 跟踪实现。对于 TSB 1.6 集群，唯一的选择是OpenCensus。建议不要继续使用此跟踪器，因为 OpenCensus 已被弃用并不再维护。未来版本的 Envoy Proxy 和 OpenTelemetry 收集器也很有可能删除该跟踪器。当升级到 TSB 1.7 及更高版本时，建议切换到OpenTelemetry跟踪器。\n用于跟踪的 OpenTelemetry Collector OpenTelemetry Collector 是跨度数据管理的“瑞士军刀”。它可以接收来自不同跟踪仪器的不同格式的跨度数据，并将这些数据导出到多个后端，可能使用不同的跨度数据格式。在本文中，我们将展示如何使用 OpenTelemetry Collector 接收来自传入 Zipkin、OpenCensus 和 OpenTelemetry 跟踪仪器的跨度数据，以导出到与 OpenTelemetry 兼容的后端以及 TSB 的嵌入式跟踪后端。\n启用 TSB 中的跟踪的 Telemetry API 要根据本文所述的跟踪配置更改 TSB 的跟踪配置，首先需要在 TSB 中启用 Istio Telemetry API 的跟踪扩展提供程序。为此，你需要调整环境中每个群集的 TSB ControlPlane 资源对象。\nTSB 运算符使用其ControlPlane资源对象来管理其 Istio 依赖的配置和部署。当应用 TSB ControlPlane 对象时，TSB 运算符将创建一个IstioOperator 资源对象。然后使用此生成的IstioOperator资源对象通过 TSBControlPlane对象使用overlay来进行（重新）配置 Istio 部署。要启用跟踪，需要通过 TSBControlPlane对象为IstioOperator对象添加一个补丁。\nTSB ControlPlane 资源对象覆盖 为了确保不覆盖在ControlPlane对象中找到的重要自定义配置，首先需要下载当前状态。需要为要调整的每个群集重复以下步骤。\n通过运行以下命令获取 ControlPlane 资源对象：\nkubectl get -n istio-system controlplane controlplane \\ -o yaml \u0026gt; controlplane.yaml 集群名称 记下在managementPlane部分中找到的clusterName的值。在配置 IstioTelemetry对象时将需要此值。 通过添加用于IstioOperator对象的补丁来编辑 ControlPlane 对象，如下所示：\napiVersion: install.tetrate.io/v1alpha1 kind: ControlPlane metadata: ... spec: components: ... istio: kubeSpec: # 开始覆盖 overlays: - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator name: tsb-istiocontrolplane patches: - path: spec.meshConfig.extensionProviders value: # 在此处列出多个跟踪配置！ # 它们可以是不同的跟踪器，也可以是相同跟踪仪器的不同配置 - name: \u0026lt;tracing-config-name\u0026gt; \u0026lt;extensionProvider\u0026gt; service: \u0026lt;ip_or_host\u0026gt; port: \u0026lt;port_number\u0026gt; # 可选的默认扩展提供程序补丁；不是必需的 # 警告：这将注入跟踪头，即使对于特定命名空间禁用了跟踪也是如此。确保这是所需的副作用。 - path: spec.meshConfig.defaultProviders.tracing value: tracing: # 即使这是一个列表，也只支持一个默认跟踪器！ - \u0026lt;tracing-config-name\u0026gt; # 结束覆盖 deployment: ... 要安装已调整的 ControlPlane 资源对象：\nkubectl apply -f controlplane.yaml 以下是翻译：\n补丁的必需部分是为spec.meshConfig.extensionProviders 配置类型提供一个或多个跟踪配置。设置一个补丁用于spec.meshConfig.defaultProviders.tracing的配置会产生副作用，即使你的 Telemetry API 配置没有为传入请求明确设置跟踪配置，所有请求流量都将使用默认跟踪配置仪器的跟踪头进行增强。我们的建议是不要将默认配置作为补丁设置，而是依赖于你的 Telemetry API 资源对象，除非你在日志中使用跟踪 ID 进行请求关联，即使分布式跟踪被禁用也是如此。\n以下是一个灵活的设置配置的示例，其中包含多个跟踪配置，允许同时使用 B3 和 W3C trace-context 跟踪器，并具有将数据发送到 TSB、外部 Jaeger 跟踪后端或两者的能力。\n多个跟踪后端 请注意，可以多次添加相同的跟踪器类型，但每个跟踪器类型可以具有不同的端点配置。这对于指定一个用于故障排除目的的跟踪后端或为应用团队提供自己的设置非常方便。 Jaeger 后端的原生 Zipkin 和 OpenTelemetry 支持 可以通过以下命令行参数 --collector.zipkin.host-port=:9411 激活 Jaeger 的 Zipkin 支持。在下面的示例中，这是必需的，因为它启用了 “jaeger-b3” 跟踪配置，以直接将数据发送到 Jaeger，而无需在中间使用 OpenTelemetry 收集器。\nJaeger 版本 v1.35 及更高版本具有对 OpenTelemetry 的 OTLP 传输的原生支持，较早版本需要在其中使用 OpenTelemetry 收集器。在下面的示例中，假定支持 OTLP，因为它启用了 “jaeger-w3c” 跟踪配置，以直接将数据发送到 Jaeger，而无需在中间使用 OpenTelemetry 收集器。 patches: - path: spec.meshConfig.extensionProviders value: - name: tsb-b3 # 发送到 TSB 后端的 Zipkin 跟踪器 zipkin: service: \u0026#34;zipkin.istio-system.svc.cluster.local\u0026#34; port: 9411 - name: jaeger-b3 # 发送到 Jaeger 后端的 Zipkin 跟踪器 zipkin: service: \u0026#34;jaeger-collector.default.svc.cluster.local\u0026#34; port: 9411 - name: jaeger-w3c # 发送到 Jaeger 后端的 OTel 跟踪器 opentelemetry: service: …","relpermalink":"/book/tsb/operations/telemetry/distributed-tracing/","summary":"了解如何将你的跟踪生态系统与 TSB 集成。","title":"分布式跟踪集成"},{"content":"工作负载无法加入 mesh 如果新的工作负载未出现在已载入的工作负载列表中，请按照以下步骤操作。\n检查 Workload Onboarding Agent 的状态 虚拟机 (VM) 工作负载 在工作负载的主机上，例如在 VM 上运行：\nsystemctl status onboarding-agent 你应该会得到类似于以下的输出：\n● onboarding-agent.service - Workload Onboarding Agent Loaded: loaded (/usr/lib/systemd/system/onboarding-agent.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2021-10-07 14:57:23 UTC; 1 minute ago # (1) Docs: https://tetrate.io/ Main PID: 3519 (bash) CGroup: /system.slice/onboarding-agent.service ├─3520 onboarding-agent --agent-config /etc/onboarding-agent/agent.config.yaml --onboarding-config /etc/onboarding-agent/onboarding.config.yaml 如果 onboarding-agent.service 单元的状态不是 Active（1），请再次检查是否按照工作负载载入说明进行操作。\n例如，返回到：\n从 VM 载入工作负载 从 VM 自动扩展组载入工作负载 AWS ECS 工作负载 检查任务是否已创建，并且 onboarding-agent 容器和应用程序容器都处于健康状态。例如，通过运行以下命令描述 ECS 服务并检查是否有任何错误：\naws ecs describe-services --cluster \u0026lt;ECS cluster name\u0026gt; --services \u0026lt;ECS service name\u0026gt; 如果存在任何问题，请仔细检查是否按照 载入 AWS ECS 工作负载 说明进行操作。\n有关进一步的 ECS 故障排除，请参阅 AWS 故障排除指南 。\n检查 Workload Onboarding Agent 的日志 虚拟机 (VM) 工作负载 在工作负载的主机上，例如在 VM 上运行：\njournalctl -u onboarding-agent -o cat AWS ECS 工作负载 如果启用了 awslogs 日志驱动程序 ，则可以在 AWS Console 和 ecs-cli 命令行工具中查看日志。\n要在 AWS Console 中访问日志，导航到 ECS 任务，打开 Logs 选项卡，并选择 onboarding-agent 容器。\n要使用可在此处下载和安装 的 ecs-cli 工具访问日志，请运行以下命令：\necs-cli logs --cluster \u0026lt;ECS cluster name\u0026gt; --task-id \u0026lt;ECS task ID\u0026gt; --container-name onboarding-agent --follow 无法连接到 Workload Onboarding Endpoint 如果你看到类似于以下行的重复行：\ninfo agent obtaining discovery information from the Workload Onboarding Plane ... error agent RPC failed: rpc error: code = Unavailable desc = connection error: desc = \u0026#34;transport: Error while dialing dial tcp: lookup \u0026lt;onboarding-endpoint-dns-name\u0026gt; on 172.31.0.2:53: no such host\u0026#34; 那么你的工作负载无法连接到 Workload Onboarding Endpoint。\n确保：\nOnboardingConfiguration （文件 /etc/onboarding-agent/onboarding.config.yaml）包含正确的 Workload Onboarding Endpoint 的 DNS 名称 DNS 名称可解析 你可能需要返回到：\n启用 Workload 载入 工作负载未被授权加入 mesh 如果你看到类似于以下行的重复行：\ninfo agent using platform-specific credential procured by \u0026#34;aws-ec2-credential\u0026#34; plugin to request authorization for onboarding ... error agent RPC failed: rpc error: code = PermissionDenied desc = Not authorized by OnboardingPolicy error agent failed to obtain authorization for onboarding using platform-specific credential procured by \u0026#34;aws-ec2-credential\u0026#34; plugin: rpc error: code = PermissionDenied desc = Not authorized by OnboardingPolicy error agent failed to obtain authorization to onboard using platform-specific credential procured by any of the plugins （请注意 failed to obtain authorization for onboarding ... Not authorized by OnboardingPolicy）\n那么你的工作负载未被授权加入 mesh。\n请仔细检查是否已创建正确的 OnboardingPolicy 资源。\n你可能需要返回到：\n允许工作负载加入 WorkloadGroup ","relpermalink":"/book/tsb/setup/workload-onboarding/guides/troubleshooting/","summary":"工作负载无法加入 mesh 如果新的工作负载未出现在已载入的工作负载列表中，请按照以下步骤操作。 检查 Workload Onboarding Agent 的状态 虚拟机 (VM) 工作负载 在工作负载的主机上，例如在 VM 上运行： systemctl status onboarding-agent 你应该会得到类似于以下的输出： ● onboarding-agent.service - Workload Onboarding","title":"故障排除指南"},{"content":"TSB 提供了细粒度的权限管理，以控制对 TSB 资源的访问。你可以授予对资源（如组织 、租户 、工作空间 等）的访问权限。一组权限可以放入角色 中，然后可以重复使用这些角色来分配权限给适当的资源，例如用户或团队。一旦定义了角色，就可以使用访问绑定 对象将角色绑定到一组用户或团队。\n资源模型 为了了解如何处理 TSB 权限，你首先需要了解 TSB 中资源的模型。\n在 TSB 中，资源被建模为一个分层树，其中组织是所有资源的根。组织包含一个或多个集群、租户、团队和用户。租户包含一个或多个工作空间。最后，工作空间可以包含一个或多个网关组、流量组和安全组。\n注意 在本文档中，术语“配置组”用于指代所有网关组、流量组和安全组。 角色 角色 是一组针对特定资源的权限。例如，你可以创建一个允许对工作空间进行读/写权限的角色，并且对其父租户仅允许读权限。\n下面是可用的权限列表：\n权限（操作） 描述 Read 允许读取资源。 Write 允许更新资源。 Create 允许创建子资源。(*1) Delete 允许删除资源。 SetPolicy 允许将资源的控制委派给其他用户。 (*1) 当用户创建子资源时，用户将被授予新创建的资源的所有权，因此对它们具有完全控制权。\nTSB 附带了一些内置角色，具有一组最常见情况的权限：\n角色 读取 写入 创建 删除 SetPolicy 管理员 ✓ ✓ ✓ ✓ ✓ 编辑 ✓ ✓ ✓ 创建者 ✓ ✓ 写手 ✓ ✓ 读者 ✓ 管理员和编辑角色都旨在赋予资源树的所有权（完全控制）。但是，管理员角色适用于系统管理员，而编辑角色适用于仅拥有资源树的用户或团队。\n创建者角色适用于可以创建自己的资源并查看其他资源的用户。例如，具有创建者角色的用户可以读取工作空间中的所有资源，包括由其他用户创建的资源，但对于他们在该工作空间中创建的资源拥有完全所有权。\n相比之下，写手角色只允许用户读取和修改现有资源，但不允许创建新资源。\n最后，读者角色仅允许用户读取现有资源。他们将无法执行其他操作。\n访问绑定 在 TSB 中，访问绑定 对象定义了一组角色 与特定TSB资源的一组用户/团队之间的绑定。下面显示了AccessBinding的示例。在指定目标资源、角色、用户和团队时，需要使用 FQN。\napiVersion: rbac.tsb.tetrate.io/v2 kind: AccessBindings metadata: fqn: organizations/myorg/tenants/mytenant/workspaces/w1 spec: allow: - role: rbac/admin subjects: - user: organizations/myorg/users/alice - team: organizations/myorg/teams/platform-team - serviceAccount: organizations/myorg/serviceaccounts/sa 角色定义了对资源的权限集，而访问绑定定义了与特定资源相关的一组用户/团队关联的角色。由于团队成员可以更轻松地动态更改，因此通常建议你将权限分配给团队而不是用户，以便具有更大的灵活性。\n注意 TSB 将自动为上述任何资源创建一个访问绑定对象，当创建新资源时。这个访问绑定将将创建资源的用户设置为管理员。\n例如，如果你创建了一个租户，TSB 将创建一个AccessBindings，将你设置为租户的管理员。\n请勿创建新的访问绑定，如果你想将权限授予你的团队，因为你将会覆盖自动创建的访问绑定。为确保不会意外覆盖自动创建的访问绑定，请首先获取目标访问绑定 tctl get，编辑必要部分，然后使用 tctl apply 应用。\n完全限定名称 为了明确定义\n一个资源，每个资源都有一个完全限定名称（FQN），描述了它们在资源层次结构中的位置。这些名称在你将在示例中使用的对象定义中使用。\n以下显示了每个资源使用的命名模式。\n资源 FQN 角色 rbac/\u0026lt;role name\u0026gt; 组织 organizations/\u0026lt;org name\u0026gt; 集群 organizations/\u0026lt;org name\u0026gt;/clusters/\u0026lt;cluster name\u0026gt; 服务 organizations/\u0026lt;org name\u0026gt;/services/\u0026lt;service name\u0026gt; 团队 organizations/\u0026lt;org name\u0026gt;/teams/\u0026lt;team name\u0026gt; 用户 organizations/\u0026lt;org name\u0026gt;/users/\u0026lt;user name\u0026gt; 服务帐户 organizations/\u0026lt;org name\u0026gt;/serviceaccounts/\u0026lt;service account name\u0026gt; WASM 扩展 organizations/\u0026lt;org name\u0026gt;/extensions/\u0026lt;extension name\u0026gt; 租户 organizations/\u0026lt;org name\u0026gt;/tenants/\u0026lt;tenant name\u0026gt; 工作空间 organizations/\u0026lt;org name\u0026gt;/tenants/\u0026lt;tenant name\u0026gt;/workspaces/\u0026lt;workspace name\u0026gt; 应用程序 organizations/\u0026lt;org name\u0026gt;/tenants/\u0026lt;tenant name\u0026gt;/applications/\u0026lt;application name\u0026gt; API organizations/\u0026lt;org name\u0026gt;/tenants/\u0026lt;tenant name\u0026gt;/applications/\u0026lt;application name\u0026gt;/apis/\u0026lt;api name\u0026gt; 网关组 organizations/\u0026lt;org name\u0026gt;/tenants/\u0026lt;tenant name\u0026gt;/workspaces/\u0026lt;workspace name\u0026gt;/gatewaygroups/\u0026lt;group name\u0026gt; 安全组 organizations/\u0026lt;org name\u0026gt;/tenants/\u0026lt;tenant name\u0026gt;/workspaces/\u0026lt;workspace name\u0026gt;/securitygroups/\u0026lt;group name\u0026gt; 交通组 organizations/\u0026lt;org name\u0026gt;/tenants/\u0026lt;tenant name\u0026gt;/workspaces/\u0026lt;workspace name\u0026gt;/trafficgroups/\u0026lt;group name\u0026gt; Istio 内部组 organizations/\u0026lt;org name\u0026gt;/tenants/\u0026lt;tenant name\u0026gt;/workspaces/\u0026lt;workspace name\u0026gt;/istiointernalgroups/\u0026lt;group name\u0026gt; 使用角色和权限 当你首次安装 TSB 时，你将使用具有超级管理员特权的平台管理员帐户。出于明显的原因，你不希望让组织的其他成员使用此帐户使用 TSB。你应该创建新用户和团队，或使用从你的 IdP（身份提供者）（如 LDAP、Azure AD 或其他）导入的用户。\n在本节中，你将了解如何将角色和权限分配给你的团队的常见场景。出于演示目的，示例将通过使用tctl命令行执行，但你可以通过 Web UI 配置 TSB，效果相同。\n对于此示例，请假设你要配置 TSB 以具有以下团队和相应的设置：\n团队 描述 平台团队 成员充当组织管理员。他们可以创建租户、工作空间、配置组，并授予特定团队对这些资源的访问权限 应用团队 成员能够配置运行在特定命名空间中的应用程序。具体来说，他们拥有工作空间中的特定流量组的所有权。 安全团队 成员能够读取租户下的所有内容，并在工作空间中配置安全组设置。 示例将引导你逐步配置权限。你需要完成以下所有步骤才能实现所需状态。\n请假设在此示例中提到的租户、工作空间、配置组、用户和团队已经创建。另外，请注意，这不是实现相同效果的唯一方法。可能存在满足上述条件的一些角色、权限和绑定的组合。\n使用访问绑定的注意事项 在下面的示例中，请务必使用_现有_访问绑定对象，而不是创建新的访问绑定对象。每个资源都存在一个访问绑定。如果创建新的绑定对象，你将实际上覆盖现有的绑定，在大多数情况下应避免这样做。\n相反，当你被指示在以下示例中编辑访问绑定时，请确保首先使用 tctl get 获取绑定，进行编辑，然后使用 tctl apply 应用。\n例如，如果你正在使用组织AccessBindings，首先获取目标绑定：\ntctl get accessbindings organizations/myorg -o yaml \u0026gt; bindings.yaml 然后在进行编辑后，应用绑定：\ntctl apply -f bindings -o yaml \u0026gt; bindings.yaml 要获取目标租户的AccessBindings，你将使用tctl get accessbindings organizations/myorg/tenants/tenant1 -o yaml。对于层次结构中下面的其他资源，你需要提供正确的 FQN。\n配置平台团队 作为超级管理员用户，你将首先创建平台团队，然后执行任何操作。授予平台团队对必要资源的管理员角色。\n在此示例中，将在特定组织myorg上授予平台团队 Admin 角色。获取有关预期组织的AccessBindings对象，并添加平台团队（orgnizations/myorg/teams/platform）：\napiVersion: rbac.tsb.tetrate.io/v2 kind: AccessBindings metadata: fqn: organizations/myorg spec: allow: - role: rbac/admin subjects: - team: organizations/myorg/teams/platform ... 使用 tctl apply -f 应用上述配置。\n其余示例假设上述平台团队已准备好所有必要资源，以便配置可以继续进行。\n配置应用团队和工作空间 应用团队应该能够查看租户中的资源，以及能够在工作空间内创建配置组，但不能执行其他操作。在这个示例和其他示例中，假定租户名称为tenant1。\n要为团队授予对租户的读取权限，请检索用于预期租户的AccessBindings对象，并将应用团队添加为读者：\napiVersion: rbac.tsb.tetrate.io/v2 kind: AccessBindings metadata: fqn: organizations/myorg/tenants/tenant1 spec: allow: - role: rbac/reader subjects: - team: organizations/myorg/teams/app ... 要为团队授予对工作空间的创建权限，请检索用于预期工作空间的AccessBindings对象，并将应用团队添加为创建者：\napiVersion: rbac.tsb.tetrate.io/v2 kind: AccessBindings metadata: fqn: organizations/myorg/tenants/tenant1/workspaces/w1 spec: allow: - role: rbac/creator subjects: - team: organizations/myorg/teams/app 然后使用 tctl apply -f 应用这些配置。\n配置安全团队和配置组 在前面的部分中，已经指出应用团队只能访问特定的流量组。然而，由于之前的工作空间AccessBindings定义将创建者角色授予了应用团队，[他们可以随意创建新的配置组。\n在下一个场景中，平台团队想要限制应用团队拥有由他们为其创建的特定流量组。\n此外，安全团队应具有对平台团队为其创建的特定安全组的类似权限。\n要实现这一目标，请再次检索预期租户的AccessBindings，并将安全团队添 …","relpermalink":"/book/tsb/operations/users/roles-and-permissions/","summary":"如何设置与 TSB 资源相关的角色和权限。","title":"角色和权限"},{"content":"本操作指南将向你展示如何配置路由到通过单个ServiceRoute配置公开多个端口的服务。\n场景 考虑一个名为tcp-echo的后端服务，它通过 TCP 公开了两个端口，9000和9001。该服务有两个版本v1和v2，需要在这两个版本之间实现对这两个端口的流量分配。为了实现这一目标，需要配置具有端口级设置的ServiceRoute。\n部署tcp-echo服务 从 Istio 的示例目录中安装tcp-echo应用程序到echo命名空间中，安装这些清单 。\nTSB 配置 部署工作区和流量组 应用以下配置来创建一个工作区和一个流量组。\n提示 这些示例假设你已经创建了一个名为 tetrateio 的组织和一个名为 tetrate 的租户。 apiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: name: tcp-multiport-ws organization: tetrateio tenant: tetrate spec: namespaceSelector: names: - \u0026#34;*/echo\u0026#34; --- apiVersion: traffic.tsb.tetrate.io/v2 kind: Group metadata: name: tcp-multiport-tg organization: tetrateio tenant: tetrate workspace: tcp-multiport-ws spec: configMode: BRIDGED namespaceSelector: names: - \u0026#34;*/echo\u0026#34; 部署ServiceRoute 应用以下配置来创建配置两个端口的ServiceRoute。\napiVersion: traffic.tsb.tetrate.io/v2 kind: ServiceRoute metadata: name: tcp-multiport-service-route organization: tetrateio tenant: tetrate workspace: tcp-multiport-ws group: tcp-multiport-tg spec: service: \u0026#34;echo/tcp-echo.svc.cluster.local\u0026#34; portLevelSettings: - port: 9000 trafficType: TCP - port: 9001 trafficType: TCP subsets: - name: v1 labels: version: v1 weight: 80 - name: v2 labels: version: v2 weight: 20 测试 为了验证路由已成功设置，多次尝试向echo pod 发送 curl 请求。由于v1:v2之间的权重比设置为80:20，大多数情况下请求将转发到v1 pod。\n对于测试 TCP 流量，请使用nc。\nkubectl -n echo exec -it \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; -- curl -sv tcp-echo.svc.cluster.local:9000 kubectl -n echo exec -it \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; -- sh -c \u0026#34;echo hello | nc -v tcp-echo.svc.cluster.local:9001\u0026#34; ","relpermalink":"/book/tsb/howto/traffic/configure-multi-port-service-route/","summary":"配置多端口服务通过单个`ServiceRoute`配置的路由的指南。","title":"配置（多端口、多协议）服务的 ServiceRoute"},{"content":"配置保护是一个功能，它有助于保护你的 Istio 配置免受意外更改。这允许你配置允许对 TSB 生成的 Istio 配置进行更改的用户，从而保护你的 Istio 配置免受意外更改的影响。默认情况下，拥有 Kube 命名空间特权的用户可以创建新的 Istio 配置或编辑 TSB 创建的配置。尽管用户管理的配置不会被更改，但当更改 TSB 管理的配置时，它们将在下一个同步周期中被 TSB 覆盖。\n此功能有两个变体：\nenableAuthorizedUpdateDeleteOnXcpConfigs：允许用户创建和管理不受 TSB 管理的 Istio 配置。你可以将一组特定的用户添加到 authorizedUsers 列表中，以赋予这些用户编辑或删除 TSB 管理的配置的权限。 enableAuthorizedCreateUpdateDeleteOnXcpConfigs：阻止未经授权的用户在集群中创建、更新或删除任何 Istio 配置。 你可以通过在你的 ControlPlane CR 中为 XCP 组件添加以下内容来配置允许对 TSB 生成的 Istio 配置进行更改的用户：\nconfigProtection: enableAuthorizedUpdateDeleteOnXcpConfigs: true enableAuthorizedCreateUpdateDeleteOnXcpConfigs: true authorizedUsers: - user1 - system:serviceaccount:ns1:serviceaccount-1 有关 ConfigProtection 配置的更多详细信息，请参阅以下部分Config Protection 。\n","relpermalink":"/book/tsb/operations/features/enable-config-protection/","summary":"如何为 TSB 创建的资源配置配置保护。","title":"配置保护"},{"content":"TSB 支持使用外部速率限制服务器。本文档将通过一个示例描述如何配置 Envoy rate limit service 并将其用作 TSB Ingress Gateway 中的外部速率限制服务器。\n在开始之前，请确保你已经完成以下准备工作：\n熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成 TSB 快速入门 。本文档假定你已经创建了租户并熟悉工作区和配置组。还需要将 tctl 配置到你的 TSB 环境中。 注意 虽然本文档只描述了如何为 Ingress Gateway 应用外部服务器的速率限制，但你可以使用类似的配置方式对 Tier-1 网关和服务对服务（通过 TSB 流量设置）应用相同的速率限制。 创建命名空间 在本示例中，我们将在 ext-ratelimit 命名空间中安装外部速率限制服务。如果目标集群中尚未存在该命名空间，请运行以下命令创建它：\nkubectl create namespace ext-ratelimit 配置速率限制服务 注意 请阅读 Envoy rate limit 文档 以了解关于域和描述符概念的详细信息。 创建名为 ext-ratelimit-config.yaml 的文件，其内容如下。此配置指定对每个唯一的请求路径的请求应限制为每分钟 4 次。\n然后使用你创建的文件创建一个 ConfigMap：\nkubectl -n ext-ratelimit apply -f ext-ratelimit-config.yaml 部署速率限制服务器和 Redis 部署 Redis 和 envoyproxy/ratelimit。创建一个名为 redis-ratelimit.yaml 的文件，其内容如下：\nkubectl -f redis-ratelimit.yaml 如果一切正常，你应该有一个正常工作的速率限制服务器。 通过执行以下命令来确保 Redis 和速率限制服务器正在运行：\nkubectl get pods -n ext-ratelimit 你应该会看到类似以下的输出：\nNAME READY STATUS RESTARTS AGE ratelimit-d5c5b64ff-m87dt 1/1 Running 0 14s redis-7d757c948f-42sxg 1/1 Running 0 14s 配置 Ingress Gateway 本示例假定你正在对 httpbin 工作负载应用速率限制。如果尚未完成，请部署 httpbin 服务，创建 httpbin 工作区和配置组，并通过 Ingress Gateway 公开服务。\n以下示例设置了 httpbin-ratelimit 域中的请求速率限制。请求路径存储在名为 request-path 的 descriptorKey 中，然后由速率限制服务器使用。\n将此内容保存到一个名为 ext-ratelimit-ingress-gateway.yaml 的文件中，并使用 tctl 应用它：\ntctl apply -f ext-ratelimit-ingress-gateway.yaml 测试 你可以通过从外部计算机或本地环境向 httpbin Ingress Gateway 发送 HTTP 请求来测试速率限制，并在一定数量的请求之后观察速率限制生效。\n在以下示例中，由于你不能控制 httpbin.tetrate.com，你需要欺骗 curl，使其认为 httpbin.tetrate.com 解析为 Ingress Gateway 的 IP 地址。\n使用以下命令获取之前创建的 Ingress Gateway 的 IP 地址。\nkubectl -n httpbin get service httpbin-ingress-gateway \\ -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39; 然后执行以下命令，通过 Ingress Gateway 向 httpbin 服务发送 HTTP 请求。将 gateway-ip 替换为你在前一步骤中获取的值。\ncurl -k \u0026#34;http://httpbin.tetrate.com/get\u0026#34; \\ --resolve \u0026#34;httpbin.tetrate.com:80:\u0026lt;gateway-ip\u0026gt;\u0026#34; \\ -s \\ -o /dev/null \\ -w \u0026#34;%{http_code}\\n\u0026#34; 在前 4 个请求中，你应该在屏幕上看到 “200”。之后，你应该开始看到 “429”。\n你可以将请求路径更改为另一个唯一值以获取成功的响应。\ncurl -k \u0026#34;http://httpbin.tetrate.com/headers\u0026#34; \\ --resolve \u0026#34;httpbin.tetrate.com:80:\u0026lt;gateway-ip\u0026gt;\u0026#34; \\ -s \\ -o /dev/null \\ -w \u0026#34;%{http_code}\\n\u0026#34; 在 4 次请求后，你应该再次开始看到 “429”，直到更改请求路径为止。\n多集群使用速率限制服务器的考虑事项 如果你想要在多个集群中共享相同的速率限制规则，有两种可能的选择：\n在一个集群中部署单个速率限制服务，并使其可以从所有共享规则的其他集群访问，或者 在每个集群中部署 速率限制服务，但让它们都使用相同的 Redis 后端。\n在第二种情况下，你将需要让 Redis 可以从所有集群访问。每个速率限制服务器还应该使用相同的域值。\n","relpermalink":"/book/tsb/howto/rate-limiting/external-rate-limiting/","summary":"TSB 支持使用外部速率限制服务器。本文档将通过一个示例描述如何配置 Envoy rate limit service 并将其用作 TSB Ingress Gateway 中的外部速率限制服务器。 在开始之前，请确保你已经完成以下准备工作： 熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安","title":"配置外部速率限制服务器"},{"content":"Tetrate Service Bridge (TSB) 提供强大的全局可观测性功能，可提供对整个服务网格基础设施的全面洞察。TSB 简化了监控和了解服务运行状况和性能的过程，从而实现高效运营和故障排除。\n全局拓扑视图 TSB 的显着特征之一是它能够提供跨所有注册集群的服务网格拓扑的综合视图。这使组织能够掌握分布在各个可用区和区域的应用程序、服务和集群之间的复杂关系。全局拓扑视图允许全面了解应用程序如何在更大的基础设施环境中进行通信和交互。\n服务指标概述 TSB 提供以服务为中心的视角，使用户能够监控其应用程序的运行状况和性能，而不管底层部署详细信息或服务版本如何。此聚合视图简化了跨所有集群和区域评估应用程序整体运行状况的过程。\n此外，TSB 允许用户深入了解服务指标、单个集群甚至特定服务实例的特定方面。这种精细的可观测性使用户能够精确识别潜在问题和瓶颈，从而促进有效的故障排除和优化。\nEnvoy 指标分析 TSB 的全局可观测性还扩展到 Envoy，它是负责在服务网格内路由和管理流量的代理。用户可以访问与各个 Envoy 实例相关的详细指标，从而监控性能指标并深入了解网格内特定组件的行为。\n","relpermalink":"/book/tsb/concepts/observability/","summary":"TSB 中的全局可观测性。","title":"全局可观测性"},{"content":"通过东西网关，任何内部服务都可以实现高可用性，实现自动的跨集群故障转移，而无需通过入口网关将其发布为对外访问的服务。\n在本指南中，你将会：\n在一个集群 cluster-1 中部署 bookinfo 应用程序 。然后在第二个集群 cluster-2 中部署相同的 bookinfo 应用程序。 使 cluster-1 中的 reviews、details 或 ratings 服务失败，并观察到 bookinfo 应用程序的失败情况。 将 reviews、details 和 ratings 服务添加到东西网关。 重复故障场景并观察应用程序不受影响，内部流量路由到 cluster-2 中的服务。 了解东西网关 东西网关是外部面向的入口网关（Tier-1 或 Tier-2 网关）的替代方案。东西网关是本地于集群的，不会自动向外部流量公开。东西网关不需要入口网关所需的资源，例如公共 DNS、公共 TLS 证书和入口配置。\n使用入口网关 用于可以外部访问且具有公共 DNS、TLS 证书和 URL 的服务。使用 Tier 1 网关 或全局服务器负载均衡（GSLB）解决方案，使这些服务具有高可用性。 使用东西网关 用于任何内部服务，这些服务应具有高可用性，具有跨集群故障转移。东西网关易于快速配置，并为服务提供高可用性，而不需要显式的服务配置。 当将服务添加到东西网关时，Tetrate Service Bridge 会维护这些服务的内部注册表。东西网关与工作空间关联，默认情况下，所有服务都与网关注册。\n当本地客户端尝试访问服务时，它将被路由到本地服务实例。如果本地服务失败并且远程集群中存在具有东西网关的备用实例，TSB 将路由客户端流量到远程东西网关。服务发现、健康检查和故障转移路由完全自动化，对服务的开发人员或最终用户是透明的。\n载入集群 要使用东西路由，你需要载入至少两个集群到 TSB 中。这两个集群必须位于不同的可用区或不同的区域。有关如何载入集群的更多详细信息，请参见集群载入 。\n确保这两个集群共享相同的信任根。在部署两个集群的控制平面之前，你必须在 cacerts 中填充正确的证书。有关详细信息，请参阅 Istio 文档中的 Plugin CA Certificates 。\nDNS 外部地址注释 如果你希望使用 DNS 主机名进行东西网关（cluster-external-addresses 注释）的配置，你还需要在 XCP 边缘启用 DNS 解析 ，以便在 XCP 边缘进行 DNS 解析。 配置 在本示例中，假设你已经有一个名为 tetrate 的组织、一个名为 tetrate 的租户，以及两个控制平面集群 cluster-1 和 cluster-2。\n将 Bookinfo 部署到集群 1 创建带有 istio-injection 标签的 bookinfo 命名空间：\nkubectl create namespace bookinfo kubectl label namespace bookinfo istio-injection=enabled 部署 bookinfo 应用程序：\nkubectl apply -n bookinfo -f https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/platform/kube/bookinfo.yaml 创建 bookinfo 工作空间。创建以下 workspace.yaml：\napiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tetrate tenant: tetrate name: bookinfo-ws spec: namespaceSelector: names: - \u0026#34;*/bookinfo\u0026#34; 使用 tctl 应用：\ntctl apply -f bookinfo-ws.yaml 测试 Bookinfo，并模拟 details 服务的故障 你将使用 sleep 服务 作为客户端。\nkubectl create namespace sleep kubectl label namespace sleep istio-injection=enabled kubectl apply -n sleep -f https://raw.githubusercontent.com/istio/istio/master/samples/sleep/sleep.yaml 发送请求到 bookinfo 的 productpage：\nkubectl exec deployment/sleep -n sleep -c sleep -- curl -s http://productpage.bookinfo:9080/productpage | grep -i details -A 8 你将看到以下响应，指示 productpage 服务可以从 details 服务获取书籍详细信息：\n\u0026lt;h4 class=\u0026#34;text-center text-primary\u0026#34;\u0026gt;Book Details\u0026lt;/h4\u0026gt; \u0026lt;dl\u0026gt; \u0026lt;dt\u0026gt;Type:\u0026lt;/dt\u0026gt;paperback \u0026lt;dt\u0026gt;Pages:\u0026lt;/dt\u0026gt;200 \u0026lt;dt\u0026gt;Publisher:\u0026lt;/dt\u0026gt;PublisherA \u0026lt;dt\u0026gt;Language:\u0026lt;/dt\u0026gt;English \u0026lt;dt\u0026gt;ISBN-10:\u0026lt;/dt\u0026gt;1234567890 \u0026lt;dt\u0026gt;ISBN-13:\u0026lt;/dt\u0026gt;123-1234567890 \u0026lt;/dl\u0026gt; 通过终止 details 微服务来模拟故障：\nkubectl scale deployment details-v1 -n bookinfo --replicas=0 重新测试并观\n察到由于组件服务已失败，对 bookinfo 的 productpage 的请求生成错误：\nkubectl exec deployment/sleep -n sleep -c sleep -- curl -s http://productpage.bookinfo:9080/productpage | grep -i details -A 8 你将看到以下响应，显示 productpage 服务无法从 details 服务获取书籍详细信息：\n\u0026lt;h4 class=\u0026#34;text-center text-primary\u0026#34;\u0026gt;Error fetching product details!\u0026lt;/h4\u0026gt; \u0026lt;p\u0026gt;Sorry, product details are currently unavailable for this book.\u0026lt;/p\u0026gt; 恢复 details 服务的部署：\nkubectl scale deployment details-v1 -n bookinfo --replicas=1 部署东西网关 你可以使用现有的 入口网关 （已经公开了端口 15443），或者如果你希望使用仅暴露端口 15443 的特定东西网关，可以通过在 IngressGateway 部署 CR 中设置 eastWestOnly: true 来部署一个。\n在示例中，你将部署一个特定的仅暴露端口 15443 的东西网关。你可以将东西网关部署在任何命名空间中。\n创建以下 eastwest-gateway.yaml：\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: eastwest-gateway namespace: eastwest spec: eastWestOnly: true 然后将其应用到 cluster-1 和 cluster-2 两个集群：\nkubectl create ns eastwest kubectl apply -f eastwest-gateway.yaml 确保你的东西网关分配了一个 IP 地址：\nkubectl get svc -n eastwest NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE eastwest-gateway LoadBalancer 10.124.221.74 12.34.56.789 15443:31860/TCP 29s 在集群 2 中部署备份服务 在 cluster-2 中部署相同的 bookinfo 服务：\nkubectl create namespace bookinfo kubectl label namespace bookinfo istio-injection=enabled kubectl apply -n bookinfo -f https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/platform/kube/bookinfo.yaml 创建以下 bookinfo WorkspaceSetting，以配置东西路由，并使用 tctl 应用。这将启用对属于 bookinfo 工作空间的所有服务的故障转移。你可以通过指定服务选择器来选择哪些服务启用故障转移，如下所示：\napiVersion: api.tsb.tetrate.io/v2 kind: WorkspaceSetting metadata: organization: tetrate tenant: tetrate workspace: bookinfo-ws name: bookinfo-ws-setting spec: defaultEastWestGatewaySettings: - workloadSelector: namespace: eastwest labels: app: eastwest-gateway tctl apply -f bookinfo-ws-setting.yaml 对高可用配置重复故障测试 在集群 1 中，验证 BookInfo 正常工作：\nkubectl exec deployment/sleep -n sleep -c sleep -- curl -s http://productpage.bookinfo:9080/productpage | grep -i details -A 8 终止集群 1 中的 details 服务：\nkubectl scale deployment details-v1 -n bookinfo --replicas=0 验证 BookInfo 仍然正常工作，因为失败的服务的流量被路由到远程集群 2：\nkubectl exec deployment/sleep -n sleep -c sleep -- curl -s http://productpage.bookinfo:9080/productpage | grep -i details -A 8 观察故障转移 要观察故障转移，你可以使用 TSB 仪表板。在 details 服务在集群 1 失败之前，Cluster 1 中的 productpage 会将请求发送到 Cluster 1 中的 details 服务。在 details 服务在集群 1 失败之后，Cluster 1 中的 productpage 会将请求发送到 Cluster 2 中的 details 服务。\n首先，恢复集群 1 中的 details 服务部署：\nkubectl scale deployment details-v1 -n bookinfo --replicas=1 然后发送大量流量，以便在 TSB 仪表板中查看服务拓扑。\nwhile true; do kubectl exec deployment/sleep -n sleep -c sleep -- curl -s …","relpermalink":"/book/tsb/howto/gateway/multi-cluster-traffic-routing-with-eastwest-gateway/","summary":"通过东西网关，任何内部服务都可以实现高可用性，实现自动的跨集群故障转移，而无需通过入口网关将其发布为对外访问的服务。 在本指南中，你将会： 在一个集群 cluster-1 中部署 bookinfo 应用程序 。然后在第二个集群 cluster-2 中部署相同的 bookinfo 应","title":"使用东西网关实现多集群流量故障转移"},{"content":"Tetrate Service Bridge (TSB) 提供了授权功能，用于授权传入的所有 HTTP 请求，包括传入 Gateways 和 Workloads 的请求。TSB 支持本地授权，使用 JWT 声明，以及外部授权 (ext-authz)，后者使用在外部运行的服务来确定是否应允许或拒绝请求。\n如果你有一个独立的内部系统，希望使用不同于 JWT 的身份验证方案，或者希望集成第三方授权解决方案，比如 Open Policy Agent (OPA) 或 PlainID ，你可以决定使用外部授权系统。\nExt-authz 可以在不同的上下文中进行配置，例如 Tier-1 Gateways 、Ingress Gateways 以及 Traffic Settings 。以下表格显示了在 TSB 中如何使用外部授权的一些可能方式：\n上下文 示例用途 Tier-1 网关 可以配置 Tier-1 网关，仅接受带有有效 JWT 和经过身份验证 API 声明的请求，以及带有适当基本授权的请求等 Ingress 网关 Ingress 网关 / Tier-2 网关 / 应用程序网关可以配置以实施基于用户权益限制 API 的业务逻辑 交通设置 交通设置中的 Ext-authz 适用于关联命名空间中的所有代理。这在限制对服务 API 的部分访问方面特别有用 使用外部授权进行服务间的授权\n在 Ingress Gateways 中配置外部授权\n使用 TLS 验证的外部授权\n在 Tier-1 网关中使用外部授权\n","relpermalink":"/book/tsb/howto/authorization/","summary":"如何授权传入请求。","title":"外部授权"},{"content":"启动 Workload Onboarding Agent 创建文件 /etc/onboarding-agent/onboarding.config.yaml，内容如下。 将 ONBOARDING_ENDPOINT_ADDRESS 替换为 你之前获取的值 。\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: \u0026#34;\u0026lt;ONBOARDING_ENDPOINT_ADDRESS\u0026gt;\u0026#34; transportSecurity: tls: sni: onboarding-endpoint.example # (1) workloadGroup: # (2) namespace: bookinfo name: ratings workload: labels: version: v5 # (3) settings: connectedOver: INTERNET # (4) 此配置指示 Workload Onboarding Agent 使用一个地址连接到 Workload Onboarding Endpoint，但对 DNS 名称 onboarding-endpoint.example 进行 TLS 证书验证（1）。\n代理将尝试加入你之前创建的 WorkloadGroup（2）。\n在（3）中指定的额外标签将与工作负载关联。它不会影响工作负载与 WorkloadGroup 的匹配。\n此配置还指示 Workload Onboarding Agent 通知此工作负载通过 INTERNET 连接到网格的其他部分（而不是 VPC）。网格中的其他节点将尝试使用工作负载的公共 IP 连接到此工作负载。由于你在不同网络中启动了 Kubernetes 集群和 EC2 实例，因此这是必需的。\n在将上述配置文件放置在正确位置后，执行以下命令启动 Workload Onboarding Agent：\n# 启用 sudo systemctl enable onboarding-agent # 启动 sudo systemctl start onboarding-agent 通过执行以下命令验证 Istio Sidecar 是否已启动：\ncurl -f -i http://localhost:15000/ready 你应该会得到类似以下的输出：\nHTTP/1.1 200 OK content-type: text/plain; charset=UTF-8 server: envoy LIVE 验证工作负载 从本地机器上，验证工作负载是否已正确加入。\n执行以下命令：\nkubectl get war -n bookinfo 如果工作负载已正确加入，你应该会得到类似以下的输出：\nNAMESPACE NAME AGENT CONNECTED AGE bookinfo ratings-aws-aws-123456789012-us-east-2b-ec2-i-1234567890abcdef0 True 1m 验证从 Kubernetes 到 VM 的流量 为了验证从 Kubernetes Pod 到 AWS EC2 实例的流量，对 Kubernetes 上部署的 Bookinfo 应用程序创建一些负载，并确认请求是否被路由到部署在 AWS EC2 实例上的 ratings 应用程序。\n在本地机器上，如果尚未这样做，请设置端口转发 。\n然后运行以下命令：\nfor i in `seq 1 9`; do curl -fsS \u0026#34;http://localhost:9080/productpage?u=normal\u0026#34; | grep \u0026#34;glyphicon-star\u0026#34; | wc -l | awk \u0026#39;{print $1\u0026#34; stars on the page\u0026#34;}\u0026#39; done 其中两次中你应该会收到消息 10 stars on the page。\n此外，你可以通过检查 Istio sidecar 代理传输的入站 HTTP 请求的访问日志 来验证 VM 是否接收流量。\n执行以下命令：\njournalctl -u onboarding-agent -o cat 你应该会看到类似以下的输出：\n[2021-10-25T11:06:13.553Z] \u0026#34;GET /ratings/0 HTTP/1.1\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 0 48 3 2 \u0026#34;-\u0026#34; \u0026#34;curl/7.68.0\u0026#34; \u0026#34;1928e798-dfe7-45a6-9020-d0f3a8641d03\u0026#34; \u0026#34;172.31.7.211:9080\u0026#34; \u0026#34;127.0.0.1:9080\u0026#34; inbound|9080|| 127.0.0.1:40992 172.31.7.211:9080 172.31.7.211:35470 - default 验证从 VM 到 Kubernetes 的流量 SSH 进入 AWS EC2 实例并执行以下命令：\nfor i in `seq 1 5`; do curl -i \\ --resolve details.bookinfo:9080:127.0.0.2 \\ details.bookinfo:9080/details/0 done 上述命令将向 Bookinfo details 应用程序\n发出 5 个 HTTP 请求。 curl 将解析 Kubernetes 集群的本地 DNS 名称 details.bookinfo，将其解析为 Istio 代理的 egress 监听器的 IP 地址（根据你之前创建的 sidecar 配置 为 127.0.0.2）。\n你应该会得到类似以下的输出：\nHTTP/1.1 200 OK content-type: application/json server: envoy {\u0026#34;id\u0026#34;:0,\u0026#34;author\u0026#34;:\u0026#34;William Shakespeare\u0026#34;,\u0026#34;year\u0026#34;:1595,\u0026#34;type\u0026#34;:\u0026#34;paperback\u0026#34;, \u0026#34;pages\u0026#34;:200,\u0026#34;publisher\u0026#34;:\u0026#34;PublisherA\u0026#34;,\u0026#34;language\u0026#34;:\u0026#34;English\u0026#34;, \u0026#34;ISBN-10\u0026#34;:\u0026#34;1234567890\u0026#34;,\u0026#34;ISBN-13\u0026#34;:\u0026#34;123-1234567890\u0026#34;} ","relpermalink":"/book/tsb/setup/workload-onboarding/quickstart/aws-ec2/onboard-vm/","summary":"启动 Workload Onboarding Agent 创建文件 /etc/onboarding-agent/onboarding.config.yaml，内容如下。 将 ONBOARDING_ENDPOINT_ADDRESS 替换为 你之前获取的值 。 apiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: \"\u003cONBOARDING_ENDPOINT_ADDRESS\u003e\" transportSecurity: tls: sni: onboarding-endpoint.example # (1) workloadGroup: # (2) namespace: bookinfo name: ratings workload: labels: version: v5 # (3) settings: connectedOver:","title":"虚拟机工作负载载入"},{"content":"本文档解释了如何测试故障转移，并运维任务，如排空和恢复集群。\n平台 Operator 可能需要手动执行以下操作：\n在执行维护之前，将工作负载集群排除在旋转之外，以允许现有请求完成 在执行维护之前，将 Edge Gateway 排除在旋转之外，以允许缓存的 DNS 记录超时 定义一个区域为“活动”或“被动”（Tetrate 默认模型是全活动） 工作负载集群故障转移 和 Edge Gateway 故障转移 指南中的示例演示了以可控且可预测的方式将组件排除在服务之外的各种方式，最佳实现将受到特定拓扑和 GSLB 解决方案选择的影响。\n将工作负载集群排除在旋转之外 选项 1：编辑 Edge Gateway 配置 编辑 Edge Gateway 集群列表不会影响服务可用性。已删除集群的请求将允许完成，并且新的请求将不会路由到该集群。\n你需要在 Edge Gateway 的 Gateway 配置中显式列出工作负载集群：\nspec: workloadSelector: namespace: edge labels: app: edgegw http: - name: bookinfo port: 80 hostname: bookinfo.tse.tetratelabs.io routing: rules: - route: clusterDestination: {} spec: workloadSelector: namespace: edge labels: app: edgegw http: - name: bookinfo port: 80 hostname: bookinfo.tse.tetratelabs.io routing: rules: - route: clusterDestination: clusters: - name: cluster-1 - name: cluster-2 选项 2：删除工作负载集群上的 Gateway 资源 删除工作负载集群上的 Gateway 资源。\nTetrate 平台将立即更新 Edge Gateway 配置，从已删除 Gateway 资源中的主机名的负载均衡集中移除该集群。\n停机时间不太可能出现，并且 Envoy Gateway 将尝试负载均衡到其他集群，如果它观察到失败（通常是 404 Not Found 响应）。\n选项 3：取消配置工作负载集群上的 Ingress Gateway 删除工作负载集群上的 IngressGateway 资源。这将取消配置 Ingress Gateway。\nTetrate 平台将立即更新 Edge Gateway 配置，从已删除 Gateway 资源中的主机名的负载均衡集中移除该集群。\n可能会出现短暂的停机时间，如果 Envoy Gateway 观察到故障（通常是连接超时），它将尝试负载均衡到其他集群。这可能会导致未完成的请求延迟。\n将区域排除在旋转之外 交通分布到各个区域的 Edge Gateway 受 GSLB 解决方案的控制。\n选项 1：配置 GSLB 解决方案 使用 GSLB 提供商的 API，并按照其最佳实践指南，将所需的区域（Edge Gateway）排除在旋转之外。\n选项 2：触发健康检查 此选项需要额外的配置，但允许管理员在无需与第三方 GSLB API 交互的情况下将区域排除在旋转之外。\n核心原则是使用健康检查，并在希望将区域下线时引发该健康检查失败。常规请求不受影响，因此对于已缓存 Edge Gateway 在该区域的 DNS 记录的客户端，不会发生任何服务中断或增加的延迟。\nEdge Gateway 故障转移 指南解释了健康检查的原理，其中一个特殊标记的请求（例如带有 X-HealthCheck: true 标头的请求）接收到触发 GSLB 解决方案中的故障转移或恢复的自定义响应。可以以许多方式实现健康检查，例如编辑 Edge Gateway 资源以返回错误，或使用特殊的 URL，将其路由到工作负载集群上的金丝雀服务（例如 http\nbin）。根据你的需求和与 Tetrate 平台互动的期望方式，请参考 Tetrate 专业服务以获得具体建议。\n","relpermalink":"/book/tsb/design-guides/ha-multicluster/operations/","summary":"本文档解释了如何测试故障转移，并运维任务，如排空和恢复集群。 平台 Operator 可能需要手动执行以下操作： 在执行维护之前，将工作负载集群排除在旋转之外，以允许现有请求完成 在执行维护之前，将 Edge Gateway 排除在旋转之外，以允许","title":"运维和测试高可用性与故障转移"},{"content":"🔔 提示：本指南假设你对 Argo CD 所基于的工具有一定的了解。请阅读了解基础知识 以了解这些工具。\n要求 安装 kubectl 命令行工具。 有一个 kubeconfig 文件（默认位置是~/.kube/config）。 CoreDNS。可以通过以下方式为 microk8s 启用microk8s enable dns \u0026amp;\u0026amp; microk8s stop \u0026amp;\u0026amp; microk8s start 1. 安装 Argo CD kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml 这将创建一个新的命名空间，argocd，Argo CD 服务和应用程序资源将驻留在其中。\n🔔 警告：安装清单包括ClusterRoleBinding引用命名空间的资源argocd。如果你要将 Argo CD 安装到不同的命名空间中，请确保更新命名空间引用。\n如果你对 UI、SSO、多集群功能不感兴趣，那么你可以仅安装核心 Argo CD 组件：\nkubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/core-install.yaml 此默认安装将具有自签名证书，如果没有一些额外的工作就无法访问。执行以下操作之一：\n按照说明配置证书 （并确保客户端操作系统信任它）。 配置客户端操作系统以信任自签名证书。 在本指南中的所有 Argo CD CLI 操作上使用 –insecure 标志。 用于argocd login --core配置 CLI 访问并跳过 步骤 3-5。\n2. 下载 Argo CD CLI 从 GitHub 下载最新的 Argo CD 版本。更详细的安装说明可以通过 CLI 安装文档 找到。\n还适用于 Mac、Linux 和 WSL Homebrew：\nbrew install argocd 3. 访问 Argo CD API 服务器 默认情况下，Argo CD API 服务器不向外部 IP 公开。要访问 API 服务器，请选择以下技术之一来公开 Argo CD API 服务器：\n服务类型负载均衡器 将 argocd-server 服务类型更改为LoadBalancer：\nkubectl patch svc argocd-server -n argocd -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;}}\u0026#39; Ingress 请按照ingress 文档 了解如何使用 ingress 配置 Argo CD。\n转发端口 Kubectl 端口转发还可用于连接到 API 服务器，而无需公开服务。\nkubectl port-forward svc/argocd-server -n argocd 8080:443 然后可以使用 https://localhost:8080 访问 API 服务器\n4. 使用 CLI 登录 帐户的初始密码admin是自动生成的，并以明文形式存储 在 Argo CD 安装命名空间中password命名的机密字段中。argocd-initial-admin-secret你可以使用 CLI 简单地检索此密码argocd：\nargocd admin initial-password -n argocd 🔔 警告：argocd-initial-admin-secret更改密码后，你应该从 Argo CD 命名空间中删除。该秘密除了以明文形式存储最初生成的密码外没有其他用途，并且可以随时安全地删除。如果必须重新生成新的管理员密码，Argo CD 将根据需要重新创建它。\n使用上面的用户名admin和密码，登录 Argo CD 的 IP 或主机名：\nargocd login \u0026lt;ARGOCD_SERVER\u0026gt; 🔔 注意：CLI 环境必须能够与 Argo CD API 服务器通信。如果无法按照上述步骤 3 中的描述直接访问它，你可以告诉 CLI 通过以下机制之一使用端口转发来访问它：1) 向每个 CLI 命令添加 --port-forward-namespace argocd标志；或 2) 设置ARGOCD_OPTS环境变量：export ARGOCD_OPTS=\u0026#39;--port-forward-namespace argocd\u0026#39;.\n使用以下命令更改密码：\nargocd account update-password 5. 注册集群以部署应用程序（可选） 此步骤将集群的凭据注册到 Argo CD，并且仅在部署到外部集群时才需要。在内部部署时（到运行 Argo CD 的同一集群），应使用 https://kubernetes.default.svc 作为应用程序的 K8s API 服务器地址。\n首先列出当前 kubeconfig 中的所有集群上下文：\nkubectl config get-contexts -o name 从列表中选择一个上下文名称并将其提供给argocd cluster add CONTEXTNAME。例如，对于 docker-desktop 上下文，运行：\nargocd cluster add docker-desktop 上面的命令将 ServiceAccount ( argocd-manager) 安装到该 kubectl 上下文的 kube-system 命名空间中，并将服务帐户绑定到管理员级别的 ClusterRole。Argo CD 使用此服务帐户令牌来执行其管理任务（即部署 / 监控）。\n🔔 注意：可以修改 argocd-manager-role 角色的规则，使其仅具有对有限的命名空间、组和类型集的 create、update、patch、delete权限。但是，要使 Argo CD 发挥作用，在集群作用域中需要get、list和watch权限。\n6. 从 Git 存储库创建应用程序 https://github.com/argoproj/argocd-example-apps.git 提供了包含 guestbook 应用程序的示例存储库，以演示 Argo CD 的工作原理。\n通过 CLI 创建应用程序 首先，我们需要运行以下命令将当前命名空间设置为 argocd：\nkubectl config set-context --current --namespace=argocd 使用以下命令创建示例 guestbook 应用程序：\nargocd app create guestbook --repo https://github.com/argoproj/argocd-example-apps.git --path guestbook --dest-server https://kubernetes.default.svc --dest-namespace default 通过 UI 创建应用程序 打开浏览器访问 Argo CD 外部 UI，通过在浏览器中访问 IP / 主机名进行登录，并使用步骤 4 中设置的凭据。\n登录后，点击 + New App 按钮，如下图：\n+ New App 按钮 为你的应用程序命名guestbook，使用项目default，并将同步策略保留为Manual：\n应用程序信息 通过将存储库 url 设置为 github 存储库 url，将 https://github.com/argoproj/argocd-example-apps.git 存储库连接到 Argo CD，将修订保留为HEAD，并将路径设置为guestbook：\n连接仓库 对于Destination，将集群 URL 设置为https://kubernetes.default.svc（或in-cluster集群名称），将命名空间设置为default：\n目的地 填写完以上信息后，点击 UI 顶部的Create guestbook即可创建应用程序：\n目的地 7. 同步（部署）应用程序 通过 CLI 同步 创建 guestbook 应用程序后，你现在可以查看其状态：\n$ argocd app get guestbook Name: guestbook Server: https://kubernetes.default.svc Namespace: default URL: https://10.97.164.88/applications/guestbook Repo: https://github.com/argoproj/argocd-example-apps.git Target: Path: guestbook Sync Policy: \u0026lt;none\u0026gt; Sync Status: OutOfSync from (1ff8a67) Health Status: Missing GROUP KIND NAMESPACE NAME STATUS HEALTH apps Deployment default guestbook-ui OutOfSync Missing Service default guestbook-ui OutOfSync Missing 应用程序状态为初始OutOfSync状态，因为应用程序尚未部署，并且尚未创建 Kubernetes 资源。要同步（部署）应用程序，请运行：\nargocd app sync guestbook 此命令从存储库检索清单并执行kubectl apply其中一个清单。guestbook 应用程序现已运行，你现在可以查看其资源组件、日志、事件和评估的健康状态。\n通过 UI 同步 guestbook 应用 查看应用 ","relpermalink":"/book/argo-cd/getting-started/","summary":"🔔 提示：本指南假设你对 Argo CD 所基于的工具有一定的了解。请阅读了解基础知识 以了解这些工具。 要求 安装 kubectl 命令行工具。 有一个 kubeconfig 文件（默认位置是~/.kube/config）。 CoreDNS。可以通过以下方式为 microk8s","title":"入门"},{"content":"本指南介绍了 Argo Rollouts 如何与Istio Service Mesh 集成进行流量塑形。本指南建立在基本入门指南 的概念基础上。\n要求 安装了 Istio 的 Kubernetes 集群 提示\n请参见 Istio 环境设置指南 ，了解如何在本地 minikube 环境中设置 Istio。\n1. 部署 Rollout、服务、Istio VirtualService 和 Istio Gateway 当使用 Istio 作为流量路由器时，Rollout 金丝雀策略必须定义以下强制字段：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollouts-demo spec: strategy: canary: # 引用控制器用于指向金丝雀副本集的 Service canaryService: rollouts-demo-canary # 引用控制器用于指向稳定副本集的 Service stableService: rollouts-demo-stable trafficRouting: istio: virtualServices: # 可以配置一个或多个 VirtualService # 引用控制器用于更新金丝雀权重的 VirtualService - name: rollouts-demo-vsvc1 # 如果 VirtualService 中有一个 HTTP 路由，则可选，否则必填 routes: - http-primary # 如果 VirtualService 中有一个 HTTPS/TLS 路由，则可选，否则必填 tlsRoutes: # 下面的字段都是可选的，但如果定义了，则必须与你的 VirtualService 中至少一个 TLS 路由匹配规则完全匹配 - port: 443 # 仅在你希望匹配包含此端口的任何规则的 VirtualService 时才需要 # 仅在你希望匹配所有这些 SNI 主机的任何规则的 VirtualService 时才需要 sniHosts: - reviews.bookinfo.com - localhost - name: rollouts-demo-vsvc2 # 如果 VirtualService 中有一个 HTTP 路由，则可选，否则必填 routes: - http-secondary # 如果 VirtualService 中有一个 HTTPS/TLS 路由，则可选，否则必填 tlsRoutes: # 下面的字段都是可选的，但如果定义了，则必须与你的 VirtualService 中至少一个 TLS 路由匹配规则完全匹配 - port: 443 # 仅在你希望匹配包含此端口的任何规则的 VirtualService 时才需要 # 仅在你希望匹配所有这些 SNI 主机的任何规则的 VirtualService 时才需要 sniHosts: - reviews.bookinfo.com - localhost tcpRoutes: # 下面的字段都是可选的，但如果定义了，则必须与你的 VirtualService 中至少一个 TCP 路由匹配规则完全匹配 - port: 8020 # 仅在你希望匹配包含此端口的任何规则的 VirtualService 时才需要 在trafficRouting.istio.virtualService或trafficRouting.istio.virtualServices中引用的 VirtualService 和路由。trafficRouting.istio.virtualServices可以帮助添加一个或多个 VirtualService，而trafficRouting.istio.virtualService只能添加单个 virtualService。这是为了有 HTTP、TLS、TCP 或混合路由规范，将稳定服务和金丝雀服务分开。如果路由是 HTTPS/TLS，则可以根据给定的端口号和/或 SNI 主机进行匹配。请注意，它们两个都是可选的，只有在你想要匹配包含这些规则的 VirtualService 时才需要它们。\n在本指南中，这两个服务分别是：rollouts-demo-stable和rollouts-demo-canary。这两个服务的权重应最初设置为稳定服务的 100% 和金丝雀服务的 0%。在更新期间，这些值将由控制器修改。如果有多个 VirtualService，则控制器将同时修改每个 VirtualService 的稳定和金丝雀服务的权重值。\n请注意，由于我们的 Rollout 规范中有 HTTP 和 HTTPS 路由，并且它们匹配 VirtualService 规范，因此权重将同时针对这两个路由进行修改。\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: rollouts-demo-vsvc1 spec: gateways: - rollouts-demo-gateway hosts: - rollouts-demo-vsvc1.local http: - name: http-primary # 应与 rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.routes 匹配 route: - destination: host: rollouts-demo-stable # 应与 rollout.spec.strategy.canary.stableService 匹配 port: number: 15372 weight: 100 - destination: host: rollouts-demo-canary # 应与 rollout.spec.strategy.canary.canaryService 匹配 port: number: 15372 weight: 0 tls: - match: - port: 443 # 应与 rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.tlsRoutes 中定义的路由的端口号匹配 sniHosts: # 应与 rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.tlsRoutes 中定义的路由的所有 SNI 主机匹配 - reviews.bookinfo.com - localhost route: - destination: host: rollouts-demo-stable # 应与 rollout.spec.strategy.canary.stableService 匹配 weight: 100 - destination: host: rollouts-demo-canary # 应与 rollout.spec.strategy.canary.canaryService 匹配 weight: 0 tcp: - match: - port: 8020 # 应与 rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.tcpRoutes 中定义的路由的端口号匹配 route: - destination: host: rollouts-demo-stable # 应与 rollout.spec.strategy.canary.stableService 匹配 weight: 100 - destination: host: rollouts-demo-canary # 应与 rollout.spec.strategy.canary.canaryService 匹配 weight: 0 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: rollouts-demo-vsvc2 spec: gateways: - rollouts-demo-gateway hosts: - rollouts-demo-vsvc2.local http: - name: http-secondary # 应该与 rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.routes 匹配 route: - destination: host: rollouts-demo-stable # 应该与 rollout.spec.strategy.canary.stableService 匹配 port: number: 15373 weight: 100 - destination: host: rollouts-demo-canary # 应该与 rollout.spec.strategy.canary.canaryService 匹配 port: number: 15373 weight: 0 tls: - match: - port: 443 # 应该与 rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.tlsRoutes 中定义的路由的端口号匹配 sniHosts: # 应该与 rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.tlsRoutes 中定义的路由的所有 SNI 主机匹配 - reviews.bookinfo.com route: - destination: host: rollouts-demo-stable # 应该与 rollout.spec.strategy.canary.stableService 匹配 weight: 100 - destination: host: rollouts-demo-canary # 应该与 rollout.spec.strategy.canary.canaryService 匹配 weight: 0 tcp: - match: - port: 8020 # 应该与 rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.tcpRoutes 中定义的路由的端口号匹配 route: - destination: host: rollouts-demo-stable # 应该与 rollout.spec.strategy.canary.stableService 匹配 weight: 100 - destination: host: rollouts-demo-canary # 应该与 rollout.spec.strategy.canary.canaryService 匹配 weight: 0 运行以下命令进行部署：\n一个 Rollout 两个服务（稳定和金丝雀） 一个或多个 Istio VirtualServices 一个 Istio Gateway kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/istio/rollout.yaml kubectl apply -f …","relpermalink":"/book/argo-rollouts/getting-started/istio/index/","summary":"本指南介绍了 Argo Rollouts 如何与Istio Service Mesh 集成进行流量塑形。本指南建立在基本入门指南 的概念基础上。 要求 安装了 Istio 的 Kubernetes 集群 提示 请参见 Istio 环境设置指南 ，了解如何在本地 minikube 环境中设置 Istio。 1. 部署 Rollout、服","title":"Istio"},{"content":"Istio 是一种服务网格，通过一组 CRD 提供了丰富的功能集，用于控制流向 Web 服务的流量。Istio 通过调整 Istio VirtualService 中定义的流量权重来实现此功能。使用 Argo Rollouts 时，用户可以部署包含至少一个 HTTP 路由 的 VirtualService，其中包含两个 HTTP 路由目标 ：一个路由目标针对金丝雀 ReplicaSet 的 pod，一个路由目标针对稳定 ReplicaSet 的 pod。Istio 提供了两种带权流量分割方法，这两种方法都可以作为 Argo Rollouts 的选项。\n主机级流量分割 子集级流量分割 主机级流量分割 使用 Argo Rollouts 和 Istio 进行流量分割的第一种方法是在两个主机名或 Kubernetes Service 之间进行分割：一个金丝雀 Service 和一个稳定 Service。这种方法类似于所有其他 Argo Rollouts mesh/ingress-controller 集成方式的工作方式（例如 ALB、SMI、Nginx）。使用此方法，用户需要部署以下资源：\nRollout Service（金丝雀） Service（稳定） VirtualService Rollout 应定义以下字段：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollout-example spec: ... strategy: canary: canaryService: canary-svc # 必需 stableService: stable-svc # 必需 trafficRouting: istio: virtualService: name: rollout-vsvc # 必需 routes: - primary # 如果 VirtualService 中只有一条路由，则为可选，否则为必需 steps: - setWeight: 5 - pause: duration: 10m VirtualService 必须包含一个 HTTP 路由，其名称在 Rollout 中引用，包含两个路由目标，其 host 值与 Rollout 中引用的 canaryService 和 stableService 匹配。如果 VirtualService 定义在与 rollout 不同的命名空间中，则其名称应为 rollout-vsvc.\u0026lt;vsvc 命名空间名称\u0026gt;。请注意，Istio 要求所有权重总和为 100，因此初始权重可以是 100％ 的稳定和 0％ 的金丝雀。\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: rollout-vsvc spec: gateways: - istio-rollout-gateway hosts: - istio-rollout.dev.argoproj.io http: - name: primary # 在 canary.trafficRouting.istio.virtualService.routes 中被引用 route: - destination: host: stable-svc # 在 canary.stableService 中被引用 weight: 100 - destination: host: canary-svc # 在 canary.canaryService 中被引用 weight: 0 最后，应部署金丝雀和稳定的 Service。这些 Service 的选择器将在更新期间由 Rollout 修改，以对齐金丝雀和稳定 ReplicaSet pod。请注意，如果 VirtualService 和目标主机位于不同的命名空间中（例如，VirtualService 和 Rollout 不在同一命名空间中），则应在目标主机中包含命名空间（例如，stable-svc.\u0026lt;namespace\u0026gt;）。\napiVersion: v1 kind: Service metadata: name: canary-svc spec: ports: - port: 80 targetPort: http protocol: TCP name: http selector: app: rollouts-demo # 这个选择器将使用 canary ReplicaSet 的pod-template-hash进行更新。例如：rollouts-pod-template-hash: 7bf84f9696 --- apiVersion: v1 kind: Service metadata: name: stable-svc spec: ports: - port: 80 targetPort: http protocol: TCP name: http selector: app: rollouts-demo # 这个选择器将使用 stable ReplicaSet 的pod-template-hash进行更新。例如：rollouts-pod-template-hash: 123746c88d 在 Rollout 更新的生命周期中，Argo Rollouts 将不断地：\n修改金丝雀 Service spec.selector，以包含金丝雀 ReplicaSet 的 rollouts-pod-template-hash 标签 修改稳定 Service spec.selector，以包含稳定 ReplicaSet 的 rollouts-pod-template-hash 标签 修改 VirtualService spec.http[].route[].weight，以匹配当前所需的金丝雀权重 🔔 注意：Rollout 不会对 VirtualService 或 Istio 网络中的其他字段做出任何假设。如果需要，用户可以为 VirtualService 指定其他配置，例如主要路由或任何其他路由的 URI 重写规则。用户还可以为每个服务创建特定的 DestinationRules。\n子集级流量分割 🔔 重要提示：自 v1.0 起可用。\n使用 Argo Rollouts 和 Istio 进行流量分割的第二种方法是在两个 Istio DestinationRule Subsets 之间进行分割：一个金丝雀子集和一个稳定子集。在按 DestinationRule 子集进行拆分时，需要用户部署以下资源：\nRollout Service VirtualService DestinationRule Rollout 应定义以下字段：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollout-example spec: ... strategy: canary: trafficRouting: istio: virtualService: name: rollout-vsvc # 必需 routes: - primary # 如果 VirtualService 中只有一条路由，则为可选，否则为必需 destinationRule: name: rollout-destrule # 必需 canarySubsetName: canary # 必需 stableSubsetName: stable # 必需 steps: - setWeight: 5 - pause: duration: 10m 应定义一个服务，该服务针对 Rollout pods。请注意，与第一种方法不同，在第一种方法中，流量拆分针对多个 Service，这些 Service 被修改为包含金丝雀/稳定 ReplicaSets 的 rollout-pod-template-hash，而此 Service 不会被 rollout 控制器修改。\napiVersion: v1 kind: Service metadata: name: rollout-example spec: ports: - port: 80 targetPort: http protocol: TCP name: http selector: app: rollout-example VirtualService 必须包含一个 HTTP 路由，其名称在 Rollout 中引用，包含两个路由目标，其 subset 值与 Rollout 中引用的 canarySubsetName 和 stableSubsetName 匹配。请注意，Istio 要求所有权重总和为 100，因此初始权重可以是 100％ 的稳定和 0％ 的金丝雀。\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: rollout-vsvc spec: gateways: - istio-rollout-gateway hosts: - istio-rollout.dev.argoproj.io http: - name: primary # 在 canary.trafficRouting.istio.virtualService.routes 中被引用 route: - destination: host: rollout-example subset: stable # 在 canary.trafficRouting.istio.destinationRule.stableSubsetName 中被引用 weight: 100 - destination: host: rollout-example subset: canary # 在 canary.trafficRouting.istio.destinationRule.canarySubsetName 中被引用 weight: 0 最后，包含在 Rollout 中引用的金丝雀和稳定子集的 DestinationRule。\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: rollout-destrule spec: host: rollout-example subsets: - name: canary # 在 canary.trafficRouting.istio.destinationRule.canarySubsetName 中引用 labels: # 标签将注入 canary 部署的 pod 模板哈希值 app: rollout-example - name: stable # 在 canary.trafficRouting.istio.destinationRule.stableSubsetName 中引用 labels: # 标签将注入 stable 部署的 pod 模板哈希值 app: rollout-example 在使用 Istio DestinationRule 的 Rollout 生命周期中，Argo Rollouts 将不断地：\n修改 VirtualService spec.http[].route[].weight，以匹配当前所需的金丝雀权重 修改 DestinationRule spec.subsets[].labels，以包含金丝雀和稳定 ReplicaSets 的 rollouts-pod-template-hash 标签 TCP 流量分割 🔔 重要提示：自 v1.2.2 起可用。\n支持拆分 TCP 流量，并要求 Rollout 定义以下字段： …","relpermalink":"/book/argo-rollouts/traffic-management/istio/index/","summary":"Istio 是一种服务网格，通过一组 CRD 提供了丰富的功能集，用于控制流向 Web 服务的流量。Istio 通过调整 Istio VirtualService 中定义的流量权重来实现此功能。使用 Argo Rollouts 时，用户可以部署包含至少一个 HTTP 路由 的 VirtualService，","title":"Istio"},{"content":"🔔 重要提示：自 v0.10.0 起可用于金丝雀发布。\n🔔 重要提示：自 v1.0 起可用于蓝绿发布。\n一个用例是，在 Rollout 标记或注释所需/稳定的 Pod 时，用用户定义的标签/注释，仅在它们是所需或稳定的集合期间，并且在 ReplicaSet 切换角色（例如从所需到稳定）时更新/删除标签。此举使 prometheus、wavefront、datadog 等查询和仪表板可以依赖一致的标签，而不是不可预测且从版本到版本不断变化的 rollouts-pod-template-hash。\n使用金丝雀策略的 Rollout 有能力使用 stableMetadata 和 canaryMetadata 字段将短暂元数据附加到稳定或金丝雀 Pod 上。\nspec: strategy: canary: stableMetadata: labels: role: stable canaryMetadata: labels: role: canary 使用蓝绿策略的 Rollout 有能力使用 activeMetadata 和 previewMetadata 字段将短暂元数据附加到活动或预览 Pod 上。\nspec: strategy: blueGreen: activeMetadata: labels: role: active previewMetadata: labels: role: preview 在更新期间，Rollout 将创建所需的 ReplicaSet，并将 canaryMetadata/previewMetadata 中定义的元数据合并到所需的 ReplicaSet 的 spec.template.metadata 中。这将导致 ReplicaSet 的所有 Pod 都使用所需的元数据创建。当 Rollout 完全升级时，所需的 ReplicaSet 变为稳定，更新为使用 stableMetadata/activeMetadata 下的标签和注释。ReplicaSet 的 Pod 然后将原地更新以使用稳定的元数据（而无需重新创建 Pod）。\n🔔 重要提示：为了让工具能够利用这个功能，他们需要识别标签和/或注释在 Pod 启动后发生的变化。并非所有工具都能检测到这一点。\n","relpermalink":"/book/argo-rollouts/rollout/ephemeral-metadata/","summary":"🔔 重要提示：自 v0.10.0 起可用于金丝雀发布。 🔔 重要提示：自 v1.0 起可用于蓝绿发布。 一个用例是，在 Rollout 标记或注释所需/稳定的 Pod 时，用用户定义的标签/注释，仅在它们是所需或稳定的集合期间，并且在 ReplicaSet 切换角色（例如从所需到","title":"短暂元数据"},{"content":"开始使用 Argo Rollouts：\n基础使用\nAmbassador\nAWS ALB\nAWS AppMesh\nIstio\nNginx\n快速开始 ","relpermalink":"/book/argo-rollouts/getting-started/","summary":"开始使用 Argo Rollouts： 基础使用 Ambassador AWS ALB AWS AppMesh Istio Nginx 快速开始","title":"入门"},{"content":"在第三章介绍的概念基础上，本章说明了 SPIFFE 标准。解释 SPIRE 实现的组成部分以及它们是如何结合在一起的。最后，讨论威胁模型以及如果特定组件被破坏会发生什么。\n什么是 SPIFFE？ 普适安全生产身份框架（SPIFFE）是一套软件身份的开源标准。为了以一种与组织和平台无关的方式实现可互操作的软件身份，SPIFFE 定义了必要的接口和文件，以完全自动化的方式获得和验证加密身份。\nSPIFFE ID，代表软件服务的名称（或身份）。 SPIFFE 可验证身份文件（SVID），这是一个可加密验证的文件，用于向对等者证明服务的身份。 SPIFFE Workload API，这是一个简单的节点本地 API，服务用它来获得身份，而不需要认证。 SPIFFE Trust Bundle（信任包），一种代表特定 SPIFFE 发行机构使用的公钥集合的格式。 SPIFFE Federation，这是一个简单的机制，通过它可以共享 SPIFFE Trust Bundle。 SPIFFE 不是什么 SPIFFE 旨在识别服务器、服务和其他通过计算机网络通信的非人类实体。这些都有一个共同点，那就是这些身份必须是可以自动发出的（没有人类在其中参与）。虽然有可能使用 SPIFFE 来识别人或其他野生动物物种，但该项目特意将这些用例排除在范围之外。除了机器人和机器之外，没有其他特别的考虑。\nSPIFFE 向服务提供身份和相关信息，同时管理该身份的生命周期，但它仅仅作为提供者，因为它不直接利用其提供的身份。利用 SPIFFE 身份是服务的责任。在使用 SPIFFE 身份时，有多种解决方案可以实现认证层，如端到端加密通信或服务间授权和访问控制，但是，这些功能也被认为不属于 SPIFFE 项目的范围，SPIFFE 不会直接解决这些问题。\nSPIFFE ID SPIFFE ID 是一个字符串，作为服务的唯一名称。它被模拟成一个 URI，由几个部分组成。前缀 spiffe://（作为 URI 的方案），信任域的名称（作为主机部分），以及特定工作负载的名称或身份（作为路径部分）。\n一个简单的 SPIFFE ID 可能只是 spiffe://example.com/myservice。\nSPIFFE ID 的第一个组成部分是 spiffe:// URI 方案。虽然很普通，但包括它是一个重要的细节，因为它有助于将 SPIFFE ID 与 URL 或其他类型的 URL 区分开来。\nSPIFFE ID 的第二个组成部分是信任域名称（example.com）。在某些情况下，整个组织只有一个信任域。在其他情况下，可能需要有许多信任域。信任域的语义将在本章后面介绍。\n最后一个组成部分是工作负载本身的名称部分，由 URI 路径表示。SPIFFE ID 的这一部分的具体格式和组成是因地制宜的。各机构可以自由选择对其最有意义的命名方案。例如，我们可以选择一个既能反映组织位置又能反映工作负载目的的命名方案，如：\nspiffe://example.com/bizops/hr/taxrun/withholding 值得注意的是，SPIFFE ID 的主要目的是以一种灵活的方式来表示工作负载的身份，使人类和机器都能轻松使用。当试图在 SPIFFE ID 的格式中灌输太多的意义时，应该谨慎行事。例如，试图编纂后来被用作授权元数据的各个部分的属性，会导致互操作性和灵活性的挑战。相反，建议使用一个单独的数据库 。\nSPIFFE 信任域 SPIFFE 规范引入了信任域的概念。信任域被用来管理组织内部和组织之间的管理和安全边界，每个 SPIFFE ID 都有其信任域的名称，如上所述。具体来说，信任域是 SPIFFE ID 命名空间的一部分，在这个命名空间中，一组特定的公钥被认为是权威的。\n由于不同的信任域有不同的签发机构，一个信任域的破坏不会危及另一个信任域。这是一个重要的属性，使互不信任的各方之间能够进行安全通信，例如，在 staging 和生产之间或一个公司和另一个公司之间。\n跨越多个信任域验证 SPIFFE 身份的能力被称为 SPIFFE Federation，在本章后面介绍。\nSPIFFE 可验证身份文件（SVID） SPIFFE 可验证身份文件（SVID）是一个可加密验证的身份文件，用于向对等体证明一个服务的身份。SVID 包括一个单一的 SPIFFE ID，并由代表服务所在的信任域的签发机构签署。\n与其发明一种新的文件类型让软件支持，SPIFFE 选择利用那些已经被广泛使用并被充分理解的文件类型。在撰写本报告时，有两种身份文件被定义为 SPIFFE 规范中的 SVID 使用：X.509 和 JWT。\nX509-SVID\nX509-SVID 将 SPIFFE 身份编码为标准 X.509 证书 。相应的 SPIFFE ID 被设置为主题替代名称（SAN）扩展字段中的 URI 类型。虽然 X509-SVID 上只允许设置一个 URI SAN 字段，但证书可以包含任何数量的其他类型的 SAN 字段，包括 DNS SAN。\n建议尽可能使用 X509-SVID，因为它们比 JWT-SVID 有更好的安全属性。具体来说，当与 TLS 结合使用时，X.509 证书不能被中间人记录和重放。\n利用 X509-SVID 可能有额外的要求，请参考 X509-SVID 规范部分 。\nJWT-SVID\nJWT-SVID 将 SPIFFE 身份编码为一个标准的 JWT —— 特别是一个 JWS 。JWT-SVID 被用作承载令牌，在应用层向对等者证明身份。与 X509-SVID 不同，JWT-SVID 受到一类被称为 重放攻击 的威胁，即令牌被未经授权的一方获得并重新使用。\nSPIFFE 规定了三种机制来缓解这种攻击媒介。首先，JWT-SVID 必须只通过安全通道传输。其次，受众声明（ aud 声明）必须被设置为与令牌的目的方严格匹配的字符串。最后，所有的 JWT-SVID 必须包括一个过期时间，限制被盗令牌的有效期限。\n尤其需要注意的是，尽管有这些缓解措施，JWT-SVID 从根本上说仍然容易受到重放攻击，因此应该谨慎使用并小心处理。也就是说，它们是 SPIFFE 规范集的一个重要部分，因为它们允许 SPIFFE 认证在不可能建立端到端通信渠道的情况下发挥作用。\n利用 JWT-SVID 可能有额外的要求，请参考 JWT-SVID 规范部分 。\nSPIFFE 信任包 SPIFFE 信任包是一个包含信任域公钥的文件。每种 SVID 类型都有一个特定的方式在这个包中表示出来（例如，对于 X509-SVID，包括代表公钥的 CA 证书）。每个 SPIFFE 信任域都有一个与之相关的捆绑包，该捆绑包中的材料被用来验证声称位于该信任域中的 SVID。\n由于信任包不包含任何秘密（只有公钥），它可以安全地与公众分享。尽管这一事实，它确实需要安全地分发，以保护其内容不被擅自修改。换句话说，保密性是不需要的，但完整性是需要的。\nSPIFFE 捆绑包的格式是 JWK Set（或 JWKS 文档），与现有的认证技术如 OpenID Connect 兼容。JWKS 是一种灵活且被广泛采用的格式，用于表示各种类型的加密密钥和文件，在新的 SVID 格式被定义的情况下，它提供了一些未来证明。\nSPIFFE Federation 通常，允许在不同信任域的服务之间进行安全通信是可取的。在许多情况下，你不能把所有的服务放在一个信任域中。一个常见的例子是两个不同的组织需要相互通信。另一个例子可能是一个组织需要建立安全边界，也许是在信任度较低的云环境和高度信任的内部服务之间。\n为了能够实现这一点，每个服务必须拥有远程服务所来自的外部信任域的捆绑包（Bundle）。因此，SPIFFE 信任域必须公开或以其他方式分享其捆绑包内容，使外部信任域中的服务能够验证来自本地信任域的身份。用于共享信任域的捆绑内容的机制被称为捆绑端点（Bundle Endpoint）。\n捆绑端点是简单的 TLS 保护的 HTTP 服务。希望与外部信任域联合的运营商必须用外部信任域的名称和捆绑端点的 URL 来配置他们的 SPIFFE 实现，允许定期获取捆绑包的内容。\nSPIFFE Workload API SPIFFE Workload API 是一个本地的、非网络化的 API，工作负载用它来获取当前的身份文件、信任捆绑和相关信息。重要的是，这个 API 是未经认证的，不要求工作负载拥有任何预先存在的证书。将这一功能作为本地 API 提供，允许 SPIFFE 实现者提出创造性的解决方案，在不需要直接认证的情况下识别调用者（例如，利用操作系统提供的功能）。Workload API 以 gRPC 服务器的形式公开，并使用双向流，允许根据需要将更新推入工作负载。\nWorkload API 不要求调用的工作负载对自己的身份有任何了解，也不要求调用 API 时拥有任何凭证。这就避免了在工作负载旁边部署任何认证秘密的需要。\nSPIFFE 工作负载 API 向工作负载提供 SVID 和信任包，并在必要时对其进行轮换。\n什么是 SPIRE？ SPIFFE 运行环境（SPIRE）是 SPIFFE 规范中所有五个部分的一个生产可用的开源实现。\nSPIRE 项目（以及 SPIFFE）由云原生计算基金会主办，该基金会由许多领先的基础设施技术公司成立，为有利于云原生社区的开源项目提供一个中立的家园。\nSPIRE 有两个主要组成部分：服务器和代理。服务器负责验证代理和构建 SVID，而代理则负责为 SPIFFE Workload API 提供服务。这两个组件都是使用面向插件的架构编写的，因此它们可以很容易地被扩展，以适应大量不同的配置和平台。\nSPIRE 架构 SPIRE 的架构由两个关键组件组成，即 SPIRE 服务器和 SPIRE 代理。\nSPIRE 服务器\nSPIRE 服务器管理和发布 SPIFFE 信任域中的所有身份。它使用一个数据存储来保存关于其代理和工作负载等的信息。SPIRE 服务器通过使用注册条目获知其管理的工作负载，注册条目是为节点和工作负载分配 SPIFFE ID 的灵活规则。\n该服务器可以通过 API 或命令行命令进行管理。需要注意的是，由于服务器掌握着 SVID 的签名密钥，它被认为是一个重要的安全组件。在决定它的位置时应特别考虑。这一点将在本书后面讨论。\n数据存储\nSPIRE 服务器使用一个数据存储来跟踪其当前的注册条目，以及它所发布的 SVID 的状态。目前，支持几种不同的 SQL 数据库。SPIRE 内置 SQLite，这是一个内存中的嵌入式数据库，用于开发和测试目的。\n上游机构\n一个信任域中的所有 SVID 都由 SPIRE 服务器签署。默认情况下，SPIRE 服务器会生成一个自签名证书（用自己随机生成的私钥签名的证书）来签署 SVID，除非配置了一个 上游证书机构（Upstream Certificate Authority） 插件接口。上游证书授权的插件接口允许 SPIRE 从另一个证书授权机构获得其签名证书。\n在许多简单的情况下，使用自签名的证书就可以了。然而，对于较大的安装，可能需要利用预先存在的证书颁发机构和 X.509 证书的分层性质，使多个 SPIRE 服务器（和其他生成 X.509 证书的软件）一起工作。\n在一些组织中，上游证书颁发机构可能是一个中央证书颁发机构，你的组织在其他方面使用它。如果你有许多不同种类的证书在使用，而且你希望它们在你的基础设施中都被信任，那么这就很有用。\nSPIRE 代理\nSPIRE 代理只有一个功能，尽管是一个非常重要的功能：为 Workload API 服务。在完成这一壮举的过程中，它解决了一些相关的问题，如确定工作负载的身份，调用它，并安全地将自己介绍给 SPIRE 服务器。在这种安排中，它是执行所有重任的代理。\n代理不需要像 SPIRE 服务器那样的主动管理。虽然它们确实需要一个配置文件，但 SPIRE …","relpermalink":"/book/spiffe/introduction-to-spiffe-and-spire-concepts/","summary":"在第三章介绍的概念基础上，本章说明了 SPIFFE 标准。解释 SPIRE 实现的组成部分以及它们是如何结合在一起的。最后，讨论威胁模型以及如果特定组件被破坏会发生什么。","title":"SPIFFE 和 SPIRE 概念介绍"},{"content":"本章大纲 介绍\n数据包流程\neBPF Map\nIptables 用法\n阅读本章 ","relpermalink":"/book/cilium-handbook/ebpf/","summary":"本章大纲 介绍 数据包流程 eBPF Map Iptables 用法 阅读本章","title":"eBPF 数据路径"},{"content":"本节介绍 Kubernetes 的网络策略方面。\n命名空间 命名空间 用于在 Kubernetes 中创建虚拟集群。包括 NetworkPolicy 和 CiliumNetworkPolicy 在内的所有 Kubernetes 对象都属于一个特定的命名空间。根据定义和创建策略的方式，会自动考虑 Kubernetes 命名空间：\n作为 CiliumNetworkPolicy CRD 和 NetworkPolicy 创建和导入的网络策略适用于命名空间内，即该策略仅适用于该命名空间内的 pod。但是，可以授予对其他命名空间中的 pod 的访问权限，如下所述。 通过 API 参考 直接导入的网络策略适用于所有命名空间，除非如下所述指定命名空间选择器。 提示 虽然有意支持通过 fromEndpoints 和 toEndpoints 中的 k8s:io.kubernetes.pod.namespace 标签指定命名空间。禁止在 endpointSelector 中指定命名空间，因为这将违反 Kubernetes 的命名空间隔离原则。endpointSelector 总是适用于与 CiliumNetworkPolicy 资源本身相关的命名空间的 pod。 示例：强制命名空间边界 此示例演示如何为命名空间强制实施基于 Kubernetes 命名空间的边界，ns1 和 ns2 通过在任一命名空间的所有 pod 上启用默认拒绝，然后允许来自同一命名空间内的所有 pod 的通信。\napiVersion: \u0026#34;cilium.io/v2\u0026#34; kind: CiliumNetworkPolicy metadata: name: \u0026#34;isolate-ns1\u0026#34; namespace: ns1 spec: endpointSelector: matchLabels: {} ingress: - fromEndpoints: - matchLabels: {} --- apiVersion: \u0026#34;cilium.io/v2\u0026#34; kind: CiliumNetworkPolicy metadata: name: \u0026#34;isolate-ns1\u0026#34; namespace: ns2 spec: endpointSelector: matchLabels: {} ingress: - fromEndpoints: - matchLabels: {} 示例：跨命名空间公开 pod 以下示例将所有 ns1 命名空间所有具有 name=leia 标签的 pod 暴露给 ns2 命名空间具有 name=luke 标签的 pod。\n请参阅示例 YAML 文件 以获取完整的功能示例，包括部署到不同命名空间的 pod。\napiVersion: \u0026#34;cilium.io/v2\u0026#34; kind: CiliumNetworkPolicy metadata: name: \u0026#34;k8s-expose-across-namespace\u0026#34; namespace: ns1 spec: endpointSelector: matchLabels: name: leia ingress: - fromEndpoints: - matchLabels: k8s:io.kubernetes.pod.namespace: ns2 name: luke 示例：允许 egress 到 kube-system 命名空间中的 kube-dns 以下示例允许 public 创建策略的命名空间中的所有 pod 与 kube-system 空间中 53/UDP 端口上的 kube-dns 通信。\napiVersion: \u0026#34;cilium.io/v2\u0026#34; kind: CiliumNetworkPolicy metadata: name: \u0026#34;allow-to-kubedns\u0026#34; namespace: public spec: endpointSelector: {} egress: - toEndpoints: - matchLabels: k8s:io.kubernetes.pod.namespace: kube-system k8s-app: kube-dns toPorts: - ports: - port: \u0026#39;53\u0026#39; protocol: UDP 服务账户 Kubernetes 服务账户 用于将身份与 Kubernetes 管理的 pod 或进程相关联，并授予身份对 Kubernetes 资源和机密的访问权限。Cilium 支持基于 Pod 的服务账户身份来规范网络安全策略。\nPod 的服务账户可以通过服务账户准入控制器 定义，也可以直接在 Pod、Deployment、ReplicationController 资源中指定，如下所示：\napiVersion: v1 kind: Pod metadata: name: my-pod spec: serviceAccountName: leia ... 例子 以下示例授予在“luke”服务账户下运行的任何 pod 向与“leia”服务账户关联的所有运行的 pod 发出 TCP 80 端口 80 上的 HTTP GET /public 请求。\n请参阅示例 YAML 文件 以获取完整的功能示例，包括部署和服务账户资源。\napiVersion: \u0026#34;cilium.io/v2\u0026#34; kind: CiliumNetworkPolicy metadata: name: \u0026#34;k8s-svc-account\u0026#34; spec: endpointSelector: matchLabels: io.cilium.k8s.policy.serviceaccount: leia ingress: - fromEndpoints: - matchLabels: io.cilium.k8s.policy.serviceaccount: luke toPorts: - ports: - port: \u0026#39;80\u0026#39; protocol: TCP rules: http: - method: GET path: \u0026#34;/public$\u0026#34; 多集群 当使用集群网格操作多个集群时，集群名称通过标签公开 io.cilium.k8s.policy.cluster，可用于将策略限制到特定集群。\napiVersion: \u0026#34;cilium.io/v2\u0026#34; kind: CiliumNetworkPolicy metadata: name: \u0026#34;allow-cross-cluster\u0026#34; description: \u0026#34;Allow x-wing in cluster1 to contact rebel-base in cluster2\u0026#34; spec: endpointSelector: matchLabels: name: x-wing io.cilium.k8s.policy.cluster: cluster1 egress: - toEndpoints: - matchLabels: name: rebel-base io.kubernetes.pod.namespace: default io.cilium.k8s.policy.cluster: cluster2 注意策略规则中的 io.kubernetes.pod.namespace: default。它确保策略适用于 cluster2 默认命名空间中的 rebel-base，而不考虑 x-wing 部署在 cluster1 中的命名空间。如果政策规则的命名空间标签被省略，它默认为策略本身应用的相同命名空间，这可能不是部署跨集群策略时想要的。\n集群范围的策略 CiliumNetworkPolicy 只允许绑定限制到特定命名空间的策略。在某些情况下，可以使用 Cilium 的 CiliumClusterwideNetworkPolicy Kubernetes 自定义资源来实现集群范围内的策略效果。该策略的规范与CiliumNetworkPolicy 的规范相同，只是它没有命名空间。\n在集群中，这个策略将允许从任何命名空间中匹配标签 name=luke 的 pod 到任何命名空间中匹配标签 name=leia 的 pod 的入站流量。\napiVersion: \u0026#34;cilium.io/v2\u0026#34; kind: CiliumClusterwideNetworkPolicy metadata: name: \u0026#34;clusterwide-policy-example\u0026#34; spec: description: \u0026#34;Policy for selective ingress allow to a pod from only a pod with given label\u0026#34; endpointSelector: matchLabels: name: leia ingress: - fromEndpoints: - matchLabels: name: luke 示例：允许所有流量进入 kube-dns 以下示例允许集群中的所有 Cilium 托管端点与 kube-system 命名空间中 53/UDP 端口上的 kube-dns 通信。\napiVersion: \u0026#34;cilium.io/v2\u0026#34; kind: CiliumClusterwideNetworkPolicy metadata: name: \u0026#34;wildcard-from-endpoints\u0026#34; spec: description: \u0026#34;Policy for ingress allow to kube-dns from all Cilium managed endpoints in the cluster\u0026#34; endpointSelector: matchLabels: k8s:io.kubernetes.pod.namespace: kube-system k8s-app: kube-dns ingress: - fromEndpoints: - {} toPorts: - ports: - port: \u0026#34;53\u0026#34; protocol: UDP 示例：添加健康端点 以下示例将健康实体添加到所有 Cilium 托管端点，以检查集群连接健康状况。\napiVersion: \u0026#34;cilium.io/v2\u0026#34; kind: CiliumClusterwideNetworkPolicy metadata: name: \u0026#34;cilium-health-checks\u0026#34; spec: endpointSelector: matchLabels: \u0026#39;reserved:health\u0026#39;: \u0026#39;\u0026#39; ingress: - fromEntities: - remote-node egress: - toEntities: - remote-node 下一章 ","relpermalink":"/book/cilium-handbook/policy/kubernetes/","summary":"本节介绍 Kubernetes 的网络策略方面。 命名空间 命名空间 用于在 Kubernetes 中创建虚拟集群。包括 NetworkPolicy 和 CiliumNetworkPolicy 在内的所有 Kubernetes 对象都属于一个特定的命名空间。根据定义和创建策略的方式，会自动考虑 Kubernetes 命名空间： 作为 CiliumNetworkPolicy CRD 和 NetworkPolicy 创建和导入的网络策略适","title":"Kubernetes 网络策略"},{"content":"如果你在 Kubernetes 上运行 Cilium，你可以从 Kubernetes 为你分发的策略中受益。在这种模式下，Kubernetes 负责在所有节点上分发策略，Cilium 会自动应用这些策略。三种格式可用于使用 Kubernetes 本地配置网络策略：\n在撰写本文时，标准的 NetworkPolicy 资源支持指定三层/四层入口策略，并带有标记为 beta 的有限出口支持。 扩展的 CiliumNetworkPolicy 格式可用作 CustomResourceDefinition 支持入口和出口的三层到七层层策略规范。 CiliumClusterwideNetworkPolicy 格式，它 是集群范围的CustomResourceDefinition ，用于指定由 Cilium 强制执行的集群范围的策略。规范与 CiliumNetworkPolicy 相同，没有指定命名空间。 Cilium 支持同时运行多个策略类型。但是，在同时使用多种策略类型时应谨慎，因为理解跨多种策略类型的完整允许流量集可能会令人困惑。如果不注意，这可能会导致意外的策略允许行为。\n网络策略 有关详细信息，请参阅官方 NetworkPolicy 文档 。\nKubernetes 网络策略的已知缺失功能：\n特征 GItHub Issue ipBlock使用 pod IP 设置 GitHub Issue 9209 SCTP GitHub Issue 5719 Cilium 网络策略 CiliumNetworkPolicy 与 标准 NetworkPolicy 非常相似。目的是提供 NetworkPolicy 尚不支持的功能。理想情况下，所有功能都将合并到标准资源格式中，并且不再需要此 CRD。\nGo 中资源的原始规范如下所示：\ntype CiliumNetworkPolicy struct { // +deepequal-gen=false metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` // +deepequal-gen=false metav1.ObjectMeta `json:\u0026#34;metadata\u0026#34;` // Spec is the desired Cilium specific rule specification. Spec *api.Rule `json:\u0026#34;spec,omitempty\u0026#34;` // Specs is a list of desired Cilium specific rule specification. Specs api.Rules `json:\u0026#34;specs,omitempty\u0026#34;` // Status is the status of the Cilium policy rule // // +deepequal-gen=false // +kubebuilder:validation:Optional Status CiliumNetworkPolicyStatus `json:\u0026#34;status\u0026#34;` } metadata\n描述策略。这包括：策略的名称，在命名空间中是唯一的注入策略的命名空间一组标签来识别 Kubernetes 中的资源。\nspec\n包含规则基础的字段 。\nSpecs\n包含Rule Basics 列表的字段。如果必须自动删除或添加多个规则，则此字段很有用。\nStatus\n提供有关策略是否已成功应用的可视性。\n例子 有关示例策略的详细列表，请参阅 三层示例 、四层示例 和 七层 示例。\nCiliumClusterwideNetworkPolicy 类似于 CiliumNetworkPolicy ，除了\nCiliumClusterwideNetworkPolicy 定义的策略是非命名空间和集群范围的 它允许使用节点选择器 。在内部，该策略与 CiliumNetworkPolicy 相同，因此该策略规范的效果也相同。 go 中资源的原始规范如下所示：\ntype CiliumClusterwideNetworkPolicy struct { // Spec is the desired Cilium specific rule specification. Spec *api.Rule // Specs is a list of desired Cilium specific rule specification. Specs api.Rules // Status is the status of the Cilium policy rule. // // The reason this field exists in this structure is due a bug in the k8s // code-generator that doesn\u0026#39;t create a `UpdateStatus` method because the // field does not exist in the structure. // // +kubebuilder:validation:Optional Status CiliumNetworkPolicyStatus } ","relpermalink":"/book/cilium-handbook/kubernetes/policy/","summary":"如果你在 Kubernetes 上运行 Cilium，你可以从 Kubernetes 为你分发的策略中受益。在这种模式下，Kubernetes 负责在所有节点上分发策略，Cilium 会自动应用这些策略。三种格式可用于使用 Kubernetes 本地配置网络策略： 在撰写本","title":"网络策略"},{"content":"近年来，云原生应用已呈指数级增长。在本章中，我将讨论为什么 eBPF 如此适合于云原生环境。为了更具象化，我将提到 Kubernetes，但同样适用于任何容器平台。\n每台主机一个内核 要理解为什么 eBPF 在云原生世界中如此强大，你需要搞清楚一个概念：每台机器（或虚拟机）只有一个内核，所有运行在该机器上的容器都共享同一个内核 1 如 图 5-1 所示，内核了解主机上运行的所有应用代码。\n图 5-1. 同一主机上的所有容器共享一个内核 通过对内核的检测，就像我们在使用 eBPF 时做的那样，我们可以同时检测在该机器上运行的所有应用程序代码。当我们将 eBPF 程序加载到内核并将其附加到事件上时，它就会被触发，而不考虑哪个进程与该事件有关。\neBPF 与 sidecar 模式的比较 在 eBPF 之前，Kubernetes 的可观测性和安全工具大多都采用了 sidecar 模式。这种模式允许你在与应用程序相同的 pod 中，单独部署一个工具容器。这种模式的发明是一个进步，因为这意味着不再需要直接在应用程序中编写工具代码。仅仅通过部署 sidecar，工具就获得了同一 pod 中的其他容器的可视性。注入 sidecar 的过程通常是自动化的，所以这提供了一种机制，以确保你的所有应用程序都被仪器化。\n每个 sidecar 容器都会消耗资源，而这要乘以注入了 sidecar 的 pod 的数量。这可能是非常重要的 —— 例如，如果每个 sidecar 需要它自己的路由信息副本，或策略规则，这就是浪费（关于这一点，Thomas Graf 写了一篇 关于服务网格 sidecar 与 eBPF 的比较 ）。\nSidecar 的另一个问题是，你不能保证机器上的每一个应用程序都被正确检测。设想下有一个攻击者设法破坏了你的一台主机，并启动了一个单独的 pod 来运行，比如，加密货币挖矿程序。他们不可能对你有礼貌，用你的 sidecar 可观测或安全工具来检测他们的挖矿 pod。你需要一个单独的系统来了解这种活动。\n但同样的加密货币矿工与运行在该主机上的合法 pod 共享内核。如果你使用基于 eBPF 的工具，如 图 5-2 所示，矿工会自动受到它的影响。\n图 5-2. 旁观者只能观测到他们自己 pod 的活动，但 eBPF 程序可以观测到所有活动 eBPF 和进程隔离 我主张将功能整合到一个单一的、基于 eBPF 的代理中，而不是每个 pod 的 sidecar 中。如果该代理可以访问机器上运行的所有 pod，这不是一种安全风险吗？我们不是失去了应用程序之间的隔离，而这种隔离可以防止它们相互干扰吗？\n作为一个容器安全领域的过来人，我可以体会到你对此的担忧，但重要的是要挖掘底层机制，以真正理解为什么它不是一开始可能出现的缺陷。\n请注意，这些 pod 共享同一个内核，而内核原生不能感知 pod 或容器。相反，内核对进程进行操作，并使用 cgroup 和 namespace 来隔离进程。这些结构由内核监管，以隔离用户空间中的进程，防止它们互相干扰。只要数据在内核中处理（例如，从磁盘中读取或发送到网络中），你就依赖于内核的正确行为。只有内核代码控制文件权限。没有其他层面的东西可以阻止内核忽略文件权限的东西，内核可以从任何文件中读取数据 —— 只是内核本身不会这样做。\n存在于 Linux 系统中的安全控制措施假定内核本身是可以信任的。它们的存在是为了防止在用户空间运行的代码产生不良行为。\n我们在 第二章 中看到，eBPF 检查器确保 eBPF 程序只能访问它有权限的内存。检查器检查程序时不可能超出其职权范围，包括确保内存为当前进程所拥有或为当前网络包的一部分。这意味着 eBPF 代码比它周围的内核代码受到更严格的控制，内核代码不需要通过任何类型的检查器。\n如果攻击者逃脱了容器化的应用程序而到了节点上，而且还能够提升权限，那么该攻击者就可以危害到同一节点上的其他应用程序。由于这些逃逸是未知的，作为一个容器安全专家，我不建议在没有额外安全工具的情况下，在共享机器上与不受信任的应用程序或用户一起运行敏感的应用程序。对于高度敏感的数据，你甚至可能不希望在虚拟机中与不受信任的用户在同一裸机上运行。但是，如果你准备在同一台虚拟机上并行运行应用程序（这在许多不是特别敏感的应用程序中是完全合理的），那么 eBPF 就不会在共享内核已经存在的风险之上增加额外的风险。\n当然，恶意的 eBPF 程序可能造成各种破坏，当然也很容易写出劣迹的 eBPF 代码 —— 例如，复制每个网络数据包并将其发送给窃听者。默认情况下，非 root 用户没有加载 eBPF 程序 2 的权限，只有当你真正信任他们时，你才应该授予用户或软件系统这种权限，就像 root 权限一样。因此，必须小心你所运行的代码的出处（有一个倡议正在进行中，以支持 eBPF 程序的签名检查来帮助解决这个问题）。你也可以使用 eBPF 程序来监视其他的 eBPF 程序！现在你已经对为什么 eBPF 是云原生工具的强大基础有了一个概念，下一章给你举一些来自云原生生态系统中的 eBPF 工具的具体例子。\n参考 这基本正确，除非你使用的是虚拟化技术，像 KataContainer、Firecracker 或 unikernels 这样的方法，每个 “容器” 在自己的虚拟机中运行。 ↩︎\nLinux CAP_BPF 授予加载 BPF 程序的权限。 ↩︎\n","relpermalink":"/book/what-is-ebpf/ebpf-in-cloud-native-environments/","summary":"近年来，云原生应用已呈指数级增长。在本章中，我将讨论为什么 eBPF 如此适合于云原生环境。为了更具象化，我将提到 Kubernetes，但同样适用于任何容器平台。 每台主机一个内核 要理解为什么 eBPF 在云原生世界中如此","title":"第五章：云原生环境中的 eBPF"},{"content":"在联邦政府的各个机构中，有几个 DevSecOps 倡议，其重点和焦点各不相同，这取决于软件和任务需求所带来的流程。尽管并不详尽，但以下是对 这些倡议 的简要概述：\nDevSecOps 管道参与构建、签入和签出一个名为 Iron Bank 的容器镜像仓库，这是一个经过国防部审查的强化容器镜像库。 空军的 Platform One，也就是实现了连续操作授权（C-ATO）概念的 DevSecOps 平台，这又简化了国防部的授权程序，以适应现代连续软件部署的速度和频率。 国家地理空间情报局（NGA）在 \u0026#34;NGA 软件之路\u0026#34; 中概述了其 DevSecOps 战略，其中为其每个软件产品规定了三个关键指标：可用性、准备时间和部署频率，以及用于实现 DevSecOps 管道的七个不同产品系列的规格，包括消息传递和工作流工具。 医疗保险和医疗补助服务中心（CMS）正在采用一种 DevSecOps 方法，其中一个重点是为软件材料清单（SBOM）奠定基 —— 这是一种正式记录，包含用于构建软件的各种组件的细节和供应链关系。制作 SBOM 的目的是为了实现持续诊断和缓解（CDM）计划下的目标。 在海军水面作战中心（NSWC），DevSecOps 原语的实施方法被用来教导和培训软件工作人员，让他们了解各种软件指标以及自动化作为实现这些指标的助推器的作用。 陆军的 DevSecOps 倡议被称为 “陆军软件工厂”，重点是建立技能组合而不是建立软件。它利用 DevSecOps 能力（管道和平台即服务功能）作为技术加速器，在产品管理、用户体验、用户界面（UI/UX）设计、平台和软件工程方面提高效率和熟练度。 ","relpermalink":"/book/service-mesh-devsecops/intro/related-devsecops-initiatives/","summary":"在联邦政府的各个机构中，有几个 DevSecOps 倡议，其重点和焦点各不相同，这取决于软件和任务需求所带来的流程。尽管并不详尽，但以下是对 这些倡议 的简要概述： DevSecOps 管道参与构建、签入和签出一个名为 Iron Bank 的容器镜像仓库，这是一","title":"1.2 相关的 DevSecOps 倡议"},{"content":"可观测性即代码在应用程序的每个服务组件中部署一个监控代理，以收集三种类型的数据（在第 4.1 节中描述），将它们发送到专门的工具，将它们关联起来，进行分析，并在仪表板上显示分析后的综合数据，以呈现整个应用程序级别的情况。这种综合数据的一个例子是日志模式，它提供了一个日志数据的视图，该视图是在使用一些标准（例如，一个服务或一个事件）对日志数据进行过滤后呈现的。数据根据共同的模式（例如，基于时间戳或 IP 地址范围）被分组，以方便解释。不寻常的发生被识别出来，然后这些发现可以被用来指导和加速 进一步的调查 。\n","relpermalink":"/book/service-mesh-devsecops/implement/ci-cd-pipeline-for-observability-as-code/","summary":"可观测性即代码在应用程序的每个服务组件中部署一个监控代理，以收集三种类型的数据（在第 4.1 节中描述），将它们发送到专门的工具，将它们关联起来，进行分析，并在仪表板上显示分析后的综合数据，以呈现整个应用程序","title":"4.5 可观测性即代码的 CI/CD 管道"},{"content":"我们在前一章中讨论了在采用云原生基础架构的前提。在部署之前，需要有由 API 驱动的基础架构（IaaS）供给。\n在本章中，我们将探讨云原生基础架构拓扑的概念，并在云中实现它们。我们将学习可以帮助运维人员控制其基础架构的常用工具和模式。\n部署基础架构的第一步应该是能够将其表述出来。传统上，可以在白板上处理，或者如果幸运的话，可以在公司 wiki 上存储的文档中处理。今天，一切都变得更加程序化，基础架构表述通常以便于应用程序解释的方式记录。无论如何表述，全面的表述基础架构的需求是不变的。\n正如人们所期望的那样，精巧的云基础架构可以从简单的设计到非常复杂的设计。无论复杂性如何，必须对基础架构的表现给予高度的重视，以确保设计的可重复性。能够清晰地传递想法更为重要。因此，明确、准确和易于理解的基础架构级资源表述势在必行。\n我们也将从制作精良的表述中获得很多好处：\n随着时间的推移，基础架构设计可以共享和版本化。 基础架构设计可以被 fork 和修改以适应特殊情况。 表述隐含的是文档。 随着本章向前推进，我们将看到基础架构表述是如何成为基础架构部署的第一步。我们将以不同的方式探索表述基础架构的能力和潜在缺陷。\n表述基础架构 首先，我们需要理解表述基础架构的两个角色：作者和观众。\n作者将定义基础架构，通常是人类运维人员或管理员。观众将负责解释基础架构表述。有时候，这是一个运维人员执行手动步骤，但希望它是一个可以自动分析和创建基础架构的部署工具。作者在准确表达基础架构方面表现得越好，我们就可以在听众解释表达的能力中获得更多的信心。\n创作基础架构表述时主要关心的是要让观众了解它。如果目标受众是人，则表述可能以技术图或抽象代码的形式出现。如果目标受众是一个程序，那么表示可能需要更详细的信息和具体的实施步骤。\n尽管有观众，作者应该让观众更容易使用。随着复杂性的增加以及人与程序共同使用基础架构，这将变得非常困难。\n表示法需要易于理解，以便能够对其进行准确分析。易于阅读但分析不准确的表述否定了整个工作。观众应该总是努力去解释他们的表述，而不是做出假设。\n为了使表达成功，解释需要可预测。如果作者忽略了一个重要的细节，那么最好的观众就会很快失败。具有可预测性将在应用变更时减少错误的发生，并有助于在作者和受众之间建立信任。\n基础架构即图 我们用到了白板，开始绘制一张基础架构图。通常情况下，这个过程始于在角落上代表互联网的云形状，以及一些指向方框的箭头。每个框代表系统中的一个组件，箭头表示它们之间的交互。图 3-1 是基础架构图的一个例子。\n图 3-1. 简单的基础架构图 这是一个非常有效的头脑风暴和将想法传达给其他人的方法。它允许对复杂的基础架构设计进行快速而强大的表示。\n图片适用于人类，大量人群和 CEO。这些图也适用，因为它们使用常用语来表示关系。例如，此框可能会将数据发送到那个框，但不会将数据发送到其他框。\n不幸的是，图表对于计算机来说几乎是不可能理解的。在计算机视觉迎头赶上之前，基础架构图仍然是一个代表，可以用眼球来解释，而不是代码。\n从图中进行部署\n在例 3-1 中，我们看一个来自 bash_history 文件的熟悉的代码片段。它代表一个基础架构运营商，作为描述基础服务器与网络、存储和转租服务运行的图表的受众。\n运维人员已经手动部署了一台新的虚拟机，并通过 SSH 连接到了该机器并开始配置它。在这种情况下，人类充当图解释者，然后在基础架构环境中采取行动。\n大多数基础架构工程师在他们的职业生涯中都这样做了，而且这些步骤对于某些系统管理员来说应该是非常熟悉的。\n例 3-1. bash_history\nsudo emacs /etc/networking/interfaces sudo ifdown eth0 sudo ifup eth0 sudo fdisk -l sudo emacs /etc/fstab sudo mount -a sudo systemctl enable kubelet 基础架构即脚本 如果您是一个系统管理员，您工作的一部分是在复杂系统中进行更改；确保这些更改是正确的也是您的责任。需要将这些变化传播到广阔的系统中是非常现实的。不幸的是，人为错误也是如此。管理员为这项工作编写便利脚本并不奇怪。\n脚本可以帮助减少重复任务中人为错误的数量，但自动化是一把双刃剑。这并不意味着准确性或成功。\n对于 SRE，自动化可以让你力量倍增，单它不是万能药。当然，加倍的力量并不会自然地改变应用力的准确性：不经意地进行自动化可能会产生很多的问题。\n——Niall Murphy、John Looney 和 Kacirek，自动化在谷歌的演变\n编写脚本是自动执行步骤以产生所需结果的好方法。该脚本可以执行各种任务，例如安装 HTTP 服务器，配置并运行它。但是，脚本中的步骤在调用时很少考虑到它们的结果或系统的状态。\n在这种情况下，脚本是编码数据，表示创建所需基础架构应该发生的情况。另一位运维人员或管理员可以评估您的脚本，并希望了解脚本正在做什么。换句话说，他们会解释你的基础架构表示。了解所需的基础架构需要了解步骤如何影响系统。\n脚本的运行时会按照它们定义的顺序执行这些步骤，但运行时不知道它正在生成什么。脚本是代码，脚本的执行结果希望是所需的基础架构。\n这适用于普遍的场景，但这种方法存在一些缺陷。最明显的缺陷是运行相同的脚本可能获得两个不同的结果。\n如果脚本第一次运行的环境与第二次运行的环境大不相同？从科学的角度来说，这将类似于程序中的缺陷，并会使实验数据无效。\n使用脚本来表示基础架构的另一个缺陷是缺少声明状态。脚本的运行时不理解结束状态，因为它只提供了执行步骤。人类需要从步骤中解释理想的结果，以了解如何进行改变。\n我们看到过很多人类难以理解的代码。随着配置脚本复杂性的增长，我们解释脚本的能力就会减弱。此外，您的基础架构页需要随时间而变化，脚本将不可避免地需要更改。\n如果不将步骤抽象为声明性状态，为了给每个可能的初始状态创建过程，脚本将不断增长。这包括抽象出操作系统（例如 apt 和 DNF）之间的步骤和差异，以及验证可以安全地跳过哪些步骤。\n基础架构即代码带来了一些工具，这些工具提供了一些抽象，以帮助减轻使用脚本管理基础架构的负担。\n从脚本部署\n创建基础架构的下一个发展是开始采用先前手动管理基础架构的流程，并通过将工作封装在脚本中来简化它。想象一下，我们有一个名为 createVm.sh 的 bash 脚本，它将在我们的本地工作站中创建一台虚拟机。\n该脚本需要两个参数。第一个是分配给虚拟机上的网络接口的静态 IP 地址。第二个是以千兆字节为单位的大小，用于创建卷并将其挂载到虚拟机。\n示例 3-2 将基础架构的基本表示形式显示为脚本。该脚本将提供新的基础架构，并在新创建的基础架构上运行任意配置脚本。该脚本可能演变为高度可定制的，并且可能是（危险地）自动化的，只需点击一下按钮即可运行。\n例 3-2. 基础架构即脚本\n#!/bin/bash # Create a VM with a NIC on 10.0.0.17 and a 100gb volume createVm.sh 10.0.0.17 100 # Transfer the bootstrapping script scp ~/vm_provision.sh user@10.0.0.17:vm_provision.sh -v # Run the bootstrap script ssh user@10.0.0.17 sh ~/vm_provision.sh 基础架构即代码 配置管理曾经是代表基础架构的主要角色。我们可以将配置管理视为抽象脚本，自动考虑初始状态以执行正确的过程。最重要的是，配置管理允许作者声明节点的期望状态，而不是实现它所需的每一步。\n配置管理是基础架构即代码的第一步，但相关工具很少超出单个服务器的范围。配置管理工具在定义特定资源和他们的状态方面做得非常出色，但由于基础架构需要资源之间的协调，所以出现了复杂性。\n例如，服务的 DNS 条目在提供服务之前不可用。在主机可用之前不应该提供该服务。如果不能在独立节点之间协调多个资源，则配置管理提供的抽象化是不足的。有些工具增加了协调资源之间配置的能力，但协调通常是程序性的，责任落到了人们的协调资源和理解所需状态上。\n您的基础架构不包含没有通信的独立实体。代表基础架构的工具需要考虑到这一点。因此，需要另一种表示来管理低级别抽象（例如操作系统）以及供应和协调。\n2014 年 7 月，有个开源工具在代码发布的时候采用了更高级别的基础架构抽象概念。这个名为 Terraform 的工具非常成功。它在配置管理完善并且公有云的采用呈上升趋势的时间节点发布。用户看到了新环境中工具的局限性，Terraform 很好的满足了他们的需求。\n在 2011 年时，我们最初将基础架构视代码。我们注意到我们正在编写工具来解决许多项目的基础架构问题，并希望将流程标准化。\n——Hashicorp 首席执行官兼 Terraform 创始人 Mitchell Hashimoto\nTerraform 使用专门的领域特定语言（DSL）表示基础架构，它在人类可理解的图像和机器可分析的代码之间做了良好的折衷。Terraform 最成功的部分是抽象的基础架构视图，资源协调以及应用时利用现有工具的能力。Terraform 与云 API 进行通信以配置基础架构，并可在必要时使用配置管理来配置节点。\n这是该行业的根本性转变，因为我们看到一次性配置脚本正在消失。越来越多的运营商开始在新的 DSL 中开发基础架构表示。过去在基础架构上手动操作的工程师现在正在开发代码。\n新的 DSL 解决了将基础架构表示为脚本的问题，并成为表示基础架构的标准。工程师发现他们正在开发更好的基础架构代码，并允许 Terraform 对其进行解释。与配置管理代码一样，工程师们开始将他们的基础架构表述存储在版本控制系统中，并将基础架构与软件等同看待。\n通过表述基础架构的标准化方式，我们摆脱了学习各种专有云 API 的痛苦。尽管并非所有云资源都可以用单一表示抽象出来，但大多数用户可以接受其代码中的云锁定。拥有人类可读并且机器可解析的基础架构表示，而不仅仅是独立的资源声明，这一点永远得改变了行业。\n从代码部署\n在面临将基础架构部署为脚本的挑战之后，我们已经创建了一个程序来解析输入并针对我们的基础架构采取行动。\n例 3-3 显示了从 Terraform 开源库中获取的 Terraform 配置。注意代码中有变量，需要在运行时解析。\n基础架构的声明性表示很重要，因为它没有定义创建基础架构的各个步骤。这使我们能够分离需要调配的部分和调配的部分。这就是使这种基础架构代表成为新范例的原因；这也是向软件基础架构演进的第一步。\n以这种方式来表示基础架构对于工程师来说是一种常见的强大做法。用户可以使用 Terraform 来应用基础架构。\n例 3-3. example.tf\n# Create our DNSimple record resource \u0026#34;dnsimple_record\u0026#34; \u0026#34;web\u0026#34; {domain = \u0026#34;${var.dnsimple_domain}\u0026#34; name = \u0026#34;terraform\u0026#34; value = \u0026#34;${hostname}\u0026#34; type = \u0026#34;CNAME\u0026#34; ttl = 3600 } 基础架构即软件 基础架构即代码是朝着正确方向发展的强大举措。但是代码是基础架构的静态表示，并且有其局限性。您可以自动执行部署代码更改的过程，但除非部署工具持续运行，否则仍会出现配置漂移。传统上，部署工具只能在一个方向上工作：它只能创建新对象，并且不能轻易删除或修改现有对象。\n为了掌握基础架构，我们的部署工具需要根据基础架构的初始表示进行工作，并对数据进行变更以创建更灵活的系统。当我们开始将基础架构表示视为一个可持续执行所需状态的可版本化数据体时，下一步就是将基础架构视为软件。\nTerraform 从配置管理中吸取教训并改进了这一概念，以 …","relpermalink":"/book/cloud-native-infra/evolution-of-cloud-native-developments/","summary":"我们在前一章中讨论了在采用云原生基础架构的前提。在部署之前，需要有由 API 驱动的基础架构（IaaS）供给。 在本章中，我们将探讨云原生基础架构拓扑的概念，并在云中实现它们。我们将学习可以帮助运维人员控制其基","title":"第 3 章：云原生部署的演变"},{"content":"DevSecOps 在早期就将安全纳入了软件工程流程。它将安全流程和工具集成到 DevOps 的所有开发工作流程（或后面解释的管道）中，并使之自动化，从而实现无缝和连续。换句话说，它可以被看作是三个过程的组合。开发 + 安全 + 运维 。\n本节讨论了 DevSecOps 的以下方面：\n组织对 DevSecOps 的准备情况 开发安全运维平台 开发安全运维的基本构件或关键原语 本章大纲 3.1 组织对 DevSecOps 的准备情况\n3.2 DevSecOps 平台\n3.3 DevSecOps 关键原语和实施任务\n开始阅读 ","relpermalink":"/book/service-mesh-devsecops/devsecops/","summary":"DevSecOps 在早期就将安全纳入了软件工程流程。它将安全流程和工具集成到 DevOps 的所有开发工作流程（或后面解释的管道）中，并使之自动化，从而实现无缝和连续。换句话说，它可以被看作是三个过程的组合。开发 + 安全 + 运维 。 本节","title":"第三章：DevSecOps 组织准备、关键基本要素和实施"},{"content":"Kubernetes，经常被缩写为 “K8s”，是一个开源的容器或编排系统，用于自动部署、扩展和管理容器化应用程序。它管理着构成集群的所有元素，从应用中的每个微服务到整个集群。与单体软件平台相比，将容器化应用作为微服务使用可以提供更多的灵活性和安全优势，但也可能引入其他复杂因素。\n图 1：Kubernetes 集群组件的高层视图 本指南重点关注安全挑战，并尽可能提出适用于国家安全系统和关键基础设施管理员的加固策略。尽管本指南是针对国家安全系统和关键基础设施组织的，但也鼓励联邦和州、地方、部落和领土（SLTT）政府网络的管理员实施所提供的建议。Kubernetes 集群的安全问题可能很复杂，而且经常在利用其错误配置的潜在威胁中被滥用。以下指南提供了具体的安全配置，可以帮助建立更安全的 Kubernetes 集群。\n建议 每个部分的主要建议摘要如下：\nKubernetes Pod 安全 使用构建的容器，以非 root 用户身份运行应用程序 在可能的情况下，用不可变的文件系统运行容器 扫描容器镜像，以发现可能存在的漏洞或错误配置 使用 Pod 安全政策来执行最低水平的安全，包括： 防止有特权的容器 拒绝经常被利用来突破的容器功能，如 hostPID、hostIPC、hostNetwork、allowedHostPath 等 拒绝以 root 用户身份执行或允许提升为根用户的容器 使用安全服务，如 SELinux®、AppArmor® 和 seccomp，加固应用程序，防止被利用。 网络隔离和加固 使用防火墙和基于角色的访问控制（RBAC）锁定对控制平面节点的访问 进一步限制对 Kubernetes etcd 服务器的访问 配置控制平面组件，使用传输层安全（TLS）证书进行认证、加密通信 设置网络策略来隔离资源。不同命名空间的 Pod 和服务仍然可以相互通信，除非执行额外的隔离，如网络策略 将所有凭证和敏感信息放在 Kubernetes Secret 中，而不是配置文件中。使用强大的加密方法对 Secret 进行加密 认证和授权 禁用匿名登录（默认启用） 使用强大的用户认证 创建 RBAC 策略以限制管理员、用户和服务账户活动 日志审计 启用审计记录（默认为禁用） 在节点、Pod 或容器级故障的情况下，持续保存日志以确保可用性 配置一个 metric logger 升级和应用安全实践 立即应用安全补丁和更新 定期进行漏洞扫描和渗透测试 当组件不再需要时，将其从环境中移除 架构概述 Kubernetes 使用集群架构。一个 Kubernetes 集群是由一些控制平面和一个或多个物理或虚拟机组成的，称为工作节点。工作者节点承载 Pod，其中包含一个或多个容器。容器是包含软件包及其所有依赖关系的可执行镜像。见图 2：Kubernetes 架构。\n图 2：Kubernetes 架构 控制平面对集群进行决策。这包括调度容器的运行，检测 / 应对故障，并在部署文件中指定的副本数量没有得到满足时启动新的 Pod。以下逻辑组件都是控制平面的一部分：\nController manager（默认端口：10252） - 监视 Kubernetes 集群，以检测和维护 Kubernetes 环境的几个方面，包括将 Pod 加入到服务中，保持一组 Pod 的正确数量，并对节点的丢失做出反应。 Cloud controller manager（默认端口：10258） - 一个用于基于云的部署的可选组件。云控制器与云服务提供商接口，以管理集群的负载均衡器和虚拟网络。 Kubernetes API Server（默认端口：6443 或 8080） - 管理员操作 Kubernetes 的接口。因此，API 服务器通常暴露在控制平面之外。API 服务器被设计成可扩展的，可能存在于多个控制平面节点上。 Etcd（默认端口范围：2379-2380） - 持久化的备份存储，关于集群状态的所有信息都保存在这里。Etcd 不应该被直接操作，而应该通过 API 服务器来管理。 Scheduler（默认端口：10251） - 跟踪工作节点的状态并决定在哪里运行 Pod。Kube-scheduler 只可以由控制平面内的节点访问。 Kubernetes 工作节点是专门为集群运行容器化应用的物理或虚拟机。除了运行容器引擎外，工作节点还承载以下两个服务，允许从控制平面进行协调：\nKubelet（默认端口：10250） - 在每个工作节点上运行，以协调和验证 Pod 的执行。 Kube-proxy - 一个网络代理，使用主机的数据包过滤能力，确保 Kubernetes 集群中数据包的正确路由。 集群通常使用云服务提供商（CSP）的 Kubernetes 服务或在企业内部托管。在设计 Kubernetes 环境时，组织应了解他们在安全维护集群方面的责任。CSP 管理大部分的 Kubernetes 服务，但组织可能需要处理某些方面，如认证和授权。\n","relpermalink":"/book/kubernetes-hardening-guidance/introduction/","summary":"Kubernetes，经常被缩写为 “K8s”，是一个开源的容器或编排系统，用于自动部署、扩展和管理容器化应用程序。它管理着构成集群的所有元素，从应用中的每个微服务到整个集群。与","title":"简介"},{"content":"到目前为止，我们已经从数据的角度讨论了现代可观测性。但是，现代可观测性还有另一个方面，从长远来看，它可能被证明同样重要：如何对待产生数据的仪表（instrumention）。\n大多数软件系统都是用现成的部件构建的：网络框架、数据库、HTTP 客户端、代理服务器、编程语言。在大多数组织中，很少有这种软件基础设施是在内部编写的。相反，这些组件是在许多组织中共享的。最常见的是，这些共享组件是以开放源码（OSS）库的形式出现的，并具有许可权。\n由于这些开放源码软件库几乎囊括了一般系统中的所有关键功能，因此获得这些库的高质量说明对大多数可观测性系统来说至关重要。\n传统上，仪表是 “单独出售” 的。这意味着，软件库不包括产生追踪、日志或度量的仪表。相反，特定解决方案的仪表是在事后添加的，作为部署可观测系统的一部分。\n什么是特定解决方案仪表？\n在本章中，术语 “特定解决方案仪表” 是指任何旨在与特定的可观测系统一起工作的仪表，使用的是作为该特定系统的数据存储系统的产物而开发的客户端。在这些系统中，客户端和存储系统常常深深地融合在一起。因此，如果一个应用要从一个观测系统切换到另一个观测系统，通常需要进行全面的重新布设。\n针对解决方案的仪表是 “三大支柱” 中固有的垂直整合的遗留问题。每个后端都摄取特定类型的专有数据；因此，这些后端的创建者也必须提供产生这些数据的仪表。\n这种工具化的方法给参与软件开发的每个人都带来了麻烦：供应商、用户和开放源码库的作者。\n可观测性被淹没在特定解决方案的仪表中 从可观测性系统的角度来看，仪表化代表了巨大的开销。\n在过去，互联网应用是相当同质化的，可以围绕一个特定的网络框架来建立可观测性系统。Java Spring、Ruby on Rails 或 .NET。但随着时间的推移，软件的多样性已经爆炸性增长。现在为每一个流行的网络框架和数据库客户端维护仪表是一项巨大的负担。\n这导致的重复劳动难以估量。传统上，供应商将他们在仪表上的投资作为销售点和把关的一种形式。但是，日益增长的软件开发速度已经开始使这种做法无法维持了。对于一个合理规模的仪表设备团队来说，覆盖面实在是太大了，无法跟上。\n这种负担对于新的、新颖的观测系统，特别是开放源码软件的观测项目来说尤其严峻。如果一个新的系统在编写了大量的仪表之前无法在生产中部署，而一个开放源码软件项目在广泛部署之前也无法吸引开发者的兴趣，那么科学进步就会陷入僵局。这对于基于追踪的系统来说尤其如此，它需要端到端的仪表来提供最大的价值。\n应用程序被锁定在特定解决方案的仪表中 从应用开发者的角度来看，特定解决方案的仪表代表了一种有害的锁定形式。\n可观测性是一个交叉性的问题。要彻底追踪、记录或度量一个大型的应用程序，意味着成千上万的仪表 API 调用将遍布整个代码库。改变可观测性系统需要把所有这些工具去掉，用新系统提供的不同工具来代替。\n替换仪表是一项重大的前期投资，即使只是为了尝试一个新的系统。更糟的是，大多数系统都太大了以致于在所有服务中同时更换所有仪表是不可行的。大多数系统需要逐步推出新的仪表设备，但这样的上线可能很难设计。\n被特定解决方案的仪表所 “困住” 是非常令人沮丧的。在可观测性供应商开始努力提供自己的仪表的同时，用户也开始拒绝采用这种仪表。由于了解到重新安装仪表的工作量，许多用户强烈希望他们正在考虑的任何新的观测系统能与他们目前使用的仪表一起工作。\n为了支持这一要求，许多可观测性系统试图与其他几个系统提供的仪表一起工作。但这种拼凑的方式降低了每个系统所摄取的数据的质量。从许多来源摄取数据意味着对输入的数据不再有明确的定义，当预期的数据不均衡且定义模糊时，分析工具就很难完成它们的工作。\n针对开源软件的特定解决方案的仪表基本上是不可能的 从一个开放源码库作者的角度来看，特定解决方案的仪表化是一个悲剧。\n来自开放源码软件库的遥测数据对于操作建立在它们之上的应用程序来说至关重要。最了解哪些数据对操作至关重要的人，以及操作者应该如何利用这些数据来补救问题的人，就是实际编写软件的开源库的开发者。\n但是，库的作者却陷入了困境。正如我们将看到的，没有任何一个特定解决方案的仪表化 API，无论写得多么好，都无法作为开放源码库可以接受的选择。\n如何挑选一个日志库？ 假设你正在编写世界上最伟大的开源网络框架。在生产过程中，很多事情都会出错，你自然希望把错误、调试和性能信息传达给你的用户。你使用哪个日志库？\n有很多体面的日志库。事实上，有很多，无论你选择哪个库，你都会有很多用户希望你选择一个不同的库。如果你的 Web 框架选择了一个日志库，而数据库客户端库选择了另一个，怎么办？如果这两个都不是用户想要使用的呢？如果他们选择了同一个库的不兼容的版本呢？\n没有一个完美的方法可以将多个特定解决方案的日志库组合成一个连贯的系统。虽然日志足够简单，不同解决方案的大杂烩可能是可行的，但对于特定解决方案的指标和追踪来说，情况并非如此。\n因此，开放源码软件库通常没有内置的日志、度量或追踪功能。取而代之的是，库提供了 “可观测性钩子”，这需要用户编写和维护一堆适配器，将使用的库连接到他们的可观测性系统上。\n作者们有大量的知识，他们想通信关于他们的系统应该如何运行的知识，但他们没有明确的方法去做。如果你问任何写过大量开源软件的人，他们会告诉你。这种情况是痛苦的！而且是不幸的！一些库的作者确实试图选择一个日志库，但却发现他们无意中为一些用户造成了版本冲突，同时迫使其他用户编写日志适配器来捕捉使用其实际日志库的数据。\n但是对于大多数库来说，可观测性只是一个事后的想法。虽然库的作者经常编写大量的测试套件，但他们很少花时间去考虑运行时的可观测性。考虑到库有大量的测试工具，但可观测性工具为零，这种结果并不令人惊讶。\n正如我们在接下来的几章中所看到的，现代可观测性的设计是为了使在可观测性管道中发挥作用的每个人的代理权最大化。但受益最大的是库的作者；对于特定解决方案的仪表，他们目前根本没有选择。\n分解问题 我们可以通过设计一个可观测性系统来解决上面列出的所有问题，以明确地解决每个人的需求。在本章的其余部分，我们将把现代观测系统的设计分解为基本要求。这些要求将为第五章中描述的 OpenTelemetry 的结构提供动力。\n要求：独立的仪表、遥测和分析 归根结底，计算机系统实际上就是人类系统。像可观测性这样的跨领域问题，几乎与每一个软件组件都有互动。同时，传输和处理遥测数据可能是一个大批量的活动，以至于一个大规模的观测系统会产生自己的操作问题。这意味着，许多不同的人，以不同的身份，需要与观测系统的不同方面交互。为了很好地服务于他们，这个系统必须确保每个参与其中的人都有他们所需要的代理权，以便快速和独立地执行任务。提供代理权是设计一个有效的观测系统的基本要求。\n让我们首先确定与运行中的软件系统有关的每个角色的责任：库的作者、应用程序的所有者、操作者和响应者。\n库的作者了解他们软件的情况\n对于封装了关键功能的软件库，如网络和请求管理，库的作者也必须管理追踪系统的各个方面：注入、提取和上下文传播。\n应用程序拥有者组织软件并管理依赖关系\n应用程序所有者选择构成其应用程序的组件，并确保它们编译成一个连贯的、有功能的系统。应用程序所有者还编写应用程序级别的工具，它必须与库作者提供的指令（和上下文传播）进行正确的交互。\n运维人员管理遥测的生产和传输\n运维管理从应用到响应者的可观测性数据的传输。他们必须能够选择数据的格式以及数据的发送地点。当数据在产生时，他们必须操作传输系统：管理缓冲、处理和传送数据所需的所有资源。\n响应者消费遥测数据并产生有用的见解\n要做到这一点，应对者必须了解数据结构及其内在意义（结构和意义将在第三章中详细描述）。当新的和改进的分析工具出现时，反应者还需要将其添加到他们的工具箱中。\n这些角色代表不同的决策点：\n库的作者只能通过发布其代码的新版本来进行修改。 应用程序所有者只能通过部署其可执行文件的新版本来进行更改。 运维人员只能通过管理可执行文件的拓扑结构和配置来进行改变。 响应者只能根据他们收到的数据做出改变。 传统的三大支柱方法扰乱了所有这些角色。垂直整合的一个副作用是，几乎所有的数据变化都需要进行代码修改。几乎任何对可观测性系统的非微不足道的改变都需要应用程序所有者进行代码修改。要求其他人进行你所关心的改变，这样会有很大的阻力，并可能导致压力、冲突和不作为。\n显然，一个设计良好的可观测性系统应该侧重于允许每个人尽可能多的代理和直接控制，它应该避免将开发者变成意外的看门人。\n要求：零依赖性 应用程序是由依赖关系（网络框架、数据库客户端），加上依赖关系的依赖关系（OpenTelemetry 或其他仪表库），加上它们的依赖关系的依赖关系的依赖关系（无论这些仪表库依赖什么）组成。这些都被称为反式的依赖关系。\n如果任何两个依赖关系之间有冲突，应用程序就无法运行。例如，两个库可能分别需要一个不同的（不兼容的）底层网络库的版本，如 gRPC。这可能会导致一些不好的情况。例如，一个新版本的库可能包括一个需要的安全补丁，但也包括一个升级的依赖关系，这就产生了依赖关系冲突。\n诸如此类的过渡性依赖冲突给应用程序所有者带来了很大的麻烦，因为这些冲突无法独立解决。相反，应用程序所有者必须联系库的作者，要求他们提供一个解决方案，这最终需要时间（假设库的作者回应了这个请求）。\n为了使现代可观测性发挥作用，库必须能够嵌入仪表，而不必担心当他们的库被用于组成应用程序时而导致问题。因此，可观测性系统必须提供不包含可能无意中引发横向依赖冲突的依赖性的仪表。\n要求：严格的后向兼容和长期支持 当一个仪表化的 API 破坏了向后的兼容性，坏事就会发生。一个精心设计的应用程序最终可能会有成千上万的仪表调用站点。由于 API 的改变而不得不更新数以千计的调用站点是一个相当大的工作量。\n这就产生了一种特别糟糕的依赖性冲突，即一个库中的仪表化不再与另一个库中的仪表化兼容。\n因此，仪表化 API 必须在很长的时间范围内具有严格的向后兼容能力。理想的情况是，仪表化 API 一旦变得稳定，就永远不会破坏向后兼容。新的、实验性的 API 功能的开发方式必须保证它们的存在不会在包含稳定仪表的库之间产生冲突。\n分离关注点是良好设计的基础 在下一章中，我们将深入研究 OpenTelemetry 的架构，看看它是如何满足上面提出的要求的。但在这之前，我想说的是一个重要的问题。\n如果你分析这些需求，你可能会注意到一些奇特的现象：它们中几乎没有任何专门针对可观测性的内容。相反，重点是尽量减少依赖性，保持向后的兼容性，并确保不同的用户可以在没有无谓干扰的情况下发挥作用。\n每一个要求都指出了关注点分离是一个关键的设计特征。但是这些特性并不是 OpenTelemetry 所独有的。任何寻求广泛采用的软件库都会很好地包括它们。在这个意义上，OpenTelemetry 的设计也可以作为设计一般的开源软件的指南。下次当你开始一个新的开放源码软件项目时，请预先考虑这些要求，相应地设计你的库及你的用户会感谢你的。\n","relpermalink":"/book/opentelemetry-obervability/supporting-open-source-and-native-instrumentation/","summary":"第 4 章：支持开源和原生监测","title":"第 4 章：支持开源和原生监测"},{"content":"在使用 ApplicationSet 之前，了解其安全性影响非常重要。\n只有管理员可以创建/更新/删除 ApplicationSet ApplicationSet 可以在任意 Project 下创建应用程序。Argo CD 设置通常包括高权限的 Project （例如 default），往往包括管理 Argo CD 自身资源的能力（例如 RBAC ConfigMap）。\nApplicationSets 还可以快速创建任意数量的应用程序，并同样快速删除它们。\n最后，ApplicationSets 可以显示特权信息。例如，git generator 可以读取 Argo CD 命名空间中的 Secrets，并将其作为 Auth 标头发送到任意 URL（例如为 api 字段提供的 URL）。 （此功能旨在为 SCM 提供程序（如 GitHub）授权请求，但可能会被恶意用户滥用。）\n出于这些原因，只有管理员可以通过 Kubernetes RBAC 或任何其他机制获得创建、更新或删除 ApplicationSets 的权限。\n管理员必须为 ApplicationSets 的真实来源应用适当的控制 即使非管理员不能创建 ApplicationSet 资源，他们也可能影响 ApplicationSets 的行为。\n例如，如果 ApplicationSet 使用 git generator ，则具有源 Git 存储库的推送访问权限的恶意用户可能会生成过多的应用程序，对 ApplicationSet 和应用程序控制器造成压力。他们还可能导致 SCM 提供者的速率限制生效，影响 ApplicationSet 服务。\n模板化的“project”字段 特别需要注意使用模板化的“project”字段的 ApplicationSet。具有写权限的恶意用户（例如，具有对 git generator 的 git repo 的推送访问权限的用户）可能会在限制不足的 Projects 下创建应用程序。具有在不受限制的 Project（如“default”Project）下创建应用程序的能力的恶意用户可能会通过修改其 RBAC ConfigMap 等方式接管 Argo CD 本身。\n如果 ApplicationSet 的模板中未硬编码“project”字段，则管理员必须控制 ApplicationSet 的生成器的所有来源。\n","relpermalink":"/book/argo-cd/operator-manual/applicationset/security/","summary":"在使用 ApplicationSet 之前，了解其安全性影响非常重要。 只有管理员可以创建/更新/删除 ApplicationSet ApplicationSet 可以在任意 Project 下创建应用程序。Argo CD 设置通常包括高权限的 Project （例如 default），往往包括管理 Argo CD 自身资源的能力（例如 RBAC Co","title":"ApplicationSet 安全性"},{"content":"总结 保持友善。 解释你的推理。 在给出明确的指示与只指出问题并让开发人员自己决定间做好平衡。 鼓励开发人员简化代码或添加代码注释，而不仅仅是向你解释复杂性。 礼貌 一般而言，对于那些正在被您审查代码的人，除了保持有礼貌且尊重以外，重要的是还要确保您（的评论）是非常清楚且有帮助的。你并不总是必须遵循这种做法，但在说出可能令人不安或有争议的事情时你绝对应该使用它。例如：\n糟糕的示例：“为什么这里你使用了线程，显然并发并没有带来什么好处？”\n好的示例：“这里的并发模型增加了系统的复杂性，但没有任何实际的性能优势，因为没有性能优势，最好是将这些代码作为单线程处理而不是使用多线程。”\n解释为什么 关于上面的“好”示例，您会注意到的一件事是，它可以帮助开发人员理解您发表评论的原因。并不总是需要您在审查评论中包含此信息，但有时候提供更多解释，对于表明您的意图，您在遵循的最佳实践，或为您建议如何提高代码健康状况是十分恰当的。\n给予指导 一般来说，修复 CL 是开发人员的责任，而不是审查者。 您无需为开发人员详细设计解决方案或编写代码。\n但这并不意味着审查者应该没有帮助。一般来说，您应该在指出问题和提供直接指导之间取得适当的平衡。指出问题并让开发人员做出决定通常有助于开发人员学习，并使代码审查变得更容易。它还可能产生更好的解决方案，因为开发人员比审查者更接近代码。\n但是，有时直接说明，建议甚至代码会更有帮助。代码审查的主要目标是尽可能获得最佳 CL。第二个目标是提高开发人员的技能，以便他们随着时间的推移需要的审查越来越少。\n接受解释 如果您要求开发人员解释一段您不理解的代码，那通常会导致他们更清楚地重写代码。偶尔，在代码中添加注释也是一种恰当的响应，只要它不仅仅是解释过于复杂的代码。\n**仅在代码审查工具中编写的解释对未来的代码阅读者没有帮助。**这仅在少数情况下是可接受的，例如当您查看一个您不熟悉的领域时，开发人员会用来向您解释普通读者已经知道的内容。\n","relpermalink":"/book/eng-practices/review/reviewer/comments/","summary":"总结 保持友善。 解释你的推理。 在给出明确的指示与只指出问题并让开发人员自己决定间做好平衡。 鼓励开发人员简化代码或添加代码注释，而不仅仅是向你解释复杂性。 礼貌 一般而言，对于那些正在被您审查代码的人，除了保","title":"如何撰写 Code Review 评论"},{"content":"SPIFFE 标准提供了一个规范，用于在异构环境和组织中引导和发放可互操作的服务身份。它定义了一个称为\u0026#34;信任域\u0026#34;的概念，用于划分管理和/或安全边界。信任域隔离发放机构并区分身份命名空间，但也可以松散耦合以提供联合身份。\n本文档描述了 SPIFFE 信任域的语义、表示方式以及它们如何耦合在一起的机制。\n引言 SPIFFE 信任域表示 SPIFFE ID 有资格的基础，指示任何给定 SPIFFE ID 已经发放的领域或发放机构。它们由发放机构支持，负责管理其相应信任域中的 SPIFFE 身份发放。尽管信任域的名称由一个简单的人类可读字符串组成，但还必须表达由信任域的发放机构使用的密码密钥，以使其他人能够验证其发放的身份。这些密钥被表示为\u0026#34;SPIFFE Bundle\u0026#34;，与其所代表的信任域紧密相连。\n本规范定义了 SPIFFE 信任域和 SPIFFE Bundle 的性质和语义。\n信任域 SPIFFE 信任域是由一组密码密钥支持的身份命名空间。这些密钥共同为驻留在信任域中的所有身份提供了密码锚点。\n信任域与支持它们的密钥之间存在一对多的关系。一个信任域可以由多个密钥和密钥类型来表示。例如，前者可以在根密钥轮换期间使用，而后者在使用多个 SVID 类型时避免多协议攻击是必要的。\n需要注意的是，虽然可以在多个信任域之间共享密码密钥，但我们强烈建议每个授权密钥仅在一个信任域中使用。密钥的重复使用可能会降低信任域的隔离性（例如，在演练和生产之间），并引入额外的安全挑战（例如，需要为辅助发放机构实施名称约束系统）。\nSPIFFE Bundle (SPIFFE Bundle) SPIFFE Bundle 是包含信任域的密码密钥的对象。Bundle 中的密钥被视为代表 Bundle 所代表的信任域的权威，并用于证明驻留在该信任域中的 SVIDs 的有效性。\nSPIFFE Bundle 设计用于在 SPIFFE 控制平面实现内部和之间使用。然而，此规范不排除直接由工作负载消费的使用。\n在存储或管理 SPIFFE Bundle 时，独立记录 Bundle 所代表的信任域的名称至关重要，通常通过使用\u0026lt;trust_domain_name, bundle\u0026gt;元组来实现。在验证 SVID 时，验证器必须选择与 SVID 所在的信任域对应的 Bundle，因此在大多数情况下需要维护此关系。\n请注意，信任域 Bundle 的内容预计会随时间变化，因为它所包含的密钥进行轮换。通过发放包含新密钥的新 Bundle 并省略已撤销的密钥来添加和撤销密钥。SPIFFE 实现负责根据需要将 Bundle 内容更新分发给工作负载。确切的格式和通过哪种方法传递这些更新超出了本规范的范围。\nSPIFFE Bundle 格式 SPIFFE Bundle 被表示为 RFC 7517 兼容的 JWK 集合。选择 JWK 的原因有两个主要原因。首先，它提供了一种灵活的格式，用于表示各种类型的密码密钥（和 X.509 等文档），从而在定义新的 SVID 格式时提供了一定程度的未来证明。其次，它得到了广泛支持和部署，主要用于域间联合，这是 SPIFFE 项目的核心目标。\nJWK 集合 本节定义了 JWK 集合的参数。未在此处定义的参数可以根据实现者的需要包含，但是 SPIFFE 实现不能要求它们的存在以使其正常工作。\n序列号 参数spiffe_sequence应该被设置。该序列号可以被 SPIFFE 控制平面用于许多目的，包括传播测量和更新顺序/替代。当存在时，其值必须为单调递增的整数，并且当 bundle 的内容被更新时必须更改。\n值得注意的是，尽管 JSON 整数类型是可变宽度且没有定义最大限制，但许多实现可能将其解析为固定宽度类型。为了防止溢出，应该确保生成的类型至少具有 64 位的精度。\n刷新提示 参数spiffe_refresh_hint应该被设置。刷新提示指示消费者应该多久检查更新。Bundle 发布者可以将刷新提示作为其密钥轮换频率的函数进行广告。值得注意的是，刷新提示还可能影响密钥撤销的传播速度。如果设置了刷新提示，其值必须是表示建议的消费者刷新间隔的整数，以秒为单位。正如名称所示，刷新间隔只是一个提示，根据实现的不同，消费者可以更频繁或更不频繁地检查更新。\n密钥 参数keys必须存在。其值是一个 JWK 数组。遇到未知的密钥类型或用途的客户端必须忽略相应的 JWK 元素。请参阅 RFC 7517 的第 5 节以了解有关keys参数语义的更多信息。\nkeys参数可以包含一个空数组。发布空密钥数组的信任域表示该信任域已撤销先前发布的任何密钥。工作负载还可能遇到经处理后不产生可用密钥（即没有 JWK 通过下面描述的验证）的 bundle，并且实际上为空。这可能表明信任域已迁移到客户端不理解的新密钥类型或用途。在这两种情况下，工作负载必须将来自信任域的所有 SVID 视为无效和不可信。\nJWK 本节定义了作为 JWK 集合一部分包含的 JWK 元素的高级要求。JWK 元素表示单个密码密钥，用于对单个类型的 SVID 进行身份验证。虽然安全使用 JWK 的确切要求因 SVID 类型而异，但在本节中我们概述了一些顶级要求。SVID 规范必须为use参数（参见下面的Public Key Use节）定义适当的值，并且可以根据需要对其 JWK 元素设置进一步的要求或限制。\n实现者不应包含在此处或相应的 SVID 规范中未定义的参数。\n密钥类型 kty 参数必须设置，并且其行为遵循 RFC 7517 的 Section 4.1。遇到未知密钥类型的客户端必须忽略整个 JWK 元素。\n公钥用途 use 参数必须设置。其值表示其具有权威性的身份文档（或 SVID）的类型。截至本文撰写时，仅支持两种 SVID 类型：x509-svid 和 jwt-svid。值区分大小写。有关 use 值的更多信息，请参见相应的 SVID 规范。遇到缺少 use 参数或未知 use 值的客户端必须忽略整个 JWK 元素。\n安全注意事项 本节概述了在实施和部署 SPIFFE 控制平面时应考虑的与安全相关的注意事项。\nSPIFFE Bundle 刷新提示 SPIFFE Bundle 包括一个可选的 refresh_hint 字段，用于指示消费者应尝试刷新其 Bundle 副本的频率。这个值对密钥的轮换速度有明显的影响，但它也影响了密钥的撤销速度。应该仔细选择刷新提示值。\n由于此字段不是必需的，因此可能会遇到没有设置 refresh_hint 的 SPIFFE Bundle。在这种情况下，客户端可以通过检查 SVID 有效期来使用合适的间隔。应该认识到，省略 refresh_hint 可能会影响信任域迅速撤销已被损坏密钥的能力。客户端应该默认使用相对较低（例如五分钟）的刷新间隔，以便及时获取更新的信任 Bundle。\n在信任域之间重用加密密钥 本规范不鼓励在信任域之间共享加密密钥，因为这种做法会降低信任域的隔离性并引入额外的安全挑战。当一个根密钥在多个信任域之间共享时，认证和授权实现必须仔细检查标识的信任域名组件，并且信任域名组件在授权策略中必须易于表达和习惯性地表达。\n假设一个天真的实现导入（即完全信任）一个特定的根密钥，并且认证系统被配置为认证链到受信任根密钥的任何 SVID 的 SPIFFE 身份。如果天真的实现未配置为仅信任特定的信任域，则任何信任域中发行的标识都可以被认证（只要 SVID 链接到受信任的根密钥）。\n继续上述例子，其中天真的实现导入了特定的 CA 证书，假设认证未区分信任域并且接受链到受信任根密钥的任何 SVID。然后，授权系统将只授权特定的信任域。换句话说，授权策略需要明确配置以检查 SVID 的信任域名组件。这里的安全关注点是天真的授权实现可能盲目地相信认证系统已过滤掉不受信任的信任域。\n总之，安全性的最佳实践是在信任域和根密钥之间维持一对一的映射，以减少细微（但灾难性的）认证和授权实现错误。重新使用跨信任域的根密钥的系统应确保（a）SVID 发行系统（例如 CA）在发行 SVID 前正确实现授权检查，并且（b）依赖方（即使用 SVID 的系统）正确实现强大的认证和授权系统，能够区分多个信任域。\n附录 A. SPIFFE Bundle 示例 在下面的示例中，我们为名为example.com的信任域配置了初始的 SPIFFE Bundle，并演示了在根密钥轮换期间如何更新 Bundle。\nexample.com信任域的初始 X.509 CA 证书：\nCertificate #1: Data: Version: 3 (0x2) Serial Number: df:d0:ad:fd:32:9f:b8:15:76:f5:d4:b9:e3:be:b5:a7 Signature Algorithm: sha256WithRSAEncryption Issuer: O = example.com Validity Not Before: Jan 1 08:00:45 2019 GMT Not After : Apr 1 08:00:45 2019 GMT Subject: O = example.com X509v3 extensions: X509v3 Key Usage: critical Certificate Sign X509v3 Basic Constraints: critical CA:TRUE X509v3 Subject Alternative Name: URI:spiffe://example.com/ [...] 请注意以下事项：\n证书是自签名的（颁发者和主题相同）； 证书的 CA 标志设置为 true； 证书是 SVID（具有 spiffe URI SAN）。 example.com的相应信任 Bundle：\nTrust bundle #1 for example.com: { \u0026#34;spiffe_sequence\u0026#34;: 1, \u0026#34;spiffe_refresh_hint\u0026#34;: 2419200, \u0026#34;keys\u0026#34;: [ { \u0026#34;kty\u0026#34;: \u0026#34;RSA\u0026#34;, \u0026#34;use\u0026#34;: \u0026#34;x509-svid\u0026#34;, \u0026#34;x5c\u0026#34;: [\u0026#34;\u0026lt;base64 DER encoding of Certificate #1\u0026gt;\u0026#34;], \u0026#34;n\u0026#34;: \u0026#34;\u0026lt;base64urlUint-encoded value\u0026gt;\u0026#34;, \u0026#34;e\u0026#34;: \u0026#34;AQAB\u0026#34; } ] } 上述信任 Bundle 是第 1 个版本，如spiffe_sequence字段所示，并且指示客户端应该每 2419200 秒（或 28 天）轮询更新 Bundle。请注意，x5c参数包含了基于 RFC7517 Section 4.7 中所指定的 base64 编码的 DER 证书。密钥特定值（例如n和e）的编码方法在 RFC7518 Section 6 中有描述。\n为了准备example.com的 CA 证书的过期，生成了一个替换证书，并将其添加到信任 Bundle：\nCertificate #2: Data: Version: 3 (0x2) Serial Number: a4:dc:5f:05:8a:a2:bf:88:9d:a4:fa:1e:9a:a5:db:74 Signature Algorithm: sha256WithRSAEncryption Issuer: O = example.com Validity Not Before: Feb 15 08:00:45 2019 GMT Not After : Jul 1 08:00:45 2019 GMT Subject: O = example.com X509v3 extensions: X509v3 Key Usage: critical Certificate Sign X509v3 Basic Constraints: critical CA:TRUE X509v3 Subject Alternative Name: …","relpermalink":"/book/spiffe-and-spire/standard/spiffe-trust-domain-and-bundle/","summary":"SPIFFE 标准提供了一个规范，用于在异构环境和组织中引导和发放可互操作的服务身份。它定义了一个称为\"信任域\"的概念，用于划分管理和/或安全边界。信任域隔离发放机构并区分身份命名空间，但也可","title":"SPIFFE 信任域和 Bundle"},{"content":" Envoy\nEnvoy + X.509\nEnvoy + JWT-SVID\n使用 Envoy 和 X.509-SVID 进行 OPA 授权\n使用 Envoy 和 JWT-SVIDs 进行 OPA 授权\n","relpermalink":"/book/spiffe-and-spire/examples/","summary":"Envoy Envoy + X.509 Envoy + JWT-SVID 使用 Envoy 和 X.509-SVID 进行 OPA 授权 使用 Envoy 和 JWT-SVIDs 进行 OPA 授权","title":"SPIRE 集成示例"},{"content":" Audit\n","relpermalink":"/book/tsb/refs/audit/","summary":"Audit","title":"audit"},{"content":"如果在配置东西向网关 的cluster-external-addresses 注释 时使用 DNS 主机名，你需要在 XCP 边缘启用 DNS 解析，以便 DNS 解析在 XCP 边缘发生。\n在 XCP 边缘启用 DNS 解析 要在 XCP 边缘启用 DNS 解析，你需要在 ControlPlane CR 或 Helm 值中编辑 xcp 组件，并添加一个名为 ENABLE_DNS_RESOLUTION_AT_EDGE 的环境变量，并将其值设置为 true：\nspec: components: xcp: ... kubeSpec: overlays: - apiVersion: install.xcp.tetrate.io/v1alpha1 kind: EdgeXcp name: edge-xcp patches: ... - path: spec.components.edgeServer.kubeSpec.deployment.env[-1] value: name: ENABLE_DNS_RESOLUTION_AT_EDGE value: \u0026#34;true\u0026#34; ... 有关如何启用东西部路由的详细信息，请参阅使用东西向网关进行多集群流量故障转移 。\n","relpermalink":"/book/tsb/operations/features/edge-dns-resolution/","summary":"如何在 XCP 边缘启用 DNS 解析。","title":"Edge 处的 DNS 解析"},{"content":"要卸载使用 Helm 安装的 TSB，你可以使用 helm uninstall 来卸载一个发布。卸载必须按照以下顺序进行：\n数据平面 控制平面 管理平面 数据平面卸载 helm uninstall dp tetrate-tsb-helm/dataplane --namespace istio-gateway 一旦 Helm 删除了与数据平面 Chart 的最后一个发布关联的所有资源，你将需要手动删除一些在卸载过程中创建的资源，这些资源不受 Helm 跟踪。\nkubectl delete serviceaccount tsb-helm-delete-hook --ignore-not-found kubectl delete clusterrole tsb-helm-delete-hook --ignore-not-found kubectl delete clusterrolebinding tsb-helm-delete-hook --ignore-not-found kubectl delete istiooperators.install.istio.io --all -n istio-gateway --ignore-not-found 控制平面卸载 helm uninstall cp tetrate-tsb-helm/controlplane --namespace istio-system 一旦 Helm 删除了与控制平面 Chart 的最后一个发布关联的所有资源，你将需要手动删除一些在卸载过程中创建的资源，这些资源不受 Helm 跟踪。\nkubectl delete serviceaccount tsb-helm-delete-hook --ignore-not-found kubectl delete clusterrole tsb-helm-delete-hook --ignore-not-found kubectl delete clusterrolebinding tsb-helm-delete-hook --ignore-not-found kubectl delete apiservices.apiregistration.k8s.io v1beta1.external.metrics.k8s.io kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io xcp-edge-istio-system kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io xcp-edge-istio-system 管理平面卸载 helm uninstall mp tetrate-tsb-helm/managementplane --namespace tsb 一旦 Helm 删除了与管理平面 Chart 的最后一个发布关联的所有资源，你将需要手动删除一些在卸载过程中创建的资源，这些资源不受 Helm 跟踪。\nkubectl delete serviceaccount tsb-helm-delete-hook --ignore-not-found kubectl delete clusterrole tsb-helm-delete-hook --ignore-not-found kubectl delete clusterrolebinding tsb-helm-delete-hook --ignore-not-found kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io xcp-central-tsb kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io xcp-central-tsb ","relpermalink":"/book/tsb/setup/helm/uninstallation/","summary":"卸载 TSB 从你的集群中的步骤。","title":"Helm TSB 卸载"},{"content":"New Relic 的流行软件分析平台使企业能够监视其应用程序、服务器和数据库的健康和性能。它从各种来源收集和分析数据，包括应用程序日志、服务器指标和用户交互，以提供详细的洞察和指标。\nTetrate 的丰富的可观测性数据可以与 New Relic 平台无缝集成。本文介绍了如何在 New Relic 中使 Istio 和 Tetrate Service Bridge 的遥测数据可用。有关与应用程序负载相关的指标，请参阅New Relic 文章 ，该文章描述了 Istio 数据平面指标的检索。\n注意 下面的步骤经过验证，但一些客户可能需要额外的定制来满足其自定义的 New Relic 设置。 数据流 下面的图表显示了 Tetrate Service Bridge 导出到 New Relic 的指标工作流程处理。\n每个 Tetrate Service Bridge 控制平面都使用OpenTelemetry Collector 来收集一组高级指标，并将它们聚合在全局 TSB 管理平面中，以及与管理平面中的活动相关的其他指标数据。一旦聚合完成，OpenTelemetry Collector 可以用于直接导出数据到 New Relic。\n配置 Tetrate Service Bridge 以适用于 New Relic 上述所述的 Tetrate 的遥测数据收集和聚合是一个开箱即用的 TSB 配置，不需要任何更改。按照以下步骤将 Tetrate 的数据与 New Relic 集成。\nNew Relic 集成 使用以下步骤配置 TSB 管理平面中的 OpenTelemetry Collector，以通过 OTLP 导出器将数据写入 New Relic 端点。\n基本步骤如下，具体说明如下：\n步骤 1： 创建一个从 TSB 管理平面复制的 OpenTelemetry configMap，并将其修改为启用 OTLP 导出器。 步骤 2： 配置在 Tetrate Service Bridge 管理平面中运行的 OpenTelemetry Collector，以使用上一步创建的 configMap。 有关更多信息，请参阅OTLP 导出器项目页面 和New Relic 文档 。\n可选的 New Relic Kubernetes 集成 New Relic 文档建议在 Kubernetes 集群内部部署 New Relic 集成。这一步骤不需要将 Tetrate Service Bridge 指标传递到 New Relic 平台。 步骤 1：创建 OpenTelemetry configMap 下载并保存此配置映射 yaml 文件，命名为 otel-cm-tsb.yaml 。\n编辑文件，将 \u0026lt;api key\u0026gt; 字段替换为 New Relic 提供的密钥（如下图所示，请识别 INGEST - LICENSE 密钥）：\n使用以下命令将配置应用到你的 Kubernetes 集群：\nkubectl apply -f otel-cm-tsb.yaml 步骤 2：配置 Tetrate Service Bridge OpenTelemetry Collector 为了指向前一步骤创建的 configMap，需要使用以下命令对 TSB 管理平面自定义资源配置进行修补。\nKubernetes 上下文 确保你当前的 Kubernetes 上下文设置为运行 Tetrate Service Bridge 管理平面的集群。 TSB 管理平面命名空间 请注意，默认情况下，tsb 是管理平面的命名空间。如果你的 TSB 管理平面部署在不同的命名空间中，请相应地修改上述命令。 kubectl patch managementplane managementplane -n tsb \\ --patch \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;components\u0026#34;:{\u0026#34;collector\u0026#34;:{\u0026#34;kubeSpec\u0026#34;:{\u0026#34;overlays\u0026#34;:[{\u0026#34;apiVersion\u0026#34;: \u0026#34;apps/v1\u0026#34;,\u0026#34;kind\u0026#34;: \u0026#34;Deployment\u0026#34;,\u0026#34;name\u0026#34;: \u0026#34;otel-collector\u0026#34;,\u0026#34;patches\u0026#34;:[{\u0026#34;path\u0026#34;:\u0026#34;spec.template.spec.volumes[0].configMap.name\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;otel-collector-modified\u0026#34;}]}]}}}}}\u0026#39; \\ --type merge 验证 New Relic 集成 Tetrate 维护了一组预构建的仪表板，可供使用作为起点；用户还可以使用自定义 New Relic 查询构建自己的仪表板集。\n以下查询将确认 New Relic 集成是否正常工作：\nSELECT rate(sum(envoy_cluster_internal_upstream_rq), 1 SECONDS) FROM Metric WHERE ((envoy_response_code RLIKE \u0026#39;2.*|3.*|401\u0026#39;) AND (component = \u0026#39;front-envoy\u0026#39;)) SINCE 60 MINUTES AGO UNTIL NOW FACET envoy_cluster_name LIMIT 100 TIMESERIES 60000 SLIDE BY 30000 总结 本页面描述了将 Tetrate Service Bridge 指标与 New Relic 平台集成所需的步骤。如果需要进一步的信息或帮助，请联系 Tetrate 支持。\n","relpermalink":"/book/tsb/operations/telemetry/new-relic/","summary":"在 New Relic 中分析服务网格指标。","title":"New Relic 集成"},{"content":" YAML API 指南\n","relpermalink":"/book/tsb/reference/yaml-api/","summary":"YAML API 指南","title":"YAML API"},{"content":"要将部署在 AWS Auto Scaling Group（ASG）上的工作负载加入，你需要在实例启动脚本 中执行所有设置操作，而不是在 EC2 实例上执行命令。\n简而言之，你需要将先前步骤中的设置命令移到与 Auto Scaling Group 中的实例关联的 cloud-init 配置中。\n具体来说，\n将来自 安装 Bookinfo Ratings 应用程序 步骤的设置命令移到云初始化配置中。 将来自 安装 Istio Sidecar 步骤的设置命令移到云初始化配置中。 将来自 在 AWS EC2 实例上安装工作负载 Onboarding Agent 步骤的设置命令移到云初始化配置中。 将来自 从 AWS EC2 实例上加入工作负载 步骤的设置命令移到云初始化配置中。 以下配置是将所有步骤合并在一起的示例。将 \u0026lt;example-ca-certificate\u0026gt; 替换为 example-ca.crt.pem 的值 ，将 \u0026lt;ONBOARDING_ENDPOINT_ADDRESS\u0026gt; 替换为 你之前获取的值 。\n#cloud-config write_files: # 自定义 CA 的证书 - content: | \u0026lt;example-ca-certificate\u0026gt; path: /usr/local/share/ca-certificates/example-ca.crt owner: root:root permissions: \u0026#39;0644\u0026#39; # Onboarding 配置 - content: | apiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: \u0026lt;ONBOARDING_ENDPOINT_ADDRESS\u0026gt; transportSecurity: tls: sni: onboarding-endpoint.example workloadGroup: namespace: bookinfo name: ratings workload: labels: version: v5 settings: connectedOver: INTERNET path: /etc/onboarding-agent/onboarding.config.yaml owner: root:root permissions: \u0026#39;0644\u0026#39; runcmd: - | #!/usr/bin/env bash set -ex # 安装最新版本的受信任 CA 证书 sudo apt-get update -y sudo apt-get install -y ca-certificates # 信任自定义 CA 的证书 sudo update-ca-certificates # 安装 Bookinfo ratings 应用程序 curl --fail --silent --location https://deb.nodesource.com/setup_14.x | sudo bash - sudo apt-get install -y nodejs curl -fLO https://dl.cloudsmith.io/public/tetrate/onboarding-examples/raw/files/bookinfo-ratings.deb sudo apt-get install -y ./bookinfo-ratings.deb rm bookinfo-ratings.deb sudo systemctl enable bookinfo-ratings sudo systemctl start bookinfo-ratings ONBOARDING_ENDPOINT_ADDRESS=\u0026lt;ONBOARDING_ENDPOINT_ADDRESS\u0026gt; # 安装 Istio Sidecar curl -fLO \\ --connect-to \u0026#34;onboarding-endpoint.example:443:${ONBOARDING_ENDPOINT_ADDRESS}:443\u0026#34; \\ \u0026#34;https://onboarding-endpoint.example/install/deb/amd64/istio-sidecar.deb\u0026#34; curl -fLO \\ --connect-to \u0026#34;onboarding-endpoint.example:443:${ONBOARDING_ENDPOINT_ADDRESS}:443\u0026#34; \\ \u0026#34;https://onboarding-endpoint.example/install/deb/amd64/istio-sidecar.deb.sha256\u0026#34; sha256sum --check istio-sidecar.deb.sha256 sudo apt-get install -y ./istio-sidecar.deb rm istio-sidecar.deb istio-sidecar.deb.sha256 # 安装工作负载 Onboarding Agent curl -fLO \\ --connect-to \u0026#34;onboarding-endpoint.example:443:${ONBOARDING_ENDPOINT_ADDRESS}:443\u0026#34; \\ \u0026#34;https://onboarding-endpoint.example/install/deb/amd64/onboarding-agent.deb\u0026#34; curl -fLO \\ --connect-to \u0026#34;onboarding-endpoint.example:443:${ONBOARDING_ENDPOINT_ADDRESS}:443\u0026#34; \\ \u0026#34;https://onboarding-endpoint.example/install/deb/amd64/onboarding-agent.deb.sha256\u0026#34; sha256sum --check onboarding-agent.deb.sha256 sudo apt-get install -y ./onboarding-agent.deb rm onboarding-agent.deb onboarding-agent.deb.sha256 sudo systemctl enable onboarding-agent sudo systemctl start onboarding-agent 一旦将数据与 Auto Scaling Group 的用户数据相关联，请尝试扩展和缩小 Auto Scaling Group，并验证工作负载是否已正确加入 。\n","relpermalink":"/book/tsb/setup/workload-onboarding/quickstart/aws-ec2/onboard-asg/","summary":"要将部署在 AWS Auto Scaling Group（ASG）上的工作负载加入，你需要在实例启动脚本 中执行所有设置操作，而不是在 EC2 实例上执行命令。 简而言之，你需要将先前步骤中的设置命令移到与 Auto Scaling Group 中的实例关联的 cloud-init 配置中。 具体来","title":"从 AWS Auto Scaling Group 上加入工作负载"},{"content":"一旦你配置了外部速率限制服务器 ，你可能希望确保与速率限制服务的通信安全。TSB 支持指定TLS 或 mTLS 参数来确保与外部速率限制服务器的通信的安全性。本文档将向你展示如何通过将 CA 证书添加到速率限制配置中来为外部速率限制服务器配置 TLS 验证。\n在开始之前，请确保你已完成以下准备工作：\n熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成 TSB 使用快速入门 。本文假定你已经创建了租户，并熟悉工作空间和配置组。还需要配置 tctl 到你的 TSB 环境。 完成了 设置外部速率限制服务器 。本文将继续你在设置外部速率限制服务器中所做的工作。你将在 ext-ratelimit 命名空间中工作，并且应该已经正确配置了带有外部速率限制的 Ingress 网关。 TLS 证书 要启用 Ingress 网关到速率限制服务的 TLS，请确保你拥有一个 TLS 证书。本文假定你已经拥有 TLS 证书，通常包括服务器证书和私钥，以及客户端将使用的 CA 作为根证书。\n本文假定以下文件已存在。如果你使用不同的文件名，请相应地更改它们：\n文件名 描述 ratelimit.crt 服务器证书 ratelimit.key 证书私钥 ratelimit-ca.crt CA 证书 :::注意 自签名证书 为了示例的目的，你可以选择使用自签名证书。 你可以使用这里显示的脚本 生成自签名证书，但请确保根据需要调整输入参数。 :::注意\n一旦你拥有证书文件，请使用服务器证书和私钥创建 Kubernetes 密钥对。\nkubectl create secret tls -n ext-ratelimit ratelimit-certs \\ --cert=ratelimit.crt \\ --key=ratelimit.key 使用 TLS 证书部署速率限制服务 在本示例中，你将使用 Envoy 速率限制服务。Envoy 代理 sidecar 作为透明代理，将在将请求发送到速率限制服务之前验证并终止 TLS。\n创建一个包含以下内容的 Envoy 配置文件，将其命名为 proxy-config-tls.yaml 执行以下命令将配置存储到 Kubernetes 中作为 ConfigMap。\nkubectl create configmap -n ext-ratelimit ratelimit-proxy \\ --from-file=proxy-config-tls.yaml 你需要使用 Envoy sidecar 部署速率限制服务以终止 TLS。创建一个名为 ratelimit-tls.yaml 的文件，其内容如下。\n然后使用 kubectl 应用此文件：\nkubectl apply -f ratelimit-tls.yaml 一旦应用了新配置，请确保 ratelimit-tls 服务正常运行。 请注意，如果你遵循了设置外部速率限制服务器 的说明，你还将看到 ratelimit 和 redis 服务。\nkubectl get pods -n ext-ratelimit NAME READY STATUS RESTARTS AGE ratelimit-d5c5b64ff-m87dt 1/1 Running 0 2h ratelimit-tls-568c5cdc69-z82xf 2/2 Running 0 89s redis-7d757c948f-42sxg 1/1 Running 0 2h 在 Ingress 网关中启用速率限制服务器的 TLS 验证 ratelimit-tls 服务现在可以终止 TLS，但是 Ingress 网关也必须配置以验证 TLS 连接。\n首先，创建一个名为 ratelimit-ca 的 ConfigMap 来存储来自 ratelimit-ca.crt 的 CA 信息：\nkubectl create configmap -n httpbin ratelimit-ca \\ --from-file=ratelimit-ca.crt 然后将 ratelimit-ca ConfigMap 添加到 Ingress 网关 pod 中。为此，你需要编辑 httpbin-ingress-gateway.yaml 文件 并添加一个读取你在先前步骤中创建的 ConfigMap 的覆盖层，然后将配置挂载到 Ingress 网关部署中。\n使用 kubectl 应用以更新现有的 Ingress 网关\nkubectl apply -f httpbin-ingress-gateway.yaml 最后，更新 ext-ratelimit-ingress-gateway.yaml 中的 Ingress 网关配置，并启用 TLS 验证：\n使用 tctl 应用\ntctl apply -f ext-ratelimit-ingress-gateway-tls.yaml 测试 要验证设置是否正常工作，你可以使用与“设置外部速率限制服务器的测试步骤” 中显示的相同的测试步骤。\n","relpermalink":"/book/tsb/howto/rate-limiting/tls-validation/","summary":"一旦你配置了外部速率限制服务器 ，你可能希望确保与速率限制服务的通信安全。TSB 支持指定TLS 或 mTLS 参数来确保与外部速率限制服务器的通信的安全性。本文档将向你展示如何通过将 CA 证书添加到速率限制配置中来为外","title":"带 TLS 验证的外部速率限制"},{"content":"Tetrate 使应用所有者能够轻松地在多个集群中部署应用程序。可以在选择的工作区中部署东西向网关，并将一些或所有服务暴露给其他集群。Tetrate 的中央服务注册表用于在其他集群中为这些暴露的服务创建虚拟端点（ServiceEntries），以便客户端可以发现和使用这些服务，就像它们在本地运行一样。\n这项功能可用于：\n在不同的集群中分发应用程序的组件 从远程集群访问集中式共享服务，如数据库微服务 在不同集群之间为服务实例提供故障切换 通过确保相同的应用程序和寻址方案在一个集群中或分布在多个集群中，简化测试 平台所有者（“Platform”）为故障转移案例准备了平台，应用程序所有者（“Apps”）可以在对其工作流程进行最少修改的情况下利用这些功能。\n平台所有者将按以下方式准备平台：\n部署东西向网关 在包含要共享的服务的命名空间中部署东西向网关。\n更新工作区以暴露所需的服务 更新工作区以暴露所需的服务，以便它们可以在其他集群中被发现和使用。\n为内部故障切换部署东西向网关 在其他集群中部署额外的东西向网关以进行服务故障切换。 然后，应用程序所有者可以：\n访问已暴露的服务 从其他集群中访问已暴露的服务。\n在集群之间进行故障切换 验证服务能否从一个集群故障转移到另一个集群中的备份实例。\n平台：开始之前 在开始之前，你需要知道：\n共享服务所在的工作区 应暴露哪些服务；默认情况下，工作区中的所有服务都会被暴露 你还可以查看 TSE 入门用例：高可用性 和 跨集群通信 。\n平台：部署东西向网关 在包含要共享服务的命名空间中部署一个东西向网关：\ncat \u0026lt;\u0026lt;EOF \u0026gt; eastwest-gateway.yaml apiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: eastwest-gateway namespace: bookinfo spec: eastWestOnly: true EOF kubectl apply -f eastwest-gateway.yaml 列出在本地集群中运行的东西向网关：\nkubectl get pods -A -l app=eastwest-gateway 平台：更新工作区以暴露所需的服务 更新与工作区相关联的 WorkspaceSettings：\napiVersion: settings.tetrate.io/v1alpha1 kind: WorkspaceSetting metadata: name: my-workspace-setting namespace: bookinfo spec: workspace: my-workspace defaultEastWestGatewaySettings: workloadSelector: app: eastwest-gateway exposedServices: - \u0026#34;details.bookinfo.svc.cluster.local\u0026#34; - \u0026#34;reviews.bookinfo.svc.cluster.local\u0026#34; 请注意，defaultEastWestGatewaySettings 使用 workloadSelector 识别东西向网关，并具有一个可选的 exposedServices 部分。\n应用程序所有者可以从远程集群中发现和使用这些共享服务。它们作为 xcp-multicluster 命名空间中的 ServiceEntries 暴露，并且使用与第一个集群中相同的 FQDN：\nkubectl get serviceentry -n xcp-multicluster 请注意，如果应用程序所有者在远程集群中部署了相同的服务，那么 Tetrate 控制平台将删除 ServiceEntries，以便该集群中的客户端更喜欢在同一集群中的服务实例。\n应用程序：访问已暴露的服务 一旦在源集群中部署了东西向网关，并使用 WorkspaceSettings:defaultEastWestGatewaySettings 选择了要暴露的服务，就可以在源集群中部署服务。匹配的服务将在远程集群中暴露出来。\n你可以通过以下方式在远程集群中查看暴露服务的列表如下：\nkubectl get serviceentry -n xcp-multicluster 这些服务的完全限定域名（FQDN）在远程集群中是可寻址的，这意味着远程集群中的任何客户端服务都可以访问原始服务，无需进行任何修改。Tetrate 平台会保持ServiceEntries与原始服务的存在和状态同步。\n平台：为内部故障切换部署东西向网关 在这种情况下，我们将准备平台以便在一个集群与另一个集群之间进行内部服务的故障转移。例如，一个应用所有者可以将一个多组件的应用程序（如 bookinfo）部署到两个集群中。每个应用程序将使用本地版本的依赖服务，除非本地实例失败，在这种情况下，Tetrate 平台将自动将流量切换到另一个集群。应用所有者不需要进行任何应用程序修改。\n先决条件 必须在每个工作负载集群中创建应用程序的命名空间。 Tetrate 工作区必须包括这些命名空间，并且必须跨足每个工作负载集群。 例如，如果要在命名空间 bookinfo 中部署应用程序，那么这个命名空间必须在每个集群中存在，并且 Workspace 配置应引用所有实例，例如，使用以下 namespaceSelector：\nspec: namespaceSelector: names: - \u0026#34;*/bookinfo\u0026#34; 部署一个东西向网关 在其他集群中部署额外的东西向网关，与之前完全相同：\ncat \u0026lt;\u0026lt;EOF \u0026gt; eastwest-gateway.yaml apiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: eastwest-gateway namespace: bookinfo spec: eastWestOnly: true EOF kubectl apply -f eastwest-gateway.yaml 应用程序：在集群之间测试故障转移 通过上述配置，你可以将应用程序部署到多个集群中。如果一个集群中的服务实例因任何原因失败，Tetrate 平台将检测到并将流量透明地切换到远程工作正常的服务实例。\nTetrate 平台为集群中的每个远程服务实例创建了影子 WorkloadEntries，这些实例存在于本地实例所在的集群和命名空间中。例如，如果你将 bookinfo 应用程序部署到两个配置有高可用性的集群中，然后你可以检查每个集群以查看影子 WorkloadEntries 是否存在。\n在下面的案例中，我们只暴露了 details 和 reviews 服务，并且云平台在三个 IP 地址上暴露了 eastwest-gateway：\nkubectl get workloadentries -n bookinfo # NAME AGE ADDRESS # k-details-fc556d47e94d1cb435e513fa016c2243 17m 18.135.167.198 # k-details-fc556d47e94d1cb435e513fa016c2243-2 17m 18.168.99.230 # k-details-fc556d47e94d1cb435e513fa016c2243-3 17m 35.179.51.164 # k-reviews-3ab8d1334c8f22513cd591f84c978f88 17m 18.135.167.198 # k-reviews-3ab8d1334c8f22513cd591f84c978f88-2 17m 18.168.99.230 # k-reviews-3ab8d1334c8f22513cd591f84c978f88-3 17m 35.179.51.164 kubectl get svc -n bookinfo eastwest-gateway # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # eastwest-gateway LoadBalancer 10.100.17.100 k8s-bookinfo-eastwest-00a17af379-bdda7f4eb8c5da5c.elb.eu-west-2.amazonaws.com 15443:30082/TCP 82m 要了解更多信息，请查看 TSE 入门练习 在集群之间进行故障切换 。\n","relpermalink":"/book/tsb/design-guides/app-onboarding/cross-cluster/","summary":"Tetrate 使应用所有者能够轻松地在多个集群中部署应用程序。可以在选择的工作区中部署东西向网关，并将一些或所有服务暴露给其他集群。Tetrate 的中央服务注册表用于在其他集群中为这些暴露的服务创建虚拟端点（Se","title":"集群之间的连接与故障切换"},{"content":"本操作指南将向你展示如何对新的示例服务进行金丝雀发布。你将学习如何在 TSB 中部署和注册服务，以及如何调整其设置以遵循金丝雀部署过程。\n你将创建一个工作区和你需要注册应用程序的组 通过应用程序入口网关公开应用程序 执行金丝雀发布。 在开始之前，请确保：\n你已经启动并运行了一个 TSB 管理平面。 你已经配置了 tctl 以与 TSB 管理平面通信。 你将要部署应用程序的集群正在运行一个 TSB 控制平面，并且已经正确注册到 TSB 管理平面。 本指南使用一个hello world应用程序，如果你将其用于生产，请根据你的应用程序的正确信息更新相关字段。\n开始 以下 YAML 文件包含三个对象 - 用于应用程序的工作区、用于配置应用程序入口的网关组以及用于配置金丝雀发布过程的流量组。将其存储为ws-groups.yaml 。\nws-groups.yaml apiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: name: helloworld organization: tetrate tenant: tetrate spec: namespaceSelector: names: - \u0026#39;*/helloworld\u0026#39; --- apiVersion: traffic.tsb.tetrate.io/v2 kind: Group metadata: name: helloworld-traffic workspace: helloworld organization: tetrate tenant: tetrate spec: namespaceSelector: names: - \u0026#39;*/helloworld\u0026#39; --- apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: name: helloworld-gateway workspace: helloworld organization: tetrate tenant: tetrate spec: namespaceSelector: names: - \u0026#39;*/helloworld\u0026#39; 使用 tctl 应用：\ntctl apply -f ws-groups.yaml 要部署你的应用程序，首先创建命名空间并启用 Istio sidecar 注入。\nkubectl create namespace helloworld kubectl label namespace helloworld istio-injection=enabled 然后部署你的应用程序。\n将文件存储为helloworld.yaml ，并使用kubectl应用：\nhelloworld.yaml apiVersion: apps/v1 kind: Deployment metadata: name: helloworld-v1 namespace: helloworld spec: replicas: 1 selector: matchLabels: app: helloworld version: v1 template: metadata: labels: app: helloworld version: v1 spec: containers: - name: hello image: \u0026#39;gcr.io/google-samples/hello-app:1.0\u0026#39; env: - name: \u0026#39;PORT\u0026#39; value: \u0026#39;8080\u0026#39; --- apiVersion: v1 kind: Service metadata: name: helloworld namespace: helloworld spec: selector: app: helloworld ports: - protocol: TCP port: 443 targetPort: 8080 kubectl apply -f helloworld.yaml 在继续之前，你应确保没有流量被意外地定向到应用程序的任何新版本。然后，在你之前创建的流量组中创建一个ServiceRoute，以便所有helloworld流量仅发送到版本v1。\n将文件存储为serviceroute.yaml，并使用tctl应用：\nserviceroute.yaml apiVersion: traffic.tsb.tetrate.io/v2 kind: ServiceRoute metadata: name: helloworld-canary group: helloworld-traffic workspace: helloworld organization: tetrate tenant: tetrate spec: service: helloworld/helloworld.helloworld.svc.cluster.local subsets: - name: v1 labels: version: v1 weight: 100 tctl apply -f serviceroute.yaml 太棒了！现在你需要让你的应用程序对外可访问。你需要为你的应用程序部署一个入口网关，并配置它将传入的流量路由到我们的应用程序服务。\n在这个示例中，你将使用网关的简单 TLS 公开应用程序。你需要为它提供存储在 Kubernetes 秘密中的 TLS 证书。\nkubectl create secret tls -n helloworld helloworld-certs \\ --cert /path/to/some/helloworld-cert.pem \\ --key /path/to/some/helloworld-key.pem 现在你可以部署你的入口网关。\n将文件存储为hello-ingress.yaml ，并使用kubectl应用：\nhello-ingress.yaml apiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: tsb-gateway-helloworld namespace: helloworld spec: kubeSpec: service: type: LoadBalancer kubectl apply -f hello-ingress.yaml 集群中的 TSB 数据平面 Operator 将获取此配置并在你的应用程序命名空间中部署网关的资源。现在所剩的就是配置网关，以便将流量路由到你的应用程序。\n将文件存储为helloworld-gateway.yaml ，并使用tctl应用：\nhelloworld-gateway.yaml apiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: name: helloworld-ingress group: helloworld-gateway workspace: helloworld organization: tetrate tenant: tetrate spec: workloadSelector: namespace: helloworld labels: app: tsb-gateway-helloworld http: - name: helloworld port: 443 hostname: helloworld.tetrate.com tls: mode: SIMPLE secretName: helloworld-certs routing: rules: - route: host: helloworld/helloworld.helloworld.svc.cluster.local tctl apply -f helloworld-gateway.yaml 此时，你可以通过向网关服务 IP 发送helloworld.tetrate.com的 HTTPS 请求来验证你的应用程序是否可访问。\ncurl -k -s --connect-to helloworld.tetrate.com:443:$GATEWAY_IP \u0026#34;https://helloworld.tetrate.com/\u0026#34; 现在，你的应用程序正在运行并提供服务请求，部署新版本的应用程序。\n将文件存储为helloworld-v2.yaml ，并使用kubectl应用：\nhelloworld-v2.yaml ```yaml apiVersion: apps/v1 kind: Deployment metadata: name: helloworld-v2 namespace: helloworld spec: replicas: 1 selector: matchLabels: app: helloworld version: v2 template: metadata: labels: app: helloworld version: v2 spec: containers: - name: hello image: \u0026#39;gcr.io/google-samples/hello-app:2.0\u0026#39; env: - name: \u0026#39;PORT\u0026#39; value: \u0026#39;8080\u0026#39; ``` kubectl apply -f helloworld-v2.yaml 由于你创建了一个针对所有流量发送到版本v1的服务路由。在此时，版本v2将不会收到任何请求。通过修改服务路由，以将 80% 的流量发送到我们已知的稳定版本v1，并将 20% 的流量发送到版本v2，开始进行金丝雀发布。\n将文件存储为serviceroute-20.yaml ，并使用tctl应用：\nserviceroute-20.yaml apiVersion: traffic.tsb.tetrate.io/v2 kind: ServiceRoute metadata: name: helloworld-canary group: helloworld-traffic workspace: helloworld organization: tetrate tenant: tetrate spec: service: helloworld/helloworld.helloworld.svc.cluster.local subsets: - name: v1 labels: version: v1 weight: 80 - name: v2 labels: version: v2 weight: 20 tctl apply -f serviceroute-20.yaml 如果你不断使用 Web 浏览器刷新你的应用程序，你会看到大多数请求到达旧的v1版本。其他请求将显示新v2版本的输出。要完成金丝雀发布，你需要重复此最后一步，直到所有流量都发送到新版本并得到改进（或者如果你发现新版本有问题，可以撤销并将所有流量发送回版本v1）。简单！\n","relpermalink":"/book/tsb/howto/traffic/canary-releases/","summary":"使用 TSB 进行金丝雀发布的指南。","title":"金丝雀发布"},{"content":"Tetrate 允许你创建一个明确定义、准确可持续发展的安全策略，可以与你的 Tetrate 管理的系统平滑地同步发展。\n平台所有者（“Platform”）和应用程序所有者（“Apps”）共同合作创建你的安全策略的元素：\n识别安全阻塞 应用程序所有者识别当前 Tetrate 安全策略阻止的必需流量。\n应用安全规则 平台所有者验证所需的安全例外，并使用 Tetrate API 实施它。\n审计安全规则 定期，平台所有者可能希望审计当前的安全策略，以确保它们足以满足安全和合规性需求。\n平台：开始之前 本指南将涵盖在零信任环境中的工作负载之间配置访问控制。工作负载通过 Istio 提供的 Tetrate 平台发放和更新 SPIFFE 标识进行身份验证。本指南不涵盖 JWT、OAuth 或 OIDC 用户身份验证等更高级别的功能。\n指南从推荐的起始姿态开始：\n所有工作负载都使用 SPIFFE 标识，并且对于所有流量，都需要 mTLS。这意味着外部第三方（如集群中的其他服务或具有对数据路径的访问权限的服务）无法读取事务、修改事务或冒充客户端或服务。 默认情况下，所有通信都被拒绝，只有解锁了工作区，并且内部流量是允许的。 默认的安全传播策略是 REPLACE 你需要了解的内容 使用 SecuritySetting 部分配置 Tetrate 安全姿态。这些部分以资源的层次结构呈现：\n组织级别的 OrganizationSetting/defaultSecuritySetting 租户级别的 TenantSetting/defaultSecuritySetting 。一个组织可以包含多个租户。 每个工作区级别的 WorkspaceSetting/defaultSecuritySetting 。一个租户包含多个工作区。 每个安全组级别的 SecuritySetting 。安全组允许将工作区细分为更小的命名空间集合。 每个服务级别的 ServiceSecuritySetting 。在安全组内，可以为单个服务应用规则。 在每个资源中，你可以配置一个 SecuritySetting 部分。\n传播策略 有关更多信息，请参阅传播策略 文档。\n常见做法 在配置安全策略时，有两种常见做法：\n自上而下，开放工作区\n首先定义高级别的默认值：\n在 OrganizationSetting/defaultSecuritySetting 中设置 denyAll，并将 propagationStrategy 设置为 REPLACE 在 WorkspaceSetting/defaultSecuritySetting 中设置 mode: WORKSPACE 以允许工作区之间的通信， 自下而上，细粒度规则\n定义初始姿态：\n不要在组织级别启用 ‘Deny-All’，但是将 propagationStrategy 设置为 STRICTER。 一旦在较低级别定义了 Allow 规则，那些不匹配该规则的请求将被拒绝，然后将这个拒绝传播到默认情况下拒绝所有其他请求的层次结构中。然后，你可以维护一个显式的 Allow 规则列表，知道其他所有东西都将被拒绝。\n这种方法更难以管理，因为你需要声明每个 Allow 规则，但它提供了更严格的安全性。\n应用程序：识别安全阻塞 当你希望访问目标服务时，你需要知道该服务的 FQDN（全限定域名）。通常情况下，这会在 Tetrate 服务注册表中列出，但也可能以其他形式存在，例如 ServiceEntry。服务所有者应该能够提供 FQDN。\n在调试访问控制问题时，你可以从客户端容器向目标服务发出简单的 HTTP 请求开始：\nCLIENT=$(kubectl get pod -n bookinfo -l app=ratings -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) kubectl exec $CLIENT -n bookinfo -c ratings -- \\ curl -s productpage.bookinfo.svc.cluster.local:9080/productpage # 如果 Tetrate 策略拒绝访问，则期望响应为 \u0026#39;RBAC: access denied\u0026#39; 如果客户端的 curl 命令得到文本响应 RBAC: access denied，则表示 Tetrate 平台配置正在阻止访问。其他错误是由其他（非访问控制）原因引起的。\n定义安全策略 安全策略是从目标服务的角度定义的；它们定义了哪些客户端被允许访问目标。目标服务的所有者应该要求平台所有者向阻止访问的 “拒绝访问” 安全策略中添加一个适当的例外，以便客户端可以访问目标服务。\n对于客户端和目标服务都是如此，平台所有者需要知道：\n服务 名称 服务 serviceAccount（kubectl get pod -n bookinfo podname -o jsonpath=\u0026#34;{.spec.serviceAccount}\u0026#34;)) 服务所在的 命名空间 和 集群 Tetrate 工作区 平台所有者将根据其安全实践确定适当的安全规则来打开所请求的流量。安全规则可以基于服务到服务、命名空间到命名空间、工作区到工作区或各种组合。\n平台：应用安全规则 安全规则是从目标服务的角度进行配置的。它们可以基于工作区中的安全组，也可以是单独的 Kubernetes 服务账户。Tetrate 平台会积累这些规则，遵循每个级别的 propagationStrategy，并生成完整的安全策略。该策略使用集群内和集群之间的 Istio 配置来实施。\n在构建规则时，请考虑以下两个问题：\n目标是什么？ 如何定义规则的位置 工作区中的所有命名空间 更新 WorkspaceSetting/defaultSecuritySetting 中的 authorization 部分。或者，你可能希望将 WorkspaceSetting/defaultSecuritySetting 视为不可变的，并在整个工作区范围内的 安全组 中进行更改 工作区中的某些命名空间 为目标创建一个适当的 安全组，并在附加到该 **安全 组** 的 SecuritySetting 中更新 authorization 部分 | | 单个命名的服务 | 为该服务创建一个 ServiceSecuritySetting，附加到工作区中的适当 安全组。在 ServiceSecuritySetting 中更新 authorization 部分 |\n来源是什么？ 如何定义规则 同一命名空间 将 AuthorizationSettings.mode 设置为 NAMESPACE 同一安全组 将 AuthorizationSettings.mode 设置为 GROUP 同一工作区 将 AuthorizationSettings.mode 设置为 WORKSPACE 同一集群 将 AuthorizationSettings.mode 设置为 CLUSTER 具名的 Kubernetes 服务账户 将 AuthorizationSettings.mode 设置为 CUSTOM，并列出服务账户 更加细粒度的控制 将 AuthorizationSettings.mode 设置为 RULES，并提供 allow 和 deny 的工作区和安全组列表 首先查看 SecuritySetting 文档 获取一组实际示例和更详细的文档。\n在 DIRECT 模式中使用 Istio 原语 对于高级情况，也可以直接使用 Istio 原语定义安全策略。要执行此操作：\n配置 Security Group 使用 configMode: DIRECT。这意味着 Tetrate SecuritySettings API 无法应用于此组 创建 PeerAuthentication 和 AuthorizationPolicy Istio Security v1beta1 资源，并使用注释将其附加到 Security Group Tetrate 管理平台将会将这些 DIRECT 模式配置与其他 BRIDGED 模式配置协调一致，并生成适当的数据平面配置。\n平台：审计安全规则 定期地，你可能希望审计 Tetrate 实施的安全规则。你可以使用几种行业标准工具来可视化规则：\nKiali 可以检查和可视化生成的 Istio 配置 Tetrate 平台可以生成源自平台所有者配置的分层访问控制策略的 L3/4 网络策略。然后，可以使用各种第三方 Kubernetes 网络策略可视化工具来检查和可视化这些 L3/4 Kubernetes 网络策略。 ","relpermalink":"/book/tsb/design-guides/app-onboarding/security/","summary":"Tetrate 允许你创建一个明确定义、准确可持续发展的安全策略，可以与你的 Tetrate 管理的系统平滑地同步发展。 平台所有者（“Platform”）和应用程序所有者（“Apps\u0026rdquo","title":"扩展安全策略"},{"content":"Tetrate Service Bridge (TSB) 采用结构化数据流机制来确保配置更改和更新在整个服务网格基础设施中高效、准确地传播。这个复杂的过程涉及各种组件，包括管理平面、全局控制平面（XCP Central）和本地控制平面（XCP Edge），每个组件在配置生命周期中都发挥着关键作用。\n管理平面 TSB 中的所有配置更改均源自管理平面。用户通过各种接口与 TSB 配置交互，例如 gRPC API、TSB UI 和 tctl 命令行界面。配置更改随后会保存在数据库中，作为整个系统的事实来源。管理平面将这些更改推送到 XCP Central 以便进一步分发。\nMPC 组件 由于遗留原因，XCP Central 通过 Kubernetes CRD 接收其配置。名为“MPC”的 shim 服务器建立到 TSB 的 API 服务器的 gRPC 流，以接收配置并将相应的 CR 推送到托管 XCP Central 的 Kubernetes 集群中。MPC 还会从 XCP Central 向 TSB 发送系统运行时状态的报告，以帮助用户管理网格。\n即将发布的 TSB 版本将删除该组件，TSB 的 API Server 和 XCP Central 将直接通过 gRPC 进行通信。\n全局控制平面 - XCP Central XCP Central 充当应用程序集群中管理平面和本地控制平面之间的中介。它处理运行时配置、服务发现信息和管理元数据的分发。这种通信通过 gRPC 流进行，从而实现 XCP Central 和 XCP Edge 实例之间的双向交互。XCP Central 发送新的用户配置，而 XCP Edge 报告服务发现更改和管理数据。XCP Central 还将其本地状态的快照存储为其运行的集群中的 Kubernetes 自定义资源 (CR)。\nXCP Central Data Store 如今，XCP Central 将其本地状态的快照作为 Kubernetes CR 存储在其部署的集群中。当 XCP Central 无法连接到管理平面并且 XCP Central 本身需要重新启动（即无法使用内存缓存）时，将使用此方法。\n在未来版本中，当 XCP Central 通过 gRPC 直接从 TSB 接收其配置时，XCP Central 会将其配置保存在类似于管理平面的数据库中。\n本地控制平面 - XCP Edge XCP Edge 负责将从 XCP Central 接收到的配置转换为特定于本地集群的本机 Istio 对象。它将这些配置发布到 Kubernetes API 服务器中，Istio 在其中照常处理它们。XCP Edge 还管理跨网格的服务公开，有助于跨集群通信和功能。从 XCP Central 接收的配置信息存储在控制平面命名空间 ( istio-system ) 中，确保本地缓存在连接丢失时可用。\n详细的数据流 TSB 内的配置数据流可以概括为一系列步骤：\n用户通过 TSB UI、API 或 CLI 启动配置更改。 TSB API 服务器将配置存储在其数据库中。 TSB 将配置推送到 XCP Central。 XCP Central 通过 gRPC 将配置分发到 XCP Edge 实例。 XCP Edge 将传入配置存储在控制平面命名空间 ( istio-system ) 中。 XCP Edge 将配置转换为本机 Istio 对象。 Istio 处理配置并将其部署到 Envoy。 此外，服务发现信息的管理如下：\nXCP Edge 将服务发现更新发送到 XCP Central。 XCP Central 将集群状态信息传播到 XCP Edge 实例。 如果需要，XCP Edge 会更新多集群命名空间配置 ( xcp-multicluster )。 Istio 处理配置并将其部署到 Envoy。 与 GitOps 集成 TSB 的结构化配置数据流可以无缝集成到 GitOps 工作流程中。这种集成通过两个主要场景进行：\n从 CI/CD 接收配置：TSB 可以从 CI/CD 系统接收配置更新，该系统在 Git 存储库中维护事实来源。 管理平面提交到 Git：在未来的版本中，TSB 的管理平面将能够将其配置更改直接提交到 Git，与 GitOps 方法保持一致。 这两种场景都可以在 TSB 生态系统内实现高效的配置管理，从而增强服务网格基础设施的可靠性和可维护性。\n","relpermalink":"/book/tsb/concepts/configuration-dataflow/","summary":"配置在 TSB 中的数据流。","title":"配置的数据流"},{"content":"在本部分中，你将了解如何使用 TSB 中的 AccessBindings 配置访问策略来管理不同团队和用户的权限。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户和工作区。 创建配置组。 授予团队对工作区的完全访问权限 你将配置一个访问策略，授予团队对工作区的完全访问权限。团队成员将能够创建并完全管理工作区中的资源，但无法修改工作区对象本身。这将通过分配 Creator 角色来实现。\n在左侧面板的“租户”下，选择“工作区”。 单击所需的工作区以访问其详细信息页面。 单击权限选项卡。 选择“按团队”选项可查看团队列表。 找到并单击所需团队右侧的编辑图标。 选择 Creator 角色。 单击右下角的保存更改按钮。 向组的用户授予写入权限 要向组的特定用户授予写入权限，请遵循类似的过程：\n导航到组的权限选项卡。 选择“按用户”选项可查看用户列表。 找到并单击所需用户旁边的编辑图标。 选择 Writer 角色。 单击右下角的保存更改按钮。 使用 tctl 你还可以通过应用 YAML 文件中定义的 AccessBindings 使用 tctl 实现相同的配置：\n使用工作区和流量组对象所需的 AccessBindings 创建以下 access-policy.yaml 文件：\napiVersion: rbac.tsb.tetrate.io/v2 kind: AccessBindings metadata: fqn: organizations/tetrate/tenants/tetrate/workspaces/bookinfo-ws spec: allow: - role: rbac/creator subjects: # Change the name of the team to the desired one - team: organizations/tetrate/teams/Platform --- apiVersion: rbac.tsb.tetrate.io/v2 kind: AccessBindings metadata: fqn: organizations/tetrate/tenants/tetrate/workspaces/bookinfo-ws/trafficgroups/bookinfo-traffic spec: allow: - role: rbac/writer subjects: # Change the name of the user to the desired one - user: organizations/tetrate/users/zack 使用 tctl 应用策略：\ntctl apply -f access-policy.yaml 通过执行这些步骤，你可以使用 AccessBindings 有效配置访问策略，以管理 TSB 环境中不同团队和用户的权限。\n","relpermalink":"/book/tsb/quickstart/permissions/","summary":"在本部分中，你将了解如何使用 TSB 中的 AccessBindings 配置访问策略来管理不同团队和用户的权限。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户和工作区。 创建配置组。","title":"配置权限"},{"content":" 事实来源 我们不建议在本文档中概述的部署架构：TSB 的单个实例设计为在所有环境（测试、QA、暂存和生产）中部署。最佳做法是在中心位置部署单个 TSB，并将所有环境指向该单个 TSB。Tetrate Service Bridge 内置的控制保持了你环境配置的隔离和安全性。 一些站点已经为每个环境部署了单独的 TSB 实例。本指南的存在是为了确保这些站点能够建立一个跨环境控制配置推广的流程，不依赖于 TSB 本身。\n配置推广注意事项和建议 TSB 配置主要由许多 Kubernetes 对象 组成，例如 Tenant 是 api.tsb.tetrate.io/v2 中的一个对象。\n因此，主要建议是将 TSB 配置视为任何其他 Kubernetes 资源定义，例如：\n声明性地定义资源。 使用 GitOps 进行资源应用。 使用 kustomize 或类似工具进行配置模板化和渲染。 在将一个 TSB 安装的资源应用到另一个 TSB 安装时的主要注意事项是在计算和评估 NGAC 访问规则、集群/服务参数等时 TSB 存储配置数据的方式。\nTetrate Service Bridge 使用持久存储（PostgreSQL）作为事实来源，并选择资源的完全限定名称作为其 主键。因此，在两个独立的 TSB 安装之间“推广”配置时，必须考虑配置资源命名，以避免 主键 冲突。\n换句话说，要在独立的实例之间可靠地推广配置，应该：\n在两个实例中将所有 TSB 资源的命名 完全相同，包括资源路径（即一个实例中具有相同名称的 Cluster 必须属于另一个实例中相同的 Tenant 等等）。 避免使用命令式的配置更改（例如通过 UI/CLI 在一个环境中添加一个 Cluster 而在另一个环境中没有），而是使用声明性的定义。 始终备份最近的持久存储数据，以便在数据模型冲突的情况下能够快速回滚。 ","relpermalink":"/book/tsb/operations/configuration-promotion/","summary":"在不同实例之间进行 TSB 配置推广。","title":"配置推广"},{"content":"本文将介绍在从 Istio 入口网关或边车转发到应用程序时，TSB 和 Istio 代理如何处理标头。\n在开始之前，请确保你已经：\n熟悉TSB 概念 安装了 TSB 环境。你可以使用TSB 演示 进行快速安装 完成了TSB 使用快速入门 。 安装了示例应用程序httpbin 。 Envoy（istio-proxy）中的请求标头大小 Envoy 或 istio-proxy 可以处理相当大的标头。传入连接的默认最大请求标头大小为 60 KiB。\n在这种情况下，对于大多数应用程序来说，这不会成为问题，并且传入连接的请求标头将通过 istio-proxy 代理。但是，根据每个 Web 服务器的标头大小配置，你的应用程序可能会有限制。\n例如： 在Spring Boot 2 和Gunicorn 中，默认的最大标头大小为 8 KiB。如果需要，你可以覆盖默认设置。\n调试请求标头大小 对于此实验，你需要在集群中部署httpbin 示例应用程序。你将执行两个请求，一个请求的标头大小低于最大值，另一个请求的标头大小超出应用程序容器的限制。\n低于最大值的标头 你的标头可以是任何内容，只需确保低于 8 KiB，你可以将其导出为变量并执行请求：\ncurl -k https://httpbin.example.io/response-headers -X POST -H \u0026#34;X-MyHeader: $SMALL\u0026#34; -sI HTTP/2 200 server: istio-envoy date: Wed, 19 Oct 2022 20:13:49 GMT content-type: application/json content-length: 68 access-control-allow-origin: * access-control-allow-credentials: true x-envoy-upstream-service-time: 5 超出最大值的标头 现在，使用可以超出 8 KiB 的标头执行请求：\ncurl -k https://httpbin.example.io/response-headers -X POST -H \u0026#34;X-MyHeader: $LONG\u0026#34; -sI HTTP/2 400 content-type: text/html content-length: 189 x-envoy-upstream-service-time: 6 date: Wed, 19 Oct 2022 20:17:37 GMT server: istio-envoy 如果请求标头超过最大标头大小，你将收到一个 HTTP 400 错误，表示坏请求。\n修改 istio-proxy 中的标头大小 正如你在上面学到的，你可以在各种 Web 服务器中限制标头大小。你可以在 istio-proxy 中进行相同的修改。\n默认的标头大小应该足够，或者你可能希望减小默认大小。\n在 istio-proxy 中减小默认标头大小 为了减小默认请求标头的大小，你需要创建一个Envoyfilter ，允许你修改 istio-proxy 的配置。\napiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: max-request-headers namespace: istio-system spec: configPatches: - applyTo: NETWORK_FILTER # http connection manager is a filter in Envoy match: context: ANY listener: filterChain: filter: name: \u0026#34;envoy.filters.network.http_connection_manager\u0026#34; patch: operation: MERGE value: typed_config: \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\u0026#34; max_request_headers_kb: 10 将此应用于你的集群后，请再次尝试请求，一个标头较小，另一个标头较大。\n在 istio-proxy 中低于最大值的标头 curl -k https://httpbin.example.io/response-headers -X POST -H \u0026#34;X-MyHeader: $SMALL\u0026#34; -sI HTTP/2 200 server: istio-envoy date: Wed, 19 Oct 2022 20:36:43 GMT content-type: application/json content-length: 68 access-control-allow-origin: * access-control-allow-credentials: true x-envoy-upstream-service-time: 5 由于标头低于 10 KiB 的最大值，你可以看到请求成功。\n在 istio-proxy 中超出最大值的标头 curl -k https://httpbin.example.io/response-headers -X POST -H \u0026#34;X-MyHeader: $LONG\u0026#34; -sI 你可以从 curl 中删除-s 标志并查看输出。\ncurl -k https://httpbin.example.io/response-headers -X POST -H \u0026#34;X-MyHeader: $LONG\u0026#34; -I curl: (92) HTTP/2 stream 0 was not closed cleanly: INTERNAL_ERROR (err 2) 请求没有返回任何内容，只有一个错误。你可以在日志中查看发生了什么。\nkubectl logs $GWPOD -n tier1 [2022-10-19T20:39:58.081Z] \u0026#34;- - HTTP/2\u0026#34; 0 - http2.too_many_headers - \u0026#34;-\u0026#34; 0 0 0 - \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; - - 10.211.129.34: 8443 10.240.0.38:63077 httpbin.example.io - ","relpermalink":"/book/tsb/troubleshooting/maximum-header-size-exceed/","summary":"本文将介绍在从 Istio 入口网关或边车转发到应用程序时，TSB 和 Istio 代理如何处理标头。 在开始之前，请确保你已经： 熟悉TSB 概念 安装了 TSB 环境。你可以使用TSB 演示 进行快速安装 完成了TSB 使用快速入门 。 安装了示例应","title":"请求头大小超限"},{"content":"Tier-2 网关或 IngressGateway 配置工作负载以充当进入网格的流量网关。入口网关还提供基本的 API 网关功能，如 JWT 令牌验证和请求授权。\n在本指南中，你将会：\n部署 bookinfo 应用程序 分为两个不同的集群，配置为 Tier-2，一个集群中有 productpage，另一个集群中有 reviews、details 和 rating。 在开始之前，请确保你已经在 cluster 1 中部署了 productpage，并在 cluster 2 中部署了 details、ratings 和 reviews。对于此演示，我们假设你已经在 TSB 中部署和配置了 Bookinfo。\n所有组件中的控制平面都需要共享相同的信任根 。 场景 在这个场景中，我们将配置两个配置为 Tier-2 的控制平面集群（tsb-tier2gcp1 和 tsb-tier2gcp2）。我们将在两个 Tier-2 集群中都部署 Bookinfo，tsb-tier2gcp1 中将安装 productpage，而 tsb-tier2gcp2 中将安装 reviews、details 和 ratings。\n因此，Tier-2 集群的场景（一旦配置完成）应如下所示：\n请确保两个集群都共享相同的信任根。在部署两个集群的控制平面之前，你必须填充正确的证书到 cacerts 中。有关更多详细信息，请参阅 Istio 文档中的 Plugin CA 证书 。\n配置 配置 TSB 对象 在此示例中，假定你已经有一个名为 tetrate 的组织、一个名为 test 的租户，以及已经配置了 Tier-2 网关的两个控制平面集群。\n首先，创建工作区和网关组：\napiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tetrate tenant: test name: bookinfo spec: displayName: bookinfo app description: Workspace for the bookinfo app namespaceSelector: names: - \u0026#34;*/bookinfo-front\u0026#34; - \u0026#34;*/bookinfo-back\u0026#34; --- apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: organization: tetrate tenant: test workspace: bookinfo name: bookinfo-gw spec: configMode: BRIDGED namespaceSelector: names: - \u0026#34;*/bookinfo-front\u0026#34; - \u0026#34;*/bookinfo-back\u0026#34; 然后应用它：\ntctl apply -f mgmt-bookinfo.yaml 在上面的示例中，使用通配符 (\u0026#34;\u0026#34;) 表示选择所有已登记集群中的 bookinfo-front 和 bookinfo-back 命名空间。如果你想要针对特定集群进行目标定位，可以将 “” 替换为你希望使用的集群名称。\n部署入口网关 现在，如果命名空间尚未创建，我们将在两者中创建它们并在两者中启用 Sidecar 注入。在 tsb-tier2gcp1 中，我们将创建 bookinfo-front 命名空间并部署 productpage，在 tsb-tier2gcp2 中，我们将创建 bookinfo-back 命名空间并部署 reviews、ratings 和 details。\n在 bookinfo-front 中创建证书，以便命名空间中的服务可以使用 HTTPS 进行公开。\nkubectl create secret tls bookinfo-cert -n bookinfo-front --cert cert.pem --key key.pem 完成后，在每个集群中创建一个 IngressGateway 部署：\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: bookinfo-front-gw namespace: bookinfo-front spec: kubeSpec: service: ports: - name: mtls port: 15443 targetPort: 15443 - name: https port: 443 targetPort: 8443 - name: http2 port: 80 targetPort: 8080 type: LoadBalancer --- apiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: bookinfo-back-gw namespace: bookinfo-back spec: kubeSpec: service: ports: - name: mtls port: 15443 targetPort: 15443 - name: https port: 443 targetPort: 8443 - name: http2 port: 80 targetPort: 8080 type: LoadBalancer 然后应用它们：\nkubectl apply -f bookinfo-\u0026lt;front|back\u0026gt;-ingress.yaml 获取这两个服务的 IP 地址：\nFRONT=$(kubectl get svc -n bookinfo-front bookinfo-front-gw -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) BACK=$(kubectl get svc -n bookinfo-back bookinfo-back-gw -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) 并配置 DNS 以使用以下配置进行访问：\nFRONT → bookinfo.tetrate.com BACK → bookinfo-back.tetrate.com（可以是你喜欢的名称）。 在这一点上，很重要的是在 productpage 部署规范中添加以下行\n，以便部署能够知道 details 和 reviews 存储在哪里：\nenv: - name: DETAILS_HOSTNAME value: bookinfo-back.tetrate.com:80 - name: REVIEWS_HOSTNAME value: bookinfo-back.tetrate.com:80 注意 对于默认的 productpage 镜像，这不会起作用，因为默认端口硬编码为 9080，这只是一个示例，但你可以修改它以获取端口和主机名。 配置 Ingress Gateway 路由 现在，我们可以通过创建 Tier-2 网关配置来配置已部署的入口网关。这可以通过创建 IngressGateway 网关资源来完成。\n注意 请注意，apiVersion 与之前的不同，因为第一个是用于安装入口网关，第二个是用于配置使用 BRIDGED API 的网关和虚拟服务。 apiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: organization: tetrate tenant: test workspace: bookinfo group: bookinfo-gw name: bookinfo-front-gw spec: workloadSelector: namespace: bookinfo-front labels: app: bookinfo-front-gw http: - name: bookinfo port: 443 hostname: bookinfo.tetrate.com tls: mode: SIMPLE secretName: bookinfo-cert routing: rules: - route: host: bookinfo-front/productpage.bookinfo-front.svc.cluster.local port: 9080 --- apiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: organization: tetrate tenant: test workspace: bookinfo group: bookinfo-gw name: bookinfo-back-gw spec: workloadSelector: namespace: bookinfo-back labels: app: bookinfo-back-gw http: - name: bookinfo-back port: 80 hostname: bookinfo-back.tetrate.com routing: rules: - match: - uri: prefix: /details route: host: bookinfo-back/details.bookinfo-back.svc.cluster.local port: 9080 - match: - uri: prefix: /reviews route: host: bookinfo-back/reviews.bookinfo-back.svc.cluster.local port: 9080 验证 通过此配置，bookinfo.tetrate.com 使用 HTTPS 公开，而 bookinfo-back.tetrate.com 使用 HTTP。\n在此时，你可以通过执行以下命令来测试它是否有效：\n$ curl -I https://bookinfo.tetrate.com/productpage 注意 productpage 服务被配置为通过端口 80 发送流量到 details 和 reviews 服务。然而，在我们配置了 TSB 对象之后，将创建一个服务条目，该服务条目将重定向此端口 80 到 15443（已配置为 mTLS），还将创建一个使用 mTLS 的目标规则。 你可以通过运行以下命令来查看这两者，并查看与 bookinfo 相关的服务条目和目标规则：\nkubectl get dr,se -n xcp-multicluster ","relpermalink":"/book/tsb/howto/gateway/multi-cluster-traffic-routing-using-tier2gw/","summary":"使用 Tier-2 网关在集群之间切换流量。","title":"使用 Tier-2 网关进行多集群流量路由"},{"content":" WASM 扩展概述\nTSB 配置\nWasm 扩展示例\n","relpermalink":"/book/tsb/howto/wasm/","summary":"WASM 扩展概述 TSB 配置 Wasm 扩展示例","title":"使用 WASM 扩展"},{"content":" 特性\n用户权限\n遥测和审计\nElasticSearch\nVault 客户端集成\n配置推广\n配置日志级别\n配置多个 IAM 令牌验证密钥\n备份和恢复 PostgreSQL\n将数据迁移到新的组织\n降低 Istio 资源消耗\n定制 TSB Kubernetes 组件\n优雅关闭 istio-proxy 连接\n","relpermalink":"/book/tsb/operations/","summary":"特性 用户权限 遥测和审计 ElasticSearch Vault 客户端集成 配置推广 配置日志级别 配置多个 IAM 令牌验证密钥 备份和恢复 PostgreSQL 将数据迁移到新的组织 降低 Istio 资源消耗 定制 TSB Kubernetes 组件 优雅关闭 istio-proxy 连接","title":"运维指南"},{"content":"本文档描述了使用工作负载载入功能将本地工作负载载入 TSB 的步骤。\n在继续之前，请确保你已完成设置工作负载载入文档 中描述的步骤。\n背景 通过工作负载载入加入网格的每个工作负载都必须具有可验证的身份。\n云中的 VM 具有开箱即用的可验证身份。此类身份由各云平台提供。\n然而，在本地环境中，是一个黑盒。你的本地工作负载是否具有可验证的身份完全取决于你自己的技术堆栈。\n因此，要能够载入本地工作负载，你需要确保它们具有JWT 令牌 形式的可验证身份。\n概述 本地工作负载的工作负载载入设置包括以下额外步骤：\n配置受信任的 JWT 发行者 允许本地工作负载加入 WorkloadGroup 配置工作负载载入代理以使用你的自定义凭证插件 载入本地工作负载 配置受信任的 JWT 发行者 要配置一组受信任的 JWT 发行者，用于断言本地工作负载的身份，请按以下方式编辑 TSB ControlPlane CR 或 Helm values：\nspec: ... meshExpansion: onboarding: ... # 专用于本地工作负载的额外配置 workloads: authentication: jwt: issuers: - issuer: \u0026lt;jwt-issuer-id\u0026gt; # (1) 必填 shortName: \u0026lt;short-name\u0026gt; # (2) 必填 jwksUri: \u0026lt;jwks-uri\u0026gt; # (3) 可选 jwks: | # { # \u0026#34;keys\u0026#34;: [ # ... # ] # } \u0026lt;inlined-jwks-document\u0026gt; # (4) 可选 tokenFields: attributes: jsonPath: \u0026lt;jwt-attributes-field-jsonpath\u0026gt; # (5) 可选 其中\n必须指定要信任的 JWT 发行者ID，例如 https://mycompany.corp 必须指定要与该发行者关联的简称，例如 my-corp 可以指定从中获取签名密钥的 JWKS 文档的 URI，例如 https://mycompany.corp/jwks.json 可以指定一个 JWKS 文档，其中包含签名密钥 可以指定 JWT 令牌内部保存有关工作负载的属性映射的字段，例如 .custom_attributes 允许本地工作负载加入 WorkloadGroup 要允许本地工作负载加入某些 WorkloadGroup，请创建以下配置的OnboardingPolicy ：\napiVersion: authorization.onboarding.tetrate.io/v1alpha1 kind: OnboardingPolicy metadata: name: \u0026lt;name\u0026gt; namespace: \u0026lt;namespace\u0026gt; spec: allow: - workloads: - jwt: issuer: \u0026lt;jwt-issuer-id\u0026gt; # (1) 必填 subjects: - \u0026lt;subject\u0026gt; # (2) 可选 attributes: - name: \u0026lt;attribute-name\u0026gt; # (3) 可选 values: - \u0026lt;attribute-value\u0026gt; onboardTo: - workloadGroupSelector: {} # 该命名空间中的任何 WorkloadGroup 其中\n必须指定此规则适用的 JWT 发行者ID，例如 https://mycompany.corp 可以指定此规则适用于的 JWT 主体的显式列表，例如 us-east-datacenter1-vm007 可以指定 JWT 必须具有的工作负载属性，以使此规则适用，例如 region=us-east 配置工作负载载入代理以使用你的自定义凭证插件 为了能够载入本地工作负载，你需要使用一个生成给定工作负载的JWT 令牌 凭证的凭证插件。\n首先，在 VM 上安装你的自定义凭证插件，例如在/usr/local/bin/onboarding-agent-\u0026lt;your-plugin-name\u0026gt;-plugin。\n然后，配置工作负载载入代理 以使用该插件。 为此，请按照以下方式编辑 /etc/onboarding-agent/onboarding.config.yaml：\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: AgentConfiguration host: custom: credential: - plugin: name: \u0026lt;your-plugin-name\u0026gt; # (1) 必填 path: /usr/local/bin/onboarding-agent-\u0026lt;your-plugin-name\u0026gt;-plugin # (2) 可选 args: - \u0026lt;your-plugin-arg\u0026gt; # (3) 可选 env: - name: \u0026lt;YOUR_PLUGIN_CONFIG\u0026gt; # (4) 可选 value: \u0026#34;\u0026lt;your-plugin-config-value\u0026gt;\u0026#34; 其中\n必须指定你的凭证插件的名称，例如 my-jwt-credential 可以指定插件二进制文件的路径，例如 /usr/local/bin/onboarding-agent-my-jwt-credential-plugin 可以指定工作负载载入代理在执行插件二进制文件时必须传递的其他命令行参数，例如 `– my-arg=my-value`\n可以指定工作负载载入代理在执行插件二进制文件时必须设置的其他环境变量，例如 MY_CONFIG=\u0026#34;some value\u0026#34; 载入本地工作负载 要载入本地工作负载，按照与云中 VM 相同的步骤 进行操作。\n","relpermalink":"/book/tsb/setup/workload-onboarding/guides/on-premise-workloads/","summary":"如何在本地载入工作负载。","title":"在本地载入工作负载"},{"content":" 证书类型\n内部证书要求\n自动证书管理\n","relpermalink":"/book/tsb/setup/certificate/","summary":"证书类型 内部证书要求 自动证书管理","title":"证书安装"},{"content":"本指南适用于想要为其他开发人员安装和配置 Argo CD 的管理员和操作员。\n🔔 注意：请确保你已完成 入门指南 。\n架构概述\nApplicationSet 控制器\n","relpermalink":"/book/argo-cd/operator-manual/","summary":"本指南适用于想要为其他开发人员安装和配置 Argo CD 的管理员和操作员。 🔔 注意：请确保你已完成 入门指南 。 架构概述 ApplicationSet 控制器","title":"操作手册"},{"content":"Argo Rollouts Kubectl 插件可以提供本地 UI 仪表板来可视化你的 Rollouts。\n要启动它，请在包含你的 Rollouts 的命名空间中运行 kubectl argo rollouts dashboard 。然后访问 localhost:3100 查看用户界面。\n列表视图 Rollouts 列表视图 单独的 Rollout 视图 Rollouts 视图 ","relpermalink":"/book/argo-rollouts/dashboard/","summary":"Argo Rollouts Kubectl 插件可以提供本地 UI 仪表板来可视化你的 Rollouts。 要启动它，请在包含你的 Rollouts 的命名空间中运行 kubectl argo rollouts dashboard 。然后访问 localhost:3100 查看用户界面。 列表视图 Rollouts 列表视图 单独的 Rollout 视图 Rollouts 视图","title":"UI Dashboard"},{"content":"本指南介绍了 Argo Rollouts 如何与 NGINX Ingress Controller 集成进行流量整形。本指南基于 基本入门指南 的概念。\n要求 安装了 NGINX Ingress 控制器的 Kubernetes 集群 🔔 提示：请参阅 NGINX 环境设置指南 以了解如何使用 nginx 设置本地 minikube 环境。\n1. 部署 Rollout、服务和 Ingress 当使用 NGINX Ingress 作为流量路由器时，Rollout 金丝雀策略必须定义以下强制字段：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollouts-demo spec: strategy: canary: # 引用控制器将更新并指向金丝雀 ReplicaSet 的服务 canaryService: rollouts-demo-canary # 引用控制器将更新并指向稳定 ReplicaSet 的服务 stableService: rollouts-demo-stable trafficRouting: nginx: # 引用一个 Ingress，该 Ingress 具有指向稳定服务（例如 rollouts-demo-stable）的规则 # 为了实现 NGINX 流量分割，此 ingress 将被克隆为一个新名称。 stableIngress: rollouts-demo-stable ... 在 canary.trafficRouting.nginx.stableIngress 中引用的 Ingress 必须具有后端，该后端指向 canary.stableService 下引用的服务。在我们的示例中，该稳定服务的名称为 rollouts-demo-stable：\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: rollouts-demo-stable annotations: kubernetes.io/ingress.class: nginx spec: rules: - host: rollouts-demo.local http: paths: - path: / backend: # 引用一个服务名称，也在 Rollout 规范的 `strategy.canary.stableService` 中指定 serviceName: rollouts-demo-stable servicePort: 80 运行以下命令以部署：\n一个 Rollout 两个服务（稳定和金丝雀） 一个 Ingress kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/rollout.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/services.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/ingress.yaml 应用清单后，你应该在集群中看到以下滚动、服务和 Ingress 资源：\n$ kubectl get ro NAME DESIRED CURRENT UP-TO-DATE AVAILABLE rollouts-demo 1 1 1 1 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE rollouts-demo-canary ClusterIP 10.96.6.241 \u0026lt;none\u0026gt; 80/TCP 33s rollouts-demo-stable ClusterIP 10.102.229.83 \u0026lt;none\u0026gt; 80/TCP 33s $ kubectl get ing NAME CLASS HOSTS ADDRESS PORTS AGE rollouts-demo-stable \u0026lt;none\u0026gt; rollouts-demo.local 192.168.64.2 80 36s rollouts-demo-rollouts-demo-stable-canary \u0026lt;none\u0026gt; rollouts-demo.local 192.168.64.2 80 35s 你还应该注意到由 rollouts 控制器创建的第二个 Ingress，rollouts-demo-rollouts-demo-stable-canary。这个 Ingress 是“金丝雀 Ingress”，是用户管理的 Ingress 的克隆，其引用在 nginx.stableIngress 下。它由 nginx ingress 控制器用于实现金丝雀流量分割。生成的 ingress 名称使用 \u0026lt;ROLLOUT-NAME\u0026gt;-\u0026lt;INGRESS-NAME\u0026gt;-canary 进行公式化。有关第二个 Ingress 的更多详细信息在下一节中讨论。\nkubectl argo rollouts get rollout rollouts-demo Rollout Nginx 2. 执行更新 通过更改镜像来更新 rollout，并等待其达到暂停状态。\nkubectl argo rollouts set image rollouts-demo rollouts-demo=argoproj/rollouts-demo:yellow kubectl argo rollouts get rollout rollouts-demo Rollout Nginx 已暂停 此时，Rollout 的金丝雀和稳定版本都正在运行，其中 5% 的流量定向到金丝雀。需要注意的是，尽管只运行了两个 pod，但是 Rollout 能够实现 5% 的金丝雀权重。这可以通过在 ingress 控制器中发生流量分割（而不是加权副本计数和 kube-proxy）来实现。\n在检查 Rollout 控制器生成的 Ingress 副本时，我们发现它与原始 Ingress 相比具有以下更改：\n添加了两个附加的 NGINX 特定的金丝雀注释 到注释中。 Ingress 规则将具有指向 金丝雀 服务的规则。 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: rollouts-demo-rollouts-demo-stable-canary annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/canary-weight: \u0026#34;5\u0026#34; spec: rules: - host: rollouts-demo.local http: paths: - backend: serviceName: rollouts-demo-canary servicePort: 80 随着 Rollout 通过步骤进行，canary-weight 注释将根据步骤的 setWeight 调整。NGINX Ingress 控制器检查原始 Ingress、金丝雀 Ingress 和 canary-weight 注释，以确定在两个 Ingress 之间拆分多少流量。\n","relpermalink":"/book/argo-rollouts/getting-started/nginx/","summary":"本指南介绍了 Argo Rollouts 如何与 NGINX Ingress Controller 集成进行流量整形。本指南基于 基本入门指南 的概念。 要求 安装了 NGINX Ingress 控制器的 Kubernetes 集群 🔔 提示：请参阅 NGINX 环境设置指南 以了解如何使用 nginx 设置本地 minikube 环境。 1. 部署 Rollout、服务和 Ingress 当使用","title":"Nginx Ingress 快速开始"},{"content":"Nginx Ingress Controller 允许通过一个或多个 Ingress 对象进行流量管理，以配置直接将流量路由到 Pod 的 Nginx 部署。每个 Nginx Ingress 都包含多个注释，可以修改 Nginx 部署的行为。对于应用程序不同版本之间的流量管理，Nginx Ingress 控制器提供了通过引入第二个 Ingress 对象（称为金丝雀 Ingress）进行流量拆分的功能。你可以在官方的 金丝雀注释文档页面 上阅读更多关于这些金丝雀注释的信息。金丝雀 Ingress 忽略任何其他非金丝雀 nginx 注释。取而代之，它利用来自主要 Ingress 的注释设置。\nRollout 控制器始终会在金丝雀 Ingress 上设置以下两个注释（使用你配置的或默认的 nginx.ingress.kubernetes.io 前缀）：\ncanary: true 表示这是金丝雀 Ingress canary-weight: \u0026lt;num\u0026gt; 表示将发送到金丝雀的流量百分比。如果所有流量都路由到稳定服务，则设置为 0 你可以通过 additionalIngressAnnotations 字段提供其他注释以添加到金丝雀 Ingress，以启用按标头或 cookie 进行路由等功能。\n与 Argo Rollouts 集成 使用 Nginx 发送版本间的分割流量需要在 Rollout 中有几个必需字段。以下是一个带有这些字段的 Rollout 示例：\napiVersion: argoproj.io/v1alpha1 kind: Rollout spec: ... strategy: canary: canaryService: canary-service # 必需 stableService: stable-service # 必需 trafficRouting: nginx: # 必须配置 stableIngress 或 stableIngress 中的一个，但不能同时配置。 stableIngress: primary-ingress stableIngresses: - primary-ingress - secondary-ingress - tertiary-ingress annotationPrefix: customingress.nginx.ingress.kubernetes.io # 可选的 additionalIngressAnnotations: # 可选的 canary-by-header: X-Canary canary-by-header-value: iwantsit 稳定 Ingress 字段是对 Rollout 同一命名空间中的 Ingress 的引用。Rollout 需要主要 Ingress 将流量路由到稳定服务。Rollout 通过确认 Ingress 是否具有与 Rollout 的 stableService 匹配的后端来检查该条件。\n控制器通过创建具有金丝雀注释的第二个 Ingress 来将流量路由到金丝雀服务。随着 Rollout 经过金丝雀步骤，控制器更新金丝雀 Ingress 的金丝雀注释，以反映 Rollout 的所需状态，从而实现两个不同版本之间的流量分配。\n由于 Nginx Ingress 控制器允许用户配置用于 Ingress 控制器的注释前缀，因此 Rollout 可以指定可选的 annotationPrefix 字段。如果设置了该字段，则金丝雀 Ingress 将使用该前缀而不是默认的 nginx.ingress.kubernetes.io。\n在一个服务中使用多个 NGINX Ingress 控制器与 Argo Rollouts 从 v1.5 开始，argo rollouts 支持多个 Nginx Ingress 控制器指向具有金丝雀部署的一个服务。如果只需要一个 Ingress 控制器，请使用现有的键 stableIngress。如果需要多个 Ingress 控制器（例如，分离内部和外部流量），请改用键 stableIngresses。它接受一个字符串值数组，这些字符串值是 Ingress 控制器的名称。金丝雀步骤在所有 Ingress 控制器上应用相同的方式。\n在自定义 NGINX ingress 控制器名称中使用 Argo Rollouts 默认情况下，Argo Rollouts 控制器仅在具有 kubernetes.io/ingress.class 注释或 spec.ingressClassName 设置为 nginx 的 Ingress 上运行。用户可以通过指定 --nginx-ingress-classes 标志将控制器配置为在具有不同类名的 Ingress 上运行。如果 Argo Rollouts 控制器应该在多个值上运行，则用户可以多次列出 --nginx-ingress-classes 标志。这解决了集群具有在不同类值上运行的多个 Ingress 控制器的情况。\n如果用户希望控制器在没有 kubernetes.io/ingress.class 注释或 spec.ingressClassName 的任何 Ingress 上运行，则用户应添加以下内容 --nginx-ingress-classes \u0026#39;\u0026#39;。\n","relpermalink":"/book/argo-rollouts/traffic-management/nginx/","summary":"Nginx Ingress Controller 允许通过一个或多个 Ingress 对象进行流量管理，以配置直接将流量路由到 Pod 的 Nginx 部署。每个 Nginx Ingress 都包含多个注释，可以修改 Nginx 部署的行为。对于应用程序不同版本之间的流量管理，Nginx Ingress 控制器提供了通过引入第二个 Ingress","title":"Nginx"},{"content":"出于各种原因，应用程序通常需要重新启动，例如出于健康目的或强制执行启动逻辑，例如重新加载修改的 Secret。在这些情况下，不希望进行整个蓝绿或金丝雀更新过程。Argo Rollouts 支持通过执行所有 Rollout 中的 Pod 的滚动重建来重启其所有 Pod 的能力，同时跳过常规的 BlueGreen 或 Canary 更新策略。\n它是如何工作的 可以通过 kubectl 插件使用 kubectl-argo-rollouts restart 命令来重新启动 Rollout：\nkubectl-argo-rollouts restart ROLLOUT 或者，如果 Rollouts 与 Argo CD 一起使用，则可以通过 Argo CD UI 或 CLI 执行捆绑的“restart”操作：\nargocd app actions run my-app restart --kind Rollout --resource-name my-rollout 这两种机制都会将 Rollout 的.spec.restartAt更新为以RFC 3339 格式 的 UTC 字符串的当前时间形式（例如 2020-03-30T21:19:35Z），这表示 Rollout 控制器应该在此时间戳之后创建 Rollout 的所有 Pod。\n在重新启动期间，控制器会迭代每个 ReplicaSet，以查看所有 Pod 的创建时间戳是否比restartAt时间新。对于早于“restartAt”时间戳的每个 Pod，将会被驱逐，允许 ReplicaSet 将该 Pod 替换为重新创建的一个。\n为了防止太多的 Pod 同时重新启动，控制器将自己限制为一次删除最多maxUnavailable个 Pod。其次，由于 Pod 被驱逐而不是删除，因此重新启动过程将遵守任何现有的 PodDisruptionBudgets。\n控制器按以下顺序重新启动 ReplicaSets：\n稳定的 ReplicaSet 当前的 ReplicaSet 从最旧的开始的所有其他 ReplicaSet 如果在重新启动过程中修改了 Rollout 的 Pod 模板规范（spec.template），则重新启动将被取消，并且将执行正常的蓝绿或金丝雀更新。\n注意：与 Deployment 不同，其中“重新启动”只是由 Pod 规范注释中的时间戳触发的正常滚动升级，Argo Rollouts 通过终止 Pod 并允许现有 ReplicaSet 替换终止的 Pod 来促进重启。为了在 Rollout 处于长时间运行的蓝绿/金丝雀更新中（例如暂停的金丝雀）时仍允许重启发生而做出了此设计选择。但是，这样做的一些后果是：\n重新启动具有单个副本的 Rollout 将导致停机，因为 Argo Rollouts 需要终止 Pod 以替换它。 与部署的滚动更新相比，重新启动 Rollout 将更慢，因为不使用 maxSurge 来更快地启动新的 Pod。 maxUnavailable 将用于一次重启多个 Pod（从 v0.10 开始）。但是，如果 maxUnavailable pods 为 0，则控制器仍将一次重启一个 Pod。 计划重新启动 用户可以通过将.spec.restartAt字段设置为将来的时间来计划重新启动 Rollout。在当前时间在 restartAt 时间之后时，控制器才开始重新启动。\n","relpermalink":"/book/argo-rollouts/rollout/restart/","summary":"出于各种原因，应用程序通常需要重新启动，例如出于健康目的或强制执行启动逻辑，例如重新加载修改的 Secret。在这些情况下，不希望进行整个蓝绿或金丝雀更新过程。Argo Rollouts 支持通过执行所有 Rollout 中的 Pod 的滚动重","title":"重启 Rollouts Pod"},{"content":"本章旨在让你为上线 SPIFFE/SPIRE 时需要做出的许多决定做好准备。\n准备人力 如果你读了前面的章节，你一定很想开始使用 SPIRE，以一种可以在许多不同类型的系统和所有组织的服务中利用的方式管理身份。然而，在你开始之前，你需要考虑，部署 SPIRE 是一个重大的基础设施变化，有可能影响到许多不同的系统。本章是关于如何开始规划 SPIRE 的部署：获得认同，以不中断的方式启用 SPIRE 支持，然后利用它来实施新的安全控制。\n组建团队并确定其他利益相关者 要部署 SPIRE，你需要确定来自安全、软件开发和 DevOps 团队的利益相关者。谁来维护 SPIRE 服务器本身？谁来部署代理？谁来编写注册条目？谁将把 SPIFFE 功能集成到应用程序中？它将如何影响现有的 CI/CD 管道？如果发生了服务中断，谁来修复它？性能要求和服务水平目标是什么？\n在这本书中，以及许多公开的博客文章和会议演讲中，都有一些成功部署 SPIRE 的组织的例子，既可以作为一种模式，也可以作为向同事宣传 SPIRE 的有用材料。\n说明你的情况并获得支持 SPIRE 跨越了几个不同的传统信息技术孤岛，因此，期望看到你的 DevOps 团队、软件开发团队和安全团队之间有更多的跨组织合作。重要的是，他们要一起工作，以确保成功和无缝部署。考虑到这些团队中的每一个都有不同的需求和优先事项，需要解决这些问题以获得他们的支持。\n在规划 SPIRE 部署时，你需要了解哪些成果对你的企业最重要，并将这些成果作为项目的驱动力和你将提供的解决方案的价值。每个团队都需要看到 SPIRE 对自己以及对整个企业的好处。本书第 2 章 “收益” 中描述了 SPIRE 部署的许多好处，在本节中，我们将把其中一些好处提炼成令人信服的论据。\n对安全团队有说服力的论点\n减少安全团队的工作量是部署 SPIRE 的一个非常有说服力的案例：他们可以专注于设计正确的注册条目，以确保每个服务获得正确的身份，而不是部署临时的安全解决方案，以及手动管理数百或数千个证书。\n一个更长期的好处是，SPIRE 可以提高组织的整体安全态势，因为 SPIRE 没有容易被盗或误用的凭证。与盗用或歪曲凭证有关的大量攻击，以及敏感数据的暴露，都得到了缓解。有可能向外部审计师证明，正确的服务正在相互安全地进行通信，没有意外疏忽的可能。即使外部人员可以破坏一个服务，他们对其他服务发起攻击的能力也是有限的。\n对软件开发团队有说服力的论点\n对于应用程序开发团队来说，他们能够通过不等待工单或手动工作流程来提供证书而加快行动，这是最有说服力的案例。如果他们目前在代码旁边手动部署秘密，并被安全团队谈话，他们不再需要忍受这些。他们也不需要在秘密存储中管理秘密。\n一个次要的好处是，软件组件可能能够以它们以前无法安全进行的方式直接进行通信。如果云服务不能访问一个关键的数据库或基本的云服务，因为没有办法安全地做到这一点，那么就有可能使用 SPIFFE 身份来创建一个安全连接，为你的团队提供新的架构潜力。\n对 DevOps 团队有说服力的论点\n部署 SPIRE 的最大收益是针对 DevOps 团队。如果每个服务都有自己的安全身份，那么服务就可以部署在任何地方 —— 在任何内部数据中心、云供应商或一个云供应商中的区域。这种新的灵活性允许降低成本，提高可扩展性，并改善可靠性，因为部署决策可以独立于安全要求。\n对 DevOps 团队来说，另一个关键的好处是，每个服务的传入请求都被贴上 SPIFFE ID 的标签，这可以被记录、测量，并报告给监控系统。这对拥有数百或数千项服务的大型组织的性能管理是非常有帮助的。\n创建一个计划 规划 SPIRE 部署的第一个目标是确定是否每项服务都需要被 SPIFFE 感知，或者非 SPIFFE 服务的 “孤岛” 是否仍然可以满足要求。将每项服务都转移到 SPIFFE 是最直接的选择，但要一下子全部实施可能是个挑战，特别是在非常大的组织中。\n岛屿和桥梁的规划 有些环境很复杂，要么有多个组织，要么是传统和新开发的结合。在这种情况下，人们往往希望只让环境的一个子集启用 SPIFFE。需要考虑两种选择，这取决于系统之间的整合程度和它们之间的复杂性。让我们来看看这两种架构，我们称之为 “独立岛” 和“桥接岛”。\n每个岛被认为是它自己的信任域，在每个岛上有工作负载或“居民”。\n独立岛\n独立岛模式允许各个信任域相互独立运行。这通常是最简单的选择，因为每个岛可以以对该岛有意义的方式运行 SPIRE。\n桥接岛\n桥接岛模式允许非 SPIFFE 岛上的非 SPIFFE 服务与网关对话。然后，网关将请求转发给支持 SPIFFE 的岛上的居民，我们称他们为 Zero。从 Zero 的角度来看，网关发出了请求。Zero 和他在支持 SPIFFE 的岛上的朋友可以向网关进行认证，并向非 SPIFFE 岛上的服务发送消息。\n在桥接岛架构下，网关是在未启用 SPIFFE 的岛上创建的。这些非 SPIFFE 岛可能不容易采用 SPIFFE 架构，原因有很多：可能有遗留软件，不能轻易修改或更新；岛屿可能使用自己的识别生态系统，如 Kerberos 或 SPIFFE 与其他技术比较一章中描述的其他选项之一；或者系统可能在不太适合现有 SPIFFE 解决方案（如 SPIRE）模式的技术上运行工作负载。\n在这些情况下，使用网关服务在 SPIFFE 世界和非 SPIFFE 岛之间架起连接的桥梁可能是有用的。当支持 SPIFFE 的工作负载想要与非 SPIFFE 岛的工作负载对话时，它与网关建立一个经过认证的连接，然后与目标工作负载建立一个连接。这个与目标工作负载的连接可能是未经认证的，或者使用该岛的非 SPIFFE 身份解决方案。同样，当非 SPIFFE 岛的工作负载想要连接到 SPIFFE 启用的工作负载时，非 SPIFFE 工作负载会连接到网关，然后创建一个 SPIFFE 认证的连接到目标 SPIFFE 启用的工作负载。\n在这种情况下，发生在网关和启用 SPIFFE 的工作负载之间的认证在网关处被终止。这意味着支持 SPIFFE 的工作负载可以验证它是在与适当的网关对话，但不能验证它是在与网关另一端的正确工作负载对话。同样，目标工作负载只知道网关服务向它发送了一个请求，但却失去了原 SPIFFE 启用的工作负载的验证背景。这种模式允许这些复杂的组织开始采用 SPIFFE，而不必一下子转换。\n在请求和工作流通过非 SPIFFE 岛的情况下，利用 JWT-SVID 进行跨请求的传播会很有用。你可以使用 X509-SVID 来签署文件（如 HTTP 消息请求签署 ），而不是只使用服务间的相互认证的 TLS，这样整个消息的真实性就可以被另一边支持 SPIFFE 的工作负载所验证。这对已知安全属性较弱的岛屿特别有用，因为它提供了对通过中间生态系统的消息没有被操纵的信心。\n文档和监控工具 在准备开始上线时，重要的是要对服务进行检测，使指标和流量日志以一种方式暴露出来。\n监督上线的人知道哪些（以及有多少）服务是支持 SPIFFE 的，哪些（以及有多少）不是。 客户端作者知道他们调用的哪些服务是支持 SPIFFE 的，哪些不是。 服务所有者知道他们的哪些客户端以及有多少客户端在调用支持 SPIFFE 的端点，哪些在调用传统端点。 重要的是，要为客户端和服务器实施者创建参考文件，预测你将收到的支持请求的种类，从而为上线做准备。\n同样重要的是，创建工具来协助完成常见的调试和故障排除任务。回顾 SPIFFE 和 SPIRE 的收益，将 SPIFFE 引入你的组织应该赋予开发人员权力并消除路障。给利益相关者留下的印象是你在增加工作或制造摩擦，最终会减缓或停止更广泛的采用。为了减少这种情况，并确保文档和工具涵盖适当的主题，我们建议采取以下准备步骤。\n步骤 备注 决定你将需要 SPIFFE 的哪些安全功能。 SPIFFE 身份可用于创建相互的 TLS 连接，用于授权，或其他功能，如审计日志。 确定使用什么格式的 SVID，用于什么目的。 最常见的是将 X509-SVID 用于相互 TLS，但要确定这是否适用以及 SVID 是否将用于任何其他应用。 确定需要身份认证的工作负载的数量。 不是每个工作负载都需要身份，特别是在早期。 确定需要的独立信任域的数量。 每个信任域都需要部署自己的 SPIRE 服务器。做出这一决定的细节在下一章。 确定你的组织正在使用的语言、框架、IPC 技术等需要与 SPIFFE 兼容。 如果使用 X.509-SVID 进行相互 TLS，请确定你的组织中使用了哪些 Web 服务器（Apache HTTPD、NGINX、Tomcat、Jetty 等）以及使用了哪些客户端库。如果客户端库期望执行 DNS 主机名验证，请确保你的 SPIFFE 部署与这种期望兼容。 了解性能影响 应将性能影响作为部署规划的一部分加以考虑。\n作为上线准备的一部分，你应该检查一系列工作负载的基准，这些工作负载代表了你的组织在生产中运行的各种应用。这可以确保你至少意识到，并希望能准备好解决在推广过程中可能出现的任何性能问题。\nTLS 性能\n在许多组织中，开发人员和运维团队提出的第一个担忧是，在服务之间建立相互的 TLS 连接会太慢。在现代硬件上，通过现代的 TLS 实现，TLS 的性能影响是最小的。\n“在我们的生产前端机器上，SSL/TLS 占 CPU 负载的比例不到 1%，每个连接占内存的比例不到 10KB，网络开销不到 2%。许多人认为，SSL/TLS 需要大量的 CPU 时间，我们希望前面的数字能帮助消除这种想法。\u0026#34;——Adam Langley， Google，Overlocking SSL ，2010 年\n“我们已经使用硬件和软件负载均衡器大规模地部署了 TLS。我们发现，在商用 CPU 上运行的基于软件的现代 TLS 实现，其速度足以处理繁重的 HTTPS 追踪负载，而不需要求助于专用加密硬件。\u0026#34;——Doug Beaver，Facebook，HTTP2 Expression of Interest ，2012 年\n一般来说，性能影响取决于多种因素，包括网络拓扑结构、API 网关、L4-L7 防火墙和其他许多因素。此外，你所使用的协议及其实现以及证书和密钥大小也可能影响性能，所以这是一个相当广泛的话题。\n下表提供了与 TCP 相比两个不同阶段的开销数据，特别是握手和数据传输阶段的数据。\nTLS 阶段 协议开销 延迟 CPU 内存 握手 TLS 为 2 kB mTLS 为 3 kB +1 kB/add’l Cert 12 - 17 ms 比 TCP 多出约 0.5% \u0026lt;10kb / 连接 数据传输 22B/packet \u0026lt;3 us 比 TCP 多不到 1% \u0026lt;10 kB / 连接 向 SPIFFE 和 SPIRE 转变 关于组织如何对变化作出反应、影响和处理的研究有着丰富的历史。关于公众和组织对新技术的接受和采用，也有许多有趣的研究。对这些主题进行真正的公正研究超出了本书的范围，但如果我们不提及这些主题，那就是我们的失职，因为它们与 SPIFFE 的成功推广相关。\n令人信服的变化发生\n有几种方法可以说服他人，在你的组织内必须发生变化。下面的清单概述了你可以通过 SPIFFE 和 SPIRE 追求这种变革的方法。\n提升感知价值：展示 SPIFFE 对工作绩效的积极影响是关键，让人们相信它对他们有实质帮助。 简化易用性：为了让 SPIFFE 易于使用，必须投入大量精力来提升开发者和运维人员的用户体验。 同行影响力：重视受尊敬人士对于 SPIFFE 的看法以及他们是否采用它，这将在组织内积累政治资本。说服关键人物通常比说服所有人更为重要。 提升形象：采用 SPIFFE 将对个人在组织中的地位产生显著影响。 自愿采用：SPIFFE 潜在用户的自愿采用程度受公司文化和个性影响。在面对“被迫”采用者和拒绝采用者时，请务必牢记这一点。 采用者角色 采用者与技术曲线相对应，可以帮助设 …","relpermalink":"/book/spiffe/before-you-start/","summary":"本章旨在让你为上线 SPIFFE/SPIRE 时需要做出的许多决定做好准备。","title":"开始前的准备"},{"content":"在 Kubernetes 中管理 pod 时，Cilium 将创建一个 CiliumEndpoint 的自定义资源定义（CRD）。每个由 Cilium 管理的 pod 都会创建一个 CiliumEndpoint，名称相同且在同一命名空间。CiliumEndpoint 对象包含的信息与 cilium endpoint get 在.status 字段下的 json 输出相同，但可以为集群中的所有 pod 获取。添加 -o json 将导出每个端点的更多信息。这包括端点的标签、安全身份和对其有效的策略。\n例如：\n$ kubectl get ciliumendpoints --all-namespaces NAMESPACE NAME AGE default app1-55d7944bdd-l7c8j 1h default app1-55d7944bdd-sn9xj 1h default app2 1h default app3 1h kube-system cilium-health-minikube 1h kube-system microscope 1h 提示 每个 cilium-agent pod 都会创建一个 CiliumEndpoint 来代表自己的 agent 间健康检查端点。这些不是 Kubernetes 中的 pod，而是在 kube-system 命名空间中。它们被命名为 cilium-health-\u0026lt;节点名\u0026gt;。 ","relpermalink":"/book/cilium-handbook/kubernetes/ciliumendpoint/","summary":"在 Kubernetes 中管理 pod 时，Cilium 将创建一个 CiliumEndpoint 的自定义资源定义（CRD）。每个由 Cilium 管理的 pod 都会创建一个 CiliumEndpoint，名称相同且在同一命名空间。CiliumEndpoint 对象包含的信息与 cilium endpoint","title":"端点 CRD"},{"content":"现在你已经了解了什么是 eBPF 以及它是如何工作的，我们再探索一些可能会在生产部署中使用的基于 eBPF 技术的工具。我们将举一些基于 eBPF 的开源项目的例子，这些项目提供了三方面的能力：网络、可观测性和安全。\n网络 eBPF 程序可以连接到网络接口和内核的网络堆栈的各个点。在每个点上，eBPF 程序可以丢弃数据包，将其发送到不同的目的地，甚至修改其内容。这就实现了一些非常强大的功能。让我们来看看通常用 eBPF 实现的几个网络功能。\n负载均衡 Facebook 正在大规模的使用 eBPF 的网络功能，因此你不必对 eBPF 用于网络的可扩展性有任何怀疑。他们是 BPF 的早期采用者，并在 2018 年推出了 Katran ，一个开源的四层负载均衡器。\n另一个高度扩展的负载均衡器的例子是来自 Cloudflare 的 Unimog 边缘负载均衡器。通过在内核中运行，eBPF 程序可以操作网络数据包，并将其转发到适当的目的地，而不需要数据包通过网络堆栈和用户空间。\nCilium 项目作为一个 eBPF Kubernetes 网络插件更为人所知（我一会儿会讨论），但作为独立的负载均衡器，它也被用于大型电信公司和企业内部部署。同样，因为它能够在早期阶段处理数据包，而不需要进入到用户空间，它具有很高的性能。\nKubernetes 网络 CNCF 项目 Cilium 最初基于 eBPF 的 CNI 实现。它最初是由一群从事 eBPF 工作的内核维护者发起的，他们认识到 eBPF 在云原生网络中的应用潜力。它现在被用作谷歌 Kubernetes 引擎、亚马逊 EKS Anywhere 和阿里云的默认数据平面。\n在云原生环境下，pod 在不断的启停，每个 pod 都会被分配一个 IP 地址。在启用 eBPF 网络之前，当 pod 启停的时候，每个节点都必须为它们更新 iptables 规则，以便在 pod 之间进行路由；而当这些 iptable 规则规模变大后，将十分不便于管理。如 图 6-1 所示，Cilium 极大地简化了路由，仅需在 eBPF 中创建的一个简单的查找表，就可以获得 可观的性能改进 。\n另一个在传统的 iptables 版本之外还增加了 eBPF 实现的 Kubernetes CNI 是 Calico 。\n图 6-1. 用 eBPF 绕过主机网络堆栈 服务网格 eBPF 作为服务网格数据平面的基础也是非常有意义的。许多服务网格在七层，即应用层运行，并使用代理组件（如 Envoy）来辅助应用程序。在 Kubernetes 中，这些代理通常以 sidecar 模式部署，每个 pod 中有一个代理容器，这样代理就可以访问 pod 的网络命名空间。正如你在 第五章 中看到的，eBPF 有一个比 sidecar 模型更有效的方法。由于内核可以访问主机中所有 pod 的命名空间，我们可以使用 eBPF 连接 pod 中的应用和主机上的代理，如 图 6-2 所示。\n图 6-2. eBPF 实现了服务网格的高效无 sidecar 模型，每个节点一个代理，而不是每个应用 pod 一个代理 我还有一篇关于使用 eBPF 实现更高效的服务网格数据平面的文章，Solo.io 上也有发布过类似文章。在写这篇文章的时候，Cilium 服务网格已进入测试阶段，并显示出比传统的 sidecar 代理方法具有更大的 性能提升 。\n可观测性 正如你在本报告前面所看到的，eBPF 程序可以获得对机器上发生的一切的可观测性。通过收集事件数据并将其传递给用户空间，eBPF 实现了一系列强大的可观测性工具，可以向你展示你的应用程序是如何执行和表现的，而不需要对这些应用程序做任何改变。\n在本报告的前面，你已经看到了 BCC 项目，几年来，Brendan Gregg 在 Netflix 做了开创性的工作，展示了这些 eBPF 工具如何被用来 观测我们感兴趣的几乎任何指标 ，而且是大规模和高性能的。\nKinvolk 的 Inspektor Gadget 将其中一些起源于 BCC 的工具带入了 Kubernetes 的世界，这样你就可以在命令行上轻松观测特定的工作负载。\n新一代的项目和工具正在这项工作的基础上，提供基于 GUI 的观测能力。CNCF 项目 Pixie 可以让你运行预先写好的或自定义的脚本，通过一个强大的、视觉上吸引人的用户界面查看指标和日志。因为它是基于 eBPF 的，这意味着你可以自动检测所有应用程序，获得性能数据，而无需进行任何代码修改或配置。图 6-3 显示的只是 Pixie 中众多可视化的一个例子。\n图 6-3. 一个小型 Kubernetes 集群上运行的所有东西的 Pixie 火焰图 另一个名为 Parca 的可观测性项目专注于连续剖析，使用 eBPF 对 CPU 使用率等指标进行有效采样，可以用来检测性能瓶颈。\nCilium 的 Hubble 组件是一个具有命令行界面和用户界面的可观测性工具（如 图 6-4 所示），它专注于 Kubernetes 集群中的网络流。\n图 6-4. Cilium 的 Hubble 用户界面显示了 Kubernetes 集群中的网络流量 在云原生环境中，IP 地址不断被动态重新分配，基于 IP 地址的传统网络观测工具的作用非常有限。作为一个 CNI，Cilium 可以访问工作负载身份信息，这意味着 Hubble 可以显示由 Kubernetes pod、服务和命名空间标识的服务映射和流量数据。这对于诊断网络问题十分有用。\n能够观测到活动，这是安全工具的基础，这些工具将正在发生的事情与策略或规则相比较，以了解该活动是预期的还是可疑的。让我们来看看一些使用 eBPF 来提供云原生安全能力的工具。\n安全 有一些强大的云原生工具，通过使用 eBPF 检测甚至防止恶意活动来增强安全性。我将其分为两类：一类是确保网络活动的安全，另一类是确保应用程序在运行时的预期行为。\n网络安全 由于 eBPF 可以检查和操纵网络数据包，它在网络安全方面有许多用途。基本原理是，如果一个网络数据包被认为是恶意的或有问题的，因为它不符合一些安全验证标准，就可以被简单地丢弃。eBPF 可以很高效的来验证这一点，因为它可以钩住内核中网络堆栈的相关部分，甚至在网卡上 1。这意味着策略外的或恶意的数据包可以在产生网络堆栈处理和传递到用户空间的处理成本之前被丢弃。\n这里有一个 eBPF 早期在生产中大规模使用的一个例子 —— Cloudflare 的 DDoS（分布式拒绝服务）保护。DDoS 攻击者用许多网络信息淹没目标机，希望目标机忙于处理这些信息，导致无法提供有效工作。Cloudflare 的工程师使用 eBPF 程序，在数据包到达后立即对其进行检查，并迅速确定一个数据包是否是这种攻击的一部分，如果是，则将其丢弃。数据包不必通过内核的网络堆栈，因此需要的处理资源要少得多，而且目标可以应对更大规模的恶意流量。\neBPF 程序也被用于动态缓解”死亡数据包“的内核漏洞 2。攻击者以这样的方式制作一个网络工作数据包——利用了内核中的一个错误，使其无法正确处理该数据包。与其等待内核补丁的推出，不如通过加载一个 eBPF 程序来缓解攻击，该程序可以寻找这些特别制作的数据包并将其丢弃。这一点的真正好处是，eBPF 程序可以动态加载，而不必改变机器上的任何东西。\n在 Kubernetes 中，网络策略 是一等资源，但它是由网络插件来执行的。一些 CNI，包括 Cilium 和 Calico，为更强大的规则提供了扩展的网络策略功能，例如允许或禁止流量到一个由完全限定域名而不是仅仅由 IP 地址指定的目的地。在 app.networkpolicy.io 有一个探索网络策略及其效果的好工具，如 图 6-5 所示。\n图 6-5. 网络策略编辑器显示了一个策略效果的可视化表示 标准的 Kubernetes 网络策略规则适用于进出应用 pod 的流量，但由于 eBPF 对所有网络流量都有可视性，它也可用于主机防火墙功能，限制进出主机（虚拟机）的流量 3。\neBPF 也可以被用来提供透明的加密，无论是通过 WireGuard 还是 IPsec 4。在这里，透明 意味着应用程序不需要任何修改 —— 事实上，应用程序可以完全不知道其网络流量是被加密的。\n运行时安全 eBPF 也被用来构建工具，检测恶意程序，防止恶意行为。这些恶意程序包括访问未经许可的文件，运行可执行程序，或试图获得额外的权限。\n事实上，你很可能已经以 seccomp 的形式使用了基于 BPF 的安全策略，这是一个 Linux 功能，限制应用程序可以调用的系统调用集。\nCNCF 项目 Falco 扩展了这种限制应用程序可以进行系统调用的想法。Falco 的规则定义是用 YAML 创建的，这比 seccomp 配置文件更容易阅读和理解。默认的 Falco 驱动是一个内核模块，但也有一个 eBPF 探针驱动，它与”原始系统调用“事件相联系。它不会阻止这些系统调用的完成，但它可以生成日志或其他通知，提醒操作人员注意潜在的恶意程序。\n正如我们在 第三章 中看到的，eBPF 程序可以附加到 LSM 接口上，以防止恶意行为或修复已知的漏洞。例如，Denis Efremov 写了一个 eBPF 程序 来防止 exec() 系统调用在没有传递任何参数的情况下运行，以修复 PwnKit 5 的高危漏洞。eBPF 也可用于缓解投机执行的”Spectre“攻击 6。\nTracee 是另一个使用 eBPF 的运行时安全开源项目。除了基于系统调用的检查之外，它还使用 LSM 接口。这有助于避免受到 TOCTTOU 竞争 条件的影响，因为只检查系统调用时可能会出现这种情况。Tracee 支持用 Open Policy Agent 的 Rego 语言定义的规则，也允许用 Go 定义的插件规则。\nCilium 的 Tetragon 组件提供了另一种强大的方法，使用 eBPF 来监控 容器安全可观测性的四个黄金信号：进程执行、网络套接字、文件访问和七层网络身份。这使操作人员能够准确地看到所有恶意或可疑事件，直击特定 pod 中的可执行文件名称和用户身份。例如，如果你受到加密货币挖矿的攻击，你可以看到到底是什么可执行程序打开了与矿池的网络连接，什么时候，从哪个 pod。这些取证是非常有价值的，可以了解漏洞是如何发生的，并使其容易建立安全策略，以防止类似的攻击再次发生。\n如果你想更深入地了解 eBPF 的安全可观测性这一主题，请查看 Natália Ivánkó 和 Jed Salazar 的报告 7。请关注云原生 eBPF 领域，因为不久之后我们就会看到利用 BPF LSM 和其他 eBPF 定制的工具来提供安全执行和可以观测能力。\n我们在网络、可观测性和安全方面对几个云原生工具进行了考察。与前几代相比，eBPF 的使用为它们两个关键优势：\n从内核中的有利位置来看，eBPF 程序对所有进程都有可视性。 通过避免内核和用户空间执行之间的转换，eBPF 程序为收集事件数据或处理网络数据包提供了一种极其有效的方式。 这并不意味着我们应该使用 eBPF 来处理所有的事情！在 eBPF 中编写特定业务的应用程序是没有意义的，就像我们不可能将应用程序写成内核模块一样。但是也有一些例外情况，比如对于高频交易这样对性能有极高的情况下。正如我们在本章中所看到的那样，eBPF 主要是用于为其他应用程序提供工具。\n参考 有些网卡或驱动支持 XDP 或 eXpress Data Path 钩子，允许 eBPF 程序完全从内核中卸载出来。 ↩︎\nDaniel Borkmann 在他的演讲 中讨论了这个问题，《BPF 更适合作为数据平面》（eBPF 峰会（线上），2020 年）。 ↩︎\n见 Cilium 的主机防火墙文档 。 ↩︎\nTailscale 有这两种加密协议的比较 。 ↩︎\n见 Bharat Jogi 的博客 ，《PwnKit: 本地权限升级漏 …","relpermalink":"/book/what-is-ebpf/ebpf-tools/","summary":"现在你已经了解了什么是 eBPF 以及它是如何工作的，我们再探索一些可能会在生产部署中使用的基于 eBPF 技术的工具。我们将举一些基于 eBPF 的开源项目的例子，这些项目提供了三方面的能力：网络、可观测性和安全。 网络 eBPF 程序可以","title":"第六章：eBPF 工具"},{"content":"由于 DevSecOps 的基本要素跨越了开发（安全的构建和测试、打包）、交付 / 部署和持续监控（以确保运行期间的安全状态），本文建议的目标受众包括软件开发、运维和安全团队。\n","relpermalink":"/book/service-mesh-devsecops/intro/target-audience/","summary":"由于 DevSecOps 的基本要素跨越了开发（安全的构建和测试、打包）、交付 / 部署和持续监控（以确保运行期间的安全状态），本文建议的目标受众包括软件开发、运维和安全团队。","title":"1.3 目标受众"},{"content":"无论代码类型如何，CI/CD 管道都有一些共同的实施问题需要解决。确保流程安全涉及到为操作构建任务分配角色。自动化工具（例如，Git Secrets）可用于此目的。为保证 CI/CD 管道的安全，以下安全任务应被视为最低限度：\n强化托管代码和工件库的服务器。 确保用于访问存储库的凭证，如授权令牌和生成拉动请求的凭证。 控制谁可以在容器镜像注册处签入和签出，因为它们是 CI 管道产生的工件的存储处，是 CI 和 CD 管道之间的桥梁。 记录所有的代码和构建更新活动。 如果在 CI 管道中构建或测试失败 —— 向开发人员发送构建报告并停止进一步的管道任务。配置代码库自动阻止来自 CD 管道的所有拉取请求。 如果审计失败，将构建报告发送给安全团队，并停止进一步的管道任务。 确保开发人员只能访问应用程序代码，而不能访问五种管道代码类型中的任何一种。 在构建和发布过程中，在每个需要的 CI/CD 阶段签署发布工件（最好是多方签署）。 在生产发布期间，验证所有需要的签名（用多个阶段的密钥生成），以确保没有人绕过管道。 ","relpermalink":"/book/service-mesh-devsecops/implement/securing-the-ci-cd-pipeline/","summary":"无论代码类型如何，CI/CD 管道都有一些共同的实施问题需要解决。确保流程安全涉及到为操作构建任务分配角色。自动化工具（例如，Git Secrets）可用于此目的。为保证 CI/CD 管道的安全，以下安全任务应被视为","title":"4.6 确保 CI/CD 管道的安全"},{"content":"在前一章中，我们了解了基础架构的各种表示以及围绕其部署工具的各种方法。在本章中，我们将看看如何设计部署和管理基础架构的应用程序。在上一章中我们重点关注基础架构即软件的开放世界，有时称为基础架构即应用。\n在云原生环境中，传统的基础架构运维人员需要转变为基础架构软件工程师。与过去的其他运维角色不同，这仍然是一种新兴的做法。我们迫切需要开始探索这种模式和制定标准。\n基础架构即软件与基础架构即代码之间的根本区别在于，软件会持续运行，并会根据调节器模式创建或改变基础架构，我们将在本章后面对其进行解释。此外，基础架构即软件的新范例是，软件现在与数据存储具有更传统的关系，并公开用于定义所需状态的 API。例如，该软件可能会根据数据存储中的需要改变基础架构的表示形式，并且可以很好地管理数据存储本身！希望进行协调的状态更改通过 API 发送到软件，而不是通过运行静态代码库中的程序。\n迈向基础架构即软件的第一步是让基础架构的运维人员意识到自己是软件工程师。我们热烈欢迎您来到这个领域！先前的工具（例如配置管理）也有类似的改变基础架构运维人员的工作职能的目标，但是运维人员通常只会在狭窄的应用范围内编写有限的 DSL（即单一节点抽象）。\n作为一名基础架构工程师，您的任务不仅是掌握设计、管理和运维基础架构的基本原则，还需要具有将您的专业知识封装成坚如磐石的应用程序的能力。这些应用程序代表了我们将要管理和改变的基础架构。\n构建管理基础架构软件工程不是一件容易的事情。我们有管理传统应用的所有问题和担忧，而且我们正处于一个尴尬的境地。基础架构软件工程看上去似乎很荒谬，构建软件来部署基础架构，这样就可以在新创建的基础架构之上运行相同的软件，这很尴尬。\n首先，我们需要了解这个新领域中工程软件的细微差别。我们将研究在云原生社区中得到验证的模式，以了解在应用程序中编写干净和逻辑代码的重要性。但首先，基础架构从哪里来？\n自举问题 1987 年 3 月 22 日，周日，Richard M. Stallman 发送了一封电子邮件到 GCC 邮件列表，报告成功使用 C 编译器完成了自行编译：\n该编译器在 68020 上编译正确，最近又在 vax 上进行了编译。最近在 68020 上正确编译了 Emacs，并且还编译了 tex-in-C 和 Kyoto Common Lisp。但是，可能仍然有许多错误，希望你能帮我找到。\n我将离开一个月，所以现在报告的错误将得不到处理。——Richard M. Stallman\n这是软件历史上的一个重要转折点，因为工程软件首次完成了自举（Bootstrap）。Stallman 开创了一个可以自行编译的编译器。即使在哲学上接受这个表述可能也是困难的。\n今天我们正在解决与基础架构相同的问题。工程师必须想办法解决几乎不可能的系统自举问题，并在运行时生效。\n有一种方法是手动创建云计算和基础架构应用程序中的第一个基础架构。尽管这种方法确实有效，但它通常伴随着警告，即运维人员应该在部署更合适的基础架构后销毁初始引导基础架构。这种方法乏味、难以重复且容易出现人为错误。\n解决这个问题的更优雅和云原生方法是做出（通常是正确的）假设，即试图引导基础架构软件的任何人都有本地机器，我们可以利用这个本地机器。现有机器（您的计算机）可作为第一个部署工具，自动在云中创建基础架构。基础架构就位后，您的本地部署工具可以将其自身部署到新创建的基础架构并持续运行。良好的部署工具可以让你在完成后轻松清理。\n在初始基础架构引导问题解决后，我们可以使用基础架构应用程序来引导新的基础架构。现在本地计算机已经被排除在外，现在我们完全运行在云端。\nAPI 在前面的章节中，我们讨论了表示基础架构的各种方法。在本章中，我们将探讨为基础架构提供 API 的概念。\n当用软件实现 API 时，很可能会通过数据结构来完成。因此，根据您使用的编程语言，将 API 视为类、字典、数组、对象或结构体是安全的。\nAPI 将是数据值的任意定义，可能是字符串、整数或布尔值。API 将通过 JSON 或 YAML 格式进行编码和解码甚至可能存储在数据库中。\n对于大多数软件工程师来说，为程序提供可版本化的 API 是很常见的做法。这允许程序随着时间移动、改变和增长。工程师可以声称支持较旧的 API 版本并提供向后兼容性保证。在基础架构即软件中，由于这些原因，使用 API 是首选的。\n寻找一个 API 作为基础架构的接口是用户使用基础架构即软件的许多线索之一。传统上，基础架构即代码是用户将要管理的基础架构的直接表示，而 API 是对要管理的具体底层资源之上的抽象。\n最终，API 只是代表基础架构的数据结构。\n状态 在基础架构即软件工具的环境中，我们要管理的对象是基础架构。因此，对象状态只是我们的程序对软件的审计表示。\n对象的状态最终将回到基础架构表示的内存中。这些内存中的表示应映射到用于声明基础架构的原始 API。审计的 API 或对象状态通常需要保存。\n存储介质（有时称为状态存储）可用于存储新审计的 API。介质可以是任何传统存储系统，例如本地文件系统、云对象存储或数据库。如果数据存储在类似文件系统的存储中，那么该工具将很可能以逻辑方式对数据进行编码，以便可以在运行时轻松对数据进行编码和解码。常见的编码包括 JSON、YAML 和 TOML。\n当设计程序时您可能会想要将用于存储其他数据的特权信息存储起来。这究竟是不是最佳实践具体取决于您的安全性要求以及您计划存储数据的位置。\n记住存储机密可能是一个漏洞，这一点很重要。在设计软件来控制堆栈最基本的部分时，安全性至关重要。所以通常值得额外的努力来确保机密信息是安全的。\n除了存储有关程序和云提供商凭证的元信息之外，工程师还需要存储有关基础架构的信息。重要的是要记住，基础架构将以某种方式呈现，理想情况下，该程序易于解码。记住对系统进行更改不会立即发生，而随着时间的推移也很重要。\n存储这些数据并能够轻松访问是设计基础架构管理应用程序的重要部分。仅基础架构定义很可能就已经是系统中最具智慧价值的部分。我们来看一个基本的例子，看看这些数据和程序如何一起工作。\n重新审视例 4-1 至 4-4，因为它们被用作本章进一步演示的具体例子。\n文件系统状态存储示例\n想象一下，数据存储在一个名为 state 的目录中。在该目录中，有三个文件：\nmeta_information.yaml secrets.yaml infrastructure.yaml 这个简单的数据存储可以准确地封装需要保留的信息，以便有效管理基础架构。\nsecrets.yaml和infrastructure.yaml文件存储基础架构的表示形式，meta_information.yaml文件（示例 4-1）存储其他重要信息，例如基础架构上次调配时间，调配时间和日志信息。\n例 4-1. state/meta_information.yaml\nlastExecution: exitCode: 0 timestamp: 2017-08-01 15:32:11 +00:00 user: kris logFile: /var/log/infra.log 第二个文件secrets.yaml保存私人信息，用于在程序执行过程中以任意方式验证（例 4-2）。\n重申一下，以这种方式存储机密可能是不安全的。我们仅以secrets.yaml为例。\n例 4-2. state/secrets.yaml\napiAccessToken: a8233fc28d09a9c27b2e2f apiSecret: 8a2976744f239eaa9287f83b23309023d privateKeyPath: ~/.ssh/id_rsa 第三个文件infrastructure.yaml将包含 API 的编码表示形式，包括使用的 API 版本（示例 4-3）。我们可以在这里找到基础架构表示，例如网络和 DNS 信息，防火墙规则和虚拟机定义。\n例 4-3. state/infrastructure.yaml\nlocation: \u0026#34;San Francisco 2\u0026#34; name: infra1 dns: fqdn: infra.example.com network: cidr: 10.0.0.0/12 serverPools: - bootstrapScript: /opt/infra/bootstrap.sh diskSize: large workload: medium memory: medium subnetHostsCount: 256 firewalls: - rules: - ingressFromPort: 22 ingressProtocol: tcp ingressSource: 0.0.0.0/0 ingressToPort: 22 image: ubuntu-16-04-x64 起初infrastructure.yaml文件可能看起来只不过是基础架构代码的一个例子。但是，如果仔细观察，您会发现许多定义的指令都是具体基础架构之上的抽象。例如，subnetHostsCount指令是一个整数值并定义了子网中主机的预定数量。该程序将设法为运维人员划分网络中定义的更大的无类别域间路由（CIDR）值。运维人员不会声明子网，只需要声明有多少主机。软件会帮运维人员完成剩下的操作。\n程序运行时可能会更新 API 并将新的表示写入数据存储区（本案例中仅是一个文件）。继续我们的subnetHostsCount示例，假设程序确实为我们挑选了一个子网 CIDR。新的数据结构可能如例 4-4 所示。\nlocation: \u0026#34;San Francisco 2\u0026#34; name: infra1 dns: fqdn: infra.example.com network: cidr: 10.0.0.0/12 serverPools: - bootstrapScript: /opt/infra/bootstrap.sh diskSize: large workload: medium memory: medium subnetHostsCount: 256 assignedSubnetCIDR: 10.0.100.0/24 firewalls: - rules: - ingressFromPort: 22 ingressProtocol: tcp ingressSource: 0.0.0.0/0 ingressToPort: 22 image: ubuntu-16-04-x64 请注意程序如何编写 assignedSubnetCIDR 指令，而不是由运维人员操作。另外，请记住应用程序如何更新 API 是用户以软件方式与基础架构进行交互的标志。\n现在，请记住，这只是一个例子，并不一定主张使用抽象计算子网 CIDR。不同的用例可能需要在应用程序中进行不同的抽象和实现。关于构建基础架构应用程序的一个好处是，用户可以以任何他们认为可以解决自己问题的方式设计软件。\n数据存储（infrastructure.yaml文件）现在可以被认为是软件工程领域的传统数据存储。也就是说，该程序可以对文件进行完全的写入控制。\n我们会发现，这会带来风险，但对工程师来说也是一个巨大的胜利。基础架构表示不必存储在文件系统的文件中。相反，它可以存储在任何数据存储中，如传统数据库或键/值存储系统。\n为了理解软件如何处理这种新的基础架构表示的复杂性，我们必须理解系统中的两种状态——API 形式的预期状态，可在infrastructure.yaml文件中找到，另一种可以在现实（或审计）中观察到的实际状态。\n在这个例子中，软件还没有做任何事情或者采取任何行动，而我们正处于管理时间线的开始。因此，实际状态将是什么都没有，而预期状态将是封装在infrastructure.yaml文件中的任何状态。\n调节器模式 调节器模式（reconciler pattern）是一种软件模式，可用于管理云原生基础架构。该模式强化了基础架构的两种表现形 …","relpermalink":"/book/cloud-native-infra/designing-infrastructure-applicaitons/","summary":"在前一章中，我们了解了基础架构的各种表示以及围绕其部署工具的各种方法。在本章中，我们将看看如何设计部署和管理基础架构的应用程序。在上一章中我们重点关注基础架构即软件的开放世界，有时称为基础架构即应用。","title":"第 4 章：设计基础架构应用程序"},{"content":"各种 CI/CD 管道都涉及到参考平台（即基于微服务的应用，有提供基础设施服务的服务网格）。虽然参考应用是基于微服务的应用，但 DevSecOps 的原语可以应用于单体应用以及既在企业内部又基于云的应用（如混合云、单一公有云和多云）。\n在第 2.1 节中，我们提到了我们参考应用环境中的五种代码类型。我们还提到，也可以为这五种代码类型中的每一种创建单独的 CI/CD 管道。这五种代码类型在参考平台组件中的位置将被讨论，然后是描述相关 CI/CD 管道的单独章节：\n参考平台中的代码类型和相关的 CI/CD 管道（4.1 节） 应用程序代码和应用服务代码的 CI/CD 管道（4.2 节） 基础设施即代码（IaC）的 CI/CD 管道（4.3 节） 策略即代码的 CI/CD 管道（4.4 节） 可观测性即代码的 CI/CD 管道（4.5 节） 所有 CI/CD 管道的实施问题，无论代码类型如何，都将在以下章节中讨论：\n确保 CI/CD 管道的安全（4.6 节） CI/CD 管道中的工作流模型（4.7 节） CI/CD 管道中的安全测试（4.8 节） 本节还将考虑 DevSecOps 的整体优势，并在第 4.9 节和第 4.10 节分别介绍参考平台的具体优势和利用 DevSecOps 进行持续授权操作（C-ATO）的能力。\n本章大纲 4.1 代码类型和参考平台组件的描述\n4.2 应用程序代码和应用服务代码的 CI/CD 管道\n4.3 基础设施即代码的 CI/CD 管道\n4.4 策略即代码的 CI/CD 管道\n4.5 可观测性即代码的 CI/CD 管道\n4.6 确保 CI/CD 管道的安全\n4.7 CI/CD 管道中的工作流模型\n4.8 安全测试——所有代码类型的 CI/CD 管道的共同要求\n4.9 DevSecOps 原语对服务网格中应用安全的好处\n4.10 利用 DevSecOps 进行持续授权操作（C-ATO）\n开始阅读 ","relpermalink":"/book/service-mesh-devsecops/implement/","summary":"各种 CI/CD 管道都涉及到参考平台（即基于微服务的应用，有提供基础设施服务的服务网格）。虽然参考应用是基于微服务的应用，但 DevSecOps 的原语可以应用于单体应用以及既在企业内部又基于云的应用（如混合云、单一公有云和多云）","title":"第四章：为参考平台实施 DevSecOps 原语"},{"content":"Kubernetes 可以成为数据和 / 或计算能力盗窃的重要目标。虽然数据盗窃是传统上的主要动机，但寻求计算能力（通常用于加密货币挖掘）的网络行为者也被吸引到 Kubernetes 来利用其底层基础设施。除了资源盗窃，网络行为者还可能针对 Kubernetes 造成拒绝服务。下面的威胁代表了 Kubernetes 集群最可能的破坏源。\n供应链风险 - 对供应链的攻击载体是多种多样的，并且在减轻风险方面具有挑战性。供应链风险是指对手可能颠覆构成系统的任何元素的风险，包括帮助提供最终产品的产品组件、服务或人员。这可能包括用于创建和管理 Kubernetes 集群的第三方软件和供应商。供应链的潜在威胁会在多个层面上影响 Kubernetes，包括： 容器 / 应用层面 - 在 Kubernetes 中运行的应用及其第三方依赖的安全性，它们依赖于开发者的可信度和开发基础设施的防御能力。来自第三方的恶意容器或应用程序可以为网络行为者在集群中提供一个立足点。 基础设施 - 托管 Kubernetes 的底层系统有其自身的软件和硬件依赖性。系统作为工作节点或控制平面一部分的，任何潜在威胁都可能为网络行为者在集群中提供一个立足点。 恶意威胁行为者 - 恶意行为者经常利用漏洞从远程位置获得访问权。Kubernetes 架构暴露了几个 API，网络行为者有可能利用这些 API 进行远程利用。 控制平面 - Kubernetes 控制平面有各种组件，通过通信来跟踪和管理集群。网络行为者经常利用缺乏适当访问控制的暴露的控制平面组件。 工作节点 - 除了运行容器引擎外，工作者节点还承载着 kubelet 和 kube-proxy 服务，这些都有可能被网络行为者利用。此外，工作节点存在于被锁定的控制平面之外，可能更容易被网络行为者利用。 容器化的应用程序 - 在集群内运行的应用程序是常见的目标。应用程序经常可以在集群之外访问，使它们可以被远程网络行为者接触到。然后，网络行为者可以从已经被破坏的应用出发，或者利用暴露的应用程序的内部可访问资源在集群中提升权限。 内部威胁 - 威胁者可以利用漏洞或使用个人在组织内工作时获得的特权。来自组织内部的个人被赋予特殊的知识和特权，可以用来威胁 Kubernetes 集群。 管理员 - Kubernetes 管理员对运行中的容器有控制权，包括在容器化环境中执行任意命令的能力。Kubernetes 强制的 RBAC 授权可以通过限制对敏感能力的访问来帮助降低风险。然而，由于 Kubernetes 缺乏双人制的完整性控制，即必须有至少一个管理账户才能够获得集群的控制权。管理员通常有对系统或管理程序的物理访问权，这也可能被用来破坏 Kubernetes 环境。 用户 - 容器化应用程序的用户可能有知识和凭证来访问 Kubernetes 集群中的容器化服务。这种程度的访问可以提供足够的手段来利用应用程序本身或其他集群组件。 云服务或基础设施供应商 - 对管理 Kubernetes 节点的物理系统或管理程序的访问可被用来破坏 Kubernetes 环境。云服务提供商通常有多层技术和管理控制，以保护系统免受特权管理员的影响。 ","relpermalink":"/book/kubernetes-hardening-guidance/threat-model/","summary":"Kubernetes 可以成为数据和 / 或计算能力盗窃的重要目标。虽然数据盗窃是传统上的主要动机，但寻求计算能力（通常用于加密货币挖掘）的网络行为者也被吸引到 Kubernetes 来利用其底层基础设施。除了资源盗窃，网络行为者还可能针对 Kubernetes 造成","title":"威胁建模"},{"content":"第 2 章描述了实现自动分析所需的数据模型，第 4 章描述了支持原生 开源仪表的额外要求，并赋予各角色（应用程序所有者、运维和响应者）自主权。这就是我们对现代可观测性的概念模型。\n在本报告的其余部分，我们描述了这个新模型的一个事实实现，即 OpenTelemetry。这一章描述了构成 OpenTelemetry 遥测管道的所有组件。后面的章节将描述稳定性保证、建议的设置以及 OpenTelemetry 现实中的部署策略。有关该项目的更多细节可以在附录中找到。\n信号 OpenTelemetry 规范被组织成不同类型的遥测，我们称之为信号（signal）。主要的信号是追踪。日志和度量是其他例子。信号是 OpenTelemetry 中最基本的设计单位。\n每一个额外的信号首先是独立开发的，然后与追踪和其他相关信号整合。这种分离允许开发新的、实验性的信号，而不影响已经变得稳定的信号的兼容性保证。\nOpenTelemetry 是一个跨领域的关注点（cross-cutting concern），它在事务通过每个库和服务时追踪其执行。为了达到这个目的，所有的信号都建立在低级别的上下文传播系统之上，该系统为信号提供了一个地方来存储它们需要与当前正在执行的代码相关联的任何事务级数据。因为上下文传播系统与追踪系统是完全分开的，其他跨领域的问题也可以利用它。图 5-1 说明了这个分层结构。\n图 5-1：所有 OpenTelemetry 信号都建立在一个共享的上下文传播系统之上。其他的，非可观测性的交叉关注也可以使用上下文传播机制来通过分布式系统传输他们的数据。 上下文（Context） 上下文对象是一个与执行上下文相关联的键值存储，例如线程或循环程序。如何实现这一点取决于语言，但 OpenTelemetry 在每种语言中都提供一个上下文对象。\n信号在上下文对象中存储它们的数据。因为 OpenTelemetry 的 API 调用总是可以访问整个上下文对象，所以信号有可能成为集成的，并在上下文共享数据，而不需要改变 API。例如，如果追踪和度量信号都被启用，记录一个度量可以自动创建一个追踪范例。日志也是如此：如果有的话，日志会自动绑定到当前的追踪。\n传播器（Propagator） 为了使分布式追踪发挥作用，追踪上下文必须被参与事务的每个服务所共享。传播器通过序列化和反序列化上下文对象来实现这一点，允许信号在网络工作请求中追踪其事务。\n追踪（Tracing） OpenTelemetry 追踪系统是基于 OpenTracing 和 OpenCensus。这两个系统，以及流行的 Zipkin 和 Jaeger 项目，都是基于谷歌开发的 Dapper 追踪系统。OpenTelemetry 试图与所有这些基于 Dapper 的系统兼容。\nOpenTelemetry 追踪包括一个叫做 链接（link） 的概念，它允许单独的追踪被组合成一个更大的图。这被用来连接事务和后台处理，以及观察大型异步系统，如 Kafka 和 AMQP。\n指标（Metric） 度量指标（metric）是一个很大的话题，包含各种各样的方法和实现。OpenTelemetry 度量信号被设计成与 Prometheus 和 StatsD 完全兼容。\n指标包括追踪样本，自动将指标与产生它们的追踪样本联系起来。手工将指标和追踪联系起来往往是一项繁琐且容易出错的任务，自动执行这项任务将为运维人员节省大量的时间。\n日志（Log） OpenTelemetry 结合了高度结构化的日志 API 和高速日志处理系统。现有的日志 API 可以连接到 OpenTelemetry，避免了对应用程序的重新测量。\n每当它出现的时候，日志就会自动附加到当前的追踪中。这使得事务日志很容易找到，并允许自动分析，以找到同一追踪中的日志之间的准确关联。\nBaggage OpenTelemetry Baggage 是一个简单但通用的键值系统。一旦数据被添加为 Baggage（包袱）它就可以被所有下游服务访问。这允许有用的信息，如账户和项目 ID，在事务的后期变得可用，而不需要从数据库中重新获取它们。例如，一个使用项目 ID 作为索引的前端服务可以将其作为 Baggage 添加，允许后端服务也通过项目 ID 对其跨度和指标进行索引。\n你可以将 Baggage 看做是一种分布式文本的形式。直接放入上下文对象的项目只能在当前服务中访问。与追踪上下文一样，作为 Baggage 添加的项目被作为 header 注入网络请求，允许下游服务提取它们。\n与上下文对象一样，Baggage 本身不是一个可观测性工具。它更像是一个通用的数据存储和传输系统。除了可观测性之外，其他跨领域的工具，例如，功能标记、A/B 测试和认证，可以使用 Baggage 来存储他们需要追踪当前事务的任何状态。\n然而，Baggage 是有代价的。因为每增加一个项目都必须被编码为一个头，每增加一个项目都会增加事务中每一个后续网络请求的大小。这就是为什么我们称它为 Baggage。我建议，Baggage 要少用，作为交叉关注的一部分。Baggage 不应该被用作明确定义的服务 API 的 “方便” 替代品，以明确地向下游应用程序发送参数。\nOpenTelemetry 客户端架构 应用程序通过安装一系列的软件库来检测 OpenTelemetry：API、SDK（软件开发工具包）、SDK 插件和库检测。这套库被称为 OpenTelemetry 客户端。图 5-2 显示了这些组件之间的关系。\n图 5-2：OpenTelemetry 客户端架构。为了帮助管理依赖性，OpenTelemetry 将实现与仪表使用的 API 分开。 在许多语言中，OpenTelemetry 提供安装程序，这有助于自动安装和设置 OpenTelemetry 客户端。然而，可用的自动化程度取决于语言。在 Java 中，OpenTelemetry 提供了一个 Java 代理，它通过动态地注入所有必要的组件来实现安装的完全自动化。在 Go 中，OpenTelemetry 包必须通过编写代码来安装和初始化，就像任何其他 Go 包一样。Python、Ruby 和 NodeJS 介于两者之间，提供不同程度的自动化。\n在学习 OpenTelemetry 时，了解在你使用的语言中如何设置是很重要的。特别是，一定要学习如何安装仪表，因为不同的语言有很大的不同。\n请查看客户端文档 ，了解更多的入门细节。\n客户端架构：仪表 API OpenTelemetry API 是指用于编写仪表的一组组件。该 API 被设计成可以直接嵌入到开放源码软件库以及应用程序中。这是 OpenTelemetry 的唯一部分，共享库和应用逻辑应该直接依赖它。\n提供者（Provider） API 与任何实现完全分开。当一个应用程序启动时，可以通过为每个信号注册一个提供者来加载一个实现。提供者成为所有 API 调用的接收者。\n当没有加载提供者时，API 默认为无操作提供者。这使得 OpenTelemetry 仪表化可以安全地包含在共享库中。如果应用程序不使用 OpenTelemetry，API 调用就会变成 no-ops，不会产生任何开销。\n对于生产使用，我们建议使用官方的 OpenTelemetry 提供商，我们称之为 OpenTelemetry SDK。\n为什么有多个实现方案？\nAPI 和实现的分离有很多好处。但是，如果用户被迫总是安装官方的 OpenTelemetry SDK，这又有什么意义？是否有必要安装另一个实现？SDK 已经具有很强的扩展性。\n我们相信是有的。虽然我们希望 OpenTelemetry 仪表是通用的，但建立一个对所有用例都理想的单一实现是不可能的。尽管我们相信 OpenTelemetry SDK 很好，但也应该有一个选择，那就是使用另一种实现。实现的灵活性是提供通用仪表 API 的一个关键特征。\n首先，这种分离保证了 OpenTelemetry 不会产生无法克服的依赖冲突。我们总是可以选择加载一个包括不同依赖链的实现。\n另一个原因是性能。OpenTelemetry SDK 是一个可扩展的、通用的框架。虽然 SDK 的设计是为了尽可能地提高性能，但扩展性和性能总是要权衡一下的。例如，通过外来函数接口创建与 OpenTelemetry C++ SDK 的绑定，有可能成为 Ruby、Python 和 Node.js 等动态脚本语言的一个非常有效的选择。\n还有一些流媒体架构显示了有希望的性能提升。在许多这样的优化解决方案中，编写插件和生命周期钩子的能力将受到严重限制；支持这些类型的功能所需的数据结构在这些优化解决方案中是不存在的。归根结底，没有 “完美的实现”；只有权衡。\nAPI/SDK 的分离是一个关键的设计选择，该项目大量使用了这一点。例如，除了 SDK 之外，每一种语言都有一个 no-op 的实现，它是默认安装的。还有一个 Fake/Mock 实现，我们用它来测试。而且，还有可能实现更多创造性的实现。例如，为分布式系统建立开发者工具，如一个实时调试器，它可以跨越网络边界工作。\n客户端架构：SDK OpenTelemetry 项目为 OpenTelemetry API 提供了一个官方实现，我们称之为 OpenTelemetry SDK。该 SDK 通过提供一个插件框架来实现 OpenTelemetry API。下面将介绍追踪 SDK；类似的架构也适用于度量和日志。\n基本数据结构是一个无锁的 SpanData 对象。当用户开始一个跨度时，SpanData 对象被创建，当用户添加属性和事件时，它被自动建立起来。一旦一个跨度结束，SpanData 对象将不再被更新，可以安全地传递给后台线程。\nSDK 的插件架构被组织成一个流水线。对于追踪来说，该管道由一连串的 SpanProcessors 组成。每个处理器对 SpanData 对象进行两次同步访问：一次是在跨度开始时，另一次是在跨度结束后。采样器、日志附加器和数据清洗器是 SpanProcessors 的例子。链中的最后一个处理器通常是一个 BatchSpanProcessor，它管理着一个已完成的跨度的缓冲区。输出器可以连接到 BatchSpanProcessor，通过网络将成批的跨度传递到遥测管道中的下一个服务，通常将它们发送到收集器或直接发送到追踪后端。一旦跨度被导出，管道就完成了，SpanData 对象也被释放。\n采样器（Sampler） OpenTelemetry 提供了几种常见的采样算法，包括前期采样和基于优先级的采样。采样可以帮助控制成本，但它是有代价的：你将会错过数据。在启用任何种类的采样算法之前，重要的是要检查你计划使用的分析工具支持哪些类型的采用。意外的采样可能会破坏某些形式的分析。一些工具需要他们自己的采样插件。例如，AWS X-Ray 使用它自己的采样算法，它可以作为 AWS 特定的采样插件使用。\n导出器（Exporter） OpenTelemetry 为 OTLP（OpenTelemetry Protocol）、Jaeger、Zipkin、Prometheus 和 StatsD 提供导出器。由第三方维护的其他导出器可以在每种语言的 OpenTelemetry-Contrib 资源库中找到。使用 OpenTelemetry 注册表 来了解目前有哪些插件可用。\n客户端架构：库仪表化 为了正常工作，OpenTelemetry 需要端到端的工具。这不是可有可无的：如果关键的库不包括仪表，上下文传播将被破坏。\n一般来说，必须检测的库包括 HTTP 客户端、HTTP 服务器、应用框架、消息传递 / 队列系统和数据库客户端。这些库经常在上下文传播中起作用。\nHTTP 客户端必须创建一个客户端 span 来记录请求。客户端还必须使用一个传播器，将当前的上下文作为一组 HTTP 头信息注入到请求中。 HTTP 服务器（应用框架）必须使用一个传播器来从 HTTP 头信息中提取 …","relpermalink":"/book/opentelemetry-obervability/architectural-overview/","summary":"第 5 章：OpenTelemetry 架构概述","title":"第 5 章：OpenTelemetry 架构概述"},{"content":"有时开发人员会拖延（Pushback）代码审查。他们要么不同意您的建议，要么抱怨您太严格。\n谁是对的？ 当开发人员不同意您的建议时，请先花点时间考虑一下是否正确。通常，他们比你更接近代码，所以他们可能真的对它的某些方面有更好的洞察力。他们的论点有意义吗？从代码健康的角度来看它是否有意义？如果是这样，让他们知道他们是对的，把问题解决。\n但是，开发人员并不总是对的。在这种情况下，审查人应进一步解释为什么认为他们的建议是正确的。好的解释在描述对开发人员回复的理解的同时，还会解释为什么请求更改。\n特别是，当审查人员认为他们的建议会改善代码健康状况时，他们应该继续提倡更改，如果他们认为最终的代码质量改进能够证明所需的额外工作是合理的。提高代码健康状况往往只需很小的几步。\n有时需要几轮解释一个建议才能才能让对方真正理解你的用意。只要确保始终保持礼貌 ，让开发人员知道你有听到他们在说什么，只是你不同意该论点而已。\n沮丧的开发者 审查者有时认为，如果审查者人坚持改进，开发人员会感到不安。有时候开发人员会感到很沮丧，但这样的感觉通常只会持续很短的时间，后来他们会非常感谢您在提高代码质量方面给他们的帮助。通常情况下，如果您在评论中表现得很有礼貌 ，开发人员实际上根本不会感到沮丧，这些担忧都仅存在于审核者心中而已。开发者感到沮丧通常更多地与评论的写作方式 有关，而不是审查者对代码质量的坚持。\n稍后清理 开发人员拖延的一个常见原因是开发人员（可以理解）希望完成任务。他们不想通过另一轮审查来完成该 CL。所以他们说会在以后的 CL 中清理一些东西，所以您现在应该 LGTM 这个 CL。一些开发人员非常擅长这一点，并会立即编写一个修复问题的后续 CL。但是，经验表明，在开发人员编写原始 CL 后，经过越长的时间这种清理发生的可能性就越小。实际上，通常除非开发人员在当前 CL 之后立即进行清理，否则它就永远不会发生。这不是因为开发人员不负责任，而是因为他们有很多工作要做，清理工作在其他工作中被丢失或遗忘。因此，在代码进入代码库并“完成”之前，通常最好坚持让开发人员现在清理他们的 CL。让人们“稍后清理东西”是代码库质量退化的常见原因。\n如果 CL 引入了新的复杂性，除非是紧急情况 ，否则必须在提交之前将其清除。如果 CL 暴露了相关的问题并且现在无法解决，那么开发人员应该将 bug 记录下来并分配给自己，避免后续被遗忘。又或者他们可以选择在程序中留下 TODO 的注释并连结到刚记录下的 bug。\n关于严格性的抱怨 如果您以前有相当宽松的代码审查，并转而进行严格的审查，一些开发人员会抱怨得非常大声。通常提高代码审查的速度 会让这些抱怨逐渐消失。\n有时，这些投诉可能需要数月才会消失，但最终开发人员往往会看到严格的代码审查的价值，因为他们会看到代码审查帮助生成的优秀代码。而且一旦发生某些事情时，最响亮的抗议者甚至可能会成为你最坚定的支持者，因为他们会看到审核变严格后所带来的价值。\n解决冲突 如果上述所有操作仍无法解决您与开发人员之间的冲突，请参阅“Code Review 标准 ”以获取有助于解决冲突的指导和原则。\n","relpermalink":"/book/eng-practices/review/reviewer/pushback/","summary":"有时开发人员会拖延（Pushback）代码审查。他们要么不同意您的建议，要么抱怨您太严格。 谁是对的？ 当开发人员不同意您的建议时，请先花点时间考虑一下是否正确。通常，他们比你更接近代码，所以他们可能真的","title":"处理 Code Review 中的拖延"},{"content":"背景 SPIFFE 规范定义了建立一个平台无关的工作负载身份框架所需的文档和接口，该框架能够在不需要实现身份转换或凭证交换逻辑的情况下连接不同域中的系统。它们定义了一个“信任域 ”，它作为一个身份命名空间。\nSPIFFE 的本质是分散的。每个信任域都根据自己的授权行事，与驻留在其他信任域中的系统在管理上是隔离的。虽然信任域划定了行政和/或安全域，但核心的 SPIFFE 用例是在需要时跨越这些边界进行通信。因此，有必要定义一种机制，使实体可以被引入到外部信任域中，从而允许其验证由“其他”SPIFFE 授权机构颁发的凭证，并允许一个信任域中的工作负载安全地验证一个外部信任域中的工作负载。\nSPIFFE 包 是一个包含验证特定信任域凭证所需的公钥材料的资源。本文档介绍了一种规范，用于安全地获取 SPIFFE 包，以便验证外部机构颁发的身份。其中包括有关如何提供 SPIFFE 包、如何检索 SPIFFE 包以及如何验证提供它们的端点的信息。\n简介 SPIFFE 联邦使得在信任域之间验证身份凭证 (SVIDs) 成为可能。具体来说，它是获取验证来自不同信任域颁发的 SVIDs 所需的 SPIFFE 包的行为，并将这些包提供给执行验证的工作负载。\n为了验证来自一个信任域的 SVIDs，必须拥有该信任域的包。因此，实现 SPIFFE 联邦需要在信任域之间交换 SPIFFE 包。这种交换应该定期发生，以允许信任域包的内容随时间变化。\n为了实现这一点，SPIFFE 联邦定义了一个“包端点”，它是一个 URL，用于为特定的信任域提供 SPIFFE 包。还定义了一组“端点配置文件”，它们指定了包端点服务器和客户端之间使用的协议和身份验证语义。最后，本文档进一步指定了包端点客户端和服务器的行为，以及联邦关系的管理和生成的包数据。\n目标用例 最终，SPIFFE 联邦使得工作负载能够对其他信任域中的对等方进行身份验证。这个功能对于支持各种用例至关重要，但我们希望重点关注三个核心用例。\nSPIFFE 信任域经常用于将同一公司或组织中不同信任级别的环境进行分割。例如，可以在暂存和生产环境之间、PCI 和非 PCI 环境之间进行分割。在这些情况下，每个域中使用的 SPIFFE 部署共享一个共同的管理机构，并且很可能由相同的实现支持。这是一个重要的区别，因为它意味着不同的部署可以就某些事情达成一致（例如命名方案），并且每个部署的安全姿态可以被其他部署了解和理解。\n其次，SPIFFE 联邦也被用于在不同公司或组织之间的信任域之间进行联邦。这种情况与第一种情况相似，我们正在对 SPIFFE 部署进行联邦，但由于可能存在的实现和管理差异，协调通常仅限于在此处描述的 SPIFFE 联邦协议中交换的数据。\n最后，SPIFFE 联邦还可以为尚未部署成熟 SPIFFE 控制平面的客户端提供用例。例如，托管产品可能希望使用客户端的 SPIFFE 身份对其客户进行身份验证，而无需内部实现或部署 SPIFFE。这可以通过允许工作负载直接获取客户端的信任域绑定来实现，以便对其调用者进行身份验证，从而避免了承诺部署完整的 SPIFFE 的需求。\nSPIFFE Bundle 端点 SPIFFE Bundle 端点是一个资源（由 URL 表示），用于提供一个信任域的 SPIFFE Bundle 的副本。SPIFFE 控制平面可以同时暴露和使用这些端点，以便在它们之间传输 bundle，从而实现联邦。\nSPIFFE Bundle 端点的语义类似于 OpenID Connect 规范中定义的jwks_uri机制，因为 bundle 包含了一个或多个用于在信任域内证明身份的公共加密密钥。Bundle 端点是一个 HTTPS URL，对 HTTP GET 请求做出 SPIFFE bundle 的响应。\n添加和删除密钥 信任域的操作者可以根据需要（例如，作为内部密钥轮换过程的一部分）引入或删除用于颁发 SVID 的密钥。在添加新密钥时，应提前发布包含密钥的更新信任捆绑包到捆绑包端点，以便外部信任域有机会检索和内部传播新捆绑包内容；建议提前时间为捆绑包的spiffe_refresh_hint的 3-5 倍。至少，在使用密钥颁发 SVID 之前，新密钥必须发布到捆绑包端点。\n当信任域不再颁发来自这些密钥的活动有效 SVID 时，应从信任捆绑包中删除已弃用的密钥。如果在将密钥添加到捆绑包中或从捆绑包中删除密钥时不遵循这些建议，可能会导致暂时的跨域身份验证失败。\n更新信任捆绑包的要求不适用于仅用于内部使用的颁发 SVID 的密钥。\n应定期轮询捆绑包端点以获取更新，因为其内容预计会随时间更改 - 常见的密钥有效期通常为几周甚至几天。客户端应以与捆绑包的spiffe_refresh_hint值相等的频率轮询。如果未设置，应适用合理低的默认值 - 建议为五分钟。\n管理获取的 Bundle Bundle 终端的客户端应在每次检索到 Bundle 时存储最新的 SPIFFE Bundle。当比较两个 Trust Bundle 的新鲜度或顺序时，应使用 Trust Bundle 的序列号字段。如果 Trust Bundle 省略了序列号，操作员应将最近检索到的 Bundle 视为最新的。\n操作员可以随时在外部信任域中本地更新 SPIFFE Bundle。在这种情况下，本地更新的 Bundle 版本将被视为最新版本，直到被后续的刷新替换。\n不同信任域的 Bundle 内容不得合并为单个更大的 Bundle。这样做将使一个信任域能够在验证器的眼中伪造属于另一个信任域的身份。因此，非常重要的是确保从外部信任域接收的 Bundle 保持清晰可辨，并明确反映它们所属的信任域名称。有关更多信息，请参阅安全注意事项部分。\n终端地址的稳定性 一旦外部信任域开始依赖于特定的终端 URL，将所有终端的客户端迁移到替代终端 URL 是一个复杂且容易出错的过程。因此，最安全的做法是优先选择稳定的终端 URL。\nSPIFFE Bundle 终端的提供和使用 本规范定义了两种基于 HTTPS 的 SPIFFE Bundle 终端服务器支持的配置文件。其中一种依赖于使用 Web PKI 对终端进行身份验证，另一种则利用 SPIFFE 身份验证。SPIFFE Bundle 终端客户端必须同时支持这两种配置文件，而 SPIFFE Bundle 终端服务器必须至少支持其中一种。\n支持基于 TLS 的配置文件（例如https_web或https_spiffe）的 Bundle 终端服务器必须遵守Mozilla 中间兼容性 要求，除非使用配置文件另有规定。\n端点参数 在从 SPIFFE 捆绑端点检索捆绑之前，客户端必须配置以下三个参数：（1）SPIFFE 捆绑端点的 URL，（2）端点配置文件类型，以及（3）与捆绑端点关联的信任域名称。前两个参数指示捆绑端点的位置和如何进行身份验证。由于信任捆绑不包含信任域名称，客户端使用第三个参数将已下载的捆绑与特定的信任域名称关联起来。特定的端点配置文件（例如https_spiffe，如下所述）可以定义其他强制的配置参数。\nBundle Endpoint URL:\t\u0026#34;\u0026lt;https://example.com/production/bundle.json\u0026gt;\u0026#34; Bundle Endpoint Profile:\t\u0026#34;https_web\u0026#34; Trust Domain:\t\u0026#34;prod.example.com\u0026#34; 图 1：用于信任域prod.example.com 的示例 SPIFFE 捆绑端点配置。管理员通过捆绑端点配置来检索外部信任捆绑。\n当控制平面将信任捆绑分发给工作负载时，必须通信信任域名称和信任捆绑之间的关联。有关这些参数的敏感性，请参见安全注意事项部分。\n图 2：在检索到外部 SPIFFE 信任捆绑后，控制平面将信任域名称和相应的捆绑分发给内部工作负载。工作负载使用此配置来验证外部信任域中的身份。有关信任捆绑内容的详细信息，请参见SPIFFE 信任域和捆绑 ，特别是SPIFFE 捆绑格式 和SPIFFE 捆绑示例 部分。\n本节中的要求适用于所有 SPIFFE 捆绑端点服务器和客户端。个别的 SPIFFE 捆绑端点配置文件可能会添加其他要求。\n端点配置文件 端点配置文件描述了在提供或使用捆绑端点时应使用的传输协议和身份验证方法。\n以下各节描述了受支持的捆绑端点配置文件。\nWeb PKI（https_web） https_web配置文件利用公信任的证书颁发机构提供了一种低摩擦的方式来配置 SPIFFE 联邦。当访问网页时，它的行为与大多数人熟悉的“https”URL 完全相同。在此配置文件中，捆绑点服务器使用由公共 CA 颁发的证书，无需额外的客户端配置；使用https_web配置文件类型的端点使用通常安装在现代操作系统中的相同公共 CA 证书进行身份验证。\n有关使用公共证书颁发机构的更多信息，请参见安全注意事项部分。\n端点 URL 要求 使用https_web的捆绑点 URL 必须将方案设置为https，并且在授权组件中不能包括用户信息。此规范不限制 URL 的其他组件（由RFC 3986 第 3 节 定义）。\n例如，URL https://host.example.com/trust_domain 是 https_web 配置文件类型的有效 SPIFFE 捆绑点 URL。\n端点参数 https_web配置文件在功能上不需要任何额外的参数，除了每个配置文件都需要的参数（即信任域名、配置文件类型和端点 URL）。\n提供 Bundle 端点 支持https_web传输类型的 SPIFFE bundle 端点服务器使用标准的 TLS 保护的 HTTP（即 HTTPS）。所使用的服务器证书应由公共证书颁发机构（根据 CA/Browser 论坛的成员名单定义）颁发，并且必须将端点的 DNS 名称或 IP 地址作为 X.509 主题备用名称（或通用名称）包含在内。\n作为互操作性问题，服务器不得要求对访问 bundle 端点进行客户端身份验证；这包括传输层（例如客户端证书）和 HTTP 层（例如身份验证标头）身份验证方案。\n在收到正确路径的 HTTP GET 请求后，bundle 端点服务器必须回复最新版本的可用 SPIFFE bundle。响应必须以 UTF-8 编码，并应在响应上设置Content-Type标头为application/json。此规范不限制提供 SPIFFE bundle 的路径。\n如果请求的 bundle 的授权机构已经更改，bundle 端点服务器可以使用 HTTP 重定向（根据RFC 7231 第 6.4 节 定义）进行响应。重定向的目标 URL 也必须是符合此配置文件中定义的有效的 bundle 端点 URL。服务器应使用临时重定向；重定向的支持是为了操作考虑（例如通过 CDN 提供 bundle），而不是作为永久迁移 bundle 端点 URL 的手段。有关详细信息，请参阅安全考虑事项。\nWeb PKI（https_web） 提供 Bundle 端点 支持https_web传输类型的 SPIFFE bundle 端点服务器使用标准的 TLS 保护的 HTTP（即 HTTPS）。所使用的服务器证书应由公共证书颁发机构（根据 CA/Browser 论坛的成员名单定义）颁发，并且必须将端点的 DNS 名称或 IP 地址作为 X.509 主题备用名称（或通用名称）包含在内。\n作为互操作性问题，服务器不得要求对访问 bundle 端点进行客户端身份验证；这包括传输层（例如客户端证书）和 HTTP 层（例如身份验证标头）身份验证方案。\n在收到正确路径的 HTTP GET 请求后，bundle 端点服务器必须回复最新版本的可用 SPIFFE bundle。响应必须以 UTF-8 编码，并应在响应上设置Content-Type标头为application/json。此规范不限制提供 SPIFFE bundle  …","relpermalink":"/book/spiffe-and-spire/standard/spiffe-federation/","summary":"背景 SPIFFE 规范定义了建立一个平台无关的工作负载身份框架所需的文档和接口，该框架能够在不需要实现身份转换或凭证交换逻辑的情况下连接不同域中的系统。它们定义了一个“信任域 ”，它作为一个身份命名空间。 SPIFFE 的本质是","title":"SPIFFE 联邦"},{"content":"本文档描述了如何为 Tetrate Service Bridge（TSB）配置 GitOps 集成。\nTSB 中的 GitOps 集成允许你与应用程序打包和部署的生命周期以及不同的持续部署（CD）系统进行集成。\n本文假设你已经具备了配置 GitOps CD 系统的工作知识，例如 FluxCD 或 ArgoCD 。\n工作原理 一旦在管理平面集群和/或应用程序集群中启用了 GitOps，CD 系统将能够将 TSB 配置应用于其中，然后将其推送到 TSB 管理平面。\n启用 GitOps 可以通过 ManagementPlane 或 ControlPlane CR 或 Helm 值为每个集群配置 GitOps 组件。\n注意 在同时在管理平面和控制平面中启用 GitOps 时，如果两个平面部署在同一个集群中（通常用于小型环境或演示安装 ），则只有两者之一会生效。具体来说，控制平面将是唯一启用的平面。这是为了避免两个平面多次推送相同的资源。 ManagementPlane 和 ControlPlane CR 都有一个名为 gitops 的组件，设置 enabled: true 将激活该集群的 GitOps。\nspec: components: ... gitops: enabled: true reconcileInterval: 600s 注意 在启用 GitOps 时，强烈建议以一种配置用户权限的方式，使得普通用户只能对 TSB 配置具有读取访问权限。这将有助于确保只有配置的集群服务账户可以管理配置。 在管理平面中启用 GitOps 以下是一个启用了 GitOps 的演示集群的自定义资源 YAML 示例，该管理平面部署在 tsb 命名空间中。如果使用 Helm，可以更新控制平面 Helm 值的 spec 部分。\nkubectl edit -n tsb managementplane/managementplane spec: components: ... gitops: enabled: true reconcileInterval: 600s 设置 enabled: true 将激活该管理平面集群的 GitOps。\n每当 CD 系统将资源应用于管理平面集群时，TSB GitOps 组件将它们推送到管理平面。此外，还有一个定期的协调过程，确保管理平面集群保持作为事实的源，并定期推送其中的信息。可以使用 reconcileInterval 属性来自定义后台协调过程运行的间隔。可以在GitOps 组件参考 中找到更多详细信息和其他配置选项。\n管理平面集群可以将配置推送到整个组织，而无需在该平面中授予任何特殊权限，一旦在该平面启用了 GitOps。\n在对 ManagementPlane CR 应用更改后，TSB Operator 将为集群激活该功能，并开始响应应用的 TSB K8s 资源。\n在控制平面中启用 GitOps 以下是一个启用了 GitOps 的演示集群的自定义资源 YAML 示例，该控制平面部署在 istio-system 命名空间中。如果使用 Helm，可以更新控制平面 Helm 值的 spec 部分。\nkubectl edit -n istio-system controlplane/controlplane spec: components: ... gitops: enabled: true reconcileInterval: 600s 设置 enabled: true 将激活该集群的 GitOps。\n每当 CD 系统将资源应用于应用程序集群时，TSB GitOps 组件将它们推送到管理平面。此外，还有一个定期的协调过程，确保应用程序集群保持作为事实的源，并定期推送其中的信息。可以使用 reconcileInterval 属性来自定义后台协调过程运行的间隔。可以在GitOps 组件参考 中找到更多详细信息和其他配置选项。\n与在管理平面中的情况不同，在授权应用程序集群将配置推送到管理平面之前，需要授予集群服务账户权限。可以通过以下方式轻松完成：\n$ tctl x gitops grant demo 这将授权推送配置到整个组织。如果要进一步限制集群服务账户可以推送配置的位置，请参阅命令文档：\n$ tctl x gitops grant --help 在对 ControlPlane CR 应用更改后，TSB Operator 将为集群激活该功能，并开始响应应用的 TSB K8s 资源。\n监控 GitOps 健康状况 GitOps 集成提供了指标和详细日志，可用于监控 GitOps 进程中涉及的不同组件的健康状况：\nGitOps 指标 提供了在将配置发送到管理平面时经历的延迟、错误率等。 ","relpermalink":"/book/tsb/operations/features/configure-gitops/","summary":"配置 Tetrate Service Bridge（TSB）资源的 GitOps 集成。","title":"GitOps"},{"content":" API 参考\nTSB CRD 参考\n","relpermalink":"/book/tsb/reference/k8s-api/","summary":"API 参考\nTSB CRD 参考","title":"Kubernetes API"},{"content":"本文描述了当标头中包含多个transfer-encoding:chunked时，TSB 将如何处理请求/响应，并帮助你确定问题是来自源还是目标。\n我们建议解决此问题的方法是确保请求头和响应头中都只有一个transfer-encoding:chunked，否则 Envoy 将拒绝请求。\n在开始之前，请确保你已经：\n熟悉TSB 概念 安装TSB 演示 环境 部署Istio Bookinfo 示例应用程序 注意：对于响应部分，我们在这里使用的应用程序特意生成了多个transfer-encoding:chunked标头，仅用于文档目的。\n我们经常看到请求/响应的标头包含多个transfer-encoding:chunked，这不是有效的标头，因为 Envoy 会拒绝这种请求。在我们安装的 bookinfo 应用程序中，我们可以更深入地看一看当 Envoy 拒绝简单请求时，它将以特定的错误代码拒绝。\n带有\u0026#34;Transfer-Encoding: chunked,chunked\u0026#34;的请求头 对于我们的 bookinfo 应用程序，我们将使用 curl 创建一个简单的请求，发送多个transfer-encoding:chunked，并观察 envoy 网关的响应。\n$ curl -kv \u0026#34;http://bookinfo.tetrate.com/productpage\u0026#34; -H \u0026#34;Transfer-Encoding: chunked\u0026#34; -H \u0026#34;Transfer-Encoding: chunked\u0026#34; [ ... ] \u0026gt; GET /productpage HTTP/1.1 \u0026gt; Host: bookinfo.tetrate.com \u0026gt; User-Agent: curl/7.79.1 \u0026gt; Accept: */* \u0026gt; Transfer-Encoding: chunked \u0026gt; Transfer-Encoding: chunked \u0026gt; * Mark bundle as not supporting multiuse \u0026lt; HTTP/1.1 501 Not Implemented \u0026lt; content-length: 15 \u0026lt; content-type: text/plain \u0026lt; date: Tue, 13 Sep 2022 11:08:56 GMT \u0026lt; server: istio-envoy \u0026lt; connection: close \u0026lt; * Closing connection 0 Not Implemented% 同时，网关 envoy 日志显示以下片段以及错误代码为501 DPE（下游协议错误）的失败。\nkubectl logs ${GWPOD} -n bookinfo [2022-09-07T08:17:38.936Z] \u0026#34;- - HTTP/1.1\u0026#34; 501 DPE http1.invalid_transfer_encoding - \u0026#34;-\u0026#34; 0 15 0 - \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; - - 10.0.2.20:8080 10.128.0.74:23365 - - 带有\u0026#34;Transfer-Encoding: chunked,chunked\u0026#34;的响应头 响应头可以在应用程序内部进行操作，并可能触发多个块。在这些情况下，该应用程序的 envoy sidecar 将拒绝响应。为了演示响应，我们使用了一个简单的应用程序，它将生成多个传输块作为默认行为，我们将从Debug-container 发送 curl 请求，使用默认值如下所示。\n$ curl -v http://transfer:8080/test [ ... ] \u0026gt; GET /test HTTP/1.1 \u0026gt; Host: transfer:8080 \u0026gt; User-Agent: curl/7.83.1 \u0026gt; Accept: */* \u0026gt; * Mark bundle as not supporting multiuse \u0026lt; HTTP/1.1 502 Bad Gateway \u0026lt; content-length: 87 \u0026lt; content-type: text/plain \u0026lt; date: Tue, 13 Sep 2022 11:17:13 GMT \u0026lt; server: envoy \u0026lt; x-envoy-upstream-service-time: 2 \u0026lt; * Connection #0 to host transfer left intact upstream connect error or disconnect/reset before headers. reset reason: protocol error/ 当我们查看 sidecar envoy 日志时，我们可以看到拒绝的详细信息，错误消息为502 UPE（上游协议错误）\nkubectl logs transfer-58c6c67c56-d8wzk -n test [2022-09-13T11:17:13.471Z] \u0026#34;GET /test HTTP/1.1\u0026#34; 502 UPE upstream_reset_before_response_started{protocol_error} - \u0026#34;-\u0026#34; 0 87 1 - \u0026#34;-\u0026#34; \u0026#34;curl/7.83.1\u0026#34; \u0026#34;fbcd5bff-1981-40a5-a2c8-fd6133161976\u0026#34; \u0026#34;transfer:8080\u0026#34; \u0026#34;10.0.2.7:8080\u0026#34; inbound|8080|| 127.0.0.6:53799 10.0.2.7:8080 10.0.0.21:59960 outbound_.8080_._.transfer.test.svc.cluster.local default 如果我们为应用程序的 sidecar 启用调试日志，我们可以看到详细错误信息如下：\n2022-09-13T12:46:48.497388Z debug envoy client [C2912] Error dispatching received data: http/1.1 protocol error: unsupported transfer encoding ","relpermalink":"/book/tsb/troubleshooting/multiple-transfer-encoding-chunked/","summary":"TSB 如何处理请求和响应标头中的多重“transfer-encoding:chunked”，以及如何确定问题是源自还是目的地。","title":"多重传输编码块处理"},{"content":"通过 TSB 和 TSE，你有多种选项可以配置你的平台，使应用程序在集群间以高可用性的方式运行。在每种情况下，一旦平台所有者（“平台”）合适地准备了平台，应用程序所有者（“应用”）只需要从两个或多个集群中部署和发布其服务，即可利用 HA 的优势。\n高可用性的选项 选项 1：使用 Tetrate 的 Edge Gateway 解决方案 使用 Tetrate 的 Edge Gateway 来为多个集群提供前端，并在它们之间分发流量。\n选项 2：使用 Tetrate 的 Route 53 控制器与 AWS Route 53 使用 AWS Route 53 控制器为从 TSE 或 TSB 发布的服务自动配置 Route 53。\n选项 3：手动配置 GSLB 解决方案 手动配置 GSLB 解决方案，以在集群入口点之间分发流量并执行健康检查。\n开始之前 在跨集群负载均衡时，你可能需要依赖 DNS GSLB 解决方案将流量分发到每个服务的入口点（边缘网关等）。在这种情况下，你需要考虑健康检查的功能。\n一旦在集群上部署了应用程序，可能需要详细的每应用程序健康检查，但首先，基础设施健康检查是一个很好的起点。健康检查的目的有两个：\n验证工作负载集群的功能和可达性：为此，通常只需运行一个简单的 ‘canary’ 服务，如 httpbin ，并验证它是否可以通过每个入口点访问。 确定到达每个工作负载集群的最佳方式：边缘和内部负载均衡器通常配置为在本地和远程代理或集群之间进行负载平衡。这确保了它们始终可以满足请求，即使这意味着使用远程目标。对于 GSLB 健康检查请求，请配置每一跃点只使用下一个本地跃点，如果不可用，则失败，以便对使用快速本地路径的入口点进行健康检查成功。 选项 1：配置 Tetrate 的 Edge Gateway Edge Gateway 解决方案 在 HA 设计指南 中有详细说明。通过 Edge Gateway，你可以在工作负载或专用集群中部署边缘负载均衡器。这些网关的目的是接收流量并将其（负载平衡）转发到目标服务的工作中的 Ingress Gateway。\n背景信息 Edge Gateways 由 Tetrate 平台管理，考虑到稳定性和可靠性。它们很少更新，并以尽可能简单的配置运行。它们通常部署在专用的 K8s 集群中，以最小化来自相邻工作负载的干扰或中断的可能性。如果你希望部署多个 Edge Gateway 以实现最大的高可用性，可以使用基本的 GSLB 解决方案来分发流量。\n请查看以下背景资源：\nTetrate HA 设计指南 你的 GSLB 供应商的最佳实践指南 启动一个新应用程序 当你使用 Tetrate 的 Edge Gateway 启动一个新应用程序时，需要在多个接触点配置流量流向：\n为工作负载集群上的 Ingress Gateways 的 Gateway 资源配置应用程序以从集群中发布应用程序。有关详细信息，请参阅 部署服务 内容。通常情况下，不需要为应用程序的工作负载集群实例配置 DNS 为边缘集群上的 Edge Gateways 的 Gateway 资源配置应用程序，以从 Edge Gateway 中发布应用程序并将流量分发到正常工作的工作负载集群实例。有关如何执行此操作以及可能适用的高可用性考虑事项的详细信息，请参阅 Tetrate HA 设计指南 。 为应用程序的 FQDN 配置 DNS，以将流量定向到 Edge Gateway 的正常实例。通常情况下，可以使用第三方的基于 DNS 的 GSLB 服务（例如，由你的云提供商提供的 AWS Route 53）或诸如 NS1、CloudFlare 或 Akamai 等云中立解决方案执行此操作。 具体的步骤由你选择的 Edge Gateway 配置和正在使用的 DNS GSLB 解决方案的性质密切定义。\n选项 2：使用 Tetrate 的 AWS Route 53 控制器与 AWS Route 53 在 Amazon EKS 上部署工作负载或 Edge Gateways 时，Tetrate 平台可以自动维护反映应用程序所有者或平台所有者对公开的应用程序和服务的意图的 Route 53 DNS 条目。Tetrate 的 Route 53 控制器监视 Gateway 资源并识别其中的 hostname DNS 值。只要匹配的 Route 53 托管区存在并且平台所有者已经允许访问，Route 53 控制器将配置并维护必要的 DNS 条目，以便客户可以通过网关访问工作负载。\n背景信息 请查看以下背景资源：\nTetrate Service Express - AWS Route 53 集成 Tetrate Service Express - 入门 - 发布服务 Tetrate Service Express - 入门 - 多集群和 Route 53 平台：准备集群 要使此功能可用于你的应用程序所有者，你需要执行三件事：\n创建 Route 53 托管区 为你计划使用的 dns 条目（域）创建必要的 Route 53 托管区，例如 .tetratelabs.io\n在每个集群上部署 Route 53 控制器 在每个集群上启用适当的 IAM 服务帐户，并部署 Tetrate Route 53 控制器。你可以使用 spec.providerSettings.route53.domainFilter 设置来限制可以从集群中管理哪些 Route 53 托管区。 我们强烈建议在每个集群上 安装 AWS 负载均衡器控制器 ，以实现 Tetrate 平台、Ingress Gateways 和 Route 53 配置及健康检查的最佳集成。\n解释应用程序所有者需要知道的内容 分享应用程序所有者需要知道以使用 Route 53 自动化的详细信息：\n对于简单的 Ingress 情况，在 Ingress Gateway 资源中使用正确的主机名就足够了，Route 53 控制器将提供 Route 53 简单的 DNS 资源 对于多集群、GSLB 情况，应用程序所有者将需要 使用 AWS 特定的注释来扩展其 Ingress Gateway 资源 选项 3：手动配置 GSLB 解决方案 选项 3： 你还可以选择使用第三方的 GSLB 解决方案来在 Tetrate 管理的端点（Edge Gateways 或 Ingress Gateways）之间分发流量。另外，CDN 可以为一组 Edge Gateways 和 Ingress Gateways 提供前端。\n应用程序：部署应用程序 你的管理员（平台所有者）将解释你需要了解的内容，以在集群间部署应用程序，配置健康检查并测试高可用性。评估标准取决于他们为准备平台采取的方法。\n通过适当的配置，你应该可以很好地控制你的服务是如何发布的，并在集群间共享流量。这将使你能够创建一个高可用性部署，以管理健康检查并操作常见任务，如为应用程序升级做准备之前排放集群。\n","relpermalink":"/book/tsb/design-guides/app-onboarding/high-availability/","summary":"通过 TSB 和 TSE，你有多种选项可以配置你的平台，使应用程序在集群间以高可用性的方式运行。在每种情况下，一旦平台所有者（“平台”）合适地准备了平台，应用程序所有者（“应","title":"跨集群实现高可用性"},{"content":"本文档介绍了如何在 TSB 中调整不同组件的日志级别，包括平台组件、Envoy Sidecar 和入口网关的运行时，以及查看日志的过程。\n在开始之前，请确保：\n你已正确安装和配置了 TSB。 你已安装和配置了kubectl以访问应用程序集群。 对于示例命令，我们假设你在helloworld命名空间中部署了一些应用程序。\nTSB 组件产生大量日志 小心在较长时间内启用 TSB 的各个范围的调试日志 - TSB 组件产生大量日志！由于自动日志摄取与 TSB 或 Sidecar 的日志级别提高相结合，可能会面临大额的日志摄取费用。 列出可用的组件 为了更改每个组件的日志级别，你需要知道哪些组件可用。为此，在tctl中有一个实用命令，它将利用当前的kubectl连接信息（上下文）并列出该集群中可用的组件。\n$ tctl experimental debug list-components PLANE COMPONENT DEPLOYMENTS management ldap ldap management mpc mpc management xcp central, xcp-operator-central management frontenvoy envoy management iamserver iam management apiserver tsb management tsb-operator tsb-operator-management-plane management web-ui web management zipkin zipkin management oap oap management collector otel-collector management postgres postgres control istio istio-operator control hpaadapter istio-system-custom-metrics-apiserver control oap oap-deployment control onboarding onboarding-operator control collector otel-collector control tsb-operator tsb-operator-control-plane control zipkin zipkin control xcp edge, xcp-operator-edge data operator istio-operator data tsb-operator tsb-operator-data-plane data bookinfo-gateway bookinfo/bookinfo-gateway data helloworld-tls-gateway helloworld-tls/helloworld-tls-gateway data helloworld-gateway helloworld/helloworld-gateway data httpbin-gateway httpbin/httpbin-gateway data tier1 tier1/tier1 如上所示，此命令将列出集群中的所有可用组件，并按平面（管理、控制或数据平面）对其进行排序。它还将显示构建每个组件的 Kubernetes 部署。输出中的PLANE和COMPONENT列是下面用于设置日志级别的命令中需要使用的内容。例如，要更改mpc组件的日志级别，你需要使用management/mpc来引用它。\nTSB 平台组件（管理和控制平面） TSB 组件能够在不重新启动 Pod 的情况下在运行时调整不同现有记录器的日志级别。为此，已添加了tctl CLI 中的新命令。\n为了检查组件的可用记录器以及检查当前级别，运行不带任何标志的命令。\ntctl experimental debug log-level management/iamserver 配置日志级别： POST /logging?level=value\t-\u0026gt; 配置全局级别 POST /logging?logger=value\t-\u0026gt; 配置\u0026#39;logger\u0026#39;的日志级别 当前的日志级别： admin info 管理服务器日志 auth info 认证消息服务器 config info 来自配置系统的消息 credentials/basic info 基本HTTP凭证解析提供程序 credentials/jwt info JWT负载凭证解析提供程序 default info 无作用域的日志消息。 dynadsn info 来自动态数据库连接池的消息 envoy-filter info Envoy过滤器消息 exchange info 令牌交换消息 grpc info 来自gRPC层的消息 health info 健康检查服务的消息 iam-server info RunGroup处理程序的消息 iam/http info 来自http服务器的消息 jwt info LDAP提供程序的消息 keyvalue/tx info 事务系统的消息 ldap info LDAP集成消息 local info 来自本地认证提供程序的消息 migrations info 数据库迁移消息 oauth info 服务器扩展的消息 oauth2 info OAuth2消息 oidc info OIDC提供程序的消息 root info 来自根凭证包的消息 server info 来自服务主要的消息 在上面的输出中，最左边的列显示记录器名称，中间的列显示为该给定记录器配置的当前日志级别，最后一列显示记录器显示的消息类型的简要描述。\n要更改日志级别，有多种方法可以完成，具体取决于level标志的不同组合。\n更改单个记录器 可以通过提供记录器名称，后跟冒号（:），然后是所需的级别来更改单个记录器。例如：\ntctl experimental debug log-level management/iamserver --level ldap:debug 配置日志级别： POST /logging?level=value\t-\u0026gt; 配置全局级别 POST /logging?logger=value\t-\u0026gt; 配置\u0026#39;logger\u0026#39;的日志级别 当前的日志级别： admin info 管理服务器日志 auth info 认证消息服务器 config info 来自配置系统的消息 credentials/basic info 基本HTTP凭 证解析提供程序 credentials/jwt info JWT负载凭证解析提供程序 default info 无作用域的日志消息。 dynadsn info 来自动态数据库连接池的消息 envoy-filter info Envoy过滤器消息 exchange info 令牌交换消息 grpc info 来自gRPC层的消息 health info 健康检查服务的消息 iam-server ldap RunGroup处理程序的消息 iam/http info 来自http服务器的消息 jwt info LDAP提供程序的消息 keyvalue/tx info 事务系统的消息 ldap info LDAP集成消息 local info 来自本地认证提供程序的消息 migrations info 数据库迁移消息 oauth info 服务器扩展的消息 oauth2 info OAuth2消息 oidc info OIDC提供程序的消息 root info 来自根凭证包的消息 server info 来自服务主要的消息 如上所示，通过指定--level标志并提供logger:level的格式，你可以更改单个记录器的日志级别。在这个例子中，我们将iamserver的日志级别更改为ldap:debug。\n更改多个记录器 要更改多个记录器的日志级别，可以使用逗号分隔它们，并将它们列在--level标志的值中。例如：\ntctl experimental debug log-level management/iamserver --level ldap:debug,auth:info 配置日志级别： POST /logging?level=value\t-\u0026gt; 配置全局级别 POST /logging?logger=value\t-\u0026gt; 配置\u0026#39;logger\u0026#39;的日志级别 当前的日志级别： admin info 管理服务器日志 auth info 认证消息服务器 config info 来自配置系统的消息 credentials/basic info 基本HTTP凭证解析提供程序 credentials/jwt info JWT负载凭证解析提供程序 default info 无作用域的日志消息。 dynadsn info 来自动态数据库连接池的消息 envoy-filter info Envoy过滤器消息 exchange info 令牌交换消息 grpc info 来自gRPC层的消息 health info 健康检查服务的消息 iam-server ldap RunGroup处理程序的消息 iam/http info 来自http服务器的消息 jwt info LDAP提供程序的消息 keyvalue/tx info 事务系统的消息 ldap debug LDAP集成消息 local info 来自本地认证提供程序的消息 migrations info 数据库迁移消息 oauth info 服务器扩展的消息 oauth2 info OAuth2消息 oidc info OIDC提供程序的消息 root info 来自根凭证包的消息 server info 来自服务主要的消息 在这个示例中，我们将iamserver的日志级别更改为ldap:debug，同时将auth的日志级别更改为info。\n更改全局级别 要更改全局日志级别，你可以使用POST /logging?level=value命令。例如，要将全局日志级别设置为debug，请运行以下命令：\ntctl experimental debug log-level management/iamserver --level debug 配置日志级别： POST /logging?level=value\t-\u0026gt; 配置全局级别 POST /logging?logger=value\t-\u0026gt; 配置\u0026#39;logger\u0026#39;的日志级别 当前的日志级别： admin debug 管理服务器日志 auth debug 认证消息服务器 config debug 来自配置系统的消息 credentials/basic debug 基本HTTP凭证解析提供程序 credentials/jwt debug JWT负载凭证解析提供程序 default debug 无作用域的日志消息。 dynadsn debug 来自动态数据库连接池的消息 envoy-filter debug Envoy过滤器消息 exchange debug 令牌交换消息 grpc debug 来自gRPC层的消息 health debug 健康检查服务的消息 iam-server debug RunGroup处理程序的消息 iam/http debug 来自http服务器的消息 jwt debug LDAP提供程序的消息 keyvalue/tx debug 事务系统的消息 ldap debug LDAP集成消息 local debug 来自本地认证提供程序的消息 migrations debug 数据库迁移消息 oauth debug 服务器扩展的消息 oauth2 debug OAuth2消息 oidc debug OIDC提供程序的消息 root debug 来自根凭证包的消息 server debug 来自服务主要的消 …","relpermalink":"/book/tsb/operations/configure-log-levels/","summary":"配置 TSB 组件的日志级别。","title":"配置日志级别"},{"content":"在本部分中，你将配置 Ingress Gateway 以允许外部流量到达 TSB 环境中的 bookinfo 应用程序。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户和工作区。 创建配置组。 配置权限。 创建入口网关对象 你将创建一个 Ingress Gateway 对象来为你的 bookinfo 应用程序启用外部流量。\n创建 ingress.yaml 创建一个名为 ingress.yaml 的文件，其中包含以下内容：\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: tsb-gateway-bookinfo namespace: bookinfo spec: selector: app: tsb-gateway-bookinfo servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; 使用 kubectl 应用配置：\nkubectl apply -f ingress.yaml 接下来，获取 Ingress Gateway IP（或 AWS 的主机名）并将其存储在环境变量中：\nexport GATEWAY_IP=$(kubectl -n bookinfo get service tsb-gateway-bookinfo -o jsonpath=\u0026#34;{.status.loadBalancer.ingress[0][\u0026#39;hostname\u0026#39;,\u0026#39;ip\u0026#39;]}\u0026#34;) 你可以使用以下方法验证分配的 IP：\necho $GATEWAY_IP 为网关配置 TLS 证书 现在，为你的网关设置 TLS 证书。如果你已为你的域准备好 TLS 证书，则可以直接使用它。或者，使用提供的脚本创建自签名证书。\n将以下脚本保存为 gen-cert.sh ，使其可执行，然后运行它：\nchmod +x gen-cert.sh ./gen-cert.sh bookinfo bookinfo.tetrate.com . 创建 Kubernetes 密钥来存储证书。将路径替换为密钥和证书文件的实际路径：\nkubectl -n bookinfo create secret tls bookinfo-certs \\ --key bookinfo.key \\ --cert bookinfo.crt 使用 UI 配置 Ingress 网关 从工作区列表中，单击“网关组”。 选择你之前创建的 bookinfo-gw 网关组。 导航到顶部选项卡上的网关设置以显示网关的配置视图。 单击配置项的名称可显示其可配置字段。如果该项目有子项，请通过单击左侧的箭头将其展开。 使用以下步骤配置网关，确保最后保存更改以避免验证错误： 添加一个新的 Ingress Gateway，默认名称为 default-ingressgateway 并将其重命名为 bookinfo-gw-ingress 。 将工作负载选择器设置为： 命名空间： bookinfo 标签： app ，值为 tsb-gateway-bookinfo 在 HTTP 服务器下，添加新的 HTTP 服务器： 姓名： bookinfo 端口： 8443 主机名： bookinfo.tetrate.com 配置服务器 TLS 设置： TLS 模式：简单 秘密名称： bookinfo-certs 在“路由设置”下，添加 HTTP 规则并配置路由： 服务主机： \u0026lt;namespace\u0026gt;/productpage.bookinfo.svc.cluster.local 端口： 9080 保存更改。 使用 tctl 配置 Ingress 网关 创建具有必要配置的 gateway.yaml 文件，然后使用 tctl 应用它：\ntctl apply -f gateway.yaml 测试入口流量 要测试你的入口是否正常工作，请使用以下 curl 命令，将 $GATEWAY_IP 替换为实际的入口网关 IP：\ncurl -k -s --connect-to bookinfo.tetrate.com:443:$GATEWAY_IP \\ \u0026#34;https://bookinfo.tetrate.com/productpage\u0026#34; | \\ grep -o \u0026#34;\u0026lt;title\u0026gt;.*\u0026lt;/title\u0026gt;\u0026#34; 访问 Bookinfo 用户界面 要访问 bookinfo UI，请更新你的 /etc/hosts 文件以使 bookinfo.tetrate.com 解析为你的 Ingress Gateway IP：\necho \u0026#34;$GATEWAY_IP bookinfo.tetrate.com\u0026#34; | sudo tee -a /etc/hosts 现在，你可以在浏览器中访问 https://bookinfo.tetrate.com/productpage 。请注意，由于是自签名证书，你的浏览器可能会显示安全警告。你通常可以通过浏览器中的“高级”或“继续”选项绕过此警告。\n","relpermalink":"/book/tsb/quickstart/ingress-gateway/","summary":"在本部分中，你将配置 Ingress Gateway 以允许外部流量到达 TSB 环境中的 bookinfo 应用程序。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户和工作区。 创建配置组。 配置权限。 创","title":"入口网关"},{"content":"在这个页面中，你将学习如何使用 tctl CLI 连接到 TSB 以及使用 CLI 的一些基础知识。\n在开始之前：\n安装 TSB 管理平面 (仅自管理) 下载 Tetrate Service Bridge CLI (tctl) 获取 TSB 的 organization 名称 - 你可以在 TSB UI 中找到，或在 TSB ManagementPlane CR 的安装时配置 TSB 提供了一个用户界面，但本站点中的大多数示例 - 以及大多数脚本和自动化 - 将使用 tctl CLI。本文档将涵盖如何登录，以便你可以使用 CLI，并提供更新凭据的步骤。\n概述 有三种配置 tctl 的方法：使用 tctl login、从 UI 下载凭据包，或手动配置它。本文档的其余部分详细描述了每种方法。\n首选的方法是使用 tctl login 连接到 TSB，使用 default 配置文件： tctl config clusters set default --bridge-address $TSB_ADDRESS tctl login Organization: tetrate Tenant: Username: admin Password: ***** Login Successful! # 现在可以开始使用: tctl get tenants NAME DISPLAY NAME DESCRIPTION tetrate Tetrate Default tenant 使用 tctl 连接的第二种简单方法是从 TSB UI 下载 bundle 并使用 tctl config import 导入： tctl config profiles import ~/Downloads/tctl-\u0026lt;your username\u0026gt;.config.yaml tctl config profiles set-current \u0026lt;your username\u0026gt;-tsb # 现在可以开始使用: tctl get tenants NAME DISPLAY NAME DESCRIPTION tetrate Tetrate Default tenant 最后，你可以创建自己的 user、cluster 和 profile（对于 LDAP 与 OIDC 使用稍有不同的标志）来登录： tctl config clusters set tsb-how-to-cluster --bridge-address $TSB_ADDRESS # 对于 OIDC tctl config users set tsb-how-to-user --org $TCTL_LOGIN_ORG --token $TSB_BEARER_TOKEN --refresh-token $TSB_REFRESH_TOKEN # 或者对于 LDAP # tctl config users set tsb-how-to-user --org $TCTL_LOGIN_ORG --username $TCTL_LOGIN_USERNAME --password $TCTL_LOGIN_PASSWORD tctl config profiles set tsb-how-to --cluster tsb-how-to-cluster --username tsb-how-to-user tctl config profiles set-current tsb-how-to # 现在可以开始使用: tctl get tenants NAME DISPLAY NAME DESCRIPTION tetrate Tetrate Default tenant 如果你使用用户名和密码登录，应该执行 tctl login 来交换密码以获取 OIDC 令牌。之后，你可以随时使用 tctl。例如，要在 TSB 中查找 tenants，只需执行 tctl get tenants。\n使用 tctl login 登录 连接到 TSB 的最简单方法是使用 tctl login 命令，该命令将处理为你交换凭据以获取 OIDC 令牌，并确保不将明文密码持久保存到磁盘上。\n要实现这一点，首先我们需要配置 tctl 使用 TSB 实例的地址。使用 default 配置文件最容易 - 如果你不想使用 default 配置文件，请跳到 手动配置 tctl 部分。\n获取 TSB 的地址 如果你的 kubeconfig 指向管理平面集群，你可以从 Kubernetes 服务中获取地址：\nexport TSB_ADDRESS=$(kubectl get svc -n tsb envoy --output jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;):8443 许多组织将通过 DNS 公开 TSB 的 UI（因此也是 API）；你应该使用它而不是原始 IP 地址。\n配置 default 配置文件 有了 TSB 的地址，我们将配置 tctl 连接到它：\ntctl config clusters set default --bridge-address $TSB_ADDRESS 使用 OIDC 登录 要使用 OIDC 凭证登录 TSB，可以使用 OIDC 设备代码流，该流已集成到 tctl 中：\ntctl login --use-device-code Organization: tetrate Tenant: Code: GGBD-NJPR Open browser page https://www.google.com/device and enter the code Login Successful! 在浏览器中进行 这将为你打开浏览器，以完成 OIDC 登录流程并生成令牌。 使用用户名和密码登录 登录时提供用户名和密码：\ntctl login Organization: tetrate Tenant: Username: admin Password: ***** Login Successful! 从 TSB 下载 tctl 配置 tctl 使用一个配置文件与 TSB 实例连接，类似于使用 kubeconfig 连接到 Kubernetes API 服务器。通过从 TSB UI 下载该配置文件，你可以使用 tctl 轻松连接。要访问这些凭据，请在浏览器中登录到 TSB UI，然后在右上角单击用户名，选择 操作 \u0026gt; 显示令牌信息 \u0026gt; 下载 tctl 配置。这将下载一个名为 tctl-\u0026lt;your username\u0026gt;.config.yaml 的文件。然后，你可以将其导入到 tctl 中，以永久保存：\ntctl config profiles import /path/to/tctl-\u0026lt;your username\u0026gt;.config.yaml tctl config profiles set-current \u0026lt;your username\u0026gt;-tsb 提示 tctl 将配置存储在你文件系统默认的配置目录中（由 Golang 确定）。在 Linux 上，这将是 $HOME/.config/tetrate/tctl/tctl_config，在 Darwin 上是 $HOME/Library/Application\\ Support/tetrate/tctl/tctl_config，在 Windows 上是 %AppData%/tetrate/tctl/tctl_config。这是保存密码或令牌的位置。当导入配置文件时，tctl 将将该文件中的凭据添加到其配置目录中的现有凭据中。 手动配置 tctl TSB 组织名称 在下面的示例中，假定你已将组织名称保存在环境变量 $TCTL_LOGIN_ORG 中。如果你刚刚完成演示安装，演示组织名称是 tetrate。你可以执行以下操作将该值保存在环境变量中：export TCTL_LOGIN_ORG=tetrate。 要使用 tctl 登录，首先必须配置 cluster (tctl config clusters)，然后是 user (tctl config users)，然后将两者结合到 profile 中，你将能够使用它来连接到 TSB 实例，就像使用 kubeconfig profile 一样 (tctl config profiles set-current ...)。有了这个 profile，你就可以使用 tctl login 命令配置凭据，并将这些凭据持久保存到磁盘上，以便将来连接到 TSB。\n为 profile 选择一个名称 tctl 有一个 default profile，就像 kubectl 一样，你可以在下面的命令中使用它，或者你可以选择自己的 profile 名称。在此演示中，创建一个名为 tsb-how-to 的 profile（但任何名称都可以，包括 default）。\n配置 tctl Cluster UI 和 TSB 的 API 都在相同的地址和端口上暴露。为了配置 tctl，你将需要该地址以开始。\n获取 TSB 的地址 如果你的 kubeconfig 指向管理平面集群，你可以从 Kubernetes 服务中获取地址：\nexport TSB_ADDRESS=$(kubectl get svc -n tsb envoy --output jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;):8443 许多组织将通过 DNS 公开 TSB 的 UI（因此也是 API）；你应该使用它而不是原始 IP 地址。\n创建 tctl Cluster 一旦获得了地址（$TSB_ADDRESS），你可以在 tctl 的配置中创建一个 cluster。将集群命名为 tsb-how-to-cluster：\ntctl config clusters set tsb-how-to-cluster --bridge-address $TSB_ADDRESS tctl 对象名称 你可以使用与 profile 相同的名称作为 cluster 的名称（例如 tctl config clusters set tsb-how-to --bridge-address $TSB_ADDRESS）。此示例使用不同的名称，以便更容易跟踪。 设置 tctl User 首先，你需要知道要使用的用户名。这将取决于 TSB 的安装方式：如果使用 OIDC，则将是你的企业电子邮件；如果使用 LDAP，则将是你通常的 LDAP 登录用户名；最后，你可以使用 TSB 的默认管理帐户登录，如果在安装中未禁用它。\nOIDC 用户登录 要使用 OIDC 凭证登录 TSB，请在浏览器中登录到 TSB UI，然后在右上角单击你的用户名，选择 操作 \u0026gt; 显示令牌信息。从该页面中，复制下 Bearer Token 和 Refresh Token，并将其导出为 TSB_BEARER_TOKEN 和 TSB_REFRESH_TOKEN：\nexport TSB_BEARER_TOKEN=HHVMW2.qhf9jBL1fMCazBe1umanDr5sNEuFcKtClAUxeWA...redacted export TSB_REFRESH_TOKEN=AJWXL6VmGUmvYfn43601RG.Bw+xr0IVQ43swidqAt1tHf...redacted tctl config users set tsb-how-to-user \\ --org $TCTL_LOGIN_ORG \\ --token $TSB_BEARER_TOKEN \\ --refresh-token $TSB_REFRESH_TOKEN LDAP（用户名 + 密码）用户登录 对于 LDAP 登录，你需要用户名和密码；你可以通过环境变量配置这些变量，或通过 CLI 传递它们：\nexport …","relpermalink":"/book/tsb/setup/tctl-connect/","summary":"配置 `tctl` 以连接到 TSB。","title":"使用 tctl 连接到 TSB"},{"content":"本文档描述了如何使用 Tier-1 网关 进行多集群流量切换。你将创建一个用于 Tier-1 网关部署的集群，以及两个用于运行 bookinfo 应用程序 的集群。\n每个应用程序集群都将配置一个 入口网关 ，用于将流量路由到 bookinfo 应用程序。最后，你将配置 Tier-1 网关，将流量从一个集群上运行的应用程序切换到另一个集群上运行的应用程序。\n在开始之前，请确保你已经：\n熟悉了 TSB 概念 熟悉了 TSB 管理平面 和 集群载入 。以下场景将假定你已经安装了 TSB 管理平面，并且已经将 tctl 配置为正确的管理平面。 Kubernetes 供应商 以下场景在 GKE Kubernetes 集群上进行了测试。然而，这里描述的步骤应该足够通用，可以在其他 Kubernetes 供应商上使用。 证书 本场景使用自签名证书来进行 Istio CA。这里的说明仅供演示目的。对于生产集群设置，强烈建议使用生产就绪的 CA。 Tier-1 网关 在 TSB 中，有两种接收入站流量的网关类型：Tier-1 网关和 Ingress 网关（也称为 Tier-2 网关）。Tier-1 网关将流量分发到其他集群中一个或多个 Ingress 网关，使用 Istio mTLS 进行通信。Ingress 网关将流量路由到部署了网关的集群中运行的一个或多个工作负载（业务应用服务）。\n关于 Tier-1 部署，有一些需要注意的地方：\n首先，默认情况下，部署了 Tier-1 网关的集群不能包含其他网关或工作负载。你必须使用专用集群进行 Tier-1 部署。从 TSB 1.6 开始，你可以通过允许在任何工作负载集群中部署 Tier-1 网关来放宽此要求。参见 在应用程序集群中运行 Tier1 网关 。\n在部署 Tier-1 和应用程序的集群上运行的 Istio 必须共享相同的根 CA。有关如何为多个集群上的 Istio 设置根和中间 CA，请参阅 Istio 文档中的 插入 CA 证书 。TSB 控制平面 Operator 将部署 Istio，并且 Istio 的 CA 将从插入 CA 证书步骤中描述的 secrets-mount 文件中读取证书。\n应用程序必须在两个集群中的相同命名空间中部署。这是因为你将为两个应用程序集群使用相同的 Ingress 网关配置。\n准备集群 下面的图像显示了你将在本文档中使用的部署架构。管理平面应该已经部署好了。\n你将创建一个 Tier-1 网关集群和两个应用程序集群。每个应用程序集群都有一个 Ingress 网关和应用程序工作负载。\n在你的云提供商中，创建上述三个集群：一个用于 Tier-1 网关，两个用于应用程序。\n然后，根据 插入 CA 证书 文档的描述，在每个集群中插入证书和密钥。\n应用程序集群中的 Tier1 网关 如果启用了 在应用程序集群中运行 Tier1 网关 ，你只能有两个集群。你需要调整后续步骤中的载入集群 YAML 和 tier1 工作区的命名空间选择器以适应这种情况。如果你选择将相同的网络分配给你的集群，网络可达性可能不相关。 载入 Tier-1 网关和应用程序集群 创建一个名为 traffic-shifting-clusters.yaml 的文件，其中包含以下内容。这将为我们的使用创建集群资源：Tier-1 集群命名为 t1，应用程序集群命名为 c1 和 c2。稍后在 TSB 配置对象中引用它们时，你将需要使用这些名称。\ntraffic-shifting-clusters.yaml # Application cluster 1. apiVersion: api.tsb.tetrate.io/v2 kind: Cluster metadata: name: c1 organization: tetrate spec: displayName: \u0026#39;Cluster 1\u0026#39; network: tier2 --- # Application cluster 2. apiVersion: api.tsb.tetrate.io/v2 kind: Cluster metadata: name: c2 organization: tetrate spec: displayName: \u0026#39;Cluster 2\u0026#39; network: tier2 --- # Tier-1 cluster apiVersion: api.tsb.tetrate.io/v2 kind: Cluster metadata: name: t1 organization: tetrate spec: displayName: \u0026#39;Tier-1 Cluster\u0026#39; network: tier1 tier1Cluster: true 使用 tctl 应用这些内容：\ntctl apply -f traffic-shifting-clusters.yaml 网络可达性 一个集群具有一个 network 字段，表示像 AWS/GCP/Azure 上的 VPC 之类的网络边界。同一网络中的所有集群都被假定为可以互相访问，用于多集群路由。如果\n你的集群位于不同的网络中，则必须适当配置它们，以使它们可以相互访问。\n请注意，在你创建的集群资源中，Tier-1 集群和应用程序集群已分配了不同的网络：Tier-1 集群的网络是 tier1，而两个应用程序集群的网络是 tier2。\n你将使用这些网络名称来告诉 TSB tier1 和 tier2 是可达的。创建一个名为 organization-settings.yaml 的文件，其中包含以下内容。\norganization-settings.yaml apiVersion: api.tsb.tetrate.io/v2 kind: OrganizationSetting metadata: name: tetrate-settings organization: tetrate spec: networkSettings: networkReachability: # clusters that belong to tier1 networks can reach # clusters that belong to tier2 networks. tier1: tier2 使用 tctl 应用这些内容：\ntctl apply -f organization-settings.yaml 在集群中安装控制平面组件 此时，集群已经注册到 TSB，但尚未载入。要载入这些集群，请按照使用 Helm 或 tctl 的方法进行集群载入步骤。\n当所有集群都正确载入时，你应该在 TSB UI 中看到以下信息。请注意，集群正在报告 Istio 和 TSB 代理的版本。\n部署应用程序和 Ingress 网关到应用程序集群 对于两个应用程序集群，执行以下操作：\n部署 bookinfo 应用程序 部署 Ingress 网关 要部署 Ingress 网关，请创建一个名为 bookinfo-ingress-deploy.yaml 的文件，其中包含以下内容：\nbookinfo-ingress-deploy.yaml ```yaml apiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: tsb-gateway-bookinfo namespace: bookinfo spec: kubeSpec: service: type: LoadBalancer ``` 使用 kubectl 应用这些内容：\nkubectl apply -f bookinfo-ingress-deploy.yaml 确保在应用 YAML 文件时将 kubectl 指向正确的集群。\n部署和配置 请注意，在部署应用程序和 Ingress 网关时，我们使用 kubectl。在 TSB 中，部署和配置是不同的概念，分别以不同的方式处理。你直接使用 kubectl 部署到集群，并通过 TSB 管理平面使用 tctl 进行配置。 网关服务类型 在此示例中，我们使用负载均衡器作为网关服务类型。根据你的 Kubernetes 环境（例如，裸机），你可能需要使用 NodePort。\n通常，负载均衡器类型在云提供商中都是可用的。在 GKE 上，这将启动一个网络负载均衡器，该负载均衡器将为你提供一个单独的 IP 地址，将所有流量转发到你的服务。如果在自己的基础设施上使用 Kubernetes，而不安装像 MetalLB 或 PureLB 这样的负载均衡器服务，那么你将需要使用 NodePort。NodePort 在所有节点（虚拟机）上打开一个特定的端口，任何发送到此端口的流量都将转发到服务。\n租户和工作区 在这个示例中，你将把 Tier-1 网关关联到一个 工作区 ，将两个 Ingress 网关关联到另一个工作区。你应该确保工作区和工作区所属的 租户 都已经正确配置。\n创建租户 如果你已经在 TSB 中配置了一个租户，可以跳过此部分。\n创建一个名为 traffic-shifting-tenant.yaml 的文件，其中包含以下内容。\ntraffic-shifting-tenant.yaml ```yaml apiVersion: api.tsb.tetrate.io/v2 kind: Tenant metadata: organization: tetrate name: tetrate ``` 使用 tctl 应用这些内容：\ntctl apply -f traffic-shifting-tenant.yaml 创建工作区 创建工作区以关联网关。创建一个名为 traffic-shifting-workspaces.yaml 的文件。\ntraffic-shifting-workspaces.yaml ```yaml # workspace for bookinfo apiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: tenant: tetrate organization: tetrate name: bookinfo-workspace spec: description: for bookinfo displayName: bookinfo namespaceSelector: names: - \u0026#39;c1/bookinfo\u0026#39; - \u0026#39;c2/bookinfo\u0026#39; --- # workspace for tier-1 apiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: tenant: tetrate organization: tetrate name: tier1-workspace spec: description: for tier1 displayName: tier1 namespaceSelector: names: - \u0026#39;t1/tier1\u0026#39; ``` 使用 tctl 应用这些内容：\ntctl apply -f traffic-shifting-workspaces.yaml 如果要使用现有的工作区，可以更新工作区以包括你刚刚创建的集群和命名空间，方法是更新工作区命名空间选择器。\n配置 Ingress 网关 接下来，你将配置 Ingress 网关以接收两个应用程序集群中的 bookinfo 应用程序的流量。\n在配置 Ingress 网关之前，请使用 此脚本 创建一个 TLS 证书。确保在两个应用程序集群的 bookinfo 命名空间中创建这些证书的 secrets。\n创建一个名为 traffic-shifting-bookinfo-ingress-config.yaml 的文件。 …","relpermalink":"/book/tsb/howto/gateway/multi-cluster-traffic-shifting/","summary":"部署 Tier-1 网关并使用它在多个集群之间切换流量","title":"使用 Tier-1 网关进行多集群流量切换"},{"content":" 技术预览 Tetrate WAF 是 TSB 未来功能的技术预览。目前不建议在生产环境中使用。 本文将描述如何在 TSB 中启用和配置 Web 应用程序防火墙（WAF）功能。\n概述 Web 应用程序防火墙（WAF）检查入站和出站的 HTTP 流量。它会将流量与一系列签名进行匹配，以检测攻击尝试、格式不正确的流量和敏感数据的泄漏。可疑的流量可以被阻止，警报可以被触发，并且流量可以被记录以供后续分析。\n传统的 WAF 解决方案在网络的边缘运行，检查从互联网进出的流量。它们基于这样的假设：恶意行为者是外部的，不在你的内部基础设施中。\nTetrate WAF 在应用程序内部运行，以非常精细的方式保护个别服务。通过 Tetrate WAF，你可以增强你的零信任 姿态，保护内部和外部攻击者。\n启用此功能的好处包括：\n使用行业标准的 OWASP 核心规则集 （CRS）检测规则来检测攻击流量和数据泄漏的检测和阻止。在撰写本文时，Tetrate WAF 使用 CRS v4.0.0-rc1 。 使用定制的自定义规则进行保护的微调。 快速应对已知 CVE 或 0day 攻击的能力。 基于你的基础设施和已知的易受攻击工作负载的灵活部署。 TSB 中的 WAF 可以配置 WAF 功能的组件包括：组织 、租户 、工作区 、安全组 、入口网关 、出口网关 和 Tier1 网关 。WAF 功能可以在 组织设置 、租户设置 、工作区设置 的 defaultSecuritySettings 属性中指定，以及在 SecuritySettings 的 spec 中指定。\nwaf: rules: - Include @recommended-conf # 基本 WAF 设置 - Include @crs-setup-conf # 核心规则集设置 - Include @owasp_crs/*.conf # 加载核心规则集规则 如上所示，WAF 规则基于 Seclang 语言 。包括以下别名，你可以轻松启用常见的配置和规则：\n@recommended-conf：基本 WAF 设置。有关更多详细信息，可以在此处 找到完整文件。此配置包括： WAF 引擎模式：DetectionOnly。检查流量并生成日志。不执行破坏性操作。这是评估和微调 WAF 的建议方式，以最小化意外行为。 请求正文访问：On。允许检查请求正文，包括 POST 参数。 响应正文访问：On。允许检查响应正文。 @crs-setup-conf：基本 CRS 配置。它假定引擎 WAF 设置已经加载（例如通过 @recommended-conf）。有关更多详细信息，可以在此处 找到完整文件。此配置包括： 操作模式：异常得分模式 偏执级别：PL1（最低虚警数）。 @owasp_crs：包含规则文件的主 CRS 文件夹的别名。文件夹组织和文件名约定与官方 CRS 存储库保持一致。有关详细信息，可以在此处 找到。 示例 在开始之前，请确保你已经：\n熟悉 TSB 概念 。 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装。 完成 TSB 快速入门 。本文假定你已经创建了一个租户，并熟悉工作区和配置组。此外，你需要将 tctl 配置到你的 TSB 环境。 在本示例中，将使用 httpbin 作为工作负载。发送到 Ingress GW 的请求将经过 WAF 过滤，如果检测到恶意攻击，将被拒绝。\n部署 httpbin 服务 请按照此文档中的所有说明 创建 httpbin 服务。\n接下来的命令将假定你有一个组织=tetrate、租户=tetrate、工作区=httpbin、网关组=httpbin-gateway。\n启用 WAF 创建一个名为 waf-ingress-gateway.yaml 的文件，其中包含 IngressGateway 的定义：\napiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: name: ingress-gw group: httpbin-gateway workspace: httpbin tenant: tetrate organization: tetrate spec: workloadSelector: namespace: httpbin labels: app: httpbin-ingress-gateway http: - name: httpbin port: 443 hostname: \u0026#34;httpbin.tetrate.io\u0026#34; routing: rules: - route: host: \u0026#34;httpbin/httpbin.httpbin.svc.cluster.local\u0026#34; waf: rules: - Include @recommended-conf # 基本 WAF 设置 - SecRuleEngine On # 覆盖 @recommended-conf 中的规则引擎模式，启用 WAF 干预 - Include @crs-setup-conf # 初始化 CRS - Include @owasp_crs/*.conf # 加载 CRS 规则 注意 规则的顺序很重要：规则按照它们在 rules 下列出的顺序进行检查。通常，主要的 WAF 设置首先出现，然后是 CRS 设置，然后是 CRS 规则。 在 TSB 上应用它：\ntctl apply -f waf-ingress-gateway.yaml 测试 现在，你应该能够发送请求到 Ingress Gateway。\nexport GATEWAY_IP=$(kubectl -n httpbin get service httpbin-ingress-gateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) 发送一个真正的负面请求：\ncurl \u0026#39;http://httpbin.tetrate.io:443?arg=argument_1\u0026#39; -kI --connect-to httpbin.tetrate.io:443:$GATEWAY_IP:443 你应该会收到类似于以下内容的 200 OK 响应代码：\nHTTP/1.1 200 OK server: istio-envoy date: Wed, 30 Nov 2022 15:20:28 GMT content-type: text/html; charset=utf-8 content-length: 9593 access-control-allow-origin: * access-control-allow-credentials: true x-envoy-upstream-service-time: 4 而对于真正的正面请求（潜在的恶意跨站脚本攻击）：\ncurl \u0026#39;http://httpbin.tetrate.io:443?arg=\u0026lt;script\u0026gt;alert(\u0026#39;0\u0026#39;)\u0026lt;/script\u0026gt;\u0026#39; -kI --connect-to httpbin.tetrate.io:443:$GATEWAY_IP:443 你应该会看到请求被拒绝，并显示 403 Forbidden 错误。这证明 WAF 检测到了恶意模式并中断了连接：\nHTTP/1.1 403 Forbidden date: Wed, 30 Nov 2022 15:33:30 GMT server: istio-envoy transfer-encoding: chunked 你可以查看 Envoy 日志，其中也包含 WAF 日志，以获取有关检测的更多详细信息：\nexport GATEWAY_POD=$(kubectl get pods -n httpbin -o jsonpath=\u0026#34;{.items[1].metadata.name}\u0026#34;) kubectl logs -n httpbin $GATEWAY_POD | tail -n 10 | grep \u0026#34;Coraza:\u0026#34; 你将看到类似以下内容的输出日志：\n2022-11-30T15:33:30.032506Z\tcritical\tenvoy wasm\twasm log httpbin.ingress-gw-tetrate-internal-waf-main0 ingress-gw-tetrate-internal-waf-main0: [client \u0026#34;\u0026#34;] Coraza: Warning. XSS Attack Detected via libinjection [file \u0026#34;@owasp_crs/REQUEST-941-APPLICATION-ATTACK-XSS.conf\u0026#34;] [line \u0026#34;0\u0026#34;] [id \u0026#34;941100\u0026#34;] [rev \u0026#34;\u0026#34;] [msg \u0026#34;XSS Attack Detected via libinjection\u0026#34;] [data \u0026#34;Matched Data: XSS data found within ARGS:arg: \u0026lt;script\u0026gt;alert(0)\u0026lt;/script\u0026gt;\u0026#34;] [severity \u0026#34;critical\u0026#34;] [ver \u0026#34;OWASP_CRS/4.0.0-rc1\u0026#34;] [maturity \u0026#34;0\u0026#34;] [accuracy \u0026#34;0\u0026#34;] [tag \u0026#34;application-multi\u0026#34;] [tag \u0026#34;language-multi\u0026#34;] [tag \u0026#34;platform-multi\u0026#34;] [tag \u0026#34;attack-xss\u0026#34;] [tag \u0026#34;paranoia-level/1\u0026#34;] [tag \u0026#34;OWASP_CRS\u0026#34;] [tag \u0026#34;capec/1000/152/242\u0026#34;] [hostname \u0026#34;\u0026#34;] [uri \u0026#34;/?arg=\u0026lt;script\u0026gt;alert(0)\u0026lt;/script\u0026gt;\u0026#34;] [unique_id \u0026#34;nacXlZaiGUmkOTfLTaz\u0026#34;] 2022-11-30T15:33:30.032829Z\tcritical\tenvoy wasm\twasm log httpbin.ingress-gw-tetrate-internal-waf-main0 ingress-gw-tetrate-internal-waf-main0: [client \u0026#34;\u0026#34;] Coraza: Warning. XSS Filter - Category 1: Script Tag Vector [file \u0026#34;@owasp_crs/REQUEST-941-APPLICATION-ATTACK-XSS.conf\u0026#34;] [line \u0026#34;0\u0026#34;] [id \u0026#34;941110\u0026#34;] [rev \u0026#34;\u0026#34;] [msg \u0026#34;XSS Filter - Category 1: Script Tag Vector\u0026#34;] [data \u0026#34;Matched Data: \u0026lt;script\u0026gt; found within ARGS:arg: \u0026lt;script\u0026gt;alert(0)\u0026lt;/script\u0026gt;\u0026#34;] [severity \u0026#34;critical\u0026#34;] [ver \u0026#34;OWASP_CRS/4.0.0-rc1\u0026#34;] [maturity \u0026#34;0\u0026#34;] [accuracy \u0026#34;0\u0026#34;] [tag \u0026#34;application-multi\u0026#34;] [tag \u0026#34;language-multi\u0026#34;] [tag \u0026#34;platform-multi\u0026#34;] [tag \u0026#34;attack-xss\u0026#34;] [tag \u0026#34;paranoia-level/1\u0026#34;] [tag \u0026#34;OWASP_CRS\u0026#34;] [tag \u0026#34;capec/1000/152/242\u0026#34;] [hostname \u0026#34;\u0026#34;] [uri \u0026#34;/?arg=\u0026lt;script\u0026gt;alert(0)\u0026lt;/script\u0026gt;\u0026#34;] [unique_id \u0026#34;nacXlZaiGUmkOTfLTaz\u0026#34;] 2022-11-30T15:33:30.035356Z\tcritical\tenvoy wasm\twasm log …","relpermalink":"/book/tsb/howto/waf/","summary":"展示如何配置和启用 WAF 功能","title":"使用 WAF 功能"},{"content":"该文档描述了如何使用工作负载载入功能将 AWS Elastic Container Service (ECS) 任务载入到 TSB。\n在继续之前，请确保你已完成 设置工作负载载入文档 中描述的步骤。如果你不计划载入虚拟机，可以跳过配置本地仓库和安装软件包的步骤，因为 ECS 任务的流程略有不同。\n背景 通过工作负载载入将工作负载引入 mesh 的每个工作负载都必须具有可验证的身份。对于 AWS ECS 任务，使用 任务 IAM 角色 来标识尝试加入 mesh 的任务。\nOnboarding Agent 容器作为 AWS ECS 任务中的边车与工作负载容器一起运行。在启动时，Onboarding Agent 与 AWS ECS 环境交互，获取与任务的 IAM 角色关联的凭据，并使用这些凭据与 Workload Onboarding Plane 进行身份验证。\n概述 与虚拟机工作负载相比，AWS ECS 工作负载的工作负载载入设置包括以下额外步骤：\n允许 AWS ECS 任务加入 WorkloadGroup 配置 AWS ECS 任务定义，包括 IAM 角色和运行 Workload Onboarding Agent 作为边车 允许 AWS ECS 任务加入 WorkloadGroup 要允许本地工作负载加入特定的 WorkloadGroup，请创建一个 OnboardingPolicy ，并设置 ecs 字段。\n示例 以下示例允许与给定 AWS 帐户关联的任何以 AWS ECS 任务形式运行的工作负载，可以加入给定 Kubernetes 命名空间中的任何可用 WorkloadGroup：\napiVersion: authorization.onboarding.tetrate.io/v1alpha1 kind: OnboardingPolicy metadata: name: allow-any-aws-ecs-task-from-given-accounts namespace: \u0026lt;namespace\u0026gt; spec: allow: - workloads: - aws: accounts: - \u0026#39;123456789012\u0026#39; - \u0026#39;234567890123\u0026#39; ecs: {} # 以上述帐户的任何 AWS ECS 任务 onboardTo: - workloadGroupSelector: {} # 该命名空间中的任何 WorkloadGroup 虽然前面的示例可能是一个相对“宽松”的策略，但更严格的载入策略可能只允许以特定 AWS 区域和/或区域、特定 AWS ECS 集群、特定 AWS IAM 角色等运行的 AWS ECS 任务。它还可能只允许工作负载加入特定的 WorkloadGroup 子集。\n以下是更为严格策略的示例：\napiVersion: authorization.onboarding.tetrate.io/v1alpha1 kind: OnboardingPolicy metadata: name: allow-narrow-subset-of-aws-ecs-tasks namespace: \u0026lt;namespace\u0026gt; spec: allow: - workloads: - aws: partitions: - aws accounts: - \u0026#39;123456789012\u0026#39; regions: - us-east-2 zones: - us-east-2b ecs: clusters: - \u0026lt;ECS cluster name\u0026gt; iamRoleNames: - \u0026lt;IAM role name\u0026gt; # 上述分区/帐户/区域/区域中特定 ECS 集群中与 IAM 角色列表中的一个关联的任何 AWS ECS 任务 onboardTo: - workloadGroupSelector: matchLabels: app: ratings 创建任务定义 配置 任务 IAM 角色 。这是任务将用于加入 mesh 的身份。 将网络模式设置为 awsvpc。不支持其他网络模式。 配置 任务执行 IAM 角色 。如果镜像仓库是 Elastic Container Registry (ECR)，则此角色应具有从中拉取镜像的权限。 配置 Workload Onboarding Agent Sidecar 按照以下步骤将 Workload Onboarding Agent 容器添加到任务定义中，与应用程序容器一起。\n该容器镜像将添加到安装 Tetrate Service Bridge 到你的 Kubernetes 集群时使用的容器仓库中。要使用不同的容器仓库，请通过 按照这些说明同步 Tetrate Service Bridge 镜像 。\n容器名称：onboarding-agent\n镜像：\u0026lt;your docker registry\u0026gt;/onboarding-agent:\u0026lt;tag\u0026gt;\n这应该与集群中其他 TSB 镜像使用的镜像仓库和标签匹配，你可以通过首先使用 kubectl 提取镜像来找到正确的镜像：\nkubectl get deploy -n istio-system onboarding-plane -o jsonpath=\u0026#34;{.spec.template.spec.containers[0].image}\u0026#34; 这将返回一个镜像，例如：\n123456789012.dkr.ecr.us-east-1.amazonaws.com/registry/onboarding-plane-server:1.5.0 相应的 Workload Onboarding Agent 镜像将是：\n123456789012.dkr.ecr.us-east-1.amazonaws.com/registry/onboarding-agent:1.5.0 用户必须设置为 root 用户，使用 UID 为 0。\n提供 载入配置 。\n使用以下内容设置一个名为 ONBOARDING_CONFIG 的环境变量。将其中的 `onboarding-endpoint-d\nns-name 替换为要连接的 Workload Onboarding Endpoint，workload-group-namespace 和 workload-group-name 替换为 Istio WorkloadGroup 的命名空间和名称。\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: \u0026lt;onboarding-endpoint-dns-name\u0026gt; workloadGroup: namespace: \u0026lt;workload-group-namespace\u0026gt; name: \u0026lt;workload-group-name\u0026gt; 假定 Workload Onboarding Endpoint 可以在 https://\u0026lt;onboarding-endpoint-dns-name\u0026gt;:15443 上访问，并且使用为适当的 DNS 名称颁发的 TLS 证书。有关更多配置选项，请参考 载入配置 文档。\n为了不包含换行符，可能更容易将配置指定为 JSON 而不是 YAML。在这种情况下，上述配置将采用以下形式：\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;config.agent.onboarding.tetrate.io/v1alpha1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;OnboardingConfiguration\u0026#34;, \u0026#34;onboardingEndpoint\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;\u0026lt;onboarding-endpoint-dns-name\u0026gt;\u0026#34; }, \u0026#34;workloadGroup\u0026#34;: { \u0026#34;namespace\u0026#34;: \u0026#34;\u0026lt;workload-group-namespace\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;\u0026lt;workload-group-name\u0026gt;\u0026#34; } } 如果用于签署 Workload Onboarding Endpoint TLS 证书的证书颁发机构 (CA) 是自签名的，即未由公共根 CA（如 Let’s Encrypt 或 Digicert）颁发，则必须提供公共根证书。\n使用根证书颁发机构 PEM 文件的内容设置一个名为 ONBOARDING_AGENT_ROOT_CERTS 的环境变量。这应该是以下形式：\n-----BEGIN CERTIFICATE----- MIIC... -----END CERTIFICATE----- 请注意，此环境变量不能通过 AWS 控制台配置，因为它会替换换行符。相反，应使用 AWS CLI 工具或基础架构即代码工具（例如 Terraform 或 CloudFormation）进行配置。\n如果需要，提供 代理配置 。在大多数情况下，默认值将起作用，此步骤是可选的。\n如果使用 Istio 隔离边界 安装 TSB，并且工作负载应连接到非默认修订版本，则需要此步骤。例如，要配置工作负载连接到 canary 修订版，设置一个名为 AGENT_CONFIG 的环境变量，其内容如下：\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: AgentConfiguration sidecar: istio: revision: canary 为了不包含换行符，可能更容易将配置指定为 JSON 而不是 YAML。在这种情况下，上述配置将采用以下形式：\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;config.agent.onboarding.tetrate.io/v1alpha1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;AgentConfiguration\u0026#34;, \u0026#34;sidecar\u0026#34;: { \u0026#34;istio\u0026#34;: { \u0026#34;revision\u0026#34;: \u0026#34;canary\u0026#34; } } } 示例任务定义 创建任务定义的示例命令如下：\n# 在 JSON 容器定义中压缩并转义 onboarding 配置中的引号 ONBOARDING_CONFIG=$(jq --compact-output . \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; | sed \u0026#39;s/\u0026#34;/\\\\\u0026#34;/g\u0026#39; { \u0026#34;apiVersion\u0026#34;: \u0026#34;config.agent.onboarding.tetrate.io/v1alpha1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;OnboardingConfiguration\u0026#34;, \u0026#34;onboardingEndpoint\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;abcdef-123456789.us-east-1.elb.amazonaws.com\u0026#34;, }, \u0026#34;workload\u0026#34;: { \u0026#34;labels\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;v5\u0026#34; } }, \u0026#34;workloadGroup\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;app\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;app-namespace\u0026#34; } } EOF ) # 在 JSON 容器定义中替换根证书 PEM 中的换行符，以进行编码 ONBOARDING_AGENT_ROOT_CERTS=$(awk \u0026#39;{printf \u0026#34;%s\\\\n\u0026#34;, $0}\u0026#39; root-ca-cert.pem) aws ecs register-task-definition \\ --task-role-arn=\u0026#34;arn:aws:iam::123456789012:role/app-task\u0026#34; \\ --execution-role-arn=\u0026#34;arn:aws:iam::123456789012:role/ecsTaskExecutionRole\u0026#34; \\ --family=\u0026#34;app\u0026#34; \\ --network-mode=\u0026#34;awsvpc\u0026#34; \\ --cpu=256 \\ --memory=512 \\ --requires-compatibilities FARGATE EC2 \\ --container-definitions=\u0026#39;[ …","relpermalink":"/book/tsb/setup/workload-onboarding/guides/ecs-workloads/","summary":"该文档描述了如何使用工作负载载入功能将 AWS Elastic Container Service (ECS) 任务载入到 TSB。 在继续之前，请确保你已完成 设置工作负载载入文档 中描述的步骤。如果你不计划载入虚拟机，可以跳过配置本地仓库和安装软件包的步骤，因为 ECS 任务","title":"载入 AWS ECS 工作负载"},{"content":"本指南适用于已安装 Argo CD 并正在管理应用程序的开发人员。\n🔔 注意：请确保你已完成 入门指南 。\n工具\n自动同步策略\n对比选项\n同步选项 环境变量\n选择性同步\n","relpermalink":"/book/argo-cd/user-guide/","summary":"本指南适用于已安装 Argo CD 并正在管理应用程序的开发人员。 🔔 注意：请确保你已完成 入门指南 。 工具 自动同步策略 对比选项 同步选项 环境变量 选择性同步","title":"用户手册"},{"content":" 部署策略\nRollout 规范\nHPA\nVPA\n短暂元数据\n重启 Rollouts\n缩小失败的 Rollout\n回滚窗口\n反亲和性\nHelm\nKustomize\n控制器指标\n","relpermalink":"/book/argo-rollouts/rollout/","summary":"部署策略\nRollout 规范\nHPA\nVPA\n短暂元数据\n重启 Rollouts\n缩小失败的 Rollout\n回滚窗口\n反亲和性\nHelm\nKustomize\n控制器指标","title":"Rollout"},{"content":"🔔 提醒：自 1.5 版本起可用 - 状态：Alpha\nArgo Rollouts 支持通过第三方插件系统获取分析指标。这允许用户扩展 Rollouts 的功能以支持原生不支持的度量提供者。Rollouts 使用一个名为 go-plugin 的插件库来实现这一点。你可以在这里找到一个示例插件：rollouts-plugin-trafficrouter-sample-nginx\n使用 Traffic Router 插件 安装和使用 Argo Rollouts 插件有两种方法。第一种方法是将插件可执行文件挂载到 rollouts 控制器容器中。第二种方法是使用 HTTP（S）服务器托管插件可执行文件。\n将插件可执行文件挂载到 rollouts 控制器容器中 有几种方法可以将插件可执行文件挂载到 rollouts 控制器容器中。其中一些将取决于你的特定基础设施。这里有几种方法：\n使用 init 容器下载插件可执行文件 使用 Kubernetes 卷挂载共享卷，如 NFS、EBS 等。 将插件构建到 rollouts 控制器容器中 然后，你可以使用 configmap 将插件可执行文件位置指向到。示例：\napiVersion: v1 kind: ConfigMap metadata: name: argo-rollouts-config data: trafficRouterPlugins: |- - name: \u0026#34;argoproj-labs/sample-nginx\u0026#34; # 插件的名称，它必须与插件所需的名称匹配，以便它可以找到它的配置 location: \u0026#34;file://./my-custom-plugin\u0026#34; # 支持 http（s）：// url 和 file：// 使用 HTTP（S）服务器托管插件可执行文件 Argo Rollouts 支持从 HTTP（S）服务器下载插件可执行文件。要使用此方法，你需要通过argo-rollouts-config configmap 配置控制器，并将pluginLocation设置为 http（s）url。示例：\napiVersion: v1 kind: ConfigMap metadata: name: argo-rollouts-config data: trafficRouterPlugins: |- - name: \u0026#34;argoproj-labs/sample-nginx\u0026#34; # 插件的名称，它必须与插件所需的名称匹配，以便它可以找到它的配置 location: \u0026#34;https://github.com/argoproj-labs/rollouts-plugin-trafficrouter-sample-nginx/releases/download/v0.0.1/metric-plugin-linux-amd64\u0026#34; # 支持 http(s)：// url 和 file：// sha256: \u0026#34;08f588b1c799a37bbe8d0fc74cc1b1492dd70b2c\u0026#34; #可选的插件可执行文件的 sha256 校验和 一些注意事项 根据你用于安装和插件的方法，需要注意一些事项。如果无法下载或找到插件可执行文件，rollouts 控制器将无法启动。这意味着如果你使用需要下载插件的安装方法，而服务器因某种原因不可用，并且 rollouts 控制器的 pod 在服务器关闭时被删除或正在首次启动，则将无法启动，直到可用插件的服务器再次可用。\nArgo Rollouts 仅在启动时下载插件一次，但如果删除了 pod，则需要在下一次启动时再次下载插件。在 HA 模式下运行 Argo Rollouts 可以在一定程度上帮助解决这种情况，因为每个 pod 在启动时都会下载插件。因此，如果单个 pod 在服务器故障期间被删除，则其他 pod 仍将能够接管，因为已经有一个可用的插件可执行文件。Argo Rollouts 管理员有责任定义插件安装方法，考虑每种方法的风险。\n可用插件列表（按字母顺序排列） 在此处添加你的插件 如果你已创建插件，请提交 PR 将其添加到此列表中。 rollouts-plugin-trafficrouter-sample-nginx 这仅是一个示例插件，可以用作创建自己的插件的起点。它不适合在生产中使用。它基于内置的 prometheus 提供者。 Contour 这是一个支持 Contour 的插件。 Gateway API 提供对 Gateway API 的支持，其中包括 Kuma、Traefix、cilium、Contour、GloodMesh、HAProxy 和许多其他 。 ","relpermalink":"/book/argo-rollouts/traffic-management/plugins/","summary":"🔔 提醒：自 1.5 版本起可用 - 状态：Alpha Argo Rollouts 支持通过第三方插件系统获取分析指标。这允许用户扩展 Rollouts 的功能以支持原生不支持的度量提供者。Rollouts 使用一个名为 go-plugin 的插件库来实现这一点。你可以在这里找到","title":"流量路由插件"},{"content":"在回滚更新时，我们可能会为所有策略缩小新的副本集。用户可以通过将 abortScaleDownDelaySeconds 设置为 0 来选择永久保留新的副本集，或者将该值调整为更大或更小的值。\n下表总结了在 Rollout 策略和 abortScaleDownDelaySeconds 的组合下的行为。请注意，abortScaleDownDelaySeconds 不适用于 argo-rollouts v1.0。 abortScaleDownDelaySeconds = nil 是默认值，这意味着在 v1.1 中，对于所有 Rollout 策略，默认情况下在 Rollout 后 30 秒内缩小新的副本集。\n策略 v1.0 行为 abortScaleDownDelaySeconds v1.1 行为 蓝绿部署 不缩小 nil 回滚后 30 秒内缩小 蓝绿部署 不缩小 0 不缩小 蓝绿部署 不缩小 N 回滚后 N 秒内缩小 基本金丝雀 回滚到稳定状态 N/A 回滚到稳定状态 带流量路由的金丝雀 立即缩小 nil 回滚后 30 秒内缩小 带流量路由的金丝雀 立即缩小 0 不缩小 带流量路由的金丝雀 立即缩小 N 回滚后 N 秒内缩小 带流量路由的金丝雀 + setCanaryScale 不缩小 (bug) * 应该像带流量路由的金丝雀一样 ","relpermalink":"/book/argo-rollouts/rollout/scaledown-aborted-rs/","summary":"在回滚更新时，我们可能会为所有策略缩小新的副本集。用户可以通过将 abortScaleDownDelaySeconds 设置为 0 来选择永久保留新的副本集，或者将该值调整为更大或更小的值。 下表总结了在 Rollout 策略和 abortScaleDownDelaySeconds 的组合下的行为。请注意，abortScaleD","title":"Rollout 失败时缩小新的 ReplicaSet"},{"content":"读者将了解到 SPIRE 部署的组成部分，有哪些部署模式，以及在部署 SPIRE 时需要考虑哪些性能和安全问题。\n你的 SPIRE 部署的设计应满足你的团队和组织的技术要求。它还应包括支持可用性、可靠性、安全性、可扩展性和性能的要求。该设计将作为你的部署活动的基础。\n身份命名方案 请记住，在前面的章节中，SPIFFE ID 是一个结构化的字符串，代表一个工作负载的身份名称，正如你在第四章中看到的那样。工作负载标识符部分（URI 的路径部分）附加在信任域名（URI 的主机部分）上，可以组成关于服务所有权的含义，以表示它在什么平台上运行，谁拥有它，它的预期目的，或其他惯例。它是特意为你定义的灵活和可定制的。\n你的命名方案可能是分层的，就像文件系统的路径。也就是说，为了减少歧义，命名方案不应该以尾部的正斜杠（/）结束。下面你将看到一些不同的样例，它们遵循三种不同的约定，你可以遵循，或者如果你感到特别有灵感，也可以想出你自己的。\n直接命名服务 你可能会发现，作为软件开发生命周期的一部分，直接通过它从应用角度呈现的功能和它运行的环境来识别一个服务是很有用的。例如，管理员可能会规定，在特定环境中运行的任何进程都应该能够以特定身份出现。比如说。\nspiffe://staging.example.com/payments/mysql 或\nspiffe://staging.example.com/payments/web-fe 上面的两个 SPIFFE ID 指的是两个不同的组件 ——MySQL 数据库服务和一个 Web 前端 —— 在 staging 环境中运行的支付服务。staging 的意思是一个环境，payment 是一个高级服务。\n前面两个例子和下面两个例子是说明性的，不是规定性的。实施者应该权衡自己的选择，决定自己喜欢的行动方案。\n识别服务所有者 通常更高级别的编排器和平台都有自己的内置身份概念（如 Kubernetes 服务账户，或 AWS/GCP 服务账户），能够直接将 SPIFFE 身份映射到这些身份是有帮助的。比如：\nspiffe://k8s-workload-cluster.example.com/ns/staging/sa/default 在这个例子中，信任域 example.com 的管理员正在运行一个 Kubernetes 集群 k8s-workload-cluster.example.com，它有一个 staging 命名空间，在这个命名空间中，有一个名为 default 的服务账户（SA）。\n不透明的 SPIFFE 身份 SPIFFE 路径可能是不透明的，然后元数据可以被保存在一个二级数据库中。这可以被查询以检索与 SPIFFE 标识符相关的任何元数据。比如：\nspiffe://example.com/9eebccd2-12bf-40a6-b262-65fe0487d4 SPIRE 的部署模式 我们将概述在生产中运行 SPIRE 的三种最常见的方式。这并不意味着我们要在这里限制可用的选择，但为了本书的目的，我们将把范围限制在这些部署 SPIRE 服务器的常见方式上。我们将只关注服务器的部署架构，因为每个节点通常安装一个代理。\n数量：大信任域与小信任域的对比 信任域的数量预计是相对固定的，只是偶尔重访，而且预计不会随时间漂移太多。另一方面，一个给定的信任域中的节点数量和工作负载的数量，预计会根据负载和增长而频繁波动。\n选择集中到一个大的信任域的单一信任根，还是分布和隔离到多个信任域，将由许多因素决定。本章的安全考虑部分谈到了使用信任域进行隔离的问题。还有一些原因，你可以选择多个小的信任域而不是一个大的信任域，包括增加可用性和租户的隔离。管理域边界、工作负载数量、可用性要求、云供应商数量和认证要求等变量也会影响这里的决策。\n例如，你可以选择为每一个行政边界设置一个单独的信任域，以便在组织中可能有不同开发实践的不同小组之间进行自治。\n类别 单信任域 嵌套 联合 部署规模 大 非常大 大 多区域 否 是 是 多云 否 是 是 表 6.1: 信任域大小的决策表\n一对一：单信任域中的单一 SPIRE 集群\n单一的 SPIRE 服务器，在高可用性的配置下，是单一信任域环境的最佳起点。\n然而，当将单个 SPIRE 服务器部署到跨越区域、平台和云提供商环境的信任域时，当 SPIRE 代理依赖于远处的 SPIRE 服务器时，会出现潜在的扩展问题。在单个部署将跨越多个环境的情况下，解决在单个信任域上使用共享数据存储的解决方案是将 SPIRE 服务器配置为嵌套拓扑结构。\n嵌套式 SPIRE SPIRE 服务器的嵌套拓扑结构可使您尽可能保持 SPIRE 代理和 SPIRE 服务器之间的通信。\n在这种配置中，顶级 SPIRE 服务器持有根证书和密钥，而下游服务器请求中间签名证书，作为下游服务器的 X.509 签名授权。如果顶层发生故障，中间服务器继续运行，为拓扑结构提供弹性。\n嵌套拓扑结构很适合多云部署。由于能够混合和匹配节点验证器，下游服务器可以在不同的云提供商环境中驻留并为工作负载和代理提供身份。\n虽然嵌套式 SPIRE 是提高 SPIRE 部署的灵活性和可扩展性的理想方式，但它并不提供任何额外的安全性。由于 X.509 没有提供任何方法来限制中间证书颁发机构的权力，每个 SPIRE 服务器可以生成任何证书。即使你的上游证书颁发机构是你公司地下室混凝土掩体中的加固服务器，如果你的 SPIRE 服务器被破坏，你的整个网络可能会受到影响。这就是为什么必须确保每台 SPIRE 服务器都是安全的。\nSPIRE 联邦 部署可能需要多个信任根基，也许是因为一个组织有不同的组织部门，有不同的管理员，或者因为他们有独立的暂存和生产环境，偶尔需要沟通。\n另一个用例是组织之间的 SPIFFE 互操作性，如云供应商和其客户之间。\n这些多个信任域和互操作性用例都需要一个定义明确、可互操作的方法，以便一个信任域中的工作负载能够认证不同信任域中的工作负载。在联合 SPIRE 中，不同信任域之间的信任是通过首先认证各自的捆绑端点，然后通过认证的端点检索外部信任域的捆绑来建立的。\n独立的 SPIRE 服务器 运行 SPIRE 的最简单方法是在专用服务器上，特别是如果有一个单一的信任域，而且工作负载的数量不大。在这种情况下，你可以在同一节点上共同托管一个数据存储，使用 SQLite 或 MySQL 作为数据库，简化部署。然而，当使用共同托管的部署模式时，记得要考虑数据库的复制或备份。如果你失去了节点，你可以迅速在另一个节点上运行 SPIRE 服务器，但如果你失去了数据库，你的所有代理和工作负载都需要重新测试以获得新的身份。\n避免单点故障\n保持简单有利也有弊。如果只有一台 SPIRE 服务器，而它丢失了，一切都会丢失，需要重建。拥有一个以上的服务器可以提高系统的可用性。仍然会有一个共享的数据存储和安全连接及数据复制。我们将在本章后面讨论这种决定的不同安全影响。\n要横向扩展 SPIRE 服务器，请将同一信任域中的所有服务器配置为对同一共享数据存储进行读和写。\n数据存储是 SPIRE 服务器保存动态配置信息的地方，如注册条目和身份映射策略。SQLite 与 SPIRE 服务器捆绑在一起，是默认的数据存储。\n数据存储建模 在进行数据存储设计时，你的首要关注点应该是冗余和高可用性。你需要确定每个 SPIRE 服务器集群是否有一个专用的数据存储，或者是否应该有一个共享的数据存储。\n数据库类型的选择可能受到整个系统可用性要求和你的运营团队能力的影响。例如，如果运维团队有支持和扩展 MySQL 的经验，这应该是首要选择。\n每个集群的专用数据存储 多个数据存储允许系统的每个专用部分更独立。例如，AWS 和 GCP 云中的 SPIRE 集群可能有独立的数据存储，或者 AWS 中的每个 VPC 可能有一个专用数据存储。这种选择的好处是，如果一个地区或云提供商发生故障，在其他地区或云提供商中运行的 SPIRE 部署就不会受到影响。\n在发生重大故障时，每个集群的数据存储的缺点变得最为明显。如果一个地区的 SPIRE 数据存储（以及所有的 SPIRE 服务器）发生故障，就需要恢复本地数据存储，或者将代理切换到同一信任域的另一个 SPIRE 服务器集群上，假设信任域是跨区域的。\n如果有必要将代理切换到一个新的集群，必须特别考虑，因为新的集群将不知道另一个 SPIRE 集群发出的身份，或该集群包含的注册条目。代理将需要对这个新集群进行重新认证，并且需要通过备份或重建来恢复注册条目。\n共享的数据存储 拥有一个共享的数据存储可以解决上述拥有单独数据存储的问题。然而，它可能会使设计和操作更加复杂，并依赖其他系统来检测故障，并在发生故障时更新 DNS 记录。此外，该设计仍然需要为每个 SPIRE 可用域、每个区域或数据中心的数据库基础设施的碎片，这取决于具体的基础设施。请查看 SPIRE 文档 以了解更多细节。\n管理失败 当基础设施发生故障时，主要的问题是如何继续向需要 SVID 才能正常运行的工作负载发放 SVID。SPIRE 代理的 SVID 内存缓存被设计为应对短期宕机的主要防线。\nSPIRE 代理定期从 SPIRE 服务器获取授权发布的 SVID，以便在工作负载需要时将其交付给它们。这个过程是在工作负载请求 SVID 之前完成的。\n性能和可靠性 SVID 缓存有两个优点：性能和可靠性。当工作负载要求获得其 SVID 时，代理不需要请求和等待 SPIRE 服务器提供 SVID，因为它已经有了缓存，这就避免了到 SPIRE 服务器的往返代价。此外，如果 SPIRE 服务器在工作负载请求其 SVID 时不可用，也不会影响 SVID 的发放，因为代理已经将其缓存起来了。\n我们需要对 X509-SVID 和 JWT-SVID 进行区分。JWT-SVID 不能提前构建，因为代理不知道工作负载所需的 JWT-SVID 的具体受众，代理只预先缓存 X509-SVID。然而，SPIRE 代理确实维护着已发布的 JWT-SVID 的缓存，只要缓存的 JWT-SVID 仍然有效，它就可以向工作负载发布 JWT-SVID，而无需与 SPIRE 服务器联系。\n存活时间 SVID 的一个重要属性是其存活时间（TTL）。如果一个 SVID 的剩余寿命小于 TTL 的一半，SPIRE 代理将更新缓存中的 SVID。这向我们表明，SPIRE 在对底层基础设施能够提供 SVID 的信心方面是保守的。它还提供了一个暗示，即 SVID TTL 在抵御中断方面的作用。较长的 TTL 可以提供更多的时间来修复和恢复任何基础设施的中断，但是在选择 TTL 的时候，需要在安全性和可用性之间做出妥协。长的 TTL 将提供充足的时间来修复故障，但代价是在较长的时间内暴露 SVID（及相关密钥）。较短的 TTL 可以减少恶意行为者利用被破坏的 SVID 的时间窗口，但需要更快地对故障作出反应。不幸的是，没有什么 “神奇” 的 TTL 可以成为所有部署的最佳选择。在选择 TTL 时，必须考虑在必须解决中断问题的时间窗口和已发布的 SVID 的可接受曝光度之间，你愿意接受什么样的权衡。\nKubernetes 中的 SPIRE 本节介绍了在 Kubernetes 中运行 SPIRE 的细节。Kubernetes 是一个容器编排器，可以在许多不同的云供应商上管理软件部署和可用性，也可以在物理硬件上管理。SPIRE 包括几种不同形式的 Kubernetes 集成。\nKubernetes 中的 SPIRE 代理 Kubernetes 包括 DaemonSet 的概念，这是一个自动部署在所有节点上的容器，每个节点有一个副本运行。这是运行 SPIRE 代理的一种完美方式，因为每个节点必须有一个代理。\n随着新的 Kubernetes 节点上线，调度器将自动轮换 SPIRE 代理的新副本。首先，每个代理需要一份引导信任包的副本。 …","relpermalink":"/book/spiffe/designing-a-spire-deployment/","summary":"读者将了解到 SPIRE 部署的组成部分，有哪些部署模式，以及在部署 SPIRE 时需要考虑哪些性能和安全问题。","title":"设计一个 SPIRE 部署"},{"content":"在 Kubernetes 中管理 pod 时，Cilium 将为 Cilium 管理的每个 pod 创建一个 CiliumEndpoint （CEP）的自定义资源定义（CRD）。如果启用了 enable-cilium-endpoint-slice，那么 Cilium 还会创建一个 CiliumEndpointSlice （CES）类型的 CRD，将一组具有相同安全身份 的 CEP 对象分组到一个 CES 对象中，并广播 CES 对象来向其他代理传递身份，而不是通过广播 CEP 来实现。在大多数情况下，这减少了控制平面上的负载，可以使用相同的主资源维持更大规模的集群。\n例如：\n$ kubectl get ciliumendpointslices --all-namespaces NAME AGE ces-548bnpgsf-56q9f 171m ces-dy4d8x6j2-qgc2z 171m ces-f6qfylrxh-84vxm 171m ces-k29rv92f5-qb4sw 171m ces-m9gs68csm-w2qg8 171m ","relpermalink":"/book/cilium-handbook/kubernetes/ciliumendpointslice/","summary":"在 Kubernetes 中管理 pod 时，Cilium 将为 Cilium 管理的每个 pod 创建一个 CiliumEndpoint （CEP）的自定义资源定义（CRD）。如果启用了 enable-cilium-endpoint-slice，那么 Cilium 还会创建一个 CiliumEndpointSlice （CES）类型的 C","title":"端点切片 CRD"},{"content":"我希望这个简短的报告能让你了解 eBPF 和它的强大之处。我真正希望的是，你已经准备好尝试一些基于 eBPF 的工具！如果你想在技术方面深入研究，可以从 ebpf.io 开始，在那里你会找到更多关于技术和 ebPF 基金会的信息。对于编码实例，可以在 GitHub 上的 ebpf-beginners 仓库里找到。\n为了了解其他人是如何利用 eBPF 工具的，请参加 eBPF Summit 和 Cloud Native eBPF Day 等活动，在这些活动中，用户分享他们的成功和学习经验。还有一个活跃的 Slack 频道 ebpf.io/slack 。我希望能在那里见到你！\n","relpermalink":"/book/what-is-ebpf/conclusion/","summary":"我希望这个简短的报告能让你了解 eBPF 和它的强大之处。我真正希望的是，你已经准备好尝试一些基于 eBPF 的工具！如果你想在技术方面深入研究，可以从 ebpf.io 开始，在那里你会找到更多关于技术和 ebPF 基金会的信息。对于编码实例，可","title":"第七章：结论"},{"content":"由于参考平台是由容器编排和资源管理平台以及服务网格软件组成的，以下出版物为确保该平台的安全提供了指导，并为本文件的内容提供了背景信息。\nSP800-204，基于微服务的应用系统的安全策略 ，讨论了基于微服务的应用的特点和安全要求，以及满足这些要求的总体策略。 SP800-204A，使用服务网格构建基于微服务的安全应用 ，为基于微服务的应用的各种安全服务（如建立安全会话、安全监控等）提供了部署指导，这些服务使用基于独立于应用代码运行的服务代理的专用基础设施（即服务网格）。 SP800-204B，使用服务网格的基于微服务的应用的基于属性的访问控制 ，为在服务网格中构建满足安全要求的认证和授权框架提供了部署指导，例如：（1）通过在任何一对服务之间的通信中实现相互认证来实现零信任；（2）基于访问控制模型，如基于属性的访问控制（ABAC）模型的强大访问控制机制，可用于表达广泛的策略集，并在用户群、对象（资源）和部署环境方面可扩展。 SP800-190，应用容器安全指南 ，解释了与容器技术相关的安全问题，并为在规划、实施和维护容器时解决这些问题提出了实用建议。这些建议是针对容器技术架构中的每个层级提供的。 ","relpermalink":"/book/service-mesh-devsecops/intro/relationship-to-other-nist-guidance-documents/","summary":"由于参考平台是由容器编排和资源管理平台以及服务网格软件组成的，以下出版物为确保该平台的安全提供了指导，并为本文件的内容提供了背景信息。 SP800-204，基于微服务的应用系统的安全策略 ，讨论了基于微服","title":"1.4 与其他 NIST 指导文件的关系"},{"content":"下一个常见问题涉及工作流模型。所有的 CI/CD 管道都可以有两种类型的工作流程模型，这取决于作为管道一部分部署的自动化工具。\n基于推的模式 基于拉的模式 在支持基于推模式的 CI/CD 工具中，在管道的一个阶段或阶段所做的改变会触发后续阶段或阶段的改变。例如，通过一系列的编码脚本，CI 系统中的新构建会触发管道中 CD 部分的变化，从而改变部署基础设施（如 Kubernetes 集群）。使用 CI 系统作为部署变化的基础，其安全方面的缺点是有可能将凭证暴露在部署环境之外，尽管已尽最大努力确保 CI 脚本的安全，因为 CI 脚本是在部署基础设施的信任域之外运行的。由于 CD 工具拥有生产系统的 key，基于推送的模式就变得不安全了。\n在基于拉的工作流程模型中，与部署环境有关的运维（例如 Kubernetes 运维、Flux、ArgoCD）一旦观察到有新镜像被推送到注册表，就会从环境内部拉动新镜像。新镜像被从注册表中拉出，部署清单被自动更新，新镜像被部署在环境（如集群）中。因此，实际的部署基础设施状态与 Git 部署库中声明性描述的状态实现了衔接。此外，部署环境凭证（例如集群凭证）不会暴露在生产环境之外。因此，强烈建议采用基于拉的模式，即通常使用 GitOps 仓库来存储源代码和构建。\n4.7.1 GitsOps 的 CI/CD 工作流程模型 —— 基于拉的模型 GitOps 工作流模型是对 CI/CD 管道的改进（针对管道的交付部分），它使用了基于拉的工作流模型，而不是许多 CI/CD 工具支持的基于推的模型。在这个模型中，流水线的 CI 部分没有变化，因为 CI 引擎（如 Jenkins、GitLab CI）仍然用于为修改后的代码创建构建。\n回归测试，以及与相关存储库中的主要源代码集成 / 合并，尽管它不用于在管道中触发持续交付（直接推送更新）。\n相反，一个单独的 GitOps Operator 根据主干代码的更新来管理部署。\nOperator（例如，Flux、ArgoCD）是一个由协调平台管理的行为体，可以继承集群的配置、安全和可用性。使用这种行为体可以提高安全性，因为在集群内部的代理会监听它被允许访问的所有代码和镜像仓库的更新，并将镜像和配置更新拉入集群。代理使用的拉方式具有以下安全特性：\n只执行协调平台中定义的授权策略所允许的操作；信任与集群共享，不单独管理。 与所有协调平台对象进行原生绑定，并了解操作是否已经完成或需要重试。 ","relpermalink":"/book/service-mesh-devsecops/implement/workflow-models-in-ci-cd-pipelines/","summary":"下一个常见问题涉及工作流模型。所有的 CI/CD 管道都可以有两种类型的工作流程模型，这取决于作为管道一部分部署的自动化工具。 基于推的模式 基于拉的模式 在支持基于推模式的 CI/CD 工具中，在管道的一个阶段或阶段所做的改变会","title":"4.7 CI/CD 管道中的工作流模型"},{"content":"Pod 是 Kubernetes 中最小的可部署单元，由一个或多个容器组成。Pod 通常是网络行为者在利用容器时的初始执行环境。出于这个原因，Pod 应该被加固，以使利用更加困难，并限制成功入侵的影响。\n图3：有 sidecar 代理作为日志容器的 Pod 组件 “非 root”容器和“无 root”容器引擎 默认情况下，许多容器服务以有特权的 root 用户身份运行，应用程序在容器内以 root 用户身份执行，尽管不需要有特权的执行。\n通过使用非 root 容器或无 root 容器引擎来防止 root 执行，可以限制容器受损的影响。这两种方法都会对运行时环境产生重大影响，因此应该对应用程序进行全面测试，以确保兼容性。\n非 root 容器：容器引擎允许容器以非 root 用户和非 root 组成员身份运行应用程序。通常情况下，这种非默认设置是在构建容器镜像的时候配置的。附录 A：非 root 应用的 Dockerfile 示例 显示了一个 Dockerfile 示例，它以非 root 用户身份运行一个应用。\n非 root 用户。另外，Kubernetes 可以在 SecurityContext:runAsUser 指定一个非零用户的情况下，将容器加载到 Pod。虽然 runAsUser 指令在部署时有效地强制非 root 执行，但 NSA 和 CISA 鼓励开发者构建的容器应用程序，以非 root 用户身份执行。在构建时集成非 root 用户执行，可以更好地保证应用程序在没有 root 权限的情况下正常运行。\n无 root 的容器引擎：一些容器引擎可以在无特权的上下文中运行，而不是使用以 root 身份运行的守护程序。在这种情况下，从容器化应用程序的角度来看，执行似乎是使用 root 用户，但执行被重新映射到主机上的引擎用户上下文。虽然无 root 容器引擎增加了一个有效的安全层，但许多引擎目前是作为实验性发布的，不应该在生产环境中使用。管理员应该了解这一新兴技术，并在供应商发布与 Kubernetes 兼容的稳定版本时寻求采用无 root 容器引擎。\n不可变的容器文件系统 默认情况下，容器在自己的上下文中被允许不受限制地执行。在容器中获得执行权限的网络行为者可以在容器中创建文件、下载脚本和修改应用程序。Kubernetes 可以锁定一个容器的文件系统，从而防止许多暴露后的活动。\n然而，这些限制也会影响合法的容器应用程序，并可能导致崩溃或异常行为。为了防止损害合法的应用程序，Kubernetes 管理员可以为应用程序需要写访问的特定目录挂载二级读 / 写文件系统。附录 B：只读文件系统的部署模板示例 显示了一个具有可写目录的不可变容器的例子。\n构建安全的容器镜像 容器镜像通常是通过从头开始构建容器或在从存储库中提取的现有镜像基础上创建的。除了使用可信的存储库来构建容器外，镜像扫描是确保部署的容器安全的关键。在整个容器构建工作流程中，应该对镜像进行扫描，以识别过时的库、已知的漏洞或错误配置，如不安全的端口或权限。\n图4：容器的构建工作流程，用 webhook 和准入控制器进行优化 实现镜像扫描的一种方法是使用准入控制器。准入控制器是 Kubernetes 的原生功能，可以在对象的持久化之前，但在请求被验证和授权之后，拦截和处理对 Kubernetes API 的请求。可以实现一个自定义或专有的 webhook，以便在集群中部署任何镜像之前执行扫描。如果镜像符合 webhook 配置中定义的组织的安全策略，这个准入控制器可以阻止部署。\nPod 安全策略 Pod 的创建应遵守最小授权原则。\nPod 安全策略（PSP）1是一个集群范围内的策略，它规定了 Pod 在集群内执行的安全要求 / 默认值。虽然安全机制通常是在 Pod/Deployment 配置中指定的，但 PSP 建立了一个所有 Pod 必须遵守的最低安全门槛。一些 PSP 字段提供默认值，当 Pod 的配置省略某个字段时使用。其他 PSP 字段被用来拒绝创建不符合要求的 Pod。PSP 是通过 Kubernetes 准入控制器执行的，所以 PSP 只能在 Pod 创建期间执行要求。PSP 并不影响已经在集群中运行的 Pod。\nPSP 很有用，可以在集群中强制执行安全措施。PSP 对于由具有分层角色的管理员管理的集群特别有效。在这些情况下，顶级管理员可以施加默认值，对低层级的管理员强制执行要求。NSA 和 CISA 鼓励企业根据自己的需要调整 附录 C：Pod 安全策略示例 中的 Kubernetes 加固 PSP 模板。下表描述了一些广泛适用的 PSP 组件。\n表 1: Pod 安全策略组件\n字段名称 使用方法 建议 privileged 控制 Pod 是否可以运行有特权的容器。 设置为 false。 hostPID、hostIPC 控制容器是否可以共享主机进程命名空间。 设置为 false。 hostNetwork 控制容器是否可以使用主机网络。 设置为 false。 allowedHostPaths 将容器限制在主机文件系统的特定路径上。 使用一个 “假的” 路径名称（比如 /foo 标记为只读）。省略这个字段的结果是不对容器进行准入限制。 readOnlyRootFilesystem 需要使用一个只读的根文件系统。 可能时设置为 true。 runAsUser, runAsGroup, supplementalGroups, fsGroup 控制容器应用程序是否能以 root 权限或 root 组成员身份运行。 - 设置 runAsUser 为 MustRunAsNonRoot。- 将 runAsGroup 设置为非零（参见附录 C 中的例子：Pod 安全策略示例）。 将 supplementalGroups 设置为非零（见附录 C 的例子）。将 fsGroup 设置为非零（参见附录 C 中的例子：Pod 安全策略示例）。 allowPrivilegeEscalation 限制升级到 root 权限。 设置为 false。为了有效地执行 runAsUser: MustRunAsNonRoot 设置，需要采取这一措施。 seLinux 设置容器的 SELinux 上下文。 如果环境支持 SELinux，可以考虑添加 SELinux 标签以进一步加固容器。 AppArmor 注解 设置容器所使用的 AppArmor 配置文件。 在可能的情况下，通过采用 AppArmor 来限制开发，以加固容器化的应用程序。 seccomp 注解 设置用于沙盒容器的 seccomp 配置文件。 在可能的情况下，使用 seccomp 审计配置文件来识别运行中的应用程序所需的系统调用；然后启用 seccomp 配置文件来阻止所有其他系统调用。 注意：由于以下原因，PSP 不会自动适用于整个集群：\n首先，在应用 PSP 之前，必须为 Kubernetes 准入控制器启用 PodSecurityPolicy 插件，这是 kube-apiserver 的一部分。 第二，策略必须通过 RBAC 授权。管理员应从其集群组织内的每个角色中验证已实施的 PSP 的正确功能。 在有多个 PSP 的环境中，管理员应该谨慎行事，因为 Pod 的创建会遵守最小限制性授权策略。以下命令描述了给定命名空间的所有 Pod 安全策略，这可以帮助识别有问题的重叠策略。\nkubectl get psp -n \u0026lt;namespace\u0026gt; 保护 Pod 服务账户令牌 默认情况下，Kubernetes 在创建 Pod 时自动提供一个服务账户（Service Account），并在运行时在 Pod 中挂载该账户的秘密令牌（token）。许多容器化的应用程序不需要直接访问服务账户，因为 Kubernetes 的协调工作是在后台透明进行的。如果一个应用程序被破坏了。Pod 中的账户令牌可以被网络行为者收集并用于进一步破坏集群。当应用程序不需要直接访问服务账户时，Kubernetes 管理员应确保 Pod 规范禁用正在加载的秘密令牌。这可以通过 Pod 的 YAML 规范中的 automountServiceAccountToken: false 指令来完成。\n加固容器引擎 一些平台和容器引擎提供了额外的选项来加固容器化环境。一个强有力的例子是使用管理程序来提供容器隔离。管理程序依靠硬件来执行虚拟化边界，而不是操作系统。管理程序隔离比传统的容器隔离更安全。在 Windows® 操作系统上运行的容器引擎可以被配置为使用内置的 Windows 管理程序 Hyper-V®，以增强安全性。\n此外，一些注重安全的容器引擎将每个容器部署在一个轻量级的管理程序中，以实现深度防御。由管理程序支持的容器可以减少容器的突破。\n译者注：Pod Security Policy 已在 1.21 版本中宣布弃用，作为替代，1.22 引入了内置的 Pod Security Admission 控制器以及新的 Pod Security Standards 标准。来源  ↩︎\n","relpermalink":"/book/kubernetes-hardening-guidance/kubernetes-pod-security/","summary":"Pod 是 Kubernetes 中最小的可部署单元，由一个或多个容器组成。Pod 通常是网络行为者在利用容器时的初始执行环境。出于这个原因，Pod 应该被加固，以使利用更加困难，并限制成功入侵的影响。 图3：有 sidecar 代理作为日志容器的 Pod","title":"Kubernetes Pod 安全"},{"content":"在构建应用程序以管理基础架构时，我们要将需要公开的 API 与要创建的应用程序等量看待。这些 API 将代表您的基础架构的抽象，而应用程序将使用 API 消费这些这些基础架构。\n务必牢牢掌握两者的重要性，和如何利用它们来创建可扩展的弹性基础架构。\n在本章中，我们将举一个虚构的云原生应用程序和 API 示例，这些应用程序和 API 会经历正常的应用程序周期。如果您想了解更多有关管理云原生应用程序的信息，请参阅第 7 章。\n设计 API 这里的 API 是指处理数据结构中的基础架构表示，而不关心如何暴露或消费这些 API。通常使用 HTTP RESTful 端点来传递数据结构，API 如何实现对本章并不重要。\n随着基础架构的不断发展，运行在基础架构之上的应用程序也要随之演变。为这些应用程序的功能将随着时间而改变，因此基础架构是也是隐性地演变。随着基础架构的不断发展，管理它的应用程序也必须发展。\n基础架构的功能、需求和发展将永无止境。如果幸运的话，云供应商的 API 将会保持稳定，不会频繁更改。作为基础架构工程师，我们需要做好准备，以适应这些需求。我们需要准备好发展我们的基础架构和运行其上的应用程序。\n我们必须创建可缩放的应用程序，并准备对其进行扩展。为了做到这一点，我们需要了解在不破坏应用程序现有流程的情况下对应用程序进行大量更改的细微差别。\n管理基础架构的工程应用的好处在于它解放了运维人员的生产力。\n应用程序中使用的抽象现在由工程师来完成。我们可以详尽或抽象的描述 API，这都可以。通过具体和抽象定义的强大组合可以帮助运维人员准确地描述他们需要管理基础架构。\n添加功能 根据功能的性质，向基础架构应用程序添加功能可能非常简单也可能非常复杂。添加功能的目标是能够添加新功能而不会危害现有功能。我们绝不希望引入会以给系统其他组件带来负面影响的功能。此外，我们一直希望确保系统输入在合理的时间内保持有效。\n例 5-1 是本书前面介绍的基础架构 API 演化的具体示例。我们称之为 API v1。\n例 5-1. v1.json\n{ \u0026#34;virtualMachines\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;my-vm\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;large\u0026#34;, \u0026#34;localIp\u0026#34;: \u0026#34;10.0.0.111\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;my-subnet\u0026#34; }], \u0026#34;subnets\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;my-subnet\u0026#34;, \u0026#34;cidr\u0026#34;: \u0026#34;10.0.100.0/24\u0026#34; }] } 想象一下，我们希望实现一项功能，允许基础架构运维人员为虚拟机定义 DNS 记录。新的 API 看起来略有不同。在例 5-2 中，我们将定义一个名为 version 的顶级指令，告诉应用程序这是 API 的 v2 版本。我们还将添加一个新的块，用于在虚拟机块的上下文中定义 DNS 记录。这是 v1 中不支持的新指令。\n例 5-2. v2.json\n{ \u0026#34;version\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;virtualMachines\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;my-vm\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;large\u0026#34;, \u0026#34;localIp\u0026#34;: \u0026#34;10.0.0.111\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;my-subnet\u0026#34;, \u0026#34;dnsRecords\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;ttl\u0026#34;: 60, \u0026#34;value\u0026#34;: \u0026#34;my-vm.example.com\u0026#34; }] }], \u0026#34;subnets\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;my-subnet\u0026#34;, \u0026#34;cidr\u0026#34;: \u0026#34;10.0.100.0/24\u0026#34; }] } 这两个对象都是有效的，应用程序应该继续支持它们。应用程序应检测到 v2 对象是否打算使用内置于应用程序中的 DNS 新功能。该应用程序应该足够聪明，以适当地导航到新功能。将资源应用于云时，新的 v2 对象的资源集将与第一个 v1 对象相同，但添加了单个 DNS 资源。\n这引入了一个有趣的问题：应用程序应该如何处理旧的 API 对象？应用程序应仍可以在云中创建资源，且支持无 DNS 的虚拟机。\n随着时间的推移，运维人员可以修改现有虚拟机对象以使用新的 DNS 功能。应用程序自动检测到增量并为新功能创建 DNS 记录。\n弃用功能 让我们快速转到下一个 API v3。在这种情况下，我们的 API 不断发展，我们在表示 IP 地址方面已陷入僵局。\n在 API v1 中，我们能够通过本地 IP 指令方便地为网络接口声明一个本地 IP 地址。我们现在的任务是为虚拟机提供多种网络接口。需要注意的是，这将与最初的 API v1 冲突。\n让我们来看一下示例 5-3 中新的 v3 版本的 API。\n例 5-3. v3.json\n{ \u0026#34;version\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;virtualMachines\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;my-vm\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;large\u0026#34;, \u0026#34;networkInterfaces\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;ip\u0026#34;: \u0026#34;10.0.0.11\u0026#34; }], \u0026#34;subnet\u0026#34;: \u0026#34;my-subnet\u0026#34;, \u0026#34;dnsRecords\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;ttl\u0026#34;: 60, \u0026#34;value\u0026#34;: \u0026#34;my-vm.example.com\u0026#34; }] }], \u0026#34;subnets\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;my-subnet\u0026#34;, \u0026#34;cidr\u0026#34;: \u0026#34;10.0.100.0/24\u0026#34; }] } 使用定义多个网络接口所需的新数据结构，我们已弃用本地 IP 指令。但是我们并没有删除定义 IP 地址的概念，我们只是简单地重组了它。这意味着我们可以分两个阶段废弃该指令。首先警告，然后是拒绝。\n在警告阶段，我们的应用程序可能会输出不再支持本地 IP 指令的警告。应用程序可以接受在对象中定义的指令，并将旧 API v2 转换为新 API v3。\n转换将采用为本地 IP 定义的值，并在新网络接口指令中创建与初始值相匹配的单个块。应用程序可以继续处理 API 对象，就好像用户发送了 v3 对象而不是 v2 对象一样。预计用户会注意到该指令已被弃用，并及时更新其表示。\n在拒绝阶段，应用程序将彻底拒绝 v2 API。用户将被迫更新他们的 API 到更新的版本，或者甘愿在基础架构中冒此风险。\n弃用是非常危险的\n这是一个极其危险的过程，成功将用户引导到新版本可能会非常困难。拒绝输入必须给出很好的理由。\n如果输入信息的会破坏应用程序保障，则应拒绝该信息。否则，最佳实践通常是警告并继续。\n破坏用户的输入很容易让运维人员感到不安和沮丧。\n基础架构工程师在对 API 进行版本控制时，必须对在何时弃用哪些功能做出最佳判断。此外，工程师需要花时间给出巧妙的解决方案，这些方案可以是警告或转换。在某些情况下，做到悄无声息的 API 转换对不断发展的云原生基础架构来说是一个巨大的胜利。\n基础架构变异 基础架构需要随着时间的推移而变化。这是云原生环境的本质。不仅应用程序频繁部署，而且运行基础架构的云供应商也在不断变化。\n基础架构的变化可以有多种形式，比如扩大或缩小基础架构，复制整个环境或消费新资源。\n当运维人员承担变更基础架构的任务时，我们可以看到 API 的真实价值。假设我们想要扩展环境中的虚拟机数量。不需要更改 API 版本，但对基础架构的表示做一些小的调整将很快反映出变化。就这么简单。\n然而，重要的是要记住，在这种情况下，运维可能是一个人，也可能是另一个软件。\n请记住，我们故意将 API 构造成易于被计算机解码。我们可以在 API 的两端使用该软件！\n使用 Operator 消费和生产 API\nOperater—— 构建云原生产品和平台的 CoreOS 公司创造了这个术语，即 Kubernetes 控制器，实现了软件取代人类参与管理特定应用的需求。通过协调预期状态和设定预期状态来实现。\nCoreOS 在他们的博客文章中这样描述 Operator：\nOperator 是特定应用程序的控制器，它代表 Kubernetes 用户扩展 Kubernetes API 以创建、配置和管理复杂有状态应用程序的实例。它建立在基本的 Kubernetes 资源和控制器概念的基础上，但包含一个域或特定于应用程序的知识体系以实现常见任务的自动化。\n该模式规定 Operator 可以通过给定声明性指令集来更改环境。Operator 是工程师应该创建的用于管理其基础架构的云原生应用程序类型的完美示例。\n设想一个简单的情景 —— 自动调节器（autoscaler）。假设我们有一个非常简单的软件，可以检查环境中虚拟机上的平均负载。我们可以定义一个规则，只要平均负载平均值高于 0.7，我们就需要创建更多的虚拟机来均匀地分配我们的负载。\nOperator 的规则会随着负载平均值的增加而不再适用，最终 Operator 需要用另一台虚拟机更新基础架构 API。这样可以扩大我们的基础架构，但同样我们也可以很容易的定义另一个规则当平均负载降至 0.2 以下时缩小虚拟机规模。请注意，Operator 这个术语在这里应该是一个应用程序，而不是一个人。\n这是自动缩放的一个非常原始的例子，但该模式清楚地表明软件现在可以开始扮演人类运维人员的角色。\n有许多工具可以帮助扩展如 Kubernetes、Nomad 和 Mesos 等基础架构上的应用程序负载。这假定应用程序层运行一个编排调度器上，它将为我们管理应用程序负载。\n想象一下，如果多个基础架构管理应用程序使用相同的 API，那么会进一步将基础架构 API 的价值最大化。这是一个非常强大的基础架构演进模式。\n我们来看看相同的 API—— 记住它只有几千字节的数据，并且在两个独立的基础架构管理应用程序运行。图 5-1 显示了一个示例，两个基础架构应用程序从相同的 API 获取数据但将基础架构部署到各自独立的云环境。\n图 5-1. 一个 API 被部署在两个云中 该模型为基础架构工程师提供了在多个云提供商之间提供通用抽象的强大功能。现在我们可以看到应用程序如何确保 API 在多个地方代表相同的基础架构。如果基础架构 API 负责提供自己的抽象和资源调配，则基础架构不必与单个云提供商的抽象相关联。用户可以在他们选择的云中创建独特的基础架构排列。\n维护云提供商兼容性 虽然保持 API 与云提供商的兼容性将会有很多工作要做，但对于部署工作流程和供应流程时，很少需要改变。请记住，人类比技术更难改变。如果您可以保证人类的环境一致，这将抵消所需的技术开销。\n您还应该权衡多云兼容性的好处。如果它不是您的基础架构的需求，您可以节省大量的工作。考虑云厂商锁定时请参阅附录 B。\n我们也可以在同一个云中运行不同的基础架构管理应用程序。这些应用程序可能会对 API 进行不同的解释，这会导致对于运维人员的意图的定义略有不同。根据运维人员定义的基础架构的意图，在管理应用程序之间进行切换可能只是我们所需要的。图 5-2 显示了两个应用程序正在读取相同的 API 源，但在实现数据时会根据环境和需要而不同。\n图 5-2. 一个 API 以不同的方式部署在同一个云中 结论 与基础架构 API 相比，基础架构应用的排列组合是无止境的。这为基础架构工程师提供了一个非常灵活和可扩展的解决方案，希望能够以不同的环境和方式掌握基础架构。\n我们为满足基础架构要求而可能构建的各种应用程序现在已成为基础架构本身的代表。这是第 3 章中定义的基础设施即软件的缩影。\n请务必记住，我们构建的应用程序本身就是云原生应用程序。这是一个有趣的故事，因为我们正在构建云原生应用程序来管理云原生基础架构。\n","relpermalink":"/book/cloud-native-infra/developing-infrastructure-applications/","summary":"在构建应用程序以管理基础架构时，我们要将需要公开的 API 与要创建的应用程序等量看待。这些 API 将代表您的基础架构的抽象，而应用程序将使用 API 消费这些这些基础架构。 务必牢牢掌握两者的重要性，和如何利用它们来创建可","title":"第 5 章：开发基础架构应用程序"},{"content":"本文为托管云原生应用的参考平台实施 DevSecOps 原语提供全面指导。它包括对参考平台的概述，并描述了基本的 DevSecOps 原语（即 CI/CD 管道）、其构建模块、管道的设计和执行，以及自动化在 CI/CD 管道中有效执行工作流程的作用。\n参考平台的架构除了应用代码和提供应用服务的代码外还包括用于基础设施、运行时策略和持续监测应用健康状况的功能元素，可以通过具有独立 CI/CD 管道类型的声明性代码来部署。还介绍了这些代码的运行时行为、实现高安全性的好处，以及使用风险管理工具和仪表盘指标的管道内的工件来提供持续授权操作（C-ATO）。\n","relpermalink":"/book/service-mesh-devsecops/summary-and-conclusion/","summary":"本文为托管云原生应用的参考平台实施 DevSecOps 原语提供全面指导。它包括对参考平台的概述，并描述了基本的 DevSecOps 原语（即 CI/CD 管道）、其构建模块、管道的设计和执行，以及自动化在 CI/CD 管道中有效执行工作流程的作用。 参考平台的架构","title":"第五章：摘要和结论"},{"content":"OpenTelemetry 被设计成允许长期稳定性和不确定性并存的局面。在 OpenTelemetry 中，稳定性保证是在每个信号的基础上提供的。与其看版本号，不如检查你想使用的信号的稳定性等级。\n信号生命周期 图 6-1 显示了新信号是如何被添加到 OpenTelemetry 的。实验性信号仍在开发中。它们可能在任何时候改变并破坏兼容性。实验性信号的开发是以规范提案的形式开始的，它是与一组原型一起开发的。一旦实验性信号准备好在生产中使用，信号的特性就会被冻结，新信号的测试版就会以多种语言创建。测试版可能不是完整的功能，它们可能会有一些突破性的变化，但它们被认为是为早期采用者的产品反馈做好准备。一旦一个信号被认为可以被宣布为稳定版本，就会发布一个候选版本。如果候选版本能够在一段时间内保持稳定，没有问题，那么该信号的规范和测试版都被宣布为稳定。\n一旦一个信号变得稳定，它就属于 OpenTelemetry 的长期支持保障范围。OpenTelemetry 非常重视向后兼容和无缝升级。详情见以下章节。\n如果 OpenTelemetry 信号的某个组件需要退役，该组件将被标记为废弃的。被废弃的组件不再获得新的功能，但它们仍然被 OpenTelemetry 的长期支持保证所覆盖。如果可能的话，该组件将永远不会被删除，并将继续发挥作用。如果一个组件必须被删除，将提前宣布删除日期。\n实验性的功能总是与稳定性的功能保持在不同的包中，稳定的功能永远不能引用实验性的功能。这确保了新的开发不会影响现有特性的稳定性。只要库只依赖于稳定的特性，它们就不会经历破坏性的 API 变化。\n图 6-1：OpenTelemetry 中的每个主要功能都被赋予了一个稳定性等级，并遵循相同的生命周期。 API 的稳定性 OpenTelemetry API 预计将被数以千计的库所依赖，有数以百万计的调用站点。因此，API 的稳定部分决不能破坏向后的兼容性。应用程序的所有者和库的开发者不应该为了升级到一个新版本的 API 而重新测量他们的应用程序。\n如果一个 OpenTelemetry API 被废弃（这不太可能），被废弃的 API 仍将保持稳定并发挥功能。\n原生工具是稳定的工具\n携带原生 OpenTelemetry 仪表的 OSS 库应该只使用稳定的 API，因为实验性功能的改变可能会造成依赖性冲突。\n也就是说，我们鼓励进行测试。如果一个库愿意为实验性的 OpenTelemetry 功能提供支持，这是一个给项目提供反馈和参与新功能设计的好方法。然而，我们建议将与实验性 OpenTelemetry 功能的集成作为可选的插件提供，终端用户必须单独安装才能启用。\n一旦功能变得稳定，就没有必要把它们作为一个单独的插件。事实上，最好是将 OpenTelemetry 原生集成，因为用户可能会忘记安装插件。这样一来，如果应用程序所有者安装了 OpenTelemetry SDK，他们就会自动开始接收来自每个库的数据。如果没有安装 SDK，API 的调用就没有意义了。原生仪表是我们希望 OpenTelemetry 能够简化应用程序所有者的观察能力的一种方式 —— 它已经存在于每一个库中，只要它需要，就可以随时使用。\nSDK 和收集器的稳定性 SDK 的稳定性集中在两个方面：插件接口和资源使用。SDK 可能偶尔会废止一个插件接口。为了确保应用程序的所有者能够干净利落地进行升级，必须在废弃的接口被删除之前添加一个替代接口，而使用废弃接口的流行插件必须被迁移到新的接口上。通过以这种方式安全地迁移插件生态系统，可以避免应用程序所有者陷入这样的境地：他们想要升级，但却被一个无法使用的插件所阻挡。在废弃和移除一个插件接口之间必须有至少 6 个月的时间，而且废弃的接口只有在维护它们会造成性能问题时才会被移除。否则，我们将无限期地保留这些被废弃的接口。\n说到性能，OpenTelemetry SDK 的稳定部分必须避免性能倒退，以确保 SDK 的较新版本在升级时不会引起资源争夺。很明显，启用新版本中增加的功能可能需要额外的资源。但是，简单地升级 SDK 不应该导致性能退步。\n与 SDK 一样，收集器试图避免性能退步，并为收集器插件生态系统提供一个渐进的升级路径。\n升级 OpenTelemetry 客户端 在运行 OpenTelemetry 时，我们希望用户能保持最新的 SDK 版本。有两个事件可能会迫使用户升级 SDK：一个库将其仪表升级到新版本的 API，或者 OpenTelemetry 发布一个重要的安全补丁。\n上面列出的稳定性保证确保了这种升级路径始终是可行的。只要一个应用程序只依赖于稳定的信号，升级应该只涉及依赖性的提升。教程不需要重写，插件也不会突然变得不受支持。\nOpenTelemetry 致力于向后兼容的一个很好的例子是它对其前身 OpenTracing 的支持。OpenTelemetry 追踪信号与 OpenTracing API 完全兼容，OpenTelemetry 和 OpenTracing API 调用可以混合到同一个应用程序中。OpenTracing 用户可以升级到 OpenTelemetry 而不需要重写现有的仪表。\n","relpermalink":"/book/opentelemetry-obervability/stability-and-long-term-support/","summary":"第 6 章：稳定和长期支持","title":"第 6 章：稳定和长期支持"},{"content":"有时候紧急 CL 必须尽快通过 code review 过程。\n什么是紧急情况？ 紧急 CL 是这样的小更新：允许主要发布继续而不是回滚，修复显著影响用户生产的错误，处理紧迫的法律问题，关闭主要安全漏洞等。\n在紧急情况下，我们确实关心 Code Review 的整体速度，而不仅仅是响应的速度。仅在这种情况下，审查人员应该更关心审查的速度和代码的正确性（是否解决了紧急情况？）。此外（显然）这类状况的审查应该优先于所有其他 code reivew。\n但是，在紧急情况解决后，您应该再次查看紧急 CL 并进行更彻底的审查 。\n什么不是紧急情况？ 需要说明的是，以下情况并非紧急情况：\n想要在本周而不是下周推出（除非有一些实际硬性截止日期 ，例如合作伙伴协议）。 开发人员已经在很长一段时间内完成了一项功能想要获得 CL。 审查者都在另一个时区，目前是夜间或他们已离开现场。 现在是星期五，在开发者在过周末之前获得这个 CL 会很棒。 今天因为软（非硬）截止日期 ，经理表示必须完成此审核并签入 CL。 回滚导致测试失败或构建破坏的 CL。 等等。\n什么是 Hard Deadline？ 硬性截止日期（Hard Deadline）是指如果你错过它会发生灾难性的事情。例如：\n对于合同义务，必须在特定日期之前提交 CL。 如果在某个日期之前没有发布，您的产品将在市场上完全失败。 一些硬件制造商每年只发送一次新硬件。如果您错过了向他们提交代码的截止日期，那么这可能是灾难性的，具体取决于您尝试发布的代码类型。 延迟发布一周并不是灾难性的。错过重要会议可能是灾难性的，但往往不是。\n大多数截止日期都是软截止日期，而非最后期限。软截止日期表示希望在特定时间内完成某项功能。它们很重要，但你不应该以牺牲代码健康为前提来达到。\n如果您的发布周期很长（几周），那么在下一个周期之前就可能会牺牲代码审查质量来获取功能。然而，如果重复这种模式，往往会给项目建立压倒性技术债务。如果开发人员在周期结束时经常提交 CL，只需要进行表面评审就必须“进入”，那么团队应该修改其流程，以便在周期的早期发生大的功能变更，并有足够的时间进行良好的审查。\n","relpermalink":"/book/eng-practices/review/emergencies/","summary":"有时候紧急 CL 必须尽快通过 code review 过程。 什么是紧急情况？ 紧急 CL 是这样的小更新：允许主要发布继续而不是回滚，修复显著影响用户生产的错误，处理紧迫的法律问题，关闭主要安全漏洞等。 在紧急情况下，我们确实关心 Code Review 的","title":"紧急情况"},{"content":"默认情况下，授权策略是使用服务器的工作负载端口来匹配流量的。但是在某些情况下，比如使用 curl 时使用 --haproxy-protocol，Envoy 代理会尝试在服务端口而不是工作负载端口上匹配传入的流量。本文档提供了一种允许用户执行此操作的方法。\n在开始之前，请确保你已经做了以下准备：\n熟悉 TSB 概念 安装 TSB 演示 环境 部署 Istio Bookinfo 示例应用程序 创建一个 Tenant 创建一个 Workspace 创建 Config Groups 配置 Permissions 配置 Ingress Gateway 应用 haproxy-protocol EnvoyFilter 在监听器上启用 haproxy-protocol。创建以下 haproxy-filter.yaml 文件：\napiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: proxy-protocol namespace: bookinfo spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: LISTENER patch: operation: MERGE value: listener_filters: - name: proxy_protocol typed_config: \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.listener.proxy_protocol.v3.ProxyProtocol\u0026#34; allow_requests_without_proxy_protocol: true - name: tls_inspector typed_config: \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspector\u0026#34; 使用 kubectl 应用：\nkubectl apply -f haproxy-filter.yaml 配置 TSB 网关 更新 gateway.yaml 文件如下：\napiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway Metadata: organization: tetrate name: bookinfo-gw-ingress group: bookinfo-gw workspace: bookinfo-ws tenant: tetrate spec: workloadSelector: namespace: bookinfo labels: app: tsb-gateway-bookinfo http: - name: bookinfo port: 443 hostname: \u0026#34;bookinfo.tetrate.com\u0026#34; tls: mode: SIMPLE secretName: bookinfo-certs routing: rules: - route: host: \u0026#34;bookinfo/productpage.bookinfo.svc.cluster.local\u0026#34; 使用 tctl 应用：\ntctl apply -f gateway.yaml 配置 Ingress Gateway 对象 要在服务端口而不是工作负载端口上启用授权，请更新你的 ingress.yaml 文件：\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: tsb-gateway-bookinfo namespace: bookinfo spec: kubeSpec: service: type: LoadBalancer annotations: xcp.tetrate.io/authz-ports: \u0026#34;443\u0026#34; # 此注释防止 TSB 在创建 Istio 授权策略时将此端口翻译为工作负载端口 使用 kubectl 应用：\nkubectl apply -f ingress.yaml 测试 要测试你的 Ingress 是否正确使用 haproxy-protocol，请尝试以下 curl 请求：\ncurl -k -s --connect-to bookinfo.tetrate.com:443:$GATEWAY_IP \\ \u0026#34;https://bookinfo.tetrate.com/productpage\u0026#34; | \\ grep -o \u0026#34;\u0026lt;title\u0026gt;.*\u0026lt;/title\u0026gt;\u0026#34; ","relpermalink":"/book/tsb/howto/gateway/https-with-proxy-protocol/","summary":"如何配置默认情况下在服务端口而不是工作负载端口生成授权策略。","title":"配置 Authz 以支持代理协议"},{"content":"本文描述如何配置管理平面 IAM 服务以具有多个密钥来验证 JWT 令牌。当轮换 IAM 签名密钥但仍允许访问使用旧密钥签发且尚未过期的令牌时，这将非常有用。\n以下示例演示了从tsb-certs证书使用主 IAM 签名密钥迁移至使用自定义签名密钥的过程。\n获取当前签名密钥 首先，你需要使用以下命令检索带有令牌签发者配置的配置：\nkubectl -n tsb get managementplane managementplane 并找到token 签发者 配置。 在此示例中，配置如下所示：\ntokenIssuer: jwt: expiration: 3600s issuers: - name: https://demo.tetrate.io signingKey: tls.key refreshExpiration: 2592000s signingKeysSecret: tsb-certs tokenPruneInterval: 3600s 这表示当前的签名密钥是tsb-certs秘密中的tls.key条目。我们可以检索它并保存以备将来使用：\nkubectl get secret -n tsb tsb-certs -o jsonpath=\u0026#39;{.data.tls\\.key}\u0026#39; | base64 -d \u0026gt;/tmp/old-key.pem 生成新的签名密钥的新密钥 接下来要做的是生成一个新的秘密，其中包含新的 IAM 签名密钥。这可以通过多种方式完成，但在此示例中，你将创建一个新的 RSA 密钥。 有关支持的密钥的详细信息，请参阅支持的密钥算法列表 。\n$ cat /opt/iam-key.pem -----BEGIN RSA PRIVATE KEY----- MIIEpgIBAAKCAQEA+KdhvSZBExMHlaWo7MdKA8Ku55iu/y4FwMPixitjs/DUgaQ5 1AVHyuWcV576qMi1pZwFGbx72sU+oMS4BHr8JNv5a1DwwCKdidD89aAWeL5gmCdB 1gh5qrIBohvQQ5clnQnl7PXYauDohy9U5sIWzrZ1222sweYHVhD7A1Hd7864faR4 103xP/kyvT3b2kBauAXiLQoqFT7Uk0eR/uiJmjkl8lBFt/s3ApChRytxjxiDZiGW x6Hw9rfEcgzu0gvpJpntCHY9WrdSO1YyXbWJ2C/59OwRkhqO1UOsl7QlHrWGGGYD 9CiGPahYhSt1qq01Dk6ievJQGv16Sd2Rv+rbNwIDAQABAoIBAQCqfOGX9k2yDV8q 7P3o8y+9alPQObDrCBwrsmOfqopfCyY5iWeZBtHVvR84OKn25j8dwN8CaWimdI1f X+IoOEb/4s+eFE4t/s3ze5alt1EREr9aM7iBTyhUsF5MTzO51D2W8f1zPpFXnsPw RLS6z6MhspsWi5ljDRxEl7nz6cL5M3LujW/bQMk/uG8noA2mRCywGij/6tEzytR9 +h7y0A2QU36YF6yS/amOyP+3LgpycyV6LMercABgnPUse7iLDWGg+uxPBTts78oS b1YGe20cSTDrfrDHkgYXuUKRiI7blH9+VDgLR4iZHYSdr+8cZ8zxCKKGzbW7UClP hNZ+nb9xAoGBAP620Azi+OplI1nLUetPm1X0VfZeMKg8w3gsw4DxECiKF9y5PPje 7E8DgQLl99fRNKGJoNCbdC5c6cwZv0MIiC2qyhTsaNiGZt+kGx3KcVQtjBu0sIyA YFmWYNFbIkcu3W7ugVrLk74u2BPN8YQMVse2sa9ODWE9ZL2A0mBIqUe5AoGBAPno vKanUg5Djk3CJPjOaDmUr9RS9Jiou/EdCWKjHwER8nNSQU/f/YKC0h8CGdiUA1HD Jj353Rn2bSkB2DO3S64jkOr5GmXZIf5G8GCMIBkRlGHZtZoOUlWYZyitv34wuf91 e/T+50mvt3KWdvvgiG3CUpCs5sagccKJGTJYp9JvAoGBALbl6IDIXjpZQ0gQEhOo xv6ygyN0QPYdI7LgWcX100d42WeZ76k40XBvMK03Gn9y7prr63i/l25PM2ZmOotU zgwUriTWGPcZkzcVbI84taXfStL+LSPGbukFbSIHkZaRlVk5k9LxiXYvxuJ5p+nM vmeLzQz3O+5OGk9k+CtBIaSpAoGBALZBIIvdjL8wT3Cv/OyjA2my4QRUt2M5806l YXnZArxyDUJDI7SP4z8yDvFkQ9sqHr2bN6GNPs03ZW a5nKYisAPAlmh24OSUFPFv ZNDUgHgn1PIDpyhB95PLALiu9e+es5b1ZEBJQf4AMyZTS1Tn7Dc3t6UhI3CKBEze VUzdUQ7rAoGBAJ9y76IWic7PIBbstNOq0ejsq3iMEoH/fn84lYwMDEzRLV3Y+HvQ mu69O2h7ud88ozXJntC0VTv2nU1cKpiMHq3jZ0vxNmJomd7wKxwunKAZj8GJczhm 8T+O1c682fgu4YPysGJw35j/oGed0pEKXhBMMJh/X8HPmBcujHZYXDy0 -----END RSA PRIVATE KEY----- 然后，你将生成一个包含新密钥和旧密钥的新秘密：\nkubectl -n tsb create secret generic iam-signing-keys \\ --from-file=new.key=/opt/iam-key.pem \\ --from-file=old.key=/tmp/old-key.pem 这将创建一个包含旧密钥和新密钥的秘密，如下所示：\nkubectl -n tsb get secret iam-signing-keys -o yaml apiVersion: v1 data: new.key: [...] old.key: [...] kind: Secret metadata: creationTimestamp: \u0026#34;2022-12-14T15:56:46Z\u0026#34; name: iam-signing-keys namespace: tsb resourceVersion: \u0026#34;3378979\u0026#34; uid: 54cea82b-4505-49bb-a12e-fe6f5fbee1de type: Opaque 更新管理平面以使用新密钥 一旦创建了包含所有 IAM 签名密钥的秘密，只需要相应地更新ManagementPlane CR 或 Helm 值中的tokenIssuer即可。在我们的示例中，如下所示：\ntokenIssuer: jwt: expiration: 3600s issuers: - name: https://newissuer.tetrate.io algorithm: RS256 signingKey: new.key - name: https://demo.tetrate.io algorithm: RS256 signingKey: old.key refreshExpiration: 2592000s signingKeysSecret: iam-signing-keys tokenPruneInterval: 3600s 需要进行的更改包括：\n更新signingKeysSecret以使用包含两个密钥的新创建的秘密。 声明两个签发者，一个用于新密钥中的每个密钥。列表中的第一个签发者将用于签署新的 JWT 令牌，因此如果要使用新密钥签署新的 JWT 令牌，请确保首先放置新密钥。其余签发者的密钥仅用于令牌验证。 注意 重要的是，你选择不同的签发者（可以是任何字符串）用于新密钥，同时保留旧密钥的旧签发者。否则，令牌验证将无法正常工作。 一旦在ManagementPlane中更新了令牌签发者信息，将使用新密钥签发新令牌，而旧令牌仍将被接受。一旦所有令牌都已迁移且不再需要旧密钥，就可以从签发者列表中删除旧签发者，还可以从秘密中删除旧密钥。\n","relpermalink":"/book/tsb/operations/multiple-iam-keys/","summary":"配置多个 IAM 令牌验证密钥。","title":"配置多个 IAM 令牌验证密钥"},{"content":"此功能允许通过 IngressGateway 或 Tier1Gateway 安装 CR 覆盖已注册集群的外部地址。然后将使用提供的 IP 地址/主机名从外部世界访问集群。请注意，此功能仅在你已经配置了其他 IP 地址/主机名以从外部世界访问你的 Kubernetes 集群时才有用。\n数据平面 要在 IngressGateway 中使用此功能，请在你的 IngressGateway 安装（数据平面）CR 中的 kubeSpec/service 下设置 xcp.tetrate.io/cluster-external-addresses 注释，并使用 kubectl 应用它。你可以使用：\n单个 IP 地址 单个 DNS 名称 多个 IP 地址（以逗号分隔） 但你不能配置多个 DNS 名称或将 IP 地址与 DNS 名称组合在一起。\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: bookinfo namespace: bookinfo spec: kubeSpec: deployment: hpaSpec: maxRepicas: 10 minReplicas: 1 metrics: - resource: name: cpu targetAverageUtilization: 75 type: Resource replicaCount: 1 strategy: rollingUpdate: maxUnavailable: 0 type: RollingUpdate service: annotations: xcp.tetrate.io/cluster-external-addresses: \u0026#34;10.10.10.10,20.20.20.20\u0026#34; ports: - name: mtls port: 15443 targetPort: 15443 - name: http2 port: 80 targetPort: 8080 - name: https port: 443 targetPort: 8443 type: NodePort 上述 CR 将为网关服务设置 kubernetesExternalAddresses 为 10.10.10.10 和 20.20.20.20。你可以通过检查 Ingressgateway 中公开的主机名的 Service Entry 来验证此行为。\n","relpermalink":"/book/tsb/operations/features/configure-cluster-external-addresses/","summary":"提供用于从集群外部访问网关服务的外部地址。","title":"配置集群外部地址"},{"content":"该设计指南同时适用于 TSB 和 TSE。\n理解高可用性\n应用载入最佳实践\n使用 Edge Gateway 实现高可用集群\n","relpermalink":"/book/tsb/design-guides/","summary":"TSB 设计指南。","title":"设计指南"},{"content":"Apache SkyWalking Cloud on Kubernetes (SWCK) 提供了一个外部度量适配器，从中 Kubernetes 水平 Pod 自动缩放（HPA） 控制器可以检索度量数据。用户可以将 SWCK 适配器部署到 TSB 控制平面中，以从 Observability Analysis Platform (OAP) 服务中获取目标度量数据。\n在开始之前，请确保你已经：\n熟悉 TSB 概念 安装了 TSB 演示 环境 部署了 Istio Bookinfo 示例应用程序 验证 SWCK 度量适配器 SWCK 适配器负责管理 OAP 组件等。\n该适配器应在安装 TSB 演示配置文件时已安装。要验证适配器是否成功部署，请检查相应的 Pod 是否已正确启动。\nkubectl get po -n istio-system ... \u0026lt;snip\u0026gt; ... istio-system-custom-metrics-apiserver-7cdbb5bdbb-zmwh7 1/1 Running 0 5m54s 如果由于某种原因以下资源未正确更新/生成，请尝试手动删除它们。删除它们应触发创建一个新的 Pod，具有最新的配置：\napiservice/v1beta1.external.metrics.k8s.io rolebinding/istio-system-custom-metrics-auth-reader（在 kube-system 命名空间中） HPA 配置 要启用使用 SWCK 适配器的 HorizontalPodAutoscaler，你需要使用 External 度量类型来设置配置。\nExternal 度量类型允许你根据 OAP 集群中可用的任何度量数据自动调整你的集群。要使用此功能，请提供一个带有名称和选择器的度量块，并使用 External 度量类型。\nkind: HorizontalPodAutoscaler metadata: name: productpage-hpa-external-metrics spec: - type: External external: metric: name: \u0026lt;metric_name\u0026gt; metricSelector: matchLabels: \u0026lt;label_key\u0026gt;: \u0026lt;label_value\u0026gt; ... target: .... metric_name 应为 Observability Analysis Language (OAL) 或其他子系统生成的度量名称。\nlabel_key 是 SkyWalking 度量的实体名称。如果 label_value 包含除 “.\u0026#34;、\u0026#34;-” 和 “_” 之外的特殊字符，则应该使用 “byte” 标签将其编码为十六进制字节。service.str.\u0026lt;number\u0026gt; 将表示标签值的文字，而service.byte.\u0026lt;number\u0026gt; 可以用于表示十六进制字节的特殊字符。\n例如，如果服务名称为 v1|productpage|bookinfo|demo，则 matchLabels 应如下所示：\nmatchLabels: \u0026#34;service.str.0\u0026#34;: \u0026#34;v1\u0026#34; \u0026#34;service.byte.1\u0026#34;: \u0026#34;7c\u0026#34; # \u0026#34;|\u0026#34; 的十六进制字节 \u0026#34;service.str.2\u0026#34;: \u0026#34;productpage\u0026#34; \u0026#34;service.byte.3\u0026#34;: \u0026#34;7c\u0026#34; \u0026#34;service.str.4\u0026#34;: \u0026#34;bookinfo\u0026#34; \u0026#34;service.byte.5\u0026#34;: \u0026#34;7c\u0026#34; \u0026#34;service.str.6\u0026#34;: \u0026#34;demo\u0026#34; 请注意，字节标签只接受单个字符。这意味着输入如 || 的情况应该被转换为两个条目，包括 \u0026#34;service.byte.0\u0026#34;:\u0026#34;7c\u0026#34; 和 \u0026#34;service.byte.1\u0026#34;:\u0026#34;7c\u0026#34;，而不是 service.byte.0:\u0026#34;7c7c\u0026#34;。\nlabel_keys 可以包含用于服务名称、服务实例、端点名称和查询标签的实体名称。例如，要编码服务名称，你将使用 \u0026#34;service.str.\u0026lt;number\u0026gt;\u0026#34; 或 \u0026#34;service.byte.\u0026lt;number\u0026gt;\u0026#34;，要编码端点，你将使用 \u0026#34;endpoint.str.\u0026lt;number\u0026gt;\u0026#34; 和 \u0026#34;endpoint.byte.\u0026lt;number\u0026gt;\u0026#34;，依此类推。\n一个综合示例 假设你想要自动调整部署，以确保始终有足够的副本接受每分钟大约 80 个请求。\n假设你的 “demo” 集群中的 “productpage-v1” 部署基于 OAP 中的 “service_cpm” 指标，并且服务名称为 “v1|productpage|demo|-\u0026#34;，则你的 HPA 清单应如下所示：\napiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: productpage-hpa-external-metrics spec: minReplicas: 1 maxReplicas: 5 metrics: - type: External external: metric: name: tsb.tetrate.io|service_cpm selector: matchLabels: \u0026#34;service.str.0\u0026#34;: \u0026#34;v1\u0026#34; \u0026#34;service.byte.1\u0026#34;: \u0026#34;7c\u0026#34; \u0026#34;service.str.2\u0026#34;: \u0026#34;productpage\u0026#34; \u0026#34;service.byte.3\u0026#34;: \u0026#34;7c\u0026#34; \u0026#34;service.str.4\u0026#34;: \u0026#34;bookinfo\u0026#34; \u0026#34;service.byte.5\u0026#34;: \u0026#34;7c\u0026#34; \u0026#34;service.str.6\u0026#34;: \u0026#34;demo\u0026#34; \u0026#34;service.byte.7\u0026#34;: \u0026#34;7c\u0026#34; \u0026#34;service.byte.8\u0026#34;: \u0026#34;2d\u0026#34; target: type: AverageValue value: 80 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: productpage-v1 一旦使用 kubectl apply -f 应用了上述清单，你应该在你的 bookinfo 命名空间中看到创建的 HPA：\nkubectl get hpa -n bookinfo NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS ...\u0026lt;snip\u0026gt;... productpage-hpa-external-metrics Deployment/productpage-v1 0/80 1 5 1 要测试你的应用程序，请使用类似 Hey 的工具生成一些负载。 最终，你应该看到 HPA 创建了更多的副本来处理负载：\nkubectl get hpa -n bookinfo NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS ...\u0026lt;snip\u0026gt;... productpage-hpa-external-metrics Deployment/productpage-v1 2252/80 1 10 4 请阅读 SWCK 度量适配器文档以获取更多详细信息。\n","relpermalink":"/book/tsb/howto/hpa-using-skywalking/","summary":"在 TSB 管理的集群中使用 SkyWalking 进行水平 Pod 自动缩放（HPA）","title":"使用 SkyWalking 进行 HPA"},{"content":" 安装 httpbin\n安装 Open Policy Agent\n安装 sleep\n","relpermalink":"/book/tsb/reference/samples/","summary":"安装 httpbin 安装 Open Policy Agent 安装 sleep","title":"示例应用"},{"content":"服务性能降级可能非常难以理解和隔离：\n数据太多，难以查找性能问题的原因 应用程序行为的专家（开发团队）通常无法访问运行中的集群 Tetrate Service Bridge 提供了一组工具，可以：\n允许 TSB Operator 从运行中的集群中检索服务性能数据的存档 允许应用程序开发人员查询此数据以识别最慢的事务（或带有错误的事务）并确定与慢响应相关的调用图。 在开始之前，请确保你已经：\n熟悉TSB 概念 安装TSB 演示 环境 部署Istio Bookinfo 示例应用程序 收集数据 TSB Operator 可以使用 tctl 收集集群状态。该状态包括来自工作负载的代理日志、Istio 控制平面信息、节点信息、istioctl analyze 和其他运行时信息。数据导出为一个 tar 文件。\nUsage: tctl collect [flags] Examples: # 收集数据，不进行任何模糊处理或删除 tctl collect # 收集数据但不存档结果（用于本地调试） tctl collect --disable-archive # 收集数据并使用用户提供的正则表达式进行模糊处理 tctl collect --redact-regexes \u0026lt;regex-one\u0026gt;,\u0026lt;regex-two\u0026gt; # 收集数据并使用预设进行模糊处理 tctl collect --redact-presets networking 运行 tctl collect 需要管理员权限。生成的 tar 文件可以与应用程序团队共享，以供分析和解释，使用 tctl troubleshoot。\n分析数据 然后，任何用户都可以运行 tctl troubleshoot 来检查收集的 tar 文件，并生成有关文件中记录的事务的各种报告：\n转储集群信息以识别工作负载 分析对命名工作负载的请求，以识别最慢的响应和错误响应 区分 sidecar 性能和应用程序性能 获取请求 ID，然后为这些请求生成完整的跟踪（调用图） 分析集群数据 Usage: tctl experimental troubleshoot log-explorer cluster [flags] Examples: tctl experimental troubleshoot log-explorer cluster [tar file] Flags: -h, --help 帮助 -n, --namespace string 仅列出指定命名空间的详细信息 --workspace string 仅列出指定的工作空间的详细信息 troubleshoot log-explorer cluster 提供了有关在集群中运行的所有工作负载的详细信息。用户可以通过应用筛选器（如 --workspace 或 --namespace）获取整个集群状态的子集。\n$: tctl experimental troubleshoot log-explorer cluster tctl-debug-1664467971183386000.tar.gz --workspace organizations/tetrate/tenants/payment/workspaces/payment-ws namespaces: payment-channel: services: details: pods: - details-v1-7d88846999-wgmSV productpage: pods: - productpage-v1-7795568889-tghhb ratings: pods: - ratings-v1-754f9c4975-x9h86 tsb-gateway-bookinfo: pods: - tsb-gateway-bookinfo-6c46758bf6-5q6vw payment-offers: services: reviews: pods: - reviews-primary-54c7dd49dc-8658t reviews-canary: pods: [] reviews-primary: pods: - reviews-primary-54c7dd49dc-8658t nodes: - gke-cp-cluster-1-default-pool-1119254c-w7i - gke-cp-cluster-1-default-pool-a03a3024-7519 - gke-cp-cluster-1-default-pool-a03a3024-btfs - gke-cp-cluster-1-default-pool-e090b6ac-trp workspaces: - organizations/tetrate/tenants/payment/workspaces/payment-ws 分析服务数据 Usage: tctl experimental troubleshoot log-explorer service [flags] Examples: tctl experimental log-explorer service [tar file] [service] Flags: --all 显示所有请求，而不仅仅是最长的请求和带有错误的请求。 --full-log 打印完整的 Envoy 访问日志，而不是摘要。 -h, --help 帮助 --limit int 要显示的请求数量（默认为 10） -n, --namespace string 包含服务的命名空间。 troubleshoot log-explorer service 提供了有关 10 个最长请求的详细信息。它输出了 Envoy sidecar 内部和应用程序服务内部消耗的时间的摘要。\n通过此报告，用户可以获取消耗时间最长的请求的请求 ID，以便在下一步进行分析。也可以使用 --full-log 标志访问 Envoy 请求日志信息。\n分析请求数据 Usage: tctl experimental troubleshoot log-explorer request [flags] Examples: tctl experimental log-explorer request [tar file] [requestID] Flags: -h, --help 帮助 -o, --output-type string 选择输出类型，可用格式为 json 和 yaml，默认格式为 yaml（默认为 \u0026#34;yaml\u0026#34;） `tr\noubleshoot log-explorer request报告了由提供的requestID标识的单个请求的跟踪。它输出了从 IngressGateway Pod IP 到最终应用工作负载的请求链，报告了 Envoy sidecar 和应用服务消耗的总时间，以及诸如requestType（指示请求是 inbound还是outbound）、工作负载的命名空间和名称以及 calledBy` IP 和端口等详细信息。\n","relpermalink":"/book/tsb/troubleshooting/identify-underperforming-services/","summary":"导出流量指标和跟踪信息，并分析性能不佳的服务。","title":"识别性能不佳的服务"},{"content":"在本部分中，你将检查 TSB 环境中 bookinfo 演示应用程序的服务拓扑和指标。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户和工作区。 创建配置组。 配置权限。 设置入口网关。 生成指标流量 在检查拓扑和指标之前，你需要为 bookinfo 应用程序生成流量以获得有意义的数据。使用提供的脚本生成流量：\n将以下脚本保存为 send-requests.sh ：\n#!/bin/bash while true; do curl -s https://bookinfo.tetrate.com/productpage \u0026gt; /dev/null sleep 1 done 使脚本可执行并运行它：\nchmod +x send-requests.sh ./send-requests.sh 该脚本每 1 秒向 bookinfo 产品页面发送一个请求，生成指标流量。\n查看拓扑和指标 现在，你可以在 TSB UI 中检查服务拓扑和指标。\n在左侧面板中的“租户”下，选择“仪表板”。 单击选择集群 - 命名空间。 检查租户 tetrate 、命名空间 bookinfo 和集群 demo 对应的条目。 单击选择。 设置你希望查看的数据的持续时间，并从 TSB UI 的顶部菜单栏启用自动刷新：\n为你要查看的数据选择时间范围，例如最近 5 分钟。 单击“刷新指标”图标可手动重新加载指标或选择刷新间隔以进行自动刷新。 拓扑视图 浏览拓扑页面以可视化服务的拓扑图并检查与流量、延迟和错误率相关的指标：\n指标和追踪 将鼠标悬停在服务实例上可查看更详细的指标，或单击查看全面的细分：\nTSB 自动对请求进行采样并收集请求子集的跟踪数据。选择一个服务并单击“跟踪”以列出通过该服务捕获的最新跟踪。你可以探索完整的跟踪来识别流量、时间和错误事件：\n通过理解来解释跟踪：\ntsb-gateway-bookinfo.bookinfo 调用 productpage.bookinfo.svc.cluster.local:9080 ，调用 bookinfo 命名空间中的 productpage 服务。 productpage.bookinfo 首先调用 details.bookinfo.svc.cluster.local:9080 ，调用 bookinfo 命名空间中的 details 服务。 productpage.bookinfo 随后调用 reviews.bookinfo.svc.cluster.local:9080 ，调用 bookinfo 命名空间中的 reviews 服务。 reviews.bookinfo 调用 ratings.bookinfo.svc.cluster.local:9080 ，调用 bookinfo 命名空间中的 ratings 服务。 你可以观察调用者进行调用和目标服务读取并响应之间的时间间隔。这些间隔对应于网络调用延迟和网格 sidecar 代理的行为。\n对于更复杂的调用图，你可以重新根显示以从内部服务开始，不包括前端网关和其他前端服务。\n服务仪表板 导航到 TSB UI 中的“服务”窗格，然后选择 TSB 管理的服务之一。这将打开一个包含多个窗格的综合仪表板，允许你深入了解与该特定服务相关的各种指标：\n","relpermalink":"/book/tsb/quickstart/observability/","summary":"在本部分中，你将检查 TSB 环境中 bookinfo 演示应用程序的服务拓扑和指标。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户和工作区。 创建配置组。 配置权限。 设置入","title":"拓扑和指标"},{"content":" 基本故障排除\nIngress Gateway 故障排除\n配置状态故障排除\n使用调试容器\nUI 指标故障排除\n请求头大小超限\n多重传输编码块处理\n识别性能不佳的服务\n集群载入故障排除\n","relpermalink":"/book/tsb/troubleshooting/","summary":"基本故障排除 Ingress Gateway 故障排除 配置状态故障排除 使用调试容器 UI 指标故障排除 请求头大小超限 多重传输编码块处理 识别性能不佳的服务 集群载入故障排除","title":"问题排查"},{"content":"当一个服务使用 Gateway 资源公开时，应用所有者可以配置一系列措施来保护他们的服务。Gateway 资源 （TSB / TSE ）支持以下功能：\n在集群或服务实例之间提供跨集群的 HTTP 支持，支持 TLS、路由、负载平衡和重定向 在集群或服务实例之间提供跨集群的 TCP 支持，支持 TLS、路由和负载平衡 使用 JWT 令牌进行身份验证 针对外部授权服务进行授权 基于客户端地址、HTTP 标头、路径和方法的每秒、每分钟或每小时的速率限制 使用 WASM 插件实现自定义流量管理功能 安全功能 从安全角度来看，两个有用的功能是：\n身份验证和授权 与其将身份验证方法构建到目标服务中，不如让 Ingress Gateway 根据 JWT 令牌的内容检查和授权流量。这简化了服务配置，集中了身份验证，确保与其他服务的一致性，并允许在没有任何应用程序更改的情况下采用不同的安全姿态（例如，开发、测试和生产）。\n速率限制 通过在 Ingress Gateway 中限制每秒/每分钟/每小时的请求，可以保护上游服务免受流量激增的影响。可以对资源的个体 ‘贪婪’ 使用者（例如爬取机器人和蜘蛛）进行限制，并且可以使试图接管账户的暴力尝试变得不可能。\n应用程序：配置身份验证和授权 Tetrate 平台提供了授权功能，以授权由 Ingress Gateway 接收的每个 HTTP 请求。它支持使用 JWT 声明进行本地授权，并支持外部授权（ext-authz ）来确定是否允许或拒绝请求。\n如果你有一个单独的内部系统，或者希望使用与 JWT 不同的其他身份验证架构，或者希望与第三方授权解决方案（如 Open Policy Agent (OPA) 或 PlainID）集成，你可能会决定使用外部授权系统：\napiVersion: gateway.tsb.tetrate.io/v2 kind: Gateway metadata: name: ingress-bookinfo group: g1 workspace: w1 tenant: tse organization: tse spec: workloadSelector: namespace: ns1 labels: app: gateway http: - name: bookinfo port: 9443 hostname: bookinfo.com tls: mode: SIMPLE secretName: bookinfo-certs authentication: rules: jwt: - issuer: https://accounts.google.com jwksUri: https://www.googleapis.com/oauth2/v3/certs - issuer: \u0026#34;auth.mycompany.com\u0026#34; jwksUri: https://auth.mycompany.com/oauth2/jwks authorization: external: uri: https://company.com/authz includeRequestHeaders: - Authorization # 将标头转发到授权服务。 routing: rules: - route: serviceDestination: host: ns1/productpage.ns1.svc.cluster.local 有关详细信息，请查看外部授权 ，这也适用于 TSE。\n应用程序：配置速率限制 速率限制允许你将流量通过你的 Ingress Gateway 限制到预定的限制。你可以对流量属性进行分类，如源 IP 地址和 HTTP 标头，分别对每个类别进行速率限制。\napiVersion: gateway.tsb.tetrate.io/v2 kind: Gateway metadata: name: ingress-bookinfo group: g1 workspace: w1 tenant: tse organization: tse spec: workloadSelector: namespace: ns1 labels: app: gateway http: - name: bookinfo port: 9443 hostname: bookinfo.com tls: mode: SIMPLE secretName: bookinfo-certs routing: rules: - route: serviceDestination: host: ns1/productpage.ns1.svc.cluster.local rateLimiting: settings: rules: # 对远程地址为 1.2.3.4 的客户端进行每小时 10 次的速率限制 - dimensions: - remoteAddress: value: 1.2.3.4 limit: requestsPerUnit: 10 unit: HOUR # 对 user-agent 标头中的每个唯一值进行每分钟 50 次的速率限制 - dimensions: - header: name: user-agent limit: requestsPerUnit: 50 unit: MINUTE # 对每个唯一的客户端远程地址进行每秒 100 次的速率限制 # 对 HTTP 请求的 GET 方法和路径前缀为 /productpage 的进行限制 - dimensions: - remoteAddress: value: \u0026#34;*\u0026#34; - header: name: \u0026#34;:path\u0026#34; value: prefix: /productpage - header: name: \u0026#34;:method\u0026#34; value: exact: \u0026#34;GET\u0026#34; limit: requestsPerUnit: 100 unit: SECOND 如果你关心以下任何方面，你可能需要考虑速率限制：\n防止恶意的基于容量的活动，如 DDoS 攻击或暴力攻击 限制不良行为的蜘蛛、机器人或爬取器的影响 防止你的应用程序及其资源（如数据库）过载 实施公平的业务逻辑，如为不同的用户组应用不同的 API 限制。 有关详细信息，请查看 TSB 速率限制文档 ，这也适用于 TSE。\n","relpermalink":"/book/tsb/design-guides/app-onboarding/gateway-security/","summary":"当一个服务使用 Gateway 资源公开时，应用所有者可以配置一系列措施来保护他们的服务。Gateway 资源 （TSB / TSE ）支持以下功能： 在集群或服务实例之间提供跨集群的 HTTP 支持，支持 TLS、路由、负载平衡和重定向 在集群","title":"针对服务配置额外的安全措施"},{"content":"本文描述了 Tetrate Service Bridge (TSB) 在管理平面和控制平面的容量规划的保守指南。\n这些参数适用于生产环境的安装：如果你使用类似演示的环境，TSB 将以最小的资源运行。\n注意事项 本文描述的资源配置指南非常保守。\n另请注意，本文描述的资源配置适用于 垂直 资源扩展。相同 TSB 组件的多个副本不会共享负载，因此你不能期望多个组件的合并资源产生相同的效果。TSB 组件的副本应仅用于实现高可用性。\n推荐的基线生产安装资源要求 对于一个具有 1 个注册集群和该集群内部部署的 1 个服务的 TSB 基线安装，建议如下资源。\n再次强调，下面描述的内存量非常保守。此外，vCPU 数量给出的实际性能会根据你的基础设施而波动。建议你在你的环境中验证结果。\n组件 vCPU 数量 内存 MiB TSB 服务器 (管理平面) 1 2 512 XCP Central 组件 2 2 128 XCP Edge 1 128 Front Envoy 1 50 IAM 1 128 TSB UI 1 256 OAP 4 5192 OTEL 收集器 2 1024 1 包括 Kubernetes Operator 和持久数据协调过程。\n2 包括 Kubernetes Operator。\n推荐的扩展资源参数 TSB 栈主要受 CPU 限制。通过 XCP 注册的附加集群会使 CPU 利用率增加约 4%。\n对内存使用情况的附加注册集群或附加部署的工作负载服务的影响几乎可以忽略不计。同样，对于大多数 TSB 组件的资源消耗，额外集群或工作负载的影响几乎可以忽略不计，但 TSB、XCP Central 组件、TSB UI 和 IAM 则是值得注意的例外。\n注意 作为可见度堆栈的一部分（例如 OTel/OAP 等），其资源利用取决于请求，因此资源扩展应遵循用户请求速率统计。一般而言，建议使用超过 1 个 vCPU。还要注意，可见度堆栈的性能主要受 Elasticsearch 性能限制。 因此，我们建议为部署的工作流程垂直扩展组件 1 个 vCPU。\n管理平面 除了 OAP 外，所有组件都不需要进行任何资源调整。这些组件的架构和测试支持非常大的集群。\n在管理平面的 OAP 需要额外的 CPU 和内存，每 1000 个服务需要约 100 毫核的 CPU 和 1024 MiB 的内存。例如，从所有 TSB 集群中聚合的 4000 个服务将总共需要大约 400 毫核的 CPU 和 4096 MiB 的内存。\n控制平面资源要求 以下表格显示了 TSB 控制平面的典型峰值资源利用情况，假设如下：\n带有 Sidecar 的 50 个服务 整个集群的流量为 500 个存储库 OAP 的跟踪采样率为流量的 1% 对于每个工作负载的每个请求都捕获度量值。 请注意，平均 CPU 利用率将是典型峰值值的一部分。\n组件 典型峰值 CPU (m) 典型峰值内存 (Mi) Istiod 300m 250Mi OAP 2500m 2500Mi XCP Edge 100m 100Mi Istio Operator - 控制平面 50m 100Mi Istio Operator - 数据平面 150m 100Mi TSB 控制平面 Operator 100m 100Mi TSB 数据平面 Operator 150m 100Mi OTEL 收集器 50m 100Mi 每个入口网关的 TSB/Istio Operator 资源使用情况 以下表格显示了每个入口网关使用的 TSB Operator 和 Istio Operator 的资源。\n入口网关 TSB Operator CPU (m) TSB Operator 内存 (Mi) Istio Operator CPU (m) Istio Operator 内存 (Mi) 0 100m 50Mi 10m 45Mi 50 2600m 125Mi 1100m 120Mi 100 3500m 200Mi 1300m 175Mi 150 3800m 250Mi 1400m 200Mi 200 4000m 325Mi 1400m 250Mi 250 4700m 325Mi 1750m 300Mi 300 5000m 475Mi 1750m 400Mi 组件资源利用 以下表格将展示 TSB 的不同组件如何随着 4000 个服务的规模增长并以 60 rpm 峰值，这是根据来自管理平面和控制平面的信息进行划分的。\n管理平面 服务 入口网关 流量 (rpm) 中央 CPU (m) 中央内存 (Mi) MPC CPU (m) MPC 内存 (Mi) OAP CPU (m) OAP 内存 (Mi) Otel CPU (m) Otel 内存 (Mi) TSB CPU (m) TSB 内存 (Mi) 0 0 0 rpm 3m 39Mi 5m 30Mi 37m 408Mi 22m 108Mi 14m 57Mi 400 2 60 rpm 4m 42Mi 15m 31Mi 116m 736Mi 24m 123Mi 50m 63Mi 800 4 60 rpm 4m 54Mi 24m 34Mi 43m 909Mi 26m 127Mi 85m 75Mi 1200 6 60 rpm 4m 59Mi 32m 41Mi 28m 1141Mi 27m 210Mi 213m 78Mi 1600 8 60 rpm 5m 63Mi 44m 48Mi 209m 1475Mi 29m 249Mi 113m 86Mi 2000 10 60 rpm 5m 73Mi 41m 51Mi 51m 1655Mi 24m 319Mi 211m 91Mi 2400 12 60 rpm 4m 84Mi 72m 62Mi 57m 1910Mi 29m 381Mi 227m 97Mi 2800 14 60 rpm 5m 90Mi 73m 65Mi 43m 2136Mi 16m 466Mi 275m 104Mi 3200 16 60 rpm 5m 106Mi 85m 78Mi 89m 2600Mi 43m 574Mi 382m 108Mi 3600 18 60 rpm 5m 123Mi 94m 71Mi 245m 2772Mi 37m 578Mi 625m 115Mi 4000 20 60 rpm 5m 147Mi 90m 81Mi 521m 3224Mi 15m 704Mi 508m 122Mi 注意 IAM 将在 5m/32Mi，LDAP 在 1m/12Mi，XCP Operator 在 3m 和 23Mi 时达到峰值。 控制平面 服务 入口网关 流量 (rpm) 边缘 CPU (m) 边缘内存 (Mi) Istiod CPU (m) Istiod 内存 (Mi) OAP CPU (m) OAP 内存 (Mi) Otel CPU (m) Otel 内存 (Mi) 0 0 0 rpm 3m 67Mi 6m 110Mi 55m 439Mi 16m 74Mi 400 2 60 rpm 2m 97Mi 33m 182Mi 334m 1138Mi 18m 75Mi 800 4 60 rpm 3m 153Mi 35m 249Mi 653m 1640Mi 21m 85Mi 1200 6 60 rpm 3m 192Mi 68m 286Mi 815m 2238Mi 23m 164Mi 1600 8 60 rpm 3m 238Mi 84m 324Mi 1217m 2766Mi 20m 202Mi 2000 10 60 rpm 3m 280Mi 84m 357Mi 1364m 3351Mi 17m 267Mi 2400 12 60 rpm 15m 270Mi 98m 370Mi 1658m 3921Mi 19m 331Mi 2800 14 60 rpm 5m 310Mi 334m 450Mi 2062m 4493Mi 19m 406Mi 3200 16 60 rpm 6m 352Mi 243m 470Mi 2406m 4866Mi 20m 506Mi 3600 18 60 rpm 22m 386Mi 130m 489Mi 2606m 5346Mi 20m 512Mi 4000 20 60 rpm 5m 501Mi 138m 523Mi 2904m 6128Mi 20m 620Mi 注意 Metric Server 将在 4m/24Mi，Onboarding Operator 在 4m/24Mi，XCP-Operator 在 3m/22Mi 时达到峰值。 ","relpermalink":"/book/tsb/setup/resource-planning/","summary":"Tetrate Service Bridge (TSB) 容量规划的通用指南。","title":"资源消耗与容量规划"},{"content":"🔔 警告：你可能不想阅读这部分文档。本手册的这一部分面向想要开发与 Argo CD 交互的第三方应用程序的人们，例如：\n聊天机器人 Slack 集成 🔔 注意：请确保你已完成 入门指南 。\n","relpermalink":"/book/argo-cd/developer-guide/","summary":"🔔 警告：你可能不想阅读这部分文档。本手册的这一部分面向想要开发与 Argo CD 交互的第三方应用程序的人们，例如： 聊天机器人 Slack 集成 🔔 注意：请确保你已完成 入门指南 。","title":"开发手册"},{"content":"🔔 重要提示：自 v1.4 起支持蓝绿部署和金丝雀部署。\n默认情况下，当重新应用旧 Rollout 操作时，控制器会像处理规范更改一样处理它，并执行完整的步骤列表，并执行分析。但有两个例外：\n控制器检测到正在返回到仍在其 scaleDownDelay 范围内且已缩放的蓝绿 ReplicaSet。 控制器检测到正在返回到金丝雀的“稳定”ReplicaSet，并且升级尚未完成。 通常，当期望行为是尽快回滚时，重新运行分析和步骤对于回滚操作来说是不可取的。为了帮助实现这一点，回滚窗口功能允许用户指示在窗口内提升到 ReplicaSet 时将跳过所有步骤。\n示例：\nspec: rollbackWindow: revisions: 3 revisionHistoryLimit: 5 假设有线性修订历史记录：1、2、3、4、5（当前）。从修订版本 5 回滚到 4 或 3 将落在窗口内，因此将快速跟踪。\n","relpermalink":"/book/argo-rollouts/rollout/rollback/","summary":"🔔 重要提示：自 v1.4 起支持蓝绿部署和金丝雀部署。 默认情况下，当重新应用旧 Rollout 操作时，控制器会像处理规范更改一样处理它，并执行完整的步骤列表，并执行分析。但有两个例外： 控制器检测到正在返回到仍在其 scaleDownDelay 范围内且已","title":"回滚窗口"},{"content":" 概览\nAmbassador\nAPISIX\nAWS ALB\nIstio\nNginx\n插件\nTraefik\n多提供方\n","relpermalink":"/book/argo-rollouts/traffic-management/","summary":"概览 Ambassador APISIX AWS ALB Istio Nginx 插件 Traefik 多提供方","title":"流量管理"},{"content":"本章探讨了 SPIFFE 和 SPIRE 如何与环境集成。\nSPIFFE 从一开始就被设计成可插拔和可扩展的，所以将 SPIFFE 和 SPIRE 与其他软件系统集成的话题是一个广泛的话题。一个特定的集成的架构超出了本书的范围。相反，本章意在捕捉一些可能的常见集成，以及一个高层次的概述，以及进行集成工作的策略。\n使软件能够使用 SVID 在考虑如何调整软件以使用 SVID 时，有许多选项可供选择。本节介绍了其中的几个选项，以及与之相关的注意事项。\n本地 SPIFFE 支持 这种方法需要修改现有的服务，以使它们能够感知 SPIFFE。当所需的修改最小，或者可以在跨应用服务使用的通用库或框架中引入时，它是首选。对于那些对延迟敏感的数据平面服务，或希望在应用层利用身份的服务，本地集成是最好的方法。SPIFFE 提供了一些库，如用于 Go 编程语言的 GO-SPIFFE 和用于 Java 编程语言的 JAVA-SPIFFE，它们有助于开发支持 SPIFFE 的工作负载。\n当用支持 SPIFFE 库的语言构建软件时，这通常是利用 SPIFFE 最直接的方式。上面提到的 Go 和 Java 库有使用 SPIFFE 与 gRPC 和 HTTPS 客户端和服务器的例子。\n也就是说，应该注意的是，你并不局限于 Java 和 Go 语言的选择。这些库是在开放规范的基础上实现的。在撰写本文时，社区正在努力开发 Python、Rust 和 C 编程语言的 SPIFFE 库。\nSPIFFE 感知代理 通常情况下，重构的成本太高，或者服务正在运行一个不能被修改的第三方代码。在这些情况下，用一个支持 SPIFFE 的代理来前置应用，往往是一个务实的选择。根据应用程序的部署模式，它可以是一个独立的代理或一组集中的代理。共用代理的优点是代理和不安全的服务之间的追踪仍然是本地的 —— 如果使用独立的代理，代理和应用之间的安全问题仍然必须得到考虑。\nEnvoy 是一个受欢迎的选择，Ghostunnel 是另一个不错的选择。虽然其他代理，如 NGINX 和 Apache 也可以工作，但它们与 SPIFFE 相关的功能是有限的。\nGhostunnel 是一个 L3/L4 代理，享有对整个 SPIFFE 规范集的完全本地支持，包括对 SPIFFE Workload API 和联邦的支持。对于需要 L7 功能的应用，建议使用 Envoy。虽然 Envoy 不支持 SPIFFE Workload API，但 SPIRE 实现了 Secret Discovery Service API（或 SDS API），这是一个 Envoy API，用于检索和维护证书和私钥。\n通过实施 SDS API，SPIRE 可以将 TLS 证书、私钥和可信 CA 证书直接推送到 Envoy。然后，SPIRE 会根据需要轮换短命的秘钥和证书，将更新推送到 Envoy，而不需要重新启动。\n服务网格\nL7 代理（如 Envoy）可以执行 SPIFFE 安全以上的许多功能。例如，服务发现、请求授权和负载均衡都是 Envoy 带来的功能。在使用共享库比较困难的环境中（例如，当应用程序是用许多不同的语言编写的，或者不能被修改），将这种功能加载到代理上，可能特别有吸引力。这种方法也将代理的部署推向了集中的模式，即每个应用实例都有一个专门的代理运行在它旁边。\n然而，这又产生了一个问题：如何管理所有这些代理？\n服务网格是一个代理机群和相关代理控制平面的部署。它们通常允许在部署工作负载时自动注入和配置集中的代理，并提供对这些代理的持续管理。通过将许多平台关注点加载到服务网格中，可以使应用程序与这些功能不相干。\n迄今为止，大多数服务网格的实现都是利用 SPIFFE 认证来实现服务间的追踪。有些使用 SPIRE 来实现这一目标，有些则实现了特定产品的 SPIFFE 身份提供者。\n辅助程序 于工作负载不支持 SPIFFE 工作负载 API，但仍然支持使用证书进行认证的情况，与工作负载一起运行的辅助程序可以弥补这一差距。SPIFFE Helper 就是一个例子。SPIFFE 辅助程序从 SPIFFE Workload API 中获取 SVID，并将其写入磁盘，以便应用程序能够接收它们。SPIFFE 辅助程序可以继续运行，确保磁盘上的证书在轮换时不断地被更新。当更新发生时，辅助程序可以向应用程序发出信号（或运行一个可配置的命令），这样，运行中的应用程序就可以接收到这些变化。\n许多支持 TLS 的现成的应用程序可以被配置成这样使用 SPIFFE。SPIFFE 辅助库有配置 MySQL 和 PostgreSQL 的例子。许多 Web 服务器，如 Apache HTTPD 和 NGINX 都可以进行类似的配置。这对客户端软件也很有用，它只能被配置为利用磁盘上的证书。\nopenssl x509curl grpcurl 需要注意的是，这比本地 SPIFFE 集成的灵活性要低，因为特别是，它可能不允许相同的信任配置粒度。例如，当使用 SPIFFE Helper 来配置 Apache HTTPD 的相互 TLS 时，不可能将 mod_ssl 配置为只接受具有特定 SPIFFE ID 的客户端。\n在无 SPIFFE 感知的软件中使用 SVID 由于 SVID 是基于众所周知的文档类型，所以相对来说，遇到支持文档类型的软件是很常见的，但其本身并不一定能识别 SPIFFE。好消息是，这是一个相对预期的情况，而且 SPIFFE/SPIRE 已经被设计成可以很好地处理这种情况。\nX509-SVID 双重用途 许多非 SPIFFE 系统支持使用 TLS（或相互 TLS），但依赖于证书在证书主体的通用名称（CN）或主体替代名称（SAN）扩展的 DNS 名称中具有身份信息。SPIRE 支持签发具有特定 CN 和 DNS SAN 值的 X.509 证书，这些值可以在每个工作负载的基础上指定（作为注册条目的一部分）。\n这一功能是一个重要的细节，因为它允许在不直接理解如何使用 SPIFFE ID 的软件中使用 X509-SVID。例如，HTTPS 客户端往往希望所出示的证书与请求的 DNS 名称相匹配。在另一个例子中，MySQL 和 PostgreSQL 可以使用通用名称来识别相互的 TLS 客户端。通过利用这个 SPIRE 功能，以及 SPIFFE 总体上赋予的灵活性，这些用例可以用 SPIFFE 用例所使用的同样的 SVID 来适应。\nJWT-SVID 双重用途 与 X509-SVID 可用于 SPIFFE 认证以及更传统的 X.509 用例的方式相似，JWT- SVID 也支持这种双重性。虽然 JWT-SVID 确实使用标准的主体（sub）声明来存储 SPIFFE ID，但验证方法与 OpenID Connect（或 OIDC）类似并兼容。\n更具体地说，SPIFFE Federation API 通过由 HTTPS 端点提供的 JWKS 文档公开密钥，这与用于获取 OIDC 验证的公开密钥的机制相同。因此，任何支持 OIDC 身份联盟的技术也将支持接受 JWT-SVID，无论它们是否具有 SPIFFE 感知。\n支持这种身份联盟的集成的一个例子是亚马逊网络服务（AWS）身份和访问管理（IAM）。通过配置 AWS 账户中的 IAM，以接受来自 SPIRE 作为 OIDC 身份供应商的身份，就有可能使用 SPIFFE 工作负载 API 的 JWT-SVID 来承担 AWS IAM 的角色。当需要访问 AWS 资源的工作负载不在 AWS 中运行时，这一点特别强大，有效地否定了存储、共享和管理长期的 AWS 访问密钥的需要。关于如何实现这一目标的详细例子，请参见 SPIFFE 网站上的 AWS OIDC 认证教程 。\n可以在 SPIFFE 的基础上建立什么 一旦 SPIFFE 作为一个通用的身份基础存在于你的生态系统中，并与你的应用程序集成，这可能是一个考虑在上面建立什么的好时机。在本节中，我们想介绍一下在 SPIFFE 和 SPIRE 的基础上可以建立什么。并不是说这个项目有所有的构件，可以让一切都开箱即用。有些集成件需要实施才能实现，而具体如何实现的细节会因部署而异。\n日志、监测、可观察性和 SPIFFE SPIFFE 可以向其他系统提供可验证的身份证明，这对以下组件来说是一个优势：\n基础设施度量 基础设施日志 可观测性 计量 分布式追踪 你可以使用 SVID 来确保这些系统的客户端 - 服务器通信安全。然而，你也可以扩展所有这些组件，用 SPIFFE ID 来充实数据。这样做有多种好处，例如，能够在多种类型的平台和运行时之间对事件进行关联。它甚至可以帮助识别仍然不使用 SPIFFE 身份的应用和服务，或者发现运行异常和攻击，而不管它们可能发生在基础设施的哪个角落。\n审计 对于任何安全系统，如你在 SPIRE 基础上建立的系统，日志不仅仅是帮助开发人员和操作员了解系统发生了什么的信息。任何安全系统的日志都是正在发生的事情的证据，所以有一个集中的位置来存储日志是一个好主意。在发生任何安全事件时，这些信息对取证分析非常有价值。\nSPIFFE 可以帮助增强审计数据，通过使用对集中式审计系统的认证调用来提供不可抵赖性。例如，在与审计系统建立会话时，通过使用 X509-SVID 和相互 TLS，我们可以确定日志行的来源 —— 攻击者不能简单地操纵正在发送的标签或其他数据。\n证书透明化 证书透明化（Certificate Transparency）通过提供一个开放的框架，几乎实时地监控和审计 X.509 证书，帮助发现对证书基础设施的攻击。证书透明化允许检测从被破坏的证书颁发机构恶意获取的证书。它还可以识别那些已经变质并恶意签发证书的证书颁发机构。要了解更多关于证书透明化的信息，请阅读介绍文件 。\nSPIRE 与证书透明度的整合有不同的可能性。通过这种整合，可以记录你的系统颁发的所有证书的信息，并用一种称为 Merkle Tree Hashes 的特殊加密机制来保护它，以防止篡改和不当行为。\n你可以考虑的另一个方法是在你的所有系统中强制执行证书透明化。这将防止与那些没有在证书透明化服务器中记录证书信息的应用程序和服务建立 TLS 和相互 TLS 连接。\n与证书透明化的整合已经超出了本书的范围。请查看 SPIFFE/SPIRE 社区，了解更多信息和最新更新。\n供应链安全 大部分关于 SPIFFE 预期用途的报道都是关于在运行时保护软件系统之间的通信安全。然而，在软件部署前的各个阶段保护软件也是至关重要的。供应链妥协是一个潜在的攻击媒介。为此，最好能保护软件供应链的完整性，以防止恶意行为者在代码中引入后门或脆弱的库。验证软件工件的出处和管道中执行的一系列步骤是验证软件没有被篡改的一种方式。\n你可以考虑使用 SPIFFE 来提供签名的信任根。它也可用于向供应链系统的软件组件发放身份。有几种方法可以让它与更新框架（TUF）等补充软件或公证处等人工制品签署服务一起工作，或者与 In-Toto 等供应链日志一起利用。\n有可能在两个层面上将 SPIRE 与供应链组件整合。\n首先，你可以用它来识别这个供应链系统的不同元素，以确保机械和控制平面的安全。其次，通过定制选择器来确保只有已知出处的二进制文件被发出身份。作为后者的一个例子，在一个非常初级的水平上，这种属性可以作为标签传递到使用现有 docker 选择器的容器镜像中，或者通过开发一个可以检查供应链元数据的工作负载验证器。\n为用户集成 SPIFFE SPIFFE 和 SPIRE 架构的主要重点是软件身份。它没有考虑到用户的身份，因为这个问题已经被认为得到了很好的解决，而且在如何向人类与软件发放身份方面存在重大差异。也就是说，这并不意味着你不能向用户分发 SPIFFE 的身份。\n为用户提供可验证的身份\n用户应该如何在一个支持 SPIFFE 的生态系统中交互？请记住，SPIFFE 是 Secure Production …","relpermalink":"/book/spiffe/integrating-with-others/","summary":"本章探讨了 SPIFFE 和 SPIRE 如何与环境集成。","title":"与其他系统集成"},{"content":"Cilium 与多个 Kubernetes API 组兼容。有些是废弃的或测试版的，可能只在 Kubernetes 的特定版本中可用。\n所有列出的 Kubernetes 版本都经过 e2e 测试，保证与 Cilium 兼容。本表中未列出的旧版 Kubernetes 不支持 Cilium。较新的 Kubernetes 版本，虽然没有列出，但将取决于 Kubernetes 提供的后向兼容性。\nKubernetes 版本 Kubernetes NetworkPolicy API CiliumNetworkPolicy 1.16, 1.17, 1.18, 1.19, 1.20, 1.21, 1.22, 1.23 networking.k8s.io/v1 cilium.io/v2 有一个 CRD Cilium 在 Kubernetes 中使用了一个网络策略的 CRD。这个 CRD 的模式验证可能会有变化，它可以验证 Cilium Clusterwide Network Policy（CCNP）或 Cilium Network Policy（CNP）的正确性。\nCRD 本身有一个注解，即 io.cilium.k8s.crd.schema.version，有模式定义版本。默认情况下，Cilium 会自动更新 CRD 及其验证，使用较新的版本。\n下表列出了所有的 Cilium 版本和它们预期的 schema 验证版本。\nCilium 版本 CNP 和 CCNP Schema 版本 v1.9.0-rc0 1.22.1 v1.9.0-rc1 1.22.2 v1.9.0-rc2 1.22.2 v1.9.0-rc3 1.22.3 v1.9.0 1.22.3 v1.9.1 1.22.3 v1.9.2 1.22.3 v1.9.3 1.22.3 v1.9.4 1.22.3 v1.9.5 1.22.3 v1.9.6 1.22.4 v1.9.7 1.22.5 v1.9.8 1.22.5 v1.9.9 1.22.6 v1.9.10 1.22.6 v1.9.11 1.22.6 v1.9.12 1.22.6 v1.9.13 1.22.6 v1.9.14 1.22.6 v1.9.15 1.22.6 v1.9.16 1.22.6 v1.9 1.22.6 v1.10.0-rc0 1.23.1 v1.10.0-rc1 1.23.2 v1.10.0-rc2 1.23.2 v1.10.0 1.23.2 v1.10.1 1.23.2 v1.10.2 1.23.3 v1.10.3 1.23.3 v1.10.4 1.23.3 v1.10.5 1.23.3 v1.10.6 1.23.4 v1.10.7 1.23.4 v1.10.8 1.23.4 v1.10.9 1.23.4 v1.10.10 1.23.4 v1.10.11 1.23.4 v1.10 1.23.4 v1.11.0-rc0 1.24.1 v1.11.0-rc1 1.24.1 v1.11.0-rc2 1.24.2 v1.11.0-rc3 1.24.2 v1.11.0 1.24.2 v1.11.1 1.24.3 v1.11.2 1.24.3 v1.11.3 1.24.3 v1.11.4 1.24.3 v1.11.5 1.24.3 v1.11 1.24.3 latest / master 1.25.4 ","relpermalink":"/book/cilium-handbook/kubernetes/compatibility/","summary":"Cilium 与多个 Kubernetes API 组兼容。有些是废弃的或测试版的，可能只在 Kubernetes 的特定版本中可用。 所有列出的 Kubernetes 版本都经过 e2e 测试，保证与 Cilium 兼容。本表中未列出的旧版 Kubernetes 不支持 Cilium。较新的 Kubernetes 版本，虽然没有列出，但将取决于 Kubernetes 提供","title":"Kubernetes 兼容性"},{"content":"本文件的结构如下：\n第二章简要介绍了参考平台，为其提供了实施 DevSecOps 原语的指导。\n第三章介绍了 DevSecOps 的基本要素（即管道），设计和执行管道的方法，以及自动化在执行中的作用。\n第四章涵盖了管道的所有方面，包括（a）所有管道需要解决的共同问题，（b）对第 1.1 节中列出的参考平台中五种代码类型的管道的描述，以及（c）DevSecOps 在整个生命周期中对整个应用环境（有五种代码类型的参考平台，因此承载着 DevSecOps 的实施）的安全保证的好处，包括 持续授权操作（C-ATO）。\n第五章提供了摘要和结论。\n下一章 ","relpermalink":"/book/service-mesh-devsecops/intro/organization-of-this-document/","summary":"本文件的结构如下： 第二章简要介绍了参考平台，为其提供了实施 DevSecOps 原语的指导。 第三章介绍了 DevSecOps 的基本要素（即管道），设计和执行管道的方法，以及自动化在执行中的作用。 第四章涵盖了管道的所有方面，包括（a）所有管","title":"1.5 本文件的组织"},{"content":"最后一个常见的问题是安全测试。无论代码类型是什么（例如，应用服务、Iac、Pac 或可观测性），基于微服务的基础设施的 DevSecOps 的 CI/CD 管道与服务网格应包括由自动化工具或作为服务提供的应用安全测试（AST）。这些工具会分析和测试应用程序的安全漏洞。根据 Gartner 的说法，有 四种 主要的 AST 技术：\n静态 AST（SAST）工具：分析应用程序的源码、字节码或二进制代码的安全漏洞，通常在编程和 / 或测试软件生命周期（SLC）阶段。具体来说，这项技术涉及到在提交中查看应用程序并分析其依赖关系的 技术 。如果任何依赖关系包含问题或已知的安全漏洞，提交将被标记为不安全的，不允许继续部署。这也可以包括在代码中找到应该被删除的硬编码密码 / 秘密。 动态 AST（DAST）工具：在测试或运行阶段，分析应用程序的动态运行状态。它们模拟针对应用程序（通常是支持网络的应用程序、服务和 API）的攻击，分析应用程序的反应，并确定它是否有漏洞。特别是，DAST 工具比 SAST 更进一步，在 CI 工作中启动生产环境的副本，以扫描所产生的容器和 可执行文件 。动态方面有助于系统捕捉在启动时正在加载的依赖关系，例如那些不会被 SAST 捕捉的依赖关系。 交互式 AST（IAST）工具：将 DAST 的元素与被测试的应用程序的仪器相结合。它们通常作为测试运行环境中的一个代理来实现（例如，对 Java 虚拟机或.NET CLR 进行检测），观察操作或识别攻击漏洞。 软件组成分析（SCA）工具：用于识别应用程序中使用的开源和第三方组件、其已知的安全漏洞以及典型的对抗性许可限制。 4.8.1 AST 工具的功能和覆盖要求 一般来说，测试工具（包括特定类别的 AST 工具）应该满足的总体指标是\n通过识别安全、隐私和合规性方面的差距，提高应用程序的发布质量。 与开发人员已经在使用的工具整合。 要尽可能少的测试工具，但提供必要的风险覆盖。 API 和微服务层面的低级单元测试应该有足够的可视性来确定覆盖率。 包括更高层次的 UI/UX 和系统测试。 具备深入的代码分析能力，以检测运行时的缺陷。 提高发布的速度。 要有成本效益。 特别是对 AST 工具的功能要求包括进行以下类型的扫描：\n漏洞扫描。探测应用程序的安全弱点，这些弱点可能会使它们受到攻击。 容器镜像扫描。分析容器镜像的内容和构建过程，以检测安全问题、漏洞或缺陷做法（例如，硬编码密码 / 秘密）。 监管 / 合规性扫描。评估对特定合规要求的遵守情况。 每当源代码库中的代码被修改时，都要进行漏洞扫描，以确保当前的修订版不包含任何有漏洞的 依赖 。\nAST 工具和 / 或服务的理想特征，以及行为分析的技术：\n分析源码、字节码或二进制代码 观察应用程序的行为，以确定引入安全漏洞的编码、设计、打包、部署和运行时条件。 作为 CI/CD 管道任务的一部分，扫描应用程序代码的安全漏洞和错误配置应涉及以下工件：\n容器镜像应被扫描以发现漏洞。 在容器从基础镜像（如上所述进行扫描）构建之后，应该对容器的文件系统进行漏洞和错误配置的扫描。 应该对 Git 存储库（包含应用程序源代码）进行扫描，以发现漏洞和错误配置。 容器镜像包括操作系统包（如 Alpine、UBI、RHEL、CentOS 等）和特定语言包（如 Bundler、Composer、npm、yarn 等）。\n对基础设施即代码进行安全漏洞扫描，通过防止这些漏洞进入生产，减少了操作工作量，尽管它不能取代对运行时安全的检查，因为漂移的风险始终存在。然而，必须对架构的所有部署后（运行时）变化（由于漂移）的原因进行分析，并通过向 IaC 推送适当的更新来解决，从而使其成为管道的一部分，并在后续部署中不再出现。这种方法有利于使用运行时检查来补救安全设计缺陷。\n基础设施即代码的文件可以在下面找到：\n容器编排平台本身，以促进部署（例如，Kubernetes YAML 基础设施即代码文件）。 作为 CI/CD 管道软件的一部分而发现的专用基础设施即代码文件（例如，HashiCorp Terraform 基础设施即代码文件，AWS CloudFormation 基础设施即代码文件）。 应用服务代码、策略即代码和可观测性即代码文件可以在专门的应用服务组件（如服务网格）的数据平面和控制平面组件中找到，并且应该对安全漏洞（如授权策略的信息泄露）和错误配置进行扫描。\n","relpermalink":"/book/service-mesh-devsecops/implement/security-testing-common-requirement-for-ci-cd-pipelines-for-all-code-types/","summary":"最后一个常见的问题是安全测试。无论代码类型是什么（例如，应用服务、Iac、Pac 或可观测性），基于微服务的基础设施的 DevSecOps 的 CI/CD 管道与服务网格应包括由自动化工具或作为服务提供的应用安全测试（AST）。这些工","title":"4.8 安全测试——所有代码类型的 CI/CD 管道的共同要求"},{"content":"基础架构是用来支撑应用程序的。可信任的软件对于工程成功至关重要。如果每次在终端输入 ls 命令，都会发生随机动作，那么你将永远也不会相信 ls，而是去找另一种方式来列出目录中的文件。\n我们的基础架构必须值得信任。本章旨在建立信任和验证基础架构的意识形态。我们将描述的实践旨在增加对应用程序和基础架构工程的信心。\n软件测试在当今的软件工程领域非常普遍。然而，如何测试基础架构还没有很明确的最佳实践。\n这意味着本书中的所有章节中，这一节应该是最令人兴奋的！在该领域像您这样的工程师有充分的空间发挥出色的影响力。\n软件测试是一种证明软件可以正常工作的有效做法，证明软件在各种特殊情况下软件仍然可以正常运行。因此，如果我们将相同的范例应用于基础架构测试，测试目标如下：\n证明基础架构按预期运行。 证明基础架构不会失败。 证明这两种情况在各种边缘情况下都是正确的。 衡量基础架构是否有效需要我们先定义什么叫有效。现在，您应该对使用基础架构和工程代表应用程序的想法感到满意。\n定义基础架构 API 的人应该花时间构思一个可以创建有效基础架构的理智的 API。例如，如果创建了一个定义虚拟机的 API 但里面没有使之可以运行的网络信息，这种做法就很愚蠢。您应该在 API 中创建有用的抽象，然后使用第 3 章和第 4 章中提出的想法来确保 API 创建出正确的基础架构组件。\n我们开始开发一个心智模型，通过定义 API 对基础架构的健全性进行检查。这意味着我们可以翻转逻辑并想象出相反的情况，这将是除了原始心智模型之外的所有东西。\n很值得为基础架构定义基本的完整性测试。测试基础架构的第一步是证明您的基础架构是按预期存在的，并且没有任何东西的违背原意而存在。\n在本章中，我们将探索基础架构测试，这将为新的测试工具奠定基础。\n我们该测试什么？ 在开始编写代码之前，我们首先必须确定要测试的内容。\n使用测试驱动开发是测试优先的常见做法。测试是为了证明测试点而编写的，从一开始就隐含失败。软件的开发周期都需要通过测试；也就是说，软件是为了满足测试中定义的每个要求而开发的。这是一个强大的实践，可以帮助软件保持专注，帮助工程师建立对软件的信心。\n这是一个可以用多种方式回答的哲学问题。建立值得信赖的基础架构是必不可少的。例如，如果存在依赖基础架构的业务问题，则应该对其进行测试。更重要的是，许多业务不仅依赖基础架构，还会在出现问题时自行修复。\n确定基础架构需要填补的问题空间代表了将需要编写的第一轮测试。\n远景规划是基础架构测试的另一个重要方面，但应该谨慎。在足够的前瞻性和过度工程化之间有一条看不见的界限。如果有疑问，坚持最少量的测试逻辑。\n在我们完全了解需要的测试之后，就可以考虑实施测试套件。\n编写可测试代码 调节器模式不仅旨在创建整洁的基础架构应用程序，而且还鼓励编写可测试的基础架构代码。\n这意味着，在应用程序的每一个重要步骤中，我们总是会重新创建一个相同类型的新对象，也就是说基础系统的每个主要组件都会使用相同的输入和输出。使用相同的输入和输出可以更轻松地以编程方式测试软件的小型组件。测试将确保您的组件按预期工作。\n然而，在编写基础架构测试代码时，还有许多经验值得借鉴。我们将在看看在一些虚拟情景测试基础架构的具体示例。通过浏览场景，您将学到测试基础架构代码的经验教训。\n我们还会给出工程师一些规则，应在编写可测试基础架构代码时遵守这些规则。\n验证 采用非常基础的基础架构定义，如示例 6-1。\n例 6-1. infrastructure.json\n{ \u0026#34;virtualMachines\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;my-vm\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;large\u0026#34;, \u0026#34;localIp\u0026#34;: \u0026#34;192.168.1.111\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;my-subnet\u0026#34; }], \u0026#34;subnets\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;my-subnet\u0026#34;, \u0026#34;cidr\u0026#34;: \u0026#34;10.0.100.0/24\u0026#34; }] } 这些数据的目的显而易见：确保一个名为my-vm的虚拟机的大小为large，IP 地址为192.168.1.111。这些数据也暗示确保一个名为my-subnet的子网将容纳虚拟机my-vm。\n希望您发现了这个数据的问题。虚拟机的 IP 地址超出了子网的可用 CIDR 范围。\n应用程序在运行此数据时应该会失败，因为虚拟机设置了无效的网络。如果将我们的应用程序构建为盲目地允许部署任何数据，那么我们将创建一个可以联网的基础架构。尽管我们应该编写测试以确保新的虚拟机能够在网络上路由，但是我们可以做一些其他事情来帮助加强我们的应用程序并使测试更加容易。\n在应用程序处理输入之前，我们可以首先尝试验证输入。这在软件工程中是很常见的做法。\n想象一下，如果不是盲目部署这个基础架构，我们首先会尝试验证输入。在运行时，应用程序可以很容易的检测到虚拟机的 IP 地址在虚拟机所连接的子网中无效。这将阻止输入到达基础架构环境。由于知道应用程序将故意拒绝无效的基础架构表示，我们可以编写 happy 和 sad 测试来确保实现此行为。\nHappy 测试可以对条件进行正面处理。换句话说，它是一种向应用程序发送有效 API 对象并确保应用程序接受有效输入的测试。Sad 测试，可以对相反的情况或负面情况进行分析。例 6-1 是一个 sad 测试的例子，它将一个无效的 API 对象发送给应用程序，并确保应用程序拒绝无效输入。\n这种新模式使测试基础架构非常快速，而且通常不用费什么力气。一个工程师就可以开发大量的 happy 和 sad 测试，即使是最奇怪的应用程序输入也是如此。此外，测试集合可以随着时间的推移而增长；在欺骗 API 对象流入环境场景中时，工程师可以快速添加测试以防止其再次发生。\n输入验证是测试最基本的事情之一。通过在我们的应用程序中编写简单的验证来检查理智的值，我们可以开始过滤应用程序的输入。这也给了我们一个很容易定义错误并快速返回错误的途径。\n验证提供信心，而不会让您等待基础架构发生变异。这为面向 API 开发的工程师创建了更快的反馈循环。\n输入代码库 编写易于测试的代码非常重要。经常出错可能会导致成本上升，因此需要围绕专有输入设计应用程序。专有输入是仅与程序中的一个点相关的输入，获得所需输入的唯一方法是线性执行程序。以这种方式线性编写代码对于人类大脑来说是有意义的，但这也是有效测试的最难的模式之一，特别是当涉及到测试基础架构时。\n专有输入陷入困境的例子如下：\n函数 DoSomething () 的调用返回 Something {}。 Something {} 传递给函数 NextStep (Something) 并返回 SomethingElse {}。 SomethingElse {} 被传递给函数 FinalStep (something else)，返回 true 或 false。 这里的问题是，为了测试 FinalStep () 函数，我们首先需要遍历步骤 1 和 2。在测试的情况下，这会引入复杂性和更多的失败点；甚至可能不会在测试执行的环境中工作。\n更优雅的解决方案是以这样一种方式构造代码，即可以在程序的其余部分使用相同的数据结构上调用最后的 step ()：\n代码初始化 GreatSomething {} 实现了方法 great.DoSomething ()。 GreatSomething {} 实现 NextStep () 方法。 GreatSomething {} 实现了 something.FinalStep () 方法。 从测试的角度来看，我们可以为我们希望测试的任何步骤填充 GreatSomething {}，并相应地调用这些方法。这个例子中的方法现在负责处理它们扩展的对象中定义的内存。这与最后一种方法不同，在这种方法中，特殊的内存中的结构被传递到每个函数中。\n这是一种更为优雅的设计，因为测试工程师可以轻松地在整个过程中的任何一步合成内存，而不必担心学习相同数据的一种表示形式。这是更加模块化的，如果可以迅速发现任何故障，我们可以介入。\n当您开始编写构成应用程序的软件时，请记住，您将需要在传统的运行时间轴上的许多地方跳入代码库。构建代码以使其易于在任何时候输入代码库并在内部进行测试，这是至关重要的。在这种情况下，您可以成为自己最好的朋友，也可以成为最大的敌人。\n自我意识 告诉我你如何衡量我，我会告诉你我的行为。\n——Eliyahu M. Goldratt\n在编写代码和测试时注意自己的置信度。自我意识是软件工程中最重要的部分之一，也是最容易被忽视的部分之一。\n测试的最终目标是增加对应用程序的信心。就基础架构领域来说，我们的目标就是增强对基础架构的信心。\n测试基础架构的方法没有对错之分。可以在应用程序中通过代码覆盖率和单元测试来建立信心，但是对于基础架构来说这样做可能会存在误导。\n代码覆盖率是以程序化方式衡量代码可以满足预期的行为。这个度量标准可以用作原始数据点，但我们要知道即使是覆盖率达到 100％的代码库仍然可能会出现极端中断，这一点至关重要。\n如果你以代码覆盖率来衡量测试结果，那么工程师就会编写更容易被测试覆盖的代码，而不是编写更适应该任务的代码。Dan Ariely 在他刊登于哈弗商业评论的文章“衡量标准决定一切” ：\n人们的行为会根据衡量指标作出相应的调整。衡量指标会促使某个人在该指标上做出优化。你想要得到什么，就要去衡量什么。\n应该衡量的唯一指标是信心，即我们的基础架构可以按预期工作，并且可以证明这一点。\n衡量信心几乎是不可能的。但是有些方法可以从工程师的心理和情绪中抽取有意义的数据集。\n问自己以下几个问题，记录下答案：\n我担心这行不通吗？ 我可以肯定，这将做我认为会做的事吗？ 如果有人更改此文件，会发生什么情况？ 从问题中提取数据的最强好的方式是比较以前的经验水平。例如，工程师可以做出如下陈述，团队的其他成员很快就会明白他想要传达的内容：\n比起上个季度，这次代码发布更令人担忧。\n现在，根据团队以前的经验，我们可以开始为我们的信心水平制定一套标准，从 0 开始表示完全没有信心，随着时间的流逝然后增加到非常自信。当我们了解了我们担心应用程序的哪些问题之后，再为了增加信心而制定测试内容就很简单了。\n测试类型 了解测试的类型以及测试方式将有助于工程师增加其对基础架构应用程序的信心。这些测试不需要编写，而且没有正确或错误之分。唯一的问题是我们相信应用程序会做我们想要它做的事情。\n基础架构断言 在软件工程中，有一个重要的概念是断言，这是一种强制的方式 —— 完全确定条件是否成立。目前已经有许多成功的框架使用断言来测试软件。断言是一个微小的函数，它将测试条件是否为真。这些功能可以在各种测试场景中使用，以证明概念正在发挥作用和增加我们的信心。\n在本章的其余部分中，我们将提到基础架构断言。您需要对这些断言的内容以及他们希望完成的内容有基本的了解。您还需要对 Go 语言有基本的了解，才能充分认识这些断言是做什么的。\n在基础架构领域需要声明我们的基础架构有效。对于您的项目来说值得练习下使用构建这些断言功能的库。开源社区也可以从这个工具包测试基础架构中受益。\n例 6-2 显示了 Go 语言中的断言模式。假设我们想测试虚拟机是否可以解析公共主机名，然后路由到它们。\n例 6-2. assertNetwork.go\ntype VirtualMachine struct {localIp string} func (v *VirtualMachine) AssertResolvesHostname (hostname string, expectedIp string, message string) error { // Logic to query DNS for a hostname, //and compare to the expectedIp return nil } func (v *VirtualMachine) AssertRouteable (hostname string,r …","relpermalink":"/book/cloud-native-infra/testing-cloud-native-infrastructure/","summary":"基础架构是用来支撑应用程序的。可信任的软件对于工程成功至关重要。如果每次在终端输入 ls 命令，都会发生随机动作，那么你将永远也不会相信 ls，而是去找另一种方式来列出目录中的文件。 我们的基础架构必须值得信任","title":"第 6 章：测试云原生基础架构"},{"content":"集群网络是 Kubernetes 的一个核心概念。容器、Pod、服务和外部服务之间的通信必须被考虑在内。默认情况下，很少有网络策略来隔离资源，防止集群被破坏时的横向移动或升级。资源隔离和加密是限制网络行为者在集群内转移和升级的有效方法。\n关键点\n使用网络策略和防火墙来隔离资源。 确保控制平面的安全。 对流量和敏感数据（例如 Secret）进行静态加密。 命名空间 Kubernetes 命名空间是在同一集群内的多个个人、团队或应用程序之间划分集群资源的一种方式。默认情况下，命名空间不会被自动隔离。然而，命名空间确实为一个范围分配了一个标签，这可以用来通过 RBAC 和网络策略指定授权规则。除了网络隔离之外，策略可以限制存储和计算资源，以便在命名空间层面上对 Pod 进行更好的控制。\n默认有三个命名空间，它们不能被删除：\nkube-system（用于 Kubernetes 组件） kube-public（用于公共资源） default（针对用户资源） 用户 Pod 不应该放在 kube-system 或 kube-public 中，因为这些都是为集群服务保留的。可以用 YAML 文件，如 附录 D：命名空间示例 ，可以用来创建新的命名空间。不同命名空间中的 Pod 和服务仍然可以相互通信，除非有额外的隔离措施，如网络策略。\n网络策略 网络策略控制 Pod、命名空间和外部 IP 地址之间的流量。默认情况下，没有网络策略应用于 Pod 或命名空间，导致 Pod 网络内的入口和出口流量不受限制。通过适用于 Pod 或 Pod 命名空间的网络策略，Pod 将被隔离。一旦一个 Pod 在网络策略中被选中，它就会拒绝任何适用的策略对象所不允许的任何连接。\n要创建网络策略，需要一个支持 NetworkPolicy API 的网络插件。使用 podSelector 和 / 或 namespaceSelector 选项来选择 Pod。附录 E 中展示了一个网络策略的例子。网络策略的格式可能有所不同，这取决于集群使用的容器网络接口（CNI）插件。管理员应该使用选择所有 Pod 的默认策略来拒绝所有入口和出口流量，并确保任何未选择的 Pod 被隔离。然后，额外的策略可以放松这些允许连接的限制。\n外部 IP 地址可以使用 ipBlock 在入口和出口策略中使用，但不同的 CNI 插件、云提供商或服务实现可能会影响 NetworkPolicy 处理的顺序和集群内地址的重写。\n资源政策 除了网络策略，LimitRange 和 ResourceQuota 是两个可以限制命名空间或节点的资源使用的策略。LimitRange 策略限制了特定命名空间内每个 Pod 或容器的单个资源，例如，通过强制执行最大计算和存储资源。每个命名空间只能创建一个 LimitRange 约束，如 附录 F 的 LimitRange 示例中所示。Kubernetes 1.10 和更新版本默认支持 LimitRange。\n与 LimitRange 策略不同的是，ResourceQuotas 是对整个命名空间的资源使用总量的限制，例如对 CPU 和内存使用总量的限制。如果用户试图创建一个违反 LimitRange 或 ResourceQuota 策略的 Pod，则 Pod 创建失败。附录 G 中显示了一个 ResourceQuota 策略的示例。\n控制平面加固 控制平面是 Kubernetes 的核心，使用户能够查看容器，安排新的 Pod，读取 Secret，在集群中执行命令。由于这些敏感的功能，控制平面应受到高度保护。除了 TLS 加密、RBAC 和强大的认证方法等安全配置外，网络隔离可以帮助防止未经授权的用户访问控制平面。Kubernetes API 服务器运行在 6443 和 8080 端口上，这些端口应该受到防火墙的保护，只接受预期的流量。8080 端口，默认情况下，可以在没有 TLS 加密的情况下从本地机器访问，请求绕过认证和授权模块。不安全的端口可以使用 API 服务器标志 --insecure-port=0 来禁用。Kubernetes API 服务器不应该暴露在互联网或不信任的网络中。网络策略可以应用于 kube-system 命名空间，以限制互联网对 kube-system 的访问。如果对所有命名空间实施默认的拒绝策略，kube-system 命名空间仍然必须能够与其他控制平面和工作节点进行通信。\n下表列出了控制平面的端口和服务。\n表 2：控制平面端口\n端口 方向 端口范围 目的 TCP Inbound 6443 or 8080 if not disabled Kubernetes API server TCP Inbound 2379-2380 etcd server client API TCP Inbound 10250 kubelet API TCP Inbound 10251 kube-scheduler TCP Inbound 10252 kube-controller-manager TCP Inbound 10258 cloud-controller-manager（可选） Etcd etcd 后端数据库是一个关键的控制平面组件，也是集群中最重要的安全部分。\netcd 后端数据库存储状态信息和集群 Secret。它是一个关键的控制平面组件，获得对 etcd 的写入权限可以使网络行为者获得对整个集群的 root 权限。Etcd 只能通过 API 服务器访问，集群的认证方法和 RBAC 策略可以限制用户。etcd 数据存储可以在一个单独的控制平面节点上运行，允许防火墙限制对 API 服务器的访问。管理员应该设置 TLS 证书以强制执行 etcd 服务器和 API 服务器之间的 HTTPS 通信。etcd 服务器应被配置为只信任分配给 API 服务器的证书。\nKubeconfig 文件 kubeconfig 文件包含关于集群、用户、命名空间和认证机制的敏感信息。Kubectl 使用存储在工作节点的 $HOME/.kube 目录下的配置文件，并控制平面本地机器。网络行为者可以利用对该配置目录的访问，获得并修改配置或凭证，从而进一步破坏集群。配置文件应该被保护起来，以防止非故意的改变，未经认证的非 root 用户应该被阻止访问这些文件。\n工作节点划分 工作节点可以是一个虚拟机或物理机，这取决于集群的实现。由于节点运行微服务并承载集群的网络应用，它们往往是被攻击的目标。如果一个节点被破坏，管理员应主动限制攻击面，将工作节点与其他不需要与工作节点或 Kubernetes 服务通信的网段分开。防火墙可用于将内部网段与面向外部的工作节点或整个 Kubernetes 服务分开，这取决于网络的情况。机密数据库或不需要互联网访问的内部服务，这可能需要与工作节点的可能攻击面分离。\n下表列出了工作节点的端口和服务。\n表 3：工作节点端口\n端口 方向 端口范围 目的 TCP Inbound 10250 kubelet API TCP Inbound 30000-32767 NodePort Services 加密 管理员应配置 Kubernetes 集群中的所有流量 —— 包括组件、节点和控制计划之间的流量（使用 TLS 1.2 或 1.3 加密）。\n加密可以在安装过程中设置，也可以在安装后使用 TLS 引导（详见 Kubernetes 文档 ）来创建并向节点分发证书。对于所有的方法，必须在节点之间分发证书，以便安全地进行通信。\nSecret 默认情况下，Secret 被存储为未加密的 base64 编码的字符串，并且可以被任何有 API 权限的人检索。\nKubernetes Secret 维护敏感信息，如密码、OAuth 令牌和 SSH 密钥。与在 YAML 文件、容器镜像或环境变量中存储密码或令牌相比，将敏感信息存储在 Secret 中提供了更大的访问控制。默认情况下，Kubernetes 将 Secret 存储为未加密的 base64 编码字符串，任何有 API 权限的人都可以检索到。可以通过对 secret 资源应用 RBAC 策略来限制访问。\n可以通过在 API 服务器上配置静态数据加密或使用外部密钥管理服务（KMS）来对秘密进行加密，该服务可以通过云提供商提供。要启用使用 API 服务器的 Secret 数据静态加密，管理员应修改 kube-apiserver 清单文件，以执行使用 --encryption-provider-config 参数执行。附录 H 中显示了一个 encryption-provider-config 的例子：加密实例。使用 KMS 提供者可以防止原始加密密钥被存储在本地磁盘上。要用 KMS 提供者加密 Secret，encryption-provider-config 文件中应指定 KMS 提供者，如附录 I 的 KMS 配置示例所示。\n在应用了 encryption-provider-config 文件后，管理员应该运行以下命令来读取和加密所有的 Secret。\nkubectl get secrets --all-namespaces -o json | kubectl replace -f - 保护敏感的云基础设施 Kubernetes 通常被部署在云环境中的虚拟机上。因此，管理员应该仔细考虑 Kubernetes 工作节点所运行的虚拟机的攻击面。在许多情况下，在这些虚拟机上运行的 Pod 可以在不可路由的地址上访问敏感的云元数据服务。这些元数据服务为网络行为者提供了关于云基础设施的信息，甚至可能是云资源的短期凭证。网络行为者滥用这些元数据服务进行特权升级。Kubernetes 管理员应通过使用网络策略或通过云配置策略防止 Pod 访问云元数据服务。由于这些服务根据云供应商的不同而不同，管理员应遵循供应商的指导来加固这些访问载体。\n","relpermalink":"/book/kubernetes-hardening-guidance/network-separation-and-hardening/","summary":"集群网络是 Kubernetes 的一个核心概念。容器、Pod、服务和外部服务之间的通信必须被考虑在内。默认情况下，很少有网络策略来隔离资源，防止集群被破坏时的横向移动或升级。资源隔离和加密是限制网络行为者在集群内转移和升","title":"网络隔离和加固"},{"content":"现在我们了解了组成 OpenTelemetry 的各个构件，我们应该如何将它们组合成一个强大的生产管道？\n答案取决于你的出发点是什么。OpenTelemetry 是模块化的，设计成可以在各种不同的规模下工作。你只需要使用相关的部分。这就是说，我们已经创建了一个建议的路线图供你遵循。\n安装 OpenTelemetry 客户端 可以单独使用 OpenTelemetry 客户端而不部署收集器。这种基本设置通常是绿地部署的充分起点，无论是测试还是初始生产。OpenTelemetry SDK 可以被配置为直接向大多数可观测性服务传输遥测数据。\n挑选一个导出器 默认情况下，OpenTelemetry 使用 OTLP 导出数据。该 SDK 提供了几种常见格式的导出器。Zipkin、Prometheus、StatsD 等。如果你使用的可观测性后端没有原生支持 OTLP，那么这些其他格式中的一种很可能会被支持。安装正确的导出器并将数据直接发送到你的后端系统。\n安装库仪表 除了 SDK，OpenTelemetry 仪表必须安装在所有 HTTP 客户端、Web 框架、数据库和应用程序的消息队列中。如果这些库中有一个缺少仪表，上下文传播就会中断，导致不完整的追踪和混乱的数据。\n在某些语言中，如 Java，仪表可以自动安装，这就更容易了。请确保了解 OpenTelemetry 如何在你使用的编程语言中管理仪表，并仔细检查仪表是否正确安装在你的应用程序中。\n选择传播器 仔细检查你的系统需要哪些传播器也很重要。默认情况下，OpenTelemetry 使用 W3C 的追踪上下文和 Baggage 传播器。然而，如果你的应用程序需要与使用不同的追踪传播器的服务进行通信，如 Zipkin 的 B3 或 AWS 的 X-Amzn，那么改变 OTEL_PROPAGATORS 配置以包括这个额外的传播器。\n如果 OpenTelemetry 最终要取代这些其他的追踪系统，我建议同时运行 trace-context 和额外的追踪传播器。这将使你在部署中逐步取代旧系统时，能够无缝地过渡到 W3C 标准。\n部署本地收集器 虽然有些系统有可能只使用客户端，但通过在你的应用程序所运行的机器上添加一个本地收集器，可以改善你的操作体验。\n运行一个本地收集器有许多好处，如图 7-1 所示。收集器可以生成机器指标（CPU、RAM 等），这是遥测的一个重要部分。收集器还可以完成任何需要的数据处理任务，如从追踪和日志数据中清除 PII。\n图 7-1：一个本地收集器（C）从本地应用程序（A）接收遥测数据，同时收集主机指标（H）。收集器将合并的遥测数据输出到管道的下一个阶段。 运行收集器后就可以将大多数遥测配置从你的应用程序中移出。遥测配置通常是特定的部署，而不是特定的应用。SDK 可以简单地设置为使用默认配置，总是将 OTLP 数据导出到预定义的本地端口。通过管理本地收集器，运维可以在不需要与应用程序开发人员协调或重新启动应用程序的情况下进行配置更改。在通过复杂的 CI/CD（持续集成 / 持续交付）管道移动应用程序时，这尤其有帮助，因为在不同的暂存和负载测试环境中，遥测需要不同的处理方式。\n快速发送遥测数据到本地收集器，可以作为一个缓冲器来处理负载，并确保在应用程序崩溃时，缓冲的遥测数据不会丢失。\n部署收集器处理器池 如果你的本地收集器开始执行大量的缓冲和数据处理，它就会从你的应用程序中窃取资源。这可以通过部署一个只运行收集器的机器池来解决，这些机器位于负载均衡器后面，如图 7-2 所示。现在可以根据数据吞吐量来管理收集器池的大小。\n图 7-2：应用程序（A）可以通过使用路由器或负载均衡器向收集器（C）池发送遥测信息。 本地收集器现在可以关闭其处理器以释放资源。它们继续收集机器级遥测数据，作为来自本地应用程序的 OTLP 的转发机制。\n添加额外的处理池 有时，单个收集器池是不够的。一些任务可能需要以不同的速度扩展。将收集器池分割成一个更专门的池的管道，可能允许更有效和可管理的扩展策略，因为每个专门的收集器池的工作负载变得更可预测。\n一旦你达到了这个规模，就没有什么部署的问题了。大规模系统的专门需求往往是独特的，这些需求将驱动你的可观测性管道的拓扑结构。利用收集器提供的灵活性，根据你的需求来定制每一件事情。我建议对每个收集器配置的资源消耗进行基准测试，并使用这些信息来创建弹性的、自动扩展的收集器池。\n用收集器管理现有的遥测数据 上面描述的路线图适用于上线 OpenTelemetry。但你应该如何处理现有的遥测？大多数运行中的系统已经有了某种形式的指标、日志和（可能有）追踪。而大型的、长期运行的系统往往最终会有多个遥测解决方案的补丁。不同的组件可能是在不同的时代建立的，有些组件可能是从外部继承的，比如收购。从可观测性的角度来看，这可能会导致混乱的局面。\n即使在这样复杂的遗留情况下，仍然有可能过渡到 OpenTelemetry，而不需要停机或一次重写所有的服务。秘诀是首先部署一个收集器，作为一个透明的代理。\n在收集器中，为接收你的系统目前产生的每一种类型的遥测设置接收器，并与以完全相同的格式发送遥测的导出器相连。一个 StatsD 接收器连接到一个 StatsD 导出器，一个 Zipkin 接收器连接到一个 Zipkin 导出器，以此类推。这种透明的代理可以逐步推出，而不会造成干扰。一旦所有的远程测量都由这些收集器来调解，就可以引入额外的处理。甚至在你把你的仪表切换到 OpenTelemetry 之前，你可能会发现这些收集器是管理和组织你当前拼凑的遥测系统的一个有用的方法。图 7-3 显示了一个收集器处理来自各种来源的数据。\n图 7-3：收集器可以帮助管理复杂的、拼凑的可观测性系统，这些系统以各种格式向各种存储系统发送数据。 为了开始将服务切换到 OpenTelemetry，可以在收集器上添加一个 OTLP 接收器，与现有的导出器相连。随着服务转向使用 OpenTelemetry 客户端，它们将 OTLP 发送到收集器，收集器将把 OTLP 翻译成这些系统以前产生的相同数据。这使得 OpenTelemetry 可以由不同的应用团队逐步上线，而不会出现中断。\n转移供应商 一旦所有的遥测流量都通过收集器发送，切换到一个新的可观测性后端就变得很容易了：只需在收集器中添加一个导出器，将数据发送到你想尝试的新系统，并将遥测数据同时发送到旧系统和新系统。通过向两个系统发送数据，你创造了一个重叠的覆盖范围。如果你喜欢新系统，你可以在一段时间后让旧系统退役，以避免在可视性方面产生差距。图 7-4 说明了这个过程。\n图 7-4：使用收集器在可观测性后端之间迁移而不中断服务。 也可以使用 OpenTelemetry 在多个供应商之间进行测验。你可以同时向多个系统发送遥测信息，并直接比较系统，看哪一个最适合你的需要。\n","relpermalink":"/book/opentelemetry-obervability/suggested-setups-and-telemetry-pipelines/","summary":"第 7 章：建议的设置和遥测管道","title":"第 7 章：建议的设置和遥测管道"},{"content":"本页面将详细解释 TSB 组件和你必须提供和连接以运行 TSB 的外部依赖项。\n在继续之前，请确保你已经：\n查看了 TSB 架构 并理解了 TSB 的四个层次 ：数据平面（Envoy 代理）、本地控制平面（Istio）、全局控制平面（XCP）和管理平面（TSB 本身）。\n管理平面 下图显示了管理平面（MP）组件：\nfront-envoy 端口 默认 Envoy 网关（或前置 Envoy）端口是 8443，可以由用户配置（例如更改为 443）。如果更改了默认端口，则通过前置 Envoy 进行通信的组件需要相应地进行调整以匹配用户定义的值。 front-envoy 作为 Elasticsearch 代理 TSB 的前置 Envoy 可以作为配置在 ManagementPlane CR 中的 Elasticsearch 的代理。 要使用此功能，请在 ControlPlane CR 中将 Elastic 主机和端口设置为 TSB 前置 Envoy 主机和端口。然后，来自控制平面 OAP 到 Elasticsearch 的流量将通过前置 Envoy 进行传递。 在管理平面中有两个运行的 Operator，用于管理 TSB 组件的生命周期：\nOperator 名称 描述 TSB 管理平面 Operator 用于管理管理平面中 TSB 组件生命周期的 Operator。 XCP Central Operator 用于管理 XCP Central 的生命周期的 Operator。TSB 管理平面 Operator 部署 XCP Operator 和该 Operator 的 CRD，将 XCP Central 的管理交给它们。 以下是管理平面组件。有关更多参考，请查看 管理平面安装 API 。\n组件名称 描述 Envoy Gateway（front-envoy） 为 TSB API 和 UI 提供单一入口点。 IAM 前置 Envoy 的外部授权。IAM 决定是否允许或拒绝对 TSB API 的传入请求。 TSB API Server TSB 的核心。存储将被发送到控制平面的配置。使用 NGAC 进行访问决策。将 PostgreSQL 用作后端存储。 Web UI 提供 UI 组件。具有帮助连接到各种 TSB 组件（API 服务器、OAP）的 BFF（前端后端）。 MPC（MP Controller） 在 TSB API 和 XCP Central 之间提供双向配置和集群状态同步。 XCP Central 协调多集群发现。将配置发送到所有连接的在集群中运行的 XCP Edge。接收来自 XCP Edge 的集群状态和配置状态更新。此组件由 XCP Central Operator 管理。 OAP（SkyWalking） 用于 UI 查询以从所有集群的 OAP 获取聚合指标和跟踪。使用 Elasticsearch 作为后端存储。 OTEL Collector 从管理平面中的不同组件中收集指标。从每个控制平面的 OpenTelemetry（OTEL）收集器接收指标。请注意，OTEL 收集器严格用于 TSB 组件监视，而不是你的应用程序。 teamsync 使用 LDAP 和 Azure AD 作为 IdP 时创建。从 IdP 检索用户和组并将其同步到 TSB 存储中。 cert-manager 使用 INTERNAL cert-manager 时创建。cert-manager 为内部 TSB 组件提供证书，例如 Webhook 证书等的目的。 控制平面 下图显示了控制平面（CP）组件以及数据平面（DP）。\n有四个 Operator 运行在控制平面中，用于管理 TSB 组件的生命周期：\nOperator 名称 描述 TSB Control Plane Operator 用于管理控制平面中 TSB 控制平面组件生命周期的 Operator。 XCP Edge Operator 用于管理 XCP Edge 的生命周期的 Operator。TSB 控制平面 Operator 部署 XCP Operator 和该 Operator 的 CRD，将 XCP Edge 的管理交给它们。 Istio Operator 用于管理 Istio 控制平面的生命周期的 Operator。TSB 控制平面 Operator 部署 Istio Operator 和该 Operator 的 CRD，将 Istio 组件的管理交给它们。 Onboarding Operator 用于管理所需组件的生命周期，以将 VM 工作负载（也称为网格扩展）纳入网格。 修订的控制平面 TSB 1.5 引入了修订的控制平面。当你使用修订的控制平面时，由 XCP Edge Operator 部署 Istio Operator，而不是 TSB 控制平面 Operator。要了解有关修订的控制平面的更多信息，请转到 Istio 隔离边界 。 以下是控制平面组件。有关更多参考，请查看 控制平面安装 API 。\n组件名称 描述 XCP edge 接收来自 XCP Central 的配置并将其转化为 Istio 配置。向 XCP Central 发送关于配置状态和集群清单的更新。由 XCP Edge Operator 管理。 Istiod 提供服务发现、将配置分发到 Envoy 代理以及工作负载证书管理的 Istio 组件。由 Istio Operator 管理。 OAP（SkyWalking） 接收来自集群中所有 Istio sidecar 和网关的访问日志和跟踪。处理这些访问日志并生成指标。指标和跟踪将被发送到 Elasticsearch。 OTEL Collector 从控制平面中的不同 TSB 组件中收集指标。将指标导出到同一 pod 中的 Prometheus 导出器以及通过前置 Envoy 到管理平面的 OTEL 收集器。请注意，OTEL 收集器严格用于 TSB 组件监视，而不是你的应用程序。 SkyWalking HPA 提供外部指标适配器，Kubernetes 水平 Pod 自动缩放（HPA）控制器可以从中检索指标。 速率限制服务器 提供内建的速率限制功能的可选组件。 VM Gateway 在启用网格扩展时部署。VM 网关提供与在 VM 中运行的 sidecar 的 Istiod 和 OAP 的连接。 Onboarding Plane 在启用网格扩展时部署。VM 中的 Onboarding 代理将连接到此组件，以便将外部网格工作负载（例如在 VM 中运行的工作负载）纳入网格。 Onboarding Repository 在启用网格扩展时部署。一个 HTTP 服务器，提供登记代理和 Istio Sidecar 的 DEB 和 RPM 包。 cert-manager 使用 INTERNAL cert-manager 时创建。cert-manager 为内部 TSB 组件提供证书，例如 Webhook 证书等的目的。 数据平面 修订的控制平面 TSB 1.5 引入了修订的控制平面。当你使用修订的控制平面时，不再需要 Data Plane Operator 来管理 Istio 网关。要了解有关修订的控制平面的更多信息，请转到 Istio 隔离边界 。 在数据平面中有两个运行的 Operator，用于管理网关部署的生命周期：\nOperator 名称 描述 TSB Data Plane Operator 用于管理 TSB 数据平面组件生命周期的 Operator。 Istio Operator 根据 数据平面安装 API 中指定的 Gateway CR 来管理 Istio 网关的生命周期的 Operator。 ","relpermalink":"/book/tsb/setup/components/","summary":"TSB 组件指南","title":"TSB 组件"},{"content":" tsb\nonboard\nistio.io\ninstall\niam\naudit\n","relpermalink":"/book/tsb/refs/","summary":"tsb\nonboard\nistio.io\ninstall\niam\naudit","title":"API"},{"content":"本文描述了在将 PostgreSQL 用作 TSB 数据存储时如何创建备份以及如何使用备份进行恢复。 建议每 24 小时创建一次 TSB 数据存储的备份，以便在发生损坏时可以轻松恢复所有信息。\n在开始之前，请确保：\n你已经安装和配置了 TSB 。 你已经安装并配置了 kubectl 以访问管理集群。 你对存储 TSB 数据的 PostgreSQL 系统具有完全访问权限。 创建 TSB 配置的备份 TSB 需要 PostgreSQL 11.1 或更高版本。我们将在示例中使用 11.1 版本。你可以通过运行以下命令创建数据库备份：\npg_dump tsb \u0026gt; tsb_backup.sql 注意 确保备份文件包含完整的信息。在文件的末尾，应该有如下的完成消息：\n-- -- PostgreSQL database dump complete -- 备份日志的大小可以通过删除审计日志来减小（请确保你有一个快照以符合你组织的合规性规则）。要进行审计日志截断，你可以使用以下命令（请根据要保留的日志的时间间隔进行调整，以下示例中为2day）：\nDELETE FROM audit_log WHERE time \u0026lt;= ROUND(EXTRACT(epoch FROM now() - INTERVAL \u0026#39;2day\u0026#39;)); 恢复备份 要恢复备份，建议将 tsb 和 iam 部署的副本数缩减为 0，因为这些部署将不断对数据库进行查询：\nkubectl scale deployment tsb iam -n tsb --replicas 0 注意 将 tsb 部署的副本数缩减为 0 仅会在还原进行中时中断在运行中的 TSB 安装中更改配置的能力，但不会干扰数据平面/正在运行的服务。 此时，你需要登录到你的 PostgreSQL 系统，并以特权用户的身份执行以下操作。 通常，数据库将具有少量活动连接。你可以通过运行以下查询来检查它们：\nSELECT * FROM pg_stat_activity WHERE datname = \u0026#39;tsb\u0026#39;; 接下来的步骤是使用以下查询终止这些连接：\nSELECT\tpg_terminate_backend (pid) FROM\tpg_stat_activity WHERE\tpg_stat_activity.datname = \u0026#39;tsb\u0026#39;; 然后立即删除 tsb 数据库：\nDROP DATABASE tsb; 此时，所有 TSB 配置都将被删除。现在，你需要重新创建 tsb 数据库：\nCREATE DATABASE tsb; 并将该数据库的所有权限授予名为 tsb 的用户：\nGRANT ALL PRIVILEGES ON DATABASE tsb TO tsb; 完成后，使用用户 tsb 登录并恢复之前创建的转储：\npsql tsb \u0026lt; tsb_backup.sql 现在，备份中的所有数据都已恢复，你可以将 tsb 和 iam 部署的副本数增加到 1：\nkubectl scale deployment tsb iam -n tsb --replicas 1 ","relpermalink":"/book/tsb/operations/postgresql/","summary":"备份和恢复 PostgreSQL。","title":"备份和恢复 PostgreSQL"},{"content":" CLI\nREST API\ngRPC API\nYAML API\nKubernetes API\n示例应用\n","relpermalink":"/book/tsb/reference/","summary":"CLI REST API gRPC API YAML API Kubernetes API 示例应用","title":"参考"},{"content":"本文档解释了将新的控制平面引入 TSB 时最常见的问题。\n连接性 部署 tsb-operator-control-plane 需要与管理平面 URL 具有连接性。通信是通过 tsb 命名空间中的 front-envoy 组件执行的，由 envoy 服务提供。\n请确保控制平面可以访问它，且没有被网络策略、安全组或任何防火墙阻止。\n故障排除 一旦你已经应用了必要的机密 ，安装了控制平面 Operator，并创建了控制平面 CR，如果存在一些配置错误，一些 Pod 可能无法启动。始终检查 tsb-operator-control-plane 的日志，因为它将提供有关可能出错的详细信息。\n服务帐号问题 如果未创建用于生成令牌的服务帐号，你将收到以下错误：\nerror\tcontrolplane\ttoken rotation failed, retrying in 15m0s: secret istio-system/cluster-service-account not found: Secret \u0026#34;cluster-service-account\u0026#34; not found [scope=\u0026#34;controlplane\u0026#34;] 或者也可能发生配置不正确的情况：\nerror\tcontrolplane\ttoken rotation failed, retrying in 15m0s: cluster has been configured with incorrect service account secret. ControlPlane CR has cluster name \u0026#34;demo\u0026#34;, but service account secret has \u0026#34;organizations/tetrate/clusters/not-demo\u0026#34; [scope=\u0026#34;controlplane\u0026#34;] 在此示例中，我们创建了名为 demo 的集群对象，但在 CP 中，我们正在为名为 not-demo 的集群生成服务帐号。要解决此问题，你需要在 values.yaml 文件中添加集群名称和服务帐号令牌以安装 CP。首先生成令牌：\ntctl install cluster-service-account --cluster demo \u0026gt; /tmp/demo.jwk 然后在 values.yaml 文件中配置集群名称和 JWK 文件：\nsecrets: tsb: ... clusterServiceAccount: clusterFQN: organizations/tetrate/clusters/demo JWK: | \u0026#39;{{ .Secrets.ClusterServiceAccount.JWK }}\u0026#39; 集群名称必须与控制平面 CR 中的 spec.managementPlane.clusterName 中添加的集群名称匹配。\n注意 记得重新启动 tsb-operator-control-plane Pod 以生成机密，一旦生成，重新启动控制平面 Pod。 控制平面证书问题 如果在管理平面中配置的 tsb-certs 证书不包含与控制平面 CR 中的 spec.managementPlane.host 配置的正确 URI SAN，或者 tsb 命名空间中的 tsb-certs 和 istio-system 命名空间中的 mp-cert 都不包含相同的 URI SAN，或者它们没有由相同的根/中间 CA 签名，你将收到以下错误：\nerror\tcontrolplane\ttoken rotation failed, retrying in 7.153870785s: generate tokens: rpc error: code = Unavailable desc = connection error: desc = \u0026#34;transport: authentication handshake failed: tls: failed to verify certificate: x509: certificate is valid for demo.tsb.tetrate.io, not tsb.tetrate.io\u0026#34; [scope=\u0026#34;controlplane\u0026#34;] 你可以通过在控制平面 values.yaml 文件中配置值 secrets.tsb.cacert 来更新 mp-cert，或者通过在管理平面 values.yaml 文件中配置值 secrets.tsb.cert 和 secrets.tsb.key 来更新 tsb-certs。\n如果在 tsb-certs 中提供的证书由公共 CA（如 Digicert 或 Let’s Encrypt）签名，你可以让控制平面 CR 的默认值保持不变，但如果此证书由内部 CA 签名或者是自签名的，你可能会收到以下错误：\nerror\tcontrolplane\ttoken rotation failed, retrying in 1.661766738s: generate tokens: rpc error: code = Unavailable desc = connection error: desc = \u0026#34;transport: authentication handshake failed: x509: certificate signed by unknown authority\u0026#34; [scope=\u0026#34;controlplane\u0026#34;] 如果是这种情况，你需要修改控制平面 CR 以将 spec.managementPlane.selfSigned 设置为 true。\n注意 记得重新启动 tsb-operator-control-plane Pod 以生成机密，一旦生成，重新启动控制平面 Pod。 XCP 连接问题 如果新引入的集群没有报告集群状态或新配置未在集群中创建，请检查 istio-system 命名空间中的 edge Pod 日志，即使 Pod 正在运行，也可能存在一些问题。例如：\nwarn\tstream\terror getting stream. retrying in 21.72809085s: rpc error: code = Unavailable desc = connection error: desc = \u0026#34;transport: authentication handshake failed: tls: failed to verify certificate: x509: certificate is valid for xcp.tetrate.io, not tsb.tetrate.io\u0026#34;\tname=configs-4d116fd6 在这种情况下，tsb 命名空间中的 xcp-central-cert 配置为 xcp.tetrate.io，但在控制平面 CR 中配置的主机是 tsb.tetrate.io。要更新证书，你需要根据这个 更新管理平面 values.yaml。\n如果 edge 无法启动，你可以描述 Pod 以获取更多信息。有时候无法启动是因为：\nWarning FailedMount 7m15s (x7 over 7m47s) kubelet MountVolume.SetUp failed for volume \u0026#34;xcp-central-auth-jwt\u0026#34; : secret \u0026#34;xcp-edge-central-auth-token\u0026#34; not found Warning FailedMount 5m44s kubelet Unable to attach or mount volumes: unmounted volumes=[xcp-central-auth-ca], unattached volumes=[config-map-volume xcp-central-auth-jwt xcp-central-auth-ca xcp-edge-webhook-ca kube-api-access-hxk8l webhook-certs]: timed out waiting for the condition Warning FailedMount 3m26s kubelet Unable to attach or mount volumes: unmounted volumes=[xcp-central-auth-ca], unattached volumes=[xcp-edge-webhook-ca kube-api-access-hxk8l webhook-certs config-map-volume xcp-central-auth-jwt xcp-central-auth-ca]: timed out waiting for the condition Warning FailedMount 95s (x11 over 7m47s) kubelet MountVolume.SetUp failed for volume \u0026#34;xcp-central-auth-ca\u0026#34; : secret \u0026#34;xcp-central-ca-bundle\u0026#34; not found Warning FailedMount 69s kubelet Unable to attach or mount volumes: unmounted volumes=[xcp-central-auth-ca], unattached volumes=[kube-api-access-hxk8l webhook-certs config-map-volume xcp-central-auth-jwt xcp-central-auth-ca xcp-edge-webhook-ca]: timed out waiting for the condition 此错误是因为 istio-system 命名空间中的秘密 xcp-central-ca-bundle 不存在。此秘密必须包含相同的 URI SAN，并且必须由 tsb 命名空间中的 xcp-central-cert 签名，并且必须由相同的根/中间 CA 签名。要配置此秘密，你需要更新控制平面 values.yaml 文件中的值 secrets.xcp.rootca。\n注意 记得重新启动 tsb-operator-control-plane Pod 以生成机密，一旦生成，重新启动 edge Pod。 ","relpermalink":"/book/tsb/troubleshooting/cluster-onboarding/","summary":"如何排除控制平面载入问题。","title":"集群载入故障排除"},{"content":"在此场景中，你将学习如何使用服务路由在演示应用程序中的不同版本的评论服务之间转移流量。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户、工作区、配置组、权限、入口网关，并检查服务拓扑和指标。 仅提供 v1 服务 使用用户界面 在左侧面板的“租户”下，选择“工作区”。 在 bookinfo-ws 工作区卡上，单击“流量组”。 单击你之前创建的 bookinfo-traffic 流量组。 选择流量设置选项卡。 在流量设置下，单击服务路由。 单击“添加新…”以使用默认名称 default-serviceroute 创建新的服务路由。 将其重命名为 bookinfo-traffic-reviews 。 将服务设置为 bookinfo/reviews.bookinfo.svc.cluster.local 。 展开 bookinfo-traffic-reviews。 展开子集。 单击“添加新子集…”以创建一个名为 subset-0 的新子集。 点击 subset-0 ： 将名称设置为 v1 。 将权重设置为 100 。 单击添加标签，并将标签设置为 version ，将值设置为 v1 。 单击保存更改。 使用 tctl 创建以下 reviews.yaml 文件：\napiVersion: traffic.tsb.tetrate.io/v2 kind: ServiceRoute Metadata: organization: tetrate name: bookinfo-traffic-reviews group: bookinfo-traffic workspace: bookinfo-ws tenant: tetrate spec: service: bookinfo/reviews.bookinfo.svc.cluster.local subsets: - name: v1 labels: version: v1 使用 tctl 应用配置：\ntctl apply -f reviews.yaml 验证结果 打开 https://bookinfo.tetrate.com/productpage 并刷新页面几次。你将看到仅显示评论 v1（无星级）。\n在 v1 和 v2 之间拆分流量 使用用户界面 选择服务路由的 v1 子集。 输入 50 作为重量。 单击 v1 子集下方的添加新子集…以创建一个名为 subset-1 的新子集。 点击 subset-1 ： 将名称设置为 v2 。 输入 50 作为重量。 单击添加标签，并将标签设置为 version ，将值设置为 v2 。 单击保存更改。 使用 tctl 更新 reviews.yaml 文件以在 v1 和 v2 之间均匀分配流量：\napiVersion: traffic.tsb.tetrate.io/v2 kind: ServiceRoute Metadata: organization: tetrate name: bookinfo-traffic-reviews group: bookinfo-traffic workspace: bookinfo-ws tenant: tetrate spec: service: bookinfo/reviews.bookinfo.svc.cluster.local subsets: - name: v1 labels: version: v1 weight: 50 - name: v2 labels: version: v2 weight: 50 使用 tctl 应用更新的配置：\ntctl apply -f reviews.yaml 验证结果 再次访问 https://bookinfo.tetrate.com/productpage 并刷新页面多次。你将看到评论在 v1（无星级）和 v2（黑色星级）版本之间切换。\n仅提供 v2 服务 使用用户界面 选择服务路由的 v1 子集。 单击“删除 v1”将其删除。 选择服务路由的 v2 子集。 将权重设置为 100 。 单击保存更改。 使用 tctl 更新 reviews.yaml 文件以将 100% 流量路由到 v2 版本：\napiVersion: traffic.tsb.tetrate.io/v2 kind: ServiceRoute Metadata: organization: tetrate name: bookinfo-traffic-reviews group: bookinfo-traffic workspace: bookinfo-ws tenant: tetrate spec: service: bookinfo/reviews.bookinfo.svc.cluster.local subsets: - name: v2 labels: version: v2 使用 tctl 应用更新的配置：\ntctl apply -f reviews.yaml 验证结果 再次访问 https://bookinfo.tetrate.com/productpage 并刷新页面多次。你将看到仅显示 v2 版本的评论（黑星评级）。\n通过执行这些步骤，你已使用 TSB 的服务路由功能成功管理演示应用程序中不同版本的评论服务之间的流量转移。\n","relpermalink":"/book/tsb/quickstart/traffic-shifting/","summary":"在此场景中，你将学习如何使用服务路由在演示应用程序中的不同版本的评论服务之间转移流量。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户、工作区、配","title":"流量转移"},{"content":"在这个操作指南中，你将使用 Keycloak 作为身份提供者，在 Ingress Gateway 中添加用户身份验证和授权。\n在开始之前，请确保你已经：\n安装了 TSB 管理平面 载入了一个 集群 安装了启用了 HTTPS 的 Keycloak 注意 本示例将使用经过 GKE 测试的 httpbin 应用程序的演示。如果你打算用于生产环境，请确保在相关字段中更新应用程序信息以适应你的情况。 在本指南中，你将：\n为演示的 httpbin 应用程序的 Ingress Gateway 添加身份验证和授权。 定义两个角色和两个用户：一个 admin 用户（称为 Jack），可以执行所有操作，以及一个 normal 用户（Sally），只能执行 GET /status 操作。 配置你的 Ingress Gateway，允许 admin 角色的用户访问所有内容，只允许 normal 角色的用户访问 GET /status。 什么是 OpenID 提供者？ OpenID 提供者是一个 OAuth 2.0 授权服务器，提供身份验证作为一项服务。它确保终端用户已经进行了身份验证，并提供了关于终端用户和身份验证事件的 claims 给客户端应用程序。在本示例中，你将使用 Keycloak 作为 OpenID 提供者。你可以使用其他 OpenID 提供者（如 Auth0 或 Okta）采用类似的步骤。\n注意 在本操作指南中，我们将使用 https://keycloak.example.com 作为 Keycloak 的 URL。你应该将其更改为你自己的 Keycloak URL。 配置 Keycloak 作为 OpenID 提供者 登录到 Keycloak 管理界面。\n注意 如果你已经创建了 Realm、Roles 和 Users，请直接转到 Client 部分。 Realm 首先创建 Realm。如果这是你第一次登录 Keycloak，你将拥有一个默认的主 Realm。该 Realm 用于管理对 Keycloak 界面的访问，不应该用于配置你的 OpenID 提供者。因此，你需要创建一个新的 Realm。\n单击 Add Realm 按钮。 设置 Realm 名称，本示例中为 tetrate。 单击 Create。 Role 在创建的 Realm 中，添加两个新角色：admin 和 normal。\n在左侧菜单中点击 Roles。 选择 Add Role 按钮。 将名称设置为 admin。 单击 Save。 再次按上述步骤添加一个名称为 normal 的角色。 Users 添加两个用户——Jack 和 Sally——并将它们映射到其新的角色：\n在左侧菜单中点击 Users。 选择 Add user 按钮。 填写 Jack 的详细信息。 单击 Save。 选择 Credentials 选项卡。 为 Jack 设置密码。 单击 Role Mappings 选项卡。 添加 admin 角色。 添加另一个用户名为 Sally 的用户，然后按照上述步骤，在 Role Mappings 选项卡中添加一个 normal 角色。 Client 客户端是可以请求 Keycloak 对用户进行身份验证的实体。在这种情况下，Keycloak 将提供单点登录，用户将登录到该单点登录，获取一个 JWT 令牌，然后使用该令牌进行对 TSB 管理的 Ingress Gateway 的身份验证。\n添加一个新的客户端。\n在左侧菜单中点击 Clients。 选择客户端 Create 按钮。 客户端 ID: tetrateapp。 客户端 Protocol: openid-connect。 根 URL: https://www.keycloak.org/app/ 是 Keycloak 网站上可用的一个 SPA 测试应用程序。 单击 Save。 接下来，在客户端中进行一些更新。\n首先，增加令牌寿命，以确保令牌不会在测试过程中过快过期。\n在设置选项卡中，滚动到底部，选择 Advanced Settings。 将 Access Token Lifespan 设置为 2 小时。 单击 Save。 然后，你需要添加两个映射器，以便 Keycloak 可以生成一个带有你在 TSB Ingress Gateway 中使用的数据的 JWT。\n你需要添加两种类型的映射器：一个 Audience 映射器和一个 Role 映射器：\n映射器 目的 Audience 映射器 将客户端 ID 添加到 JWT 令牌中的 audience 字段。这可以确保你可以将 JWT 令牌限制为特定客户端。 Role 映射器 将 JWT 令牌中的角色从嵌套结构更改为数组。当前，TSB 无法处理 JWT 申明中的嵌套字段。这在 Istio 1.8 中已修复，并将在未来版本中添加到 TSB 中。 选择 Mappers 选项卡。\n单击 Create 按钮，然后输入以下信息：\n名称：Audience 映射器。 映射器类型：Audience。 包含客户端受众：tetrateapp。 单击 Save。\n返回到 Mappers 选项卡。\n单击 Create 按钮，然后输入以下信息：\n名称：Role 映射器。 映射器类型：User Realm Role。 令牌声明名称：roles。 申明 JSON 类型：String。 不要修改 multi-valued，添加到 ID token，添加到访问令牌和添加用户信息到 ‘on’。 单击 Save。\n测试用户登录 现在，你已经配置了客户端，请使用 Keycloak 示例应用程序或之前解释的 curl 来获取并检查你的 JWT 令牌。\n前往 https://www.keycloak.org/app/ ，并输入以下信息： Keycloak URL: https://keycloak.example.com/auth Realm: tetrate Client: tetrateapp 单击 Save。 要检查 JWT 令牌，请执行以下操作：\n打开浏览器控制台。 单击 Network 选项卡。 使用 Jack 的凭证登录。 查找一个请求 token。在响应中，获取 access_token。 将你的令牌粘贴到 https://jwt.io/ 。 你将从 JWT 令牌中看到以下信息。你只需要注意三个字段，这些字段将在你的 Ingress Gateway 配置中使用：iss、aud 和 roles。\n{ \u0026#34;exp\u0026#34;: 1606908135, \u0026#34;iat\u0026#34;: 1606900935, \u0026#34;auth_time\u0026#34;: 1606900917, \u0026#34;jti\u0026#34;: \u0026#34;c1e45982-38c6-4d0d-b201-9d823eed4c0a\u0026#34;, \u0026#34;iss\u0026#34;: \u0026#34;https://keycloak.example.com/auth/realms/tetrate\u0026#34;, \u0026#34;aud\u0026#34;: [ \u0026#34;tetrateapp\u0026#34;, \u0026#34;account\u0026#34; ], \u0026#34;sub\u0026#34;: \u0026#34;06765a3f-b09f-4c46-a0f9-0285c3924409\u0026#34;, \u0026#34;typ\u0026#34;: \u0026#34;Bearer\u0026#34;, \u0026#34;azp\u0026#34;: \u0026#34;tetrateapp\u0026#34;, \u0026#34;nonce\u0026#34;: \u0026#34;f96cd9eb-af9e-4e41-8591-ffc01fd94dd0\u0026#34;, ... \u0026#34;scope\u0026#34;: \u0026#34;openid email profile\u0026#34;, \u0026#34;email_verified\u0026#34;: true, \u0026#34;roles\u0026#34;: [ \u0026#34;offline_access\u0026#34;, \u0026#34;admin\u0026#34;, \u0026#34;uma_authorization\u0026#34; ], \u0026#34;name\u0026#34;: \u0026#34;Jack White\u0026#34;, \u0026#34;preferred_username\u0026#34;: \u0026#34;jack\u0026#34;, \u0026#34;given_name\u0026#34;: \u0026#34;Jack\u0026#34;, \u0026#34;family_name\u0026#34;: \u0026#34;White\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;jack@tetrate.com\u0026#34; } 你还可以使用 OAuth 的 Resource Owner Password Flow 获取用户 JWT 令牌。当你创建一个 Keycloak 客户端时，默认情况下会启用此流程。\ncurl --request POST \\ --url https://keycloak.example.com/auth/realms/tetrate/protocol/openid-connect/token \\ --header \u0026#39;Content-Type: application/x-www-form-urlencoded\u0026#39; \\ --data client_id=tetrateapp \\ --data password=\u0026lt;user_password\u0026gt; \\ --data username=jack \\ --data grant_type=password \\ --data \u0026#39;scope=openid email profile\u0026#39; 使用 Ingress Gateway 部署 Httpbin 应用程序 与 Ingress Gateway 一起部署 httpbin 应用程序。\n创建以下 httpbin.yaml 。\n使用 kubectl 命令将 httpbin 部署到你的接入集群中：\nkubectl create namespace httpbin kubectl label namespace httpbin istio-injection=enabled --overwrite=true kubectl apply -n httpbin -f httpbin.yaml 确认所有服务和 Pod 都在运行：\nkubectl get pods -n httpbin 创建 Ingress Gateway ingress.yaml 。\n应用更改：\nkubectl apply -n httpbin -f ingress.yaml 确保所有服务和 Pod 都在运行。请等待，直到 Ingress Gateway 分配了其外部 IP。\nkubectl get pods -n httpbin kubectl get svc -n httpbin 获取 Ingress Gateway 的 IP：\nexport GATEWAY_HTTPBIN_IP=$(kubectl -n httpbin get service tsb-gateway-httpbin -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) 配置工作区和 Ingress Gateway 现在，你的应用程序正在运行，你需要创建工作区并配置 Ingress Gateway。为此，你需要 TSB 运行和 tctl。\n注意 如果你运行 TSB 演示安装，你将拥有一个名为 tetrate 的默认租户和一个名为 demo 的默认集群，我们在以下配置 YAML 中使用了它们。如果你在生产环境中使用，请将其更改为你自己的租户和集群。 工作区 创建一个 workspace.yaml 。\n应用更改：\ntctl apply -f workspace.yaml 确保工作区已创建：\ntctl get workspaces httpbin-ws 预期输出：\nNAME httpbin-ws 接下来，创建一个 Ingress Gateway，允许从网格外部访问 httpbin。你将从一个没有身份验证的不安全 Gateway 开始。\nIngressGateway 创建以下 gateway-no-auth.yaml 。在此示例中，已经为 HTTPS 连接设置了 httpbin-certs。\n使用 tctl 应用：\ntctl apply -f gateway-no-auth.yaml 验证你在 httpbin 命名空间中创建了一个网关：\nkubectl get gateway -n httpbin httpbin-gw-ingress -o yaml 示例输出：\napiVersion: …","relpermalink":"/book/tsb/howto/gateway/end-user-auth-keycloak/","summary":"在 Ingress Gateway 中使用 Keycloak 作为身份提供者进行终端用户身份验证和授权。","title":"使用 Keycloak 进行终端用户身份验证"},{"content":"TSB 服务账号可以在平台内部用于管理集群载入 tctl install cluster-service-account 和 GitOps 功能，也可以在外部用于第三方系统执行各种 TSB 功能的配置，利用 TSB API 接口。本文将重点介绍如何创建和使用 TSB 服务账号，以及如何利用 tctl 实用工具作为处理程序。\n使用 tctl 实用工具处理 TSB 服务账号 对于服务账号，你所需的大多数交互已经在 tctl experimental service-account 命令中可用：\n$ tctl x sa -h Commands to manage TSB service accounts Usage: tctl experimental service-account [command] Aliases: service-account, sa Available Commands: get Get one or multiple service accounts create Creates a new service account delete Deletes a service account gen-key Generate a new key pair for the given service account revoke-key Revoke a given key pair for the given service account token Generate a new token that can be used to authenticate to TSB 使用 tctl experimental service-account create 创建一个 TSB 服务账号。当创建服务账号时，会返回私钥，但 TSB 不会存储它们。客户端需要自行安全存储。要了解更多信息，请参阅 YAML API 参考指南 ：\n$ tctl experimental service-account create pipeline-sa1 \u0026gt; pipeline-sa1-jwk-private-key.jwk $ cat pipeline-sa1-jwk-private-key.jwk { \u0026#34;alg\u0026#34;: \u0026#34;RS256\u0026#34;, \u0026#34;d\u0026#34;: \u0026#34;DXxlZZcNodMTZv0XIYXglgNilwyL4gxnmu6e1zZetmtbm0oHKUx4CTlnWt_nBAinlxTzirEXClBNoDPqCh27Jg-WwbBeW01l2RPoSO7g4eM9Sz1r2KCy5o7NgptAq-_uZLy609gWDPgk8EjFT1QWMtGVXICi5StR9D0RbKazFVpgekIBPAlKoMDqwMUVM5nldIXyI6iwy4C19ZAdf0cW2HHw8rKBEMQ-bqXuD7RVkMWp18wPrnxbMpR8Xw1n4F_Wj7DqAepYezk8Vp1-uuUEnIP3rtMYbFVL1wn-nupQSAyIQIQsqvwSsGU-RD00YuPQ6hbeRTb201Ev-DvFYA1XUQ\u0026#34;, \u0026#34;dp\u0026#34;: \u0026#34;lZdU20cP-G8q9dCEbFAYt15pVfzAfjy82cRlfGLjcYJFiTRyc-J8zj4VjDJSDg5CQfufQ_q_0duQi40HQH-8ihK1mPe-OZlvDc7syxbVlWIiwD4w1if-YuNWEvfyWOfa6nHsZY3utW5_SL4nvw2E-9iv_HJIJ3MkLEhZDysGvZE\u0026#34;, \u0026#34;dq\u0026#34;: \u0026#34;v--gNJHrSbUMgZEuy3jfjmrgHjBM3ee6141zL3KmfeWrEK6OW8TYrVV0HBzk7Whj7ehxQmLGHVH-MykyrlKGggGtnQ1OgUpTPBhKE8j5QaXmAuO7pY1oDcOWQmqg8qu1X0X61-LmMQ42he8gGSBvcL3jWxpDSGuGeYwPJeJ9FZc\u0026#34;, \u0026#34;e\u0026#34;: \u0026#34;AQAB\u0026#34;, \u0026#34;kid\u0026#34;: \u0026#34;zuAiwPFQu2eI3GAGddaS1UHG08A01BA4XStF2C45uiA\u0026#34;, \u0026#34;kty\u0026#34;: \u0026#34;RSA\u0026#34;, \u0026#34;n\u0026#34;: \u0026#34;s5ENuvPJ9C2gMsnqFUXosXYY4k8AcnCjfUFQgUJc1FBpM15EnrgwkArZNsgHscH7ngnqIvwIf7SvM10CSkKj7dWZ6oabmdY-IFaeKIZ96EoFicNpRgkhJQREunLNtwHjvZZ_j86Vbnt4YGn6Y09y42HlEAT2NjUBiZI9C_gUmWl7smW-gZBGa4U6PsAOpi0H6Ct5dKpYJUO0qj1JLqC739nG2Exr4QEQGkFo-UaBBTTq1miHXfs1ptytYqfd64xTg0PIX0-9CfjtKrXS3hWEAWHHcChl9eHp89RU7a3bjWHbVJJVjYwcht6kFR_GX6oScGGnM4vQSR2ifh034vSA3w\u0026#34;, \u0026#34;p\u0026#34;: \u0026#34;y4ynCbHHJW984_nC4UKCSF3kFjqAWG4E7K4_qJ7b5sXN7aQsWgBi6Jt6c9Paf4X3HUPDs9rbQ8ab4PJNP4r3JNc90wpvSR0b_w3E_bOtfQhbLbG5T17eO2laEpJCYWK71EVuZ2ykvuf6rkgTi4T27c9KdgJHMKQGNH7TwQFJKUU\u0026#34;, \u0026#34;q\u0026#34;: \u0026#34;4dZZugK6vTlt_i2ySEuvRTAErLAVK7UWIuLQN9ee O8viX_vgoNe1L1rEN1Lb-OjdV4j5hyGMqkJ3kbCm0awDmxaR4nXVZ-GKC_mvilpfuyoYK4rm9iod_ZSuLytqr9LPnvtalaYeToNT9U7KqbzVsFY0nKTF6_ujRfqD8g282dM\u0026#34;, \u0026#34;qi\u0026#34;: \u0026#34;anAZOAEZNUHf9HjqVeZiMExSZf7_OhHDceyKQ3KKI7CZSHaSj-aRtXqfAzArwpi3jDkiVQK79pt5zYKg0K47Z-X2PJ_W1tqqzAQX3Fqkdvs1c3L3Fy3w_C59N_B_QiA5e-y9J5qM1Qk12jnhlCn0DnlolwadfrkciUIS4ZdHMcs\u0026#34; } 当通过 TSB 服务账号创建步骤获取 JWK 私钥时，使用 tctl experimental service-account token --key-path 生成会话身份验证的访问令牌，并指定其持续时间：\n$ tctl experimental service-account token pipeline-sa1 --key-path pipeline-sa1-jwk-private-key.jwk --expiration 1h30m0s eyJhbGciOiJSUzI1NiIsImtpZCI6Inp1QWl3UEZRdTJlSTNHQUdkZGFTMVVIRzA4QTAxQkE0WFN0FjDdpUEiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE2NTcwNTIyNTksImlhdCI6MTY1NzA1MDQ1OSwic3ViIjoibXktc2EjenVBaXdQRlF1MmVJM0dBR2RkYVM1VUhHMDhBMDFCQTRYU3RGMkM0NXVpQSIsInRzYi50ZXRyYXRlLmlvL3VzZSI6InRjdGwifQ.PRN5noVwB5RT0kFL75XjBe8pO3l90QvqpeUrR-Cw_Wt3-I4jTEWOVZXwkg6BJp0sL3cdq4wBPOCjQ8FXKrd527bIujh8f0E0Cj0obhbbSGUmAFwJO2UrvovjfXr1Ra35KHsFY6HCnTjKRxFVZ_czdYAc4s3YbOYRhiz74v1O6U9nX5jgTLl_vg9dxDUxiYYeUn1gR9_Jf0APkM48JSiZa4Bz0Ly6oGKm_GkUY003xPl4PSMFhR-4i1rYrcFH2YYP_6uUieToTrCSNchPk8S6Mh3rnkMiKTazrUnAuO5Anc3C6UlbDw9-ax18dvyKKi47wdRcjeDNPxjCSX27Qe-ryA 然后将该令牌配置到所需的 tctl 用户配置文件 中。你也可以一次完成所有操作：\ntctl config users set pipeline-sa1 --token $(tctl x sa token pipeline-sa1 --key-path pipeline-sa1-jwk-private-key.jwk --expiration 1h30m0s) 有关如何使用 tctl 连接到 TSB 的详细信息，请参阅 使用 tctl 连接到 TSB 。\n","relpermalink":"/book/tsb/howto/service-accounts/","summary":"本文将重点介绍如何创建和使用 TSB 服务账号，以及如何利用 `tctl` 实用工具作为处理程序。","title":"使用 TSB 服务账号"},{"content":"Tetrate 概念 组织（Organization） ‘组织’ 是对象层次结构的根。组织包含了租户、用户、团队、集群和工作区的层次结构。组织还是定义了 TSB 全局设置的位置。\n配置组织 在 TSB 中，你可以定义根组织的名称和属性，以反映你的企业层次结构。\n在 TSE 中，组织名称是硬编码为 tse，不可编辑。\n组织用户（Organization Users） TSB 和 TSE 中的组织与身份提供者（IdP）关联。用户和团队代表组织结构，定期从你提供的 IdP 同步，以便进行访问策略配置。\n用户、团队和角色 TSB 允许你定义多个团队，并关联具有精细化角色的团队，然后将用户映射到团队成员。\nTSE 使用默认的 tse-admin 用户，以及单个团队和角色。其他用户可以从 IdP 同步，它们都继承了单个团队成员和角色。\n租户（Tenant） 租户是服务桥对象层次结构中组织内的一个自包含实体。租户可以是业务单元、组织单元或与公司结构匹配的任何逻辑分组。\nTSE 简化 而 TSB 支持多个租户和丰富的组织层次结构，TSE 提供了一个单一的 tse 租户和扁平的层次结构。 工作区（Workspace） 工作区是由平台所有者定义的一组一个或多个命名空间。这些命名空间可以跨多个 K8s 集群。\n用户可以为工作区分配配置或策略，然后 TSB/TSE 将这些转化为 Istio 每个命名空间的配置。工作区提供了一个便捷的高级抽象，与组织的应用程序相一致，通常跨越多个命名空间和/或集群。\n组（Group） 在工作区的上下文中，组是相关 Istio 配置类型的集合。例如，“安全组”包含与工作区的访问控制、身份验证和授权相关的所有配置。流量组和网关组类似地包含与流量管理和入口网关相关的配置。\n工作区中的组通常是可以独立管理的最小自包含配置集合。\n安全组（Security Group）：安全组包含工作区的所有与安全相关的 Istio 配置。这包括 mTLS 和身份验证策略。 流量组（Traffic Group）：流量组包含工作区的所有与流量管理相关的 Istio 配置。这包括速率限制、金丝雀和流量分割配置。 网关组（Gateway Group）：网关组定义了工作区的入口网关策略。这些策略适用于管理流向该工作区服务的入口网关。 直接模式和桥接模式 工作区和组配置为直接模式或桥接模式之一。\n在 直接模式 中，用户使用相应类型的 Istio 配置配置工作区和组。例如，用户可以将 Istio 安全配置分配给工作区和其安全组。然后验证此配置并将其部署到底层网格实例。 在 桥接模式 中，用户仅使用 Tetrate 的高级抽象配置工作区和组。这些抽象设计得更简单，更符合常见用例，例如为工作区定义入口。这些抽象会转化为 Istio 配置并部署到底层网格实例。 直接模式和桥接模式以细粒度应用。例如，对于给定的工作区，安全和流量组可以在直接模式下运行，而网关组可以在桥接模式下运行。这是因为工作区的配置完全由三个组覆盖，并且这些组不重叠或干扰彼此。\n服务与应用 服务（Service） 服务是一个可识别且具有独立身份验证的网络可寻址目标。此目标由一组服务实例提供，这些实例是承载服务的个体工作负载。\n在 Kubernetes 中，服务对应于服务资源类型。在 Kubernetes 之外，服务可以标识其他可寻址的工作负载，例如基于 VM 的工作负载或第三方 API。\n服务具有以下属性：\n身份（Identity）：服务的多个实例可以分布在不同的节点上，以提供可伸缩的集体服务。所有实例都使用相同的服务身份进行验证和验证。服务身份是 SPIFFE SVID 。 端点（Endpoint）：服务端点是一个完全限定的域名（FQDN）、URI 路径或具有一组方法的类。 端点方法（Endpoint Method）：托管端点的服务可以是不同协议的。这些定义了特定于服务提供的协议的动 词集。例如，每个端点的 HTTP 动词可以是用来表达 CRUD 操作的方法，对于 gRPC，它将是 protobuf 类的一部分的方法。\n类型（Kind）：服务类型是服务的类型或类。类型声明为元组 {Protocol, Internal|External}，例如 {HTTP, Internal} 或 {SQL, Internal}，或 {grpc, External}。‘外部’ 表示通过网关跳跃使用该服务。 可见性（Visibility）：可见性采用私有、共享或公共的值，并控制通过服务注册表呈现服务的方式 服务身份验证 使用 mTLS 和 SPIFFE 标准。 服务授权 是基于交易中双方的身份验证和定义允许操作的安全策略的许可决策。授权在中心定义，并在每个事务中本地评估。 服务访问控制 是基于权限（允许操作）和其他策略（如速率限制或配额）的访问决策。\n工作负载（Workload） 工作负载是个体可寻址的服务组件。\n在 Kubernetes 中，工作负载对应于个体 Pod。在其他环境中，“工作负载”可能指的是单个 VM 或在该 VM 中运行的应用程序。\n服务注册表（Service Registry） 服务注册表是可由机器读取的正在运行的可被其他服务或用户调用的服务列表。这些服务是可发现、可寻址、可路由和可控制的。\nTetrate 管理的网格动态使用服务注册表来发现服务实例，确定如何路由流量，并了解其他访问控制参数。\n应用程序（Application） 应用程序是通过一个或多个 API 可访问的服务的逻辑分组。\n应用程序资源是 TSE 中的便捷方式，用于定义一组服务以及它们公开的 API，可以供最终用户或其他服务使用。应用程序的内容，就像一个 ‘对象’，是不透明的。\nAPI（API） API 是为应用程序提供功能的端点。\nAPI 是一种类似于 Ingress 资源的资源。API 资源包装了一个 OAS 定义 ，如果需要，还包含了 Tetrate 特定的注释。API 资源还包含了包含 API 的应用程序资源（application）、公开 API 的 Ingress 网关，以及实现 API 的 Kubernetes 服务。\nAPI 资源为应用程序所有者提供了一种便捷的方式来公开由应用程序实现的 API。\n架构组件 更多信息可在 TSB 架构页面 或 TSE 架构页面 中找到。\n管理平面（Management Plane） 管理平面是在 Tetrate 管理的环境中访问所有内容的主要访问点。它配置网络、安全性和可观察性，并公开 UI、CLI 和 API 以进行更新和更改。它在多个控制平面服务网格中提供集中控制。\n中央控制平面（Central Control Plane） 中央控制平面是管理平面的一部分。其角色是接受管理平面提供的“配置意图”和由每个受管理网格提供的“运行时状态”，然后将这些协调成 Istio 配置。然后将此配置可靠地分发到每个远程网格。\n控制平面（Control Plane） 边缘控制平面部署在每个受管理的 Kubernetes 集群中。它负责在该集群中部署 Istio 实例，接收和应用来自中央控制平面的 Istio 配置，并收集和转发遥测和其他运行时数据到管理平面。\nIstio Istio 是领先的服务网格实现。边缘控制平面在每个已管理（已“登机”的）集群中部署 Istio 实例，并使用 Istio 作为集群内的本地控制点，以管理网络活动。\nIstio 通过主动配置集群内网格数据平面提供加密、安全、流量管理等功能。\n数据平面（Dataplane） 由 Envoy 驱动，网格数据平面拦截并控制集群内的网络流量，工作在第 7 层。它与处理 Ingress（进入集群）和 Egress（离开集群）事务的各种网关进行交互。数据平面实现为一组 Sidecar 代理。\nSidecar 代理（Sidecar Proxy） Sidecar 代理是 Envoy 的一个实例，部署在你的应用程序旁边。它透明地拦截进出应用程序的流量，使服务网格能够实现其流量管理、安全性和可观察性功能。这与负载均衡器或网关不同，后者是单独部署在应用程序之外的。\n网关 网关是位于网格边缘的代理，用于接收进入或离开的 HTTP/TCP 连接。网关通常使用 L7 代理（Envoy）实现，并执行安全性和流量管理功能（身份验证、TLS 终止、负载平衡等）。它们是策略执行的关键控制点。\n在部署中，可以找到几种类型的网关。\n入口网关（Ingress Gateway） 入口网关部署在工作负载环境的边界，例如一个网格。入口网关的目的是接收流向环境内服务的流量，应用安全性和流量管理策略，并将每个请求转发到目标服务。\n入口网关通常部署在每个 Kubernetes / Mesh 集群中，处理来自公共互联网等外部流量，服务于该单个集群内的服务。\n边缘网关（Edge Gateway） 边缘网关（有时称为“Tier-1 网关”）部署在客户运营基础设施的外部边界。边缘网关的目的是接收流向基础设施内服务的流量，应用策略，然后将每个请求转发到基础设施内的下一跳。\n边缘网关通常部署在多个工作负载环境前面，例如多个 EKS 集群。它负载平衡跨这些集群的流入流量，实现高可用性和负载分发。\n东西网关（EastWest Gateway） 东西网关部署在工作负载环境的边界，例如一个网格。与入口网关不同，东西网关的目的不是处理外部流量；相反，东西网关处理来自对等环境（例如其他网格）的流量。\n东西网关可用于连接内部网格网络，以便一个集群中的服务可以访问另一个集群中的服务，而不需要将这些服务中的任何一个暴露给公共访问。它在 Tetrate 的内部服务的高可用性解决方案中使用。\n出口网关（Egress Gateway） 出口网关部署在工作负载环境的边界，例如一个网格。它的目的是处理离开网格的流量，即来自希望使用外部（例如公共）服务和 API 的内部服务。\n出口网关在访问外部服务时提供了一个控制点。它可以用于应用访问控制策略，或者添加使用外部服务所需的身份验证令牌。\n过渡网关（Transit Gateway） 过渡网关位于非网格工作负载旁边，例如虚拟机。它用于将网格边界扩展到更靠近外部工作负载的地方。\nAPI 网关（API Gateway） API 网关是一种配置用于传送 API 流量的 Ingress 网关形式。API 网关通常使用 API 规范（例如 OpenAPI 规范）进行配置，而不是使用完全限定域名和路径，它们通常允许用户配置特定于 API 的策略，如速率限制、身份验证和 CORS 头管理。\n","relpermalink":"/book/tsb/concepts/glossary/","summary":"Tetrate 概念 组织（Organization） ‘组织’ 是对象层次结构的根。组织包含了租户、用户、团队、集群和工作区的层次结构。组织还是定义了 TSB 全局设置的位置。 配置组织 在 TSB 中，你可以定义","title":"Tetrate 术语表与定义"},{"content":"我删除/损坏了我的存储库，无法删除我的应用程序。 如果 Argo CD 无法生成清单，则无法删除应用程序。你需要：\n恢复/修复你的存储库。 使用cascade=false删除应用程序，然后手动删除资源。 为什么我的应用程序在成功同步后仍然处于 OutOfSync 状态？ 请查看 差异比较 文档，了解资源可能处于 OutOfSync 状态的原因，以及配置 Argo CD 忽略字段的方法 当存在不同之处时。\n为什么我的应用程序一直处于“Progressing”状态？ Argo CD 为几种标准 Kubernetes 类型提供健康状态。 Ingress、StatefulSet 和 SealedSecret 类型存在已知问题，可能会导致健康检查返回 Progressing 状态而不是 Healthy。\nIngress 如果 status.loadBalancer.ingress 列表是非空的，并且至少有一个值为 hostname 或 IP，则被视为健康状态。一些 Ingress 控制器（contour 、traefik ）不会更新 status.loadBalancer.ingress 字段，这会导致 Ingress 永久停留在 Progressing 状态。 StatefulSet 如果 status.updatedReplicas 字段的值与 spec.replicas 字段匹配，则被视为健康状态。由于 Kubernetes bug kubernetes/kubernetes#68573 ，status.updatedReplicas 没有填充。因此，除非你运行包括修复程序 kubernetes/kubernetes#67570 的 Kubernetes 版本，否则 StatefulSet 可能会保持在 Progressing 状态。 你的 StatefulSet 或 DaemonSet 正在使用 OnDelete 而不是 RollingUpdate 策略。请参见 #1881 。 对于 SealedSecret，请参阅 为什么 SealedSecret 类型的资源处于 Progressing 状态？ 作为解决方法，Argo CD 允许提供 健康检查 自定义，覆盖默认行为。\n我忘记了管理员密码，如何重置密码？ 对于 Argo CD v1.8 及更早版本，初始密码设置为服务器 pod 的名称，如 入门指南 中所述。对于 Argo CD v1.9 及更高版本，初始密码可从名为 argocd-initial-admin-secret 的 secret 中获取。\n要更改密码，请编辑 argocd-secret secret，并使用新的 bcrypt 哈希更新 admin.password 字段。\n生成 bcrypt 哈希 使用以下命令生成 admin.password 的 bcrypt 哈希：\nargocd account bcrypt --password \u0026lt;YOUR-PASSWORD-HERE\u0026gt; 要应用新的密码哈希，请使用以下命令（用你自己的哈希替换哈希）：\n# bcrypt(password)=$2a$10$rRyBsGSHK6.uc8fntPwVIuLVHgsAhAX7TcdrqW/RADU0uh7CaChLa kubectl -n argocd patch secret argocd-secret \\\\\\\\ -p \u0026#39;{\u0026#34;stringData\u0026#34;: { \u0026#34;admin.password\u0026#34;: \u0026#34;$2a$10$rRyBsGSHK6.uc8fntPwVIuLVHgsAhAX7TcdrqW/RADU0uh7CaChLa\u0026#34;, 另一个选项是删除 admin.password 和 admin.passwordMtime 两个键，然后重新启动 argocd-server。这将根据 入门指南 生成新密码，因此要么使用 pod 名称 (Argo CD 1.8 及更早版本)，要么使用存储在 secret 中的随机生成的密码 (Argo CD 1.9 及更高版本)。\n如何禁用管理员用户？ 将 admin.enabled: \u0026#34;false\u0026#34; 添加到 argocd-cm ConfigMap 中 ( 参见 用户管理 )。\nArgo CD 无法在没有互联网访问的情况下部署基于 Helm Chart 的应用程序，该怎么办？ 如果 Helm Chart 有位于外部存储库中的依赖项，则 Argo CD 可能无法生成 Helm Chart 清单。要解决此问题，你需要确保 requirements.yaml 仅使用内部可用的 Helm 存储库。即使 chart 仅使用来自内部存储库的依赖项，Helm 也可能决定刷新 stable 存储库。作为解决方法，可以在 argocd-cm config map 中覆盖 stable 存储库 URL：\ndata: repositories: | - type: helm url: http://\u0026lt;internal-helm-repo-host\u0026gt;:8080 name: stable 使用 Argo CD 部署 Helm 应用程序后，我无法使用 helm ls 和其他 Helm 命令看到它 在部署 Helm 应用程序时，Argo CD 仅使用 Helm 作为模板机制。它运行 helm template，然后在集群上部署生成的清单，而不是执行 helm install。这意味着你无法使用任何 Helm 命令查看/验证应用程序。它由 Argo CD 完全管理。请注意，Argo CD 支持一些 Helm 中可能缺少的本地功能（例如历史记录和回滚命令）。\n做出这个决定是为了使 Argo CD 对所有清单生成器都是中立的。\n我已经配置了 cluster secret，但是在 CLI/UI 中没有显示，我该如何修复？ 检查集群机密是否具有 argocd.argoproj.io/secret-type: cluster 标签。如果机密具有该标签但仍然无法看到集群，则可能是权限问题。尝试使用 admin 用户列出集群 ( 例如，argocd login --username admin \u0026amp;\u0026amp; argocd cluster list)。\nArgo CD 无法连接到我的集群，我该如何进行故障排除？ 使用以下步骤重建配置的集群配置并使用 kubectl 手动连接到集群：\nkubectl exec -it \u0026lt;argocd-pod-name\u0026gt; bash # ssh 到任何 argocd 服务器 pod argocd admin cluster kubeconfig https://\u0026lt;cluster-url\u0026gt; /tmp/config --namespace argocd # 生成你的集群配置 KUBECONFIG=/tmp/config kubectl get pods # 手动测试连接 现在，你可以手动验证 Argo CD pod 可以访问集群。\n如何终止同步？ 要终止同步，请单击“synchronisation”，然后单击“terminate”：\n即使同步了，我的应用程序仍然“Out Of Sync”，为什么？ 在某些情况下，你使用的工具可能会通过添加 app.kubernetes.io/instance 标签与 Argo CD 冲突。例如，使用 Kustomize 的常见标签功能。\nArgo CD 自动设置 app.kubernetes.io/instance 标签，并使用它来确定哪些资源形成应用程序。如果工具也这样做，这会导致混淆。你可以通过在 argocd-cm configmap 中设置 application.instanceLabelKey 值来更改此标签。我们建议你使用 argocd.argoproj.io/instance。\n🔔 提示：更改此设置后，你的应用程序将变为“不同步”，需要重新同步。\n请参见 #1482 。\nArgo CD 每隔多长时间检查我的 Git 或 Helm 存储库中的更改？ 默认轮询间隔为 3 分钟 (180 秒)。你可以通过更新 argocd-cm config map 中的 timeout.reconciliation 值来更改此设置。如果有任何 Git 更改，ArgoCD 仅会更新启用了 auto-sync setting 的应用程序。如果将其设置为 0，则 Argo CD 将停止自动轮询 Git 存储库，你只能使用替代方法（例如 webhooks 和/或手动同步）来部署应用程序。\n为什么我的资源限制“不同步”？ Kubernetes 在应用资源限制时对其进行了规范化，然后 Argo CD 比较了生成清单中的版本和 K8s 中的规范化版本 - 它们不会匹配。\n例如：\n\u0026#39;1000m\u0026#39; 规范化为 \u0026#39;1\u0026#39; \u0026#39;0.1\u0026#39; 规范化为 \u0026#39;100m\u0026#39; \u0026#39;3072Mi\u0026#39; 规范化为 \u0026#39;3Gi\u0026#39; 3072 规范化为 \u0026#39;3072\u0026#39; (添加引号) 要解决此问题，请使用差异化自定义设置 。\n如何修复“invalid cookie, longer than max length 4093”？ Argo CD 使用 JWT 作为身份验证令牌。你可能是许多组的一部分，并超过了设置为 cookie 的 4KB 限制。你可以通过打开“开发人员工具-\u0026gt;网络”来获取组列表：\n单击登录 找到调用 \u0026lt;argocd_instance\u0026gt;/auth/callback?code=\u0026lt;random_string\u0026gt; 在 https://jwt.io/ 上解码令牌。这将提供你可以从中删除自己的团队列表。\n请参见 #2165 。\n在使用 CLI 时为什么会出现“rpc error: code = Unavailable desc = transport is closing”？ 也许你在使用不支持 HTTP 2 的代理？尝试使用 --grpc-web 标志：\nargocd ... --grpc-web 在使用 CLI 时为什么会出现“x509: certificate signed by unknown authority”？ Argo CD 默认创建的证书未被 Argo CD CLI 自动识别，为了创建安全的系统，你必须遵循安装证书 的说明，并配置客户端操作系统以信任该证书。\n如果你不在生产系统中运行（例如，你正在测试 Argo CD），请尝试使用 --insecure 标志：\nargocd ... --insecure 🔔 警告：不要在生产中使用 --insecure。\n我已经通过 dex.config 在 argocd-cm 中配置了 Dex，但它仍然显示 Dex 未配置。为什么？ 很可能你忘记将 argocd-cm 中的 url 设置为指向你的 ArgoCD。另请参见 文档 。\n为什么SealedSecret资源会报告Status? SealedSecret的版本包括v0.15.0（特别是通过 helm 1.15.0-r3）不包括现代 CRD，因此状态字段将不会在 k8s 1.16+上公开。如果你的 Kubernetes 部署是modern ，请确保使用固定的 CRD，如果你想要此功能工作。\nSealedSecret类型资源为什么停留在Progressing状态中？ SealedSecret资源的控制器可以在它提供的资源上公开状态条件。自v2.0.0版本以来，ArgoCD 会获取该状态条件以为SealedSecret推导出健康状态。\nSealedSecret控制器的v0.15.0版本之前受到有关此状态条件更新的问题的影响，因此在这些版本中默认禁用此功能。可以通过使用--update-status命令行参数启动SealedSecret控制器或通过设置SEALED_SECRETS_UPDATE_STATUS环境变量来启用状态条件更新。\n要禁用 ArgoCD 检查SealedSecret资源上的状态条件，请通 …","relpermalink":"/book/argo-cd/faq/","summary":"我删除/损坏了我的存储库，无法删除我的应用程序。 如果 Argo CD 无法生成清单，则无法删除应用程序。你需要： 恢复/修复你的存储库。 使用cascade=false删除应用程序，然后手动删除资源。 为什么我的应用程序","title":"FAQ"},{"content":"你可以使用Traefik Proxy 来进行 Traffic Management with Argo Rollouts。\nTraefikService 是支持Traefik 作为 ingress 时加权轮询负载均衡 和Traefik 作为 ingress 时流量镜像 能力的对象。\n如何将 TraefikService 与 Argo Rollouts 集成，作为加权轮询负载均衡器 首先，我们需要使用 TraefikService 对象的加权轮询负载平衡能力创建 TraefikService 对象。\napiVersion: traefik.containo.us/v1alpha1 kind: TraefikService metadata: name: traefik-service spec: weighted: services: - name: stable-rollout # 为稳定应用程序版本创建的 k8s 服务名称 port: 80 - name: canary-rollout # 为新应用程序版本创建的 k8s 服务名称 port: 80 请注意，我们不指定“weight”字段。它需要与 ArgoCD 同步。如果我们指定此字段，而 Argo Rollouts 控制器更改它，则 ArgoCD 控制器将注意到并将显示此资源不同步（如果你正在使用 Argo CD 管理 Rollout）。\n其次，我们需要创建 Argo Rollouts 对象。\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollouts-demo spec: replicas: 5 strategy: canary: canaryService: canary-rollout stableService: stable-rollout trafficRouting: traefik: weightedTraefikServiceName: traefik-service # 指定我们之前创建的 traefikService 资源的名称 steps: - setWeight: 30 - pause: {} - setWeight: 40 - pause: {duration: 10} - setWeight: 60 - pause: {duration: 10} - setWeight: 80 - pause: {duration: 10} ... 如何将 TraefikService 与 Argo Rollouts 集成，作为流量镜像 首先，我们还需要创建 TraefikService 对象，但使用其流量镜像功能。\napiVersion: traefik.containo.us/v1alpha1 kind: TraefikService metadata: name: traefik-service spec: mirroring: name: some-service port: 80 mirrors: - name: stable-rollout # 为稳定应用程序版本创建的 k8s 服务名称 port: 80 - name: canary-rollout # 为新应用程序版本创建的 k8s 服务名称 port: 80 请注意，我们不指定“percent”字段。它需要与 ArgoCD 同步。如果我们指定此字段，而 Argo Rollouts 控制器更改它，则 ArgoCD 控制器将注意到并将显示此资源不同步（如果你正在使用 Argo CD 管理 Rollout）。\n其次，我们需要创建 Argo Rollouts 对象。\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollouts-demo spec: replicas: 5 strategy: canary: canaryService: canary-rollout stableService: stable-rollout trafficRouting: traefik: mirrorTraefikServiceName: traefik-service # 指定我们之前创建的 traefikService 资源的名称 steps: - setWeight: 30 - pause: {} - setWeight: 40 - pause: {duration: 10} - setWeight: 60 - pause: {duration: 10} - setWeight: 80 - pause: {duration: 10} ... ","relpermalink":"/book/argo-rollouts/traffic-management/traefik/","summary":"你可以使用Traefik Proxy 来进行 Traffic Management with Argo Rollouts。 TraefikService 是支持Traefik 作为 ingress 时加权轮询负载均衡 和Traefik 作为 ingress 时流量镜像 能力的对象。 如何将 TraefikService 与 Argo Rollouts 集成，作为加权轮询负载均衡器 首先，我们","title":"Traefik"},{"content":"背景 根据集群的配置，蓝绿部署（或使用流量管理的金丝雀部署）可能会导致新创建的 Pod 在部署新版本后重新启动。这可能会导致问题，特别是对于无法快速启动或无法正常退出的应用程序。\n这种行为发生的原因是集群自动缩放器想要缩小创建的额外容量以支持运行在双倍容量中的部署。当节点缩小时，它所拥有的 Pod 会被删除并重新创建。这通常发生在部署具有自己的专用实例组时，因为部署对集群自动缩放器的影响更大。因此，具有大量共享节点的集群较少经历这种行为。\n例如，此处有一个正在运行的部署，它有 8 个 Pod 分布在 2 个节点上。每个节点最多可容纳 6 个 Pod：\n原 Rollout 正在运行，跨越两个节点 当部署的 spec.template 发生变化时，控制器会创建一个新的 ReplicaSet，其中包含规范更新和 Pod 总数翻倍的版本。在这种情况下，Pod 的数量增加到 16。\n由于每个节点只能容纳 6 个 Pod，所以集群自动缩放器必须将节点数增加到 3 个来容纳所有 16 个 Pod。Pod 在节点之间的分布如下所示：\nRollout 容量扩大到两倍 部署完成后，旧版本会被缩小。这会使集群拥有比必要的更多的节点，从而浪费资源（如下所示）。\n原 Rollout 正在运行，跨越两个节点 集群自动缩放器终止了额外的节点，Pod 重新调度到剩余的 2 个节点上。\n原 Rollout 正在运行，跨越两个节点 为减少此行为的发生几率，部署可以在 ReplicaSet 中注入反亲和性。这可以防止新 Pod 在具有先前版本 Pod 的节点上运行。\n你可以在此处 了解有关反亲和性的更多信息。\n启用反亲和性的示例如下所示。当 spec.template 发生变化时，由于反亲和性，新 Pod 无法在运行旧 ReplicaSet 的 Pod 的节点上调度。因此，集群自动缩放器必须创建 2 个节点来托管新的 ReplicaSet 的 Pod。在这种情况下，Pod 不会启动，因为缩小后的节点保证不具有新的 Pod。\n原 Rollout 正在运行，跨越两个节点 在部署中启用反亲和性 通过将反亲和性结构添加到蓝绿或金丝雀策略中启用反亲和性。设置反亲和性结构时，控制器会将 PodAntiAffinity 结构注入到 ReplicaSet 的 Affinity 中。此功能不会修改 ReplicaSet 的任何现有亲和性规则。\n用户可以在以下调度规则之间进行选择：RequiredDuringSchedulingIgnoredDuringExecution 和 PreferredDuringSchedulingIgnoredDuringExecution。\nRequiredDuringSchedulingIgnoredDuringExecution 要求新版本的 Pod 在与以前版本不同的节点上。如果无法做到这一点，则不会调度新版本的 Pod。\nstrategy: bluegreen: antiAffinity: requiredDuringSchedulingIgnoredDuringExecution: {} 与 Required 策略不同，PreferredDuringSchedulingIgnoredDuringExecution 不会强制要求新版本的 Pod 在与以前版本不同的节点上。调度程序会尝试将新版本的 Pod 放置在单独的节点上。如果不可能，新版本的 Pod 仍将被调度。Weight 用于创建首选反亲和性规则的优先级顺序。\nstrategy: canary: antiAffinity: preferredDuringSchedulingIgnoredDuringExecution: weight: 1 # Between 1 - 100 🔔 重要提示：采用这种方法的主要缺点是，部署可能需要更长时间，因为为了根据反亲和性规则调度 Pod，可能会创建新节点。当部署具有自己的专用实例组时，这种延迟最常见，因为为了遵守反亲和性规则，可能会创建新节点。\n","relpermalink":"/book/argo-rollouts/rollout/anti-affinity/index/","summary":"背景 根据集群的配置，蓝绿部署（或使用流量管理的金丝雀部署）可能会导致新创建的 Pod 在部署新版本后重新启动。这可能会导致问题，特别是对于无法快速启动或无法正常退出的应用程序。 这种行为发生的原因是集群自动缩放","title":"反亲和性"},{"content":" 概览\n插件\n指标\n","relpermalink":"/book/argo-rollouts/analysis/","summary":"概览 插件 指标","title":"分析"},{"content":"本章解释了如何实施使用 SPIFFE 身份的授权策略。\n在 SPIFFE 的基础上建立授权 SPIFFE 专注于软件安全加密身份的发布和互操作性，但正如本书前面提到的，它并不直接解决这些身份的使用或消费问题。\nSPIFFE 经常作为一个强大的授权系统的基石，而 SPIFFE ID 本身在这个故事中扮演着重要角色。在这一节中，我们将讨论使用 SPIFFE 来建立授权的选择。\n认证与授权（AuthN Vs AuthZ） 一旦一个工作负载有了安全的加密身份，它就可以向其他服务证明其身份。向外部服务证明身份被称为认证（Authentication）。一旦通过认证，该服务就可以选择允许哪些行动。这个过程被称为授权（Authorization）。\n在一些系统中，任何被认证的实体也被授权。因为 SPIFFE 会在服务启动时自动授予其身份，所以清楚地认识到并不是每一个能够验证自己的实体都应该被授权，这一点至关重要。\n授权类型 有很多方法可以对授权进行建模。最简单的解决方案是在每个资源上附加一个授权身份的允许列表（allowlist）。然而，随着我们的探索，我们会注意到在处理生态系统的规模和复杂性时，允许列表的方法有几个限制。我们将研究两个更复杂的模型：基于角色的访问控制（RBAC）和基于属性的访问控制（ABAC）。\n允许列表 在小型生态系统中，或者在刚刚开始使用 SPIFFE 和 SPIRE 时，有时最好保持简单。例如，如果你的生态系统中只有十几个身份，对每个资源（即服务、数据库）的访问可以通过维护一个有访问权限的身份列表来管理。\nghostunnel server --allow-uri spiffe://example.com/blog/web 在这里，ghostunnel 服务器仅根据客户的身份明确地授权访问。\n这种模式的优势在于它很容易理解。只要你有数量有限的身份不改变，就很容易定义和更新资源的访问控制。然而，可扩展性会成为一个障碍。如果一个组织有成百上千的身份，维护允许名单很快就会变得无法管理。例如，每次增加一个新的服务，可能需要运维团队更新许多允许列表。\n基于角色的访问控制（RBAC） 在基于角色的访问控制（RBAC）中，服务被分配给角色，然后根据角色来指定访问控制。然后，随着新服务的增加，只有相对较少的角色需要被编辑。\n虽然有可能将一个服务的角色编码到它的 SPIFFE ID 中，但这通常是一种不好的做法，因为 SPIFFE ID 是静态的，而它被分配到的角色可能要改变。相反，最好是使用 SPIFFE ID 到角色的外部映射。\n基于属性的访问控制（ABAC） 基于属性的访问控制（ABAC）是一个模型，授权决定是基于与服务相关的属性。结合 RBAC，ABAC 可以成为一个强大的工具来加强授权策略。例如，为了满足法律要求，可能有必要限制来自特定地区的服务对数据库的访问。区域信息可以是 ABAC 模型中的一个属性，用于授权并在 SPIFFE ID 方案中编码。\n设计用于授权的 SPIFFE ID 方案 SPIFFE 规范没有规定或限制你可以或应该将哪些信息编码到 SPIFFE ID 中。你需要注意的唯一限制来自于最大长度的 SAN 扩展和你被允许使用的字符。\n忠告 在将授权元数据编码成你的组织的 SPIFFE ID 格式时，要特别小心。下面的例子说明了如何做到这一点，因为我们并不想引入额外的授权概念。 SPIFFE 方案实例\n为了对 SPIFFE 身份子串做出授权决定，我们必须定义身份的每一部分意味着什么。你可以用按顺序编码信息的格式来设计你的方案。在这种情况下，第一部分可能代表一个地区，第二部分代表环境，以此类推。\n下面是一个计划和身份的例子。\nspiffe://trust.domain.org/\u0026lt;地区\u0026gt;/\u0026lt;dev,stage,prod\u0026gt;/\u0026lt;组织\u0026gt;/\u0026lt;工作负载名称\u0026gt;。 身份方案不仅可以采取一系列固定字段的形式，还可以采取更复杂的结构，这取决于一个组织的需求。我们可以看的一个常见的例子是跨不同协调系统的工作负载身份。例如，在 Kubernetes 和 OpenShift 中，工作负载的命名规则是不同的。下面的图示就是一个例子。你可能注意到，这些字段不仅指的是不同的属性和对象，而且 SPIFFE ID 的结构也取决于上下文。\n消费者可以通过观察身份的前缀来区分方案的结构。例如，一个前缀为 spiffe://trust.domain.org/Kubernetes/... 的身份将根据下图的方案结构被解析为一个 Kubernetes 身份。\n方案变更 更多时候，组织会发生变化，对身份方案的要求也会发生变化。这可能是由于组织结构的调整，甚至是技术栈的转变。可能很难预测你的环境在几年后会有多大的变化。因此，在设计 SPIFFE 身份识别方案时，关键是要考虑到未来可能发生的变化，以及这些变化将如何影响基于 SPIFFE 身份识别的其他系统。你应该考虑如何将后向和前向兼容性纳入该方案。正如我们之前已经提到的，在一个有序的方案中，你只需要在你的 SPIFFE ID 的末端添加新的实体；但是如果你需要在中间添加一些东西呢？\n一种方法是用基于键值对的方案，另一种方法是我们都很熟悉的方法：版本管理！\n基于键值对的方案\n我们注意到，上面的方案设计都是有序的。方案的评估是通过查看身份的前缀来决定如何评估后面的后缀。然而，我们注意到，由于这种排序，很难轻易地在方案中增加新的字段。\n键值对，就其性质而言，是无序的，这也是一种方法，可以轻松地将字段扩展到身份识别方案中，而不需要太多改变。例如，你可以使用带有已知分隔符的键值对，例如，身份内的列：字符。在这种情况下，上面的标识可能被编码为以下方式。\nspiffe://trust.domain.org/environment:dev/region:us/organization:zero/name:turtle 因为身份的消费者将其处理成一组键值对，所以可以在不改变方案的基本结构的情况下增加更多的键。另外，SPIFFE 还有可能在将来支持将键值对纳 SVID。\n像往常一样，应该考虑结构化和非结构化数据类型之间的权衡。\n版本管理\n这里可能的解决方案之一是将版本控制纳入方案。版本可以是你的方案中的第一个项目，也是最关键的部分。其余的系统在处理 SPIFFE ID 数据时需要遵循版本和编码实体之间的映射关系。\nspiffe://trust.domain.org/v1/region/environment/organization/workload v1 scheme: 0 = version 1 = region 2 = environment 3 = organization 4 = workload spiffe://trust.domain.org/v2/region/datacenter/environment/organization/wor kload v2 scheme: 0 = version 1 = region 2 = datacenter 3 = environment 4 = organization 5 = workload 在 SPIFFE 中，一个工作负载可以有多个身份。然而，由你的工作负载来决定使用哪个身份。为了保持授权的简单性，每个工作负载最好先有一个身份，必要时再增加。\n使用 HashiCorp Vault 的授权示例 让我们通过一个工作负载可能希望与之对话的服务的例子：Hashicorp Vault。我们将通过一个 RBAC 的例子和一个 ABAC 的例子，并涵盖一些使用 SPIFFE/SPIRE 执行授权时的问题和注意事项。\nVault 是一个秘密存储器（secret store）：管理员可以用它来安全地存储秘密，如密码、API 密钥和服务可能需要的私人密钥。由于许多组织仍然需要安全地存储秘密，即使在使用 SPIFFE 提供安全身份之后，使用 SPIFFE 来访问 Vault 是一个常见的请求。\nspiffe://example.org/\u0026lt;区域\u0026gt;/\u0026lt;dev,stage,prod\u0026gt;/\u0026lt;组织\u0026gt;/\u0026lt;工作负载名称\u0026gt;。 为 SPIFFE 身份配置 Vault 在处理客户请求时，Vault 同时处理身份的认证和授权任务。像许多其他处理资源（在这里是指秘密）管理的应用程序一样，它有一个可插入各种认证和授权机制的接口。\n在 Vault 中，这是通过 TLS 证书认证方法 或 JWT/OIDC 认证方法 ，可以配置为识别和验证从 SPIFFE 生成的 JWT 和 X509-SVID。为了使 Vault 能够使用 SPIFFE 身份来使用，信任包需要配置这些可插拔的接口，以便它能够验证 SVID。\n这就解决了认证问题，但我们仍然需要配置它来执行授权。要做到这一点，需要为 Vault 制定一套授权规则，以决定哪些身份可以访问秘密。\n一个 SPIFFE RBAC 的例子\n在下面的例子中，我们将假设我们使用的是 X509-SVID。Vault 允许创建规则，它可以表达哪些身份可以访问哪些秘密。这通常包括创建一组访问权限，并创建一个将其与访问绑定的规则。\n例如，一个简单的 RBAC 策略：\n{ \u0026#34;display_name\u0026#34;: \u0026#34;medical-access-role\u0026#34;, \u0026#34;allowed_common_names\u0026#34;: [\u0026#34;spiffe://example.org/eu-de/prod/medical/data-proc-1\u0026#34;, \u0026#34;spiffe://example.org/eu-de/prod/medical/data-proc-2\u0026#34; ], \u0026#34;token_policies\u0026#34;: \u0026#34;medical-use\u0026#34;, } 这编码了一条规则，说明如果身份为 spiffe://example.org/eu-de/prod/medical/data-proc-1，或 spiffe://example.org/eu-de/prod/medical/data-proc-2 的客户能够获得一组权限（medical-use），它将授予医疗数据的访问权。\n在这种情况下，我们已经授予这两个身份对秘密的访问权。Vault 负责将两个不同的 SPIFFE ID 映射到相同的访问控制策略中，这使得这成为 RBAC 而不是 allowlist。\n一个 SPIFFE ABAC 的例子\n在某些情况下，基于属性而不是基于角色来设计授权策略是比较容易的。通常情况下，当有多个不同的属性集可以单独与策略相匹配时，就需要这样做，而要创建足够多的独特角色来匹配每种情况是很有挑战性的。\n根据上述例子，我们可以创建一个策略，授权具有某个 SPIFFE ID 前缀的工作负载。\n{ ... \u0026#34;display_name\u0026#34;: \u0026#34;medical-access-role\u0026#34;, \u0026#34;allowed_common_names\u0026#34;: [\u0026#34;spiffe://example.org/eu/prod/medical/batch-job*\u0026#34;], \u0026#34;token_policies\u0026#34;: \u0026#34;medical-use\u0026#34;, } 该策略规定，所有前缀为 spiffe://example.org/eu/prod/medical/batch-job 的工作负载将被授权访问该秘密。这可能很有用，因为批处理工作是短暂的，可能会被随机分配一个后缀。\n另一个例子是一个有以下内容的策略：\n{ ... \u0026#34;display_name\u0026#34;: \u0026#34;medical-access-role\u0026#34;, \u0026#34;allowed_common_names\u0026#34;: [\u0026#34;spiffe://example.org/eu-*/prod/medical/data-proc\u0026#34;], \u0026#34;token_policies\u0026#34;: \u0026#34;medical-use\u0026#34;, } 该政策的预期效果是说明只有任何欧盟数据中心的 data-proc 工作负载可以访问医疗秘密。因此，如果在欧盟的一个新数据中心启动一个新的工作负载，任何 data-proc 工作负载将被授权访问医疗秘密。\nOpen …","relpermalink":"/book/spiffe/using-spiffe-identities-to-inform-authorization/","summary":"本章解释了如何实施使用 SPIFFE 身份的授权策略。","title":"使用 SPIFFE 身份通知授权"},{"content":"验证安装 检查 DaemonSet 的状态并验证所有需要的实例都处于 ready 状态：\n$ kubectl --namespace kube-system get ds NAME DESIRED CURRENT READY NODE-SELECTOR AGE cilium 1 1 0 \u0026lt;none\u0026gt; 3s 在此示例中，我们看到 1 个期望的状态，0 个 ready 状态。这表明有问题。下一步是通过在 k8s-app=cilium 标签上匹配列出所有 cilium pod，并根据每个 pod 的重启次数对列表进行排序，以便轻松识别失败的 pod：\n$ kubectl --namespace kube-system get pods --selector k8s-app=cilium \\ --sort-by=\u0026#39;.status.containerStatuses[0].restartCount\u0026#39; NAME READY STATUS RESTARTS AGE cilium-813gf 0/1 CrashLoopBackOff 2 44s cilium-813gf pod 失败并且已经重新启动了 2 次。让我们打印该 pod 的日志文件来调查原因：\n$ kubectl --namespace kube-system logs cilium-813gf INFO _ _ _ INFO ___|_| |_|_ _ _____ INFO | _| | | | | | | INFO |___|_|_|_|___|_|_|_| INFO Cilium 0.8.90 f022e2f Thu, 27 Apr 2017 23:17:56 -0700 go version go1.7.5 linux/amd64 CRIT kernel version: NOT OK: minimal supported kernel version is \u0026gt;= 4.8 在此示例中，失败的原因是在工作节点上运行的 Linux 内核不符合系统要求 。\n如果根据这些简单的步骤无法发现问题的原因，请来我们的 Slack channel 。\n集群外的 APIserver 如果你出于某种原因在集群外部运行 Kubernetes Apiserver（例如将主节点保留在防火墙后面），请确保您也在主节点上运行 Cilium。否则，由 Apiserver 创建的 Kubernetes pod 代理将无法路由到 pod IP，并且你在尝试将流量代理到 pod 时可能会遇到错误。\n你可以将 Cilium 作为静态 pod 运行，或者为 Cilium DaemonSet 设置容忍 ，以确保将 Cilium pod 安排在你的主节点上。执行此操作的确切方法取决于你的设置。\n","relpermalink":"/book/cilium-handbook/kubernetes/troubleshooting/","summary":"验证安装 检查 DaemonSet 的状态并验证所有需要的实例都处于 ready 状态： $ kubectl --namespace kube-system get ds NAME DESIRED CURRENT READY NODE-SELECTOR AGE cilium 1 1 0 \u003cnone\u003e 3s 在此示例中，我们看到 1 个期望的状态，0 个 ready 状态。这表明有问题。下一步是通过在 k8s-app=cilium 标签上匹配列出所有 cilium pod，并根","title":"故障排除"},{"content":"DevSecOps 的好处包括：\n各个 IT 团队之间，特别是开发人员、运维和安全团队以及其他利益相关者之间更好的沟通和协作。导致 更好的生产力 。 简化软件开发、交付和部署过程 —— 由于自动化，停机时间减少，发布时间加快，基础设施和运维成本降低，效率提高。 通过实施零信任来减少攻击面，这也限制了横向移动，从而防止攻击升级。具有现代行为预防能力的持续监控进一步促进了这一点。 安全优势。通过对每个请求的验证监控、警报和反馈机制来提高安全性，因为可观测性是代码。这些将在以下段落中详细描述。具体的能力包括： a. 运行时：杀死恶意容器。 b. 反馈：由于一个错误的程序更新了代码并重新触发了管道，所以反馈到了正确的存储库。 c. 监测新的和终止的服务，并调整相关服务（如服务代理）。 d. 启用安全断言。不可绕过 —— 通过在同一空间执行的代理、安全会话、强大的认证和授权以及安全的状态转换。 启用持续授权操作（C-ATO），在本节末尾详细描述。 对每个请求的验证和上述的反馈机制将在下面进一步描述： 每个请求的验证。来自用户或客户端应用程序（服务）的每个请求都要经过验证和授权（使用 OPA 或任何外部授权引擎或 接纳控制器 等机制，它们是平台的组成部分）。授权引擎提供特定于应用域的策略执行，而接纳控制器则提供与特定平台的端点对象（如 Pod、部署、命名空间）相关的平台特定策略。具体来说，接纳控制器会进行突变和验证。突变的接纳控制器解析每个请求，并在将其向下转发之前对请求进行修改（突变）。一个例子是为没有被用户在请求中设置的规格设置默认值，以确保在集群上运行的工作负载是统一的，并遵循集群管理员定义的特定标准。另一个例子是为 Pod 添加特定的资源限制（如果资源限制没有为该 Pod 设置），然后向下转发（如果请求中没有这个字段，通过添加这个字段来突变请求）。通过这样做，集群中的所有 Pod 将始终有一个根据规范设置的资源限制，除非明确说明。验证接纳控制器会拒绝那些不遵循特定规范的请求。例如，没有一个 Pod 请求可以将安全上下文设置为以根用户身份运行。 反馈机制： 一些在运行时发现的问题的补救措施可能需要在源代码中处理或修复。应该有一个流程，针对正确的代码库自动打开一个问题，以修复问题并重新触发 DevSecOps 管道。 向应用程序托管平台提供反馈回路（例如，杀死包含恶意容器的 Pod 的通知）。 通过监控应用程序的配置，提供主动的动态安全（例如，监控引入到应用程序的新荚 / 容器，并生成和注入代理以照顾其安全通信需求）。 启用关于应用程序的几个安全断言：不可绕过（即在所有使用场景下始终执行的策略）、整个应用程序代码的受信任和不受信任部分、没有凭证和特权泄漏、受信任的通信路径和安全状态转换。 启用关于性能参数的断言（例如，网络弹性参数，如在故障、冗余和可恢复性功能下继续运行）。 总的来说，更快地吸收反馈意见，使软件得到更快的改进。 ","relpermalink":"/book/service-mesh-devsecops/implement/benefits-of-devsecops-primitives-to-application-security-in-the-service-mesh/","summary":"DevSecOps 的好处包括： 各个 IT 团队之间，特别是开发人员、运维和安全团队以及其他利益相关者之间更好的沟通和协作。导致 更好的生产力 。 简化软件开发、交付和部署过程 —— 由于自动化，停机时间减少，发布时间加快，基础设施和","title":"4.9 DevSecOps 原语对服务网格中应用安全的好处"},{"content":"云原生应用程序依赖基础架构才能运行，反过来说，云原生基础架构也需要云原生应用程序来维持。\n对于传统基础架构，维护和升级基本上都是由人工完成。可能是在单机上手动运行服务或使用自动化工具定义基础结构和应用程序的快照。\n但是，如果基础架构可以由应用程序管理，同时基础架构又可以管理应用程序，那么基础架构工具就会成为另一种应用程序。工程师对于基础架构的责任可以用调解器模式表示，内置于该基础架构上运行的应用程序中。\n我们花了三章来说明如何构建可以管理基础架构的应用程序。本章将介绍如何在基础架构上运行云原生应用或其它任何应用。\n如前所述，保持基础架构和应用程序的简单非常重要。解决应用程序复杂性最常用的方法就是把应用程序分解成小的，易于理解的组件。通常通过创建单一职责的服务来实现，或者将代码分解为一系列事件触发的函数。\n随着小型、可部署单元的扩容成多份即使是最自动化的基础架构也可能被压垮。管理大量应用程序的唯一方法是让它们承担第 1 章中所述的功能性操作。应用程序需要在可以按规模管理之前变成原生云。\n学习完本章不会帮助您建下一个伟大的应用程序，但您将了解一些基础知识，这块可以让您的应用程序在云原生基础架构是良好运行。\n应用程序设计 已经有很多教您如何构建应用程序的书了，本书不打算再讨论。但是，了解应用程序架构如何影响基础架构设计仍然很重要。\n正如我们在第 1 章中讨论的那样，我们假设应用程序设计成云原生的，这样可以从云原生基础架构中获得最大收益。云原生的本质是应用程序由软件而不是人类来管理。\n应用程序的设计和打包方式是分开考虑的。应用程序可以是云原生的，打包为 RPM 或 DEB 文件，可以部署到虚拟机而不是容器。它们可以是单体应用或微服务，可以用 Java 或 Go 编写。\n这些实现细节不影响应用程序被设计成在云上运行。\n假设我们有一个用 Go 编写的应用程序，使用容器打包，运行在 Kubernetes 上，视为微服务运行。\n我们假想的这个应用是“云原生”的吗？\n如果应用程序将所有活动日志记录到文件，还硬编码数据库 IP 地址呢？也许它不接受运行时配置并将状态存储在本地磁盘上。如果它不以可预见的方式存在或挂起并等待人工调试呢？\n这个应用所选择的语言和打包方式，可能会让您觉的它是云原生的，但实际上它根本不是。像 Kubernetes 这样的框架可以通过各种功能来管理这个应用，但即使它可以运行，但还是需要人类来维护。\n第 1 章详细介绍了使应用程序在云原生基础架构上运行得更好的一些特点。如果我们具有第 1 章中规定的特点，应用程序还有另一个考虑因素：我们如何有效地管理它们？\n实施云原生模式 诸如弹性伸缩、服务发现、配置、日志、健康检查和相关监控指标等功能都可以以不同方式在应用程序中实现。实现这些功能的常见做法是通过导入实现相关功能的标准语言库。Netflix OSS 和 Twitter 的 Finagle 是在 Java 语言库中实现这些功能的很好的例子。\n应用程序可以导入和使用库，自动获得库中提供许多相关的功能，无需额外的代码。当一个组织内支持的语言很少时，这种模式很有意义。这种模式很容易实现最佳实践。\n当组织开始实施微服务时，它们往往倾向于使用多语言服务。这样可以自由地为不同的服务选择正确的语言，但是很难为每种语言维护库。\n获得该功能的另一种方法是通过所谓的 “Sidecar” 模式。此模式应用程序本身与实现管理功能的应用程序绑定在一起。通常作为单独的容器来实现，但也可以通过在虚拟机上运行另一个守护进程来实现。\nSidecar 的例子包括以下内容：\nEnvoy 代理\n为服务增加弹性伸缩和监控指标\n注册\n通过外部服务发现注册服务\n动态配置\n订阅配置更改并通知服务进程重新加载\n健康检查\n提供用于检查应用程序运行状况的 HTTP 端点 (Endpoints)\nSidecar 容器还可以用来适配 Polyglot 容器，通过暴露特定于语言的端点与使用库的应用程序进行交互。来自 Netflix 的 Prana 正是为那些不使用标准 Java 库的应用程序而定制的。\n当有团队集中管理特定的 Sidecar 进程时，Sidecar 模式很有意义。如果工程师想要在它们的服务中暴露监控指标，它们可以将其构建到应用程序中，或者一个单独的团队也可以提供处理日志记录输出并公开计算出的监控指标的 Sidecar 应用。\n在这两种情况下，都可以在不修改应用程序的情况下为应用添加功能。在可以使用软件管理应用程序后，我们来看看如何管理应用程序的生命周期。\n应用程序生命周期 云原生应用程序除了生命周期应该由软件管理以外，其生命周期本身与传统应用程序并没有什么不同。\n本章不打算解释管理应用程序时涉及的所有模式和选项。我们将简要讨论几个阶段，在云原生基础架构之上运行云原生应用，这几个阶段受益程度最高：部署、运行和下线。\n这些主题并不都包含所有选项，但还有很多其他书籍和文章可供参考，这取决于应用程序的架构，语言和所选库。\n部署 部署是应用程序最依赖基础架构的一个领域。虽然没有什么东西会阻止应用程序自行部署，但基础架构管理还可以管理更多的方面。\n本文不会涉及集成和交付，但是在这个领域的一些做法很明确。应用程序部署不仅仅是获取和运行代码。\n云原生应用程序旨在由软件管理应用生命周期的各个阶段。这包括周期性的健康检查和部署初始化。应尽可能地消除技术、流程和策略中人为造成的瓶颈。\n应用程序的部署首先应该是自动、自助的，如果应用正在活跃开发中，则应该是被频繁触发的。也应该可以被测试、验证可稳定运行。\n新版本和新功能的发布时很少会有一次性替换应用程序的所有实例的情况。新功能在配置标志成 “gated”，可以在不重启应用的情况下选择性地动态启用新功能。版本升级部分发布，通过测试进行验证，并在所有测试通过时以受控方式发布。\n当启用新功能或部署新版本时，应该存在控制流向或隔离应用流量的机制（请参阅附录 A）。通过缓慢的部署和更快的应用性能反馈循环进行新功能的试用，可以限制中断带来的影响。\n基础架构应负责部署软件的所有细节。工程师可以定义应用程序版本，基础架构要求和依赖关系，并且基础架构将朝着该状态发展，直至满足所有要求或需求更改。\n运行 运行应用程序应该是应用程序生命周期中最平稳最稳定的阶段。运行软件最重要的两个的方面在第 1 章中讨论：了解应用程序在做什么以及可操作性即可以根据需要更改应用程序。\n我们已经在第 1 章中详细介绍了关于应用报告健康和遥测数据的可观测性，但是当事情不按预期工作时，你会做什么？如果应用程序的遥测数据显示它不符合 SLO，那么如何解决和调试应用程序？\n对于云原生应用程序，你不应该通过 SSH 连接到服务器的形式查看日志。如果你需要 SSH，更应该考虑使用日志或其它的服务替代。\n你仍然需要访问应用程序（API）和日志数据（云日志记录）以及获取在堆栈中的服务，但这值得通过演练来查看是否需要传统工具。当事件中断时，你需要一个调试应用程序和基础架构组件的方法。\n在做系统调试时，你应该首先查看你的基础架构测试，如第 5 章所述。测试应公开所有未正确配置或未提供预期性能的基础架构组件。\n不能说因为你不管理底层基础架构就意味着基础架构不可能出问题。通过测试来验证期望值将确保你的基础架构能够以你期望的方式运行。\n在排除基础架构后，你应该查看应用程序以获取更多信息。应用程序调试的最佳位置是应用性能管理（APM）以及可能通过 OpenTracing 等标准进行的分布式应用程序跟踪。\nOpenTracing 示例、实现和 APM 不在本书的范围之内。总而言之，OpenTracing 允许你在整个应用程序中跟踪调用，以更轻松地识别网络和应用程序通信问题。OpenTracing 的示例可视化可以在图 7-1 中看到。APM 为你的应用程序添加了用于向收集服务报告指标和故障的工具。\n图 7-1. OpenTracing 可视化 当测试和跟踪仍然没有暴露出问题时，有时你只需要在应用程序上启用更详细的日志记录。但是，如何在不破坏问题的情况下启用调试？\n运行时配置对于应用程序很重要，但在云原生环境中，无须重启应用程序，配置就应该是动态的。配置选项仍然通过应用程序中的库实现，但标志值应该能够通过集中协调器，应用程序 API 调用，HTTP 协议 Header 或多种方式进行动态更改。\nNetflix 的 Archaius 和 Facebook 的 GateKeeper 是动态配置的两个例子。前 Facebook 工程师经理 Justin Mitchell 在 Quora 的帖子中分享到：\nGateKeeper 是从代码部署中解耦出来的功能。我们可以在几天或几周内发布新功能，因为我们观察了用户指标、性能并确保服务可以随时扩展。\n允许对应用程序配置进行动态控制，可以实现更多对曝光过度的新功能的控制并更好地测试已部署代码的覆盖范围。可以很容易的发布新代码并不意味着这是适合所有情况的正确解决方案。\n基础架构可以帮助解决此问题，并通过协调何时启用新功能和基于高级网络策略的路由控制来启用更灵活的应用程序。这种模式还允许更细粒度的控制和更好的协调发布或回滚场景。\n在动态的自助服务环境中，部署的应用程序数量将快速增长。你需要确保有一个简单的方法来动态调试应用类似自助服务模型中部署的应用。\n工程师喜欢发布新应用程序一样，反过来很难让它们下线旧应用程序。即使如此，旧应用下线仍然是应用程序生命周期中的关键阶段。\n下线 部署新的应用程序和服务在快速迭代的环境中很常见。下线应用程序应该像创建应用一样自动化。\n如果新的服务和资源被自动化部署与监控，则它们应该按照相同标准下线。尽快部署新服务而不删除未使用的服务是应对技术债务的最简单方法。\n识别应该下线的服务和资源，这是一个特定的业务。你可以使用应用程序遥测的经验数据来了解某个应用程序是否正在被使用，但是下线应用程序的决定应由该业务决定。\n基础架构组件（例如，VM 实例和负载均衡器端点）应在不需要时被自动清理。自动化组件清理的一个例子是 Netflix 的 Janitor Monkey。该公司在一篇博文中解释道：\nJanitor Monkey 通过应用一组规则来决定资源是否应该成为候选的清理内容。如果任何规则确定该资源是被清理的候选内容，则 Janitor Monkey 标记该资源并安排清理时间。\n所有这些应用阶段的目标是让基础架构和软件来管理原本由人类管理的方面。我们采用协调模式与组件元数据相结合的方式来不断运行，并根据当前上下文对需要采取的高层次操作做出决策。以此来取代由人类编写的临时自动化脚本。\n应用程序的生命周期不是唯一一个需要依赖于基础架构的阶段。还有一些每个阶段都要依赖于基础架构服务的程序。我们将在下一节讨论一些提供给这类应用的支持服务和基础架构 API。\n对运行于基础架构上的应用的要求 云原生应用程序对基础架构的期望不仅只是执行二进制文件，它们还需要抽象、隔离与保证应用程序运行和管理。对于应用程序来说，需要提供 hook 和 API 以允许基础架构管理它们。为了实现这种模式，两者就需要有一种共生关系。\n我们在第 1 章中定义了云原生应用程序，并刚刚讨论了一些生命周期的要求。现在让我们看看云原生应用程序对从运行它们的基础架构建设的更多期望：\n运行与隔离 资源分配和调度 环境隔离 服务发现 状态管理 监控和记录 监控指标聚合 调试和跟踪 所有这些期望都应该是服务的默认选项，或者是由自助 API 提供。我们将更详细地解释每个要求，以确保这些期望被明确的定义。\n应用程序运行和隔离 除了有时候需要的解释器，传统应用程序只需要一个内核就可以运行。云原生应用仍然需要它们，但云原生应用运行时同样也需要与操作系统和其他应用程序隔离。隔离使多个应用能够在同一台服务器上运行并控制它们的依赖和资源。\n应用隔离有时被称为多租户。该术语可用于在同一服务器上运行的多个应用程序以及在共享集群中运行应用程序的多个用户。用户可以运行经过验证的可信代码，也可以运行你不能控制且不信任的代码。\n云原 …","relpermalink":"/book/cloud-native-infra/managing-cloud-native-applications/","summary":"云原生应用程序依赖基础架构才能运行，反过来说，云原生基础架构也需要云原生应用程序来维持。 对于传统基础架构，维护和升级基本上都是由人工完成。可能是在单机上手动运行服务或使用自动化工具定义基础结构和应用程","title":"第 7 章：管理云原生应用程序"},{"content":"认证和授权是限制访问集群资源的主要机制。如果集群配置错误，网络行为者可以扫描知名的 Kubernetes 端口，访问集群的数据库或进行 API 调用，而不需要经过认证。用户认证不是 Kubernetes 的一个内置功能。然而，有几种方法可以让管理员在集群中添加认证。\n认证 管理员必须向集群添加一个认证方法，以实现认证和授权机制。\nKubernetes 集群有两种类型的用户：服务账户和普通用户账户。服务账户代表 Pod 处理 API 请求。认证通常由 Kubernetes 通过 ServiceAccount Admission Controller 使用承载令牌自动管理。不记名令牌被安装到 Pod 中约定俗成的位置，如果令牌不安全，可能会在集群外使用。正因为如此，对 Pod Secret 的访问应该限制在那些需要使用 Kubernetes RBAC 查看的人身上。对于普通用户和管理员账户，没有自动的用户认证方法。管理员必须在集群中添加一个认证方法，以实现认证和授权机制。\nKubernetes 假设由一个独立于集群的服务来管理用户认证。Kubernetes 文档中 列出了几种实现用户认证的方法，包括客户端证书、承载令牌、认证插件和其他认证协议。至少应该实现一种用户认证方法。当实施多种认证方法时，第一个成功认证请求的模块会缩短评估的时间。管理员不应使用静态密码文件等弱方法。薄弱的认证方法可能允许网络行为者冒充合法用户进行认证。\n匿名请求是被其他配置的认证方法拒绝的请求，并且不与任何个人用户或 Pod 相联系。在一个设置了令牌认证并启用了匿名请求的服务器中，没有令牌的请求将作为匿名请求执行。在 Kubernetes 1.6 和更新的版本中，匿名请求是默认启用的。当启用 RBAC 时，匿名请求需要 system:anonymous 用户或 system:unauthenticated 组的明确授权。匿名请求应该通过向 API 服务器传递 --anonymous-auth=false 选项来禁用。启用匿名请求可能会允许网络行为者在没有认证的情况下访问集群资源。\n基于角色的访问控制 RBAC 是根据组织内个人的角色来控制集群资源访问的一种方法。在 Kubernetes 1.6 和更新的版本中，RBAC 是默认启用的。要使用 kubectl 检查集群中是否启用了 RBAC，执行 kubectl api-version。如果启用，应该列出rbac.authorization.k8s.io/v1 的 API 版本。云 Kubernetes 服务可能有不同的方式来检查集群是否启用了 RBAC。如果没有启用 RBAC，在下面的命令中用 --authorization-mode 标志启动 API 服务器。\nkube-apiserver --authorization-mode=RBAC 留下授权模式标志，如 AlwaysAllow，允许所有的授权请求，有效地禁用所有的授权，限制了执行最小权限的访问能力。\n可以设置两种类型的权限：Roles 和 ClusterRoles。Roles 为特定命名空间设置权限，而 ClusterRoles 则为所有集群资源设置权限，而不考虑命名空间。Roles 和 ClusterRoles 只能用于添加权限。没有拒绝规则。如果一个集群被配置为使用 RBAC，并且匿名访问被禁用，Kubernetes API 服务器将拒绝没有明确允许的权限。附录 J 中显示了一个 RBAC 角色的例子：pod-reader RBAC 角色。\n一个 Role 或 ClusterRole 定义了一个权限，但并没有将该权限与一个用户绑定。RoleBindings 和 ClusterRoleBindings 用于将一个 Roles 或 ClusterRoles 与一个用户、组或服务账户联系起来。角色绑定将角色或集群角色的权限授予定义的命名空间中的用户、组或服务账户。ClusterRoles 是独立于命名空间而创建的，然后可以使用 RoleBinding 来限制命名空间的范围授予个人。ClusterRoleBindings 授予用户、群组或服务账户跨所有集群资源的 ClusterRoles。RBAC RoleBinding 和 ClusterRoleBinding 的例子在附录 K：RBAC RoleBinding 和 ClusterRoleBinding 示例 中。\n要创建或更新 Roles 和 ClusterRoles，用户必须在同一范围内拥有新角色所包含的权限，或者拥有对 rbac.authorization.k8s.io API 组中的 Roles 或 ClusterRoles 资源执行升级动词的明确权限。创建绑定后，Roles 或 ClusterRoles 是不可改变的。要改变一个角色，必须删除该绑定。\n分配给用户、组和服务账户的权限应该遵循最小权限原则，只给资源以必要的权限。用户或用户组可以被限制在所需资源所在的特定命名空间。默认情况下，为每个命名空间创建一个服务账户，以便 Pod 访问 Kubernetes API。可以使用 RBAC 策略来指定每个命名空间的服务账户的允许操作。对 Kubernetes API 的访问是通过创建 RBAC 角色或 ClusterRoles 来限制的，该角色具有适当的 API 请求动词和所需的资源，该行动可以应用于此。有一些工具可以通过打印用户、组和服务账户及其相关分配的 Roles 和 ClusterRoles 来帮助审计 RBAC 策略。\n","relpermalink":"/book/kubernetes-hardening-guidance/authentication-and-authorization/","summary":"认证和授权是限制访问集群资源的主要机制。如果集群配置错误，网络行为者可以扫描知名的 Kubernetes 端口，访问集群的数据库或进行 API 调用，而不需要经过认证。用户认证不是 Kubernetes 的一个内置功能。然而，有几种方法可以让管理员在集","title":"认证和授权"},{"content":"上线一个新的遥测系统可能是一项复杂的工作。它需要整个工程组织的支持，不能一蹴而就。不要低估这可能会产生的问题！\n在大型组织中，通常有许多服务团队负责系统的不同部分。通常情况下，每个团队都需要付出一定的努力来使他们所管理的服务得到充分的工具化。而这些团队都有自己积压的工作，他们当然希望能够优先处理这些工作。\n不幸的是，可观测性计划在开始提供价值和证明其价值之前就会耗尽人的耐心。但通过仔细的计划和协调，这种情况是可以避免的。\n主要目标 在推广 OpenTelemetry 时，重要的是要记住，任何基于分布式追踪的可观测性系统都需要对参与事务的每个服务进行检测，以提供最大价值。如果只有部分服务被检测到，那么追踪就会被分割成小的、不相连的部分。\n这种散乱的仪表的结果是不可取的。这种情况——不一致的仪表和断裂的追踪是你想要避免的主要事情。如果追踪是断开的，运维人员仍然需要在他们的头脑中把所有的东西拼凑起来，以获得他们系统的情况。更糟糕的是，自动分析工具可以使用的数据非常有限。与人类运维人员不同，他们可以运用直觉，跳出框框来思考问题，而分析工具却只能使用他们得到的数据。由于数据有限，他们提供有用的见解的机会也将是有限的。\n为了避免这个陷阱，集中精力对一个工作流程进行检测，在进入下一个工作流程之前对其进行完整的追踪。大多数工作流程并不涉及大系统的每一个部分，所以这种方法将最大限度地减少分析开始和价值实现之前所需的工作量。\n选择一个高价值的目标 谈到实现价值，在开始进行仪表测量工作时，重要的是要有一个有吸引力的目标！这一点很重要。\n最有可能的是，有一个特别的问题促使人们去部署 OpenTelemetry。如果是这样的话，就把重点放在解决该问题所需的最小的推广上。否则，想一想那些众所周知的问题，解决它们就值得公布了。\n目前有哪些痛苦的、长期的问题在困扰着运维？减少系统延迟在哪里可以直接转化为商业价值？当人们问 “为什么这么慢？” 时，他们说的是系统的哪一部分？在选择第一个工作流程时，请以这些信息为指导。\n识别一个有吸引力的目标有两个好处。首先，它可以更有利于说服众人，因为有一个具体的理由来做这项工作。这使得它更容易说服有关团队优先考虑增加指导，并以协调的方式进行。\n第二，速战速决给你的新的可观测性系统一个闪亮的机会。第一次通过分布式追踪的视角来分析一个软件系统时，几乎总是会产生有用的见解。证明可观测性的价值可以引起很多人的兴趣，并有助于降低采用时任何挥之不去的障碍。\n如果直接进入生产是困难的，“生产支持” 系统也是一个好的开始。可以对 CI/CD 系统进行检测，以帮助了解构建和部署的性能。在这里，整个组织都会感受到性能的大幅提升，并可以为将 OpenTelemetry 转移到生产中提供良好的理由。\n集中遥测管理 展开和管理遥测系统从集中化中获益良多。在一些组织中，会有一个平台或信息结构团队可以接触到每一项服务。像这样的团队是集中管理遥测的一个好地方，这可以提供巨大的帮助。遥测管道最好被认为是它自己的系统；允许一个团队操作整个遥测管道，往往比要求许多团队各自拥有系统的一部分要好。\n在软件层面，将 OpenTelemetry 设置与已经广泛部署的代码管理工具——例如共享的启动脚本和应用框架整合起来，减少了每个团队需要管理的代码量。这有助于确保服务与最新的版本和配置保持同步，并使采用更加容易。\n另一个关键工具是一个集中的知识库。OpenTelemetry 有文档，但它是通用的。创建特定于在你的组织内部署、管理和使用 OpenTelemetry 的文档。大多数工程师都是第一次接触 OpenTelemetry 和分布式追踪，这对他们的帮助怎么强调都不为过。\n先广度后深度 还有一个关于合理分配精力的说明。当对一个工作流程进行端对端检测时，通常只需安装 OpenTelemetry 附带的仪表，加上你的组织所使用的任何内部或自创框架的仪表即可。没有必要深入检测应用程序代码，至少在开始时没有必要。OpenTelemetry 附带的标准跨度和指标足以让你识别大多数问题；必要时可以有选择地增加更深层次的检测。在添加这种细节之前，请确保你已经建立并运行了端到端的追踪。\n这就是说，有一个快速的方法可以为你的追踪增加很多细节，那就是把任何现有的日志转换成追踪事件。这可以通过创建一个简单的日志附加器来实现，它可以抓取当前的跨度，并将日志作为一个事件附加到它上面。现在，当你查找追踪时，你所有的应用日志都可以得到，这比在传统的日志工具中寻找它们要容易得多。OpenTelemetry 确实为一些常见的日志系统提供了日志附加程序，但它们也很容易编写。\n与管理层合作 如果你是一个工程师在读这篇文章，我有一个补充说明。推广一个新的遥测系统可能需要组织很多人。幸运的是，有些人已经在做这种组织工作了 —— 经理们！这就是我们的工作。\n但是，如果你想启动其中的一个项目，说服工程或项目经理来帮助你是很好的第一步。他们将对如何完成项目有宝贵的见解，并能在你可能不参加的会议上推销该项目。有时候，组织人比组织代码更难，所以不要害怕寻求帮助！\n加入社区 最后，在个人和组织层面上，考虑加入 OpenTelemetry 社区！维护者和项目负责人都很友好，非常平易近人。社区是一个很好的获取援助和专业知识的资源的地方；我们总是很乐意帮助新的用户得到指导。还有一种汗水文化：如果有你想看到的 OpenTelemetry 功能，加入一个工作组并提供帮助是使它们得到优先考虑的一个好办法。\n至少，一定要给我们反馈。我们从用户那里听到的越多，我们就越能专注于最重要的问题。我们的目标是建立一个推动下一代可观测性的标准。没有你，我们做不到，我们的大门永远是敞开的。\n谢谢你的阅读 我希望你喜欢这份关于 OpenTelemetry 的可观测性的未来的报告。如果你有任何问题、评论、反馈或基于你所读的内容的灵感，请随时在 Twitter 上与我联系，我是 @tedsuo。\n要想获得 OpenTelemetry 的帮助，请加入云原生计算基金会（CNCF）Slack 上的 #OpenTelemetry 频道。我希望能在那里见到你！\n","relpermalink":"/book/opentelemetry-obervability/roll-out/","summary":"第 8 章：如何在组织中推广 OpenTelemetry","title":"第 8 章：如何在组织中推广 OpenTelemetry"},{"content":"在此场景中，你将了解如何使用 TSB 安全设置来限制来自工作区外部的访问。这有助于通过控制服务之间的通信来增强环境的安全性。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户、工作区和配置组。 为团队和用户配置权限。 设置入口网关。 使用可观测性工具检查服务拓扑和指标。 配置流量转移。 部署 sleep 服务 首先，让我们在不属于 bookinfo 应用程序工作区的另一个命名空间中部署“睡眠”服务。这将用于测试安全设置。\n创建以下 sleep.yaml 文件：\nsleep.yaml # Copyright Istio Authors # # Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################################## # Sleep service ################################################################################################## apiVersion: v1 kind: ServiceAccount metadata: name: sleep --- apiVersion: v1 kind: Service metadata: name: sleep labels: app: sleep spec: ports: - port: 80 name: http selector: app: sleep --- apiVersion: apps/v1 kind: Deployment metadata: name: sleep spec: replicas: 1 selector: matchLabels: app: sleep template: metadata: labels: app: sleep spec: serviceAccountName: sleep containers: - name: sleep image: governmentpaas/curl-ssl command: [\u0026#34;/bin/sleep\u0026#34;, \u0026#34;3650d\u0026#34;] imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /etc/sleep/tls name: secret-volume volumes: - name: secret-volume secret: secretName: sleep-secret optional: true --- 根据你的环境（标准 Kubernetes 或 OpenShift），使用适当的命令部署 sleep 服务：\n标准 Kubernetes ```bash kubectl create namespace sleep kubectl label namespace sleep istio-injection=enabled --overwrite=true kubectl apply -n sleep -f sleep.yaml ``` 等待配置传播后，你可以从 sleep 服务 pod 调用 bookinfo 产品页面：\nkubectl exec \u0026#34;$(kubectl get pod -l app=sleep -n sleep -o jsonpath={.items..metadata.name})\u0026#34; -c sleep -n sleep -- curl -s http://productpage.bookinfo:9080/productpage | grep -o \u0026#34;\u0026lt;title\u0026gt;.*\u0026lt;/title\u0026gt;\u0026#34; OpenShift oc create namespace sleep oc label namespace sleep istio-injection=enabled cat \u0026lt;\u0026lt;EOF | oc -n sleep create -f - apiVersion: \u0026#34;k8s.cni.cncf.io/v1\u0026#34; kind: NetworkAttachmentDefinition metadata: name: istio-cni EOF oc adm policy add-scc-to-group anyuid \\ system:serviceaccounts:sleep oc apply -n sleep -f sleep.yaml 等待配置传播后，你可以从 sleep 服务 pod 调用 bookinfo 产品页面：\noc exec \u0026#34;$(oc get pod -l app=sleep -n sleep -o jsonpath={.items..metadata.name})\u0026#34; -c sleep -n sleep -- curl -s http://productpage.bookinfo:9080/productpage | grep -o \u0026#34;\u0026lt;title\u0026gt;.*\u0026lt;/title\u0026gt;\u0026#34; 你应该看到输出：\n\u0026lt;title\u0026gt;Simple Bookstore App\u0026lt;/title\u0026gt; 这表明工作区外部的服务与 bookinfo 应用程序工作区之间的通信是允许的。\n创建安全设置 你可以配置安全设置以限制来自不同工作区或集群的服务之间的通信。在这种情况下，我们将配置同一工作区和集群中的服务之间的通信。\n使用用户界面 在“租户”下，选择“工作区”。 在 bookinfo-ws 工作区卡上，单击“安全组”。 单击你之前创建的 bookinfo-security 安全组。 选择安全设置选项卡。 单击添加新…以默认名称 default-setting 创建新的安全设置。 将新的安全设置重命名为 bookinfo-security-settings 。 展开 bookinfo-security-settings 以显示其他配置：身份验证设置和授权设置。 单击身份验证设置并将流量模式字段设置为必需。 单击授权设置并将模式字段设置为工作空间。 单击保存更改。 使用 tctl 创建以下 security.yaml 文件：\n# ... (The contents of security.yaml) 使用 tctl 应用配置：\ntctl apply -f security.yaml 验证安全设置 等待配置传播后，通过尝试从 sleep 服务访问服务来测试安全设置。\n标准 Kubernetes kubectl exec \u0026#34;$(kubectl get pod -l app=sleep -n sleep -o jsonpath={.items..metadata.name})\u0026#34; -c sleep -n sleep -- curl http://productpage.bookinfo:9080/productpage -v OpenShift oc exec \u0026#34;$(oc get pod -l app=sleep -n sleep -o jsonpath={.items..metadata.name})\u0026#34; -c sleep -n sleep -- curl http://productpage.bookinfo:9080/productpage -v 你应该收到类似于以下内容的输出：\nHTTP/1.1 403 Forbidden ... RBAC: access denied 这表示由于安全设置，从 sleep 服务到 bookinfo 产品页面的通信被拒绝。这确保不允许来自工作区外部的服务访问应用程序工作区内的服务。\n允许访问特定服务 允许访问特定服务安全组，你可以添加 ServiceSecuritySetting 来覆盖该服务的规则。\n使用 tctl 创建以下 service-security.yaml 文件：\napiVersion: security.tsb.tetrate.io/v2 kind: ServiceSecuritySetting metadata: organization: tetrate name: bookinfo-allow-reviews group: bookinfo-security workspace: bookinfo-ws tenant: tetrate spec: service: bookinfo/reviews.bookinfo.svc.cluster.local settings: authenticationSettings: trafficMode: REQUIRED authorization: mode: CLUSTER 使用 tctl 应用配置：\ntctl apply -f service-security.yaml 等待配置传播后，测试从 sleep 服务对评论服务的访问。\n标准 Kubernetes kubectl exec \u0026#34;$(kubectl get pod -l app=sleep -n sleep -o jsonpath={.items..metadata.name})\u0026#34; -c sleep -n sleep -- curl http://reviews.bookinfo:9080/reviews/0 -v OpenShift oc exec \u0026#34;$(oc get pod -l app=sleep -n sleep -o jsonpath={.items..metadata.name})\u0026#34; -c sleep -n sleep -- curl http://reviews.bookinfo:9080/reviews/0 -v 你应该收到成功的响应：\nHTTP/1.1 200 OK ... 这表示允许从 sleep 服务到 bookinfo 评论服务的通信，因为你添加了 ServiceSecuritySetting 来允许访问。\n通过执行这些步骤，你已成功配置 TSB 安全设置以限制来自工作区外部的访问，从而增强环境的安全性。\n","relpermalink":"/book/tsb/quickstart/security/","summary":"在此场景中，你将了解如何使用 TSB 安全设置来限制来自工作区外部的访问。这有助于通过控制服务之间的通信来增强环境的安全性。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例","title":"安全"},{"content":"什么是安全域（Security Domains）？ 安全域（Security Domains） 允许你在配置层次结构的任何位置 - 租户 、工作区 或 安全组 - 创建跨 TSB 层次结构 的配置分组。将 securityDomain 视为可以附加到这些 TSB 资源中的名称，然后可以在 TSB 规则中使用它们。\n一旦资源被标识为具有 securityDomain，就可以在创建规则时将安全域用作源或目标。这允许 Operator 在新对象创建时在 Tetrate Service Bridge (TSB) 层次结构中持续建立一组要求。\n何时应该使用安全域（Security Domains）？ 随着服务网格的扩展，你需要执行的策略数量也会增加。在某些情况下，这些安全控制与你当前的 TSB 层次结构选择不太匹配：你有一组服务、工作区和租户，共享一组公共安全控制。\n安全域（Security Domains）为你提供了创建单一的 授权规则 的能力，该规则可以使用它们共享的 securityDomain 名称来处理多个租户、工作区和安全组。然后，你可以使用简单的 from 和 to 子句创建广泛和包容性的规则，反映高级访问控制意图。\n使用安全域（Security Domains） 注意：下面的示例假设你已经知道如何 创建租户 。\n我们从一个简单的单集群 TSB 部署开始，其中有两个租户 dev_US_East 和 stg_US_East，代表开发和暂存环境。随着我们对 TSB 的使用增加，我们想要添加一个用于冗余的美国西部集群，这将需要我们创建两个新租户：dev_US_West 和 stg_US_West。\n我们将使用安全域（Security Domains）来创建一个简单的、广泛的授权规则，允许来自所有 stg 暂存租户的流量到达所有 dev 开发租户。\n步骤 1 在编辑你的 tenant.yaml 文件中，为新创建的租户添加 dev 和 stg 安全域（Security Domains）\nkind: Tenant metadata: organization: tetrate tenant: dev_US_West spec: displayName: Dev US West securityDomain: organizations/tetrate/securitydomains/dev --- kind: Tenant metadata: organization: tetrate tenant: stg_US_East spec: displayName: Stg US West securityDomain: organizations/tetrate/securitydomains/stg 步骤 2 使用 tctl edit organizationsettings 添加所需的 dev 和 stg 安全域（Security Domains）之间的规则\n在这个示例的 TSB 环境中，我们想要确保流量可以从暂存租户到达开发租户，但流量不能从开发租户到达暂存租户。如果没有安全域（Security Domains），我们需要在每个租户之间创建单独的规则，创建、管理和验证这些规则的复杂性会随着租户数量的增加而增加。使用安全域（Security Domains），我只需要将每个租户与适当的 securityDomain 关联起来。我的授权规则然后将 securityDomain 作为 from 和 to 子句中的目标引用：\nkind: OrganizationSetting metadata: displayName: tetrate-settings name: tetrate-settings organization: tetrate resourceVersion: \u0026#39;\u0026#34;XI8Jtnl6JaE=\u0026#34;\u0026#39; spec: defaultSecuritySetting: authorization: mode: RULES rules: allow: - from: fqn: organizations/tetrate/securitydomains/stg to: fqn: organizations/tetrate/securitydomains/dev displayName: tetrate-settings etag: \u0026#39;\u0026#34;XI8Jtnl6JaE=\u0026#34;\u0026#39; fqn: organizations/tetrate/settings/tetrate-settings 最后一步是可选的，但为了完整起见或者如果你关心现有租户之间的流量存在的话，建议执行此步骤。\n步骤 3 测试新租户之间的行为并编辑你的 tenant.yaml 文件，将现有租户添加到新创建的安全域（Security Domains）\n更新你的 tenant.yaml 文件，将你现有的租户添加到新创建的安全域（Security Domains）中。\nkind: Tenant metadata: organization: tetrate tenant: dev_US_West spec: displayName: Dev US West securityDomain: organizations/tetrate/securitydomains/dev --- kind: Tenant metadata: organization: tetrate tenant: stg_US_East spec: displayName: Stg US West securityDomain: organizations/tetrate/securitydomains/stg 我们取得了什么成就？ 我们成功配置了我们的新 US West 租户，并将它们添加到了它们各自的安全域（Security Domains）dev 和 stg 中。随着我们添加更多的 dev 和 stg 租户对，我们可以将它们与适当的 securityDomain 关联起来。TSB 将自动扩展授权规则，以在所有租户上应用访问控制。\n安全域（Security Domains）的未来是什么？ 早期特性实施 安全域（Security Domains）是 Tetrate Service Bridge 1.6 中的一项新功能。随着我们解锁其他用例和可视化，实现可能会在后续版本中得到扩展。 上面的示例只是展示了使用 安全域（Security Domains） 可以实现的一小部分功能。虽然它们在简化大型环境中创建授权规则的任务方面取得了重大进展，但即使安全域关系也最终可能变得复杂。Tetrate 正在考虑通过 UI 可视化和其他扩展来使安全域更具可伸缩性和更容易准确配置，从而实现丰富的用例：\n如果你正在使用安全域（Security Domains），Tetrate 非常愿意听取你的意见。请通过你的 Tetrate 帐户团队联系我们。\n","relpermalink":"/book/tsb/howto/security-domains/","summary":"学习何时以及如何使用安全域（Security Domains）。","title":"创建安全域"},{"content":"如果你的环境有严格的网络策略，防止两个命名空间之间进行任何未经授权的通信，你可能需要添加一个或多个例外到你的网络策略，以允许 sidecar 与本地 Istio 控制平面之间的通信，以及本地 Istio 控制平面与 TSB 管理平面之间的通信。\n以下信息可用于推导适当的防火墙规则集。\nTSB、控制平面和工作负载之间的通信 TSB 和 Istio 之间 TSB 负载均衡器端口 TSB 负载均衡器（也称为 front-envoy）的默认端口为 8443。此端口值是可配置的。例如，它可以更改为 443。如果更改了默认端口，则通过 front-envoy 通信的所有组件都需要相应调整以匹配用户定义的 front-envoy 端口的值。 源 目标 xcp-edge.istio-system TSB 负载均衡器 IP，端口 9443 oap.istio-system TSB 负载均衡器 IP，端口 8443 或用户定义的 front-envoy 端口 otel-collector.istio-system TSB 负载均衡器 IP，端口 8443 或用户定义的 front-envoy 端口 oap.istio-system Elasticsearch 目标 IP 和端口 (如果使用 Elasticsearch 的演示部署或使用 front-envoy 作为 Elasticsearch 代理，请更改为 TSB 负载均衡器 IP，端口 8443 或用户定义的 front-envoy 端口) k8s 上的 Sidecars 和 Istio 控制平面之间 源 目标 任何应用程序命名空间中的 sidecar 或负载均衡器，或 任何命名空间中的共享负载均衡器以访问 Istio Pilot xDS 服务器。 istiod.istio-system，端口 15012 任何应用程序命名空间中的 sidecar 或负载均衡器，或 任何命名空间中的共享负载均衡器以访问 SkyWalking OAP 指标服务器。 oap.istio-system，端口 11800 任何应用程序命名空间中的 sidecar 或负载均衡器，或 任何命名空间中的共享负载均衡器以访问 SkyWalking OAP 跟踪服务器。 oap.istio-system，端口 9411 VM 上的 Sidecars 和 Istio 控制平面之间 源 目标 VM 上的 sidecar 以访问 Istio Pilot xDS 服务器、SkyWalking OAP 指标服务器、跟踪服务器 VM 网关（vmgateway.istio-system）负载均衡器 IP，\n端口 15443 VM 上的 Sidecars 和 k8s 中的工作负载之间 源 目标 VM 上的 sidecar 以访问 k8s 中的工作负载 要么 k8s pod 直接，要么 VM 网关（vmgateway.istio-system）负载均衡器 IP，\n端口 15443 k8s 中的工作负载和 VM 上的 Sidecars 之间 源 目标 k8s pod 以访问 VM 上的工作负载 VM IP 集群 A 中的工作负载和集群 B 中的工作负载之间 源 目标 k8s pod 或 VM（集群 A） 每个服务网关负载均衡器 IP，端口 15443（集群 B） k8s pod 或 VM（集群 B） 每个服务网关负载均衡器 IP，端口 15443（集群 A） 共享负载均衡器 如果使用共享负载均衡器，则负载均衡器 envoy 需要能够与所有附加的应用程序及其服务通信。由于这些信息事先未知，我们无法确定在防火墙中打开的端口的确切信息。 TSB 组件端口 以下是 TSB 组件使用的端口和协议。\nCert manager 端口 协议 描述 10250 HTTPS Webhooks 服务端口 6080 HTTP 健康检查 管理平面 端口 协议 描述 管理平面 Operator tsb-operator-management-plane.tsb 8383 HTTP Prometheus 遥测 443 HTTPS Webhooks 服务端口 9443 HTTPS Webhook 容器端口，从 443 转发 TSB API 服务器 tsb.tsb 8000 HTTP HTTP API 9080 GRPC GRPC API 42422 HTTP Prometheus 遥测 9082 HTTP 健康检查 Open Telemetry otel-collector.tsb 9090 HTTP Prometheus 遥测 9091 HTTP 收集器端点 13133 HTTP 健康检查 TSB 前端 Envoy envoy.tsb 8443 HTTP/GRPC TSB HTTP 和 GRPC API 端口 9443 TCP XCP 端口 IAM iamserver.tsb 8000 HTTP HTTP API 9080 GRPC GRPC API 42422 HTTP Prometheus 遥测 9082 HTTP 健康检查 MPC mpc.tsb 9080 GRPC GRPC API 42422 HTTP Prometheus 遥测 9082 HTTP 健康检查 OAP oap.tsb 11800 GRPC GRPC API 12800 HTTP REST API 1234 HTTP Prometheus 遥测 9411 HTTP 追踪查询 9412 HTTP 追踪收集 TSB UI web.tsb 8080 HTTP HTTP 服务端口和健康检查 XCP Operator 中心 xcp-operator-central.tsb 8383 HTTP Prometheus 遥测 443 HTTPS Webhooks 服务端口 XCP 中心 central.tsb 8090 HTTP 调试接口 9080 GRPC GRPC API 8080 HTTP Prometheus 遥测 443 HTTPS Webhooks 服务端口 8443 HTTPS Webhook 容器端口，从 443 转发 控制平面 端口 协议 描述 控制平面 Operator tsb-operator-control-plane.istio-system 8383 HTTP Prometheus 遥测 443 HTTPS Webhooks 服务端口 9443 HTTPS Webhook 容器端口，从 443 转发 Open Telemetry otel-collector.tsb 9090 HTTP Prometheus 遥测 9091 HTTP 收集器端点 13133 HTTP 健康检查 OAP oap.istio-system 11800 GRPC GRPC API 12800 HTTP REST API 1234 HTTP Prometheus 遥测 15021 HTTP Envoy sidecar 健康检查 15020 HTTP Envoy sidecar 合并的 Prometheus 遥测，来自 Istio 代理、Envoy 和应用程序 9411 HTTP 追踪查询 9412 HTTP 追踪收集 Istio Operator istio-operator.istio-system 443 HTTPS Webhooks 服务端口 8383 HTTP Prometheus 遥测 Istiod istiod.istio-system 443 HTTPS Webhooks 服务端口 8080 HTTP 调试接口 15010 GRPC XDS 和 CA 服务（明文，仅限安全网络） 15012 GRPC XDS 和 CA 服务（TLS 和 mTLS，推荐用于生产环境） 15014 HTTP 控制平面监控 15017 HTTPS Webhook 容器端口，从 443 转发 XCP Operator 中心 xcp-operator-edge.istio-system 8383 HTTP Prometheus 遥测 443 HTTPS Webhooks 服务端口 XCP 中心 edge.istio-system 8090 HTTP 调试接口 9080 GRPC GRPC API 8080 HTTP Prometheus 遥测 443 HTTPS Webhooks 服务端口 8443 HTTPS Webhook 容器端口，从 443 转发 Onboarding Operator onboarding-operator.istio-system 443 HTTPS Webhooks 服务端口 9443 HTTPS Webhook 容器端口，从 443 转发 9082 HTTP 健康检查 Onboarding 仓库 onboarding-repository.istio-system 8080 HTTP HTTP 服务端口 9082 HTTP 健康检查 Onboarding 平面 onboarding-plane.istio-system 8443 HTTP Onboarding API 9082 HTTP 健康检查 VM 网关 vmgateway.istio-system 15021 HTTP 健康检查 15012 HTTP Istiod 11800 HTTP OAP 指标 9411 HTTP 追踪 15443 HTTPS mTLS 流量端口 443 HTTPS HTTPS 端口 数据平面 端口 协议 描述 数据平面 Operator tsb-operator-data-plane.istio-gateway 8383 HTTP Prometheus 遥测 443 HTTPS Webhooks 服务端口 9443 HTTPS Webhook 容器端口，从 443 转发 Istio Operator istio-operator.istio-gateway 443 HTTPS Webhooks 服务端口 8383 HTTP Prometheus 遥测 Istiod istiod.istio-gateway 443 HTTPS Webhooks 服务端口 8080 HTTP 调试接口 15010 GRPC XDS 和 CA 服务（明文，仅限安全网络） 15012 GRPC XDS 和 CA 服务（TLS 和 mTLS，推荐用于生产环境） 15014 HTTP 控制平面监控 15017 HTTPS Webhook 容器端口，从 443 转发 Sidecars 参考 Istio 使用的端口 查看 Istio Sidecar 代理使用的端口和协议列表。\n","relpermalink":"/book/tsb/setup/firewall-information/","summary":"防火墙规则指南。","title":"防火墙信息"},{"content":"本文描述了如何将所有配置、用户和组从一个组织 迁移到一个新创建的组织。\n获取数据 首先，提取每个租户的所有配置。对于每个租户，执行以下命令：\ntctl get all --tenant \u0026lt;tenant\u0026gt; \u0026gt; config.yaml 一旦你将所有配置保存在 config.yaml 中，请确保手动复制其中的各种绑定（例如 ApplicationAccessBindings、APIAccessBindings 等）到一个名为 bindings.yaml 的文件中，并从 config.yaml 中删除它们。\n这是因为当你稍后使用 config.yaml 的内容时，绑定的全限定名称将不存在，这将导致在应用配置时出现错误。绑定必须在将对象移动到新组织后才能应用。\n你还需要编辑 config.yaml，将文件中每个 metadata.organization 字段中的值替换为指向你将要创建的新组织。\n如果你正在创建一个新的租户，你还应该更改租户部分。\n以下是一个示例 YAML 文件，显示了你的 config.yaml 中的一个条目应该是什么样子的。还请注意，下面的 YAML 文件不包含全限定名称、etag 和 resourceVersion。你还应该从你的 config.yaml 中删除这些内容。\napiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: displayName: Bookinfo app name: bookinfo organization: ${myorg} tenant: ${mytenant} spec: displayName: Bookinfo app namespaceSelector: names: - \u0026#39;*/bookinfo\u0026#39; 应用配置 编辑 config.yaml 后，你需要创建新的组织。创建一个名为 myorg.yaml 的文件，其中包含以下内容，将名称替换为你的新组织名称：\napiVersion: api.tsb.tetrate.io/v2 kind: Organization metadata: name: \u0026lt;myorg\u0026gt; 然后应用新的配置以创建该组织。\ntctl apply -f myorg.yaml 对于你旧组织中的每个租户，你需要在新组织中创建一个等效的租户。创建一个包含所需租户的文件。内容应该类似于以下示例，其中组织和租户名称已替换为你环境中的有效值。\napiVersion: api.tsb.tetrate.io/v2 kind: Tenant metadata: organization: \u0026lt;myorg\u0026gt; name: \u0026lt;mytenant\u0026gt; 然后应用新的配置以创建新的租户（们）。示例假定你在文件 mytenants.yaml 中列出了所有必要的租户。\ntctl apply -f mytenants.yaml 最后，应用存储在你之前编辑的 config.yaml 文件中的配置：\ntctl apply -f config.yaml 此时，旧组织和新组织都将存在，但只有旧组织将工作，因为你尚未更新管理平面中的配置以指向新组织。\n载入集群 创建一个名为 clusters.yaml 的文件，其内容类似于以下示例，将集群名称和组织替换为你环境中的有效值。为所有应该属于新组织的集群添加更多条目。\napiVersion: api.tsb.tetrate.io/v2 kind: Cluster metadata: name: \u0026lt;cluster\u0026gt; organization: \u0026lt;myorg\u0026gt; labels: env: qa tier: one spec: displayName: \u0026#34;Cluster T1\u0026#34; network: tier1 tier1Cluster: true 然后应用配置。这将将集群与新组织关联起来。\ntctl apply -f clusters.yaml 创建一个名为 controlplane.yaml 的文件，其内容类似于以下示例，将集群名称替换为你环境中的有效值。\napiVersion: install.tetrate.io/v1alpha1 kind: ControlPlane metadata: name: controlplane namespace: istio-system spec: hub: ... telemetryStore: ... managementPlane: host: ... port: ... clusterName: \u0026lt;mycluster\u0026gt; 同步用户/组 此时，你应该已经将所有集群和控制平面迁移到新组织。现在，你需要将用户和组同步到新组织。为此，创建一个作业，如下所示：\nkubectl create job --from=cronjob/teamsync teamsync -n tsb 一段时间后，作业完成后，你将能够从 TSB UI 中查看新组织中的用户和组。\n确保从绑定中删除 tetrate-agents。从 bindings.yaml 中的每个绑定中删除下面示例中显示的部分：\n- role: rbac/envreader subjects: - team: organizations/\u0026lt;myorg\u0026gt;/teams/tetrate-agents 然后，在完成此操作后，应用这些绑定：\ntctl apply -f bindings.yaml 迁移组织 此时，你已将所有内容迁移到新组织，但管理平面仍然配置为使用旧组织。\n创建一个名为 managementplane.yaml 的文件，并指向它以使用新组织：\napiVersion: install.tetrate.io/v1alpha1 kind: ManagementPlane metadata: name: managementplane namespace: tsb spec: hub: ... organization: \u0026lt;myorg\u0026gt; dataStore: ... telemetryStore: ... tokenIssuer: ... 现在是一个好时机，确保你没有错误配置任何内容，因为应用此配置可能导致你的应用断开连接并被关闭。\n确保旧组\n织中没有尚未应用到新创建组织的缺少配置。例如，如果你配置了 tier1-tier2，则需要明确允许网络从 tier1 到 tier2 进行通信。\n一旦你满意，应用新的配置：\nkubectl apply -f managementplane.yaml 最后，使用新组织登录到 TSB：\ntctl login 一旦你确认一切正常工作，你可以继续删除旧的工作区、租户和组织。\n验证用户与旧组织的关联 如果你已配置了与 TSB 一起使用的外部 LDAP，并且用户仍然在旧组织中进行验证，你需要手动修复存储在 Postgres 中的数据。如果你按照本文档提供的确切顺序执行了步骤，这不应该发生。\n如果你需要修复 Postgres，请确保首先备份数据库。准备好后，从 Postgres 命令行发出以下命令，将 \u0026lt;your_old_org\u0026gt; 替换为旧组织的名称：\ndelete from node where name like \u0026#39;%\u0026lt;your_old_org\u0026gt;%\u0026#39;; 这将删除所需的表，并通过外键也会删除其他相关数据。\n","relpermalink":"/book/tsb/operations/migrate-organization/","summary":"将所有配置、用户和组从一个组织迁移到一个新创建的组织。","title":"将数据迁移到新的组织"},{"content":"Egress Gateway 充当流出网格的流量网关。用户可以定义允许通过网关向外部服务发送流量的服务。\n目前只能发送外部的 HTTPS 流量。但是，原始的出站请求应使用 HTTP。这些出站 HTTP 请求会转换为 HTTPS 请求并发送到外部服务。例如，从通过 Egress Gateway 经过的服务发送到 http://tetrate.io 的请求会被转换为对 https://tetrate.io 的请求，并代表原始服务进行代理。目前不支持最终需要是 HTTP 的请求。例如，如果你的最终目的地是 http://tetrate.io，则无法使用 Egress Gateway。\n本文将描述如何配置 Egress Gateway，允许服务只能向特定服务发送出站请求。以下图示显示了在使用 Egress Gateway 时的请求和响应流程：\n在开始之前，请确保你已经：\n熟悉 TSB 概念 安装了 TSB 环境。你可以使用 TSB 演示 进行快速安装。 完成了 TSB 快速入门 。本文假定你已经创建了租户，并熟悉 Workspace 和 Config Group。还需要将 tctl 配置到你的 TSB 环境中。 请注意，在以下示例中，你将在使用 TSB 演示安装创建的演示集群中部署 Egress Gateway。如果你使用其他集群，请相应更改示例中的集群名称。\n部署 Sleep 服务 在本示例中，你将使用两个 sleep 服务，它们各自位于不同的命名空间中。\n创建命名空间 sleep-one 和 sleep-two：\nkubectl create namespace sleep-one kubectl create namespace sleep-two 然后按照“在 TSB 中安装 sleep Workload” 文档中的说明，在 demo 集群中安装两个 sleep 服务。将服务 sleep-one 安装在命名空间 sleep-one 中，将服务 sleep-two 安装在命名空间 sleep-two 中。\n你无需创建 Workspace，因为你将在本示例中稍后创建。\n为 Sleep 服务创建 Workspace 和 Traffic Group 你将需要一个 Traffic Group 来与稍后创建的 Egress Gateway 关联。由于 Traffic Group 属于 Workspace，你还需要创建一个 Workspace。\n创建一个名为 sleep-workspace.yaml 的文件，其内容如下。根据需要替换 cluster、organization 和 tenant 的值。在演示安装中，你可以为 cluster 使用值 demo，对于 organization 和 tenant，都可以使用值 tetrate。\napiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: \u0026lt;organization\u0026gt; tenant: \u0026lt;tenant\u0026gt; name: sleep spec: displayName: Sleep Workspace namespaceSelector: names: - \u0026#34;\u0026lt;cluster\u0026gt;/sleep-one\u0026#34; - \u0026#34;\u0026lt;cluster\u0026gt;/sleep-two\u0026#34; --- apiVersion: traffic.tsb.tetrate.io/v2 kind: Group metadata: organization: \u0026lt;organization\u0026gt; tenant: \u0026lt;tenant\u0026gt; workspace: sleep name: sleep-tg spec: displayName: Sleep Traffic namespaceSelector: names: - \u0026#34;\u0026lt;cluster\u0026gt;/sleep-one\u0026#34; - \u0026#34;\u0026lt;cluster\u0026gt;/sleep-two\u0026#34; configMode: BRIDGED 使用以下命令应用：\ntctl -f sleep-workspace.yaml 部署 Egress Gateway 创建 Egress Gateway 命名空间 通常，Egress Gateway 由与开发应用程序不同的团队管理（在本例中是 sleep 服务），以避免混淆所有权。\n在本示例中，我们创建一个名为 egress 的单独命名空间来管理 Egress Gateway。执行以下命令创建新命名空间：\nkubectl create namespace egress 部署 Egress Gateway 创建一个名为 egress-deploy.yaml 的文件，其内容如下：\napiVersion: install.tetrate.io/v1alpha1 kind: EgressGateway metadata: name: cluster-egress namespace: egress spec: kubeSpec: service: type: NodePort 使用 kubectl 应用：\nkubectl apply -f egress-deploy.yaml 为 Egress Gateway 创建 Workspace 和 Gateway Group 你还需要为刚刚创建的 Egress Gateway 创建一个 Workspace 和 Gateway Group。\n创建一个名为 egress-workspace.yaml 的文件，其内容如下。根据需要替换 cluster、organization 和 tenant 的值。在演示安装中，你可以为 cluster 使用值 demo，对于 organization 和 tenant，都可以使用值 tetrate。\napiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: \u0026lt;organization\u0026gt; tenant: \u0026lt;tenant\u0026gt; name: egress spec: displayName: Egress Workspace namespaceSelector: names: - \u0026#34;\u0026lt;cluster\u0026gt;/egress\u0026#34; --- apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: organization: \u0026lt;organization\u0026gt; tenant: \u0026lt;tenant\u0026gt; workspace: egress name: egress-gw spec: displayName: Egress Gateway namespaceSelector: names: - \u0026#34;\u0026lt;cluster\u0026gt;/egress\u0026#34; configMode: BRIDGED 使用 tctl 应用：\ntctl apply -f egress-workspace.yaml 配置 Egress Gateway 在本示例中，你将对两个 sleep 服务应用不同的配置。\nsleep-one 将被配置为可以访问所有外部 URL，但 sleep-two 只允许访问单\n一目标（在此示例中为 “edition.cnn.com”）。\n创建一个名为 egress-config.yaml 的文件，其内容如下。根据需要替换 organization 和 tenant 的值。在演示安装中，你可以为 organization 和 tenant 使用值 tetrate。\napiVersion: gateway.tsb.tetrate.io/v2 kind: EgressGateway metadata: organization: \u0026lt;organization\u0026gt; tenant: \u0026lt;tenant\u0026gt; workspace: egress group: egress-gw name: cluster-egress spec: workloadSelector: namespace: egress labels: app: cluster-egress authorization: - from: mode: CUSTOM serviceAccounts: [\u0026#34;sleep-one/sleep\u0026#34;] to: [\u0026#34;*\u0026#34;] - from: mode: CUSTOM serviceAccounts: [\u0026#34;sleep-two/sleep\u0026#34;] to: [\u0026#34;edition.cnn.com\u0026#34;] 使用 tctl 应用：\ntctl apply -f egress-config.yaml 创建 TrafficSettings 以使用 Egress Gateway 最后，创建 TrafficSettings，将服务关联到 Traffic Group，并与刚刚创建的 Egress Gateway 关联。\n创建一个名为 sleep-traffic-setting-egress.yaml 的文件，其内容如下。根据需要替换 organization 和 tenant 的值。在演示安装中，你可以为 organization 和 tenant 使用值 tetrate。\nhost 值的格式为 \u0026lt;namespace\u0026gt;/\u0026lt;fqdn\u0026gt;。fqdn 值是从前面步骤中指定的 namespace 和 metadata.name 值派生的：\napiVersion: traffic.tsb.tetrate.io/v2 kind: TrafficSetting metadata: organization: \u0026lt;organization\u0026gt; tenant: \u0026lt;tenant\u0026gt; workspace: sleep group: sleep-tg name: sleep-traffic-settings spec: egress: host: egress/cluster-egress.egress.svc.cluster.local 使用 tctl 应用：\ntctl apply -f sleep-traffic-setting-egress.yaml 测试 要测试 Egress Gateway 是否正常工作，你将从 sleep 服务发送请求到外部服务。\n为此，你需要找出 sleep-one 和 sleep-two 的 Pod 名称。执行以下命令查找 Pod 名称：\nexport SLEEP_ONE_POD=$(kubectl get pod -n sleep-one -l app=sleep -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;) export SLEEP_TWO_POD=$(kubectl get pod -n sleep-two -l app=sleep -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39;) 对 sleep-one 执行以下命令。由于你已经配置了 Egress Gateway，使得 sleep-one 允许访问所有外部服务，因此以下命令应该都显示 “200”：\nkubectl exec ${SLEEP_ONE_POD} -n sleep-one -c sleep -- \\ curl http://twitter.com \\ -s \\ -o /dev/null \\ -L \\ -w \u0026#34;%{http_code}\\n\u0026#34; kubectl exec ${SLEEP_ONE_POD} -n sleep-one -c sleep -- \\ curl http://github.com \\ -s \\ -o /dev/null \\ -L \\ -w \u0026#34;%{http_code}\\n\u0026#34; kubectl exec ${SLEEP_ONE_POD} -n sleep-one -c sleep -- \\ curl http://edition.cnn.com \\ -s \\ -o /dev/null \\ -L \\ …","relpermalink":"/book/tsb/howto/gateway/egress-gateways/","summary":"使用 Egress Gateway 配置对外部服务的访问。","title":"控制对外部服务的访问"},{"content":" 服务网格与 GitOps\nTSB 常见问题解答\n","relpermalink":"/book/tsb/knowledge-base/","summary":"服务网格与 GitOps TSB 常见问题解答","title":"知识库和 FAQ"},{"content":"Argo Rollouts 将始终响应 Rollouts 资源的更改，无论更改是如何进行的。这意味着 Argo Rollouts 与你可能用来管理部署的所有模板解决方案兼容。\nArgo Rollouts 清单可以使用 Helm 包管理器 进行管理。如果你的 Helm Chart 包含 Rollout 资源，那么一旦你安装或升级 Chart，Argo Rollouts 将接管并启动渐进式交付流程。\n以下是使用 Helm 管理的 Rollout 示例：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: {{ template \u0026#34;helm-guestbook.fullname\u0026#34; . }} labels: app: {{ template \u0026#34;helm-guestbook.name\u0026#34; . }} chart: {{ template \u0026#34;helm-guestbook.chart\u0026#34; . }} release: {{ .Release.Name }} heritage: {{ .Release.Service }} spec: replicas: {{ .Values.replicaCount }} revisionHistoryLimit: 3 selector: matchLabels: app: {{ template \u0026#34;helm-guestbook.name\u0026#34; . }} release: {{ .Release.Name }} strategy: blueGreen: activeService: {{ template \u0026#34;helm-guestbook.fullname\u0026#34; . }} previewService: {{ template \u0026#34;helm-guestbook.fullname\u0026#34; . }}-preview template: metadata: labels: app: {{ template \u0026#34;helm-guestbook.name\u0026#34; . }} release: {{ .Release.Name }} spec: containers: - name: {{ .Chart.Name }} image: \u0026#34;{{ .Values.image.repository }}:{{ .Values.image.tag }}\u0026#34; imagePullPolicy: {{ .Values.image.pullPolicy }} ports: - name: http containerPort: 80 protocol: TCP livenessProbe: httpGet: path: / port: http readinessProbe: httpGet: path: / port: http resources: {{ toYaml .Values.resources | indent 12 }} {{- with .Values.nodeSelector }} nodeSelector: {{ toYaml . | indent 8 }} {{- end }} {{- with .Values.affinity }} affinity: {{ toYaml . | indent 8 }} {{- end }} {{- with .Values.tolerations }} tolerations: {{ toYaml . | indent 8 }} {{- end }} 你可以在此 找到完整示例。\n","relpermalink":"/book/argo-rollouts/rollout/helm/","summary":"Argo Rollouts 将始终响应 Rollouts 资源的更改，无论更改是如何进行的。这意味着 Argo Rollouts 与你可能用来管理部署的所有模板解决方案兼容。 Argo Rollouts 清单可以使用 Helm 包管理器 进行管理。如果你的 Helm Chart 包含 Rollout 资源，那么一旦你安装或升级 Chart，A","title":"将 Argo Rollouts 和 Helm 一起使用"},{"content":"🔔 提示：自 Argo Rollouts v1.2 起提供了多个 trafficRouting。\n多个提供程序的使用旨在涵盖一些情况，例如我们必须在南北和东西流量路由或需要使用多个提供程序的任何混合架构的情况下。\n何时可以使用多个提供程序的示例 避免在 Ingress 控制器上注入 Sidecars 这是服务网格的常见要求，通过使用多个 trafficRoutings，你可以利用南北交通转移到 NGiNX 和西东交通转移到 SMI，避免将 Ingress 控制器添加到网格中。\n避免操作 Ingress 中的主机 Header 将一些 Ingress 控制器添加到网格中的另一个常见副作用是使用这些网格主机标头将其指向网格主机名以进行路由。\n避免大爆炸 这发生在存在的机群中，其中停机时间非常短或几乎不可能。为了避免大爆炸采用 ，使用多个提供程序可以缓解团队如何逐步实施新技术。例如，现有机群正在使用提供程序（例如大使），已经在其推出的一部分中使用金丝雀方式进行南北方向的金丝雀测试，可以逐渐实现更多提供程序，例如 Istio，SMI 等。\n混合方案 在这种情况下，它非常类似于避免大爆炸，无论是作为平台路线图的一部分还是架构的新重新设计，都有多种情况需要使用多个 trafficRoutings 的能力：逐步实施，简化架构回滚，甚至为了回退。\n要求 使用多个提供程序要求两个提供程序分别独立地符合其最低要求。例如，如果要使用 Nginx 和 SMI，则需要同时安装 SMI 和 Nginx，并为两者生成推出配置。\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollouts-demo spec: strategy: canary: canaryService: rollouts-demo-canary # 引用指向金丝雀 ReplicaSet 的 Service stableService: rollouts-demo-stable # 引用指向稳定 ReplicaSet 的 Service trafficRouting: nginx: stableIngress: rollouts-demo-stable # 引用指向稳定 Service 的 Ingress，以便进行 NGINX 流量分离 smi: {} ","relpermalink":"/book/argo-rollouts/traffic-management/mixed/","summary":"🔔 提示：自 Argo Rollouts v1.2 起提供了多个 trafficRouting。 多个提供程序的使用旨在涵盖一些情况，例如我们必须在南北和东西流量路由或需要使用多个提供程序的任何混合架构的情况下。 何时可以使用多个提供程序的示","title":"多提供方"},{"content":"什么是实验 CRD？ 实验 CRD 允许用户对一个或多个 ReplicaSet 进行短暂运行。除了运行短暂 ReplicaSet 外，实验 CRD 还可以在 ReplicaSet 旁边启动 AnalysisRuns。通常，这些 AnalysisRun 用于确认新的 ReplicaSet 是否按预期运行。\n如果设置了权重（需要流量路由）或该实验的 Service 属性，则还会生成一个服务，用于将流量路由到实验 ReplicaSet。\n实验用例 用户想要运行应用程序的两个版本以进行 Kayenta 风格的分析来启用。实验 CRD 基于实验的 spec.templates 字段创建 2 个 ReplicaSet（基线和金丝雀），并等待两者都健康。经过一段时间后，实验会缩小 ReplicaSet 的规模，用户可以开始 Kayenta 分析运行。 用户可以使用实验来启用 A/B/C 测试，通过为不同版本的应用程序启动多个实验来进行长时间测试。每个实验都有一个 PodSpec 模板，定义用户要运行的特定版本。实验允许用户同时启动多个实验，并保持每个实验的独立性。 使用不同的标签启动现有应用程序的新版本，以避免从 Kubernetes 服务中接收流量。用户可以在继续 Rollout 之前在新版本上运行测试。 实验规范 以下是创建两个具有 1 个副本的 ReplicaSet 并在两者可用后运行它们 20 分钟的实验示例。此外，还运行了多个 AnalysisRun 以针对实验的 pod 进行分析。\napiVersion: argoproj.io/v1alpha1 kind: Experiment metadata: name: example-experiment spec: # 实验持续时间，从所有 ReplicaSet 变为健康状态开始（可选） # 如果省略，将无限期运行，直到终止或所有标记为“requiredForCompletion”的分析都完成。 duration: 20m # 一个 ReplicaSet 应在其中取得进展的截止时间（以秒为单位）。 # 如果超过，则实验将失败。 progressDeadlineSeconds: 30 # 要在实验中运行的 Pod 模板规范列表，作为 ReplicaSets templates: - name: purple # 要运行的副本数（可选）。如果省略，将运行单个副本 replicas: 1 # 创建此实验的服务标志（可选） # 如果未指定，则不会创建服务。 service: # 服务名称（可选）。如果省略，则 service: {} 也可以接受。 name: service-name selector: matchLabels: app: canary-demo color: purple template: metadata: labels: app: canary-demo color: purple spec: containers: - name: rollouts-demo image: argoproj/rollouts-demo:purple imagePullPolicy: Always ports: - name: http containerPort: 8080 protocol: TCP - name: orange replicas: 1 minReadySeconds: 10 selector: matchLabels: app: canary-demo color: orange template: metadata: labels: app: canary-demo color: orange spec: containers: - name: rollouts-demo image: argoproj/rollouts-demo:orange imagePullPolicy: Always ports: - name: http containerPort: 8080 protocol: TCP # 要在实验期间执行的 AnalysisTemplate 引用列表 analyses: - name: purple templateName: http-benchmark args: - name: host value: purple - name: orange templateName: http-benchmark args: - name: host value: orange - name: compare-results templateName: compare # 如果对于分析引用设置了 requiredForCompletion 为 true，则在此分析完成之前，实验不会完成 requiredForCompletion: true args: - name: host value: purple 实验生命周期 实验旨在临时运行一个或多个模板。实验的生命周期如下：\n为 spec.templates 下指定的每个 Pod 模板创建并扩展一个 ReplicaSet。如果在一个 Pod 模板下指定了 service，则还会为该 Pod 创建一个服务。 等待所有 ReplicaSet 达到完全可用性。如果 ReplicaSet 在 spec.progressDeadlineSeconds 内未变为可用，则实验将失败。一旦可用，实验将从“挂起”状态转换为“运行”状态。 一旦实验被视为“运行中”，它将为 spec.analyses 下引用的每个 AnalysisTemplate 开始一个 AnalysisRun。 如果在 spec.duration 下指定了持续时间，则实验将等待持续时间结束，然后完成实验。 如果 AnalysisRun 失败或出错，则实验将过早结束，状态等于不成功的 AnalysisRun（即“失败”或“错误”）。 如果其中一个引用的 AnalysisTemplates 被标记为 requiredForCompletion：true，则实验将不会在这些 AnalysisRuns 完成之前完成，即使超过实验持续时间。 如果未指定 spec.duration 或 requiredForCompletion：true，则实验将无限期运行，直到显式终止（通过设置 spec.terminate：true）。 一旦实验完成，ReplicaSets 将缩小到零，并终止任何未完成的 AnalysisRuns。 🔔 注意：ReplicaSet 名称是通过将实验名称与模板名称组合而成的。\n与 Rollouts 集成 使用金丝雀策略的 Rollout 可以使用 experiment 步骤创建一个实验。实验步骤作为 Rollout 的阻塞步骤，只有当实验成功时，Rollout 才会继续。Rollout 会使用 Rollout 实验步骤中的配置创建实验。如果实验失败或出错，则 Rollout 将中止。\n🔔 注意：实验名称是通过将 Rollout 的名称、新 ReplicaSet 的 PodHash、Rollout 的当前版本和当前步骤索引组合而成的。\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: guestbook labels: app: guestbook spec: ... strategy: canary: steps: - experiment: duration: 1h templates: - name: baseline specRef: stable - name: canary specRef: canary analyses: - name : mann-whitney templateName: mann-whitney args: - name: baseline-hash value: \u0026#34;{{templates.baseline.podTemplateHash}}\u0026#34; - name: canary-hash value: \u0026#34;{{templates.canary.podTemplateHash}}\u0026#34; apiVersion: [argoproj.io/v1alpha1](\u0026lt;http://argoproj.io/v1alpha1\u0026gt;) kind: Rollout metadata: name: guestbook labels: app: guestbook spec: ... strategy: canary: trafficRouting: alb: ingress: ingress ... steps: - experiment: duration: 1h templates: - name: experiment-baseline specRef: stable weight: 5 - name: experiment-canary specRef: canary weight: 5 在上面的示例中，在 Rollout 的更新期间，Rollout 将启动一个实验。实验将创建两个 ReplicaSets：baseline 和 canary，每个 ReplicaSet 都有一个副本，并将运行一个小时。baseline 模板使用稳定 ReplicaSet 的 PodSpec，而 canary 模板使用金丝雀 ReplicaSet 的 PodSpec。\n此外，实验将使用名为 mann-whitney 的 AnalysisTemplate 进行分析。AnalysisRun 会提供基线和金丝雀的 pod-hash 详细信息，以执行必要的指标查询，使用 {{templates.baseline.podTemplateHash}} 和 {{templates.canary.podTemplateHash}} 变量。\n🔔 注意：实验的 baseline/canary ReplicaSets 创建的 pod-hash 值与 Rollout 创建的 stable/canary ReplicaSets 的 pod-hash 值不同。这是有意行为，以便允许对实验的 pod 进行分隔和单独查询指标，而不是和 Rollout 的 pod 混淆。\n带流量路由的加权实验步骤 🔔 重要提醒：从 v1.1 开始可用\n使用金丝雀策略和流量路由的 Rollout 可以将流量以细粒度的方式分配到实验堆栈中。启用流量路由时，Rollout 实验步骤允许将流量转移到实验 pod。\n🔔 注意：目前，此功能仅适用于 SMI、ALB 和 Istio 流量路由器。\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: guestbook labels: app: guestbook spec: ... strategy: canary: trafficRouting: alb: ingress: ingress ... steps: - experiment: duration: 1h templates: - name: experiment-baseline specRef: stable weight: 5 - name: experiment-canary specRef: canary weight: 5 在上面的示例中，在更新期间，第一步将启动基线与金丝雀实验。当 Pod 准备就绪时（实验进入运行阶段），Rollout 将将 5% 的流量分配到 experiment-canary，并将 5% 的流量分配到 experiment-baseline，使其余 90% 的流量留给旧堆栈。\n!!! note 当使用带有流量路由的加权实验步骤时，将为每个实验模板自动创建服务。流量路由器使用此服务将流量发送到实验 pod。\n默认情 …","relpermalink":"/book/argo-rollouts/experiment/","summary":"什么是实验 CRD？ 实验 CRD 允许用户对一个或多个 ReplicaSet 进行短暂运行。除了运行短暂 ReplicaSet 外，实验 CRD 还可以在 ReplicaSet 旁边启动 AnalysisRuns。通常，这些 AnalysisRun 用于确认新的 ReplicaSet 是否按预期运行。 如果设置了权重（需要流量路由）","title":"实验 CRD"},{"content":"本章将 SPIFFE 与其他解决类似问题的技术进行了比较。\n简介 SPIFFE 和 SPIRE 所解决的问题并不新鲜。每一个分布式系统都必须有某种形式的身份认证才是安全的。网络公钥基础设施、Kerberos/Active Directory、OAuth、秘密存储和服务网格就是例子。\n然而，这些现有的身份识别形式并不适合用于识别组织内的内部服务。网络 PKI 的实施具有挑战性，对于典型的内部部署来说也是不安全的。Kerberos，Active Directory 的认证组件，需要一个永远在线的票证授予服务器，并且没有任何同等的证明。服务网格、秘密管理器和覆盖网络都解决了服务身份的部分难题，但并不完整。SPIFFE 和 SPIRE 是目前服务身份问题的唯一完整解决方案。\n网络公钥基础设施 网络公钥基础设施（Web PKI）是广泛使用的从我们的网络浏览器连接到安全网站的方法。它利用 X.509 证书来断言用户正在连接到他们打算访问的网站。由于你可能对这种模式很熟悉，所以有理由问：为什么我们不能在我们的组织内使用 Web PKI 进行服务识别？\n在传统的 Web PKI 中，证书的发放和更新完全是手工操作。这些手工过程不适合现代基础设施，因为在现代基础设施中，服务实例可能随时动态地增长和缩小。然而，在过去的几年里，网络 PKI 已经转向了一种自动的证书颁发和更新过程，称为域名验证（Domain Validation）。\n在域名验证中，证书颁发机构向证书请求者发送一个令牌。证书请求者使用 HTTP 服务器共享这个令牌。证书颁发机构访问该令牌，对其进行验证，然后签署该证书。\n这种安排的第一个问题是，内部服务经常没有单独的 DNS 名称或 IP 地址。如果你想在所有服务之间进行相互的 TLS，那么即使是客户端也需要 DNS 名称来获得证书，这对配置来说是个挑战。为在一台主机上运行的多个服务分配身份需要单独的 DNS 名称，这对配置来说也是一个挑战。\n一个更微妙的问题是，任何能够成功响应请求的人都可以成功获得证书。这可能是在同一台服务器上运行的不同服务，甚至是在可以篡改本地二层网络的不同服务器上。\n一般来说，虽然网络 PKI 对互联网上的安全网站很有效，但它并不适合用于服务身份。许多需要证书的内部服务并没有 DNS 名称。如果攻击者成功渗透到本地网络的任何服务中，目前可用的做证书验证的过程很容易被破坏。\nActive Directory（AD）和 Kerberos Kerberos 是一个认证协议，最初于 20 世纪 80 年代末在 MIT 开发。最初，它被设计为允许使用一个集中的用户数据库进行人对服务的认证。后来，Kerberos 被扩展到支持服务对服务的认证，以及除了用户账户之外还可以使用机器账户。Kerberos 协议本身是与帐户数据库无关的。然而，Kerberos 最常见的用法是在 Windows 域中进行认证，使用 Active Directory (AD) 作为账户数据库。\nKerberos 的核心凭证被称为 票据（ticket）。一个票据是一个可以被单个客户用来访问单个资源的凭证。客户端通过调用 Ticket Granting Service (TGS) 获得票据。客户端在访问每一个资源时都需要一个新的票据。这种设计导致了更多的聊天协议并降低了可靠性。\n所有服务都与 TGS 建立了信任关系。当一个服务在 TGS 注册时，它与 TGS 共享密钥材料，如对称秘密或公钥。TGS 使用密钥材料来创建票据，以验证对该服务的访问。轮换密钥材料需要服务和 TGS 之间的协调。服务必须接受以前的密钥材料，并保持对它的了解，以便现有的票据保持有效。\nSPIRE 如何缓解 Kerberos 和 AD 的弊端 在 SPIRE 中，每个客户端和资源将调用 SPIRE 服务器一次，以获得其凭证（SVID），所有资源都可以在信任域（和联合信任域）中验证这些凭证，而无需再调用 SPIRE 服务器。SPIRE 的架构避免了为每个需要访问的资源获取新凭证的所有开销。\n基于 PKI 的认证机制，如 SPIRE，使凭证轮换更简单，因为这种服务和集中式验证器之间的密钥材料协调并不存在。\n最后，值得注意的是，Kerberos 协议将服务与主机名紧密结合在一起，这使得每个主机和集群的多个服务变得复杂。另一方面，SPIRE 很容易支持每个工作负载和集群的多个 SVID。也有可能将同一个 SVID 分配给多个工作负载。这些特性提供了一个强大的、高度可扩展的身份识别方法。\nOAuth 和 OpenID Connect（OIDC） OAuth 是一个旨在实现访问 ** 委托（delegation）** 协议，而不一定是作为一个实现认证本身的协议。OIDC 的主要目的是允许人类允许一个二级网站（或移动应用程序）代表他们对不同的一级网站采取行动。在实践中，该协议能够在二级网站上对用户进行认证，因为被委托的访问凭证（OAuth 协议中的访问令牌）是来自一级网站的证明，即用户针对该网站进行了认证。\n如果主网站包括用户信息或提供了一种使用访问令牌检索用户信息的方法，那么二级网站可以使用主网站的令牌来验证用户。OpenID Connect 是 OAuth 的一种观点，是一个很好的例子。\nOAuth 是为人类设计的，而不是为非人类实体设计的。OAuth 的登录过程需要浏览器的重定向与交互式密码。OAuth 2.0 与其前身相似，包括对非人类实体的支持，通常是通过创建服务账户（即代表工作负载而不是人类的用户身份）。当一个工作负载想要获得 OAuth 访问令牌以访问远程系统时，它必须使用 OAuth 客户端的秘密、密码或刷新令牌来验证 OAuth 提供者并接收访问令牌。工作负载都应该有独立的凭证，以实现工作负载身份的高度精细化。对于弹性计算来说，这些凭证的管理很快就会变得复杂和困难，因为每个工作负载和身份都必须向 OAuth 提供商注册。当秘密必须被撤销时，长期存在的秘密会带来更多的复杂性。由于轮换，秘密在环境中的传播减少了基础设施的流动性，在某些情况下，如果开发人员手动管理秘密，可能会出现攻击的载体。\nSPIFFE 和 SPIRE 如何减轻 OAuth 和 OIDC 的复杂性 依靠预先存在的凭证来识别工作负载，如 OAuth 客户秘密或刷新令牌，无法解决底层乌龟的问题（如第 1 章所解释）。在这些情况下，利用 SPIRE 作为身份提供者，允许在与 OAuth 基础设施联系之前发布引导凭证（bootstrap credential）或底层乌龟。SPIRE 极大地提高了安全性，因为没有长期的静态凭证需要与工作负载本身共同部署。SPIFFE 可以作为 OAuth 的补充。它消除了直接管理 OAuth 客户端凭证的需要 —— 应用程序可以根据需要使用他们的 SPIFFE ID 来验证 OAuth 提供商。事实上，OAuth 访问令牌本身可以是 SVID，允许用户以与工作负载相同的方式对 SPIFFE 生态系统中的服务进行认证。参见与 OIDC 的集成来了解更多。\n秘密管理者 秘密管理器通常代表工作负载或管理员控制、审计和安全地存储敏感信息（共享秘密，通常是密码）。一些秘密管理器可以执行额外的功能，如加密和解密数据。许多秘密管理器的一个共同特征是中央存储，即所谓的保险库（vault），它对数据进行加密。工作负载在执行秘密检索或数据解密等操作前必须单独对保险库进行认证。\n部署秘密管理器的一个典型的架构挑战是如何安全地存储工作负载用来验证秘密管理器本身的凭证。这有时被称为 “零号凭证”、“引导凭证”，或者更广泛地说，安全引入的过程。\n通过提供一个可以存储、检索、轮换和撤销这些秘密的安全位置，使用一个秘密管理器极大地改善了依赖共享秘密的系统的安全状况。然而，大量的使用会使共享秘密的使用永久化，而不是使用强大的身份识别。\n如何利用 SPIFFE 和 SPIRE 来减轻秘密管理人员的挑战 如果你确实需要使用一个秘密管理器，它可以被配置为使用 SPIFFE 证书进行认证。这允许你在服务之间使用相同的 SPIFFE 证书进行直接认证，并检索秘密来与非 SPIFFE 证书对话。\n服务网格 服务网格旨在通过提供自动认证、授权和强制执行工作负载之间的相互 TLS 来简化工作负载之间的通信。服务网格通常提供集成的工具：\n确定工作负载。 调解工作负载之间的通信，通常通过部署在每个工作负载附近的代理（sidecar 模式）。 确保每个相邻的代理执行一致的认证和授权策略（一般通过授权策略引擎）。 所有主要的服务网格都包括一个原生的平台特定服务认证机制。\n虽然服务网格可以在没有加密身份平面的情况下运行，但为了允许服务间的通信和发现，不可避免地要创建弱形式的身份。本实施方案中的服务网格不提供安全功能，也不解决前面讨论的现有信任根身份问题。\n许多服务网格实现了自己的加密身份平面，或与现有的身份解决方案集成，以提供过境通信安全和信任根的解决。大多数服务网格实现了 SPIFFE 或其部分内容。许多服务网格实现都采用了 SPIFFE 规范的部分实现（包括 Istio 和 Consul），并可被视为 SPIFFE 身份提供商。有些将 SPIRE 作为其解决方案的一个组成部分（如 GreyMatter 或 Network Service Mesh）。\n例如，Istio 使用 SPIFFE 进行节点识别，但其身份模型与 Kubernetes 的特定基元紧密耦合，并完全基于 Kubernetes。没有办法在 Istio 中基于 Kubernetes 之外的属性来识别服务。IBM 解释了为什么目前的 Istio 机制是不够的 。与 SPIRE 这样的通用身份控制平面相比，当希望获得更丰富的证明机制时，或者当服务需要使用通用身份系统在 Istio 之外认证时，这对 Istio 构成了制约因素。使用 SPIRE 进行工作负载身份认证的另一个优势是，它可以确保不受服务网格控制的通信。出于这样的原因，组织有时会将 SPIRE 与 Istio 集成，并使用 SPIFFE 身份而不是内置的 Istio 身份。IBM 发布了一个例子，位于 IBM/istio-spire：Istio 身份与 SPIFFE/SPIRE 。\n服务网格不是 SPIFFE/SPIRE 的直接替代品，相反，它们是互补的 SPIFFE/SPIRE 作为网格内更高层次抽象的身份解决方案。\n专门实现 SPIFFE 工作负载 API 的服务网格解决方案支持任何期望该 API 可用的软件。能够为其工作负载提供 SVID 并支持 SPIFFE Federation API 的服务网格解决方案可以在网格识别的工作负载和运行 SPIRE 或运行在不同网格实现的工作负载之间自动建立信任。\n覆盖网络 覆盖网络（Overlay Network）模拟了一个单一的统一网络，用于跨多个平台的服务。与服务网格不同，覆盖网络使用标准的网络概念，如 IP 地址和路由表来连接服务。数据被封装并跨过其他网络进行路由，创建一个建立在现有网络之上的节点和逻辑链接的虚拟网络。\n虽然最常见的覆盖网络没有认证功能，但最新的网络有。然而，它们在允许服务连接之前仍然不能证明它们的身份。通常情况下，它们依赖于一个预先存在的证书。SPIFFE 很适合为覆盖网络节点提供证书。\n","relpermalink":"/book/spiffe/comparing-spiffe-to-other-security-technologies/","summary":"本章将 SPIFFE 与其他解决类似问题的技术进行了比较。","title":"SPIFFE 与其他安全技术对比"},{"content":"在参考平台中，整个应用系统的运行状态或执行状态是由于基础设施代码（例如，用于服务间通信的网络路由、资源配置代码）、策略代码（例如，指定认证和授权策略的代码）和会话管理代码（例如，建立 mTLS 会话的代码、生成 JWT 令牌的代码）的执行的组合，这些代码由可观测性作为代码的执行所揭示。服务网格的可观测性代码在运行期间将基础设施、策略和会话管理代码的执行输出转发给各种监控工具，这些工具产生适用的指标和日志聚合工具以及追踪工具，这些工具又将其输出转发给集中式仪表板。作为这些工具输出的组成部分的分析，使系统管理员能够获得整个应用系统运行时状态的综合全局视图。正是通过持续监控和零信任设计功能实现的 DevSecOps 平台的运行时性能，为云原生应用提供了所有必要的安全保障。\n在 DevSecOps 管道中，实现持续 ATO 的活动是：\n检查合规的代码。可以检查以下代码是否符合《风险管理框架》的规定 (a) IaC：生成网络路线，资源配置 (b) 策略即代码：对 AuthN 和 AuthZ 策略进行编码 (c) 会话管理代码：mTLS 会话，JWT 令牌 (d) 可观测性代码\n具体的风险评估功能包括生成可操作的任务的能力，指定代码级别的指导，以及用于验证合规性的测试计划。此外，风险评估工具可以为仪表盘中显示的所有工件提供完整的可追溯性，以及持续 ATO 所需的报告能力。\n显示运行时状态的仪表板。通过触发新管道的过程，提供修复安全和性能瓶颈问题（影响可用性）所需的警报和反馈。此外，仪表盘生成工具有一些功能，使系统管理员能够分析宏观层面的特征，以及根据不断发展的系统和消费者对应用程序运行环境的需求，动态地改变要显示的工件的构成。 下一章 ","relpermalink":"/book/service-mesh-devsecops/implement/leveraging-devsecops-for-continuous-authorization-to-operate-c-ato/","summary":"在参考平台中，整个应用系统的运行状态或执行状态是由于基础设施代码（例如，用于服务间通信的网络路由、资源配置代码）、策略代码（例如，指定认证和授权策略的代码）和会话管理代码（例如，建立 mTLS 会话的代码、生成","title":"4.10 利用 DevSecOps 进行持续授权操作（C-ATO）"},{"content":"我们讨论过只能使用云原生应用程序来创建基础架构。同时基础架构也负责运行这些应用程序。\n运行由应用程序配置和控制的基础架构可以轻松得扩展。我们通过学习如何通过扩展应用的方式来扩展基础架构。我们还通过学习如何保护应用程序来保护基础架构。\n在动态环境中，无法通过增加人手来管理这样的复杂性，同样也不能靠增加人手来处理策略和安全问题。\n这意味着，就像我们必须创建通过协调器模式强制执行基础架构状态的应用程序一样，我们需要创建实施安全策略的应用程序。在创建应用程序以执行的策略之前，我们需要以机器可解析的格式编写策略。\n策略即代码 由于策略没有明确定义的技术实现，所以策略难以纳入代码。它更多地关注业务如何实现而不是谁来实现。\n如何实现和谁来实现都会经常变化，但是实现方式变化更频繁且不容易被抽象化。它也是组织特定的，可能需要了解创建基础架构人员的沟通结构的具体细节。\n策略需要应用于应用程序生命周期的多个阶段。正如我们在第 7 章中所讨论的，应用程序通常有三个阶段：部署、运行和退役。\n部署阶段将在应用程序和基础架构变更发布之前先应用策略。这将包括部署规则和一致性测试。运行阶段将包括持续的遵守和执行访问控制和隔离。退役阶段很重要，以确保没有服务落后于未安排或未维护的状态。\n在这些阶段中，您需要将策略分解为明确的，可操作的实现。模糊的策略无法执行。您需要将实现放在代码中，然后创建应用程序或使用现有的应用程序来执行策略规则。\n您应该将策略视为代码。策略更改应视为应用程序更改并在版本控制中进行跟踪。\n控制应用程序部署的相同策略也应该适用于您的新策略部署。您可以使用与部署应用程序相同的工具跟踪和部署的基础架构组件越多，就越容易了解正在运行的内容以及变更如何影响系统。\n将策略作为代码带来的巨大好处是您可以轻松地添加或删除策略并对其进行跟踪，因此记录了谁执行了策略，何时执行了策略以及提交和提交请求的评论。由于该策略以代码形式存在，因此您现在可以为自己的策略编写测试！如果你想验证一个策略是否可以正常工作，你可以使用第 5 章中的测试实践。\n让我们更仔细地看看如何将策略应用到应用程序生命周期。\n部署 Gateway 部署 gateway 确保应用程序的部署符合业务规则。这意味着您将需要构建部署流水线，并且不允许从用户机器进行直接生产部署。\n在实施集中化策略之前，您需要集中控制，但是应该从小规模开始，并在实施之前证明解决方案可行。部署流水线的好处远不止于策略执行，而且应该是任何拥有少数开发人员的组织中的标准。\n以下是一些策略示例：\n部署只有在所有测试都通过后才能进行。 新应用程序要求高级开发人员检查更改并对提取请求发表评论。 生产工件推送只能从部署流水线发生。 Gateway 不应该强制运行状态或应用程序的 API 请求。应用程序应该知道如何配置基础架构组件，并通过合规性和审计将策略应用于这些组件，而不是在应用程序部署期间应用。\n部署 gateway 策略的一个例子是，星期五下午 3 点之后，如果您的组织没有获得经理批准，不允许部署代码。\n这个很容易放入代码中。图 8-1 是代表策略的非常简化的图。\n图 8-1. 部署策略 您可以看到该策略简单地检查了允许部署部署的一周中的时间和日期。如果是星期五和下午 3 点以后，那么策略会检查管理员确定。\n该策略可以通过经理发送的经过验证的电子邮件，经过验证的 API 调用或各种其他方式来获得 OK 通知。决定首选通信方法的内容以及等待批准的时间长度取决于策略。\n这个简单的例子可以用很多不同的选项进行扩展，但确保该策略不符合人类解析和执行是很重要的。人的解释是不同的，而不明确的策略通常不会得到执行。\n通过确保策略阻止新的部署，可以为解决生产环境的状态节省很多工作。有一系列可以通过软件验证的事件可以帮助您理解系统。版本控制和持续部署流水线可以验证代码；使用策略和流程作为代码可以验证软件的部署方式和时间。\n除了确保通过公司策略部署正确的事情之外，我们还应该轻松地使用模板部署受支持的事物，并通过一致性测试强制执行它们。\n合规性测试 在任何基础架构中，您都需要提供建议的方式来创建特定类型的应用程序。这些建议成为用户根据需要消费和拼凑在一起的基石。\n这些建议是可堆砌的，但又不能太小。需要在其预期的功能和自助服务方面可以理解。我们已经建议将云原生应用程序打包为容器并通过协调器使用；由您决定什么最适合您的用户以及您想要提供哪些组件\n您可以通过多种方式为用户提供模板化基础架构。\n提供一个模板化的代码，比如 Jsonnet，或者完全模板化的应用程序，例如 Helm 的 chart。\n您还可以通过您的部署流水线提供模板。可以是 Terraform 模块或特定于部署的工具，例如 Spinnaker 模板。\n创建部署模板允许用户成为模板的消费者，随着最佳实践的发展，用户将自动受益。\n基础架构模板的最关键的一点是使做正确的事情变得容易，并且很难做错事情。如果您满足客户的需求，那么获得适配器将会容易得多。\n但是，我们需要模板化基础架构的根本原因是可以执行合规性测试。合规性测试的存在是为了确保应用程序和基础架构组件符合组织的标准。\n对不符合标准的基础架构进行测试的一些示例如下：\n不在自动缩放组中的服务或端点 不在负载均衡器后面的应用程序 前端层直接与数据库通信 这些标准是关于基础架构的信息，您可以通过调用云提供商的 API 来找到这些信息。合规性测试应持续运行，并强制执行公司采用的架构标准。\n如果发现基础架构组件或应用程序架构违反了公司提供的标准，则应尽早终止它们。越早可以对模板进行编码，就可以越早检查出不符合标准的应用程序。在应用程序的生命早期解决不受支持的体系结构非常重要，因此可以最大限度地减少对体系结构决策的依赖。\n合规性处理如何构建应用程序和维护可操作性。合规性测试确保应用程序和基础架构安全。\n一致性测试 合规性测试不会测试架构设计，而是集中于组件的实施，以确保它们遵守定义的策略。放在代码中的最简单的策略是那些有助于安全的策略。围绕组织需求（例如 HIPAA）的策略也应定义为代码并在合规性测试期间进行测试。\n合规策略的一些例子：\n对象存储的用户访问受限，并且不能被公共因特网读取或写入 API 端点全部使用 HTTPS 并具有有效证书 虚拟机实例（如果有的话）没有过分宽松的防火墙规则 虽然这些策略不会使应用程序免受所有漏洞或错误的影响，但是如果应用程序确实被利用，合规性策略应将影响范围降至最低。\nNetflix 在其博客文章“The Netflix Simian Army”中通过其 Security Monkey 解释了其执行合规性测试的目的：\nSecurity Monkey 是 Conformity Monkey 的延伸。它会查找安全违规或漏洞（如未正确配置的 AWS 安全组），并终止违规实例。它还确保我们所有的 SSL 和 DRM 证书都是有效的，并且不会延期。\n将您的策略放入代码并通过观察云提供商 API 继续运行它，可以让您保持更高的安全性，立即捕获不安全的设置，并随时跟踪版本控制系统的策略。根据这些策略不断测试您的基础架构的模型也非常适合调节器模式。\n如果您认为策略是需要应用和实施的配置类型，那么实施它可能会更简单。请务必记住，随着您的基础架构和业务需求的变化，您的合规策略也应该如此。\n部署测试在将应用程序部署到基础架构之前进行监视，合规性和合规性测试都处理正在运行的应用程序。确保您拥有策略的最后一个应用程序生命周期阶段是了解何时以及如何废止应用程序和生命周期组件。\n活动测试 合规性和一致性测试应该删除那些未通过定义策略的应用和基础架构。还应该有一个应用程序清理旧的和未使用的基础架构组件。高级使用模式应基于应用程序遥测数据，但仍有其他基础架构组件很容易被遗忘并需要退役。\n在云环境中，您可以根据需要消费资源，但很容易会忘记需求。如果没有自动清理旧的或未使用的资源，最后您会对账单感到惊讶，或者需要耗费大量人力进行手动审计和清理。\n您应该测试并自动清理的资源的一些示例包括：\n旧磁盘快照 测试环境 以前的应用程序版本 负责清理的应用程序需要根据默认策略做正确的事情，并为工程师指定的异常提供灵活性。\n正如第 7 章所提到的，Netflix 已经实现了它所称的“Janitor Monkey”，它的实现完美地描述了这种需要的模式：\nJanitor Monkey 在“标记、通知和删除”过程中工作。当 Janitor Monkey 将资源标记为候选清理对象时，它会安排删除资源的时间。删除时间在标记资源的规则中指定。\n每个资源都与一个所有者电子邮件相关联，该电子邮件可以在资源上指定为标签，或者您可以快速扩展 Janitor Monkey 以从您的内部系统获取信息。最简单的方法是使用默认的电子邮件地址，例如您的团队的电子邮件列表中的所有资源。您可以配置若干天，以指定何时让 Janitor Monkey 在计划终止之前向资源所有者发送通知。默认情况下，数字为 3，表示业主将在终止日期前 3 个工作日收到通知。\n在这 3 天期间，资源所有者可以决定资源是否可以删除。如果资源需要保留更长时间，则所有者可以使用简单的 REST 接口将资源标记为未被 Janitor Monkey 清除。所有者总是可以使用另一个 REST 接口来删除该标志，然后 Janitor Monkey 将能够再次管理该资源。\n当 Janitor Monkey 看到标记为清理候选者的资源并且预定的终止时间已经过去时，它将删除资源。如果资源所有者想要提前释放资源以节省成本，则还可以手动删除该资源。当资源状态改变而使资源不是清理候选者时，例如一个分离的 EBS 卷被附加到一个实例，Janitor Monkey 将取消该资源的标记并且不会终止。\n拥有自动清理基础架构的应用程序可以降低您的复杂性和成本。该测试将协调模式应用到应用程序的最后生命周期阶段。\n还有其他一些基础架构的实践很重要，需要考虑。其中一些实践适用于传统基础架构，但在云原生环境中需要进行不同的处理。\n不断测试基础架构的各个方面有助于您了解自己遵守的策略。当基础架构频繁变更时，很难审计哪些变更可能导致停机或使用历史数据来预测未来趋势。\n如果您希望从账单声明中获取该信息或通过基础架构的当前快照来推断，您会很快发现它们所提供的信息是没用的。为了跟踪变化并预测未来，我们需要有审计工具，可以快速提供我们需要的信息。\n审计基础架构 在这个意义上审计云原生基础架构与审核调解器模式中的组件不同，它与第 6 章中讨论的测试框架也不同。相反，当我们谈论审计时，我们指的是对变更的高级概述和基础架构内的组件关系。\n跟踪基础架构中存在的内容以及它与其他组件的关系，为我们了解当前状态提供了重要的背景。当某些事情中断时，第一个问题几乎总是“什么改变了？”审计为我们回答了这个问题，并且可以用来告诉我们如果我们应用变更会受到什么影响。\n在传统的基础架构中，配置管理数据库（CMDB）通常是基础架构当前状态的真相来源。但是，CMDB 不会跟踪基础架构或资产关系的历史版本。\n云提供商可以通过库存 API 为您提供 CMDB 替代服务，但它们可能不会激励您显示历史趋势或让您查询您需要进行故障排除的特定详细信息，例如主机名。\n一个好的云审计工具可以让你显示当前基础架构与昨天或上周相比的差异（diff）。它应该能够将云提供商数据与其他来源（例如容器编排器）相结合，以便您可以查询云提供商可能没有的数据的基础架构，理想情况下，它可以自动构建组件关系的拓扑表示。\n如果您的应用程序完全在单个平台上运行（例如 Kubernetes），则收集资源依赖关系的拓扑信息要容易得多。自动可视化关系的另一种方法是统一关系发生的层次。\n对于在云中运行的服务，可以在服务之间的网络通信中识别关系。有很多方法可以识别服务之间的网络流量，但审计的重要考虑是对信息进行历史跟踪。您需要能够轻松识别关系何时发生变化，就像您识别组件之间关系一样容易。\n自动识别服务关系的方法\n跟踪服务之间的网络流量意味着您需要了解 …","relpermalink":"/book/cloud-native-infra/securing-applications/","summary":"我们讨论过只能使用云原生应用程序来创建基础架构。同时基础架构也负责运行这些应用程序。 运行由应用程序配置和控制的基础架构可以轻松得扩展。我们通过学习如何通过扩展应用的方式来扩展基础架构。我们还通过学习如","title":"第 8 章：保护应用程序"},{"content":"日志记录了集群中的活动。审计日志是必要的，这不仅是为了确保服务按预期运行和配置，也是为了确保系统的安全。系统性的审计要求对安全设置进行一致和彻底的检查，以帮助识别潜在威胁。Kubernetes 能够捕获集群操作的审计日志，并监控基本的 CPU 和内存使用信息；然而，它并没有提供深入的监控或警报服务。\n关键点\n在创建时建立 Pod 基线，以便能够识别异常活动。 在主机层面、应用层面和云端（如果适用）进行日志记录。 整合现有的网络安全工具，进行综合扫描、监控、警报和分析。 设置本地日志存储，以防止在通信失败的情况下丢失。 日志 在 Kubernetes 中运行应用程序的系统管理员应该为其环境建立一个有效的日志、监控和警报系统。仅仅记录 Kubernetes 事件还不足以了解系统上发生的行动的全貌。还应在主机级、应用级和云上（如果适用）进行日志记录。而且，这些日志可以与任何外部认证和系统日志相关联，以提供整个环境所采取的行动的完整视图，供安全审计员和事件响应者使用。\n在 Kubernetes 环境中，管理员应监控 / 记录以下内容：\nAPI 请求历史 性能指标 部署情况 资源消耗 操作系统调用 协议、权限变化 网络流量 当一个 Pod 被创建或更新时，管理员应该捕获网络通信、响应时间、请求、资源消耗和任何其他相关指标的详细日志以建立一个基线。正如上一节所详述的，匿名账户应被禁用，但日志策略仍应记录匿名账户采取的行动，以确定异常活动。\n应定期审计 RBAC 策略配置，并在组织的系统管理员发生变化时进行审计。这样做可以确保访问控制的调整符合基于角色的访问控制部分中概述的 RBAC 策略加固指导。\n审计应包括将当前日志与正常活动的基线测量进行比较，以确定任何日志指标和事件的重大变化。系统管理员应调查重大变动——例如，应用程序使用的变化或恶意程序的安装，如密码器，以确定根本原因。应该对内部和外部流量日志进行审计，以确保对连接的所有预期的安全限制已被正确配置，并按预期运行。管理员还可以在系统发展过程中使用这些审计，以确定何时不再需要外部访问并可以限制。\n日志可以导向外部日志服务，以确保集群外的安全专业人员的可用使用它们，尽可能接近实时地识别异常情况，并在发生损害时保护日志不被删除。如果使用这种方法，日志应该在传输过程中用 TLS 1.2 或 1.3 进行加密，以确保网络行为者无法在传输过程中访问日志并获得关于环境的宝贵信息。在利用外部日志服务器时，要采取的另一项预防措施是在 Kubernetes 内配置日志转发器，只对外部存储进行追加访问。这有助于保护外部存储的日志不被删除或被集群内日志覆盖。\nKubernetes 原生审计日志配置 Kubernetes 的审计功能默认是禁用的，所以如果没有写审计策略，就不会有任何记录。\nkube-apiserver 驻留在 Kubernetes 控制平面上，作为前端，处理集群的内部和外部请求。每个请求，无论是由用户、应用程序还是控制平面产生的，在其执行的每个阶段都会产生一个审计事件。当审计事件注册时，kube-apiserver 检查审计策略文件和适用规则。如果存在这样的规则，服务器会在第一个匹配的规则所定义的级别上记录该事件。Kubernetes 的内置审计功能默认是不启用的，所以如果没有写审计策略，就不会有任何记录。\n集群管理员必须写一个审计策略 YAML 文件，以建立规则，并指定所需的审计级别，以记录每种类型的审计事件。然后，这个审计策略文件被传递给 kube-apiserver，并加上适当的标志。一个规则要被认为是有效的，必须指定四个审计级别中的一个：none、Meatadataa、Request 或 RequestResponse。附录 L：审计策略 展示了一个审计策略文件的内容，该文件记录了 RequestResponse 级别的所有事件。附录 M 向 kube-apiserver 提交审计策略文件的标志示例显示了 kube-apiserver 配置文件的位置，并提供了审计策略文件可以被传递给 kube-apiserver 的标志示例。附录 M 还提供了如何挂载卷和在必要时配置主机路径的指导。\nkube-apiserver 包括可配置的日志和 webhook 后端，用于审计日志。日志后端将指定的审计事件写入日志文件，webhook 后端可以被配置为将文件发送到外部 HTTP API。附录 M 中的例子中设置的 --audit-log-path 和 --audit-log-maxage 标志是可以用来配置日志后端的两个例子，它将审计事件写到一个文件中。log-path 标志是启用日志的最小配置，也是日志后端唯一需要的配置。这些日志文件的默认格式是 JSON，尽管必要时也可以改变。日志后端的其他配置选项可以在 Kubernetes 文档中找到。\n为了将审计日志推送给组织的 SIEM 平台，可以通过提交给 kube-apiserver 的 YAML 文件手动配置 webhook 后端。webhook 配置文件以及如何将该文件传递给 kube-apiserver 可以在附录 N：webhook 配置 的示例中查看。关于如何在 kube-apiserver 中为 webhook 后端设置的配置选项的详尽列表，可以在 Kubernetes 文档中找到。\n工作节点和容器的日志记录 在 Kubernetes 架构中，有很多方法可以配置日志功能。在日志管理的内置方法中，每个节点上的 kubelet 负责管理日志。它根据其对单个文件长度、存储时间和存储容量的策略，在本地存储和轮转日志文件。这些日志是由 kubelet 控制的，可以从命令行访问。下面的命令打印了一个 Pod 中的容器的日志。\nkubectl logs [-f] [-p] POD [-c CONTAINER] 如果要对日志进行流式处理，可以使用 -f 标志；如果存在并需要来自容器先前实例的日志，可以使用 -p 标志；如果 Pod 中有多个容器，可以使用 -c 标志来指定一个容器。如果发生错误导致容器、Pod 或节点死亡，Kubernetes 中的本地日志解决方案并没有提供一种方法来保存存储在失败对象中的日志。NSA 和 CISA 建议配置一个远程日志解决方案，以便在一个节点失败时保存日志。\n远程记录的选项包括：\n远程日志选项 使用的理由 配置实施 在每个节点上运行一个日志代理，将日志推送到后端 赋予节点暴露日志或将日志推送到后端的能力，在发生故障的情况下将其保存在节点之外。 配置一个 Pod 中的独立容器作为日志代理运行，让它访问节点的应用日志文件，并配置它将日志转发到组织的 SIEM。 在每个 Pod 中使用一个 sidecar 容器，将日志推送到一个输出流中 用于将日志推送到独立的输出流。当应用程序容器写入不同格式的多个日志文件时，这可能是一个有用的选项。 为每种日志类型配置 sidecar 容器，并用于将这些日志文件重定向到它们各自的输出流，在那里它们可以被 kubelet 处理。然后，节点级的日志代理可以将这些日志转发给 SIEM 或其他后端。 在每个 Pod 中使用一个日志代理 sidecar，将日志推送到后端 当需要比节点级日志代理所能提供的更多灵活性时。 为每个 Pod 配置，将日志直接推送到后端。这是连接第三方日志代理和后端的常用方法。 从应用程序中直接向后端推送日志 捕获应用程序的日志。Kubernetes 没有内置的机制直接来暴露或推送日志到后端。 各组织将需要在其应用程序中建立这一功能，或附加一个有信誉的第三方工具来实现这一功能。 Sidecar 容器与其他容器一起在 Pod 中运行，可以被配置为将日志流向日志文件或日志后端。Sidecar 容器也可以被配置为作为另一个标准功能容器的流量代理，它被打包和部署。\n为了确保这些日志代理在工作节点之间的连续性，通常将它们作为 DaemonSet 运行。为这种方法配置 DaemonSet，可以确保每个节点上都有一份日志代理的副本，而且对日志代理所做的任何改变在集群中都是一致的。\nSeccomp: 审计模式 除了上述的节点和容器日志外，记录系统调用也是非常有益的。在 Kubernetes 中审计容器系统调用的一种方法是使用安全计算模式（seccomp）工具。这个工具默认是禁用的，但可以用来限制容器的系统调用能力，从而降低内核的攻击面。Seccomp 还可以通过使用审计配置文件记录正在进行的调用。\n自定义 seccomp 配置文件用于定义哪些系统调用是允许的，以及未指定调用的默认动作。为了在 Pod 中启用自定义 seccomp 配置文件，Kubernetes 管理员可以将他们的 seccomp 配置文件 JSON 文件写入到 /var/lib/kubelet/seccomp/ 目录，并将 seccompProfile 添加到 Pod 的 securityContext。自定义的 seccompProfile 还应该包括两个字段。Type: Localhost 和 localhostProfile: myseccomppolicy.json。记录所有的系统调用可以帮助管理员了解标准操作需要哪些系统调用，使他们能够进一步限制 seccomp 配置文件而不失去系统功能。\nSYSLOG Kubernetes 默认将 kubelet 日志和容器运行时日志写入 journald，如果该服务可用的话。如果组织希望对默认情况下不使用的系统使用 syslog 工具，或者从整个集群收集日志并将其转发到 syslog 服务器或其他日志存储和聚合平台，他们可以手动配置该功能。Syslog 协议定义了一个日志信息格式化标准。Syslog 消息包括一个头——由时间戳、主机名、应用程序名称和进程 ID（PID）组成，以及一个以明文书写的消息。Syslog 服务，如 syslog-ng® 和 rsyslog，能够以统一的格式收集和汇总整个系统的日志。许多 Linux 操作系统默认使用 rsyslog 或 journald——一个事件日志守护程序，它优化了日志存储并通过 journalctl 输出 syslog 格式的日志。在运行某些 Linux 发行版的节点上，syslog 工具默认在操作系统层面记录事件。运行这些 Linux 发行版的容器，默认也会使用 syslog 收集日志。由 syslog 工具收集的日志存储在每个适用的节点或容器的本地文件系统中，除非配置了一个日志聚合平台来收集它们。\nSIEM 平台 安全信息和事件管理（SIEM）软件从整个组织的网络中收集日志。SIEM 软件将防火墙日志、应用程序日志等汇集在一起；将它们解析出来，提供一个集中的平台，分析人员可以从这个平台上监控系统安全。SIEM 工具在功能上有差异。一般来说，这些平台提供日志收集、威胁检测和警报功能。有些包括机器学习功能，可以更好地预测系统行为并帮助减少错误警报。在其环境中使用这些平台的组织可以将它们与 Kubernetes 集成，以更好地监测和保护集群。用于管理 Kubernetes 环境中的日志的开源平台是作为 SIEM 平台的替代品存在的。\n容器化环境在节点、Pod、容器和服务之间有许多相互依赖的关系。在这些环境中，Pod 和容器不断地在不同的节点上被关闭和重启。这给传统的 SIEM 带来了额外的挑战，它们通常使用 IP 地址来关联日志。即使是下一代的 SIEM 平台也不一定适合复杂的 Kubernetes 环境。然而，随着 Kubernetes 成为最广泛使用的容器编排平台，许多开发 SIEM 工具的组织已经开发了专门用于 Kubernetes 环境的产品变化，为这些容器化环境提供全面的监控解决方案。管理员应该了解他们平台的能力，并确保他们的日志充分捕捉到环境，以支持未来的事件响应。\n警报 Kubernetes 本身并不支持警报功能；然而，一些具有警报功能的监控工具与 Kubernetes 兼容。如果 Kubernetes …","relpermalink":"/book/kubernetes-hardening-guidance/logging/","summary":"日志记录了集群中的活动。审计日志是必要的，这不仅是为了确保服务按预期运行和配置，也是为了确保系统的安全。系统性的审计要求对安全设置进行一致和彻底的检查，以帮助识别潜在威胁。Kubernetes 能够捕获","title":"日志审计"},{"content":"OpenTelemetry 是一个大型项目。OpenTelemetry 项目的工作被划分为特殊兴趣小组（SIG）。虽然所有的项目决策最终都是通过 GitHub issue 和 pull request 做出的，但 SIG 成员经常通过 CNCF 的官方 Slack 保持联系，而且大多数 SIG 每周都会在 Zoom 上会面一次。\n任何人都可以加入一个 SIG。要想了解更多关于当前 SIG、项目成员和项目章程的细节，请查看 GitHub 上的 OpenTelemetry 社区档案库 。\n规范 OpenTelemetry 是一个规范驱动的项目。OpenTelemetry 技术委员会负责维护该规范，并通过管理规范的 backlog 来指导项目的发展。\n小的改动可以以 GitHub issue 的方式提出，随后的 pull request 直接提交给规范。但是，对规范的重大修改是通过名为 OpenTelemetry Enhancement Proposals（OTEPs）的征求意见程序进行的。\n任何人都可以提交 OTEP。OTEP 由技术委员会指定的具体审批人进行审查，这些审批人根据其专业领域进行分组。OTEP 至少需要四个方面的批准才能被接受。在进行任何批准之前，通常需要详细的设计，以及至少两种语言的原型。我们希望 OTEP 的作者能够认真对待其他社区成员的要求和关注。我们的目标是确保 OpenTelemetry 适合尽可能多的受众的需求。\n一旦被接受，将根据 OTEP 起草规范变更。由于大多数问题已经在 OTEP 过程中得到了解决，因此规范变更只需要两次批准。\n项目治理 管理 OpenTelemetry 项目如何运作的规则和组织结构由 OpenTelemetry 治理委员会定义和维护，其成员经选举产生，任期两年。\n治理成员应以个人身份参与，而不是公司代表。但是，为同一雇主工作的委员会成员的数量有一个上限。如果因为委员会成员换了工作而超过了这个上限，委员会成员必须辞职，直到雇主代表的人数降到这个上限以下。\n发行版 OpenTelemetry 有一个基于插件的架构，因为有些观察能力系统需要一套插件和配置才能正常运行。\n发行版（distros）被定义为广泛使用的 OpenTelemetry 插件的集合，加上一组脚本或辅助功能，可能使 OpenTelemetry 与特定的后端连接更简单，或在特定环境中运行 OpenTelemetry。\n需要澄清的是，如果一个可观测性系统声称它与 OpenTelemetry 兼容，那么它应该总是可以使用 OpenTelemetry 而不需要使用某个特定的发行版。如果一个项目没有经过规范过程就扩展了 OpenTelemetry 的核心功能，或者包括任何导致它与上游 OpenTelemetry 仓库不兼容的变化，那么这个项目就是一个分叉，而不是一个发行版。\n注册表 为了便于发现目前有哪些语言、插件和说明，OpenTelemetry 提供了一个注册表。任何人都可以向 OpenTelemetry 注册表 提交插件；在 OpenTelemetry 的 GitHub 组织内托管插件不是必须的。\n","relpermalink":"/book/opentelemetry-obervability/organization/","summary":"附录 A：OpenTelemetry 项目组织","title":"附录 A：OpenTelemetry 项目组织"},{"content":"Istio 隔离边界可以在 Kubernetes 集群内或跨多个集群中运行多个由 TSB（Tetrate Service Bridge）管理的 Istio 环境。这些 Istio 环境在服务发现和配置分发方面彼此隔离。隔离边界带来了以下几个好处：\n强大的网络隔离默认提供了在高度受管制的环境中严格且易于演示的安全性。 在集群内运行不同的 Istio 版本允许你在同一集群中支持传统和现代应用程序。 金丝雀发布提供了在测试和部署 TSB 升级时的灵活性。 安装 升级 要从非修订的控制平面升级到修订的控制平面，请按照 非修订版到修订版的升级 中提到的步骤进行操作。 OpenShift 如果你使用 OpenShift，请将以下 kubectl 命令替换为 oc。 对于全新安装，你可以按照使用 tctl 或 helm 来加入控制平面集群的标准步骤，以及以下更改进行操作：\n通过将 ISTIO_ISOLATION_BOUNDARIES 设置为 true 在 TSB 控制平面 Operator 中启用隔离边界。 在 ControlPlane CR 或控制平面 Helm 值中添加隔离边界定义。 在以下示例中，你将使用 Helm 启用隔离边界来加入一个集群。\n使用 Helm 安装 按照 使用 Helm 安装控制平面 中的说明，使用以下 Helm 值来启用 Istio 隔离边界。\noperator: deployment: env: - name: ISTIO_ISOLATION_BOUNDARIES value: \u0026#34;true\u0026#34; spec: managementPlane: clusterName: \u0026lt;cluster-name-in-tsb\u0026gt; host: \u0026lt;tsb-address\u0026gt; port: \u0026lt;tsb-port\u0026gt; telemetryStore: elastic: host: \u0026lt;tsb-address\u0026gt; port: \u0026lt;tsb-port\u0026gt; version: \u0026lt;elastic-version\u0026gt; selfSigned: \u0026lt;is-elastic-use-self-signed-certificate\u0026gt; components: xcp: isolationBoundaries: - name: dev revisions: - name: dev-stable - name: qa revisions: - name: qa-stable secrets: clusterServiceAccount: clusterFQN: organizations/jupiter/clusters/\u0026lt;cluster-name-in-tsb\u0026gt; JWK: \u0026#39;$JWK\u0026#39; 安装步骤完成后，请查看 istio-system 命名空间中的 deployments、configmaps 和 webhooks。所有属于修订的 Istio 控制平面的资源都将在名称中具有 revisions.name 作为后缀。这些资源将存在于配置在 isolationBoundaries 下的每个修订中。\n控制平面 Operator 会验证跨隔离边界的修订名称是否唯一。此修订名称值将用于配置修订的命名空间并启动修订的数据平面网关。\nkubectl get deployment -n istio-system | grep stable # 输出 istio-operator-dev-stable 1/1 1 1 2d1h istio-operator-qa-stable 1/1 1 1 45h istiod-dev-stable 1/1 1 1 2d1h istiod-qa-stable 1/1 1 1 45h kubectl get configmap -n istio-system | grep stable # 输出 istio-dev-stable 2 2d1h istio-qa-stable 2 45h istio-sidecar-injector-dev-stable 2 2d1h istio-sidecar-injector-qa-stable 2 45h 使用 tctl 安装 如果你更喜欢使用 tctl 安装 ，你可以使用以下命令生成启用 Istio 隔离边界的集群 Operator。\ntctl install manifest cluster-operators \\ --registry \u0026lt;registry-location\u0026gt; \\ --set \u0026#34;operator.deployment.env[0].name=ISTIO_ISOLATION_BOUNDARIES\u0026#34; \\ --set \u0026#34;operator.deployment.env[0].value=true\u0026#34; \u0026gt; clusteroperators.yaml 然后，在你的 ControlPlane CR 或 Helm 值中使用以下方式更新 xcp 组件与 isolationBoundaries：\nspec: ... components: xcp: isolationBoundaries: - name: dev revisions: - name: dev-stable - name: qa revisions: - name: qa-stable 无论你选择哪种安装方式，使用隔离边界的示例都是相同的。\n在修订中指定 TSB 版本 Istio 隔离边界还提供了一种控制用于部署控制平面组件和数据平面代理的 Istio 版本的方式。可以在隔离边界配置中指定如下：\nspec: ... components: xcp: isolationBoundaries: - name: dev revisions: - name: dev-stable istio: tsbVersion: 1.6.1 - name: qa revisions: - name: qa-stable istio: tsbVersion: 1.6.0 使用这些配置，会部署两个修订的控制平面，使用相应的 TSB 发布的 Istio 镜像。 在单个隔离边界中具有多个修订有助于从一个 tsbVersion 升级到另一个 tsbVersion 中的特定隔离边界的工作负载。有关更多详细信息，请参阅 已修订版本间的升级 。\n如果将 tsbVersion 字段留空，则 ControlPlane 资源将默认为当前 TSB 发布的版本。\n使用隔离边界和修订 应用部署 现在可以在具有修订标签的适当命名空间中部署工作负载。修订标签 istio.io/rev 确定了代理用于连接服务发现和 xDS 更新的修订控制平面。确保如下配置工作负载命名空间：\napiVersion: v1 kind: Namespace metadata: labels: istio.io/rev: dev-stable name: dev-bookinfo --- apiVersion: v1 kind: Namespace metadata: labels: istio.io/rev: qa-stable name: qa-bookinfo 这些命名空间中的应用程序 Pod 将被注入 Istio 代理配置，使它们能够连接到相应的修订 Istio 控制平面。\nVM 工作负载载入 单一隔离边界 工作负载载入 仅支持单一隔离边界。多个隔离边界的支持将在后续版本中提供。 默认情况下，工作负载载入将使用非修订的 Istio 控制平面。要使用修订的控制平面，你需要使用 修订链接 从 TSB 工作负载载入存储库下载 Istio Sidecar。\n你还需要在 VM 的 /etc/onboarding-agent/agent.config.yaml 中更新 代理配置 以添加修订值。\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: AgentConfiguration sidecar: istio: revision: dev-stable 然后重新启动载入代理。\nsystemctl restart onboarding-agent 如果你使用 cloud-init 来配置 VM，请在 cloud-init 文件中添加上述 AgentConfiguration。由于文件 /etc/onboarding-agent/agent.config.yaml 可能提前创建，因此对于基于 Debian 的操作系统，需要在非交互式安装时安装载入代理时传递 -o Dpkg::Options::=\u0026#34;--force-confold\u0026#34;。\nsudo apt-get install -y -o Dpkg::Options::=\u0026#34;--force-confold\u0026#34; ./onboarding-agent.deb 工作区配置 可以通过指定隔离边界名称来配置每个工作区，如下所示：\napiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tetrate tenant: tetrate name: qa-ws spec: isolationBoundary: qa namespaceSelector: names: - \u0026#34;*/qa-bookinfo\u0026#34; apiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tetrate tenant: tetrate name: dev-ws spec: isolationBoundary: dev namespaceSelector: names: - \u0026#34;*/dev-bookinfo\u0026#34; 设置 在 Brownfiled 设置中，现有的工作区将不会配置为任何特定的隔离边界。在这种情况下，如果启用并配置了 Istio 隔离边界，则工作区将视为属于一个名为 “global” 的隔离边界。如果未在 ControlPlane CR 中配置此 “global” 隔离边界，则工作区将不属于任何隔离边界。因此，建议为未在其规范中指定任何隔离边界的工作区创建一个名为 “global” 的备用隔离边界。\nspec: ... components: xcp: isolationBoundaries: - name: global revisions: - name: default 网关部署 对于每个网关（入口/出口/Tier1）资源，必须设置属于所需隔离边界的 revision。\n例如，在你的入口网关部署中：\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: tsb-gateway-dev-bookinfo namespace: dev-bookinfo spec: revision: dev-stable # 修订值 应用后，这将导致 修订的 网关部署。\nIstio 隔离 在单个/多个集群中设置多个隔离边界允许用户运行在服务发现方面分隔的多个网格环境。这意味着一个隔离边界中的服务仅对相同隔离边界中的客户端可发现，从而允许流量流向相同隔离边界中的服务。通过隔离边界分离的服务将无法发现彼此，从而导致不跨边界流量。\n作为一个简单的示例，考虑以下隔离边界配置：\n... spec: ... components: xcp: isolationBoundaries: - name: dev revisions: - name: dev-stable revisions: - name: dev-testing - name: qa revisions: - name: qa-stable 这对应于三个单独的命名空间 dev-bookinfo、dev-bookinfo-testing …","relpermalink":"/book/tsb/setup/isolation-boundaries/","summary":"如何使用隔离边界部署或升级多个隔离的控制平面集群","title":"Istio 隔离边界"},{"content":"在本部分中，你将创建一个名为 bookinfo 的应用程序并向其附加一个 API。API 将根据 OpenAPI 规范进行配置。\nTSB 中的应用程序是服务的逻辑分组，这些服务公开了应用程序的不同功能和用例。它们属于租户，可用于观察服务的行为并配置如何使用这些服务及其 API。应用程序公开一组 API 对象，这些对象定义可用内容和使用条件。\n先决条件 在继续阅读本指南之前，请确保你已完成以下步骤：\n熟悉 TSB 概念 安装 TSB 演示环境 部署 Istio Bookinfo 示例应用程序 创建租户 创建工作区 配置权限 设置 Ingress 网关 检查服务拓扑和指标 配置流量转移 配置安全控制 创建应用程序 创建 application.yaml 文件：\napiVersion: application.tsb.tetrate.io/v2 kind: Application metadata: organization: tetrate tenant: tetrate name: bookinfo spec: displayName: Bookinfo description: Bookinfo application workspace: organizations/tetrate/tenants/tetrate/workspaces/bookinfo-ws gatewayGroup: organizations/tetrate/tenants/tetrate/workspaces/bookinfo-ws/gatewaygroups/bookinfo-gw 使用 tctl 应用配置：\ntctl apply -f application.yaml 上述步骤将创建 bookinfo 应用程序并将其链接到指定的工作区和网关组。\n你可以通过以下方式验证应用程序的状态：\ntctl x status application bookinfo --tenant tetrate -o yaml 附加 OpenAPI 规范 接下来，你将附加 OpenAPI 规范来配置应用程序的 Ingress Gateway。创建 bookinfo-api.yaml 文件：\nbookinfo-api.yaml apiversion: application.tsb.tetrate.io/v2 kind: API metadata: organization: tetrate tenant: tetrate application: bookinfo name: bookinfo spec: displayName: Bookinfo API description: Bookinfo API workloadSelector: namespace: bookinfo labels: app: tsb-gateway-bookinfo openapi: | openapi: 3.0.0 info: description: This is the API of the Istio BookInfo sample application. version: 1.0.0 title: BookInfo API termsOfService: https://istio.io/ license: name: Apache 2.0 url: http://www.apache.org/licenses/LICENSE-2.0.html x-tsb-service: productpage.bookinfo servers: - url: http://bookinfo.tetrate.com/api/v1 tags: - name: product description: Information about a product (in this case a book) - name: review description: Review information for a product - name: rating description: Rating information for a product externalDocs: description: Learn more about the Istio BookInfo application url: https://istio.io/docs/samples/bookinfo.html paths: /products: get: tags: - product summary: List all products description: List all products available in the application with a minimum amount of information. operationId: getProducts responses: \u0026#34;200\u0026#34;: description: successful operation content: application/json: schema: type: array items: $ref: \u0026#34;#/components/schemas/Product\u0026#34; \u0026#34;/products/{id}\u0026#34;: get: tags: - product summary: Get individual product description: Get detailed information about an individual product with the given id. operationId: getProduct parameters: - name: id in: path description: Product id required: true schema: type: integer format: int32 responses: \u0026#34;200\u0026#34;: description: successful operation content: application/json: schema: $ref: \u0026#34;#/components/schemas/ProductDetails\u0026#34; \u0026#34;400\u0026#34;: description: Invalid product id \u0026#34;/products/{id}/reviews\u0026#34;: get: tags: - review summary: Get reviews for a product description: Get reviews for a product, including review text and possibly ratings information. operationId: getProductReviews parameters: - name: id in: path description: Product id required: true schema: type: integer format: int32 responses: \u0026#34;200\u0026#34;: description: successful operation content: application/json: schema: $ref: \u0026#34;#/components/schemas/ProductReviews\u0026#34; \u0026#34;400\u0026#34;: description: Invalid product id \u0026#34;/products/{id}/ratings\u0026#34;: get: tags: - rating summary: Get ratings for a product description: Get ratings for a product, including stars and their color. operationId: getProductRatings parameters: - name: id in: path description: Product id required: true schema: type: integer format: int32 responses: \u0026#34;200\u0026#34;: description: successful operation content: application/json: schema: $ref: \u0026#34;#/components/schemas/ProductRatings\u0026#34; \u0026#34;400\u0026#34;: description: Invalid product id components: schemas: Product: type: object description: Basic information about a product properties: id: type: integer format: int32 description: Product id title: type: string description: Title of the book descriptionHtml: type: string description: Description of the book - may contain HTML tags required: - id - title - descriptionHtml ProductDetails: type: object description: Detailed information about a product properties: id: type: integer format: int32 description: Product id publisher: type: string description: Publisher of the book language: type: string description: Language of the book author: type: string description: Author of the book ISBN-10: type: string description: ISBN-10 of the book ISBN-13: type: string description: ISBN-13 of the book year: type: integer format: int32 description: Year the book was first published in type: type: string enum: - paperback - hardcover description: Type of the book pages: type: integer format: int32 description: Number of pages of the book required: - id - publisher - language - author - ISBN-10 - ISBN-13 - year - type - pages ProductReviews: type: object description: Object containing reviews for a product properties: id: type: integer format: int32 description: Product id reviews: type: array description: List of reviews items: $ref: \u0026#34;#/components/schemas/Review\u0026#34; required: - id - reviews Review: …","relpermalink":"/book/tsb/quickstart/apps/","summary":"在本部分中，你将创建一个名为 bookinfo 的应用程序并向其附加一个 API。API 将根据 OpenAPI 规范进行配置。 TSB 中的应用程序是服务的逻辑分组，这些服务公开了应用程序的不同功能和用例。它们属于租户，可用于观察服务的行为并配","title":"创建应用程序和 API"},{"content":"对于此场景，你需要两个已入网的集群，以配置它们之间的轮询和故障切换。\n先决条件 在开始之前，请确保你已经：\n熟悉 TSB 概念 安装了 TSB 演示 环境 创建了 租户 创建工作区和网关组 以下 YAML 文件包含两个对象：一个用于应用程序的 工作区，以及一个 网关 组，以便你可以配置应用程序的入口。\n将其保存为 httpbin-mgmt.yaml ，然后使用 tctl 应用：\ntctl apply -f httpbin-mgmt.yaml 部署 httpbin 以下配置应该应用于两个集群，以部署你的应用程序，首先创建命名空间并启用 Istio sidecar 注入。\nkubectl create namespace httpbin kubectl label namespace httpbin istio-injection=enabled 然后部署你的应用程序。\nkubectl apply -f \\ https://raw.githubusercontent.com/istio/istio/master/samples/httpbin/httpbin.yaml \\ -n httpbin 配置入口网关 在这个示例中，你将使用入口处的简单 TLS 暴露应用程序。你需要提供一个存储在 Kubernetes 密钥中的 TLS 证书。\nkubectl create secret tls -n httpbin httpbin-cert \\ --cert /path/to/some/cert.pem \\ --key /path/to/some/key.pem 现在，你可以部署入口网关。\n另存为 httpbin-ingress.yaml ，然后使用 kubectl 应用：\nkubectl apply -f httpbin-ingress.yaml 将上述配置应用于两个集群，将为它们创建相同的环境，现在我们将部署网关和虚拟服务。\n集群中的 TSB 数据面运算符将获取此配置并在应用程序命名空间中部署网关的资源。剩下的工作就是配置网关，以便它将流量路由到你的应用程序。\n另存为 httpbin-gw.yaml ，然后使用 tctl 应用：\ntctl apply -f httpbin-gw.yaml 现在，你可以将两个入口网关服务的 IP 配置为你的 DNS 条目，并在它们之间配置轮询，或者只配置一个 IP 并将另一个集群用作故障切换。\n你可以通过运行以下命令测试两个入口网关是否正常工作：\ncurl -s -o /dev/null --insecure -w \u0026#34;%{http_code}\u0026#34; \\ \u0026#34;https://httpbin.tetrate.com\u0026#34; \\ --resolve \u0026#34;httpbin.tetrate.com:443:$CLUSTER1_IP\u0026#34; curl -s -o /dev/null --insecure -w \u0026#34;%{http_code}\u0026#34; \\ \u0026#34;https://httpbin.tetrate.com\u0026#34; \\ --resolve \u0026#34;httpbin.tetrate.com:443:$CLUSTER2_IP\u0026#34; ","relpermalink":"/book/tsb/howto/gateway/distributed-ingress/","summary":"使用分布式入口网关实现弹性网格。","title":"分布式入口网关"},{"content":"本文描述了如何通过使用 TSB 流量设置来降低网格中的控制平面和所有网关使用的 CPU 和内存数量，该设置将生成一个 sidecar 资源。\n先决条件 熟悉 TSB 概念 。 安装 TSB 演示 环境。 创建一个 租户 。 准备环境 在此场景中，我们将部署三个不同的应用程序：bookinfo、httpbin 和 helloworld。每个应用程序都将在接收流量并将其转发到应用程序的同一命名空间中拥有其 ingressgateway。\n首先，为每个应用程序创建一个命名空间并启用 sidecar 注入：\n$ kubectl create ns \u0026lt;ns\u0026gt; $ kubectl label namespace \u0026lt;ns\u0026gt; istio-injection=enabled 现在，在每个命名空间中部署应用程序：\n$ kubectl apply -n bookinfo -f https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/platform/kube/bookinfo.yaml $ kubectl apply -n helloworld -f https://raw.githubusercontent.com/istio/istio/master/samples/helloworld/helloworld.yaml $ kubectl apply -n httpbin -f https://raw.githubusercontent.com/istio/istio/master/samples/httpbin/httpbin.yaml 然后，在每个命名空间中部署 ingressgateway：\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: tsb-gateway-\u0026lt;ns\u0026gt; namespace: \u0026lt;ns\u0026gt; spec: kubeSpec: service: type: LoadBalancer EOF 你现在应该有三个命名空间：bookinfo、httpbin 和 helloworld。现在创建不同的工作区和网关组，以将应用程序引入到 TSB。你可以使用此示例来为所有应用程序使用它：\n$ cat \u0026lt;\u0026lt;EOF | tctl apply -f - apiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tetrate tenant: tetrate name: bookinfo spec: namespaceSelector: names: - \u0026#34;demo/bookinfo\u0026#34; --- apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: organization: tetrate tenant: tetrate workspace: bookinfo name: bookinfo-gg spec: namespaceSelector: names: - \u0026#34;demo/bookinfo\u0026#34; configMode: BRIDGED EOF 最后，应用 ingressgateways 以生成网关和虚拟服务：\n$ cat \u0026lt;\u0026lt;EOF | tctl apply -f - apiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: organization: tetrate tenant: tetrate workspace: httpbin group: httpbin-gg name: httpbin-gw spec: workloadSelector: namespace: httpbin labels: app: tsb-gateway-httpbin istio: ingressgateway http: - name: httpbin port: 80 hostname: httpbin.tetrate.io routing: rules: - route: host: httpbin/httpbin.httpbin.svc.cluster.local --- apiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: organization: tetrate tenant: tetrate workspace: helloworld group: helloworld-gg name: helloworld-gw spec: workloadSelector: namespace: helloworld labels: app: tsb-gateway-helloworld istio: ingressgateway http: - name: helloworld port: 80 hostname: helloworld.tetrate.io routing: rules: - route: host: helloworld/helloworld.helloworld.svc.cluster.local --- apiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: organization: tetrate tenant: tetrate workspace: bookinfo group: bookinfo-gg name: bookinfo-gw spec: workloadSelector: namespace: bookinfo labels: app: tsb-gateway-bookinfo istio: ingressgateway http: - name: bookinfo port: 80 hostname: bookinfo.tetrate.io routing: rules: - match: - uri: exact: /productpage - uri: prefix: /static - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: host: bookinfo/productpage.bookinfo.svc.cluster.local port: 9080 EOF 场景将如下所示：\n降低控制平面和 sidecar 资源 此时，所有 sidecar 都已连接，并具有关于彼此的信息。你可以从 istio-proxy 获取配置转储，其中将看到它了解的所有端点。对于此示例，你可以使用 helloworld pod：\n$ kubectl exec \u0026lt;pod\u0026gt; -c istio-proxy -n helloworld -- pilot-agent request GET config_dump \u0026gt; config_dump.json 由于这是一个小型场景，你可能不会注意到 CPU 和内存资源的许多改进，但为了了解你将要执行的操作，可以检查配置大小。在应用任何限制之前，这是\n当前的大小：\n$ du -h config_dump.json 2.1M\tconfig_dump.json 这是由控制平面生成并发送给所有代理的所有端点信息。因此，为了限制网关生成的信息量，你可以使用 sidecar 资源选择特定网关需要的信息。\n由于你有三个完全不同的应用程序，它们之间不进行通信，因此你可以创建一个流量设置，以允许所有工作负载在同一工作区下进行通信。由于流量设置与流量组相关联，因此需要创建两个资源：\n$ cat \u0026lt;\u0026lt;EOF | tctl apply -f - apiVersion: traffic.tsb.tetrate.io/v2 kind: Group metadata: organization: tetrate tenant: tetrate workspace: helloworld name: helloworld-tg spec: namespaceSelector: names: - \u0026#34;demo/helloworld\u0026#34; configMode: BRIDGED --- apiVersion: traffic.tsb.tetrate.io/v2 kind: TrafficSetting metadata: organization: tetrate tenant: tetrate workspace: helloworld group: helloworld-tg name: default spec: reachability: mode: NAMESPACE EOF 有多种可达性模式可供选择，你可以选择工作区中的所有命名空间，或创建自定义配置来限制 sidecar 配置适用的工作负载范围，从而提高粒度。此流量设置将创建一个带有配置的 sidecar 资源，该配置用于确定工作负载应了解的服务范围。通过此配置，控制平面将配置所选工作负载，以仅接收有关如何访问 helloworld 命名空间中的服务的配置，而不是推送有关网格中所有服务的配置。\n由控制平面推送的配置大小减少了服务网格内存和网络使用。现在，你可以再次获取配置转储并比较大小：\n$ kubectl exec \u0026lt;pod\u0026gt; -c istio-proxy -n helloworld -- pilot-agent request GET config_dump \u0026gt; config_dump.json $ du -h config_dump.json 1.0M\tconfig_dump.json 请注意，由于当前 sidecar 不具有关于其他命名空间中其他端点的信息，因此无法访问它们，请在应用 sidecar 配置时小心。你可以运行以下命令查看生成的 sidecar 资源：\n$ kubectl get sidecar -n helloworld -o yaml 注意 有关如何改进 Istio 控制平面性能的更多信息，你可以阅读这篇博客文章 ，其中详细解释了此过程。 ","relpermalink":"/book/tsb/operations/lower-istio-resources/","summary":"如何通过使用 TSB 流量设置来降低 Istio 控制平面和所有网关的 CPU 和内存使用量。","title":"降低 Istio 资源消耗"},{"content":"网格内的网络策略 在服务网格的背景下，网络策略 是一组规则和配置，定义了各种微服务如何通过网络相互通信。\n这些策略在控制流量流动、强制安全措施和维护服务网格架构的合规性方面起着关键作用。\n网络策略使组织能够通过指定允许的服务、协议、端口以及可交换的数据类型来控制服务之间的通信，这发生在 L3/L4 层。\n这种精确的控制可以阻止未经授权的访问，减少容易受攻击的区域，并确保只在可信任的源之间进行通信。\n网络策略与零信任架构 除了服务网格的思想，零信任架构（ZTA）安全方法认为，不应自动信任任何用户或设备，无论其是否来自组织内部或外部网络。\nZTA 强调通过严格的身份验证和授权步骤来确认身份，并允许访问，而不是依赖于老式的边界防御。你可以在这里 了解更多信息。\n面临的挑战是什么？ 尽管 TSB 目前通过智能用户控制和定制的分层访问策略提供了许多安全优势，但当组织试图将 Cilium 或 Calico 网络策略与服务网格基础架构中的现有基于身份的访问策略集成时，就会面临挑战。\n这种情况需要管理两组不同的策略，由不同的角色监督 - 安全管理员和平台所有者，这增加了许多组织今天所遇到的复杂性。\n组织如何保持基于身份的网格访问控制策略和基于 L3/L4 的网络策略保持同步。这就是 TSB 引入 网络策略建议 来解决的问题。\n什么是网络策略建议？ 从 TSB 1.7.0 版本开始，TSB 具备了建议 Kubernetes 网络策略的能力。这些建议是根据平台所有者或应用程序所有者在 TSB 中设置的分层访问控制策略而导出的。\n建议的网络策略已经通过 TSB 配置允许/拒绝的流量，这些策略作为一种便利提供，并以一种容易由安全团队和 Kubernetes 管理员管理和理解的形式提供。你可以检查网络策略，以验证 TSB 是否对你的网格应用适用适当的访问控制策略。\n可以通过在 ControlPlane CR 的 XCP 组件中将 ENABLE_NETWORK_POLICY_TRANSLATION 设置为 true 来在每个控制平面集群上启用此功能。一旦启用，建议的网络策略将作为专用控制平面集群中的命名空间范围配置映射存储。管理网络特定访问控制的平台所有者/安全所有者可以从配置映射中检索这些策略，并在其所需的命名空间中验证和应用这些策略。\n用例是什么？ TSB 主要关注以下用例。\n用例 1：建议保护南北流量 当用户将 Gateway 对象配置为 Edge Gateway 或 Ingress Gateway 时，TSB 可以确保通过建议的网络策略仅允许外部流量到达的暴露端口，例如 80 或 443，仅路由到 TSB 网关工作负载的端口，即 8080，8443 和 15443。\n先决条件 在应用 TSB 推荐的策略以使其生效之前，需要确保在各自的 Kubernetes 集群中启用了容器网络接口（CNI）插件的网络策略强制执行。 TSB 配置 配置一个 Edge Gateway 以暴露多个主机并路由到其他 Tier2 集群，其中部署了 IngressGateway 并配置了相同的主机名。\n# gateway-config.yaml apiVersion: v1 kind: List items: - apiVersion: install.tetrate.io/v1alpha1 kind: Tier1Gateway metadata: name: edge-gateway namespace: edge-gw spec: kubeSpec: service: type: LoadBalancer - apiVersion: gateway.tsb.tetrate.io/v2 kind: Gateway metadata: name: edge-gateway namespace: edge-gw annotations: tsb.tetrate.io/organization: tetrate tsb.tetrate.io/tenant: tetrate tsb.tetrate.io/workspace: edge-gw-ws tsb.tetrate.io/gatewayGroup: edge-gw-gp spec: workloadSelector: namespace: edge-gw labels: app: edge-gateway http: - name: bookinfo hostname: bookinfo.tetrate.io port: 80 routing: rules: - match: - uri : prefix: \u0026#34;/productpage\u0026#34; route: clusterDestination: clusters: - name: gke-tetrate-us-west1-1 weight: 100 - match: - uri: prefix: \u0026#34;/api/v1/products\u0026#34; route: clusterDestination: clusters: - name: gke-tetrate-us-east1-2 weight: 100 当你应用上述配置后，根据 TSB 创建的默认 Istio AuthZ 策略和创建用于暴露端口的 k8s 服务对象的 Tier1Gateway 安装 API 配置，TSB 将开始将两者转换为建议的网络策略配置的配置映射。\nkubectl get configmap -l xcp.tetrate.io/recommended-network-policy=true -n edge-gw NAME DATA AGE np-authz-edge-gateway 1 31s np-edge-gateway 1 31s 在应用这些策略之前，检索配置映射并验证建议的网络策略，这些策略将限制仅在服务配置的暴露端口上进行入站流量。\nkubectl get configmap -n edge-gw np-edge-gateway -o jsonpath=\u0026#39;{.data.policy}\u0026#39; \u0026gt; edge-gw-policy.yaml 建议的网络策略将限制仅在服务配置的暴露端口上进行入站流量。\n# edge-gw-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: creationTimestamp: null labels: xcp.tetrate.io/recommended-network-policy: \u0026#34;true\u0026#34; xcp.tetrate.io/service: edge-gateway name: np-edge-gateway namespace: edge-gw spec: ingress: - ports: - port: 15443 - port: 8080 - port: 8443 podSelector: matchLabels: app: edge-gateway istio: ingressgateway policyTypes: - Ingress status: {} 在集群上应用网络策略\nkubectl apply -f edge-gw-policy.yaml kubectl describe networkpolicy np-edge-gateway -n edge-gw Name: np-edge-gateway Namespace: edge-gw Created on: 2023-08-16 21:40:49 +0530 IST Labels: xcp.tetrate.io/recommended-network-policy=true xcp.tetrate.io/service=edge-gateway Annotations: \u0026lt;none\u0026gt; Spec: PodSelector: app=edge-gateway,istio=ingressgateway Allowing ingress traffic: To Port: 15443/TCP To Port: 8080/TCP To Port: 8443/TCP From: \u0026lt;any\u0026gt; (traffic not restricted by source) Not affecting egress traffic Policy Types: Ingress 用例 2：建议保护东西流量 在典型的基于租户/团队/业务单元的访问限制中，当用户配置 2 个租户，即 Tenant A 和 Tenant B，并将 deny_all 配置为 OrganizationSetting 的默认值以拒绝所有服务之间的通信时， 当用户为 Tenant A 配置允许策略以与 Tenant B 下的 workspace-frontend 进行通信，但不允许与 Tenant B 下的 workspace-backend 进行通信时， 当 TSB 创建 Istio AuthZ 策略以执行此行为时，TSB 还将为用户创建建议的网络策略作为配置映射，以便用户在 L3/L4 层强制执行相同的行为。\nTSB 配置 创建以下配置：\n创建一个新的租户 Marketing 并在 Marketing 租户下创建 2 个工作区 工作区 marketing-frontend 映射到 cluster-1 中的 marketing-frontend 命名空间 在 marketing-frontend 命名空间中部署 productpage 工作区 marketing-backend 映射到 cluster-1 中的 marketing-backend 命名空间 在 marketing-backend 命名空间中部署 details、reviews 和 ratings 创建一个新的租户 Payment 并在 Payment 租户下创建一个工作区 工作区 payment-chanel 映射到 cluster-1 中的 payment-channel 命名空间 在 payment-channel 命名空间中部署 sleep 服务 允许 payment-channel 仅与 marketing-frontend 进行通信，使用 TenantSettings apiVersion: tsb.tetrate.io/v2 kind: TenantSetting metadata: name: default-setting annotations: tsb.tetrate.io/organization: tetrate tsb.tetrate.io/tenant: marketing spec: defaultSecuritySetting: authenticationSettings: trafficMode: REQUIRED authorization: mode: RULES rules: allow: - from: fqn: organizations/tetrate/tenants/payment to: fqn: organizations/tetrate/tenants/marketing/workspaces/marketing-frontend - from: fqn: organizations/tetrate/tenants/marketing to: fqn: organizations/tetrate/tenants/marketing displayName: default-setting displayName: default-setting 应用上述配置后，TSB 将创建以下 AuthZ 策略。\nkubectl get authorizationpolicy -A NAMESPACE NAME AGE marketing-backend …","relpermalink":"/book/tsb/howto/network-polices/","summary":"网格内的网络策略 在服务网格的背景下，网络策略 是一组规则和配置，定义了各种微服务如何通过网络相互通信。 这些策略在控制流量流动、强制安全措施和维护服务网格架构的合规性方面起着关键作用。 网络策略使组织能够通","title":"网络策略"},{"content":"可以通过使用转换器配置 来扩展 Kustomize 以理解 CRD 对象。使用转换器配置，可以“教授”kustomize 有关 Rollout 对象的结构，并利用 kustomize 功能，例如 ConfigMap/Secret 生成器、变量引用以及通用标签和注释。要将 Rollouts 与 kustomize 结合使用：\n下载 rollout-transform.yaml 到你的 kustomize 目录。\n在你的 kustomize configurations 部分中包含 rollout-transform.yaml：\nkind: Kustomization apiVersion: kustomize.config.k8s.io/v1beta1 configurations: - rollout-transform.yaml 展示了使用 Rollouts 中的转换器的能力的 kustomize 应用程序示例可以在这里 看到。\n在 Kustomize 3.6.1 中，可以直接从远程资源引用配置： configurations: - https://argoproj.github.io/argo-rollouts/features/kustomize/rollout-transform.yaml 使用 Kustomize 4.5.5，kustomize 可以使用 Kubernetes OpenAPI 数据获取关于资源类型 的合并键和补丁策略信息。例如，给定以下 Rollout： apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollout-canary spec: strategy: canary: steps: # 详细的金丝雀步骤已省略 template: metadata: labels: app: rollout-canary spec: containers: - name: rollouts-demo image: argoproj/rollouts-demo:blue imagePullPolicy: Always ports: - containerPort: 8080 用户可以通过 kustomization 文件中的补丁来更新 Rollout，将镜像更改为 nginx：\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - rollout-canary.yaml openapi: path: https://raw.githubusercontent.com/argoproj/argo-schema-generator/main/schema/argo_all_k8s_kustomize_schema.json patchesStrategicMerge: - |- apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollout-canary spec: template: spec: containers: - name: rollouts-demo image: nginx OpenAPI 数据是自动生成的，并在此文件 中定义。\n展示了使用 OpenAPI 数据与 Rollouts 的 kustomize 应用程序示例可以在这里 看到。\n","relpermalink":"/book/argo-rollouts/rollout/kustomize/index/","summary":"可以通过使用转换器配置 来扩展 Kustomize 以理解 CRD 对象。使用转换器配置，可以“教授”kustomize 有关 Rollout 对象的结构，并利用 kustomize 功能，例如 ConfigMap/Secret 生成器、变量引用以及通用标签和注释。要将 Rollouts 与 kustomize 结合使用： 下载 rollout-transform.yaml 到你的 kustomize 目","title":"Kustomize 集成"},{"content":" 概览\n服务\n","relpermalink":"/book/argo-rollouts/notifications/","summary":"概览 服务","title":"通知"},{"content":"本章包括五个业界案例，他们是现实世界中部署了 SPIFFE 和 SPIRE 的企业的工程师。\nUber：用加密身份确保下一代和传统基础设施的安全 Ryan Turner，软件工程师，Uber\n在过去十年中，Uber 已经成为爆炸性增长的典型代表。随着软件服务的数量和我们运营的地理规模的增长，复杂性和风险也在增加。为了满足不断增长的需求，我们开始建立我们的下一代基础设施平台。同时，几年前，我们看到开源项目 SPIFFE 和 SPIRE 的一些早期动力。\n我们立即看到了 SPIFFE 所能带来的价值，使我们能够加强我们的下一代基础设施安全态势。我们在 Uber 上线了 SPIRE，现在正使用它在各种工作负载环境中使用可加密验证的身份建立信任。我们从一些应用服务和内部服务开始，比如一个工作流引擎，它通过访问整个平台的数据，旋转多个动态工作负载来完成特定任务。SPIRE 向我们的工作负载提供 SPIFFE 身份，跨越我们的应用周期。SPIFFE 用于验证服务，帮助我们避免可能导致生产问题的错误配置。\n使用 SPIRE 改造传统堆栈 SPIRE 现在是 Uber 的下一个基础设施的关键组成部分，但我们也在使用 sidecar 的方法，将认证改造成传统的基础设施。虽然 SPIFFE 和 SPIRE 通常都是在现代的云原生架构中工作，但我们可以将这些项目迅速适应我们专有的遗留堆栈。SPIRE 可以在 Uber 的下一代和传统基础设施中提供一个关键的信任桥梁，并对内部安全和开发人员的效率产生积极影响。\n在我们的旅程中，SPIFFE 社区一直非常支持我们，帮助我们找到解决方案。因此，我们的工程师也积极为项目做出代码贡献。\n安全、开发和审计团队正在受益于 SPIFFE SPIFFE 使我们的安全团队对后端基础设施更有信心，对基于网络的安全控制的依赖性更低。由于我们处理的是金融数据，而且是跨地域经营，我们必须控制对金融和客户数据的访问。有了 SPIRE，我们可以为访问控制提供一个强有力的证明身份。它帮助我们满足这些要求，并在这个过程中减少审计团队的负担。\n我们 Uber 的开发团队使用一致的客户端库，使用基于 SPIFFE 的身份创建 AuthZ 策略。这些项目使开发团队能够利用 X.509 和 JWT 等工作量大的身份基元，而不需要深入了解信任引导、安全引入、凭证供应或轮换等复杂主题。\nPinterest：用 SPIFFE 克服身份危机 Jeremy Krach，高级安全工程师，Pinterest\n2015 年，Pinterest 出现了身份危机。该公司的基础设施正在向不同的方向发展。每个新系统都以其独特的方式解决认证 —— 身份问题。开发人员每个月都要花几个小时的时间在会议和安全审查上，以设计、建立威胁模型和实施他们的定制身份解决方案，或将他们的新服务与不同的身份模型的依赖关系整合。很明显，安全团队需要建立一个通用的基础设施，以一种通用的方式提供身份，可以在我们的异构服务中使用。\n这个系统的最初草案将身份委托给机器，作为基于主机名的 X.509 证书。它被大量用于秘密管理（见 Knox ），但还没有出现更广泛的采用。随着我们不断扩大规模，特别是像 Kubernetes 这样的多租户系统，我们需要更精细的身份，这些身份并不与我们基础设施中的特定主机相联系，而是与服务本身的身份相联系。进入 SPIFFE。\n用 SPIFFE 将复杂的问题扁平化 SPIFFE 现在为我们的大部分基础设施提供统一的身份。我们最初从 Kubernetes 开始，因为在那个多租户环境中需求是最明确的。后来，我们将其他基础设施转移到 SPIFFE，作为其主要的身份识别形式。因此，Pinterest 的几乎每个服务都有一个标准化的名字，我们可以使用，没有晦涩的惯例或不连贯的方案。它帮助我们统一和规范了我们的身份惯例，这与其他内部项目保持一致，以确定服务属性，如服务所有权。\n我们利用 SPIFFE 作为 ACL 中的身份，用于秘密管理、TLS 服务之间的相互通信，甚至是通用授权策略（通过 OPA ，另一个 CNCF 项目）。Knox，Pinterest 的开源秘密管理服务，使用 SPIFFE X.509 身份文件作为支持的认证方法之一。请看我们关于 在 Knox 中添加 SPIFFE 支持的博文 。\n开发、安全和运维又开始和谐相处了 SPIFFE 使安全团队更容易编写授权策略。开发者的速度明显提高，因为我们的工程师不必担心自定义方案或不同的集成认证。由于我们现在有一个标准的方式来解释我们整个基础设施的身份，所以计费和所有权团队更容易确定谁拥有一项服务。有了强大的身份意识，对于记录和追踪一致性也很方便。我们对 SPIFFE 项目的未来感到兴奋，并感谢它能够帮助我们解决身份危机！。\n字节跳动：为网络规模的服务提供拨号音认证 Eli Nesterov，安全工程经理，字节跳动\nTikTok 背后的公司字节跳动已经在全球范围内建立和部署了大规模的互联网服务，为数百万用户提供服务。我们支持这些服务的基础设施是私有数据中心和公共云供应商的组合。我们的应用程序以数千个微服务的形式在多个 Kubernetes 集群和跨平台的专用节点上运行。\n随着我们规模的扩大，我们的平台上有多种认证机制，包括 PKI、JWT 令牌、Kerberos、OAuth 和自定义框架。在这些认证机制中加入大量的编程语言，操作的复杂性和风险就更大了。对我们的安全和运维团队来说，管理这些认证方案在操作上变得复杂。在认证框架出现已知漏洞的情况下，我们无法迅速采取行动，因为每个框架都必须单独处理。在某些情况下，他们有代码级的依赖性，这使得改变变得更加困难。跨地域的审计和合规性是一个挑战，因为每个平台特定的认证方法都必须被单独审查和管理。\n总体上走向基于零信任的架构，以及努力提高我们的开发人员的生产力，迫使我们为我们的服务建立一个统一的身份管理平面，以满足我们不断增长的需求。\n使用 SPIRE 构建网络规模的 PKI 要建立一个能在不同的基础设施岛屿或像我们这样的平台上工作的身份系统是很难的。我们可以创建自己的，但这需要大量的努力。我们选择了 SPIRE，因为它在支持我们需要的各种平台和网络规模方面提供了规模和灵活性。由于它提供了基于标准 X.509 证书的加密身份，它可以帮助我们轻松地启用相互 TLS，在默认情况下，它符合许多合规性要求。可扩展性和开源性是另一个优点，因为我们可以很容易地将它与我们现有的控制平面和数据堆栈集成。\n透明的认证简化了操作 有了 SPIRE，我们可以在我们所有的平台上部署一致的“拨号音 “认证。现在，认证和安全的负担从开发人员那里被封装起来，因此他们可以专注于业务或应用逻辑。这从整体上提高了我们的部署速度。我们也不太可能因为配置问题而出现” 生产错误 “，例如在生产中使用开发凭证。\n使用 SPIRE 的标准化认证也简化了合规性和审计，因为我们有跨信任域和平台的相互 TLS。SPIRE 还允许我们在身份分配方面转向一个更半分散的模式，即身份系统是本地的，比如一个数据中心。这提高了我们的整体可用性，使我们能够很好地恢复。\n有了 SPIRE，我们几乎是“面向未来 \u0026#34; 的，因为它可以扩展和适应，以满足我们不断增长的业务需求。\nAnthem：用 SPIFFE 保护云原生医疗应用的安全 Bobby Samuel，Anthem 人工智能工程副总裁\n行业内不断上升的医疗成本迫使像 Anthem 这样的组织迅速创新，重新思考我们与供应商、雇主团体和个人的互动方式。作为这一举措的一部分，我们正在开发大量的应用程序，这些应用程序将帮助我们通过安全地开放医疗数据的访问来推动成本下降。我们已经开始在 Kubernetes 等云原生技术的基础上建立配套的下一代基础设施。这个新的基础设施将推动快速创新，并吸引更广泛的组织和开发人员的生态系统。这方面的一个例子是我们的 HealthOS 平台。HealthOS 将使第三方能够建立 HealthApp 能力，以提供到前端界面，利用去识别的健康数据的海洋。\n但是，在几乎每一个大型企业，特别是医疗机构，都有人试图以恶意的方式获取他们的数据。受保护的医疗信息（PHI）的售价比金融信息高得多；因此，黑客和脚本小子等恶意行为者发现医疗系统和相应的健康信息非常有利可图。随着云原生架构的采用，风险和复杂性进一步上升。由于威胁半径大大增加，而人工安全审查和流程成为云规模的抑制因素，因此发生漏洞的风险更高。\n为零信任架构打下基础 我们不能依靠传统的基于参数的安全工具和流程来保护我们的下一代应用程序和基础设施。零信任是一种精细的、自动化的安全方法，对我们来说很有意义，特别是在未来，因为我们计划跨组织边界和云供应商进行操作。用户和服务的身份和认证是零信任安全模型的核心原则之一。零信任使我们能够减少对基于网络的控制的依赖，而不是对每个系统或工作负载进行认证。SPIFFE 和 SPIRE 为我们的零信任安全架构提供了一个基础认证层。它们允许每个工作负载在开始通信之前以加密方式证明 \u0026#34; 他们是谁”。\n摆脱秘密管理 通常，当你想到认证时，你会想到用户名、密码和 bear token。不幸的是，这些类型的凭证正在成为 Amthem 的风险和复杂性的来源。它们往往是长期存在的，对它们的管理或轮换是很棘手的。我们想摆脱这种一般的秘密管理做法。与其问一项服务 “你有什么”，我们想问的是” 你是谁 “。简而言之，我们想转向加密身份，如 SPIFFE。我们可以看到未来使用强证明身份的额外好处，比如在工作负载之间建立相互的 TLS，并将身份传导到应用程序中。\n利用 SPIFFE 将安全作为基础设施的一部分来建设 安全往往被开发团队认为是部署的障碍。DevOps 团队希望能更快地部署新的创新功能。然而，他们不得不通过与安全控制有关的人工工单、流程、集成和审查。在 Anthem，我们加倍努力为我们的开发团队消除障碍，使安全成为基础设施的一项功能。随着 SPIFFE 等技术的采用，我们可以将安全控制的复杂性从开发团队中抽象出来，并在各种平台上提供一致的规则。SPIFFE 以及其他基于零信任的技术，将帮助我们在大多数情况下将系统供应时间从三个月缩短到两周以内。在 SPIFFE 的引领下，安全正在成为 Anthem 的一个推动因素。\nSquare：将信任扩展到云端 Michael Weissbacher 和 Mat Byczkowski，高级安全工程师，Square\nSquare 提供各种各样的金融服务。在其生命周期中，该公司从内部发展了新的业务部门，如资本和现金，但也收购了 Weebly 和 Stitch Labs 等公司。不同的业务部门使用不同的技术，可以从不同的数据中心和云端运作，同时仍然需要无缝沟通。\n我们内部开发的服务识别系统需要扩展到 Square 为其数据中心开发的内部架构之外。我们希望将该系统扩展到云端，我们希望提供一个同样安全的系统，并在未来几年内为我们提供良好的服务。我们最理想的是寻找一个基于开放标准的工具，同时能与 Envoy 代理无缝集成。SPIFFE 和 SPIRE 都支持我们的发展目标，以及与多个云和部署工具合作的独立平台。\n一个能与流行的开源项目合作的开放标准 由于 SPIFFE 是基于现有的开放标准，如 X.509 证书，它为我们的服务身份提供了一条清晰的升级路径。Envoy 是 Square 的应用程序如何进行通信的基础构建块。由于 SPIRE 支持 Envoy 的 Secrets Discovery API，因此获得 X509-SVID 很容易。Envoy 内置访问控制，可以使用 SPIFFE 身份来决定允许哪些应用程序进行通信。\n我们将 SPIRE 架构与现有的服务身份识别系统并行部署，然后对各种内部工具和框架进行修改，以支持这两个系统。接下来，我们将 SPIRE 与部署系统集成，在 SPIRE 中注册所有服务。这意味着我们可以对 SPIRE 的频繁的 SVID 轮换进行压力测试。最后， …","relpermalink":"/book/spiffe/practitioners-stories/","summary":"本章包括五个业界案例，他们是现实世界中部署了 SPIFFE 和 SPIRE 的企业的工程师。 Uber：用加密身份确保下一代和传统基础设施的安全 Ryan Turner，软件工程师，Uber 在过去十年中，Uber 已经成为爆炸性增长的典型代","title":"业界案例"},{"content":"如果您认为云原生基础架构是可购买的产品或是从云供应商那购买的服务器，我们很抱歉让您失望了。如果不采用这些做法并改变您建设和维护基础架构的方式，您就不会受益。\n它不仅仅影响服务器、网络和存储。它关乎的是工程师如何管理应用程序，就像接受故障一样。\n围绕云原生实践建立的文化与传统技术和工程组织有很大不同。我们并不是解决组织文化或结构问题的专家，但如果您希望改变组织结构，我们建议您从高绩效组织中实施 DevOps 实践的角度来看待价值观和经验教训。\n一些需要探索的地方是 Netflix 的文化套餐，它促进了自由和责任感，还有亚马逊的双比萨团队，这些团队以低开销推广自治团体。云原生应用程序需要与构建它们的团队具有相同的解耦特征。康威定律很好地描述了这一点：“设计系统的架构受制于产生这些设计的组织的沟通结构。”\n在我们结束本书时，我们希望关注哪些领域是您采用云原生实践时最重要的。我们还将讨论一些预测变化的基础架构模式，以便您知道将来要寻找什么。\n关注改变的地方 如果您拥有现有的基础架构或传统数据中心，则过渡到云原生不会在一夜之间发生。如果您有足够大的基础架构或两个以上的人员管理它，试图强制实施基础架构管理的新方法可能会失败。\n采用这些模式与配置新服务器或购买新软件无关。要开始采用云原生基础架构，最重要的是是首先关注这些领域：\n人 架构 混乱 应用程序 直到您准备好这些区域以使用本书中描述的实践，才能开始更改基础架构。\n人 比竞争对手更快学习的能力可能是唯一的可持续竞争优势。\n——Arie de Geus\n正如我们在第 2 章中所讨论的，人是实施任何变革中最难的部分。他们的抵制有很多原因，而那些要求进行变更以帮助其影响的人有责任。\n当需要改变的驱动激励时，变更更容易。主动提高潜力是一个很好的激励因素，但如果没有紧迫感，就很难改变行为。\n人们抗拒改变的原因大多来自恐惧。人们喜欢习惯，因为他们可以控制并避免意外。\n为了使任何技术取得成功的重大转变，您需要与人们合作以最大限度地减少他们的恐惧。给他们一种主人翁感，并解释变化的明确目标。确保突出新旧技术之间的相似之处，特别是在改变后他们将扮演的角色。\n人们了解这种变化并不是因为他们在旧系统或现有系统中的失败，这一点也很重要。他们需要明白，要求已经改变，环境不同，并且希望他们成为变革的一部分，因为你尊重他们所做的并且对他们能做的事有信心。\n他们需要学习新事物，为此，失败是必要的，预期的，并且是进步的标志。\n鼓励学习和实验，并奖励适应数据洞察的人员和系统。让工程师通过诸如“百分之二十的时间”之类的自由来探索新的可能性，可以做到这一点。\n如果敏捷系统没有改变，它就没有好处。一个不适应和改进的系统将无法满足正在改变和学习的企业的需求。\n一旦你能够激发人们寻求改变，你应该用信任和自由来赋予他们力量。不断引导他们将自己的目标与业务需求结合起来，并赋予他们应聘的管理职责。\n如果人们已经准备好采用本书中的做法，那么实现它就没有什么限制。关于创建组织变革的更深入的指导，我们推荐阅读 John P. Kotter（哈佛商业评论出版社）的“领导变革”。\n改变环境文化需要组织的大量努力和支持，以及更改应用程序运行的基础架构。您选择的架构可能会对采用云原生模式的能力产生重大影响。\n架构 弹性、安全性、可伸缩性、可部署性、可测试性是架构问题。\n——Jez Humble\n将应用程序迁移到云原生基础架构时，您需要考虑如何管理和设计应用程序。例如，作为云原生应用程序前身的 12 因子应用程序受益于在平台上运行。它们被设计为最小化手动管理，频繁更改和弹性。许多传统应用程序的架构都是为了抵制自动化，不经常升级和失败。在迁移它之前，您应该考虑应用程序的架构。\n单个应用程序架构是一个问题，但您还需要考虑应用程序如何与基础架构内的其他服务通信。应用程序应已云环境中支持的协议以及通过明确界定的接口进行通信，通过采用微服务保持应用程序范围很小可以帮助定义应用程序间接口和提高应用程序部署速度。但是，采用微服务会暴露出新的问题，如应用程序通信速度较慢以及分布式跟踪和策略控制网络的需求。如果不能为您的基础架构提供好处，就不要采用微服务。\n虽然您几乎可以适应任何应用程序在容器中运行并使用容器编排器进行部署，但如果首选迁移所有关键业务数据库服务器，您将很快就会后悔。\n首先确定接近具有第 1 章概述的特性的应用程序并获得在云原生环境中运行它们的经验。一旦您围绕简单的应用程序集体获得经验和良好实践，那么您就可以决定接下来要做什么。\n您的服务器和服务也不例外。在将基础架构切换为不可变的之前，您应确保它解决了当前的问题，并意识到新问题。\n可靠系统中最重要的架构考虑是争取简单的解决方案。软件和系统自然会变得复杂，这会造成不稳定。在云中，您可以释放对许多区域的控制，因此在仍然可以控制的区域保持简单性很重要。\n无论您将内部部署基础架构迁移到云中还是创建新的解决方案，都要确保您在第 1 章中进行可用性数学计算，并为混乱情况做好准备。\n混沌管理 拥抱失败并期待混乱。\n——Netflix 的 Andrew Spyker\n当您构建云原生应用程序和基础架构时，其目标是创建最小可行产品（MVP）和迭代。尝试指定您的客户需要什么或他们将如何使用您的产品可能会有用一段时间，但成功的应用程序将适应而不是预测。\n客户需求改变；应用程序需要随它们改变。您无法计划和构建完整的解决方案，因为在产品准备就绪时，需求已发生变化。\n保持敏捷并在现有技术基础上发展很重要。就像您为应用程序导入库一样，您应该为基础架构使用 IaaS 和 SaaS。你越努力建立自己，你就越能提供价值。\n无论何时释放对某物的控制，都会冒着意想不到的风险。如果因为导入的库已更新而导致应用程序中断，您将会知道这是什么感觉。\n您的应用程序依赖于库所提供的功能，该功能已更改。你是否应该删除库并编写自己的功能以便控制它？答案几乎总是不。相反，您更新应用程序以使用新库，或者将正在运行的较旧版本的库与应用程序捆绑在一起，以暂时避免损坏。\n运行在公有云上的基础架构也是如此。您不再控制硬连线网络，使用什么 RAID 控制器，或者虚拟机的哪个版本的虚拟机管理程序运行。您所拥有的只是可以在底层技术之上提供抽象的 API。\n您无法控制底层技术何时发生变化。如果云提供商弃用所有大型内存实例类型，则您别无选择，只能遵守。您要么适应新的尺寸，要么支付更换提供商的成本（时间和金钱）（请参阅附录 B 关于锁定）。\n最重要的是，您构建的基础架构不再可以从单个关键服务器获得。如果您采用云原生基础架构，无论您是否喜欢，您正在构建一个分布式系统。\n通过简单地避免失败来保持服务可用的旧做法不起作用。目标不再是您可以设计的最大数目的几个 9—— 而是你可以摆脱的最小数量的几个 9。\n站点可靠性工程（Site Reliability Engineering）以这种方式解释它：\n坚持 SLO 将百分之百满足是不现实的和不可取的：这样做可能会降低创新和部署的速度，需要昂贵的，过于保守的解决方案，或两者兼而有之。相反，最好允许一个错误预算 —— 一个可能错过 SLO 的速率，并且每天或每周对其进行跟踪。\n工程的目标不可用；它创造了商业价值。你应该制造弹性系统，但不要以过度工程解决方案为代价来避免混乱。\n测试更改以防止停机的旧方法也不起作用。为大型分布式系统创建测试环境并不重要。当服务经常更新并且部署是自助服务时尤其如此。\n当 Netflix 每天有 4,000 次部署或 Facebook 有 10,000 个同时运行的版本时，对环境进行快照是不可能的。测试环境需要动态分离生产部分。基础架构需要支持这种测试方法，并支持经常测试生产中的新代码所带来的失败。\n你可以测试一些混乱（参见第 5 章），但混沌根据定义是不可预测的。准备您的基础架构和应用程序，以便可预测地对混乱做出反应，而不要试图避免它。\n应用程序 基础架构的目的是运行应用程序。如果您的业务完全基于向其他公司提供基础架构，那么您的成功仍取决于其运行应用程序的能力。\n如果你建立它，他们不能保证来用。您创建的任何抽象需要提高运行应用程序的能力。\n避免“泄漏抽象”（Leaky Abstraction）\n抽象并不完全隐藏抽象的实现细节。泄漏抽象定律指出：“所有非平凡的抽象在一定程度上都是泄漏的。”\n这意味着你进一步抽象某事，隐藏事物的细节就越困难。\n例如，应用程序资源请求通常通过请求 CPU 核心的百分比，内存量和磁盘存储量来抽象化。这些资源的物理分配不直接由应用程序管理（API 不是由它们规定的），但对资源的请求很明显地表明运行应用程序的系统具有可用的这些类型的资源。\n相反，如果抽象是服务器或数据中心（例如，50 台服务器，0.2 个数据中心）的百分比，那么抽象并不具有相同的含义，因为不存在单一大小的服务器或数据中心单元。确保创建的抽象对于将使用它们的应用程序有意义。\n云原生实践的好处是，随着您提高运行应用程序的能力，您还可以提高运行基础架构的能力。如果您遵循第 3-6 章的模式，您将很好地适应使用应用程序不断改进和调整您的基础架构以满足应用程序的需求。\n重点关注构建范围小，易于适应，易于操作和故障发生时具有弹性的应用程序。确保这些应用程序负责所有基础架构管理和更改。如果你这样做，你将创建云原生基础架构。\n预测未来 如果您已经采用了本书中的模式和实践，那么您现在处于未知领域。运行全球最大基础架构的公司已采用了这些做法。无论他们运行基础架构的新模式是否公开披露或仍在发现之中。\n好消息是您的基础架构现在被设计为敏捷和变化。您可以更轻松地适应您遇到的任何新挑战。\n为了展望未来的道路，我们可以不考虑现有的基础架构模式，而是考虑基础架构在哪里获得灵感 —— 软件。几乎所有基础架构采用的模式都来自软件开发模式。\n例如，分布式应用程序。很多年前，当软件暴露于互联网时，在单一代码库下的单个服务器上运行应用程序不会扩展。这包括管理应用程序的性能和流程限制。\n该应用程序需要复制到其他服务器上，然后进行负载均衡以满足需求。当达到限制时，它被分解成更小的组件，创建 API 以通过 HTTP 远程调用功能，而不是通过应用程序库。\n基础架构采取了类似的方法来扩大规模；在大多数领域只适应比软件更慢的速度。像 Kubernetes 这样的现代化基础架构平台将基础架构管理分解成更小的组件。这些组件可以根据其性能瓶颈（CPU、内存和 I/O）独立扩展，并且可以快速迭代。\n有些应用程序没有相同的扩展需求，也不需要适应新的架构。工程师的需要知道何时采用新技术。只有那些了解其堆栈的局限性和瓶颈的人才能决定什么是正确的方向。\n当发现新的限制时，请密切注意新的解决方案应用程序的发展。如果您了解限制是什么以及进行更改的原因，那么您将始终处于基础架构的领先地位。\n结论 本书的目标是帮助您更好地理解什么是云原生基础架构以及您为什么要采用它。虽然我们相信它比传统基础架构有很多优势，但我们不希望您在不理解它的情况下盲目使用任何技术。\n不要期望在不采用其伴随的文化和流程的情况下获得所有好处。只是在公有云中运行基础架构或运行容器编排器不会改变该基础架构如何适应的过程。\n在 AWS 中手动创建少量虚拟机，通过 SSH 连接到每台虚拟机，创建 Kubernetes 集群比改变人们的工作方式更容易。前者不是云原生的。\n请记住，配置基础架构的应用程序不是及时的静态快照；它们应该不断运行并将基础架构推向期望的状态。管理基础架构不是维护主机。您需要为资源创建抽象，用 API 来表示这些抽象，以及使用它们的应用程序。\n目标是满足您的应用程序的需求，并编写与您的业务流程和工作职能相当的软件应用程序。随着流程和功能的频繁变化，软件需要轻松适应。\n掌握它时，您将不断发现在创建新抽象，保持服务弹性以及推动可伸缩性极限方面的挑战。如果您能够找到新的限制和有用的抽象，请回馈给您所属的社区。\n开源并通过研究和社区回馈是这些模式是使得它们从先驱环境中出现。共享使更多的创新和见解得以传播，并使每个人更容易 …","relpermalink":"/book/cloud-native-infra/implementing-cloud-native-infrasctructure/","summary":"如果您认为云原生基础架构是可购买的产品或是从云供应商那购买的服务器，我们很抱歉让您失望了。如果不采用这些做法并改变您建设和维护基础架构的方式，您就不会受益。 它不仅仅影响服务器、网络和存储。它关乎的是工","title":"第 9 章：实施云原生基础架构"},{"content":"遵循本文件中概述的加固指南是确保在 Kubernetes 协调容器上运行的应用程序安全的一个步骤。然而，安全是一个持续的过程，跟上补丁、更新和升级是至关重要的。具体的软件组件因个人配置的不同而不同，但整个系统的每一块都应尽可能保持安全。这包括更新：Kubernetes、管理程序、虚拟化软件、插件、环境运行的操作系统、服务器上运行的应用程序，以及 Kubernetes 环境中托管的任何其他软件。\n互联网安全中心（CIS）发布了保护软件安全的基准。管理员应遵守 Kubernetes 和任何其他相关系统组件的 CIS 基准。管理员应定期检查，以确保其系统的安全性符合当前安全专家对最佳实践的共识。应定期对各种系统组件进行漏洞扫描和渗透测试，主动寻找不安全的配置和零日漏洞。任何发现都应在潜在的网络行为者发现和利用它们之前及时补救。\n随着更新的部署，管理员也应该跟上从环境中删除任何不再需要的旧组件。使用托管的 Kubernetes 服务可以帮助自动升级和修补 Kubernetes、操作系统和网络协议。然而，管理员仍然必须为他们的容器化应用程序打补丁和升级。\n","relpermalink":"/book/kubernetes-hardening-guidance/upgrading-and-application-security-practices/","summary":"遵循本文件中概述的加固指南是确保在 Kubernetes 协调容器上运行的应用程序安全的一个步骤。然而，安全是一个持续的过程，跟上补丁、更新和升级是至关重要的。具体的软件组件因个人配置的不同而不同，但整个系统的每一块都应尽","title":"升级和应用安全实践"},{"content":"路线图很快就会过时。有关最新的路线图，请参见 OpenTelemetry 状态 页面。也就是说，以下是截至目前 OpenTelemetry 的状态。\n核心组件 目前，OpenTelemetry 追踪信号已被宣布为稳定的，并且在许多语言中都有稳定的实现。\n度量信号刚刚被宣布稳定，测试版的实施将在 2022 年第一季度广泛使用。\n日志信号预计将在 2022 年第一季度宣布稳定，测试版实现预计将在 2022 年第二季度广泛使用。2021 年，Stanza 项目被捐赠给 OpenTelemetry，为 OpenTelemetry 收集器增加了高效的日志处理能力。\n用于 HTTP/RPC、数据库和消息系统的语义公约预计将在 2022 年第一季度宣布稳定。\n未来 完成上述路线图就完成了对 OpenTelemetry 核心功能的稳定性要求。这是一个巨大的里程碑，因为它打开了进一步采用 OpenTelemetry 的大门，包括与数据库、管理服务和 OSS 库的原生集成。这些集成工作大部分已经以原型和测试版支持的形式在进行。随着 OpenTelemetry 的稳定性在 2022 年上半年完成，我预计在 2022 年下半年将出现支持 OpenTelemetry 的宣言，使 2022 年成为 “OpenTelemetry 之年”。\n但我们并没有就此止步。下一步是什么？\neBPF Extended Berkeley Packet Filter（eBPF）是一种在 Windows、Linux 和其他类似 Unix 的操作系统上提供底层网络访问的机制。利用 eBPF 将给 OpenTelemetry 提供一种极其有效的网络监控形式，不需要任何开发人员的工具。\n2021 年，Flowmill 项目被捐赠给了 OpenTelemetry。Flowmill 是一个基于 eBPF 的可观测性解决方案，专门设计用于观测分布式系统。Flowmill 的开发者与 Pixie 项目的开发者一起，正在努力为 OpenTelemetry Collector 增加 eBPF 支持。Pixie 是专门为 Kubernetes 设计的基于 eBPF 的观测工具，是 CNCF 旗下 OpenTelemetry 的一个姊妹项目。我们还在一起研究如何将底层（第 2 层）eBPF 数据与高层（第 7 层）分布式追踪数据进一步关联起来，这在业界尚属首次。\nRUM 真实用户监控（RUM）是一种可观测性工具，用于描述用户在长期运行的用户会话中如何与移动、网络和桌面客户端进行交互。RUM 与分布式追踪不同，因为图形用户界面（GUI）往往是基于反应器的系统，与数据库和网络服务器等基于事务的系统有根本的架构差异。长话短说：你不能把一个用户会话建模为一个追踪，然后就收工了。\nClient Instrumentation SIG 目前正在开发一个新的 RUM 设计，它将扩展并与 OpenTelemetry 现有的分布式追踪、指标和日志信号完全整合。\nOpenTelemetry 控制平面 目前，OpenTelemetry 收集器和 SDK 是作为独立的单元来管理的，必须重新启动才能改变它们的配置。目前，Agent Management SIG 正在开发一个控制平面，它可以报告这些组件的当前状态，并允许实时改变配置。它将允许运维人员或自动服务动态地控制整个遥测管道的处理。\n动态配置将允许对采样、日志水平和其他形式的资源管理进行细粒度、反应灵敏的控制，从而减少成本。最终，这个控制平面可以实现更先进的基于尾部的采样形式，即需要在整个部署中协调的采样技术。\n列式编码的 OTLP 目前的 OpenTelemetry 协议是对 OpenTelemetry 数据模型的一种有效而直接的编码。跨度、度量和日志被编码为跨度、度量和日志。这可以被认为是一个基于行的模型，每个跨度、度量和日志都被编码为元素列表中的一个离散元素。基于列的模型则将每个元素编码为一个单一的表格，其中所有元素共享相同的列，用于重叠的概念，如时间戳和属性。\n这种列式表示法有望优化数据批次的创建、大小和处理。这种方法的主要好处是：\n更好的数据压缩率（一组相似的数据）。 更快的数据处理（更好的数据定位意味着更好地使用 CPU 缓存线）。 更快的序列化和反序列化（更少的对象需要处理）。 更快的批量创建（更少的内存分配）。 更好的 I/O 效率（传输的数据更少）。 这种方法的好处与批次的大小成比例地增加。使用现有的 “面向行” 的表示方法很适合小批量的情况。因此，列式编码将扩展当前的协议。目前提出的实施方案是基于 Apache Arrow，这是一种成熟的列式内存格式。\n这种优化的重点是允许高容量的数据源，如 CDN 和大型多租户系统，像常规服务一样参与可观测性。列式编码也将减少跨网络边界的遥测出口的成本。\n","relpermalink":"/book/opentelemetry-obervability/roadmap/","summary":"附录 B：OpenTelemetry 项目路线图","title":"附录 B：OpenTelemetry 项目路线图"},{"content":"欢迎使用 Tetrate Service Bridge (TSB) 版本 1.6 的发行说明。此版本引入了多项新功能，可增强可用性、安全性和可见性。TSB 继续提供统一的方法来连接和保护不同环境中的服务，包括 Kubernetes 集群、虚拟机和裸机工作负载。\n主要亮点 跨集群高可用性和安全性 TSB 1.6 专注于通过将远程集群更紧密地结合在一起以简化管理和可扩展性来提高可用性和安全性：\n所有服务的跨集群高可用性：引入 EastWestGateway 功能，实现集群之间的自动服务故障转移，无需外部网关。最大限度地提高服务可用性、简化故障转移并增强安全性。 跨集群身份传播和安全域：创建跨集群的可扩展安全策略，确保本地、远程和故障转移服务的访问控制规则一致。 增强可见性和故障排除 高级可见性和跟踪工具：使应用程序开发人员能够解决跨集群的分布式应用程序中的性能问题。利用 tctl collect 导出运行时数据以进行离线分析，并使用 tctl troubleshoot 进行深入调查。 附加功能和灵活性 WASM 扩展支持：使用 WebAssembly (WASM) 扩展通过自定义功能扩展代理（网关和服务代理）的功能。加速创新、降低成本并执行全球应用政策。 红帽 OpenShift 集成 Red Hat OpenShift 上的可用性：TSB 1.6 可通过 Red Hat 生态系统目录在 Red Hat OpenShift 上使用。获得多集群 OpenShift 环境的可观测性、安全性和流量管理。 面向未来的安全 技术预览：Tetrate Web 应用程序防火墙 (WAF)：了解 Tetrate 即将推出的 Web 应用程序防火墙，为内部和外部的所有服务提供高级 L7 保护。 TSB 1.6 的受益者 TSB 1.6 为组织内的各种角色带来好处：\n平台运维者：高效管理多集群平台，提高平台用户的可用性、安全性和可见性能力，轻松驾驭异构环境。 服务所有者：增强跨集群的服务可用性，远程解决性能问题，并与应用程序开发人员有效协作。 安全团队：在零信任架构中应用精确的安全策略，确保跨集群的准确且一致的访问控制。 平台运维者、服务所有者和安全团队：通过 WASM 扩展使用自定义功能扩展代理功能。 TSB 1.6 中的显着功能 跨集群高可用 EastWestGateway：实现集群之间无缝、自动的服务故障转移。最大限度地提高可用性、确保透明度并增强安全性。 增强的故障排除功能 高级可见性和跟踪工具：为应用程序开发人员提供快速识别和解决性能问题的工具。 OpenShift 兼容性 经过认证的 OpenShift 兼容性：使用红帽生态系统目录在红帽 OpenShift 上自信地部署 TSB 1.6。 WASM 扩展 自定义功能：利用 WebAssembly (WASM) 扩展来增强应用程序功能并强制执行策略。 安全与身份 安全域和身份传播：部署一致的安全策略并跨集群安全地传播服务身份。 Istio 增强功能 分段和多 Istio 支持：实现隔离边界并支持集群内的多个 Istio 版本。 Tetrate Web 应用程序防火墙 (WAF) - 技术预览 高级 L7 保护：深入了解即将推出的 Tetrate Web 应用程序防火墙，以实现全面的服务保护。 有关改进的完整列表，请参阅 TSB 1.6 发行说明。\n入门 开始使用 Tetrate Service Bridge 1.6：\n查看初始要求 并选择合适的平台。 根据你的需求选择部署选项：快速演示安装、生产就绪设置或升级现有部署。 请联系 Tetrate 支持寻求任何帮助。 感谢你选择 Tetrate Service Bridge！\n","relpermalink":"/book/tsb/release-notes-announcements/","summary":"Tetrate Service Bridge 1.6 发行说明。","title":"TSB 1.6 发行说明"},{"content":"从版本 1.5 开始，TSB 提供了一种自动获取来自远程私有 Docker 容器仓库的镜像的方式，方法是在 ManagementPlane 和 ControlPlane CRs 中定义 imagePullSecrets。 如果定义了 imagePullSecrets，则将使用秘密中的凭据对必需的 ServiceAccounts 进行补丁，从而允许安全地访问存储在远程私有仓库中的容器。以下步骤概述了配置过程：\n同步镜像 TSB 镜像位于 Tetrate 的仓库中，只能复制到你的仓库（不允许直接在任何环境中下载）。第一步是将镜像传输到你的仓库。要同步镜像，你需要按照 文档 使用 tctl install image-sync（需要 Tetrate 提供的许可密钥）。\n获取私有仓库的 JSON 密钥 指定为 imagePullSecrets 的秘密将存储凭据，允许 Kubernetes 从私有仓库中拉取所需的容器。获取凭据的方式取决于仓库。请参考以下链接以获得主要云提供商的指导 - AWS 、GCP 和 Azure 。\n在 TSB 使用的每个命名空间中创建机密 如 Kubernetes 文档 中所述，机密只能被在其创建的相同命名空间中的 pod 访问。因此，必须为 TSB 使用的每个命名空间创建一个单独的机密。请注意，可用的命名空间可能会根据 Kubernetes 平台而变化。\n目前，以下命名空间需要一个单独的机密：\n对于 TSB 管理平面集群 tsb 和 cert-manager（如果使用内部 TSB 打包的 cert-manager） 对于 TSB 控制平面集群 istio-system、istio-gateway、cert-manager（如果使用内部 TSB 打包的 cert-manager）和 kube-system（如果使用 Istio CNI） 附加的命名空间 上述提供的列表不是详尽无遗的。不同平台上可能会使用额外的命名空间来运行 TSB 组件，因此需要创建单独的机密。要检查是否有任何 pod 无法获取容器镜像的问题，可以使用命令 kubectl get pods -A | grep ImagePullBackOff。 应用程序命名空间 为了确保启用 Istio 的应用程序能够下载镜像，需要在每个启用了 Istio Sidecar 的 pod 和 Ingress Gateway 的应用程序命名空间中存在仓库凭据机密。\n安装 TSB 要安装 TSB，请使用你喜欢的方法，但确保 ManagementPlane 和 ControlPlane CRs 配置为如下所示的 imagePullSecrets：\nspec: ... imagePullSecrets: - name: \u0026lt;在上一步中创建的机密名称\u0026gt; ... 补丁 Operator ServiceAccounts 在 Operator 能够将 imagePullSecrets 传播到其余组件之前，TSB Operator 的镜像需要凭据。\n步骤如下：\n补丁 istio-system 和 istio-gateway 命名空间中 TSB 运算符的 ServiceAccounts：\nkubectl patch serviceaccount tsb-operator-control-plane -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;\u0026lt;在上述步骤中创建的机密名称\u0026gt;\u0026#34;}]}\u0026#39; -n istio-system kubectl patch serviceaccount tsb-operator-data-plane -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;\u0026lt;在上述步骤中创建的机密名称\u0026gt;\u0026#34;}]}\u0026#39; -n istio-gateway 在这些命名空间中重新启动运算符：\nkubectl delete pod -n istio-system -l=name=tsb-operator kubectl delete pod -n istio-gateway -l=name=tsb-operator kubectl delete pod -n istio-gateway -l=name=istio-operator Helm chart 安装 可以使用 Helm 安装 自动执行创建机密和定义 imagePullSecrets 的步骤。 步骤的顺序 非常重要的是，在安装 TSB 之前创建私有仓库的 Kubernetes 机密。遵循这个正确的顺序将允许有效的部署并将任何停机时间降到最低。 ","relpermalink":"/book/tsb/setup/remote-registry/","summary":"配置 TSB 以从远程私有仓库获取容器镜像。","title":"仓库机密"},{"content":"本文描述了如何自定义 Kubernetes 部署的 TSB 组件，包括使用覆盖来执行由 Tetrate Service Bridge (TSB) Operator 部署的资源的高级配置，使用示例说明。\n背景 TSB 广泛使用Operator（Operator） 模式在 Kubernetes 中部署和配置所需的部分。\n通常，通过 Operator 来进行自定义和微调参数，Operator 负责创建必要的资源并控制其生命周期。\n例如，当你创建一个 IngressGateway CR 时，TSB Operator会获取此信息并部署和/或更新相关资源，如 Kubernetes Service 对象，通过创建清单并应用它们。清单将使用你提供的某些参数，以及由 TSB 计算的其他默认值。\n然而，TSB 并不一定会暴露用于微调 Service 对象的所有参数。如果 TSB 提供了所有用于配置 Service 对象的钩子，那么 TSB 将不得不实际上复制整个 Kubernetes API，这在现实中既不可行也不可取。\n这就是我们使用覆盖的地方，它允许你覆盖并应用于正在部署的资源的自定义配置。有关覆盖工作原理的更多详细信息，请阅读参考文档中关于覆盖的文档 。\n注意 覆盖作为 TSB 功能的一种逃生机制提供，应谨慎使用。当前可能通过覆盖可用的配置很可能会在未来通过 TSB Operator 来执行。 示例说明 在接下来的示例中，使用 kubectl edit 直接编辑部署的清单来应用必要的配置。如果你拥有原始清单，你也可以选择使用 kubectl apply，但你必须提供整个资源定义，而不仅仅是要编辑的部分。\n示例清单只显示了需要指定的最小信息，以及指定要进行这些更改的上下文（位置）的信息。\n注意 根据你的特定 Kubernetes 环境，你可能需要修改示例的内容以使其正常运行。 Helm 安装 本文档中提供的所有示例也可以应用于 Helm 安装，通过编辑管理平面或控制平面 Helm 值来实现。\n对于管理平面，Helm 值 中的 spec 字段与 TSB ManagementPlane CR 相同。\n对于控制平面，Helm 值 中的 spec 字段与 TSB ControlPlane CR 相同。\nOpenShift 如果你使用 OpenShift，请使用以下命令将下面的 kubectl 命令替换为 oc。 一旦你研究了这些示例，你可能会使用更复杂的覆盖来进行工作。你应该知道的一个注意事项是，你只能为每个对象有一个覆盖。例如，以下规范在语法上是有效的，但只会应用于 quux.corge.grault 的最后一个 patch：\nkubeSpec: overlays: - apiVersion: v1 kind: .... name: my-object patches: - path: foo.bar.baz value: 1 - apiVersion: v1 kind: .... name: my-object patches: - path: quux.corge.grault value: hello 这是因为清单在 overlays 下包含了指向同一个对象（my-object）的多个条目，在这种情况下只有最后一个条目实际应用。要对 foo.bar.baz 和 quux.corge.grault 进行修补，必须将所有 patch 规范合并到单个对象下，如下所示：\nkubeSpec: overlays: - apiVersion: v1 kind: .... name: my-object patches: - path: foo.bar.baz value: 1 - path: quux.corge.grault value: hello 覆盖示例用法 配置具有提升权限的 CNI 某些环境，如 SELinux 或 OpenShift，需要特殊权限才能在主机系统中写入文件。要启用此功能，必须通过编辑 ControlPlane CR 来将 install-cni.securityContext.privileged 属性设置为 true。\n使用 kubectl edit 编辑 TSB 控制平面的 ControlPlane CR，并使用以下代码片段作为如何编辑清单的示例。\nkubectl edit controlplane -n istio-system spec: components: istio: kubespec: overlays: - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator name: tsb-istiocontrolplane patches: - path: spec.components.cni.k8s value: overlays: - apiVersion: extensions/v1beta1 kind: DaemonSet name: istio-cni-node patches: - path: spec.template.spec.containers.[name:install-cni].securityContext value: privileged: true 更改 XCP 服务类型 对于某些环境，XCP 边缘无法使用 LoadBalancer 服务类型，或者需要添加注释。你可以通过将以下覆盖应用于 ControlPlane CR 来修改它们：\nspec: components: xcp: kubeSpec: overlays: - apiVersion: install.xcp.tetrate.io/v1alpha1 kind: EdgeXcp name: edge-xcp patches: - path: spec.components.edgeServer.kubeSpec.service.annotations value: traffic.istio.io/nodeSelector: \u0026#39;{\u0026#34;beta.kubernetes.io/os\u0026#34;: \u0026#34;linux\u0026#34;}\u0026#39; - path: spec.components.edgeServer.kubeSpec.overlays value: - apiVersion: v1 kind: Service name: xcp-edge patches: - path: spec.type value: NodePort 保留端点 IP 地址 Kubernetes 提供了一种保留连接到应用程序的客户端的 IP 地址的方法，可以用来将流量路由到节点本地或整个集群范围的端点。\n对于此示例，假设你已部署了以下 Ingress Gateway，并且其服务类型为 LoadBalancer：\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: tsb-gateway-bookinfo namespace: bookinfo spec: kubeSpec: service: type: LoadBalancer 使用 kubectl edit 编辑应用程序的 IngressGateway CR，并使用以下代码片段作为如何编辑清单的示例。\nkubectl edit tsb-gateway-bookinfo -n bookinfo spec: connectionDrainDuration: 10s kubeSpec: overlays: - apiVersion: v1 kind: Service name: tsb-gateway-bookinfo patches: - path: spec.externalTrafficPolicy value: Local 为 istiod 添加主机别名 在某些情况下，istiod 可能需要与没有 DNS 记录的服务通信。典型的示例是当需要从 Vault 或其他密钥管理器中获取自定义 Istio CA 时。hostAlias 补丁将直接将主机名映射到 IP 地址，类似于在 VM 主机文件中静态添加条目。\n使用 kubectl edit 编辑 TSB 控制平面的 ControlPlane CR，并使用以下代码片段作为如何编辑清单的示例。将 \u0026lt;hostname-FQDN\u0026gt; 和 \u0026lt;ip address\u0026gt; 替换为适当的值。\nkubectl edit controlplane -n istio-system spec: components: istio: kubeSpec: overlays: - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator name: tsb-istiocontrolplane patches: - path: spec.components.pilot.k8s.overlays value: - apiVersion: apps/v1 kind: Deployment name: istiod patches: - path: spec.template.spec.hostAliases value: - hostnames: - \u0026lt;hostname-FQDN\u0026gt; ip: \u0026lt;ip address\u0026gt; 配置 Sidecar 资源限制 Sidecar API 资源不允许你指定 sidecar 的资源使用限制或定义，但通过向 ControlPlane CR 添加覆盖是可能的。在此示例中，我们将在 resources 字段下覆盖资源限制。\n使用 kubectl edit 编辑 TSB 控制平面的 ControlPlane CR，并使用以下代码片段作为如何编辑清单的示例。根据需要更新实际的资源限制值。\nkubectl edit controlplane -n istio-system spec: components: istio: kubeSpec: overlays: - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator name: tsb-istiocontrolplane patches: - path: spec.values.global.proxy value: resources: limits: cpu: 2000m memory: 1024Mi requests: cpu: 100m memory: 128Mi 转发客户端信息 某些应用程序需要了解连接到应用程序的客户端的证书信息。TSB 使用 x-forwarded-client-cert 标头将此信息传递给后端服务器。为启用此功能，你需要配置 ControlPlane 和 IngressGateway(s) 的 Envoy 代理。\n对于 ControlPlane，请使用 kubectl edit 编辑 TSB 控制平面的 ControlPlane CR，并使用以下代码片段作为如何编辑清单的示例。\nkubectl edit controlplane -n istio-system spec: components: istio: kubeSpec: overlays: - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator name: tsb-istiocontrolplane patches: - path: spec.meshConfig.defaultConfig.gatewayTopology value: forwardClientCertDetails: APPEND_FORWARD 对于 IngressGateway，请使用 kubectl edit …","relpermalink":"/book/tsb/operations/kube-customization/","summary":"说明如何在 Kubernetes 中配置 TSB 组件，包括使用覆盖进行高级资源配置的示例。","title":"定制 TSB Kubernetes 组件"},{"content":"本操作指南将向你展示如何配置 TSB 中的非 HTTP 服务器。阅读本文档后，你应该熟悉在 IngressGateway 和 Tier1Gateway API 中使用 TCP 部分的方法。\n概要 工作流程与配置 IngressGateway 和 Tier1Gateway 中的 HTTP 服务器完全相同。但是，非 HTTP 支持多端口服务。\n在开始之前，请确保你已经：\n熟悉 TSB 概念 熟悉 载入集群 创建了 租户 设置 安装了 TSB 的四个集群 - 管理平面、Tier-1 和 Tier-2 边缘集群。 在 Tier-2 集群中，在 echo 命名空间部署了 Tier-2 网关。 在 Tier-1 集群中，在 tier1 命名空间部署了 Tier-1 网关。 在两个网关中，端口 8080 和 9999 应可用（为简单起见，我们认为服务和目标端口相同）。要安装网关，请参阅此处 。 在 Tier-2 集群中部署与 HTTP 和非 HTTP 流量配合使用的应用程序。在此演示中，你将部署 Istio 示例目录中的应用程序到 echo 命名空间中。 helloworld 用于 HTTP 流量。清单在这里 tcp-echo 用于非 HTTP 流量。清单在这里 确保你具有部署网关配置所需的权限。 TSB 配置 配置工作区和组 首先创建工作区。在这里，我们假设集群已作为 cluster-0、cluster-1、cluster-2 和 cluster-3 载入。还假设 cluster-3 是 Tier-1，cluster-0 安装了 TSB 管理平面。\napiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: name: tcp-http-demo organization: tetrateio tenant: tetrate spec: namespaceSelector: names: - \u0026#34;cluster-1/echo\u0026#34; - \u0026#34;cluster-2/echo\u0026#34; - \u0026#34;cluster-3/tier1\u0026#34; 为 Tier-1 和 Tier-2 网关分别配置不同的组。\napiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: name: tcp-http-test-t2-group organization: tetrateio tenant: tetrate workspace: tcp-http-demo spec: namespaceSelector: names: - \u0026#34;cluster-1/echo\u0026#34; - \u0026#34;cluster-2/echo\u0026#34; --- apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: name: tcp-http-test-t1-group organization: tetrateio tenant: tetrate workspace: tcp-http-demo spec: configMode: BRIDGED namespaceSelector: names: - \u0026#34;cluster-3/tier1\u0026#34; 为网关提供证书和密钥 你必须在部署网关工作负载所在的命名空间中创建两个密钥。\nhello-tlscred 用于 Helloworld 应用程序 echo-tlscred 用于 TCP-echo 应用程序 你可以使用工具如 openssl 为证书提供密钥并创建密钥，如下所示。\n# 为 helloworld 应用程序创建密钥。这里证书和密钥被提供在 helloworld.crt 和 helloworld.key 中 kubectl --context=\u0026lt;kube-cluster-context\u0026gt; -n \u0026lt;gateway-ns\u0026gt; create secret tls hello-tlscred \\ --cert=helloworld.crt --key=helloworld.key # 为 tcp-echo 应用程序创建密钥 kubectl --context=\u0026lt;kube-cluster-context\u0026gt; -n \u0026lt;gateway-ns\u0026gt; create secret tls echo-tlscred \\ --cert=echo.crt --key=echo.key 配置 Tier-2 集群中的 Ingress Gateway 一些注意事项：\n针对端口 8080 的配置需要 TLS，主机名必须不同。否则，将会出错。 凭据存储为命名空间中运行网关工作负载的秘密。在 Tier-2 中，它是 echo 命名空间，在 Tier-1 中是 tier1 命名空间。 apiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: name: tcp-http-t2-gateway organization: tetrateio tenant: tetrate workspace: tcp-http-demo group: tcp-http-test-t2-group spec: workloadSelector: namespace: echo labels: app: tsb-gateway-echo http: - name: http-hello port: 8080 hostname: hello.tetrate.io tls: mode: SIMPLE secretName: hello-tlscred routing: rules: - route: host: echo/helloworld.echo.svc.cluster.local port: 5000 tcp: # echo.tetrate.io:8080 接收非 HTTP 流量。还有 hello.tetrate.io:8080 接收此端口上的 HTTP 流量。 # 为了区分两个服务，你需要具有不同的主机名和 TLS，以便客户端可以使用不同的 SNI 来区分它们。这是“多协议”/“多流量类型”的一部分。 - name: tcp-echo port: 8080 # 与前一个 HTTP 服务器相同的端口，但主机名不同。 hostname: echo.tetrate.io tls: mode: SIMPLE secretName: echo-tlscred route: host: echo/tcp-echo.echo.svc.cluster.local port: 9000 # 已经定义了一个名为 echo.tetrate.io 的服务，端口为 8080。还可以有另一个 TCP 服务，使用相同的主机名但在不同端口上。这是“多端口”部分。 - name: tcp-echo-2 port: 9999 hostname: echo.tetrate.io route: host: echo/tcp-echo.echo.svc.cluster.local port: 9001 配置 Tier-1 网关 此处定义的主机：端口应与 IngressGateway 中定义的主机：端口完全匹配，流量类型也应与在 IngressGateway 中定义的流量类型完全匹配。\napiVersion: gateway.tsb.tetrate.io/v2 kind: Tier1Gateway metadata: name: tcp-http-t1-gateway organization: tetrateio tenant: tetrate workspace: tcp-http-demo group: tcp-http-test-t1-group spec: workloadSelector: namespace: tier1 labels: app: tsb-gateway-tier1 externalServers: # 这与 Tier-2 网关配置中定义的 hello.tetrate.io:8080 相匹配 # 注意：配置之间的名称不需要相同，但主机名必须匹配。 - name: http-hello hostname: hello.tetrate.io port: 8080 tls: mode: SIMPLE secretName: hello-tlscred tcpExternalServers: # 这与 echo.tetrate.io:8080 相匹配。配置之间的名称不需要相同，但主机名必须匹配。 - name: tcp-echo hostname: echo.tetrate.io port: 8080 tls: mode: SIMPLE secretName: echo-tlscred # 这与 Tier-2 配置中的 echo.tetrate.io:9999 相匹配。 - name: tcp-echo-2 hostname: echo.tetrate.io port: 9999 集群之间的流量路由 南北流量（从 Tier-1 到 Tier-2 集群） 首先，找到 Tier-1 网关的外部 IP 地址，并将其保存在 TIER1_IP 变量中。\n$ export TIER1_IP=\u0026lt;tier1-gateway-ip\u0026gt; 路由 HTTPS 流量 $ curl -svk --resolve hello.tetrate.io:8080:$TIER1_IP https://hello.tetrate.io:8080/hello 注意：除非用于测试目的，否则不要使用 -k 标志。它会跳过服务器证书验证，不安全。\n路由非 HTTP 流量 TLS 流量 - 可能会出现与服务器证书相关的一些警告。由于这是演示，可以忽略它们。 $ openssl s_client -connect $TIER1_IP:8080 -servername echo.tetrate.io 普通 TCP 流量 $ echo hello | nc -v $TIER1_IP 9999 东西流量（在 Tier-2 集群之间） 在路由东西流量时，将使用在 hostname 字段中定义的 DNS 名称。从你希望发送流量的已注入 Istio sidecar 的 pod 中执行 nc 来进行非 HTTP（但是 TCP）流量的路由。此处不需要在此处启动 TLS，因为 TLS 发起是由 sidecar 执行的。\nkubectl -n echo exec -it \u0026lt;pod-name\u0026gt; -c app -- sh -c \u0026#34;echo hello | nc -v echo.tetrate.io 8080\u0026#34; kubectl -n echo exec -it \u0026lt;pod-name\u0026gt; -c app -- sh -c \u0026#34;echo hello | nc -v echo.tetrate.io 9999\u0026#34; kubectl -n echo exec -it \u0026lt;pod-name\u0026gt; -c app -- curl -sv hello.tetrate.io:8080 ","relpermalink":"/book/tsb/howto/gateway/configure-and-route-nonhttp-traffic/","summary":"配置使用 TSB 的非 HTTP 服务器的指南，以及在网关中配置 HTTP 和非 HTTP（多端口、多协议）服务器。","title":"配置和路由 TSB 中的 HTTP、非 HTTP（多协议）和多端口服务流量"},{"content":" 概览\n命令\n","relpermalink":"/book/argo-rollouts/kubectl-plugin/","summary":"概览\n命令","title":"Kubectl plugin"},{"content":"Argo Rollouts 控制器已经安装了Prometheus 指标 ，可以在 8090 端口的/metrics中获取。你可以使用这些指标查看控制器的健康状况，无论是通过仪表板还是通过其他 Prometheus 集成。\n安装和配置 Prometheus 要利用指标，你需要在 Kubernetes 集群中安装 Prometheus。如果你没有现有的 Prometheus 安装，你可以使用任何常见的方法在你的集群中安装它。流行的选项包括Prometheus Helm Chat 或 Prometheus Operator 。\n一旦 Prometheus 在你的集群中运行，你需要确保它抓取 Argo Rollouts 端点。Prometheus 已经包含了针对 Kubernetes 的服务发现机制，但你需要首先进行配置 。根据你的安装方法，你可能需要采取其他操作来抓取 Argo Rollouts 端点。\n例如，如果你使用了 Prometheus 的 Helm 图表，则需要使用以下注释标注你的 Argo Rollouts Controller：\nspec: template: metadata: annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/path: /metrics prometheus.io/port: \u0026#34;8090\u0026#34; 你始终可以在 Prometheus“目标”屏幕中查看控制器是否成功到达：\n一旦你的 Prometheus 实例读取了控制器指标，你可以像使用任何其他 Prometheus 数据源一样使用它们。\n创建 Grafana Dashboard 你可以使用Grafana Dashboard 轻松可视化控制器的指标。在集群中安装 Grafana 并将其连接到 Prometheus 实例 。然后，你可以使用可用指标（在下一节中详细描述）创建任何仪表板。\n作为起点，你可以在这里 中找到现有的 dashboard。\n你可以将此 dashboard 作为 JSON 文件导入到 Grafana 安装中 。\nRollout 对象的可用指标 Argo Rollouts 控制器发布了有关 Argo Rollout 对象的以下 Prometheus 指标。\n名称 描述 rollout_info 关于发布的信息。 rollout_info_replicas_available 每个发布可用的副本数。 rollout_info_replicas_unavailable 每个发布不可用的副本数。 rollout_info_replicas_desired 每个发布所需的副本数。 rollout_info_replicas_updated 每个发布更新的副本数。 rollout_phase [已弃用 - 使用 rollout_info]关于发布状态的信息。 rollout_reconcile 发布和调解表现。 rollout_reconcile_error 发布期间发生的错误。 experiment_info 有关实验的信息。 experiment_phase 有关实验状态的信息。 experiment_reconcile 实验和调解表现。 experiment_reconcile_error 实验期间发生的错误。 analysis_run_info 有关分析运行的信息。 analysis_run_metric_phase 有关分析运行中特定指标的持续时间的信息。 analysis_run_metric_type 有关分析运行中特定指标类型的信息。 analysis_run_phase 有关分析运行状态的信息。 analysis_run_reconcile 分析运行和调解表现。 analysis_run_reconcile_error 分析运行期间发生的错误。 控制器本身的可用指标 控制器还发布以下 Prometheus 指标，以描述控制器的健康状况。\n名称 描述 controller_clientset_k8s_request_total 在应用程序调解期间执行的 kubernetes 请求数量。 workqueue_adds_total 工作队列处理的总添加数 workqueue_depth 工作队列的当前深度 workqueue_queue_duration_seconds 项目在被请求之前在工作队列中停留的时间（以秒为单位）。 workqueue_work_duration_seconds 从工作队列处理项目所需的时间（以秒为单位）。 workqueue_unfinished_work_seconds 进行中且未被 work_duration 观察到的工作所花费的时间（以秒为单位）。大的值表明卡住的线程。可以通过观察它增加的速度来推断卡住的线程数。 workqueue_longest_running_processor_seconds 处理工作队列的最长运行处理器的时间 workqueue_retries_total 工作队列处理的总重试数 此外，Argo-rollouts 提供了有关 CPU、内存和文件描述符使用情况以及当前 Go 进程的进程启动时间和内存统计信息的指标。\n","relpermalink":"/book/argo-rollouts/rollout/controller-metrics/index/","summary":"Argo Rollouts 控制器已经安装了Prometheus 指标 ，可以在 8090 端口的/metrics中获取。你可以使用这些指标查看控制器的健康状况，无论是通过仪表板还是通过其他 Prometheus 集成。 安装和配置 Prometheus 要利用指标，你需要在 Kubernetes 集群中","title":"控制器指标"},{"content":"下面的例子是一个 Dockerfile，它以非 root 用户和非 group 成员身份运行一个应用程序。\nFROM ubuntu:latest # 升级和安装 make 工具 RUN apt update \u0026amp;\u0026amp; apt install -y make # 从一个名为 code 的文件夹中复制源代码，并使用 make 工具构建应用程序。 COPY ./code RUN make /code # 创建一个新的用户（user1）和新的组（group1）；然后切换到该用户的上下文中。 RUN useradd user1 \u0026amp;\u0026amp; groupadd group1 USER user1:group1 # 设置容器的默认入口 CMD /code/app ","relpermalink":"/book/kubernetes-hardening-guidance/appendix/a/","summary":"下面的例子是一个 Dockerfile，它以非 root 用户和非 group 成员身份运行一个应用程序。 FROM ubuntu:latest # 升级和安装 make 工具 RUN apt update \u0026\u0026 apt install -y make # 从一个名为 code 的文件夹中复制源代码，并使用 make 工具构建应用程序。 COPY ./code RUN make /code # 创建一","title":"附录 A：非 root 应用的 Dockerfile 示例"},{"content":"在云环境中运行时，应用程序需要具有弹性。网络通信方面特别容易出现故障。添加网络弹性的一种常见模式是创建一个导入到应用程序中的库，该库提供本附录中描述的网络弹性模式。但是，导入的库很难维护以多种语言编写的服务，且当新版本的网络库发布时，会增加应用程序测试和重新部署的负担。\n取代应用程序处理网络弹性逻辑的另一种方式是，可以将代理置于适当的位置，作为应用程序的保护和增强层。代理的优势在于避免应用程序需要额外的复杂代码，尽量减少开发人员的工作量。\n可以在连接层（物理或 SDN），应用程序或透明代理中处理网络弹性逻辑。虽然代理不是传统网络堆栈的一部分，但它们可用于透明地管理应用程序的网络弹性。\n透明代理可以在基础架构中的任何位置运行，但与应用程序的距离越近越有利。代理支持的协议还要尽可能全面，且可以代理的开放系统互连模型（OSI 模型）层。\n通过实施以下模式，代理在基础架构的弹性中扮演着积极的角色：\n负载均衡 负载切分（Load shedding） 服务发现 重试和 deadline 断路 代理也可以用来为应用程序添加功能。包括：\n安全和认证 路由（入口和出口） 洞察和监测 负载均衡 应用程序负载均衡的方式有很多，应始终将负载均衡器放在云原生应用程序之前的原因有：\nDigitalOcean 在“5 个 DigitalOcean 负载均衡器使用案例”中给出了一些很好的理由：\n水平缩放 高可用性 应用程序部署 动态流量路由 协调透明代理（例如 Envoy 和 Linkerd）是负载均衡应用程序的一种方式。具有透明代理句柄负载均衡的一些好处是：\n对所有端点的请求视图允许更好的负载均衡决策。 基于软件的负载均衡器可灵活选择正确的方式来平衡负载。 透明代理不必盲目地将流量传递给下一个路由器。正如 Bunyan 在其博客文章“超越循环：负载均衡延迟”中指出的，他们可以集中协调以对基础架构有更广泛的了解。这使得负载均衡能够从全局优化流量路由，而不是仅为本地优化快速分组切换。\n随着对端点中哪些服务正在发送和接收请求有了更深的了解，代理可以更合理地向发送流量。\n负载切分 “站点可靠性工程”手册解释了负载切分（Load shedding）与负载均衡不同。尽管负载均衡试图找到正确的后端来发送流量，但是如果应用程序无法接受请求，负载剔除会有意地丢弃流量。\n通过删除负载来保护应用程序实例，可以确保应用程序不会重新启动或被迫进入不利条件。删除请求比等待超时并要求重新启动应用程序要快得多。\n当发生中断或流量过多时，减载可以帮助保护应用程序实例。一切正常时，应用程序应该通过服务发现发现其他相关服务。\n服务发现 服务发现通常由运行服务的编排系统处理。透明代理可以绑定到相同的数据并提供附加功能。\n代理可以通过将多个源绑定在一起（例如，DNS 和键值数据库）并将它们呈现在统一接口中来增强标准服务发现。这允许实现者在不重写所有应用程序代码的情况下改变其后端。如果在应用程序之外处理服务发现，则可以更改服务发现工具而不必重写任何应用程序代码。\n由于代理可以对基础架构中的请求提供更全面的视图，因此可以决定端点何时健康与否。这与其他功能配合使用，例如负载均衡和重试，以将流量路由到最佳端点。\n当允许服务彼此发现时，代理服务器还可以考虑额外的元数据。它们可以实现逻辑，如节点延迟或“距离”，以确保为请求发现正确的服务。\n重试和 deadline 通常，应用程序会使用内置逻辑来知道如何处理对外部服务失败的请求。这也可以由代理无需额外的应用程序代码来处理。\n代理拦截应用程序的所有入口和出口流量并路由请求。如果传出请求失败，代理可以自动重试，而无需涉及应用程序。如果请求因任何其他原因返回，则代理可以根据其配置中的规则进行适当处理。\n这很好，只要应用程序对延迟有弹性。否则，代理应根据申请截止日期返回失败通知。\n截止日期允许应用程序指定允许请求的时间长度。由于代理可以“追踪”到目的地和返回的请求，因此它可以在使用代理的所有应用程序中强制执行最终期限策略。\n当超过 deadline 时，失败将返回给应用程序，并且可以决定适当的操作。选项可能会降级服务，但应用程序也可能选择将错误发回给用户。\n断路 该模式根据家用的断路器命名。当一切正常时，电路默认为“关闭”状态，允许流量流过断路器。当检测到故障时，电路“打开”并切断流量。\n重试模式使应用程序能够重试操作，以期成功。断路器模式阻止应用程序执行可能失败的操作。\n——Alex Homer，云设计模式：云应用程序指令性架构指南\n断开的电路可以是单个端点或整个服务。打开后，不会发送任何流量，所有发送流量的尝试都将立即返回失败。\n与家用电路不同，即使处于打开状态，代理也可以测试失败的端点。当检测到故障后再次可用时，可将损坏的端点置于“半开”状态。此状态将发送少量流量，直到端点被标记为失败或健康。\n这种模式可以使得应用程序快速失败，并且只能发送到健康端点，从而使应用程序更快。通过不断检查端点，网络可以自行修复并智能地路由流量。\n除了这些弹性功能外，代理还可以通过以下方式增强应用程序。\nTLS 和身份验证 代理可以终止传输层安全性（TLS）或代理支持的任何其他安全性。这使得安全逻辑能够集中管理，而不是在每个应用程序中重新实现。然后可以在整个基础架构中更新安全协议或证书，而无需重新部署应用程序。\n身份验证也是如此。但是，授权仍应由应用程序管理，因为它通常是更细粒度的应用程序特定功能。在应用程序监督它们之前，用户会话 cookie 可以由代理验证。这意味着只有通过认证的流量才会被应用程序看到。\n这不仅可以节省应用程序的时间，还可以防止某些类型的滥用导致的停机。\n路由（入口和出口） 当代理在所有应用程序之前运行时，它们控制流入和流出应用程序的流量。它们还可以管理流入和流出集群的流量。\n正如反向代理可以用于在 N 层体系结构中将流量路由到后端一样，服务代理也可能暴露于集群外部，用来路由到达的请求。这里的代理知道流量的另一个数据点来自何处和目的地。\n具有对所有服务间通信的深入了解的反向代理，可以比传统的反向代理更好地了解路由选择。\n洞察和监控 利用所有关于基础架构内流量流的知识，代理系统可以公开关于单个端点和整个集群范围内的流量视图的指标。这些数据点传统上已经在专有网络系统或难以自动化的协议中暴露出来（例如，SNMP）。\n由于代理可以立即知道何时端点无法访问，所以它们也最先了解端点何时不健康。虽然编排系统也可以检查应用程序运行状况，但应用程序可能不知道向编排工具报告不健康的状态。有了对同一服务所有端点的了解后，代理也可以成为监控服务运行状况的最佳位置。\n","relpermalink":"/book/cloud-native-infra/appendix-a-patterns-for-network-resiilency/","summary":"在云环境中运行时，应用程序需要具有弹性。网络通信方面特别容易出现故障。添加网络弹性的一种常见模式是创建一个导入到应用程序中的库，该库提供本附录中描述的网络弹性模式。但是，导入的库很难维护以多种语言编写","title":"附录 A：网络弹性模式"},{"content":"Argo CD 具有在检测到 Git 中所需清单与集群中的实际状态之间存在差异时自动同步应用程序的功能。自动同步的好处是，CI/CD 管道不再需要直接访问 Argo CD API 服务器以执行部署。相反，管道将更改的清单提交并推送到跟踪 Git 存储库中。\n要配置自动同步，请运行：\nargocd app set \u0026lt;APPNAME\u0026gt; --sync-policy automated 或者，如果创建应用程序清单，则使用 automated 策略指定 syncPolicy。\nspec: syncPolicy: automated: {} 自动修整 默认情况下（作为一种安全机制），当 Argo CD 检测到资源不再在 Git 中定义时，自动同步不会删除资源。始终可以执行手动同步（并检查修整）来修整资源。也可以通过运行以下命令设置自动修整：\nargocd app set \u0026lt;APPNAME\u0026gt; --auto-prune 或通过在自动同步策略中将 prune 选项设置为 true：\nspec: syncPolicy: automated: prune: true 带有允许空值的自动修整（v1.8） 默认情况下（作为一种安全机制），自动修整具有保护机制，可防止出现任何自动化或人为错误，因为没有目标资源。它会防止应用程序具有空资源。要允许应用程序具有空资源，请运行：\nargocd app set \u0026lt;APPNAME\u0026gt; --allow-empty 或通过在自动同步策略中将 allowEmpty 选项设置为 true：\nspec: syncPolicy: automated: prune: true allowEmpty: true 自动自愈 默认情况下，对实时集群进行的更改不会触发自动同步。要在实时集群的状态偏离在 Git 中定义的状态时启用自动同步，请运行：\nargocd app set \u0026lt;APPNAME\u0026gt; --self-heal 或通过在自动同步策略中将 selfHeal 选项设置为 true：\nspec: syncPolicy: automated: selfHeal: true 自动同步语义 仅当应用程序处于 OutOfSync 状态时，才会执行自动同步。已同步或错误状态的应用程序不会尝试自动同步。 自动同步将仅针对每个唯一的提交 SHA1 和应用程序参数组合尝试一次同步。如果历史记录中最近的成功同步已经针对相同的提交 SHA 和参数执行，那么不会尝试第二个同步，除非将 selfHeal 标志设置为 true。 如果将 selfHeal 标志设置为 true，则在自我修复超时（默认为 5 秒）之后，将再次尝试同步，该超时由 argocd-application-controller 部署的 self-heal-timeout-seconds 标志控制。 如果上一个同步尝试针对相同的提交 SHA 和参数失败，则自动同步不会重新尝试同步。 无法对启用了自动同步的应用程序执行回滚。 自动同步间隔由 argocd-cm ConfigMap 中的 timeout.reconciliation 值确定，默认为 180s（3 分钟）。 ","relpermalink":"/book/argo-cd/user-guide/auto-sync/","summary":"Argo CD 具有在检测到 Git 中所需清单与集群中的实际状态之间存在差异时自动同步应用程序的功能。自动同步的好处是，CI/CD 管道不再需要直接访问 Argo CD API 服务器以执行部署。相反，管道将更改的清单提交并推送到跟踪 Git 存储","title":"自动同步策略"},{"content":" 非修订版到修订版的升级\n已修订版本间的升级\n网关升级\n修订的 Istio CNI 和升级\n","relpermalink":"/book/tsb/setup/upgrades/","summary":"非修订版到修订版的升级 已修订版本间的升级 网关升级 修订的 Istio CNI 和升级","title":"控制平面升级"},{"content":"在 TSB 中，Application 表示一组逻辑上相关的 Services ，这些服务与彼此相关，并公开一组实现完整业务逻辑的 APIs 。\nTSB 可以在配置 API 运行时策略时利用 OpenAPI 注解。在本文档中，你将启用通过 Open Policy Agent (OPA) 进行授权，以及通过外部服务进行速率限制。每个请求都需要经过基本授权，并为每个有效用户强制执行速率限制策略。\n在开始之前，请确保你已经：\n熟悉 TSB 概念 熟悉 Open Policy Agent (OPA) 熟悉 Envoy 外部授权和速率限制 安装了 TSB 演示 环境 熟悉 Istio Bookinfo 示例应用程序 创建了 租户 部署 httpbin 服务 按照 本文档中的说明 创建 httpbin 服务。完成该文档中的所有步骤。\nTSB 特定注解 以下额外的 TSB 特定注解可以添加到 OpenAPI 规范中，以配置 API。\n注解 描述 x-tsb-service TSB 中提供 API 的上游服务名称，如在 TSB 服务注册表中看到的（可以使用 tctl get services 来检查）。 x-tsb-cors 服务器的 CORS 策略 。 x-tsb-tls 服务器的 TLS 设置。如果省略，则服务器将配置为提供纯文本连接。secretName 字段必须指向集群中现有的 Kubernetes 密钥。 x-tsb-external-authorization 服务器的 OPA 设置。 x-tsb-ratelimiting 外部速率限制服务器（例如 envoyproxy/ratelimit ）设置。 配置 API 在名为 httpbin-api.yaml 的文件中创建以下 API 定义。\nhttpbin-api.yaml apiversion: application.tsb.tetrate.io/v2 kind: API metadata: organization: \u0026lt;organization\u0026gt; tenant: \u0026lt;tenant\u0026gt; application: httpbin name: httpbin-ingress-gateway spec: description: Httpbin OpenAPI workloadSelector: namespace: httpbin labels: app: httpbin-gateway openapi: | openapi: 3.0.1 info: version: \u0026#39;1.0-oas3\u0026#39; title: httpbin description: An unofficial OpenAPI definition for httpbin x-tsb-service: httpbin.httpbin servers: - url: https://httpbin.tetrate.com x-tsb-cors: allowOrigin: - \u0026#34;*\u0026#34; x-tsb-tls: mode: SIMPLE secretName: httpbin-certs paths: /get: get: tags: - HTTP methods summary: | Returns the GET request\u0026#39;s data. responses: \u0026#39;200\u0026#39;: description: OK content: application/json: schema: type: object 在此场景中，你将仅使用 httpbin 服务提供的一个 API (/get)。如果要使用 httpbin 的所有 API，请从此链接 获取它们的 OpenAPI 规范。\n使用 tctl 应用：\ntctl apply -f httpbin-api.yaml 此时，你应该能够向 httpbin Ingress Gateway 发送请求。\n由于你无法控制 httpbin.tetrate.com，因此必须欺骗 curl，让它认为 httpbin.tetrate.com 解析为 Ingress Gateway 的 IP 地址。\n使用以下命令获取之前创建的 Ingress Gateway 的 IP 地址。\nkubectl -n httpbin get service httpbin-gateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39; 执行以下命令，通过 Tier-1 Gateway 向 httpbin 服务发送 HTTP 请求。将 gateway-ip 替换为你在上一步中获取的值。还需要传递 CA 证书，你应该在部署 httpbin 服务的步骤中创建。\ncurl -I \u0026#34;https://httpbin.tetrate.com/get\u0026#34; \\ --resolve \u0026#34;httpbin.tetrate.com:443:\u0026lt;gateway-ip\u0026gt;\u0026#34; \\ --cacert httpbin.crt 你应该会看到成功的 HTTP 响应。\n使用 OPA 进行授权 一旦通过 OpenAPI 注解正确公开 API，就可以配置 OPA 以与 API 网关进行通信。\n在此示例中，你将创建一个策略，检查请求头中的基本身份验证。如果用户已经通过身份验证，用户名称将被添加到 x-user 头，以便稍后由速率限制服务用于强制执行每个用户的配额。\n配置 OPA 创建 opa 命名空间，用于部署 OPA 及其配置：\nkubectl create namespace opa 创建名为 openapi-policy.rego 的文件：\nopenapi-policy.rego package demo.authz default allow = false # username and password database user_passwords = { \u0026#34;alice\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;bob\u0026#34;: \u0026#34;password\u0026#34; } allow = response { # check if password from header is same as in database for the specific user basic_auth.password == user_passwords[basic_auth.user_name] response := { \u0026#34;allowed\u0026#34;: true, \u0026#34;headers\u0026#34;: {\u0026#34;x-user\u0026#34;: basic_auth.user_name} } } basic_auth := {\u0026#34;user_name\u0026#34;: user_name, \u0026#34;password\u0026#34;: password} { v := input.attributes.request.http.headers.authorization startswith(v, \u0026#34;Basic \u0026#34;) s := substring(v, count(\u0026#34;Basic \u0026#34;), -1) [user_name, password] := split(base64url.decode(s), \u0026#34;:\u0026#34;) } 然后使用你创建的文件创建一个 ConfigMap：\nkubectl -n opa create configmap opa-policy \\ --from -file=openapi-policy.rego 创建使用上面的策略配置的 Deployment 和 Service 对象，文件名为 opa.yaml 。\nopa.yaml apiVersion: v1 kind: Service metadata: name: opa namespace: opa spec: selector: app: opa ports: - name: grpc protocol: TCP port: 9191 targetPort: 9191 --- apiVersion: apps/v1 kind: Deployment metadata: name: opa namespace: opa spec: replicas: 1 selector: matchLabels: app: opa template: metadata: labels: app: opa name: opa spec: containers: - image: openpolicyagent/opa:0.29.4-envoy-2 name: opa securityContext: runAsUser: 1111 ports: - containerPort: 8181 args: - \u0026#39;run\u0026#39; - \u0026#39;--server\u0026#39; - \u0026#39;--addr=localhost:8181\u0026#39; - \u0026#39;--diagnostic-addr=0.0.0.0:8282\u0026#39; - \u0026#39;--set=plugins.envoy_ext_authz_grpc.addr=:9191\u0026#39; - \u0026#39;--set=plugins.envoy_ext_authz_grpc.path=demo/authz/allow\u0026#39; - \u0026#39;--set=decision_logs.console=true\u0026#39; - \u0026#39;--ignore=.*\u0026#39; - \u0026#39;/policy/openapi-policy.rego\u0026#39; livenessProbe: httpGet: path: /health?plugins scheme: HTTP port: 8282 initialDelaySeconds: 5 periodSeconds: 5 readinessProbe: httpGet: path: /health?plugins scheme: HTTP port: 8282 initialDelaySeconds: 5 periodSeconds: 5 volumeMounts: - readOnly: true mountPath: /policy name: opa-policy volumes: - name: opa-policy configMap: name: opa-policy 然后应用该清单：\nkubectl apply -f opa.yaml 最后，打开之前创建的 httpbin-api.yaml 文件，并在 server 组件中添加 x-tsb-external-authorization 注解：\n... servers: - url: https://httpbin.tetrate.com ... x-tsb-external-authorization: uri: grpc://opa.opa.svc.cluster.local:9191 然后再次应用更改：\ntctl apply -f httpbin-api.yaml 测试 要进行测试，请执行以下命令，根据需要替换用户名、密码和 gateway-ip 的值。\ncurl -u \u0026lt;username\u0026gt;:\u0026lt;password\u0026gt; \\ \u0026#34;https://httpbin.tetrate.com/get\u0026#34; \\ --resolve \u0026#34;httpbin.tetrate.com:443:\u0026lt;gateway-ip\u0026gt;\u0026#34; \\ --cacert httpbin.crt -s \\ -o /dev/null \\ -w \u0026#34;%{http_code}\\n\u0026#34; 用户名 密码 状态码 alice password 200 bob password 200 \u0026lt;anything else\u0026gt; \u0026lt;anything else\u0026gt; 403 (*1) (*1) 请参阅文档以获取更多详细信息 使用外部服务进行速率限制 TSB 支持速率限制的内部和外部模式 。在此示例中，你将部署一个单独的 Envoy 速率限制服务。\n配置速率限制 创建 ext-ratelimit 命名空间，用于部署速率限制服务器及其配置：\nkubectl create namespace …","relpermalink":"/book/tsb/howto/gateway/application-gateway-with-openapi-annotations/","summary":"如何使用 OpenAPI 注解配置应用程序网关。","title":"使用 OpenAPI 注解配置应用程序网关"},{"content":"本文档解释了当删除一个启用了 istio-proxy sidecar 的 pod 时会发生什么，特别是连接是如何处理的，以及如何配置 sidecar 以优雅地处理正在进行的连接。\n注意 本文档仅适用于 TSB 版本 \u0026lt;= 1.4.x。 在开始之前，请确保你已经完成了以下工作：\n熟悉 TSB 概念 安装了 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成了 TSB 使用快速入门 。本文假设你已经创建了租户并熟悉了工作区和配置组。另外，你需要将 tctl 配置到 TSB 环境 安装 httpbin 当你发出删除请求来删除 Kubernetes 集群中的一个 pod 时，将发送 SIGTERM 给该 pod 中的所有容器。如果 pod 中仅包含一个容器，则它将接收到 SIGTERM 并进入终止状态。 然而，如果 pod 包含一个 sidecar（在我们的情况下是一个 istio-proxy sidecar），则不能自动保证主应用程序在 sidecar 之前终止。\n如果在应用程序之前终止了 istio-proxy sidecar，可能会发生以下问题：\n所有 TCP 连接（包括入站和出站）都会突然终止。 来自应用程序的任何连接将失败。 虽然有一个针对此问题的提案 KEP ，但目前没有直接的方法告诉 Kubernetes 在 sidecar 之前终止应用程序。\n但是，可以通过配置 drainDuration 参数来解决此问题。此配置参数控制底层的 envoy 代理在完全终止之前排空正在进行的连接的时间。\n要利用 drainDuration 参数，你需要在容器 sidecar 和 TSB 网关中都对其进行配置。\n为 istio-proxy 容器配置 drainDuration 时间 你需要向 ControlPlane CR 或 Helm 值应用一个 overlay 来设置 drainDuration。考虑以下示例。注意，只显示了适用部分 – 你很可能需要为控制平面配置更多内容。\nspec: ... components: istio: kubeSpec: overlays: - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator name: tsb-istiocontrolplane patches: - path: spec.meshConfig.defaultConfig.drainDuration value: 50s ... 在将 overlay 添加到配置后，使用 kubectl 命令将其应用于 ControlPlane CR：\nkubectl apply -f controlplane.yaml 如果使用 Helm，可以更新控制平面 Helm 值的 spec 部分，然后执行 helm upgrade。\n验证 drainDuration 必须重新启动具有 istio-proxy 的工作负载，以使 drainDuration 生效。一旦重新启动了工作负载，你可以通过检查 envoy 的配置转储来验证它：\nkubectl exec helloworld-v1-59fdd6b476-pjrtr -n helloworld -c istio-proxy -- pilot-agent request GET config_dump |grep -i drainDuration \u0026#34;drainDuration\u0026#34;: \u0026#34;50s\u0026#34;, 为 TSB 网关配置 drainDuration 如果你使用的是 TSB 网关（例如 IngressGateway、EgressGateway 或 Tier1Gateway），则需要使用 connectionDrainDuration 参数配置适当的网关类型。\n你可以通过发出以下命令来查询网关自定义资源上的 connectionDrainDuration 字段的当前值：\nkubectl get ingress helloworld-gateway -n helloworld -oyaml | grep connectionDrainDuration: connectionDrainDuration: 22s 以下示例显示了如何设置 connectionDrainDuration。请阅读规范以获取有关此字段的更多信息。\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: helloworld-gateway spec: connectionDrainDuration: 10s # ... \u0026lt;snip\u0026gt; ... 在 TSB 网关中验证 drainDuration 要检查在 pod 上设置的 drainDuration 值，你可以查询环境变量：\nkubectl describe po helloworld-gateway-7d5d4c8d57-msfd6 -n helloworld | grep -i DRAIN TERMINATION_DRAIN_DURATION_SECONDS: 22 你还可以在终止网关时查看网关 pod 的日志来验证此值。如果在终止网关 pod 时观察日志，你应该会看到类似以下的消息：\n2022-03-29T06:02:50.423789Z info Graceful termination period is 22s, starting... 2022-03-29T06:03:12.423988Z info Graceful termination period complete, terminating remaining proxies. ","relpermalink":"/book/tsb/operations/graceful-connection-drain/","summary":"如何优雅地关闭 `istio-proxy` sidecar 并减少正在处理的连接失败。","title":"优雅关闭 istio-proxy 连接"},{"content":"有两种方法可以迁移到 Rollout：\n将现有的 Deployment 资源转换为 Rollout 资源。 使用 workloadRef 字段从 Rollout 引用现有的 Deployment。 将 Deployment 转换为 Rollout 将 Deployment 转换为 Rollout 时，需要更改三个字段：\n将 apiVersion 从 apps/v1 更改为 argoproj.io/v1alpha1 将 kind 从 Deployment 更改为 Rollout 使用蓝绿或金丝雀策略替换部署策略 以下是使用金丝雀策略的 Rollout 资源示例。\napiVersion: argoproj.io/v1alpha1 # 从 apps/v1 更改而来 kind: Rollout # 从 Deployment 更改而来 metadata: name: rollouts-demo spec: selector: matchLabels: app: rollouts-demo template: metadata: labels: app: rollouts-demo spec: containers: - name: rollouts-demo image: argoproj/rollouts-demo:blue ports: - containerPort: 8080 strategy: canary: # 从 rollingUpdate 或 recreate 更改而来 steps: - setWeight: 20 - pause: {} 🔔 注意：在迁移已经提供实时生产流量的 Deployment 时，应先在 Deployment 旁边运行 Rollout，然后再删除 Deployment 或缩小 Deployment。不遵循此方法可能导致停机。这也允许在删除原始部署之前测试 Rollout。\n从 Rollout 引用 Deployment 不要删除 Deployment，而是将其缩小为零，并从 Rollout 资源中引用它：\n创建一个 Rollout 资源。 使用 workloadRef 字段引用现有的 Deployment。 通过更改现有 Deployment 的 replicas 字段将现有 Deployment 缩小为零。 要执行更新，应更改 Deployment 的 Pod 模板字段。 以下是引用 Deployment 的 Rollout 资源示例。\napiVersion: argoproj.io/v1alpha1 # 创建一个 rollout 资源 kind: Rollout metadata: name: rollout-ref-deployment spec: replicas: 5 selector: matchLabels: app: rollout-ref-deployment workloadRef: # 使用 workloadRef 字段引用现有的 Deployment apiVersion: apps/v1 kind: Deployment name: rollout-ref-deployment strategy: canary: steps: - setWeight: 20 - pause: {duration: 10s} --- apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/instance: rollout-canary name: rollout-ref-deployment spec: replicas: 0 # 缩小现有部署 selector: matchLabels: app: rollout-ref-deployment template: metadata: labels: app: rollout-ref-deployment spec: containers: - name: rollouts-demo image: argoproj/rollouts-demo:blue imagePullPolicy: Always ports: - containerPort: 8080 如果你的 Deployment 在生产中运行，请考虑以下内容：\n同时运行 Rollout 和 Deployment 创建 Rollout 后，它会与 Deployment Pod 并排启动所需数量的 Pod。Rollout 不会尝试管理现有的 Deployment Pod。这意味着你可以安全地将 Rollout 添加到生产环境中而不会中断任何操作，但是在迁移期间会运行两倍的 Pod。\nArgo-rollouts 控制器使用注解 rollout.argoproj.io/workload-generation 对 Rollout 对象的 spec 进行修补，该注解等于引用部署的生成。用户可以通过检查 Rollout 状态中的workloadObservedGeneration来检测 Rollout 是否与所需的部署生成匹配。\n迁移期间的流量管理 Rollout 提供流量管理功能，可管理路由规则并将流量流向应用程序的不同版本。例如，蓝绿部署策略 操纵 Kubernetes 服务选择器并仅将生产流量定向到“绿色”实例。\n如果你正在使用此功能，则 Rollout 将切换生产流量到其管理的 Pod。切换发生 仅在所需数量的 Pod 正在运行且健康时才会发生，因此在生产环境中是安全的。然而，如果你想要更加小心，请考虑创建一个临时的 Service 或 Ingress 对象来验证 Rollout 行为。一旦完成测试，删除临时 Service/Ingress 并将 Rollout 切换到生产模式。\n回滚到 Deployment 如果用户想要从 Rollout 回滚到 Deployment 类型，那么与迁移到 Rollouts 中的情况相一致，有两种情况：\n将 Rollout 资源转换为 Deployment 资源。 使用 workloadRef 字段从 Rollout 引用现有的 Deployment。 将 Rollout 转换为 Deployment 将 Rollout 转换为 Deployment 时，需要更改三个字段：\n将 apiVersion 从 argoproj.io/v1alpha1 更改为 apps/v1 将 kind 从 Rollout 更改为 Deployment 在 spec.strategy.canary 或 spec.strategy.blueGreen 中删除 Rollout 策略 🔔 注意：在迁移已经提供实时生产流量的 Rollout 时，应先在 Rollout 旁边运行 Deployment，然后再删除 Rollout 或缩小 Rollout。不遵循此方法可能导致停机。这也允许在删除原始 Rollout 之前测试 Deployment。\n从 Rollout 引用 Deployment 当 Rollout 引用部署时：\n通过将其 replicas 字段更改为所需的 Pod 数来增加现有的 Deployment。 等待 Deployment Pod 变为 Ready。 通过将其 replicas 字段更改为零来缩小现有 Rollout。 请参见同时运行 Rollout 和 Deployment 和迁移期间的流量管理以获取注意事项。\n","relpermalink":"/book/argo-rollouts/migrating/","summary":"有两种方法可以迁移到 Rollout： 将现有的 Deployment 资源转换为 Rollout 资源。 使用 workloadRef 字段从 Rollout 引用现有的 Deployment。 将 Deployment 转换为 Rollout 将 Deployment 转换为 Rollout 时，需要更改三个字段： 将 apiVersion 从 apps/v1 更改为 argoproj.io/v1alpha1 将 kind 从 Deployment 更改为 Rollout 使用蓝绿或金","title":"迁移到 Rollouts"},{"content":"关于使用云提供商和避免供应商锁定存在很多争议。这种辩论充满了意识心态之争。\n锁定通常是工程师和管理层关心的问题。应该将其与选择编程语言或框架一样作为应用程序的风险来权衡。编程语言和云提供商的选择是锁定的形式，工程师有责任了解风险并评估这些风险是否可以接受。\n当您选择供应商或技术时，请记住以下几点：\n锁定是不可避免的。 锁定是一种风险，但并不总是很高。 不要外包思维。 锁定是不可避免的 在技术上有两种类型的锁定：\n技术锁定\n整个开发技术栈中底层技术\n供应商锁定\n大多数情况下，作为项目的一部分而使用的服务和软件（供应商锁定还可能包括硬件和操作系统，但我们只关注服务）\n技术锁定 开发人员将选择他们熟悉的技术或为正在开发的应用程序提供最大利益的技术。这些技术可以是供应商提供的技术（例如.NET 和 Oracle 数据库）到开源软件（例如 Python 和 PostgreSQL）。\n在此级别提供的锁定通常要求符合 API 或规范，这将影响应用程序的开发。也可以选择一些替代技术，但是这通常有很高的转换成本，因为技术对应用程序的设计有很大影响。\n供应商锁定 供应商，如云提供商，是另一种不同形式的锁定。在这种情况下，您正在消费供应商的资源。这可以是基础架构资源（例如，计算和存储），或者是托管软件（例如，Gmail）。\n消费资源的堆栈越高，应该从消耗的资源（例如 Heroku）中获得的价值就越高。高层次资源是从底层资源中抽离出来的，能产品的生产速度更快。\n锁定是一种风险 技术锁定通常是一次性决定或与供应商达成使用该技术的协议。如果您不再与供应商达成支持协议，则您的软件不会立即中断 —— 只是变得自我支持。\n开源软件可以在一定程度上减少来自技术的锁定，但并不能完全消除。使用开放标准可以进一步减少锁定，但了解开放标准与开放源代码之间的差异很重要。\n仅仅因为别人编写代码并不能使其成为标准。同样，专有系统可以形成非官方标准，允许从它们迁移出去（如 AWS S3）。\n供应商锁定的原因通常不仅仅是技术锁定，而是因为供应商锁定的风险高于技术风险。如果您不向供应商不支付费用，您的申请将停止运行；你不再能够访问你所支付的资源。\n如前所述，供应商服务提供更多价值，因为它们允许产品开发不需要所有较低级别的实现。不要避免托管服务来消除风险；你应该像对待其他任何事情一样权衡服务的风险和回报。\n如果服务提供标准接口，则风险非常低。接口越是自定义，或者产品越独特，切换的风险就越高。\n不要外包思维 本书的目标之一就是帮助您自己做出决定。在不了解建议的背景和是否适用于您的情况下，不要盲目听从其他人的意见或报告。\n如果您可以通过使用托管云服务更快地交付产品，则应该选择一个供应商开始使用。虽然衡量风险是好的，但将大把的时间花费在具有类似解决方案的多家供应商的争论上，又自己构建服务并不能节约您的时间。\n如果多个供应商提供类似的服务，请选择最容易采用的服务。开始使用该服务后，限制将很快显现。选择供应商时最重要的因素是选择一个与您具有相同创新步伐的供应商。\n如果供应商的创新速度比您快，那么您将无法利用其最新技术，还可能不得不花大量时间迁移旧技术。如果供应商的创新过于缓慢，那么您将不得不根据供应商提供的内容构建自己的抽象，而且您不会专注于您的业务目标。\n为了保持竞争力，您可能需要消费尚未拥有标准或替代品的资源（例如，新的和实验性的服务）。不要因为它们会使你陷入这项服务而害怕。重视保持竞争力或失去市场份额的风险，您的竞争对手可能会更快地创新。\n了解您无法避免的风险以及您的业务有多大风险。做可以最大化回报和将风险降至最低的决策。\n","relpermalink":"/book/cloud-native-infra/appendix-b-lock-in/","summary":"关于使用云提供商和避免供应商锁定存在很多争议。这种辩论充满了意识心态之争。 锁定通常是工程师和管理层关心的问题。应该将其与选择编程语言或框架一样作为应用程序的风险来权衡。编程语言和云提供商的选择是锁定的","title":"附录 B：锁定"},{"content":"下面是一个使用只读根文件系统的 Kubernetes 部署模板的例子。\napiVersion: apps/v1 kind: Deployment metadata: labels: app: web name: web spec: selector: matchLabels: app: web template: metadata: labels: app: web name: web spec: containers: - command: [\u0026#34;sleep\u0026#34;] args: [\u0026#34;999\u0026#34;] image: ubuntu:latest name: web securityContext: readOnlyRootFilesystem: true #使容器的文件系统成为只读 volumeMounts: - mountPath: /writeable/location/here #创建一个可写卷 name: volName volumes: - emptyDir: {} name: volName ","relpermalink":"/book/kubernetes-hardening-guidance/appendix/b/","summary":"下面是一个使用只读根文件系统的 Kubernetes 部署模板的例子。 apiVersion: apps/v1 kind: Deployment metadata: labels: app: web name: web spec: selector: matchLabels: app: web template: metadata: labels: app: web name: web spec: containers: - command: [\"sleep\"] args: [\"999\"] image: ubuntu:latest name: web securityContext: readOnlyRootFilesystem: true #使容器的文件系统成为只读 volumeMounts: - mountPath: /writeable/location/here #创建一个可写卷 name: volName volumes: - emptyDir: {} name: volName","title":"附录 B：只读文件系统的部署模板示例"},{"content":"这份文档将覆盖如何迁移使用 tctl 的 TSB 的实时安装，并迁移到 Helm。文档假设 Helm 已经安装 在系统中。\n在开始之前，请确保你：\n熟悉 TSB 概念 安装了 TSB 环境。你可以使用 TSB 演示 进行快速安装。 完成了 TSB 使用快速入门 。 准备 Helm Chart 在进行之前，你必须熟悉 Helm。请按照我们的指南中的 先决条件 安装 TSB 与 Helm。\n迁移管理平面 迁移当前的安装只需要标记和注释平面安装的资源。所有其他组件将由 tsb-operator 升级和管理。以下是标记每个将由 Helm 管理的资源的命令列表。\nkubectl -n tsb label deployment tsb-operator-management-plane \u0026#34;app.kubernetes.io/managed-by=Helm\u0026#34; kubectl -n tsb annotate deployment tsb-operator-management-plane \u0026#34;meta.helm.sh/release-name=mp\u0026#34; \u0026#34;meta.helm.sh/release-namespace=tsb\u0026#34; kubectl -n tsb annotate service tsb-operator-management-plane \u0026#34;meta.helm.sh/release-name=mp\u0026#34; \u0026#34;meta.helm.sh/release-namespace=tsb\u0026#34; kubectl -n tsb annotate sa tsb-operator-management-plane \u0026#34;meta.helm.sh/release-name=mp\u0026#34; \u0026#34;meta.helm.sh/release-namespace=tsb\u0026#34; kubectl -n tsb label secret elastic-credentials \u0026#34;app.kubernetes.io/managed-by=Helm\u0026#34; kubectl -n tsb annotate secret elastic-credentials \u0026#34;meta.helm.sh/release-name=mp\u0026#34; \u0026#34;meta.helm.sh/release-namespace=tsb\u0026#34; kubectl -n tsb label secret es-certs \u0026#34;app.kubernetes.io/managed-by=Helm\u0026#34; kubectl -n tsb annotate secret es-certs \u0026#34;meta.helm.sh/release-name=mp\u0026#34; \u0026#34;meta.helm.sh/release-namespace=tsb\u0026#34; kubectl -n tsb label secret ldap-credentials \u0026#34;app.kubernetes.io/managed-by=Helm\u0026#34; kubectl -n tsb annotate secret ldap-credentials \u0026#34;meta.helm.sh/release-name=mp\u0026#34; \u0026#34;meta.helm.sh/release-namespace=tsb\u0026#34; kubectl -n tsb label secret postgres-credentials \u0026#34;app.kubernetes.io/managed-by=Helm\u0026#34; kubectl -n tsb annotate secret postgres-credentials \u0026#34;meta.helm.sh/release-name=mp\u0026#34; \u0026#34;meta.helm.sh/release-namespace=tsb\u0026#34; kubectl -n tsb label secret admin-credentials \u0026#34;app.kubernetes.io/managed-by=Helm\u0026#34; kubectl -n tsb annotate secret admin-credentials \u0026#34;meta.helm.sh/release-name=mp\u0026#34; \u0026#34;meta.helm.sh/release-namespace=tsb\u0026#34; kubectl annotate clusterrole tsb-operator-management-plane-tsb \u0026#34;meta.helm.sh/release-name=mp\u0026#34; \u0026#34;meta.helm.sh/release-namespace=tsb\u0026#34; kubectl annotate clusterrolebinding tsb-operator-management-plane-tsb \u0026#34;meta.helm.sh/release-name=mp\u0026#34; \u0026#34;meta.helm.sh/release-namespace=tsb\u0026#34; kubectl -n tsb label managementplane managementplane \u0026#34;app.kubernetes.io/managed-by=Helm\u0026#34; kubectl -n tsb annotate managementplane managementplane \u0026#34;meta.helm.sh/release-name=mp\u0026#34; \u0026#34;meta.helm.sh/release-namespace=tsb\u0026#34; 注意 release-name 和 release-namespace 应该与 Helm 安装命令中使用的发行名称和命名空间匹配。 在所有资源都正确标记后，然后继续安装发布：\n### 示例 helm upgrade mp tetrate-tsb-helm/managementplane --install --namespace tsb -f upgrade-mpt1/helm/values-mp.yaml --set image.registry=${HUB} --set image.tag=${TSB_VERSION} --set spec.hub=${HUB} ### 输出： Release \u0026#34;mp\u0026#34; does not exist. Installing it now. NAME: mp LAST DEPLOYED: Thu May 25 11:03:18 2023 NAMESPACE: tsb STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing the TSB Management plane 1.5.11. Chart: managementplane Version: 1.5.11 Your Management Plane is ready to be used. Next step might be to onboard the cluster from the control plane. You could choose between: - install `controlplane` chart - manually following # TODO url to docs. # Discover the TSB entrypoint Check the IP for the envoy loadbalancer service. This is one example. Consider a time for the service to be ready: kubectl get svc -n \u0026#34;tsb\u0026#34; envoy --output jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39; # Configure the TCTL admin profile, using the IP in the previous step. # Setup the TSB address as follows. If specific settings are needed to trust the certificate configured in TSB, # refer to the `tctl config clusters set --help` command to see all the available options. tctl config clusters set helm --bridge-address \u0026lt;IP\u0026gt;:8443 tctl config users set helm --username admin --password \u0026#34;NotAPassword\u0026#34; --org \u0026#34;tetrate\u0026#34; tctl config profiles set helm --cluster helm --username helm tctl config profiles set-current helm 迁移控制平面 该过程相同，只有一些资源和机密发生了变化。\nkubectl -n istio-system label deployment tsb-operator-control-plane \u0026#34;app.kubernetes.io/managed-by=Helm\u0026#34; kubectl -n istio-system annotate deployment tsb-operator-control-plane \u0026#34;meta.helm.sh/release-name=cp\u0026#34; \u0026#34;meta.helm.sh/release-namespace=istio-system\u0026#34; kubectl -n istio-system annotate service tsb-operator-control-plane \u0026#34;meta.helm.sh/release-name=cp\u0026#34; \u0026#34;meta.helm.sh/release-namespace=istio-system\u0026#34; kubectl -n istio-system annotate sa tsb-operator-control-plane \u0026#34;meta.helm.sh/release-name=cp\u0026#34; \u0026#34;meta.helm.sh/release-namespace=istio-system\u0026#34; kubectl -n istio-system annotate secret elastic-credentials \u0026#34;app.kubernetes.io/managed-by=Helm\u0026#34; kubectl -n istio-system annotate secret elastic-credentials \u0026#34;meta.helm.sh/release-name=cp\u0026#34; \u0026#34;meta.helm.sh/release-namespace=istio-system\u0026#34; kubectl -n istio-system label secret cluster-service-account \u0026#34;app.kubernetes.io/managed-by=Helm\u0026#34; kubectl -n istio-system annotate secret cluster-service-account \u0026#34;meta.helm.sh/release-name=cp\u0026#34; \u0026#34;meta.helm.sh/release-namespace=istio-system\u0026#34; kubectl -n istio-system label secret mp-certs \u0026#34;app.kubernetes.io/managed-by=Helm\u0026#34; kubectl -n istio-system annotate secret mp-certs \u0026#34;meta.helm.sh/release-name=cp\u0026#34; \u0026#34;meta.helm.sh/release-namespace=istio-system\u0026#34; kubectl …","relpermalink":"/book/tsb/setup/migrate-tctl-to-helm/","summary":"这份文档将覆盖如何迁移使用 tctl 的 TSB 的实时安装，并迁移到 Helm。文档假设 Helm 已经安装 在系统中。 在开始之前，请确保你： 熟悉 TSB 概念 安装了 TSB 环境。你可以使用 TSB 演示 进行快速安装。 完成了 TSB 使用快速入门 。 准备 Helm Chart 在进","title":"从 tctl 迁移到 Helm"},{"content":"应用程序入口是一个 L7 入口，允许应用程序开发人员直接利用可用的 Envoy 功能。 与 TSB 中其他类型的 Ingress 不同，配置它不需要管理员权限， 并以一种使应用程序开发人员更容易定义意图的方式公开 Envoy 代理的功能。\n应用程序入口是 Istio 的一个简化版本，使用 Istiod 作为控制平面组件和 Istio IngressGateway（即 Envoy 代理）作为数据平面组件。应用程序入口在由应用程序拥有的命名空间中为每个应用程序部署，并只能使用来自其部署的命名空间的 Istio 配置。\n应用程序入口还具有一个 OpenAPI 翻译器附加组件，允许用户使用 OpenAPI 规范配置入口。\n此功能需要 tctl 版本 1.4.5 或更高版本。\n使用 Istio 进行配置 在此示例中，你将在一个命名空间中安装 httpbin，并在相同命名空间中创建一个应用程序入口，以路由访问 httpbin 工作负载。\n创建名为 httpbin-appingress 的命名空间。\nkubectl create namespace httpbin-appingress 你将在同一个命名空间中安装工作负载和应用程序入口。\n在此示例中，工作负载将是 httpbin 服务。按照这些说明安装 httpbin 。\n使用以下命令安装应用程序入口。\ntctl experimental app-ingress kubernetes generate -n httpbin-appingress | \\ kubectl apply -f - 注意 你可能会看到错误消息 unable to recognize \u0026#34;STDIN\u0026#34;: no matches for kind \u0026#34;IstioOperator\u0026#34; in version \u0026#34;install.istio.io/v1alpha1\u0026#34;。如果遇到此错误，请重新运行上面的命令。这是因为尚未部署 IstioOperator CRD，通常在重试后会消失。 验证命名空间 httpbin-appingress 中的 httpbin、istio-ingressgateway 和 istiod pod 是否正常运行：\nkubectl get pod -n httpbin-appingress NAME READY STATUS RESTARTS AGE httpbin-74fb669cc6-lc4qm 1/1 Running 0 10m istio-ingressgateway-6f9c469bd5-r7z4t 1/1 Running 0 8m8s istio-operator-1-11-3-f88d885b5-8wb9k 1/1 Running 0 8m42s istiod-1-11-3-597999c56f-5f2xr 1/1 Running 0 8m19s 你需要通过主机名 httpbin-appingress.example.com 访问 httpbin 服务。为此， 在 httpbin-appingress 命名空间中部署一个 Istio Gateway 和 Virtual Service，以路由 HTTP 流量到 httpbin pod。\n创建一个名为 httpbin-appingress-virtualservice.yaml 的文件，内容如下：\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: httpbin-gateway spec: selector: istio: ingressgateway # 使用 Istio 默认网关实现 servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;httpbin-appingress.example.com\u0026#34; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: httpbin spec: hosts: - \u0026#34;httpbin-appingress.example.com\u0026#34; gateways: - httpbin-gateway http: - match: - uri: prefix: /status - uri: prefix: /delay route: - destination: port: number: 8000 host: httpbin 使用 kubectl 应用这些配置：\nkubectl -n httpbin-appingress -f httpbin-appingress-virtualservice.yaml 由于尚未为 httpbin-appingress.example.com 设置 DNS，你需要设置你的环境或更改发出 HTTP 请求的方式，以访问你创建的服务。在此示例中，你将使用 kubectl port-forward 来建立端口转发。\n在不同的终端中，使用本地端口 4040 设置到 httpbin-appingress 命名空间中的 istio-ingressgateway 服务的端口转发：\nkubectl -n httpbin-appingress port-forward svc/istio-ingressgateway 4040:80 现在，你应该能够通过 istio-ingressgateway 服务访问 httpbin 应用程序，该服务在 httpbin-appingress 命名空间中运行， 使用以下命令：\ncurl -s -I \\ -H \u0026#34;Host: httpbin-appingress.example.com\u0026#34; \\ http://localhost:4040/status/200 使用 OpenAPI 翻译器 如果你的应用程序提供 OpenAPI 规范（3.0.0 或更高版本），你可以使用它来生成路由规则到你的应用程序。OpenAPI 翻译器插件将应用程序的 OpenAPI 规范，并将其翻译为 Istio 配置，并将其应用于应用程序入口。\n在此示例中，你将使用 bookinfo 示例应用程序，并使用其 OpenAPI 规范。\n创建一个新的命名空间 bookinfo-openapi：\nkubectl create namespace bookinfo-openapi 将 bookinfo 示例部署到 bookinfo-openapi 命名空间中：\nkubectl apply -n bookinfo-openapi \\ -f https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/platform/kube/bookinfo.yaml 一旦验证应用程序已正确部署，就在 bookinfo-openapi 命名空间中部署应用程序入口。你需要指定 OpenAPI 规范描述的 “backend” 服务（应用程序）。\ntctl experimental app-ingress kubernetes generate \\ -n bookinfo-openapi \\ --openapi-translator \\ --openapi-backend-service http://productpage.bookinfo-openapi.svc.cluster.local:9080 \\ | kubectl apply -f - 上面的命令创建一个应用程序入口，该入口期望在名为 openapi-translator 的 ConfigMap 中提供 OpenAPI 规范。由于你尚未提供规范，因此无法正确配置应用程序入口。\n你需要获取 bookinfo 的 OpenAPI 规范，但 Istio 提供的示例仅以 OpenAPI 2.0 格式提供 。已将 bookinfo OpenAPI 规范转换为 OpenAPI 3.0.0 的版本可通过 此链接 获取。将文件下载为 bookinfo-openapi.yaml。\n使用以下命令使用此文件创建 ConfigMap：\nkubectl -n bookinfo-openapi create configmap openapi-translator \\ --from-file=bookinfo-openapi.yaml 当 OpenAPI 翻译器捕获配置时，将在命名空间中提供 Istio 资源，可以通过执行 kubectl get gateway 和 kubectl get virtualservice 命令来验证这一点：\nkubectl -n bookinfo-openapi get gateway NAME AGE istio-ingressgateway-f6fb54b17b9120eb 64s kubectl -n bookinfo-openapi get virtualservice NAME GATEWAYS HOSTS AGE istio-ingressgateway-f6fb54b17b9120eb-www-bookinfo-com [\u0026#34;bookinfo-openapi/istio-ingressgateway-f6fb54b17b9120eb\u0026#34;] [\u0026#34;www.bookinfo.com\u0026#34;] 5m13s 还可以通过添加注解来利用更多 TSB 功能，如速率限制、身份验证和授权，通过添加注释 到你的 OpenAPI 规范中。\n使用 IstioOperator 扩展应用程序入口 可以使用 Istio Operator 进一步配置应用程序入口中的 Istio 组件。\n例如，如果要在应用程序入口中配置自定义 CA 证书（在 Kubernetes 版本高于 1.22 的情况下特别有用，因为 Kubernetes 的 pilotCertProvider 已弃用），创建一个名为 configure-plug-in-certs.yaml 的文件：\napiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: bookinfo-appingress name: bookinfo-appingress spec: values: global: pilotCertProvider: istiod 创建包含证书和密钥的 cacerts 密钥。有关更多信息，请单击此处 。 你可以通过使用 Istio 发行包提供的示例证书运行以下命令来进行测试。\ncurl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.11.3 sh - 运行以下命令创建 cacerts 密钥。\nkubectl create secret generic cacerts -n bookinfo-appingress --from-file=ca-cert.pem=istio-1.11.3/samples/certs/ca-cert.pem --from-file=ca-key.pem=istio-1.11.3/samples/certs/ca-key.pem --from-file=root-cert.pem=istio-1.11.3/samples/certs/root-cert.pem --from-file=cert-chain.pem=istio-1.11.3/samples/certs/cert-chain.pem 然后，通过指定 -f（--filename）标志来提供此文件，以生成应用程序入口的清单：\ntctl experimental app-ingress kubernetes generate \\ -n …","relpermalink":"/book/tsb/howto/gateway/app-ingress/","summary":"应用程序入口是一个 L7 入口，允许应用程序开发人员直接利用可用的 Envoy 功能。 与 TSB 中其他类型的 Ingress 不同，配置它不需要管理员权限， 并以一种使应用程序开发人员更容易定义意图的方式公开 Envoy 代理的功能。 应用程序入口是 Istio 的一","title":"应用程序入口"},{"content":"本文介绍了在使用 Argo Rollouts 时的一些最佳实践、技巧和窍门。\nIngress 目标/稳定主机路由 出于各种原因，通常希望外部服务能够访问预期的 pod（即 canary/preview）或特定的稳定 pod，而不会将流量任意分配给两个版本。一些使用场景包括：\n新版本的服务可以在内部/私有环境中访问（例如进行手动验证），然后再将其外部公开。 外部 CI/CD 管道在将蓝/绿预览堆栈升级到生产环境之前运行测试。 运行比较旧版本和新版本行为的测试。 如果使用 Ingress 来将流量路由到服务，则可以添加其他主机规则到 Ingress 规则中，以便能够特别到达期望的（canary/preview）pod 或稳定的 pod。\napiVersion：networking.k8s.io/v1beta1 kind：Ingress metadata： name：guestbook spec： rules： ＃仅到达所需的Pod（也称为金丝雀/预览）的主机规则 -主机：guestbook-desired.argoproj.io http： paths： -后端： serviceName：guestbook-desired servicePort：443 path：/ * ＃仅到达稳定Pod的主机规则 -主机：guestbook-stable.argoproj.io http： paths： -后端： serviceName：guestbook-stable servicePort：443 path：/ * ＃默认规则，省略主机，将流量分为所需的与稳定的 - http： paths： -后端： serviceName：guestbook-root servicePort：443 path：/ * 上述技术具有一个好处，即不会产生额外的负载均衡器分配成本。\n减少运算符内存使用 在具有数千个 rollouts 的集群上，可以通过将 RevisionHistoryLimit 从默认值 10 更改为较低的数字来显着减少 argo-rollouts 运算符的内存使用。Argo Rollouts 的一个用户通过将 RevisionHistoryLimit 从 10 更改为 0，为一个具有 1290 个 rollouts 的集群减少了 27％ 的内存使用率。\n","relpermalink":"/book/argo-rollouts/best-practices/","summary":"本文介绍了在使用 Argo Rollouts 时的一些最佳实践、技巧和窍门。 Ingress 目标/稳定主机路由 出于各种原因，通常希望外部服务能够访问预期的 pod（即 canary/preview）或特定的稳定 pod，而不会将流量任意分配给两个","title":"最佳实践"},{"content":" 以下内容最初由 CNCF 发布在 Kubernetes.io 上，并且在此获得许可。\n在 2014 年夏天，Box 对沉淀了十年的硬件和软件基础架构的痛苦，这无法与公司的需求保持一致。\n该平台为超过 5000 万用户（包括政府和大型企业如通用电气公司）管理和共享云中的内容，Box 最初是一个使用 PHP 写的具有数百万行的庞大代码，内置裸机数据中心。它已经开始将单体应用分解成微服务。 “随着我们扩展到全球各地，公有云战争正在升温，我们开始专注于如何在许多不同的环境和许多不同的云基础架构提供商之间运行我们的工作负载”，Box 联合创始人和服务架构师 Sam Ghods 说。 “迄今为止，这是一个巨大的挑战，因为所有这些不同的提供商，特别是裸机，都有非常不同的接口和与合作方式。”\n当 Ghods 参加 DockerCon 时，Box 的云原生之旅加速了。该公司已经认识到，它不能再仅仅使用裸机来运行应用程序，正在研究 Docker 容器化，使用 OpenStack 进行虚拟化以及支持公有云。\n在那次会议上，Google 宣布发布 Kubernetes 容器管理系统，Ghods 成功了。 “我们研究了许多不同的选择，但 Kubernetes 确实很出色，特别是因为 Borg 老兵的团队非常强大，可以以基础架构不可知的方式来运行云软件，”他谈到 Google 内部的容器调度器 Borg。 “事实上，一开始它设计与裸机一样运行，就像我们可以在数据中心内迁移到它一样，然后也使用相同的工具和概念在公有云提供商上运行。”\n另外：Ghods 喜欢 Kubernetes 拥有一套通用的 API 对象，如 pod、服务、副本集和部署，这些对象创建了一个一致的接口来构建工具。 “甚至像 OpenShift 或 Deis 这样的构建在 Kubernetes 之上的 PaaS 层仍然将这些对象视为统一的原则，”他说。 “我们很高兴能够在整个生态系统中共享这些抽象概念，这会产生比我们在其他潜在解决方案中更多的动力。”\n六个月后 Box 在一个生产数据中心的集群中部署了 Kubernetes。Kubernetes 在 0.11 版本之前仍然是测试版。他们从小版本开始：Ghods 的团队在 Kubernetes 上运行的第一个服务就是 Box API 监视器，确认了 Box 可以运行。 “这只是一个让整个管道运作正常的测试服务，”他说。接下来是一些处理作业的守护进程，它们“很好而且安全，因为如果他们遇到任何中断，也不会让来自客户的同步传入请求失败。”\n几个月后，该团队可以发送并要求提供信息的第一个实时服务启动。那时，Ghods 说：“我们对 Kubernetes 集群的稳定性感到满意。我们开始迁移一些服务，然后我们将扩大集群的规模和端口数量，最后每个数据中心的服务器数量大约为 100 台，这些服务器纯粹专用于 Kubernetes。在未来的 12 个月里，这个数字将会增长很多，达到数百甚至数千。“\n在观察开始使用 Kubernetes 进行微服务的团队时，“我们看到正在发布的微服务数量有所增加，”Ghodsnotes 说。 “显然，通过微服务构建软件的方式已被压抑很久，随着灵活性的提高帮助我们的开发人员提高了生产力，并为更好的架构选择做好了准备。”\nGhods 反映，作为早期采用者，Box 经历了不同的旅程。他说：“我们肯定是在等待某些事情稳定和功能发布，我们在这一步上被锁定，”他说。 “在早期，我们对 Kubectl 应用等组件做了很多贡献，并等待 Kubernetes 发布，然后我们会升级，贡献更多，并来回多次。整个项目从我们第一次在 Kubernetes 上进行实际部署到 GA 需要大约 18 个月的时间。如果我们今天自己来做一遍同样的事情，可能会少于六个月。“\n无论如何，Box 无需为 Kubernetes 做过多修改。Ghods 说：“我们团队在 Box 中实施 Kubernetes 所做的绝大多数工作一直致力于在我们现有的（往往是遗留下来的）基础架构内的工作，例如将我们的基础操作系统从 RHEL6 升级到 RHEL7 或将其整合纳入到我们的监控基础架构 Nagios。但总体而言，Kubernetes 非常灵活，能够适应我们的许多限制因素，并且它在我们的裸机基础架构上运行非常成功。“\n对于 Box 来说，更大的挑战也许是文化上的挑战。Ghods 说：“Kubernetes 和一般的云本身代表了一个非常大的范式转换，并且它不是非常渐进的，”Ghods 说。 “我们可以这样说，Kubernetes 将会解决所有问题，因为它能够以正确的方式做事，一切都会变得更好。但是要记住，它不像其他许多解决方案那样可靠。你不能说这家或那家公司花了多少时间做这件事，因为还没有那么多。我们的团队必须真正为资源而战，因为我们的项目有一点点的恐惧。“\n从经验中学习，Ghods 为经历类似挑战的公司提供了以下两条建议：\n提前和经常交付。对于 Box 来说，服务发现是一个巨大的问题，团队必须决定是建立一个临时解决方案还是等待 Kubernetes 本身满足 Box 的独特要求。经过多次辩论之后，“我们刚开始专注于提供可行的解决方案，然后处理可能在稍后迁移到更原始的解决方案，”Ghods 说。 “无论多么微不足道，团队的上述目标应始终是为基础架构上的实际生产用例服务。这有助于保持团队本身和组织对项目的看法。“ 保持开放的态度，了解公司必须从开发人员那里抽象出什么和没有抽象出什么。早期，团队在 Dockerfiles 之上构建了一个抽象，以帮助确保所有容器镜像具有正确的安全更新。事实证明这是多余的工作，因为容器镜像是不可变的，您可以在构建后扫描它们以确保它们不包含漏洞。因为通过容器化来管理基础架构是一个不连续的飞跃，所以最好先直接使用本地工具学习其独特的优势和注意事项。抽象只能在实际需要出现之后才能建立。 最后，影响力非常强大。“在 Kubernetes 之前，”Ghods 说，“我们的基础架构非常陈旧，需要 6 个多月才能部署一个新的微服务。现在，一个新的微服务部署时间不到五天。我们正在努力让它达到不到一天。诚然，这六个月的大部分时间都是由于我们的系统有多么糟糕，但裸机本质上是一个难以支持的平台，除非您有像 Kubernetes 这样的系统来帮助管理它。“\n按 Ghods 的估计，Box 距离完成 90％运行在 Kubernetes 上的目标还有几年的时间。 “到目前为止，我们已经完成了一项稳定的，关键任务的 Kubernetes 部署，它提供了很多价值，”他说。 “现在我们 10％左右服务器都运行在 Kubernetes 上，我认为明年我们可能会超过一半。我们正在努力实现所有无状态服务使用案例，并计划在此之后将我们的重点转移到有状态服务。“\n事实上，这就是他在整个行业中的设想：Ghods 预测 Kubernetes 有机会成为新的云平台。Kubernetes 提供了一个涵盖不同云平台的 API，包括裸机，以及“当我们可以针对单一界面进行编程时，我不认为人们已经看到了可能的全部潜力”，他说。 “与 AWS 改变基础架构一样，您不必再考虑服务器或机柜或网络设备，Kubernetes 使您能够专注于您正在运行的软件，这非常令人兴奋。这是愿景。“\nGhods 指出了已经在开发或最近发布的作为云平台的项目：集群联邦，Dashboard UI 和 CoreOS 的 etcd operator。 “我真的相信这是我在云基础架构中看到的最激动人心的事情，”他说，“因为它是一个前所未有的自动化和智能环境，其基础架构对每个基础架构平台都是可移植和不可知的。”\n由于早期决定使用裸机，Box 不得已开始了 Kubernetes 之旅。但是 Ghods 表示，即使公司现在不必对云提供商不可知，Kubernetes 也可能很快成为行业标准，因为越来越多的工具和扩展是围绕 API 构建的。\n“同样的方式，偏离 Linux 是没有意义的，因为它是如此的标准，”Ghods 说，“我认为 Kubernetes 正在走相同的道路。现在还处于早期阶段 —— 文档仍然需要工作，用于编写和发布 YAML 到 Kubernetes 集群的用户体验仍然很艰难。当你处于潮流最前线时，你可能会做出一些牺牲。但底线是，这是行业发展的方向。从现在开始的三到五年，如果您还会以其他方式运行基础架构，那么真的会让人非常震惊。“\n","relpermalink":"/book/cloud-native-infra/appendix-c-box-case-study/","summary":"以下内容最初由 CNCF 发布在 Kubernetes.io 上，并且在此获得许可。 在 2014 年夏天，Box 对沉淀了十年的硬件和软件基础架构的痛苦，这无法与公司的需求保持一致。 该平台为超过 5000 万用户（包括政府和大型企业如通用电气公司）管理和共享云","title":"附录 C Box：案例研究"},{"content":"下面是一个 Kubernetes Pod 安全策略的例子，它为集群中运行的容器执行了强大的安全要求。这个例子是基于官方的 Kubernetes 文档 。我们鼓励管理员对该策略进行修改，以满足他们组织的要求。\napiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: restricted annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: \u0026#39;docker/default,runtime/default\u0026#39; apparmor.security.beta.kubernetes.io/allowedProfileNames: \u0026#39;runtime/default\u0026#39; seccomp.security.alpha.kubernetes.io/defaultProfileName: \u0026#39;runtime/default\u0026#39; apparmor.security.beta.kubernetes.io/defaultProfileName: \u0026#39;runtime/default\u0026#39; spec: privileged: false # 需要防止升级到 root allowPrivilegeEscalation: false requiredDropCapabilities: - ALL volumes: - \u0026#39;configMap\u0026#39; - \u0026#39;emptyDir\u0026#39; - \u0026#39;projected\u0026#39; - \u0026#39;secret\u0026#39; - \u0026#39;downwardAPI\u0026#39; - \u0026#39;persistentVolumeClaim\u0026#39; # 假设管理员设置的 persistentVolumes 是安全的 hostNetwork: false hostIPC: false hostPID: false runAsUser: rule: \u0026#39;MustRunAsNonRoot\u0026#39; # 要求容器在没有 root 的情况下运行 seLinux rule: \u0026#39;RunAsAny\u0026#39; # 假设节点使用的是 AppArmor 而不是 SELinux supplementalGroups: rule: \u0026#39;MustRunAs\u0026#39; ranges: # 禁止添加到 root 组 - min: 1 max: 65535 runAsGroup: rule: \u0026#39;MustRunAs\u0026#39; ranges: # 禁止添加到 root 组 - min: 1 max: 65535 fsGroup: rule: \u0026#39;MustRunAs\u0026#39; ranges: # 禁止添加到 root 组 - min: 1 max: 65535 readOnlyRootFilesystem: true ","relpermalink":"/book/kubernetes-hardening-guidance/appendix/c/","summary":"下面是一个 Kubernetes Pod 安全策略的例子，它为集群中运行的容器执行了强大的安全要求。这个例子是基于官方的 Kubernetes 文档 。我们鼓励管理员对该策略进行修改，以满足他们组织的要求。 apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: restricted annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default,runtime/default' apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default' seccomp.security.alpha.kubernetes.io/defaultProfileName: 'runtime/default' apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default' spec: privileged: false # 需要防","title":"附录 C：Pod 安全策略示例"},{"content":"忽略无关资源 v1.1\n在某些情况下，你可能希望从应用程序的整体同步状态中排除资源。例如。如果它们是由工具生成的。这可以通过在你想要排除的资源上添加此注释来完成：\nmetadata: annotations: argocd.argoproj.io/compare-options: IgnoreExtraneous 对比选项需要修整 🔔 提示：这仅影响同步状态。如果资源的运行状况降级，那么应用程序也会降级。\nKustomize 具有允许你生成配置映射的功能（了解更多 ）。你可以设置 generatorOptions 添加此注释，以便你的应用保持同步：\nconfigMapGenerator: - name: my-map literals: - foo=bar generatorOptions: annotations: argocd.argoproj.io/compare-options: IgnoreExtraneous kind: Kustomization 🔔 提示：generatorOptions 向配置映射和秘密添加注释（了解更多 ）。\n你可能希望将其与 Prune=false 同步选项 结合起来。\n","relpermalink":"/book/argo-cd/user-guide/compare-options/","summary":"忽略无关资源 v1.1 在某些情况下，你可能希望从应用程序的整体同步状态中排除资源。例如。如果它们是由工具生成的。这可以通过在你想要排除的资源上添加此注释来完成： metadata: annotations: argocd.argoproj.io/compare-options: IgnoreExtraneous 对比选项需要修整 🔔 提示：这仅影响同步状态","title":"对比选项"},{"content":"一般问题 Argo Rollouts 是依赖 Argo CD 或其他 Argo 项目吗？ Argo Rollouts 是一个独立的项目。虽然它与 Argo CD 和其他 Argo 项目配合使用效果很好，但它也可以单独用于渐进式交付场景。更具体地说，Argo Rollouts 不需要你也在同一集群上安装 Argo CD。\nArgo Rollouts 如何与 Argo CD 集成？ 通过 Argo CD 的 Lua 健康检查 ，Argo CD 可以了解 Argo Rollouts 资源的健康状况。这些健康检查了解 Argo Rollout 对象何时在进展、暂停、退化或健康。此外，Argo CD 具有基于 Lua 的资源操作，可以改变 Argo Rollouts 资源（例如取消暂停 Rollout）。\n因此，操作员可以构建自动化程序以反应 Argo Rollouts 资源的状态。例如，如果 Argo CD 创建的 Rollout 被暂停，Argo CD 会检测到并将该应用程序标记为暂停。一旦确定新版本是好的，操作员就可以使用 Argo CD 的 resume 资源操作来取消暂停 Rollout，以便它可以继续前进。\n我们可以通过 Argo CD 运行 Argo Rollouts kubectl 插件命令吗？ Argo CD 支持运行 Lua 脚本来修改资源类型（例如，通过将 .spec.suspend 设置为 true 暂停 CronJob）。这些 Lua 脚本可以在 argocd-cm ConfigMap 中或上游的 Argo CD resource_customizations 目录中进行配置。这些自定义操作有两个 Lua 脚本：一个用于修改该资源，另一个用于检测是否可以执行该操作（例如，用户不应该能够恢复未暂停的 Rollout）。Argo CD 允许用户通过 UI 或 CLI 执行这些操作。\n在 CLI 中，用户（或 CI 系统）可以运行\nargocd app actions run \u0026lt;APP_NAME\u0026gt; \u0026lt;ACTION\u0026gt; 该命令执行列出在应用程序上的操作。\n在 UI 中，用户可以单击资源的汉堡包按钮，操作将在几秒钟内出现。用户可以单击并确认该操作以执行它。\n目前，Argo CD 中的 Rollout 操作有两个可用的自定义操作：resume 和 restart。\nresume：取消暂停具有 PauseCondition 的 Rollout。 restart：设置 RestartAt 并导致所有 pod 重新启动。 Argo Rollout 需要像 Istio 这样的 Service Mesh 吗？ Argo Rollouts 不需要使用服务网格或入口控制器。在缺乏流量路由提供程序的情况下，Argo Rollouts 管理金丝雀/稳定 ReplicaSets 的复制计数，以实现所需的金丝雀权重。正常的 Kubernetes Service 路由（通过 kube-proxy）用于在 ReplicaSets 之间分配流量。\nArgo Rollout 需要我们在组织中遵循 GitOps 吗？ Argo Rollouts 是一个 Kubernetes 控制器，无论如何更改清单，它都会对其做出反应。清单可以通过 Git 提交、API 调用、另一个控制器甚至是手动的 kubectl 命令进行更改。你可以使用 Argo Rollouts 与任何不遵循 GitOps 方法的传统 CI/CD 解决方案。\n我们可以以 HA 模式运行 Argo Rollouts 控制器吗？ 可以。k8s 集群可以运行多个 Argo-rollouts 控制器副本以实现 HA。要启用此功能，请使用 --leader-elect 标志运行控制器，并增加控制器的部署清单中的副本数。该实现基于 k8s client-go 的 leaderelection package 。该实现对副本之间的任意时钟偏差具有容错能力。偏差率的容忍度可以通过适当设置 --leader-election-lease-duration 和 --leader-election-renew-deadline 来配置。有关详细信息，请参阅package documentation 。\nRollouts Argo Rollouts 支持哪些部署策略？ Argo Rollouts 支持 BlueGreen、Canary 和 Rolling Update。此外，可以在蓝绿/金丝雀更新之上启用渐进式交付功能，进一步提供高级部署，例如自动分析和回滚。\nRollout 对象在创建时是否遵循提供的策略？ 与 Deployments 一样，Rollouts 在初始部署时不遵循策略参数。控制器尝试通过从提供的 .spec.template 创建完全扩展的 ReplicaSet 来尽快将 Rollout 逐步引入稳定状态。一旦 Rollout 有一个稳定的 ReplicaSet 可以过渡，控制器就开始使用提供的策略将前一个 ReplicaSet 过渡到所需的 ReplicaSet。\n蓝绿回滚如何工作？ BlueGreen Rollout 保留旧的 ReplicaSet 运行 30 秒或 scaleDownDelaySeconds 的值。控制器通过添加名为 argo-rollouts.argoproj.io/scale-down-deadline 的注释来跟踪缩小规模之前的剩余时间。如果在旧 ReplicaSet 缩小之前应用旧的 Rollout 清单，则控制器执行所谓的快速回滚。控制器立即将活动服务的选择器切换回旧 ReplicaSet 的 rollout-pod-template-hash，并从该 ReplicaSet 中删除缩小的注释。控制器在尝试引入新版本时不执行任何常规操作，因为它试图尽快恢复。如果缩小范围注释过去，并且旧的 ReplicaSet 已经缩小，则会发生非快速跟踪回滚。在这种情况下，Rollout 将 ReplicaSet 视为任何新的 ReplicaSet，并遵循部署新的 ReplicaSet 的常规程序。\n什么是 argo-rollouts.argoproj.io/managed-by-rollouts 注释？ Argo Rollouts 在控制器修改的服务和 Ingress 中添加了 argo-rollouts.argoproj.io/managed-by-rollouts 注释。它们用于在删除管理这些资源的 Rollout 时，控制器尝试将它们还原回它们的先前状态。\nRollbacks Argo Rollouts 在回滚时会在 Git 中写回吗？ 不会。Argo Rollouts 不会读取/写入 Git 中的任何内容。实际上，Argo Rollouts 对 Git 存储库一无所知（只有 Argo CD 有此信息，如果它管理 Rollout）。当回滚发生时，Argo Rollouts 将应用程序标记为“降级”，并将集群上的版本更改回已知稳定版本。\n如果我同时使用 Argo Rollouts 和 Argo CD，回滚的情况下是否会出现无限循环？ 不会出现无限循环。如前面的问题中已经解释的那样，Argo Rollouts 在任何方式上都不会干扰 Git。如果你同时使用这两个 Argo 项目，回滚的事件序列如下：\n版本 N 在 Rollout（由 Argo CD 管理）上在集群上运行。Git 存储库已更新为 Rollout/Deployment 清单中的版本 N+1 Argo CD 在 Git 中看到更改并使用新的 Rollout 对象更新集群中的实时状态 Argo Rollouts 接管，因为它观察所有 Rollout 对象的更改。Argo Rollouts 完全不知道 Git 中正在发生的事情。它只关心在集群中实时运行的 Rollout 对象正在发生什么。 Argo Rollouts 尝试使用所选策略（例如，蓝/绿）应用版本 N+1。 版本 N+1 由于某种原因无法部署。 Argo Rollouts 再次缩小（或切换回流量）到集群中的版本 N。Argo Rollouts 不会在 Git 中进行任何更改 集群正在运行版本 N，完全健康 Rollout 在 ArgoCD 和 Argo Rollouts 中都标记为“降级”。 如果 Git 中的 Rollout 对象与集群中的相同，则 Argo CD 同步不会采取进一步措施。它们都提到版本 N+1 那么我如何使 Argo Rollouts 在回滚时写回 Git？ 如果你只想使用 Argo CD 回退到以前的版本，则不需要这样做。当部署失败时，Argo Rollouts 会自动将集群设置回稳定/以前的版本，如前面的问题中所述。你不需要写入 Git 以实现此目的。集群仍然健康，你已避免了停机时间。然后，你应该修复问题并向前滚动（即部署下一个版本），如果你想以严格的方式遵循 GitOps。如果你想在部署失败后使 Argo Rollouts 在 Git 中写回，则需要使用外部系统或编写自定义粘合代码来协调此操作。但通常不需要这样做。\nRollouts with Argo Rollouts 和 Rollbacks with Argo CD 之间有什么关系？ 它们完全没有关系。Argo Rollouts 的 “rollbacks” 将集群切换回上一个版本，如前一个问题所述。它们不会以任何方式触及或影响 Git。Argo CD 的 rollbacks 只是将集群指回以前的 Git 哈希。通常，如果你有 Argo Rollouts，就不需要使用 Argo CD 回滚命令。\n如何在单个步骤中部署多个服务并根据它们的依赖关系回滚？ Rollout 规范专注于单个应用程序/部署。Argo Rollouts 不知道应用程序依赖关系。如果你想以智能方式同时部署多个应用程序（例如，如果后端部署失败，则自动回滚前端），则需要在 Argo Rollouts 之上编写自己的解决方案。在大多数情况下，你需要为要部署的每个应用程序创建一个 Rollout 资源。理想情况下，你还应该使你的服务向前向后兼容（即前端应该能够与 backend-preview 和 backend-active 一起工作）。\n如何运行自己的自定义测试（例如烟雾测试）以决定是否应进行回滚？ 使用自定义 Job 或 Web Analysis。你可以将所有烟雾测试打包到单个容器中，并将它们作为 Job 分析运行。Argo Rollouts 将使用分析结果自动回滚，如果测试失败。\n实验 为什么我的实验没有结束？ 实验的持续时间由 .spec.duration 字段和为实验创建的分析控制。.spec.duration 指示实验创建的 ReplicaSet 应运行多长时间。一旦时间过去，实验将缩小它创建的 ReplicaSet，并将 AnalysisRuns 标记为成功，除非实验中使用了 requiredForCompletion 字段。如果启用了该字段，ReplicaSet 仍然会被缩小，但是直到 Analysis Run 完成，实验才会结束。\n此外，.spec.duration 是一个可选字段。如果它未设置，并且实验未创建任何 AnalysisRuns，则 ReplicaSets 将无限期运行。实验创建没有 requiredForCompletion 字段的 AnalysisRuns，当所创建的 AnalysisRun 失败或出错时，实验失败。如果设置了 requiredForCompletion 字段，则实验仅在 AnalysisRun 完成成功时标记自己为成功并缩小创建的 ReplicaSets。\n此外，如果 .spec.terminate 字段设置为 true，则实验会结束，无论实验的状态如何。\n分析 为什么我的 AnalysisRun 没有结束？ AnalysisRun 的持续时间由指定的度量标准控 …","relpermalink":"/book/argo-rollouts/faq/","summary":"一般问题 Argo Rollouts 是依赖 Argo CD 或其他 Argo 项目吗？ Argo Rollouts 是一个独立的项目。虽然它与 Argo CD 和其他 Argo 项目配合使用效果很好，但它也可以单独用于渐进式交付场景。更具体地说，Argo Rollouts 不需要你也在同一集群上安装 Argo CD。 Argo Rollouts 如何","title":"FAQ"},{"content":"下面的例子是为每个团队或用户组，可以使用 kubectl 命令或 YAML 文件创建一个 Kubernetes 命名空间。应避免使用任何带有 kube 前缀的名称，因为它可能与 Kubernetes 系统保留的命名空间相冲突。\nKubectl 命令来创建一个命名空间。\nkubectl create namespace \u0026lt;insert-namespace-name-here\u0026gt; 要使用 YAML 文件创建命名空间，创建一个名为 my-namespace.yaml 的新文件，内容如下：\napiVersion: v1 kind: Namespace metadata: name: \u0026lt;insert-namespace-name-here\u0026gt; 应用命名空间，使用：\nkubectl create –f ./my-namespace.yaml 要在现有的命名空间创建新的 Pod，请切换到所需的命名空间：\nkubectl config use-context \u0026lt;insert-namespace-here\u0026gt; 应用新的 Deployment，使用：\nkubectl apply -f deployment.yaml 另外，也可以用以下方法将命名空间添加到 kubectl 命令中：\nkubectl apply -f deployment.yaml --namespace=\u0026lt;insert-namespace-here\u0026gt; 或在 YAML 声明中的元数据下指定 namespace：\u0026lt;insert-namespace-here\u0026gt;。\n一旦创建，资源不能在命名空间之间移动。必须删除该资源，然后在新的命名空间中创建。\n","relpermalink":"/book/kubernetes-hardening-guidance/appendix/d/","summary":"下面的例子是为每个团队或用户组，可以使用 kubectl 命令或 YAML 文件创建一个 Kubernetes 命名空间。应避免使用任何带有 kube 前缀的名称，因为它可能与 Kubernetes 系统保留的命名空间相冲突。 Kubectl 命令来创建一个命名空间。 kubectl create namespace \u003cinsert-namespace-name-here\u003e 要使用 YAML 文件创建命名","title":"附录 D：命名空间示例"},{"content":"报告漏洞 如果你在 Argo Rollouts 中发现与安全相关的错误，我们恳请你负责任地进行披露，并给我们适当的时间来反应、分析和开发修复程序，以减轻发现的安全漏洞。\n请通过电子邮件将漏洞报告至以下地址：\ncncf-argo-security@lists.cncf.io 所有漏洞和相关信息都将得到完全保密。\n公开披露 我们将使用 GitHub 安全建议 功能发布安全建议，以使我们的社区充分了解情况，并将感谢你的发现（当然，除非你愿意保持匿名）。\nInternet Bug Bounty 合作 我们很高兴地宣布，Argo 项目正在与 Hacker One 及其互联网错误赏金计划 的优秀人员合作，以奖励在四个主要 Argo 项目（CD、活动、推出和工作流程）中发现安全漏洞的优秀人员然后与我们一起以负责任的方式修复和披露它们。\n如果你按照本安全政策中的规定向我们报告漏洞，我们将与你共同确定你的发现是否符合领取赏金的条件，以及如何领取赏金。\n","relpermalink":"/book/argo-rollouts/security/","summary":"报告漏洞 如果你在 Argo Rollouts 中发现与安全相关的错误，我们恳请你负责任地进行披露，并给我们适当的时间来反应、分析和开发修复程序，以减轻发现的安全漏洞。 请通过电子邮件将漏洞报告至以下地址： cncf-argo-security@lists.cncf.io 所有漏洞和相关信息都将","title":"Argo Rollouts 的安全策略"},{"content":"网络策略根据使用的网络插件而不同。下面是一个网络策略的例子，参考 Kubernetes 文档 将 nginx 服务的访问限制在带有标签访问的 Pod 上。\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: example-access-nginx namespace: prod #这可以是任何一个命名空间，或者在不使用命名空间的情况下省略。 spec: podSelector: matchLabels: app: nginx ingress: - from: - podSelector: matchLabels: access: \u0026#34;true\u0026#34; 新的 NetworkPolicy 可以通过以下方式应用：\nkubectl apply -f policy.yaml 一个默认的拒绝所有入口的策略：\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-ingress spec: podSelector: {} policyType: - Ingress 一个默认的拒绝所有出口的策略：\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-egress spec: podSelector: {} policyType: - Egress ","relpermalink":"/book/kubernetes-hardening-guidance/appendix/e/","summary":"网络策略根据使用的网络插件而不同。下面是一个网络策略的例子，参考 Kubernetes 文档 将 nginx 服务的访问限制在带有标签访问的 Pod 上。 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: example-access-nginx namespace: prod #这可以是任何一个命名空间，或者在不使用命名空间的情况下省略。 spec: podSelector: matchLabels: app: nginx","title":"附录 E：网络策略示例"},{"content":"Argo CD 允许用户定制同步目标集群中所需状态的某些方面。某些同步选项可以定义为特定资源中的注释。大多数同步选项在应用程序资源 spec.syncPolicy.syncOptions 属性中配置。使用 argocd.argoproj.io/sync-options 注释配置的多个同步选项可以在注释值中使用 , 进行连接；空格将被删除。\n下面你可以找到有关每个可用同步选项的详细信息：\n无修整资源 v1.1\n你可能希望防止修整对象：\nmetadata: annotations: argocd.argoproj.io/sync-options: Prune=false 在 UI 中，Pod 将仅显示为不同步：\n同步选项无修整 同步状态面板显示跳过修整的原因：\n同步选项无修正 如果 Argo CD 期望剪切资源，则应用程序将失去同步。你可能希望与 比较选项 结合使用。\n禁用 Kubectl 验证 对于某些对象类，需要使用 --validate=false 标志使用 kubectl apply 将其应用。例如使用 RawExtension 的 Kubernetes 类型，例如 ServiceCatalog 。你可以使用以下注释执行此操作：\nmetadata: annotations: argocd.argoproj.io/sync-options: Validate=false 如果要全局排除整个对象类，请考虑在 系统级配置 中设置 resource.customizations。\n跳过新的自定义资源类型的干预运行 在同步尚未知道集群的自定义资源时，通常有两个选项：\nCRD 清单是同步的一部分。然后，Argo CD 将自动跳过干预运行，将应用 CRD 并创建资源。 在某些情况下，CRD 不是同步的一部分，但可以通过其他方式创建，例如通过集群中的控制器。例如是 gatekeeper ，它根据用户定义的 ConstraintTemplates 创建 CRD。Argo CD 无法在同步中找到 CRD，并将出现错误 the server could not find the requested resource。 要跳过缺少资源类型的干预运行，请使用以下注释：\nmetadata: annotations: argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true 如果 CRD 已经存在于集群中，则仍将执行干预运行。\n无资源删除 对于某些资源，你可能希望在删除应用程序后仍保留它们，例如持久卷索赔。在这种情况下，你可以使用以下注释阻止在删除应用程序时清除这些资源：\nmetadata: annotations: argocd.argoproj.io/sync-options: Delete=false 选择性同步 当前，在使用自动同步进行同步时，Argo CD 应用程序中的每个对象都会应用。对于包含数千个对象的应用程序，这需要相当长的时间，并对 API 服务器施加不必要的压力。打开选择性同步选项，仅同步不同步的资源。\n你可以通过以下方式添加此选项\n在清单中添加 ApplyOutOfSyncOnly=true 示例：\napiVersion: argoproj.io/v1alpha1 kind: Application spec: syncPolicy: syncOptions: - ApplyOutOfSyncOnly=true 通过 argocd cli 设置同步选项 示例：\n$ argocd app set guestbook --sync-option ApplyOutOfSyncOnly=true 资源修整删除传播策略 默认情况下，使用前台删除策略删除多余的资源。可以控制传播策略 使用 PrunePropagationPolicy 同步选项。支持的策略是 background、foreground 和 orphan。有关这些策略的更多信息可以在 这里 找到。\napiVersion: argoproj.io/v1alpha1 kind: Application spec: syncPolicy: syncOptions: - PrunePropagationPolicy=foreground 修整最后 此功能是为了允许在同步操作的最后一个隐式波之后，对资源进行修整，在其他资源已部署并变得健康之后，所有其他波成功完成之后。\napiVersion: argoproj.io/v1alpha1 kind: Application spec: syncPolicy: syncOptions: - PruneLast=true 这也可以在个体资源级别进行配置。\nmetadata: annotations: argocd.argoproj.io/sync-options: PruneLast=true 替换资源而不是应用更改 默认情况下，Argo CD 执行 kubectl apply 操作以应用存储在 Git 中的配置。在某些情况下， kubectl apply 不适用。例如，资源规范可能太大，无法适合 添加的 kubectl.kubernetes.io/last-applied-configuration 注释。在这种情况下，你 可能会使用 Replace=true 同步选项：\napiVersion: argoproj.io/v1alpha1 kind: Application spec: syncPolicy: syncOptions: - Replace=true 如果设置了 Replace=true 同步选项，Argo CD 将使用 kubectl replace 或 kubectl create 命令来应用更改。\n🔔 警告：在同步过程中，资源将使用 ‘kubectl replace/create’ 命令进行同步。此同步选项具有破坏性，可能导致必须重新创建资源，从而可能导致你的应用程序停机。\n这也可以在单个资源级别进行配置。\nmetadata: annotations: argocd.argoproj.io/sync-options: Replace=true 服务器端应用 此选项启用 Kubernetes 服务器端应用 。\n默认情况下，Argo CD 执行 kubectl apply 操作以应用存储在 Git 中的配置。这是一个客户端操作，依赖于 kubectl.kubernetes.io/last-applied-configuration 注释以存储上一个资源状态。\n但是，有些情况下，你希望使用 kubectl apply --server-side 而不是 kubectl apply：\n资源太大，无法适应允许的注释大小 262144 字节。在这种情况下，可以使用服务器端应用程序来避免此问题，因为在此情况下不使用注释。 对集群上不完全由 Argo CD 管理的现有资源进行修补。 使用更具声明性的方法，它跟踪用户的字段管理，而不是用户的上一次应用状态。 如果设置了 ServerSideApply=true 同步选项，Argo CD 将使用 kubectl apply --server-side 命令来应用更改。\n它可以在应用程序级别启用，如下例所示：\napiVersion: argoproj.io/v1alpha1 kind: Application spec: syncPolicy: syncOptions: - ServerSideApply=true 要为单个资源启用 ServerSideApply，可以使用 sync-option 注释：\nmetadata: annotations: argocd.argoproj.io/sync-options: ServerSideApply=true ServerSideApply 还可用于通过提供部分 yaml 来修补现有资源。例如，如果有一个要求仅更新给定部署中的副本数的部署，可以向 Argo CD 提供以下 yaml：\napiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 3 请注意，根据部署模式规范，这不是有效的清单。在这种情况下，必须提供一个额外的同步选项 必须 以跳过模式验证。下面的示例显示了如何配置应用程序以启用两个必要的同步选项：\napiVersion: argoproj.io/v1alpha1 kind: Application spec: syncPolicy: syncOptions: - ServerSideApply=true - Validate=false 在这种情况下，Argo CD 将使用 kubectl apply --server-side --validate=false 命令应用更改。\n注意：Replace=true 优先于 ServerSideApply=true。\n如果发现共享资源，则同步失败 默认情况下，Argo CD 将应用在 Application 中配置的 Git 路径中找到的所有清单，而不管 yamls 中定义的资源是否已被另一个应用程序应用。如果设置了 FailOnSharedResource 同步选项，则在当前应用程序中发现已由另一个应用程序在集群中应用的资源时，Argo CD 将使同步失败。\napiVersion: argoproj.io/v1alpha1 kind: Application spec: syncPolicy: syncOptions: - FailOnSharedResource=true 尊重忽略差异配置 此同步选项用于使 Argo CD 在同步阶段期间也考虑 spec.ignoreDifferences 属性中所做的配置。默认情况下，Argo CD 仅使用 ignoreDifferences 配置来计算实际状态和期望状态之间的差异，从而定义应用程序是否已同步。但是，在同步阶段期间，将按原样应用期望状态。使用三方合并计算补丁，其中包括实际状态、期望状态和 last-applied-configuration 注释。这有时会导致不希望的结果。可以通过将 RespectIgnoreDifferences=true 同步选项设置如下来更改此行为：\napiVersion: argoproj.io/v1alpha1 kind: Application spec: ignoreDifferences: - group: \u0026#34;apps\u0026#34; kind: \u0026#34;Deployment\u0026#34; jsonPointers: - /spec/replicas syncPolicy: syncOptions: - RespectIgnoreDifferences=true 上面的示例显示了如何配置 Argo CD 应用程序，以便在同步阶段期间它将忽略期望状态（git）中的 spec.replicas 字段。这是通过在应用之前计算和预打补丁期望状态来实现的。请注意，仅当资源已在集群中创建时，RespectIgnoreDifferences 同步选项才有效。如果正在创建应用程序并且不存在实际状态，则期望状态将按原样应用。\n创建命名空间 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: namespace: argocd spec: destination: server: https://kubernetes.default.svc namespace: some-namespace syncPolicy: syncOptions: - CreateNamespace=true 上面的示例显示了如何配置 Argo CD 应用程序， …","relpermalink":"/book/argo-cd/user-guide/sync-options/","summary":"Argo CD 允许用户定制同步目标集群中所需状态的某些方面。某些同步选项可以定义为特定资源中的注释。大多数同步选项在应用程序资源 spec.syncPolicy.syncOptions 属性中配置。使用 argocd.argoproj.io/sync-options 注释配置的多个同步选项可以在注释值中使用 , 进行连接；空格将被删","title":"同步选项 "},{"content":"在 Kubernetes 1.10 和更新版本中，LimitRange 支持被默认启用。下面的 YAML 文件为每个容器指定了一个 LimitRange，其中有一个默认的请求和限制，以及最小和最大的请求。\napiVersion: v1 kind: LimitRange metadata: name: cpu-min-max-demo-lr spec: limits - default: cpu: 1 defaultRequest: cpu: 0.5 max: cpu: 2 min: cpu 0.5 type: Container LimitRange 可以应用于命名空间，使用：\nkubectl apply -f \u0026lt;example-LimitRange\u0026gt;.yaml --namespace=\u0026lt;Enter-Namespace\u0026gt; 在应用了这个 LimitRange 配置的例子后，如果没有指定，命名空间中创建的所有容器都会被分配到默认的 CPU 请求和限制。命名空间中的所有容器的 CPU 请求必须大于或等于最小值，小于或等于最大 CPU 值，否则容器将不会被实例化。\n","relpermalink":"/book/kubernetes-hardening-guidance/appendix/f/","summary":"在 Kubernetes 1.10 和更新版本中，LimitRange 支持被默认启用。下面的 YAML 文件为每个容器指定了一个 LimitRange，其中有一个默认的请求和限制，以及最小和最大的请求。 apiVersion: v1 kind: LimitRange metadata: name: cpu-min-max-demo-lr spec: limits - default: cpu: 1 defaultRequest: cpu: 0.5 max: cpu: 2 min:","title":"附录 F：LimitRange 示例"},{"content":"通过将 YAML 文件应用于命名空间或在 Pod 的配置文件中指定要求来创建 ResourceQuota 对象，以限制命名空间内的总体资源使用。下面的例子是基于 Kubernetes 官方文档 的一个命名空间的配置文件示例：\napiVersion: v1 kind: ResourceQuota metadata: name: example-cpu-mem-resourcequota spec: hard: requests.cpu: \u0026#34;1\u0026#34; requests.memory: 1Gi limits.cpu: \u0026#34;2\u0026#34; limits.memory: 2Gi 可以这样应用这个 ResourceQuota：\nkubectl apply -f example-cpu-mem-resourcequota.yaml -- namespace=\u0026lt;insert-namespace-here\u0026gt; 这个 ResourceQuota 对所选择的命名空间施加了以下限制：\n每个容器都必须有一个内存请求、内存限制、CPU 请求和 CPU 限制。 所有容器的总内存请求不应超过 1 GiB 所有容器的总内存限制不应超过 2 GiB 所有容器的 CPU 请求总量不应超过 1 个 CPU 所有容器的总 CPU 限制不应超过 2 个 CPU ","relpermalink":"/book/kubernetes-hardening-guidance/appendix/g/","summary":"通过将 YAML 文件应用于命名空间或在 Pod 的配置文件中指定要求来创建 ResourceQuota 对象，以限制命名空间内的总体资源使用。下面的例子是基于 Kubernetes 官方文档 的一个命名空间的配置文件示例： apiVersion: v1 kind: ResourceQuota metadata: name: example-cpu-mem-resourcequota spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi 可以这样应用","title":"附录 G：ResourceQuota 示例"},{"content":"以下环境变量可与 argocd CLI 一起使用：\n环境变量 描述 ARGOCD_SERVER 不带 https:// 前缀的 ArgoCD 服务器地址（而不是为每个命令指定 --server ）例如：ARGOCD_SERVER=argocd.mycompany.com 如果通过 DNS 入口提供服务 ARGOCD_AUTH_TOKEN ArgoCD apiKey 以便你的 ArgoCD 用户能够进行身份验证 ARGOCD_OPTS 传递到 argocd CLI 的命令行选项，例如 ARGOCD_OPTS=\u0026#34;--grpc-web\u0026#34; ","relpermalink":"/book/argo-cd/user-guide/environment-variables/","summary":"以下环境变量可与 argocd CLI 一起使用： 环境变量 描述 ARGOCD_SERVER 不带 https:// 前缀的 ArgoCD 服务器地址（而不是为每个命令指定 --server ）例如：ARGOCD_SERVER=argocd.mycompany.com 如果通过 DNS 入口提供服务 ARGOCD_AUTH_TOKEN ArgoCD apiKey 以便","title":"环境变量"},{"content":"要对秘密数据进行静态加密，下面的加密配置文件提供了一个例子，以指定所需的加密类型和加密密钥。将加密密钥存储在加密文件中只能稍微提高安全性。Secret 将被加密，但密钥将在 EncryptionConfiguration 文件中被访问。这个例子是基于 Kubernetes 的官方文档 。\napiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: \u0026lt;base 64 encoded secret\u0026gt; - identity: {} 要使用该加密文件进行静态加密，请在重启 API 服务器时设置 --encryption-provider-config 标志，并注明配置文件的位置。\n","relpermalink":"/book/kubernetes-hardening-guidance/appendix/h/","summary":"要对秘密数据进行静态加密，下面的加密配置文件提供了一个例子，以指定所需的加密类型和加密密钥。将加密密钥存储在加密文件中只能稍微提高安全性。Secret 将被加密，但密钥将在 EncryptionConfiguration 文件中被访问。这个例子是基于","title":"附录 H：加密示例"},{"content":"本章记录了用于在 Cilium 中配置网络策略的策略语言。安全策略可以通过以下机制指定和导入：\n使用 Kubernetes NetworkPolicy、CiliumNetworkPolicy 和 CiliumClusterwideNetworkPolicy 资源。更多细节请参见网络策略一节。在这种模式下，Kubernetes 将自动向所有代理分发策略。 通过代理的 CLI 或 API 参考直接导入到代理中。这种方法不会自动向所有代理分发策略。用户有责任在所有需要的代理中导入策略。 本章内容包括：\n策略执行模式 规则基础 三层示例 四层示例 七层示例 拒绝政策 主机策略 七层协议可视性 在策略中使用 Kubernetes 构造 端点生命周期 故障排除 阅读本章 ","relpermalink":"/book/cilium-handbook/policy/","summary":"本章记录了用于在 Cilium 中配置网络策略的策略语言。安全策略可以通过以下机制指定和导入： 使用 Kubernetes NetworkPolicy、CiliumNetworkPolicy 和 CiliumClusterwideNetworkPolicy 资源。更多细节请参见网络策略一节。在这种模式","title":"网络策略"},{"content":"要用密钥管理服务（KMS）提供商插件来加密 Secret，可以使用以下加密配置 YAML 文件的例子来为提供商设置属性。这个例子是基于 Kubernetes 的官方文档 。\napiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets providers: - kms: name: myKMSPlugin endpoint: unix://tmp/socketfile.sock cachesize: 100 timeout: 3s - identity: {} 要配置 API 服务器使用 KMS 提供商，请将 --encryption-provider-config 标志与配置文件的位置一起设置，并重新启动 API 服务器。\n要从本地加密提供者切换到 KMS，请将 EncryptionConfiguration 文件中的 KMS 提供者部分添加到当前加密方法之上，如下所示。\napiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets providers: - kms: name: myKMSPlugin endpoint: unix://tmp/socketfile.sock cachesize: 100 timeout: 3s - aescbc: keys: - name: key1 secret: \u0026lt;base64 encoded secret\u0026gt; 重新启动 API 服务器并运行下面的命令来重新加密所有与 KMS 供应商的 Secret。\nkubectl get secrets --all-namespaces -o json | kubectl replace -f - ","relpermalink":"/book/kubernetes-hardening-guidance/appendix/i/","summary":"要用密钥管理服务（KMS）提供商插件来加密 Secret，可以使用以下加密配置 YAML 文件的例子来为提供商设置属性。这个例子是基于 Kubernetes 的官方文档 。 apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets providers: - kms: name: myKMSPlugin endpoint: unix://tmp/socketfile.sock cachesize: 100 timeout: 3s - identity: {} 要配置 API 服务器使用","title":"附录 I：KMS 配置实例"},{"content":"要创建一个 pod-reader 角色，创建一个 YAML 文件，内容如下：\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: your-namespace-name name: pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] # \u0026#34;\u0026#34; 表示核心 API 组 resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] 应用角色：\nkubectl apply --f role.yaml 要创建一个全局性的 pod-reader ClusterRole：\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: default # \u0026#34;namespace\u0026#34; 被省略了，因为 ClusterRoles 没有被绑定到一个命名空间上 name: global-pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] # \u0026#34;\u0026#34; 表示核心 API 组 resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] 应用角色：\nkubectl apply --f clusterrole.yaml ","relpermalink":"/book/kubernetes-hardening-guidance/appendix/j/","summary":"要创建一个 pod-reader 角色，创建一个 YAML 文件，内容如下： apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: your-namespace-name name: pod-reader rules: - apiGroups: [\"\"] # \"\" 表示核心 API 组 resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] 应用角色： kubectl apply --f role.yaml 要创建一个全局性的 pod-reader ClusterRole： apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: default # \"namespace\" 被省略了，因为 ClusterRoles 没有被绑","title":"附录 J：pod-reader RBAC 角色"},{"content":"要创建一个 RoleBinding，需创建一个 YAML 文件，内容如下：\napiVersion: rbac.authorization.k8s.io/v1 # 这个角色绑定允许 \u0026#34;jane\u0026#34; 读取 \u0026#34;your-namespace-name\u0026#34; 的 Pod 命名空间 # 你需要在该命名空间中已经有一个名为 \u0026#34;pod-reader\u0026#34;的角色。 kind: RoleBinding metadata: name: read-pods namespace: your-namespace-name subjects: # 你可以指定一个以上的 \u0026#34;subject\u0026#34; - kind: User name: jane # \u0026#34;name\u0026#34; 是大小写敏感的 apiGroup: rbac.authorization.k8s.io roleRef: # \u0026#34;roleRef\u0026#34; 指定绑定到一个 Role/ClusterRole kind: Role # 必须是 Role 或 ClusterRole name: pod-reader # 这必须与你想绑定的 Role 或 ClusterRole 的名字相匹配 apiGroup: rbac.authorization.k8s.io 应用 RoleBinding：\nkubectl apply --f rolebinding.yaml 要创建一个ClusterRoleBinding，请创建一个 YAML 文件，内容如下：\napiVersion: rbac.authorization.k8s.io/v1 # 这个集群角色绑定允许 \u0026#34;manager\u0026#34; 组中的任何人在任何命名空间中读取 Pod 信息。 kind: ClusterRoleBinding metadata: name: global-pod-reader subjects: # 你可以指定一个以上的 \u0026#34;subject\u0026#34; - kind: Group name: manager # Name 是大小写敏感的 apiGroup: rbac.authorization.k8s.io roleRef: # \u0026#34;roleRef\u0026#34; 指定绑定到一个 Role/ClusterRole kind: ClusterRole # 必须是 Role 或 ClusterRole name: global-pod-reader # 这必须与你想绑定的 Role 或 ClusterRole 的名字相匹配 apiGroup: rbac.authorization.k8s.io 应用 RoleBinding：\nkubectl apply --f clusterrolebinding.yaml ","relpermalink":"/book/kubernetes-hardening-guidance/appendix/k/","summary":"要创建一个 RoleBinding，需创建一个 YAML 文件，内容如下： apiVersion: rbac.authorization.k8s.io/v1 # 这个角色绑定允许 \"jane\" 读取 \"your-namespace-name\" 的 Pod 命名空间 # 你需要在该命名空间中已经有一个名为 \"pod-reader\"的角色。 kind: RoleBinding metadata: name: read-pods namespace:","title":"附录 K：RBAC RoleBinding 和 ClusterRoleBinding 示例"},{"content":"下面是一个审计策略，它以最高级别记录所有审计事件：\napiVersion: audit.k8s.io/v1 kind: Policy rules: - level: RequestResponse # 这个审计策略记录了 RequestResponse 级别的所有审计事件 这种审计策略在最高级别上记录所有事件。如果一个组织有可用的资源来存储、解析和检查大量的日志，那么在最高级别上记录所有事件是一个很好的方法，可以确保当事件发生时，所有必要的背景信息都出现在日志中。如果资源消耗和可用性是一个问题，那么可以建立更多的日志规则来降低非关键组件和常规非特权操作的日志级别，只要满足系统的审计要求。如何建立这些规则的例子可以在 Kubernetes 官方文档 中找到。\n","relpermalink":"/book/kubernetes-hardening-guidance/appendix/l/","summary":"下面是一个审计策略，它以最高级别记录所有审计事件： apiVersion: audit.k8s.io/v1 kind: Policy rules: - level: RequestResponse # 这个审计策略记录了 RequestResponse 级别的所有审计事件 这种审计策略在最高级别上记录所有事件。如果一个组织有可用的资源来存储、解析和检查大量的日志，那","title":"附录 L：审计策略"},{"content":"选择性同步是仅同步某些资源的同步。你可以从 UI 中选择哪些资源：\n选择性同步 这样做时，请记住： 你的同步不会记录在历史记录中，因此无法回滚。 Hook 未运行。 选择性同步选项 v1.8\n打开选择性同步选项，该选项将仅同步不同步的资源。有关更多详细信息，请参阅同步选项 文档。\n","relpermalink":"/book/argo-cd/user-guide/selective-sync/","summary":"选择性同步是仅同步某些资源的同步。你可以从 UI 中选择哪些资源： 选择性同步 这样做时，请记住： 你的同步不会记录在历史记录中，因此无法回滚。 Hook 未运行。 选择性同步选项 v1.8 打开选择性同步选项，该选项将仅同步不同步的","title":"选择性同步"},{"content":" 介绍\n入门\n用例\n安全\n","relpermalink":"/book/argo-cd/operator-manual/applicationset/","summary":"介绍 入门 用例 安全","title":"ApplicationSet 控制器"},{"content":"在控制平面，用文本编辑器打开 kube-apiserver.yaml 文件。编辑 kube-apiserver 配置需要管理员权限。\nsudo vi /etc/kubernetes/manifests/kube-apiserver.yaml 在 kube-apiserver.yaml 文件中添加以下文字：\n--audit-policy-file=/etc/kubernetes/policy/audit-policy.yaml --audit-log-path=/var/log/audit.log --audit-log-maxage=1825 audit-policy-file 标志应该设置为审计策略的路径，而 audit-log-path 标志应该设置为所需的审计日志写入的安全位置。还有一些其他的标志，比如这里显示的 audit-log-maxage 标志，它规定了日志应该被保存的最大天数，还有一些标志用于指定要保留的最大审计日志文件的数量，最大的日志文件大小（兆字节）等等。启用日志记录的唯一必要标志是 audit-policy-file 和 audit-log-path 标志。其他标志可以用来配置日志，以符合组织的政策。\n如果用户的 kube-apiserver 是作为 Pod 运行的，那么就有必要挂载卷，并配置策略和日志文件位置的 hostPath 以保留审计记录。这可以通过在 Kubernetes 文档中 指出的 kube-apiserver.yaml 文件中添加以下部分来完成：\nvolumeMounts: - mountPath: /etc/kubernetes/audit-policy.yaml name: audit readOnly: true - mountPath: /var/log/audit.log name: audit-log readOnly: false volumes: - hostPath: path: /etc/kubernetes/audit-policy.yaml type: File name: audit - hostPath: path: /var/log/audit.log type: FileOrCreate name: audit-log ","relpermalink":"/book/kubernetes-hardening-guidance/appendix/m/","summary":"在控制平面，用文本编辑器打开 kube-apiserver.yaml 文件。编辑 kube-apiserver 配置需要管理员权限。 sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml 在 kube-apiserver.yaml 文件中添加以下文字： --audit-policy-file=/etc/kubernetes/policy/audit-policy.yaml --audit-log-path=/var/log/audit.log --audit-log-maxage=1825 audit-policy-file 标志应该设置为审计策略的路径，而 audit-log-path 标志应该设置为所需的审计日志写入的安全位置。还有一些其他的标志，比","title":"附录 M：向 kube-apiserver 提交审计策略文件的标志示例"},{"content":"YAML 文件示例：\napiVersion: v1 kind: Config preferences: {} clusters: - name: example-cluster cluster: server: http://127.0.0.1:8080 #web endpoint address for the log files to be sent to name: audit-webhook-service users: - name: example-users user: username: example-user password: example-password contexts: - name: example-context context: cluster: example-cluster user: example-user current-context: example-context #source: https://dev.bitolog.com/implement-audits-webhook/ 由 webhook 发送的审计事件是以 HTTP POST 请求的形式发送的，请求体中包含 JSON 审计事件。指定的地址应该指向一个能够接受和解析这些审计事件的端点，无论是第三方服务还是内部配置的端点。\n向 kube-apiserver 提交 webhook 配置文件的标志示例：\n在控制面编辑 kube-apiserver.yaml 文件\nsudo vi /etc/kubernetes/manifests/kube-apiserver.yaml 在 kube-apiserver.yaml 文件中添加以下文字\n--audit-webhook-config-file=/etc/kubernetes/policies/webhook-policy.yaml --audit-webhook-initial-backoff=5 --audit-webhook-mode=batch --audit-webhook-batch-buffer-size=5 audit-webhook-initial-backoff 标志决定了在一个初始失败的请求后要等待多长时间才能重试。可用的 webhook 模式有 batch、block 和 blocking-stric 的。当使用批处理模式时，有可能配置最大等待时间、缓冲区大小等。Kubernetes 官方文档包含了其他配置选项的更多细节审计 和 kube-apiserver 。\n","relpermalink":"/book/kubernetes-hardening-guidance/appendix/n/","summary":"YAML 文件示例： apiVersion: v1 kind: Config preferences: {} clusters: - name: example-cluster cluster: server: http://127.0.0.1:8080 #web endpoint address for the log files to be sent to name: audit-webhook-service users: - name: example-users user: username: example-user password: example-password contexts: - name: example-context context: cluster: example-cluster user: example-user current-context: example-context #source: https://dev.bitolog.com/implement-audits-webhook/ 由 webhook 发送的审计事件是以 HTTP POST 请求的形式发送的，请求体中包含 JSON 审计事件。指定的地址应该指向一个能","title":"附录 N：webhook 配置"},{"content":"本章大纲 介绍\n概念\n要求\n网络策略\n端点 CRD\n端点切片 CRD\nKubernetes 兼容性\n故障排除\n阅读本章 ","relpermalink":"/book/cilium-handbook/kubernetes/","summary":"本章大纲 介绍 概念 要求 网络策略 端点 CRD 端点切片 CRD Kubernetes 兼容性 故障排除 阅读本章","title":"Kubernetes 集成"},{"content":"本文将介绍 Tetrate 新推出的工具——Tetrate Vulnerability Scanner (TVS) ，一款针对 Istio 和 Envoy 定制的 CVE 扫描器。在深入了解 TVS 的具体功能前，我们先简要回顾 CVE 的概念及其在软件安全性中的核心作用。\nCVE 概览 CVE，即通用漏洞和暴露，是一个公开的漏洞数据库，由 MITRE 公司负责维护。它旨在为软件中的漏洞提供一个标准化的命名体系，包含了漏洞的标识号、详细描述及参考链接。虽然 CVE 本身不提供漏洞的严重性评分，但它为网络安全专家、开发人员和企业提供了一个获取关键安全信息的重要平台。每个 CVE 记录的唯一标识号便于追踪漏洞相关信息，诸如受影响的软件系统、修复措施等。例如，2021 年著名的 Log4j 漏洞（CVE-2021-44228）由于影响广泛，严重程度评级为 10。\nCVE 的实用场景 一个典型的 CVE 使用案例是将 CVE 扫描功能集成到 CI/CD 管道中，以自动化安全测试，从而阻止带有已知漏洞的代码合并到代码库并接收警报。这一过程有助于确保应用程序不会使用带有漏洞的包或库，从而提高了软件的安全性。\n例如 Github 将 CVE 检测和处理作为供应链安全 的关键一环，如果你在 Github 上托管了开源项目，并开启了 Dependabot，每当你的 PR 或 commit 存在漏洞时，你就可能收到类似下面的 CVE 通知：jQuery Cross Site Scripting vulnerability 。收到通知后你可以选择容忍该漏洞或者应用 patch。下图展示了 CVE 的处理流程。\ngraph TD A([Start]) --\u0026gt; B(Discover Vulnerability) B --\u0026gt;|Security Researchers| C{Is it a new vulnerability?} C --\u0026gt;|Yes| D[Assign CVE ID] C --\u0026gt;|No| E[Refer to existing CVE entry] D --\u0026gt;|\u0026#34;CVE Numbering Authorities (CNAs)\u0026#34;| F[Publicly disclose details] F --\u0026gt;|Vendors/Project Owners| G{Is patch available?} G --\u0026gt;|Yes| H[Develop and Release Patch] G --\u0026gt;|No| I[Issue Workaround or Mitigation Steps] H --\u0026gt; J[Deploy Patch] I --\u0026gt; J J --\u0026gt;|End Users/System Administrators| K[Apply Patch or Mitigation] K --\u0026gt; L[Monitor for Issues/Compliance] L --\u0026gt; M([End]) TVS 功能介绍 Istio 经常在其官网发布 CVE 通知，例如 ISTIO-SECURITY-2024-001 。以前，你必须手动跟踪这些通知，但现在你可以使用 TVS 自动执行 CVE 扫描任务，从而显着减少安全团队的工作量。\n目前 TVS 仅提供命令行工具，未来将作为服务集成到 TIS 中。下图展示了 TVS 运行结果。\nTVS CLI 下图说明了 TVS 的工作流程。\ngraph TD A([Start]) --\u0026gt; B(Istio Containers Installed) B --\u0026gt; C(Collect SHA Digests) C --\u0026gt; D(Send Digests to Tetrate\u0026#39;s APIs) D --\u0026gt; E{Is CVE Detected?} E --\u0026gt;|Yes| F[Log SHA Digests Without Personal Info] E --\u0026gt;|No| G[End, No Action Required] F --\u0026gt; H[Notify Users] H --\u0026gt; I[Apply Patches/Workarounds] I --\u0026gt; J([End]) 在 Istio 容器安装后开始。 收集安装的 Istio 容器的 SHA 摘要。 将摘要发送到 Tetrate 的 API。 API 检测是否存在 CVE。 如果检测到 CVE，记录 SHA 摘要但不包含任何个人信息，并通知用户。 如果未检测到 CVE，则不需要采取任何行动。 用户收到通知后，应用补丁或缓解措施。 流程结束。 所有人都可以免费下载和使用 TVS，不过在执行 CVE 扫描之前你需要先注册，详见 TIS 文档 。\n供应链安全建议 有报告 指出，现在即使最流行的容器最新版本也有数百个 CVE，下面是为了保证的一些建议：\n尽早开始漏洞扫描和处理，而不是等到最后 在 CI/CD 中集成漏洞扫描工具 定期更新 Istio 和 Envoy 到最新版本 使用 Istio 官方推出的 distroless 镜像 ，可以有效的减少攻击面并包含尽可能少的漏洞 遵循 CNCF 推出的软件供应链最佳实践 遵循 Istio 安全最佳实践 TVS 的独特价值 TVS 通过其命令行工具提供简便的 CVE 扫描操作，未来还计划在 Kubernetes 和 Tetrate Istio Subscription (TIS) 中集成，以进一步简化 Istio 和 Envoy 的 CVE 管理过程。TIS 提供自 Istio 发布起 14 个月内的 CVE 补丁和向后兼容支持，帮助用户及时获得安全更新，同时保持系统的稳定运行。\nTVS 为所有用户免费开放下载使用，使用前需进行简单的注册。更多信息请参阅 TIS 文档 。\n通过采纳 TVS 这一自动化的 CVE 扫描工具，企业能够更有效地识别和处理 Istio 及 Envoy 中的安全漏洞，提升基础设施的安全性，同时减轻安全团队的负担，推动安全管理流程的高效运作。\n","relpermalink":"/blog/tetrate-vulnerability-scaner/","summary":"本文将介绍 Tetrate 新推出的工具——Tetrate Vulnerability Scanner (TVS)，一款针对 Istio 和 Envoy 定制的 CVE 扫描器。","title":"TVS：Istio 和 Envoy CVE 扫描解决方案"},{"content":"上周我在巴黎参加了 KubeCon EU 2024 ，这也是我第一次参加中国以外的 KubeCon。本次大会可谓盛况空前，据说有 1.2 万人参加了会议。本文将为你分享我对本次 KubeCon 的一些观察，主要着重在我关注的服务网格与云原生基础架构领域。\nIstio Contributor 在 KubeCon EU Istio 展台 Istio、Cilium 及服务网格 Istio 和 Service Mesh 成为了热门讨论的话题，集中展示了在云原生生态系统中这两项技术的最新进展和应用。本次大会涵盖了从基础设施优化、数据本地化、分布式追踪到多集群部署等多个领域，反映了 Service Mesh 技术在实际应用中的广泛关注和持续创新。\n数据本地化和全局请求路由 Pigment 的 Arthur Busser 和 Baudouin Herlicq 分享了如何利用 Kubernetes 和 Istio 实现数据本地化的需求。他们介绍了利用 Istio 基于自定义头部进行请求路由的方法，这对于满足如 GDPR 和 CCPA 等法规的数据驻留要求至关重要。\n分布式跟踪和可观测性增强 ThousandEyes (part of Cisco) 的 Chris Detsicas 探讨了如何配置 Istio 以使用 OpenTelemetry 实现有效的分布式跟踪，这为微服务生态系统提供了宝贵的可见性，有助于问题诊断和性能优化。\n多集群部署和流量管理 China Mobile 的 Haiwen Zhang 和 Yongxi Zhang 介绍了一个简化 Istio 多集群部署的新方法，该方法使用一个全局唯一的 Istio 控制平面，通过主集群的 Apiserver 实现全局服务发现，自动连接多个集群的容器网络，为 Pod 提供直接网络连接。特别强调了 Kosmos 项目 ，它提供了一种新的解决方案，以简化多集群环境下的服务网格部署和管理。\nGoogle 的 Ameer Abbas 和 John Howard 探讨了如何在基础设施可靠性为 99.9% 的情况下构建出 99.99% 可靠性的服务，并提出了一系列应用架构原型（Archetypes），这些原型可以帮助设计和实现高可靠性的多集群应用程序。\n原型 1：活动 - 被动区域（Active Passive Zones） - 在单个区域的两个区域部署所有服务，使用 SQL 数据库的只读副本，通过 L4 负载均衡器实现区域内的故障转移。 原型 2：多区域（Multi Zonal） - 在单个区域的三个区域部署所有服务，使用高可用性 SQL 数据库，通过全局或区域负载均衡器实现区域内的故障转移。 原型 3：活动 - 被动区域（Active Passive Region） - 在两个区域的三个区域部署所有服务，使用跨区域复制的 SQL 数据库，通过 DNS 和负载均衡器实现区域间的故障转移。 原型 4：隔离区域（Isolated Regions） - 在两个区域的三个区域部署所有服务，使用 Spanner 或 CockroachDB 等全局数据库，通过区域负载均衡器和 DNS 实现区域间的故障转移。 原型 5：全局（Global） - 在两个或更多区域的三个区域部署所有服务，使用 Spanner 或 CockroachDB 等全局数据库，通过全球负载均衡器实现全球范围内的故障转移。 安全和零信任架构 多个议题，如 Microsoft 的 Niranjan Shankar 所介绍的，聚焦于在生产环境中加固 Istio 的重要性和方法。他讨论了利用 Istio 与网络策略、第三方 Kubernetes 工具和云提供的安全服务相结合，构建零信任和深层防御架构的步骤和策略。\nAmbient Mesh 的基础设施兼容性及未来 Benjamin Leggett 和 Yuval Kohavi 引入了一种创新的方法，使 Istio 的 Amibent mode 能够支持任意 Kubernetes CNI，详见 Istio 博客 。这一进步解决了 Ambient mesh 中 CNI 支持有限的问题，无需重启应用程序 Pod 即可将其纳入 Ambient mode，这对于简化操作和降低基础设施成本具有重要意义。\nIstio 社区宣布在即将到来的 Istio 1.22 版本，Ambient 模式将成为 beta，详见 CNCF 博客 。多个演讲和讨论聚焦于 Istio Ambient Mesh 的未来，特别是其简化工作负载操作和降低基础设施成本的潜力。Istio Ambient Mesh 的介绍预示了服务网格技术的一个新方向，即无 sidecar 的数据平面架构，提供了更高的性能和更低的资源消耗。\nSidecar-less 服务网格的革新 在 KubeCon EU 2024 上，关于 Sidecar 的讨论主要集中在评估和比较使用 Sidecar 与无 Sidecar（如 Istio 的 Ambient Mesh）服务网格模式的优缺点。特别是 Christian Posta 对 Cilium 和 Istio 在无 sidecar 服务网格实现方面的设计决策和权衡进行了深入分析，突出了这种模式在提高性能、降低资源消耗和简化运维操作方面的潜力。通过分析纽约时报从 Istio 过渡到 Cilium 的案例，进一步证明了无 sidecar 模式在处理复杂、多区域服务网格时的有效性，同时指出了在这一转变过程中的挑战和实施考虑。这些讨论预示着服务网格技术未来可能朝向更加灵活和高效的方向发展，其中无 Sidecar 架构可能成为优化云原生应用性能和资源使用的关键策略。\nCilium 与服务网格的交集 Cilium 在 KubeCon EU 2024 上被广泛讨论，作为一种基于 eBPF 的技术，Cilium 不仅被看作是一个高效的容器网络接口（CNI），而且还展示了其在服务网格领域的强大潜力。通过 Isovalent 和其他组织的演讲，Cilium 被展示为一种能够提供连接、观测和保障服务网格安全的先进解决方案。特别是 Cilium 的无 Sidecar 服务网格实现方式被认为是未来方向，其利用 eBPF 技术在不增加传统 Sidecar 代理负担的情况下实现了微服务的安全通信和精细流量管理。此外，Cilium 在服务网格之外的扩展能力，例如在多云网络和负载平衡方面的应用，凸显了其作为云原生生态系统基础设施核心组件的地位。Cilium 的这些讨论和案例研究证明了其在推动服务网格和云原生技术创新方面的重要作用。\n云原生趋势 当前云原生领域的几个主要趋势：\n可持续性和环保意识的增强：例如，Deutsche Bahn 将开发者引入其基础设施绿化过程，强调了在设计和运营云原生解决方案时，越来越多的公司开始考虑环境因素。这反映了一个趋势，即企业在追求技术进步的同时，也在努力减少对环境的影响，通过绿色计算和能效优化来实现可持续的技术生态。\n人工智能与云原生技术的融合：人工智能（AI）正在成为 Kubernetes 和云原生生态系统面临的下一个主要挑战。Nvidia 关于 AI 策略的讨论、CNCF 对 AI 在云原生未来中标准化工作的推动，以及各种关于 AI 和机器学习（ML）集成的工具和平台的更新，都突显了这一点。这一趋势表明，将 AI 和 ML 无缝集成到云原生架构中，不仅可以加速应用开发和部署，还能够提供更加智能和自动化的操作能力。同时 CNCF 还宣布成立 AI WG，并发布了人工智能白皮书 。\nWebAssembly（Wasm）的兴起：Cosmonic 对最新 Wasm 标准的支持，以及 Fermyon 将其开源 Wasm 平台 SpinKube 捐赠给 CNCF，显示了 WebAssembly 在云原生应用开发中日益增长的重要性。Wasm 提供了一种高效、安全的方式来运行在浏览器外的客户端和服务器端代码，这对于构建跨平台、高性能的云原生应用尤为重要。\n云原生观测性的强化：例如，New Relic 在其可观测性平台中添加了原生 Kubernetes 支持，凸显了对云原生应用的监控、日志记录和性能分析需求的增加。随着云原生架构的复杂性增加，企业需要更加强大的工具来保持系统的透明度和健康，从而优化性能和可靠性。\n云原生社区的协作和开源精神的强化：CNCF 成立最终用户技术咨询委员会 、Red Hat 与 Docker 合作开发 Testcontainers Cloud 框架等举措，反映了云原生社区致力于促进协作和分享的文化。这种开放的协作精神不仅加速了新技术的发展和采纳，也为云原生生态系统的健康成长提供了坚实的基础。\n这些趋势共同描绘了一个多元化、持续创新且日益成熟的云原生技术景观，其中可持续性、AI/ML 集成、WebAssembly、加强的可观测性和社区协作是推动这一领域前进的关键因素。\n总结 KubeCon EU 2024 的见闻为我们揭示了云原生技术领域的多个重要进展和未来方向。从服务网格的持续创新到云原生生态系统对环境可持续性的关注，再到人工智能与机器学习技术的深度整合，以及 WebAssembly 在应用开发中的日益重要性，这些趋势共同构成了当前云原生技术的前沿。\n特别值得注意的是，Istio 和 Cilium 在服务网格领域的最新动态，展现了无 Sidecar 架构的潜力以及 eBPF 技术在提升性能、安全性和可观测性方面的作用。这些进展不仅为开发者提供了更为高效和灵活的工具，也为云原生应用的设计和运营提出了新的思路。\n同时，云原生社区的持续发展和对开源精神的坚持，为技术创新和知识共享提供了坚实的基础。通过强化观测性、推动环境可持续性和促进技术标准化，云原生生态正展现出其深厚的发展潜力和广阔的应用前景。\n作为一名观察者和参与者，我深感云原生技术的快速发展给我们带来了前所未有的机遇和挑战。未来，随着技术的不断演进和社区的共同努力，我们有理由相信，云原生技术将在推动数字化转型和创造更加智能、可持续的技术世界方面发挥更大的作用。让我们拭目以待，并积极参与这一令人兴奋的技术旅程。\n","relpermalink":"/blog/kubecon-eu-paris-recap/","summary":"探索 KubeCon EU 2024：从 Istio  与Cilium 的最新动态，到云原生趋势如 AI 融合、Wasm 崛起、增强观测性的深入解读。","title":"KubeCon EU 2024 巴黎见闻与回顾"},{"content":"ICA 认证的变化和提醒 即将发生的政策变更：请注意，ICA 认证期限政策将于 2024 年 4 月 1 日 00:00UTC 发生变更。在此日期或之后获得的认证将于满足计划认证要求（包括通过考试）之日起 24 个月后到期。我们鼓励任何有兴趣并准备好的人在政策变更之前安排并参加考试。请在此处查看更多详细信息。\n当前的认证有效期是 3 年，4 月 1 日之后通过的认证有效期是 2 年。\nTetrate Academy 背景 Tetrate 运营的 Tetrate Academy 已经有好几年时间，期间推出了 Istio 基础教程、Envoy 基础教程和 CIAT 认证考试，一共有 13000 多人学习了 Tetrate Academy 的课程。去年 9 月 Tetrate 将 CIAT 贡献给了 CNCF，改名为 ICA 考试 ，至 11 月该认证考试正式上线。\nICA 的考试内容（来自 Linux Foundation ）\n考试背景 ICA 是由 Tetrate 贡献的 CIAT 考试而来，提供了对 Istio 的广泛知识和技能的验证。 考试采用线上远程监考方式进行，时间限制为 2 小时，考生需要在虚拟机环境中操作 Kubernetes 集群和 Istio。 考试过程中需要解决一系列问题还有一些多选题，需要达到 75% 的分数才能通过。 PSI 系统提供在线考试环境，考试结果将在考试结束后 24 小时内通过邮件发送给考生。 考试环境中提供了 Kubernetes 集群、Istio 安装、VS Code、kubectl、istioctl 等工具。 考试过程中可以访问 Istio 文档，试题和作答仅支持英语。 建议与提醒 考试前务必熟悉 Istio 文档 ，并提前做好准备，包括检查 PSI 系统、身份证明和考试环境。 参加 Tetrate 的免费 Istio 基础教程 ，加深对 Istio 的理解。 在考试中保持冷静，先解答熟悉的问题，再解答不熟悉的问题，确保高效完成考试。 参考链接 ICA 认证常见问题解答 ICA 认证重要说明 ","relpermalink":"/blog/ica-certificate/","summary":"本文介绍了 ICA 认证的起源和最近的变化，以及考试准备指南。ICA 考试是由 Tetrate 贡献的 CIAT 考试演变而来，为 Istio 技能认证提供了全面的考核。读者将了解考试的背景、考试过程、建议以及参考资料，有助于准备和通过考试。ICA 考试对 Istio 文档的熟悉度至关重要，建议先完成 Tetrate 的 Istio 基础教程。参加考试前应熟悉 PSI 系统和考试环境，保持冷静和专注，以确保顺利通过考试。","title":"ICA 认证：Istio 技能认证的最新变化和考试准备指南"},{"content":"我很高兴地宣布，我将会参加今年的 KubeCon\u0026amp;CloudNativeCon 活动，并且这也是我第一次有幸能够前往欧洲参加 KubeCon。我非常期待能够在巴黎与大家相聚，分享关于云原生技术的见解和经验。除了参加主要活动外，我还将出席 Istio Day ，并在 Tetrate 的展台（J14）与大家见面。我希望能够和你们一起度过这个令人兴奋的时刻，欢迎来找我交流和聊天！\n","relpermalink":"/notice/kubecon-eu-2024/","summary":"期待在巴黎与你见面。","title":"KubeCon 欧洲 2024 巴黎见！"},{"content":"本博文解析了在 Istio 服务网格中服务端获取客户端源 IP 的挑战，并提供了解决方案。将探讨以下问题：\n数据包传输中源 IP 丢失的原因； 如何确定客户端源 IP； 在南北向和东西向请求中传递源 IP 的策略； 针对 HTTP 和 TCP 协议的处理方法。 源 IP 保留的重要性 保留客户端源 IP 的主要理由包括：\n访问控制策略：基于源 IP 执行身份验证或安全策略； 负载均衡：实现基于客户端 IP 的请求路由； 数据分析：包含真实源地址的访问日志和监控指标，助力开发人员进行分析。 保留源 IP 的含义 保留源 IP 指的是在请求从客户端发出、经过负载均衡器或反向代理后，避免真实的客户端源 IP 被替换的情况。\n以下是源 IP 地址丢失的流程示例：\nsequenceDiagram participant C as Client participant LB as Load Balancer participant IG as Ingress Gateway participant S as Server C-\u0026gt;\u0026gt;LB: Initial Request LB-\u0026gt;\u0026gt;IG: Altered Request (IP Changed) IG-\u0026gt;\u0026gt;S: Forwarded Request Note over IG,S: Source IP Lost 上面图只是最常见的一种情况。本文考虑到以下几种情况：\n南北向流量：客户端通过负载均衡器（网关）访问服务端 只有一层网关 两层或两层以上网关 东西向流量：网格内部的服务间访问 协议：HTTP 和 TCP 如何确认客户端源 IP？ 在 Istio 服务网格中，Envoy 代理通常会将客户端 IP 添加到 HTTP 请求的 “X-Forwarded-For” 头部中。以下是确认客户端 IP 的步骤：\n检查 x-forwarded-for 头部：包含请求路径上各代理的 IP 地址。 选择最后一个 IP：通常，最后一个 IP 是最接近服务器的客户端 IP。 验证 IP 的可信性：检查代理服务器的信任度。 使用 x-envoy-external-address：Envoy 可以设置此头部，包含客户端真实 IP。 详情请见 Envoy 文档中对 x-forwarded-for 标头 的说明。对于 TCP/IP 连接，可以通过协议字段解析客户端 IP。\n测试环境 GKE\nClient Version: v1.28.4 Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 Server Version: v1.27.7-gke.1121000 Istio\nclient version: 1.20.1 control plane version: 1.20.1 data plane version: 1.20.1 (12 proxies) CNI\n我们使用了 Cilium CNI，但是没有开启无 kube-proxy 模式。\ncilium-cli: v0.15.18 compiled with go1.21.5 on darwin/amd64 cilium image (default): v1.14.4 cilium image (stable): unknown cilium image (running): 1.14.5 Node\n节点名称 内部 IP 备注 gke-cluster1-default-pool-5e4152ba-t5h3 10.128.0.53 gke-cluster1-default-pool-5e4152ba-ubc9 10.128.0.52 gke-cluster1-default-pool-5e4152ba-yzbg 10.128.0.54 Ingress Gateway Pod 所在节点 执行测试的本地客户端电脑的公网 IP：123.120.247.15\n部署测试示例 下图展示了测试方式：\nsequenceDiagram participant C as Client participant LB as Load Balancer participant IG as Ingress Gateway participant S as Source IP App C-\u0026gt;\u0026gt;LB: Initial Request LB-\u0026gt;\u0026gt;IG: Forward Request IG-\u0026gt;\u0026gt;S: Forwarded Request 首先参考 Istio 文档 部署 Istio，然后为 default 命名空间开启 sidecar 自动注入：\nkubectl label namespace default istio-injection=enabled 在 Istio 中部署 echo-server 应用测试。echo-server 是一个基于 Nginx 的服务器，用于回显客户端发送的请求信息，例如请求头、客户端地址、请求方法等。\nkubectl create deployment echo-server --image=registry.k8s.io/echoserver:1.4 kubectl expose deployment echo-server --name=clusterip --port=80 --target-port=8080 创建 Ingress Gateway：\ncat\u0026gt;config.yaml\u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: clusterip-gateway spec: selector: istio: ingressgateway # 根据你的环境选择适当的 selector servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;clusterip.jimmysong.io\u0026#34; # 替换成你想要使用的主机名 --- apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: clusterip-virtualservice spec: hosts: - \u0026#34;clusterip.jimmysong.io\u0026#34; # 替换成与 Gateway 中相同的主机名 gateways: - clusterip-gateway # 这里使用 Gateway 的名称 http: - route: - destination: host: clusterip.default.svc.cluster.local # 替换成你的 Service 的实际主机名 port: number: 80 # Service 的端口 EOF kubectl apply -f config.yaml 查看 Ingress Gateway 中的 Envoy 日志：\nkubectl logs -f deployment/istio-ingressgateway -n istio-system 查看 Sleep Pod 中的 Envoy 日志：\nkubectl logs -f deployment/sleep -n default -c istio-proxy 查看 Source IP App 中的 Envoy 日志：\nkubectl logs -f deployment/echo-server -n default -c istio-proxy 获取网关公网 IP：\nexport GATEWAY_IP=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) 在本地使用 curl 测试：\ncurl -H \u0026#34;Host: clusterip.jimmysong.io\u0026#34; $GATEWAY_IP 资源 IP 当部署好测试应用后，你需要获取与以下资源的 IP 地址。在接下来的实验环节中将会用到。\nPod\n下面是初始状况下的 Pod IP，随着对 Deployment 的补丁，Pod 会重建，名称和 IP 地址都会变。\nPod 名称 Pod IP echo-server-6d9f5d97d7-fznrq 10.32.1.205 sleep-9454cc476-2dskx 10.32.3.202 istio-ingressgateway-6c96bdcd74-zh46d 10.32.1.221 Service\nService 名称 Cluster IP External IP clusterip 10.36.8.86 - sleep 10.36.14.12 - istio-ingressgateway 10.36.4.127 35.188.212.88 南北向流量 我们首先考虑客户端位于 Kubernetes 集群外，通过负载均衡器来访问集群内部服务的情况。\n测试 1：Cluster 流量策略、iptables 流量劫持 这是通过以上步骤部署完测试应用后的默认情况，也是大家遇到的所谓的源 IP 地址丢失的情况。\ncurl 测试：\ncurl -H \u0026#34;Host: clusterip.jimmysong.io\u0026#34; $GATEWAY_IP 查看结果 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 CLIENT VALUES: client_address=127.0.0.6 command=GET real path=/ query=nil request_version=1.1 request_uri=http://clusterip.jimmysong.io:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=clusterip.jimmysong.io user-agent=curl/8.4.0 x-b3-parentspanid=03c124c5f910001a x-b3-sampled=1 x-b3-spanid=103dc912ec14f3b4 x-b3-traceid=140ffa034822077f03c124c5f910001a x-envoy-attempt-count=1 x-envoy-internal=true x-forwarded-client-cert=By=spiffe://cluster.local/ns/default/sa/default;Hash=79253e34e1c28d389e9bfb1a62ffe8944b2c3c369b46bf4a9faf055b55dedb7f;Subject=\u0026#34;\u0026#34;;URI=spiffe://cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account x-forwarded-for=10.128.0.54 x-forwarded-proto=http x-request-id=b3c05e22-594e-98da-ab23-da711a8f53ec BODY: -no body in request- 你只需要关注 client_address 和 x-forwarded-for 这两个结果即可。下文的 curl 测试结果中将省略其他信息。\n说明 该结果中字段的含义：\nclient_address：通过解析 TCP/IP 协议而获取的客户端 IP 地 …","relpermalink":"/blog/preserve-source-ip-in-istio/","summary":"本文专注于如何在 Istio 服务网格中保持客户端源 IP 的透明性。","title":"维持请求的透明度：在 Istio 中保留客户端请求的源 IP"},{"content":"在这篇博客中我将指导你如何安装 TIS 并启用监控插件。\n什么是 TIS？ Tetrate Istio Subscription（TIS）是由 Tetrate 提供的企业级、全面支持的产品，提供了经过全面测试且适用于所有主要云平台的 Istio 版本。TIS 基于开源的 Tetrate Istio Distro 项目，增加了对这些构建版本的全面高级支持，并可选提供 FIPS 验证的加密模块。此外，TIS 包含一系列经过测试和支持的 Add-Ons 和 Integrations，使得 Istio 的功能扩展和与常用基础设施工具的集成变得简单安全。\n为什么使用 TIS？ TIS 不是 Istio 的一个分支，而是提供针对特定环境进行测试的上游发行版。我们对 Istio 所做的任何增强都会应用于上游。TIS与普通的Istio相比有以下关键优势：\n长期支持：TIS提供14个月的安全更新支持，确保稳定性和安全性。 商业支持：TIS有商业支持选项，适用于企业用例，包括合规性需求。 易于管理：TIS提供简化的安装和管理过程，减少了操作复杂性。 多环境适配：TIS支持不同云环境，满足各种部署需求。 FIPS验证：提供FIPS验证版本，适用于高安全性要求的场景。 想要了解 TIS 的更多信息请访问：https://docs.tetrate.io/istio-subscription/ 准备条件 在安装 TIS 及其插件前，你需要准备：\n安装 Terraform ：用于导入 Dashboard 到 Grafana 向 Tetrate 请求安装 TIS 所需的 credentials 安装 Istio 及 Monitoring addons 首先使用下面的命令查看 TIS 支持的 Istio 版本：\nhelm search repo tetratelabs/base --versions NAME CHART VERSION APP VERSION DESCRIPTION tetratelabs/base\t1.20.1+tetrate0\t1.20.1-tetrate0\tHelm chart for deploying Istio cluster resource... tetratelabs/base\t1.20.0+tetrate0\t1.20.0-tetrate0\tHelm chart for deploying Istio cluster resource... tetratelabs/base\t1.19.5+tetrate0\t1.19.5-tetrate0\tHelm chart for deploying Istio cluster resource... tetratelabs/base\t1.19.4+tetrate0\t1.19.4-tetrate0\tHelm chart for deploying Istio cluster resource... tetratelabs/base\t1.19.3+tetrate0\t1.19.3-tetrate0\tHelm chart for deploying Istio cluster resource... tetratelabs/base\t1.18.6+tetrate0\t1.18.6-tetrate0\tHelm chart for deploying Istio cluster resource... tetratelabs/base\t1.18.5+tetrate0\t1.18.5-tetrate0\tHelm chart for deploying Istio cluster resource... tetratelabs/base\t1.18.3+tetrate0\t1.18.3-tetrate0\tHelm chart for deploying Istio cluster resource... tetratelabs/base\t1.17.8+tetrate0\t1.17.8-tetrate0\tHelm chart for deploying Istio cluster resource... tetratelabs/base\t1.17.6+tetrate0\t1.17.6-tetrate0\tHelm chart for deploying Istio cluster resource... tetratelabs/base\t1.16.7+tetrate0\t1.16.7-tetrate0\tHelm chart for deploying Istio cluster resource... tetratelabs/base\t1.16.6+tetrate0\t1.16.6-tetrate0\tHelm chart for deploying Istio cluster resource... 我们将安装当前最新的 Istio 1.20.1 版本。\nexport TIS_USER=\u0026lt;tis_username\u0026gt; export TIS_PASS=\u0026lt;tis-password\u0026gt; # Helm chart version export VERSION=1.20.1+tetrate0 # Image tag export TAG=1.20.1-tetrate0 kubectl create namespace istio-system kubectl create secret docker-registry tetrate-tis-creds \\ --docker-server=\u0026#34;addon-containers.istio.tetratelabs.com\u0026#34; \\ --docker-username=${TIS_USER} \\ --docker-password=${TIS_PASS} \\ -n istio-system # Install Istio helm install istio-base tetratelabs/base -n istio-system \\ --set global.tag=${TAG} \\ --set global.hub=\u0026#34;addon-containers.istio.tetratelabs.com\u0026#34; \\ --set \u0026#34;global.imagePullSecrets[0]=tetrate-tis-creds\u0026#34; \\ --version ${VERSION} helm install istiod tetratelabs/istiod -n istio-system \\ --set global.tag=${TAG} \\ --set global.hub=\u0026#34;addon-containers.istio.tetratelabs.com\u0026#34; \\ --set \u0026#34;global.imagePullSecrets[0]=tetrate-tis-creds\u0026#34; \\ --version ${VERSION} \\ --wait # install ingress Gateway kubectl create namespace istio-ingress kubectl create secret docker-registry tetrate-tis-creds \\ --docker-server=\u0026#34;addon-containers.istio.tetratelabs.com\u0026#34; \\ --docker-username=${TIS_USER} \\ --docker-password=${TIS_PASS} \\ -n istio-ingress helm install istio-ingress tetratelabs/istio-ingress -n istio-ingress \\ --set global.tag=${TAG} \\ --set global.hub=\u0026#34;addon-containers.istio.tetratelabs.com\u0026#34; \\ --set \u0026#34;global.imagePullSecrets[0]=tetrate-tis-creds\u0026#34; \\ --version ${VERSION} \\ --wait # Install TIS addon helm install istio-monitoring-demo tis-addons/istio-monitoring-demo --namespace tis --create-namespace 端口转发 Grafana 服务，然后在本地浏览器中打开 Grafana：http://localhost:3000\nkubectl port-forward --namespace tis svc/grafana 3000:3000 注意：请保持该命令持续运行，因为我们在向 Grafana 导入 dashboard 时还需要访问该端口。\n安装 Istio Monitoring Addons 使用默认的用户名密码 admin/admin 登录后，在左侧导航栏中选择 Administration - Service accounts，参考 Grafana 文档上的说明创建一个 admin 权限的 Service account。\n在 Grafana 中创建一个 Service account 记下这个 Token，我们将在下面的操作中用到。\n使用 Terraform 向 Grafana 中导入 dashboard：\ncat\u0026gt;~/.terraformrc\u0026lt;\u0026lt;EOF credentials \u0026#34;terraform.cloudsmith.io\u0026#34; { token = \u0026#34;tetrate/tis-containers/kuhb8CPZhaOiR3v6\u0026#34; } EOF # Create a terraform module file cat\u0026gt;istio-monitoring-grafana.tf\u0026lt;\u0026lt;EOF module \u0026#34;istio_monitoring_grafana\u0026#34; { source = \u0026#34;terraform.cloudsmith.io/tis-containers/istio-monitoring-grafana/tetrate\u0026#34; version = \u0026#34;v0.2.0\u0026#34; gf_url = \u0026#34;\u0026lt;http://localhost:3000\u0026gt;\u0026#34; gf_auth = \u0026#34;\u0026lt;grafana_token\u0026gt;\u0026#34; } EOF # Run the commands terraform init terraform plan terraform apply -auto-approve 恭喜你现在已经成功的向 Grafana 中导入了以下四个 dashboard：\nTIS Workload Dashboard TIS Service Dashboard TIS Wasm Extension Dashboard TIS Control Plan Dashboard 但是现在有些 dashboard 还没有数据，我们需要在网格中制造一些流量。\n测试监控 部署 Bookinfo 应用和入口网关：\nkubectl create secret docker-registry tetrate-tis-creds \\ --docker-server=\u0026#34;addon-containers.istio.tetratelabs.com\u0026#34; \\ --docker-username=${TIS_USER} \\ --docker-password=${TIS_PASS} \\ -n default kubectl label namespace default istio-injection=enabled kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml -n default kubectl apply -f …","relpermalink":"/blog/enhancing-istio-with-tis-comprehensive-installation-and-monitoring-guide/","summary":"在这篇博客中我将指导你如何安装 TIS 并启用监控插件。","title":"使用 TIS 增强 Istio：安装与监控指南"},{"content":"前言 我经常在 Istio 的 GitHub Discussions 上回答网友的问题，最近我遇到了一个关于 Istio 主 - 远程部署的讨论 ，问题是关于远程集群中网关如何最初验证到外部 Istiod 实例的。这个问题触及到服务网格在多集群配置中的核心安全机制，我认为这值得在社区中进行更深入的分享。\n正文 在 Istio 官方的不同网络上安装 Primary-Remote 文档中，有一个步骤是将 cluster2 作为 cluster1 的远程集群进行附加 。这个过程中会创建一个包含 kubeconfig 配置的 Secret，这个配置文件中含有访问远程集群（cluster2）所需的证书和令牌。\n# 这个文件是自动生成的，请不要编辑。 apiVersion: v1 kind: Secret metadata: annotations: networking.istio.io/cluster: cluster2 creationTimestamp: null labels: istio/multiCluster: \u0026#34;true\u0026#34; name: istio-remote-secret-cluster2 namespace: istio-system stringData: cluster2: | apiVersion: v1 clusters: - cluster: certificate-authority-data: {CERTIFICATE} server: {CLUSTER2-APISERVER-ADDRESS} name: cluster2 contexts: - context: cluster: cluster2 user: cluster2 name: cluster2 current-context: cluster2 kind: Config preferences: {} users: - name: cluster2 user: token: {TOKEN} 这个 Secret 的关键作用是让主集群（cluster1）中的 Istio 能够访问远程集群的 API 服务器，从而获得服务信息。此外，远程集群（cluster2）中的 Istiod 服务，通过端点指向主集群中 Istiod 服务的 LoadBalancer IP（端口 15012 和 15017），使得 cluster2 能够通过 Istiod 与主集群进行通信。\n因为这两个集群共用一个 CA（由主集群提供），并且远程集群能够访问自己的 API 服务器，所以主集群中的 Istiod 可以验证来自远程集群（cluster2）的请求。下面的序列图清晰地展示了这一过程：\nsequenceDiagram participant IG as Ingress Gateway（远程集群） participant K8s as Kubernetes API（远程集群） participant SA as Service Account（远程集群） participant Istiod as Istiod（主集群） Note over IG: 启动 IG-\u0026gt;\u0026gt;K8s: 请求Service Account令牌 K8s-\u0026gt;\u0026gt;SA: 创建/检索令牌 SA--\u0026gt;\u0026gt;IG: 返回令牌 Note over IG: 令牌挂载在Pod中 IG-\u0026gt;\u0026gt;Istiod: 使用令牌进行身份验证 Note over Istiod: 验证令牌 Istiod-\u0026gt;\u0026gt;Istiod: 生成mTLS证书 Istiod--\u0026gt;\u0026gt;IG: 发送mTLS证书 Note over IG: 使用mTLS证书进行Mesh内的安全通信 这一过程是 Istio 多集群配置中的关键一环，确保了服务网格中跨集群通信的安全性。正如我们在这次讨论中看到的，无论是远程网关还是服务都依赖于主集群的 CA 来进行初始的 mTLS 认证，这为整个服务网格的安全通信提供了坚实的基础。\n总结 在本篇博客中，我们探讨了在 Istio 主 - 远程部署中，远程集群的网关如何进行初始验证以连接到外部的 Istiod。我们解释了如何通过创建一个含有 kubeconfig 的 Secret 来允许主集群的 Istio 访问远程集群的 API，以及如何通过共享的 CA 和服务账户令牌来确保 mTLS 认证的安全性。这一过程确保了服务网格中的跨集群通信的安全，为理解和实施 Istio 的多集群配置提供了重要的见解。\n","relpermalink":"/blog/primary-remote-istio-ingress-gateway-mtls/","summary":"这篇博客为你深度解析 Istio 主 - 远程部署模式下的远程网关的初始身份验证及 mTLS 连接流程。","title":"Istio 多集群身份验证与 mTLS 连接解析"},{"content":"欢迎阅读本周的 Istio 社区周报！随着年末的临近，我们很高兴与 Istio 社区的成员分享一些更新和见解。从月度社区会议到如何有效使用 Istio 的专业技巧，本周报中包含了对每个人都有价值的信息，无论您是经验丰富的 Istio 用户还是刚刚入门。\n社区更新 加入月度 Istio 社区会议 成为 Istio 月度社区会议的一部分，与其他热爱 Istio 的人士互动和交流。\n时间：每月第四个星期四，美国太平洋时间上午 10 点。查看您当地的时间 。 保持联系 日历更新：通过加入此群组 ，自动将会议添加到您的日历中。 会议详情：在我们的工作文档 中查找议程和记录。 录制会话：错过了会议？可以在YouTube 上追赶。 在会议上做演讲 分享您的见解！将您的演讲摘要提交至 istio-steering@googlegroups.com 。有关演示指南和更多详细信息，请参阅这里 和这里 。\nIstio 专业技巧 在 VirtualService 路由中使用正则表达式 在 VirtualService 中使用基于标头的路由时，使用 Envoy 的RE2 正则表达式 格式至关重要。例如：\n匹配 “Google”：regex: \u0026#34;.\\*Google.\\*\u0026#34; 匹配 “Microsoft”：regex: \u0026#34;.\\*Microsoft.\\*\u0026#34; 这个 .\\*\u0026lt;字符串\u0026gt;.\\* 模式对于在 Envoy 中正确匹配非常重要。请记住，PCRE 格式与 Istio 不兼容。\n关键点：使用与 Envoy 兼容的 RE2 正则表达式进行精确路由。详细讨论内容可在GitHub 上找到。 无停机迁移到相互 TLS 以下图表说明了零停机迁移过程，以实现相互 TLS，并展示了远程 IngressGateway 和 Istiod 在主要 - 远程多集群部署中建立 mTLS 的过程。\nTLS 迁移过程：\n零停机 TLS 迁移过程 mTLS 建立的顺序图：\nsequenceDiagram participant IG as Ingress Gateway (Remote Cluster) participant K8s as Kubernetes API (Remote Cluster) participant SA as Service Account (Remote Cluster) participant Istiod as Istiod (Primary Cluster) Note over IG: 启动 IG-\u0026gt;\u0026gt;K8s: 请求服务账号令牌 K8s-\u0026gt;\u0026gt;SA: 创建/检索令牌 SA--\u0026gt;\u0026gt;IG: 返回令牌 Note over IG: 令牌挂载在 Pod 中 IG-\u0026gt;\u0026gt;Istiod: 使用令牌进行身份验证 Note over Istiod: 验证令牌 Istiod-\u0026gt;\u0026gt;Istiod: 生成 mTLS 证书 Istiod--\u0026gt;\u0026gt;IG: 发送 mTLS 证书 Note over IG: 使用 mTLS 证书进行网格内的安全通信 欲了解更多信息，请访问GitHub 讨论 。\n随着节日的临近，我们祝愿 Istio 社区的所有成员圣诞快乐！愿这个季节带来喜悦、和平以及创新和合作的新机会。节日快乐！\n","relpermalink":"/blog/istio-community-weekly-02/","summary":"Istio 社区周报第 2 期（2023.12.18 - 2023.12.24）","title":"Istio 社区周报（第 2 期）"},{"content":"欢迎来到 Istio 社区周报 Istio 社区朋友们，你们好！\n我很高兴呈现第一期 Istio 社区周报。作为 Istio 社区的一员，每周我将为您带来 Istio 的最新发展、有见地的社区讨论、专业提示和重要安全新闻内容。\n祝你阅读愉快，并在下一期中与您再见！\n社区更新 切换到 GitHub Discussions 进行 Istio 社区问答 Istio 团队宣布了社区互动和问答方式的重大变化。当前的论坛 discuss.istio.io 将于 12 月 20 日前归档，团队将转向 GitHub Discussions 进行未来的互动。这个新平台被设想为用户提问和与社区互动的中心。值得注意的是，在 GitHub 讨论中的贡献将被视为官方的 Istio 贡献，影响着 steering contributor 席位的分配。这是对维护者、供应商和最终用户积极参与这些讨论的号召。\nIstio Office Hours 提案 Istio 社区引入了一个新的倡议：“Istio Office Hours”。该提案旨在建立一个定期的社区会议，分享技术知识并促进社区增长。Office Hours 旨在帮助新的贡献者，并为当前成员提供分享见解的论坛。\n请参阅提案文件 以获取更详细的信息，并参与塑造这个倡议。\nKubernetes v1.29 发布：从 iptables 切换到 nftables 及其对 Istio 的影响 最近发布的 Kubernetes v1.29 引入了其网络方法的重大变化。Kubernetes 现在将 nftables 作为 kube-proxy 后端的 Alpha 特性，取代了 iptables。这一变化是由于 iptables 存在已久的性能问题，随着规则集大小的增加而恶化。内核中 iptables 的开发已经显著减缓，大部分活动已停止，新功能也已停滞不前。\n为什么选择 nftables？ nftables 解决了 iptables 的限制，并提供更好的性能。 像 RedHat 和 Debian 这样的主要发行版正在摆脱 iptables。RedHat 在 RHEL 9 中已弃用 iptables，而 Debian 在 Debian 11（Bullseye）中将其从必需的软件包中删除。 对 Istio 的影响 Istio 目前依赖 iptables 来进行流量劫持，可能需要考虑使用 nftables 来适应这一变化。这个变化与 Linux 生态系统中更广泛的向 nftables 迁移一致。\n管理员的作用 要启用此功能，管理员必须使用 NFTablesProxyMode 特性门和运行 kube-proxy 时使用 -proxy-mode=nftables 标志。 可能会存在兼容性问题，因为目前依赖 iptables 的 CNI 插件、NetworkPolicy 实现和其他组件需要更新以适应 nftables。 Kubernetes v1.29 中的这一变化是一个前瞻性的步骤，但需要谨慎考虑和规划，以与像 Istio 这样的系统集成。Istio 社区需要密切关注这些发展，并为未来的 Istio 版本可能需要进行的调整做好准备，以保持与 Kubernetes 的兼容性。\n项目更新 ISTIO-SECURITY-2023-005：Istio CNI RBAC 权限的更改 解决的安全漏洞 Istio 安全委员会已经确定并解决了与 Istio CNI（容器网络接口）有关的一个重大安全漏洞。这个问题在 ISTIO-SECURITY-2023-005 中详细说明，涉及到 istio-cni-repair-roleClusterRole 的潜在滥用。\n安全风险 如果节点被入侵，Istio CNI 的现有高级权限可能会被利用。这种滥用可能会将本地入侵升级为集群范围的安全漏洞。\n采取的措施 针对这一发现，已对 Istio CNI 的 RBAC（基于角色的访问控制）权限进行了修改，以减轻这一风险。\n有关更详细信息，请访问官方安全咨询 Istio Security 2023-005 。\n新的小版本发布 Istio 发布了新的小版本以增强安全性和功能性：\nIstio 1.18.6 Istio 1.19.5 Istio 1.20.1 这些更新包括各种改进和修复，反映了对维护和增强 Istio 服务网格安全性和性能的持续承诺。\n有关这些发布的更多详细信息，请访问 Istio 最新消息 。\n热门话题讨论 关于 Istio 1.21 中环境模式转入 Beta 的讨论 在最近的联合工作组会议上，关于环境模式在即将发布的 Istio 1.21 版本（Q124）中转入 Beta 的讨论引发了激烈的讨论。\n分歧的观点 Solo 团队的立场：主张延迟 1.21 版本的发布，以确保环境模式在该版本中达到 Beta 状态。 其他所有人的观点：反对延迟版本发布，强调环境模式尚未准备好发布 Beta 版。 当前展望 共识倾向于在不包括环境模式的 Beta 版本中维持 1.21 版本的发布计划。主要观点是，环境模式需要进一步开发，不太可能在 1.21 版本中达到 Beta 状态。\n讨论反映了 Istio 开发过程中对质量和准备性的持续承诺。不急于将环境模式推向 Beta 版本的决定强调了社区在每个发布中确保稳定性和可靠性的奉献精神。\nIstio 专业提示 确定 Ingress Gateway 的上游 IP 地址 在处理 Ingress 网关之前，特别是对于 TLS 卸载，确定上游源工作负载是至关重要的。在 istio_requests_total 中，如果 source_workload 是一个 Ingress 网关，就需要识别真正的源工作负载。一个实用的方法是利用 HTTP 头来实现这一目的。XFF（X-Forwarded-For）是 Istio 文档 中概述的标准方法，但也可以通过虚拟服务添加头部来实现自定义解决方案。\nIstio 发行版中的 Envoy 版本 Istio 团队维护他们的 Envoy 构建，独立决定补丁版本。这种方法确保更快地获得必要的更新，而不必等待官方的 Envoy 发布，因此导致了 Istio 中使用的 Envoy 版本与 Envoy 的最新版本不一致。更多详细信息可以在这个 GitHub 讨论 中找到。\n总结 当我们结束本周的 Istio Community Weekly，让我们回顾一下我们所分享的信息。我们看到了 Istio 社区的活力和创新，以及与服务网格技术相关的最新趋势和讨论。\n本周，我们了解到 Istio 社区将转向 GitHub Discussions 作为主要的问答和交流平台，这标志着更加开放和协作的未来。同时，“Istio Office Hours” 倡议将帮助社区成员共享知识，促进成长。\n在技术方面，Kubernetes v1.29 的变化将对 Istio 和整个生态系统产生影响，我们需要密切关注和适应这些变化。此外，我们还了解到 Istio CNI RBAC 权限的改变，以及新的 Istio 小版本发布，旨在提高安全性和性能。\n最后，我们深入讨论了 Istio 1.21 版本中环境模式转入 Beta 的问题，以及如何确定 Ingress Gateway 的上游 IP 地址的技巧。\n作为 Istio 社区的一员，您的反馈和参与至关重要。让我们继续在 GitHub 上分享见解、问题和意见，共同塑造 Istio 的未来。展望下周，我们期待更多精彩的更新和社区互动。敬请关注，下期再见！\n","relpermalink":"/blog/istio-community-weekly-01/","summary":"这是 Istio 社区周报第一期，汇总 Istio 社区最近一周的新闻和动态。本期覆盖的时间范围为 2023 年 12 月 11 日到 17 日。","title":"Istio 社区周报（第 1 期）"},{"content":"在上一篇博客中，我向大家介绍了我开发的Istio Advisor Plus GPT 。但是，要使用这个工具，需要满足一个前提条件，即您必须已经订阅了 ChatGPT Plus。对于很多中国用户来说，注册 ChatGPT 账户可能已经费了不少力气，但是在订阅 ChatGPT Plus 时可能会遇到一些困难。在这篇文章中，我将指导您如何通过正规渠道、无需使用国外信用卡，也无需虚拟信用卡来成功订阅和续费 ChatGPT Plus。\nPockytShop 中的礼品卡 答案其实很简单，首先您需要拥有一个美国地区的 Apple ID。然后，您可以前往 https://shop.pockyt.io/ 这个网站购买App Store \u0026amp; iTunes礼品卡，您可以选择礼品卡的金额，范围从 2 美元到 500 美元不等。接下来，您可以使用支付宝进行支付。您也可以直接在您自己的支付宝中将地区设置为美国，然后搜索礼品卡，即可找到购买链接。我为了方便起见，选择在网站上直接购买。购买完成后，您将收到礼品卡代码，登录您的美国地区的 Apple Store 账户后，即可兑换礼品卡。\n通过这种方式，您可以轻松订阅 ChatGPT Plus，无需繁琐的操作，同时也避免了使用国外信用卡或虚拟信用卡的麻烦。这是一个简单而有效的方法，让更多人能够享受 ChatGPT Plus 的服务。\n","relpermalink":"/blog/how-to-subscribe-chatpgt-plus-in-china/","summary":"了解如何无需国外信用卡，使用美区 Apple ID 和礼品卡订阅 ChatGPT Plus，轻松获得 Istio Advisor Plus GPT。","title":"如何订阅 ChatGPT Plus：只需美区 ID 无需虚拟信用卡"},{"content":"大家好，今天我要向你们隆重介绍一位特别的“老中医” —— Istio Advisor Plus GPT （以下简称“Istio 老中医”）。不要误会，他不是给人看病的，而是专治您在 Istio 这个“网络世界”中遇到的各种疑难杂症！\nIstio 老中医的崭新登场 “老中医”来自于 OpenAI 最近推出的一项革命性技术 —— GPT。这个技术简直是黑科技，能让你不用写一行代码就能创建自定义的 GPT 模型。想象一下，只需几次点击，就能得到专门解答 Istio 问题的智能助手！而且，不久后 GPT Store 就要上线，届时你将能看到更多像“老中医”这样的高手。\n我准备了一个视频，展示“老中医”在实战中的表现。例如拿一个 Istio 的 GitHub Issue 问他，看看他会给你开什么样的处方？\n你可以在 Github 获取导出 GitHub Issue 的代码。\nIstio 老中医的神奇力量 解释 Istio 概念和特性：从流量管理到安全性，再到可观测性，无所不知，无所不能。\n指导配置：无论您需要怎样的配置建议，“老中医”总有办法。\n诊断和解决问题：遇到棘手的问题？不用怕，“老中医”在此。\n性能优化建议：让您的 Istio 运行得更加流畅，更加高效。\n安全最佳实践：零信任安全原则在此不再是难题。\n可视化表示：用图表展示复杂的网络，让一切变得易懂。\n协助升级：升级 Istio 就像喝水一样简单。\nBug 报告指导：遇到 bug？“老中医”帮您梳理、上报。\n参考相关文档：针对复杂问题，提供深度资料。\n生态系统工具建议：让您在 Istio 生态中如鱼得水。\n“老中医”的丰富知识库 Istio 基础：基础知识一应俱全。\nEnvoy 代理：深入探索 Envoy 的奥秘。\n服务网格概念：掌握服务网格的本质。\nTetrate 文档：了解 Tetrate 的一切。\n零信任安全：安全问题，轻松解决。\nSkyWalking 集成：深入了解 APM 系统。\nEnvoy 网关：流量管理和安全性的高手。\n用例和案例研究：实际应用，一目了然。\n“老中医”的智慧回应 TLDR：简洁明了，一目了然。\n查询的解释：提供广泛的上下文。\n详细答案：深入浅出，条理清晰。\n实际示例：实操演示，直观易懂。\n建议的下一步操作：明确指引，方便操作。\n参考资料：海量资源，随手可得。\n接受“老中医”的局限性 虽然“老中医”很厉害，但也有局限。比如，要使用它，您需要有 ChatGPT Plus 订阅。而且，当回答太长时，可能需要分多次提问。还有，目前可能存在一些隐私问题，所以建议暂时只用公开数据。\n总结 综上所述，Istio 老中医是您在 Istio 世界旅行的必备良伴。无论是丰富的知识库、定制的回应，还是实际的指导，它都能帮您轻松驾驭 Istio 的复杂世界。所以，一起来尝试一下吧，和 Istio 老中医一起探索无限可能！\n","relpermalink":"/blog/introducing-istio-advisor-plus-gpt/","summary":"探索 Istio Advisor Plus GPT，即“Istio 老中医”，这款基于 ChatGPT 的工具能提供 Istio 相关问题的专业解答和优化建议，助力技术挑战。","title":"Istio 老中医，在线问诊，专治各种疑难杂症"},{"content":"随着 Kubernetes 不断演进，Istio 功能逐渐在 Kubernetes 中找到对应实现，如 Sidecar 容器 、Gateway API 以及本文的主题 ExternalName 。ExternalName 和 ServiceEntry 都能起到引入 Kubernetes 集群外部服务的作用，但是它们的功能和使用场景也有所区别，本文将为你详细解析。\nExternalName vs ServiceEntry 下表从多个方面对比了 ExternalName 和 ServiceEntry ：\n特性/用例 ExternalName ServiceEntry 流量控制 有限，仅支持 TCP 和 UDP 更灵活，支持 TCP、UDP、HTTP 等多种协议，可以指定端口、TLS 等选项 服务发现 适用于外部服务的简单别名 适用于描述网格内外服务，包括外部和内部服务的详细配置 配置复杂性 简单，适用于基本的服务发现需求 较复杂，适用于需要高级流量控制和详细配置的场景 TLS 支持 有限，较简单 更丰富的 TLS 支持，可以指定证书等详细选项 安全性 较基本，适用于简单的用例 更强大的安全性支持，可以定义 subjectAltNames 等选项 用途 适用于简单的外部服务别名 适用于复杂的流量管理和服务发现需求，尤其是在多协议和复杂网络拓扑中 使用场景 ExternalName 的使用情况：\n简单的服务别名： 外部服务只需一个简单别名，无需复杂流量控制，可选用 ExternalName。 无详细流量控制需求： 不需要对服务流量进行详细控制，只需简单的服务别名访问，选用 ExternalName。 ServiceEntry 的使用情况：\n复杂流量控制需求： 需要更复杂的流量控制，如指定协议、端口、TLS 选项等，选择 ServiceEntry。 描述网格内外服务： 需要描述网格内外服务，包括外部和内部服务的详细配置，ServiceEntry 更适合。 对服务详细属性有要求： 需要为服务定义特殊属性，如 subjectAltNames 等，需使用 ServiceEntry。 在 Istio 中使用 ExternalName 可能遇到的问题 在 Istio 1.20 以前，网格内存在 ExternalName 类型的 Service 时，若该 Service 的端口与其他外部服务的端口重叠，流量可能错误路由到该 ExternalName Service。该问题已在 Istio 1.20 版本中解决，详见 Better support ExternalName #37331 。\n总结 在服务网格的选择中，ExternalName 和 ServiceEntry 分别提供了简单的服务别名和更复杂的流量管理与服务发现选项。ExternalName 适用于简单的外部服务别名，而 ServiceEntry 在处理复杂流量控制和网格内外服务时更具优势。在实际应用中，根据具体需求和配置的复杂性权衡，灵活选择合适的机制。随着 Istio 和 Kubernetes 的不断演进，这些功能的使用方式可能会受到影响，因此保持关注相关社区的更新和最佳实践是保持系统健康和高效运行的关键。选择合适的服务网格组件将有助于构建可靠、安全且高度可扩展的微服务架构。\n","relpermalink":"/blog/externalname-and-serviceentry/","summary":"了解 ExternalName 和 ServiceEntry 的优劣，根据需求选用。ExternalName 简单，适用于基本服务发现；ServiceEntry 复杂，适合复杂流量管理和服务发现。","title":"外部服务别名：ExternalName 与 ServiceEntry 对比"},{"content":"Istio 1.20 代表了 Istio 服务网格能力的显著进步，为运维人员和开发人员提供了更好的体验。这个新版本引入了一些关键的功能和更新，将影响到服务网格架构的设计和实施。\nGateway API 支持 Istio 1.20 全面支持 Kubernetes Gateway API，并已正式发布（GA）。这标志着服务网格生态系统的重大进步，为用户提供了一组稳定且丰富的网络 API，与 Kubernetes 的核心服务相一致。Istio 对 Gateway API 的支持是实现更无缝和灵活的流量管理的重要一步，使用户能够利用一致的声明方式定义在 Kubernetes 集群内如何路由流量。如果你想了解更多关于 Gateway API 的信息，可以阅读我的博客 Istio 1.19 有哪些更新：Gateway API 还有更多 。\n增强的 ExternalName 服务支持 在服务发现领域，Istio 1.20 对于ExternalName服务的处理进行了重要更新（见 Better support ExternalName #37331 ），使得 Istio 的行为更加符合 Kubernetes 的行为。这个变化简化了配置，并使得 Istio 能够更好地处理 DNS，对于依赖于外部终点的服务至关重要。关于 ExternalName 服务的更多信息，你可以参考 Kubernetes 官方文档 。\nExternalName 和 Istio 中的 ServiceEntry 都可以用于处理服务发现，特别是引入 Kubernetes 集群之外的服务，但有一些关键区别：\nExternalName 是 Kubernetes 的原生 Service 类型，相当于给集群外部服务这设置了一个别名，使得外部服务在 Kubernetes 内部的表现与原生 Service 保持一致，从而可以统一管理和使用内部和外部服务。你可以先定义 ExternalName 类型的服务，如果后来你决定将服务移到集群中，则可以启动其 Pod，添加适当的选择算符或端点并更改服务的类型。使用时需要注意不要在多个命名空间中使用相同的 ExternalName，可能会引起命名冲突或混淆。 ServiceEntry 是 Istio 特有的配置对象，它提供了更灵活的控制，可以描述网格内或网格外的服务，以及指定特定的协议、端口等属性。例如，可以使用ServiceEntry将网格内服务访问网格外的服务，或者定义自定义的服务入口点。 其他更新 一致的 Envoy 过滤器排序： 在新版本中，Envoy 过滤器的排序在所有流量方向和协议上变得一致了。这确保了过滤器的统一应用，对于服务网格的可预测行为和安全性至关重要。\n网络 Wasm 插件扩展： Istio 继续通过引入新的NETWORK类型扩展网络 Wasm 插件的支持，推动了可扩展性的边界。这个扩展巩固了 Istio 作为服务网格创新领域的领导地位，为用户提供了更多的控制和定制选项。\nTCP 元数据交换增强： Istio 1.20 中的两个更新旨在改进 TCP 元数据交换：回退元数据发现过程和控制 ALPN 令牌的能力。这些改进显示了 Istio 对强大高效的网络的承诺。\n流量镜像到多个目的地： 新版本扩展了 Istio 的流量镜像功能以支持多个目的地。这个功能对于调试和监控非常宝贵，可以提供关于跨不同服务版本或配置的流量行为的见解。\n可插拔的根证书轮换： 加强了安全性，Istio 现在支持可插拔的根证书轮换，增强了服务网格在使用更新的加密凭证时保持服务间信任的能力。\nSidecar 容器中的 StartupProbe: 为了改善启动时间，Istio 在 Sidecar 容器中引入了startupProbe，它可以在初始阶段进行积极的轮询，而不会在整个 Pod 的生命周期中持续存在。\nOpenShift 安装增强： 通过去除某些特权要求，Istio 简化了在 OpenShift 上的安装过程，从而降低了 OpenShift 用户的使用门槛。\n总结 在 Istio 1.20 中的这些功能和增强将简化运维操作，加强安全性，并提供更具动态和可定制的服务网格体验。随着服务网格领域的不断发展，Istio 的最新版本证明了社区对改进和创新的不懈追求。\n","relpermalink":"/blog/istio-120-release/","summary":"Istio 1.20 带来关键更新：全面支持 Gateway API、ExternalName 优化、Envoy 过滤器统一排序等，提升服务网格灵活性和性能。","title":"Istio 1.20 有哪些更新？"},{"content":"上周 Kubernetes Gateway API 的正式发布公告 标志着 Kubernetes 生态系统内 Gateway 能力的重要里程碑。与此同时，Kubernetes 社区一致认同Backstage 是内部开发平台和门户的领先解决方案。Kubernetes Gateway API 和 Backstage 都从一开始就鼓励社区的可扩展性。可以说 API Gateway 的出现为增强 Kubernetes 网络提供了巨大的机会。\nGateway API vs Istio 服务网格 不过也有人对 Gateway API 与 Istio 服务网格的关系存在疑问。对于 Gateway API 和 Istio 服务网格，两者都是为了解决 Kubernetes 网络中的问题。然而，Gateway API 着重于提供一种标准化和简化的方式来配置和部署 Ingress 和 Egress，是一个更加通用的 API。另一方面，Istio 服务网格更关注于服务到服务的通信，提供丰富的流量管理，安全，策略和遥测功能。\nKubernetes Gateway API 的未来 Kubernetes Gateway API 代表了 API Gateway 的关键基础，引入了一种标准，基于角色的，高度适应性的方法来配置和部署 Gateway。Kubernetes Gateway API 相比现有的 Kubernetes Ingress 的显著改进之一是其基于角色的 API 结构。这使得基础设施，平台和应用程序领域的各种角色能够拥有直接与他们的用例相关的 API 的各个方面。Gateway API 的另一个关键特性是其针对可扩展性的设计 - API 专注于核心 Gateway 和路由用例，具有扩展附加能力的可能性，例如安全性，速率限制和转换。\n什么是 Backstage？ Backstage 是一个开源的开发者平台，它集成了所有开发者需要的服务，提供了一个统一的视图。这包括版本控制系统、持续集成/持续部署（CI/CD）系统、监控、日志、警报和文档。它旨在让开发者更高效地进行日常任务，而无需在多个工具之间切换，它也可以帮助开发者更好地理解和管理他们的软件。\nBackstage UI Backstage 可以应用在多种使用场景中：\n作为服务目录：Backstage 的软件目录功能可以帮助开发者找到并了解公司内部的所有服务和应用。 作为自动化工具：Backstage 的软件模板可以自动化 API 上线流程，使得开发者能够更快速、更安全地部署他们的 API。 提供中心化的 API 文档：Backstage 的 Tech Docs 功能可以提供中心化的 API 文档，使得开发者能够在一个地方查找所有的 API 文档，而无需在多个工具间切换。 作为开发者门户：Backstage 可以集成多种开发工具，提供一站式的开发者服务，简化开发者的工作流程。 Backstage 通过其软件目录 用于发现 API，软件模板 用于提供带有防护栏的自动 API 上线流程，以及Tech Docs 用于提供 API 文档的中心用例，用于围绕 API Gateway 的协作。\nBackstage 的目标是简化开发者工作流程，提供一站式的解决方案，它使开发者能够在一个平台上查找他们需要的所有信息，而不是在多个工具间切换。此外，Backstage 可以让开发团队专注于编码，而不是管理工具。它还支持多种插件，可以根据团队的需求进行定制。\n关于未来 Backstage 和 Kubernetes Gateway API 已经牢固地将自己建立为云原生 API Gateway 的基础支柱，两个项目都在各自的路线图中充满创新。其中最有趣的领域是 Kubernetes Gateway API 超越其传统的南北入口能力，包括东西服务至服务通信，通过引入GAMMA API 。在真实的流量在每个方向上流动的情况下，为南北和东西流量提供单一基础将有助于提高任何容器化应用的安全性，弹性和可观察性。\n","relpermalink":"/blog/kubernetes-gateway-api-enhancement-cloud-native-network/","summary":"Kubernetes Gateway API 和 Backstage 的结合标志着 Kubernetes 生态系统内 Gateway 能力的重要里程碑。","title":"Kubernetes Gateway API 如何增强云原生网络"},{"content":"我很高兴呈现 Istio 的最新版本— Istio 1.19 。这篇博客将概述此版本中的更新内容。\nGateway API: 彻底改变服务网格 我们的前一篇博客 强调了 Gateway API 将 Kubernetes 和服务网格中的入口网关统一的潜力，为跨命名空间的流量支持打开了大门。有了 Istio 的官方支持，Gateway API 成为了焦点。它不仅用于北 - 南流量（网格的入口和出口），现在还扩展到东 - 西流量的领域，也就是网格内部的流量。\n在 Kubernetes 中，服务承担多项职责，从服务发现和 DNS 到工作负载选择、路由和负载均衡。然而，对这些功能的控制一直有限，工作负载选择是显著的例外。Gateway API 改变了这一局面，让你可以控制服务路由。这与 Istio 的 VirtualService 有一些重叠，因为两者都对流量路由有影响。以下是三种情况的简介：\nKubernetes 内部请求：在没有 Istio 的情况下，Kubernetes 的所有内部流量都走服务路由。 北 - 南流量：通过将 Gateway API 应用到入口网关，流入 Kubernetes 的流量会按照 xRoute（目前支持 HTTPRoute, TCPRoute 和 gRPCRoute）到服务。 东 - 西流量：在 Istio 内部，当流量进入数据平面时，Gateway API 的 xRoute 接管。它引导流量到原始服务或新的目标服务。 图 1：流量路由 Gateway API 与 Istio 的这种动态结合不仅精细化了服务网络，也巩固了 Istio 在 Kubernetes 生态系统中的地位。\n服务网格的 Gateway API: 深入探讨 在当前的实验阶段（v0.8.0 版本），服务网格的 Gateway API 引入了一种新的方法来配置 Kubernetes 中的服务网格支持。它直接将单个路由资源（如 HTTPRoute）与服务资源关联起来，简化了配置过程。\n以下是一些关键点：\n实验阶段：在 v0.8.0 版本中，服务网格的 Gateway API 仍处于实验阶段。建议不要在生产环境中使用。\n服务与路由关联：在配置服务网格时，与使用 Gateway 和 GatewayClass 资源不同，单个路由资源直接与服务资源关联。\n服务的前端和后端：服务的前端包括其名称和集群 IP，后端由其端点 IP 的集合组成。这种区分使得在网格内进行路由无需引入冗余资源。\n路由附加到服务：将路由附加到服务上以将配置应用到指向该服务的任何流量。如果没有附加路由，流量会遵循网格的默认行为。\n命名空间关系：\n相同命名空间： 在与其服务相同的命名空间中的路由，被称为生产者路由，通常由工作负载创建者创建以定义可接受的使用情况。它影响来自任何命名空间中的任何工作负载的客户端的所有请求。 不同命名空间： 在与其服务不同的命名空间中的路由，被称为消费者路由，细化了给定工作负载的消费者提出请求的方式。这个路由只影响与路由在同一命名空间中的工作负载的请求。 图 2：生产者路由和消费者路由 组合路由：在单个命名空间中的同一服务的多个路由，无论是生产者路由还是消费者路由，都将根据 Gateway API 路由合并规则进行合并。这意味着在同一命名空间中为多个消费者定义不同的消费者路由是不可能的。\n请求流程：\n客户端工作负载发起对命名空间中特定服务的请求。 网格数据平面拦截请求并识别目标服务。 基于关联的路由，请求被允许、拒绝，或根据匹配规则转发到适当的工作负载。 请记住，在实验阶段，服务网格的 Gateway API 可能会有更多的变化，不建议在生产环境中使用。\n但等等，还有更多！我们的旅程并没有结束 - 使用 API 的入口流量支持正快速向通用可用性 (GA) 进发，预计还会有更多动态的发展！\n让我们进一步探讨这个版本中的其他增强功能。\nAmbient Mesh 增强 Istio 团队一直在不断优化 ambient mesh，这是一种创新的部署模型，提供了一个替代传统 sidecar 方法的选择。如果你还没有探索 ambient，现在是深入了解介绍博客 的好时机。\n在这次更新中，我们强化了对ServiceEntry、WorkloadEntry、PeerAuthentication以及 DNS 代理的支持。并且，修复了一些 bug，增强了可靠性，以确保无缝的体验。\n请记住，ambient mesh 在这个版本中处于 alpha 阶段。Istio 社区热切期待你的反馈，以推动它向 Beta 阶段前进。\n简化虚拟机和多集群体验 简单易用是关键，特别是在处理虚拟机和多集群设置的时候。在这个版本中，我们在WorkloadEntry资源中使地址字段变为可选。这个看似小小的调整将大大简化你的工作流程。\n提升安全配置 你现在可以为你的 Istio 入口网关的 TLS 设置配置OPTIONAL_MUTUAL，提供可选的客户端证书验证的灵活性。此外，你可以通过MeshConfig微调你偏好的非 Istio mTLS 流量使用的密码套件。\n有了这些更新，Istio 1.19 赋予你在管理你的服务网格时更大的控制、灵活性和安全性。\n欢迎你探索这些增强功能，并与 Istio 社区分享你的体验。更多详细信息，请参考官方发布说明 。\n祝你网格愉快！\n本博客最初在tetrate.io 上发布。\n","relpermalink":"/blog/istio-119-release/","summary":"我很高兴呈现 Istio 的最新版本— Istio 1.19。这篇博客将概述此版本中的更新内容。","title":"Istio 1.19 有哪些更新：Gateway API 还有更多"},{"content":"云计算专业人士，自 Istio Certified Associate (ICA) 认证 旅程开始已经一个月了。这是云原生计算基金会（CNCF）、Linux 基金会培训与认证以及 Tetrate 合作的成果，为微服务管理设立了新的标杆。\nTetrate 是 Istio 项目的关键贡献者，最初创建了 Tetrate 认证的 Istio 管理员（CIAT），为 ICA 认证奠定了基础。自从推出以来，ICA 已成为 Kubernetes 生态系统的重要组成部分，帮助数千人掌握服务网格技术。\n认证课程涵盖了诸如安装、流量管理和安全等基本领域，反映出该领域所需的全面专业知识。在短短时间内，ICA 获得了显著动力，得益于 Tetrate 对云原生领域的愿景和投入。\n在 ICA 成立一个月之际，Tetrate 继续通过其 Tetrate Academy 承诺开源教育，提供有关服务网格和 Kubernetes 安全的免费、高质量课程。对于那些打算在生产中部署 Istio 的人来说，Tetrate 提供了Tetrate Istio Distribution (TID)，一种安全且得到支持的 Istio 发行版。\n如果您还没有加入，现在是时候加入这个不断增长的 Istio 专家社区了。提升您的职业生涯，为云原生技术的演进做出贡献。\n什么是 ICA 认证？ ICA 是专为工程师，CI/CD 从业者或任何对 Istio 有特殊兴趣的人设计的专业前认证。\n获得认证的 ICA 学员可确认他们对 Istio 原则、术语和最佳实践的基础知识，并展示他们建立 Istio 的能力。\nICA 认证考试展示了考生对 Istio 原则、术语和建立 Istio 的最佳实践的深入理解。\nICA 认证考试内容 ICA 认证考试包括这些一般领域及其在考试中的权重：\n安装、升级和配置：7% 流量管理：40% 弹性和故障注入：20% 保护工作负载：20% 高级场景：13% 详细内容如下：\n安装、升级和配置：7%\n使用 IstioCLI 安装基本集群 使用 IstioOperatorAPI 定制 Istio 安装 使用覆盖来管理 Istio 组件设置 流量管理：40%\n控制服务网格内的网络流量 配置 Sidecar 注入 使用 Gateway 资源配置入口和出口流量 了解如何使用 ServiceEntry 资源向内部服务注册中心添加条目 使用 DestinationRule 定义流量策略 配置流量镜像功能 弹性和故障注入：20%\n配置断路器 (带或不带离群值检测) 使用弹性特性 创建故障注入 固定工作负载：20%\n了解 Istio 安全特性 在mesh中为HTTP/TCP流量建立Istio授权 在网格、命名空间和工作负载级别配置相互 TLS 高级场景：13%\n了解如何将非 kubernetes 工作负载装载到网格 排除配置问题 如何获得 ICA 认证？ 点击开始您的 ICA 认证吧！ 这只是一个开始。让我们期待在服务网格熟练度上达到更多里程碑，由 Tetrate 和 CNCF 引领潮流。\n注：ICA 认证提供中文认证考试，证书有效期 3 年。\n","relpermalink":"/blog/ica-review/","summary":"CNCF 与 Tetrate 共同推出的 Istio 认证庆祝一个月，提升了在微服务、安全和流量管理方面的 Kubernetes 技能。","title":"CNCF 与 Tetrate 合作推出 Istio Certificated Associate（ICA）认证"},{"content":"近期，我一直在深入研究云原生身份以及如何在 Istio 服务网格中集成 SPIFFE。这项研究主要针对的是面向多集群和多网格环境的 SPIRE Federation 部署。SPIFFE 是一个复杂但非常重要的主题，因此我花了大量时间来理解和掌握。\n在这个过程中，我发现 spiffe.io 官网的信息非常有用，但因为其以英文为主，可能对一些人来说阅读有点困难。因此我决定翻译这个网站，以便更多的人能理解和使用 SPIFFE。\n不仅如此，我还对文档的顺序进行了重新调整，以便读者能更容易地找到他们需要的信息。我还增加了一些插图，因为我相信这些插图可以帮助读者更好地理解 SPIFFE 和 SPIRE Federation 的工作原理以及它们如何在 Istio 服务网格中集成。\n希望我对 spiffe.io 官网的翻译和调整能为那些在云原生身份和 Istio 服务网格中集成 SPIFFE 的人提供帮助。\n","relpermalink":"/notice/spiffe-and-spire/","summary":"希望我对 spiffe.io 官网的翻译和调整能为那些在云原生身份和 Istio 服务网格中集成 SPIFFE 的人提供帮助。","title":"资料分享：使用 SPIFFE 和 SPIRE 实现零信任安全身份"},{"content":" 关于本文 本文根据笔者在 KubeCon\u0026amp;CloudNativeCon China 2023 的通话仓活动 IstioCon China 上的分享整理而成，原标题为《如何在 Argo、Istio 和 SkyWalking 中实现 GitOps 和可观察性的完美结合》。 云原生应用的发展导致开发左移，应用迭代频率更高，这就催生了 GitOps 的需求。本文将介绍如何使用 Argo 项目，包括 ArgoCD 和 Argo Rollouts，通过 Istio 实现 GitOps 和金丝雀部署。文中还有一个演示，展示了如何基于 Tetrate Service Express（也适用于 Tetrate Service Bridge）提供的 Istio 环境实现 GitOps。\n本文 demo 的部署架构图如图 1 所示。如果您已经熟悉本文介绍的部署策略和 Argo 项目，可以直接跳到 demo 部分 。\n图 1：在 TSE/TSB 中使用 Istio 和 Argo 项目实现 GitOps 和金丝雀发布的架构图 部署策略 首先，我想简单介绍一下 Argo Rollouts 支持的两种部署策略，可以实现零停机部署。\n蓝绿部署和金丝雀部署的步骤如图 2 所示。\nFigure 2: Steps of blue-green deployment and canary deployment 蓝绿部署是一种在单独的环境中并行部署新版本应用程序而不影响当前生产环境的策略。在蓝绿部署中，当前的生产环境称为“蓝色环境”，部署新版本应用程序的环境称为“绿色环境”。一旦绿色环境被认为稳定并通过测试，流量将逐渐从蓝色环境切换到绿色环境，让用户逐步接入新版本。如果切换过程中出现问题，可以快速回滚到蓝环境，最大程度地减少对用户的影响。蓝绿部署的优点是可以提供高可用性和零停机部署。 金丝雀部署是一种逐步将新版本或功能引入生产环境的策略。在金丝雀部署中，新版本或新功能首先部署到生产环境中的少数用户，称为“金丝雀用户”。通过监控金丝雀用户的反馈和性能指标，开发团队可以评估新版本或功能的稳定性和可靠性。如果没有问题，可以逐步将更多用户纳入金丝雀部署中，直到所有用户都使用新版本。如果发现问题，可以快速回滚或修复，避免对整个用户群造成负面影响。金丝雀部署的优点是可以快速发现问题并在影响较小的范围内进行调整。 蓝绿部署和金丝雀部署的主要区别在于部署方式和变更规模。蓝绿部署是将整个应用部署在新的环境中，然后进行切换，适合大规模的变更，比如整个应用的重大升级。金丝雀部署逐渐引入新版本或功能，适合小规模更改，例如添加或修改单个功能。\n从应用场景来看，蓝绿部署适合对高可用、零宕机部署要求较高的系统。在部署大规模变更时，蓝绿部署可以保证稳定性和可靠性，并且可以快速回滚以应对突发情况。金丝雀部署适合需要快速验证新功能或版本的系统。通过逐步引入变更，可以及早发现问题并进行调整，尽量减少对用户的影响。\nKubernetes Deployment 的发布策略 在 Kubernetes 中，Deployment 资源对象是管理应用程序部署和更新的主要工具之一。部署提供了一种声明式方式来定义应用程序的预期状态，并通过控制器的功能实现发布策略。Deployment 的架构如图 3 所示，其中彩色方块代表不同版本的 Pod。\n图 3：Kubernetes Deployment 架构图 发布策略可以在 Deployment 的 spec 字段中配置。以下是一些常见的发布策略选项：\nReplicaSet 的管理：Deployment 使用 ReplicaSet 来创建和管理应用程序的副本。可以通过设置 spec.replicas 字段来指定所需的副本数量。在发布过程中，Kubernetes 控制器保证新版本 ReplicaSet 的副本数量在创建时逐渐增加，旧版本 ReplicaSet 的副本数量在删除时逐渐减少，实现平滑切换。 滚动更新策略：部署支持多种滚动更新策略，可以通过设置 spec.strategy.type 字段来选择。常见政策包括： RollingUpdate：默认策略以一定的时间间隔逐渐更新副本。不同时可用的副本数量以及额外可用的副本数量可以通过设置 spec.strategy.rollingUpdate.maxUnavailable 和 spec.strategy.rollingUpdate.maxSurge 字段来控制。 ReCreate：该策略在更新过程中首先删除旧版本的所有副本，然后创建新版本的副本。此策略将导致应用程序在更新期间暂时不可用。 版本控制：Deployment 通过 spec.template.metadata.labels 字段为每个版本的 ReplicaSet 设置标签，以便控制器准确跟踪和管理。这样 ReplicaSet 的多个版本可以共存，并且可以精确控制每个版本的副本数量。 通过使用这些配置选项，Deployment 可以实现不同的发布策略。更新 Deployment 对象的 spec 字段可以触发新版本的发布。Kubernetes 控制器会根据指定的策略自动处理副本的创建、更新和删除，以实现平滑的应用更新和部署策略。\n使用 ArgoCD 实施 GitOps 可以使用 Deployment 来手动管理发布策略，但要实现自动化，我们还需要使用 ArgoCD 等 GitOps 工具。\nArgoCD 是一个基于 GitOps 的持续交付工具，用于自动化和管理 Kubernetes 应用程序的部署。它为提高应用程序部署的效率和可靠性提供了一些关键的帮助。\n以下是 ArgoCD 为 Kubernetes 应用程序部署提供的一些帮助：\n声明式配置：ArgoCD 使用声明式方式定义应用程序的预期状态，并将应用程序配置存储在 Git 存储库中。通过版本控制和持续集成/持续交付 (CI/CD) 流程，可以轻松跟踪和管理应用程序配置更改。 持续部署：ArgoCD 可以监控 Git 存储库中的配置变化，并自动将应用程序部署到 Kubernetes 环境。提供可定制的同步策略，自动触发应用部署和更新，实现持续部署。 状态比较和自动修复：ArgoCD 定期检查应用程序的当前状态并将其与预期状态进行比较。如果发现不一致，它会自动尝试修复并将应用程序恢复到所需状态，以确保预期状态与实际状态的一致性。 多环境管理：ArgoCD 支持管理多个 Kubernetes 环境，例如开发、测试和生产环境。可以轻松地在不同环境之间部署和同步应用配置，确保一致性和可控性。 与 Deployment 资源对象相比，ArgoCD 提供了更高级的功能和工作流程，补充了原生 Kubernetes 资源对象的功能：\n基于 GitOps 的配置管理：ArgoCD 将应用程序配置存储在 Git 存储库中，从而实现基于 GitOps 的配置管理。这种方法确保配置更改是可跟踪、可审计的，并且可以与现有的 CI/CD 管道集成。\n自动化部署和持续交付：ArgoCD 可以自动检测 Git 存储库中的配置更改并将应用程序部署到 Kubernetes 环境，从而实现自动化部署和持续交付。\n状态管理和自动恢复：ArgoCD 持续监控应用程序的状态并将其与预期状态进行比较。如果检测到不一致，它会自动恢复并确保应用程序状态与预期状态保持一致。\n使用 Istio 实现细粒度的流量路由 虽然 ArgoCD 可以实现 GitOps，但它本质上是在 Kubernetes 部署上运行并通过副本数量控制流量路由。为了实现细粒度的流量路由，使用了 Istio 等服务网格。\nIstio 通过以下方法实现更细粒度的流量路由和应用发布：\nVirtualService：Istio 使用 VirtualService 来定义流量路由规则。通过配置 VirtualService，可以根据请求头、路径、权重等请求属性对流量进行路由和分发，将请求定向到不同的服务实例或版本。\nDestinationRule：Istio 的 DestinationRule 用于定义服务版本策略和负载均衡设置。通过指定不同版本服务实例之间不同的流量权重，可以实现金丝雀发布、蓝绿部署等高级应用发布策略。\n流量控制和策略：Istio 提供了丰富的流量控制和策略能力，如流量限制、故障注入、超时设置、重试机制等，这些功能帮助应用程序实现更高级别的负载均衡、容错和可靠性要求。\n与 ArgoCD 和 Kubernetes Deployment 对象相比，Istio 在应用部署方面提供了以下优势：\n细粒度的流量路由控制：Istio 提供了更丰富的流量路由能力，可以根据多种请求属性进行灵活的路由和分发，从而实现更细粒度的流量控制和管理。\n高级发布策略支持：Istio 的 DestinationRule 可以指定不同版本服务实例之间的流量权重，支持金丝雀发布、蓝绿部署等高级应用发布策略。这使得应用程序的版本管理和发布更加灵活可控。\n强大的流量控制和策略能力：Istio 提供了丰富的流量控制和策略能力，如流量限制、故障注入、超时设置、重试机制等，这些功能帮助应用程序实现更高级别的负载均衡、容错和可靠性要求。\n将 Istio 与 Argo Rollouts 相结合，可以充分发挥 Istio 细粒度流量路由的优势。现在让我们一起进行演示。在我们的演示中，我们将使用 TSE 提供的 Kubernetes 和 Istio 环境，使用 ArgoCD 实现 GitOps，并使用 Argo Rollouts 实现金丝雀发布。\nDemo 我们的演示中使用的软件版本是：\nKubernetes v1.24.14 Istio v1.15.7 Argo CD v2.7.4 Argo 发布 v1.5.1 TSE Preview 2 我们将使用 Istio 的 VirtualService 和 DestinationRule 来实现基于 Subset 的流量分组路由，并使用 ArgoCD Rollouts 进行渐进式发布。\n部署 ArgoCD 和 Argo Rollouts 我提前创建了一个 Kubernetes 集群并将其添加到 TSE 中，TSE 会自动为集群安装 Istio 控制平面。我们还需要安装 ArgoCD 和 Argo Rollouts：\n# Install ArgoCD kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml # Install ArgoCD CLI on macOS brew install argocd # Change the service type of argocd-server to LoadBalancer kubectl patch svc argocd-server -n argocd -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;}}\u0026#39; # Get the ArgoCD UI address ARGOCD_ADDR=$(kubectl get svc argocd-server -n argocd -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) # Login using ArgoCD CLI, see https://argo-cd.readthedocs.io/en/stable/getting_started/#4-login-using-the-cli to get password argocd login $ARGOCD_ADDR --skip-test-tls --grpc-web --insecure # Install Argo …","relpermalink":"/blog/implementing-gitops-and-canary-deployment-with-argo-project-and-istio/","summary":"本文讨论如何使用 Kubernetes Deployment、Argo 项目和 Istio 来实现 GitOps 和金丝雀部署。","title":"使用 Argo 项目 Istio 及 SkyWalking 实现 GitOps 和金丝雀部署"},{"content":" 编者按 本文介绍了 Istio 中支持的负载均衡类型，然后提出多集群网格负载均衡的解决方案。如果您已经了解 Istio 中负载均衡，可以直接从多集群网格中的负载均衡 部分开始阅读。 在之前的博客为什么在使用了 Kubernetes 后你可能还需要 Istio 中提到 Istio 是在 Kubernetes 的基础之上构建起来的，Kubernetes 中的组件 kube-proxy 本身已有负载均衡功能，但是只支持四层流量的负载均衡，而且无法实现服务超时、熔断等高级功能。具体来说，服务网格比起 Kubernetes 新增了以下负载均衡及韧性（Resiliency）特性：\nLayer 7 负载均衡：服务网格在应用层（Layer 7）操作和管理流量，可以更细粒度地识别和控制流量。这使得它可以实现更高级的负载均衡策略，如基于 HTTP 请求头、URL 路径、Cookie 等的路由和流量分发。\n动态负载均衡：服务网格通常具备自动负载均衡的能力，可以根据后端服务的实时健康状态和性能指标来动态分发流量。这允许它实现智能负载均衡策略，将流量路由到性能最佳的服务实例。\n故障检测和自动故障转移：服务网格具备高级的故障检测和自动故障转移功能。它可以检测到后端服务实例的故障，并自动将流量从故障实例转移到健康实例，以确保应用程序的可用性。\nA/B 测试和金丝雀发布：服务网格允许实施高级部署策略，如 A/B 测试和金丝雀发布。这些策略允许在不同的服务版本之间动态分配流量，并根据结果进行决策。\n熔断和重试：服务网格通常包含熔断和重试机制，以提高应用程序的可用性和稳定性。它可以根据后端服务的性能和可用性情况来自动执行熔断操作，并在必要时重试请求。\n全局流量控制：服务网格提供了集中式的流量控制和策略定义，允许对整个服务网格中的流量进行全局管理。这使得实现统一的安全性、监控和策略成为可能。\n深度集成的监控和追踪：服务网格通常集成了强大的监控和追踪工具，可以提供有关流量性能和可见性的详细信息，帮助进行故障排除和性能优化。\n虽然 Kubernetes 提供了基本的负载均衡能力，但服务网格在其之上构建了更高级的负载均衡和流量管理功能，以满足微服务架构中复杂的需求。\n客户端负载均衡 vs 服务端负载均衡 客户端负载均衡和服务端负载均衡是两种不同的负载均衡方法，它们在不同的场景和应用中有各自的优势。以下是对它们的解释，适用的场景，实现案例以及相关的开源项目：\n客户端负载均衡（Client-Side Load Balancing）\n客户端负载均衡的示意图如图 1 所示。\n图 1：客户端负载均衡 定义：在客户端负载均衡中，负载均衡决策是由服务的消费者（客户端）来做出的。客户端负载均衡器通常会维护一个服务实例列表，并根据配置和策略选择要发送请求的实例。\n适用场景：客户端负载均衡适用于以下情况：\n多个客户端需要访问同一组后端服务，每个客户端可以根据自己的需求和策略选择后端服务。 服务消费者需要更多的流量控制和策略定义，例如 A/B 测试、金丝雀发布等。 实现案例：常见的实现客户端负载均衡的开源项目包括：\nRibbon：Netflix Ribbon 是一个用于客户端负载均衡的开源项目，它可以与 Spring Cloud 集成。 Envoy：Envoy 是一个高性能的代理服务器，支持客户端负载均衡，广泛用于服务网格和微服务架构中。 NGINX：虽然 NGINX 通常用于反向代理，但也可以用作客户端负载均衡器。 服务端负载均衡（Server-Side Load Balancing）\n服务端负载均衡如图 2 所示。\n图 2：服务端负载均衡 定义：在服务端负载均衡中，负载均衡决策是由服务端的负载均衡器或代理来做出的。客户端只需将请求发送到服务端，然后服务端决定将请求路由到哪个后端服务实例。\n适用场景：服务端负载均衡适用于以下情况：\n客户端不关心后端服务的具体实例，只关心发送请求到服务的名称或地址。 负载均衡策略需要在服务端进行全局配置，客户端不需要关心负载均衡细节。 实现案例：常见的实现服务端负载均衡的开源项目包括：\nNGINX：NGINX 可以用作反向代理服务器，执行服务端负载均衡，将请求路由到后端服务。 HAProxy：HAProxy 是一种高性能的负载均衡器，通常用于服务端负载均衡。 Amazon ELB（Elastic Load Balancer）：亚马逊提供的负载均衡服务，用于将请求路由到 AWS 后端服务实例。 在实际应用中，有时也会将客户端负载均衡和服务端负载均衡结合使用，以满足特定的需求。选择哪种负载均衡方法通常取决于您的架构、部署需求以及性能要求。服务网格（如 Istio）通常使用客户端负载均衡来实现细粒度的流量控制和策略定义，而在云服务提供商中，服务端负载均衡通常用于自动扩展和流量管理。\nIstio 如何实现负载均衡 在服务网格（如 Istio）中，客户端负载均衡通常是通过 Envoy 代理实现的。Envoy 是一个高性能的代理服务器，它可以用于构建服务网格的数据平面，用于处理服务之间的通信。客户端负载均衡是一种在服务消费者（客户端）一侧实现的负载均衡策略，它决定了请求应该如何路由到后端服务实例。\n单集群单网络 Istio 服务网格的负载均衡如图 3 所示。\n图 3：单集群单网络的 Istio 服务网格的负载均衡 以下是服务网格中如何实现客户端负载均衡的一般流程：\nSidecar 代理: 在服务网格中，每个部署的服务实例通常都与一个 Sidecar 代理（通常是 Envoy）关联。这个 Sidecar 代理位于服务实例旁边，负责处理该服务实例的入站和出站流量。\n服务注册与发现: 在服务网格中，服务实例的注册和发现通常由服务注册表（Kubernetes 的服务发现机制）处理。这些注册表维护了服务实例的信息，包括它们的网络地址和健康状态。\n客户端负载均衡配置: 在客户端（服务消费者）发送请求时，Sidecar 代理会执行负载均衡策略。这些负载均衡策略通常在服务注册表中获取的服务实例列表上操作。策略可以基于多种因素进行选择，例如权重、健康状态、延迟等。\n请求路由: 根据负载均衡策略，Sidecar 代理将请求路由到选择的后端服务实例。这可以包括使用轮询、加权轮询、最少连接等算法来选择目标实例。\n通信处理: 一旦选择了目标实例，Sidecar 代理将请求转发给该实例，然后将响应传回给客户端。它还可以处理连接管理、故障检测和自动故障转移等任务。\n总之，客户端负载均衡是在服务消费者一侧（通常是 Envoy 代理）实现的负载均衡策略，它使服务网格能够有效地分发流量并处理后端服务实例的故障。这种方式使得负载均衡决策在服务消费者的控制下，并允许更精细的流量控制和策略定义。Envoy 代理是实现客户端负载均衡的关键组件之一，它具有丰富的配置选项，可用于满足不同的负载均衡需求。\nIstio 中的负载均衡类型 在 Istio 的DestinationRule 资源中，loadBalancer部分用于配置负载均衡策略，控制请求如何分发到不同的服务实例或版本，如下图所示。\n图 4：Istio 中的负载均衡配置参数 从图中我们可以看出，Istio 支持三种类型的负载均衡，分别是：\nsimple：基于常用负载均衡算法的简单负载均衡 consistentHashLB：基于一致性哈希算法的负载均衡 localityLbSetting：基于地域的本地性负载均衡 以下是与负载均衡配置相关的字段的含义：\nsimple：这个部分定义了一些简单的负载均衡策略选项，其中包括以下选项： ROUND_ROBIN：请求会依次分发到所有可用的后端实例，以轮询的方式。 LEAST_CONN：请求将路由到当前连接数最少的后端实例。 RANDOM：请求将随机路由到后端实例。 PASSTHROUGH：Istio 不会进行负载均衡，而是将请求直接路由到服务的一个实例，适用于特定用例。 consistentHashLB：这个部分允许你配置一致性哈希负载均衡，其中包括以下字段： httpHeaderName：用于哈希计算的 HTTP 标头的名称。 httpCookie：用于哈希计算的 HTTP Cookie 的配置，包括名称、路径和生存时间（TTL）。 useSourceIp：是否使用请求的源 IP 地址进行哈希计算。 httpQueryParameterName：用于哈希计算的 HTTP 查询参数的名称。 ringHash：配置环形哈希负载均衡，包括最小环大小（minimumRingSize）。 maglev：配置 Maglev 负载均衡，包括表格大小（tableSize）和最小环大小（minimumRingSize）。 localityLbSetting：这个部分用于配置本地性负载均衡设置，其中包括以下字段： distribute：定义了请求的分布，包括起始位置（from）和结束位置（to）。 failover：定义了故障切换，包括起始位置（from）和结束位置（to）。 failoverPriority：故障切换的优先级设置。 enabled：是否启用本地性负载均衡。 这些字段允许你根据你的需求选择适当的负载均衡策略，并配置额外的选项，以确保请求按照所需的方式分发到后端服务实例。不同的策略和配置选项可以满足各种负载均衡需求，如性能、可靠性和流量控制。关于这些字段的详细介绍请见 Istio 文档 。\n如何在 Istio 中为服务设置负载均衡 正如我在如何理解 Istio 中的 VirtualService 和 DestinationRule 这篇文章中所说的，VirtualService 主要用于设置路由规则，而服务弹性（负载均衡、超时、重试、熔断等）需要靠它和 DestinationRule 来共同维持。只有同时部署了以上两种资源，负载均衡才能真正生效。\n以下是设置负载均衡的一般步骤：\n创建 DestinationRule 资源：首先，你需要创建一个 DestinationRule 资源，该资源定义了服务的流量策略和目标规则。在 DestinationRule 中，你可以指定要设置负载均衡的服务的名称（host）以及负载均衡策略。\n以下是一个 DestinationRule 的示例，其中将流量分发到具有标签 “version: v1” 和 “version: v2” 的两个子集中，并使用 ROUND_ROBIN 负载均衡策略：\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: my-destination-rule spec: host: my-service.example.com trafficPolicy: loadBalancer: simple: ROUND_ROBIN subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 应用 DestinationRule：创建 DestinationRule 后，将其应用于要进行负载均衡的服务。这通常可以通过 Istio 的 VirtualService 资源来完成，通过在 VirtualService 中引用 DestinationRule。\n以下是一个 VirtualService 示例，将流量引导到名为 “my-destination-rule” 的 DestinationRule：\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: my-virtual-service spec: hosts: - my-service.example.com http: - route: - destination: host: my-service.example.com …","relpermalink":"/blog/demystifying-the-load-balancing-in-istio/","summary":"Istio 负载均衡详解尤其是多集群如何实现路由与负载均衡的，并给出一个多集群路由的演示。","title":"Istio 中的负载均衡详解及多集群路由实践"},{"content":" AccessBindings is an assignment of roles to a set of users or teams to access resources. The user or team information is obtained from an user directory (such an LDAP server or an external OIDC server) that should have been configured as part of Service Bridge installation. Note that an AccessBinding can be created or modified only by users who have SET_POLICY permission on the target resource.\nThe following example assigns the workspace-admin role to users alice, bob, and members of the t1 team for the workspace w1 owned by the tenant mycompany.\nUse fully-qualified name (fqn) when specifying the target resource, as well as for the users and teams.\napiVersion: rbac.tsb.tetrate.io/v2 kind: AccessBindings metadata: fqn: organizations/myorg/tenants/mycompany/workspaces/w1 spec: allow: - role: rbac/workspace-admin subjects: - user: organizations/myorg/users/alice - user: organizations/myorg/users/bob - team: organizations/myorg/teams/t1 AccessBindings AccessBindings assigns permissions to users of any TSB resource.\nField Description Validation Rule allow\nList of tetrateio.api.tsb.rbac.v2.Binding The list of allowed bindings configures the different access profiles that are allowed on the target resource.\n–\n","relpermalink":"/book/tsb/refs/tsb/rbac/v2/access-bindings/","summary":"Configuration for assigning access roles to users of any resource in TSB.","title":"Access Bindings"},{"content":" Agent Configuration specifies configuration of the Workload Onboarding Agent.\nIn most cases, Workload Onboarding Agent can automatically recognize the host environment, e.g. AWS EC2, which makes explicit Agent Configuration optional.\nBy default, Workload Onboarding Agent comes with the minimal configuration:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: AgentConfiguration which at runtime is interpreted as an equivalent of:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: AgentConfiguration host: auto: {} sidecar: istio: {} stdout: filename: /dev/stdout stderr: filename: /dev/stderr The above configuration means that Workload Onboarding Agent should infer host environment automatically, should be in control of the Istio Sidecar pre-installed on that host, should redirect standard output of the Istio Sidecar into its own output.\nMost users do not need to change the default configuration.\nUsers who make use of Istio revisions, need to specify the revision the pre-installed Istio Sidecar corresponds to, e.g.:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: AgentConfiguration sidecar: istio: revision: canary Users who want to redirect standard output of the Istio Sidecar into a separate file (instead of mixing together output of the Workload Onboarding Agent and output of the Istio Sidecar), should use the following configuration:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: AgentConfiguration sidecar: stdout: filename: ./relative/path/to/file stderr: filename: /absolute/path/to/file Relative path of a log file is interpreted as relative to the working directory of the Workload Onboarding Agent.\nAdvanced users who would like to utilize Workload Onboarding Agent in an environment that is not supported out-of-the-box, can develop custom Workload Onboarding Agent Plugins and use them by providing an explicit Agent Configuration, e.g.:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: AgentConfiguration host: custom: credential: - plugin: name: custom-credential-provider path: /path/to/custom-credential-provider-binary hostinfo: plugin: name: custom-hostinfo-provider path: /path/to/custom-hostinfo-provider-binary env: - name: CONFIG value: /path/to/config args: - --name=value settings: connection: timeout: 60s retryPolicy: exponentialBackoff: initialInterval: 10s maxInterval: 120s Workload Onboarding Agent Plugin is an auxiliary executable (e.g. binary, shell script, Python script, etc) installed in addition to the Workload Onboarding Agent.\nWorkload Onboarding Agent executes a Workload Onboarding Agent Plugin to procure platform-specific information.\n+--------------------------------------------------------+ | Host (e.g., VM or container) | | | | +------------------+ +------------------+ | | | | | | | | | Workload | ---------\u0026gt; | Workload | | | | Onboarding Agent | (executes) | Onboarding Agent | | | | | | Plugin | | | +------------------+ +------------------+ | | | +--------------------------------------------------------+ Workload Onboarding Agent Plugin is modeled as a gRPC service with unary call method(s). However, Workload Onboarding Agent Plugin does not run a network server. Instead, semantics of an unary RPC call is mapped onto execution of a process.\nTo make a call to the plugin, Workload Onboarding Agent:\nruns executable of the Workload Onboarding Agent Plugin passes parameters in via environment variables with the following names: PLUGIN_NAME - mandatory - e.g., aws-ec2-credential RPC_SERVICE_NAME - mandatory - e.g. tetrateio.api.onboarding.private.component.agent.plugin.credential.v1alpha1.CredentialPlugin RPC_METHOD_NAME - mandatory - e.g. GetCredential writes request message serialized into JSON to the stdin of the plugin process if plugin process exists with a 0 code, reads from stdout response message serialized into JSON if plugin process exists with a non-0 code, reads from stdout RPC status message serialized into JSON in a corner case where plugin process starts writing to stdout a response message, then encounters a failure and continues by writing to stdout an RPC status message, Workload Onboarding Agent should look at the exit code of the plugin process to decide how to interpret contents of stdout plugin process must only print to stdout either a response message or an RPC status message plugin process may print to stderr any data, e.g. diagnostic messages In some cases instead of developing a custom plugin it is possible to reuse a built-in behavior.\nE.g., instead of developing a custom HostInfo plugin you can reuse built-in behavior that simply lists available network interfaces instead of interacting with the platform-specific metadata API.\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: AgentConfiguration host: custom: credential: - plugin: name: custom-credential-provider path: /path/to/custom-credential-provider-binary hostinfo: basic: networkInterfaces: include: - ^eth[0-9]*$ AgentConfiguration …","relpermalink":"/book/tsb/refs/onboarding/config/agent/v1alpha1/agent-configuration/","summary":"Specifies configuration of the Onboarding Agent.","title":"Agent Configuration"},{"content":" API objects define a set of servers and endpoints that expose the business logic for an Application. APIs are attached to existing Applications to configure how the features exposed by the different services that are part of the Application can be accessed.\nThe format used to define APIs is based on the OpenAPI v3 spec. Users can attach OpenAPI documents to the applications, and Service Bridge will generate all the configuration that is needed to make the APIs available. Service Bridge also provides a set of custom extensions to the OpenAPI spec that can be used to further customize the APIs in those cases where the standard OpenAPI properties are not sufficient.\nThe following example shows how an API can be attached to an existing application:\napiversion: application.tsb.tetrate.io/v2 kind: API metadata: organization: my-org tenant: tetrate application: example-app name: ezample-app-api spec: description: An example OpenAPI based API workloadSelector: namespace: exampleapp labels: app: exampleapp-gateway openapi: | openapi: 3.0.0 info: title: Sample API description: An example API defined in an OpenAPI spec version: 0.1.9 x-tsb-service: sample-app.sample-ns # service exposing this api servers: - url: http://api.example.com/v1 description: Optional server description, e.g. Main (production) server - url: http://staging-api.example.com paths: /users: get: summary: Returns a list of users. description: Optional extended description in CommonMark or HTML. responses: \u0026#39;200\u0026#39;: # status code description: A JSON array of user names content: application/json: schema: type: array items: type: string API An API configuring a set of servers and endpoints that expose the Application business logic.\nField Description Validation Rule openapi\nstring REQUIRED The raw OpenAPI spec for this API.\nstring = { min_len: 1}\nworkloadSelector\ntetrateio.api.tsb.types.v2.WorkloadSelector Optional selector to specify the gateway workloads (pod labels and Kubernetes namespace) under the application gateway group that should be configured with this gateway. There can be only one gateway for a workload selector in a namespace. If the selector is omitted, then the following default workload selector will be applied, based on the name of the Application and the API objects.\nworkloadSelector: namespace: exampleapp labels: app: application-name api: api-name –\nservers\nList of tetrateio.api.tsb.gateway.v2.HttpServer OUTPUT_ONLY DEPRECATED: For new created APIs, the exposed servers will be available at httpServers. For APIs created before version 1.7, will still be available in this field.\nList of ingress gateways servers that expose the API. Server hostnames must be unique in the system, and only one API can expose a specific hostname.\n–\nendpoints\nList of tetrateio.api.tsb.application.v2.HTTPEndpoint OUTPUT_ONLY List of endpoints exposed by this API. This field is read-only and generated from the configured OpenAPI spec.\n–\nhttpServers\nList of tetrateio.api.tsb.gateway.v2.HTTP OUTPUT_ONLY List of gateways servers that expose the API. Server hostnames must be unique in the system, and only one API can expose a specific hostname.\n–\nExposedBy The exposer of an HTTPEndpoint.\nField Description Validation Rule service\nstring oneof exposer OUTPUT_ONLY The FQN of the service in the service registry that is exposing a concrete endpoint.\n–\nclusterGroup\ntetrateio.api.tsb.application.v2.ExposedByClusters oneof exposer OUTPUT_ONLY The clusters that are exposing a concrete endpoint.\n–\nExposedByCluster ExposedByCluster is a cluster or set of clusters identified by the labels that are exposing an endpoint.\nField Description Validation Rule name\nstring The name of the cluster exposing the endpoint. Only one of name or labels must be specified.\n–\nlabels\nmap\u0026lt;string , string \u0026gt; Labels associated with the cluster. Any cluster with matching labels will be selected as an exposer. Only one of name or labels must be specified.\n–\nweight\nuint32 The weight for traffic to a cluster exposing the endpoint.\n–\nExposedByClusters ExposedByClusters represents the clusters that are exposing a concrete endpoint.\nField Description Validation Rule clusters\nList of tetrateio.api.tsb.application.v2.ExposedByCluster The clusters that contain gateways exposing the HTTPEndpoint.\n–\nHTTPEndpoint An HTTP Endpoint represents an individual HTTP path exposed in the API.\nField Description Validation Rule path\nstring OUTPUT_ONLY The HTTP path of the endpoint, relative to the hostnames exposed by the API.\n–\nmethods\nList of string OUTPUT_ONLY The list of HTTP methods this endpoint supports.\n–\nhostnames\nList of string OUTPUT_ONLY The list of hostnames where this endpoint is exposed. If omitted, the endpoint is assumed to be exposed in all hostnames defined for the API.\n–\nservice\nstring OUTPUT_ONLY DEPRECATED: For new created APIs, the exposed servers will be available at httpServers. For APIs created before version 1.7, will still be available in this field. The FQN of the service in the service …","relpermalink":"/book/tsb/refs/tsb/application/v2/api/","summary":"Configuration for APIs exposed by an Application.","title":"API"},{"content":" DEPRECATED: use Access Bindings instead.\nAPIAccessBindings is an assignment of roles to a set of users or teams to access API resources. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a APIAccessBinding can be created or modified only by users who have set_rbac permission on the API resource.\nThe following example assigns the api-admin role to users alice, bob, and members of the t1 team for the APIs openapi in the application app owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team\napiVersion: rbac.tsb.tetrate.io/v2 kind: APIAccessBindings metadata: organization: myorg tenant: mycompany application: app api: openapi spec: allow: - role: rbac/api-admin subjects: - user: organization/myorg/users/alice - user: organization/myorg/users/bob - team: organization/myorg/teams/t1 APIAccessBindings APIAccessBindings assigns permissions to users of APIs.\nField Description Validation Rule allow\nList of tetrateio.api.tsb.rbac.v2.Binding The list of allowed bindings configures the different access profiles that are allowed on the resource configured by the policy.\n–\n","relpermalink":"/book/tsb/refs/tsb/rbac/v2/api-access-bindings/","summary":"Configuration for assigning access roles to users of APIs.","title":"API Access Bindings"},{"content":" Applications are logical groupings of services that are related to each other, typically within a trusted group. A common example are three tier applications composed of a frontend, a backend and a datastore service.\nApplications are often consumed through APIs, and a single Application can expose one or more of those APIs. These APIs will define the hostnames that are exposed and the methods exposed in each hostname.\napiVersion: application.tsb.tetrate.io/v2 kind: Application metadata: name: three-tier organization: myorg tenant: tetrate spec: workspace: organizations/myorg/tenants/tetrate/three-tier Application An Application represents a set of logical groupings of services that are related to each other and expose a set of APIs that implement a complete set of business logic.\nField Description Validation Rule workspace\nstring REQUIRED FQN of the workspace this application is part of. The application will configure IngressGateways for the attached APIs in the different namespaces exposed by this workspace.\nstring = { min_len: 1}\nnamespaceSelector\ntetrateio.api.tsb.types.v2.NamespaceSelector oneof scope INPUT_ONLY Optional set of namespaces this application can configure. If configured, a Gateway Group including these namespaces will be created for the application. If no namespaces are configured and no existing gateway group is set, a new gateway group claiming all namespaces in the workspace (*/*) will be created by default. All Ingress Gateway resources created for the APIs attached to the application will be created in the application’s gateway group.\n–\ngatewayGroup\nstring oneof scope Optional FQN of the Gateway Group to be used by the application. If configured, this gateway group will be used by the application. If no namespaces are configured and no existing gateway group is set, a new gateway group claiming all namespaces in the workspace (*/*) will be created by default. All Ingress Gateway resources created for the APIs attached to the application will be created in the application’s gateway group.\n–\nservices\nList of string Optional list of services that are part of the application. This is a list of FQNs of services in the service registry. If omitted, the application is assumed to own all the services in the workspace. Note that a service can only be part of one application. If any of the services in the list is already in use by an existing application, application creation/modification will fail. If the list of services is not explicitly set and any service in the workspace is already in use by another application, application creation/modification will fail.\nrepeated = { items: {string:{min_len:1}}}\n","relpermalink":"/book/tsb/refs/tsb/application/v2/application/","summary":"Configuration for Applications.","title":"Application"},{"content":" DEPRECATED: use Access Bindings instead.\nApplicationAccessBindings is an assignment of roles to a set of users or teams to access Application resources. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a ApplicationAccessBinding can be created or modified only by users who have SET_POLICY permission on the Application.\nThe following example assigns the application-admin role to users alice, bob, and members of the t1 team for the application app owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team\napiVersion: rbac.tsb.tetrate.io/v2 kind: ApplicationAccessBindings metadata: organization: myorg tenant: mycompany application: app spec: allow: - role: rbac/application-admin subjects: - user: organization/myorg/users/alice - user: organization/myorg/users/bob - team: organization/myorg/teams/t1 ApplicationAccessBindings ApplicationAccessBindings assigns permissions to users of applications.\nField Description Validation Rule allow\nList of tetrateio.api.tsb.rbac.v2.Binding The list of allowed bindings configures the different access profiles that are allowed on the resource configured by the policy.\n–\n","relpermalink":"/book/tsb/refs/tsb/rbac/v2/application-access-bindings/","summary":"Configuration for assigning access roles to users of applications.","title":"Application Access Bindings"},{"content":" Service to manage Applications and APis\nApplications The Applications service exposes methods to manage Applications and API definitions in Service Bridge.\nCreateApplication rpc CreateApplication (tetrateio.api.tsb.application.v2.CreateApplicationRequest ) returns (tetrateio.api.tsb.application.v2.Application )\nRequires CREATE\nCreates a new Application in TSB.\nGetApplication rpc GetApplication (tetrateio.api.tsb.application.v2.GetApplicationRequest ) returns (tetrateio.api.tsb.application.v2.Application )\nRequires READ\nGet the details of an existing application.\nUpdateApplication rpc UpdateApplication (tetrateio.api.tsb.application.v2.Application ) returns (tetrateio.api.tsb.application.v2.Application )\nRequires WRITE\nModify an existing application.\nListApplications rpc ListApplications (tetrateio.api.tsb.application.v2.ListApplicationsRequest ) returns (tetrateio.api.tsb.application.v2.ListApplicationsResponse )\nList all existing applications for the given tenant.\nDeleteApplication rpc DeleteApplication (tetrateio.api.tsb.application.v2.DeleteApplicationRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete an existing Application. Note that deleting resources in TSB is a recursive operation. Deleting a application will delete all API objects that exist in it.\nGetApplicationStatus rpc GetApplicationStatus (tetrateio.api.tsb.application.v2.GetStatusRequest ) returns (tetrateio.api.tsb.application.v2.ResourceStatus )\nRequires ReadApplication\nGet the configuration status of an existing application.\nCreateAPI rpc CreateAPI (tetrateio.api.tsb.application.v2.CreateAPIRequest ) returns (tetrateio.api.tsb.application.v2.API )\nRequires CREATE\nAttach a new API to the given application.\nGetAPI rpc GetAPI (tetrateio.api.tsb.application.v2.GetAPIRequest ) returns (tetrateio.api.tsb.application.v2.API )\nRequires READ\nGet the details of an API.\nUpdateAPI rpc UpdateAPI (tetrateio.api.tsb.application.v2.API ) returns (tetrateio.api.tsb.application.v2.API )\nRequires WRITE\nDeprecated. Use the UpdateAPIWithParams method instead. Modifies an existing API object if its status is not DIRTY.\nUpdateAPIWithParams rpc UpdateAPIWithParams (tetrateio.api.tsb.application.v2.UpdateAPIRequest ) returns (tetrateio.api.tsb.application.v2.API )\nRequires WriteAPI\nModify an existing API object. By default, API objects that are in DIRTY state cannot be modified. This state is reached when the configurations generated for the API object are not in sync with the contents of the API object itself, so updates are rejected to prevent accidental changes. In these situations, the force flag can be used to force the update and to overwrite any changes that have been done to the generated config resources. When using the HTTP APIs, the force flag must be set as a query parameter.\nListAPIs rpc ListAPIs (tetrateio.api.tsb.application.v2.ListAPIsRequest ) returns (tetrateio.api.tsb.application.v2.ListAPIsResponse )\nList all APIs attached to the given application.\nDeleteAPI rpc DeleteAPI (tetrateio.api.tsb.application.v2.DeleteAPIRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete an existing API.\nGetAPIStatus rpc GetAPIStatus (tetrateio.api.tsb.application.v2.GetStatusRequest ) returns (tetrateio.api.tsb.application.v2.ResourceStatus )\nRequires ReadAPI\nGet the configuration status of an existing API.\nCreateAPIRequest Request to create an API and register it in the management plane so configuration can be generated for it.\nField Description Validation Rule parent\nstring REQUIRED Parent resource where the API will be created. This is the FQN of the application where the API belongs to.\nstring = { min_len: 1}\nname\nstring REQUIRED The short name for the resource to be created.\nstring = { min_len: 1}\napi\ntetrateio.api.tsb.application.v2.API REQUIRED Details of the API to be created.\nmessage = { required: true}\nCreateApplicationRequest Request to create an application and register it in the management plane so configuration can be generated for it.\nField Description Validation Rule parent\nstring REQUIRED Parent resource where the application will be created. This is the FQN of the tenant where the application belongs to.\nstring = { min_len: 1}\nname\nstring REQUIRED The short name for the resource to be created.\nstring = { min_len: 1}\napplication\ntetrateio.api.tsb.application.v2.Application REQUIRED Details of the application to be created.\nmessage = { required: true}\nDeleteAPIRequest Request to delete an API.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the API.\nstring = { min_len: 1}\nDeleteApplicationRequest Request to delete an application.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the application.\nstring = { min_len: 1}\nforceDeleteProtectedGroups\nbool Force the deletion of internal groups even if they are protected against deletion.\n–\nGetAPIRequest Request to retrieve an API.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the API. …","relpermalink":"/book/tsb/refs/tsb/application/v2/application-service/","summary":"Service to manage Applications and APis","title":"Application Service"},{"content":" Service to manage centralized approval policies.\nApprovals The Approvals service exposes methods for working with approval policies. $hide_from_yaml\nSetPolicy rpc SetPolicy (tetrateio.api.tsb.q.v2.ApprovalPolicy ) returns (google.protobuf.Empty )\nRequires CreateApprovalPolicy, WriteApprovalPolicy\nSetPolicy enables authorization policy checks for the given resource and applies any provided request or approval settings. If the resource has existing policies settings, they will be replaced. Once the policy is set, authorization checks will be performed for the given resource.\nGetPolicy rpc GetPolicy (tetrateio.api.tsb.q.v2.GetPolicyRequest ) returns (tetrateio.api.tsb.q.v2.ApprovalPolicy )\nRequires ReadApprovalPolicy\nGetPolicy returns the approval policy for the given resource.\nQueryPolicies rpc QueryPolicies (tetrateio.api.tsb.q.v2.QueryPoliciesRequest ) returns (tetrateio.api.tsb.q.v2.QueryPoliciesResponse )\nDeletePolicy rpc DeletePolicy (tetrateio.api.tsb.q.v2.DeletePolicyRequest ) returns (google.protobuf.Empty )\nRequires DeleteApprovalPolicy\nDeletePolicy deletes the approval policy configuration for the given resource. When deleted, authorization checks will no longer be performed, the resource will no longer accept approval requests and all existing approvals will be revoked.\nAddAccessRequest rpc AddAccessRequest (tetrateio.api.tsb.q.v2.AccessRequest ) returns (google.protobuf.Empty )\nRequires CreateApprovalPolicyAccessRequest, WriteApprovalPolicyAccessRequest\nAddAccessRequest adds a new access request entry in the access request list for the given resource. If the policy approval mode is “ALLOW_REQUESTED”, access is allowed immediately. If the policy approval mode is “REQUIRE_APPROVAL” access will be pending until the request is approved.\nDeleteAccessRequest rpc DeleteAccessRequest (tetrateio.api.tsb.q.v2.ResourceAndSubject ) returns (google.protobuf.Empty )\nRequires DeleteApprovalPolicyAccessRequest\nDeleteAccessRequest removes an existing entry from the access request list for the given resource. If the request is already approved, the request no longer exists and this operation will return NotFound. Deleting an approved request should be done using the DeleteApproved operation.\nApproveAccessRequest rpc ApproveAccessRequest (tetrateio.api.tsb.q.v2.AccessRequest ) returns (google.protobuf.Empty )\nRequires WriteApprovalPolicyApproveAccess\nApproveAccessRequest approves an existing access request for the given resource. Once approved, the request will be removed from the requested list and added to the approved list. If any of the permissions are changed, the requested permissions will be discarded and only the approved permissions will be added to the approved list.\nAddApprovedAccess rpc AddApprovedAccess (tetrateio.api.tsb.q.v2.AccessRequest ) returns (google.protobuf.Empty )\nRequires CreateApprovalPolicyApprovedAccess, WriteApprovalPolicyApprovedAccess\nAddApprovedAccess adds a new entry in the approved access list for the given resource.\nDeleteApprovedAccess rpc DeleteApprovedAccess (tetrateio.api.tsb.q.v2.ResourceAndSubject ) returns (google.protobuf.Empty )\nRequires DeleteApprovalPolicyApprovedAccess\nDeleteApprovedAccess deletes an entry from the approved list for the given resource.\nAccess Access is an access request for a subject with a set of permission.\nExample: Access { Subject: “organizations/demo/tenants/demo/applications/caller”, Permissions: []string{“GET”} }\nField Description Validation Rule subject\nstring REQUIRED Subject is the subject that is requested to access the resource.\nstring = { min_len: 1}\npermissions\nList of string REQUIRED Permissions is a list of permissions that the subject is allowed to use.\nrepeated = { min_items: 1 items: {string:{min_len:1}}}\nmetadata\ntetrateio.api.tsb.q.v2.Metadata Metadata is additional information about this Access entity.\n–\nAccessRequest AccessRequest is a request used for requesting or approving access to a resource.\nExample: AccessRequest { Resource: “organizations/demo/tenants/demo/applications/target”, Access: []Access{{ Subject: “organizations/demo/tenants/demo/applications/calling-app”, Permissions: []string{“GET”, “POST”} }} }\nField Description Validation Rule resource\nstring REQUIRED Resource for which the access request is made.\nstring = { min_len: 1}\naccess\ntetrateio.api.tsb.q.v2.Access REQUIRED Access is the subject and permissions for the access request.\n–\nApprovalPolicy ApprovalPolicy is a set of authorization rules that define access to a resource. When applied to a resource, the rules enforce access to the resource based on the permission set.\nExample: ApprovalPolicy { Mode: ApprovalPolicy_REQUIRE_APPROVAL, Resource: “organizations/demo/tenants/demo/applications/target-app”, Approved: []Access {{ Subject: “organizations/demo/tenants/demo/applications/calling-app”, Permissions: []string{“GET”, “POST”} }} }\nField Description Validation Rule mode\ntetrateio.api.tsb.q.v2.ApprovalPolicy.Mode REQUIRED Mode indicates how access to the resource …","relpermalink":"/book/tsb/refs/tsb/q/v2/approvals-service/","summary":"Service to manage centralized approval policies.","title":"Approvals Service"},{"content":" Audit Log Service\nAuditService The Audit Service provides access to the Service Bridge audit log APIs.\nAll operations performed against TSB resources generate audit log events that can be queried using the Audit log APIs. Those events include information about the users that performed each action and about the actions themselves.\nThis API is integrated with the TSB permission system, and all its methods will only return audit logs for those resources the users making the queries have permissions on.\nListAuditLogs rpc ListAuditLogs (tetrateio.api.audit.v1.ListAuditLogsRequest ) returns (tetrateio.api.audit.v1.ListAuditLogsResponse )\nList audit logs. If no ‘count’ parameter has been specified, the last 25 audit logs are returned. This method will only return audit logs for those resources the user making the query has permissions on.\nAuditLog AuditLog\nA system log describing something that happened in the system.\nField Description Validation Rule createTime\ngoogle.protobuf.Timestamp Time when the audit log was generated.\ntimestamp = { required: true}\nseverity\nstring Log severity (INFO, WARN, ERROR…).\nstring = { min_len: 1}\nkind\nstring The kind of the audit log (PolicyAssigned, ServiceOrphaned, etc).\nstring = { min_len: 1}\nmessage\nstring Audit log details.\nstring = { min_len: 1}\ntriggeredBy\nstring Person who triggered the audit log, or “SYSTEM” if the log was automatically triggered by the system.\nstring = { min_len: 1}\nproperties\nmap\u0026lt;string , string \u0026gt; Key value pairs with additional information for the audit log.\n–\nfqn\nstring Fully-qualified name of object that made this record.\n–\noperation\nstring Operation that was performed on the resource.\n–\nListAuditLogsRequest Request to get the audit logs.\nField Description Validation Rule count\nint32 Number of audit logs to retrieve. By default is 25.\n–\nsinceTimestamp\ngoogle.protobuf.Timestamp Moment in time since we retrieve logs.\n–\nseverity\nstring Severity level to filter logs.\n–\nkind\nstring The kind of the audit log to filter (PolicyAssigned, ServiceOrphaned, etc).\n–\ntriggeredBy\nstring Filter by what triggered the event.\n–\ntext\nstring Text to filter by.\n–\nrecursive\nbool If set to true, the audit log search will include the logs for all child resources for the one configured in the fqn field\n–\noperation\nstring Operation that was performed on the resource.\n–\nListAuditLogsResponse The list of audit logs.\nField Description Validation Rule auditLogs\nList of tetrateio.api.audit.v1.AuditLog –\n","relpermalink":"/book/tsb/refs/audit/v1/audit/","summary":"Audit Log Service","title":"Audit"},{"content":" Authentication and authorization configs at gateways, security group level\nAuthentication Field Description Validation Rule jwt\ntetrateio.api.tsb.auth.v2.Authentication.JWT oneof authn Authenticate an HTTP request from a JWT Token attached to it.\n–\nrules\ntetrateio.api.tsb.auth.v2.Authentication.Rules oneof authn List of rules how to authenticate an HTTP request.\n–\nJWT Field Description Validation Rule issuer\nstring REQUIRED Identifies the issuer that issued the JWT. See issuer A JWT with different iss claim will be rejected.\nExample: https://foobar.auth0.com Example: 1234567-compute@developer.gserviceaccount.com string = { min_len: 1}\naudiences\nList of string The list of JWT audiences . that are allowed to access. A JWT containing any of these audiences will be accepted.\nThe service name will be accepted if audiences is empty.\n–\njwksUri\nstring oneof keys URL of the provider’s public key set to validate signature of the JWT. See OpenID Discovery .\nOptional if the key set document can either (a) be retrieved from OpenID Discovery of the issuer or (b) inferred from the email domain of the issuer (e.g. a Google service account).\nExample: https://www.googleapis.com/oauth2/v1/certs\nNote: Only one of jwks_uri and jwks should be used. jwks_uri will be ignored if it does.\n–\njwks\nstring oneof keys JSON Web Key Set of public keys to validate signature of the JWT. See https://auth0.com/docs/jwks .\nNote: Only one of jwks_uri and jwks should be used. jwks_uri will be ignored if it does.\n–\noutputPayloadToHeader\nstring This field specifies the header name to output a successfully verified JWT payload to the backend. The forwarded data is base64_encoded(jwt_payload_in_JSON). If it is not specified, the payload will not be emitted.\n–\noutputClaimToHeaders\nList of tetrateio.api.tsb.auth.v2.Authentication.JWT.ClaimToHeader This field specifies a list of operations to copy the claim to HTTP headers on a successfully verified token. This differs from the output_payload_to_header by allowing outputting individual claims instead of the whole payload. Only claims of type string, boolean, and integer are supported. Array type claims are not supported at this time. The header specified in each operation in the list must be unique. Nested claims of type string/int/bool is supported as well.\noutputClaimToHeaders: - header: x-my-company-jwt-group claim: my-group - header: x-test-environment-flag claim: test-flag - header: x-jwt-claim-group claim: nested.key.group [Experimental] This feature is a experimental feature.\n[TODO:Update the status whenever this feature is promoted.]\n–\nfromHeaders\nList of tetrateio.api.tsb.auth.v2.Authentication.JWT.JWTHeader This field specifies the locations to extract JWT token. If no explicit location is specified the following default locations are tried in order:\n1) The Authorization header using the Bearer schema, e.g. Authorization: Bearer \u0026lt;token\u0026gt;. (see [Authorization Request Header Field](https://tools.ietf.org/html/rfc6750#section-2.1)) 2) The `access_token` query parameter (see [URI Query Parameter](https://tools.ietf.org/html/rfc6750#section-2.3)) List of header locations from which JWT is expected. For example, below is the location spec if JWT is expected to be found in x-jwt-assertion header, and have Bearer prefix:\nfromHeaders: - name: x-jwt-assertion prefix: \u0026#34;Bearer \u0026#34; Note: Multiple tokens present on the same request are not supported. The behaviour of authorization policies when there is more than one user identity is undefined\n–\nClaimToHeader This message specifies the detail for copying claim to header.\nField Description Validation Rule header\nstring REQUIRED The name of the header to be created. The header will be overridden if it already exists in the request.\nstring = { min_len: 1}\nclaim\nstring REQUIRED The name of the claim to be copied from. Only claim of type string/int/bool is supported. The header will not be there if the claim does not exist or the type of the claim is not supported.\nstring = { min_len: 1}\nJWTHeader This message specifies a header location to extract JWT token.\nField Description Validation Rule name\nstring REQUIRED The HTTP header name.\n–\nprefix\nstring The prefix that should be stripped before decoding the token. For example, for Authorization: Bearer \u0026lt;token\u0026gt;, prefix=Bearer with a space at the end. If the header doesn’t have this exact prefix, it is considered invalid.\n–\nRules Field Description Validation Rule jwt\nList of tetrateio.api.tsb.auth.v2.Authentication.JWT List of rules how to authenticate an HTTP request from a JWT Token attached to it. A JWT Token, if present in the HTTP request, must satisfy one of the rules defined here. The order in which rules are being checked at runtime might differ from the order in which they are defined here. If the JWT Token doesn’t satisfy any of the rules, the request will be rejected. If the JWT Token does satisfy one of the rules, the identity of the request will be extracted from the JWT Token.\nNotice that an HTTP request …","relpermalink":"/book/tsb/refs/tsb/auth/v2/auth/","summary":"Authentication and authorization configs at gateways, security group level","title":"Auth"},{"content":" Authentication and authorization configs at gateways\n","relpermalink":"/book/tsb/refs/tsb/gateway/v2/auth/","summary":"Authentication and authorization configs at gateways","title":"Auth"},{"content":" AwsIdentity represents an AWS-specific identity of a workload.\nE.g.,\nAWS EC2 instance identity:\npartition: aws account: \u0026#39;123456789012\u0026#39; region: ca-central-1 zone: ca-central-1b ec2: instance_id: i-1234567890abcdef0 iam_role: name: example-role AWS ECS task identity:\npartition: aws account: \u0026#39;123456789012\u0026#39; region: ca-central-1 zone: ca-central-1b ecs: task_id: 16aeded318d842bb8226e5bc678cd446 cluster: bookinfo iam_role: name: example-role AwsIdentity AwsIdentity represents an AWS-specific identity of a workload.\nField Description Validation Rule partition\nstring REQUIRED AWS Partition.\nE.g., aws, aws-cn, aws-us-gov, etc.\nSee https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html string = { min_len: 1}\naccount\nstring REQUIRED AWS Account.\nE.g., 123456789012.\nSee https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html string = { pattern: ^[0-9]{12}$}\nregion\nstring REQUIRED AWS Region.\nE.g., us-east-2, eu-west-3, cn-north-1, etc.\nSee https://docs.aws.amazon.com/general/latest/gr/rande.html#regional-endpoints string = { min_len: 1}\nzone\nstring REQUIRED AWS Availability Zone.\nE.g., us-east-2a, eu-west-3b, ap-southeast-1c, etc.\nSee https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html string = { min_len: 1}\nec2\ntetrateio.api.onboarding.config.types.identity.aws.v1alpha1.Ec2Instance oneof kind AWS EC2 instance.\n–\nEc2Instance Ec2Instance represents AWS EC2 instance.\nField Description Validation Rule instanceId\nstring REQUIRED EC2 instance ID.\nE.g., i-1234567890abcdef0.\nSee https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-instances.html string = { min_len: 1}\niamRole\ntetrateio.api.onboarding.config.types.identity.aws.v1alpha1.IamRole AWS IAM Role associated with the AWS EC2 instance.\nSee https://docs.aws.amazon.com/cli/latest/reference/iam/add-role-to-instance-profile.html –\nIamRole IamRole represents AWS IAM Role.\nField Description Validation Rule name\nstring REQUIRED Role name.\nE.g., example-role.\nSee https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html string = { min_len: 1}\n","relpermalink":"/book/tsb/refs/onboarding/config/types/identity/aws/v1alpha1/aws/","summary":"AWS-specific identity of a workload.","title":"AWS Identity"},{"content":" AwsIdentityMatcher specifies matching workloads with AWS-specific identities.\nFor example, the following configuration will match any EC2 VM instance in account 123456789012, region ca-central-1 and zone ca-central-1b:\npartitions: - aws accounts: - \u0026#39;123456789012\u0026#39; regions: - ca-central-1 zones: - ca-central-1b ec2: {} The matcher can also be used to to limit to VMs associated with a specific IAM role as shown below:\npartitions: - aws accounts: - \u0026#39;123456789012\u0026#39; regions: - ca-central-1 zones: - ca-central-1b ec2: iamRoleNames: - example-role The following matcher will limit to ECS instances in the bookinfo cluster and with a specific IAM role:\npartitions: - aws accounts: - \u0026#39;123456789012\u0026#39; regions: - ca-central-1 zones: - ca-central-1b ecs: clusters: - prod-cluster iamRoleNames: - example-role AwsIdentityMatcher AwsIdentityMatcher specifies matching workloads with AWS-specific identities.\nField Description Validation Rule partitions\nList of string Match workloads in these AWS Partitions.\nE.g., aws, aws-cn, aws-us-gov, etc.\nEmpty list means match any partition.\nSee https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html repeated = { items: {string:{min_len:1}}}\naccounts\nList of string REQUIRED Match workloads in these AWS Accounts.\nE.g., 123456789012.\nCannot be empty.\nSee https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html repeated = { min_items: 1 items: {string:{pattern:^[0-9]{12}$}}}\nregions\nList of string Match workloads in these AWS Regions.\nE.g., us-east-2, eu-west-3, cn-north-1, etc.\nEmpty list means match any region.\nSee https://docs.aws.amazon.com/general/latest/gr/rande.html#regional-endpoints repeated = { items: {string:{min_len:1}}}\nzones\nList of string Match workloads in these AWS Availability Zones.\nE.g., us-east-2a, eu-west-3b, ap-southeast-1c, etc.\nEmpty list means match any availability zone.\nSee https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html repeated = { items: {string:{min_len:1}}}\nec2\ntetrateio.api.onboarding.authorization.aws.v1alpha1.Ec2InstanceMatcher oneof kind Match AWS EC2 instances with these instance specific criteria.\nIf present but empty, it matches any EC2 instance matching the other fields.\n–\nEc2InstanceMatcher Ec2Instance specifies matching AWS EC2 instances.\nField Description Validation Rule iamRoleNames\nList of string Match AWS EC2 instances associated with these AWS IAM Role names.\nE.g., example-role.\nEmpty list means match any EC2 instance (no matter whether it has an AWS IAM Role associated with it or not).\nSee https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html repeated = { items: {string:{min_len:1}}}\n","relpermalink":"/book/tsb/refs/onboarding/config/authorization/aws/v1alpha1/aws/","summary":"Specification of matching workloads with AWS-specific identities.","title":"AWS Identity Matcher"},{"content":" Service to manage clusters onboarded in TSB.\nClusters The Clusters service exposes methods to manage the registration of clusters that are managed by TSB. Before TSB can takeover networking for a given cluster, it must be onboarded in the platform. This onboarding process usually involves two steps:\nCreating the cluster object so the platform knows about it. Generate the agent tokens for the cluster, so the TSB agents installed in the actual cluster can talk to TSB. Once a cluster has been onboarded into TSB, it will start receiving configuration updates from the management plane, and the agents will keep the management updated with the status of the cluster.\nCreateCluster rpc CreateCluster (tetrateio.api.tsb.v2.CreateClusterRequest ) returns (tetrateio.api.tsb.v2.Cluster )\nRequires CREATE\nCreates a new cluster object in TSB. This is needed during cluster onboarding to let the management plane know about the existence of a cluster. Once a cluster has been created and fully onboarded, the management plane will manage the mesh for that cluster and keep this cluster entity up to date with the information that is reported by the cluster agents. This method returns the created cluster, that will be continuously updated by the local cluster agents. This entity can be monitored to have an overview of the resources (namespaces, services, etc) that are known to be running in the cluster.\nThis action will also create a service account with permissions to manage this cluster. This service account (aka cluster service account) can be used in the ControlPlane installation to authenticate it through the ManagementPlane.\nAs part of the response, a template will be provided (in the field installTemplate) with minimum configuration to be able to install the TSB Operator in the cluster running as ControlPlane. This data is not stored and will be only available in the response of this action.\nGetCluster rpc GetCluster (tetrateio.api.tsb.v2.GetClusterRequest ) returns (tetrateio.api.tsb.v2.Cluster )\nRequires READ\nGet the last known state for an onboarded cluster. Once a cluster has been onboarded into the platform, the agents will keep it up to date with its runtime status. Getting the cluster object will return the last known snapshot of existing namespaces and services running in it.\nUpdateCluster rpc UpdateCluster (tetrateio.api.tsb.v2.Cluster ) returns (tetrateio.api.tsb.v2.Cluster )\nRequires WRITE\nModify an existing cluster. Updates a cluster with the given data. Note that most of the data in the cluster is read-only and automatically populated by the local cluster agents.\nListClusters rpc ListClusters (tetrateio.api.tsb.v2.ListClustersRequest ) returns (tetrateio.api.tsb.v2.ListClustersResponse )\nGet the list of all clusters that have been onboarded into the platform.\nDeleteCluster rpc DeleteCluster (tetrateio.api.tsb.v2.DeleteClusterRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nUnregisters a cluster from the platform. Deleting a cluster will unregister it from the management plane, and the agents will stop receiving configuration updates. Agent tokens for the cluster are revoked as well, so agents that are still running will fail to report back cluster status to the management plane. Note that unregistering the cluster is a management plane only operation. This does not uninstall the agents from the local cluster. Agents will continue running and the services that are deployed in that cluster will be able to continue operating with the last applied configuration. Unregistering a cluster from the management plane should not generate downtime to services that are running on that cluster.\nGenerateTokens rpc GenerateTokens (tetrateio.api.tsb.v2.GenerateTokensRequest ) returns (tetrateio.api.tsb.v2.ClusterStatus )\nRequires WriteCluster\nGenerate the tokens for the cluster agents so they can talk to the management plane. Once a cluster object has been registered in the management plane, this method can be used to generate the JWT tokens that need to be configured in the local cluster agents in order to let them talk to the management plane. These tokens contain the necessary permissions to allow the agents to download the configuration for their cluster and to push cluster status updates to the management plane.\nCreateClusterRequest Request to create a cluster and register it in the management plane so configuration can be generated for it.\nField Description Validation Rule parent\nstring REQUIRED Parent resource where the cluster will be created. This is the FQN of the organization or the tenant.\nstring = { min_len: 1}\nname\nstring REQUIRED The short name for the resource to be created.\nstring = { min_len: 1}\ncluster\ntetrateio.api.tsb.v2.Cluster REQUIRED Details of the cluster to be created.\nmessage = { required: true}\nDeleteClusterRequest Request to delete a cluster.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the cluster.\nstring = { min_len: 1}\nGenerateTokensRequest Request to …","relpermalink":"/book/tsb/refs/tsb/v2/cluster-service/","summary":"Service to manage clusters onboarded in TSB.","title":"Cluster Service"},{"content":" Each Kubernetes cluster managed by Service Bridge should be onboarded first before configurations can be applied to the services in the cluster. Onboarding a cluster is a two step process. First, create a cluster object under the appropriate tenant. Once a cluster object is created, its status field should provide the set of join tokens that will be used by the Service Bridge agent on the cluster to talk to Service Bridge management plane. The second step is to deploy the Service Bridge agent on the cluster with the join tokens and deploy Istio on the cluster. The following example creates a cluster named c1 under the tenant mycompany, indicating that the cluster is deployed on a network “vpc-01” corresponding to the AWS VPC where it resides.\napiVersion: api.tsb.tetrate.io/v2 kind: Cluster metadata: name: c1 organization: myorg labels: env: uat-demo spec: tokenTtl: \u0026#34;1h\u0026#34; network: vpc-01 Note that configuration profiles such as traffic, security and gateway groups will flow to the Bridge agents in the cluster as long their requested cluster exists in the Service Bridge hierarchy.\nCluster A Kubernetes cluster managing both pods and VMs.\nField Description Validation Rule tokenTtl\ngoogle.protobuf.Duration Lifetime of the tokens. Defaults to 1hr.\n–\nnetwork\nstring The network (e.g., VPC) where this cluster is present. All clusters within the same network will be assumed to be reachable for the purposes of multi-cluster routing. In addition, networks marked as reachable from one another in SystemSettings will also be used for multi-cluster routing.\n–\ntier1Cluster\ngoogle.protobuf.BoolValue Indicates whether this cluster is hosting a tier1 gateway or not. Tier1 clusters cannot host other gateways or workloads. Defaults to false if not specified.\n–\nlocality\ntetrateio.api.tsb.v2.Locality Deprecated. For backward compatibility, still honoured but will be ignored in future releases, so better not to set it. Locality of the service endpoints will be dynamically discovered by the xcp-edge\nLocation information about the cluster which can be used for routing.\n–\ntrustDomain\nstring Trust domain for this cluster, used for multi-cluster routing. It must be unique for every cluster and should match the one configured in the local control plane. This value is optional, and will be updated by the local control plane agents. However, it is recommended to set it, if known, so that multi-cluster routing works without having to wait for the local control planes to update it.\n–\nnamespaceScope\ntetrateio.api.tsb.v2.NamespaceScoping Configure the default scoping of namespaces in this cluster.\n–\nstate\ntetrateio.api.tsb.v2.Cluster.State OUTPUT_ONLY –\nserviceAccount\ntetrateio.api.tsb.v2.ServiceAccount OUTPUT_ONLY The service account created with permissions to manage the current cluster. The service account is not stored and it is only returned in the ClusterCreate response.\n–\ninstallTemplate\ntetrateio.api.tsb.v2.Cluster.InstallTemplate OUTPUT_ONLY Template to be used to install this TSB cluster in the k8s cluster\n–\nInstallTemplate InstallTemplate provides templates ready to be used in the ControlPlane (cluster onboard) installation.\nField Description Validation Rule message\nstring OUTPUT_ONLY can provide useful information to the user\n–\nhelm\ntetrateio.api.install.helm.controlplane.v1alpha1.Values OUTPUT_ONLY valid values.yaml to be used with controlplane helm chart. This field is an alpha API, so future versions could include breaking changes.\n–\nState State represents the cluster info learned from the onboarded cluster\nField Description Validation Rule lastSyncTime\ngoogle.protobuf.Timestamp last time xcp edge(cp) synced with central(mp) in the UTC format\n–\nprovider\nstring cluster provider. Ex: GKE, EKS, AKS\n–\nistioVersions\nList of string This shows currently running istio versions in the cluster.\n–\nxcpVersion\nstring xcp-edge version which is running at the cluster\n–\ntsbCpVersion\nstring TSB controlplane version\n–\ndiscoveredLocality\ntetrateio.api.tsb.v2.Locality Discovered locality is the locality/region of the cluster as discovered by the xcp from the k8s endpoints\n–\nClusterStatus The status message for a cluster resource contains the set of join tokens that should be used by Service Bridge’s agents on the cluster.\nField Description Validation Rule tokens\nmap\u0026lt;string , string \u0026gt; Tokens for various agents.\n–\nLocality The region the cluster resides. Used for failover based routing when configured in the workspace or global settings.\nField Description Validation Rule region\nstring REQUIRED The geographic location of the cluster.\nstring = { min_len: 1}\nNamespaceScoping Configure the default scoping of namespaces in this cluster.\nField Description Validation Rule scope\ntetrateio.api.tsb.v2.NamespaceScoping.Scope Default scope for namespaces in this cluster (global, local)\n–\nexceptions\nList of string Namespaces to be excluded form the default scope. If the scope is set to global, this list will contain namespaces that are considered local. If the …","relpermalink":"/book/tsb/refs/tsb/v2/cluster/","summary":"Configuration for onboarding clusters.","title":"Clusters"},{"content":" Common configuration objects shared by the different install APIs.\nCertManagerSettings CertManagerSettings represents the settings used for the cert-manager installation. TSB supports installing and managing the lifecycle of the cert-manager installation.\nField Description Validation Rule managed\ntetrateio.api.install.common.CertManagerSettings.Managed Managed specifies whether TSB should manage the lifecycle of cert-manager.\n–\ncertManagerSpec\ntetrateio.api.install.common.CertManagerSettings.CertManagerSpec Configure kubernetes specific settings for cert-manager.\n–\ncertManagerWebhookSpec\ntetrateio.api.install.common.CertManagerSettings.CertManagerWebhookSpec Configure kubernetes specific settings for cert-manager-webhook.\n–\ncertManagerCaInjector\ntetrateio.api.install.common.CertManagerSettings.CertManagerCAInjector Configure kubernetes specific settings for cert-manager-cainjector.\n–\ncertManagerStartupapicheck\ntetrateio.api.install.common.CertManagerSettings.CertManagerStartupAPICheck Configure kubernetes specific settings for cert-manager-startupapicheck. DEPRECATED. Startup API Check is disabled.\n–\nCertManagerCAInjector CertManagerCAInjector represents the settings used for cert-manager CAInjector installation in the clusters.\nField Description Validation Rule kubeSpec\ntetrateio.api.install.kubernetes.KubernetesComponentSpec Configure kubernetes specific settings for cert-manager-cainjector.\n–\nCertManagerSpec CertManagerSpec represents the settings used for cert-manager controller installation in the clusters.\nField Description Validation Rule kubeSpec\ntetrateio.api.install.kubernetes.KubernetesComponentSpec Configure kubernetes specific settings for cert-manager.\n–\nCertManagerStartupAPICheck CertManagerStartupAPICheck represents the settings used for cert-manager startup API check job installation in the clusters. DEPRECATED. StartupAPICheck is disabled.\nField Description Validation Rule kubeSpec\ntetrateio.api.install.kubernetes.KubernetesJobComponentSpec Configure kubernetes specific settings for cert-manager-startupapicheck.\n–\nCertManagerWebhookSpec CertManagerWebhookSpec represents the settings used for cert-manager Webhook installation in the clusters.\nField Description Validation Rule kubeSpec\ntetrateio.api.install.kubernetes.KubernetesComponentSpec Configure kubernetes specific settings for cert-manager-webhook.\n–\nConfigProtection ConfigProtection contains settings for enabling/disabling config protection over XCP created resources. Config protections are disabled by default. Example:\nconfigProtection: enableAuthorizedUpdateDeleteOnXcpConfigs: true enableAuthorizedCreateUpdateDeleteOnXcpConfigs: true authorizedUsers: - user1 - system:serviceaccount:ns1:serviceaccount-1 Field Description Validation Rule enableAuthorizedUpdateDeleteOnXcpConfigs\nbool When enabled, no other user or svc account except AuthorizedUsers would be allowed to delete or update the XCP/Istio API resources created by XCP.\n–\nenableAuthorizedCreateUpdateDeleteOnXcpConfigs\nbool When enabled, no other user or svc account except AuthorizedUsers would be allowed to create, delete or update the XCP/Istio API resources. This acts as a superset of the enableAuthorizedUpdateDeleteOnXcpConfigs.\n–\nauthorizedUsers\nList of string List of usernames of authorized users or svc accounts to create/update/delete XCP configs when config protection is enabled.\n–\nCustomCertProviderSettings CustomCertProviderSettings represents the settings used for the custom certificate provider. Users can configure the CSR signer required for certificate signing and point to the CA bundle to be used to validate the certificates.\nField Description Validation Rule csrSignerName\nstring REQUIRED Name of Kubernetes CSR signer to be used to sign the CSR request by different TSB components for internal purposes.\nstring = { min_len: 1}\ncaBundleSecretName\nstring REQUIRED Configure the CABundleSecretName to be used to verify the signed CSR request by different TSB components. If not specified, TSB would use the secret with the name ca-bundle-management-plane in the management plane namespace or ca-bundle-control-plane in the control plane namespace. The secret should contain the file ca.crt with the cert data.\nstring = { min_len: 1}\nGitOps The GitOps component configures the features that allow integrating the Management Plane and/or the Control Plane cluster with Continuous Deployment pipelines.\nField Description Validation Rule enabled\nbool The GitOps component is in beta and disabled by default. If Management and Control Planes are installed in the same cluster, Continuous Deployment Integration should only be enabled in one of both planes. However, if the GitOps component is enabled in both planes, only the Control Plane GitOps component will remain enabled. The Management Plane GitOps component will not be enabled, even though it is explicitly enabled.\n–\nreconcileInterval\ngoogle.protobuf.Duration Interval at which the reconcile process will run. The reconcile process …","relpermalink":"/book/tsb/refs/install/common/common-config/","summary":"Common TSB configurations shared between TSB components.","title":"Common Configuration Objects"},{"content":" Definition of objects shared by different APIs.\nConfigGenerationMetadata ConfigGenerationMetadata allows to setup extra metadata that will be added in the final Istio generated configurations. Like new labels or annotations. Defining the config generation metadata in tenancy resources (like organization, tenant, workspace or groups) works as default values for those configs that belong to it. Defining same config generation metadata in configuration resources (like ingress gateways, service routes, etc.) will replace the ones defined in the tenancy resources.\nField Description Validation Rule labels\nmap\u0026lt;string , string \u0026gt; Set of key value paris that will be added into the metadata.labels field of the Istio generated configurations.\n–\nannotations\nmap\u0026lt;string , string \u0026gt; Set of key value paris that will be added into the metadata.annotations field of the Istio generated configurations.\n–\nNamespaceSelector NamespaceSelector selects a set of namespaces across one or more clusters in a tenant. Namespace selectors can be used at Workspace level to carve out a chunk of resources under a tenant into an isolated configuration domain. They can be used in a Traffic, Security, or a Gateway group to further scope the set of namespaces that will belong to a specific configuration group. Names in namespaces selector must be in the form cluster/namespace where:\ncluster must be a cluster name or an * to mean all clusters namespace must be a namespace name, an * to mean all namespaces or a prefix like ns-* to mean all those namespaces starting by ns- Field Description Validation Rule names\nList of string REQUIRED Under the tenant/workspace/group:\n*/ns1 implies ns1 namespace in any cluster.\nc1/ns1 implies ns1 namespace from c1 cluster.\nc1/* implies all namespaces in c1 cluster.\n*/* implies all namespaces in all clusters.\nc1/ns* implies all namespaces prefixes by ns in c1 cluster.\nrepeated = { min_items: 1 items: {string:{pattern:^(\\\\*|[^/*]+)/(\\\\*|[^/*]+\\\\*?)$}}}\nPortSelector PortSelector is the criteria for specifying if a policy can be applied to a listener having a specific port.\nField Description Validation Rule number\nuint32 REQUIRED Port number\nuint32 = { lte: 65535 gte: 1}\nRegionalFailover Specify the traffic failover policy across regions. Since zone and sub-zone failover is supported by default this only needs to be specified for regions when the operator needs to constrain traffic failover so that the default behavior of failing over to any endpoint globally does not apply. This is useful when failing over traffic across regions would not improve service health or may need to be restricted for other reasons like regulatory controls.\nField Description Validation Rule from\nstring Originating region.\n–\nto\nstring Destination region the traffic will fail over to when endpoints in the ‘from’ region become unhealthy.\n–\nServiceSelector ServiceSelector represents the match criteria to select services within a particular scope (namespace, workspace, cluster etc)\nField Description Validation Rule serviceLabels\nmap\u0026lt;string , string \u0026gt; REQUIRED One or more labels that indicate a specific set of services within a particular scope\nmap = { min_pairs: 1 keys: {string:{min_len:1}} values: {string:{min_len:1}}}\nTrafficSelector TrafficSelector provides a mechanism to select a specific traffic flow for which this Wasm Extension will be enabled. When all the sub conditions in the TrafficSelector are satisfied, the traffic will be selected.\nField Description Validation Rule mode\ntetrateio.api.tsb.types.v2.WorkloadMode Criteria for selecting traffic by their direction. Note that CLIENT and SERVER are analogous to OUTBOUND and INBOUND, respectively. For the gateway, the field should be CLIENT or CLIENT_AND_SERVER. If not specified, the default value is CLIENT_AND_SERVER.\nenum = { defined_only: true}\nports\nList of tetrateio.api.tsb.types.v2.PortSelector Criteria for selecting traffic by their destination port. More specifically, for the outbound traffic, the destination port would be the port of the target service. On the other hand, for the inbound traffic, the destination port is the port bound by the server process in the same Pod.\nIf one of the given ports is matched, this condition is evaluated to true. If not specified, this condition is evaluated to true for any port.\n–\nWasmExtensionAttachment WasmExtensionAttachment defines the WASM extension attached to this resource including the name to identify the extension and also the specific configuration that will override the global extension configuration. Only those extensions globally enabled will be considered although they can be associated to the target resources. Match configuration allows you to specify which traffic is sent through the Wasm extension. Users can select the traffic based on different workload modes and ports.\napiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: name: ingress-bookinfo group: g1 workspace: w1 tenant: mycompany organization: myorg spec: …","relpermalink":"/book/tsb/refs/tsb/types/v2/types/","summary":"Definition of objects shared by different APIs.","title":"Common Object Types"},{"content":" Condition contains details for one aspect of the current state of an API Resource.\nCondition Condition contains details for one aspect of the current state of an API Resource.\nField Description Validation Rule type\nstring REQUIRED Type of condition in CamelCase or in foo.example.com/CamelCase.\nstring = { min_len: 1}\nstatus\nstring REQUIRED Status of the condition, one of True, False, Unknown.\nstring = { in: True,False,Unknown}\nreason\nstring REQUIRED Reason contains a programmatic identifier indicating the reason for the condition’s last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string.\nstring = { min_len: 1}\nmessage\nstring Message is a human readable message indicating details about the transition.\n–\nlastTransitionTime\ngoogle.protobuf.Timestamp LastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable.\n–\nobservedGeneration\nint64 ObservedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance.\n–\n","relpermalink":"/book/tsb/refs/onboarding/config/types/core/v1alpha1/condition/","summary":"Contains details for one aspect of the current state of an API Resource.","title":"Condition"},{"content":" ControlPlane resource exposes a set of configurations necessary to automatically install the Service Bridge control plane on a cluster. The installation API is an override API so any unset fields that aren’t required will use sensible defaults.\nPrior to creating the ControlPlane resource, a cluster needs to be created in the management plane. Control plane install scripts would create the following secrets in the Kubernetes namespace the control plane is deployed into. Make sure they exist:\noap-token otel-token If your Elasticsearch backend requires authentication, ensure you create the following secret:\nelastic-credentials A minimal resource must have the container registry hub, telemetryStore, and managementPlane fields set.\napiVersion: install.tetrate.io/v1alpha1 kind: ControlPlane metadata: name: controlplane namespace: istio-system spec: hub: docker.io/tetrate telemetryStore: elastic: host: elastic port: 5678 managementPlane: host: tsb.tetrate.io port: 8443 clusterName: cluster To configure infrastructure specific settings such as resource limits in Kubernetes, set the relevant field in a component. Remember that the installation API is an override API so if these fields are unset the operator will use sensible defaults. Only a subset of Kubernetes configuration is available and only for individual components.\napiVersion: install.tetrate.io/v1alpha1 kind: ControlPlane metadata: name: controlplane namespace: istio-system spec: hub: docker.io/tetrate imagePullSecrets: - name: my-registry-creds telemetryStore: elastic: host: elastic port: 5678 managementPlane: host: tsb.tetrate.io port: 8443 clusterName: cluster components: collector: kubeSpec: resources: limits: memory: 750Mi requests: memory: 500Mi ControlPlaneComponentSet The set of components that make up the control plane. Use this to override application settings or Kubernetes settings for each individual component.\nField Description Validation Rule collector\ntetrateio.api.install.controlplane.v1alpha1.OpenTelemetryCollector –\noap\ntetrateio.api.install.controlplane.v1alpha1.Oap –\nxcp\ntetrateio.api.install.controlplane.v1alpha1.XCP –\nistio\ntetrateio.api.install.controlplane.v1alpha1.Istio –\nrateLimitServer\ntetrateio.api.install.controlplane.v1alpha1.RateLimitServer –\nhpaAdapter\ntetrateio.api.install.controlplane.v1alpha1.HpaAdapter –\nonboarding\ntetrateio.api.install.controlplane.v1alpha1.Onboarding Workload Onboarding.\n–\nsatellite\ntetrateio.api.install.controlplane.v1alpha1.Satellite Satellite provide load balancing capabilities for data content before the data from Envoy reaches the SPM in Control Plane. When envoy points the address to Satellite, it can load balance the traffic to the SPM service.\n–\nngac\ntetrateio.api.install.controlplane.v1alpha1.NGAC –\ngitops\ntetrateio.api.install.common.GitOps Configuration for the integration of the Control Plane with Continuous Deployment pipelines.\n–\ninternalCertProvider\ntetrateio.api.install.common.InternalCertProvider Configure the Kubernetes CSR certificate provider for TSB internal purposes like Webhook TLS certificates. This configuration is required for kubernetes version 1.22 and above.\n–\ndefaultKubeSpec\ntetrateio.api.install.kubernetes.KubernetesSpec Configure Kubernetes default settings for all components. These settings will be merged to all components’ settings, only if the component does not define the same setting. In that case, the setting defined at the component level prevails over the global default.\n–\nwasmfetcher\ntetrateio.api.install.controlplane.v1alpha1.WASMFetcher Configuration for the WASM Fetcher component.\n–\ndefaultLogLevel\nstring The default log level for all components if the per component log level config is not specified. Note that the supported log level for different components can be different.\n–\nControlPlaneSpec ControlPlaneSpec defines the desired installed state of control plane components. Specifying a minimal ControlPlaneSpec with hub, clusterName, and managementPlane set will create an installation with sensible defaults.\nField Description Validation Rule hub\nstring REQUIRED TSB container hub path e.g. docker.io/tetrate.\nstring = { min_len: 1}\nimagePullSecrets\nList of tetrateio.api.install.kubernetes.LocalObjectReference Pull secrets can be specified globally for all components, or defined into the kubeSpec.serviceAccount of every component if needed. In case both are defined, the most specific one (the one defined at the component) level is used.\nList of references to secrets in the same namespace to use for pulling any images in pods that reference this ServiceAccount. ImagePullSecrets are distinct from Secrets because Secrets can be mounted in the pod, but ImagePullSecrets are only accessed by the kubelet. More info: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#service_account-v1-core –\ncomponents …","relpermalink":"/book/tsb/refs/install/controlplane/v1alpha1/spec/","summary":"Configuration to describe a TSB control plane installation.","title":"Control Plane"},{"content":" Core types.\nNamespacedName NamespacedName specifies a namespace-scoped name.\nField Description Validation Rule namespace\nstring REQUIRED Namespace name.\nstring = { min_len: 1}\nname\nstring REQUIRED Resource name.\nstring = { min_len: 1}\n","relpermalink":"/book/tsb/refs/onboarding/config/types/core/v1alpha1/namespaced-name/","summary":"Reusable types.","title":"Core types"},{"content":" A minimal resource should have an empty spec.\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: bookinfo namespace: bookinfo spec: {} To configure infrastructure specific settings such as the service type, set the relevant field in kubeSpec. Remember that the installation API is an override API so if these fields are unset the operator will use sensible defaults. Only a subset of Kubernetes configuration is available.\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: bookinfo namespace: bookinfo spec: kubeSpec: service: type: NodePort EgressGateway and Tier1Gateway are configured in the same manner.\nEgressGatewaySpec EgressGatewaySpec defines the desired installed state of a single egress gateway for a given namespace in Service Bridge. Specifying a minimal EgressGatewaySpec with a hub will create a default gateway with sensible values.\nField Description Validation Rule connectionDrainDuration\ngoogle.protobuf.Duration The amount of time the gateway will wait on shutdown for connections to complete before terminating the gateway. During this drain period, no new connections can be created but existing ones are allowed complete.\n–\nkubeSpec\ntetrateio.api.install.kubernetes.KubernetesComponentSpec Configure Kubernetes specific settings.\n–\nrevision\nstring Specifies the istio revision to reconcile with. If specified, TSB control plane operator will reconcile this gateway only if operator’s revision matches with it. TSB data plane operator, which would be running only when TSB control plane operator is not configured a revision, will ignore revision field and will reconcile gateway as usual. Internally, this revision will guide to pick matching istio control plane for the gateway deployment https://istio.io/latest/blog/2020/multiple-control-planes/#configuring –\nIngressGatewaySpec IngressGatewaySpec defines the desired installed state of a single ingress gateway for a given namespace in Service Bridge. Specifying a minimal IngressGatewaySpec with a hub will create a default gateway with sensible values.\nField Description Validation Rule connectionDrainDuration\ngoogle.protobuf.Duration The amount of time the gateway will wait on shutdown for connections to complete before terminating the gateway. During this drain period, no new connections can be created but existing ones are allowed complete.\n–\nkubeSpec\ntetrateio.api.install.kubernetes.KubernetesComponentSpec Configure Kubernetes specific settings.\n–\nrevision\nstring Specifies the istio revision to reconcile with. If specified, TSB control plane operator will reconcile this gateway only if operator’s revision matches with it. TSB data plane operator, which would be running only when TSB control plane operator is not configured a revision, will ignore revision field and will reconcile gateway as usual. Internally, this revision will guide to pick matching istio control plane for the gateway deployment https://istio.io/latest/blog/2020/multiple-control-planes/#configuring –\neastWestOnly\nbool If set to true, the ingress gateway will be configured for east west routing only. This means that only port 15443 will be exposed.\n–\nTier1GatewaySpec Tier1GatewaySpec defines the desired installed state of a single tier 1 gateway for a given namespace in Service Bridge. Specifying a minimal Tier1GatewaySpec with a hub will create a default gateway with sensible values.\nField Description Validation Rule connectionDrainDuration\ngoogle.protobuf.Duration The amount of time the gateway will wait on shutdown for connections to complete before terminating the gateway. During this drain period, no new connections can be created but existing ones are allowed complete.\n–\nkubeSpec\ntetrateio.api.install.kubernetes.KubernetesComponentSpec Configure Kubernetes specific settings.\n–\nrevision\nstring Specifies the istio revision to reconcile with. If specified, TSB control plane operator will reconcile this gateway only if operator’s revision matches with it. TSB data plane operator, which would be running only when TSB control plane operator is not configured a revision, will ignore revision field and will reconcile gateway as usual. Internally, this revision will guide to pick matching istio control plane for the gateway deployment https://istio.io/latest/blog/2020/multiple-control-planes/#configuring –\n","relpermalink":"/book/tsb/refs/install/dataplane/v1alpha1/spec/","summary":"Configuration to describe components in the TSB data plane.","title":"Data Plane"},{"content":" Configuration for east/west gateway settings\nEastWestGateway EastWestGateway is for configuring a gateway to handle east-west traffic of the services that are not exposed through Ingress or Tier1 gateways (internal services). Currently, this is restricted to specifying at Workspace level in WorkspaceSetting.\nField Description Validation Rule workloadSelector\ntetrateio.api.tsb.types.v2.WorkloadSelector REQUIRED Specify the gateway workloads (pod labels and Kubernetes namespace) under the gateway group that should be configured with this gateway. There can be only one gateway for a workload selector in a namespace.\nmessage = { required: true}\nexposedServices\nList of tetrateio.api.tsb.types.v2.ServiceSelector Exposed services is used to specify the match criteria to select specific services for internal multicluster routing (east-west routing between clusters). If it is not defined or contains no elements, all the services within the workspace will be exposed to the configured gateway.\n–\nconfigGenerationMetadata\ntetrateio.api.tsb.types.v2.ConfigGenerationMetadata Metadata values that will be add into the Istio generated configurations. When using YAML APIs liketctl or gitops, put them into the metadata.labels or metadata.annotations instead. This field is only necessary when using gRPC APIs directly.\n–\n","relpermalink":"/book/tsb/refs/tsb/gateway/v2/eastwest-gateway/","summary":"Configuration for east/west gateway settings","title":"East/West Gateway"},{"content":" EgressGateway configures a workload to act as a gateway for traffic exiting the mesh. The egress gateway is meant to be the destination of unknown traffic within the mesh (traffic sent to non-mesh services). The gateway allows authorization control of traffic sent to it to more finely tune which services are allowed to send unknown traffic through the gateway. Only HTTP is supported at this time.\nThe following example declares an egress gateway running on pods in istio-system with the label app=istio-egressgateway. This gateway is setup to allow traffic from anywhere in the cluster to access www.httpbin.org and from the bookinfo details app specifically, you can access any external host. EgressGateways need to be paired with TrafficSettings in order to be usable. You must set the egress field in the TrafficSettings to point to the egress gateway and send traffic to port 15443. Once this is set up, mesh internal apps will send unknown traffic to the egress gateway over mTLS. The gateway will then decide whether to forward the traffic or not, and use one-way TLS for external calls.\napiVersion: gateway.tsb.tetrate.io/v2 kind: EgressGateway metadata: name: my-egress group: g1 workspace: w1 tenant: mycompany organization: myorg spec: workloadSelector: namespace: ns1 labels: app: istio-egressgateway authorization: - from: mode: WORKSPACE to: [\u0026#34;www.httpbin.org\u0026#34;] - from: mode: CUSTOM serviceAccounts: [\u0026#34;default/bookinfo-details\u0026#34;] to: [\u0026#34;*\u0026#34;] apiVersion: traffic.tsb.tetrate.io/v2 kind: TrafficSetting metadata: name: defaults group: t1 workspace: w1 tenant: mycompany organization: myorg spec: reachability: mode: CUSTOM hosts: - \u0026#34;./*\u0026#34; - \u0026#34;istio-system/*\u0026#34; egress: host: istio-system/istio-egressgateway.istio-system.svc.cluster.local The following example customizes the Extensions field to enable the execution of the specified WasmExtensions list and details custom properties for the execution of each extension.\napiVersion: gateway.tsb.tetrate.io/v2 kind: EgressGateway metadata: name: my-egress group: g1 workspace: w1 tenant: mycompany organization: myorg spec: workloadSelector: namespace: ns1 labels: app: istio-egressgateway authorization: - from: mode: WORKSPACE to: [\u0026#34;www.httpbin.org\u0026#34;] - from: mode: CUSTOM serviceAccounts: [\u0026#34;default/bookinfo-details\u0026#34;] to: [\u0026#34;*\u0026#34;] extension: - fqn: hello-world # fqn of imported extensions in TSB config: foo: bar EgressAuthorization EgressAuthorization is used to dictate which service accounts can access a set of external hosts\nField Description Validation Rule from\ntetrateio.api.tsb.security.v2.AuthorizationSettings The workloads or service accounts this authorization rule applies to. If not set, the rule applies to all workloads or service accounts.\n–\nto\nList of string REQUIRED The external hostnames the workload(s) described in this rule can access. Hosts cannot be specified more than once. Use “*” to allow access to any external host\nrepeated = { min_items: 1}\nEgressGateway EgressGateway configures a workload to act as an egress gateway in the mesh.\n–\u0026gt;\nField Description Validation Rule workloadSelector\ntetrateio.api.tsb.types.v2.WorkloadSelector REQUIRED Specify the gateway workloads (pod labels and Kubernetes namespace) under the gateway group that should be configured with this gateway. There can be only one gateway for a workload selector in a namespace.\nmessage = { required: true}\nauthorization\nList of tetrateio.api.tsb.gateway.v2.EgressAuthorization The description of which service accounts can access which hosts. If the list of authorization rules is empty, this egress gateway will deny all traffic.\n–\nextension\nList of tetrateio.api.tsb.types.v2.WasmExtensionAttachment Extensions specifies all the WasmExtensions assigned to this EgressGateway with the specific configuration for each extension. This custom configuration will override the one configured globally to the extension. Each extension has a global configuration including enablement and priority that will condition the execution of the assigned extensions.\n–\nconfigGenerationMetadata\ntetrateio.api.tsb.types.v2.ConfigGenerationMetadata Metadata values that will be add into the Istio generated configurations. When using YAML APIs liketctl or gitops, put them into the metadata.labels or metadata.annotations instead. This field is only necessary when using gRPC APIs directly.\n–\n","relpermalink":"/book/tsb/refs/tsb/gateway/v2/egress-gateway/","summary":"Configurations to build an egress gateway.","title":"Egress Gateway"},{"content":" The Gateway configuration combines the functionalities of both the existing Tier1Gateway and IngressGateway, providing a unified approach for configuring a workload as a gateway in the mesh. Each server within the Gateway is configured to route requests to either destination clusters or services, but configuring one server to route requests to a destination cluster and another server to route requests to a service is not supported. To ensure consistency and compatibility, the Gateway configuration requires that all servers within the gateway either forward traffic to other clusters, similar to a Tier1Gateway, or route traffic to specific services, similar to an IngressGateway.\nThe following example declares a gateway running on pods with app: gateway labels in the ns1 namespace. The gateway exposes a host bookinfo.com on https port 9443 and http port 9090. The port 9090 is configured to receive plaintext traffic and send a redirect to the https port 9443 (site-wide HTTP -\u0026gt; HTTPS redirection). At port 9443, TLS is terminated using the certificates in the Kubernetes secret bookinfo-certs. Clients are authenticated using JWT tokens, whose keys are obtained from the OIDC provider www.googleapis.com. The request is then authorized by an the user’s authorization engine hosted at https://company.com/authz before being forwarded to the productpage service in the backend. Here, the gateway is configured in a manner similar to an existing IngressGateway with HTTP server.\napiVersion: gateway.tsb.tetrate.io/v2 kind: Gateway metadata: name: ingress-bookinfo group: g1 workspace: w1 tenant: mycompany organization: myorg spec: workloadSelector: namespace: ns1 labels: app: gateway http: - name: bookinfo-plaintext port: 9090 hostname: bookinfo.com routing: rules: - redirect: authority: bookinfo.com port: 9443 redirectCode: 301 scheme: https - name: bookinfo port: 9443 hostname: bookinfo.com tls: mode: SIMPLE secretName: bookinfo-certs authentication: rules: jwt: - issuer: https://accounts.google.com jwksUri: https://www.googleapis.com/oauth2/v3/certs - issuer: \u0026#34;auth.mycompany.com\u0026#34; jwksUri: https://auth.mycompany.com/oauth2/jwks authorization: external: uri: https://company.com/authz includeRequestHeaders: - Authorization # forwards the header to the authorization service. routing: rules: - route: serviceDestination: host: ns1/productpage.ns1.svc.cluster.local rateLimiting: settings: rules: # Ratelimit at 10 requests/hour for clients with a remote address of 1.2.3.4 - dimensions: - remoteAddress: value: 1.2.3.4 limit: requestsPerUnit: 10 unit: HOUR # Ratelimit at 50 requests/minute for every unique value in the user-agent header - dimensions: - header: name: user-agent limit: requestsPerUnit: 50 unit: MINUTE # Ratelimit at 100 requests/second for every unique client remote address # with the HTTP requests having a GET method and the path prefix of /productpage - dimensions: - remoteAddress: value: \u0026#34;*\u0026#34; - header: name: \u0026#34;:path\u0026#34; value: prefix: /productpage - header: name: \u0026#34;:method\u0026#34; value: exact: \u0026#34;GET\u0026#34; limit: requestsPerUnit: 100 unit: SECOND If the productpage.ns1 service on Kubernetes has a ServiceRoute with multiple subsets and weights, the traffic will be split across the subsets accordingly.\nThe following example declares a gateway running on pods with app: gateway labels in the ns1 namespace. The gateway exposes host movieinfo.com on ports 8080, 8443 and kafka.internal on port 9000. Traffic for these hosts at the ports 8443 and 9000 are TLS terminated and forwarded over Istio mutual TLS to the ingress gateways hosting movieinfo.com host on clusters c3 for matching prefix v1 and c4 for matching v2, and the internal kafka.internal service in cluster c3 respectively. The server at port 8080 is configured to receive plaintext HTTP traffic and redirect to port 8443 with “Permanently Moved” (HTTP 301) status code. Here, the gateway is configured in a manner similar to an existing Tier1Gateway with external servers.\napiVersion: gateway.tsb.tetrate.io/v2 kind: Gateway metadata: name: tier1 group: g1 workspace: w1 tenant: mycompany organization: myorg spec: workloadSelector: namespace: ns1 labels: app: gateway http: - name: movieinfo-plain hostname: movieinfo.com # Plaintext and HTTPS redirect port: 8080 routing: rules: - redirect: authority: movieinfo.com port: 8443 redirectCode: 301 scheme: https uri: \u0026#34;/\u0026#34; - name: movieinfo hostname: movieinfo.com # TLS termination and Istio mTLS to upstream port: 8443 tls: mode: SIMPLE secretName: movieinfo-secrets routing: rules: - match: - uri: prefix: \u0026#34;/v1\u0026#34; route: clusterDestination: clusters: - name: c3 # the target gateway IPs will be automatically determined weight: 100 - match: - uri: prefix: \u0026#34;/v2\u0026#34; route: clusterDestination: clusters: - name: c4 # the target gateway IPs will be automatically determined weight: 100 authentication: rules: jwt: - issuer: \u0026#34;auth.mycompany.com\u0026#34; jwksUri: https://auth.mycompany.com/oauth2/jwks - issuer: \u0026#34;auth.othercompany.com\u0026#34; jwksUri: …","relpermalink":"/book/tsb/refs/tsb/gateway/v2/gateway/","summary":"Configurations to build a gateway for traffic entering into the mesh.","title":"Gateway"},{"content":" DEPRECATED: use Access Bindings instead.\nGatewayAccessBindings is an assignment of roles to a set of users or teams to access resources under a Gateway group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a GatewayAccessBinding can be created or modified only by users who have SET_POLICY permission on the Gateway group.\nThe following example assigns the gateway-admin role to users alice, bob, and members of the gateway-ops team for all the gateways in group g1 under workspace w1 owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team\napiVersion: rbac.tsb.tetrate.io/v2 kind: GatewayAccessBindings metadata: organization: myorg tenant: mycompany workspace: w1 group: g1 spec: allow: - role: rbac/gateway-admin subjects: - user: organization/myorg/users/alice - user: organization/myorg/users/bob - team: organization/myorg/teams/gateway-ops GatewayAccessBindings GatewayAccessBindings assigns permissions to users of gateway groups.\nField Description Validation Rule allow\nList of tetrateio.api.tsb.rbac.v2.Binding The list of allowed bindings configures the different access profiles that are allowed on the resource configured by the policy.\n–\n","relpermalink":"/book/tsb/refs/tsb/rbac/v2/gateway-access-bindings/","summary":"Configuration for assigning access roles to users of gateway groups.","title":"Gateway Access Bindings"},{"content":" Configurations used to build gateways.\nClusterDestination Field Description Validation Rule name\nstring The name of the destination cluster. Only one of name or labels must be specified.\n–\nlabels\nmap\u0026lt;string , string \u0026gt; Labels associated with the cluster. Any cluster with matching labels will be selected as a target. Only one of name or labels must be specified.\n–\nnetwork\nstring The network associated with the destination clusters. In addition to name/label selectors, only clusters matching the selected networks will be used as a target. At least one of name/labels, and/or network must be specified.\nDeprecated: The network field is deprecated and will be removed in future releases. Only labels matching against the cluster object is supported.\n–\nweight\nuint32 The weight for traffic to a given destination.\n–\nCorsPolicy Field Description Validation Rule allowOrigin\nList of string The list of origins that are allowed to perform CORS requests. The content will be serialized into the Access-Control-Allow-Origin header. Wildcard * will allow all origins.\n–\nallowMethods\nList of string List of HTTP methods allowed to access the resource. The content will be serialized into the Access-Control-Allow-Methods header.\n–\nallowHeaders\nList of string List of HTTP headers that can be used when requesting the resource. Serialized to Access-Control-Allow-Headers header.\n–\nexposeHeaders\nList of string A white list of HTTP headers that the browsers are allowed to access. Serialized into Access-Control-Expose-Headers header.\n–\nmaxAge\ngoogle.protobuf.Duration Specifies how long the results of a preflight request can be cached. Translates to the Access-Control-Max-Age header.\n–\nallowCredentials\ngoogle.protobuf.BoolValue Indicates whether the caller is allowed to send the actual request (not the preflight) using credentials. Translates to Access-Control-Allow-Credentials header.\n–\nExternalRateLimitServiceSettings Configuration for ratelimiting using an external ratelimit server The ratelimit server must expose Envoy’s Rate Limit Service gRPC API .\nIf the rate limit service is called, and the response for any of the descriptors is over limit, a 429 response is returned. The rate limit filter also sets the x-envoy-ratelimited header.\nIf there is an error in calling rate limit service or rate limit service returns an error and failure_mode_deny is set to true, a 500 response is returned.\nField Description Validation Rule domain\nstring REQUIRED The rate limit domain to use when calling the rate limit service. Ratelimit settings are namespaced to a domain.\nstring = { min_bytes: 1}\nfailClosed\nbool If the rate limit service is unavailable, the request will fail if failClosed is set to true. Defaults to false.\n–\nrateLimitServerUri\nstring REQUIRED The URI at which the external rate limit server can be reached.\nstring = { min_bytes: 1}\nrules\nList of tetrateio.api.tsb.gateway.v2.ExternalRateLimitServiceSettings.RateLimitRule REQUIRED A set of rate limit rules. Each rule describes a list of dimension to match on. Once matched, a list of descriptors are sent to the external rate limit server\nrepeated = { min_items: 1}\ntimeout\ngoogle.protobuf.Duration The timeout in seconds for the external rate limit server RPC. Defaults to 0.020 seconds (20ms). Traffic will not be allowed to the destination if failClosed is set to true and the request to the rate limit server times out.\n–\ntls\ntetrateio.api.tsb.auth.v2.ClientTLSSettings Configure TLS parameters to be used when connecting to the external rate limit server. By default, the client will not validate the certificates it is presented with.\n–\nRateLimitDimension RateLimitDimension is a set of conditions to match HTTP requests Once the conditions are satisfied, corresponding descriptors (set of keys and values) are emitted and sent to the external rate limit server. The server is expected to make a rate limit decision based on these descriptors. Please go through the Envoy RateLimit descriptor to get more information on descriptors\nField Description Validation Rule sourceCluster\ntetrateio.api.tsb.gateway.v2.ExternalRateLimitServiceSettings.RateLimitDimension.SourceCluster oneof dimension_specifier Rate limit on source envoy cluster.\n–\ndestinationCluster\ntetrateio.api.tsb.gateway.v2.ExternalRateLimitServiceSettings.RateLimitDimension.DestinationCluster oneof dimension_specifier Rate limit on destination envoy cluster.\n–\nremoteAddress\ntetrateio.api.tsb.gateway.v2.ExternalRateLimitServiceSettings.RateLimitDimension.RemoteAddress oneof dimension_specifier Rate limit on remote address of client.\n–\nrequestHeaders\ntetrateio.api.tsb.gateway.v2.ExternalRateLimitServiceSettings.RateLimitDimension.RequestHeaders oneof dimension_specifier Rate limit on the value of certain request headers.\n–\nheaderValueMatch\ntetrateio.api.tsb.gateway.v2.ExternalRateLimitServiceSettings.RateLimitDimension.HeaderValueMatch oneof dimension_specifier Rate limit on the existence of certain request headers.\n–\nHeaderValueMatch Emit …","relpermalink":"/book/tsb/refs/tsb/gateway/v2/gateway-common/","summary":"Configurations used to build gateways.","title":"Gateway Common Configuration Messages"},{"content":" Gateway Groups allow grouping the gateways in a set of namespaces owned by its parent workspace. Gateway related configurations can then be applied on the group to control the behavior of these gateways. The group can be in one of two modes: BRIDGED and DIRECT. BRIDGED mode is a minimalistic mode that allows users to quickly configure the most commonly used features in the service mesh using Tetrate specific APIs, while the DIRECT mode provides more flexibility for power users by allowing them to configure the gateways’s traffic and security properties using a restricted subset of Istio Networking and Security APIs.\nThe following example creates a gateway group for the gateways in ns1, ns2 and ns3 namespaces owned by its parent workspace w1 under tenant mycompany\napiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: name: g1 workspace: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \u0026#34;*/ns1\u0026#34; - \u0026#34;*/ns2\u0026#34; - \u0026#34;*/ns3\u0026#34; configMode: BRIDGED It is possible to create a gateway group for namespaces in a specific cluster as long as the parent workspace owns those namespaces in that cluster. For example,\napiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: name: g1 workspace: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \u0026#34;c1/ns1\u0026#34; # pick ns1 namespace only from c1 cluster - \u0026#34;*/ns2\u0026#34; - \u0026#34;*/ns3\u0026#34; configMode: BRIDGED In the DIRECT mode, it is possible to directly attach Istio Networking v1beta1 APIs - VirtualService, and Gateway, and Istio Security v1beta1 APIs - RequestAuthentication, and AuthorizationPolicy to the gateway group. These configurations will be validated for correctness and conflict free operations and then pushed to the appropriate Istio control planes.\nThe following example declares a Gateway and a VirtualService for a specific workload in the ns1 namespace:\napiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: ingress namespace: ns1 annotations: tsb.tetrate.io/organization: myorg tsb.tetrate.io/tenant: mycompany tsb.tetrate.io/workspace: w1 tsb.tetrate.io/gatewayGroup: g1 spec: selector: app: my-ingress-gateway servers: - port: number: 80 name: http protocol: HTTP hosts: - uk.bookinfo.com - eu.bookinfo.com and the associated VirtualService\napiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: ingress-rule namespace: ns1 annotations: tsb.tetrate.io/organization: myorg tsb.tetrate.io/tenant: mycompany tsb.tetrate.io/workspace: w1 tsb.tetrate.io/gatewayGroup: g1 spec: hosts: - uk.bookinfo.com - eu.bookinfo.com gateways: - ns1/ingress # Has to bind to the same gateway http: - route: - destination: port: number: 7777 host: reviews.ns1.svc.cluster.local The namespace where the Istio APIs are applied will need to be part of the parent gateway group. In addition, each API object will need to have annotations to indicate the organization, tenant, workspace and the gateway group to which it belongs to.\nGroup A gateway group manages the gateways in a group of namespaces owned by the parent workspace.\nField Description Validation Rule namespaceSelector\ntetrateio.api.tsb.types.v2.NamespaceSelector REQUIRED Set of namespaces owned exclusively by this group. If omitted, applies to all resources owned by the workspace. Use */* to claim all cluster resources under the workspace.\nmessage = { required: true}\nconfigMode\ntetrateio.api.tsb.types.v2.ConfigMode The Configuration types that will be added to this group. BRIDGED mode indicates that configurations added to this group will use Tetrate APIs such as IngressGateway. DIRECT mode indicates that configurations added to this group will use Istio Networking v1beta1 APIs such as Gateway and VirtualService, Istio Security v1beta1 APIs such as RequestAuthentication and AuthorizationPolicy. Defaults to BRIDGED mode.\n–\ndeletionProtectionEnabled\nbool When set, prevents the resource from being deleted. In order to delete the resource this property needs to be set to false first.\n–\nconfigGenerationMetadata\ntetrateio.api.tsb.types.v2.ConfigGenerationMetadata Default metadata values that will be propagated to the children Istio generated configurations. When using YAML APIs liketctl or gitops, put them into the metadata.labels or metadata.annotations instead. This field is only necessary when using gRPC APIs directly.\n–\n","relpermalink":"/book/tsb/refs/tsb/gateway/v2/gateway-group/","summary":"Configurations to group a set of gateways in a workspace.","title":"Gateway Group"},{"content":" Service to manage the configuration for Gateways.\nGateways The Gateway service provides methods to manage gateway settings in TSB.\nIt provides methods to create and manage gateway groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the gateway configuration features.\nThe Gateway service also provides methods to configure the different gateway settings that are allowed within each group.\nCreateGroup rpc CreateGroup (tetrateio.api.tsb.gateway.v2.CreateGatewayGroupRequest ) returns (tetrateio.api.tsb.gateway.v2.Group )\nRequires CreateGatewayGroup\nCreate a new gateway group in the given workspace.\nGroups will by default configure all the namespaces owned by their workspace, unless explicitly configured. If a specific set of namespaces is set for the group, it must be a subset of the namespaces defined by its workspace.\nGetGroup rpc GetGroup (tetrateio.api.tsb.gateway.v2.GetGatewayGroupRequest ) returns (tetrateio.api.tsb.gateway.v2.Group )\nRequires ReadGatewayGroup\nGet the details of the given gateway group.\nUpdateGroup rpc UpdateGroup (tetrateio.api.tsb.gateway.v2.Group ) returns (tetrateio.api.tsb.gateway.v2.Group )\nRequires WriteGatewayGroup\nupdate the given gateway group.\nListGroups rpc ListGroups (tetrateio.api.tsb.gateway.v2.ListGatewayGroupsRequest ) returns (tetrateio.api.tsb.gateway.v2.ListGatewayGroupsResponse )\nList all gateway groups that exist in the workspace.\nDeleteGroup rpc DeleteGroup (tetrateio.api.tsb.gateway.v2.DeleteGatewayGroupRequest ) returns (google.protobuf.Empty )\nRequires DeleteGatewayGroup\nDelete the given gateway group. Note that deleting resources in TSB is a recursive operation. Deleting a gateway group will delete all configuration objects that exist in it.\nCreateIngressGateway rpc CreateIngressGateway (tetrateio.api.tsb.gateway.v2.CreateIngressGatewayRequest ) returns (tetrateio.api.tsb.gateway.v2.IngressGateway )\nRequires CREATE\nCreate an Ingress Gateway object in the gateway group.\nGetIngressGateway rpc GetIngressGateway (tetrateio.api.tsb.gateway.v2.GetIngressGatewayRequest ) returns (tetrateio.api.tsb.gateway.v2.IngressGateway )\nRequires READ\nGet the details of the given Ingress Gateway object.\nUpdateIngressGateway rpc UpdateIngressGateway (tetrateio.api.tsb.gateway.v2.IngressGateway ) returns (tetrateio.api.tsb.gateway.v2.IngressGateway )\nRequires WRITE\nModify the given Ingress Gateway object.\nListIngressGateways rpc ListIngressGateways (tetrateio.api.tsb.gateway.v2.ListIngressGatewaysRequest ) returns (tetrateio.api.tsb.gateway.v2.ListIngressGatewaysResponse )\nList all Ingress Gateway objects in the gateway group.\nDeleteIngressGateway rpc DeleteIngressGateway (tetrateio.api.tsb.gateway.v2.DeleteIngressGatewayRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given Ingress Gateway object.\nCreateEgressGateway rpc CreateEgressGateway (tetrateio.api.tsb.gateway.v2.CreateEgressGatewayRequest ) returns (tetrateio.api.tsb.gateway.v2.EgressGateway )\nRequires CREATE\nCreate an Egress Gateway object in the gateway group.\nGetEgressGateway rpc GetEgressGateway (tetrateio.api.tsb.gateway.v2.GetEgressGatewayRequest ) returns (tetrateio.api.tsb.gateway.v2.EgressGateway )\nRequires READ\nGet the details of the given Egress Gateway object.\nUpdateEgressGateway rpc UpdateEgressGateway (tetrateio.api.tsb.gateway.v2.EgressGateway ) returns (tetrateio.api.tsb.gateway.v2.EgressGateway )\nRequires WRITE\nModify the given Egress Gateway object.\nListEgressGateways rpc ListEgressGateways (tetrateio.api.tsb.gateway.v2.ListEgressGatewaysRequest ) returns (tetrateio.api.tsb.gateway.v2.ListEgressGatewaysResponse )\nList all Egress Gateway objects in the gateway group.\nDeleteEgressGateway rpc DeleteEgressGateway (tetrateio.api.tsb.gateway.v2.DeleteEgressGatewayRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given Egress Gateway object.\nCreateTier1Gateway rpc CreateTier1Gateway (tetrateio.api.tsb.gateway.v2.CreateTier1GatewayRequest ) returns (tetrateio.api.tsb.gateway.v2.Tier1Gateway )\nRequires CREATE\nCreate a Tier1 Gateway object in the gateway group.\nGetTier1Gateway rpc GetTier1Gateway (tetrateio.api.tsb.gateway.v2.GetTier1GatewayRequest ) returns (tetrateio.api.tsb.gateway.v2.Tier1Gateway )\nRequires READ\nget the details of the given Tier1 Gateway object.\nUpdateTier1Gateway rpc UpdateTier1Gateway (tetrateio.api.tsb.gateway.v2.Tier1Gateway ) returns (tetrateio.api.tsb.gateway.v2.Tier1Gateway )\nRequires WRITE\nListTier1Gateways rpc ListTier1Gateways (tetrateio.api.tsb.gateway.v2.ListTier1GatewaysRequest ) returns (tetrateio.api.tsb.gateway.v2.ListTier1GatewaysResponse )\nList all Tier1 Gateway objects that have been created in the gateway group.\nDeleteTier1Gateway rpc DeleteTier1Gateway (tetrateio.api.tsb.gateway.v2.DeleteTier1GatewayRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given Tier1 …","relpermalink":"/book/tsb/refs/tsb/gateway/v2/gateway-service/","summary":"Service to manage the configuration for Gateways.","title":"Gateway Service"},{"content":" Host Info specifies information about the host the workload is running on.\nAddress Address specifies network address.\nField Description Validation Rule ip\nstring REQUIRED IP address.\nstring = { ip: true}\ntype\ntetrateio.api.onboarding.config.types.registration.v1alpha1.AddressType REQUIRED Address type.\nenum = { not_in: 0}\nHostInfo HostInfo specifies information about the host the workload is running on.\nField Description Validation Rule addresses\nList of tetrateio.api.onboarding.config.types.registration.v1alpha1.Address REQUIRED Network addresses of the host the workload is running on.\nrepeated = { min_items: 1 items: {message:{required:true}}}\nAddressType AddressType specifies type of a network address associated with the workload.\nField Number Description UNSPECIFIED\n0\nNot specified.\nVPC\n1\nIP address from the VPC range. Commonly referred to as Private IP or Internal IP.\nINTERNET\n2\nIP address from the Internet range. Commonly referred to as Public IP or External IP.\n","relpermalink":"/book/tsb/refs/onboarding/config/types/registration/v1alpha1/hostinfo/","summary":"Information about the host the workload is running on.","title":"Host Info"},{"content":" IAM APIs for authentication.\nOAuth Token rpc Token (tetrateio.api.iam.v2.GrantRequest ) returns (tetrateio.api.iam.v2.GrantResponse )\nGrants tokens for a given grant type.\nThis is used by clients to obtain an access token by presenting required parameters for the requested grant type. Current only “urn:ietf:params:oauth:grant-type:device_code” is supported. When an error occurs, this will return a 4xx status code with an Error and ErrorMessage in the response.\nDeviceCode rpc DeviceCode (tetrateio.api.iam.v2.DeviceCodeRequest ) returns (tetrateio.api.iam.v2.DeviceCodeResponse )\nRequests device codes that can be used with a token grant with grant type “urn:ietf:params:oauth:grant-type:device_code”. For additional information please refer to the Device Authorization Grant RFC https://datatracker.ietf.org/doc/html/rfc8628 DeviceCodeResponse Response with device codes for use with the Device Authorization flow. For additional information on the response parameters please refer to the Device Authorization Response section of the RFC https://datatracker.ietf.org/doc/html/rfc8628#section-3.2 Field Description Validation Rule deviceCode\nstring Code that the device uses to poll for tokens\n–\nuserCode\nstring Code the user enters in the verification URI\n–\nverificationUri\nstring URI where to enter the user code\n–\ninterval\nint32 Rate in which to poll the token endpoint with the device code\n–\nexpiresIn\nint32 Expiration time of the device code in seconds\n–\nerror\ntetrateio.api.iam.v2.Error Optional error code presented when an error or validation check failed.\n–\nerrorMessage\nstring Optional error message that contains more details about the error that occurred.\n–\nGrantRequest Token grant request.\nField Description Validation Rule grantType\ntetrateio.api.iam.v2.GrantType REQUIRED Token grant type as specified in the OAuth2 specification. Current supported grant types are “urn:ietf:params:oauth:grant-type:device_code” and “refresh_token”\nenum = { defined_only: true}\ndeviceCode\nstring OPTIONAL Device code issued by the device authorization code endpoint when device code grant is used. This field is required when using a device_code grant.\n–\nrefreshToken\nstring OPTIONAL Refresh token issued from a previous grant request. This field is required when using a refresh_token grant.\n–\nscope\nList of string OPTIONAL List of requested scopes. This is a list that can include any of the scopes that are allowed by the client configuration. For refresh_token grants, this list may not include any scopes that were not part of the original token request.\n–\nclientId\nstring OPTIONAL Client ID for which the token grant request is being made. This is optional and when absent, TSB will use an appropriate client ID from configuration for the grant type being request. For a refresh grant type, this parameter may be required to ensure the appropriate client configuration is used.\n–\nresource\nstring OPTIONAL A URI that indicates the target service or resource where the client intends to use the requested token. This is used with the token exchange grant and should be the URI of TSB.\n–\nsubjectToken\nstring OPTIONAL A token that represents the identity of the party on behalf of whom the request is being made. This is used with the token exchange grant and should be either an ID Token or Access Token from the configured offline token grant client.\n–\nsubjectTokenType\ntetrateio.api.iam.v2.TokenType OPTIONAL An identifier that indicates the type of the security token in the “subject_token” parameter. This is used with the token exchange grant.\n–\nGrantResponse Token grant response.\nField Description Validation Rule accessToken\nstring Access token issued by the authorization server.\n–\ntokenType\nstring Access token type such as “bearer” or “mac”.\n–\nexpiresIn\nint32 Expiration time of the access token in seconds.\n–\nrefreshToken\nstring Optional refresh token issued when the authorization server and client are configured to use refresh tokens.\n–\nclientId\nstring Optional client ID used during the grant process. When present the client ID for subsequent refresh grant calls. While not a standard field on an OAuth grant response, this helps remove ambiguity when multiple OIDC configurations are present in TSB.\n–\nerror\ntetrateio.api.iam.v2.Error Optional error code presented when an error or validation check failed.\n–\nerrorMessage\nstring Optional error message that contains more details about the error that occurred.\n–\nError OAuth2 error codes\nField Number Description NO_ERROR\n0\nINVALID_REQUEST\n1\nINVALID_CLIENT\n2\nINVALID_GRANT\n3\nUNAUTHORIZED_CLIENT\n4\nUNSUPPORTED_GRANT_TYPE\n5\nAUTHORIZATION_PENDING\n6\nSLOW_DOWN\n7\nACCESS_DENIED\n8\nEXPIRED_TOKEN\n9\nSERVER_ERROR\n10\nGrantType OAuth2 grant types that are currently supported.\nField Number Description UNSPECIFIED\n0\nREFRESH_TOKEN\n1\nDEVICE_CODE_URN\n2\nCLIENT_CREDENTIALS\n3\nTOKEN_EXCHANGE\n4\nTokenType Field Number Description TOKEN_TYPE_UNSPECIFIED\n0\nTOKEN_TYPE_ACCESS_TOKEN\n1\nTOKEN_TYPE_REFRESH_TOKEN\n2\nTOKEN_TYPE_ID_TOKEN\n3\nTOKEN_TYPE_JWT …","relpermalink":"/book/tsb/refs/iam/v2/oauth-service/","summary":"IAM APIs for authentication.","title":"IAM (OAuth)"},{"content":" IAM APIs for authentication.\nOIDC The IAM OIDC service is a service used with Open ID Connect provider integrations.\nCallback rpc Callback (tetrateio.api.iam.v2.CallbackRequest ) returns (google.protobuf.Empty )\nCallback endpoint for OAuth2 Authorization Code grant flows as part of the OIDC spec.\nLogin rpc Login (tetrateio.api.iam.v2.LoginRequest ) returns (google.protobuf.Empty )\nLogin endpoint to start an OIDC Authentication flow.\nCallbackRequest Request with parameters for an OAuth2 Authorization Code grant redirect.\nField Description Validation Rule code\nstring oneof result OAuth2 Authorization Code. When present this indicates the user authorized the request. TSB will use this code to acquire a token from the OIDC token endpoint and complete the login flow.\n–\nerror\nstring oneof result OAuth2 Error Code. When present this indicates that either the authorization request has an error, the OIDC provider encountered an error or the user failed to log in. When set TSB will display information to the user indicating what went wrong.\nStandard error codes can be found found here. https://datatracker.ietf.org/doc/html/rfc6749#section-4.1.2.1 https://openid.net/specs/openid-connect-core-1_0.html#AuthError –\nstate\nstring REQUIRED The state parameter sent to the OIDC provider on the authorization request.\nstring = { min_len: 1}\nerrorDescription\nstring OPTIONAL Optional error description sent by the OIDC provider when an error occurs.\n–\nerrorUri\nstring OPTIONAL Optional error URI of a web page that includes additional information about the error.\n–\nLoginRequest Request to initiate an OIDC Authentication flow.\nField Description Validation Rule redirectUri\nstring OPTIONAL URl where the user will be redirected when the authentication flow completes.\n–\n","relpermalink":"/book/tsb/refs/iam/v2/oidc-service/","summary":"IAM APIs for authentication.","title":"IAM (OIDC)"},{"content":" Provide information about the Service bridge platform.\nInfo The Info service provides information about the service Bridge platform.\nGetVersion rpc GetVersion (tetrateio.api.tsb.v2.GetVersionRequest ) returns (tetrateio.api.tsb.v2.Version )\nGetVersion returns the version of the TSB binary\nGetCurrentUser rpc GetCurrentUser (tetrateio.api.tsb.v2.GetCurrentUserRequest ) returns (tetrateio.api.tsb.v2.CurrentUser )\nGetCurrentUser returns the information of the user or service account that made the request.\nCurrentUser CurrentUser contains the information of the user or service account that made the request.\nField Description Validation Rule loginName\nstring login_name is the name used in the login credentials.\n–\ntype\ntetrateio.api.tsb.v2.CurrentUser.Type The type of the current user, e.g. USER or SERVICE_ACCOUNT\n–\nsourceType\nstring Indicates the Identity Provider where the user has been synchronized from. It will be empty for service accounts.\n–\nemail\nstring The email for the user where alerts and other notifications will be sent. It will be empty for service accounts.\n–\ndisplayName\nstring The display name is the user friendly name for the resource. It will be empty for service accounts.\n–\nfirstName\nstring The first name is the first name of the user. It will be empty for service accounts.\n–\nlastName\nstring The last name of the user, if any. It will be empty for service accounts.\n–\norganization\nstring The name of the organization the user belongs to\n–\nVersion The version of the Service Bridge platform.\nField Description Validation Rule version\nstring version is the TSB binary version\n–\nType Field Number Description USER\n0\nSERVICE_ACCOUNT\n1\nINTERNAL\n2\n","relpermalink":"/book/tsb/refs/tsb/v2/info/","summary":"Provide information about the Service bridge platform.","title":"Info"},{"content":" DEPRECATION: The functionality provided by the IngressGateway is now provided in Gateway object, and using it is the recommended approach. The IngressGateway resource will be removed in future releases.\nIngressGateway configures a workload to act as a gateway for traffic entering the mesh. The ingress gateway also provides basic API gateway functionalities such as JWT token validation and request authorization. Gateways in privileged workspaces can route to services outside the workspace while those in unprivileged workspaces can only route to services inside the workspace.\nThe following example declares an ingress gateway running on pods with app: gateway labels in the ns1 namespace. The gateway exposes a host bookinfo.com on https port 9443 and http port 9090. The port 9090 is configured to receive plaintext traffic and send a redirect to the https port 9443 (site-wide HTTP -\u0026gt; HTTPS redirection). At port 9443, TLS is terminated using the certificates in the Kubernetes secret bookinfo-certs. Clients are authenticated using JWT tokens, whose keys are obtained from the OIDC provider www.googleapis.com. The request is then authorized by an the user’s authorization engine hosted at https://company.com/authz before being forwarded to the productpage service in the backend.\napiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: name: ingress-bookinfo group: g1 workspace: w1 tenant: mycompany organization: myorg spec: workloadSelector: namespace: ns1 labels: app: gateway http: - name: bookinfo-plaintext port: 9090 hostname: bookinfo.com routing: rules: - redirect: authority: bookinfo.com port: 9443 redirectCode: 301 scheme: https - name: bookinfo port: 9443 hostname: bookinfo.com tls: mode: SIMPLE secretName: bookinfo-certs authentication: rules: jwt: - issuer: https://accounts.google.com jwksUri: https://www.googleapis.com/oauth2/v3/certs - issuer: \u0026#34;auth.mycompany.com\u0026#34; jwksUri: https://auth.mycompany.com/oauth2/jwks authorization: external: uri: https://company.com/authz includeRequestHeaders: - Authorization # forwards the header to the authorization service. routing: rules: - route: host: ns1/productpage.ns1.svc.cluster.local rateLimiting: settings: rules: # Ratelimit at 10 requests/hour for clients with a remote address of 1.2.3.4 - dimensions: - remoteAddress: value: 1.2.3.4 limit: requestsPerUnit: 10 unit: HOUR # Ratelimit at 50 requests/minute for every unique value in the user-agent header - dimensions: - header: name: user-agent limit: requestsPerUnit: 50 unit: MINUTE # Ratelimit at 100 requests/second for every unique client remote address # with the HTTP requests having a GET method and the path prefix of /productpage - dimensions: - remoteAddress: value: \u0026#34;*\u0026#34; - header: name: \u0026#34;:path\u0026#34; value: prefix: /productpage - header: name: \u0026#34;:method\u0026#34; value: exact: \u0026#34;GET\u0026#34; limit: requestsPerUnit: 100 unit: SECOND If the productpage.ns1 service on Kubernetes has a ServiceRoute with multiple subsets and weights, the traffic will be split across the subsets accordingly.\nThe following example illustrates defining non-HTTP servers (based on TCP) with TLS termination. Here, kafka.myorg.internal uses non-HTTP protocol and listens on port 9000. The clients have to connect with TLS with the SNI kafka.myorg.internal. The TLS is terminated at the gateway and the traffic is routed to kafka.infra.svc.cluster.local:8000.\nIf subsets are defined in the ServiceRoute referencing kafka.infra.svc.cluster.local service, then it is also considered while routing.\napiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: name: ingress-bookinfo group: g1 workspace: w1 tenant: mycompany organization: myorg spec: workloadSelector: namespace: ns1 labels: app: gateway tcp: - name: kafka-gateway hostname: kafka.myorg.internal port: 9000 tls: mode: SIMPLE secretName: kafka-cred route: host: kafka.infra.svc.cluster.local port: 8000 The following example customizes the Extensions to enable the execution of the specified WasmExtensions list and details custom properties for the execution of each extension.\napiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: name: ingress-bookinfo group: g1 workspace: w1 tenant: mycompany organization: myorg spec: workloadSelector: namespace: ns1 labels: app: gateway extension: - fqn: hello-world # fqn of imported extensions in TSB config: foo: bar http: - name: bookinfo port: 80 hostname: bookinfo.com routing: rules: - route: host: ns1/productpage.ns1.svc.cluster.local `IngressGateway` also allows you to apply ModSecurity/Coraza compatible Web Application Firewall rules to traffic passing through the gateway. ```yaml apiVersion: gateway.xcp.tetrate.io/v2 kind: IngressGateway metadata: name: waf-gw namespace: ns1 labels: app: waf-gateway http: - name: bookinfo port: 9443 hostname: bookinfo.com waf: rules: - Include @recommended-conf - SecResponseBodyAccess Off - Include @owasp_crs/*.conf HttpRouting Field Description Validation Rule corsPolicy …","relpermalink":"/book/tsb/refs/tsb/gateway/v2/ingress-gateway/","summary":"Configurations to build an ingress gateway.","title":"Ingress Gateway"},{"content":" Image Values for the TSB operator image.\nField Description Validation Rule registry\nstring Registry used to download the operator image.\n–\ntag\nstring The tag of the operator image.\n–\nOperator Operator values for the TSB operator application.\nField Description Validation Rule deployment\ntetrateio.api.install.helm.common.v1alpha1.Operator.Deployment Values for the TSB operator deployment.\n–\nservice\ntetrateio.api.install.helm.common.v1alpha1.Operator.Service Values for the TSB operator service.\n–\nserviceAccount\ntetrateio.api.install.helm.common.v1alpha1.Operator.ServiceAccount Values for the TSB operator service account.\n–\nDeployment Values for the TSB operator deployment.\nField Description Validation Rule affinity\ntetrateio.api.install.kubernetes.Affinity Affinity configuration for the pod. https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity –\nannotations\nmap\u0026lt;string , string \u0026gt; Custom collection of annotations to add to the deployment. https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/ –\nenv\nList of tetrateio.api.install.kubernetes.EnvVar Custom collection of environment vars to add to the container. https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/ –\npodAnnotations\nmap\u0026lt;string , string \u0026gt; Custom collection of annotations to add to the pod.\n–\nreplicaCount\nint32 Number of replicas managed by the deployment.\n–\nstrategy\ntetrateio.api.install.kubernetes.DeploymentStrategy Deployment strategy to use. Remove Any when working on https://github.com/tetrateio/tetrate/issues/15885 https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#deploymentstrategy-v1-apps –\ntolerations\nList of k8s.io.api.core.v1.Toleration Toleration collection applying to the pod scheduling. https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ –\nService Values for the TSB operator service.\nField Description Validation Rule annotations\nmap\u0026lt;string , string \u0026gt; Custom collection of annotations to add to the service. https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/ –\nServiceAccount Values for the TSB operator service account.\nField Description Validation Rule annotations\nmap\u0026lt;string , string \u0026gt; Custom collection of annotations to add to the service account.\n–\nimagePullSecrets\nList of string Collection of secrets names required to be able to pull images from the registry.\n–\npullSecret\nstring A Docker config JSON to be stored in a secret to be used as an image pull secret. If this secret is provided, it will be included in the operator service account as reference. https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#create-a-secret-by-providing-credentials-on-the-command-line –\npullUsername\nstring Used along pull password and the provided image registry to generate a Docker config JSON that will be stored as a pull secret.\n–\npullPassword\nstring Used along pull username and the provided image registry to generate a Docker config JSON that will be stored as a pull secret.\n–\n","relpermalink":"/book/tsb/refs/install/helm/common/v1alpha1/common/","summary":"install/helm/common/v1alpha1/common.proto","title":"install/helm/common/v1alpha1/common.proto"},{"content":" Secrets Secrets available in the ControlPlane installation.\nField Description Validation Rule tsb\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.TSB Secrets to reach the TSB Management Plane.\n–\nelasticsearch\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.ElasticSearch Secrets to reach the Elasticsearch.\n–\nxcp\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.XCP Secrets to reach the XCP Central in the Management Plane.\n–\nclusterServiceAccount\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.ClusterServiceAccount Cluster service account used to authenticate to the Management Plane.\n–\nClusterServiceAccount Cluster service account used to authenticate to the Management Plane.\nField Description Validation Rule clusterFQN\nstring TSB FQN of the onboarded cluster resource. This will be generate tokens for all Control Plane agents.\n–\nJWK\nstring Literal JWK used to generate and sign the tokens for all the Control Plane agents.\n–\nencodedJWK\nstring Base64-encoded JWK used to generate and sign the tokens for all the Control Plane agents.\n–\nElasticSearch Secrets to reach the Elasticsearch.\nField Description Validation Rule username\nstring The username to access Elasticsearch.\n–\npassword\nstring The password to access Elasticsearch.\n–\ncacert\nstring Elasticsearch CA cert TLS used by control plane to verify TLS connection.\n–\nTSB Secrets to reach the TSB Management Plane.\nField Description Validation Rule cacert\nstring CA certificate used to verify TLS certs exposed the Management Plane (front envoy).\n–\nXCP Secrets to reach the XCP Central in the Management Plane.\nField Description Validation Rule autoGenerateCerts\nbool Enabling this will auto generate XCP Edge certificate if mTLS is enabled to authenticate to XCP Central. Requires cert-manager.\n–\nrootca\nstring CA certificate of XCP components.\n–\nrootcakey\nstring Key of the CA certificate of XCP components.\n–\nedge\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.XCP.Edge Secrets for the XCP Edge component.\n–\nEdge Secrets for the XCP Edge component.\nField Description Validation Rule cert\nstring Edge certificate used for mTLS with XCP Central.\n–\nkey\nstring Key of the Edge certificate used for mTLS with XCP Central.\n–\ntoken\nstring JWT token used to authenticate XCP Edge against the XCP Central.\n–\nValues Values available for the TSB Control Plane chart. This is an alpha API, so future versions could include breaking changes.\nField Description Validation Rule image\ntetrateio.api.install.helm.common.v1alpha1.Image Values for the TSB operator image.\n–\nspec\ntetrateio.api.install.controlplane.v1alpha1.ControlPlaneSpec Values for the Control Plane CR spec.\n–\nsecrets\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets Values for the Control Plane secrets.\n–\noperator\ntetrateio.api.install.helm.common.v1alpha1.Operator Values for the TSB operator application.\n–\n","relpermalink":"/book/tsb/refs/install/helm/controlplane/v1alpha1/values/","summary":"install/helm/controlplane/v1alpha1/values.proto","title":"install/helm/controlplane/v1alpha1/values.proto"},{"content":" Service to manage gateway settings in Istio Direct mode.\nIstioGateway The Istio Gateway service provides methods to manage gateway settings in Istio direct mode.\nThe methods in this service allow users to push Istio gateway configuration resources into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreateVirtualService rpc CreateVirtualService (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires CREATE\nCreate a new Istio VirtualService in the gateway group. Note that the VirtualService must be in one of the namespaces owned by the group.\nGetVirtualService rpc GetVirtualService (tetrateio.api.tsb.types.v2.GetIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires READ\nGet the details of the given Istio VirtualService.\nUpdateVirtualService rpc UpdateVirtualService (tetrateio.api.tsb.types.v2.IstioObject ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires WRITE\nModify the given Istio VirtualService.\nListVirtualServices rpc ListVirtualServices (tetrateio.api.tsb.types.v2.ListIstioObjectsRequest ) returns (tetrateio.api.tsb.types.v2.ListIstioObjectsResponse )\nList all Istio VirtualServices that have been attached to the gateway group.\nDeleteVirtualService rpc DeleteVirtualService (tetrateio.api.tsb.types.v2.DeleteIstioObjectRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the Istio VirtualService.\nCreateGateway rpc CreateGateway (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires CREATE\nCreate an Istio Gateway object in the gateway group.\nGetGateway rpc GetGateway (tetrateio.api.tsb.types.v2.GetIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires READ\nGet the details of the given Istio Gateway object.\nUpdateGateway rpc UpdateGateway (tetrateio.api.tsb.types.v2.IstioObject ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires WRITE\nModify the given Istio Gateway object.\nListGateways rpc ListGateways (tetrateio.api.tsb.types.v2.ListIstioObjectsRequest ) returns (tetrateio.api.tsb.types.v2.ListIstioObjectsResponse )\nList all the Istio Gateway objects that have been attached to the gateway group.\nDeleteGateway rpc DeleteGateway (tetrateio.api.tsb.types.v2.DeleteIstioObjectRequest ) returns (google.protobuf.Empty )\nRequires DELETE\ndelete the given Istio Gateway object.\nCreateRequestAuthentication rpc CreateRequestAuthentication (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires CREATE\nCreate an Istio RequestAuthentication in the gateway group.\nGetRequestAuthentication rpc GetRequestAuthentication (tetrateio.api.tsb.types.v2.GetIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires READ\nGet the details for the given Istio RequestAuthentication.\nUpdateRequestAuthentication rpc UpdateRequestAuthentication (tetrateio.api.tsb.types.v2.IstioObject ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires WRITE\nModify the given Istio RequestAuthentication.\nListRequestAuthentications rpc ListRequestAuthentications (tetrateio.api.tsb.types.v2.ListIstioObjectsRequest ) returns (tetrateio.api.tsb.types.v2.ListIstioObjectsResponse )\nList all Istio RequestAuthentications that have been attached to the gateway group.\nDeleteRequestAuthentication rpc DeleteRequestAuthentication (tetrateio.api.tsb.types.v2.DeleteIstioObjectRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given Istio RequestAuthentication.\nCreateAuthorizationPolicy rpc CreateAuthorizationPolicy (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires CREATE\nCreate an Istio AuthorizationPolicy in the gateway group.\nGetAuthorizationPolicy rpc GetAuthorizationPolicy (tetrateio.api.tsb.types.v2.GetIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires READ\nGet the details of the given Istio AuthorizationPolicy.\nUpdateAuthorizationPolicy rpc UpdateAuthorizationPolicy (tetrateio.api.tsb.types.v2.IstioObject ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires WRITE\nModify the given Istio AuthorizationPolicy.\nListAuthorizationPolicies rpc ListAuthorizationPolicies (tetrateio.api.tsb.types.v2.ListIstioObjectsRequest ) returns (tetrateio.api.tsb.types.v2.ListIstioObjectsResponse )\nList all Istio AuthorizationPolies that have been attached to the gateway group.\nDeleteAuthorizationPolicy rpc DeleteAuthorizationPolicy (tetrateio.api.tsb.types.v2.DeleteIstioObjectRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given Istio AuthorizationPolicy.\n","relpermalink":"/book/tsb/refs/tsb/gateway/v2/istio-gateway-direct/","summary":"Service to manage gateway settings in Istio Direct mode.","title":"Istio Direct Mode Gateway Service"},{"content":" Service to manage security settings in Istio Direct mode.\nIstioSecurity The Istio Security service provides methods to manage security settings in Istio direct mode.\nThe methods in this service allow users to push Istio security configuration resources into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreatePeerAuthentication rpc CreatePeerAuthentication (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires CREATE\nCreate a new Istio PeerAuthentication resource in the given group.\nGetPeerAuthentication rpc GetPeerAuthentication (tetrateio.api.tsb.types.v2.GetIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires READ\nGet the details of the given Istio PeerAuthentication resource.\nUpdatePeerAuthentication rpc UpdatePeerAuthentication (tetrateio.api.tsb.types.v2.IstioObject ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires WRITE\nModify a Istio PeerAuthentication resource.\nListPeerAuthentications rpc ListPeerAuthentications (tetrateio.api.tsb.types.v2.ListIstioObjectsRequest ) returns (tetrateio.api.tsb.types.v2.ListIstioObjectsResponse )\nList all Istio PeerAuthentication resources that have been attached to the security group.\nDeletePeerAuthentication rpc DeletePeerAuthentication (tetrateio.api.tsb.types.v2.DeleteIstioObjectRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given Istio PeerAuthentication resource.\nCreateAuthorizationPolicy rpc CreateAuthorizationPolicy (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires CREATE\nCreate an Istio AuthorizationPolicy in the given security group.\nGetAuthorizationPolicy rpc GetAuthorizationPolicy (tetrateio.api.tsb.types.v2.GetIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires READ\nGet the details of the given Istio AuthorizationPolicy.\nUpdateAuthorizationPolicy rpc UpdateAuthorizationPolicy (tetrateio.api.tsb.types.v2.IstioObject ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires WRITE\nModify an Istio AuthorizationPolicy.\nListAuthorizationPolicies rpc ListAuthorizationPolicies (tetrateio.api.tsb.types.v2.ListIstioObjectsRequest ) returns (tetrateio.api.tsb.types.v2.ListIstioObjectsResponse )\nList all Istio AuthorizationPolies that have been attached to the security group.\nDeleteAuthorizationPolicy rpc DeleteAuthorizationPolicy (tetrateio.api.tsb.types.v2.DeleteIstioObjectRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given Istio AuthorizationPolicy.\nCreateRequestAuthentication rpc CreateRequestAuthentication (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires CREATE\nCreate an Istio RequestAuthentication in the security group.\nGetRequestAuthentication rpc GetRequestAuthentication (tetrateio.api.tsb.types.v2.GetIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires READ\nGet the details for the given Istio RequestAuthentication.\nUpdateRequestAuthentication rpc UpdateRequestAuthentication (tetrateio.api.tsb.types.v2.IstioObject ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires WRITE\nModify the given Istio RequestAuthentication.\nListRequestAuthentications rpc ListRequestAuthentications (tetrateio.api.tsb.types.v2.ListIstioObjectsRequest ) returns (tetrateio.api.tsb.types.v2.ListIstioObjectsResponse )\nList all Istio RequestAuthentications that have been attached to the security group.\nDeleteRequestAuthentication rpc DeleteRequestAuthentication (tetrateio.api.tsb.types.v2.DeleteIstioObjectRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given Istio RequestAuthentication.\n","relpermalink":"/book/tsb/refs/tsb/security/v2/istio-security-direct/","summary":"Service to manage security settings in Istio Direct mode.","title":"Istio Direct Mode Security Service"},{"content":" Service to manage traffic settings in Istio Direct mode.\nIstioTraffic The Istio Traffic service provides methods to manage traffic settings in Istio direct mode.\nThe methods in this service allow users to push Istio traffic configuration resources into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreateVirtualService rpc CreateVirtualService (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires CREATE\nCreate an Istio VirtualService in the given traffic group. Note that the VirtualService must be in one of the namespaces owned by the group.\nGetVirtualService rpc GetVirtualService (tetrateio.api.tsb.types.v2.GetIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires READ\nGet the details of the given Istio VirtualService\nUpdateVirtualService rpc UpdateVirtualService (tetrateio.api.tsb.types.v2.IstioObject ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires WRITE\nModify an existing Istio VirtualService\nListVirtualServices rpc ListVirtualServices (tetrateio.api.tsb.types.v2.ListIstioObjectsRequest ) returns (tetrateio.api.tsb.types.v2.ListIstioObjectsResponse )\nList all Istio VirtualServices that are attached to the given traffic group.\nDeleteVirtualService rpc DeleteVirtualService (tetrateio.api.tsb.types.v2.DeleteIstioObjectRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given Istio VirtualService.\nCreateDestinationRule rpc CreateDestinationRule (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires CREATE\nCreate an Istio DestinationRule in the given traffic group.\nGetDestinationRule rpc GetDestinationRule (tetrateio.api.tsb.types.v2.GetIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires READ\nget the details of the given Istio DestinationRule.\nUpdateDestinationRule rpc UpdateDestinationRule (tetrateio.api.tsb.types.v2.IstioObject ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires WRITE\nModify the given Istio DestinationRule.\nListDestinationRules rpc ListDestinationRules (tetrateio.api.tsb.types.v2.ListIstioObjectsRequest ) returns (tetrateio.api.tsb.types.v2.ListIstioObjectsResponse )\nList all Istio DestinationRules that have been attached to the given traffic group.\nDeleteDestinationRule rpc DeleteDestinationRule (tetrateio.api.tsb.types.v2.DeleteIstioObjectRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given Istio DestinationRule.\nCreateSidecar rpc CreateSidecar (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires CREATE\nCreate an Istio Sidecar resource in the given traffic group.\nGetSidecar rpc GetSidecar (tetrateio.api.tsb.types.v2.GetIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires READ\nGet the details of the given Istio Sidecar resource.\nUpdateSidecar rpc UpdateSidecar (tetrateio.api.tsb.types.v2.IstioObject ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires WRITE\nModify the given Istio Sidecar resource.\nListSidecars rpc ListSidecars (tetrateio.api.tsb.types.v2.ListIstioObjectsRequest ) returns (tetrateio.api.tsb.types.v2.ListIstioObjectsResponse )\nList all Istio Sidecar resources that have been attached to the given traffic group.\nDeleteSidecar rpc DeleteSidecar (tetrateio.api.tsb.types.v2.DeleteIstioObjectRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given Istio Sidecar resource.\n","relpermalink":"/book/tsb/refs/tsb/traffic/v2/istio-traffic-direct/","summary":"Service to manage traffic settings in Istio Direct mode.","title":"Istio Direct Mode Traffic Service"},{"content":" DEPRECATED: use Access Bindings instead.\nIstioInternalAccessBindings is an assignment of roles to a set of users or teams to access resources under a Istio internal group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a IstioInternalAccessBinding can be created or modified only by users who have SET_POLICY permission on the Istio internal group.\nThe following example assigns the istiointernal-admin role to users alice, bob, and members of the istiointernal-ops team for istio internal group g1 under workspace w1 owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team\napiVersion: rbac.tsb.tetrate.io/v2 kind: IstioInternalAccessBindings metadata: organization: myorg tenant: mycompany workspace: w1 group: g1 spec: allow: - role: rbac/istiointernal-admin subjects: - user: organization/myorg/users/alice - user: organization/myorg/users/bob - team: organization/myorg/teams/istiointernal-ops IstioInternalAccessBindings IstioInternalAccessBindings assigns permissions to users of istio internal groups.\nField Description Validation Rule allow\nList of tetrateio.api.tsb.rbac.v2.Binding The list of allowed bindings configures the different access profiles that are allowed on the resource configured by the policy.\n–\n","relpermalink":"/book/tsb/refs/tsb/rbac/v2/istio-internal-access-bindings/","summary":"Configuration for assigning access roles to users of istio internal groups.","title":"Istio Internal Access Bindings"},{"content":" IstioInternalDirect service provides methods to manage resources in Istio direct mode.\nIstioInternalDirect IstioInternalDirect service provides methods to manage resources in Istio direct mode.\nThe methods in this service allow users to push resources like Istio Envoy filters or service entries, into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreateEnvoyFilter rpc CreateEnvoyFilter (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires CREATE\nCreate an Istio EnvoyFilter in the given istio internal group. Note that the EnvoyFilter must be in one of the namespaces owned by the group.\nGetEnvoyFilter rpc GetEnvoyFilter (tetrateio.api.tsb.types.v2.GetIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires READ\nGet the details of the given Istio EnvoyFilter.\nUpdateEnvoyFilter rpc UpdateEnvoyFilter (tetrateio.api.tsb.types.v2.IstioObject ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires WRITE\nModify an existing Istio EnvoyFilter.\nListEnvoyFilters rpc ListEnvoyFilters (tetrateio.api.tsb.types.v2.ListIstioObjectsRequest ) returns (tetrateio.api.tsb.types.v2.ListIstioObjectsResponse )\nList all Istio EnvoyFilter that are attached to the given Istio internal group.\nDeleteEnvoyFilter rpc DeleteEnvoyFilter (tetrateio.api.tsb.types.v2.DeleteIstioObjectRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given Istio EnvoyFilter.\nCreateServiceEntry rpc CreateServiceEntry (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires CREATE\nCreate an Istio ServiceEntry resource in the given Istio internal group.\nGetServiceEntry rpc GetServiceEntry (tetrateio.api.tsb.types.v2.GetIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires READ\nGet the details of the given Istio ServiceEntry resource.\nUpdateServiceEntry rpc UpdateServiceEntry (tetrateio.api.tsb.types.v2.IstioObject ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires WRITE\nModify the given Istio ServiceEntry resource.\nListServiceEntries rpc ListServiceEntries (tetrateio.api.tsb.types.v2.ListIstioObjectsRequest ) returns (tetrateio.api.tsb.types.v2.ListIstioObjectsResponse )\nList all Istio ServiceEntry resources that have been attached to the given Istio internal group.\nDeleteServiceEntry rpc DeleteServiceEntry (tetrateio.api.tsb.types.v2.DeleteIstioObjectRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given Istio ServiceEntry resource.\nCreateWasmPlugin rpc CreateWasmPlugin (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires CREATE\nCreate an Istio WasmPlugin resource in the given Istio internal group.\nGetWasmPlugin rpc GetWasmPlugin (tetrateio.api.tsb.types.v2.GetIstioObjectRequest ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires READ\nGet the details of the given Istio WasmPlugin resource.\nUpdateWasmPlugin rpc UpdateWasmPlugin (tetrateio.api.tsb.types.v2.IstioObject ) returns (tetrateio.api.tsb.types.v2.IstioObject )\nRequires WRITE\nModify the given Istio WasmPlugin resource.\nListWasmPlugins rpc ListWasmPlugins (tetrateio.api.tsb.types.v2.ListIstioObjectsRequest ) returns (tetrateio.api.tsb.types.v2.ListIstioObjectsResponse )\nList all Istio WasmPlugin resources that have been attached to the given Istio internal group.\nDeleteWasmPlugin rpc DeleteWasmPlugin (tetrateio.api.tsb.types.v2.DeleteIstioObjectRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given Istio WasmPlugin resource.\n","relpermalink":"/book/tsb/refs/tsb/istiointernal/v2/istio-istiointernal-direct/","summary":"Service to resources in Istio Direct mode.","title":"Istio Internal Direct Mode Service"},{"content":" IstioInternal service provides methods to manage istio internal TSB resources.\nIstioInternal IstioInternal service provides methods to manage istio internal TSB resources.\nIt provides methods to create and manage istio internal groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the istio internal configuration features.\nCreateGroup rpc CreateGroup (tetrateio.api.tsb.istiointernal.v2.CreateIstioInternalGroupRequest ) returns (tetrateio.api.tsb.istiointernal.v2.Group )\nRequires CREATE\nCreate a new Istio internal group in the given workspace.\nGroups will by default configure all the namespaces owned by their workspace, unless explicitly configured. If a specific set of namespaces is set for the group, it must be a subset of the namespaces defined by its workspace.\nGetGroup rpc GetGroup (tetrateio.api.tsb.istiointernal.v2.GetIstioInternalGroupRequest ) returns (tetrateio.api.tsb.istiointernal.v2.Group )\nRequires READ\nGet the details of the given Istio internal group.\nUpdateGroup rpc UpdateGroup (tetrateio.api.tsb.istiointernal.v2.Group ) returns (tetrateio.api.tsb.istiointernal.v2.Group )\nRequires WRITE\nModify a Istio internal group.\nListGroups rpc ListGroups (tetrateio.api.tsb.istiointernal.v2.ListIstioInternalGroupsRequest ) returns (tetrateio.api.tsb.istiointernal.v2.ListIstioInternalGroupsResponse )\nList all Istio internal groups in the given workspace.\nDeleteGroup rpc DeleteGroup (tetrateio.api.tsb.istiointernal.v2.DeleteIstioInternalGroupRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given Istio internal group. Note that deleting resources in TSB is a recursive operation. Deleting a Istio internal group will delete all configuration objects that exist in it.\nCreateIstioInternalGroupRequest Request to create an Istio internal group.\nField Description Validation Rule parent\nstring REQUIRED Parent resource where the Group will be created.\nstring = { min_len: 1}\nname\nstring REQUIRED The short name for the resource to be created.\nstring = { min_len: 1}\ngroup\ntetrateio.api.tsb.istiointernal.v2.Group REQUIRED Details of the Group to be created.\nmessage = { required: true}\nDeleteIstioInternalGroupRequest Request to delete a Istio internal Group.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the Group.\nstring = { min_len: 1}\nforce\nbool Force the deletion of the object even if deletion protection is enabled. If this is set, then the object and all its children will be deleted even if any of them has the deletion protection enabled.\n–\nGetIstioInternalGroupRequest Request to retrieve a Istio internal Group.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the Group.\nstring = { min_len: 1}\nListIstioInternalGroupsRequest Request to list Istio internal Groups.\nField Description Validation Rule parent\nstring REQUIRED Parent resource to list Groups from.\nstring = { min_len: 1}\nListIstioInternalGroupsResponse List of all Istio internal in the workspace.\nField Description Validation Rule groups\nList of tetrateio.api.tsb.istiointernal.v2.Group The list of requested groups.\n–\n","relpermalink":"/book/tsb/refs/tsb/istiointernal/v2/istiointernal-service/","summary":"Service to resources in Istio Direct mode.","title":"Istio Internal Direct Mode Service"},{"content":" Istio internal groups only allow grouping DIRECT mode mesh resources in a set of namespaces owned by its parent workspace. This group is aimed for grouping resources not directly related to traffic, security, or gateway like EnvoyFilters and ServiceEntry for instance. Istio internal group is meant to group highly coupled and implementation-detailed oriented istio resources that don’t provide any BRIDGE mode guarantees or backward/forward compatibilities that other groups like traffic, security of gateway can provide. Especially, and mainly because resources like EnvoyFilters, are highly customizable and can interfere in unpredictable ways, with any other routing, security, listeners, or filter chains among other configurations that TSB may have setup. Therefore, this group is only meant to be used for users/administrators that are confident with those advanced features, knowing that the defined resources under this group will not interfere with the TSB provided mesh governance functionalities.\nThe following example creates an istio internal group for resources in ns1, ns2 and ns3 namespaces owned by its parent workspace w1 under tenant mycompany.\napiVersion: istiointernal.tsb.tetrate.io/v2 kind: Group metadata: name: t1 workspace: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \u0026#34;*/ns1\u0026#34; - \u0026#34;*/ns2\u0026#34; - \u0026#34;*/ns3\u0026#34; It is possible to directly attach Istio APIs such as EnvoyFilter, and ServiceEntry to the istio internal group. These configurations will then pushed to the appropriate Istio control planes.\nThe following ServiceEntry example declares a few external APIs accessed by internal applications over HTTPS. The sidecar inspects the SNI value in the ClientHello message to route to the appropriate external service.\napiVersion: networking.istio.io/v1beta1 kind: ServiceEntry metadata: name: external-svc-https namespace: ns1 annotations: tsb.tetrate.io/organization: myorg tsb.tetrate.io/tenant: mycompany tsb.tetrate.io/workspace: w1 tsb.tetrate.io/istioInternalGroup: t1 spec: hosts: - api.dropboxapi.com - www.googleapis.com - api.facebook.com location: MESH_EXTERNAL ports: - number: 443 name: https protocol: TLS resolution: DNS The namespace where the Istio APIs are applied will need to be part of the parent istio internal group. In addition, each API object will need to have annotations to indicate the organization, tenant, workspace and the istio internal group to which it belongs to.\nGroup An Istio Internal Group only allows grouping DIRECT mode mesh resources in a set of namespaces owned by its parent workspace. This group is aimed for grouping resources not directly related to traffic, security, or gateway like EnvoyFilters and ServiceEntry.\nField Description Validation Rule namespaceSelector\ntetrateio.api.tsb.types.v2.NamespaceSelector REQUIRED Set of namespaces owned exclusively by this group. If omitted, applies to all resources owned by the workspace. Use */* to claim all cluster resources under the workspace.\nmessage = { required: true}\ndeletionProtectionEnabled\nbool When set, prevents the resource from being deleted. In order to delete the resource this property needs to be set to false first.\n–\nconfigGenerationMetadata\ntetrateio.api.tsb.types.v2.ConfigGenerationMetadata Default metadata values that will be propagated to the children Istio generated configurations. When using YAML APIs liketctl or gitops, put them into the metadata.labels or metadata.annotations instead. This field is only necessary when using gRPC APIs directly.\n–\n","relpermalink":"/book/tsb/refs/tsb/istiointernal/v2/istio-internal-group/","summary":"Group of istio resources that are not directly related to traffic, security, and gateways.","title":"Istio internal Group"},{"content":" Configuration affecting Istio control plane installation version and shape. Note: unlike other Istio protos, field names must use camelCase. This is asserted in tests. Without camelCase, the json tag on the Go struct will not match the user’s JSON representation. This leads to Kubernetes merge libraries, which rely on this tag, to fail. All other usages use jsonpb which does not use the json tag.\nistio.operator.v1alpha1.Affinity See k8s.io.api.core.v1.Affinity.\nField Description Validation Rule nodeAffinity\nistio.operator.v1alpha1.NodeAffinity –\npodAffinity\nistio.operator.v1alpha1.PodAffinity –\npodAntiAffinity\nistio.operator.v1alpha1.PodAntiAffinity –\nistio.operator.v1alpha1.BaseComponentSpec Configuration for base component.\nField Description Validation Rule enabled\ngoogle.protobuf.BoolValue Selects whether this component is installed.\n–\nk8s\nistio.operator.v1alpha1.KubernetesResourcesSpec Kubernetes resource spec.\n–\nistio.operator.v1alpha1.ClientIPConfig See k8s.io.api.core.v1.ClientIPConfig.\nField Description Validation Rule timeoutSeconds\nint32 –\nistio.operator.v1alpha1.ComponentSpec Configuration for internal components.\nField Description Validation Rule enabled\ngoogle.protobuf.BoolValue Selects whether this component is installed.\n–\nnamespace\nstring Namespace for the component.\n–\nhub\nstring Hub for the component (overrides top level hub setting).\n–\ntag\ngoogle.protobuf.Value Tag for the component (overrides top level tag setting).\n–\nspec\ngoogle.protobuf.Struct Arbitrary install time configuration for the component.\n–\nk8s\nistio.operator.v1alpha1.KubernetesResourcesSpec Kubernetes resource spec.\n–\nistio.operator.v1alpha1.ConfigMapKeySelector See k8s.io.api.core.v1.ConfigMapKeySelector.\nField Description Validation Rule localObjectReference\nistio.operator.v1alpha1.LocalObjectReference –\nkey\nstring –\noptional\nbool –\nistio.operator.v1alpha1.ContainerResourceMetricSource See k8s.io.api.autoscaling.v2beta2.ContainerResourceMetricSource.\nField Description Validation Rule name\nstring –\ntarget\nistio.operator.v1alpha1.MetricTarget –\ncontainer\nstring –\nistio.operator.v1alpha1.ContainerResourceMetricStatus See k8s.io.api.autoscaling.v2beta2.ContainerResourceMetricStatus.\nField Description Validation Rule name\nstring –\ncurrent\nistio.operator.v1alpha1.MetricValueStatus –\ncontainer\nstring –\nistio.operator.v1alpha1.CrossVersionObjectReference See k8s.io.api.autoscaling.v2beta2.CrossVersionObjectReference.\nField Description Validation Rule kind\nstring –\nname\nstring –\napiVersion\nstring –\nistio.operator.v1alpha1.DeploymentStrategy See k8s.io.api.apps.v1.DeploymentStrategy.\nField Description Validation Rule type\nstring –\nrollingUpdate\nistio.operator.v1alpha1.RollingUpdateDeployment –\nistio.operator.v1alpha1.EnvVar See k8s.io.api.core.v1.EnvVar.\nField Description Validation Rule name\nstring –\nvalue\nstring –\nvalueFrom\nistio.operator.v1alpha1.EnvVarSource –\nistio.operator.v1alpha1.EnvVarSource See k8s.io.api.core.v1.EnvVarSource.\nField Description Validation Rule fieldRef\nistio.operator.v1alpha1.ObjectFieldSelector –\nresourceFieldRef\nistio.operator.v1alpha1.ResourceFieldSelector –\nconfigMapKeyRef\nistio.operator.v1alpha1.ConfigMapKeySelector –\nsecretKeyRef\nistio.operator.v1alpha1.SecretKeySelector –\nistio.operator.v1alpha1.ExecAction See k8s.io.api.core.v1.ExecAction.\nField Description Validation Rule command\nList of string –\nistio.operator.v1alpha1.ExternalComponentSpec Configuration for external components.\nField Description Validation Rule enabled\ngoogle.protobuf.BoolValue Selects whether this component is installed.\n–\nnamespace\nstring Namespace for the component.\n–\nspec\ngoogle.protobuf.Struct Arbitrary install time configuration for the component.\n–\nchartPath\nstring Chart path for addon components.\n–\nschema\ngoogle.protobuf.Any Optional schema to validate spec against.\n–\nk8s\nistio.operator.v1alpha1.KubernetesResourcesSpec Kubernetes resource spec.\n–\nistio.operator.v1alpha1.ExternalMetricSource See k8s.io.api.autoscaling.v2beta2.ExternalMetricSource.\nField Description Validation Rule metricName\nstring –\nmetricSelector\nk8s.io.apimachinery.pkg.apis.meta.v1.LabelSelector –\ntargetValue\nistio.operator.v1alpha1.IntOrString –\ntargetAverageValue\nistio.operator.v1alpha1.IntOrString –\nmetric\nistio.operator.v1alpha1.MetricIdentifier –\ntarget\nistio.operator.v1alpha1.MetricTarget –\nistio.operator.v1alpha1.ExternalMetricStatus See k8s.io.autoscaling.v2beta2.ExternalMetricStatus.\nField Description Validation Rule metric\nistio.operator.v1alpha1.MetricIdentifier –\ncurrent\nistio.operator.v1alpha1.MetricValueStatus –\nistio.operator.v1alpha1.GatewaySpec Configuration for gateways.\nField Description Validation Rule enabled\ngoogle.protobuf.BoolValue Selects whether this gateway is installed.\n–\nnamespace\nstring Namespace for the gateway.\n–\nname\nstring Name for the gateway.\n–\nlabel\nmap\u0026lt;string , string \u0026gt; Labels for the gateway.\n–\nhub\nstring Hub for the component (overrides top level hub setting).\n–\ntag\ngoogle.protobuf.Value Tag for the component …","relpermalink":"/book/tsb/refs/istio.io/api/operator/v1alpha1/operator/","summary":"Configuration affecting Istio control plane installation version and shape.","title":"IstioOperator Options"},{"content":" JwtIdentity represents an JWT identity of a workload.\nE.g.,\nJWT identity of a workload:\nissuer: https://mycompany.corp subject: us-east-datacenter1-vm007 attributes: region: us-east datacenter: datacenter1 instance_name: vm007 instance_hostname: vm007.internal.corp instance_role: app-ratings JwtIdentity JwtIdentity represents an JWT identity of a workload.\nField Description Validation Rule issuer\nstring REQUIRED JWT Issuer identifier.\nThe value must be a case sensitive URL using the https scheme that contains scheme, host, and optionally, port number and path components and no query or fragment components.\nE.g., https://mycompany.corp, https://accounts.google.com, https://sts.windows.net/9edbd6c9-0e5b-4cfd-afec-fdde27cdd928/, etc.\nSee https://openid.net/specs/openid-connect-core-1_0.html#IDToken string = { prefix: https:// uri: true}\nsubject\nstring REQUIRED Workload identifier (JWT subject).\nA locally unique identifier within the Issuer.\nPreferably, the value should consist of lower case alphanumeric characters and ‘-’, should start and end with an alphanumeric character.\nOtherwise, if the value includes ASCII characters other than lower case alphanumeric characters and ‘-’, it will be encoded in a special way and will appear in that encoded form in metrics, in diagnostics, on UI. It might become non-trivial to infer the original workload identifier from the encoded form.\nThe value that includes non-ASCII characters is not valid.\nE.g., us-east-datacenter1-vm007.\nSee https://openid.net/specs/openid-connect-core-1_0.html#IDToken string = { pattern: ^[\u0000-]+$}\nattributes\nmap\u0026lt;string , string \u0026gt; Additional attributes associated with the workload.\nThe value is a map with free-form keys and values.\nE.g.,\nregion: us-east datacenter: datacenter1 instance_name: vm007 instance_hostname: vm007.internal.corp instance_role: app-ratings map = { keys: {string:{min_len:1}} values: {string:{min_len:1}}}\n","relpermalink":"/book/tsb/refs/onboarding/config/types/identity/jwt/v1alpha1/jwt/","summary":"JWT identity of a workload.","title":"JWT Identity"},{"content":" JwtIdentityMatcher specifies matching workloads with JWT identities .\nFor example, the following configuration will match only those workloads that were authenticated by means of an OIDC ID Token issued by https://mycompany.corp for one of the subjects us-east-datacenter1-vm007 or us-west-datacenter2-vm008:\nissuer: \u0026#34;https://mycompany.corp\u0026#34; subjects: - \u0026#34;us-east-datacenter1-vm007\u0026#34; - \u0026#34;us-west-datacenter2-vm008\u0026#34; In those cases where an OIDC ID Token from a given issuer includes a map of fine-grained attributes associated with a workload, it is possible to define rules that match those attributes.\nE.g., the following configuration will match a set workloads that were authenticated by means of an OIDC ID Token issued by https://mycompany.corp and include 1) attribute region with one of the values us-east or us-west and 2) attribute instance_role with the value app-ratings:\nissuer: \u0026#34;https://mycompany.corp\u0026#34; attributes: - name: \u0026#34;region\u0026#34; values: - \u0026#34;us-east\u0026#34; - \u0026#34;us-west\u0026#34; - name: \u0026#34;instance_role\u0026#34; values: - \u0026#34;app-ratings\u0026#34; AttributeMatcher AttributeMatcher specifies a matching attribute.\nField Description Validation Rule name\nstring REQUIRED OIDC ID Token must include an attribute with the given name.\nE.g., region, instance_role, etc.\nstring = { min_len: 1}\nvalues\nList of string OIDC ID Token must include the attribute with one of the following values.\nE.g., us-east, app-ratings, etc.\nEmpty list means match any value.\nrepeated = { items: {string:{min_len:1}}}\nJwtIdentityMatcher JwtIdentityMatcher specifies matching workloads with JWT identities .\nField Description Validation Rule issuer\nstring REQUIRED Match workloads authenticated by means of an OIDC ID Token issued by a given issuer.\nThe value must be a case sensitive URL using the https scheme that contains scheme, host, and optionally, port number and path components and no query or fragment components.\nE.g., https://mycompany.corp, https://accounts.google.com, https://sts.windows.net/9edbd6c9-0e5b-4cfd-afec-fdde27cdd928/, etc.\nSee https://openid.net/specs/openid-connect-core-1_0.html#IDToken string = { prefix: https:// uri: true}\nsubjects\nList of string Match workloads authenticated by means of an OIDC ID Token issued for one of the subjects in a given list.\nThe value must consist of ASCII characters.\nE.g., us-east-datacenter1-vm007.\nEmpty list means match OIDC ID Tokens with any subject.\nrepeated = { items: {string:{pattern:^[\\u0000-]+$}}}\nattributes\nList of tetrateio.api.onboarding.authorization.jwt.v1alpha1.AttributeMatcher REQUIRED Match workloads authenticated by means of an OIDC ID Token that includes all of the following attributes.\nEmpty list means match OIDC ID Tokens with any attributes.\nrepeated = { items: {message:{required:true}}}\n","relpermalink":"/book/tsb/refs/onboarding/config/authorization/jwt/v1alpha1/jwt/","summary":"Specification of matching workloads with JWT identities.","title":"JWT Identity Matcher"},{"content":" JwtIssuer specifies configuration associated with a JWT issuer.\nFor example,\nissuer: \u0026#34;https://mycompany.corp\u0026#34; jwksUri: \u0026#34;https://mycompany.corp/jwks.json\u0026#34; shortName: \u0026#34;mycorp\u0026#34; tokenFields: attributes: jsonPath: .custom_attributes JwtIssuer JwtIssuer specifies configuration associated with a JWT issuer.\nField Description Validation Rule issuer\nstring REQUIRED JWT Issuer identifier.\nThe value must be a case sensitive URL using the https scheme that contains scheme, host, and optionally, port number and path components and no query or fragment components.\nE.g., https://mycompany.corp, https://accounts.google.com, https://sts.windows.net/9edbd6c9-0e5b-4cfd-afec-fdde27cdd928/, etc.\nSee https://openid.net/specs/openid-connect-core-1_0.html#IDToken string = { prefix: https:// uri: true}\njwksUri\nstring oneof jwks_source URL of the JSON Web Key Set document.\nSource of public keys the Workload Onboarding Plane should use to validate the signature of an OIDC ID Token .\nE.g., https://mycompany.corp/jwks.json.\nWhen unspecified, URL the JSON Web Key Set document will be resolved using OpenID Connect Discovery protocol.\nstring = { prefix: https:// uri: true}\njwks\nstring oneof jwks_source Inlined JSON Web Key Set document.\nSpecifies public keys the Workload Onboarding Plane should use to validate the signature of an OIDC ID Token .\nstring = { min_len: 1}\nshortName\nstring REQUIRED Unique short name associated with the issuer.\nThe value must consist of lower case alphanumeric characters and hyphen (-).\nSince this value will be included into the auto-generated name of the WorkloadAutoRegistration resource, keep it as short as possible.\nE.g., my-corp, prod, test, etc.\nstring = { pattern: ^[0-9a-z]+(-[0-9a-z]+)*$}\ntokenFields\ntetrateio.api.onboarding.config.install.v1alpha1.JwtTokenFields Description of the custom fields included in the OIDC ID Token .\nBy default, Workload Onboarding Plane interprets only one field that is always present in a valid OIDC ID Token sub (subject). If you want Workload Onboarding Plane to interpret custom fields included in the OIDC ID Token , you have to provide an explicit configuration.\nE.g., you can instruct the Workload Onboarding Plane to treat a certain field as a map of fine-grained attributes associated with the subject. It will allow you to define OnboardingPolicy(s) that match those attributes.\nNotice that this description instructs how to interpret custom fields if they are present in an OIDC ID Token . A token in which custom fields are not present is still valid. An OnboardingPolicy that does not put constraints on attributes extracted from custom fields can still match a workload with that token.\n–\nJwtTokenField JwtTokenField specifies a custom field included into the OIDC ID Token .\nField Description Validation Rule jsonPath\nstring REQUIRED Simple JSON Path which is evaluated against custom claims of the OIDC ID Token to produce the value of the field.\nE.g., .custom_attributes, .google.compute_engine, etc.\nJSON Path must start either from . or from $. Use of $ is mandatory when followed by the array notation.\nE.g., $[\u0026#39;custom_attributes\u0026#39;], $[\u0026#39;google\u0026#39;].compute_engine, etc.\nSpecial symbols (such as . or ) in property names must be escaped.\nE.g., .custom\\.attributes, $[\u0026#39;custom\\.attributes\u0026#39;], etc.\nSee https://goessner.net/articles/JsonPath/ string = { pattern: ^[.$].+$}\nJwtTokenFields JwtTokenFields specifies custom fields included into the OIDC ID Token .\nField Description Validation Rule attributes\ntetrateio.api.onboarding.config.install.v1alpha1.JwtTokenField Field that carries a map of fine-grained attributes associated with the subject of the OIDC ID Token .\nIf specified, Workload Onboarding Plane will treat the name/value pairs extracted from this field as attributes associated with the workload. It will allow you to define OnboardingPolicy(s) that match those attributes.\nE.g., if an OIDC ID Token includes the following fields:\n{ \u0026#34;iss\u0026#34;: \u0026#34;https://mycompany.corp\u0026#34;, \u0026#34;aud\u0026#34;: \u0026#34;ef67c7b9-10da-4542-ad3b-b95acc1e05ba\u0026#34;, \u0026#34;sub\u0026#34;: \u0026#34;us-east-datacenter1-vm007\u0026#34;, \u0026#34;azp\u0026#34;: \u0026#34;us-east-datacenter1-vm007\u0026#34;, \u0026#34;iat\u0026#34;: 1613404941, \u0026#34;exp\u0026#34;: 1613408541, \u0026#34;custom_attributes\u0026#34;: { \u0026#34;region\u0026#34;: \u0026#34;us-east\u0026#34;, \u0026#34;datacenter\u0026#34;: \u0026#34;datacenter1\u0026#34;, \u0026#34;instance_name\u0026#34;: \u0026#34;vm007\u0026#34;, \u0026#34;instance_hostname\u0026#34;: \u0026#34;vm007.internal.corp\u0026#34;, \u0026#34;instance_role\u0026#34;: \u0026#34;app-ratings\u0026#34; } } then, you can indicate to the Workload Onboarding Plane to treat the contents of field custom_attributes as fine-grained attributes associated with the workload.\n–\nOnboardingPlaneComponentSet The set of components that make up the control plane. Use this to override application settings or Kubernetes settings for each individual component.\nField Description Validation Rule instance\ntetrateio.api.onboarding.config.install.v1alpha1.OnboardingPlaneInstance Workload Onboarding Plane Instance component.\n–\nOnboardingPlaneInstance Kubernetes settings for the Workload Onboarding Plane Instance component.\nField Description Validation Rule kubeSpec\ntetrateio.api.install.kubernetes.KubernetesComponentSpec …","relpermalink":"/book/tsb/refs/onboarding/config/install/v1alpha1/jwt-issuer/","summary":"Configuration associated with a JWT issuer.","title":"JWT Issuer"},{"content":" When installing on Kubernetes, these configuration settings can be used to override the default Kubernetes configuration. Kubernetes configuration can be set on each component in the install API using the kubeSpec field.\nThe API allows for customization of every field in the rendered Kubernetes manifests. The more common configuration fields, such as resources and service type, are supported directly; and can be configured like so:\napiVersion: install.tetrate.io/v1alpha1 kind: ManagementPlane metadata: name: managementplane spec: hub: docker.io/tetrate components: apiServer: kubeSpec: service: type: LoadBalancer deployment: resources: limits: memory: 750Mi requests: memory: 500Mi All components have a deployment and service object. Some, such as apiServer, also have a job object associated with them. This can be configured in a similar manner:\napiVersion: install.tetrate.io/v1alpha1 kind: ManagementPlane metadata: name: managementplane spec: hub: docker.io/tetrate components: apiServer: kubeSpec: job: podAnnotations: annotation-key: annotation-value Not all fields in a Kubernetes manifest can be configured directly. This is to avoid re-implementing the entire Kubernetes API within the install API. Instead, the kubeSpec object provides an overlays mechanism. This field is applied after the operator renders the initial manifests and enables support for customization of any field in a rendered manifest.\nOverlays can be applied by selecting the Kubernetes object you wish to overlay and then describe a list of patches you wish to apply. For example, to add a hostPort on port 8443 to the frontEnvoy component, do the following:\napiVersion: install.tetrate.io/v1alpha1 kind: ManagementPlane metadata: name: managementplane spec: hub: docker.io/tetrate components: frontEnvoy: kubeSpec: overlays: - apiVersion: apps/v1 kind: Deployment name: envoy patches: - path: spec.template.spec.containers.[name:envoy].ports.[containerPort:8443].hostPort value: 8443 The path refers to the location of the field in the Kubernetes object you with to patch. The format is a.[key1:value1].b.[:value2]. Where [key1:value1] is a selector for a key-value pair to identify a list element and [:value] is a value selector to identify a list element in a leaf list. All path intermediate nodes must exist.\nOverlays are inspired by and bear a loose resemblance to [kustomize](https://kustomize.io/). We use the library from the Istio Operator. For more examples of how to construct paths take a look at the tests in the upstream .\nAffinity The scheduling constraints for the pod. https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity Field Description Validation Rule nodeAffinity\ntetrateio.api.install.kubernetes.NodeAffinity Group of node affinity scheduling rules. https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#nodeaffinity-v1-core –\npodAffinity\ntetrateio.api.install.kubernetes.PodAffinity Group of inter-pod affinity scheduling rules. https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#podaffinity-v1-core –\npodAntiAffinity\ntetrateio.api.install.kubernetes.PodAntiAffinity Group of inter-pod anti-affinity scheduling rules. https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#podantiaffinity-v1-core –\nCNI Configure Istio’s CNI plugin For further details see: https://istio.io/docs/setup/additional-setup/cni/ Field Description Validation Rule binaryDirectory\nstring Directory on the host to install the CNI binary. Must be the same as the environment’s --cni-bin-dir setting (kubelet parameter).\n–\nconfigurationDirectory\nstring Directory on the host to install the CNI config. Must be the same as the environment’s --cni-conf-dir setting (kubelet parameter).\n–\nchained\nbool Whether to deploy the configuration file as a plugin chain or as a standalone file in the configuration directory. Some Kubernetes flavors (e.g. OpenShift) do not support the chain approach.\n–\nconfigurationFileName\nstring Leave unset to auto-find the first file in the cni-conf-dir (as kubelet does). Primarily used for testing install-cni plugin configuration. If set, install-cni will inject the plugin configuration into this file in the cni-conf-dir.\n–\nclusterRole\nstring The ClusterRole Istio CNI will bind to in the ControlPlane namespace. This is useful if you use Pod Security Policies and want to allow istio-cni to run as privileged Pods.\n–\nrevision\nstring The revisioned istio-operator that will reconcile the Istio CNI component. A revision can only be specified when Isolation Boundaries are enabled and configured with at least one revision. Revision specified here must be an enabled revision under xcp.isolationBoundaries. If not provided, it defaults to the latest enabled revision based on their corresponding tsbVersion. If multiple such revisions are found, revision names are alphabetically sorted and the first revision is considered as the default.\n–\nCapabilities See k8s.io.api.core.v1.Capabilities. …","relpermalink":"/book/tsb/refs/install/kubernetes/k8s/","summary":"Common Kubernetes configuration shared by all components in the install API planes.","title":"Kubernetes"},{"content":" ManagementPlane resource exposes a set of configurations necessary to automatically install the Service Bridge management plane on a cluster. The installation API is an override API so any unset fields that are not required will use sensible defaults.\nPrior to creating the ManagementPlane resource, verify that the following secrets exist in the namespace the management plane will be installed into:\ntsb-certs ldap-credentials custom-host-ca (if you are using TLS connection and need a custom CA to connect to LDAP host) postgres-credentials (non-demo deployments) admin-credentials es-certs (if your Elasticsearch is using a self-signed certificate) elastic-credentials (if your Elasticsearch backend requires authentication) A resource containing only the container registry hub will install a demo of Service Bridge, create a default Organization and install local instances of external dependencies, such as Postgres, Elasticsearch, and LDAP server.\nPlease note that these local instances are for demonstrative purposes only and should not be used in production. Production setups should point to a user managed Postgres and Elasticsearch as well as the enterprise LDAP server.\napiVersion: install.tetrate.io/v1alpha1 kind: ManagementPlane metadata: name: managementplane spec: hub: docker.io/tetrate organization: tetrate To move from the demo installation to production readiness, configure the top level settings that enable TSB to connect to external dependencies. When one of these settings stanzas are added the operator will delete the relevant demo component and configure the management plane to talk to the dependencies described.\napiVersion: install.tetrate.io/v1alpha1 kind: ManagementPlane metadata: name: managementplane spec: hub: docker.io/tetrate imagePullSecrets: - name: my-registry-creds organization: tetrate dataStore: postgres: address: postgres:1234 telemetryStore: elastic: host: elastic port: 5678 identityProvider: ldap: host: ldap port: 389 search: baseDN: dc=tetrate,dc=io iam: matchDN: \u0026#34;cn=%s,ou=People,dc=tetrate,dc=io\u0026#34; matchFilter: \u0026#34;(\u0026amp;(objectClass=person)(uid=%s))\u0026#34; sync: usersFilter: \u0026#34;(objectClass=person)\u0026#34; groupsFilter: \u0026#34;(objectClass=groupOfUniqueNames)\u0026#34; membershipAttribute: uniqueMember tokenIssuer: jwt: expiration: 1h issuers: - name: https://jwt.tetrate.io algorithm: RS256 signingKey: tls.key Top level settings deal with higher level concepts like persistence, but some configuration can also be overridden per component. For example, to configure the team synchronization schedule in the API server, set the schedule field in the apiServer component\napiVersion: install.tetrate.io/v1alpha1 kind: ManagementPlane metadata: name: managementplane spec: hub: docker.io/tetrate organization: tetrate components: apiServer: teamSyncSchedule: 17 * * * * dataStore: postgres: address: postgres:1234 telemetryStore: elastic: host: elastic port: 5678 identityProvider: ldap: host: ldap port: 389 search: baseDN: dc=tetrate,dc=io iam: matchDN: \u0026#34;cn=%s,ou=People,dc=tetrate,dc=io\u0026#34; matchFilter: \u0026#34;(\u0026amp;(objectClass=person)(uid=%s))\u0026#34; sync: usersFilter: \u0026#34;(objectClass=person)\u0026#34; groupsFilter: \u0026#34;(objectClass=groupOfUniqueNames)\u0026#34; membershipAttribute: uniqueMember tokenIssuer: jwt: expiration: 1h issuers: - name: https://jwt.tetrate.io algorithm: RS256 signingKey: tls.key To configure infrastructure specific settings such as resource limits on the deployment in Kubernetes, set the relevant field in a component. Remember that the installation API is an override API so if these fields are unset the operator will use sensible defaults. Only a subset of Kubernetes configuration is available and only for individual components.\napiVersion: install.tetrate.io/v1alpha1 kind: ManagementPlane metadata: name: managementplane spec: hub: docker.io/tetrate organization: tetrate components: collector: kubeSpec: deployment: resources: limits: memory: 750Mi requests: memory: 500Mi dataStore: postgres: address: postgres:1234 telemetryStore: elastic: host: elastic port: 5678 identityProvider: ldap: host: ldap port: 389 search: baseDN: dc=tetrate,dc=io iam: matchDN: \u0026#34;cn=%s,ou=People,dc=tetrate,dc=io\u0026#34; matchFilter: \u0026#34;(\u0026amp;(objectClass=person)(uid=%s))\u0026#34; sync: usersFilter: \u0026#34;(objectClass=person)\u0026#34; groupsFilter: \u0026#34;(objectClass=groupOfUniqueNames)\u0026#34; membershipAttribute: uniqueMember tokenIssuer: jwt: expiration: 1h issuers: - name: https://jwt.tetrate.io algorithm: RS256 signingKey: tls.key ManagementPlaneComponentSet The set of components that make up the management plane. Use this to override application settings or Kubernetes settings for each individual component.\nField Description Validation Rule apiServer\ntetrateio.api.install.managementplane.v1alpha1.ApiServer –\niamServer\ntetrateio.api.install.managementplane.v1alpha1.IamServer –\nwebUI\ntetrateio.api.install.managementplane.v1alpha1.WebUI –\nfrontEnvoy\ntetrateio.api.install.managementplane.v1alpha1.FrontEnvoy –\noap\ntetrateio.api.install.managementplane.v1alpha1.Oap –\ncollector …","relpermalink":"/book/tsb/refs/install/managementplane/v1alpha1/spec/","summary":"Configuration to describe a TSB management plane installation.","title":"Management Plane"},{"content":" A metric is a measurement about a service, captured at runtime. Logically, the moment of capturing one of these measurements is known as a metric event which consists not only of the measurement itself, but the time that it was captured and associated metadata..\nThe key aspects of a metric are the measure, the metric type, the metric origin, and the metric detect point:\nThe measure describes the type and unit of a metric event also known as measurement. The metric type is the aggregation over time applied to the measurements. The metric origin tells from where the metric measurements come from. The detect point is the point from which the metric is observed, in service, server side, or client side. It is useful to differentiate between metrics that observe a concrete service (often self observing), or metrics that focus on service to service communications. An TSB controlled (is part of the mesh and has a proxy we can configure) service has several metrics available which leverages a consistent monitoring of services. Some of them cover what is known as the RED metrics set, which are a set of very useful metrics for HTTP/RPC request based services. RED stands for:\nRate (R): The number of requests per second. Errors (E): The number of failed requests. Duration (D): The amount of time to process a request. To understand a bit better which metrics are available given a concrete telemetry source, let’s assume we have deployed the classic Istio bookinfo demo application . Let’s see some RED based metrics available for an observed and managed service by TSB, for instance the review service using the GLOBAL scoped telemetry source.\nThe following metric is the number of request per minute that the reviews service is handling at a GLOBAL scope:\napiVersion: observability.telemetry.tsb.tetrate.io/v2 kind: Metric metadata: organization: myorg service: reviews.bookinfo source: reviews name: service_cpm spec: observedResource: organizations/myorg/services/reviews.bookinfo measure: type: REQUESTS unit: \u0026#34;{request}\u0026#34; metricType: type: CPM origin: MESH_OBSERVED detectPoint: SERVER_SIDE The metric for the average duration of the handled request by the reviews service at a GLOBAL scope:\napiVersion: observability.telemetry.tsb.tetrate.io/v2 kind: Metric metadata: organization: myorg service: reviews.bookinfo source: reviews name: service_resp_time spec: observedResource: organizations/myorg/services/reviews.bookinfo measure: type: LATENCY unit: ms metricType: type: AVERAGE origin: MESH_OBSERVED detectPoint: SERVER_SIDE The metric for the errors of the handled request by the reviews at a GLOBAL scope. In this case the number of errors are expresses as a percentage of the total number of handled requests:\napiVersion: observability.telemetry.tsb.tetrate.io/v2 kind: Metric metadata: organization: myorg service: reviews.bookinfo source: reviews name: service_sla spec: observedResource: organizations/myorg/services/reviews.bookinfo measure: type: STATUS unit: NUMBER metricType: type: PERCENT origin: MESH_OBSERVED detectPoint: SERVER_SIDE Using a different telemetry source for the same metric will gives a different view of the same observed measurements. For instance, if we want to know how many requests per minute subset v1 from the reviews is handling, we need to use the same metric but from a different telemetry source, in this case reviews-v1:\napiVersion: observability.telemetry.tsb.tetrate.io/v2 kind: Metric metadata: organization: myorg service: reviews.bookinfo source: reviews-v1 name: service_cpm spec: observedResource: organizations/myorg/services/reviews.bookinfo measure: type: REQUESTS unit: NUMBER metricType: type: CPM origin: MESH_OBSERVED detectPoint: SERVER_SIDE The duration or latency measurements can also be aggregated in different percentiles over time. The duration percentiles for the handled request by the reviews at a GLOBAL scope:\napiVersion: observability.telemetry.tsb.tetrate.io/v2 kind: Metric metadata: organization: myorg service: reviews.bookinfo source: reviews name: service_percentile spec: observedResource: organizations/myorg/services/reviews.bookinfo measure: type: LATENCY unit: ms metricType: type: PERCENTILE labels: - key: \u0026#34;0\u0026#34; value: \u0026#34;p50\u0026#34; - key: \u0026#34;1\u0026#34; value: \u0026#34;p75\u0026#34; - key: \u0026#34;2\u0026#34; value: \u0026#34;p90\u0026#34; - key: \u0026#34;3\u0026#34; value: \u0026#34;p05\u0026#34; - key: \u0026#34;4\u0026#34; value: \u0026#34;p99\u0026#34; origin: MESH_OBSERVED detectPoint: SERVER_SIDE Measure A measure represents the name and unit of a measurement. For example, request latency in ms and the number of errors are examples of measures to collect from a server. In this case latency would be the type and ms (millisecond) is the unit.\nField Description Validation Rule name\nstring The name of the measure. For instance latency in ms. More reference values can be found at MeshControlledMeasureNames.\n–\nunit\nstring The unit of measure, which follow the unified code for units of measure . For COUNTABLE measures, as number of requests or network packets, SHOULD use the default unit, the unity, and annotations with …","relpermalink":"/book/tsb/refs/tsb/observability/telemetry/v2/metric/","summary":"A metric is a measurement about a service, captured at runtime.","title":"Metric"},{"content":" Service to manage telemetry metrics.\nMetrics The Metrics service exposes methods to manage Telemetry Metrics from Telemetry Sources.\nGetMetric rpc GetMetric (tetrateio.api.tsb.observability.telemetry.v2.GetMetricRequest ) returns (tetrateio.api.tsb.observability.telemetry.v2.Metric )\nGet the details of an existing telemetry metric.\nListMetrics rpc ListMetrics (tetrateio.api.tsb.observability.telemetry.v2.ListMetricsRequest ) returns (tetrateio.api.tsb.observability.telemetry.v2.ListMetricsResponse )\nList the telemetry metrics that are available for the requested telemetry source.\nGetMetricRequest Request to retrieve a telemetry metric from a parent telemetry source resource.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the telemetry metric.\nstring = { min_len: 1}\nListMetricsRequest Request to retrieve the list of telemetry metrics from a parent telemetry source resource.\nField Description Validation Rule parent\nstring REQUIRED Fully-qualified name of the parent telemetry source resource to retrieve the telemetry metrics.\nstring = { min_len: 1}\nListMetricsResponse List of telemetry metrics from the resource.\nField Description Validation Rule metrics\nList of tetrateio.api.tsb.observability.telemetry.v2.Metric –\n","relpermalink":"/book/tsb/refs/tsb/observability/telemetry/v2/metric-service/","summary":"Service to manage telemetry metrics.","title":"Metric Service"},{"content":" Onboarding Configuration specifies where to onboard the workload to.\nTo be able to onboard a workload into a service mesh, a user must configure Workload Onboarding Agent with the location of the Workload Onboarding Endpoint and a name of the WorkloadGroup to join.\nBy default, Workload Onboarding Agent will read Onboarding Configuration from a file /etc/onboarding-agent/onboarding.config.yaml, which must be created by the user.\nIf Onboarding Configuration file is missing or its contents is not valid, Workload Onboarding Agent will not be able to start.\nConsider the following example of the minimal valid configuration:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: onboarding.example.org # (1) workloadGroup: namespace: bookinfo # (2) name: ratings # (2) The above configuration instructs Workload Onboarding Agent to connect to the Workload Onboarding Endpoint reachable at the address onboarding.example.org:15443 (1) and automatically register the workload as a member of the WorkloadGroup \u0026#34;bookinfo/ratings\u0026#34; (2).\nWorkload Onboarding Endpoint might decline the workload from being registered in the mesh in case there is no WorkloadGroup with that name or there is no OnboardingPolicy that authorizes this particular workload to join this particular WorkloadGroup.\nIf a registration attempt fails, Workload Onboarding Agent will continue trying indefinitely until both the WorkloadGroup and allowing OnboardingPolicy exist in the mesh.\nOnboarding Configuration can also be used to customize workload’s appearance in the mesh, e.g. a set of labels associated with the workload, IP address of the workload that other mesh members should use to make requests to it, etc.\nE.g., consider the following example:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: onboarding.example.org workloadGroup: namespace: bookinfo name: ratings workload: labels: version: v1 # (1) The above configuration instructs Workload Onboarding Agent to register the workload in the mesh and customize the set of labels associated with it.\nBy default, a workload is associated only with labels configured in the respective WorkloadGroup resource.\nBy means of the above configuration, the workload will receive one extra label version=v1 (1).\nNotice that Onboarding Configuration gives users an option to specify additional labels for the workload, but it does not allow to override labels specified in the WorkloadGroup. E.g., if a WorkloadGroup specifies label app=ratings, Onboarding Configuration can not be used to override it with app=details.\nAs a rule of thumb, users should specify labels common to all workloads in the group using the WorkloadGroup resource, e.g. app label. Labels that can be unique to every individual workload in the group, e.g. version label, should be specified using Onboarding Configuration.\nNext, Onboarding Configuration can be used to fine-tune workload registration in the mesh on a feature-by-feature basis.\nE.g., consider the following example:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: onboarding.example.org workloadGroup: namespace: bookinfo name: ratings settings: connectedOver: INTERNET # (1) The above configuration instructs Workload Onboarding Agent to register the workload in the mesh by its Public IP address (aka Internet IP) rather than Private IP (aka VPC IP) (1).\nNormally, workloads should be connected to the rest of the mesh over a private network, which improves security and therefore is the default behavior.\nHowever, in those cases where private network connectivity between the workload and the rest of the mesh is not possible or not practical, users can use Onboarding Configuration to opt for connectivity over Internet instead.\nIn the above example, the workload will be registered in the mesh by its Public IP address (aka Internet IP). In practice, it means that other workloads in the mesh will use that Public IP to connect to this workload.\nLastly, Onboarding Configuration provides support for non-production scenarios, such as getting started try-outs and disposable environments for demos and trials.\nIn those cases where users find it acceptable to disable certain security safeguards in favour of simplicity of the setup, they can utilize the following configuration options.\nIf a user has no means to create a DNS record for the Workload Onboarding Endpoint, he/she can workaround this constraint by fine-tuning Onboarding Configuration the following way:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: 1.2.3.4 # (1) transportSecurity: tls: sni: onboarding.example.org # (2) workloadGroup: namespace: bookinfo name: ratings The above configuration instructs Workload Onboarding Agent to connect to 1.2.3.4 (1), yet validate server certificate as if the connection was made to …","relpermalink":"/book/tsb/refs/onboarding/config/agent/v1alpha1/onboarding-configuration/","summary":"Specifies where to onboard the workload to.","title":"Onboarding Configuration"},{"content":" Onboarding Policy authorizes matching workloads to join the mesh and become a part of a WorkloadGroup .\nBy default, none of the workloads are allowed to join the mesh.\nA workload is only allowed to join the mesh if there is an OnboardingPolicy resource that explicitly authorizes that.\nFor the purposes of authorization, a workload is considered to have the identity of the host it is running on.\nE.g., workloads that run on VMs in the cloud are considered to have cloud-specific identity of that VM. In case of AWS EC2 instances, VM identity includes AWS Partition, AWS Account number, AWS Region, AWS Zone, EC2 instance id, AWS IAM Role name, etc.\nAs part of the Workload Onboarding flow, Workload Onboarding Agent (that runs alongside the workload) will interact with cloud-specific metadata APIs to procure a credential (digitally signed data item) that can be passed to a third-party (Workload Onboarding Endpoint) as a proof of identity.\nOnce Workload Onboarding Endpoint has verified validity of the credential, i.e. audience, expiration time, digital signature, etc, it looks for an OnboardingPolicy resource that allows a workload with that identity to join the mesh.\nOnboardingPolicy resource consists of a list of rules.\nEach rule describes what workload identities it is applicable to and what WorkloadGroups the workload is allowed to join.\nE.g., consider the following example of a very permissive OnboardingPolicy:\napiVersion: authorization.onboarding.tetrate.io/v1alpha1 kind: OnboardingPolicy metadata: name: allow-aws-ec2-vms namespace: bookinfo spec: allow: - workloads: - aws: accounts: - \u0026#39;123456789012\u0026#39; ec2: {} # any AWS EC2 instance from the above account onboardTo: - workloadGroupSelector: {} # any WorkloadGroup from that namespace The above policy allows any workload running on an AWS EC2 instance of the AWS Account 123456789012 to join any WorkloadGroup in the bookinfo namespace.\nThe next example adds a constraint on AWS Regions the AWS EC2 instance may belong to:\napiVersion: authorization.onboarding.tetrate.io/v1alpha1 kind: OnboardingPolicy metadata: name: allow-aws-ec2-vms namespace: bookinfo spec: allow: - workloads: - aws: regions: - ca-central-1 accounts: - \u0026#39;123456789012\u0026#39; ec2: {} # any AWS EC2 instance from the above account and region onboardTo: - workloadGroupSelector: {} # any WorkloadGroup from that namespace The next example puts a constraint on WorkloadGroups the workload may join:\napiVersion: authorization.onboarding.tetrate.io/v1alpha1 kind: OnboardingPolicy metadata: name: allow-aws-ec2-vms namespace: bookinfo spec: allow: - workloads: - aws: accounts: - \u0026#39;123456789012\u0026#39; ec2: {} # any AWS EC2 instance from the above account onboardTo: - workloadGroupSelector: matchLabels: app: ratings # any WorkloadGroup from that namespace that has a label `app=ratings` The following example puts a constraint on AWS IAM Role an AWS EC2 instance must be associated with to limit the scope of the rule to a narrow subset of AWS EC2 instances in that AWS Account:\napiVersion: authorization.onboarding.tetrate.io/v1alpha1 kind: OnboardingPolicy metadata: name: allow-aws-ec2-vms namespace: bookinfo spec: allow: - workloads: - aws: accounts: - \u0026#39;123456789012\u0026#39; ec2: iamRoleNames: - ratings-role # any AWS EC2 instance from the above account that is # associated with one of IAM Roles on that list onboardTo: - workloadGroupSelector: matchLabels: app: ratings # any WorkloadGroup from that namespace that has a label `app=ratings` - workloads: - aws: accounts: - \u0026#39;123456789012\u0026#39; ec2: iamRoleNames: - reviews-role # any AWS EC2 instance from the above account that is # associated with one of IAM Roles on that list onboardTo: - workloadGroupSelector: matchLabels: app: reviews # any WorkloadGroup from that namespace that has a label `app=reviews` The above policy will allow AWS EC2 instances associated with AWS IAM Role ratings-role to join WorkloadGroups that have label app=ratings, while AWS EC2 instances associated with AWS IAM Role reviews-role to join WorkloadGroups that have label app=reviews.\nThe final example demonstrates other constraints that can be put on AWS EC2 instances:\napiVersion: authorization.onboarding.tetrate.io/v1alpha1 kind: OnboardingPolicy metadata: name: allow-aws-ec2-vms namespace: bookinfo spec: allow: - workloads: - aws: partitions: - aws accounts: - \u0026#39;123456789012\u0026#39; regions: - ca-central-1 zones: - ca-central-1b ec2: {} # any AWS EC2 instance from the above partitions/accounts/regions/zones - aws: partitions: - aws accounts: - \u0026#39;123456789012\u0026#39; regions: - us-east-1 zones: - us-east-1a ec2: iamRoleNames: - example-role # any AWS EC2 instance from the above partitions/accounts/regions/zones # associated with one of IAM Roles on that list onboardTo: - workloadGroupSelector: matchLabels: app: ratings To onboard workloads from custom on-premise environments, you can leverage support for OIDC ID Tokens .\nIf workloads in your custom environment can authenticate themselves by means of an OIDC ID Token , you can define …","relpermalink":"/book/tsb/refs/onboarding/config/authorization/v1alpha1/policy/","summary":"Authorizes matching workloads to join the mesh and become a part of a WorkloadGroup.","title":"Onboarding Policy"},{"content":" OpenAPI Extensions available to configure APIs.\nOpenAPIExtension Metadata describing an extension to the OpenAPI spec.\nField Description Validation Rule name\nstring The name of the OpenAPI extension as it should appear in the OpenAPI document. For example: x-tsb-service\n–\nappliesTo\nList of string Parts of the OpenAPI spec where this custom extension is allowed. This is a list of names of the OpenAPI elements where the extension is supported. For example: [“info”, “path”]\n–\nrequired\nbool Flag that configures if the extension is mandatory for the elements where it is supported.\n–\nOpenAPIExtensions Available OpenAPI extensions to configure APi Gateway features in Service Bridge.\nField Description Validation Rule xTsbService\nstring Short name of the service in the TSB service registry where the path is exposed. If the extension is configured in the info section, all paths in the spec will be mapped to this service.\nThis service name will be used to generate all the routes for traffic coming to the associated paths.\nOpenAPI extension name: x-tsb-service Applies to: info, path Required: Required unless x-tsb-redirect or x-tsb-clusters is defined for the target paths.\nExample:\nopenapi: 3.0.0 info: title: Sample API version: 0.1.9 x-tsb-service: productpage.bookinfo paths: /users: x-tsb-service: productpage.bookinfo get: summary: Returns a list of users. $docs_field_name = x-tsb-service\n–\nxTsbRedirect\ntetrateio.api.tsb.gateway.v2.Redirect Configures a redirection for the given path. If a redirection is configured for a given path, the x-tsb-service extension can be omitted.\nOpenAPI extension name: x-tsb-redirect Applies to: path Required: false\nExample:\npaths: /users: x-tsb-redirect: uri: /v2/users get: summary: Returns a list of users. $docs_field_name = x-tsb-redirect\n–\nxTsbTls\ntetrateio.api.tsb.gateway.v2.ServerTLSSettings Configures the TLS settings for a given server. If omitted, the server will be configured to serve plain text connections.\nOpenAPI extension name: x-tsb-redirect Applies to: server Required: false\nExample:\nopenapi: 3.0.0 servers: - url: http://api.example.com/v1 x-tsb-tls: mode: SIMPLE secretName: api-certs $docs_field_name = x-tsb-tls\n–\nxTsbCors\ntetrateio.api.tsb.gateway.v2.CorsPolicy Configures CORS policy settings for the given server. Note that Service Bridge does not currently support per-path CORS settings, so this applies at a server level.\nOpenAPI extension name: x-tsb-cors Applies to: server Required: false\nExample:\nopenapi: 3.0.0 servers: - url: http://api.example.com/v1 x-tsb-cors: allowOrigin: - \u0026#34;*\u0026#34; $docs_field_name = x-tsb-cors\n–\nxTsbAuthentication\ntetrateio.api.tsb.auth.v2.Authentication Configures Authentication rules for the given server. This extension must be configured if the Authorization is configured with rules based on JWT tokens.\nOpenAPI extension name: x-tsb-authentication Applies to: server Required: Required if Authorization is based on JWT tokens.\nExample:\nopenapi: 3.0.0 servers: - url: http://api.example.com/v1 x-tsb-authentication: jwt: issuer: https://www.googleapis.com/oauth2/v1/certs audience: bookinfo $docs_field_name = x-tsb-authentication\n–\nxTsbExternalAuthorization\ntetrateio.api.tsb.auth.v2.Authorization.ExternalAuthzBackend Configures an external authorization server to handle all authorization requests for the configured server.\nOpenAPI extension name: x-tsb-external-authorization Applies to: server Required: false\nExample:\nopenapi: 3.0.0 servers: - url: http://api.example.com/v1 x-tsb-external-authorization: uri: http://authz-server.example.com includeRequestHeaders: - Authorization # forwards the header to the authorization service. $docs_field_name = x-tsb-external-authorization\n–\nxTsbJwtAuthorization\ntetrateio.api.tsb.application.v2.OpenAPIExtensions.JWTAuthz Configures Authorization based on JWT tokens.\nNote that if this is configured, the x-tsb-authentication extension must be configured for the servers so the tokens can be properly validated and trusted before reading their contents to enforce access control rules.\nThis can be applied at the server or path level. When applied at the server level, the authorization rules will be enforced for all paths in that server.\nOpenAPI extension name: x-tsb-jwt-authorization Applies to: server, path Required: false\nExample:\nopenapi: 3.0.0 servers: - url: http://api.example.com/v1 x-tsb-authorization: claims: - iss: https://www.googleapis.com/oauth2/v1/certs sub: expected-subject other: # Additional claims to require int he token group: engineering paths: /users: x-tsb-authorization: # Override the server settings for the given path claims: - other: group: admin $docs_field_name = x-tsb-jwt-authorization\n–\nxTsbRatelimiting\ntetrateio.api.tsb.gateway.v2.RateLimitSettings Configures settings for ratelimiting requests\nThis can be applied at the server, path and operation level. Each rate limit setting is independent of the other. Top level fields such as failClosed and timeout can be only be specified at the …","relpermalink":"/book/tsb/refs/tsb/application/v2/openapi-extensions/","summary":"OpenAPI Extensions available to configure APIs.","title":"OpenAPI Extensions"},{"content":" Organization is a root of the Service Bridge object hierarchy. Each organization is completely independent of the other with its own set of tenants, users, teams, clusters and workspaces.\nOrganizations in TSB are tied to an Identity Provider (IdP). Users and teams, representing the organizational structure, are periodically synchronized from the IdP into TSB in order to make them available for access policy configuration.\nThe following example creates an organization named myorg.\napiVersion: api.tsb.tetrate.io/v2 kind: Organization metadata: name: myorg Organization Organization is the root of the Service Bridge object hierarchy.\nField Description Validation Rule deletionProtectionEnabled\nbool When set, prevents the resource from being deleted. In order to delete the resource this property needs to be set to false first.\n–\nconfigGenerationMetadata\ntetrateio.api.tsb.types.v2.ConfigGenerationMetadata Default metadata values that will be propagated to the children Istio generated configurations. When using YAML APIs liketctl or gitops, put them into the metadata.labels or metadata.annotations instead. This field is only necessary when using gRPC APIs directly.\n–\n","relpermalink":"/book/tsb/refs/tsb/v2/organization/","summary":"Configuration for creating an organization in Service Bridge.","title":"Organization"},{"content":" DEPRECATED: use Access Bindings instead.\nOrganizationAccessBindings is an assignment of roles to a set of users or teams to access resources under an Organization. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a OrganizationAccessBinding can be created or modified only by users who have SET_POLICY permission on the Organization.\nThe following example assigns the org-admin role to users alice, bob, and members of the t1 team owned by the organization myorg. Use fully-qualified name (fqn) when specifying user and team\napiVersion: rbac.tsb.tetrate.io/v2 kind: OrganizationAccessBindings metadata: organization: myorg spec: allow: - role: rbac/org-admin subjects: - user: organization/myorg/users/alice - user: organization/myorg/users/bob - team: organization/myorg/teams/t1 OrganizationAccessBindings OrganizationAccessBindings assigns permissions to users of organizations.\nField Description Validation Rule allow\nList of tetrateio.api.tsb.rbac.v2.Binding The list of allowed bindings configures the different access profiles that are allowed on the resource configured by the policy.\n–\n","relpermalink":"/book/tsb/refs/tsb/rbac/v2/organization-access-bindings/","summary":"Configuration for assigning access roles to users under an organization.","title":"Organization Access Bindings"},{"content":" Organization Setting allows configuring global settings for the organization. Settings such as network reachability or regional failover that apply globally to the organization are configured in the Organizations Setting object.\nThis is a global object that uniquely configures the organization, and there can be only one Organization Setting object defined for each organization.\nThe following example shows how these settings can be used to describe the organization’s network reachability settings and some regional failover configurations.\napiVersion: api.tsb.tetrate.io/v2 kind: OrganizationSetting metadata: name: org-settings organization: myorg spec: networkSettings: networkReachability: vpc01: vpc02,vpc03 regionalFailover: - from: us-east1 to: us-central1 OrganizationSetting Settings that apply globally to the entire organization.\nField Description Validation Rule networkSettings\ntetrateio.api.tsb.v2.OrganizationSetting.NetworkSettings Reachability between clusters on various networks.\n–\nregionalFailover\nList of tetrateio.api.tsb.types.v2.RegionalFailover Default locality routing settings for all gateways.\nExplicitly specify the region traffic will land on when endpoints in local region becomes unhealthy. Should be used together with OutlierDetection to detect unhealthy endpoints. Note: if no OutlierDetection specified, this will not take effect.\n–\ndefaultSecuritySetting\ntetrateio.api.tsb.security.v2.SecuritySetting Security settings for all proxy workloads in this organization. This can be overridden at TenantSettings, WorkspaceSettings, or security group’s SecuritySetting for specific cases. The override strategy used will be driven by the SecuritySetting propagation strategy. The default propagation strategy is REPLACE, in which a lower level SecuritySetting in the configuration hierarchy replaces a higher level SecuritySetting defined in the configuration hierarchy. For instance, a WorkspaceSettings defined SecuritySetting will replace any tenant or organization defined SecuritySetting. Proxy workloads without a specific security group will inherit these settings. If omitted, the following semantics apply:\nSidecars will accept connections from clients using Istio Mutual TLS as well as legacy clients using plaintext (i.e. any traffic not using Istio Mutual TLS authentication), i.e. authentication mode defaults to OPTIONAL.\nNo authorization will be performed, i.e., authorization mode defaults to DISABLED.\n–\ndefaultTrafficSetting\ntetrateio.api.tsb.traffic.v2.TrafficSetting Traffic settings for all proxy workloads in this organization. This can be overridden at TenantSettings or WorkspaceSettings for specific cases. Proxy workloads without a specific traffic group will inherit these settings. If omitted, the following semantics apply:\nSidecars will be able to reach any service in the cluster, i.e. reachability mode defaults to CLUSTER.\nTraffic to unknown destinations will be directly routed from the sidecar to the destination.\n–\nNetworkSettings Network related settings for clusters.\nField Description Validation Rule networkReachability\nmap\u0026lt;string , string \u0026gt; Reachability between clusters on various networks. Each cluster has a “network” field representing a network boundary like a VPC on AWS/GCP/Azure. All clusters within the same network are assumed to be reachable to each other for multi-cluster routing. In addition, you can specify additional connectivity between various networks in the mesh here. For example on AWS, each VPC can be treated as a distinct network. VPCs that are reachable to one another (through peering or transit gateways) can be listed as reachable networks. The key is the network name and the value is a comma separated list of networks whose clusters are reachable from this network. For instance, vpc01: vpc02,vpc03 means that the clusters in the network can reach those in vpc02 and vpc03.\nNote that reachability is not bidirectional. That is, if vpc01: vpc02 is specified, then vpc01 can reach vpc02, but not the other way around. Hence, the workloads in clusters in vpc01 can access the services through the exposed gateway hostnames in clusters in vpc02 . However, the workloads in clusters in vpc02 cannot access the services exposed through the gateway hostnames in vpc01.\n–\n","relpermalink":"/book/tsb/refs/tsb/v2/organization-setting/","summary":"Configuration for specifying global settings in an organization.","title":"Organization Setting"},{"content":" Service to manage Organizations in TSB\nOrganizations The Organizations service exposes methods to manage the organizations that exist in TSB. Organizations are the root of the Service Bridge object hierarchy. Each organization is completely independent of the other with its own set of tenants, users, teams, clusters and workspaces.\nGetOrganization rpc GetOrganization (tetrateio.api.tsb.v2.GetOrganizationRequest ) returns (tetrateio.api.tsb.v2.Organization )\nRequires READ\nGet the details of an organization.\nSyncOrganization rpc SyncOrganization (tetrateio.api.tsb.v2.SyncOrganizationRequest ) returns (tetrateio.api.tsb.v2.SyncOrganizationResponse )\nRequires CreateUser, CreateTeam, DeleteUser, DeleteTeam, WriteTeam\nSyncOrganization is used by processes that monitor the identity providers to synchronize the users and teams with the ones in TSB.\nThis method will update the state of users and groups in the organization and will create, modify, and delete groups according to the incoming request. Sync requests are assumed to be a full-sync and to contain all existing users and groups. Existing TSB users and groups that are not contained in a sync request will be deleted from the platform, as it will assume they have been removed from the Identity Provider.\nCreateSettings rpc CreateSettings (tetrateio.api.tsb.v2.CreateOrganizationSettingsRequest ) returns (tetrateio.api.tsb.v2.OrganizationSetting )\nRequires CreateOrganizationSetting\nCreate a settings object for the given organization.\nGetSettings rpc GetSettings (tetrateio.api.tsb.v2.GetOrganizationSettingsRequest ) returns (tetrateio.api.tsb.v2.OrganizationSetting )\nRequires ReadOrganizationSetting\nGet the details for the given settings object.\nUpdateSettings rpc UpdateSettings (tetrateio.api.tsb.v2.OrganizationSetting ) returns (tetrateio.api.tsb.v2.OrganizationSetting )\nRequires WriteOrganizationSetting\nModify the given settings in the given Organization.\nListSettings rpc ListSettings (tetrateio.api.tsb.v2.ListOrganizationSettingsRequest ) returns (tetrateio.api.tsb.v2.ListOrganizationSettingsResponse )\nList all the settings objects that have been attached to the given Organization.\nDeleteSettings rpc DeleteSettings (tetrateio.api.tsb.v2.DeleteOrganizationSettingsRequest ) returns (google.protobuf.Empty )\nRequires DeleteOrganizationSetting\nDelete the given settings object from the Organization.\nCreateOrganizationSettingsRequest Request to create a Organization Settings.\nField Description Validation Rule parent\nstring REQUIRED Parent resource where the Organization Settings will be created.\nstring = { min_len: 1}\nname\nstring REQUIRED The short name for the resource to be created.\nstring = { min_len: 1}\nsettings\ntetrateio.api.tsb.v2.OrganizationSetting REQUIRED Details of the Organization Settings to be created.\nmessage = { required: true}\nDeleteOrganizationSettingsRequest Request to delete a Organization Settings.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the Organization Settings.\nstring = { min_len: 1}\nGetOrganizationRequest Request to retrieve a organization.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the organization.\nstring = { min_len: 1}\nGetOrganizationSettingsRequest Request to retrieve a Organization Settings.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the Organization Settings.\nstring = { min_len: 1}\nListOrganizationSettingsRequest Request to list Organization Settings.\nField Description Validation Rule parent\nstring REQUIRED Parent resource to list Organization Settings from.\nstring = { min_len: 1}\nListOrganizationSettingsResponse List of all existing Organization settings objects in the Organization group.\nField Description Validation Rule settings\nList of tetrateio.api.tsb.v2.OrganizationSetting –\nSyncOrganizationRequest Request to synchronize the users and teams in an organization from the configured identity provider.\nField Description Validation Rule fqn\nstring REQUIRED Internal use only. Auto populated field.\nstring = { min_len: 1}\nsourceType\ntetrateio.api.tsb.v2.SourceType we cannot use the enum_only validation as protoc-gen-validate does not properly import the enum package in the generated code, and it breaks :(\n–\nusers\nList of tetrateio.api.tsb.v2.SyncOrganizationRequest.SyncUser –\nteams\nList of tetrateio.api.tsb.v2.SyncOrganizationRequest.SyncTeam –\nSyncTeam Information of a team as synchronized from the team source. This differs slightly from a TSB user since the fields here are raw info that does not have the context of the TSB hierarchy.\nField Description Validation Rule id\nstring REQUIRED Unique ID for the group.\nstring = { min_len: 1}\ndescription\nstring Optional description for the group.\n–\nmemberUserIds\nList of string List of user ids for the users that belong to this group.\n–\nmemberGroupIds\nList of string List of group ids for the groups that are nested into this group.\n–\ndisplayName\nstring Friendly name to show the group in the …","relpermalink":"/book/tsb/refs/tsb/v2/organization-service/","summary":"Service to manage Organizations in TSB","title":"Organizations Service"},{"content":" Permissions.\nPermission A permission defines an action that can be performed on a resource. By default access to resources is denied unless an explicit permission grants access to perform an operation against it.\nField Number Description INVALID\n0\nDefault value to designate no value was explicitly set for the permission.\nREAD\n1\nThe read permission grants read-only access to the resource.\nWRITE\n2\nThe write permission allows the subject to modify an existing resource.\nCREATE\n3\nThe create permission allows subjects to create child resources on the resource.\nDELETE\n4\nThe delete permission grants permissions to delete the resource.\nSET_POLICY\n5\nThe set-iam permission allows subjects to manage the access policies for the resources.\n","relpermalink":"/book/tsb/refs/tsb/rbac/v2/permissions/","summary":"Permissions.","title":"Permissions"},{"content":" Service to manage centralized approval policies.\nPermissions The Permissions service exposes methods to query permission information on existing records. $hide_from_yaml\nQueryResourcePermissions rpc QueryResourcePermissions (tetrateio.api.tsb.q.v2.QueryResourcePermissionsRequest ) returns (tetrateio.api.tsb.q.v2.QueryResourcePermissionsResponse )\nQueryResourcePermission looks up permissions that are allowed for the current principal. Multiple records can be queried with a single request. Query limit is 100, multiple requests are required to lookup more than the limit.\nGetResourcePermissions rpc GetResourcePermissions (tetrateio.api.tsb.q.v2.GetResourcePermissionsRequest ) returns (tetrateio.api.tsb.q.v2.GetResourcePermissionsResponse )\nGetResourcePermission looks up permissions that are allowed for the current principal. on the given resource FQN. This is similar to QueryResourcePermission but limited to a single resource FQN.\nGetResourcePermissionsRequest Request to query permissions on a single record by FQN.\nField Description Validation Rule fqn\nstring Fully-qualified name of the resource\n–\nGetResourcePermissionsResponse Response with permission rules.\nField Description Validation Rule rules\nList of tetrateio.api.tsb.rbac.v2.Role.Rule –\nQuery Query format of the resource lookup for the permission check\nField Description Validation Rule queryId\nstring OPTIONAL Optional ID that is an open string the caller can use for correlation purposes.\n–\nfqn\nstring oneof kind Fully-qualified name of the resource.\n–\nQueryResourcePermissionsRequest Request to query permissions on multiple records.\nExample: QueryResourcePermissionsRequest { Queries: []Query{ Query{ QueryID: “1234”, Kind: Query_Fqn{ Fqn: “tetrate/tenants/default/workspaces/example” } } } }\nField Description Validation Rule queries\nList of tetrateio.api.tsb.q.v2.Query One or more resources to query permissions on, limited to 100 per request.\nrepeated = { min_items: 1 max_items: 100}\nQueryResourcePermissionsResponse Response with permissions for the requested queries.\nExample: QueryResourcePermissionsResponse { Results: []Result{ Result{ Request: Query{ QueryID: “1234”, Kind: Query_Fqn{ Fqn: “tetrate/tenants/default/workspaces/example” } }, Rules: []*Role_Rule{ { Types: []*Role_ResourceType{ { ApiGroup: “api.tsb.tetrate.io/v2”, Kinds: []string{“Workspace”} } }, Permissions: []Permission{“READ”} } } } } }\nField Description Validation Rule results\nList of tetrateio.api.tsb.q.v2.QueryResourcePermissionsResponse.Result List of permission results for the requested queries\n–\nResult Represents a result for the requested query\nField Description Validation Rule request\ntetrateio.api.tsb.q.v2.Query REQUIRED –\nrules\nList of tetrateio.api.tsb.rbac.v2.Role.Rule set of allowed RBAC rules that the current principal has on the matching resource. If the query produced no results, the rules set will be empty.\n–\n","relpermalink":"/book/tsb/refs/tsb/q/v2/permissions-service/","summary":"Service to manage centralized approval policies.","title":"Permissions Service"},{"content":" Access Policy Bindings.\nBinding A binding associates a role with a set of subjects.\nBindings are used to configure policies, where different roles can be assigned to different sets of subjects to configure a fine-grained access control to the resource protected by the policy.\nField Description Validation Rule role\nstring REQUIRED The role that defines the permissions that will be granted to the target resource.\nstring = { min_len: 1}\nsubjects\nList of tetrateio.api.tsb.rbac.v2.Subject The set of subjects that will be allowed to access the target resource with the permissions defined by the role.\n–\nSubject Subject identifies a user or a team under an organization. Roles are assigned to subjects for specific resources in the system.\nField Description Validation Rule user\nstring oneof sub A user in TSB, created through LDAP sync or API. Must use the fully-qualified name (fqn) of the user. E.g. organization/myorg/users/alice\n–\nteam\nstring oneof sub A team in TSB, created through LDAP sync or API. Must use the fully-qualified name (fqn) of the team. E.g. organization/myorg/teams/t1\n–\nserviceAccount\nstring oneof sub A service account in TSB. Must use the fully-qualified name (fqn) of the service account. E.g. organization/myorg/serviceaccounts/sa1\n–\nRequiredPermission RequiredPermission\nConfigures the sets of permissions that are required to invoke the method where this option is applied.\nField Description Validation Rule permissions\nList of tetrateio.api.tsb.rbac.v2.Permission The required set of permissions. The full name of each permission (such as ReadApplication) will be inferred from the name of the method where this option is applied.\n–\nrawPermissions\nList of string Set of raw permission names values. Only use this if the method being protected does not follow the common naming convention and the proper name of the permission cannot be inferred just by using the Permission enum and the method name.\n–\ndeferPermissionCheckToApplication\nbool When this flag is set to true, the permission checks will not be made at the API surface. This is usually needed when there is not an explicit set of permissions that can be preconfigured for the API methods, so the access control checks will be implemented at runtime by the application. The default value is ‘false’ and will only be taken into account if the permission properties are empty. If any permission is set, this flag will be ignored.\n–\n","relpermalink":"/book/tsb/refs/tsb/rbac/v2/binding/","summary":"Access Policy Bindings.","title":"Policy Bindings"},{"content":" Service to manage access control policies for TSB resources\nPolicy The Policy service provides methods to configure the access control policies for TSB resources.\nAll TSB resources have one and exactly one policy document that configures access for it. When resources are created, a default policy is attached to the resource, assigning administration privileges on the resource to the user that created it.\nGetPolicy rpc GetPolicy (tetrateio.api.tsb.rbac.v2.GetPolicyRequest ) returns (tetrateio.api.tsb.rbac.v2.AccessPolicy )\nGet the access policy for the given resource.\nSetPolicy rpc SetPolicy (tetrateio.api.tsb.rbac.v2.AccessPolicy ) returns (google.protobuf.Empty )\nSet the access policy for the given resource.\nGetRootPolicy rpc GetRootPolicy (tetrateio.api.tsb.rbac.v2.GetAdminPolicyRequest ) returns (tetrateio.api.tsb.rbac.v2.AccessPolicy )\nRequires SET_POLICY\nGet the root access policy. The root access policy configures global permissions for the platform. Subjects assigned to a root policy will be granted the permissions described in the policy to all objects ion TSB.\nSetRootPolicy rpc SetRootPolicy (tetrateio.api.tsb.rbac.v2.AccessPolicy ) returns (google.protobuf.Empty )\nRequires SET_POLICY\nSet the root access policy. The root access policy configures global permissions for the platform. Subjects assigned to a root policy will be granted the permissions described in the policy to all objects ion TSB.\nGetRBACPolicy rpc GetRBACPolicy (tetrateio.api.tsb.rbac.v2.GetAdminPolicyRequest ) returns (tetrateio.api.tsb.rbac.v2.AccessPolicy )\nRequires SET_POLICY\nGet the global RBAC access policy. The global RBAC access policy configures who can manage the Role objects in TSB.\nSetRBACPolicy rpc SetRBACPolicy (tetrateio.api.tsb.rbac.v2.AccessPolicy ) returns (google.protobuf.Empty )\nRequires SET_POLICY\nSet the global RBAC access policy. The global RBAC access policy configures who can manage the Role objects in TSB.\nAccessPolicy Policy\nA policy defines the set of subjects that can access a resource and under which conditions that access is granted.\nField Description Validation Rule allow\nList of tetrateio.api.tsb.rbac.v2.Binding The list of allowed bindings configures the different access profiles that are allowed on the resource configured by the policy.\n–\nGetPolicyRequest Request to get the access policy for a resource.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the policy.\nstring = { min_len: 1}\n","relpermalink":"/book/tsb/refs/tsb/rbac/v2/policy-service/","summary":"Service to manage access control policies for TSB resources","title":"Policy Service"},{"content":" Services in the registry represent logically a service that can be running in different compute platforms and different locations. The same service could be running on different Kubernetes clusters at the same time, on VMS, etc. A service in the registry represents an aggregated and logical view for all those individual services, and provides high-level features such as aggregated metrics.\nPort Port exposed by a service. Registration RPC will complete the instances field by assigning the physical services FQNs.\nField Description Validation Rule number\nuint32 REQUIRED A valid non-negative integer port number.\nuint32 = { lte: 65535 gte: 1}\nname\nstring Name assigned to the port.\n–\nserviceDeployments\nList of string OUTPUT_ONLY The list of FQNs of the instances that expose this port\n–\nService A service in the registry that represents an aggregated and logical view for all those individual services, and provides high-level features such as aggregated metrics.\nField Description Validation Rule fqn\nstring OUTPUT_ONLY Fully-qualified name of the resource. This field is read-only.\n–\ndisplayName\nstring User friendly name for the resource.\n–\netag\nstring The etag for the resource. This field is automatically computed and must be sent on every update to the resource to prevent concurrent modifications.\n–\ndescription\nstring A description of the resource.\n–\nshortName\nstring REQUIRED Short name for the service, used to uniquely identify it within the organization.\nstring = { min_len: 1}\nhostnames\nList of string The hostnames by which this service is accessed. Can correspond to the hostname of an internal service or that ones of a virtual host on a gateway.\n–\nports\nList of tetrateio.api.tsb.registry.v2.Port The set of ports on which this service is exposed.\n–\nsubsets\nList of string OUTPUT_ONLY Deprecated. Use subset_deployments instead. Subset denotes a specific version of a service. By default the ‘version’ label is used to designate subsets of a workload. Known subsets for the service.\n–\nserviceType\ntetrateio.api.tsb.registry.v2.ServiceType REQUIRED Internal/external/load balancer service.\nenum = { defined_only: true}\nexternalAddresses\nList of string For kubernetes services of type load balancer, this field contains the list of lb hostnames or IPs assigned to the service.\n–\nstate\ntetrateio.api.tsb.registry.v2.State REQUIRED State of the service (registered/observed/controlled)\nenum = { defined_only: true}\nmetrics\nList of tetrateio.api.tsb.registry.v2.Service.MetricConfig OUTPUT_ONLY Services may expose different metrics. For example, a regular service may expose the usual red metrics for incoming requests. Services running in multiple clusters, may provide different aggregation levels, such as aggregation by cluster, by subset, etc. This list provides a complete list of all the aggregation keys that are available for this particular service. For example, a service that has instances in multiple clusters could provide the following metrics:\nglobal: |productpage|bookinfo||* v1: v1|productpage|bookinfo|| v1 (cluster1): v1|productpage|bookinfo|cluster1|* This is only available for Observed and Controlled services.\n–\nserviceDeployments\nList of tetrateio.api.tsb.registry.v2.Service.ServiceDeployment OUTPUT_ONLY List of the existing deployments for this service. This is only available for internal and load balancer services and correspond to physical services in the onboarded clusters. This field is read-only.\n–\nsubsetDeployments\nList of tetrateio.api.tsb.registry.v2.Subset OUTPUT_ONLY Subset denotes a specific version of a service. By default the ‘version’ label is used to designate subsets of a workload. Known subsets for the service.\n–\ncanonicalName\nstring The canonical name of the service defined by user\n–\nspiffeIds\nList of string List of SPIFFE identities used by the workloads of the service.\n–\nMetricConfig Configuration for metric aggregation\nField Description Validation Rule name\nstring A user friendly name for this metric.\n–\ndescription\nstring A helpful description of what this metric represents.\n–\naggregationKey\nstring An aggregation key that can be queried to get metrics for this service.\n–\ntype\ntetrateio.api.tsb.registry.v2.Service.MetricConfig.MetricType Type of the metric (single_instance/aggregated).\n–\nserviceDeployment\nstring The FQN of the service deployment related with this metric. Will be empty for group metrics.\n–\nparentMetric\nstring The name of the metric config that aggregates this one in a higher level. For example, for a subset in a cluster metric, this field has the name of the metric of the same subset across the clusters\n–\nServiceDeployment ServiceDeployment represents the physical service in a cluster.\nField Description Validation Rule fqn\nstring OUTPUT_ONLY Fully-qualified name of the instance. This field is read-only.\n–\nsource\nstring OUTPUT_ONLY Source of the instance. This field is read-only.\n–\nSubset Subset exposed by a service. Registration RPC will complete the instances field by assigning the …","relpermalink":"/book/tsb/refs/tsb/registry/v2/service/","summary":"Configuration for onboarding clusters.","title":"Registered Service"},{"content":" Role is a named collection of permissions that can be assigned to any user or team in the system. The set of actions that can be performed by a user, such as the ability to create, delete, or update configuration will depend on the permissions associated with the user’s role. Roles are global resources that are defined once. AccessBindings in each configuration group will bind a user to a specific role defined apriori.\nTSB comes with the following predefined roles:\nRole Permissions Description rbac/admin * Grants full access to the target resource and its child objects rbac/editor Read Write Create Grants read/write access to a resource and allows creating child resources rbac/creator Read Create Useful to delegate access to a resource without giving write access to the object itself. Users with this role will be able to manage sub-resources but not the resource itself rbac/writer Read Write Grants Read and Write access permissions rbac/reader Read Grants read-only permissions to a resource The following example declares a custom workspace-admin role with the ability to create, delete configurations and the ability to set RBAC policies on the groups within the workspace.\napiVersion: rbac.tsb.tetrate.io/v2 kind: Role metadata: name: role1 spec: rules: - types: - apiGroup: api.tsb.tetrate.io/v2 kinds: - WorkspaceSetting permissions: - CREATE - READ - DELETE - WRITE - SET_POLICY Role Role is a named collection of permissions that can be assigned to any user or team in the system.\nField Description Validation Rule rules\nList of tetrateio.api.tsb.rbac.v2.Role.Rule A set of rules that define the permissions associated with each API group.\nrepeated = { min_items: 1}\nResourceType The type of API resource for which the role is being created.\nField Description Validation Rule apiGroup\nstring REQUIRED A specific API group such as traffic.tsb.tetrate.io/v2.\nstring = { min_len: 1}\nkinds\nList of string Specific kinds of APIs under the API group. If omitted, the role will apply to all kinds under the group.\n–\nRule A rule defines the set of api groups\nField Description Validation Rule types\nList of tetrateio.api.tsb.rbac.v2.Role.ResourceType The set of API groups and the api Kinds within the group on which this rule is applicable. If omitted, the permissions will globally apply to all resource types.\n–\npermissions\nList of tetrateio.api.tsb.rbac.v2.Permission REQUIRED The set of actions allowed for these APIs. The current version supports requires the kind, but this constraint will be relaxed in upcoming releases so that rules can apply globally to an entire API group.\nrepeated = { min_items: 1}\n","relpermalink":"/book/tsb/refs/tsb/rbac/v2/role/","summary":"Configuration for creating various access roles.","title":"Role"},{"content":" Service to manage access roles in Service Bridge.\nRBAC The RBAC service provides methods to manage the roles in the Service Bridge platform. It provides method to configure the roles that can be used in the management plane access control policies and their permissions.\nCreateRole rpc CreateRole (tetrateio.api.tsb.rbac.v2.CreateRoleRequest ) returns (tetrateio.api.tsb.rbac.v2.Role )\nRequires CREATE\nCreate a new role.\nListRoles rpc ListRoles (tetrateio.api.tsb.rbac.v2.ListRolesRequest ) returns (tetrateio.api.tsb.rbac.v2.ListRolesResponse )\nRequires READ\nList all existing roles.\nGetRole rpc GetRole (tetrateio.api.tsb.rbac.v2.GetRoleRequest ) returns (tetrateio.api.tsb.rbac.v2.Role )\nRequires READ\nGet the details of the given role.\nUpdateRole rpc UpdateRole (tetrateio.api.tsb.rbac.v2.Role ) returns (tetrateio.api.tsb.rbac.v2.Role )\nRequires WRITE\nModify a role.\nDeleteRole rpc DeleteRole (tetrateio.api.tsb.rbac.v2.DeleteRoleRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete a role. NRoles that are in use by policies attached to existing resources cannot be deleted.\nCreateRoleRequest Request to create a Role.\nField Description Validation Rule name\nstring REQUIRED The short name for the resource to be created.\nstring = { min_len: 1}\nrole\ntetrateio.api.tsb.rbac.v2.Role REQUIRED Details of the Role to be created.\nmessage = { required: true}\nDeleteRoleRequest Request to delete a Role.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the Role.\nstring = { min_len: 1}\nGetRoleRequest Request to retrieve a Role.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the Role.\nstring = { min_len: 1}\nListRolesResponse List of all existing roles.\nField Description Validation Rule roles\nList of tetrateio.api.tsb.rbac.v2.Role –\n","relpermalink":"/book/tsb/refs/tsb/rbac/v2/role-service/","summary":"Service to manage access roles in Service Bridge.","title":"Role Service"},{"content":" DEPRECATED: use Access Bindings instead.\nSecurityAccessBindings is an assignment of roles to a set of users or teams to access resources under a Security group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a SecurityAccessBinding can be created or modified only by users who have SET_POLICY permission on the Security group.\nThe following example assigns the security-admin role to users alice, bob, and members of the security-ops team for the security group g1 under workspace w1 owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team\napiVersion: rbac.tsb.tetrate.io/v2 kind: SecurityAccessBindings metadata: organization: myorg tenant: mycompany workspace: w1 group: g1 spec: allow: - role: rbac/security-admin subjects: - user: organization/myorg/users/alice - user: organization/myorg/users/bob - team: organization/myorg/teams/security-ops SecurityAccessBindings SecurityAccessBindings assigns permissions to users of security groups.\nField Description Validation Rule allow\nList of tetrateio.api.tsb.rbac.v2.Binding The list of allowed bindings configures the different access profiles that are allowed on the resource configured by the policy.\n–\n","relpermalink":"/book/tsb/refs/tsb/rbac/v2/security-access-bindings/","summary":"Configuration for assigning access roles to users of security groups.","title":"Security Access Bindings"},{"content":" Security Groups allow grouping the proxy workloads in a set of namespaces owned by its parent workspace. Security related configurations can then be applied on the group to control the behavior of these proxy workloads. The group can be in one of two modes: BRIDGED and DIRECT. BRIDGED mode is a minimalistic mode that allows users to quickly configure the most commonly used features in the service mesh using Tetrate specific APIs, while the DIRECT mode provides more flexibility for power users by allowing them to configure the proxy workload’s security properties using a restricted subset of Istio Security APIs.\nThe following example creates a security group for the proxy workloads in ns1, ns2 and ns3 namespaces owned by its parent workspace w1 under tenant mycompany\napiVersion: security.tsb.tetrate.io/v2 kind: Group metadata: name: t1 workspace: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \u0026#34;*/ns1\u0026#34; - \u0026#34;*/ns2\u0026#34; - \u0026#34;*/ns3\u0026#34; configMode: BRIDGED And the associated security settings for the proxy workloads in the group\napiVersion: security.tsb.tetrate.io/v2 kind: SecuritySetting metadata: name: defaults group: t1 workspace: w1 tenant: mycompany organization: myorg spec: authentication: REQUIRED Under the hood, Service Bridge translates these minimalistic settings into Istio APIs such as PeerAuthentication, AuthorizationPolicy, etc. for the namespaces managed by the security group. These APIs are then pushed to the Istio control planes of clusters where the workspace is applicable.\nIt is possible to create a security group for namespaces in a specific cluster as long as the parent workspace owns those namespaces in that cluster. For example,\napiVersion: security.tsb.tetrate.io/v2 kind: Group metadata: name: t1 workspace: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \u0026#34;c1/ns1\u0026#34; # pick ns1 namespace only from c1 cluster - \u0026#34;*/ns2\u0026#34; - \u0026#34;*/ns3\u0026#34; configMode: BRIDGED In the DIRECT mode, it is possible to directly attach Istio Security v1beta1 APIs - PeerAuthentication, and AuthorizationPolicy to the security group. These configurations will be validated for correctness and conflict free operations and then pushed to the appropriate Istio control planes.\nThe following example declares a PeerAuthentication policy for a specific workload in the ns1 namespace:\napiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: workload-mtls-disable namespace: ns1 annotations: tsb.tetrate.io/organization: myorg tsb.tetrate.io/tenant: mycompany tsb.tetrate.io/workspace: w1 tsb.tetrate.io/securityGroup: t1 spec: selector: matchLabels: app: reviews mtls: mode: DISABLE The namespace where the Istio APIs are applied will need to be part of the parent security group. In addition, each API object will need to have annotations to indicate the organization, tenant, workspace and the security group to which it belongs to.\nGroup A security group manages the security properties of proxy workloads in a group of namespaces owned by the parent workspace.\nField Description Validation Rule namespaceSelector\ntetrateio.api.tsb.types.v2.NamespaceSelector REQUIRED Set of namespaces owned exclusively by this group. If omitted, applies to all resources owned by the workspace. Use */* to claim all cluster resources under the workspace.\nmessage = { required: true}\nconfigMode\ntetrateio.api.tsb.types.v2.ConfigMode The Configuration types that will be added to this group. BRIDGED mode indicates that configurations added to this group will use Tetrate APIs such as SecuritySetting. DIRECT mode indicates that configurations added to this group will use Istio Security v2beta1 APIs such as PeerAuthentication, and AuthorizationPolicy. Defaults to BRIDGED mode.\n–\nsecurityDomain\nstring Security domains can be used to group different resources under the same security domain. Although security domain is not resource itself currently, it follows a fqn format organizations/myorg/securitydomains/mysecuritydomain, and a child cannot override any ancestor’s security domain. Once a security domain is assigned to a Security group, all the children resources will belong to that security domain in the same way a Security setting belongs to a Security group, a Security setting will also belong to the security domain assigned to the Security group. Security domains can also be used to define Security settings Authorization rules in which you can allow or deny request from or to a security domain.\n–\ndeletionProtectionEnabled\nbool When set, prevents the resource from being deleted. In order to delete the resource this property needs to be set to false first.\n–\nconfigGenerationMetadata\ntetrateio.api.tsb.types.v2.ConfigGenerationMetadata Default metadata values that will be propagated to the children Istio generated configurations. When using YAML APIs liketctl or gitops, put them into the metadata.labels or metadata.annotations instead. This field is only necessary when using gRPC APIs directly.\n–\n","relpermalink":"/book/tsb/refs/tsb/security/v2/security-group/","summary":"Configurations to group a set of proxy workloads in a workspace for security.","title":"Security Group"},{"content":" Service to manage security settings.\nSecurity The Security service provides methods to manage security settings in TSB.\nIt provides methods to create and manage security groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the security configuration features.\nThe Security service also provides methods to configure the different security settings that are allowed within each group.\nCreateGroup rpc CreateGroup (tetrateio.api.tsb.security.v2.CreateSecurityGroupRequest ) returns (tetrateio.api.tsb.security.v2.Group )\nRequires CREATE\nCreate a new security group in the given workspace.\nGroups will by default configure all the namespaces owned by their workspace, unless explicitly configured. If a specific set of namespaces is set for the group, it must be a subset of the namespaces defined by its workspace.\nGetGroup rpc GetGroup (tetrateio.api.tsb.security.v2.GetSecurityGroupRequest ) returns (tetrateio.api.tsb.security.v2.Group )\nRequires READ\nGet the details of the given security group.\nUpdateGroup rpc UpdateGroup (tetrateio.api.tsb.security.v2.Group ) returns (tetrateio.api.tsb.security.v2.Group )\nRequires WRITE\nModify a security group.\nListGroups rpc ListGroups (tetrateio.api.tsb.security.v2.ListSecurityGroupsRequest ) returns (tetrateio.api.tsb.security.v2.ListSecurityGroupsResponse )\nList all security groups in the given workspace.\nDeleteGroup rpc DeleteGroup (tetrateio.api.tsb.security.v2.DeleteSecurityGroupRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given security group. Note that deleting resources in TSB is a recursive operation. Deleting a security group will delete all configuration objects that exist in it.\nCreateSettings rpc CreateSettings (tetrateio.api.tsb.security.v2.CreateSecuritySettingsRequest ) returns (tetrateio.api.tsb.security.v2.SecuritySetting )\nRequires CreateSecuritySetting\nCreate a security settings object in the security group.\nGetSettings rpc GetSettings (tetrateio.api.tsb.security.v2.GetSecuritySettingsRequest ) returns (tetrateio.api.tsb.security.v2.SecuritySetting )\nRequires ReadSecuritySetting\nGet the details of the given security settings object.\nUpdateSettings rpc UpdateSettings (tetrateio.api.tsb.security.v2.SecuritySetting ) returns (tetrateio.api.tsb.security.v2.SecuritySetting )\nRequires WriteSecuritySetting\nModify the given security settings object.\nListSettings rpc ListSettings (tetrateio.api.tsb.security.v2.ListSecuritySettingsRequest ) returns (tetrateio.api.tsb.security.v2.ListSecuritySettingsResponse )\nList all security settings objects that have been attached to the security group.\nDeleteSettings rpc DeleteSettings (tetrateio.api.tsb.security.v2.DeleteSecuritySettingsRequest ) returns (google.protobuf.Empty )\nRequires DeleteSecuritySetting\nDelete the given security settings from the group.\nCreateServiceSecuritySettings rpc CreateServiceSecuritySettings (tetrateio.api.tsb.security.v2.CreateServiceSecuritySettingsRequest ) returns (tetrateio.api.tsb.security.v2.ServiceSecuritySetting )\nRequires CREATE\nCreate a service security settings object in the security group.\nGetServiceSecuritySettings rpc GetServiceSecuritySettings (tetrateio.api.tsb.security.v2.GetServiceSecuritySettingsRequest ) returns (tetrateio.api.tsb.security.v2.ServiceSecuritySetting )\nRequires READ\nGet the details of the given service security settings object.\nUpdateServiceSecuritySettings rpc UpdateServiceSecuritySettings (tetrateio.api.tsb.security.v2.ServiceSecuritySetting ) returns (tetrateio.api.tsb.security.v2.ServiceSecuritySetting )\nRequires WRITE\nModify the given service security settings object.\nListServiceSecuritySettings rpc ListServiceSecuritySettings (tetrateio.api.tsb.security.v2.ListServiceSecuritySettingsRequest ) returns (tetrateio.api.tsb.security.v2.ListServiceSecuritySettingsResponse )\nList all service security settings objects that have been attached to the security group.\nDeleteServiceSecuritySettings rpc DeleteServiceSecuritySettings (tetrateio.api.tsb.security.v2.DeleteServiceSecuritySettingsRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given service security settings from the group.\nCreateSecurityGroupRequest Request to create a Security Group.\nField Description Validation Rule parent\nstring REQUIRED Parent resource where the Group will be created.\nstring = { min_len: 1}\nname\nstring REQUIRED The short name for the resource to be created.\nstring = { min_len: 1}\ngroup\ntetrateio.api.tsb.security.v2.Group REQUIRED Details of the Group to be created.\nmessage = { required: true}\nCreateSecuritySettingsRequest Request to create a Security Settings.\nField Description Validation Rule parent\nstring REQUIRED Parent resource where the Security Settings will be created.\nstring = { min_len: 1}\nname\nstring REQUIRED The short name for the resource to be created.\nstring = { min_len: 1} …","relpermalink":"/book/tsb/refs/tsb/security/v2/security-service/","summary":"Service to manage security settings.","title":"Security Service"},{"content":" SecuritySetting allows configuring security related properties such as TLS authentication and access control for traffic arriving at a proxy workload in a security group.\nSecurity settings can be propagated along any defined security settings in the configuration hierarchy. How security settings are propagated can be configured by specifying a PropagationStrategy.\nThe following example creates a security group for the proxy workloads in ns1, ns2 and ns3 namespaces owned by its parent workspace w1 under tenant mycompany and defines a security setting that only allows mutual TLS authenticated traffic from other proxy workloads in the same group.\napiVersion: security.tsb.tetrate.io/v2 kind: Group metadata: name: t1 workspace: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \u0026#34;*/ns1\u0026#34; - \u0026#34;*/ns2\u0026#34; - \u0026#34;*/ns3\u0026#34; configMode: BRIDGED And the associated security settings for all proxy workloads in the group\napiVersion: security.tsb.tetrate.io/v2 kind: SecuritySetting metadata: name: defaults group: t1 workspace: w1 tenant: mycompany organization: myorg spec: authenticationSettings: trafficMode: REQUIRED authorization: mode: GROUP The following example customizes the allowedSources to allow traffic from the namespaces within the group as well as the catalog-sa service account from ns4 namespace.\napiVersion: security.tsb.tetrate.io/v2 kind: SecuritySetting metadata: name: custom group: t1 workspace: w1 tenant: mycompany organization: myorg spec: authenticationSettings: trafficMode: REQUIRED http: rules: jwt: - issuer: \u0026#34;https://auth.tetrate.io\u0026#34; jwksUri: \u0026#34;https://oauth2.auth.tetrate.io/certs\u0026#34; - issuer: \u0026#34;https://auth.tetrate.internal\u0026#34; jwksUri: \u0026#34;https://oauth2.auth.tetrate.internal/certs\u0026#34; authorization: mode: CUSTOM serviceAccounts: - \u0026#34;ns1/*\u0026#34; - \u0026#34;ns2/*\u0026#34; - \u0026#34;ns3/*\u0026#34; - \u0026#34;ns4/catalog-sa\u0026#34; http: external: uri: \u0026#34;https://policy.auth.tetrate.io\u0026#34; includeRequestHeaders: - authorization The following example rejects all traffic arriving at workloads from namespaces that belong to security group t1.\napiVersion: security.tsb.tetrate.io/v2 kind: SecuritySetting metadata: name: defaults group: t1 workspace: w1 tenant: mycompany organization: myorg spec: authenticationSettings: trafficMode: REQUIRED authorization: mode: RULES rules: denyAll: true The following example accepts all traffic arriving at workloads from namespaces that belong to security group t1. All authenticated requests are accepted because any workload is targeted to be allowed nor denied.\napiVersion: security.tsb.tetrate.io/v2 kind: SecuritySetting metadata: name: defaults group: t1 workspace: w1 tenant: mycompany organization: myorg spec: authenticationSettings: trafficMode: REQUIRED authorization: mode: RULES The following example accepts all traffic arriving at workloads in namespaces that belong to security group t1 traffic, except from workloads belonging to workspace w2.\napiVersion: security.tsb.tetrate.io/v2 kind: SecuritySetting metadata: name: defaults group: t1 workspace: w1 tenant: mycompany organization: myorg spec: authenticationSettings: trafficMode: REQUIRED authorization: mode: RULES rules: deny: - from: fqn: organizations/myorg/tenants/mycompany/workspaces/w2 to: fqn: organizations/myorg/tenants/mycompany/workspaces/w1/securitygroups/t1 The following example accepts traffic arriving at workloads in namespaces that belong to security group t1 traffic, from workloads belonging to workspace w2. Hence, only authenticated request to workloads in security group t1 coming from workloads in workspace w2 are accepted. All other request will be rejected.\napiVersion: security.tsb.tetrate.io/v2 kind: SecuritySetting metadata: name: defaults group: t1 workspace: w1 tenant: mycompany organization: myorg spec: authenticationSettings: trafficMode: REQUIRED authorization: mode: RULES rules: allow: - from: fqn: organizations/myorg/tenants/mycompany/workspaces/w2 to: fqn: organizations/myorg/tenants/mycompany/workspaces/w1/securitygroups/t1 The following example uses a combination of allows and denies to show how rules are evaluated. Let’s say we have a workspace w3 which contains 3 security groups, sg31, sg32, and sg33. Besides we also have workspace w1 and w2. Security group sg31 contains workloads that handle sensitive data, and we want to only accept requests arriving from the same workspace w3 and explicitly reject requests coming from sg32. Hence, only authenticated request to workloads in security group sg31 coming from workloads in workspace w3 and security group sg31 or sg33 will be accepted. Requests coming from sg32 will be rejected. Moreover, a request coming from any workload that belongs to another workspace (w1, or w2), or security group that belong to another workspace, will also be reject by default because it is not in the list of allowed resource FQNs.\napiVersion: security.tsb.tetrate.io/v2 kind: SecuritySetting metadata: name: defaults group: sg31 workspace: w3 tenant: mycompany organization: myorg spec: authenticationSettings: …","relpermalink":"/book/tsb/refs/tsb/security/v2/security-setting/","summary":"Security settings for proxy workloads in a security group.","title":"Security Setting"},{"content":" Service to map registered services to configuration groups.\nLookup The Lookup API allows resolving the groups that configure a particular service in the registry. It allows lookups given a service, but also reverse lookups to get all the services in the registry that are configured by a particular workspace or group.\nGroups rpc Groups (tetrateio.api.tsb.registry.v2.GroupLookupRequest ) returns (tetrateio.api.tsb.registry.v2.GroupLookupResponse )\nRequires ReadTrafficGroup, ReadSecurityGroup, ReadGatewayGroup, ReadIstioInternalGroup\nGet all the groups that configure the given service in the registry.\nServices rpc Services (tetrateio.api.tsb.registry.v2.ServiceLookupRequest ) returns (tetrateio.api.tsb.registry.v2.ServiceLookupResponse )\nRequires ReadRegisteredService\nGet all the services in the registry that are part of the given selector. This method can be used to resolve the registered services that are part of a workspace or group. This method can be also used to figure out how applying a selector could affect the platform and have an understanding of which of the existing services would be included in the selection.\nGroupLookupRequest Request to lookup the groups that configure a particular registered service.\nField Description Validation Rule fqn\nstring REQUIRED The FQN of the registered service to lookup.\nstring = { min_len: 1}\nGroupLookupResponse List of groups that configure the requested service.\nField Description Validation Rule trafficGroups\nList of tetrateio.api.tsb.traffic.v2.Group The traffic groups that configure the given registered service.\n–\nsecurityGroups\nList of tetrateio.api.tsb.security.v2.Group The security groups that configure the given registered service.\n–\ngatewayGroups\nList of tetrateio.api.tsb.gateway.v2.Group The gateway groups that configure the given registered service.\n–\nistioInternalGroups\nList of tetrateio.api.tsb.istiointernal.v2.Group The istio internal groups that configure the given registered service.\n–\nServiceLookupRequest Request for all the services in the registry that are part of the given selector.\nField Description Validation Rule selector\ntetrateio.api.tsb.types.v2.NamespaceSelector REQUIRED Selector used to filter services.\n–\nparent\nstring REQUIRED The FQN of the parent object where services will be looked up\nstring = { min_len: 1}\nServiceLookupResponse List of services that are included in the provided namespace selector.\nField Description Validation Rule services\nList of tetrateio.api.tsb.registry.v2.Service The affected services\n–\n","relpermalink":"/book/tsb/refs/tsb/registry/v2/lookup-service/","summary":"Service to map registered services to configuration groups.","title":"Service Registry Lookup Service"},{"content":" Service Routes can be used by service owners to configure traffic shifting across different versions of a service in a Traffic Group. The traffic to this service can originate from sidecars in the same or different traffic groups, as well as gateways.\nThe following example yaml defines a Traffic Group g1 in the namespaces ns1, ns2 and ns3, owned by its parent Workspace w1. Then it defines a Service Route for the reviews service in the ns1 namespace with two subsets: v1 and v2, where 80% of the traffic to the reviews service is sent to v1 while the remaining 20% is sent to v2.\napiVersion: traffic.tsb.tetrate.io/v2 kind: Group metadata: name: t1 workspace: w1 tenant: mycompany organization: myorg spec: namespaceSelectors: - name: \u0026#34;*/ns1\u0026#34; - name: \u0026#34;*/ns2\u0026#34; - name: \u0026#34;*/ns3\u0026#34; configMode: BRIDGED --- apiVersion: traffic.tsb.tetrate.io/v2 kind: ServiceRoute metadata: name: reviews group: g1 workspace: w1 tenant: mycompany organization: myorg spec: service: ns1/reviews.svc.cluster.local subsets: - name: v1 labels: version: v1 weight: 80 - name: v2 labels: version: v2 weight: 20 Server side load balancing can be set through the combination of portLevelSettings and stickySession. The following ServiceRoute will generate two routes:\nAn HTTP route matching traffic on port 8080 and routing it 80:20 between v1:v2, targeting port 8080. The server side load balancing will be based on header. A TCP route matching traffic on port 443, and routing it 80:20 between v1:v2, targeting port 443. The server side load balancing will be based on source IP. apiVersion: traffic.tsb.tetrate.io/v2 kind: ServiceRoute metadata: name: reviews group: g1 workspace: w1 tenant: mycompany organization: myorg spec: service: ns1/reviews.svc.cluster.local portLevelSettings: - port: 8080 trafficType: HTTP stickySession: header: x-session-hash - port: 443 trafficType: TCP stickySession: useSourceIp: true subsets: - name: v1 labels: version: v1 weight: 80 - name: v2 labels: version: v2 weight: 20 Note: For TCP routes, only source IP (useSourceIp: true) is a valid load balancing hash key. Any other hash keys will be invalid.\nYou can also apply port settings just to a subset, such as in the following example where for subset v2 the source IP is used for sticky sessions.\napiVersion: traffic.tsb.tetrate.io/v2 kind: ServiceRoute metadata: name: reviews group: t1 workspace: w1 tenant: mycompany organization: myorg spec: service: ns1/reviews.svc.cluster.local portLevelSettings: - port: 8000 trafficType: TCP - port: 443 trafficType: HTTP stickySession: header: x-sticky-hash subsets: - name: v1 labels: version: v1 weight: 80 - name: v2 labels: version: v2 weight: 20 portLevelSettings: - port: 8000 trafficType: TCP stickySession: useSourceIp: true If the service exposes more than one port, then all such ports with protocols need to be specified in top level portLevelSettings. Explicit routes can be specified within httpRoutes or tcpRoutes sections. You can also specify match conditions within each httpRoute to match the incoming traffic and route the traffic accordingly.\nThe ServiceRoute below has two HTTP routes:\nThe first route matches traffic on reviews.svc.cluster.local:8080/productpage endpoint and end-user: jason header and routes 80% of traffic to subset “v1” and 20% to subset “v2”. The second route is the default HTTP route, which matches traffic on reviews.ns1.svc.cluster.local:8080/productpage endpoint, and routes 50% of traffic to subset “v1” and remaining 50% to subset “v2”. apiVersion: traffic.xcp.tetrate.io/v2 kind: ServiceRoute metadata: name: reviews group: t1 workspace: w1 tenant: mycompany organization: myorg spec: service: ns1/reviews.svc.cluster.local portLevelSettings: - port: 8080 trafficType: HTTP subsets: - name: v1 labels: version: v1 weight: 80 - name: v2 labels: version: v2 weight: 20 httpRoutes: - name: http-route-match-productpage-endpoint match: - name: match-productpage-endpoint uri: prefix: /productpage headers: end-user: exact: jason port: 8080 destination: - subset: v1 weight: 80 port: 8080 - subset: v2 weight: 20 port: 8080 - name: http-route-default match: - name: match-default uri: prefix: /productpage port: 8080 destination: - subset: v1 weight: 50 port: 8080 - subset: v2 weight: 50 port: 8080 Note: Default routes will be generated automatically only if a port is specified in top level portLevelSettings but not used in any match conditions of httpRoutes, tcpRoutes or tlsRoutes (or if no routes are specified). In all other conditions, all routes have to defined explicitly.\nFor example, the ServiceRoute below will generate a default-http-route matching on port 8080 and will route traffic in the ratio 80:20 between v1:v2.\napiVersion: traffic.xcp.tetrate.io/v2 kind: ServiceRoute metadata: name: reviews group: t1 workspace: w1 tenant: mycompany organization: myorg spec: service: ns1/reviews.ns1.svc.cluster.local portLevelSettings: - port: 8080 trafficType: HTTP subsets: - name: v1 labels: version: v1 weight: 80 - name: v2 labels: …","relpermalink":"/book/tsb/refs/tsb/traffic/v2/service-route/","summary":"Configuration affecting routing for services in a traffic group.","title":"Service Route"},{"content":" ServiceSecuritySetting allows configuring security related properties such as TLS authentication and access control for traffic arriving at a particular service in a security group. These settings will replace the security group wide settings for this service.\nThe following example defines a security setting that applies to the service foo in namespace ns1 that only allows mutual TLS authenticated traffic from other proxy workloads in the same group.\napiVersion: security.tsb.tetrate.io/v2 kind: ServiceSecuritySetting metadata: name: foo-auth group: sg1 workspace: w1 tenant: mycompany org: myorg spec: service: ns1/foo.ns1.svc.cluster.local settings: authentication: REQUIRED authorization: mode: GROUP The following example customizes the Extensions to enable the execution of the WasmExtensions list specified, detailing custom properties for the execution of each extension.\napiVersion: security.tsb.tetrate.io/v2 kind: ServiceSecuritySetting metadata: name: foo-wasm-plugin group: sg1 workspace: w1 tenant: mycompany org: myorg spec: service: ns1/foo.ns1.svc.cluster.local settings: extension: - fqn: hello-world # fqn of imported extensions in TSB config: foo: bar ServiceSecuritySetting A service security setting applies configuration to a service in a security group. Missing fields will inherit values from the workspace-wide setting if any.\nField Description Validation Rule service\nstring REQUIRED The service on which the configuration is being applied. Must be in namespace/FQDN format.\nstring = { pattern: ^[^/]+/[^/]+$}\nsettings\ntetrateio.api.tsb.security.v2.SecuritySetting Security settings to apply to this service.\n–\nsubsets\nList of tetrateio.api.tsb.security.v2.ServiceSecuritySetting.Subset Subset specific settings that will replace the service wide settings for the specified service subsets.\n–\nconfigGenerationMetadata\ntetrateio.api.tsb.types.v2.ConfigGenerationMetadata Metadata values that will be add into the Istio generated configurations. When using YAML APIs liketctl or gitops, put them into the metadata.labels or metadata.annotations instead. This field is only necessary when using gRPC APIs directly.\n–\nSubset Subset allows replacing the settings for a specific version of a service.\nField Description Validation Rule name\nstring REQUIRED Name used to refer to the subset. This must match a subset defined in the ServiceRoute for this service, else it will be omitted.\nstring = { min_len: 1}\nsettings\ntetrateio.api.tsb.security.v2.SecuritySetting REQUIRED Security settings to apply to this service subset.\nmessage = { required: true}\n","relpermalink":"/book/tsb/refs/tsb/security/v2/service-security-setting/","summary":"Service specific security settings for proxy workloads in a security group.","title":"Service Security Setting"},{"content":" Service to manage registration of services in the TSB Service Registry.\nRegistration The service registration API allows to manage the services that exist in the catalog. It exposes methods to register and unregister individual services as well as methods to keep all the services in a given cluster in sync.\nListServices rpc ListServices (tetrateio.api.tsb.registry.v2.ListServicesRequest ) returns (tetrateio.api.tsb.registry.v2.ListServicesResponse )\nList the services that have been registered in an organization\nGetService rpc GetService (tetrateio.api.tsb.registry.v2.GetServiceRequest ) returns (tetrateio.api.tsb.registry.v2.Service )\nRequires ReadRegisteredService\nGet the details of a registered service\nGetServiceRequest Request to retrieve a registered service.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the registered service.\nstring = { min_len: 1}\nListServicesRequest Request to list registered services.\nField Description Validation Rule parent\nstring REQUIRED Parent resource to list registered services from.\nstring = { min_len: 1}\nListServicesResponse Response with a list of registered services\nField Description Validation Rule services\nList of tetrateio.api.tsb.registry.v2.Service The requested registered services\n–\nRegisterServiceRequest Request to register a service in a given parent (organization).\nField Description Validation Rule parent\nstring REQUIRED Organization where the service will be registered\nstring = { min_len: 1}\ncluster\nstring REQUIRED Name of the cluster where the service belongs to. This will be used to load the deduplication settings that have been configured for the cluster where the service belongs.\nstring = { min_len: 1}\nshortName\nstring REQUIRED Short name for the service, used to uniquely identify it within the organization.\nstring = { min_len: 1}\nnamespace\nstring REQUIRED Namespace associated with the service. It will be used in deduplication logic.\nstring = { min_len: 1}\nhostnames\nList of string The hostnames by which this service is accessed. Can correspond to the hostname of an internal service or that ones of a virtual host on a gateway.\n–\nports\nList of tetrateio.api.tsb.registry.v2.Port The set of ports on which this service is exposed.\n–\nsubsets\nList of string Subset denotes a specific version of a service. By default the ‘version’ label is used to designate subsets of a workload. Known subsets for the service.\n–\nserviceType\ntetrateio.api.tsb.registry.v2.ServiceType REQUIRED Internal/external/load balancer service.\nenum = { defined_only: true}\nexternalAddresses\nList of string For kubernetes services of type load balancer, this field contains the list of lb hostnames or IPs assigned to the service.\n–\nstate\ntetrateio.api.tsb.registry.v2.State REQUIRED State of the service (registered/observed/controlled)\nenum = { defined_only: true}\nsource\nstring REQUIRED Source of the service: Kubernetes, Istio, Consul, etc.\nstring = { min_len: 1}\ncanonicalName\nstring optional canonical name that identify this service.\n–\nspiffeIds\nList of string List of SPIFFE identities used by the workloads of the service.\n–\nUnregisterServiceRequest Request to unregister a service from the registry\nField Description Validation Rule parent\nstring REQUIRED Organization from where the service will be unregistered\nstring = { min_len: 1}\nshortName\nstring REQUIRED Name attribute of the service\nstring = { min_len: 1}\ncluster\nstring REQUIRED Name of the cluster of the service.\nstring = { min_len: 1}\nnamespace\nstring REQUIRED Namespace of the service.\nstring = { min_len: 1}\n","relpermalink":"/book/tsb/refs/tsb/registry/v2/registration-service/","summary":"Service to manage registration of services in the TSB Service Registry.","title":"Servive Registry Registration Service"},{"content":" Source describes a set of observed resources that have a group of metrics that emit measurements at runtime. A source specifies what is being observed (which resource types: service, ingress hostnames, relation, …) and how it is being observed (with which scope of observation).\nA telemetry source can observe different types of resources in a single or aggregated way depending on the defined scope. A scope can be of type ServiceScope, IngressScope, or RelationScope, and they define the wingspan of the telemetry source in the mesh. Each scope contains information to determine if it is a single standalone source or an aggregation of standalone sources of the same type.\nServiceScope can be one of the following types which define the span of a service’s telemetry source in the mesh as:\nINSTANCE: A single specific service instance (pod or VM) in a cluster. SERVICE: An aggregation of all instances of a specific service of a concrete version (subset) in a cluster. SUBSET: An aggregation of all instances of a specific service of a concrete version (subset) across clusters. GLOBAL: An aggregation of all instances from all the versions of a specific service across clusters. IngressScope can be one the following types which define the span of Ingress hostname’s telemetry source in the mesh as:\nHOSTNAME: A ingress’s hostname in a concrete cluster. GLOBAL: A ingress’s hostname across clusters. A Telemetry source can also observe relation between resources. A relation is the physical connection between resources when a call between them has been done. For instance, a relation exists when a gateway calls a service or vice versa, or a service calls another service. That relation (call) can be seen (detected) from the server side, client-side, or both. For instance, when a gateway calls a service, and both resources are observed, the relation can be seen from both sides, the client and the server. In this case, the gateway is the client-side of the relation observation and the service is the server side of the observation. Each of the observation points (client or server) in the relation will produce different measurements for the same metric. Which means that, if we take the duration metric of the relation, we will have a concrete value (measurement) from the client point of view and another value from the server point view, where the client-side observed duration will be greater than the server side duration, in which the difference between durations is the network/transport introduced latency.\nRelationScope can be one the following types which define the span of a relation telemetry source in the mesh as:\nSERVICE: A relation between logical services. To understand a bit better what and how a telemetry source can observe, let’s assume we have deployed the classic Istio bookinfo demo application in 2 clusters, demo and demo-disaster-recovery. If we take as an example the reviews service which has 3 different versions (subsets V1, V2, and V3) for what is being observed, we will have different telemetry sources available which will tell us how (which scope) they are being observed.\nAn INSTANCE scoped telemetry source for a concrete review service instance (pod) running on the demo cluster will be:\napiVersion: observability.telemetry.tsb.tetrate.io/v2 kind: Source metadata: organization: myorg service: reviews.bookinfo name: reviews-v1-545db77b95-vhtlj spec: belongsTo: organizations/myorg/services/reviews.bookinfo metric_source_key: djF8cmV2aWV3c3xib29raW5mb3xkZW1vfC0=.1_cmV2aWV3cy12MS01NDVkYjc3Yjk1LXZodGxq service_scopes: - type: INSTANCE scope: instance: reviews-v1-545db77b95-vhtlj subset: v1 service: reviews namespace: bookinfo cluster: demo deployment: organizations/myorg/clusters/demo/namespaces/bookinfo/services/reviews A SUBSET scoped telemetry source for the reviews service of v1 subset running on the demo cluster will be:\napiVersion: observability.telemetry.tsb.tetrate.io/v2 kind: Source metadata: organization: myorg service: reviews.bookinfo name: reviews-v1-demo spec: belongsTo: organizations/myorg/services/reviews.bookinfo metric_source_key: djF8cmV2aWV3c3xib29raW5mb3xkZW1vfC0=.1 service_scopes: - type: SUBSET scope: subset: v1 service: reviews namespace: bookinfo cluster: demo deployment: organizations/myorg/clusters/demo/namespaces/bookinfo/services/reviews A GLOBAL_SUBSET scope telemetry source for the reviews services of version v1 running across clusters will be:\napiVersion: observability.telemetry.tsb.tetrate.io/v2 kind: Source metadata: organization: myorg service: reviews.bookinfo name: reviews-v1 spec: belongsTo: organizations/myorg/services/reviews.bookinfo metric_source_key: djF8cmV2aWV3c3xib29raW5mb3wqfCo=.1 service_scopes: - type: GLOBAL_SUBSET scope: subset: v1 service: reviews namespace: bookinfo deployment: organizations/myorg/clusters/demo/namespaces/bookinfo/services/reviews A GLOBAL scoped telemetry source for the reviews service of all subsets(v1, v2, and v3) running across all clusters will …","relpermalink":"/book/tsb/refs/tsb/observability/telemetry/v2/source/","summary":"A group of metrics observing scoped resources.","title":"Source"},{"content":" Each resource in TSB is able to provide a status to let the user know it’s current integrity. Some resources, like configurations for ingress, traffic and security, are not immediately applied as soon as TSB accepts any modification from user. In these cases, the status will provide enough information to know when it is really applying to the affected workloads. This allows any user or CI/CD process to poll the status of any desired resource and proceed accordingly.\nThere are two types of resources, the ones that aggregate the status of children resources and the ones that do not. Check the documentation for the different details object types for further information.\nAs an example, lets say the user pushes an IngressGateway configuration. IngressGateway does not aggregate status of children resources, but the other way around: its parent resource GatewayGroup does aggregate its status.\nWhen the requests succeeds in TSB server, that resource’s status will reach the ACCEPTED status with a TSB_ACCEPTED event in its configEvents details:\napiVersion: api.tsb.tetrate.io/v2 kind: ResourceStatus metadata: name: bookinfo-gateway organization: my-org tenant: my-tenant workspace: bookinfo-ws gatewaygroup: bookinfo-gw-group spec: status: ACCEPTED configEvents: events: - etag: \u0026#39;\u0026#34;sMlEWPbvm6M=\u0026#34;\u0026#39; timestamp: \u0026#34;2022-01-11T10:11:41.784168161Z\u0026#34; type: TSB_ACCEPTED Then, when pushed to MPC it succeeds and stays in ACCEPTED status, and the event list reflects the new event data, which will become:\n// omiting the rest of the fields for simplicity spec: status: ACCEPTED configEvents: events: - etag: \u0026#39;\u0026#34;sMlEWPbvm6M=\u0026#34;\u0026#39; timestamp: \u0026#34;2022-01-11T10:11:43.264330637Z\u0026#34; type: MPC_ACCEPTED - etag: \u0026#39;\u0026#34;sMlEWPbvm6M=\u0026#34;\u0026#39; timestamp: \u0026#34;2022-01-11T10:11:41.784168161Z\u0026#34; type: TSB_ACCEPTED Later on, if there is an error in the MPC underlying layers such as XCP Central, a new event will be propagated and appended to the resource status that will change to status FAILED with the corresponding message.\n# omiting the rest of the fields for simplicity spec: status: FAILED message: \u0026#34;IngressGateway.xcp.tetrate.io \\\u0026#34;INVALID-96010ce1d9b7df5c\\\u0026#34; is invalid: metadata.name: Invalid value: \\\u0026#34;INVALID-96010ce1d9b7df5c\\\u0026#34;: a DNS-1123 subdomain must consist of lower case alphanumeric characters, \u0026#39;-\u0026#39; or \u0026#39;.\u0026#39;, and must start and end with an alphanumeric character (e.g. \u0026#39;example.com\u0026#39;, regex used for validation is \u0026#39;[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*\u0026#39;)\u0026#34; configEvents: events: - etag: \u0026#39;\u0026#34;sMlEWPbvm6M=\u0026#34;\u0026#39; message: \u0026#34;IngressGateway.xcp.tetrate.io \\\u0026#34;INVALID-96010ce1d9b7df5c\\\u0026#34; is invalid: metadata.name: Invalid value: \\\u0026#34;INVALID-96010ce1d9b7df5c\\\u0026#34;: a DNS-1123 subdomain must consist of lower case alphanumeric characters, \u0026#39;-\u0026#39; or \u0026#39;.\u0026#39;, and must start and end with an alphanumeric character (e.g. \u0026#39;example.com\u0026#39;, regex used for validation is \u0026#39;[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*\u0026#39;)\u0026#34; reason: \u0026#34;ValidationFailed\u0026#34; timestamp: \u0026#34;2022-01-11T10:11:43.444335769Z\u0026#34; type: XCP_REJECTED - etag: \u0026#39;\u0026#34;sMlEWPbvm6M=\u0026#34;\u0026#39; timestamp: \u0026#34;2022-01-11T10:11:43.264330637Z\u0026#34; type: MPC_ACCEPTED - etag: \u0026#39;\u0026#34;sMlEWPbvm6M=\u0026#34;\u0026#39; timestamp: \u0026#34;2022-01-11T10:11:41.784168161Z\u0026#34; type: TSB_ACCEPTED Another example of a status of a resource that aggregates its children status could be the following:\napiVersion: api.tsb.tetrate.io/v2 kind: ResourceStatus metadata: name: bookinfo organization: tetrate tenant: tetrate workspace: bookinfo spec: aggregatedStatus: configEvents: events: - etag: \u0026#39;\u0026#34;XAdtTSjZGic=\u0026#34;\u0026#39; timestamp: \u0026#34;2022-01-11T16:50:15.571985056Z\u0026#34; type: XCP_ACCEPTED - etag: \u0026#39;\u0026#34;XAdtTSjZGic=\u0026#34;\u0026#39; timestamp: \u0026#34;2022-01-11T16:50:15.545956009Z\u0026#34; type: MPC_ACCEPTED - etag: \u0026#39;\u0026#34;XAdtTSjZGic=\u0026#34;\u0026#39; timestamp: \u0026#34;2022-01-11T16:50:13.547777908Z\u0026#34; type: TSB_ACCEPTED status: ACCEPTED In case of errors, the children_errors map would be filled.\nFinally, an example of a status of a non-configurable resource like a Tenant would not have any details. This kind of resources don’t aggregate status either. This kind of resource will reach the READY status once it’s request has been processed by the TSB server.\napiVersion: api.tsb.tetrate.io/v2 kind: ResourceStatus metadata: name: tetrate organization: tetrate spec: status: READY AggregatedStatus AggregatedStatus is used by resources with children to aggregate both the sequence of events and the status of its children resources.\nField Description Validation Rule configEvents\ntetrateio.api.tsb.v2.ConfigEvents ConfigEvents is the list of resource events that occurred during the lifecycle of the resource.\n–\nchildren\nmap\u0026lt;string , tetrateio.api.tsb.v2.AggregatedStatus.ChildStatus \u0026gt; Map of children resource FQNs to their status.\n–\nchildrenStatus\ntetrateio.api.tsb.v2.AggregatedStatus.ChildStatus Children status is a status summary of all the children statuses. If all of them are READY, children_status will be READY as well. If any is not READY, the worst status will be used for children_status\n–\nChildStatus ChildStatus contains the status details for a particular child resource, and a …","relpermalink":"/book/tsb/refs/tsb/v2/status/","summary":"Status API for TSB resources","title":"Status"},{"content":" Service to retrieve the status for TSB resources\nStatus The Status services exposes methods to retrieve the status for any resource managed by TSB.\nGetStatus rpc GetStatus (tetrateio.api.tsb.v2.GetStatusRequest ) returns (tetrateio.api.tsb.v2.ResourceStatus )\nGiven a resource fully-qualified name of a resource returns its current status.\nGetStatusRequest Request to retrieve the status of a resource.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the resource to retrieve the status.\nstring = { min_len: 1}\n","relpermalink":"/book/tsb/refs/tsb/v2/status-service/","summary":"Service to retrieve the status for TSB resources","title":"Status Service"},{"content":"tctl\nSynopsis\nTetrate Service Bridge CLI\nOptions\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -h, --help help for tctl ","relpermalink":"/book/tsb/reference/cli/reference/tctl/","summary":"Tctl command","title":"tctl"},{"content":"Apply a configuration to a resource by filename or stdin\ntctl apply [flags] Examples\ntctl apply -f config.yaml Options\n-f, --file string File or directory containing configuration to apply [required] -h, --help help for apply -o, --output-type string Response output type: table, yaml, json Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) ","relpermalink":"/book/tsb/reference/cli/reference/apply/","summary":"Apply command","title":"tctl apply"},{"content":"Collect the state of a Kubernetes cluster for debugging.\ntctl collect [flags] Examples\n# Collect without any obfuscation or redaction tctl collect # Collect without archiving results (useful for local debugging) tctl collect --disable-archive # Collect and redact with user-provided regex tctl collect --redact-regexes \u0026lt;regex-one\u0026gt;,\u0026lt;regex-two\u0026gt; # Collect and redact with presets tctl collect --redact-presets networking Options\n--disable-archive output files rather than tarball -h, --help help for collect -o, --output-directory string the path to write the collected files under (default \u0026#34;tctl-[timestamp]\u0026#34;) --redact-presets strings Comma-separated list of redaction presets to use in collection data obfuscation. Available presets: - \u0026#34;networking\u0026#34;: Obfuscate any data that matches IPv4 or IPv6 addresses. Any matches are replaced with SHA-256 hashes that are converted back to a valid IPv4 or IPv6. --redact-regexes strings Obfuscate the data collected based on the list of provided regexes. Any matches are replaced with SHA-256 hashes of the matched string. Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) ","relpermalink":"/book/tsb/reference/cli/reference/collect/","summary":"Collect command","title":"tctl collect"},{"content":"Generates tab completion scripts\ntctl completion \u0026lt;bash|zsh|fish|powershell\u0026gt; Examples\nBash: $ source \u0026lt;(tctl completion bash) # To load completions for each session, execute once: Linux: $ tctl completion bash | sudo tee -a /etc/bash_completion.d/tctl \u0026gt; /dev/null MacOS: $ tctl completion bash | sudo tee -a $(brew --prefix)/etc/bash_completion.d/tctl \u0026gt; /dev/null Zsh: # If shell completion is not already enabled in your environment you will need # to enable it. You can execute the following once: $ echo \u0026#34;autoload -U compinit; compinit\u0026#34; \u0026gt;\u0026gt; ~/.zshrc # To load completions for each session, execute once: $ tctl completion zsh \u0026gt; \u0026#34;${fpath[1]}/_tctl\u0026#34; # You will need to start a new shell for this setup to take effect. Fish: $ tctl completion fish | source # To load completions for each session, execute once: $ tctl completion fish \u0026gt; ~/.config/fish/completions/tctl.fish Options\n-h, --help help for completion Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) ","relpermalink":"/book/tsb/reference/cli/reference/completion/","summary":"Completion command","title":"tctl completion"},{"content":"Manages CLI configuration.\nOptions\n-h, --help help for config Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) tctl config clusters Manages configuration of clusters\nOptions\n-h, --help help for clusters Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) tctl config clusters delete Delete cluster configurations\ntctl config clusters delete \u0026lt;name\u0026gt; [\u0026lt;name\u0026gt; ... \u0026lt;name\u0026gt;] [flags] Options\n-h, --help help for delete Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) tctl config clusters list List all available clusters\ntctl config clusters list [flags] Options\n-h, --help help for list Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) tctl config clusters rename Rename a cluster configuration\ntctl config clusters rename \u0026lt;current-name\u0026gt; \u0026lt;new-name\u0026gt; [flags] Options\n-h, --help help for rename Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) tctl config clusters set Set a cluster configuration\ntctl config clusters set \u0026lt;name\u0026gt; [flags] Options\n--bridge-address string Bridge address (for example, tsb.tetrate.io:8443, 192.0.2.1:8443, [2001:db8::1]:8443) --timeout duration bridge client timeout (default 30s) --tls-disabled Don\u0026#39;t use TLS when connecting to TSB --tls-insecure Don\u0026#39;t validate TLS server certificates when connecting to TSB --tls-custom-ca-file string Path to a custom CA bundle to use when validating TLS connections to TSB --managementplane string Management plane namespace --controlplane string Control plane namespace --context string The name of the kubeconfig context --kubeconfig string Kubernetes configuration directory --auto Automatically get management plane address from current kube context --max-grpc-msg-size int Max gRPC message size (default 20971520) -h, --help help for set Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) tctl config profiles Manages configuration profiles\nOptions\n-h, --help help for profiles Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) tctl config profiles delete Delete configuration profiles\ntctl config profiles delete \u0026lt;name\u0026gt; [\u0026lt;name\u0026gt; ... \u0026lt;name\u0026gt;] [flags] Options\n-h, --help help for delete -r, --recursive Delete the linked clusters and users as well Options inherited …","relpermalink":"/book/tsb/reference/cli/reference/config/","summary":"Config command","title":"tctl config"},{"content":"Delete an object\ntctl delete [\u0026lt;apiVersion/kind\u0026gt; \u0026lt;name\u0026gt;] [flags] Examples\n# Delete a cluster using the apiVersion/Kind pattern tctl delete api.tsb.tetrate.io/v2/Cluster my-cluster # Delete a single workspace using the short form tctl delete ws my-workspace These are the available short forms: aab\tApplicationAccessBindings ab\tAccessBindings ap\tAuthorizationPolicy apiab\tAPIAccessBindings app\tApplication cs\tCluster dr\tDestinationRule ef\tEnvoyFilter eg\tEgressGateway gab\tGatewayAccessBindings gg\tGatewayGroup gw\tnetworking.istio.io/v1beta1/Gateway gwt\tgateway.tsb.tetrate.io/v2/Gateway iab\tIstioInternalAccessBindings ig\tIngressGateway iig\tIstioInternalGroup oab\tOrganizationAccessBindings org\tOrganization os\tOrganizationSetting otm\tMetric ots\tSource pa\tPeerAuthentication ra\tRequestAuthentication sa\tServiceAccount sab\tSecurityAccessBindings sd\tSidecar se\tServiceEntry sg\tSecurityGroup sr\tServiceRoute ss\tSecuritySetting sss\tServiceSecuritySetting svc\tService t1\tTier1Gateway tab\tTrafficAccessBindings tg\tTrafficGroup tnab\tTenantAccessBindings tns\tTenantSetting ts\tTrafficSetting vs\tVirtualService wab\tWorkspaceAccessBindings wext\tWasmExtension wp\tWasmPlugin ws\tWorkspace wss\tWorkspaceSetting For API version and kind, please refer to: https://docs.tetrate.io/service-bridge/latest/en-us/reference Options\n-f, --file string File containing configuration to apply --org string Organization the object belongs to --tenant string Tenant the object belongs to -w, --workspace string Workspace the object belongs to -g, --group string Group the object belongs to -t, --trafficgroup string Traffic group the object belongs to -s, --securitygroup string Security group the object belongs to -l, --gatewaygroup string Gateway group the object belongs to -i, --istiointernalgroup string Istio internal group the object belongs to -a, --application string Application the object belongs to --api string API the object belongs to --force Force object deletion even if deletion protection is enabled -h, --help help for delete Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) ","relpermalink":"/book/tsb/reference/cli/reference/delete/","summary":"Delete command","title":"tctl delete"},{"content":"Edit one or multiple objects\ntctl edit \u0026lt;apiVersion/kind | kind | shortform\u0026gt; [\u0026lt;name\u0026gt;] [flags] Examples\nEdit will perform a get on the given object and launch $EDITOR (environment variable needs to be set) for editing it then apply the changes back. # Edit a workspace. tctl edit workspace foo # Edit a tenant tctl edit tenant my-department # Edit an IngressGateway tctl edit ingressgateway myIng --workspace foo --gatewaygroup bar # You can also edit lists of objects # Edit multiple gateway groups at once tctl edit gatewaygroup --workspace foo --gatewaygroup bar baz # Or even all workspaces at once tctl edit workspace --tenant foo These are the available short forms: aab\tApplicationAccessBindings ab\tAccessBindings ap\tAuthorizationPolicy apiab\tAPIAccessBindings app\tApplication cs\tCluster dr\tDestinationRule ef\tEnvoyFilter eg\tEgressGateway gab\tGatewayAccessBindings gg\tGatewayGroup gw\tnetworking.istio.io/v1beta1/Gateway gwt\tgateway.tsb.tetrate.io/v2/Gateway iab\tIstioInternalAccessBindings ig\tIngressGateway iig\tIstioInternalGroup oab\tOrganizationAccessBindings org\tOrganization os\tOrganizationSetting otm\tMetric ots\tSource pa\tPeerAuthentication ra\tRequestAuthentication sa\tServiceAccount sab\tSecurityAccessBindings sd\tSidecar se\tServiceEntry sg\tSecurityGroup sr\tServiceRoute ss\tSecuritySetting sss\tServiceSecuritySetting svc\tService t1\tTier1Gateway tab\tTrafficAccessBindings tg\tTrafficGroup tnab\tTenantAccessBindings tns\tTenantSetting ts\tTrafficSetting vs\tVirtualService wab\tWorkspaceAccessBindings wext\tWasmExtension wp\tWasmPlugin ws\tWorkspace wss\tWorkspaceSetting For API version and kind, please refer to: https://docs.tetrate.io/service-bridge/latest/en-us/reference Options\n--org string Organization the object belongs to --tenant string Tenant the object belongs to -w, --workspace string Workspace the object belongs to -g, --group string Group the object belongs to -t, --trafficgroup string Traffic group the object belongs to -s, --securitygroup string Security group the object belongs to -l, --gatewaygroup string Gateway group the object belongs to -i, --istiointernalgroup string Istio internal group the object belongs to -a, --application string Application the object belongs to --api string API the object belongs to -o, --output-directory string Response output type: table, yaml, json -h, --help help for edit Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) ","relpermalink":"/book/tsb/reference/cli/reference/edit/","summary":"Edit command","title":"tctl edit"},{"content":"Experimental commands that may be modified or deprecated\nOptions\n-h, --help help for experimental Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) tctl experimental app-ingress Run a Istio based Ingress Controller for your application\nSynopsis\nInstall a dedicated Ingress Controller in your environment to allow incoming/ingress traffic to be routed to your application. This controller comprises of 3 components - Istiod, Istio Ingressgateway and a TSB OpenAPI Translator. You can configure the Ingress Controller by either specifying Istio config directly in the application namespace or by specifying the OpenAPI document for your application - the OpenAPI translator converts the specification into Istio compatible configuration and applies it on your behalf.\nOptions\n-h, --help help for app-ingress --istio-hub string The hub for the Istio images in App Ingress (default \u0026#34;docker.io/istio\u0026#34;) --istio-tag string The tag for the Istio images in App Ingress (default \u0026#34;1.17.2\u0026#34;) -b, --openapi-backend-service string Name of the backend service implementing the OpenAPI specification -o, --openapi-translator Enable the OpenAPI translator which generates Istio configs from an OpenAPI specfication and applies them --openapi-translator-hub string The hub for the OpenAPI Translator images in App Ingress (default \u0026#34;docker.io/tetrate\u0026#34;) --openapi-translator-tag string The tag for the OpenAPI Translator images in App Ingress (default \u0026#34;a0851637f824f2ae51fe10182c49c3c3fa32ed87\u0026#34;) Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) tctl experimental app-ingress docker-compose Run App Ingress in Docker using docker-compose.\nOptions\n-h, --help help for docker-compose --network string Docker network (default \u0026#34;app-ingress\u0026#34;) --output-dir string Output directory (default \u0026#34;./\u0026#34;) Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. --istio-hub string The hub for the Istio images in App Ingress (default \u0026#34;docker.io/istio\u0026#34;) --istio-tag string The tag for the Istio images in App Ingress (default \u0026#34;1.17.2\u0026#34;) -b, --openapi-backend-service string Name of the backend service implementing the OpenAPI specification -o, --openapi-translator Enable the OpenAPI translator which generates Istio configs from an OpenAPI specfication and applies them --openapi-translator-hub string The hub for the OpenAPI Translator images in App Ingress (default \u0026#34;docker.io/tetrate\u0026#34;) --openapi-translator-tag string The tag for the OpenAPI Translator images in App Ingress (default \u0026#34;a0851637f824f2ae51fe10182c49c3c3fa32ed87\u0026#34;) -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) tctl experimental app-ingress docker-compose generate Generate the required docker-compose.yaml and folder structure to bootstrap App Ingress.\ntctl experimental app-ingress docker-compose generate [OPTIONS] [flags] Options\n-h, --help help for generate Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. --istio-hub string The hub for the Istio images in App Ingress (default \u0026#34;docker.io/istio\u0026#34;) --istio-tag string The tag for the Istio images in App Ingress (default \u0026#34;1.17.2\u0026#34;) --network string Docker network (default \u0026#34;app-ingress\u0026#34;) -b, --openapi-backend-service string Name of the backend service implementing the OpenAPI specification -o, --openapi-translator Enable the OpenAPI translator which generates Istio configs from an OpenAPI specfication and applies them --openapi-translator-hub string The hub for the OpenAPI Translator images in App Ingress (default \u0026#34;docker.io/tetrate\u0026#34;) --openapi-translator-tag string The tag for the …","relpermalink":"/book/tsb/reference/cli/reference/experimental/","summary":"Experimental command","title":"tctl experimental"},{"content":"Get one or multiple objects\ntctl get \u0026lt;apiVersion/kind | kind | shortform\u0026gt; [\u0026lt;name\u0026gt;] [flags] Examples\n# List tenants using the apiVersion/Kind pattern tctl get api.tsb.tetrate.io/v2/Tenant # List workspaces using the kind tctl get workspace # Get a single workspace using the short form tctl get ws my-workspace # List gateway groups of a workspace tctl get --workspace my-workspace GatewayGroup # Get the access bindings of an ingress gateway tctl get accessbindings organizations/foo/tenants/foo/workspaces/foo/gatewaygroups/foo/ingressgateways/foo # Get all resources within a tenant tctl get all --tenant foo # Get all resources within a workspace tctl get all --tenant foo --workspace bar # Get all resources within a given group tctl get all --tenant foo --workspace bar --gatewaygroup baz # Get all resources within a tenant, referrencing a given FQDN tctl get all --tenant foo --fqdn some.fqdn.local # Get all IngressGateway within a given tenant tctl get all --tenant foo --kind IngressGateway # Get all access bindings within a given tenant tctl get all --tenant foo --api-version rbac.tsb.tetrate.io/v2 NOTE. Filters supplied to \u0026#34;tctl get all\u0026#34; are ANDed. # Get all IngressGateway within a given tenant AND that include the FQDN foo.local tctl get all --tenant foo --kind IngressGateway --fqdn foo.local Group kind is available for different APIs, so these helpers are available to easily retrieve them: - TrafficGroup - SecurityGroup - GatewayGroup - IstioInternalGroup These are the available short forms: aab\tApplicationAccessBindings ab\tAccessBindings ap\tAuthorizationPolicy apiab\tAPIAccessBindings app\tApplication cs\tCluster dr\tDestinationRule ef\tEnvoyFilter eg\tEgressGateway gab\tGatewayAccessBindings gg\tGatewayGroup gw\tnetworking.istio.io/v1beta1/Gateway gwt\tgateway.tsb.tetrate.io/v2/Gateway iab\tIstioInternalAccessBindings ig\tIngressGateway iig\tIstioInternalGroup oab\tOrganizationAccessBindings org\tOrganization os\tOrganizationSetting otm\tMetric ots\tSource pa\tPeerAuthentication ra\tRequestAuthentication sa\tServiceAccount sab\tSecurityAccessBindings sd\tSidecar se\tServiceEntry sg\tSecurityGroup sr\tServiceRoute ss\tSecuritySetting sss\tServiceSecuritySetting svc\tService t1\tTier1Gateway tab\tTrafficAccessBindings tg\tTrafficGroup tnab\tTenantAccessBindings tns\tTenantSetting ts\tTrafficSetting vs\tVirtualService wab\tWorkspaceAccessBindings wext\tWasmExtension wp\tWasmPlugin ws\tWorkspace wss\tWorkspaceSetting For API version and kind, please refer to: https://docs.tetrate.io/service-bridge/latest/en-us/reference Options\n--org string Organization the object belongs to --tenant string Tenant the object belongs to -w, --workspace string Workspace the object belongs to -g, --group string Group the object belongs to -t, --trafficgroup string Traffic group the object belongs to -s, --securitygroup string Security group the object belongs to -l, --gatewaygroup string Gateway group the object belongs to -i, --istiointernalgroup string Istio internal group the object belongs to -a, --application string Application the object belongs to --api string API the object belongs to -o, --output-type string Response output type: table, yaml, json (default \u0026#34;table\u0026#34;) --fqdn string FQDN to filter results of get all --kind string Kind to filter results of get all --api-version string apiVersion to filter results of get all --service string Service the object belongs to --telemetry-source string Telemetry source the object belongs to --max-concurrent-requests int Maximum of concurrent requests sent to TSB -h, --help help for get Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) ","relpermalink":"/book/tsb/reference/cli/reference/get/","summary":"Get command","title":"tctl get"},{"content":"Generates install manifests and applies it to a cluster\nOptions\n-h, --help help for install Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) tctl install cluster-certs Generate cluster certs for securely communicating with the management plane\ntctl install cluster-certs [flags] Examples\n# Retrieve cluster certs tctl install cluster-certs --cluster \u0026lt;cluster-name\u0026gt;\u0026#34; Options\n--cluster string The name of the cluster to generate certs for. -x, --context string The kube context for the management plane cluster. -n, --controlplane string The namespace in the cluster that the control plane is installed in. (default \u0026#34;istio-system\u0026#34;) -h, --help help for cluster-certs -k, --kubeconfig string The kubeconfig file for the management plane cluster. Must be able to manage secrets and cert-manager custom resources. -m, --managementplane string The namespace that the management plane is installed in. (default \u0026#34;tsb\u0026#34;) Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) tctl install cluster-service-account Generate a cluster service account key for securely communicating with the management plane\ntctl install cluster-service-account [flags] Examples\n# Create a cluster service account key tctl install cluster-service-account --cluster \u0026lt;cluster-name\u0026gt; Options\n--cluster string The name of the cluster to generate certs for. --create-cluster Create a cluster object in Service Bridge if it doesn\u0026#39;t exist (default true) -h, --help help for cluster-service-account Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) tctl install demo Install a batteries-included Service Bridge into a single Kubernetes cluster.\nSynopsis\nInstall a batteries-included Service Bridge into a single Kubernetes cluster.\nThe CLI will be automatically preconfigured to connect to the installed Service Bridge as an Administrator. The configuration will be saved in a profile named after the configured Kubernetes context, and the Bridge connection configuration and the user configuration will be named after the Kubernetes cluster where Service Bridge has been installed.\nThe Kubernetes context to deploy to is read from the environment’s configured kubeconfig. See https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/ for more information on kubeconfig.\ntctl install demo [flags] Examples\ntctl install demo --registry \u0026lt;registry-location\u0026gt; Options\n--admin-password string The password for the superuser. By default a secure password will be auto-generated. --cluster string The name of the demo cluster. (default \u0026#34;demo\u0026#34;) -h, --help help for demo -o, --org string The organization to configure (default \u0026#34;tetrate\u0026#34;) -r, --registry string The docker registry with the service bridge images [required] --set stringArray set values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2) --set-file stringArray set values from respective files specified via the command line (can specify multiple or separate values with commas: key1=path1,key2=path2) --set-string stringArray set STRING values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2) --timeout duration Timeout to login to the management plane. (default 30s) -f, --values strings specify values in a YAML file or a URL (can specify multiple) Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) tctl install image-sync Copy …","relpermalink":"/book/tsb/reference/cli/reference/install/","summary":"Install command","title":"tctl install"},{"content":"Configures the credentials for the given user\nSynopsis\nConfigures the credentials for the given user.\nThis command will exchange the given credentials for an access token that can be stored in the configuration profile. If the credentials are not provided as arguments to the login command, an interactive prompt will ask for all required information.\nOrganization and Tenant can also be configured with the following environment variables:\nTCTL_LOGIN_ORG TCTL_LOGIN_TENANT Both password based and OpenID Connect based authentication are supported. Depending on the configured authentication server there are several different authentication flags.\nWhen password based authentication is configured, both –username and –password are required flags. User credentials can also be configured with the following environment variables:\nTCTL_LOGIN_USERNAME TCTL_LOGIN_PASSWORD When OpenID Connect based authentication is configured, users should authenticate with a browser using an OpenID Connect Device Code. Device Code authentication is initiated with –use-device-code. Automation systems may choose to use a trusted token exchange with –use-token-exchange and –access-token or –id-token. In both cases the username is inferred from the OpenID Connect token subject claim and not the commandline flag.\nThe token exchange is done using the cluster settings defined in the current profile, and the user information will be stored in a user with the name \u0026lt;cluster name\u0026gt;-\u0026lt;username\u0026gt;. This user will be also set as the user for the current profile.\ntctl login [flags] Options\n--org string Name of the organization --tenant string Name of the tenant --username string Username --password string Password --use-device-code Use OIDC device code login --use-token-exchange Use OIDC token exchange --access-token string Access Token --id-token string ID Token -h, --help help for login Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) ","relpermalink":"/book/tsb/reference/cli/reference/login/","summary":"Login command","title":"tctl login"},{"content":"Opens the TSB console in the browser\ntctl ui [flags] Examples\ntctl ui Options\n-h, --help help for ui Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) ","relpermalink":"/book/tsb/reference/cli/reference/ui/","summary":"Ui command","title":"tctl ui"},{"content":"Offline validates a configuration from filename or stdin\ntctl validate [flags] Examples\ntctl validate -f config.yaml This syntactically validates the provided config and verifies the config of the defined selectors. Options\n-f, --file string File or directory containing configuration to validate [required] -h, --help help for validate Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) ","relpermalink":"/book/tsb/reference/cli/reference/validate/","summary":"Validate command","title":"tctl validate"},{"content":"Show the version of tctl and TSB\ntctl version [flags] Options\n--ascii Display the ASCII art for the TSB release -h, --help help for version --local-only If true, shows client version only (no server required) Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) ","relpermalink":"/book/tsb/reference/cli/reference/version/","summary":"Version command","title":"tctl version"},{"content":"Show the current user info\ntctl whoami [flags] Options\n-h, --help help for whoami -o, --output-type string Response output type: table, yaml, json (default \u0026#34;table\u0026#34;) Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) ","relpermalink":"/book/tsb/reference/cli/reference/whoami/","summary":"Whoami command","title":"tctl whoami"},{"content":" User represents a user that has been loaded from a configured Identity Provider (IdP) that can log into the platform. Currently, users are automatically synchronized by TSB from a configured LDAP server.\nThe following example creates a user named john under the organization myorg.\napiVersion: api.tsb.tetrate.io/v2 kind: User metadata: name: john organization: myorg spec: loginName: john firstName: John lastName: Doe displayName: John Doe email: john.doe@acme.com ServiceAccount can be created to leverage machine authentication via JWT tokens. Each service account has a key-pair that can be used to create signed JWT tokens that can be used to authenticate to TSB.\nThe following example creates a service account named my-sa under the organization myorg.\napiVersion: api.tsb.tetrate.io/v2 kind: ServiceAccount metadata: name: my-sa organization: myorg spec: displayName: My Service Account description: Service account used for service integrations Team is a named collection of users, service accounts, and other teams. Teams can be assigned access permissions on various resources. All members of a team inherit the access permissions assigned to the team.\nThe following example creates a team named org under the organization myorg with all members of product1 and product2 teams, and users alice and bob.\napiVersion: api.tsb.tetrate.io/v2 kind: Team metadata: name: org organization: myorg spec: members: - organizations/myorg/users/alice - organizations/myorg/users/bob - organizations/myorg/teams/product1 - organizations/myorg/teams/product2 ServiceAccount ServiceAccount represents a service account that can be used to access the TSB platform. Service accounts have a set of associated public and private keys that can be used to generate signed JWT tokens that are suitable to authenticate to TSB. A default key-pair is generated on service account creation and the public key is stored in TSB. Private keys are returned when service accounts are created, but TSB will not store them. It is up to the client to store them securely.\nField Description Validation Rule description\nstring A description of the resource.\n–\nkeys\nList of tetrateio.api.tsb.v2.ServiceAccount.KeyPair OUTPUT_ONLY Keys associated with the service account. A default key-pair is automatically created when the Service Account is created. Note that TSB does not store the private keys, so it is up to the client to store the returned private keys securely, as they are only returned once after creation. Additional keys can be added (and deleted) by using the corresponding key management APIs.\n–\nKeyPair Represents key-pair associated to the service account.\nField Description Validation Rule id\nstring OUTPUT_ONLY Unique identifier for this key-pair. This should be used as the kid (key id) when generating JWT tokens that are signed with this key-pair.\n–\npublicKey\nstring OUTPUT_ONLY The encoded public key associated with the service account. The encoding format is determined by the encoding field.\n–\nprivateKey\nstring OUTPUT_ONLY The encoded private key associated with the service account. TSB does not store the private key and it is up to the client to store it safely. The encoding format is determined by the encoding field.\n–\nencoding\ntetrateio.api.tsb.v2.ServiceAccount.KeyPair.Encoding Format in which the public and private keys are encoded. By default keys are returned in PEM format.\n–\ndefaultToken\nstring OUTPUT_ONLY A default access token that can be used to authenticate to TSB on behalf of the service account. TSB does not store this token and it is only returned when a service account key is created, similar to the private key. It is up to the client to store the token for future use or to use the TSB CLI to generate new tokens as explained in: https://docs.tetrate.io/service-bridge/latest/en-us/howto/service-accounts –\nTeam Team is a named collection of users under a tenant.\nField Description Validation Rule members\nList of string List of members under the team. The elements of this list are the FQNs of the team members. Team members can be users, service accounts or other teams.\n–\nsourceType\ntetrateio.api.tsb.v2.SourceType Where the team comes from. It can be a local team that exists only in TSB (type LOCAL) or it can be a team that has been synchronized from the Identity Provider (for example: type LDAP).\nenum = { defined_only: true}\nUser User represents a user from the Identity Provider that is allowed to log into the platform.\nField Description Validation Rule loginName\nstring REQUIRED The username used in the login credentials.\nstring = { min_len: 1}\nfirstName\nstring The first name of the user.\n–\nlastName\nstring The last name of the user, if any.\n–\nemail\nstring Email for the user where alerts and other notifications will be sent.\n–\nsourceType\ntetrateio.api.tsb.v2.SourceType Where the user comes from. It can be a local user that exists only in TSB (type LOCAL) or it can be a user that has been synchronized from the Identity Provider (for example: type …","relpermalink":"/book/tsb/refs/tsb/v2/team/","summary":"Configuration for managing users and teams.","title":"Teams and Users"},{"content":" Service to manage Users and Teams in TSB\nTeams The Teams service provides methods to manage the Users and Teams that exist in an Organization.\nUsers and Teams are periodically synchronized from the Identity Provider (IdP) configured for the Organization, but TSB allows creating local teams to provide extended flexibility in how Users and Teams are grouped, and to provide a comprehensive way of creating more fine-grained access control policies.\nGetUser rpc GetUser (tetrateio.api.tsb.v2.GetUserRequest ) returns (tetrateio.api.tsb.v2.User )\nRequires READ\nGet the details of an existing user.\nListUsers rpc ListUsers (tetrateio.api.tsb.v2.ListUsersRequest ) returns (tetrateio.api.tsb.v2.ListUsersResponse )\nList existing users.\nGenerateTokens rpc GenerateTokens (tetrateio.api.tsb.v2.GenerateTokensRequest ) returns (tetrateio.api.tsb.v2.TokenResponse )\nRequires CreateUser\nDeprecated. This method will be removed in future versions of TSB. Use Service Accounts instead.\nGenerate the tokens for a local user account so it can authenticate against management plane. This method will return an error if the user account is not of type MANUAL. Credentials for normal platform users must be configured in the corresponding Identity Provider.\nCreateTeam rpc CreateTeam (tetrateio.api.tsb.v2.CreateTeamRequest ) returns (tetrateio.api.tsb.v2.Team )\nRequires CREATE\nCreate a new team.\nGetTeam rpc GetTeam (tetrateio.api.tsb.v2.GetTeamRequest ) returns (tetrateio.api.tsb.v2.Team )\nRequires READ\nGet the details of an existing team.\nUpdateTeam rpc UpdateTeam (tetrateio.api.tsb.v2.Team ) returns (tetrateio.api.tsb.v2.Team )\nRequires WRITE\nModify an existing team.\nListTeams rpc ListTeams (tetrateio.api.tsb.v2.ListTeamsRequest ) returns (tetrateio.api.tsb.v2.ListTeamsResponse )\nList all existing teams.\nDeleteTeam rpc DeleteTeam (tetrateio.api.tsb.v2.DeleteTeamRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete a team. Note that deleting a team only deletes the team itself, but not its members.\nCreateServiceAccount rpc CreateServiceAccount (tetrateio.api.tsb.v2.CreateServiceAccountRequest ) returns (tetrateio.api.tsb.v2.ServiceAccount )\nRequires CREATE\nCreate Service Account in TSB. Service Accounts are local to TSB and can be used to access the platform using JWT tokens signed with the Service Account’s private key for authentication.\nGetServiceAccount rpc GetServiceAccount (tetrateio.api.tsb.v2.GetServiceAccountRequest ) returns (tetrateio.api.tsb.v2.ServiceAccount )\nRequires READ\nGet the details of an existing Service Account.\nGetServiceAccountJWKS rpc GetServiceAccountJWKS (tetrateio.api.tsb.v2.GetServiceAccountJWKSRequest ) returns (tetrateio.api.tsb.v2.JWKS )\nGet all the public keys available in the service account and return them in a JWKS document. See: https://datatracker.ietf.org/doc/html/rfc7517 Requests to this endpoint require read permissions on the service account, or a token signed with one of the service account keys.\nUpdateServiceAccount rpc UpdateServiceAccount (tetrateio.api.tsb.v2.ServiceAccount ) returns (tetrateio.api.tsb.v2.ServiceAccount )\nRequires WRITE\nUpdate the details of a service account. Updating the details of the service account does not regenerate its keys.\nListServiceAccounts rpc ListServiceAccounts (tetrateio.api.tsb.v2.ListServiceAccountsRequest ) returns (tetrateio.api.tsb.v2.ListServiceAccountsResponse )\nList existing Service Accounts.\nDeleteServiceAccount rpc DeleteServiceAccount (tetrateio.api.tsb.v2.DeleteServiceAccountRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given Service account.\nGenerateServiceAccountKey rpc GenerateServiceAccountKey (tetrateio.api.tsb.v2.GenerateServiceAccountKeyRequest ) returns (tetrateio.api.tsb.v2.ServiceAccount )\nRequires WriteServiceAccount\nGenerate a new key-pair for the service account. Note that TSB does not store the generated private key, so the client must read it and store it securely.\nDeleteServiceAccountKey rpc DeleteServiceAccountKey (tetrateio.api.tsb.v2.DeleteServiceAccountKeyRequest ) returns (tetrateio.api.tsb.v2.ServiceAccount )\nRequires WriteServiceAccount\nDelete a key-pair associated the service account.\nCreateServiceAccountRequest Request to create a ServiceAccount.\nField Description Validation Rule parent\nstring REQUIRED Parent resource where the User will be created.\nstring = { min_len: 1}\nname\nstring REQUIRED The short name for the resource to be created.\nstring = { min_len: 1}\nserviceAccount\ntetrateio.api.tsb.v2.ServiceAccount REQUIRED Details of the Service Account to be created.\nmessage = { required: true}\nkeyEncoding\ntetrateio.api.tsb.v2.ServiceAccount.KeyPair.Encoding The format in which the generated key pairs will be returned. If not set keys are returned in PEM format.\n–\nCreateTeamRequest Request to create a Team.\nField Description Validation Rule parent\nstring REQUIRED Parent resource where the Team will be created.\nstring = { min_len: 1}\nname\nstring REQUIRED The short name for the resource to be …","relpermalink":"/book/tsb/refs/tsb/v2/team-service/","summary":"Service to manage Users and Teams in TSB","title":"Teams Service"},{"content":" Service to manage the Telemetry Sources.\nSources The Sources service exposes methods to manage telemetry sources from resources.\nGetSource rpc GetSource (tetrateio.api.tsb.observability.telemetry.v2.GetSourceRequest ) returns (tetrateio.api.tsb.observability.telemetry.v2.Source )\nGet the details of an existing telemetry source.\nListSources rpc ListSources (tetrateio.api.tsb.observability.telemetry.v2.ListSourcesRequest ) returns (tetrateio.api.tsb.observability.telemetry.v2.ListSourcesResponse )\nList the telemetry sources that are available for the requested parent. It will return telemetry sources that belong to the requested parent and from all its child resources.\nGetSourceRequest Request to retrieve a Telemetry Sources from a parent resource.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the Telemetry Sources.\nTODO(marcnavarro): Add pagination information.\nstring = { min_len: 1}\nListSourcesRequest Request to retrieve the list of telemetry sources from a parent resource.\nField Description Validation Rule parent\nstring REQUIRED Fully-qualified name of the parent resource to retrieve the telemetry sources.\nstring = { min_len: 1}\nscopeTypes\nList of tetrateio.api.tsb.observability.telemetry.v2.SourceScopeType The scope type that a telemetry source needs to match. Telemetry sources that matches any requested scope type will be returned.\n–\nbelongTos\nList of string Which resources the telemetry sources must belong to. Telemetry sources that belongs to any requested resource will be returned.\n–\nexisted\ntetrateio.api.tsb.observability.telemetry.v2.ListSourcesRequest.TimeRange Time range during which telemetry sources must have existed. If no existed time range is provided, only the actual available Telemetry sources will be returned. Otherwise, telemetry Sources that existed during the time range will be returned.\n–\nTimeRange TimeRange is a closed time range. If since or until are not provided they will not be used to filter.\nField Description Validation Rule since\ngoogle.protobuf.Timestamp Moment in time since we retrieve Telemetry Sources.\n–\nuntil\ngoogle.protobuf.Timestamp Moment in time until we retrieve Telemetry Sources.\n–\nListSourcesResponse List of telemetry sources from the resource.\nField Description Validation Rule sources\nList of tetrateio.api.tsb.observability.telemetry.v2.Source –\n","relpermalink":"/book/tsb/refs/tsb/observability/telemetry/v2/source-service/","summary":"Service to manage the Telemetry Sources.","title":"Telemetry Source Service"},{"content":" Tenant is a self-contained entity within an organization in the Service Bridge object hierarchy. Tenants can be business units, organization units, or any logical grouping that matches a corporate structure.\nThe following example creates a tenant named mycompany in an organization named myorg.\napiVersion: api.tsb.tetrate.io/v2 kind: Tenant metadata: organization: myorg name: mycompany Tenant Tenant is a self-contained entity within an organization in the Service Bridge hierarchy.\nField Description Validation Rule securityDomain\nstring Security domains can be used to group different resources under the same security domain. Although security domain is not resource itself currently, it follows a fqn format organizations/myorg/securitydomains/mysecuritydomain, and a child cannot override any ancestor’s security domain. Once a security domain is assigned to a Tenant, all the children resources will belong to that security domain in the same way a Workspace belongs to a Tenant, a Workspace will also belong to the security domain assigned to the Tenant. Security domains can also be used to define Security settings Authorization rules in which you can allow or deny request from or to a security domain.\n–\ndeletionProtectionEnabled\nbool When set, prevents the resource from being deleted. In order to delete the resource this property needs to be set to false first.\n–\nconfigGenerationMetadata\ntetrateio.api.tsb.types.v2.ConfigGenerationMetadata Default metadata values that will be propagated to the children Istio generated configurations. When using YAML APIs liketctl or gitops, put them into the metadata.labels or metadata.annotations instead. This field is only necessary when using gRPC APIs directly.\n–\n","relpermalink":"/book/tsb/refs/tsb/v2/tenant/","summary":"Configuration for creating a tenant in Service Bridge.","title":"Tenant"},{"content":" DEPRECATED: use Access Bindings instead.\nTenantAccessBindings is an assignment of roles to a set of users or teams to access resources under a Tenant. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a TenantAccessBinding can be created or modified only by users who have SET_POLICY permission on the Tenant.\nThe following example assigns the tenant-admin role to users alice, bob, and members of the t1 team owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team\napiVersion: rbac.tsb.tetrate.io/v2 kind: TenantAccessBindings metadata: organization: myorg tenant: mycompany spec: allow: - role: rbac/tenant-admin subjects: - user: organization/myorg/users/alice - user: organization/myorg/users/bob - team: organization/myorg/teams/t1 TenantAccessBindings TenantAccessBindings assigns permissions to users of tenants.\nField Description Validation Rule allow\nList of tetrateio.api.tsb.rbac.v2.Binding The list of allowed bindings configures the different access profiles that are allowed on the resource configured by the policy.\n–\n","relpermalink":"/book/tsb/refs/tsb/rbac/v2/tenant-access-bindings/","summary":"Configuration for assigning access roles to users under a tenant.","title":"Tenant Access Bindings"},{"content":" Service to manage TSB tenants.\nTenants The Tenant service can be used to manage the tenants in TSB. Tenants can be seen as organization units and line of business that have a set of resources. Every resource in TSB belongs to a tenant, and users can be assigned to tenants to get access to those resources (such as workspaces, traffic settings, etc). This service provides methods to manage the tenants that are available in the platform.\nCreateTenant rpc CreateTenant (tetrateio.api.tsb.v2.CreateTenantRequest ) returns (tetrateio.api.tsb.v2.Tenant )\nRequires CREATE\nCreate a new tenant in the platform that will be the home for a set of resources.\nGetTenant rpc GetTenant (tetrateio.api.tsb.v2.GetTenantRequest ) returns (tetrateio.api.tsb.v2.Tenant )\nRequires READ\nGet the details of an existing tenant.\nUpdateTenant rpc UpdateTenant (tetrateio.api.tsb.v2.Tenant ) returns (tetrateio.api.tsb.v2.Tenant )\nRequires WRITE\nModify the details of the given tenant.\nListTenants rpc ListTenants (tetrateio.api.tsb.v2.ListTenantsRequest ) returns (tetrateio.api.tsb.v2.ListTenantsResponse )\nList all tenants that are available.\nDeleteTenant rpc DeleteTenant (tetrateio.api.tsb.v2.DeleteTenantRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete a tenant from the platform. Deleting a tenant will recursively delete all resources attached to the tenant, so use with caution. It will delete all workspaces and all settings that have been created in that tenant, so this operation should be done carefully, when it’s safe to do so.\nCreateSetting rpc CreateSetting (tetrateio.api.tsb.v2.CreateTenantSettingRequest ) returns (tetrateio.api.tsb.v2.TenantSetting )\nRequires CreateTenantSetting\nCreate a settings object for the given tenant.\nGetSetting rpc GetSetting (tetrateio.api.tsb.v2.GetTenantSettingRequest ) returns (tetrateio.api.tsb.v2.TenantSetting )\nRequires ReadTenantSetting\nGet the details for the given settings object.\nUpdateSetting rpc UpdateSetting (tetrateio.api.tsb.v2.TenantSetting ) returns (tetrateio.api.tsb.v2.TenantSetting )\nRequires WriteTenantSetting\nModify the given settings in the given tenant.\nListSettings rpc ListSettings (tetrateio.api.tsb.v2.ListTenantSettingsRequest ) returns (tetrateio.api.tsb.v2.ListTenantSettingsResponse )\nList all the settings objects that have made available to the given tenant.\nListWasmExtensions rpc ListWasmExtensions (tetrateio.api.tsb.v2.ListTenantExtensionsRequest ) returns (tetrateio.api.tsb.v2.ListTenantExtensionsResponse )\nRequires ReadWasmExtension\nList all the WASM extensions that have been attached to the given tenant.\nDeleteSetting rpc DeleteSetting (tetrateio.api.tsb.v2.DeleteTenantSettingRequest ) returns (google.protobuf.Empty )\nRequires DeleteTenantSetting\nDelete the given settings object from the tenant.\nCreateTenantRequest Request to create a tenant.\nField Description Validation Rule parent\nstring Parent resource where the Tenant will be created. $only_beta\n–\nname\nstring REQUIRED The short name for the resource to be created.\nstring = { min_len: 1}\ntenant\ntetrateio.api.tsb.v2.Tenant REQUIRED Details of the tenant to be created.\nmessage = { required: true}\nCreateTenantSettingRequest Request to create a Tenant Setting.\nField Description Validation Rule parent\nstring REQUIRED Parent resource where the Tenant Setting will be created.\nstring = { min_len: 1}\nname\nstring REQUIRED The short name for the resource to be created.\nstring = { min_len: 1}\nsetting\ntetrateio.api.tsb.v2.TenantSetting REQUIRED Details of the Tenant Setting to be created.\nmessage = { required: true}\nDeleteTenantRequest Request to delete a tenant.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the tenant.\nstring = { min_len: 1}\nforce\nbool Force the deletion of the object even if deletion protection is enabled. If this is set, then the object and all its children will be deleted even if any of them has the deletion protection enabled.\n–\nDeleteTenantSettingRequest Request to delete a Tenant Setting.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the Tenant Setting.\nstring = { min_len: 1}\nGetTenantRequest Request to retrieve a tenant.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the tenant.\nstring = { min_len: 1}\nGetTenantSettingRequest Request to retrieve a Tenant Settings.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the Tenant Setting.\nstring = { min_len: 1}\nListTenantExtensionsRequest Request to list Tenant extensions.\nField Description Validation Rule parent\nstring REQUIRED Parent resource to list Tenant Extensions from.\nstring = { min_len: 1}\nListTenantExtensionsResponse List of all existing WasmExtensions objects assigned to the Tenant.\nField Description Validation Rule extensions\nList of tetrateio.api.tsb.extension.v2.WasmExtension –\nListTenantSettingsRequest Request to list Tenant Settings.\nField Description Validation Rule parent\nstring REQUIRED Parent resource to list …","relpermalink":"/book/tsb/refs/tsb/v2/tenant-service/","summary":"Service to manage TSB tenants.","title":"Tenant Service"},{"content":" Tenant Setting allows configuring default settings for the tenant.\nTraffic and Security settings can be defined as default for a tenant, meaning that they will be applied to all the workspaces of the tenant. These defaults settings can be overridden by creating proper WorkspaceSetting, TrafficSetting or SecuritySetting into the desired workspace or group.\napiVersion: api.tsb.tetrate.io/v2 kind: TenantSetting metadata: name: tenant-settings organization: myorg tenant: mytenant spec: defaultTrafficSetting: reachability: mode: WORKSPACE egress: host: bookinfo-perimeter/tsb-egress defaultSecuritySetting: authenticationSettings: trafficMode: REQUIRED authorization: mode: GROUP TenantSetting Default settings that apply to all workspaces under a tenant.\nField Description Validation Rule defaultSecuritySetting\ntetrateio.api.tsb.security.v2.SecuritySetting Security settings for all proxy workloads in this tenant. This can be overridden at WorkspaceSettings or security group’s SecuritySetting for specific cases. The override strategy used will be driven by the SecuritySetting propagation strategy. The default propagation strategy is REPLACE, in which a lower level SecuritySetting in the configuration hierarchy replaces a higher level SecuritySetting defined in the configuration hierarchy. For instance, a WorkspaceSettings defined SecuritySetting will replace any tenant or organization defined SecuritySetting. Proxy workloads without a specific security group will inherit these settings. If omitted, the following semantics apply:\nSidecars will accept connections from clients using Istio Mutual TLS as well as legacy clients using plaintext (i.e. any traffic not using Istio Mutual TLS authentication), i.e. authentication mode defaults to OPTIONAL.\nNo authorization will be performed, i.e., authorization mode defaults to DISABLED.\n–\ndefaultTrafficSetting\ntetrateio.api.tsb.traffic.v2.TrafficSetting Traffic settings for all proxy workloads in this tenant. This can be overridden at WorkspaceSetting or TrafficSetting for specific cases. Proxy workloads without a specific traffic group will inherit these settings. If omitted, the following semantics apply:\nSidecars will be able to reach any service in the cluster, i.e. reachability mode defaults to CLUSTER.\nTraffic to unknown destinations will be directly routed from the sidecar to the destination.\n–\n","relpermalink":"/book/tsb/refs/tsb/v2/tenant-setting/","summary":"Configuration for specifying global settings in a tenant.","title":"Tenant Setting"},{"content":" DEPRECATION: The functionality provided by the Tier1Gateway is now provided in Gateway object, and using it is the recommended approach. The Tier1Gateway resource will be removed in future releases.\nTier1Gateway configures a workload to act as a gateway that distributes traffic across one or more ingress gateways in other clusters.\nNOTE: Tier1 gateways cannot be used to route traffic to the same cluster. A cluster with tier1 gateway cannot have any other gateways or workloads.\nThe following example declares a tier1 gateway running on pods with app: gateway labels in the ns1 namespace. The gateway exposes host movieinfo.com on ports 8080, 8443 and kafka.internal on port 9000. Traffic for these hosts at the ports 8443 and 9000 are TLS terminated and forwarded over Istio mutual TLS to the ingress gateways hosting movieinfo.com host on clusters c3 and c4 and the internal kafka.internal service in cluster c3 respectively. The server at port 8080 is configured to receive plaintext HTTP traffic and redirect to port 8443 with “Permanently Moved” (HTTP 301) status code.\napiVersion: gateway.tsb.tetrate.io/v2 kind: Tier1Gateway metadata: name: tier1 group: g1 workspace: w1 tenant: mycompany organization: myorg spec: workloadSelector: namespace: ns1 labels: app: gateway externalServers: - name: movieinfo-plain hostname: movieinfo.com # Plaintext and HTTPS redirect port: 8080 redirect: authority: movieinfo.com uri: \u0026#34;/\u0026#34; redirectCode: 301 port: 8443 scheme: https - name: movieinfo hostname: movieinfo.com # TLS termination and Istio mTLS to upstream port: 8443 tls: mode: SIMPLE secretName: movieinfo-secrets clusters: - name: c3 # the target gateway IPs will be automatically determined weight: 90 - name: c4 weight: 10 authentication: rules: jwt: - issuer: \u0026#34;auth.mycompany.com\u0026#34; jwksUri: https://auth.mycompany.com/oauth2/jwks - issuer: \u0026#34;auth.othercompany.com\u0026#34; jwksUri: https://auth.othercompany.com/oauth2/jwks authorization: external: uri: \u0026#34;https://auth.company.com\u0026#34; includeRequestHeaders: - authorization tcpExternalServers: - name: kafka hostname: kafka.internal port: 9000 tls: mode: SIMPLE secretName: kafka-cred clusters: - name: c3 weight: 100 Tier1 gateways can also be used to forward mesh internal traffic for Gateway hosts from one cluster to another. This form of forwarding will work only if the two clusters cannot reach each other directly (e.g., they are on different VPCs that are not peered). The following example declares a tier1 gateway running on pods with app: gateway labels in the ns1 namespace. The gateway exposes hosts movieinfo.com, bookinfo.com, and a non-HTTP server called kafka.org-internal within the mesh. Traffic to movieinfo.com is load balanced across all clusters on vpc-02, while traffic to bookinfo.com and kafka.org-internal is load balanced across ingress gateways exposing bookinfo.com on any cluster. Traffic from the source (sidecars) is expected to arrive on the tier1 gateway over Istio mTLS.\napiVersion: gateway.tsb.tetrate.io/v2 kind: Tier1Gateway metadata: name: tier1 group: g1 workspace: w1 tenant: mycompany organization: myorg spec: workloadSelector: namespace: ns1 labels: app: gateway internalServers: # forwarding gateway (HTTP traffic only) - name: movieinfo hostname: movieinfo.com clusters: - labels: network: vpc-02 # the target gateway IPs will be automatically determined authentication: rules: jwt: - issuer: \u0026#34;auth.mycompany.com\u0026#34; jwksUri: https://auth.company.com/oauth2/jwks - issuer: \u0026#34;auth.othercompany.com\u0026#34; jwksUri: https://auth.othercompany.com/oauth2/jwks authorization: external: uri: \u0026#34;https://auth.company.com\u0026#34; includeRequestHeaders: - authorization - name: bookinfo hostname: bookinfo.com # route to any ingress gateway exposing bookinfo.com tcpInternalServers: # forwarding non-HTTP traffic within the mesh - name: kafka hostname: kafka.org-internal ** NOTE:** If two clusters have direct connectivity, declaring a tier1 internal server will have no effect.\nTier1 gateways can also be configured to expose hostnames in the TLS passthrough mode. Tier1 gateway will forward the pasthrough server traffic to any tier2 pass through servers exposing the same hostname. In other words, To be able to leverage passthrough at tier1, it is a MUST that passthrough is configured at t2 IngressGateway as well.\n** NOTE:** A hostname like abc.com can only be exposed either in passthrough mode OR in terminating tls mode(External/Internal servers), not in both the modes.\napiVersion: gateway.tsb.tetrate.io/v2 kind: Tier1Gateway metadata: name: tier1-tls-gw group: g1 workspace: w1 tenant: mycompany organization: myorg spec: workloadSelector: namespace: ns1 labels: app: gateway passthroughServers: - name: nginx port: 8443 hostname: nginx.example.com The Tier1Gateway above will require the corresponding, at least one or more, IngressGateway(s), e.g.:\napiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: name: tls-gw group: g1 workspace: w1 tenant: mycompany organization: myorg spec: workloadSelector: …","relpermalink":"/book/tsb/refs/tsb/gateway/v2/tier1-gateway/","summary":"Configurations to build a tier1 gateway.","title":"Tier1 Gateway"},{"content":" DEPRECATED: use Access Bindings instead.\nTrafficAccessBindings is an assignment of roles to a set of users or teams to access resources under a Traffic group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a TrafficAccessBinding can be created or modified only by users who have SET_POLICY permission on the Traffic group.\nThe following example assigns the traffic-admin role to users alice, bob, and members of the traffic-ops team for traffic group g1 under workspace w1 owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team\napiVersion: rbac.tsb.tetrate.io/v2 kind: TrafficAccessBindings metadata: organization: myorg tenant: mycompany workspace: w1 group: g1 spec: allow: - role: rbac/traffic-admin subjects: - user: organization/myorg/users/alice - user: organization/myorg/users/bob - team: organization/myorg/teams/traffic-ops TrafficAccessBindings TrafficAccessBindings assigns permissions to users of traffic groups.\nField Description Validation Rule allow\nList of tetrateio.api.tsb.rbac.v2.Binding The list of allowed bindings configures the different access profiles that are allowed on the resource configured by the policy.\n–\n","relpermalink":"/book/tsb/refs/tsb/rbac/v2/traffic-access-bindings/","summary":"Configuration for assigning access roles to users of traffic groups.","title":"Traffic Access Bindings"},{"content":" Traffic Groups allow grouping the proxy workloads in a set of namespaces owned by its parent workspace. Networking and routing related configurations can then be applied on the group to control the behavior of these proxy workloads. The group can be in one of two modes: BRIDGED and DIRECT. BRIDGED mode is a minimalistic mode that allows users to quickly configure the most commonly used features in the service mesh using Tetrate specific APIs, while the DIRECT mode provides more flexibility for power users by allowing them to configure the proxy workload behavior using a restricted subset of Istio Networking APIs.\nThe following example creates a traffic group for the proxy workloads in ns1, ns2 and ns3 namespaces owned by its parent workspace w1 under tenant mycompany and sets up a TrafficSetting defining the resilience properties for proxy workloads in these namespaces.\napiVersion: traffic.tsb.tetrate.io/v2 kind: Group metadata: name: t1 workspace: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \u0026#34;*/ns1\u0026#34; - \u0026#34;*/ns2\u0026#34; - \u0026#34;*/ns3\u0026#34; configMode: BRIDGED And the associated traffic settings for the proxy workloads in the group\napiVersion: traffic.tsb.tetrate.io/v2 kind: TrafficSetting metadata: name: defaults group: t1 workspace: w1 tenant: mycompany organization: myorg spec: resilience: circuitBreakerSensitivity: MEDIUM Under the hood, Service Bridge translates these minimalistic settings into Istio APIs such as Sidecar, DestinationRule, etc. for the namespaces managed by the traffic group. These APIs are then pushed to the Istio control planes of clusters where the workspace is applicable.\nIt is possible to create a traffic group for namespaces in a specific cluster as long as the parent workspace owns those namespaces in that cluster. For example,\napiVersion: traffic.tsb.tetrate.io/v2 kind: Group metadata: name: t1 workspace: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \u0026#34;c1/ns1\u0026#34; # pick ns1 namespace only from c1 cluster - \u0026#34;*/ns2\u0026#34; - \u0026#34;*/ns3\u0026#34; configMode: BRIDGED In the DIRECT mode, it is possible to directly attach Istio APIs such as VirtualService, DestinationRule, and Sidecar to the traffic group. These configurations will be validated for correctness and conflict free operations and then pushed to the appropriate Istio control planes.\nThe following example declares a DestinationRule with two subsets, for the ratings service in the ns1 namespace:\napiVersion: networking.istio.io/v1beta1 kind: DestinationRule metadata: name: ratings-subsets namespace: ns1 annotations: tsb.tetrate.io/organization: myorg tsb.tetrate.io/tenant: mycompany tsb.tetrate.io/workspace: w1 tsb.tetrate.io/trafficGroup: t1 spec: host: ratings.ns1.svc.cluster.local subsets: - name: stableversion labels: app: ratings env: prod - name: testversion labels: app: ratings env: uat The namespace where the Istio APIs are applied will need to be part of the parent traffic group. In addition, each API object will need to have annotations to indicate the organization, tenant, workspace and the traffic group to which it belongs to.\nGroup A traffic group manages the routing properties of proxy workloads in a group of namespaces owned by the parent workspace.\nField Description Validation Rule namespaceSelector\ntetrateio.api.tsb.types.v2.NamespaceSelector REQUIRED Set of namespaces owned exclusively by this group. If omitted, applies to all resources owned by the workspace. Use */* to claim all cluster resources under the workspace.\nmessage = { required: true}\nconfigMode\ntetrateio.api.tsb.types.v2.ConfigMode The Configuration types that will be added to this group. BRIDGED mode indicates that configurations added to this group will use Tetrate APIs such as TrafficSetting and ServiceRoute. DIRECT mode indicates that configurations added to this group will use Istio Networking APIs such as VirtualService, DestinationRule, and Sidecar. Defaults to BRIDGED mode.\n–\ndeletionProtectionEnabled\nbool When set, prevents the resource from being deleted. In order to delete the resource this property needs to be set to false first.\n–\nconfigGenerationMetadata\ntetrateio.api.tsb.types.v2.ConfigGenerationMetadata Default metadata values that will be propagated to the children Istio generated configurations. When using YAML APIs liketctl or gitops, put them into the metadata.labels or metadata.annotations instead. This field is only necessary when using gRPC APIs directly.\n–\n","relpermalink":"/book/tsb/refs/tsb/traffic/v2/traffic-group/","summary":"Configurations to group a set of proxy workloads in a workspace for traffic management.","title":"Traffic Group"},{"content":" Service to manage traffic settings.\nTraffic The Traffic service provides methods to manage traffic settings in TSB.\nIt provides methods to create and manage traffic groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the traffic configuration features.\nThe Traffic service also provides methods to configure the different traffic settings that are allowed within each group.\nCreateGroup rpc CreateGroup (tetrateio.api.tsb.traffic.v2.CreateTrafficGroupRequest ) returns (tetrateio.api.tsb.traffic.v2.Group )\nRequires CREATE\nCreate a new traffic group in the given workspace.\nGroups will by default configure all the namespaces owned by their workspace, unless explicitly configured. If a specific set of namespaces is set for the group, it must be a subset of the namespaces defined by its workspace.\nGetGroup rpc GetGroup (tetrateio.api.tsb.traffic.v2.GetTrafficGroupRequest ) returns (tetrateio.api.tsb.traffic.v2.Group )\nRequires READ\nGet the details of the given traffic group.\nUpdateGroup rpc UpdateGroup (tetrateio.api.tsb.traffic.v2.Group ) returns (tetrateio.api.tsb.traffic.v2.Group )\nRequires WRITE\nModify the given traffic group.\nListGroups rpc ListGroups (tetrateio.api.tsb.traffic.v2.ListTrafficGroupsRequest ) returns (tetrateio.api.tsb.traffic.v2.ListTrafficGroupsResponse )\nList all traffic groups in the given workspace.\nDeleteGroup rpc DeleteGroup (tetrateio.api.tsb.traffic.v2.DeleteTrafficGroupRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given traffic group. Note that deleting resources in TSB is a recursive operation. Deleting a traffic group will delete all configuration objects that exist in it.\nCreateSettings rpc CreateSettings (tetrateio.api.tsb.traffic.v2.CreateTrafficSettingsRequest ) returns (tetrateio.api.tsb.traffic.v2.TrafficSetting )\nRequires CreateTrafficSetting\nCreate a settings object for the given traffic group.\nGetSettings rpc GetSettings (tetrateio.api.tsb.traffic.v2.GetTrafficSettingsRequest ) returns (tetrateio.api.tsb.traffic.v2.TrafficSetting )\nRequires ReadTrafficSetting\nGet the details for the given settings object.\nUpdateSettings rpc UpdateSettings (tetrateio.api.tsb.traffic.v2.TrafficSetting ) returns (tetrateio.api.tsb.traffic.v2.TrafficSetting )\nRequires WriteTrafficSetting\nModify the given settings in the given traffic group.\nListSettings rpc ListSettings (tetrateio.api.tsb.traffic.v2.ListTrafficSettingsRequest ) returns (tetrateio.api.tsb.traffic.v2.ListTrafficSettingsResponse )\nList all the settings objects that have been attached to the given traffic group.\nDeleteSettings rpc DeleteSettings (tetrateio.api.tsb.traffic.v2.DeleteTrafficSettingsRequest ) returns (google.protobuf.Empty )\nRequires DeleteTrafficSetting\nDelete the given settings object from the traffic group.\nCreateServiceRoute rpc CreateServiceRoute (tetrateio.api.tsb.traffic.v2.CreateServiceRouteRequest ) returns (tetrateio.api.tsb.traffic.v2.ServiceRoute )\nRequires CREATE\nCreate a new service route in the given traffic group.\nGetServiceRoute rpc GetServiceRoute (tetrateio.api.tsb.traffic.v2.GetServiceRouteRequest ) returns (tetrateio.api.tsb.traffic.v2.ServiceRoute )\nRequires READ\nGet the details of the given service route.\nUpdateServiceRoute rpc UpdateServiceRoute (tetrateio.api.tsb.traffic.v2.ServiceRoute ) returns (tetrateio.api.tsb.traffic.v2.ServiceRoute )\nRequires WRITE\nModify a service route.\nListServiceRoutes rpc ListServiceRoutes (tetrateio.api.tsb.traffic.v2.ListServiceRoutesRequest ) returns (tetrateio.api.tsb.traffic.v2.ListServiceRoutesResponse )\nList all service routes that have been attached to the traffic group.\nDeleteServiceRoute rpc DeleteServiceRoute (tetrateio.api.tsb.traffic.v2.DeleteServiceRouteRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete the given service route.\nCreateServiceRouteRequest Request to create a ServiceRoute.\nField Description Validation Rule parent\nstring REQUIRED Parent resource where the ServiceRoute will be created.\nstring = { min_len: 1}\nname\nstring REQUIRED The short name for the resource to be created.\nstring = { min_len: 1}\nserviceRoute\ntetrateio.api.tsb.traffic.v2.ServiceRoute REQUIRED Details of the ServiceRoute to be created.\nmessage = { required: true}\nCreateTrafficGroupRequest Request to create a Traffic Group.\nField Description Validation Rule parent\nstring REQUIRED Parent resource where the Group will be created.\nstring = { min_len: 1}\nname\nstring REQUIRED The short name for the resource to be created.\nstring = { min_len: 1}\ngroup\ntetrateio.api.tsb.traffic.v2.Group REQUIRED Details of the Group to be created.\nmessage = { required: true}\nCreateTrafficSettingsRequest Request to create a Traffic Settings.\nField Description Validation Rule parent\nstring REQUIRED Parent resource where the Traffic Settings will be created.\nstring = { min_len: 1}\nname\nstring REQUIRED The …","relpermalink":"/book/tsb/refs/tsb/traffic/v2/traffic-service/","summary":"Service to manage traffic settings.","title":"Traffic Service"},{"content":" Traffic Settings allow configuring the behavior of the proxy workloads in a set of namespaces owned by a traffic group. Specifically, it allows configuring the dependencies of proxy workloads on namespaces outside the traffic group as well as reliability settings for outbound calls made by the proxy workloads to other services.\nThe following example creates a traffic group for the proxy workloads in ns1, ns2 and ns3 namespaces owned by its parent workspace w1 under tenant mycompany. It then defines a traffic setting for the all workloads in these namespaces, adding a dependency on all the services in the shared db namespace, and forwarding all unknown traffic via the egress gateway in the istio-system namespace.\napiVersion: traffic.tsb.tetrate.io/v2 kind: Group metadata: name: t1 workspace: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \u0026#34;*/ns1\u0026#34; - \u0026#34;*/ns2\u0026#34; - \u0026#34;*/ns3\u0026#34; configMode: BRIDGED And the associated traffic settings for the proxy workloads:\napiVersion: traffic.tsb.tetrate.io/v2 kind: TrafficSetting metadata: name: defaults group: t1 workspace: w1 tenant: mycompany organization: myorg spec: reachability: mode: CUSTOM hosts: - \u0026#34;ns1/*\u0026#34; - \u0026#34;ns2/*\u0026#34; - \u0026#34;ns3/*\u0026#34; - \u0026#34;db/*\u0026#34; resilience: circuitBreakerSensitivity: MEDIUM egress: host: istio-system/istio-egressgateway The following traffic setting confines the reachability of proxy workloads in the traffic group t1 to other namespaces inside the group. The resilience and egress gateway settings will be inherited from the workspace wide traffic setting.\napiVersion: traffic.tsb.tetrate.io/v2 kind: TrafficSetting metadata: name: defaults group: t1 workspace: w1 tenant: mycompany organization: myorg spec: reachability: mode: GROUP HTTPRetry HTTPRetry defines the parameters for retrying API calls to a service.\nField Description Validation Rule attempts\nint32 REQUIRED Number of retries for a given request. The interval between retries will be determined automatically (25ms+).\nActual number of retries attempted depends on the httpReqTimeout.\nint32 = { gte: 1}\nperTryTimeout\ngoogle.protobuf.Duration Timeout per retry attempt for a given request. format: 1h/1m/1s/1ms. MUST BE \u0026gt;=1ms.\n–\nretryOn\nstring Specifies the conditions under which retry takes place. One or more policies can be specified using a ‘,’ delimited list. See the retry policies and gRPC retry policies for more details.\nstring = { pattern: ^$|^(5xx|gateway-error|reset|connect-failure|envoy-ratelimited|retriable-4xx|refused-stream|retriable-status-codes|retriable-headers|cancelled|deadline-exceeded|internal|resource-exhausted|unavailable)(,(5xx|gateway-error|reset|connect-failure|envoy-ratelimited|retriable-4xx|refused-stream|retriable-status-codes|retriable-headers|cancelled|deadline-exceeded|internal|resource-exhausted|unavailable))*$}\nKeepAliveSettings Keep Alive Settings.\nField Description Validation Rule tcp\ntetrateio.api.tsb.traffic.v2.KeepAliveSettings.TcpKeepAliveSettings TCP Keep Alive settings associated with the upstream and downstream TCP connections.\n–\nTcpKeepAliveSettings TCP Keep Alive Settings.\nField Description Validation Rule downstream\ntetrateio.api.tsb.traffic.v2.KeepAliveSettings.TcpKeepAliveSettings.TcpKeepAlive TCP Keep Alive Settings associated with the downstream (client) connection.\n–\nupstream\ntetrateio.api.tsb.traffic.v2.KeepAliveSettings.TcpKeepAliveSettings.TcpKeepAlive TCP Keep Alive Settings associated with the upstream (backend) connection.\n–\nTcpKeepAlive Field Description Validation Rule probes\ngoogle.protobuf.UInt32Value The total number of unacknowledged probes to send before deciding the connection is dead. Default is to use the OS level configuration, Linux defaults to 9.\n–\nidleTime\ngoogle.protobuf.UInt32Value The number of seconds a connection needs to be idle before keep-alive probes start being sent. Default is to use the OS level configuration, Linux defaults to 7200s.\n–\ninterval\ngoogle.protobuf.UInt32Value The number of seconds between keep-alive probes. Default is to use the OS level configuration, Linux defaults to 75s.\n–\nReachabilitySettings ReachabilitySettings define the set of services and hosts accessed by a workload (and hence its sidecar) in the mesh. Defining the set of services accessed by a workload (i.e. its dependencies) in advance reduces the memory and CPU consumption both the Istio control plane and the individual Envoy proxy workloads in the data plane.\nField Description Validation Rule mode\ntetrateio.api.tsb.traffic.v2.ReachabilitySettings.Mode A short cut for specifying the set of services accessed by the workload.\n–\nhosts\nList of string When the mode is CUSTOM, hosts specify the set of services that the sidecar should be able to reach. Must be in the \u0026lt;namespace\u0026gt;/\u0026lt;fqdn\u0026gt; format.\n./* indicates all services in the namespace where the sidecar resides.\nns1/* indicates all services in the ns1 namespace.\nns1/svc1.com indicates svc1.com service in ns1 namespace.\n*/svc1.com indicates svc1.com service in any namespace.\n– …","relpermalink":"/book/tsb/refs/tsb/traffic/v2/traffic-setting/","summary":"Traffic settings for proxy workloads in a traffic group.","title":"Traffic Setting"},{"content":" Transport layer security config specifies configuration of a TLS client.\nClientTransportSecurity ClientTransportSecurity specifies transport layer security configuration.\nField Description Validation Rule tls\ntetrateio.api.onboarding.config.types.config.v1alpha1.TlsClient oneof kind TLS client configuration.\n–\nnone\ntetrateio.api.onboarding.config.types.config.v1alpha1.PlainTextClient oneof kind Plain-text client configuration.\n–\nTlsClient TlsClient specifies configuration of a TLS client.\nField Description Validation Rule sni\nstring SNI string to present to the server during TLS handshake instead of the default value (host address).\nDefaults to empty string, in which case the default SNI value (host address) will be used.\nThis setting is meant for use in non-production scenarios, such as:\nwhen the server is not reachable by a DNS name (e.g., because user has no means to create a DNS record)\nwhen the server is only reachable by a DNS name different from the name TLS certificate was issued for\nWhen set to a non-empty string, TLS client will validate certificate presented by the server against the SNI value rather than host address.\nTODO(yaro): add [(validate.rules).string = { address: true, ignore_empty: true } ]\n–\ninsecureSkipVerify\nbool once protoc-gen-validate tool is updated up to 0.5.0+ that support ignore_empty option When set to true, TLS client will not verify validity of the server certificate (:warning:).\nDefaults to false.\n:warning: WARNING: This setting makes TLS connections insecure because client does not validate identity of the server and might end up sending security-sensitive information to an attacker (man-in-the-middle).\n:warning: NEVER use this setting in production scenarios!\nThis setting is meant for use in non-production scenarios, such as:\ngetting started guides\ndisposable test and demo environments\nlocal development environments\n–\n","relpermalink":"/book/tsb/refs/onboarding/config/types/config/v1alpha1/transport-security/","summary":"Specifies configuration of a TLS client.","title":"Transport layer security config"},{"content":"本节向你介绍 TSB Operator 的基本概念。你将深入了解 TSB Operator 如何管理 TSB 的整个生命周期，包括跨各个平面的安装、升级和运行时行为。\nKubernetes 知识 如果你不熟悉 Kubernetes 命名空间、Operator、清单和自定义资源，建议你熟悉这些概念。此背景将极大地增强你对 TSB Operator 的理解以及维护 TSB 服务网格的能力。\n你可以查阅 Kubernetes 文档 来了解有关 Operator 模式的更多信息。\nTSB Operator 在控制 TSB 管理、控制和数据平面组件的安装、升级和运行时行为方面发挥着关键作用。为了确保兼容性并提供平滑的升级体验，基于 Kubernetes 的 TSB 组件清单已集成到 TSB Operator 中。因此，管理、控制和数据平面组件的版本与管理它们的 TSB Operator 部署的版本相关联。TSB Operator 利用用户创建的自定义资源 (CR) 来配置和实例化这些组件。\n为了有效管理 TSB 生命周期，TSB Operator 与 tctl CLI 工具密切协作。使用 tctl ，你可以生成跨管理、控制和数据平面安装和配置 TSB Operator 所需的初始 TSB Operator 清单。\n每个平面都需要其 TSB Operator 实例。安装后，TSB Operator 将配置为监控特定平面的相关 CR。TSB Operator 的行为受到多种因素的影响，包括：\n捆绑的 TSB 组件在 TSB Operator 中体现。 在 TSB Operator 监视的命名空间中检测到的 CR 内容。 由 TSB Operator 管理的 TSB 组件的存在。 通过 TSB Operator 对 TSB 生命周期的管理通常遵循当前状态和期望状态之间的协调过程。\n以下是与 TSB Operator 生命周期操作相关的要点：\nCR 的可用性向 TSB Operator 表明，应使用 CR 中指定的配置来部署固定 TSB 版本的所有组件。 CR 缺失会提示 TSB Operator 确保没有 TSB 组件正在运行。TSB Operator 将删除在其控制下部署的任何组件。 如果 CR 已可用，则使用较新版本的 Operator 更新 TSB Operator 引导清单会触发 TSB 升级。 更新 CR 会导致现有 TSB 安装重新配置以采用新的配置详细信息。 运行与 Operator 嵌入式清单中列出的版本不同的 TSB 组件将自动删除，以支持指定版本。 任何丢失的 TSB 组件（例如用户意外删除的组件）都会根据 TSB Operator 指定的固定版本和 CR 配置重新创建。 ","relpermalink":"/book/tsb/concepts/operators/","summary":"TSB Operator 概念介绍。","title":"TSB Operator"},{"content":"如何确定 Envoy 是否正常？ 确定 Envoy 是否正常的最佳方法是检查其健康和就绪端点（healthz）。要检查已加入的集群中应用程序的 Envoy 的 healthz 端点，你需要直接连接到应用程序的旁路 Envoy 边车。\n假设你在集群的 bookinfo 命名空间中有一个名为 details-v1-57f8794694-hc7gd 的 Pod，该 Pod 托管你的应用程序。\n使用 kubectl port-forward 建立本地机器到 Envoy 边车上的端口 15021 的端口转发：\nkubectl port-forward -n bookinfo details-v1-57f8794694-hc7gd 15021:15021 一旦上述命令成功执行，你现在应该能够将你喜爱的工具指向 URL http://localhost:15021/healthz/ready 并直接访问 Envoy 的 healthz 端点。请避免使用浏览器进行此操作，因为如果正确配置并运行，则 Envoy 代理将返回一个带有空主体的 200 OK 响应。\n例如，你可以使用 curl 以详细模式执行如下：\ncurl -v http://localhost:15021/healthz/ready 这应该会产生类似以下的输出。如果响应状态为 200 OK，则 Envoy 正常工作。\ncurl -v http://localhost:15021/healthz/ready * Trying 127.0.0.1:15021... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 15021 (#0) \u0026gt; GET /healthz/ready HTTP/1.1 \u0026gt; Host: localhost:15021 \u0026gt; User-Agent: curl/7.68.0 \u0026gt; Accept: */* \u0026gt; * Mark bundle as not supporting multiuse \u0026lt; HTTP/1.1 200 OK \u0026lt; date: Fri, 02 Jul 2021 13:32:05 GMT \u0026lt; content-length: 0 \u0026lt; x-envoy-upstream-service-time: 0 \u0026lt; server: envoy \u0026lt; * Connection #0 to host localhost left intact tctl 连接到集群失败 请检查你的 tctl 配置文件中是否包含与集群相关的正确组织和租户信息。\n首先，通过执行以下命令获取当前活动配置文件：\ntctl config profiles list 你应该会看到类似以下的输出。\nCURRENT NAME CLUSTER ACCOUNT default default admin-user * gke-tsb gke-tsb gke-user 带有星号（*）的条目是当前活动配置文件。要配置当前配置文件 gke-tsb，使 gke-user 使用组织名称 organization-name 和租户名称 tenant-name 连接到集群，请执行以下命令：\ntctl config users set \u0026#34;gke-user\u0026#34; \\ --org \u0026lt;organization-name\u0026gt; \\ --tenant \u0026lt;tenant-name\u0026gt; \\ --username \u0026lt;username\u0026gt; \\ --password \u0026lt;password\u0026gt; 组织名称和租户名称可以通过 Web 用户界面获取。\n此后，当你执行 tctl 命令时，将会针对指定的组织和租户运行。对于需要身份验证的每个 tctl 子命令，也可以通过显式指定 --org 和 --tenant 参数来完成相同的操作。\n是否可以在多个集群之间共享单个 TSB 实例？ 是的。单个 TSB 管理平面 能够管理大量集群。你需要将要关联到同一管理平面的每个集群都加入。此外，请参阅文档 TSB 资源消耗和容量规划 以获取有关随着参与集群数量增加可能需要的资源量的详细信息。\n如果需要为每个集群配置不同的权限或团队，请使用 工作区 和 组 进行逻辑分区。\n请查看我们的安装指南，了解如何将集群加入 TSB 。\n使用自定义证书时出现 “OPENSSL_VERIFY 失败” 错误。 当你使用中间 CA 或自己的证书时，客户端 Envoy 可能会出现 “OPENSSL_VERIFY 失败” 错误。\n“OPENSSL_VERIFY 失败” 错误可能由多种原因引起。你应该采取的一般方法是获取证书并验证其内容。请注意，诊断证书本身不在本文档的范围之内，你将不得不自行准备进行此操作。\nistioctl 提供了用于比较工作负载之间的 CA 包的内置命令：istioctl proxy-config rootca-compare pod/\u0026lt;pod-1\u0026gt;.\u0026lt;namespace-1\u0026gt; pod/\u0026lt;pod-2\u0026gt;.\u0026lt;namespace-2\u0026gt;。该命令自动化了下面的手动过程，并应该是在诊断 OPENSSL_VERIFY 错误时的首选选择。\n手动检查证书 要获取目标 Envoy 实例正在使用的证书，可以使用以下示例中的 istioctl。将 \u0026lt;server-pod-ID\u0026gt; 替换为你正在调试的 Envoy 实例的适当值：\nistioctl proxy-config secret \u0026lt;server-pod-ID\u0026gt; -ojson \u0026gt; server-tls.json 文件 server-tls.json\n将包含 Istio 互联网 TLS 证书，我们可以从中提取单独的证书。\ncat server-tls.json | \\ jq -r `.dynamicActiveSecrets[0].secret.tlsCertificate.certificateChain.inlineBytes\u0026#39; | \\ base64 --decode \u0026gt; server.crt 在以下示例中，我们将分离出服务器证书和其余链以进行演示，并使用 openssl verify 来检查证书。将以下 bash 脚本复制到名为 check-chain.sh 的文件中：\n#!/bin/bash # 用户提供的文件名。 usercert=$1 # 临时文件和清理 tmpfirst=$(mktemp) tmpchain=$(mktemp) function cleanup_tmpfiles { [ -f \u0026#34;$tmpfirst\u0026#34; ] \u0026amp;\u0026amp; rm -f \u0026#34;$tmpfirst\u0026#34;; [ -f \u0026#34;$tmpchain\u0026#34; ] \u0026amp;\u0026amp; rm -f \u0026#34;$tmpchain\u0026#34;; } trap cleanup_tmpfiles EXIT trap \u0026#39;trap - EXIT; cleanup_tmpfiles; exit -1\u0026#39; INT PIPE TERM outfile=\u0026#34;$tmpfirst\u0026#34; count=0 while IFS= read -r line do if [[ \u0026#34;$line\u0026#34; == *-\u0026#34;BEGIN CERTIFICATE\u0026#34;-* ]]; then ((count = $count + 1)) if [[ $count == 2 ]]; then outfile=\u0026#34;$tmpchain\u0026#34; fi fi echo $line \u0026gt;\u0026gt; \u0026#34;$outfile\u0026#34; done \u0026lt; \u0026#34;$usercert\u0026#34; openssl verify -CAfile \u0026#34;$tmpchain\u0026#34; \u0026#34;$tmpfirst\u0026#34; \u0026gt; /dev/null if [[ $? == 0 ]]; then echo \u0026#34;OK\u0026#34; fi 然后针对你在上一步中获得的文件运行它：\n$ bash check-chain.sh server.crt 如果在执行上述脚本时验证失败，则证书未正确链接。例如，CA 证书主体可能与工作负载证书的发行者不匹配。\nIstio CNI 如何与像 Cilium 或 Calico 这样的 Kubernetes CNI 协同工作？它会替代它们吗？ Istio 的 CNI 不会替代 Cilium 或 Calico 等 CNI 插件，但 Istio 的 CNI 会作为这些插件的附加组件与任何其他 Kubernetes CNI 协同工作（在 CNI 规范的术语中称为 “链接插件 \u0026#34;）。\n你的主要 CNI 插件将运行并构建你的 Pod 的 Kubernetes 网络，然后 Istio 的 CNI 将运行并重写网络规则以通过 Envoy 捕获流量。Istio 的 CNI 执行与 istio-init 容器完全相同的代码来重写这些网络规则（请查看 Istio 网站上的此博客 以深入了解流量拦截的工作原理）。\n来自 官方网站 的解释描述得很好：\n默认情况下，Istio 在部署到网格中的 Pod 中注入一个名为 istio-init 的初始化容器。istio-init 容器设置了将流量重定向到/从 Istio sidecar 代理的 Pod 网络流量。这需要部署到网格中的用户或服务帐户具有足够的 Kubernetes RBAC 权限以部署具有 NET_ADMIN 和 NET_RAW 能力的 容器 。要求 Istio 用户具备提升的 Kubernetes RBAC 权限对某些组织的安全合规性来说是有问题的。Istio CNI 插件是 istio-init 容器的替代品，执行相同的网络功能，但无需 Istio 用户启用提升的 Kubernetes RBAC 权限。\n如何在 TSB 中启用 Istio CNI？ 请查看我们的 Istio CNI 管理指南 ，了解如何在 TSB 中配置 Istio CNI。\n如果更改我的 CNI 插件，我需要在 TSB 或 Istio 中进行哪些操作？ 不需要进行任何操作：Istio 的 CNI 插件会自行配置以在主要插件之后运行。更改你的 CNI 提供程序并重新构建集群会确保 Istio 的 CNI 仍然在主要插件之后运行。\n配置 AWS 内部 ELB 在某些情况下，你可能希望部署在 EKS 集群中的服务所产生的 AWS 负载均衡器是内部的，而不是暴露给互联网。TSB 运算符 API 为你提供了在每个特定组件的 Kubernetes 服务中设置注释的途径，以便你可以添加 service.beta.kubernetes.io/aws-load-balancer-scheme 或 service.beta.kubernetes.io/aws-load-balancer-internal 注释。\n例如，以下代码片段：\nspec: components: frontEnvoy: kubeSpec: service: annotations: service.beta.kubernetes.io/aws-load-balancer-scheme: internal 将配置前端 Envoy（TSB API 和 UI 的主要入口点）的 Kubernetes 服务为内部负载均衡器。类似地，你可以为集群中部署的网关执行相同操作。\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: bookinfo namespace: bookinfo spec: kubeSpec: service: annotations: service.beta.kubernetes.io/aws-load-balancer-scheme: internal ","relpermalink":"/book/tsb/knowledge-base/faq/","summary":"关于 TSB 的常见问题","title":"TSB 常见问题解答"},{"content":" The following example creates a security group for the sidecars in ns1, ns2 and ns3 namespaces owned by its parent workspace w1 under tenant mycompany, and a security setting that applies the WAF Settings. And the security group and security settings to which this WAF Settings is applied to.\napiVersion: security.tsb.tetrate.io/v2 kind: Group metadata: name: t1 workspace: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \u0026#34;*/ns1\u0026#34; - \u0026#34;*/ns2\u0026#34; - \u0026#34;*/ns3\u0026#34; configMode: BRIDGED --- apiVersion: security.tsb.tetrate.io/v2 kind: SecuritySetting metadata: name: defaults group: t1 workspace: w1 tenant: mycompany organization: myorg spec: waf: rules: - Include @recommended-conf In the following examples, the security rule for blocking XSS requests is enabled on Tier1Gateway and IngressGateway respectively, with an ad-hoc debug configuration, instead of the one defined in the security rule.\napiVersion: gateway.xcp.tetrate.io/v2 kind: Tier1Gateway metadata: name: tier1-waf-gw group: g1 workspace: w1 tenant: mycompany organization: myorg spec: workloadSelector: namespace: ns1 labels: app: gateway passthroughServers: - name: nginx port: 8443 hostname: nginx.example.com waf: rules: - Include @owasp_crs/REQUEST-941-APPLICATION-ATTACK-XSS.conf apiVersion: gateway.xcp.tetrate.io/v2 kind: IngressGateway metadata: name: waf-gw group: g1 workspace: w1 tenant: mycompany organization: myorg spec: workloadSelector: namespace: ns1 labels: app: waf-gateway waf: rules: - SecRuleEngine DETECTION_ONLY - SecDebugLogLevel 5 - Include @owasp_crs/REQUEST-941-APPLICATION-ATTACK-XSS.conf http: - name: bookinfo port: 9443 hostname: bookinfo.com WAFSettings WAFSettings configure WAF based on seclang See https://github.com/SpiderLabs/ModSecurity/wiki/Reference-Manual-%28v3.x%29#Configuration_Directives Field Description Validation Rule rules\nList of string REQUIRED Rules to be leveraged by WAF. The parser evaluates the list of rules from the top to the bottom.\nrepeated = { min_items: 1 items: {string:{min_len:1}}}\n","relpermalink":"/book/tsb/refs/tsb/security/v2/waf-settings/","summary":"Settings for the Web Application Firewall component, based on the Modsecurity/Coraza Seclang.","title":"WAF Settings"},{"content":" The WASM extension resource allows defining custom WASM extensions that are packaged in OCI images. The resource allows specifying extension metadata that helps understand how extensions work and how they can be used. Once defined, extensions can be referenced in Ingress and Egress Gateways and Security Groups so that traffic is captured and processed by the extension accordingly. By default, extensions are globally available, but they can be assigned to specific Tenants as well to further control and constraint where in the Organization the extensions are allowed to be used.\napiVersion: extension.tsb.tetrate.io/v2 kind: WasmExtension metadata: organization: org name: wasm-auth spec: allowedIn: - organizations/org/tenants/tenant1 url: oci://docker.io/example/my-wasm-extension:1.0 source: https://github.com/example/wasm-extension description: | Long description for the extension such as an entire README file phase: AUTHZ priority: 1000 config: some_key: some_value WASM extensions can also reference HTTP endpoints:\napiVersion: extension.tsb.tetrate.io/v2 kind: WasmExtension metadata: organization: org name: wasm-http spec: url: http://tetrate.io/my-extension.wasm source: https://github.com/example/wasm-extension description: | Long description for the extension such as an entire README file phase: AUTHZ priority: 1000 config: some_key: some_value EnvVar Field Description Validation Rule name\nstring REQUIRED Name of the environment variable. Must be a C_IDENTIFIER, by following this regex: [A-Za-z_][A-Za-z0-9_]*\nstring = { pattern: [A-Za-z_][A-Za-z0-9_]*}\nvalueFrom\ntetrateio.api.tsb.extension.v2.EnvValueSource REQUIRED Source for the environment variable’s value.\nenum = { defined_only: true}\nvalue\nstring Value for the environment variable. Note that if value_from is HOST, it will be ignored. Defaults to “”.\n–\nGlobalTrafficSelector GlobalTrafficSelector provides a mechanism to select a specific traffic flow for which this Wasm Extension will be enabled. This setting applies to all WASM Extension attachments. These selectors can be overridden at attachments. When all the sub conditions in the TrafficSelector are satisfied, the traffic will be selected.\nField Description Validation Rule mode\ntetrateio.api.tsb.types.v2.WorkloadMode Criteria for selecting traffic by their direction. Note that CLIENT and SERVER are analogous to OUTBOUND and INBOUND, respectively. For the gateway, the field should be CLIENT or CLIENT_AND_SERVER. If not specified, the default value is CLIENT_AND_SERVER.\n–\nVmConfig Configuration for a Wasm VM. more details can be found here .\nField Description Validation Rule env\nList of tetrateio.api.tsb.extension.v2.EnvVar Specifies environment variables to be injected to this VM. Note that if a key does not exist, it will be ignored.\n–\nWasmExtension Field Description Validation Rule allowedIn\nList of string List of fqns where this extension is allowed to run. If it is empty, the extension can be used across the entire organization. Currently only Tenant resources are considered.\nrepeated = { items: {string:{min_len:1}}}\nimage\nstring Deprecated. Use the url field instead. Repository and tag of the OCI image containing the WASM extension.\n–\nsource\nstring Source to find the code for the WASM extension\n–\nphase\ntetrateio.api.tsb.extension.v2.WasmExtension.PluginPhase The phase in the filter chain where the extension will be injected. https://istio.io/latest/docs/reference/config/proxy_extensions/wasm-plugin/#PluginPhase enum = { defined_only: true}\npriority\nint32 Determines the ordering of WasmExtensions in the same phase. When multiple WasmExtensions are applied to the same workload in the same phase, they will be applied by priority, in descending order. If no priority is assigned it will use the default 0 value. In case of several extensions having the same priority in the same phase, the fqn will be used to sort them.\n–\nconfig\ngoogle.protobuf.Struct Configuration parameters sent to the WASM plugin execution The configuration can be overwritten when instantiating the extensions in IngressGateways or Security groups. The config is serialized using proto3 JSON marshaling and passed to proxy_on_configure when the host environment starts the plugin.\n–\nimagePullPolicy\ntetrateio.api.tsb.extension.v2.WasmExtension.PullPolicy The pull behaviour to be applied when fetching Wasm module by either OCI image or http/https. Only relevant when referencing Wasm module without any digest, including the digest in OCI image URL or sha256 field in vm_config. Defaults to IfNotPresent, except when an OCI image is referenced in the url and the latest tag is used, in which case Always is the default, mirroring K8s behaviour.\nenum = { defined_only: true}\nimagePullSecret\nstring Credentials to use for OCI image pulling. Name of a K8s Secret that contains a docker pull secret which is to be used to authenticate against the registry when pulling the image. If TSB is configured to use the WASM download proxy, this secret must …","relpermalink":"/book/tsb/refs/tsb/extension/v2/wasm-extension/","summary":"WASM Extension definition","title":"WASM Extension"},{"content":" Service to manage WASM extensions.\nWasmExtensions The WasmExtension service provides methods to manage the extensions inside an Organization. WasmExtensions are created inside TSB and assigned later to SecuritySettings and IngressGateways.\nGetWasmExtension rpc GetWasmExtension (tetrateio.api.tsb.extension.v2.GetWasmExtensionRequest ) returns (tetrateio.api.tsb.extension.v2.WasmExtension )\nRequires READ\nGet a WASM extension\nListWasmExtension rpc ListWasmExtension (tetrateio.api.tsb.extension.v2.ListWasmExtensionRequest ) returns (tetrateio.api.tsb.extension.v2.ListWasmExtensionResponse )\nList the WASM extensions that are defined for the Organization.\nCreateWasmExtension rpc CreateWasmExtension (tetrateio.api.tsb.extension.v2.CreateWasmExtensionRequest ) returns (tetrateio.api.tsb.extension.v2.WasmExtension )\nRequires CREATE\nCreates a new WasmExtension object in TSB. This is needed to let the extensions run. Once a WasmExtension has been created, it can be assigned to IngressGateway and SecuritySetting. This method returns the created extension.\nUpdateWasmExtension rpc UpdateWasmExtension (tetrateio.api.tsb.extension.v2.WasmExtension ) returns (tetrateio.api.tsb.extension.v2.WasmExtension )\nRequires WRITE\nModify an existing WasmExtension. When modifying the details of an extension in use, such as the image property, enabled flag, phase, or default configuration, a redeploy or reconfiguration of the extension may be triggered, affecting live traffic in all those places that reference the extension. Similarly, changes to the allowed_in property may trigger the removal of the extension from all places where the extension was in use that are not allowed to use it anymore, affecting live traffic on the relevant namespaces as well.\nDeleteWasmExtension rpc DeleteWasmExtension (tetrateio.api.tsb.extension.v2.DeleteWasmExtensionRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete a WasmExtension. Note that deleting a WasmExtension will delete the extension itself, and also its assignments to IngressGateway and SecuritySetting.\nCreateWasmExtensionRequest Request to create a WasmExtension and make it available to be assigned to IngressGateway and SecuritySetting.\nField Description Validation Rule parent\nstring REQUIRED Parent resource where the extension will be created. This is the FQN of the organization.\nstring = { min_len: 1}\nname\nstring REQUIRED The short name for the resource to be created.\nstring = { min_len: 1}\nwasmExtension\ntetrateio.api.tsb.extension.v2.WasmExtension REQUIRED Details of the extension to be created.\nmessage = { required: true}\nDeleteWasmExtensionRequest Request to delete a WasmExtension.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the WasmExtension.\nstring = { min_len: 1}\nGetWasmExtensionRequest Request to retrieve a WASM extension.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the extension.\nstring = { min_len: 1}\nListWasmExtensionRequest Request to retrieve the list of WASM extensions for a given Organization.\nField Description Validation Rule parent\nstring REQUIRED Parent resource where the WasmExtension will be created.\nstring = { min_len: 1}\nListWasmExtensionResponse List of WASM Extensions.\nField Description Validation Rule extensions\nList of tetrateio.api.tsb.extension.v2.WasmExtension –\n","relpermalink":"/book/tsb/refs/tsb/extension/v2/wasm-service/","summary":"Service to manage WASM extensions.","title":"WasmExtension Service"},{"content":" Workload Auto Registration represents a registry record of a workload onboarded into the mesh.\nWorkload Auto Registration captures essential information about the workload allowing Workload Onboarding Plane to generate boot configuration for the Istio Sidecar that will be started alongside this workload.\nWorkloadAutoRegistration resource is not supposed to be edited by the users. Instead, it gets created automatically as part of the Workload Onboarding flow.\nUsers can introspect WorkloadAutoRegistration resources for the purposes of observability and troubleshooting of Workload Onboarding.\nTo leverage k8s resource garbage collection (i.e. cascade removal),\nWorkloadAutoRegistration resource is owned by the WorkloadGroup resource the workload has joined to WorkloadAutoRegistration resource owns the Istio WorkloadEntry resource that describes the workload to the Istio Control Plane. WorkloadGroup | | (owns) | └── WorkloadAutoRegistration | | (owns) | └── WorkloadEntry E.g.,\napiVersion: runtime.onboarding.tetrate.io/v1alpha1 kind: WorkloadAutoRegistration metadata: namespace: bookinfo name: ratings-aws-aws-123456789012-ca-central-1b-ec2-i-1234567890abcdef0 ownerReferences: - apiVersion: networking.istio.io/v1beta1 blockOwnerDeletion: true controller: true kind: WorkloadGroup name: ratings uid: fb67dbad-b063-40e5-a958-098fbe7b40f4 spec: identity: aws: partition: aws account: \u0026#39;123456789012\u0026#39; region: ca-central-1 zone: ca-central-1b ec2: instance_id: i-1234567890abcdef0 registration: agent: version: \u0026#39;1.4.0\u0026#39; sidecar: istio: version: \u0026#39;1.8.5-abcd\u0026#39; host: addresses: - ip: 10.0.0.1 type: VPC - ip: 1.2.3.4 type: INTERNET workload: labels: cloud: aws class: ec2 version: v3 settings: connectedOver: INTERNET status: activeAgentConnection: connectedTo: onboarding-plane-745bf76677-974tq conditions: - type: AgentConnected status: True reason: ConnectionEstablished lastTransitionTime: \u0026#34;2020-12-02T18:26:08Z\u0026#34; AgentConnection AgentConnection specifies information about the persistent connection between the Workload Onboarding Agent and a Workload Onboarding Plane instance.\nField Description Validation Rule connectedTo\nstring REQUIRED Identifier of the Workload Onboarding Plane instance the Agent is connected to.\nstring = { min_len: 1}\nWorkloadAutoRegistrationSpec WorkloadAutoRegistrationSpec is the specification of the workload’s registration within the mesh.\nField Description Validation Rule identity\ntetrateio.api.onboarding.config.types.identity.v1alpha1.WorkloadIdentity REQUIRED Platform-specific identity of the workload.\nmessage = { required: true}\nregistration\ntetrateio.api.onboarding.config.types.registration.v1alpha1.Registration REQUIRED Workload registration information.\nmessage = { required: true}\nWorkloadAutoRegistrationStatus WorkloadAutoRegistrationStatus represents the current status of the workload’s registration within the mesh.\nField Description Validation Rule observedGeneration\nint64 The most recent generation observed by the WorkloadAutoRegistration controller.\n–\nconditions\nList of tetrateio.api.onboarding.config.types.core.v1alpha1.Condition Currently observed conditions.\nrepeated = { items: {message:{required:true}}}\nactiveAgentConnection\ntetrateio.api.onboarding.config.runtime.v1alpha1.AgentConnection Information about the active persistent connection between the Workload Onboarding Agent and a Workload Onboarding Plane instance.\n–\n","relpermalink":"/book/tsb/refs/onboarding/config/runtime/v1alpha1/registration/","summary":"Registry record of a workload onboarded into the mesh.","title":"Workload Auto Registration"},{"content":" WorkloadConfiguration specifies configuration of the workload handling.\nFor example,\nauthentication: jwt: issuers: - issuer: \u0026#34;https://mycompany.corp\u0026#34; jwksUri: \u0026#34;https://mycompany.corp/jwks.json\u0026#34; shortName: \u0026#34;mycorp\u0026#34; tokenFields: attributes: jsonPath: .custom_attributes deregistration: propagationDelay: 15s JwtAuthenticationConfiguration JwtAuthenticationConfiguration specifies configuration of the workload authentication by means of an OIDC ID Token .\nField Description Validation Rule issuers\nList of tetrateio.api.onboarding.config.install.v1alpha1.JwtIssuer List of permitted JWT issuers.\nIf a workload authenticates itself by means of an OIDC ID Token , the issuer of that token must be present in this list, otherwise authentication attempt will be declined.\nrepeated = { items: {message:{required:true}}}\nWorkloadAuthenticationConfiguration WorkloadAuthenticationConfiguration specifies configuration of the workload authentication.\nField Description Validation Rule jwt\ntetrateio.api.onboarding.config.install.v1alpha1.JwtAuthenticationConfiguration JWT authentication configuration.\n–\nWorkloadConfiguration WorkloadConfiguration specifies configuration of the workload handling.\nField Description Validation Rule authentication\ntetrateio.api.onboarding.config.install.v1alpha1.WorkloadAuthenticationConfiguration Workload authentication configuration.\n–\nderegistration\ntetrateio.api.onboarding.config.install.v1alpha1.WorkloadDeregistrationConfiguration Workload deregistration configuration.\n–\nWorkloadDeregistrationConfiguration WorkloadDeregistrationConfiguration specifies configuration of the workload deregistration.\nField Description Validation Rule propagationDelay\ngoogle.protobuf.Duration Estimated amount of time it takes to propagate the unregistration event across all affected mesh nodes.\nDuring this time interval affected proxies will continue making requests to the deregistered workload until the respective configuration update arrives.\nTo prevent traffic loss, Workload Onboarding Agent SHOULD delay shutdown of the the workload’s sidecar for that time period.\nAs a rule of thumb, this value should remain relatively small, e.g. under 15 seconds. The reason for this is that shutdown flow on the workload’s side is time-boxed. E.g., on VMs there is a stop timeout enforced by SystemD, while on AWS ECS there is a stop timeout enforced by ECS Agent. If you pick a delay value that is too big, Workload Onboarding Agent will delay shutdown of the sidecar for too long; as a result sidecar risks to get terminated abruptly instead of graceful connection draining.\nDefaults to 10s.\nduration = { required: true gte: {nanos:0}}\n","relpermalink":"/book/tsb/refs/onboarding/config/install/v1alpha1/workload-configuration/","summary":"Configuration of the workload handling.","title":"Workload Configuration"},{"content":" WorkloadIdentity represents a platform-specific identity of a workload joining the mesh.\nE.g.,\nAWS EC2 instance identity:\naws: partition: aws account: \u0026#39;123456789012\u0026#39; region: ca-central-1 zone: ca-central-1b ec2: instance_id: i-1234567890abcdef0 iam_role: name: example-role GCP GCE instance identity:\ngcp: project_number: \u0026#39;234567890121\u0026#39; project_id: gcp-example region: us-central1 zone: us-central1-a gce: instance_id: \u0026#39;693197132356332126\u0026#39; Azure Compute instance identity:\nazure: subscription: 531bed28-f708-4fc5-b0c1-2c1edde46e4f resource_group: azure-example compute: instance_id: fc13d26e-d3c0-458e-b353-686d5ca19506 JWT identity:\njwt: issuer: https://mycompany.corp subject: us-east-datacenter1-vm007 attributes: region: us-east datacenter: datacenter1 instance_name: vm007 instance_hostname: vm007.internal.corp instance_role: app-ratings WorkloadIdentity WorkloadIdentity represents a platform-specific identity of a workload joining the mesh.\nField Description Validation Rule aws\ntetrateio.api.onboarding.config.types.identity.aws.v1alpha1.AwsIdentity oneof kind AWS-specific identity of a workload.\n–\njwt\ntetrateio.api.onboarding.config.types.identity.jwt.v1alpha1.JwtIdentity oneof kind JWT identity of a workload.\n–\n","relpermalink":"/book/tsb/refs/onboarding/config/types/identity/v1alpha1/identity/","summary":"Platform-specific identity of a workload joining the mesh.","title":"Workload Identity"},{"content":" Workload Registration specifies information sent by the Workload Onboarding Agent to the Workload Onboarding Plane to register the workload in the mesh.\nAgentInfo AgentInfo specifies information about the Workload Onboarding Agent installed alongside the workload.\nField Description Validation Rule version\nstring REQUIRED Version of the Workload Onboarding Agent.\nstring = { min_len: 1}\nIstioSidecarInfo IstioInfo specifies information about the Istio Sidecar installed alongside the workload.\nField Description Validation Rule version\nstring REQUIRED Version of the Istio Sidecar.\nstring = { min_len: 1}\nrevision\nstring Istio revision the pre-installed Istio Sidecar corresponds to.\nE.g., canary, alpha, etc.\nIf omitted, it is assumed that the pre-installed Istio Sidecar corresponds to the default Istio revision.\nNotice that the value constraints here are stricter than the ones in Istio. Apparently, Istio validation rules allow values that lead to internal failures at runtime, e.g. values with capital letters or values longer than 56 characters. Stricter validation rules here are meant to prevent those hidden pitfalls.\nstring = { min_len: 1 max_len: 56 pattern: ^[a-z0-9](?:[-a-z0-9]*[a-z0-9])?$ ignore_empty: true}\nRegistration Registration specifies information sent by the Workload Onboarding Agent to the Workload Onboarding Plane to register the workload in the mesh.\nField Description Validation Rule agent\ntetrateio.api.onboarding.config.types.registration.v1alpha1.AgentInfo REQUIRED Information about the Workload Onboarding Agent installed alongside the workload.\nmessage = { required: true}\nsidecar\ntetrateio.api.onboarding.config.types.registration.v1alpha1.SidecarInfo REQUIRED Information about the sidecar installed alongside the workload.\nmessage = { required: true}\nhost\ntetrateio.api.onboarding.config.types.registration.v1alpha1.HostInfo REQUIRED Information about the host the workload is running on.\nmessage = { required: true}\nworkload\ntetrateio.api.onboarding.config.types.registration.v1alpha1.WorkloadInfo Information about the workload.\n–\nsettings\ntetrateio.api.onboarding.config.types.registration.v1alpha1.Settings Registration settings.\n–\nSettings Settings specifies registration settings.\nField Description Validation Rule connectedOver\ntetrateio.api.onboarding.config.types.registration.v1alpha1.AddressType ConnectedOver specifies how the workload is connected to the mesh, i.e. over VPC or over Internet. When unspecified, workload is assumed connected to the mesh over VPC.\n–\nSidecarInfo SidecarInfo specifies information about the sidecar installed alongside the workload.\nField Description Validation Rule istio\ntetrateio.api.onboarding.config.types.registration.v1alpha1.IstioSidecarInfo oneof kind Information about the Istio Sidecar installed alongside the workload.\n–\nWorkloadInfo WorkloadInfo specifies information about the workload.\nField Description Validation Rule labels\nmap\u0026lt;string , string \u0026gt; Labels associated with the workload.\n–\n","relpermalink":"/book/tsb/refs/onboarding/config/types/registration/v1alpha1/registration/","summary":"Information sent by the Workload Onboarding Agent to the Workload Onboarding Plane to register the workload in the mesh.","title":"Workload Registration"},{"content":"List of annotations on a WorkloadEntry resource supported by the tctl x sidecar-bootstrap command.\nUsage example apiVersion: networking.istio.io/v1beta1 kind: WorkloadEntry metadata: name: my-vm namespace: my-namespace annotations: sidecar-bootstrap.istio.io/ssh-user: istio-proxy sidecar-bootstrap.istio.io/proxy-config-dir: /etc/istio-proxy sidecar-bootstrap.istio.io/proxy-instance-ip: 10.0.0.1 sidecar.istio.io/logLevel: debug sidecar.istio.io/componentLogLevel: upstream:info,config:trace sidecar.istio.io/statsInclusionRegexps: .* # enable all Envoy metrics proxy.istio.io/config: | concurrency: 3 spec: ... Standard Istio annotations proxy.istio.io/config Overrides for the proxy configuration for this specific proxy. Available options can be found at https://istio.io/docs/reference/config/istio.mesh.v1alpha1/#ProxyConfig .\nsidecar.istio.io/interceptionMode Specifies the mode used to redirect inbound connections to Envoy (REDIRECT or TPROXY).\nsidecar.istio.io/proxyImage Specifies the Docker image to be used by the Envoy sidecar.\nsidecar.istio.io/logLevel Specifies the log level for Envoy.\nsidecar.istio.io/componentLogLevel Specifies the component log level for Envoy.\nsidecar.istio.io/agentLogLevel Specifies the log output level for pilot-agent.\nsidecar.istio.io/statsInclusionPrefixes Specifies the comma separated list of prefixes of the stats to be emitted by Envoy.\nsidecar.istio.io/statsInclusionSuffixes Specifies the comma separated list of suffixes of the stats to be emitted by Envoy.\nsidecar.istio.io/statsInclusionRegexps Specifies the comma separated list of regexes the stats should match to be emitted by Envoy.\nAnnotations specific to tctl x sidecar-bootstrap command sidecar-bootstrap.istio.io/istio-revision Istio revision the Istio proxy should connect to.\nBy default, default Istio revision is assumed.\nsidecar-bootstrap.istio.io/mesh-expansion-configmap Name of the Kubernetes config map that holds configuration intended for those Istio Proxies that expand the mesh.\nConfigMap should include the following keys:\nPROXY_CONFIG - (mandatory) ProxyConfig in YAML format (see https://istio.io/latest/docs/reference/config/istio.mesh.v1alpha1/#ProxyConfig ) This configuration is applied on top of mesh-wide default ProxyConfig, but prior to the workload-specific ProxyConfig from proxy.istio.io/config annotation on a WorkloadEntry.\nBy default, config map is considered undefined and thus expansion proxies will have the same configuration as the regular ones.\nsidecar-bootstrap.istio.io/ssh-host IP address or DNS name of the machine represented by this WorkloadEntry to use instead of WorkloadEntry.Address for SSH connections initiated by the tctl x sidecar-bootstrap command.\nThis setting is intended for those scenarios where tctl x sidecar-bootstrap command will be run on a machine without direct connectivity to the WorkloadEntry.Address. E.g., one might set WorkloadEntry.Address to the Internal IP of a VM and set value of this annotation to the External IP of that VM.\nBy default, value of WorkloadEntry.Address is assumed.\nsidecar-bootstrap.istio.io/ssh-port Port of the SSH server on the machine represented by this WorkloadEntry to use for SSH connections initiated by the tctl x sidecar-bootstrap command.\nBy default, “22” is assumed.\nsidecar-bootstrap.istio.io/ssh-user User on the machine represented by this WorkloadEntry to use for SSH connections initiated by the tctl x sidecar-bootstrap command.\nMake sure that user has enough permissions to create the config dir and to run Docker container without sudo.\nBy default, a user running tctl x sidecar-bootstrap command is assumed.\nsidecar-bootstrap.istio.io/scp-path Path to the scp binary on the machine represented by this WorkloadEntry to use in SSH connections initiated by the tctl x sidecar-bootstrap command.\nBy default, “/usr/bin/scp” is assumed.\nsidecar-bootstrap.istio.io/proxy-config-dir Directory on the machine represented by this WorkloadEntry where tctl x sidecar-bootstrap command should copy bootstrap bundle to.\nBy default, “/tmp/istio-proxy” is assumed (the most reliable default value for out-of-the-box experience).\nsidecar-bootstrap.istio.io/proxy-image-hub Hub with Istio Proxy images that the machine represented by this WorkloadEntry should pull from instead of the mesh-wide hub.\nBy default, mesh-wide hub is assumed.\nsidecar-bootstrap.istio.io/proxy-container-name Name for a container with Istio Proxy.\nIf you need to run multiple Istio Proxy containers on the same machine, make sure each of them has a unique name.\nBy default, “istio-proxy” is assumed.\nsidecar-bootstrap.istio.io/proxy-instance-ip IP address of the machine represented by this WorkloadEntry that Istio Proxy should bind inbound listeners to.\nThis setting is intended for those scenarios where Istio Proxy cannot bind to the IP address specified in the WorkloadEntry.Address (e.g., on AWS EC2 where a VM can only bind the private IP but not the public one).\nBy default, WorkloadEntry.Address is …","relpermalink":"/book/tsb/reference/cli/reference/workload-entry-annotations/","summary":"WorkloadEntry Annotations","title":"WorkloadEntry Annotations"},{"content":" A Workspace carves a chunk of the cluster resources owned by a tenant into an isolated configuration domain.\nThe following example claims ns1 and ns2 namespaces across all clusters owned by the tenant mycompany.\napiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: name: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \u0026#34;*/ns1\u0026#34; - \u0026#34;*/ns2\u0026#34; The following example claims ns1 namespace only from the c1 cluster and claims all namespaces from the c2 cluster.\napiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: name: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \u0026#34;c1/ns1\u0026#34; - \u0026#34;c2/*\u0026#34; Custom labels and annotations can be propagated to the final Istio translation that will be applied at the clusters. This could help with third-party integrations or to set custom identifier. The following example configures the annotation my.org.environment to be applied to all final Istio translations generated under this Workspace, for example Gateways or Virtual Services.\napiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: name: w1 tenant: mycompany organization: myorg annotations: my.org.environment: dev spec: namespaceSelector: names: - \u0026#34;*/ns1\u0026#34; Workspace A Workspace is a collection of related namespaces in one or more clusters.\nField Description Validation Rule namespaceSelector\ntetrateio.api.tsb.types.v2.NamespaceSelector REQUIRED Set of namespaces owned exclusively by this workspace. A workspace can own all namespaces of a cluster or a set of namespaces across any cluster or a set of namespaces in a specific cluster. Use */* to claim all cluster resources under the tenant.\nmessage = { required: true}\nprivileged\ngoogle.protobuf.BoolValue If set to true, allows Gateways in the workspace to route to services in other workspaces. Set this to true for workspaces owning cluster-wide gateways shared by multiple teams.\n–\nisolationBoundary\nstring OPTIONAL Istio Isolation Boundary name to which this workspace belongs. If not provided explicitly, the workspace looks for an isolation boundary with name set as “global”. Therefore, in order to move existing workspaces to isolation boundaries, and be a part of revisioned control plane, it is recommended to configure an isolation boundary with the name “global”.\n–\nsecurityDomain\nstring Security domains can be used to group different resources under the same security domain. Although security domain is not resource itself currently, it follows a fqn format organizations/myorg/securitydomains/mysecuritydomain, and a child cannot override any ancestor’s security domain. Once a security domain is assigned to a Workspace, all the children resources will belong to that security domain in the same way a Security group belongs to a Workspace, a Security group will also belong to the security domain assigned to the Workspace. Security domains can also be used to define Security settings Authorization rules in which you can allow or deny request from or to a security domain.\n–\ndeletionProtectionEnabled\nbool When set, prevents the resource from being deleted. In order to delete the resource this property needs to be set to false first.\n–\nconfigGenerationMetadata\ntetrateio.api.tsb.types.v2.ConfigGenerationMetadata Default metadata values that will be propagated to the children Istio generated configurations. When using YAML APIs liketctl or gitops, put them into the metadata.labels or metadata.annotations instead. This field is only necessary when using gRPC APIs directly.\n–\n","relpermalink":"/book/tsb/refs/tsb/v2/workspace/","summary":"Configurations that group a set of related namespaces.","title":"Workspace"},{"content":" DEPRECATED: use Access Bindings instead.\nWorkspaceAccessBindings is an assignment of roles to a set of users or teams to access resources under a Workspace. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a WorkspaceAccessBinding can be created or modified only by users who have SET_POLICY permission on the Workspace.\nThe following example assigns the workspace-admin role to users alice, bob, and members of the t1 team for all workspace w1 owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team\napiVersion: rbac.tsb.tetrate.io/v2 kind: WorkspaceAccessBindings metadata: organization: myorg tenant: mycompany workspace: w1 spec: allow: - role: rbac/workspace-admin subjects: - user: organization/myorg/users/alice - user: organization/myorg/users/bob - team: organization/myorg/teams/t1 WorkspaceAccessBindings WorkspaceAccessBindings assigns permissions to users of workspaces.\nField Description Validation Rule allow\nList of tetrateio.api.tsb.rbac.v2.Binding The list of allowed bindings configures the different access profiles that are allowed on the resource configured by the policy.\n–\n","relpermalink":"/book/tsb/refs/tsb/rbac/v2/workspace-access-bindings/","summary":"Configuration for assigning access roles to users of workspaces.","title":"Workspace Access Bindings"},{"content":" Service to manage TSB workspaces.\nWorkspaces The Workspaces service provides methods to manage the workspaces for a given tenant.\nWorkspaces are the main containers for the different configuration resources available in TSB, and provide infrastructure isolation constraints.\nCreateWorkspace rpc CreateWorkspace (tetrateio.api.tsb.v2.CreateWorkspaceRequest ) returns (tetrateio.api.tsb.v2.Workspace )\nRequires CREATE\nCreate a new workspace. The workspace will own exclusively the namespaces configured in the namespaces selector for the workspace.\nGetWorkspace rpc GetWorkspace (tetrateio.api.tsb.v2.GetWorkspaceRequest ) returns (tetrateio.api.tsb.v2.Workspace )\nRequires READ\nGet the details of an existing workspace\nUpdateWorkspace rpc UpdateWorkspace (tetrateio.api.tsb.v2.Workspace ) returns (tetrateio.api.tsb.v2.Workspace )\nRequires WRITE\nModify an existing workspace\nListWorkspaces rpc ListWorkspaces (tetrateio.api.tsb.v2.ListWorkspacesRequest ) returns (tetrateio.api.tsb.v2.ListWorkspacesResponse )\nList all existing workspaces for the given tenant.\nDeleteWorkspace rpc DeleteWorkspace (tetrateio.api.tsb.v2.DeleteWorkspaceRequest ) returns (google.protobuf.Empty )\nRequires DELETE\nDelete an existing workspace. Note that deleting resources in TSB is a recursive operation. Deleting a workspace will delete all groups and configuration objects that exist in it.\nCreateSettings rpc CreateSettings (tetrateio.api.tsb.v2.CreateWorkspaceSettingsRequest ) returns (tetrateio.api.tsb.v2.WorkspaceSetting )\nRequires CreateWorkspaceSetting\nCreate default settings for a workspace. Default settings will apply to the services owned by the workspace, unless more specific settings are provided at the group level.\nGetSettings rpc GetSettings (tetrateio.api.tsb.v2.GetWorkspaceSettingsRequest ) returns (tetrateio.api.tsb.v2.WorkspaceSetting )\nRequires ReadWorkspaceSetting\nGet the details of a settings object for the given workspace.\nUpdateSettings rpc UpdateSettings (tetrateio.api.tsb.v2.WorkspaceSetting ) returns (tetrateio.api.tsb.v2.WorkspaceSetting )\nRequires WriteWorkspaceSetting\nModify the given workspace settings.\nListSettings rpc ListSettings (tetrateio.api.tsb.v2.ListWorkspaceSettingsRequest ) returns (tetrateio.api.tsb.v2.ListWorkspaceSettingsResponse )\nList all settings available for the given workspace.\nDeleteSettings rpc DeleteSettings (tetrateio.api.tsb.v2.DeleteWorkspaceSettingsRequest ) returns (google.protobuf.Empty )\nRequires DeleteWorkspaceSetting\nDelete the given workspace settings.\nCreateWorkspaceRequest Request to create a Workspace.\nField Description Validation Rule parent\nstring REQUIRED Parent resource where the Workspace will be created.\nstring = { min_len: 1}\nname\nstring REQUIRED The short name for the resource to be created.\nstring = { min_len: 1}\nworkspace\ntetrateio.api.tsb.v2.Workspace REQUIRED Details of the Workspace to be created.\nmessage = { required: true}\nCreateWorkspaceSettingsRequest Request to create a Workspace Settings.\nField Description Validation Rule parent\nstring REQUIRED Parent resource where the Workspace Settings will be created.\nstring = { min_len: 1}\nname\nstring REQUIRED The short name for the resource to be created.\nstring = { min_len: 1}\nsettings\ntetrateio.api.tsb.v2.WorkspaceSetting REQUIRED Details of the Workspace Settings to be created.\nmessage = { required: true}\nDeleteWorkspaceRequest Request to delete a Workspace.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the Workspace.\nstring = { min_len: 1}\nforce\nbool Force the deletion of the object even if deletion protection is enabled. If this is set, then the object and all its children will be deleted even if any of them has the deletion protection enabled.\n–\nDeleteWorkspaceSettingsRequest Request to delete a Workspace Settings.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the Workspace Settings.\nstring = { min_len: 1}\nGetWorkspaceRequest Request to retrieve a Workspace.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the Workspace.\nstring = { min_len: 1}\nGetWorkspaceSettingsRequest Request to retrieve a Workspace Settings.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the Workspace Settings.\nstring = { min_len: 1}\nListWorkspaceSettingsRequest Request to list Workspace Settings.\nField Description Validation Rule parent\nstring REQUIRED Parent resource to list Workspace Settings from.\nstring = { min_len: 1}\nListWorkspaceSettingsResponse The existing settings objects for the given workspace.\nField Description Validation Rule settings\nList of tetrateio.api.tsb.v2.WorkspaceSetting –\nListWorkspacesRequest Request to list Workspaces.\nField Description Validation Rule parent\nstring REQUIRED Parent resource to list Workspaces from.\nstring = { min_len: 1}\nListWorkspacesResponse The existing workspaces for the given tenant.\nField Description Validation Rule workspaces\nList of tetrateio.api.tsb.v2.Workspace –\n","relpermalink":"/book/tsb/refs/tsb/v2/workspace-service/","summary":"Service to manage TSB workspaces.","title":"Workspace Service"},{"content":" Workspace Setting allows configuring the default traffic, security and east-west gateway settings for all the workloads in the namespaces owned by the workspace. Any namespace in the workspace that is not part of a traffic or security group with specific settings will use these default settings.\nThe following example sets the default security policy to accept either mutual TLS or plaintext traffic, and only accept connections at a proxy workload from services within the same namespace. The default traffic policy allows unknown traffic from a proxy workload to be forwarded via an egress gateway tsb-egress in the perimeter namespace in the same cluster.\napiVersion: api.tsb.tetrate.io/v2 kind: WorkspaceSetting metadata: name: w1-settings workspace: w1 tenant: mycompany organization: myorg spec: defaultSecuritySetting: authenticationSettings: trafficMode: REQUIRED defaultTrafficSetting: egress: host: bookinfo-perimeter/tsb-egress This other example sets the defaults for east-west traffic configuring gateways for two different app groups. The first setting configures the gateway from the namespace platinum to manage the traffic for all those workloads with the labels tier: platinum and critical: true. The second one configures the gateway from the namespace internal to manage the traffic for all those workloads with the labels app: eshop or internal-critical: true. Setting up multiple east-west gateways allows isolating also the cross-cluster traffic.\napiVersion: api.tsb.tetrate.io/v2 kind: WorkspaceSetting metadata: name: w1-settings workspace: w1 tenant: mycompany organization: myorg spec: defaultEastWestGatewaySettings: - workloadSelector: namespace: platinum labels: app: eastwest-gw exposedServices: - serviceLabels: tier: platinum critical: \u0026#34;true\u0026#34; - workloadSelector: namespace: internal labels: app: eastwest-gw exposedServices: - serviceLabels: app: eshop - serviceLabels: internal-critical: \u0026#34;true\u0026#34; This example configures the workspace settings for different workspaces with a list of gateway hosts that they can reach. The first setting configures the hostname `echo-1.tetrate.io` which is reachable from workspace w1. The second setting configures the hostnames `echo-1.tetrate.io` and `echo-2.tetrate.io` which are reachable from workspace w2. The thrid setting configures nothing. The fourth setting configures an empty hostname list. ```yaml apiVersion: api.tsb.tetrate.io/v2 kind: WorkspaceSetting metadata: name: w1-settings workspace: w1 tenant: mycompany organization: myorg spec: hostsReachability: hostnames: - echo-1.tetrate.io apiVersion: api.tsb.tetrate.io/v2 kind: WorkspaceSetting metadata: name: w2-settings workspace: w2 tenant: mycompany organization: myorg spec: hostsReachability: hostnames: - echo-1.tetrate.io - echo-2.tetrate.io apiVersion: api.tsb.tetrate.io/v2 kind: WorkspaceSetting metadata: name: w3-settings workspace: w3 tenant: mycompany organization: myorg spec: apiVersion: api.tsb.tetrate.io/v2 kind: WorkspaceSetting metadata: name: w4-settings workspace: w4 tenant: mycompany organization: myorg spec: hostsReachability: hostnames: [] From the above settings, here’s a summary of the host reachability: echo-1.tetrate.io host is reachable from namespaces configured in w1, w2 and w3. echo-2.tetrate.io host is reachable from namespaces configured in w2 and w3. All hosts are reachable from namespaces configured in workspace w3. Workspace w4 has no access to any hosts.\nWorkspaceSetting Default security and traffic settings for all proxy workloads in the workspace.\nField Description Validation Rule defaultSecuritySetting\ntetrateio.api.tsb.security.v2.SecuritySetting Security settings for all proxy workloads in this workspace. This can be overridden at security group’s SecuritySetting for specific cases. The override strategy used will be driven by the SecuritySetting propagation strategy. The default propagation strategy is REPLACE, in which a lower level SecuritySetting in the configuration hierarchy replaces a higher level SecuritySetting defined in the configuration hierarchy. Proxy workloads without a specific security group will inherit these settings. If omitted, the following semantics apply:\nSidecars will accept connections from clients using Istio Mutual TLS as well as legacy clients using plaintext (i.e. any traffic not using Istio Mutual TLS authentication), i.e. authentication mode defaults to OPTIONAL.\nNo authorization will be performed, i.e., authorization mode defaults to DISABLED.\n–\ndefaultTrafficSetting\ntetrateio.api.tsb.traffic.v2.TrafficSetting Traffic settings for all proxy workloads in this workspace. Proxy workloads without a specific traffic group will inherit these settings. If omitted, the following semantics apply:\nSidecars will be able to reach any service in the cluster, i.e. reachability mode defaults to CLUSTER.\nTraffic to unknown destinations will be directly routed from the sidecar to the destination.\n–\nregionalFailover\nList of …","relpermalink":"/book/tsb/refs/tsb/v2/workspace-setting/","summary":"Configuration for specifying default traffic and security settings in a workspace.","title":"Workspace Setting"},{"content":"我想向大家介绍 Istio Steering Committee 2023 年度选举 ，并呼吁大家参与投票。Istio Steering Committee 是 Istio 项目的决策机构，由 9 个按比例分配的贡献席位和 4 个选举产生的社区席位组成。社区席位的候选人必须是 Istio 项目的活跃用户，而贡献席位的候选人必须是 Istio 项目的贡献者。\n谁可以参与投票 过去 12 个月内有为 Istio 提交并合并过 PR，或者是为 Istio 社区做出过卓越贡献的人将被加入到投票列表。只有在投票列表中的人才有资格投票，您可以在 GitHub 上查看您是否在投票列表（共 201 人）中。\n时间安排 选举时间表如下：\n2023 年 7 月 18 日：宣布选举并公布投票人列表。 2023 年 7 月 30 日：候选人提名截止。 2023 年 7 月 31 日：选举投票开始。 2023 年 8 月 13 日：选举投票截止。 2023 年 8 月 14 日当周：选举结果公布。 如何参与投票 如果您想要投票，请遵循选举流程并在规定的时间内参与。如果您认识 Istio 社区的成员，可以向他们宣传您支持的候选人。特别是 Tetrate 的员工 Zack，他是一位优秀的候选人，为 Istio 社区做出了很多贡献。最近一次 Istio Steering Committee 的选举是在 2022 年，选举结果已经公布。Istio 社区希望在 2023 年的选举中，更多的人参与投票，为 Istio 项目的发展贡献自己的力量。\n投票地址：https://elections.istio.io 请使用你在投票列表中登记的 GitHub 账户登录后投票。一共 8 位候选人，按 1 到 8 排列名次。请将你更倾向于的候选人排在前列。\n请为 Zack Butcher 投一票 他在 2016 年成为 Istio 项目的创始工程师之一，开始了他的工作。自那时以来的七年里，他一直从事 Mixer 上工作，后来转而专注于 Pilot 和 Istio 的网络 API。他很自豪能够在 2020 年至 2022 年期间担任两届 Istio 指导委员会的社区选举成员。在任期期间，他还担任了行为准则小组的创始成员之一。他还担任了几个 IstioCon 的评论员，并共同主持即将举行的 IstioCon 2023。\n在加入 Istio 之前，他曾在 GCP 工作，帮助交付企业功能，如资源层次结构（项目，文件夹，组织），IAM 和服务管理。他于 2018 年离开 Google，帮助创立了 Tetrate - 一家以 Istio 为核心的公司，并担任了多个职位，从工程，市场营销到开发关系和销售。整个时间，他一直专注于促进 Istio 在行业中的采用。为此，他合著了《Istio: Up and Running》（O’Rielly，2019），并在世界各地的会议，播客和印刷品上定期发表有关 Istio 的演讲。\n他积极与国家标准和技术研究所（NIST）合作，编写特别出版物，为微服务安全和零信任提供指导，所有这些都专注于服务网格，Istio 作为安全架构的关键部分：NIST SP 800-204A，NIST SP 800-204B 和 NIST SP 800-207A。这些指南正在被政府和银行等主要行业采用，作为他们未来策略的基石，有助于确保 Istio 在未来几年内继续增长和繁荣。\n他坚信 Istio，以至于他把他职业生涯的大部分都押在了它身上。他热衷于看到项目成功，并且在社区内外都是一个平衡和合理的声音。他倡导最适合项目的东西，而不是任何特定的公司或供应商。随着 Istio 及其社区的不断发展壮大，他相信他很适合为项目的健康和增长在内部和外部倡导。\n","relpermalink":"/notice/istio-steering-commmittee-election-voting-2023/","summary":"Istio 社区指导委员会选举已经开始，投票截止到 8 月 13 日。","title":"Istio 2023 届指导委员会选举投票正在进行中"},{"content":"第一届「云原生社区峰会 2023」计划 9 月 24 日（星期日，KubeCon China 的前两天）在上海举办，目前正在招募赞助商，感兴趣的请与我联系吧！\n","relpermalink":"/notice/cloud-native-community-summit-2023/","summary":"9 月 24 日，上海，欢迎赞助","title":"云原生社区峰会赞助商正在招募中"},{"content":"在本文中，我将解释如何使用 GraphQL 与 Postman 一起从 SkyWalking 查询数据。它包括获取不记名令牌、构建查询以检索特定服务的负载指标以及使用 GraphQL 自省来查看 SkyWalking GraphQL API 架构的步骤。本文还提供了更多信息的参考。\n什么是 GraphQL？ GraphQL 是 Facebook 开发的一种 API 查询语言和运行时。它允许客户端准确指定他们需要的数据并仅接收该数据作为响应，从而为传统 REST API 提供了更高效、更强大、更灵活的替代方案。使用 GraphQL，客户端可以在单个请求中查询多个资源，从而减少与服务器的往返次数并提高性能。\nGraphQL 和 REST API 之间有什么区别？ GraphQL 允许客户端仅请求他们需要的数据，而 REST API 要求客户端检索资源中的所有内容，无论他们是否需要。此外，GraphQL 允许客户端在单个请求中查询多个资源，这使其比 REST API 更高效、更简洁。\n如何查询 SkyWalking 数据？ SkyWalking 定义了查询阶段的通信协议。SkyWalking 原生 UI 和 CLI 使用此协议从后端持续获取数据，无需担心后端更新。\n从 SkyWalking 查询指标有两种方法：\nGraphQL API PromQL API 本文提供了有关如何使用 GraphQL 从 SkyWalking 查询指标的指南。如果你对 PromQL API 感兴趣，可以参阅文章为 Apache SkyWalking 构建 Grafana Dashboard - 原生 PromQL 支持 。继续执行以下步骤需要安装 TSB。如果你没有，但仍想体验使用 GraphQL 在 SkyWalking 中查询数据，你可以使用 SkyWalking 提供的免费演示环境 （用户名 / 密码：skywalking/skywalking）。登录演示网站并获取查询令牌。GraphQL 查询的端点地址是 http://demo.skywalking.apache.org/graphql 。构造查询的步骤与下面描述的相同。\n观察 TSB 中的 GraphQL 查询 在我们使用 Postman 构建自己的 GraphQL 查询之前，我们首先观察 TSB 如何从 SkyWalking 获取数据。\n打开 Chrome DevTools 并切换到“Network”选项卡。 访问网站上的Organization - Services 选项卡。 观察网络请求列表并右键单击其中一个 graphql 请求，如下图所示：\n图 1：Chrome DevTool 你看到的 curl 命令将如下所示。在终端中执行该命令，你将从 SkyWalking 中获取 TSB 管理的服务列表。\ncurl \u0026#39;\u0026lt;https://saturn.tetrate.work/ui/graphql\u0026gt;\u0026#39; \\\\ -H \u0026#39;Accept-Language: en,zh-CN;q=0.9,zh;q=0.8,en-US;q=0.7,zh-TW;q=0.6\u0026#39; \\\\ -H \u0026#39;Cache-Control: no-cache\u0026#39; \\\\ -H \u0026#39;Client-Timestamp: 1686104776136\u0026#39; \\\\ -H \u0026#39;Connection: keep-alive\u0026#39; \\\\ -H \u0026#39;Content-Type: application/json\u0026#39; \\\\ -H \u0026#39;Cookie: ...\u0026#39; \\\\ -H \u0026#39;Origin: \u0026lt;https://saturn.tetrate.work\u0026gt;\u0026#39; \\\\ -H \u0026#39;Pragma: no-cache\u0026#39; \\\\ -H \u0026#39;Referer: \u0026lt;https://saturn.tetrate.work/mp/services\u0026gt;\u0026#39; \\\\ -H \u0026#39;Request-Id: ...\u0026#39; \\\\ -H \u0026#39;Sec-Fetch-Dest: empty\u0026#39; \\\\ -H \u0026#39;Sec-Fetch-Mode: cors\u0026#39; \\\\ -H \u0026#39;Sec-Fetch-Site: same-origin\u0026#39; \\\\ -H \u0026#39;User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\u0026#39; \\\\ -H \u0026#39;X-Bridge-Csrf-Token: IOmJszLAqY3TRIUNhTuGu7vQgnfQY1FtgYFm+l/+Mu4EmVQU5T8EaQ7bngkCv4hQ12ZGids+I21pHMdepE9/qQ==\u0026#39; \\\\ -H \u0026#39;X-Csrf-Token: xTbxZerD3t8N3PaS7nbjKCfxk1Q9dtvvrx4D+IJohHicb0VfB4iAZaP0zh1eXDWctQyCYZWaKLhAYT3M6Drk3A==\u0026#39; \\\\ -H \u0026#39;accept: application/json\u0026#39; \\\\ -H \u0026#39;sec-ch-ua: \u0026#34;Not.A/Brand\u0026#34;;v=\u0026#34;8\u0026#34;, \u0026#34;Chromium\u0026#34;;v=\u0026#34;114\u0026#34;, \u0026#34;Google Chrome\u0026#34;;v=\u0026#34;114\u0026#34;\u0026#39; \\\\ -H \u0026#39;sec-ch-ua-mobile: ?0\u0026#39; \\\\ -H \u0026#39;sec-ch-ua-platform: \u0026#34;macOS\u0026#34;\u0026#39; \\\\ --data-raw $\u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;query ServiceRegistryListMetrics(...)}\u0026#39; \\\\ --compressed 注意： 上例中的某些字段太长，用点 (…) 替换。\n接下来，我将指导你构建一个查询来检索特定服务的负载指标。\n获取 Bearer Token 首先，你需要获取网站的 Bearer Token。登录到 TSB UI，点击右上角的用户按钮，然后点击“Show token information”。在弹出窗口中，你将看到 Bearer Token，如下图所示。\n图 2：从 TSB UI 获取 Bear Token 注意：Bearer token 的有效期比较短。当它过期时，你需要重新登录 TSB 以获取新的 token。\n我们已经预先部署了bookinfo 应用程序 并发送了一些测试流量。要在 Postman 客户端中使用 GraphQL 查询reviews的负载指标，请执行以下步骤：\n创建一个新的 GraphQL 请求，并输入请求 URL：$TSB_ADDRESS/graphql 添加带有值Bearer $TOKEN的Authorization头 使用 GraphQL Introspection 查看 SkyWalking GraphQL APIs 的模式。查找并单击readMetricsValues项。你将在右侧看到变量。填写condition和duration项目，如下图所示。 图 3：Postman 查询 变量如下：\nquery ReadMetricsValues { readMetricsValues(condition: { name: \u0026#34;service_cpm\u0026#34;, entity: {scope: Service, serviceName: \u0026#34;reviews\u0026#34;, normal: true} }, duration: { start: \u0026#34;2023-06-05 0625\u0026#34;, end: \u0026#34;2023-06-05 0627\u0026#34;, step: MINUTE }) { label values { values { id value } } } } 单击 Query 按钮以获取结果。它应该类似于以下内容：\n{ \u0026#34;data\u0026#34;: { \u0026#34;readMetricsValues\u0026#34;: { \u0026#34;label\u0026#34;: null, \u0026#34;values\u0026#34;: { \u0026#34;values\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;service_cpm_202306050625_cmV2aWV3cw==.1\u0026#34;, \u0026#34;value\u0026#34;: 0 }, { \u0026#34;id\u0026#34;: \u0026#34;service_cpm_202306050626_cmV2aWV3cw==.1\u0026#34;, \u0026#34;value\u0026#34;: 0 }, { \u0026#34;id\u0026#34;: \u0026#34;service_cpm_202306050627_cmV2aWV3cw==.1\u0026#34;, \u0026#34;value\u0026#34;: 0 } ] } } } } 以上是使用 SkyWalking Demo 环境测试 GraphQL 查询。TSE 也支持 GraphQL 查询，并且端点地址为$TSB_SERVER/graphql。\n注意：此处的查询端点与 DevTool 中看到的不同。TSB UI 特定的 GraphQL 查询端点是$TSB_SERVER/ui/graphql。\n有关 SkyWalking GraphQL 查询协议的详细信息，请参见GitHub 。\n总结 在本文中，我介绍了如何在 Postman 中使用 GraphQL 查询协议查询 SkyWalking 中的数据。你可以根据 SkyWalking 的 GraphQL 模式构建自己的查询条件。TSB / TSE 中也提供了此功能。\n","relpermalink":"/blog/how-to-use-graphql-to-query-observability-data-from-skywalking-with-postman/","summary":"本文介绍如何使用 GraphQL 从 SkyWalking 查询可观察性数据。它首先介绍了 GraphQL 和 SkyWalking，然后介绍了如何设置 Postman 发送 GraphQL 查询，最后提供了一些示例 GraphQL 查询，可用于从 SkyWalking 查询可观察性数据。","title":"如何使用 GraphQL 和 Postman 查询 SkyWalking 中的数据"},{"content":"我跟我的两位好朋友罗广明、马若飞一起翻译的《Istio in Action》在本月出版，由电子工业出版社博文视点出版，原作者 Christian Posta 和 Rinor Maloku，中文名称为《Istio 最佳实战》，感兴趣的读者可以到京东 上了解详情和购买。\n《Istio 最佳实战》封面 ","relpermalink":"/notice/istio-in-action/","summary":"《Istio in Action》中文版出版","title":"最新译作《Istio 最佳实战》出版"},{"content":"Argo Rollouts 是一个 Kubernetes 控制器，它提供了在应用程序部署过程中执行渐进式发布和蓝绿部署等高级部署策略的能力。它是基于 Kubernetes 原生的 Deployment 资源构建的，通过引入新的 Rollout 资源来扩展和增强部署控制。\n跳转到阅读 Argo Rollouts 中文文档 🔔 注意：本文档根据 Argo Rollouts v1.5 Commit 1d53b25 （北京时间 2023 年 6 月 21 日 3 时）翻译。\n","relpermalink":"/notice/argo-rollouts-docs/","summary":"根据 2023 年 6 月 21 的 v1.5 版本翻译。","title":"Argo Rollouts 中文文档（非官方）"},{"content":"本文根据 KubeCon EU 2023 上的 Istio Day 上 Lin Sun 和 Louis Ryan 的分享整理。\n2022 年要点 Istio 是 CNCF 的一部分 引入 Istio ambient 功能提升：External Authorization、Gateway API、WorkloadGroup 增强安全性：\n正式审核 模糊测试 额外功能：\nARM 支持 双栈实验 2023 主题 加速服务网格的价值实现（0 → mTLS） TCO（Total Cost of Ownership）革命：需要考虑软件的全生命周期成本，包括采购、安装、配置、使用、升级、维护等各个方面。通过对软件全生命周期成本的全面考虑，企业可以更好地控制软件成本，并实现更好的 ROI。 开放的社区成长 持续保持平凡和可预测 2023 重点领域 Ambient mesh 到生产 Gateway API \u0026amp; GAMMA 继续稳定和功能提升 与其他开源云原生项目和标准的集成 Ambient Mesh 到生产 多集群支持 虚拟机支持 使用 zTunnel 进行性能改进 多个 Kubernetes 平台支持 Ambient 和 Network CNI 兼容性 Ambient mesh 发布计划：\nIstio 1.18 - Alpha Istio 1.19 - Beta Istio 1.20 - Production Gateway API Kubernetes 标准的流量管理 Istio 社区是定义和实现的领导者 Istio 的实现达到 Beta，并通过 100% 的一致性测试 稳定性和功能提升 Istio 安全（未来）模式 Telemetry API IPV6 和 Dual Stack 基于 eBPF 的流量重定向 WasmPlugin 带有 Gateway API 的 gRPC 控制平面 集成 与企业生态系统合作：\nOpen Telemetry SPIRE Kiali Prometheus Jaeger WASM ","relpermalink":"/notice/istio-roadmap-2023/","summary":"根据 KubeCon EU Istio Day 上的分享整理。","title":"Istio 2023 年路线图"},{"content":"Envoy Gateway 是一款基于 Envoy 代理和 Kubernetes Gateway API 开发的开源 API 网关，最近发布了 0.4.0 版本 。此次发布的版本着重于自定义功能，旨在为最终用户提供更多的用例。在本文中，我们将讨论此版本中可用的新自定义选项及其对用户的重要性。\n自定义 Envoy 代理架构 此次版本中最主要的自定义功能之一是配置 EnvoyProxy （Envoy Gateway 定义的 CRD）部署的确切类型。你可以定义 EnvoyProxy 部署的副本数、镜像和资源限制。还可以向 EnvoyProxy 部署和服务添加注解（Annotation）。这使得不同的用例成为可能，例如：\n将 Envoy Gateway 与 AWS、NLB、ELB 和 GCP 等外部负载均衡器链接起来。 在 EnvoyProxy 旁边注入 Sidecar，这对于 Ingress 层管理南北向流量的 Envoy Gateway 和服务网格层用于管理东西向流量互联 TLS（mTLS）的 Envoy Sidecar 非常有用。此自定义功能消除了用户创建自己证书的需要，因为它基于历史的证书管理。 关于 Envoy Gateway 的更多自定义功能请参考 Envoy Gateway 文档 。\n此外，Envoy Gateway 除了默认的 Kubernetes 单租户模式以外还新增其他部署模式支持，例如多租户，如下图所示。\nEnvoy Gateway 的多租户模式示意图 分别在每个租户的 namespace 部署一个 Envoy Gateway Controller，它们监视 Kubernetes 中的 HTTPRoute 和 Service 资源，并在各自的 namespace 中创建和管理 EnvoyProxy 部署。\n自定义 Envoy xDS 引导程序 此版本中的另一个重要自定义功能是自定义 Envoy xDS 引导程序 。使用此功能，用户可以提供引导配置，在启动 EnvoyProxy 时配置一些静态资源。例如配置访问日志记录、跟踪和指标以发送到 SkyWalking（可以作为 APM）非常有用。此外，此版本添加了大量 CLI 工具，以帮助验证用户配置。用户可以将 CLI 用作干运行以更改引导程序中的特定字段，如果配置在语法上不正确，则将失败。\n扩展控制平面 Envoy Gateway 现在允许供应商和扩展开发人员在 Envoy Gateway 管道的不同阶段添加 gRPC 钩子，以进一步扩展其功能，允许用户做一些事情，比如增强发送给 EnvoyProxy 的 xDS 配置，这在以前是不可能的。\n总结 最后，Envoy Gateway 0.4.0 扩展了自定义 API，并为最终用户提供了更多用例。新的自定义功能包括自定义 Envoy 部署、Envoy xDS 引导程序以及扩展控制平面。这些新功能消除了用户创建自己的证书的需要，配置访问日志记录、跟踪和指标，并使供应商能够扩展 XDS 翻译用例。通过此版本的发布，Envoy Gateway 正变得更加用户友好，成为 Istio 的绝佳替代品。\n","relpermalink":"/blog/envoy-gateway-customization/","summary":"Envoy Gateway 是一款基于 Envoy 代理和 Kubernetes Gateway API 开发的开源 API 网关，最近发布了 0.4.0 版本。此次发布的版本着重于自定义功能，旨在为最终用户提供更多的用例。在本文中，我们将讨论此版本中可用的新自定义选项及其对用户的重要性。","title":"Envoy Gateway 0.4.0 发布：自定义 API 扩展"},{"content":"ChatGPT 是一个强大的工具，可以根据提示生成文本响应。然而，使用浏览器版本可能会很繁琐和耗时。幸运的是，有几个浏览器扩展可以帮助您更有效地使用 ChatGPT。在本文中，我们将讨论几个我个人测试并推荐的最有用的 ChatGPT 浏览器扩展。这些插件都是完全免费的，不存在订阅或者后期收费选项，大家可以放心使用。\n下图展示了安装了插件的 Chrome 浏览器中的 ChatGPT 页面。\n安装了 Chrome 插件的 ChatGPT 页面 ChatGPT Prompt Genius GitHub:https://github.com/benf2004/ChatGPT-Prompt-Genius 这个项目是一个多功能的 ChatGPT 浏览器扩展，它可以帮助你发现、分享、导入和使用最好的 ChatGPT 提示。你也可以把你的聊天记录保存在本地，方便以后查看和参考。你可以使用扩展的提示模板功能，轻松地找到并添加提示到你的收藏。你可以在 ChatGPT 页面上搜索、分类和选择提示，找到有创意和有用的方式使用 ChatGPT。还可以添加一些主题，比如短信、温馨的壁炉和黑客。\nChatGPT for Google 在搜索引擎结果中同时显示 ChatGPT 的回答，功能点如下：\n支持所有主流的搜索引擎 在获得搜索结果后可直接开始聊天 支持 OpenAI 官方 API 从插件弹窗里快速使用 ChatGPT 支持 Markdown 渲染 支持代码高亮 支持深色模式 可自定义 ChatGPT 触发模式 你可以在 Chrome 网上应用商店 下载。\nKeepChatGPT GitHub:https://github.com/xcanwin/KeepChatGPT 让我们在使用 ChatGPT 过程中更高效、更顺畅，完美解决 ChatGPT 网络错误，不再频繁地刷新网页，足足省去 10 个多余的步骤。还可以取消后台监管审计。\n解决了这几类报错：\nNetworkError when attempting to fetch resource. Something went wrong. If this issue persists please contact us through our help center at help.openai.com . This content may violate our content policy. Conversation not found. TalkBerry 这个 Chrome 扩展的主要内容是让你可以用语音和 ChatGPT 交流，而不需要打字。它的功能有：\n选择你想要聊天的语言，支持多种语言。 点击麦克风按钮，开始对话，ChatGPT 会用语音回复你。 用语音命令控制 ChatGPT，比如说“停止”或“继续”。 在设置菜单中调整语音识别和文本转语音的选项。 这个扩展可以让你更方便地使用 ChatGPT，也可以帮助你学习外语或提高口语能力。它是一个免费和开源的项目，你可以在 Chrome 网上应用商店 下载。\n需要注意的是，你无法关闭用语音朗读，除非你卸载掉该扩展。\nWebChatGPT 这个免费的扩展程序将相关的网络结果添加到您对 ChatGPT 的提示中，以获得更准确和最新的对话。\n为您的查询获取网络结果 从任何 URL 中提取网页文本 添加和使用提示模板 使用 Duckduckgo bangs 从数千个网站中获取搜索结果 当你不想使用时，也可以在 ChatGPT 页面上随时关闭它。你可以在 Chrome 网上应用商店 下载该扩展。\n总结 这几个 ChatGPT 浏览器扩展可以帮助您更有效地使用 ChatGPT。无论您需要更便捷的界面、带有有用功能的侧边栏，都有一款扩展可以帮助。我建议尝试每个扩展，以确定哪个最适合您的需求。有了这些工具，您可以改善您的 ChatGPT 体验，快速轻松地生成高质量的文本响应。以后再发现其他更好的插件，笔者会在本文中更新，也欢迎读者朋友们有推荐的插件可以在评论中留言。\n","relpermalink":"/blog/chatgpt-chrome-extensions/","summary":"本文整理了一些可以帮助你高效使用 ChatGPT 的浏览器插件，经过本人实测有效，推荐大家。","title":"免费的 ChatGPT 浏览器插件工具推荐 | 亲测有效"},{"content":"Docker 多平台构建是一种用于构建 Docker 镜像以在多种 CPU 架构和操作系统上运行的技术。它可以让用户在一个 Dockerfile 中定义一个通用的构建过程，然后使用 Docker CLI 命令将其构建为多个不同平台的镜像。这些镜像可以在不同的计算机、云平台和容器编排系统上运行，从而为用户提供更广泛的部署选项。\n在多平台构建中，用户需要使用 Docker Buildx 插件来构建镜像。Docker Buildx 可以构建并输出多个不同平台的镜像，包括 x86、ARM、IBM Power 等。用户可以使用该插件创建多种平台的构建环境，并使用这些环境构建镜像。\n需要注意的是，多平台构建需要在支持多平台的 Docker 主机上进行。在这种主机上，Docker 可以使用 QEMU 等模拟器来模拟其他平台的环境，从而实现构建多种平台的镜像。\n什么是 docker buildx? Docker Buildx 是 Docker 的一个插件，它提供了一种简单、高效的方式来构建和打包 Docker 镜像。它能够在多个平台上构建和输出 Docker 镜像，包括 Linux、Windows、macOS 等，支持 CPU 架构和操作系统等多种参数的设置。\nDocker Buildx 在构建镜像时使用了 BuildKit ，这是 Docker 官方推出的一个基于 Go 语言实现的高性能构建引擎。BuildKit 提供了更快的构建速度、更小的镜像体积、更好的缓存管理等优势，也可以在 Docker Buildx 之外使用。\n使用 Docker Buildx，可以将不同平台上的 Docker 镜像构建合并到一个 manifest 中，使得用户只需要下载一个 manifest，就可以获取多个平台的镜像。这为跨平台开发和分发应用程序提供了很大的便利。\nDocker buildx 实现多平台构建的原理 Docker buildx 实现多平台镜像构建的原理是基于 Docker 的多架构支持。Docker 可以在一个主机上运行多个容器，每个容器运行在自己的隔离环境中，相互独立。而 Docker 镜像则是用于创建容器的基础文件系统。\n在 Docker 中，不同的 CPU 架构和操作系统可以使用不同的 base image（基础镜像）进行构建。而 Docker buildx 可以自动识别当前主机的架构和操作系统，并选择合适的 base image 进行构建。在构建过程中，Docker buildx 会使用 BuildKit 引擎进行构建，支持多平台的交叉编译和镜像打包。\n在构建完成后，Docker buildx 会将不同平台上的镜像打包成一个 manifest 文件，其中包含了所有平台的镜像信息。用户可以通过 Docker CLI 命令或者 Docker registry 接口来操作 manifest 文件，获取不同平台上的镜像。对于不支持多架构的 Docker 版本，可以通过安装 Docker CLI 的 experimental 版本来使用 Docker buildx。\nDocker buildx 利用了 Docker 的多架构支持和 BuildKit 引擎，实现了跨平台的 Docker 镜像构建和分发。\nDocker BuildKit 引擎简介 BuildKit 是 Docker 官方推出的一个高性能的构建引擎，它可以用于构建 Docker 镜像、构建应用程序以及执行其他构建任务。BuildKit 引擎采用了分布式的架构，可以并行地执行多个构建任务，提高构建效率。\nBuildKit 引擎的主要特点包括：\n高性能：BuildKit 引擎采用了高效的缓存管理机制，能够快速地执行增量构建，减少构建时间。同时，它还能够自动优化构建过程，选择最佳的构建路径和策略，进一步提高构建性能。 多平台支持：BuildKit 引擎支持多种 CPU 架构和操作系统，能够在不同平台上构建和输出 Docker 镜像。在 Docker buildx 中，BuildKit 引擎可以自动识别当前主机的架构和操作系统，并选择合适的构建方案。 模块化设计：BuildKit 引擎采用了模块化的设计，可以根据需要动态加载和卸载不同的模块。这使得 BuildKit 引擎更加灵活和可扩展，可以支持各种不同的构建任务。 安全性：BuildKit 引擎采用了安全的构建方式，可以自动执行一系列的安全检查，确保构建过程中不会引入漏洞或其他安全问题。同时，BuildKit 引擎还支持签名和加密等安全功能，保护用户的构建数据和镜像。 Docker buildx 支持哪些平台？ Docker buildx 支持的平台主要包括以下几种：\nLinux：包括多种 CPU 架构和操作系统，如 x86_64、ARM、IBM Power、IBM Z 等。 Windows：包括多种 CPU 架构和操作系统，如 x86_64、ARM64 等。 macOS：支持 Intel、Apple M1 架构。 除了以上平台外，Docker buildx 还支持构建和输出多种其他平台的 Docker 镜像，包括 FreeBSD、Solaris 等。用户可以通过指定对应的 platform 参数来构建和输出不同平台的 Docker 镜像，例如：\ndocker buildx build --platform linux/amd64,linux/arm64 . 这个命令将会构建一个同时支持 x86_64 和 ARM64 架构的 Docker 镜像。用户也可以通过指定不同的 buildx 构建配置来支持更多的平台，例如使用 qemu-user-static 等模拟器来支持其他的 CPU 架构。总之，Docker buildx 的多平台支持非常强大，为跨平台开发和分发应用程序提供了便利。\nDocker buildx 引擎的架构与组成 Docker buildx 引擎的架构是一个分布式的构建系统，通过多阶段、多组件的设计，实现了高性能、多平台支持、安全性等优点，为 Docker 镜像构建和应用程序构建提供了强大的支持。它由以下几个主要组成部分组成：\nCLI：提供了命令行接口，用户可以通过命令行来执行构建任务、管理构建配置等操作。 BuildKit 引擎：作为 Docker buildx 的构建引擎，它负责执行构建任务，生成 Docker 镜像等。BuildKit 引擎具有高性能、多平台支持、安全性等优点。 构建器（Builder）：构建器是一个 Docker 容器，它包含了构建所需要的环境和工具，可以执行构建任务。在 Docker buildx 中，可以配置多个构建器，以支持多个平台和多个构建环境。 并行器（Scheduler）：并行器是负责协调和管理多个构建器的组件，它可以自动选择最佳的构建器执行构建任务，并将任务分配给合适的构建器。并行器还可以执行构建任务的并行处理，提高构建效率。 缓存管理器（Cache Manager）：缓存管理器是负责管理构建过程中的缓存数据，可以快速执行增量构建，减少构建时间。在 Docker buildx 中，缓存管理器可以自动选择合适的缓存方案，包括本地缓存和远程缓存等。 输出器（Exporter）：输出器负责将构建生成的 Docker 镜像输出到指定的仓库或者本地文件系统中。在 Docker buildx 中，输出器可以自动识别当前平台和目标平台，选择合适的镜像格式和输出路径。 docker buildx 命令的使用 使用 docker buildx 命令可以方便地进行 Docker 镜像的构建和输出。下面是一些常用的 docker buildx 命令及其用法：\n查看当前的 buildx 构建器列表\ndocker buildx ls 创建新的 buildx 构建器\ndocker buildx create --name mybuilder 切换到指定名称的 buildx 构建器\ndocker buildx use mybuilder 设置 buildx 构建器的平台支持\ndocker buildx inspect --bootstrap docker buildx inspect --platform docker buildx build --platform linux/amd64,linux/arm64 . 构建 Docker 镜像：\ndocker buildx build --tag myimage . 输出 Docker 镜像到本地文件系统\ndocker buildx build --output=type=local,dest=./output . 输出 Docker 镜像到 Docker Hub 或其他远程仓库\ndocker buildx build --tag myrepo/myimage --push . 删除指定名称的 buildx 构建器\ndocker buildx rm mybuilder 除了以上命令外，docker buildx 还支持许多其他的参数和选项，例如设置构建缓存、并行处理、构建标签等。用户可以通过查看官方文档或者使用 –help 选项来了解更多详情。\n理解 buildx 构建器 在 Docker 中，构建器（Builder）是指一个 Docker 容器，它包含了构建所需要的环境和工具，可以执行构建任务。Docker buildx 构建器是指使用 BuildKit 引擎的多平台构建器，可以通过 Docker CLI 命令进行管理和操作。在使用 Docker buildx 构建器时，用户可以配置多个构建器，以支持多个平台和多个构建环境。\n用户可以通过创建、切换、查看和删除构建器，来管理和维护 Docker buildx 的构建环境。构建器的主要作用是提供一个干净、独立的构建环境，避免构建过程中的依赖冲突和环境污染。此外，构建器还可以方便地进行版本管理和共享，以便多个用户或者团队协同构建 Docker 镜像。\nDocker buildx 构建器还支持多平台构建，用户可以在同一个构建器中设置多个平台，以便生成跨平台的 Docker 镜像。通过 Docker buildx 构建器，用户可以轻松实现 Docker 镜像的多平台构建，提高构建效率和应用程序的兼容性。\n为什么本地看不到 Docker buildx 构建的镜像？ 这通常是因为你当前使用的 Docker context 不支持编译出来的镜像架构。例如 Orbstack ，虽然它支持编译跨平台的镜像，但是执行 docker buildx 构建出来的镜像不会直接保存在本地的 Docker 镜像仓库中，而是保存在构建器（Builder）的缓存中。这是因为 Docker buildx 采用了分层构建的方式，构建出的每一层镜像都可以被重用，以减少构建时间和磁盘空间的占用。\n你应该使用 docker context 命令切换会 Docker 默认的上下文环境再执行构建，这样构建出来的跨平台镜像就可以在本地看见了。\n如何将多平台镜像保存到本地？ 要将 Docker buildx 构建的多平台镜像保存到本地，可以使用 --output 选项指定输出类型为 type=local，并指定输出目录，例如：\ndocker buildx build --platform linux/amd64,linux/arm64 --output type=local,dest=./output . 上述命令将构建包含 linux/amd64 和 linux/arm64 两种平台的镜像，并将输出类型设置为本地（type=local），输出目录为 ./output。\n构建完成后，输出目录中会生成多个子目录，每个子目录分别对应一个平台，其中包含该平台下的镜像文件。\n如果只想保存其中一个平台的镜像，可以在 --output 选项中指定要保存的平台，例如：\ndocker buildx build --platform …","relpermalink":"/blog/docker-multi-platform-image-building/","summary":"本文介绍了 Docker buildx 的多平台构建指南，包括构建器、并行器、缓存管理器和输出器等组件，以及常用的 docker buildx 命令及其用法。同时还介绍了如何管理和维护 Docker buildx 的构建环境，以及构建 WebAssembly 镜像的一般步骤和注意事项。","title":"Docker 多平台构建指南：构建 WebAssembly 镜像"},{"content":"两天前刘训灼的公众号 上公布成立 Envoy Gateway 中国兴趣组，并拉了一个 Envoy Gateway 的微信群，截至到 2023 年 04 月 12 日已经有 243 人（超 200 人需要邀请入群），欢迎大家加入，可以联系我 、刘训灼 、赵化冰 等入群。\n为促进项目发展和社区沟通，Envoy Gateway 中国兴趣组会通过腾讯会议定期召开中国时间友好的社区会议，会议时间和纪要更新在 Google Doc 中。\nEnvoy Gateway 资源 GitHub 地址：https://github.com/envoyproxy/gateway 官方文档：https://gateway.envoyproxy.io 会议文档：Google Doc 什么是 Envoy Gateway？ Envoy Gateway 是 EnvoyProxy 创始人 Matt Klein 牵头发起，联合 Contour、Emissary 等基于 Envoy 的 API 网关开源社区，共同参与并维护的 API 网关项目。\nEnvoy Gateway 从 2022 年 5 月发起，发展速度很快，至今已经历 v0.1.0、v0.2.0、v0.3.0，以及即将发布的 v0.4.0 的版本的迭代，基于 Kubernetes Gateway API 为 配置语言的 API Gateway，已实现了所有最新 Gateway API 的能力，以及通过 extensionRef 去扩展更高级的流量治理的能力，如认证、限流等功能。\nEnvoy Gateway 中国兴趣组 从项目发起到现在，中国贡献者在 Envoy Gateway 社区参与的比重很大，至今社区已经有以下同学，在 EG 社区中担任着重要的角色：\n一位 Steering Committee 成员：\nXunzhuo 两位 Maintainer：\nXunzhuo zirain: Steering Committee of Istio 两位 Reviewer：\nhuabingzhao: Founder of Aeraki Mesh qicz: PMC of Apache ShenYu 于是在上周五，赵化冰、刘训灼发起了 Envoy Gateway 中国兴趣组，EG SIG 是为对 Envoy Gateway 感兴趣的中国贡献者，提供交流、分享、探讨的一个平台，也会提供最新的 Envoy Gateway 的动态。\nEnvoy Gateway 资料 Envoy Gateway 0.3 发布 —— 扩展 Kubernetes Gateway API 使用 Envoy Gateway 0.2 体验新的 Kubernetes Gateway API 开源项目 Envoy Gateway 简介 Envoy API Gateway—— 推动网关的进一步发展 面向未来的网关：新的 Kubernetes Gateway API 和 Envoy Gateway 0.2 介绍 为什么 Envoy Gateway 是云原生时代的七层网关？ ","relpermalink":"/notice/envoy-gateway-group/","summary":"Envoy Gateway 中国兴趣组成立，欢迎加入。","title":"欢迎加入 Envoy Gateway 中国兴趣组"},{"content":"在 Docker 发展史：四个重大举措，影响深远！ 这篇文章中我提到了 Docker 从一开始引领容器运行时，再到在容器编排这一维度上落后于 Kubernetes。为了在保住容器运行时的统治地位，Docker 公司提出了 OCI 并通过 containerd-wasm-shim 支持更多的 WebAssembly 运行时。\n为了解决 Docker 在安全、稳定性、性能及可移植性方面的局限性，Kubernetes 社区开发了具有不同实现和功能的其他容器运行时，并为其制定了容器运行时接口（CRI）规范。目前实现该规范的容器运行时有 containerd、cri-o。还有 katacontainers、gvisor 等未实现 CRI 但是可以通过添加虚拟化层在 Kubernetes 上运行的其他容器运行时。\n开放容器倡议 (OCI) 旨在定义容器镜像格式和运行时的行业标准，Docker 捐赠了其运行时 runc 作为该标准的第一个实现。最近，WASM 社区对 OCI 工具链表现出了兴趣，Docker 现在支持 WebAssembly 模块作为其工件之一。现在 Docker Hub 已经支持除了镜像以外的，Helm、Volume 和 WebAssembly 等常用工件。\n使用 Docker 构建包含 WebAssembly 模块的镜像，并保存在 Docker Hub 中。通过 containerd-wasm-shim ，可以让它们在 Kubernetes 中运行，如下图所示。\n在 Kubernetes 中运行 WebAssembly 模块 Containerd 是一种符合 CRI（Container Runtime Interface）规范的容器运行时，是由 Docker 公司开源并贡献给 CNCF 的。只要支持 CRI 规范的运行时都可以在 Kubernetes 中运行。\n关于以上提到的名词 containerd、CRI、OCI 等的关系介绍，可以参考 Docker，containerd，CRI，CRI-O，OCI，runc 分不清？看这一篇就够了 在 Docker 中运行 WebAssembly 应用相对 Linux 镜像有什么优势？ 使用 Docker 运行 WebAssembly 应用相对运行 Linux 镜像有以下优势。\n更高的性能\nWebAssembly 应用的启动时间更短，因为它不需要启动整个操作系统，而 Linux 容器需要。WebAssembly 模块的冷启动时间比 Docker 容器快 100 倍。WebAssembly 模块的内存占用更小，因为它是一个二进制格式，可以高效地压缩代码和依赖，而 Docker 容器需要打包整个镜像。WebAssembly 模块的大小一般在 1MB 以内，而 Docker 镜像的大小可以达到 100 或 200 MB。\n更高的可移植性\nWebAssembly 应用是一个架构中立的格式，只要有相应的运行时，就可以在任何底层架构上运行，而不需要考虑不同架构之间的兼容性问题。Docker 容器需要针对不同的架构构建不同的镜像，可能会存在一些潜在的安全风险或漏洞。\n更好的安全性和隔离性\nWebAssembly 应用可以提供代码级别的安全性，防止恶意代码访问系统资源，具体来说：\nWebAssembly 应用是运行在一个沙箱环境中的二进制字节码，不需要访问主机系统的资源，也不会受到主机系统的影响。Docker 容器虽然也是运行在一个隔离的环境中，但是仍然需要在主机系统上运行，可能会受到主机系统的攻击或干扰。 WebAssembly 应用是通过 WebAssembly System Interface (WASI) 来与外部交互的，WASI 是一种标准化的 API 集合，可以提供一些基本的系统功能，比如文件操作、网络访问、环境变量等。WASI 可以限制 WebAssembly 应用的权限和能力，防止它们做一些危险的操作。Docker 容器虽然也可以通过设置一些安全选项来限制容器的权限和能力，但是仍然需要依赖主机系统提供的功能和服务。 WebAssembly 应用是一个架构中立的格式，只要有相应的运行时，就可以在任何底层架构上运行，而不需要考虑不同架构之间的兼容性问题。Docker 容器需要针对不同的架构构建不同的镜像，可能会存在一些潜在的安全风险或漏洞。 因为有以上优势，WebAssembly 在一些场景下比 Docker 容器更有优势，例如边缘计算、云原生应用和微服务。当然，WebAssembly 应用也有一些局限性，比如不支持多线程、垃圾回收和二进制打包等。因此，并不是所有的场景都适合使用 WebAssembly 应用。你可以根据你的具体需求和偏好来选择合适的技术方案。\n如何在 Docker 中运行 WebAssembly 应用？ 在 Docker 中运行 WebAssembly 应用的方式与普通的 Linux 镜像没有太大的不同，只是在运行时需要指定下平台和运行时。下面的例子来自 Docker 官方文档 ，以在 Docker Desktop 中为例运行 WebAssembly 应用：\ndocker run -dp 8080:8080 --name=wasm-example --runtime=io.containerd.wasmedge.v1 --platform=wasi/wasm32 michaelirwin244/wasm-example 其中：\n--runtime=io.containerd.wasmedge.v1 指定使用 WasmEdge 运行时，替代默认的 Linux 容器运行时。 --platform=wasi/wasm32 指定镜像的架构。通过利用 Wasm 架构，无需为不同的机器架构构建单独的镜像。Wasm 运行时负责将 Wasm 二进制文件转换为机器指令的最后一步。 目前 Docker 支持四种 WebAssembly 运行时，分别为：\n运行时名称 API 名称 开发者 基金会托管 spin io.containerd.spin.v1 Fermyon 无 SpiderLightning io.containerd.slight.v1 DeisLabs 无 WasmEdge io.containerd.wasmedge.v1 SecondState CNCF 沙箱项目 Wasmtime io.containerd.wasmtime.v1 Mozilla、Fastly、Intel、Red Hat 等公司 字节码联盟项目 在命令行终端中输入以下命令可以查看 WebAssembly 应用的运行情况：\ncurl http://localhost:8080/ 你将看到如下输出：\nHello world from Rust running with Wasm! Send POST data to /echo to have it echoed back to you 你还可以向/echo端点发送 POST 测试请求：\ncurl localhost:8080/echo -d \u0026#39;{\u0026#34;message\u0026#34;:\u0026#34;Hello\u0026#34;}\u0026#39; -H \u0026#34;Content-type: application/json\u0026#34; 你将看到如下输出：\n{\u0026#34;message\u0026#34;:\u0026#34;hello\u0026#34;} 总结 本文介绍了 Docker 为什么要增加对 WebAssembly 的支持，以及在 Docker 中运行 WebAssembly 应用的优势和方法。WebAssembly 应用相对于 Linux 镜像有更高的性能、可移植性和安全性，适用于边缘计算、云原生应用和微服务等场景。Docker 支持四种 WebAssembly 运行时，分别为 spin、spiderlightning、WasmEdge 和 wasmtime。在接下来的文章中我将介绍如何开发一个 WebAssembly 应用，敬请期待。\n参考 Announcing Docker Hub OCI Artifacts Support Docker，containerd，CRI，CRI-O，OCI，runc 分不清？看这一篇就够了 Build, Share, and Run WebAssembly Apps Using Docker Why Containers and WebAssembly Work Well Together Docker + WebAssembly: a quick intro ","relpermalink":"/blog/why-docker-support-wasm/","summary":"本文介绍了 Docker 为什么要增加对 WebAssembly 的支持，以及在 Docker 中运行 WebAssembly 应用的优势和方法。WebAssembly 应用相对于 Linux 镜像有更高的性能、可移植性和安全性，适用于边缘计算、云原生应用和微服务等场景。Docker 支持四种 WebAssembly 运行时，分别为 spin、spiderlightning、WasmEdge 和 wasmtime。","title":"为什么 Docker 要增加 WebAssembly 运行时？"},{"content":"在 2017 年的容器编排大战中，Docker 公司失败后沉寂了几年，但近年来又开始频繁行动，例如腾退开源组织账号，支持 WebAssembly 等。本文将回顾 Docker 公司发展过程中的四个重大举措，这些措施深深地影响了 Docker 公司的发展，也对 Docker 甚至 Kubernetes 社区产生了深远的影响。\n当我们在谈论 Docker 时我们在谈论什么？ 首先我们需要先确定 Docker 这个词的含义。当人们在谈论 Docker 时可能指的是：\nDocker 公司 Docker 软件栈 Docker 命令行工具 Docker 容器运行时 为什么同一个词会有这么多不同的意思呢？这都是有历史原因的。Docker 软件于 2013 年发布，起初定位为开发者工具。作为最早发布的容器工具，它迅速走红，并成为容器技术的代名词。但它最初只是在单机上运行，有太多耦合的接口设计。后来容器集群出现，才需要用到容器编排调度工具。因为 Kubernetes 具有丰富的功能和扩展性，Docker 公司推出的 Swarm 在这场容器编排大战中败下阵来。归根结底，Docker 面向开发者，而容器运行时则面向机器，只需要对应的接口即可，不需要那么丰富的管理工具。如今，Docker 仍然是最受开发者喜爱的容器工具之一，其 Docker Hub 是全球最大的镜像仓库。\n将 Docker 项目改名为 Moby 2017 年 4 月，Docker 公司将 Docker 项目重命名为 Moby，详见 Introducing Moby Project: a new open-source project to advance the software containerization movement ：\nMoby Project 是 Docker 公司为了应对容器技术在各个领域和用例中普及的趋势而发起的一个开源项目。 Moby Project 是 Docker 公司作为一个开放的研发实验室，与整个生态系统合作，实验，开发新的组件，并协作构建容器技术的未来。 Moby Project 不是 Docker 产品的替代品，而是 Docker 产品的基础。 Moby Project 包括三个层次：组件层，框架层和装配层。 组件层包括一些可复用的开源组件，如 runc, containerd, LinuxKit, InfraKit 等，可以用于构建各种类型的容器系统。 框架层提供了一些工具和库，用于将组件组装成系统，并管理其生命周期。 装配层是一个社区驱动的平台，用于分享和协作构建基于 Moby 框架的系统。 Moby Project 是一个新的开源项目，旨在推动软件容器化运动的发展，帮助生态系统让容器技术走向主流。它提供了一个组件库，一个用于将组件组装成定制的基于容器的系统的框架，以及一个让所有容器爱好者可以实验和交流想法的地方。\nMoby Project 和 Docker 的区别和联系是：\nMoby Project 是一个开源项目，Docker 是一个商业产品。 Moby Project 是 Docker 产品的基础，Docker 产品是 Moby Project 的一个实例。 Moby Project 是一个通用的框架，可以用于构建各种类型和用例的容器系统，Docker 是一个针对特定用例的容器系统，即构建，运行和共享应用程序。 Moby Project 是一个开放的研发实验室，用于实验和协作开发新的容器技术，Docker 是一个成熟的产品，用于提供稳定和可靠的容器服务。 支持 Kubernetes 调度 Docker 公司在 2017 年 12 月发布的 Docker 17.12 版本中开始支持 Kubernetes。在此之前，Docker 公司一直在发展自己的容器编排和调度工具 Docker Swarm。然而，Kubernetes 在容器编排和调度方面具有更广泛的支持和社区贡献，已经成为了业界标准。因此，Docker 公司决定将 Kubernetes 集成到 Docker 平台中，以提供更广泛的选择和更好的用户体验。Docker 公司在 Docker Desktop 和 Docker Enterprise 中提供了 Kubernetes 的集成支持，使得 Kubernetes 和 Docker 容器可以更加方便地部署和管理。同时，Docker 公司也开发了一些工具，如 Kompose 和 Docker Compose，使得用户可以将 Docker Compose 配置文件转换为 Kubernetes YAML 文件，以便更加方便地将应用程序从 Docker Swarm 迁移到 Kubernetes。\nKubernetes 不再支持 Docker 运行时 Kubernetes 从 v1.20 起不再支持 Docker 运行时并在 2022 年 4 月发布的 v1.24 中被完全移除，如下图所示。这意味着在 Kubernetes 中只能使用 containerd 或 CRI-O 容器运行时，不过你依然可以使用 Docker 镜像，只是无需使用 docker 命令或 Docker 守护程序。\nKubernetes v1.24 正式移除 Docker 运行时 Kubernetes v1.24 正式移除 Docker 运行时\n腾退开源组织账号 2023 年 3 月，据 Alex Ellis 的博客 介绍，Docker 公司决定删除一些开源组织的账户和镜像，除非他们升级到付费的团队计划，这对开源社区造成了很大的困扰和不安。很多 Docker 忠实拥护者和贡献者对 Docker 的这一举动表示了不满和失望。\n这一事件是这样的：\nDocker 公司给所有创建过组织的 Docker Hub 用户发了一封邮件，告知他们如果不升级到付费的团队计划，他们的账户和镜像都将被删除。 这一举动只影响开源社区经常使用的组织账户，个人账户没有变化。 付费的团队计划每年需要 420 美元，很多开源项目没有足够的资金支持。 Docker 公司的开源项目计划（DSOS）要求非常苛刻，与开源项目的可持续性相悖。 Docker 公司的沟通方式非常脱节，引起了开源社区的反感和担忧。 文章作者建议开源项目使用其他的容器镜像仓库，如 GitHub Container Registry、Quay.io 、各大云厂商的镜像仓库等。 开源社区还提供了一些迁移镜像和重命名镜像的方法和工具。 增加对 WebAssembly 运行时的支持 2022 年 10 月，Docker 公司发布了 Docker+Wasm 技术预览，这是一个特殊的构建，可以让开发者更容易地使用 Docker 运行 Wasm 工作负载。作为这次发布的一部分，Docker 还宣布将加入 Bytecode Alliance 作为一个投票成员。\nWasm 是一种新技术，可让你在沙箱环境中运行 40 多种语言的应用程序代码，包括 Rust，C，C++，JavaScript 和 Golang。最初，Wasm 的用例是在浏览器中运行本地代码，如 Figma，AutoCAD 和 Photoshop 等。现在，一些公司如 Vercel，Fastly，Shopify 和 Cloudflare 等支持使用 Wasm 在边缘和云端运行代码。\nDocker+Wasm 技术预览包括：\nDocker 的目标是帮助开发者通过克服应用开发的复杂性来实现他们的想法。 Docker 将 Wasm 视为与 Linux 容器相辅相成的技术，开发者可以根据用例选择使用哪种技术（或两者都用）。 Docker 想要帮助开发者更容易地使用熟悉的经验和工具来开发，构建和运行 Wasm 应用。 要获取技术预览，需要下载并安装适合你系统的版本，然后启用 containerd 镜像存储（设置 \u0026gt; 开发中的功能 \u0026gt; 使用 containerd 拉取和存储镜像）。 这个预览支持使用 WasmEdge 运行时引擎运行 Wasm 容器，并可以通过容器仓库如 DockerHub 等分享。 2023 年 3 月 Docker 公司又发布了 Docker+Wasm 技术预览 2，包括了三个新的 Wasm 运行时引擎：Fermyon 的 spin，Deislabs 的 slight，和 Bytecode Alliance 的 wasmtime。\n该版本的主要更新是：\nDocker+Wasm 技术预览 2 是在 Docker Desktop 4.15 版本中发布的，旨在让开发者更容易地运行 Wasm 工作负载，并扩展运行时支持。 Docker+Wasm 技术预览 2 支持四种 Wasm 运行时引擎，包括之前已经支持的 WasmEdge，以及新增加的 spin，slight，和 wasmtime。 这四种 Wasm 运行时引擎都基于 runwasi 库，这是一个 Rust 库，可以让容器管理器 containerd 运行 Wasm 工作负载，并创建一种新的容器类型。 runwasi 库基于 WASI 标准，这是一个为 WebAssembly 提供通用平台接口的模块化系统接口。这意味着如果一个程序编译成目标是 WASI，它就可以在任何符合 WASI 的运行时上运行。 Wasm 容器通常只包含一个编译好的 Wasm 字节码文件，不需要任何额外的二进制库，这使得它比 Linux 容器更小。这也意味着 Wasm 容器通常启动更快，更可移植。 由于 Wasm 容器直接被 containerd 支持，在 Docker Desktop 最新版本中尝试 Docker+Wasm 技术预览 2 只需要启用“使用 containerd”选项。 通过这种方式，Wasm 容器可以与 Linux 容器一起使用 Docker Compose 或其他编排平台如 Kubernetes 运行。 此外，Docker Desktop 还能够将一个 Wasm 应用打包成一个 OCI 容器，并在其中嵌入一个 Wasm 运行时，以便通过容器仓库如 DockerHub 等分享。 总结 本文介绍了 Docker 发展过程中的四个重大举措：Moby Project、支持 Kubernetes、删除开源组织账号和增加对 WebAssembly 运行时的支持。其中，Moby Project 旨在推动容器技术走向主流，支持 Kubernetes 的举措提供了更广泛的选择和更好的用户体验，删除开源组织账号的举措引起了开源社区的不满和失望，增加对 WebAssembly 运行时的支持的举措则扩展了 Docker 的应用场景。\n参考 Introducing Moby Project: a new open-source project to advance the software containerization movement Docker for Windows Desktop… Now With Kubernetes! Introducing the Docker+Wasm Technical Preview Announcing Docker+Wasm Technical Preview 2 Docker is deleting Open Source organisations - what you need to know 别慌：Kubernetes 和 Docker ","relpermalink":"/blog/docker-four-milestones/","summary":"在 2017 年的容器编排大战中，Docker 公司失败后沉寂了几年，但近年来又开始频繁行动，例如腾退开源组织账号，支持 WebAssembly 等。本文将回顾 Docker 公司发展过程中的四个重大举措，这些措施深深地影响了 Docker 公司的发展，也对 Docker 甚至 Kubernetes 社区产生了深远的影响。","title":"Docker 发展史：四个重大举措，影响深远！"},{"content":"上周我申请加入 Google Bard waitlist，没想到这么快今天就收到了试用邀请，比起其他的生成式 AI 聊天工具申请试用要几个周甚至几个月才能通过要快的多。\nGoogle Bard 简介 Google Bard 是一个早期的实验性项目，它让你可以和生成式 AI 合作。它是由一个大型语言模型（LLM）驱动的，具体来说是 LaMDA 的一个轻量化和优化版本。你可以用 Google Bard 来提高你的效率，加速你的想法，激发你的好奇心。你可以向 Google Bard 提出各种问题，它会尝试给你有用的回答和建议。例如，你可以问 Google Bard 如何达到今年读更多书的目标，如何用简单的语言解释量子物理，或者如何为一篇博客文章写一个大纲。\nGoogle Bard 目前仅对英国和美国用户开放，且只支持英文。你可以在这里 申请加入 Google Bard 体验的 Waitlist。在申请通过之后你会收到一封告知邮件如图 1 所示，然后就可以去 bard.google.com 体验了。\n图 1：Google Bard 体验资格通过邮件 Google Bard 的用户界面如图 2 所示。\n图 2：Google Bard 的用户界面 测试：马云去哪了？ 下面我就同一个问题向 ChatGPT、必应聊天、Google Bard 提问——”Where is Jack Ma now?“，它们的回答如图 3 所示。\n图 3：“马云去哪了？”在三种工具中的回答对比 你觉得哪个回答更好呢？我觉得是必应聊天。\nGoogle Bard vs Bing Chat vs ChatGPT 前段时间我也体验了新必应的聊天，见体验新必应——聊天式的搜索引擎辅助工具 。\n正好使用生成式 AI 工具已经有好多个月了，三样工具我都用过了，我在下表中简单比较一下它们。\n技术 开发者 应用场景 主要功能 特色 缺点 Google Bard Google 帮助用户更有效地思考和创造 回答问题和提供建议 同时提供三个回答供用户选择 界面体验一般，缺乏个性 Bing Chat 微软 提供聊天交互式服务，聊天式搜索 回答常见问题，帮助预订机票或查找餐厅 增加了引用来源，使得回答更可信；可检索互联网上的实时数据；提供联想问题可供用户追问 缺乏个性，数据来源不够广泛 ChatGPT OpenAI 聊天交互、问题回答、文本摘要等多个领域 应用广泛，包括自然语言处理、信息检索、机器翻译、编程等 简单直接，更加人性化，提供开发者 API 时常胡言乱语，数据只更新到 2021 年；服务不稳定，免费账户需要经常刷新才能使用 总结 这篇文章介绍了 Google Bard，它是一个由 Google 开发的生成式 AI 工具，可以与用户合作，帮助他们更有效地思考和创造。文章还比较了 Bing Chat、OpenAI ChatGPT 和 Google Bard 的不同，以及它们对同一个问题的回答。笔者认为必应聊天的回答最好。\n","relpermalink":"/blog/hello-google-bard/","summary":"这篇文章介绍了 Google Bard，它是一个由 Google 开发的生成式 AI 工具，可以与用户合作，帮助他们更有效地思考和创造。文章还比较了 Bing Chat、OpenAI ChatGPT 和 Google Bard 的不同，以及它们对同一个问题的回答。笔者认为必应聊天的回答最好。","title":"体验过 Google Bard 之后我有些话要说"},{"content":"在我之前的博客 分享中提到 Istio Ambient Mesh 使用 iptables 和 Geneve 隧道将应用程序 Pod 中的流量拦截到 Ztunnel 中。很多读者可能还不了解这种隧道协议，本文将为你介绍 Geneve 隧道的定义，报文结构，以及与 VXLAN 协议的比较有哪些优势。最后，本文将介绍 Istio Ambient Mesh 如何应用 Geneve 隧道来实现流量拦截，以及 Istio 1.18 中新推出的 eBPF 模式。\nGeneve 隧道简介 GENEVE（Generic Network Virtualization Encapsulation）是一种网络虚拟化封装（隧道）协议，它的设计的初衷是为了解决当前数据传输缺乏灵活性和安全性的问题。Geneve 只定义了一种数据封装格式，不包括控制平面的信息。GENEVE 相较于 VXLAN 封装的关键优势在于其通过添加 TLV 格式的选项来扩展可封装的协议类型。\nGeneve vs VXLAN VXLAN 和 Geneve 都是网络虚拟化协议，它们之间有很多共同点。虚拟化协议是一种将虚拟网络与物理网络分离的技术，它允许网络管理员在虚拟环境中创建多个虚拟网络，每个虚拟网络都可以拥有自己的 VLAN 标识符、IP 地址和路由。此外，VXLAN 和 Geneve 协议都使用 UDP 封装，这使得它们能够通过现有网络基础设施进行扩展。VXLAN 和 Geneve 协议还具有灵活性，它们可以在不同的网络拓扑结构中使用，并且可以与不同的虚拟化平台兼容。\n图 1 展示了 VXLAN 与 Geneve 协议的报文结构及其各自的 Header 区别。\n图 1：VXLAN 与 Geneve 报文格式示意图 从图中我们可以看到，VXLAN 与 Geneve 隧道报文的结构类似，其主要区别在于使用不同的 UDP 端口号和协议头 ——VXLAN 使用 4789 端口，Geneve 使用 6081 端口；Geneve 协议头比 VXLAN 更具扩展性。\nGeneve 隧道协议比 VXLAN 更加可扩展是因为 Geneve 隧道协议中增加了变长选项，它可以包含零或多个 TLV 格式的选项数据。TLV 是指类型 - 长度 - 值（Type-Length-Value）格式，用于传输和解析网络包的元数据信息。在 Geneve 协议中，每个元数据信息都由一个 TLV 格式的字段组成，以便于灵活地添加、删除和修改这些元数据。\n具体来说，TLV 格式的字段包括：\nType：8 位的类型字段。 Length：5 位的选项长度字段，以 4 字节倍数表示，不包括选项头。 Data：可变长的选项数据字段，可以不存在或者为 4 到 128 字节之间。 通过使用 TLV 格式，Geneve 协议可以轻松地扩展和修改元数据信息，同时保持兼容性和灵活性。\n关于 VXLAN 隧道报文的详细信息请参考 RFC 7348 Virtual eXtensible Local Area Network (VXLAN): A Framework for Overlaying Virtualized Layer 2 Networks over Layer 3 Networks 。\n关于 Geneve 隧道报文的详细信息请参考 RFC 8926 Geneve: Generic Network Virtualization Encapsulation 。\n工作原理 Geneve 隧道主要应用在云计算和虚拟机化场景，它可以将数据包封装在一个新的数据包中，以便在虚拟网络中传输。Geneve 隧道使用一个 24 位的虚拟网络标识符 (VNI)，将数据包从一个物理网络传输到另一个物理网络。Geneve 隧道还可以使用安全性协议，如 IPsec、TL，来保护数据包的传输。\n当数据包到达目的主机时，Geneve 隧道协议会将数据包从 Geneve 协议头中解封装出来，并将其传递给虚拟网络中的目的地。在解封装过程中，Geneve 协议头中的 VNI 信息会被来判断数据包的目的地，以确保数据包被正确地路由到虚拟网络中的目的地。\n假设有一个虚拟网络，其 VNI 为 1001。当数据包从一个物理网络传输到另一个物理网络时，可以使用隧道将数据包从一个物理网络传输到另一个物理网络。在这种情况下，隧道将源物理网络和目标物理网络之间的虚拟网络标识符 (VNI) 设置为 1001，以便在传输期间跟踪数据包。当数据包到达目标物理网络时，隧道将 VNI 从数据包中删除，并将数据包传递给目标物理网络。\n安全性 Geneve 隧道协议本身并没有提供任何安全机制，因此在 Geneve 隧道中传输的数据包可能会受到威胁，例如数据包被篡改、截获、重放等。\n为了保障 Geneve 隧道中传输的数据包的安全性，可以使用一些安全协议。以下是一些常见的安全协议：\nIPsec（Internet Protocol Security）：IPsec 是一种网络层安全协议，可以对 Geneve 隧道中的数据包进行加密、认证和完整性保护。使用 IPsec 可以提供端到端的安全性。 TLS（Transport Layer Security）：TLS 是一种基于传输层的加密协议，可以对 Geneve 隧道中的数据包进行加密和认证。使用 TLS 可以提供端到端的安全性。 MACSec（Media Access Control Security）：MACSec 是一种数据链路层的安全协议，可以对 Geneve 隧道中的数据包进行加密和认证。使用 MACSec 可以提供链路层的安全性。 需要注意的是，以上安全协议都需要进行相应的配置和部署，且可能会对性能产生一定的影响。在选择合适的安全协议时，需要考虑安全性、性能、可管理性等方面的因素。\n为什么选择 Geneve？ 下表对比了 VXLAN 与 Geneve 在多个方面的特点。\n特性 VXLAN Geneve 头部格式 固定格式 可扩展格式 可扩展性 更多地专注于 L2 扩展 更好地支持新兴网络服务 可操作性 较难管理和扩展 更容易管理和扩展 性能 头部较短，性能较高 头部较长，性能略低 使用 Geneve 协议的主要原因是将目前网络虚拟化封装技术（例如 VXLAN、NVGRE 和 STT）中的优点合并到一个协议中。我们通过多年的网络虚拟化开发经验得知，其中一个重要的需求是可扩展性。Geneve 协议使用可扩展的 TLV 结构对元数据进行编码，因此可以独立地发展软件和硬件端点的功能，以满足不断增长的需求。\nIstio Ambient Mesh 如何应用 Geneve 隧道 在之前的博客 中，我讲解了 Istio Ambient Mesh 如何使用 Ztunnel 实现 L4 代理的，图 2 展示使用 iptables 和 Geneve 隧道的 L4 透明流量劫持路径。\n图 2：使用 iptables 和 Geneve 隧道的 L4 透明流量劫持路径示意图 从图中我们可以看到：\nIstio CNI 在节点上创建 istioout 网卡和 iptables 规则，将节点中的出站流量透明劫持到 pistioout 虚拟网卡； Istio CNI 在节点上创建 istioin 网卡和 iptables 规则，将节点中的入站流量透明劫持到 pistioin 虚拟网卡； Istio CNI 在 ztunnel 中创建 pistioin 和 pistioout 网卡，用于接收 Geneve 隧道中的发来的数据包； pistioin 和 pistioout 这两个网卡是由 ztunnel 中的 init 容器或 Istio CNI（见 CreateRulesWithinNodeProxyNS 函数）创建的，其 IP 地址和端口也是固定的。应用容器发出的数据包需要经过 istioout 网卡并使用 Geneve 隧道封装后转发给 ztunnel 容器。\n使用 eBPF 进行透明流量劫持 eBPF（extended Berkeley Packet Filter）是一个功能强大的技术，它可以在 Linux 内核中运行安全的用户态程序。eBPF 最初是一种用于过滤网络数据包的技术，但现在已经扩展到其他领域，如跟踪系统调用、性能分析和安全监控等。eBPF 的优势在于其轻量级、高效、安全和可编程性。它可以被用于实时监控、网络安全、应用程序调试和优化、容器网络等多个领域。\n在 Istio 1.18 之前，Ambient 模式中使用 iptables 和 Geneve 隧道将应用程序流量透明劫持到 ztunnel 中。在 Istio 1.18 中，增加了 eBPF 选项，你可以选择使用 iptables 或 eBPF 来做流量劫持。如图 3 所示，eBPF 程序直接运行在宿主机内核，将应用程序的流量转发到 ztunnel 中。\n图 3：使用 eBPF 劫持应用程序的流量 图 3：使用 eBPF 劫持应用程序的流量\n对比项 eBPF 方式 使用 iptables 和 Geneve 隧道 效率 更高 略低 兼容性 需要较高的 Linux 内核版本 更好 实现难度 较高 较低 扩展性 较好 较差 根据 Istio 官方博客 介绍，使用 eBPF 方式避免了部分 iptables 规则和隧道封装，相比使用 iptables 和 Geneve 隧道更加高效。然而，eBPF 对 Linux 内核版本的要求更高（至少 4.20），而 iptables 方式则具有更好的兼容性。此外，eBPF 方式的实现难度较高，但扩展性较好。\n要想使用 eBPF 模式运行 Ambient Mesh，只需要在安装 Istio 时设置 values.cni.ambient.redirectMode 参数即可，如下：\nistioctl install --set profile=ambient --set values.cni.ambient.redirectMode=\u0026#34;ebpf\u0026#34; 总结 本文介绍了 Geneve 隧道协议的工作原理、安全性和与 VXLAN 的比较。此外，还介绍了 Istio Ambient Mesh 如何使用 Geneve 隧道实现流量拦截，并讨论了使用 eBPF 进行透明流量劫持的优缺点。Geneve 隧道协议是一种通用的隧道协议，可以在虚拟网络中传输数据包，具有更多的优势，因此在选择隧道协议时，用户可以考虑使用 Geneve 隧道。在 Istio 1.18 中新推出了 Ambient Mesh 的的 eBPF 模式，可以提供网络效率，但对 Linux 内核版本有更高要求，用户可以根据自己的实际情况选用。\n参考 RFC 7348 Virtual eXtensible Local Area Network (VXLAN): A Framework for Overlaying Virtualized Layer 2 Networks over Layer 3 Networks RFC 8926 Geneve: Generic Network Virtualization Encapsulation Istio Ambient Mesh Open vSwitch Geneve(8) man page ","relpermalink":"/blog/traffic-interception-with-geneve-tunnel-with-istio-ambient-mesh/","summary":"本文介绍了什么是 Geneve 隧道，与 VXLAN 的区别，以及它在 Istio Ambient Mesh 中的应用，最后谈及使用 eBPF 优化流量拦截。","title":"使用 Geneve 隧道实现 Istio Ambient Mesh 的流量拦截"},{"content":"更新时间：2023 年 04 月 16 日\n人工智能（AI）正在改变我们的生活，使我们的工作更加高效和智能化。在这个快速发展的领域中，有许多 AI 实用工具可以帮助我们更好地完成工作。在未来熟练使用各种 AI 工具优化你的工作流并提高工作效率将是每个人的必备技能，是时候收集一些便宜好用的实用 AI 工具了。下面是笔者推荐的一些值得收藏的实用 AIGC 工具，你可以直接上手使用这些工具，不需要额外的编程知识。这些工具大多可以免费使用，或者提供免费使用额度，或者使用成本很低。\n注意：本列表暂时只关注文本工具，不涉及以下内容：\n图像、声音、视频等多媒体内容生成和编辑，因为编辑那些内容需要更多专业知识，一般供专业人士使用； 必须付费才能使用的工具或者笔者认为收费过高的工具； 什么是 AIGC 工具？ AIGC（AI-Generated Content）可以自动生成各种文本内容，如文章、新闻、产品描述、广告语等，它使用人工智能技术，通过对大量文本数据的学习和分析来生成通顺、连贯、语法正确的文本内容。对于需要大量重复性内容的场景（如电商平台的商品描述、新闻媒体的稿件生成等），它能大幅提高内容创作效率。AIGC 工具的实现方式有很多种，如基于语言模型的自动生成文本、基于模板的文本生成、基于知识图谱的自动生成文本等。随着技术的不断进步，AIGC 工具在自动生成文本方面的能力也在不断提升，能够生成更加贴近人类写作风格的文本内容。 1. ChatGPT ChatGPT 网站界面 ChatGPT 是一个基于 GPT 技术的智能聊天机器人，可以与用户进行自然语言交互。它使用深度学习技术和大规模训练的语言模型来理解用户的问题并提供有用的答案。\nChatGPT 可以回答各种问题，用户可以直接在网站上输入问题或话题，并获得快速和准确的答案。需要注意的是，ChatGPT 是一个在线聊天机器人，它的回答可能不是 100% 准确。此外，ChatGPT 模型训练的数据截止到 2021 年，它还处于早期开发阶段，仍在不断改进和优化中。\n推荐理由\nChatGPT 的回答速度超快，几秒就能得到答案，特别适合需要快速回复的场合。还可以用 OpenAI 的 API 做自己的工具。但是 ChatGPT 的免费版响应速度有时会慢，而且在追问时经常需要刷新页面，问题和答案的字数也有限制。\n比较常用的功能包括：\n编写代码 翻译 文章润色 总结某一篇文章（你可以输出一个 URL） 学习一个你不了解的知识领域 另外推荐一款 ChatGPT 桌面应用 以及 ChatGPT 提示项目 。\nChatGPT Plus ChatGPT Plus 是 ChatGPT 的增强版，它使用 GPT-3 技术，提供了更加强大和智能的自然语言处理功能。它可以帮助用户完成各种任务，如自动生成文章、翻译、问答、语音转换和聊天机器人等。\nChatGPT Plus 的界面简洁美观，使用起来非常直观和友好。用户可以使用多种方式输入文本，如键盘输入、语音输入和图像输入等。ChatGPT Plus 还提供了多种语言和风格选择，可以帮助用户轻松生成高质量的文本和语音。\n2. Smodin Smodin 界面 Smodin.io 是一个基于人工智能和自然语言处理技术的在线工具，可以帮助用户自动生成文章、新闻稿、博客和社交媒体帖子等内容。该网站使用深度学习技术和语言模型来自动化生成高质量的文本，并提供了多种语言和风格选择。\nSmodin.io 的使用非常简单，只需要输入您需要生成的主题、关键词和所需的语言和风格，然后点击“生成”按钮即可获得一篇高质量的文章。您还可以根据需要进行微调和编辑，以满足您的具体需求。\n除了自动生成文本外，Smodin.io 还提供了其他有用的功能，如语法和拼写检查、SEO 优化建议和实时翻译等。该网站的使用非常灵活和方便，适合需要高质量文本的个人和企业用户，如营销人员、编辑、作家和博主等。\n需要注意的是，虽然 Smodin.io 可以为您节省大量时间和精力，但由于是自动生成的文本，可能会存在一定的语法或逻辑错误，因此在使用时需要进行适当的审查和编辑。\n推荐理由\n该网站支持 40 多种语言，对于中文的改写功能比较强悍，限制一次性输入 1000 字，可以迅速的返回改写结果，免费用户有调用次数限制，付费用户可无限制。付费分为两个档次，价格比较便宜。\n3. Bing 新必应聊天界面 Bing 是微软公司开发的搜索引擎，集成了人工智能技术，可以提供更加智能化的搜索结果。最近，微软宣布向 Bing 中添加了一项新功能，即“聊天”，用户可以在聊天框中输入问题，Bing 会自动回答问题，就像一个智能聊天机器人一样。Bing 还提供多种对话样式可供选择。\n需要注意的是，聊天功能目前还处于测试阶段，可能存在一些问题和限制。此外，Bing 在一些国家和地区的可用性可能会受到限制。但是，它仍然是一款非常有用的搜索引擎，可以帮助用户快速找到所需的信息。\n推荐理由\n新必应集成的聊天功能比较稳定，因为它可以联网和实时爬取因特网中的数据，你可以把它作为搜索引擎的辅助工具，或者询问它一些实时的信息。\n关于新必应与 ChatGPT 的区别请见我的前一篇博客 。\n4. GitHub Copilot GitHub Copilot 界面 GitHub Copilot 是一款由 OpenAI 和 GitHub 共同开发的 AI 代码助手，可以帮助开发人员更快、更准确地编写代码。它使用了自然语言处理和机器学习等技术，可以自动为用户生成高质量的代码，提高了开发效率和代码质量。\nGitHub Copilot 可以与多种开发工具和编程语言集成，如 VS Code、Visual Studio、Python、JavaScript 等。用户只需要在编辑器中输入少量的自然语言描述，Copilot 就可以自动为用户生成高质量的代码。例如，当用户输入“Create a function that takes two arrays as arguments and returns their dot product.”时，Copilot 可以自动为用户生成如下代码：\nfunction dotProduct (arr1, arr2) { return arr1.map ((n, i) =\u0026gt; n * arr2 [i]).reduce ((a, b) =\u0026gt; a + b); } 需要注意的是，GitHub Copilot 目前还处于 beta 版本，仍在不断优化和改进中。此外，GitHub Copilot 的代码生成质量和准确性可能会受到一些因素的影响，如输入的描述、编程语言、编码规范等。\n推荐理由\nGitHub Copilot 的代码生成速度非常快，可以帮助开发人员节省大量的时间和精力。此外，GitHub Copilot 还可以从用户的代码库中学习，提高了代码生成的质量和准确性。\n5. Notion Notion 界面 Notion 是一款集笔记、待办事项、项目管理、知识库和团队协作等功能于一体的综合性工具。它提供了强大的编辑和组织功能，可以帮助用户轻松创建各种类型的文档，包括文本、图片、视频、表格和数据库等。\nNotion 的界面简洁美观，使用起来非常直观和友好。用户可以使用多种方式组织和查找自己的文档，如标签、目录和搜索等。此外，Notion 还提供了丰富的协作和分享功能，可以帮助用户轻松与团队成员协作和共享文档。\n对于 AI 开发人员来说，Notion 是一个非常好用的工具，可以帮助他们轻松管理项目、记录实验数据、分享笔记、翻译和文档等。Notion 还提供了丰富的第三方应用程序和 API，可以轻松集成其他工具和服务。\n需要注意的是，Notion 的免费版本有一些限制，如文件上传大小限制和 API 调用次数限制等。但是，付费版本提供了更多的功能和服务，如无限制的文件上传、API 调用和团队成员数等，适合团队和企业用户使用。\n推荐理由\nNotion 提供了丰富的模板和插件，可以帮助用户快速创建各种类型的文档和项目，如产品路线图、项目计划和工作日志等。此外，Notion 的 API 和 Webhook 功能也非常强大，用户可以利用这些功能轻松集成其他工具和服务。免费用户可以创建无限多的文档，但是只有 20 次调用 Notion AI 的权限。超过该次数需要付费 $10 / 月 订阅（注意：Notion AI 功能需要单独订阅）。\nNotion 是我最喜欢的 AI 文本编辑器，它对中文支持很好。而且，它支持 Markdown 格式，方便导出。它可以代替 Smodin 的部分功能，但是对于文本改写的效果还有待验证。\n6. Grammarly Grammarly 界面 Grammarly 是一款使用人工智能帮助用户提高写作能力的写作助手。它可以识别和纠正语法错误、拼写错误和标点符号错误，还可以提供改善写作清晰度和风格的建议。Grammarly 可以作为浏览器扩展、桌面应用程序和移动应用程序使用，可用于各种写作任务，如电子邮件、社交媒体帖子和学术论文。Grammarly 有免费版本和高级版本，具有其他功能。\nGrammarly 的使用非常简单，只需在浏览器中安装其扩展程序或在电脑或移动设备上下载其应用程序，然后在需要的文本框中开始输入即可。Grammarly 会自动检查文本，并在需要时提供建议和纠正错误。用户还可以选择不同的写作风格和语言设置，以适应他们的写作需求。\n除了基本的拼写和语法检查外，Grammarly 还提供了一些高级功能，如词汇增强、句子结构调整、文本简化和风格建议等。这些功能可以帮助用户改善写作技巧，提高写作质量。此外，Grammarly 还提供了一些其他有用的工具，如写作目标设定、批注和评论等，可以帮助用户更好地组织和管理自己的写作任务。\n需要注意的是，尽管 Grammarly 可以提供有用的建议和纠正错误，但它并不能完全代替人类编辑或审查。在使用 Grammarly 时，用户仍应该进行适当的审查和编辑，以确保写作质量和准确性。\n推荐理由\nGrammarly 的语法检查和纠错功能非常准确和实用，可以帮助用户快速纠正拼写错误和语法错误。此外，Grammarly 还提供了一些高级功能，如句子结构调整和文本简化，可以帮助用户改善写作风格和质量。对于需要频繁写作的人来说，Grammarly 是一个非常有用的工具，可以提高写作效率和质量。\n注意：Grammarly 不支持中文。\n7. Quillbot Quillbot 界面 Quillbot 是一款使用人工智能技术帮助用户改写和翻译文本的工具。它可以帮助用户将现有文本转化为更加简洁、清晰和易于理解的版本，以满足不同的写作需求。\nQuillbot 的使用非常简单，只需将需要改写或翻译的文本粘贴到指定的文本框中，然后选择所需的改写或翻译选项，即可获得高质量的改写或翻译结果。Quillbot 还提供了多种语言和风格的选择，以满足不同用户的需求。\n除了改写和翻译文本外，Quillbot 还提供了其他有用的功能，如语法检查、拼写检查、同义词替换和学术论文检查等。这些功能可以帮助用户提高写作质量和效率，节省时间和精力。\n需要注意的是，尽管 Quillbot 可以为用户节省大量时间和精力，但改写和翻译文本需要一定的技巧和经验，因此在使用时需要进行适当的审查和编辑。\n推荐理由\nQuillbot 的改写和翻译功能非常强大和实用，可以帮助用户快速转化现有文本为更加简洁、清晰和易于理解的版本。此外，Quillbot 还提供了其他有用的功能，如语法检查、拼写检查和同义词替换等，可以帮助用户提高写作质量和效率。Quillbot 提供多款插件，可以轻松集成到浏览器和其他应用程序中使用。\n注意：Quillbot 不支持中文。\n8. ChatPDF ChatPDF 界面 ChatPDF 是一个可以让你和任何 PDF 文件进行交流的工具，就像和一个人聊天一样。它的原理是通过分析 PDF 文件来创建语义索引，然后根据你的问题，把相关的段 …","relpermalink":"/blog/ai-tools-collection/","summary":"本篇博客旨在为大家推荐几款实用且易于上手的 AI 工具，希望能够帮助大家提高工作效率，更好地应用 AI 技术。","title":"AIGC 工具推荐：利用 AI 提高工作效率的利器"},{"content":"更新时间：2023 年 04 月 20 日 17:16:22\n昨天我的新必应体验申请通过了，立马去体验了下，本文记录下我对新必应的最初印象。\n如何体验新必应 访问必应国际站（bing.com ）并下载 Bing App（非中国版），申请加入 waitlist，我是在 waitlist 推出第一天就申请了，直到昨天才正式获批。\n新必应推荐（强制）你使用微软 Edge 浏览器和使用 bing.com 进行搜索，如果你在中国大陆使用 Chrome、Safari、火狐或其他浏览器访问的话，无论搜索什么网页都不会给你返回任何结果，你将看到白屏，这令人很无语。\n使用 Chrome 访问新必应 如果你身处中国大陆，习惯使用了 Chrome 浏览器，不想为了体验新必应而安装一个 Edge 浏览器，可以使用下面的方法在 Chrome 中使用新必应：\n方式一：修改 HTTP Header 首先在 Chrome 应用商店中安装 ModHeader ，方便我们修改 HTTP 请求中的 Header，模仿 Edge 浏览器。\n然后配置 ModeHeader，添加规则如下：\n在 Request headers 处添加：\n模拟 Edge 浏览器：user-agent = Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36 Edg/110.0.1587.41 模拟香港 IP：X-Forwarded-For = 1.36.x.y 在 FILTER 处选择「(Request) URL filter」，填入 .*://www.bing.com/.*\n注意：这种方式在 2023 年 4 月 20 日发现失效。\n方式二：安装专用浏览器插件 安装 Bing chat for all browsers 浏览器插件。推荐这种方式，安装好后你不需要任何配置。\n现在访问 bing.com 你就可以看到顶部的【聊天】选项，可以顺利的使用新必应了。\n新必应初体验 在新必应推出前，我已经使用过几个月的 ChatGPT 了，因此下面主要将新必应的聊天与 ChatGPT 做对比。通过同一个问题两者给出的不同答案，以小见大。\n问题：北京的亮马河发源于哪里？\nChatGPT 和新必应给出了截然不同的回答，如图 1 所示。ChatGPT 的答案究竟来自哪里我们无从知晓，如果你对北京的地面一无所知的话也许会觉得它的回答有模有样，貌似很合理，但实际上稍有了解的人都会觉得它是【一本正经的胡说八道】。而新必应的回答则明确了出处，是来自维基百科和百度百科等渠道。\n图 1：ChatGPT 与新必应给出的不同回答 从上面的回答及我一天的使用体会：\nChatGPT 并非直接引用网络上的内容，因为在网上找不到同样的回答，所以我们无法直接验证答案的准确性，需要与它进一步对话来纠正它的错误； 新必应对于问答是问题，很多答案都引用自知乎； 新必应的每条回答都会给出引用来源，这一点方便我们去验证，这就像是搜索引擎的辅助工具，如果你直接搜索也可以很方便找到答案； 新必应会根据你的问题推荐一些相关的问题，类似搜索引擎中的推荐功能； 因为新必应是联网的，如果问一些近期的问题，也可以找到答案，而 ChatGPT 只有 2021 年以前的数据； 新必应聊天功能更稳定，在我试用一天的时间内没有遇到需要刷新重连的情况，而 ChatGPT 免费用户会经常遇到刷新问题； 新必应的上下文联想能力比较差，更像是一个聊天式的搜索工具，它可以将一些搜索到的网页主题进行归纳输出，然后给出引用地址； 结论：聊天是新必应的搜索引擎辅助工具，如果 Google 推出同样的服务，比如 Bart，我可能还会继续使用 Google 搜索。\n网友看法 我昨天在朋友圈里分享了这张图片，看看大家对这两种回答的看法。收到了很多网友的朋友，我精选了以下几条给大家品味一下：\nChatGPT 有自己的人格，他就像一个中年油腻男，一本正经的胡说八道的样子，确实很强大。比那些老老实实实的回答的 AI 强多了； ChatGPT 是数学白痴，只要语义自洽逻辑合理，它可以无中生有一本正经胡扯。而且数据陈旧只有 2021 年之前的数据； ChatGPT 目前不太适合客观问题的解答，说白了它的核心就是“生成/编造”，但是非常适合一些基于已有事实的创作，比如润色/翻译文章之类的，新必应更像是传统 NLP + 搜索引擎，其实对于搜索这个场景我觉得反而更靠谱一点； 新必应更好，ChatGPT 没联网而且有时候会捏造答案。新必应用了 ReAct prompt 会自动 search + reason； ChatGPT 是一个发散思路的工具，然后具体的实施和工作，还会根据相关的关键词，交叉验证一下。但是新必应不能把一个功能详细合理的写出来，内容也有限制。现在还是使用 ChatGPT 进行辅助工作。 关于 ChatGPT 和新必应大家有什么看法，欢迎在评论区回复。\n参考 无需梯子！新 Bing ChatGPT 申请教程 by LiXin - ilx.notion.site ","relpermalink":"/blog/new-bing-chat/","summary":"昨天我的新必应体验申请通过了，立马去体验了下，本文记录下我对新必应的最初印象。","title":"体验新必应——聊天式的搜索引擎辅助工具"},{"content":"本文假设你熟悉服务网格和 Istio，对 Tetrate 出品的商业化服务网格管理工具 Tetrate Service Bridge 感兴趣，那么这篇文章可以解答你心中的一些困惑。\n本文中包含了：\n关于 TSB 的一些 FAQ TSB 的一些基本资源对象 TSB 实现的一些细节 TSB FAQ 笔者将罗列一些关于 TSB 的知识点，主要是关于它与 Istio 之间的不同点：\nTSB 全称 Tetrate Service Bridge，是 Tetrate 公司的旗舰产品，首发于 2018 年，目前版本是 1.7，这是一款商业化产品，你需要购买才能使用，如果想要了解更多或请求试用请在这里 登记； TSB 可以私有化部署也可以在 GKE、AWS、Azure、OpenShift、EKS Anywhere 等平台中单独或跨平台部署； TSB 并不是对 Istio 的改进版，它是在 Istio 之上建立的管理平面，它可以与 Istio 完全兼容，并适配最新版的 Istio； TSB 并不是 Kubernetes 管理平台，它不能用于管理 Kubernetes 中的资源； TSB 是基于 Istio、Envoy 和 Apache SkyWalking 构建； TSB 通过定义 Tenant、Workspace 等资源对象为 Istio 增加了多租户和多集群管理功能； 为了实现多集群服务网格的管理，TSB 中定义了一系列 Group 资源，如 Traffic、Security、Gateway 等，这些 CRD 最终将转化为 Istio 中的 CRD 应用到服务网格； TSB 定义了 Tier1Gateway CRD 实现了跨集群的二层网关； TSB 部署和管理比较复杂，Tetrate 即将推出轻量级的 Tetrate Service Express（TSE）简化服务网格管理； TSB 中的资源对象 图 1 展示了 TSB 的基本资源对象，其中只要分为五大类：\n为了实现多租户和多集群管理的资源对象，如 Tenant、Organization、Workspace 等； 主要功能对象配置组，如 Traffic、Security、Gateway 等； Istio 原生 CRD； 角色和权限管理； 其他扩展； 图 1：Tetrate Service Bridge 中的资源对象 TSB 是建立在 Istio 之上的，下表中列出了 TSB 中的可以通过 tctl 命令获取的资源对象，其中部分名称与 Kubernetes 中原生资源对象重复，但它们并不相同：\n名称 说明 ApplicationAccessBindings 配置应用程序用户访问权限。 AccessBindings 为 TSB 中任何资源的用户分配访问角色的配置。 AuthorizationPolicy 同 Istio 中的授权策略。 APIAccessBindings 配置 API 用户访问权限。 Application 应用程序的配置，应用程序表示一组相互关联的服务逻辑分组，并公开一组实现完整业务逻辑的 API。 Cluster Kubernetes 集群。要想将 Kubernetes 集群纳入 TSB 管理，首先需要声明 Cluster 添加，然后是部署 TSB Agent 和控制平面（包括 Istio、XCP、GitOps、OAP 等组件）。 DestinationRule Istio 原生 CRD，主要用来划分可路由的集群及负载均衡规则。 EnvoyFilter Istio 原生 CRD，主要用来扩展 Envoy 的功能。 EgressGateway 配置工作负载作为出口网关。 GatewayAccessBindings 配置网关组用户访问权限。 GatewayGroup 网关组。 Gateway 管理 TSB 中设置的网关，而非 Istio 中的原生 Gateway CRD。 IstioInternalAccessBindings 为 Istio 内部组的用户分配访问角色的配置。 IngressGateway 配置负载作为入口网关，类似于 Istio 中的原生 Gateway。 IstioInternalGroup 配置 Istio 内部的 TSB 资源。 OrganizationAccessBindings 配置组织用户访问权限。 Organization 组织。 OrganizationSetting 组织的默认配置，如区域 Failover、安全、流量、网络配置等。 Metric 运行时获取的服务度量。 Source Sources 服务公开了管理来自资源的遥测源。 PeerAuthentication Istio 原生 CRD，配置对等认证（配置 mTLS）。 RequestAuthentication Istio 原生 CRD，配置请求认证（JWT 规则配置）。 ServiceAccount 不同于 Kubernetes 中的原生资源对象，TSB 自定义的服务账号配置。 SecurityAccessBindings 配置安全组用户访问权限。 Sidecar 配置预安装的 Istio Sidecar。 ServiceEntry Istio 原生 CRD，添加服务对象。 SecurityGroup 安全配置组。 ServiceRoute 配置服务路由。 SecuritySetting 安全设置将配置应用于 SecurityGroup 或 Workspace 中的一组代理工作负载。当应用于 SecurityGroup 时，缺失的字段将从 Workspace 范围设置继承值 (如果有的话)。 ServiceSecuritySetting 安全组配置。 Service 注册中心中的服务，表示所有这些单独服务的聚合和逻辑视图，并提供聚合指标等高级功能。 Tier1Gateway TSB 一级网关配置，指定网关负载。 TrafficAccessBindings 流量访问角色配置。 TrafficGroup 流量管理组。 TenantAccessBindings 租户角色配置。 TenantSetting 租户配置。 TrafficSetting 流量配置。 VirtualService Istio 原生 CRD，配置流量路由。 WorkspaceAccessBindings Workspace 角色配置。 WasmExtension 配置管理 Wasm 扩展。 WasmPlugin Istio 原生 CRD，配置 Wasm 插件。 Workspace 划定工作空间。 WorkspaceSetting 配置工作空间。 你可以使用 tctl 命令行工具来管理 TSB，它的使用方法与 kubectl 类似，上面的列表是使用 tctl get 命令可以列出的资源对象。实际上 TSB 中的资源对象不止这些，关于 TSB 中 API 资源的详细说明请参考 TSB 文档 。\nTSB 架构 我在上文中提到，TSB 是在 Istio 之上构建的管理平面，为 Istio 增加了多租户和多集群管理功能，TSB 的组件架构及其功能如图 2 所示。\n图 2：TSB 组件架构图 TSB 的全局控制平面可以和管理平面部署在同一个 Kubernetes 集群中，也可以单独部署。\nTSB 管理平面的架构如图 3 所示。\n图 3：TSB 管理平面架构图 管理平面与各个 Kubernetes 集群中的 Istio 联系，管理多集群环境下的服务网格。\nTSB 控制平面的架构如图 4 所示。\n图 4：TSB 控制平面架构 为了实现跨集群的多租户管理，TSB 定义了一系列逻辑对象，如图 5 所示。\n图 5：TSB 中的多租户管理层级 更多 如果您不熟悉服务网格和 Kubernetes 安全性，我们在 Tetrate Academy 提供了一系列免费在线课程，可以让您快速了解 Istio 和 Envoy。\n如果您正在寻找一种快速将 Istio 投入生产的方法，请查看 Tetrate Istio Distribution (TID) 。TID 是 Tetrate 的强化、完全上游的 Istio 发行版，具有经过 FIPS 验证的构建和支持。这是开始使用 Istio 的好方法，因为您知道您有一个值得信赖的发行版，有一个支持您的专家团队，并且如果需要，还可以选择快速获得 FIPS 合规性。\n一旦启动并运行 Istio，您可能需要更简单的方法来管理和保护您的服务，而不仅仅是 Istio 中可用的方法，这就是 Tetrate Service Bridge 的用武之地。您可以在这里 详细了解 Tetrate Service Bridge 如何使服务网格更安全、更易于管理和弹性，或联系我们进行快速演示 。\n参考 TSB 简介 - tetrate.io TSB 文档 - docs.tetrate.io TSB API 参考 - docs.tetrate.io ","relpermalink":"/blog/tsb-tips/","summary":"如果你对服务网格（Service Mesh）和 Tetrate Service Bridge 感兴趣，那么这篇文章可以解答你心中很多疑惑。","title":"关于 TSB 你应该知道的事情"},{"content":"最近 Istio 社区正在处理 Issue 17763 ，以增加 Istio 对证书撤销的支持，目前该功能正在开发工程中，见 PR #42859 - OSCP Stapling 和 OCSP Stapling for Istio Gateways 。这篇博客将向你介绍证书撤销的方式和 Istio 的解决方案进展。\nIstio 使用 Envoy SDS 分发证书，一共有两种情形：\n分发内部证书，用于 mTLS，这种情况需要分别验证客户端和服务端的证书是否被撤销 分发外部服务的证书给集群内的服务，用于向集群外的通信加密，此时集群内的服务相当于客户端，仅需验证服务端证书是否被撤销 本文探讨的是第二种情形，即 Istio Gateway 的证书撤销，如下图所示：\nIstio Gateway 的证书撤销示意图 CA 通过设置 TTL 控制证书的寿命，如果要提前结束证书可以将证书撤销，有两种方式撤销证书：\n配置证书撤销列表（CRL） 在线证书状态协议（OCSP） CRL（证书撤销列表） CRL（证书吊销列表）是一种用于管理和验证证书有效性的机制。它包含一个已被撤销的证书列表，颁发机构（CA）定期更新该列表。当验证证书的客户端（如浏览器）收到证书时，它会检查该证书是否在 CRL 中被列为已撤销，如果是，则该证书被视为无效。\nCRL 通常存储在颁发机构（CA）的服务器上，并可以通过互联网公开访问。验证证书的客户端（例如，浏览器）可以下载并检查 CRL 以确定证书是否有效。CRL 可以以多种格式（如 DER 或 PEM）存储，并通过 HTTP，LDAP 或其他协议发布，以便进行验证。\nCRL 文件通常是以二进制形式存储的，不是直接可读的文本文件。但是，可以使用工具（例如 OpenSSL）转换为其他格式，例如 PEM，以方便阅读。下面是一个名为 crl.pem 的 CRL 示例文件，以 PEM 格式表示：\n-----BEGIN X509 CRL----- MIIBtjCBqwIBATANBgkqhkiG9w0BAQUFADBBMQswCQYDVQQGEwJVUzERMA8GA1UE CBMITmV3IFlvcmsxETAPBgNVBAcTCE5ldyBZb3JrMREwDwYDVQQKEwhNeSBDQTEQ MA4GA1UECxMHQ2VydGlmaWNhdGUxGDAWBgNVBAMTD01ZIEJ1c2luZXNzIENBMB4X DTA5MDUwMzE1MTQwMFoXDTEwMDUwMjE1MTQwMFqgLzAtMB8GA1UdIwQYMBaAFJ4f 6Nz7Vuw/NcZXG0U8O6OZ9XB0MAsGA1UdDwQEAwIFoDAdBgNVHQ4EFgQUU6G9fjRK op+JU6vQPnnhxBxHtzUwDQYJKoZIhvcNAQEFBQADgYEAbYoLz7MnJ4ltIS+nCquG N/v+8rE00/N0pGzN/dCv/8t0W0tGTGr2zGRb0mv67MPOmWmHcZZq3sOxgEIp8T+O OJxDCD/xJN6hB0NgN/Z0+fX9hU6bL/6zhwUy/l51xddmSd5KKhJ7FmOh2Py5m9Au 4fJh7w+OLyjKV7rz55WKjvY= -----END X509 CRL----- 使用 OpenSSL 将其转换为可读格式：\nopenssl crl -inform PEM -in crl.pem -noout -text 输出结果为：\nCertificate Revocation List (CRL): Version 2 (0x1) Signature Algorithm: sha256WithRSAEncryption Issuer: /C=US/ST=New York/L=New York/O=My CA/OU=Certificate/CN=My Business CA Last Update: May 3 11:40:00 2009 GMT Next Update: May 2 11:40:00 2010 GMT Revoked Certificates: Serial Number: 1 (0x1) Revocation Date: May 3 11:40:00 2009 GMT 注意：该 CRL 文件的列表中只保存了一个撤销证书。\nCRL 虽被广泛使用，但使用 CRL 文件来撤销证书存在以下几个缺点：\n时效性：CRL 文件必须定期更新，否则将无法识别近期撤销的证书； 可用性：如果无法访问 CRL 服务器，则无法验证证书是否已被撤销； 效率：CRL 文件随着证书数量的增加而变得越来越大，因此验证证书的过程可能会变慢； 安全性：CRL 文件本身也需要安全保护，否则攻击者可能会篡改 CRL 以逃避撤销证书的限制； 因此，现在有更高效和安全的证书撤销机制，例如 OCSP（Online Certificate Status Protocol），它可以实时验证证书的状态，并且不需要存储所有已撤销证书的列表。\nOCSP Stapling OCSP Stapling（正式名称为 TLS 证书状态查询扩展）是一种 TLS（Transport Layer Security）扩展，用于证明证书的状态是有效的。它允许服务器预先检索证书状态信息，并将该信息“钉”到 TLS 证书中，以减少对证书颁发机构的依赖，并提高证书状态验证的效率。可代替 OCSP 来查询 X.509 证书的状态。服务器在 TLS 握手时发送事先缓存的 OCSP 响应，用户只需验证该响应的有效性而不用再向数字证书认证机构（CA）发送请求。详见维基百科 。\nOCSP 只适用于单个证书，而不是列表。客户端在收到证书后，与 CA 通信以检查撤销状态。响应可以是 “good”、“revoked” 或 “unknown” 之一。\n使用 OSCP Stapling 虽然省去了维护 CRL 的负担，但是它依然有以下几个缺点：\n额外的资源开销：CA 服务器需要不断地响应 OCSP 质询以确保证书的有效性，这将对服务器的 CPU 和网络带宽造成额外的开销； 可用性问题：如果 OCSP 服务器不可用，则客户端将无法获得证书的有效性信息； 安全问题：如果 OCSP 响应被篡改或服务器不安全，则证书的有效性信息可能被篡改，从而影响安全； 兼容性问题：OCSP Stapling 不是所有浏览器都支持的功能，因此可能需要在旧浏览器上实现额外的兼容性。 OCSP 的挑战是，它给 CA 带来了很大的负担，因为每个客户端都需要独立地获得证书的验证。总体而言，OCSP Stapling 可以提高证书验证的效率和安全性，但是也存在一些需要考虑的问题。因此，在采用该技术时需要综合考虑多方面的因素。\nIstio 对 OSCP Stapling 支持 很多 Web 服务器都支持 OSCP Stapling，云原生边缘代理 Envoy 也支持该功能，需要对 Envoy 进行以下配置：\n配置 DownstreamTlsContext 中的 oscp_staple_policy： LENIENT_STAPLING：OCSP 响应是可选的，此为默认值 STRICT_STAPLING：OCSP 响应是可选的，但如果存在并且有效，就会使用。 MUST_STAPLE: OCSP 响应是必需的 配置 TlsCertificate 中的 oscp_staple，响应必须是 DER 编码的，只能通过文件名或 inline_bytes 提供。响应可能只与一个证书有关。 目前 Envoy 已支持 OSCP Stapling，其作为 Istio 的数据平面和 Istio Gateway 中的代理，理论上 Istio 也可以支持该功能。不过 Istio 的 OSCP Stapling 证书撤销功能支持仍在进行中，详见 PR #42859 - OSCP Stapling 。该功能的新进展将在本文中更新，请保持关注。\n","relpermalink":"/blog/ocsp-stapling-for-istio-gateways/","summary":"这篇博客将向你介绍证书撤销的方式和 Istio 的解决方案。","title":"Istio Gateway 中对证书撤销的支持"},{"content":"2023 年你有什么学习计划？我计划要学习一门新技术——WebAssembly！\n为什么要学习 WebAssembly？ 2019 年，Docker 创始人 Solomon Hykes 发布了一条推特在业界引起了轩然大波（见下面的推文），他说如果 2008 年 WebAssembly 和 WASI 就存在的话，他就没必要创建 Docker。一时间，关于 WebAssembly 取代 Docker 的讨论此起彼伏。也是从那时起，WebAssembly 正式进入我的视线。\nIf WASM+WASI existed in 2008, we wouldn\u0026#39;t have needed to created Docker. That\u0026#39;s how important it is. Webassembly on the server is the future of computing. A standardized system interface was the missing link. Let\u0026#39;s hope WASI is up to the task! https://t.co/wnXQg4kwa4\n— Solomon Hykes / @shykes@hachyderm.io (@solomonstre) March 27, 2019 到了 2021 年，网上突然多了很多关于 WebAssembly 的炒作文章，包括我长期关注的 Istio 也在当年发布的 1.12 版本开始支持 WebAssembly（见 Istio 1.12 引入 Wasm 插件配置 API 用于扩展 Istio 生态 ），通过引入 WasmPlugin API，使开发人员更方便扩展服务网格和网关。\n最近我看到一篇介绍 WebAssembly 在 2023 年有哪些新趋势的文章 ，文章的作者 Matt Butcher 颇有来头，他是 WebAssembly Cloud 公司 Fermyon 的联合创始人和 CEO，也是 Helm、Brigade、CNAB、OAM、Glide 和 Krustlet 的原始创建者之一。通过他的介绍让我笃定，WebAssembly 是一门颇有前景的技术，是时候学习它了。\nWebAssembly 的市场前景 更何况 WebAssembly 的应用领域越来越广，像 WasmEdge 这样的公司正在使用 Tensorflow 来突破可以使用 Wasm 运行的边界。Fermyon 正在构建用于微服务的 WebAssembly 工具，而 Vercel 、Fastly 、Shopify 和 Cloudflare 使用 WebAssembly 在边缘运行代码。Figma （2022 年以 200 亿美元被 Adobe 公司收购）正在使用 WebAssembly 为其应用程序在浏览器中提供更高的性能，而他们的新母公司 Adobe 正在使用 WebAssembly 将他们的桌面应用程序带到 Web。\n如何学习 WebAssembly？ 为了学习 WebAssembly，我制定了以下学习目标：\n了解 WebAssembly 的基本概念，包括它是什么、为什么要使用它、如何在浏览器中运行它；\n学习 WebAssembly 的语言，这是一种类似于汇编语言的低级语言，可以编译成二进制文件；\n使用工具将代码编译成 WebAssembly 格式；\n在 JavaScript 中调用 WebAssembly 模块；\n学习 WebAssembly 的其他特性，如内存管理、多线程和 WebAssembly System Interface（WASI）；\n了解 WebAssembly 如何增强安全防护；\n学习 WebAssembly 的最佳实践，如代码优化和调试；\n在 Istio 中开发 WebAssembly 插件；\n学习使用 WebAssembly 开发的开源项目；\nWebAssembly 参考资料 下面列出了一些有助于学习 WebAssembly 的参考资料，包括网站和图书：\nWebAssembly 官网 WebAssembly 教程 WebAssembly 在线编译器 WebAssembly 学习资源 《WebAssembly：权威指南：安全、快速和可移植的代码，第 1 版，2022 年 1 月》 wazero: the zero dependency WebAssembly runtime for Go developers 总结 WebAssembly 不仅是用于浏览器端有效弥补 JavaScript 缺陷的一门技术，凭借它的小巧、高效和可移植性在后端也会有很广泛的应用。2023 年，让我们一起来学习 WebAssembly 吧！我也会适时得在云原生社区中创建 Wasm 学习小组，欢迎大家加入云原生社区 共同交流学习。\n参考 Istio 1.12 引入 Wasm 插件配置 API 用于扩展 Istio 生态 - cloudnative.to 在 Istio 中引入 Wasm 意味着什么？- cloudnative.to 2023 年 WebAssembly 技术五大趋势预测 - cloudnative.to ","relpermalink":"/blog/why-you-should-learn-wasm/","summary":"2023 年我决定学习一门新技术——WebAssembly。","title":"为什么要学习 WebAssembly？"},{"content":"本文将以 Bookinfo 应用为例，为 Istio 的入口网关设置一个真实的 TLS/SSL 证书。我们将使用 Let’s Encrypt、cert-manager 来管理 Istio 中入口网关的证书。\n准备 请先参考 Istio 文档 安装 Istio 和 Bookinfo 应用 ，笔者在 GKE 中安装了 Istio 1.16。\n本文中安装的各组件版本信息如下：\nKubernetes 1.24.7 Istio 1.16 Gateway API 0.5.1 cert-manager 1.10.1 架构 本实验中包含以下关键组件：\n使用 Cloudflare 提供 DNS 解析 使用 Let’s Encrypt 创建证书 使用 cert-manager 自动申请和续期证书 使用 Gateway API 来创建入口网关 所有组件部署在 GKE 中 图 1 展示了本实验的架构以及流量路由过程。\n图 1：Istio 入口网关证书挂载模式 流量路由过程如下：\n在 Gateway 创建完成后通过 LoadBalancer 暴露网关 IP，将该 IP 配置在 DNS 解析记录中； Gateway 通过注解引用 ACME Issuer ； ACME Issuer 向 cert-manager 发送请求证书（order 和 challenge ），并使用 DNS01 Challenge Provider ； cert-manager 向 ACME 服务器 Let’s Encrypt 请求证书并创建 Kubernetes Secret； 在 Gateway 中通过应用 Secret 挂载 TLS 证书； HTTPRoute 将入口流量路由到 productpage 服务； ACME Issuer Istio 包含了开箱即用的 mTLS 支持，你也可以使用自定义 CA 或 SPIRE 来管理集群内证书，但是对于入口网关的证书，就需要我们单独设置。你可以手动为入口网关配置证书 ，不过管理起来会比较麻烦，因为你需要负责证书的轮换以防止证书过期，或使用 Let’s Encrypt 这样的 ACME Issuer 来自动化管理证书。\nACME (Automated Certificate Management Environment) Issuer 是一种认证机构，可以使用 ACME 协议为客户端申请和管理证书。ACME 是一种用于自动化 SSL/TLS 证书颁发和管理的开放协议。它通常用于网站或其他在线服务的证书管理，以确保安全连接。\nLet’s Encrypt 是一个非营利性的 ACME Issuer，可以为网站提供免费的 SSL/TLS 证书。它的目标是使加密技术普及化，并帮助提升网络安全水平。Let’s Encrypt 使用 ACME 协议与客户端通信，可以为客户端申请和管理证书。ACME 协议是开放的，因此任何机构都可以成为 ACME Issuer，只要它们遵守 ACME 协议的规定。\n详细步骤 安装 Gateway API：\nkubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v0.5.1/standard-install.yaml 安装 cert-manager\nkubectl apply -f https://gist.githubusercontent.com/rootsongjc/78487acdea70a3c27c1a1b794546d031/raw/0df08b91dfaff6412bbd891ccedffaa882a9a99f/cert-manager.yaml 它为 cert-manager Deployment 增加了以下启动项：\nargs: - --feature-gates=ExperimentalGatewayAPISupport=true 在 Cloudflare 中创建一个名为 lets-encrypt-token 的 API token，自定义模板设置如下：\nPermissions：\nZone - DNS - Edit Zone - Zone - Read Zone Resources:\nInclude - All Zones 将该 token 存储在一个 Secret 中：\nkubectl apply -n default -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Secret metadata: name: cloudflare-api-token-secret namespace: istio-system type: Opaque stringData: api-token: \u0026lt;API Token\u0026gt; EOF 注意 本次实验中该 Token 实际上并没起到作用，正常情况下 cert-manager 会通过 Cloudflare API 与 Cloudflare 交互，为我们配置 DNS 记录。该问题还需要进一步排查。 配置 Let’s Encrypt Issuer：\nkubectl apply -n default -f - \u0026lt;\u0026lt;EOF apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt spec: acme: email: rootsongjc@gmail.com server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: name: lets-encrypt-issuer-account-key solvers: - dns01: cloudflare: apiTokenSecretRef: name: cloudflare-api-token-secret key: api-token selector: dnsNames: - \u0026#39;bookinfo.jimmysong.io\u0026#39; EOF 配置 Gateway：\nkubectl apply -n default -f - \u0026lt;\u0026lt;EOF apiVersion: gateway.networking.k8s.io/v1beta1 kind: Gateway metadata: name: bookinfo-gateway annotations: cert-manager.io/issuer: letsencrypt spec: gatewayClassName: istio listeners: - name: http hostname: bookinfo.jimmysong.io port: 443 protocol: HTTPS allowedRoutes: namespaces: from: Same tls: mode: Terminate certificateRefs: kind: Secret group: \u0026#34;\u0026#34; name: bookinfo-tls EOF 在 Gateway 创建完成后，会在 default 命名空间中创建一个网关 Pod 以及 LoadBalancer 资源的服务。\n查看 default 命名空间中的 Secret，你会发现 bookinfo-tls，它是由 cert-manager 创建的，查看该 Secret 中保存的证书，你将会看到由 Let’s Encrypt 颁发的证书信任链：\nbookinfo.jimmysong.io ISRG Root X1 DST Root CA X3 配置 HTTPRoute：\nkubectl apply -n default -f - \u0026lt;\u0026lt;EOF apiVersion: gateway.networking.k8s.io/v1beta1 kind: HTTPRoute metadata: name: bookinfo spec: parentRefs: - name: bookinfo-gateway rules: - matches: - path: type: Exact value: /productpage - path: type: PathPrefix value: /static - path: type: Exact value: /login - path: type: Exact value: /logout - path: type: PathPrefix value: /api/v1/products backendRefs: - name: productpage port: 9080 EOF 在 Cloudflare 中配置域名记录：将网关服务的外网 IP 及域名 bookinfo.jimmysong.io 添加到 Cloudflare 的 DNS 记录中就可以实现域名解析。\n注意 本实验中发现网关 Pod 并没有挂载 bookinfo-tls Secret 中的证书，我们只好通过 Cloudflare 来配置 TLS 证书：为网站开启全（严格）SSL/TLS，这将使用 Cloudflare 颁发的 TLS 证书。 在浏览器中访问 https://bookinfo.jimmysong.io/productpage 就可以访问 bookinfo 应用了。\n总结 本次实验虽然实现了网关的 TLS 加密，也为网关生成了 TLS 证书，但实际上网关使用的是 Cloudflare 颁发的证书。这并不是我们最初的目标，即使用 ACME Server（Let’s Encrypt）为网关颁发的证书。为什么网关 Pod 没有挂载我们应用的 Secret 中的证书，Cloudflare DNS01 Challenge Provider 为什么没有生效，这两个问题还需要我们进一步调查。\n参考 Acquire SSL Certificates In Kubernetes From Let’s Encrypt With Cert-Manager - thinktecture.com How To Secure Kubernetes NGINX Ingress With Cert-Manager Securing gateway.networking.k8s.io Gateway Resources - cert-manager.io ","relpermalink":"/blog/secure-ingress-gateway-of-istio/","summary":"本文就将使用 Let's Encrypt、cert-manager 来管理 Istio 中入口网关的证书。","title":"使用 cert-manager ACME Issuer 为 Istio 中的入口网关设置证书"},{"content":"云原生社区之可观测性峰会回来了！云原生社区将于 2023 年 4 月 15 号（星期六）在北京举办可观性峰会。现在开始招募讲师及赞助商，名额有限，请尽快提交！本次活动的详细日程及报名方式会在后续公布，敬请关注！\n目前可观性峰会已经完成了前期筹备，现面向社区征集演讲议题及赞助商。\n议题提交截止时间为北京时间 2023 年 3 月 10 日晚上 11:59。\n议题及赞助商申请请见此链接 。\n","relpermalink":"/notice/observability-summit-2023-beijing/","summary":"正在征集议题及赞助商。","title":"可观测性峰会 2023 北京"},{"content":"在上一篇博客 中我介绍了 Istio 中是如何管理证书的，这篇文章将指导你如何使用外置 CA，通过集成 SPIRE 和 cert-manager 实现细粒度的证书管理和自动证书轮换。\n如果你还不了解什么是 SPIRE 以及为什么我们要使用 SPIRE，推荐你阅读以下内容：\n为什么 Istio 要使用 SPIRE 做身份认证？ 如何在 Istio 中集成 SPIRE？ 零信任的基石：使用 SPIFFE 为基础设施创建通用身份 证书签发管理流程简介 下图展示了本文中使用的基于 cert-manager 和 SPIRE 的证书信任链：\n基于 cert-manager、SPIRE 的证书信任链 从图中你可以看出：\ncert-manager 作为根 CA 为 istiod 和 SPIRE 颁发证书，我们使用了自签名 Issuer ，你还可以为其配置使用 Let’s Encrypt、Vault、Venafi 等内置 Issuer，或其他外置的 Issuer；另外你也可以选择使用其他 UpstreamAuthority ，例如 Vault、SPIRE 联邦等； SPIRE 为 Istio 网格内工作负载和 Ingress Gateway、Egress Gateway 颁发 SVID 证书，用于服务间 mTLS； 其中网格外访问 Ingress Gateway 时的使用的证书及 Egress Gateway 访问网格外服务使用的证书需要额外配置； 下图展示了在 Istio 中集成 SPRIE 和 cert-manger 后的证书颁发和更新流程。\nIstio 集成 SPIRE 和 cert-manager 后的证书颁发和更新流程 SPIRE Server 中的 Kubernetes Workload Registar 自动注册 Kubernetes 中的工作负载，为所有工作负载生成 SPIFFE 标准的身份； cert-manager 为 istiod 颁发并管理 CA 证书； 工作负载中的 Envoy 代理通过 UNIX Domain Socket（UDS）通过 SDS API 向同节点上的 SPIRE Agent 发送 CSR 请求； SPIRE Agent 向 SPIRE Server 发送 CSR； SPIRE Server 向 SPIRE Agent 返回签名后的证书； SPIRE Agent 向工作负载返回签名后的证书； SPIRE 负责工作负载的证书管理和更新； 在了解了大致流程后，下面我们将从手安装各个组件。\n安装 cert-manager 运行下面的命令安装 cert-manager，我们将使用它来实现自动证书轮换：\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.10.1/cert-manager.yaml 根 CA 是用自签名证书，运行下面的命令配置根 CA：\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: selfsigned namespace: cert-manager spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: selfsigned-ca namespace: cert-manager spec: isCA: true duration: 21600h secretName: selfsigned-ca commonName: certmanager-ca subject: organizations: - cert-manager issuerRef: name: selfsigned kind: Issuer group: cert-manager.io --- apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: selfsigned-ca spec: ca: secretName: selfsigned-ca EOF 然后为 istiod 配置证书：\nkubectl create namespace istio-system cat \u0026lt;\u0026lt; EOF | kubectl apply -f - --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: cacerts namespace: istio-system spec: secretName: cacerts duration: 1440h renewBefore: 360h commonName: istiod.istio-system.svc isCA: true usages: - digital signature - key encipherment - cert sign dnsNames: - istiod.istio-system.svc issuerRef: name: selfsigned-ca kind: ClusterIssuer group: cert-manager.io EOF 现在我们已经安装好了 cert-manager，并创建了名为 selfsigned-ca 的 clusterIssuer，接下来，我们来安装 SPIRE 并将 cert-manager 作为 SPIRE 的 UpstreamAuthority 。\n安装 SPIRE 运行下面的命令快速安装 SPIRE：\nkubectl apply -f https://gist.githubusercontent.com/rootsongjc/5dac0518cc432cbf844114faca74aa40/raw/814587f94bbef8fb1dd376282249dcb2a8f7fa1b/spire-with-cert-manager-upstream-authority-quick-start.yaml 该 YAML 文件比起 Istio 1.16 安装包中的 samples/security/spire/spire-quickstart.yaml 文件增加了对 cert-manager 的适配，如：\n为 spire-server-trust-role ClusterRole 增加了对 cert-manager.io API 组的权限； 在 SPIRE Server 的配置中增加了 UpstreamAuthority \u0026#34;cert-manager\u0026#34; 配置； 注意 SPIRE Server 配置中的 trust_domain 应与安装 Istio 时指定的 TRUST_DOMAIN 环境变量的值保持一致。 该命令中会安装 Kubernetes Workload Registrar ，自动注册 Kubernetes 中的工作负载。所有工作负载将根据其服务账户注册 SPIFFE 标准的服务身份格式 spiffe://\u0026lt;trust-domain\u0026gt;/ns/\u0026lt;namespace\u0026gt;/sa/\u0026lt;service-account\u0026gt;。\n如果你想调整 SPIRE CA 和 SVID 证书的 TTL，可以在 SPIRE Server 的配置中修改 ca_ttl（默认 24h）和 default_svid_ttl（默认 1h），详见 SPIRE Server 配置 。\n安装 Istio 运行下面的命令安装 Istio 并启用 CA 证书自动轮换：\nistioctl operator init istioctl install --skip-confirmation -f - \u0026lt;\u0026lt;EOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system spec: profile: default meshConfig: # 信任域应与 SPIRE Server 中配置的信任域相同 trustDomain: example.org values: global: # 自定义 sidecar 模板 sidecarInjectorWebhook: templates: spire: | spec: containers: - name: istio-proxy volumeMounts: - name: workload-socket mountPath: /run/secrets/workload-spiffe-uds readOnly: true volumes: - name: workload-socket csi: driver: \u0026#34;csi.spiffe.io\u0026#34; readOnly: true components: pilot: k8s: env: # 如果启用，如果用户引入新的中间插件 CA，用户不需要重新启动 istiod 来获取证书。Istiod 会获取新添加的中间插件 CA 的证书并更新它。不支持插入新的 Root-CA。 - name: AUTO_RELOAD_PLUGIN_CERTS value: \u0026#34;true\u0026#34; ingressGateways: - name: istio-ingressgateway enabled: true label: istio: ingressgateway k8s: overlays: - apiVersion: apps/v1 kind: Deployment name: istio-ingressgateway patches: - path: spec.template.spec.volumes.[name:workload-socket] value: name: workload-socket csi: driver: \u0026#34;csi.spiffe.io\u0026#34; readOnly: true - path: spec.template.spec.containers.[name:istio-proxy].volumeMounts.[name:workload-socket] value: name: workload-socket mountPath: \u0026#34;/run/secrets/workload-spiffe-uds\u0026#34; readOnly: true EOF 因为我们要使用 Istio Operator 中声明的 spire 模板来部署工作负载，因此我们运行下面的命令部署 Bookinfo 应用：\nistioctl kube-inject -f bookinfo-with-spire-template.yaml | kubectl apply -f - 注意：上面命令中使用的 bookinfo-with-spire-template.yaml 文件可以在这个 Gist 中找到，与 Istio 安装包中的 samples/bookinfo/platform/kube/bookinfo.yaml 文件唯一的区别就是每个 Deployment 的 template 中都增加了以下注解：\nannotations: inject.istio.io/templates: \u0026#34;sidecar,spire\u0026#34; 使用下面的命令可以检查 SPIRE 是否给工作负载颁发了身份证明：\nkubectl exec -i -t spire-server-0 -n spire -c spire-server -- /bin/sh -c \u0026#34;bin/spire-server entry show …","relpermalink":"/blog/cert-manager-spire-istio/","summary":"本文介绍如何集成 SPIRE 和使用 cert-manager 实现细粒度的证书管理以及证书轮换。","title":"使用 cert-manager 和 SPIRE 管理 Istio 中的证书"},{"content":"我在如何理解 Istio 中的 mTLS 流量加密 这篇文章中提出流量加密的关键是证书管理。我们可以使用 Istio 中内置了 CA（证书授权机构）也可以使用自定义 CA 来管理网格内的证书。这篇博客将为你讲解 Istio 是如何进行证书管理的。\n什么是证书？ 在介绍 Istio 的证书管理方式之前，我们先来了解一下什么是证书。若你已了解证书的作用及原理，请直接跳到 Istio 中的证书管理部分。\n证书（Certificate），又称电子证书，是用于身份认证和加密通信的一种数字证明文件。在了解 Istio 的证书管理之前，我们先来了解一下什么是证书。如果你已经了解了证书，可以跳过这一节。\n证书有很多类别，本文中的证书特指的是 X.509 V3 证书 。X509 证书是一种常见的数字证书格式，用于在计算机网络中识别实体的身份。X509 是公钥基础设施（PKI）的国际标准，主要用于身份认证和信息加密，例如 TLS。X.509 证书中包含了个人、组织或计算机的身份信息和公钥。V3 是它的最新版本。它主要用于在客户端和服务器之间进行安全通信，例如在通过 HTTPS 访问网站时。x509 证书通常由 CA 颁发，该 CA 会验证实体的身份，并将这些信息编码到证书中。当客户端连接到服务器时，服务器会向客户端提供其 x509 证书，客户端会验证证书的有效性，并通过该证书来识别服务器的身份。通过这种方式，双方可以安全地进行通信，并确保数据传输的完整性和保密性。\n哈希函数 谈到证书就不得不提哈希（Hash）函数，因为证书的内容会使用哈希函数进行哈希处理，然后用证书颁发者的私钥进行签名。这样，当收到一份证书时，接收者就可以使用证书颁发者的公钥来验证证书的合法性。\n哈希函数是一种将任意长度的输入（也称为消息）映射为固定长度的输出的函数。这个输出也称为哈希值或消息摘要。\n哈希函数有许多用途，其中一个重要的用途是密码存储。当用户在系统中设置密码时，通常不会将真实的密码直接存储在系统中。相反，会将密码进行哈希处理，并将哈希值存储在系统中。当用户登录时，系统会将用户输入的密码进行哈希处理，然后与存储的哈希值进行比较。如果两者相同，则证明用户输入的密码正确，反之则错误。\n哈希函数有很多种类型，例如 MD5、SHA-1 等。这些函数都有一些共同的特点，比如输出固定长度、不可逆、散列冲突少等。\n哈希函数的安全性与其输出的长度有关。一般来说，输出长度越长，哈希函数就越安全。但是，输出长度越长，哈希处理的时间就越长，因此要在安全性和效率之间进行平衡。\n证书的作用 证书的用途广泛，凡是需要加密、认证、授权的场景都会用到它，比如：\n在 Kubernetes 中你需要给各个组件配置证书，你可以选择手动生成证书 ； Istio 中为实现自动 mTLS 给各个工作负载颁发的证书； 访问 HTTPS 网站所用到的证书等； 证书就像是由权威机构印发的名片，供使用者表明其身份，同时还可以为信息加密，保证通信的安全性和完整性。下图展示的是 TLS 通信的大概步骤，其中证书承担了证明服务器身份和加密通信的职责。\n下面以一个网站的 HTTP 链接为例，颁发数字证书、验证和加密通信的过程如下图所示。\nTLS 证书颁发和校验过程 详细步骤如下：\n服务器（网站所有者）向 CA 提交证书签名请求； CA 验证服务器的身份和网站的真实性后为服务器颁发数字证书，服务器安装该证书，以便访问者能够验证网站的安全性； 用户通过浏览器（客户端）向网站发送请求； 服务器向客户端返回 TLS 证书； 客户端向 CA 验证证书的有效性，若有效则建立连接，若无效则提示用户拒绝连接； 客户端生成一对随机的公钥和私钥； 客户端将并将自己的公钥发送给服务端； 服务端使用客户端的公钥加密消息； 服务端将加密后的数据发送给客户端； 客户端使用自己的私钥解密服务端发送的数据； 至此，双方建立了一个安全的通道，并可以通过该通道进行双向加密的数据传输。\n如何生成证书？ 你可以通过以下开源工具生成 X.509 证书：\nEasy-RSA ：一个简单地命令行工具，由 OpenVPN 项目组维护，使用 EasyRSA 可以轻松地为 OpenVPN 网络生成安全的证书和密钥； OpenSSL ：由个人发起于 1995 年，现由独立组织维护，只提供命令行工具； CFSSL ：由 CloudFlare 开发和维护，不仅仅是一个用于生成证书的命令行工具，还可以作为 PKI 服务器； BoringSSL ：Google 开发和维护的 OpenSSL 分支，已用于 Chrome 浏览器和安卓操作系统； 因为可能大多数人都对 OpenSSL 比较熟悉，所以下文中我们将使用 OpenSSL 来创建证书。\n证书的组成 下面以 X.509 V3 证书为例，讲解数字证书的组成。下面是该证书的一个示例：\nCertificate: Data: Version: 3 (0x2) Serial Number: fc:c6:18:2e:20:bd:27:b5:6b:60:bc:47:23:6b:8b:d9 Signature Algorithm: sha256WithRSAEncryption Issuer: O=cluster.local Validity Not Before: Dec 15 07:25:32 2022 GMT Not After : Dec 16 07:27:32 2022 GMT Subject: Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public-Key: (2048 bit) Modulus: 00:eb:40:16:87:6c:17:5a:9c:b2:91:00:94:d1:31: 37:bb:d7:1e:e6:06:1c:a1:c1:35:64:54:82:54:af: b8:4b:40:6f:e0:73:86:4e:c1:c6:75:b8:c8:30:ac: 69:16:e8:68:25:cb:dd:e8:53:55:ec:7a:bd:a9:d3: 42:44:7f:e5:f5:52:dd:99:ae:c2:1a:a2:06:1f:be: 1b:e6:3e:69:87:a3:fc:91:21:39:b0:a7:67:11:f2: 3c:55:c6:4b:04:15:1b:ff:49:14:88:c4:58:87:79: 96:5b:6e:00:1c:c1:e7:2c:53:0c:d1:77:dc:a8:82: cc:fa:26:c1:bb:6c:df:a8:43:0c:b7:cc:f0:a2:11: 9b:e8:3f:8a:1d:ed:2a:ff:1f:d1:03:eb:8a:b9:98: 40:18:83:24:4f:14:95:a3:59:ef:67:0f:35:6d:ae: 91:81:b2:04:02:16:80:d1:39:bd:70:cf:0f:cb:9a: 81:39:d9:fe:52:a5:cf:79:4f:a3:69:d8:0d:39:6a: 48:24:8d:2b:88:04:fa:81:de:65:50:7d:1a:3d:cd: f3:1c:42:63:29:75:a0:9b:8e:16:44:3a:89:d6:2b: 41:76:65:a5:2e:c8:b6:d2:89:42:5d:21:24:33:30: f0:9d:0b:4d:cf:78:d5:45:2d:49:5a:55:50:98:93: 03:f5 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Authority Key Identifier: keyid:BA:31:8A:9C:ED:EB:49:D2:54:09:98:D9:4C:3A:9C:42:D0:64:8D:B2 X509v3 Subject Alternative Name: critical URI:spiffe://cluster.local/ns/default/sa/httpbin Signature Algorithm: sha256WithRSAEncryption 90:7f:cb:6f:0b:16:cb:59:7d:f4:87:a7:5a:38:fa:0a:16:d8: 83:0d:b1:36:77:a2:4a:fe:38:52:ab:49:e9:89:50:1a:4c:e9: 94:07:37:7f:27:bc:2c:ce:c1:d2:33:75:5d:b6:ab:ae:cb:2e: 71:f4:22:c0:40:15:27:02:75:c1:32:2e:83:49:73:6c:9a:ea: 04:ef:55:2d:8d:71:30:9b:e4:30:dd:95:20:0d:7c:d2:f4:30: 2f:07:2e:9f:53:37:e6:3d:14:c7:41:f4:09:8b:a3:76:56:c7: c7:92:0f:fc:17:5a:5a:32:6c:9e:87:18:2e:51:75:54:68:d8: 01:c1:07:cc:b0:35:bf:0b:6c:62:a6:5b:23:61:35:c8:4f:7f: e7:1f:a0:e9:11:44:a6:17:52:4d:00:40:21:de:63:ee:02:c8: 2b:5d:a1:7a:5d:7f:d5:d3:c1:7d:5f:00:40:e8:80:8d:cc:e9: 8a:c6:b4:98:fe:7a:7d:37:0c:6f:4c:31:91:7a:79:30:84:cd: 01:a7:14:f6:1b:33:8f:0f:50:1c:36:38:6b:24:da:cf:49:8a: 5b:28:cf:27:76:e1:a5:c7:e6:d5:6e:d8:36:85:aa:1f:a5:ac: fa:f1:2e:a2:36:2e:25:b0:71:24:d1:3e:d5:e5:19:2b:0b:6f: b7:17:e4:75 证书中各个字段的含义如下：\n证书版本（Version）：表示证书的版本号。 序列号（Serial Number）：表示证书的唯一序列号。 签名算法（Signature Algorithm）：表示证书的签名算法，例如 RSA、DSA 或 ECDSA 等。 证书颁发机构（Issuer）：表示颁发该证书的可信的第三方机构的名称。 有效期（Validity）：表示证书的有效期，包括证书生效日期（Not Before）和证书失效日期（Not After）。 使用者（Subject）：表示证书所有者的名称。 公钥信息（Subject Public Key Info）：表示证书所有者的公钥以及公钥的算法。 扩展（Extensions）：表示证书的扩展信息，包括： 密钥用法扩展（Key Usage Extension）：表示证书所有者的密钥可用于哪些操作，例如数字签名、密钥加密等。 扩展密钥用法扩展（Extended Key Usage Extension）：表示证书所有者的密钥可用于哪些扩展的操作，例如 TLS Web Server Authentication、TLS Web Client Authentication 等。 基本约束扩展（Basic Constraints Extension）：表示证书所有者是否是证书颁发机构的下级机构，以及是 …","relpermalink":"/blog/istio-certificates-management/","summary":"本文介绍了数字证书和 Istio 中的证书管理方式。","title":"Istio 中的证书管理方式介绍"},{"content":"在云原生应用中，一次请求往往需要经过一系列的 API 或后台服务处理才能完成，这些服务有些是并行的，有些是串行的，而且位于不同的平台或节点。那么如何确定一次调用的经过的服务路径和节点以帮助我们进行问题排查？这时候就需要使用到分布式追踪。\n本文将向你介绍：\n分布式追踪的原理 如何选择分布式追踪软件 在 Istio 中如何使用分布式追踪 以 Bookinfo 和 SkyWalking 为例说明如何查看分布式追踪数据 分布式追踪基础 分布式追踪是一种用来跟踪分布式系统中请求的方法，它可以帮助用户更好地理解、控制和优化分布式系统。分布式追踪中用到了两个概念：TraceID 和 SpanID。\nTraceID 是一个全局唯一的 ID，用来标识一个请求的追踪信息。一个请求的所有追踪信息都属于同一个 TraceID，TraceID 在整个请求的追踪过程中都是不变的；\nSpanID 是一个局部唯一的 ID，用来标识一个请求在某一时刻的追踪信息。一个请求在不同的时间段会产生不同的 SpanID，SpanID 用来区分一个请求在不同时间段的追踪信息；\nTraceID 和 SpanID 是分布式追踪的基础，它们为分布式系统中请求的追踪提供了一个统一的标识，方便用户查询、管理和分析请求的追踪信息。\n分布式追踪原理图 下面是分布式追踪的过程：\n当一个系统收到请求后，分布式追踪系统会为该请求分配一个 TraceID，用于串联起整个调用链； 分布式追踪系统会为该请求在系统内的每一次服务调用生成一个 SpanID 和 ParentID，用于记录调用的父子关系，没有 ParentID 的 Span 将作为调用链的入口； 每个服务调用过程中都要传递 TraceID 和 SpanID； 在查看分布式追踪时，通过 TraceID 查询某次请求的全过程； Istio 如何实现分布式追踪 Istio 中的分布式追踪是基于数据平面中的 Envoy 代理实现的。服务请求在被劫持到 Envoy 中后，Envoy 在转发请求时会附加大量 Header，其中与分布式追踪相关的有：\n作为 TraceID：x-request-id 用于在 LightStep 追踪系统中建立 Span 的父子关系：x-ot-span-context 用于 Zipkin，同时适用于 Jaeger、SkyWalking，详见 b3-propagation ： x-b3-traceid x-b3-spanid x-b3-parentspanid x-b3-sampled x-b3-flags b3 用于 Datadog： x-datadog-trace-id x-datadog-parent-id x-datadog-sampling-priority 用于 SkyWalking：sw8 用于 AWS X-Ray：x-amzn-trace-id 关于这些 Header 的详细用法请参考 Envoy 文档 。\nEnvoy 会在 Ingress Gateway 中为你产生用于追踪的 Header，不论你的应用程序使用何种语言开发，Envoy 都会将这些 Header 转发到上游集群。但是，你还要对应用程序代码做一些小的修改，才能为使用分布式追踪功能。这是因为应用程序无法自动传播这些 Header，可以在程序中集成分布式追踪的 Agent，或者在代码中手动传播这些 Header。Envoy 会将追踪数据发送到 tracer 后端处理，然后就可以在 UI 中查看追踪数据了。\n例如在 Bookinfo 应用中的 Productpage 服务，如果你查看它的代码可以发现，其中集成了 Jaeger 客户端库，并在 getForwardHeaders (request) 方法中将 Envoy 生成的 Header 同步给对 Details 和 Reviews 服务的 HTTP 请求：\ndef getForwardHeaders(request): headers = {} # 使用 Jaeger agent 获取 x-b3-* header span = get_current_span() carrier = {} tracer.inject( span_context=span.context, format=Format.HTTP_HEADERS, carrier=carrier) headers.update(carrier) # 手动处理非 x-b3-* header if \u0026#39;user\u0026#39; in session: headers[\u0026#39;end-user\u0026#39;] = session[\u0026#39;user\u0026#39;] incoming_headers = [ \u0026#39;x-request-id\u0026#39;, \u0026#39;x-ot-span-context\u0026#39;, \u0026#39;x-datadog-trace-id\u0026#39;, \u0026#39;x-datadog-parent-id\u0026#39;, \u0026#39;x-datadog-sampling-priority\u0026#39;, \u0026#39;traceparent\u0026#39;, \u0026#39;tracestate\u0026#39;, \u0026#39;x-cloud-trace-context\u0026#39;, \u0026#39;grpc-trace-bin\u0026#39;, \u0026#39;sw8\u0026#39;, \u0026#39;user-agent\u0026#39;, \u0026#39;cookie\u0026#39;, \u0026#39;authorization\u0026#39;, \u0026#39;jwt\u0026#39;, ] for ihdr in incoming_headers: val = request.headers.get(ihdr) if val is not None: headers[ihdr] = val return headers 关于 Istio 中分布式追踪的常见问题请见 Istio 文档 。\n分布式追踪系统如何选择 分布式追踪系统的原理类似，市面上也有很多这样的系统，例如 Apache SkyWalking 、Jaeger 、Zipkin 、LightStep 、Pinpoint 等。我们将选择其中三个，从多个维度进行对比。之所以选择它们是因为：\n它们是当前最流行的开源分布式追踪系统； 都是基于 OpenTracing 规范； 都支持与 Istio 及 Envoy 集成； 类别 Apache SkyWalking Jaeger Zipkin 实现方式 基于语言的探针、服务网格探针、eBPF agent、第三方指标库（当前支持 Zipkin） 基于语言的探针 基于语言的探针 数据存储 ES、H2、MySQL、TiDB、Sharding-sphere、BanyanDB ES、MySQL、Cassandra、内存 ES、MySQL、Cassandra、内存 支持语言 Java、Rust、PHP、NodeJS、Go、Python、C++、.NET、Lua Java、Go、Python、NodeJS、C#、PHP、Ruby、C++ Java、Go、Python、NodeJS、C#、PHP、Ruby、C++ 发起者 个人 Uber Twitter 治理方式 Apache Foundation CNCF CNCF 版本 9.3.0 1.39.0 2.23.19 Star 数量 20.9k 16.8k 15.8k 分布式追踪系统对比表（数据截止时间 2022-12-07） 虽然 Apache SkyWalking 的 Agent 支持的语言没有 Jaeger 和 Zipkin 多，但是 SkyWalking 的实现方式更丰富，并且与 Jaeger、Zipkin 的追踪数据兼容，开发更为活跃，且为国人开发，中文资料丰富，是构建遥测平台的最佳选择之一。\n实验 参考 Istio 文档 来安装和配置 Apache SkyWalking。\n环境说明 以下是我们实验的环境：\nKubernetes 1.24.5 Istio 1.16 SkyWalking 9.1.0 安装 Istio 安装之前可以先检查下环境是否有问题：\n$ istioctl experimental precheck ✔ No issues found when checking the cluster. Istio is safe to install or upgrade! To get started, check out https://istio.io/latest/docs/setup/getting-started/ 然后使用 istioctl 安装 Istio 同时配置发送追踪信息的目的地为 SkyWalking：\n# 将安装配置保存到文件中 cat\u0026lt;\u0026lt;EOF\u0026gt;istio-install.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system name: istio-with-skywalking spec: meshConfig: defaultProviders: tracing: - \u0026#34;skywalking\u0026#34; enableTracing: true extensionProviders: - name: \u0026#34;skywalking\u0026#34; skywalking: service: tracing.istio-system.svc.cluster.local port: 11800 EOF # 使用 istioctl 安装 Istio istioctl install -f istio-install.yaml 部署 Apache SkyWalking Istio 1.16 支持使用 Apache SkyWalking 进行分布式追踪，执行下面的代码安装 SkyWalking：\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/master/samples/addons/extras/skywalking.yaml 它将在 istio-system 命名空间下安装：\nSkyWalking OAP (Observability Analysis Platform) ：用于接收追踪数据，支持 SkyWalking 原生数据格式，Zipkin v1 和 v2 以及 Jaeger 格式。 UI ：用于查询分布式追踪数据。 关于 SkyWalking 的详细信息请参考 SkyWalking 文档 。\n部署 Bookinfo 应用 执行下面的命令安装 bookinfo 示例：\nkubectl label namespace default istio-injection=enabled kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml 然后给 productpage 服务发送请求，如果你是在 Kind 中安装的 Istio，则默认的 IngressGateway 将是 LoadBalancer 类型，你可以使用端口转发的方式来暴露网关。\nkubectl port-forward -n istio-system svc/istio-ingressgateway 8081:80 通过 http://localhost:8081/productpage 就可以访问到 productpage 服务。多访问几次来制造一些流量，后面我们就能在 SkyWalking UI 上看到拓扑图了。\n打开 SkyWalking UI：\nistioctl dashboard skywalking SkyWalking 的 General Service 页面展示了 bookinfo 应用中的所有服务。\nSkyWalking 的 General Service 页面 你还可以看到实例、端点、拓扑、追踪等信息。例如下图展示了 bookinfo 应用的服务拓扑。 …","relpermalink":"/blog/distributed-tracing-with-skywalking-in-istio/","summary":"这篇文章将介绍一些关于分布式追踪的基础知识以及如何在 Istio 中使用分布式追踪。","title":"如何在 Istio 中使用 SkyWalking 进行分布式追踪？"},{"content":"Istio 服务网格可以帮助云原生应用实现自动 mTLS，完成网格内的流量加密，有助于缩小云原生部署的攻击面，是构建零信任应用网络的关键框架。为了理解 Istio 中的 mTLS 流量加密，本文将包括以下内容：\n介绍什么是 TLS、mTLS 和 TLS 终止 介绍 Istio 中如何实现 TLS 加密 如何使用 Istio 为 Kubernetes 中的服务实现 mTLS？ 何时需要 mTLS？何时不需要 mTLS？ 什么是 TLS 和 mTLS？ TLS（Transport Layer Security，传输层安全性）是一种广泛采用的安全协议，用于在联网计算机之间建立经过身份验证和加密的链接，旨在促进互联网通信的私密性和数据安全性。TLS 作为 SSL （Secure Socket Layer，安全套接字层）的继任者，实际上是由 SSL 改名而来，因此人们经常将 TLS/SSL 混用，在本文中我们将统称为 TLS。TLS 1.0 发布于 1999 年，最新版本为 1.3（发布于 2018 年 8 月），1.0 和 1.1 版本已弃用。\n我们在浏览网页时看到的 HTTPS 实际上就使用了 TLS，如下图所示。TLS 是建立在 TCP 之上的，作为 OSI 模型中的会话层。为了保证兼容性，TLS 通常使用 443 端口，但是你也可以使用任意端口。\nHTTP vs HTTPS 当客户端需要验证服务端身份，以防中间人攻击同时保证通信安全的情况下，在和服务端通信时会要求 TLS 加密。下图展示了的是 TLS 加密通信的流程。\nTLS 加密通信流程 服务器向受信任的 CA（证书管理机构）申请并获得证书（X.509 证书）； 客户端向服务端发起请求，其中包含客户端支持的 TLS 版本和密码组合等信息； 服务器回应客户端请求并附上数字证书； 客户端验证证书的状态、有效期和数字签名等信息，确认服务器的身份； 客户端和服务器使用共享秘钥实现加密通信； 以上仅是对 TLS 通信流程的一个概要描述，实际的 TLS 握手过程比较复杂，请参考这篇文档 。\n从以上过程中你会发现，证书是代表服务器身份的关键要素，对于互联网公开服务，服务器需要使用权威认证的 CA 颁发的证书，而对于私有环境内部的服务，可以使用 PKI（Private Key Infrastructure，私钥基础设施）来管理证书。\n双向 TLS 或相互 TLS（Mutual TLS 或 mTLS）是指在服务端和客户端之间使用双向加密通道，需要双方相互提供证书并验证对方身份。关于如何在 Kubernetes 中使用 mTLS 请参考这篇文章 。关于 mTLS 的详细介绍请见这篇文章 。\n什么是 TLS 终止？ TLS 终止（TLS Termination）指的是在将 TLS 加密流量传递给 Web 服务器之前对其进行解密的过程。将 TLS 流量卸载到入口网关或专用设备上，可以提高 Web 应用的性能，同时确保加密流量的安全性。一般运行在集群入口处，当流量到达入口处时实施 TLS 终止，入口与集群内服务器之间的通信将直接使用 HTTP 明文，这样可以提高服务性能。\nTLS 终止 Istio 默认在入口网关处终止 TLS，然后再为网格内的服务开启 mTLS。你也可以让流量直通（passthrough）到后端服务处理，例如：\napiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: sample-gateway spec: servers: - port: number: 443 name: https protocol: HTTPS tls: mode: PASSTHROUGH 详见网关 TLS 配置 。\nIstio 中如何实现自动 mTLS？ 下图中展示的是 Istio 安全架构图，从图中可以看到在入口处使用 JWS + TLS 认证和加密，在 Istio 网格内部的所有服务间都开启了 mTLS。\nIstio 安全架构图 Istio 中内置了 CA，使用 xDS 中的 SDS（Secret Discovery Service，秘密发现服务）实现 SVID 证书的签发和轮换。Istio 网格内的 mTLS 流程如下：\nSidecar 代替工作负载向 Istiod 申请证书，Istiod 签发 SVID 证书（该过程比较复杂，我将在今后的博客中说明）； 客户端请求被 Pod 内的 sidecar 拦截； 客户端 sidecar 与服务端 sidecar 开始 mTLS 握手。在握手的同时，客户端 sidecar 中的 JWT 和认证过滤器将对请求的身份进行认证，认证通过后将身份存储在过滤器元数据中，然后请求经过授权过滤器，判断请求权限。 若请求通过了认证与授权，则客户端和服务端开始建立连接进行通信。 Istio 中有三个资源对象可用于配置服务间的认证与授权：\nRequestAuthentication：用于定义服务支持的请求级认证方式，目前只支持 JWT（查看 JWT 组件详解 ）； PeerAuthentication：配置服务间的传输认证模式，如 STRICT、PERMISSIVE 或 DISABLE 等，以开启 mTLS 或明文请求； AuthorizationPolicy：用于授权服务间的流量，定义谁可以做什么？例如主体 A 允许（ALLOW）或拒绝（DENY）来自主体 B 的流量； 如何使用 Istio 为服务开启自动 mTLS？ 你可以在 PeerAuthentication 中指定对目标工作负载实施的 mTLS 模式。对等认证支持以下模式：\nPERMISSIVE：默认值，工作负载可接受双向 TLS 或纯文本流量； STRICT：工作负载仅接受 mTLS 流量； DISABLE：禁用 mTLS。从安全角度来看，除非你有自己的安全解决方案，否则不应禁用 mTLS； UNSET：从父级继承，优先级为服务特定 \u0026gt; 命名空间范围 \u0026gt; 网格范围的设置； Istio 的对等认证默认使用 PERMISSIVE 模式，自动将 mTLS 流量发送到这些工作负载，将纯文本流量发送到没有 sidecar 的工作负载。在将 Kubernetes 服务纳入 Istio 网格后，为了防止服务无法通过 mTLS，我们可以先使用 PERMISSIVE 模式。当我想为某些服务开启严格的 mTLS 模式时，可以使用以下两种方式之一：\n使用 PeerAuthentication 定义流量如何在 sidecar 之间传输； 使用 DestinationRule 定义流量路由策略中的 TLS 设置； 下面以为 default 命名空间下的 reviews 服务设置 mTLS 为例说明。\n使用 PeerAuthentication 为工作负载设置 mTLS 你可以使用 namespace 和 selector 指定某个命名空间下的某个工作负载开启严格的 mTLS。例如下面的配置：\napiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: foo-peer-policy namespace: default spec: selector: matchLabels: app: reviews mtls: mode: STRICT 你也可以给安装 Istio 的命名空间 istio-system 设置严格的 mTLS，那样会为网格中的所有服务开启严格的 mTLS，详细步骤请参考 Istio 文档 。\n使用 DestinationRule 为工作负载设置 mTLS DestinationRule 用于设置流量路由策略，例如负载均衡、异常点检测、TLS 设置等。其中 TLS 设置中包含多种模式 ，使用 ISTIO_MUTUAL 模式可以为工作负载开启 Istio 的自动 TLS，如下所示。\napiVersion: networking.istio.io/v1beta1 kind: DestinationRule metadata: name: reviews namespace: default spec: host: reviews trafficPolicy: tls: mode: ISTIO_MUTUAL 什么时候用 mTLS？ 互联网客户端对 Web 服务的访问，一般使用单向 TLS，即只需要服务端提供身份证明，而不关心客户端的身份。当你需要验证客户端身份时，使用单向 TLS 可以使用密码、token、双因子认证等方式。不过这样的认证方式需要应用程序内部支持，而双向 TLS 是运行在应用程序之外的，不需要多应用逻辑进行修改。\n当你需要正如你在上文中看到的，实施 mTLS 的服务间需要交换证书，当服务数量变大时，就需要管理大量的证书，这需要消耗大量的精力，使用服务网格可以帮助你实现自动 mTLS，彻底解决证书管理的难题。\n什么时候不用 mTLS？ 虽然 mTLS 是确保云原生应用程序服务间通信安全的首选协议，但是应用 mTLS 需要完成复杂的对称加密、解密过程，这将非常耗时且消耗大量的 CPU 资源。对于某些安全级别不高的流量，如果我们在流量入口处终止 TLS，并网格内部仅对针对性的服务开启 mTLS，就可以加快请求响应和减少计算资源消耗。\n另外当有的服务无法获取证书，例如 Kubelet 上使用 HTTP 的健康检查，无法通过 TLS 访问服务内的健康检查端点，这时候就需要为 Pod 禁用探针重写 。\n最后当网格中的服务访问一些外部服务时，也不需要 mTLS。\n总结 mTLS 实现了网格内流量的加密，是构建零信任应用网络的关键一步。借助 Istio 我们可以很方便的为 Kubernetes 中的服务开启自动 mTLS，省去管理证书的麻烦。同时，我们也可以针对性的为网格内的部分服务开启 mTLS，便于我们将 Kubernetes 中的服务迁移到网格内。关于 Istio 中的证书管理，我们将在今后的博客中再做说明。\n参考 什么是 TLS（传输层安全性）？- cloudflare.com 什么是相互 TLS（mTLS）？- cloudflare.com What happens in a TLS handshake? | SSL handshake - cloudflare.com 写给 Kubernetes 工程师的 mTLS 指南 - lib.jimmysong.io 云原生安全白皮书中文版 - github.com Istio 安全 - istio.io JWT 组件详解 - lib.jimmysong.io ","relpermalink":"/blog/understanding-the-tls-encryption-in-istio/","summary":"本文介绍了 TLS、mTLS 相关知识，并介绍了 Istio 中如何开启 mTLS 和其应用场景。","title":"如何理解 Istio 中的 mTLS 流量加密？"},{"content":"在 Istio 最新的 Ambient 模式中，使用了 tproxy 做透明流量劫持（见此博客 ），这与 Sidecar 模式中基于 IPtables 的流量劫持方式有些许不同，这篇文文章，我们就就一起来探究下什么是 tproxy。\n什么是代理？ 在介绍透明代理之前，我们先了解下什么是代理。\n代理的功能 代理在互联网中的用途非常广泛，例如：\n缓存请求：加快网络响应速度，作用类似于 CDN； 请求过滤：用于网络监管，屏蔽或允许对某些主机、网站的访问； 请求转发：用于负载均衡或作为网络中继； 流量管理：对进出代理的流量进行细粒度的管理，例如按百分比发布到不同的后端、超时和重试设置、熔断等； 安全审计：记录和限制客户端请求，用于计费或审计； 代理的分类 代理的分类方式有很多，下图根据代理的位置将其划分为了两类：\n代理示意图 前向代理（Forward Proxy）：运行在客户端侧，代替客户端想服务端发送请求，例如我们日常使用的各种科学上网代理； 反向代理（Reverse Proxy）：代替服务端接受互联网或外部请求，然后将请求路由到对应的服务端，例如各种 Web 服务器，在这里 你可以看到一个代理列表； 代理可能与客户端或服务器位于同一节点（或网络空间，如 Kubernetes 中的 Pod），也可以位于远端。另外还可以根据代理对客户端或服务端是否可见（visible）来分为透明代理和非透明代理。下图展示了客户端（A）通过代理（B）向服务端（C）发送请求的过程。\n非透明代理和透明代理 非透明代理：客户端需要修改目的地址为代理服务器的地址，并使用代理协议连接代理服务器； 透明代理：所谓透明代理，即客户端和服务端感知不到代理的存在，客户端无需修改目的地址，也不需要采用代理协议连接代理服务器，所有目的地址转换都是在透明代理中完成的； 使用 tproxy 透明代理 tproxy 是 Linux 的内核模块（自 Linux 2.2 版本开始引入），用于实现透明代理，其名称中的字母 t 即代表透明（transparent）。\n要使用透明代理首先需要把指定的数据包使用 iptables 拦截到指定的网卡上，然后在该网卡监听并转发数据包。\n使用 tproxy 实现透明代理的步骤如下：\n首先需要实现流量拦截：在 iptables 的 PREROUTING 链的 mangle 表中创建一个规则，拦截流量发送给 tproxy 处理，例如 iptables -t mangle -A PREROUTING -p tcp -dport 9080 -j TPROXY --on-port 15001 --on-ip 127.0.0.1 --tproxy-mark 0x1/0x1，给所有目的地为 9080 端口的 TCP 数据包打上标记 1，你还可以指定来源 IP 地址或者 IP 集 ，进一步缩小标记范围，tproxy 监听在 15001 端口； 创建一个路由规则，将所有带有标记 1 的数据包查找特定的路由表：例如 ip rule add fwmark 1 lookup 100，让所有 fwmark 为 1 的数据包查找 100 路由表； 将数据包映射到特定的本地地址：例如 ip rule add local 0.0.0.0/0 dev lo table 100，在 100 路由表中将所有 IPv4 地址声明为本地，当然这只是一个例子，实际使用时需要请将特定的 IP 的数据包转发到本地的 lo 回环网卡； 至此流量已被拦截到 tproxy 的监听端口 15001（从 Linux 内核空间进入用户空间），你可以编写网络应用处理数据包或使用 Squid 或 Envoy 等支持 tproxy 的软件来处理数据包； 透明代理的优点 透明代理具有以下优点：\n透明代理提供更高的带宽并减少传输延迟，从而提高服务质量；\n用户无需配置网络和主机；\n企业可以控制对其网络服务的访问；\n用户可以通过透明代理连接互联网以绕过一些监管；\n透明代理的缺点 透明代理有以下缺点：\n如果透明代理配置不当，可能导致用户无法连接互联网，而对于不知情的用户来说，他们无法排查和修改透明代理中的错误； 透明代理的安全性无法得到保证，因为被拦截的用户流量可能被透明代理篡改； 透明代理可能缓存用户信息，导致用户隐私泄露的风险； 总结 透明代理作为代理中的一类重要类型，它的用途广泛，不论是 xray、clash 等代理软件，还是 Istio 服务网格中得使用了应用。了解它的原理和工作方式有助于我们科学正确的使用代理，而是否使用透明代理取决于你对它的信任和了解程度。\n参考 tproxy-example - github.com Linux transparent proxy support - powerdns.org Feature: TPROXY version 4.1+ Support - wiki.squid-cache.org ","relpermalink":"/blog/what-is-tproxy/","summary":"本文介绍了 tproxy 透明代理及其使用方法。","title":"什么是 tproxy 透明代理？"},{"content":"在上一篇博客中 我介绍了 Ambient 模式中的透明流量劫持和四层流量路由，在这一篇博客中，我将向你介绍在 Istio 的 Ambient 模式中，七层流量是如何路由的。\n下图展示了 Ambient 模式中七层网络流量路径。\nAmbient Mesh 中的七层网络流量路径 注意：Waypoint Proxy 可以位于应用程序所在节点，甚至图中的服务 A、服务 B 和 Waypoint Proxy 都可以位于同一个节点，之所以将它们画在三个节点上是为了方便展示，但是对于实际的流量路径没有大的影响，只不过是不再通过 eth0 发送到另外一个节点。\n下文我们将从手操作探究图中过程。\n环境说明 我们继续使用上一篇博客中部署的 Ambient 模式的 Istio，查看环境说明 。为了说明七层网路路由，我们需要在此基础上再创建一个 Gateway：\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: gateway.networking.k8s.io/v1alpha2 kind: Gateway metadata: name: productpage annotations: istio.io/service-account: bookinfo-productpage spec: gatewayClassName: istio-mesh EOF 执行完该命令后，default 命名空间下会创建了一个 Waypoint proxy，在我的环境中这个 pod 的名字是 bookinfo-productpage-waypoint-proxy-6f88c55d59-4dzdx，专门用于处理发往 productpage 服务（服务 B）的 L7 流量，我将它称之为 Waypoint Proxy B。\nWaypoint 代理可以位于与工作负载相同或者不同的节点上，它也可以部署在独立的命名空间中，不论它位于哪个节点，对于 L7 流量路径没有影响。\nAmbient mesh 中透明流量的方式在 L4 和 L7 网络中没有什么不同，因此在这篇博客中我们将略过 Inbound 和 Outbound 流量劫持部分，你可以查看上一篇博客 了解详情。\n下面我们将直接从流量被劫持到 Ztunnel A 后，被转发到 Envoy 的 15006 端口开始。\nZtunnel A 上的出站流量路由 使用下面的命令导出 Ztunnel A 上的 Envoy 代理配置：\nkubectl exec -n istio-system ztunnel-hptxk -c istio-proxy -- curl \u0026#34;127.0.0.1:15000/config_dump?include_eds\u0026#34;\u0026gt;ztunnel-a-all-include-eds.json 查看 ztunnel-a-all-include-eds.json 文件中的 Listener 配置部分，根据目的端口和来源 IP 的匹配关系，你将看到 ztunnel_outbound 监听器中有如下配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \u0026#34;10.8.14.226\u0026#34;: { \u0026#34;matcher\u0026#34;: { \u0026#34;matcher_tree\u0026#34;: { \u0026#34;input\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;port\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.matching.common_inputs.network.v3.DestinationPortInput\u0026#34; } }, \u0026#34;exact_match_map\u0026#34;: { \u0026#34;map\u0026#34;: { \u0026#34;9080\u0026#34;: { \u0026#34;action\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/google.protobuf.StringValue\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage\u0026#34; } } } } } } } } } 10.8.14.226 是目标服务的 Cluster IP，服务端口是 9080。流量将被路由到 spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage 集群，查看该集群的配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;version_info\u0026#34;: \u0026#34;2022-11-17T03:27:45Z/82\u0026#34;, \u0026#34;cluster\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.cluster.v3.Cluster\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;EDS\u0026#34;, \u0026#34;eds_cluster_config\u0026#34;: { \u0026#34;eds_config\u0026#34;: { \u0026#34;ads\u0026#34;: {}, \u0026#34;initial_fetch_timeout\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;resource_api_version\u0026#34;: \u0026#34;V3\u0026#34; } }, /* 省略 */ } 该集群使用 EDS 服务发现。查看该集群的 EDS 信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.endpoint.v3.ClusterLoadAssignment\u0026#34;, \u0026#34;endpoints\u0026#34;: [ { \u0026#34;locality\u0026#34;: {}, \u0026#34;lb_endpoints\u0026#34;: [ { \u0026#34;endpoint\u0026#34;: { \u0026#34;address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;10.4.3.14\u0026#34;, \u0026#34;port_value\u0026#34;: 15006 } }, \u0026#34;health_check_config\u0026#34;: {} }, \u0026#34;health_status\u0026#34;: \u0026#34;HEALTHY\u0026#34;, \u0026#34;load_balancing_weight\u0026#34;: 1 } ] } ], \u0026#34;policy\u0026#34;: { \u0026#34;overprovisioning_factor\u0026#34;: 140 } } 注意：这里还是缺少输出 cluster_name 字段。\n在这里直接将流量转发给 Waypoint Proxy 的端点 10.4.3.14:15006。\nWaypoint Proxy B 上的流量转发 我们再导出 Waypoint Proxy B 中的 Envoy 配置：\nkubectl exec -n default bookinfo-productpage-waypoint-proxy-6f88c55d59-4dzdx -c istio-proxy -- curl \u0026#34;127.0.0.1:15000/config_dump?include_eds\u0026#34;\u0026gt;waypoint-a-all-include-eds.json 查看 inbound_CONNECT_terminate 监听器的配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 { \u0026#34;name\u0026#34;: \u0026#34;inbound_CONNECT_terminate\u0026#34;, \u0026#34;active_state\u0026#34;: { \u0026#34;version_info\u0026#34;: \u0026#34;2022-11-17T03:27:45Z/82\u0026#34;, \u0026#34;listener\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.listener.v3.Listener\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;inbound_CONNECT_terminate\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port_value\u0026#34;: 15006 } }, \u0026#34;filter_chains\u0026#34;: [{ \u0026#34;filters\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;capture_tls\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/udpa.type.v1.TypedStruct\u0026#34;, \u0026#34;type_url\u0026#34;: \u0026#34;type.googleapis.com/istio.tls_passthrough.v1.CaptureTLS\u0026#34; } }, { \u0026#34;name\u0026#34;: \u0026#34;envoy.filters.network.http_connection_manager\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\u0026#34;, \u0026#34;stat_prefix\u0026#34;: \u0026#34;inbound_hcm\u0026#34;, \u0026#34;route_config\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;local_route\u0026#34;, \u0026#34;virtual_hosts\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;connect\u0026#34;, \u0026#34;domains\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;routes\u0026#34;: [{...}, { \u0026#34;match\u0026#34;: { \u0026#34;headers\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;:authority\u0026#34;, \u0026#34;exact_match\u0026#34;: \u0026#34;10.8.14.226:9080\u0026#34; }], \u0026#34;connect_matcher\u0026#34;: {} }, \u0026#34;route\u0026#34;: { \u0026#34;cluster\u0026#34;: \u0026#34;inbound-vip|9080|internal|productpage.default.svc.cluster.local\u0026#34;, \u0026#34;upgrade_configs\u0026#34;: [{ \u0026#34;upgrade_type\u0026#34;: \u0026#34;CONNECT\u0026#34;, \u0026#34;connect_config\u0026#34;: {} }] } } ] }], \u0026#34;validate_clusters\u0026#34;: false }, \u0026#34;http_filters\u0026#34;: [...], \u0026#34;tracing\u0026#34;: {...}, \u0026#34;http2_protocol_options\u0026#34;: { \u0026#34;allow_connect\u0026#34;: true }, \u0026#34;use_remote_address\u0026#34;: false, \u0026#34;upgrade_configs\u0026#34;: [{ \u0026#34;upgrade_type\u0026#34;: \u0026#34;CONNECT\u0026#34; }], \u0026#34;stream_idle_timeout\u0026#34;: …","relpermalink":"/blog/ambient-mesh-l7-traffic-path/","summary":"本文以图示和实际操作的形式详细介绍了 Ambient Mesh 中的七层（L7）流量路径。","title":"Istio Ambient 模式中的七层流量路由路径详解"},{"content":"本文通过动手操作，带领读者一步步了解 Istio ambient 模式中的四层流量路径。如果你还不了解什么是 Ambient 模式，以下文章可以帮助你了解：\n关于 Istio 推出 Ambient 数据平面模式的看法 Istio 无 sidecar 代理数据平面 ambient 模式简介 Istio 服务网格 ambient 模式安全详解 什么是 Ambient Mesh，它与 sidecar 模式有什么区别？ 如果你想略过实际动手步骤，只是想知道 Ambient 模式中的四层流量路径，请看下面服务 A 的一个 Pod 访问不同节点上服务 B 的 Pod 的四层流量路径图。\nAmbient 模式中的四层流量路径 原理 Ambient 模式使用 tproxy 和 HBONE 这两个关键技术实现透明流量劫持和路由的：\n使用 tproxy 将主机 Pod 中的流量劫持到 Ztunnel（Envoy Proxy）中，实现透明流量劫持； 使用 HBONE 建立在 Ztunnel 之间传递 TCP 数据流隧道； 什么是 tproxy？ tproxy 是 Linux 内核自 2.2 版本以来支持的透明代理（Transparent proxy），其中的 t 代表 transparent，即透明。你需要在内核配置中启用 NETFILTER_TPROXY 和策略路由。通过 tproxy，Linux 内核就可以作为一个路由器，将数据包重定向到用户空间。详见 tproxy 文档 。\n什么是 HBONE？ HBONE 是 HTTP-Based Overlay Network Environment 的缩写，是一种使用 HTTP 协议提供隧道能力的方法。客户端向 HTTP 代理服务器发送 HTTP CONNECT 请求（其中包含了目的地址）以建立隧道，代理服务器代表客户端与目的地建立 TCP 连接，然后客户端就可以通过代理服务器透明的传输 TCP 数据流到目的服务器。在 Ambient 模式中，Ztunnel（其中的 Envoy）实际上是充当了透明代理，它使用 Envoy Internal Listener 来接收 HTTP CONNECT 请求和传递 TCP 流给上游集群。\n环境说明 在开始动手操作之前，需要先说明一下笔者的演示环境，本文中对应的对象名称：\n代号 名称 IP 服务 A Pod sleep-5644bdc767-2dfg7 10.4.4.19 服务 B Pod productpage-v1-5586c4d4ff-qxz9f 10.4.3.20 Ztunnel A Pod ztunnel-rts54 10.4.4.18 Ztunnel B Pod ztunnel-z4qmh 10.4.3.14 节点 A gke-jimmy-cluster-default-pool-d5041909-d10i 10.168.15.222 节点 B gke-jimmy-cluster-default-pool-d5041909-c1da 10.168.15.224 服务 B Cluster productpage 10.8.14.226 注意：因为这些名称将在后续的命令行中用到，文中将使用代称，以便你在自己的环境中实验。\n笔者在 GKE 中安装了 Ambient 模式的 Istio，请参考该步骤 安装，注意不要安装 Gateway，以免启用 L7 功能，否则流量路径将于 L4 流量不同。\n下面我们将动手实验，深入探究 sleep 服务的 Pod 访问不同节点上 productpage 服务的 Pod 的四层流量路径。我们将分别检视 Pod 的 outbound 和 inbound 流量。\nOutbound 流量劫持 Ambient mesh 的 pod 出站流量的透明流量劫持流程如下：\nIstio CNI 在节点上创建 istioout 网卡和 iptables 规则，将 Ambient mesh 中的 Pod IP 加入 IP 集 ，并通过 netfilter nfmark 标记和路由规则，将 Ambient mesh 中的出站流量通过 Geneve 隧道透明劫持到 pistioout 虚拟网卡； ztunnel 中的 init 容器创建 iptables 规则，将 pistioout 网卡中的所有流量转发到 ztunnel 中的 Envoy 代理的 15001 端口； Envoy 对数据包进行处理，并与上游端点建立 HBONE 隧道（HTTP CONNECT），将数据包转发到上游。 检查节点 A 上的路由规则 登录到服务 A 所在的节点 A，使用 iptables-save 查看规则：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 $ iptables-save /* 省略 */ -A PREROUTING -j ztunnel-PREROUTING -A PREROUTING -m comment --comment \u0026#34;kubernetes service portals\u0026#34; -j KUBE-SERVICES -A ztunnel-POSTROUTING -m mark --mark 0x100/0x100 -j ACCEPT -A ztunnel-PREROUTING -m mark --mark 0x100/0x100 -j ACCEPT /* 省略 */ *mangle /* 省略 */ -A PREROUTING -j ztunnel-PREROUTING -A INPUT -j ztunnel-INPUT -A FORWARD -j ztunnel-FORWARD -A OUTPUT -j ztunnel-OUTPUT -A OUTPUT -s 169.254.169.254/32 -j DROP -A POSTROUTING -j ztunnel-POSTROUTING -A ztunnel-FORWARD -m mark --mark 0x220/0x220 -j CONNMARK --save-mark --nfmask 0x220 --ctmask 0x220 -A ztunnel-FORWARD -m mark --mark 0x210/0x210 -j CONNMARK --save-mark --nfmask 0x210 --ctmask 0x210 -A ztunnel-INPUT -m mark --mark 0x220/0x220 -j CONNMARK --save-mark --nfmask 0x220 --ctmask 0x220 -A ztunnel-INPUT -m mark --mark 0x210/0x210 -j CONNMARK --save-mark --nfmask 0x210 --ctmask 0x210 -A ztunnel-OUTPUT -s 10.4.4.1/32 -j MARK --set-xmark 0x220/0xffffffff -A ztunnel-PREROUTING -i istioin -j MARK --set-xmark 0x200/0x200 -A ztunnel-PREROUTING -i istioin -j RETURN -A ztunnel-PREROUTING -i istioout -j MARK --set-xmark 0x200/0x200 -A ztunnel-PREROUTING -i istioout -j RETURN -A ztunnel-PREROUTING -p udp -m udp --dport 6081 -j RETURN -A ztunnel-PREROUTING -m connmark --mark 0x220/0x220 -j MARK --set-xmark 0x200/0x200 -A ztunnel-PREROUTING -m mark --mark 0x200/0x200 -j RETURN -A ztunnel-PREROUTING ! -i veth300a1d80 -m connmark --mark 0x210/0x210 -j MARK --set-xmark 0x40/0x40 -A ztunnel-PREROUTING -m mark --mark 0x40/0x40 -j RETURN -A ztunnel-PREROUTING ! -s 10.4.4.18/32 -i veth300a1d80 -j MARK --set-xmark 0x210/0x210 -A ztunnel-PREROUTING -m mark --mark 0x200/0x200 -j RETURN -A ztunnel-PREROUTING -i veth300a1d80 -j MARK --set-xmark 0x220/0x220 -A ztunnel-PREROUTING -p udp -j MARK --set-xmark 0x220/0x220 -A ztunnel-PREROUTING -m mark --mark 0x200/0x200 -j RETURN -A ztunnel-PREROUTING -p tcp -m set --match-set ztunnel-pods-ips src -j MARK --set-xmark 0x100/0x100 iptables 规则说明：\n第 3 行：PREROUTING 链是最先运行的，所有数据包将先进入 ztunnel-PREROUTING 链； 第 4 行：将数据包发往 KUBE-SERVICES 链，在那里将 Kubernetes Service 的 Cluster IP 进行 DNAT 转换为 Pod IP； 第 6 行：带有 0x100/0x100 标记的数据包通过 PREROUTING 链，不再经过 KUBE-SERVICES 链； 第 35 行：这是添加到 ztunnel-PREROUTING 链上的最后一条规则，进入 ztunnel-PREROUTING 链中的在 ztunnel-pods-ips IP 集中的所有 TCP 数据包都会被打上 0x100/0x100 的标记，它将覆盖前面的所有标记； 关于 iptables 设置 mark 和 xmark 标记 MARK 这个扩展目标可以用来给数据包打标记，标记分两种：一种是用于标记链接的 ctmark，一种是用于标记数据包的 nfmark 。nfmark占四个字节共 32 位，我们可以把它看成是一个长度为 32 位的无符号整数，一般用 16 进制来表示。\nMark 的设置一共有五个选项，分别是 --set-xmark、--set-mark、--and-mark、--and-mark、--or-mark 和 --xor-mark。在本文用到了前两种，下面将分别为大家介绍。\n--set-xmark value[/mask]\n上面的 value 和掩码 mask 都是 32 位无符号整数，一般用 16 进制表示。内核设置数据包 nfmark 值的流程分为两步：\n首先，内核会先用 mask 预处理数据包原来的 nfmark，处理方法是：如果 mask 的第 N 位（二进制）为 1，则将数据包的 nfmark 第的 N 位（二进制）清零（Zero out） ，如果 mask 的第 N 位为 0，那么数据包的 nfmark 位保持不变 再用上面预处理后的 nfmark 和 value 做异或运算，得到数据包最后的 nfmark 值。 举个例子：假设我们设置了 --set-xmark 0x4000/0xffffffff，掩码为 0xffffffff，掩码表示为二 …","relpermalink":"/blog/ambient-mesh-l4-traffic-path/","summary":"本文以图示和实际操作的形式详细介绍了 Ambient Mesh 中的透明流量劫持和四层（L4）流量路径。","title":"Istio Ambient 模式中的透明流量劫持四层网络路由路径详解"},{"content":"这篇文章是根据笔者在 Linux Foundation APAC“源来如此” 开源软件学园技术公开课 《Istio 架构与流量管理机制解析》分享内容整理而成。\n本次分享的幻灯片可以在腾讯文档中观看 。\n在 B 站 中观看直播回放。\n幻灯片视图 前言 Istio 自 2017 年开源，至今已有 5 年多时间，业界已经出版了很多本介绍 Istio 的图书，包括笔者参与编写的《深入理解 Istio》，网上也有很多教程和文章介绍 Istio 配置的用法，但是笔者觉得都还不够生动形象，本文将发挥互联网多媒体的优势，笔者整理了以前撰写的介绍 Istio 的文章及绘制的图片，同时结合 Istio 的最新进展，重新撰写一篇介绍 Istio 架构和基础的流量管理功能的文章。读者可以跟着我一起来动手体验，一步步深入了解 Istio。\n本次分享内容包括：\nIstio 的架构与部署模式：Sidecar、Proxyless 和 Ambient 模式解析 Istio 中的流量管理机制及资源对象介绍 Istio 部署与安装示例 Istio 中的流量拦截与路由过程详解 准备条件 为了能够自己动手实验，你需要准备：\nKubernetes 集群 1.21+ Istio 1.15 Kubectl Lens（我推荐的一个 Kubernetes UI） Istio 的架构 下面我就以 Istio 官方 task Requst routing 中的例子来说明，sidecar 模式下 VirtualService 是如何运作的。\n我们先简要描述下这个例子在开始前的 Istio Mesh 状态：\n安装好 Istio，并为 default namespace 开启了自动 sidecar 注入； 在 default namespace 下安装了 bookinfo 示例，Bookinfo 示例中的 Kubernetes Service 会自动注册到 Istio Mesh 的 Cluster 中，例如 outbound|9080||details.default.svc.cluster.local，注意在其中没有 subset 信息； 创建了一系列的 Istio CR，包括： Gateway ： bookinfo-gateway 用于选择 Istio 的 Ingress Gateway，作为 bookinfo 的对外流量入口； DestinationRule：productpage、reviews、ratings、details 将这些流量路径通过 subset 与 Kubernetes 的 Service 关联起来，将用于未来的分版本路由。实际上为了让 Bookinfo 可以运行起来，这些 DestinationRule 目前都是不必要的；但是你创建了这些 DestinationRule 之后，就会在 Istio Mesh 中创建新的 Cluster 配置，比如 outbound|9080|v1|reviews.default.svc.cluster.local 这些 dynamic_active_clusters ，在创建 VirtualService 指定路由的时候，Istiod 就会下发 dynamic_route_configs 给 sidecar，其中会包含 reviews.default.svc.cluster.local:9080 路由，其中指定了将路由到的 cluster，这个 cluster 就是在 VirtualService 中配置的那个 host 的 subset； VirtualService：bookinfo 指定了流量在通过 bookinfo-gateway 进入 Istio Mesh 之后怎么走，其中指定了 URI 匹配的目的地是 productpage，请注意这里的 productpage 对应的是 Kubernetes 中的 Service。 要想实现在 bookinfo 示例的网页中每次刷新显示的书籍评分都显示星级，只需要创建并应用如下的 VirtualService 即可：\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 下面是部署了 VirtualService 之后 Istio 网格背后发生的事情。\n步骤一：流量拦截 当你为某个 namespace 开启了 sidecar 自动注入或者手动向 Deployment 的 pod 中注入了 sidecar，进出该 pod 的 TCP 服务流量经过 iptables 拦截到 Envoy 的 15006 端口，详细过程请见Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解 。\n步骤二：配置下发 你使用 kubectl 应用了 YAML 配置到 Istio Mesh，默认情况下，Istiod 使用 xDS 将配置下发到所有 namespace 下的 pod 中，除非你为 VirtualService 配置了 exportTo 字段。这些配置被应用到 pod 内的 Envoy 上，你可以用执行下面的命令查看某个 pod 中的 Envoy 的当前配置：\nkubectl -n default port-forward deploy/productpage-v1 15000 然后在浏览器中打开 http://localhost:15000 就可以进入 Envoy 的 admin 页面，查看 Envoy 的当前配置。在应用新的 VirtualService 之前，你可以保存当前 Envoy 的配置，然后应用后的 Envoy 配置做对比。关于 sidecar 中各个端口的详细用法请见Istio 中的各组件端口及功能详解 。\n步骤三：Envoy 处理流量 被拦截的流量在进入 Pod 的 Envoy Inbound Handler 后，然后进入 Envoy 的 Filter Chain，对于 HTTP 流量会进入 HttpConnectionManager（HCM）这个高级网络过滤器链，这里面有一系列的 HTTP 过滤器。Productpage 页面对 reviews 服务的访问究竟走哪个 subset，还得看 prodcutpage pod 中的 Envoy 配置。在步骤二的那个页面上查看 config_dump，你将看到 Envoy 的详细配置，其中的 dynamic_route_configs 中，可以看到对 reviews.default.svc.cluster.local:9080 服务的 Route 配置是 outbound|9080|v2|reviews.default.svc.cluster.local Cluster，再查看这个 Cluster 的配置，可以看到是用 EDS 来获取的，你可以使用 istioctl proxy-config endpoint xxx 查看该 pod 上可识别的所有 Endpoint。\n关于详细流程请参考分享的幻灯片 及演示视频。\n更多资源 归根结底，在 Istio 网格中是 Envoy 处理的七层流量，要想了解更底层的原理，需要对 Envoy 有更详细的了解。推荐大家学习 Envoy 基础教程，还有下面这些学习资源：\nTetrate 学院 Istio 基础教程 Envoy 基础教程 云原生资料库：lib.jimmysong.io 云原生社区：cloudnative.to Istio 管理员认证 关于 “源来如此”是由 Linux 基金会开源软件学园主办的开源技术公开课系列活动。Linux 基金会开源软件学园是 Linux 基金会中国区官方培训平台，致力于为中国软件行业培养具备专业开源技能的人才，不仅为中国开发者提供来自源头的开源技术课程，更发挥 Linux 基金会开源领导能力，积极与国内权威技术专家、知名软件企业合作，开展开源技术公开课系列活动，让更多人了解开源知识，以开源技术公开课为窗口了解开源世界。\nLinux Foundation 开源软件学园（LFOSSA）依托于全球最大的开源软件组织，是领先全球的高端专业软件人才教育机构，为科技企业培养了大量软件人才，Linux 基金会开源软件学园不仅拥有丰富的线上专业课程，面授课程的导师同样是由业内资深专家担任，所颁发的证书更是全球认可的专业资质。Linux 基金会做为非牟利国际技术组织，致力于通过开源推动创新和促进科技发展，我们唯一的目标就是帮助您的事业发展更上一层楼。\n","relpermalink":"/notice/istio-traffic-routing-deep-dive/","summary":"本文详述了从用户配置 Istio 流量管理资源对象到应用到配置下发并作用于 Envoy 的全过程。","title":"Linux 基金会开源软件学园分享：Istio 的架构与流量管理机制解析"},{"content":"本文将以 Kubernetes Ingress、Istio 和 Envoy Gateway 为例，向你介绍 Kubernetes 中的入口网关和 Gateway API，同时介绍 Gateway API 使得 Kubernetes 和服务网格入口网关融合的新趋势。\n本文观点 Ingress 作为 Kubernetes 的初代入口网关，它的资源模型过于简单以致于无法适应当今的可编程网络； Gateway API 作为 Kubernetes 入口网关的最新成果，它通过角色划分将关注点分离，并提供跨 namespace 支持使其更适应多云环境，已获得大多数 API 网关的支持； 入口网关（南北向）与服务网格（东西向，集群内路由）存在部分功能重叠，Gateway API 为两者的融合提供了新的参考模型； Kubernetes 入口网关的历史 2014 年 6 月 Kubernetes 开源，起初只能使用 NodePort 和 LoadBalancer 类型的 Service 对象来暴露集群内服务，后来才诞生了 Ingress ，两年后（Kubernetes 1.2）Ingress API 进入 Beta 版本，随后为了保持其轻量和可移植的特性，Ingress API 相较于 Kubernetes 其他 API 发展得比较缓慢，直到 Kubernetes 1.19 它才升级到 GA。\nIngress 的主要目标是用简单的、声明性的语法来暴露 HTTP 应用。你可以在 Kubernetes 中部署多种 Ingress Controller，并在创建 Ingress 的时候通过 IngressClass 指定该网关使用的控制器，或者在 Kubernetes 中设置默认的默认的 IngressClass。Kubernetes 默认只支持 AWS、GCE 和 Nginx Ingress Controller，同时还支持大量的第三方 Ingress Controller 。\n下图展示了 Kubernetes Ingress 的工作流程。\nKubernetes Ingress 工作流程 详细流程如下：\nKubernetes 集群管理员在 Kubernetes 中部署 Ingress Controller； Ingress Controller 会持续监视 Kubernetes API Server 中的 IngressClass 和 Ingress 对象的变动； 管理员应用 IngressClass 和 Ingress 来部署网关； Ingress Controller 会根据管理员的配置来创建对应的入口网关并配置路由规则； 如果在云中，客户端会访问该入口网关的负载均衡器； 网关将根据 HTTP 请求中的 host 和 path 将流量路由到对应的后端服务； Istio 同时支持 Ingress 和 Gateway API，下面是一个使用 Istio 入口网关的配置示例，在后文中会使用 Gateway API 创建该配置。\napiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: istio spec: controller: istio.io/ingress-controller --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress spec: ingressClassName: istio rules: - host: httpbin.example.com http: paths: - path: / pathType: Prefix backend: service: name: httpbin port: 8000 注意：Ingress 的 spec 中必须在 ingressClassName 字段中指定使用的 IngressClass，否则将无法创建对应的入口网关。\nKubernetes Ingress 的局限性 虽然 IngressClass 实现了入口网关与后台实现的解耦，但是它仍然有着巨大的局限性：\nIngress 的配置过于简单，仅支持 HTTP 协议路由； HTTP 路由仅支持 host 和 path 匹配，对于高级路由功能没有通用配置，只能通过 annotation 来实现，比如使用 Nginx Ingress Controller 实现 URL 重定向 ，需要配置 nginx.ingress.kubernetes.io/rewrite-target annotation，已经无法适应可编程路由的需求； 不同命名空间中的服务要绑定到同一个网关中的情况在实际情况下经常出现，而入口网关无法在多个命名空间中共享； 入口网关的创建和管理的职责没有划分界限，导致开发者不仅要配置网关路由，还需要自己创建和管理网关； Kubernetes Gateway API Gateway API 是一个 API 资源的集合 —— GatewayClass、Gateway、HTTPRoute、TCPRoute、ReferenceGrant 等。Gateway API 暴露了一个更通用的代理 API，可以用于更多的协议，而不仅仅是 HTTP，并为更多的基础设施组件建模，为集群运营提供更好的部署和管理选项。\n另外 Gateway API 通过将资源对象分离，实现配置上的解耦，可以由不同的角色的人员来管理，其中的 API 对象如下图所示。\nGateway API 及角色 下面是在 Istio 中使用 Gateway API 的示例。\napiVersion: gateway.networking.k8s.io/v1alpha2 kind: Gateway metadata: name: gateway namespace: istio-ingress spec: gatewayClassName: istio listeners: - name: default hostname: \u0026#34;*.example.com\u0026#34; port: 80 protocol: HTTP allowedRoutes: namespaces: from: All --- apiVersion: gateway.networking.k8s.io/v1alpha2 kind: HTTPRoute metadata: name: http namespace: default spec: parentRefs: - name: gateway namespace: istio-ingress hostnames: [\u0026#34;httpbin.example.com\u0026#34;] rules: - matches: - path: type: PathPrefix value: / backendRefs: - name: httpbin port: 8000 与 Ingress 类似，Gateway 使用 gatewayClassName 声明其使用的控制器，该控制器需要平台管理员创建，并允许客户端对 *.example.com 域名的请求。应用开发者可以在其服务所在的命名空间中，在此示例中是 default 创建路由规则，并通过 parentRefs 绑定到 Gateway 上，当然这必须是在 Gateway 明确允许其绑定的情况下（通过 allowRoutes 字段中的规则设置）。\n当你应用上面的配置后，Istio 会自动为你创建一个负载均衡网关，下图展示了 Gateway API 的工作流程。\nGateway API 工作流程 详细流程如下：\n基础设施供应商提供了 GatewayClass 和 Gateway 控制器； 平台运维部署 Gateway（可以部署多个，或使用不同的 GatewayClass）； Gateway Controller 会持续监视 Kubernetes API Server 中的 GatewayClass 和 Gateway 对象的变动； Gateway Controller 会根据集群运维的配置来创建对应的网关； 应用开发者应用 xRoute 并绑定服务上； 如果在云中，客户端会访问该入口网关的负载均衡器； 网关将根据流量请求中的匹配条件将路由到对应的后端服务； 从以上步骤中我们可以看出 Gateway API 相比 Ingress 有了明确的角色划分，而且路由规则可以与网关配置解耦，这大大增加了管理的灵活性。\n下图展示了流量接入网关后经过处理的流程。\n网关处理流程图 从图中我们可以看出路由是与网关绑定的，路由一般与其后端服务部署在同一个命名空间中，如果在不同的命名空间中时，需要在 ReferenceGrant 中明确赋予该路由跨命名空间的引用权限，例如下面的 foo 命名空间中的 HTTPRoute foo 可以引用 bar 命名空间中的 bar 服务。\nkind: HTTPRoute metadata: name: foo namespace: foo spec: rules: - matches: - path: /bar forwardTo: backend: - name: bar namespace: bar --- kind: ReferenceGrant metadata: name: bar namespace: bar spec: from: - group: networking.gateway.k8s.io kind: HTTPRoute namespace: foo to: - group: \u0026#34;\u0026#34; kind: Service 目前，Gateway API 仅支持 HTTPRoute，TCPRoute、UDPRoute、TLSRoute 和 GRCPRoute 还在实验阶段。Gateway API 已经得到了大量的网关和服务网格项目的支持，请在 Gateway 官方文档中查看支持状况 。\n入口网关与服务网格 服务网格主要关注的是东西向流量，即 Kubernetes 集群内部的流量，但是大部分服务网格同样提供了入口网关功能，例如 Istio。但是 Istio 的功能和 API 过于复杂，在本文中我们就以 SMI 为例来说明入口网关和服务网格的关系。\nSMI （Service Mesh Interface）是 CNCF 的孵化项目，开源与 2019 年，它定义了独立于供应商的在 Kubernetes 中运行的服务网格通用标准。\n下图说明 Gateway API 与服务网格 API 的重叠点。\nGateway API 与 SMI 有部分重合 从图中我们可以看到 Gateway API 与 SMI 在流量规范部分有明显的重叠。这些重叠导致同样的功能，需要在 Gateway API 和服务网格中重复实现。\nIstio 服务网格 当然，并不是所有的服务网格是完全符合 SMI 标准，Istio 是目前最流行的服务网格实现，它提供了丰富的流量管理功能，但是没有对这些功能制定单独的策略 API，而是耦合在 VirtualService 和 DestinationRule 中，如下所示。\nVirtualService\n路由：金丝雀发布、基于用户身、URI、Header 等匹配路由等； 错误注入：HTTP 错误代码注入、HTTP 延时注入； 流量切分：基于百分比的流量切分路由； 流量镜像：将一定百分比的流量镜像发送到其他集群； 超时：设置超时时间，超过设置的时间请求将失败； 重试：设置重试策略，如触发条件、重试次数、间隔时间等； DestinationRule\n负载均衡：设置负载均衡策略，如简单负载均衡、区域感知负载均衡、区域权重负载均衡； 熔断（Circuit Breaking）：通过异常点检测（Outlier Detection）和连接池设置将异常节点从负载均衡池中剔除； …","relpermalink":"/blog/why-gateway-api-is-the-future-of-ingress-and-mesh/","summary":"本文介绍 Kubernetes 中的入口网关及 Gateway API，入口网关与服务网格融合的新趋势。","title":"Gateway API：Kubernetes 和服务网格入口中网关的未来"},{"content":"今年五月 Envoy 社区宣布成立一个新的项目 Envoy Gateway ，经过五个月时间的开发，今天它的首个开源版本 v0.2 发布 ，本文将为你介绍什么是 Envoy Gateway，它的架构、快速入门和使用指南。\n什么是 Envoy Gateway？ Envoy Gateway 是一个用于管理 Envoy Proxy 的开源项目，可单独使用或作为 Kubernetes 中应用的网关。它通过了 Gateway API 核心一致性测试，使用 Gateway API 作为其唯一的配置语言来管理 Envoy 代理，支持 GatewayClass、Gateway、HTTPRoute 和 TLSRoute 资源。\nEnvoy Gateway 的目标是降低用户采用 Envoy 作为 API 网关的障碍，以吸引更多用户采用 Envoy。它通过入口和 L4/L7 流量路由，表达式、可扩展、面向角色的 API 设计，使其成为供应商建立 API 网关增值产品的基础。\nEnvoy Gateway 的核心优势是轻量级、开放、可动态编程，尤其是为后端增加了安全功能，这些优势使得它很适合作为后端 API 网关。\n架构 下图展示的是 Envoy Gateway 的架构，图中的阴影部分表示是 Envoy Gateway。你可以通过静态和动态两种方式来配置它，其中的 Provider 是针对不同的供应商开发的。\nEnvoy Gateway 架构图 该架构图基于 Envoy Gateway v0.2 版本绘制，并参考了 Envoy Gateway 文档 。\n配置流程 下面是配置 Envoy Gateway 的流程：\n你可以通过配置文件为其 Provider 提供静态配置（目前仅支持 Kubernetes 和文件方式，将来有可能支持更多不同平台供应商），在 Envoy Gateway 启动后，你还可以通过 Kubernetes 动态配置 Provider； 这些配置会被 Provider 中的资源监视器看到后应用到 Envoy Gateway 的资源转义器上； 资源转义器将配置分别转义为针对不同 Provider 开发的基础设施管理器的中间表示（Infra IR）和 xDS 中间表示（xDS IR）； 两种中间表示（IR）分别应用到其对应的基础设施管理器和 xDS 转义上； 基础设施通过增删改查（CRDU）Kubernetes Deployment、Service 等资源来运行 Envoy，xDS 管理器通过将 xDS 协议配置 xDS Server 的方式配置 Envoy 代理； 对于 Envoy 代理的流量请求将应用以上配置并转发到对应的后端； 以上就是对 Envoy Gateway 配置的流程，关于 Envoy 代理设计的更多细节请参考 Envoy Gateway 文档 快速开始 下面我们将在 Kubernetes 集群中安装 Envoy Gateway 并部署一个测试网站来看看它是否可以正常运行。\n前提 在使用 Envoy Gateway 前，请注意它的兼容性问题，参考兼容性矩阵 。\nEnvoy Gateway 版本 Envoy 代理版本 Gateway API 版本 Kubernetes 最低版本 v0.2.0 v1.23 - 最新 v0.5.1 v1.24 安装 因为在 Kubernetes 集群中 Gateway API 不是默认安装的，因此你需要手动安装 Gateway CRD。执行下面的命令安装 Gateway CRD 和 Envoy Gateway：\nkubectl apply -f https://github.com/envoyproxy/gateway/releases/download/v0.2.0/install.yaml 该命令将为你创建 envoy-gateway-system、gateway-system 两个命令空间，同时创了一系列 CRD。还有一些 Envoy Gateway 运行所需要的 ConfigMap、服务账户、RBAC、角色等。\n测试 执行下面的命令安装 GatewayClass、Gateway、HTTPRoute 和示例应用程序：\nkubectl apply -f https://github.com/envoyproxy/gateway/releases/download/v0.2.0/quickstart.yaml 端口转发到 Envoy 服务：\nkubectl -n envoy-gateway-system port-forward service/${ENVOY_SERVICE} 8888:8080 \u0026amp; 通过 Envoy 代理 curl 示例应用程序：\ncurl --verbose --header \u0026#34;Host: www.example.com\u0026#34; http://localhost:8888/get 你将看到如下输出：\n* Trying 127.0.0.1:8888... * Connected to localhost (127.0.0.1) port 8888 (#0) \u0026gt; GET /get HTTP/1.1 \u0026gt; Host: www.example.com \u0026gt; User-Agent: curl/7.79.1 \u0026gt; Accept: */* \u0026gt; * Mark bundle as not supporting multiuse \u0026lt; HTTP/1.1 200 OK \u0026lt; content-type: application/json \u0026lt; x-content-type-options: nosniff \u0026lt; date: Sat, 22 Oct 2022 07:10:34 GMT \u0026lt; content-length: 513 \u0026lt; x-envoy-upstream-service-time: 22 \u0026lt; server: envoy \u0026lt; x-envoy-decorator-operation: backend.default.svc.cluster.local:3000/* \u0026lt; { \u0026#34;path\u0026#34;: \u0026#34;/get\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;www.example.com\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;proto\u0026#34;: \u0026#34;HTTP/1.1\u0026#34;, \u0026#34;headers\u0026#34;: { \u0026#34;Accept\u0026#34;: [ \u0026#34;*/*\u0026#34; ], \u0026#34;User-Agent\u0026#34;: [ \u0026#34;curl/7.79.1\u0026#34; //内容省略... }, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;ingress\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;pod\u0026#34;: \u0026#34;backend-764c65b4dd-lp6jw\u0026#34; * Connection #0 to host localhost left intact } 如果你看到以上输出就证明你的 Envoy Gateway 安装成功并可正常运行。\n如果你的 Kubernetes 集群部署在云上，可以使用云负载均衡器的 IP 地址来访问测试：\nexport GATEWAY_HOST=$(kubectl get svc/${ENVOY_SERVICE} -n envoy-gateway-system -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) curl --verbose --header \u0026#34;Host: www.example.com\u0026#34; http://$GATEWAY_HOST:8080/get 笔者使用的 GKE，运行上面的命令，GATEWAY_HOST 环境变量的值几位负载均衡器的 IP 地址，最后同样可以类似上文的 curl 输出。\nEnvoy Gateway 中使用的 CRD 简介 上文说到安装 Envoy Gateway 的时候创建了一系列 CRD，在此我们将简要介绍一下这些 CRD：\nenvoyproxies.config.gateway.envoyproxy.io：Envoy Proxy API 的 Schema。 gatewayclasses.gateway.networking.k8s.io：GatewayClass 描述了用户可用于创建 Gateway 资源的一类 Gateways。建议将该资源作为 Gateway 的模板。这意味着一个 Gateway 是基于创建时 GatewayClass 的状态，对 GatewayClass 或相关参数的改变不会向下传播到现有的 Gateway。这项建议的目的是限制 GatewayClass 或相关参数的变化的爆炸半径。如果实现者选择将 GatewayClass 的变化传播给现有 Gateway，实现者必须清楚地记录这一点。每当一个或多个 Gateway 使用一个 GatewayClass 时，实现必须在相关的 GatewayClass 上添加 gateway-exists-finalizer.gateway.networking.k8s.io finalizer。这可以确保与 Gateway 相关的 GatewayClass 在使用中不会被删除。GatewayClass 是一个集群级的资源。 gateways.gateway.networking.k8s.io：Gateway 通过将 Listener 与一组 IP 地址绑定，代表了一个服务流量处理基础设施的实例。 httproutes.gateway.networking.k8s.io：HTTPRoute 提供了一种路由 HTTP 请求的方法。这包括通过主机名、路径、标头或查询参数来匹配请求的能力。过滤器可以用来指定额外的处理步骤。后端指定匹配的请求应该被路由到哪里。 referencegrants.gateway.networking.k8s.io：ReferenceGrant 标识了其他命名空间中的资源种类，这些资源被信任为引用与策略相同的名称空间中的指定资源种类。每个 ReferenceGrant 都可以用来代表一个独特的信任关系。额外的引用授权可以用来添加到它们所定义的命名空间的入站引用的信任源集合中。Gateway API 中的所有跨命名空间引用（除了跨命名空间的 Gateway-route 附件）都需要一个 ReferenceGrant。 referencepolicies.gateway.networking.k8s.io：该资源已被重新命名为 ReferenceGrant，且将在 Gateway API v0.6.0 中被删除，而采用相同的 ReferenceGrant 资源。 tcproutes.gateway.networking.k8s.io：TCPRoute 提供了一种路由 TCP 请求的方法。当与 Gateway 监听器结合使用时，它可以用来将监听器指定的端口上的连接转发到 TCPRoute 指定的一组后端。 tlsroutes.gateway.networking.k8s.io：TLSRoute 资源与 TCPRoute 类似，但可以配置为与 TLS 特定的元数据相匹配。这使得为特定的 TLS 监听器匹配数据流时有更大的灵活性。如果你需要将流量转发到一个 TLS 监听器的单一目标，你可以选择同时使用 TCPRoute 和 TLS 监听器。 udproutes.gateway.networking.k8s.io：UDPRoute 提供了一种路由 UDP 流量的方法。当与网关监听器结合使用时，它可以用来将监听器指定的端口上的流量转发到 UDPRoute 指定的一组后端。 关于这些 CRD 的具体用法以及 Envoy Gateway 的用户指南，将在以后的文章中分享。\n下面两篇我同事写的关于 Envoy Gateway 的文章推荐给大家阅读：\n使用 Envoy Gateway 0.2 体验新的 Kubernetes Gateway API 面向未来的网关：新的 Kubernetes Gateway API 和 Envoy …","relpermalink":"/blog/envoy-gateway-release/","summary":"今天 Envoy Gateway v0.2 发布，本文将为你介绍什么是 Envoy Gateway，它的架构、快速入门和使用指南。","title":"Envoy Gateway 首个正式开源版本介绍"},{"content":"Istio 在刚开源的时候就定义了几十个 CRD，其中用于流量治理的有 RouteRule、DestinationPolicy、EgressRule 等，后来推出了 v1alpha3 API 使用 VirtualService 和 DestinationRule 等取代了之前的 API。但是这些资源对象的定义，并不像 Kubernetes 中那么直观，反而会有些难以理解，比如 VirtualService，只看名字你可能认为只是第一个了一个“虚拟的服务”，但实际并非如此。\n本文将为你通过与实际的交通做类比，直观简要的介绍 Istio 中的两个核心的用于流量治理的对象——VirtualService 和 DestinationRule。\n流量 vs 交通 很多刚接触 Istio 的人可能对 VirtualService 和 DestinationRule 和两个资源对象不是很理解，如果我们将环境仅限定于 Kubernetes 集群，我们可以将路由比喻成现实世界中的交通，有很多车辆在道路上行驶，这就是流量。DestinationRule 相当于开辟了道路/路径/车道，确保了两点之间可达并控制车道的数量、宽度等。而 VirtualService 就像是红绿灯和道路标线，指挥车辆向哪行驶和如何行驶。也就是说如果你只定义了 DR 将不会对流量产生任何影响，因为你没有指挥流量怎么走，所以你必须定义 VirtualService 才可以控制流量的走向。\n一句话来概括：DestinationRule 打通了两地之间的通路，犹如修路架桥通隧道，同时控制车道设路障；VirtualService 做车辆指挥调度。\n请看下面这张将流量与交通的对比图，可以帮助你更直观的理解这种比喻。\n流量与交通对比图 流量治理 下面列举了你分别可以在这两个资源对象上做的流量治理行为。\nVirtualService\n路由：金丝雀发布、基于用户身、URI、Header 等匹配路由等； 错误注入：HTTP 错误代码注入、HTTP 延时注入； 流量切分：基于百分比的流量切分路由； 流量镜像：将一定百分比的流量镜像发送到其他集群； 超时：设置超时时间，超过设置的时间请求将失败； 重试：设置重试策略，如触发条件、重试次数、间隔时间等； DestinationRule\n负载均衡：设置负载均衡策略，如简单负载均衡、区域感知负载均衡、区域权重负载均衡； 熔断（Circuit Breaking）：通过异常点检测（Outlier Detection）和连接池设置将异常节点从负载均衡池中剔除； 总结 本文将抽象的流量治理与现实中的交通管理相类比，帮助你更直观的理解 Istio 中的流量管理对象 VirtualService 和 DestinationRule。同时介绍了基于它们可以进行的流量治理功能。VirtualService 主要用于设置路由规则，而服务弹性（超时、重试、熔断等）需要靠它和 DestinationRule 来共同维持。\n参考 Introducing the Istio v1alpha3 routing API - istio.io Traffic Management - istio.io ","relpermalink":"/blog/understand-istio-vs-and-dr/","summary":"本文将 Istio 中的流量治理与现实世界中的交通管理类比，可以帮助你快速理解 VirtualService 和 DestinationRule 设置的功能。","title":"如何理解 Istio 中的 VirtualService 和 DestinationRule？"},{"content":"开始之前 截止到撰写本文时 Istio 的最高版本为 1.15.2，1.13 版本的官方支持已经结束。请对照 Istio 文档中的发布状态描述 确定是否需要对 Istio 进行升级。\nIstio 官网上给出了升级 Istio 的几种方式 ：\n金丝雀升级 原地升级 使用 Helm 升级 但实际上，为了减少在升级时对网格内业务的影响，建议在升级 Istio 的时候，使用 canary upgrade ，它比 in-place upgrade 更加安全，而且支持回滚。使用 canary upgrade 支持跨越两个小版本，而 in-place upgrade 必须一个一个小版本的升级。不论使用哪种方式，其中 Ingress Gateway 都是 in-place upgrade 的。\nIstio 官方文档 对升级的步骤描述的不是很详细，本文是对官方文档的一个补充，在升级完成后有两个注意事项：\n为需要自动 sidecar 注入的 namespace 打上对应的 label； 删除原有的 validatingwebhookconfiguration 并添加新的； 下面是详细的升级步骤。\n升级步骤 使用的是以下命令安装的 canary 版本：\n# 将新版本的 revision 命名为 canary istioctl install --set revision=canary # 取消原先自动注入 sidecar 的 namespace 中的 label 并设置新的 label，这样该 namespace 就可以注入 canary 版本对应的 sidecar kubectl label namespace test-ns istio-injection- istio.io/rev=canary # 重启数据平面中的工作负载，将完成新版本的 sidecar 自动注入 kubectl rollout restart deployment -n test-ns 注意在升级完成后，为新的 namespace 开启 sidecar 自动注入时，需要给 namespace 打上安装 canary Istio 时候设置的 label，执行下面的命令：\nkubectl label namespace new-ns istio-injection- istio.io/rev=canary Istio 升级完成后的注意事项 在升级完成后，还有一些注意事项。例如如果你已经为其他 namespace 打上了 sidecar 自动注入的 label，请一定要将它删掉，并将 label 设置为 istio.io/rev=canary，因为可以保证在 pod 中注入新版被 sidecar，并且连接到新版的 Istiod。\n另外，你需要把最早安装 Istio 时设置的 ValidatingWebhookConfiguration 删掉，执行下面的命令：\nkubectl delete validatingwebhookconfiguration istiod-default-validator 关于 ValidatingWebhookConfiguration 在你安装新版本的 Istio 的时候，会自动创建一个名为 istio-validator-canary-istio-system 的 ValidatingWebhookConfiguration，该配置的目的是在创建和更新 Istio CR 的时候，先检测所有连接的 Istiod 是否有效。关于动态准入控制的详细描述请见 Kubernetes 文档 。 因为在安装新版本 Istio 的时候，安装了新的 istio-validator-canary-istio-system。如果你不将旧的删除话，你在创建 Istio CR 的时候将会看到如下错误。\nError from server (InternalError): error when creating \u0026#34;samples/bookinfo/networking/bookinfo-gateway.yaml\u0026#34;: Internal error occurred: failed calling webhook \u0026#34;validation.istio.io\u0026#34;: failed to call webhook: Post \u0026#34;https://istiod.istio-system.svc:443/validate?timeout=10s\u0026#34;: service \u0026#34;istiod\u0026#34; not found 以上内容在 Istio 的官方文档中里并没有说明，但是在 Istio Issue-36526 中有提及。\n参考 动态准入控制 - kubernetes.io Istio Supported Releases - istio.io Canary Upgrades - istio.io ","relpermalink":"/blog/istio-canary-upgrade/","summary":"本文详述了使用金丝雀升级 Istio 的步骤及升级后的注意事项。","title":"如何不停机升级 Istio？"},{"content":"国庆假期除了去浙江和安徽玩了一圈之外，还抽空翻译了一本电子书。本书译自 Solving the Bottom Turtle — a SPIFFE Way to Establish Trust in Your Infrastructure via Universal Identity。你可以选择在线阅读（推荐）：https://lib.jimmysong.io/spiffe/ ，或者下载本书中文版 PDF（关注「几米宋」公众号在后台回复 SPIFFE 即可获得下载链接）。\n目录 SPIFFE 的历史和动机 收益 身份背后的通用概念 SPIFFE 和 SPIRE 概念介绍 开始前的准备 设计一个 SPIRE 部署 与其他系统集成 使用 SPIFFE 身份通知授权 SPIFFE 与其他安全技术对比 从业者故事 关于本书 本书介绍了服务身份的 SPIFFE 标准，以及 SPIFFE 的参考实现 SPIRE。这些项目为现代异构基础设施提供了一个统一的身份控制平面。这两个项目都是开源的，是云原生计算基金会（CNCF）的一部分。\n随着企业发展他们的应用架构以充分利用新的基础设施技术，他们的安全模式也必须不断发展。软件已经从一个盒子上的单片机发展到几十或几百个紧密联系的微服务，这些微服务可能分布在公共云或私人数据中心的数千个虚拟机上。在这个新的基础设施世界里，SPIFFE 和 SPIRE 帮助保持系统的安全。\n本书努力提炼 SPIFFE 和 SPIRE 的最重要的专家的经验，以提供对身份问题的深刻理解，帮助你解决这个问题。通过这些项目，开发和运维可以使用新的基础设施技术构建软件，同时让安全团队从昂贵和耗时的人工安全流程中解脱出来。\n关于零号乌龟 访问控制、秘密管理和身份都是相互依赖的。大规模地管理秘密需要有效的访问控制；实施访问控制需要身份；证明身份需要拥有一个秘密。保护一个秘密需要想出一些办法来保护另一个秘密，这就需要保护那个秘密，以此类推。\n这让人想起一个著名的轶事：一个女人打断了一位哲学家的讲座，告诉他世界是在乌龟的背上。当哲学家问她乌龟靠的是什么时，她说：“还是乌龟！\u0026#34;。找到底层的乌龟，即所有其他安全所依赖的坚实基础，是 SPIFFE 和 SPIRE 项目的目标。\n本书封面上的“零号乌龟”就是这只底层乌龟。零代表了数据中心和云计算的安全基础。零号是值得信赖的，愉快地支持所有其他的乌龟。\nSPIFFE 和 SPIRE 是帮助你为你的组织找到底层乌龟的项目。通过这本书中的工具，我们希望你也能为“底层乌龟”找到一个家。\n","relpermalink":"/notice/spiffe-book/","summary":"目前能找到的关于 SPIFEE 最完整系统的资料了，我特地将它翻译了一下。","title":"《零信任的基石：使用 SPIFFE 为基础设施创建通用身份》翻译电子书分享"},{"content":"今天 Istio 社区推出了Ambient Mesh ，这是一种新的 Istio 数据平面模式，旨在简化操作、扩大应用兼容性并降低基础设施成本。用户可以选择将 Ambient Mesh 集成到其基础设施的网格数据平面，放弃 sidecar 代理，同时保持 Istio 的零信任安全、遥测和流量管理等核心功能。该模式目前还是预览版，Istio 社区准备在未来几个月内将其推向生产就绪。\nAmbient Mesh 推出的消息对于社区来说可能显得有些突然，但其实关于 sidecar 模式对于资源的消耗过大，以及简化服务网格的呼声在社区里已经存在很久了，Google 从多年前就在寻求 HBONE（HTTP-Based Overlay Network Environment，基于 HTTP 的重叠网络环境）解决方案，还有社区提出的多种 sidecar 部署模式 、proxyless 模式 等都是为了解决这个问题。\n什么是 Ambient 模式？ Ambient 模式是 Istio 社区在 2022 年 9 月推出的一种无 sidecar 的 Istio 数据平面部署模式，下图展示了 Ambient 模式的架构。\nAmbient 模式架构 从图中我们可以看到 Ambient 模式对应用程序本身没有任何侵入，而是在应用程序外围：\n同 node 上部署 ztunnel：使用 Envoy 实现的共享代理，多租户模式，负责 L4 网络，主要是安全性方面； 以服务账户为单位部署 Waypoint proxy：同样使用 Envoy 实现，单租户模式，使用 Gateway API 部署的 Gateway 资源，负责 L7 网络，当服务需要 L7 网络功能的时候才部署； 下面是 Ambient 模式的功能分层。\nAmbient 模式的功能分层 关于 Ambient 模式的更多介绍请阅读：\nIstio 无 sidecar 代理数据平面 ambient 模式简介 Istio 服务网格 ambient 模式安全详解 什么是 Ambient Mesh，它与 sidecar 模式有什么区别？ Ambient 模式的核心技术 Ambient 模式中的 ztunnel 和 Waypoint proxy 目前还是使用 Envoy 来实现的，未来不排除使用其他语言（非 C++，如 Rust）来实现一个轻量级的 ztunnel。该模式的核心是所谓的 HBONE（HTTP Based Overlay Network Environment，基于 HTTP 的覆盖网络环境）。\nHBONE 基于 HTTP/2 CONNECT，将工作负载之间的请求以流的形式进行隧道传输，尽可能地复用 HTTP/2 连接。HBONE 对工作负载来说是透明的，支持更好的传输机制：\n支持多协议，包括 Server First 协议 ，如 MySQL。Istio 可以自动检测出 HTTP 和 HTTP/2 流量。如果未自动检测出协议，流量将会视为普通 TCP 流量。对于 Server First 协议必须明确声明，否则将作为 TCP 流量处理； 对于使用自己 TLS 证书的应用程序可以逐步采用 Istio； 支持绕过 Istio mTLS 封装直接调用 Pod IP； Sidecar 模式的限制 其实 ambient 模式的出现，主要是因为 sidecar 模式有以下限制：\nSidecar 容器不是 pod 中的一等公民，它的生命周期不受控制，有可能在 sidecar 就没准备好的情况下，pod 就开始接收连接，让 sidecar 的生命周期与应用程序 pod 绑定，这本身就是对应用程序的一种侵入 Sidecar 无法解释不规范的七层系列，如 HTTP 和 gRPC； 如果仅需服务网格的安全功能，那么引入 sidecar 是一次过大的投资，因为它增加了很多七层网络功能，这些是用不到的，客户无法做到渐进式采用服务网格； Sidecar 升级时，应用程序需要重新部署或者启动，这需要对应用程序进行协调； 关于 Ambient 模式的看法 本文我将谈谈对 ambient 模式的几点看法：\n关于 Ambient Mesh 的命名：我觉得叫做 Ambient Mode 会更好，有些接触 Istio 的初学者可能会觉得它是一种全新的不同于 Istio 的 service mesh；另外关于这个模式的中文翻译，如果直接翻译成“环境网格”似乎让人很难理解，我还想到了其他词汇，如“外围”、“氛围”、“周围”、“环绕”、”情景”等，没有一个汉语词汇可以准确表达这个 ambient 的含义，因为相对于 sidecar 模式，ambient 模式对应用程序 pod 没有侵入性，暂且将其称之为外围模式。 Ambient Mode 的本质：它的本质是分离 sidecar proxy（Envoy）中的 L4 和 L7 功能，让一部分仅需要安全功能的用户可以最小阻力（低资源消耗、运维成本）地使用 Istio service mesh。 Ambient Mode 的意义：因为它 sidecar 模式兼容，用户在采纳 Ambient Mode 获得了 mTLS 和有限的可观察性及 TPC 路由等 L4 功能，之后可以更方便的过度到 sidecar mode 以获得完全的 L7 功能。这给用户采纳 Istio 提供了更多模式选择，优化了 Istio 采纳路径。 Ambient Mode 的坏处：Proxyless、sidecar、ambient 模式，使得 Istio 越来越复杂，用户理解起来更加费力；控制平面为了支持多种数据平面部署模式，其实现将更加复杂。 与其他服务网格的关系：有的 service mesh 从原先的 per-proxy per-node 模式转变为 sidecar mode，如 Linkerd；还有的从 CNI 做到 service mesh，如 Cilium 使用 per-proxy per-node 模式；如今 Istio 在 sidecar mode 的基础上增加了 ambient mode，这也是目前唯一同时支持这两种部署模式的服务网格，为用户提供了多样的选择。 安全问题：虽然 Istio 服务网格 ambient 模式安全详解 说明了 ambient 模式的设计主旨是为了将应用程序与数据平面分离，让安全覆盖层的组件（ztunnel）处于类似于 CNI 的网格底层，考虑到 ztunnel 有限的 L4 攻击面，该模式的安全风险是可以接受的；但是，ztunnel 作为 DaemonSet 部署在每个节点上，需要处理和分发调度到该节点上的所有 pod 的证书来建立 mTLS 连接，一旦 一个 ztunnel 被攻破，它的爆炸半径确实是大于一个 sidecar，安全详解的博客中说 Envoy 的 CVE 问题会影响所有 sidecar，升级 sidecar 也会带来很大的运营成本，所以权衡之下选择 ambient 模式，安全问题再次给用户造成了困惑，不过最终选择的权利还是在用户自己。 安装试用 参考 Istio 官网中的步骤 安装：\n下载 Ambient Mesh 预览版 ；\n检查 Kubernetes 版本，建议的大于等于 1.21；\n安装 Ambient profile：\nistioctl install --set profile=ambient 部署示例应用：\nkubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml kubectl apply -f https://raw.githubusercontent.com/linsun/sample-apps/main/sleep/sleep.yaml kubectl apply -f https://raw.githubusercontent.com/linsun/sample-apps/main/sleep/notsleep.yaml kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml 将应用添加到 Ambient Mesh：\nkubectl label namespace default istio.io/dataplane-mode=ambient 发送测试流量：\nkubectl exec deploy/sleep -- curl -s http://istio-ingressgateway.istio-system/productpage | head -n1 kubectl exec deploy/sleep -- curl -s http://productpage:9080/ | head -n1 kubectl exec deploy/notsleep -- curl -s http://productpage:9080/ | head -n1 在将 Pod 加入到 Ambient Mesh 中之后，就可以给这些 Pod 应用 L4 的授权策略：\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: productpage-viewer namespace: default spec: selector: matchLabels: app: productpage action: ALLOW rules: - from: - source: principals: [\u0026#34;cluster.local/ns/default/sa/sleep\u0026#34;, \u0026#34;cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account\u0026#34;] EOF 上面这个策略只允许 sleep 服务和 istio ingress gateway 访问 productpage。\n应用 L7 策略：\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: gateway.networking.k8s.io/v1alpha2 kind: Gateway metadata: name: productpage annotations: istio.io/service-account: bookinfo-productpage spec: gatewayClassName: istio-mesh EOF 使用 Gateway API 创建了一个 Gateway，这里实际是在 default 命名空间下创建了一个 waypoint proxy，专门用于处理 L7 流量。\n在给它应用授权策略：\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: productpage-viewer namespace: default spec: selector: matchLabels: app: productpage action: ALLOW rules: - from: - source: principals: [\u0026#34;cluster.local/ns/default/sa/sleep\u0026#34;, \u0026#34;cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account\u0026#34;] to: - operation: methods: [\u0026#34;GET\u0026#34;] EOF 这个策略跟之前有所不同的是最后的 to 字段定义了 HTTP 方法，现在你在 sleep pod 中对 productpage 服务执行除 GET 以 …","relpermalink":"/blog/istio-ambient-mode/","summary":"本文介绍了 Ambient 模式并阐述了笔者对于 Istio 新推出的 ambient mesh（环境网格）的看法。","title":"关于 Istio 推出 ambient 数据平面模式的看法"},{"content":"昨天 COSS（Commercial Open Source）公司的创始人也是投资者 Joseph（JJ）Jacks 给我发消息，让我看下他们新发布的这个报告。\nJJ 给我发的报告 我早就知道 JJ 创业搞了一个 COSS 投资公司并创立了一个基金，这次他整理的这个全球 COSS 公司融资数据还是比较全面的。\n什么是 COSS 公司？ COSS 是英文 Commercial Open Source Software（商业化开源软件）的简称，COSS 公司指的依靠这些 COSS 而生存的公司，即如果没有这些软件，这家公司就不会存在，例如：\n如果没有 Kafka 就没有 Confluent； 如果没有 Hadoop 就没有 Cloudera； 如果没有 Spark，就没有 Databricks； 如果没有 Git，就没有 GitLab； 如果没有 Linux，就没有 Red Hat、SUSE 等； COSS 与 SaaS 类公司的区别见下图。\nCOSS 与 SaaS 类公司对比 关于 COSS 公司融资的一些数据 自 2020 年一月至 2022 年 8 月（32 个月）这段时间内全球公开的 COSS 公司融资数据如下：\n500+ 次融资 360+ 家 COSS 初创公司 340+ 个开源项目 230+ 家风投公司参与 总融资 240+ 亿美元 平均每月融资 7.5 亿美元 2021 年 2 月和 2021 年 9 月单月融资规模最大 更多详细数据请阅读博客 Global VC Funding In COSS: $24B+ Raised From Jan 2020 to August 2022 和原始数据表格 。\n中国的开源商业化公司 近两年中国也涌现了一系列的开源商业化（COSS）公司，例如：\n开源软件 公司名称 Apache ShardingSphere SphereEx TDengine 涛思数据 OceanBase OceanBase Apache APISIX API7 Apache Pulsar StreamNative Apache DolphinScheduler 未知 Nebula Graph 悦数科技 Milvus Zilliz 表：中国的 COSS 公司 以上仅列举了部分笔者观察到的 COSS 公司，如有遗漏，欢迎补充。\n总结 在 2020 年之前国内也有不少开源商业化公司，比如 PingCAP、Kylingence、EasyStack 等，2020 年后似乎在全球都有一种 COSS 创业的趋势，国内的开源商业化相对于国外还处于比较早期的阶段，一是有影响力的开源项目太少，二是国内用户尚未对商业化开源软件的付费的习惯，还有一些其他政策和法规问题。\n","relpermalink":"/blog/coss-vc-funding-since-2020/","summary":"近日 COSS 社区发布了自 2020 年 1 月以来的全球风险投资报告，全球共有超过 240 亿美元开源商业化软件公司融资。","title":"自 2020 年以来全球的开源商业化软件融资情况"},{"content":"Istio 是基于容器的云原生技术栈的三大核心技术之一，另外两个是 Kubernetes 和 Knative。其中 Kubernetes 和 Knative 早已支持了 arm64 架构，甚至连 Istio 的数据平面 Envoy 早在 1.16 版本 就已支持 arm64 架构（2020 年 10 月）。随着 Istio 1.15 的发布 ，你可以开箱即用得在 arm64 架构上部署 Istio，不需要自己来编译 arm 架构的镜像。\n在 Istio 1.15 之前如何在 arm 架构上安装 Istio？ Istio 默认使用 Docker Hub 作为生产镜像仓库，Google Container Registry 作为生产和测试仓库。对于 1.14 及以前的版本，Istio 官方的镜像仓库中只有 amd64 架构的镜像，如果你的 Kubernetes 集群是运行在 arm 架构下，在安装 Istio 时会出现出现如下错误：\nexec user process caused: exec format error 这时你需要为 Istio 安装重新指定一个包含 arm64 架构镜像的仓库，在安装 Istio 时执行下面的命令指定该镜像仓库：\n$ istioctl install --set profile=demo --set hub=docker.io/mydockerhub -y 此时要想在 arm64 架构上使用 Istio，你可以使用 Istio 社区中有人为 Istio 单独构建了 arm64 架构的镜像 ，或者自己构建镜像。\nIstio 为了支持 arm 做了哪些工作？ 为了让 Istio 支持 arm，需要将以下二进制文件或者镜像基于 arm 架构编译：\nistioctl：这是最简单的部分，只需要使用 Go 语言的交叉编译即可，Istio 的早期版本就已经支持； pilot：控制平面 Istiod 中运行的镜像； proxyv2：在 Ingress Gateway、Egress Gateway 和 Sidecar 中使用的镜像，通过 Kubernetes mutating webhook 自动注入； Istio 数据平面中的 Envoy 是从 Envoy 官方仓库中 fork 出来的，但是 Envoy 早就支持了 arm64，为什么 Istio 官方还不支持呢？这是因为一方面 Istio 的官方 CI 环境 prow.istio.io 运行在 GKE 上的，而 GKE 上并没有 arm64 架构的环境，所以无法执行测试。直到 2022 年 7 月 GKE 才正式提供 arm64 架构的虚拟机，那时才可以方便的编译和测试 arm64 架构的 Istio，详见 Run your Arm workloads on Google Kubernetes Engine with Tau T2A VMs 。\n注意 Istio 官方仅提供了 amd64 和 arm64 架构的镜像，不支持 arm32。 至于 arm 架构的镜像构建，可以使用 Docker BuildKit 来实现多平台构建，你可以使用下面的命令编译指定 arm 平台架构的镜像：\ndocker buildx build --platform linux/arm64 关于 docker buildx 的详细信息请参考 Docker 文档 。\n你可以像往常一样来安装 Istio，Kubernetes Node 会根据节点的架构自动拉起对应平台架构的镜像。\n","relpermalink":"/blog/istio-arm64-support/","summary":"随着 Istio 1.15 的发布，你可以很方便得在 arm64 架构上部署 Istio。","title":"Istio 1.15 新增对 arm64 架构处理器的支持"},{"content":"今天云原生社区里有好几个人问我 servicemesher.com 网站为什么访问不了了，我想在这里做一个统一的回复。\n通知 简单来说，该 servicemesher.com 归蚂蚁集团所有，2020 年时 ServiceMesher 与云原生社区合并后就该网站已停止更新，所有博客资料都已迁移到了云原生社区 。今天蚂蚁正式停止运营该网站，是时候与开通运营了四年多的 servicemesher.com 网站说再见了，谢谢大家的参与！ 关于 servicemesher.com servicemesher.com 的历史分为以下几个阶段：\n活跃期：2018 年 5 月 - 2021 年 2 月 停止维护：2021 年 3 月至 2022 年 8 月 停止运营：2022 年 8 月 24 日 servicemesher.com 网站建立于 2018 年 5 月，由蚂蚁集团建立（服务器资源、域名均由蚂蚁集团提供）并社区化运营。在该网站运营的近 3 年时间里，共发布了几百篇社区原创或翻译的服务网格和云原生相关博客，为服务网格技术在中国的启蒙发挥了重要作用。同时网站中还发布了第一份完整翻译的《Envoy 中文文档》（现已有更新的中文版本，见云原生社区翻译版 ）、《Kantive 入门》（因为原文基于的 Knative 版本太老，现已过时）、《Istio 服务网格进阶实战》（本书已出版发行，查看详情 ）等翻译或原创的电子书籍。\n内容迁移至 cloudnative.to 随着时间的推移，服务网格技术早已成为云原生技术栈中不可或缺的一部分，因此笔者在 2020 年 5 月成立了云原生社区 ，ServiceMesher 与云原生社区合并，servicemesher.com 网站也于 2021 年 2 月停止维护（见此通知 ，但还可以访问，目前你依然可以在 GitHub 上查看它的代码），所有博客都已迁移到了云原生社区官网 cloudnative.to 。\n欢迎之前参与 ServiceMesher 的朋友和其他云原生技术爱好者关注和参与到云原生社区 中来，谢谢。\n","relpermalink":"/notice/servicemesher-website-halt/","summary":"是时候与开通运营了四年多的 servicemesher.com 网站说再见了，所有博客迁移至云原生社区。","title":"关于 servicemesher.com 停止运营的通知"},{"content":"前言 这篇回顾文章可能来的有点晚了，之前在云原生社区中国就有好几次有人问我，什么时候可以在 B 站上观看到 IstioCon 2022 的录像（IstioCon 组委会早已将录像上传到了 YouTube，但是国内有些人可能无法访问 YouTube），正好最近我跟会务人员要到了会议录像，将其全部上传到了 B 站。\n视频回放与 PPT B 站观看视频回放 IstioCon 2022 Sessions 大部分内容已上传到 B 站，除了以下三个，有人已经提前上传到了 B 站。\n有三个视频撞车了 PPT 可以直接通过上面的链接选择你感兴趣的话题下载。\n总结 IstioCon 2022 今年是第二届了，于 4 月 25 日到 29 日在线上举行，笔者作为 IstioCon 的中文场组织者之一参与了本届活动中文场开场及圆桌讨论环节，我们讨论的议题是《Istio 的开源生态展望》，下面是论坛嘉宾。\n中文场开场演讲：Istio 的开源生态展望论坛嘉宾 最近两年来，围绕 Istio，在中国涌现出了几个代表性的开源项目，比如腾讯开源的 Aeraki、DaoCloud 开源的 Merbridge 还有网易开源的 Slime 等。基于 Istio 开发的扩展，我看到的基本都是来自中国，这一点可以说是中国特色。\n本次 IstioCon 的关键词如下：\n零信任 多集群 Proxyless eBPF Gateway 安全 同时，Google Cloud 的 VP Eric Brewer 宣布了一个大消息，将 Istio 捐献给 CNCF，一旦成功，那么由 Google 主导的三个开源项目 Kubernetes、Istio、Knative 将成为 CNCF 中容器编排、服务网格、Serverless 等 Kubernetes 技术栈的三驾马车。\nKubernetes 技术栈的三驾马车 关于 Istio 你应该了解 Istio 只适用于特别场景与规模 不会一口吃出一个胖子，要一个一个版本的渐进式提高 理解 Envoy 对于应用 Istio 特别重要 用户不想再学习另一套 CRD，请考虑在 Istio 之上增加一层抽象 不要低估 Day 2 operation 需要消耗的精力 Istio 在 2022 年的计划 本届大会上还公布了 Istio 在 2022 年的关注点。\n稳定性与重新定义 Istio 的 API 平面 推动当前特性和 API 到稳定版 将 API 配置从 MeshConfig 中移到数据平面中并使其稳定 继续帮助 Kubernetes API 定义与 Istio API 对齐（Kubernetes Gateway API） 继续增强 Telemetry API：增加对使用 OpenTelemetry 等供应商的日志记录的支持，过滤访问日志，以及自定义跟踪服务名称 增强升级与故障排查 推动基于修订标签的升级到稳定版 将 Helm 安装推广到 Beta 版 在 istioctl 中添加更多的分析器，并扩展当前的分析器，包括针对 Kubernetes 集群以外环境的分析器 用户希望用服务网格来排除服务故障，而不是对服务网格进行故障排除 增强可扩展性 Wasm Plugin 自定义授权 增加标准化集成点以加入自定义的 CA 或网关 扩展 Istio 的使用场景 支持 IPv6 和双栈网络（Dual Stack Networking） 支持 ARM 扩展使用 gRPC 的 proxyless Istio 性能增强：增量 xDS、降低 sidecar 延迟 什么是双栈网络？ 双协议栈技术就是指在一台设备上同时启用 IPv4 协议栈和 IPv6 协议栈。这样的话，这台设备既能和 IPv4 网络通信，又能和 IPv6 网络通信。如果这台设备是一个路由器，那么这台路由器的不同接口上，分别配置了 IPv4 地址和 IPv6 地址，并很可能分别连接了 IPv4 网络和 IPv6 网络。Kubernetes 中支持 IPv4/IPv6 双协议栈，详见 Kubernetes 文档 。 安全加固 默认尽可能安全 继续完善安全最佳实践文档 推动 distroless 镜像 软件 BOM（Bill of Materials，依赖服务清单） 增加格外的模糊测试 其他增强 用自动化使升级更容易：让 Istio 的升级像其他升级一样工作，发布升级自动化的参考实现 多规模和大规模集群 对于开发者来说，我们已经将每周的工作小组会议合并到。对于开发者来说，我们已经将每周的工作组会议合并，一个在美国，一个在亚太地区。 写在最后 希望 Istio 正式进入 CNCF 的那天早日到来，我也希望能够在社区里看到更多源于 Istio 的终端案例分享，也欢迎大家加入到云原生社区 中来，我们有专门的 Istio 讨论群。\n","relpermalink":"/blog/istiocon-2022-recap/","summary":"本文总结了 IstioCon 2022 并分享了 Istio 社区 2022 年的工作重点，同时公布了 B 站视频链接和 PPT 下载地址。","title":"IstioCon 2022 回顾及录像、PPT 分享"},{"content":" 关于本文 本文根据笔者在 GIAC 深圳 2022 年大会上的的演讲《Beyond Istio OSS —— Istio 的现状及未来》 整理而成，演讲幻灯片见 腾讯文档 。 本文回顾了 Istio 开源近五年来的发展，并展望了 Istio 服务网格的未来方向。本文的主要观点如下：\n因为 Kubernetes、微服务、DevOps 及云原生架构的流行，导致服务网格技术的兴起； Kubernetes 和可编程代理，为 Istio 的出现打下了坚实的基础； 虽然 eBPF 可以加速 Istio 中的透明流量劫持，但无法取代服务网格中的 sidecar； Istio 的未来在于构建基于混合云的零信任网络； Istio 诞生的前夜 2013 年起，随着移动互联网的爆发，企业对应用迭代的效率要求更高，应用程序架构开始从单体转向微服务，DevOps 也开始变得流行。同年随着 Docker 的开源，解决了应用封装和隔离的问题，使得应用在编排系统中调度变得更容易。2014 年 Kubernetes、Spring Boot 开源，Spring 框架开发微服务应用开始流行，在接下来的几年间大批的 RPC 中间件开源项目出现，如 Google 在 2016 年发布 gRPC 1.0，蚂蚁在 2018 年开源 SOFAStack 等，微服务框架百花齐放。为了节约成本，增加开发效率，使应用更具弹性，越来越多的企业正在迁移上云，但这不仅仅是将应用搬到云上那么简单，为了更高效地利用云计算，一套「云原生」方法和理念也呼之欲出。\nIstio 开源时间线 Istio 开源发展时间线如下图所示。\nIstio 开源发展时间线示意图 下面我们来简单回顾下 Istio 开源大事件：\n2016 年 9 月：因为 Envoy 是 Istio 中的重要组成，Istio 的开源时间线应该有 Envoy 一部分。起初 Envoy 在 Lyft 内部仅作为边缘代理，开源前已在 Lyft 内部得到大规模生产验证并受到了 Google 工程师的注意 1，那时候 Google 正打算推出一个服务网格的开源项目。2017 年，Lyft 将 Envoy 捐献给了 CNCF 。 2017 年 5 月：Istio 由 Google、IBM 和 Lyft 联合宣布开源 2。一开始就使用了微服务架构，确定了数据平面和控制平面的组成以及 Sidecar 模式。 2018 年 3 月：Kubernetes 顺利的成为从 CNCF 中第一个毕业的项目，变得越来越「无聊」，基础 API 已经定型，CNCF 正式将服务网格（Service Mesh）写入到了云原生的第二版定义 3 中。笔者当前就职的公司 Tetrate ，也是在那时由 Google Istio 初创团队创业成立的。服务网格在中国开始爆发，ServiceMesher 社区也在蚂蚁集团的支持下成立，在中国布道服务网格技术。 2018 年 7 月：Istio 1.0 发布，号称「生产可用」，Istio 团队重组。 2020 年 3 月：Istio 1.5 发布，架构回归单体，发布周期确定，每三个月发布一个大版本，API 趋于稳定。 2020 年至今：Istio 的发展主要着重于 Day 2 Operation 4、性能优化和扩展性发面，多个围绕 Istio 生态的开源项目开始出现，例如 Slime 、Areaki 、Merbridge 。 为什么 Istio 会在 Kubernetes 之后出现？ 微服务和容器化之后，异构语言使用的增加，服务的数量激增，容器的生命周期变短是导致服务网格出现的根本原因。\n我们先来看下服务从部署在 Kubernetes 到 Istio 中架构的变迁，然后再探讨架构演进过程中 Istio 的需求，下文假定读者已了解 Kubernetes 和 Istio 的架构 。\nKubernetes 到 Istio 的架构改变示意图 从 Kubernetes 到 Istio，概括的讲应用的部署架构有如下特点：\nKubernetes 管理应用的生命周期，具体来说，就是应用的部署和管理（扩缩容、自动恢复、发布策略）；\n基于 Kubernetes 的自动 sidecar 注入，实现了透明流量拦截。先通过 sidecar 代理拦截到微服务间流量，再通过控制平面配置管理微服务的行为。如今服务网格的部署模式也迎来了新的挑战，sidecar 已经不是 Istio 服务网格所必须的，基于 gRPC 的无代理的服务网格 5 也在测试中。\n服务网格将流量管理从 Kubernetes 中解耦，服务网格内部的流量无须 kube-proxy 组件的支持，通过类似于微服务应用层的抽象，管理服务间的流量，实现安全性和可观察性功能。\n控制平面通过 xDS 协议发放代理配置给数据平面，已实现 xDS 的代理有 Envoy 和蚂蚁开源的 MOSN 。\nKubernetes 集群外部的客户端访问集群内部服务时，原先是通过 Kubernetes Ingress ，在有了 Istio 之后，会通过 Gateway 来访问 6。\nKubernetes 容器编排与可编程代理 Envoy 为 Istio 的出现打下了坚实的基础。\n从上面 Kubernetes 到 Istio 的架构的转变的描述中，我们可以看到为了让开发者最小成本地管理服务间的流量，Istio 需要解决三个问题：\n透明劫持应用间的流量：Istio 开源最初的目标是成为网络基础设施，就像水和电人类的基础设施一样，我们使用水电不需要关心如何取水和发电，只需要打开水龙头，按下开关即可。透明流量劫持对于开发者来说，就像使用水和电，不需要修改应用程序就可以快速使用 Istio 带来的流量管理能力； 代理集群的运维：如何为每个应用注入一个代理，同时高效地管理这些分布式的 sidecar 代理； 可编程代理：代理可以通过 API 动态配置，还要有出色的性能与可扩展性； 以上三个条件对于 Istio 服务网格来说缺一不可，而且，从中我们可以看到，这些要求基本都是对于 sidecar 代理的要求，这个代理的选择将直接影响该项目的走向与成败。为了解决以上三个问题，Istio 选择了 Kubernetes 容器编排和可编程代理 Envoy。\n透明流量劫持 如果你使用的是如 gRPC 这类中间件开发微服务，在程序中集成 SDK 后，SDK 中的拦截器会自动为你拦截流量，如下图所示。\ngRPC 的拦截器示意图 如何让 Kubernetes pod 中的流量都通过代理呢？答案是在每个应用程序 pod 中注入一个代理，与应用共享网络空间，再通过修改 pod 内的流量路径，让所有进出 pod 的流量都经过 sidecar，其架构如下图所示。\nIstio 中的透明流量劫持示意图 从图中我们可以看到其中有一套非常复杂的 iptables 流量劫持逻辑（详见 Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解 ），使用 iptables 的好处是适用于任何 Linux 操作系统。但是这也带来了一些副作用：\nIstio 网格中所有的服务都需要在进出 pod 时都增加了一个网络跳跃点（hop），虽然每次 hop 可能只有两三毫秒，但是随着网格中服务和服务间的依赖增加，这种延迟可能会显著增加，对于那种追求低延迟的服务可能就不适用于服务网格了； 因为 Istio 向数据平面中注入了大量的 sidecar，尤其是当服务数量增大时，控制平面需要下发更多的 Envoy 代理配置到数据平面，这样会使数据平面占用大量的系统内存和网络资源； 针对这两个问题，如何优化服务网格呢？\n使用 proxyless 模式：取消 sidecar 代理，重新回到 SDK； 优化数据平面：减少下发到数据平面的配置的频率和大小； eBPF：使用 eBPF 优化网络劫持； 本文将在后面性能优化 一节讲解这些细节。\nSidecar 运维管理 Istio 是在 Kubernetes 的基础上构建的，它可以利用 Kubernetes 的容器编排和生命周期管理，在 Kubernetes 创建 pod 时，通过准入控制器自动向 pod 中注入 sidecar。\n为了解决 Sidecar 的资源消耗问题，有人为服务网格提出了有四种部署模式，如下图所示。\n服务网格的四种部署模式示意图 下表中详细对比了这四种部署方式，它们各有优劣，具体选择哪种根据实际情况而定。\n模式 内存开销 安全性 故障域 运维 Sidecar 代理 因为为每个 pod 都注入一个代理，所以开销最大。 由于 sidecar 必须与工作负载一起部署，工作负载有可能绕过 sidecar。 Pod 级别隔离，如果有代理出现故障，只影响到 Pod 中的工作负载。 可以单独升级某个工作负载的 sidecar 而不影响其他工作负载。 节点共享代理 每个节点上只有一个代理，为该节点上的所有工作负载所共享，开销小。 对加密内容和私钥的管理存在安全隐患。 节点级别隔离，如果共享代理升级时出现版本冲突、配置冲突或扩展不兼容等问题，则可能会影响该节点上的所有工作负载。 不需要考虑注入 Sidecar 的问题。 Service Account / 节点共享代理 服务账户 / 身份下的所有工作负载都使用共享代理，开销小。 工作负载和代理之间的连接的认证及安全性无法保障。 节点和服务账号之间级别隔离，故障同“节点共享代理”。 同“节点共享代理”。 带有微代理的共享远程代理 因为为每个 pod 都注入一个微代理，开销比较大。 微代理专门处理 mTLS，不负责 L7 路由，可以保障安全性。 当需要应用 7 层策略时，工作负载实例的流量会被重定向到 L7 代理上，若不需要，则可以直接绕过。该 L7 代理可以采用共享节点代理、每个服务账户代理，或者远程代理的方式运行。 同“Sidecar 代理”。 服务网格的四种部署模式对比 可编程代理 Flomesh 的张晓辉曾在 为什么需要可编程代理 博客中详细说明了代理软件的发展演化过程，我下面将引用他的一些观点，说明可编程代理 Envoy 在 Istio 中的关键作用。\n下图展示了代理从配置到可编程模式的演化过程，及每个阶段中的代表性代理软件。\n代理软件的演化示意图 整个代理演化过程都是随着应用从本地和单体，越来越走向大规模和分布式。下面我将简要概括代理软件的发展过程：\n配置文件时代：几乎所有软件都有配置文件，代理软件因为其相对复杂的功能，更离不开配置文件。该阶段的代理主要使用 C 语言开发，包括其扩展模块，突出的代理本身的能力。这也是我们使用代理最原始最基础的形式，这些代理包括 Nginx、Apache HTTP Server、Squid 等； 配置语言时代：这个时代的代理，更具扩展性和灵活性，比如动态数据获取和配套的逻辑判断。代表性代理包括扩 Varnish 和 HAProxy； 脚本语言时代：从脚本语言的引入开始，代理软件才真正走向的可编程，我们可以更方便的使用脚本在代理中增加动态逻辑，增加了开发效率。代表性的代理是 Nginx 及其支持的脚本语言； 集群时代：随着云计算的普及，大规模部署和动态配置 API 成了代理所必需的能力，而且随着网络流量的增加，大规模代理集群也应运而生。这个时代的代表性代理有 Envoy、Kong 等； 云原生时代：多租户、弹性、异构混合云、多集群、安全和可观测，这些都是云原生时代对代理所提出的更高要求，代表性软件有 Istio、Linkerd、Pypi ，它们都为代理构建了控制平面。 这些都是服务网格吗？ 现在我将列举一些流行的服务网格开源项目，让我们一起探索服务网格的发展规律和本质。下表对比了当前流行的服务网格开源项目 7。\n对比项 Istio Linkerd Consul Connect Traefik Mesh Kuma Open Service Mesh (OSM) 当前版本 1.14 2.11 1.12 1.4 1.5 1.0 许可证 Apache …","relpermalink":"/blog/beyond-istio-oss/","summary":"本文从云原生大背景下重新审视 Istio，讲解 Istio 诞生，在云原生技术栈中的地位及发展方向。","title":"Beyond Istio OSS —— Istio 服务网格的现状与未来"},{"content":" 读者须知 SPIRE 支持是 Istio 1.14 的新特性，请确保安装 1.14 及以上版本的 Istio。 SPIRE 是 SPIFFE 规范的一个生产就绪的实现，它可以执行节点和工作负载证明，以便安全地将加密身份发给在异构环境中运行的工作负载。通过与 Envoy 的 SDS API 集成，SPIRE 可以被配置为 Istio 工作负载的加密身份来源。Istio 可以检测到 UNIX 域套接字的存在，该套接字在定义的套接字路径上实现了 Envoy SDS API，允许 Envoy 直接从它那里进行通信和获取身份。\n我在上一篇文章 中介绍了为什么 Istio 要使用 SPIRE 做身份认证，在这篇博客中我将为你介绍如何在 Istio 中集成 SPIRE。\n安装 Istio 到 GitHub 上下载 Istio 安装包，解压后，建议使用 istioctl 安装 Istio。\nistioctl install --set profile=demo 如果你安装了低于 1.14 版本的 Istio，请先升级 Istio。\n安装 SPIRE 我们使用 Istio 提供的快速安装方式：\nkubectl apply -f samples/security/spire/spire-quickstart.yaml 这将安装以下组件：\nnamespace/spire created csidriver.storage.k8s.io/csi.spiffe.io created customresourcedefinition.apiextensions.k8s.io/spiffeids.spiffeid.spiffe.io created clusterrolebinding.rbac.authorization.k8s.io/k8s-workload-registrar-role-binding created clusterrole.rbac.authorization.k8s.io/k8s-workload-registrar-role created configmap/k8s-workload-registrar created serviceaccount/spire-server created configmap/trust-bundle created clusterrole.rbac.authorization.k8s.io/spire-server-trust-role created clusterrolebinding.rbac.authorization.k8s.io/spire-server-trust-role-binding created configmap/spire-server created statefulset.apps/spire-server created service/spire-server created serviceaccount/spire-agent created clusterrole.rbac.authorization.k8s.io/spire-agent-cluster-role created clusterrolebinding.rbac.authorization.k8s.io/spire-agent-cluster-role-binding created configmap/spire-agent created daemonset.apps/spire-agent created 需要给打 patch，这是为了让所有 sidecar 和 Ingress 都可以共享 SPIRE agent 的 UNIX Domain Socket。\nistioctl install --skip-confirmation -f - \u0026lt;\u0026lt;EOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system spec: profile: default meshConfig: trustDomain: example.org values: global: # This is used to customize the sidecar template sidecarInjectorWebhook: templates: spire: | spec: containers: - name: istio-proxy volumeMounts: - name: workload-socket mountPath: /run/secrets/workload-spiffe-uds readOnly: true volumes: - name: workload-socket csi: driver: \u0026#34;csi.spiffe.io\u0026#34; components: ingressGateways: - name: istio-ingressgateway enabled: true label: istio: ingressgateway k8s: overlays: - apiVersion: apps/v1 kind: Deployment name: istio-ingressgateway patches: - path: spec.template.spec.volumes.[name:workload-socket] value: name: workload-socket csi: driver: \u0026#34;csi.spiffe.io\u0026#34; - path: spec.template.spec.containers.[name:istio-proxy].volumeMounts.[name:workload-socket] value: name: workload-socket mountPath: \u0026#34;/run/secrets/workload-spiffe-uds\u0026#34; readOnly: true EOF 安装好 Istio 和 SPIRE 后，我们就可以注册负载了。\n自动注册 Kubernetes 负载 在快速安装 SPIRE 的时候，我们已经安装了 SPRIE Kubernetes Workload Registrar ，也就是说我们已经开启自动负载注册。现在检查一下 SPIRE 是否给负载颁发了身份证明。\nkubectl exec -i -t spire-server-0 -n spire -c spire-server -- /bin/sh -c \u0026#34;bin/spire-server entry show -socketPath /run/spire/sockets/server.sock\u0026#34; 你将看到例如下面这样的结果：\n# Node Entry ID : 3f17c8be-1379-4b7c-9a01-90805165d59f SPIFFE ID : spiffe://example.org/k8s-workload-registrar/demo-cluster/node/gke-jimmy-cluster-default-pool-d5041909-3atb Parent ID : spiffe://example.org/spire/server Revision : 0 TTL : default Selector : k8s_psat:agent_node_uid:cbab5123-b32f-49d0-89f2-0a7e4d2b0edd Selector : k8s_psat:cluster:demo-cluster # Ingress gateway Entry ID : ffc76b2e-e602-4ad3-8069-993ffbf4440e SPIFFE ID : spiffe://example.org/ns/istio-system/sa/istio-ingressgateway-service-account Parent ID : spiffe://example.org/k8s-workload-registrar/demo-cluster/node/gke-jimmy-cluster-default-pool-d5041909-0ucb Revision : 1 TTL : default Selector : k8s:node-name:gke-jimmy-cluster-default-pool-d5041909-0ucb Selector : k8s:ns:istio-system Selector : k8s:pod-uid:be32b10a-b5a4-4716-adaf-eab778f58c13 DNS name : istio-ingressgateway-6989cbc776-87qb6 DNS name : istio-ingressgateway.istio-system.svc # SPIRE Server Entry ID : 54444848-95ec-4b4d-a7c5-902a4049c96a SPIFFE ID : spiffe://example.org/ns/spire/sa/spire-server Parent ID : spiffe://example.org/k8s-workload-registrar/demo-cluster/node/gke-jimmy-cluster-default-pool-d5041909-0ucb Revision : 1 TTL : default Selector : k8s:node-name:gke-jimmy-cluster-default-pool-d5041909-0ucb Selector : k8s:ns:spire Selector : k8s:pod-uid:4917defc-9b5a-42d8-9b98-c0e0a48c0313 DNS name : spire-server-0 DNS name : spire-server.spire.svc 每个 node 的 parent ID 是 spiffe://example.org/ns/spire/sa/spire-server，而 spiffe://example.org/ns/spire/sa/spire-server 的 parent 是 spiffe://example.org/k8s-workload-registrar/demo-cluster/node/gke-jimmy-cluster-default-pool-d5041909-0ucb。\n也就是说工作负载的层级是这样的：\nSPIRE Server：spiffe://example.org/spire/server\nKubernetes 节点：spiffe://example.org/k8s-workload-registrar/demo-cluster/node/\n普通服务账户：spiffe://example.org/{namespace}/spire/sa/{service_acount}\nSPIRE 通过标签选择器选择工作负载，为它们创建 SPIFFE ID。\n工作负载的 SPIFEE ID 格式为 spiffe://\u0026lt;trust.domain\u0026gt;/ns/\u0026lt;namespace\u0026gt;/sa/\u0026lt;service-account\u0026gt;。\n部署应用 下面我们部署一个应用，然后检查下 SPIRE 为该应提供的身份。\n使用下面的命令部署 Istio 提供的示例应用 sleep：\nistioctl kube-inject --filename samples/security/spire/sleep-spire.yaml …","relpermalink":"/blog/how-to-integrate-spire-with-istio/","summary":"本文将带你一步一步在 Istio 中集成 SPIRE 身份认证。","title":"如何在 Istio 中集成 SPIRE？"},{"content":"近日 InfoQ 发布了 DevOps and Cloud InfoQ Trends Report – June 2022 ，因为报告中所覆盖的技术领域过于宽泛，本文仅仅是对这篇报告的一点个人解读。我基本认同我关注的这些技术中「创新者」和「后期大众」阶段技术的划分。\n报告概括 下面这段节选自原文的 takeaways：\n数据可观察性将帮助企业更好地了解和排除其数据密集型系统的故障。 云原生应用采用无服务器和分布式 SQL 数据库的情况也越来越多。 FinOps 将走向成熟。 eBPF 和 WASM 是令人振奋的新技术，它们被用来在服务网格内开启可观察性、监控和安全的新方法。我们认为这处于创新者阶段。 低代码或无代码平台继续成熟，特别是用于内部工具和自动化用途。 我们还看到「开发者体验作为决策驱动力」的趋势得到了更多的关注，特别是在云平台领域。「平台工程师」的角色正在许多规模的组织中出现，以支持相关平台抽象、API 和工具的建设。 报告解读 这篇报告为什么命名为「DevOps 和云」我就不太清楚了，我觉得把名字换成「云计算」、「云原生」也是可以的，可能是为了延续之前的报告风格吧，毕竟 InfoQ 已经推出过很多期此类报告了。这类报告都是根据「鸿沟理论」将当前流行的技术分成以下阶段：\n创新者 早期采用者 早期大众 后期大众 落后者 不过 InfoQ 的报告中没有「落后者」这个阶段。\n什么是鸿沟理论？ 鸿沟理论指的就是高科技产品在市场营销过程中遭遇的最大障碍：高科技企业的早期市场和主流市场之间存在着一条巨大的鸿沟，能否顺利跨越鸿沟并进入主流市场，成功赢得实用主义者的支持，就决定了一项高科技产品的成败。实际上每项新技术都会经历鸿沟。关键在予采取适当的策略令高科技企业成功地“跨越鸿沟”，摩尔在这本书中就告诉了人们一些欠经考验的制胜秘诀。 下图展示的是跨越鸿沟理论中不同阶段人群的分类及占比。\n跨越鸿沟理论中不同阶段人群的分类及占比 我们来看下 InfoQ 6 月新出的「云和 DevOps」趋势报告。\n软件开发云和 DevOps 趋势图（2022 年 6 月） 我们可以看到像低代码、eBPF、Data Mesh、WASM 已经出现在创新者视线里了。Service Mesh 还在「早期采用者」阶段，这点比我预想的要慢好多，我以为服务网格已经跨越鸿沟了，你觉得呢？\n","relpermalink":"/blog/cloud-and-devops-trend-2022/","summary":"近日 InfoQ 发布了 DevOps and Cloud InfoQ Trends Report – June 2022，因为报告中所覆盖的技术领域过于宽泛，本文仅仅是对这篇报告的一点个人解读。","title":"2022 年云和 DevOps 趋势报告"},{"content":"零信任（Zero Trust）是一种安全理念，而不是一种所有安全团队都要遵循的最佳实践。零信任概念的提出是为了给云原生世界带来更安全的网络。零信任是一种理论状态，即网络内的所有消费者都不仅没有任何权限，而且也不具备对周围网络的感知。\n零信任的基础 零信任网络中的所有用户，包括机器和人类，都需要通过一个密码学验证的身份。要是实践零信任，需要从引入用户身份开始，然后考虑限制用户的最小访问权限。零信任实践的基础是认证和授权，这是比起传统的安全策略来说，零信任中的认证变得更加严格，而授权将变得更加细化。举个例子，传统的授权是：“用户 A 可以访问数据中心 D 吗”，而在零信任的框架下将变成“在某个特定的时间点，在某个特定的地区，使用某个特定的设备的用户 A 可以访问某个特定应用中的某个特定文件吗？”\n越来越细化的授权 在 Kubernetes 中，我们使用 RBAC 来管理权限。所有用户都是以组为基础来授予或拒绝访问权限，单个用户（服务账户）会被授予过多的访问权限。零信任的一个重要特征就是更细的粒度，基于角色来授予访问权限是不够安全的。我们需要细化用户对单一资源在限定时间内的访问权限。这正好与微服务背后的原则相契合——随着服务和数据被分解成更小的部分，就有可能允许我们细化地位服务授予访问权限。\n有时间限制的授权 关于授权，我们往往会存在一种误解，即一个用户一旦被认证和授权，他就成了一个“受信任”的用户，该用户就可以随时的对系统进行访问。然后，在零信任网络中，没有受信任的用户或设备。用户的每一次访问都需要经过认证和授权。而且，授权还会有一个时间窗口，用户只能在这个规定的实践窗口中执行规定的动作。\n如何在企业内实行零信任网络 因为网络是企业系统的命脉之一，牵一发而动全身，要在企业内实行零信任网络，通常需要战略高层管理人员接纳零信任，通过自上而下的方式强加给安全团队。然后渐进式的改进你的网络，从一个关键业务开始，使其变成零信任的。\n在零信任网络里，默认是拒绝一切访问。需要在应用程序开发中，积极主动的允许来自应用程序的某些适当的请求。身份是零信任的基础，而不是网络。零信任的关注对象是访问点、身份认证与授权和攻击面。对于云原生应用，因为它们的生命周期短暂且是动态的，为了实现零信任，你必须为每个访问点配置一个规则，不断更新应用程序的证书和访问规则，这时候手动配置几乎是不可能的，你必须实现自动化。\n在 IstioCon 2022 的主题演讲 有提到，Istio 正在成为零信任的一个重要组成部分。其中最主要的是面向身份的控制，而不是面向网络的控制。这方面的核心原则在谷歌白皮书《BeyondProd：云原生安全的新方法》 上有描写。\n如果我们能将身份概念扩展到用户，并为我们提供灵活而丰富的策略机制来指定、监控和跟踪访问控制，我们就能达到一个可操作的零信任架构 —— 将用户、服务和数据统一到一个管理层。我所工作的公司 Tetrate 创建了 Tetrate Service Bridge —— 可供大型组织使用的管理平面，也是践行了零信任的理念。\n总结 零信任是一种安全理念，它的基础是认证和授权。但比起传统网络安全方法来说，零信任具有如下特点：\n系统中的所有工作负载都有一个密码学验证的身份 零信任网络默认拒绝所有访问 具有更细粒度的访问授权。 为了在云原生应用中实行零信任，你需要：\n自上而下的推行 从关键业务入口 建立自动化工具 参考 下面有一些资料你可以参考：\n写给 Kubernetes 工程师的 mTLS 指南 利用服务网格为基于微服务的应用程序实施 DevSecOps Istio 捐献给 CNCF 意味着什么？ ","relpermalink":"/blog/what-is-zero-trust/","summary":"本文向你介绍零信任 —— 什么是零信任，它跟传统网络安全的区别，如何在企业内实施零信任。","title":"什么是零信任？"},{"content":"今年 6 月初，Istio 1.14 发布 ，该版本中最值得关注的特性是新增对 SPIRE 的支持。SPIFFE 和 SPIRE 都是 CNCF 孵化项目，其中 SPIRE 是 SPIFFE 的实现之一。本文将带你了解 SPIRE 对于零信任架构的意义，以及 Istio 是为何使用 SPIRE 实现身份认证。\nKubernetes 中的身份认证 我们都知道 Istio 最初是基于 Kubernetes 建立起来的，在谈在 Istio 中使用 SPIRE 做身份认证之前，我们先来看下 Kubernetes 中如何做身份认证。\n我们来看一个 pod 的 token 的例子，下面是 default 命名空间下 sleep pod 的 Service Account 的 token。\napiVersion: v1 data: ca.crt: {CA_CRT} namespace: ZGVmYXVsdA== token: {TOKEN_STRING} kind: Secret metadata: annotations: kubernetes.io/service-account.name: sleep kubernetes.io/service-account.uid: 2c0d00e8-13a2-48d0-9ff8-f987f3325ecf creationTimestamp: \u0026#34;2022-06-14T03:01:35Z\u0026#34; name: sleep-token-gwhwd namespace: default resourceVersion: \u0026#34;244535398\u0026#34; uid: b8822ceb-9553-4a17-96dc-d525bbaed0e0 type: kubernetes.io/service-account-token 我们看到其中有 ca.crt 和 token 字段，如果这个 token 被窃取，会有什么后果？Kubernetes 中使用 Service Account 来管理 Pod 的身份，然后利用 RBAC 指定具有某 Service Account 的 Pod 对 Kubernetes API 的权限。Service Account 的 token 存储在 Secret 中，token 中并不包含工作负载所运行的节点、pod 的声明，一旦 token 被窃取破坏者就获得了该账户的所有权限，伪装成该用户窃取信息或破坏。\n一个 token 只能在一个集群中标记负载身份，Istio 同时支持 Kubernetes 环境和虚拟机，还有多集群多网格，如何统一这些异构环境中的工作负载身份？这时，一个统一的工作负载身份标准就呼之欲出了。\nSPIFFE 与 SPIRE 简介 SPIFFE 的目的是基于零信任的理念，建立一个开放、统一的工作负载身份标准，这有助于建立一个零信任的全面身份化的数据中心网络。SPIFFE 的核心是通过简单 API 定义了一个短期的加密身份文件 SVID，用作工作负载认证时使用的身份文件，例如建立 TLS 连接或签署和验证 JWT 令牌等。SPIRE 可以根据管理员定义的策略自动轮换 X.509 SVID 证书和秘钥。Istio 可以通过 SPIRE 动态的消费工作负载标识，SPIRE 可以动态的提供工作负载标识。\n下面我将为你简单介绍一下与 SPIFFE 相关的一些术语。\nSPIFFE（Secure Production Identity Framework For Everyone）是一套身份认证标准。 SPIRE（SPIFFE Runtime Environment）是 SPIFFE 标准的一套生产就绪实现。 SVID（SPIFFE Verifiable Identity Document）是工作负载向资源或调用者证明其身份的文件。SVID 包含一个 SPIFFE ID，代表了服务的身份。它将 SPIFFE ID 编码在一个可加密验证的文件中，目前支持两种格式：X.509 证书或 JWT 令牌。 SPIFFE ID 是一个统一资源标识符（URI），其格式如下：spiffe://trust_domain/workload_identifier。 SPIRE 包含 Server 和 Agent 两个部分，它们的作用如下。\nSPIRE Server\n身份映射 节点认证 SVID 颁发 SPIRE Agent\n工作负载认证 提供工作负载 API SPIFFE 与零信任安全 零信任的本质是以身份为中心的动态访问控制。动态证书轮换、动态证书下发、动态权限控制。SPIFFE 解决的是标识工作负载的问题。\n在虚拟机时代我们可能根据一个 IP 地址和端口来标识一个工作负载，基于 IP 地址标识存在多个服务共享一个 IP 地址，IP 地址伪造和访问控制列表过大等问题。到了 Kubernetes 时代，容器的生命周期是短暂的，我们无法再用 IP 地址来标识负载，而是通过 pod 或 service 名称。但是，不同的云、软件平台对工作负载标识的方法不同，相互之间存在兼容性问题。尤其是在异构混合云的中，同时存在虚拟机和容器的工作负载。这时，建立一个细粒度、具有互操作性的标识系统，将具有重要意义。\n在 Istio 中使用 SPIRE 做身份认证 Istio 会利用 SPIRE 为每个工作负载提供一个唯一标识，服务网格中的工作负载在进行对等身份认证、请求身份认证和授权策略都会使用到服务标识，用于验证访问是否被允许。SPIRE 原生支持 Envoy SDS API，SPIRE Agent 中的通过与工作负载中共享的 UNIX Domain Socket 通信，为工作负载颁发 SVID。请参考 Istio 文档 了解如何在 Istio 中使用 SPIRE 做身份认证。\nSDS 最重要的好处就是简化了证书管理。如果没有这个特性，在 Kubernetes deployment 中，证书就必须以 secret 的方式被创建，然后挂载进代理容器。如果证书过期了，就需要更新 secret 且代理容器需要被重新部署。如果使用 SDS，Istio 可以使用 SDS 服务器会将证书推送给所有的 Envoy 实例。如果证书过期了，服务器仅需要将新证书推送至 Envoy 实例，Envoy 将会立即使用新证书且不需要重新部署代理容器。\n下图展示了 Istio 中使用 SPIRE 进行身份认证的架构。\nIstio 中使用 SPIRE 进行身份认证的架构图 在 Kubernetes 集群中的 spire 命名空间中使用 StatefulSet 部署 SPIRE Server 和 Kubernetes Workload Registrar，使用 DaemonSet 资源为每个节点部署一个 SPIRE Agent。假设你在安装 Kubernetes 时使用的是默认的 DNS 名称 cluster.local，Kubernetes Workload Registar 会为 Istio Mesh 中的工作负载创建如下格式的身份：\nSPRRE Server:spiffe://cluster.local/ns/spire/sa/server SPIRE Agent:spiffe://cluster.local/ns/spire/sa/spire-agent Kubernetes Node:spiffe://cluster.local/k8s-workload-registrar/demo-cluster/node/ Kubernetes Worload Pod:spiffe://cluster.local/{namespace}/spire/sa/{service_acount} 这样不论是节点还是每个工作负载都有它们全局唯一的身份，而且还可以根据集群（信任域）扩展。\nIstio 中的工作负载身份验证过程如下图所示。\nIstio 服务网格中的工作负载身份认证过程示意图 详细过程如下：\n工作负载的 sidecar 中的 pilot-agent 会通过共享的 UDS 调用 SPIRE Agent 来获取 SVID SPIRE Agent 询问 Kubernetes（准确的说是节点上的 kubelet）获取负载的信息 Kubelet 将从 API server 查询到的信息返回给工作负载验证器 验证器将 kubelet 返回的结果与 sidecar 共享的身份信息比对，如果相同，则将正确的 SVID 缓存返回给工作负载，如果不同，则身份认证失败 关于工作负载的注册和认证的详细过程请参考 SPIRE 文档 。\n总结 身份是零信任网络的基础，SPIFFE 统一了异构环境下的身份标准。在 Istio 中不论我们是否使用 SPIRE，身份验证对于工作负载来说是不会有任何感知的。通过 SPIRE 来为工作负载提供身份验证，可以有效的管理工作负载的身份，为实现零信任网络打好基础。\n","relpermalink":"/blog/why-istio-need-spire/","summary":"本文将带你了解 SPIRE 对于零信任架构的意义，以及 Istio 是为什么使用 SPIRE 实现身份认证。","title":"为什么 Istio 要使用 SPIRE 做身份认证？"},{"content":"最近开始对 eBPF 技术和 Cilium 开源项目颇为感兴趣，也翻译了一本 eBPF 相关的资料《什么是 eBPF 》，以及撰写了 请暂时抛弃使用 eBPF 取代服务网格和 sidecar 模式的幻想 的文章，想要再深入了解一下 Cilium 和 eBPF 这样的技术就服务网格领域究竟能发挥什么样的作用，那么就先从 Cilium 开始吧！\n这本《Cilium 中文指南》目前译自 Cilium 官方文档 ，根据 1.11 版本，节选了以下章节：\n概念：描述了 Cilium 的组件以及部署 Cilium 的不同模式。提供高层次的 运行一个完整的 Cilium 部署并理解其行为所需的高层次理解。 开始：快速开始使用 Cilium。 策略：详细介绍了策略语言结构和支持的格式。 内部原理：介绍了一些组件的内部细节。 其他未翻译部分主要涉及命令、API 及运维，本指南未来会加入笔者个人观点及其他内容，欢迎阅读和评论 。\n","relpermalink":"/notice/cilium-handbook/","summary":"一本关于 Cilium 和 eBPF 的电子书发布。","title":"《Cilium 中文指南》发布"},{"content":" 云原生社区最新力作 —— 《深入理解 Istio》上市开售 2017 年 5 月，Google、IBM 和 Lyft 联合 宣布 将 Istio 开源，不知不觉中距今已 5 年有余。在这 5 年多的时间里，Istio 项目从一颗种子长成了参天大树。尤其是在 2018 年 Istio 1.0 版本发布的接下来两年里，国内有多本关于 Istio 服务网格的图书上市。在 Istio 图书出版领域，我国走在了世界的前列。\nIstio 开源时间线 服务网格：云原生的核心技术之一 如今在国内，Istio 几乎可以作为服务网格的代名词，作为 CNCF（云原生计算基金会）定义的云原生 关键技术之一，服务网格发展至今经历了以下几个阶段。\n探索阶段：2017 —2018 年 早期采用者阶段：2019—2020 年 大规模落地及生态发展阶段：2021 年至今 2018 年，CNCF 对云原生的定义是：云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式 API。\n可见，CNCF 将服务网格加入了云原生定义中，即服务网格是云原生的代表性技术之一。如今，Google 正在将 Istio 捐献给 CNCF，我们有理由相信，成为 CNCF 项目后，Istio 的社区会开放，它未来的发展之路也会更顺畅。\n服务网格与云原生应用 云原生的发展方兴未艾，虽然不断有新的技术和产品出现，但作为整个云原生技术栈的一部分，服务网格在过去一年里不断夯实了它作为“云原生网络基础设施”的定位。下图展示了云原生技术栈模型，其中的每一层都有一些代表性的技术来定义标准。作为新时代的中间件，服务网格与其他云原生技术交相辉映，如 Dapr（分布式应用程序运行时）定义了云原生中间件的能力模型，OAM 定义了云原生应用程序模型等，而服务网格定义了云原生七层网络模型。\n云原生应用技术栈 为什么需要服务网格 使用服务网格并非意味着与 Kubernetes 决裂，而是自然而然的事情。Kubernetes 的本质是通过声明配置对应用进行生命周期管理，而服务网格的本质是提供应用间的流量控制和安全性管理，以及可观察性。假如已经使用 Kubernetes 构建了稳定的微服务平台，那么如何设置服务间调用的负载均衡和流量控制呢？\nEnvoy 创造的 xDS 协议被众多开源软件所支持，如 Istio、Linkerd、MOSN 等。Envoy 对服务网格或云原生而言最大的贡献就是定义了 xDS。Envoy 本质上是一个网络代理，是通过 API 配置的现代版代理，基于它衍生出了很多不同的使用场景，如 API 网关、服务网格中的 sidecar 代理和边缘代理。\n技术发展从 Kubernetes 到 Istio，概括来讲有以下原因。\nKubernetes 的本质是应用的生命周期管理，具体来说，就是应用的部署和管理（扩缩容、自 动恢复、发布）。 Kubernetes 为微服务提供了可扩展、高弹性的部署和管理平台。 服务网格的基础是透明代理，先通过 sidecar 代理拦截微服务间的流量，再通过控制平面配置管理微服务的行为。如今，服务网格的部署模式也迎来了新的挑战，sidecar 已经不是服务网格所必须的，基于 gRPC 的无代理的服务网格也在测试中。 xDS 定义了服务网格配置的协议标准，目前基于 gRPC 的 xDS 也正在开发中。 服务网格将流量管理从 Kubernetes 中解耦，服务网格内部的流量无须 kube-proxy 组件的支持，通过接近微服务应用层的抽象，管理服务间的流量，实现安全性和可观察性功能。 服务网格是对 Kubernetes 中 service 更上层的抽象，它的下一步是 Serverless，这也是 Google 在 Istio 之后紧接着推出基于 Kubernetes 和 Istio 之上的 Knative 的原因。 以社区之名成就开源 2018 年 5 月，在蚂蚁金服的支持下，ServiceMesher 社区成立。随后，国内刮起了服务网格的旋风，由社区领导的 Istio 官方文档翻译工作也进入白热化阶段。\n随着时间的推移，我感受到系统介绍 Istio 的中文资料匮乏，于是在 2018 年 9 月开始构思写一本关于 Istio 的图书，并在 GitHub 上发起了 Istio Handbook 的开源电子书项目。几个月后，随着服务网格技术的推广及 ServiceMesher 社区规模的扩大，我在社区的线上线下活动中结识了很多同样热衷于 Istio 和服务网格技术的朋友。我们一致决定，一起写一本 Istio 的开源电子书，将社区积累的宝贵文章和经验集结成系统的文字，分享给广大开发者。\n2019 年 3 月，在社区管理委员会的组织下，几十位成员自愿参与并开始共同撰写此书。2020 年 5 月，为了更好地推广云原生技术，丰富社区分享的技术内容，我们成立了云原生社区，并将原有的 ServiceMesher 社区纳入其中，社区运营的内容也从服务网格技术扩展到更加全面的云原生技术。\n2020 年 10 月，这本书主要的内容贡献者组成了编委会，成员分别有我、马若飞、王佰平、王炜、罗广明、赵化冰、钟华和郭旭东。我们在出版社的指导与帮助下，对本书进行了后续的版本升级、完善、优化等工作。经过反复的迭代，这本《深入理解 Isito：云原生服务网格进阶实战》终于和大家见面了。\n《深入理解 Istio —— 云原生服务网格进阶实战》封面 关于本书 Istio 在 1.5 版本后有了重大的架构变化，同时引入或改进了多项功能，例如，引入了智能 DNS 代理、新的资源对象，改进了对虚拟机的支持等。\n本书以 Istio 新版本为基础编写而成，在持续追踪 Istio 社区最新动向的基础上，力求为读者提 供最新、最全面的内容。另外，本书的多位作者都是一线的开发或运维工程师，具有丰富的 Istio 实战经验，为本书提供了翔实、宝贵的参考案例。\n目前，这本书已经在京东平台上线，要想了解更多有关 Istio 的相关知识，就来读一读这本《深入理解 Isito：云原生服务网格进阶实战》吧！\n京东 618，满 100 减 50，扫码即购！\n点此购买\n","relpermalink":"/blog/istio-service-mesh-book/","summary":"三年磨一剑，云原生社区著《深入理解 Istio —— 云原生服务网格进阶实战》正式上市开售啦！","title":"云原生社区著《深入理解 Istio》正式上市开售"},{"content":"最近 eBPF 技术在云原生社区中持续火热，在我翻译了《什么是 eBPF 》之后，当阅读“云原生环境中的 eBPF”之后就一直在思考 eBPF 在云原生环境中究竟处于什么地位，发挥什么样的作用。当时我评论说“eBPF 开启了上帝视角，可以看到主机上所有的活动，而 sidecar 只能观测到 pod 内的活动，只要搞好进程隔离，基于 eBPF 的 proxy per-node 才是最佳选择”，再看到 William Morgan 的这篇文章 1之后，让我恍然大悟。下面节选翻译了文章我比统同意的观点，即 eBPF 无法替代服务网格和 sidecar，感兴趣的读者可以阅读 William 的原文。\n什么是 eBPF 在过去，如果你想让应用程序处理网络数据包，那是不可能的。因为应用程序运行在 Linux 用户空间，它是不能直接访问主机的网络缓冲区。缓冲区是由内核管理的，受到内核保护，内核需要确保进程隔离，进程之间不能直接读取对方的网络数据包。正确的做法是，应用程序通过系统调用（syscall）来请求网络数据包信息，这本质上是内核 API 调用——应用程序调用 syscall，内核检查应用程序是否有权限获得其请求的数据包；如果有，就把返回数据包。\n有了 eBPF 之后，应用程序不再需要 syscall，数据包不需要在内核空间和用户空间之间来回交互传递。而是我们将代码直接交给内核，让内核自己执行，这样就可以让代码全速运行，效率更高。eBPF 允许应用程序和内核以安全的方式共享内存，eBPF 允许应用程序直接向内核提交代码，目标都是通过超越系统调用的方式来实现性能提升。\neBPF 不是银弹，你不能用 eBPF 运行任意程序，实际上 eBPF 可以做的事情是非常有限的。\neBPF 的局限性 eBPF 的局限性也是因为内核造成的。内核中运行的应用程序应当有自己的租户，这些租户之间会争抢系统的内存、磁盘和网络，内核的职责就是隔离和调度这些应用程序的资源，同时内核还要保护确认应用程序的权限，保护其不被其他程序破坏。\n因为我们直接将 eBPF 代码交给内核执行，这绕过了内核安全保护（如 syscall），内核将面临直接的安全风险。为了保护内核，所有 eBPF 程序要想运行都必须先通过一个验证器。但是要想自动验证程序是很困难的，验证器可能会过度限制程序的功能。比如 eBPF 程序不能是阻塞的，不能有无限循环，不能超过预定的大小；其复杂性也受到限制，验证器会评估所有可能的执行路径，如果 eBPF 程序不能在某些范围内完成，或者不能证明每个循环都有一个退出条件，那么验证器就不会允许该程序运行。有很多应用程序都违反了这些限制，要想将它们作为 eBPF 程序来运行的话，要么重写以满足验证器的需求，要么给内核打补丁，来绕过一些验证（这可能比较困难）。不过随着内核版本的升级，这些验证器也变得更加智能，限制也逐渐变得宽松，也有一些创造性的方法来绕过这些限制。\n但总的来说，eBPF 程序能做的事情非常有限。对于一些重量级事件的处理，例如处理全局范围内的 HTTP/2 流量，或者 TLS 握手协商不能在纯 eBPF 环境中完成。充其量，eBPF 可以做其中的一小部分工作，然后调用用户空间应用程序来处理对于 eBPF 来说过于复杂而无法处理的部分。\neBPF 与服务网格的关系 因为上文所述的 eBPF 的各项限制，七层流量仍然需要用户空间的网络代理来完成，eBPF 并不能替代服务网格。eBPF 可以与 CNI（容器网络接口）一起运行，处理三层/四层流量，而服务网格处理七层流量。\n每个主机一个代理的模式比 sidecar 更糟 对于每个主机一个代理（per-host）的模式，服务网格的早期实践者 Linkerd 1.x 就是这么用的，笔者也是从那个时候开始关注服务网格，Linkerd 1.x 还使用了 JVM 虚拟机！但是经过 Linkerd 1.x 的用户实践证明，这种模式相对于 sidecar 模式，对于运维和安全来说会更糟糕。\n为什么说 sidecar 模式比 per-host 模式更好呢？因为 sidecar 模式有以下几个优势，这是 per-host 模式所不具备的：\n代理的资源消耗随着应用程序的负载而变化。随着实例流量的增加，sidecar 会消耗更多的资源，就像应用程序一样。如果应用程序的流量非常小，那么 sidecar 就不需要消耗很多资源。Kubernetes 现有的管理资源消耗的机制，如资源请求和限制以及 OOM kill，都会继续工作。 代理失败的爆炸半径只限于一个 pod。代理失败与应用失败相同，由 Kubernetes 负责处理失败的 pod。 代理维护。例如代理版本的升级，是通过如滚动更新，灰度发布等应用程序本身相同的机制完成的。 安全边界很清楚（而且很小）：在 pod 级别。Sidecar 在应用程序实例的同一安全上下文中运行。它是 pod 的一部分，与应用程序具有一样的 IP 地址。Sidecar 执行策略，并将 mTLS 应用于进出该 pod 的流量，而且它只需要该 pod 的密钥。 而对于 per-host 模式，就没有上述好处了。代理与应用程序 pod 完全解耦，处理主机上所有 pod 的流量，这样会代理各种问题：\n代理消耗的资源是高度可变的，这取决于在某个时间点 Kubernetes 调度了多少个 pod 在该主机上。你无法有效的预测特定代理的资源消耗情况，这样代理就有崩溃的风险（原文是这么说的，这点笔者还是存疑的，希望有点读者能解帮忙解释下）。 主机上 pod 之间的流量争抢问题。因为主机上的所有流量都经过同一个代理，如果有一个应用程序 pod 的流量极高，消耗了代理的所有资源，主机上的其他应用程序就有被饿死的危险。 代理的爆炸半径很大，而且是不断变化的。代理的故障和升级现在影响到随机的应用程序集合中的一个随机的 pod 子集，意味着任何故障或维护任务都有难以预测的风险。 使得安全问题更加复杂。以 TLS 为例，主机上的代理必须包含该主机上所有应用程序的密钥，这使得它成为一个新的攻击媒介，容易受到混淆代理 问题的影响——代理中的任何 CVE 或漏洞都是潜在的密钥泄露风险。 简而言之，sidecar 模式继续贯彻了容器级别的隔离保护——内核可以在容器级别执行所有安全保护和公平的多租户调度。容器的隔离仍然可以完美的运行，而 per-host 模式却破坏了这一切，重新引入了争抢式的多租户隔离问题。\n当然 per-host 也不是一无是处，该模式最大的好处是可以成数量级的减少代理的数量，减少网络跳数，这也就减少了资源消耗和网络延迟。但是与该模式带来的运维和安全性问题相比，这些优势都是次要的。我们也可以通过持续优化 sidecar 来弥补 sidecar 模式在这方面的不足，而 per-host 模式的缺陷确是致命性的。\n其实归根结底还是回到了争抢式多租户问题上，那么能否利用现有的内核解决方案，改进一下 per-host 模式中的代理，让其支持多租户呢？比如改造 Envoy 代理，使其支持多租户模式。虽然从理论来说这是可行的，但是工作量巨大，Matt Klein 也觉得不值得这样做 2，还不如使用容器来实现租户隔离。而且即使让 per-host 模式中的代理支持了多租户，仍然还有爆炸半径和安全问题需要解决。\n总结 不管有没有 eBPF，在可预见的未来，服务网格都会基于运行在用户空间的 sidecar 代理（proxyless 模式除外）。Sidecar 模式虽然也有弊端，但它依然是既能保持容器隔离和操作的优势，又能处理云原生网络复杂性的最优方案。eBPF 的能力将来是否会发展到可以处理七层网络流量，从而替代服务网格和 sidecar，也许吧，但那一天可能很遥远。\n参考 William Morgan 的 eBPF, sidecars, and the future of the service mesh 这篇文章正好回答了我的关于 eBPF、sidecar 的疑问。 ↩︎\n关于 per-host 模式中的代理改造问题，Twitter 上有一个精彩的讨论 。 ↩︎\n","relpermalink":"/blog/ebpf-sidecar-and-service-mesh/","summary":"不管有没有 eBPF，在可预见的未来，服务网格都会基于运行在用户空间的 sidecar 代理（proxyless 模式除外）。","title":"请暂时抛弃使用 eBPF 取代服务网格和 sidecar 模式的幻想"},{"content":"《什么是 eBPF —— 新一代网络、安全和可观测性工具介绍》译自 O’Reilly 发布的报告“What is eBPF”，作者是 Liz Rice，由 JImmy Song 翻译，英文原版可以在 O’Reilly 网站 上获取，中文版请在 云原生资料库 中阅读。\n","relpermalink":"/notice/what-is-ebpf/","summary":"新翻译了一本 O'Reilly 出品的报告。","title":"《什么是 eBPF》翻译电子书"},{"content":"不知道大家听说没有 PingCAP 推出的一个 OSSInsight.io 网站，可以根据 GitHub 上的事件，提供开源软件洞察，这个项目也开源在 GitHub 上。它可以提供以下方面的洞察能力，有点类似于 Google Analytics、Trends：\n比较 GitHub 仓库历史 Star 趋势图 开发者地理位置分布 开发者贡献时间热力图 编码活力，如每月 PR 数量、代码行数变化 分类趋势排名 网站截图 以下图片来自 OSSInsight 博客 ，展示了该网站的一些功能。\nKubernetes 和 Moby 的标记 star 的人员地理分布 K8s（上）和 Moby（下）的月度推送和提交 分类排名 你可以在首页输入一个 GitHub 仓库，查看该仓库的一些洞察信息。我查看了我的 rootsongjc/kubernetes-handbook 之后，发现它还以获得关注者的公司信息，如下图。\nrootsongjc/kubernetes-handbook 关注者的公司分布 这个网站有点类似于 CNCF 推出的 DevStats ，不过 DevStats 只能洞察 CNCF 托管的项目。\nDevStats 页面 评论 OSSInsight 也可以算是 CHAOSS 类软件的一种，比如 Linux 基金会下的 CHAOSS（Community Health Analytics Open Source Software）工作组有一个开源项目 GrimoireLab 就是做软件开发分析的。\nGrimoireLab 网站页面 如果你关注开源和技术趋势的话，网上还有一些类似的 GitHub 趋势网站，大家可以根据自己的需要选用。\n","relpermalink":"/blog/oss-insight/","summary":"推荐一个 PingCAP 推出的 OSSInsight.io 网站，可以根据 GitHub 上的事件，提供开源软件洞察，这个项目本身也开源在 GitHub 上。","title":"开源项目千千万，如何发现好项目？"},{"content":" 近日听闻 O’Reilly 将永久关闭在线学习网站 KataCoda，对于广大程序员和学习者来说，这无疑是一件痛心疾首的事情，以后我们再也看不到那只会变成的功夫猫了。\nKataCoda 简介 KataCoda 成立于 2016 年，它是一个在线学习平台，提供了上百个交互课程，用户可以登录免费学习。另外用户还可以基于 KataCoda 提供的基础镜像来构建和发布自己的在线课程，这一切都是免费的。\n你可以根据提示，在一个临时的容器环境中操作，输入命令、观察结果，这种实时的反馈式学习方式，让你不需要再为准备环境而操心，大大降低了众多技术的上手门槛，可以说这种方式对于计算机技术教育来说是一种“革命式”的。\nKataCoda 网站界面 这些交互环境没有任何网络限制，你可以访问任何网站，还可以构建临时的公开网站让互联网中的所有用户访问。这种便利可以说是云原生或者容器时代赋予我们的，\nO’Reilly 为什么关闭 KataCoda？ O’Reilly 与 2019 年底收购了 KataCoda，如今关闭该网站应该也实属无奈。在 O’Reilly 官网发布的 仅在 O’Reilly 内部利用 Katacoda 技术以及关闭 katacoda.com 的决定 这篇博客中，我们可以获得以下数据：\nKataCoda 有 28 万会员 KataCoda 上有超过 387,866 名独立用户已花费超过 74,711 小时在平台上学习 O’Reilly 有 280 万会员 以上数据仍然无法支撑 KataCoda 高昂的运营成本，主要是因为免费课程被滥用，比如用来挖矿，发送不良信息（所有免费课程连接互联网没有任何限制而且网速极快）。\nKataCoda 关闭之后怎么办？ KataCoda 上的很多免费课程其实都有在 GitHub 上开源，只有有另一个平台来托管，这些课程就可以继续使用。KataCoda 关闭后，还有众多交互式课程平台可以选择，比如下面这两个：\nKillercoda CloudYuga Instruqt 关于 O’Reilly 关闭 KataCoda 你有什么想法，欢迎在下面留言评论。\n","relpermalink":"/blog/goodbye-katacoda/","summary":"O'Reilly 宣布将于 6 月 15 日关闭在线学习网站 KataCoda。","title":"再见 KataCoda！"},{"content":"本文将指导你如何在 macOS 上编译 Istio 二进制文件和 Docker 镜像。\n构建前的准备 在正式开始构建前，参考这篇文档 ，以下是我的构建环境信息：\nmacOS 12.3.1 Darwin AMD64 Docker Desktop 4.8.1(78998) Docker Engine v20.10.14 开始构建 参考这篇文档 编译 Istio。\n首先在 GitHub 上 下载 Istio 代码，将代码下载到 $GOPATH/src/istio.io/istio 目录下，下文中的命令都在该根目录下执行。\n编译成二进制文件 执行下面的命令下载 Istio 依赖的包，这些包将下载到 vendor 目录下：\ngo mod vendor 然后执行下面的命令构建 Istio：\nsudo make build 如果你没有在命令前加 sudo，你可能遇到下面的错误：\nfatal: unsafe repository (\u0026#39;/work\u0026#39; is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work fatal: unsafe repository (\u0026#39;/work\u0026#39; is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work Makefile.core.mk:170: *** \u0026#34;TAG cannot be empty\u0026#34;. Stop. make: *** [build] Error 2 即使你按照提示执行了 git config --global --add safe.directory /work 在编译过程中还是会出现错误。\n构建完的二进制文件将保存在 out 目录下，其目录结构如下：\nout ├── darwin_amd64 │ ├── bug-report │ ├── client │ ├── envoy │ ├── extauthz │ ├── install-cni │ ├── istio-cni │ ├── istio-cni-taint │ ├── istio-iptables │ ├── istio_is_init │ ├── istioctl │ ├── logs │ ├── operator │ ├── pilot-agent │ ├── pilot-discovery │ ├── release │ └── server └── linux_amd64 ├── envoy ├── envoy-centos ├── logs └── release 同时构建出了 linux_amd64 和 darwin_amd64 架构的二进制文件。\n编译成 Docker 镜像 执行下面的将 Istio 编译成 Docker 镜像：\nsudo make docker 编译根据你的网络情况，大概耗时 3 到 5 分钟。编译完成后，执行下面的命令你将看到 Istio 的 Docker 镜像。\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE localhost:5000/app_sidecar_centos_7 latest 2044037df94b 51 seconds ago 524MB localhost:5000/app_sidecar_ubuntu_jammy latest 5d8ae5ed55b7 About a minute ago 362MB localhost:5000/proxyv2 latest d4679412385f About a minute ago 243MB localhost:5000/install-cni latest 78f46d5771d2 About a minute ago 270MB localhost:5000/istioctl latest c38130a5adc8 About a minute ago 190MB localhost:5000/pilot latest 2aa9185ec202 About a minute ago 190MB localhost:5000/app latest 473adafaeb8d About a minute ago 188MB localhost:5000/operator latest 9ac1fedcdd12 About a minute ago 191MB localhost:5000/ext-authz latest 1fb5aaf20791 About a minute ago 117MB localhost:5000/app_sidecar_debian_11 latest 61376a02b95d 2 minutes ago 407MB localhost:5000/app_sidecar_ubuntu_xenial latest 7e8efe666611 2 minutes ago 418MB 编译出镜像以后，你就可以修改镜像名字并推送到自己的镜像仓库里了。\n总结 以上就是在 macOS 上构建 Istio 的过程，如果你已经下载好了构建所需要的的 Docker 镜像，那么构建时间将不超过一分钟，构建 Docker 镜像也只需要几分钟时间。\n参考 Using the Code Base - github.com ","relpermalink":"/blog/how-to-build-istio/","summary":"本文将指导你如何在 macOS 上编译 Istio。","title":"如何编译 Istio？"},{"content":"本文最早是基于 Istio 1.11 撰写，之后随着 Istio 的版本陆续更新，最新更新时间为 2022 年 5 月 12 日，关于本文历史版本的更新说明请见文章最后。本文记录了详细的实践过程，力图能够让读者复现，因此事无巨细，想要理解某个部分过程的读者可以使用目录跳转到对应的小节阅读。\n为了使读者能够更加直观的了解本文中执行的操作，在阅读本文前你也可以先观看下 Istio Workshop 第八讲视频 。\n观看视频点击观看\n为了理解本文希望你先阅读以下内容：\n理解 iptables Istio 数据平面 Pod 启动过程详解 内容介绍 本文基于 Istio 1.13 版本，将为大家介绍以下内容：\n什么是 sidecar 模式和它的优势在哪里。 Istio 中是如何做 sidecar 注入的。 Sidecar 代理是如何做透明流量劫持的。 iptables 的路由规则。 Envoy 代理是如何路由流量到上游的。 请大家结合下图理解本文中的内容，本图基于 Istio 官方提供的 Bookinfo 示例绘制，展示的是 reviews Pod 的内部结构，包括 Linux Kernel 空间中的 iptables 规则、Sidecar 容器、应用容器。\nIstio 流量劫持示意图 productpage 访问 reviews Pod，入站流量处理过程对应于图示上的步骤：1、2、3、4、Envoy Inbound Handler、5、6、7、8、应用容器。\nreviews Pod 访问 rating 服务的出站流量处理过程对应于图示上的步骤是：9、10、11、12、Envoy Outbound Handler、13、14、15。\n注意：图中的路径 16 近用于路由规则说明，它不不出现在当前示例中。实际上仅当 Pod 内发出的对当前 Pod 内的服务访问的时候才会途径它。\n上图中关于流量路由部分，包含：\nproductpage 服务请求访问 http://reviews.default.svc.cluster.local:9080/，当流量进入 reviews Pod 内部时，流量是如何被 iptables 劫持到 Envoy 代理被 Inbound Handler 处理的； reviews 请求访问 ratings 服务的 Pod，应用程序发出的出站流量被 iptables 劫持到 Envoy 代理的 Outbound Handler 的处理。 在阅读下文时，请大家确立以下已知点：\n首先，productpage 发出的对 reivews 的访问流量，是在 Envoy 已经通过 EDS 选择出了要请求的 reviews 服务的某个 Pod，知晓了其 IP 地址，直接向该 IP 发送的 TCP 连接请求。 reviews 服务有三个版本，每个版本有一个实例，三个版本中的 sidecar 工作步骤类似，下文只以其中一个 Pod 中的 sidecar 流量转发步骤来说明。 所有进入 reviews Pod 的 TCP 流量都根据 Pod 中的 iptables 规则转发到了 Envoy 代理的 15006 端口，然后经过 Envoy 的处理确定转发给 Pod 内的应用容器还是透传。 Sidecar 模式 将应用程序的功能划分为单独的进程运行在同一个最小调度单元中（例如 Kubernetes 中的 Pod）可以被视为 sidecar 模式。如下图所示，sidecar 模式允许您在应用程序旁边添加更多功能，而无需额外第三方组件配置或修改应用程序代码。\nSidecar 模式示意图 就像连接了 Sidecar 的三轮摩托车一样，在软件架构中，Sidecar 连接到父应用并且为其添加扩展或者增强功能。Sidecar 应用与主应用程序松散耦合。它可以屏蔽不同编程语言的差异，统一实现微服务的可观察性、监控、日志记录、配置、断路器等功能。\n使用 Sidecar 模式的优势 使用 sidecar 模式部署服务网格时，无需在节点上运行代理，但是集群中将运行多个相同的 sidecar 副本。在 sidecar 部署方式中，每个应用的容器旁都会部署一个伴生容器（如 Envoy 或 MOSN），这个容器称之为 sidecar 容器。Sidecar 接管进出应用容器的所有流量。在 Kubernetes 的 Pod 中，在原有的应用容器旁边注入一个 Sidecar 容器，两个容器共享存储、网络等资源，可以广义的将这个包含了 sidecar 容器的 Pod 理解为一台主机，两个容器共享主机资源。\n因其独特的部署结构，使得 sidecar 模式具有以下优势：\n将与应用业务逻辑无关的功能抽象到共同基础设施，降低了微服务代码的复杂度。 因为不再需要编写相同的第三方组件配置文件和代码，所以能够降低微服务架构中的代码重复度。 Sidecar 可独立升级，降低应用程序代码和底层平台的耦合度。 Sidecar 注入示例分析 以 Istio 官方提供的 bookinfo 中 productpage 的 YAML 为例，关于 bookinfo 应用的详细 YAML 配置请参考 bookinfo.yaml 。\n下文将从以下几个方面讲解：\nSidecar 容器的注入 iptables 规则的创建 路由的详细过程 apiVersion: apps/v1 kind: Deployment metadata: name: productpage-v1 labels: app: productpage version: v1 spec: replicas: 1 selector: matchLabels: app: productpage version: v1 template: metadata: labels: app: productpage version: v1 spec: serviceAccountName: bookinfo-productpage containers: - name: productpage image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9080 volumeMounts: - name: tmp mountPath: /tmp volumes: - name: tmp emptyDir: {} 再查看下 productpage 容器的 Dockerfile 。\nFROM python:3.7.4-slim COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt COPY test-requirements.txt ./ RUN pip install --no-cache-dir -r test-requirements.txt COPY productpage.py /opt/microservices/ COPY tests/unit/* /opt/microservices/ COPY templates /opt/microservices/templates COPY static /opt/microservices/static COPY requirements.txt /opt/microservices/ ARG flood_factor ENV FLOOD_FACTOR ${flood_factor:-0} EXPOSE 9080 WORKDIR /opt/microservices RUN python -m unittest discover USER 1 CMD [\u0026#34;python\u0026#34;, \u0026#34;productpage.py\u0026#34;, \u0026#34;9080\u0026#34;] 我们看到 Dockerfile 中没有配置 ENTRYPOINT，所以 CMD 的配置 python productpage.py 9080 将作为默认的 ENTRYPOINT，记住这一点，再看下注入 sidecar 之后的配置。\n$ istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml 我们只截取其中与 productpage 相关的 Deployment 配置中的部分 YAML 配置。\ncontainers: - image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 # 应用镜像 name: productpage ports: - containerPort: 9080 - args: - proxy - sidecar - --domain - $(POD_NAMESPACE).svc.cluster.local - --configPath - /etc/istio/proxy - --binaryPath - /usr/local/bin/envoy - --serviceCluster - productpage.$(POD_NAMESPACE) - --drainDuration - 45s - --parentShutdownDuration - 1m0s - --discoveryAddress - istiod.istio-system.svc:15012 - --zipkinAddress - zipkin.istio-system:9411 - --proxyLogLevel=warning - --proxyComponentLogLevel=misc:error - --connectTimeout - 10s - --proxyAdminPort - \u0026#34;15000\u0026#34; - --concurrency - \u0026#34;2\u0026#34; - --controlPlaneAuthPolicy - NONE - --dnsRefreshRate - 300s - --statusPort - \u0026#34;15020\u0026#34; - --trust-domain=cluster.local - --controlPlaneBootstrap=false image: docker.io/istio/proxyv2:1.5.1 # sidecar proxy name: istio-proxy ports: - containerPort: 15090 name: http-envoy-prom protocol: TCP initContainers: - command: - istio-iptables - -p - \u0026#34;15001\u0026#34; - -z - \u0026#34;15006\u0026#34; - -u - \u0026#34;1337\u0026#34; - -m - REDIRECT - -i - \u0026#39;*\u0026#39; - -x - \u0026#34;\u0026#34; - -b - \u0026#39;*\u0026#39; - -d - 15090,15020 image: docker.io/istio/proxyv2:1.5.1 # init 容器 name: istio-init Istio 给应用 Pod 注入的配置主要包括：\nInit 容器 istio-init：用于 pod 中设置 iptables 端口转发 Sidecar 容器 istio-proxy：运行 sidecar 代理，如 Envoy 或 MOSN。 iptables 规则注入解析 为了查看 iptables 配置，我们需要登陆到 sidecar 容器中使用 root 用户来查看，因为 kubectl 无法使用特权模式来远程操作 docker 容器，所以我们需要登陆到 productpage pod 所在的主机上使用 docker 命令登陆容器中查看。\n如果您使用 minikube 部署的 Kubernetes， …","relpermalink":"/blog/sidecar-injection-iptables-and-traffic-routing/","summary":"本文基于 Istio 1.13 版本，介绍了 sidecar 模式及其优势 sidecar 如何注入到数据平面，Envoy 如何做流量劫持和路由转发的，包括 Inbound 流量和 Outbound 流量。","title":"Istio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解"},{"content":"本文将为你讲解：\nIstio 中 sidecar 自动注入过程 Istio 中的 init 容器启动过程 启用了 Sidecar 自动注入的 Pod 的启动流程 下图中展示了 Istio 数据平面中的 Pod 启动完后的组件。\nIstio 数据平面 Pod 内部组件 Istio 中的 sidecar 注入 Istio 中提供了以下两种 sidecar 注入方式：\n使用 istioctl 手动注入。 基于 Kubernetes 的 突变 webhook 准入控制器（mutating webhook addmission controller 的自动 sidecar 注入方式。 不论是手动注入还是自动注入，sidecar 的注入过程都需要遵循如下步骤：\nKubernetes 需要了解待注入的 sidecar 所连接的 Istio 集群及其配置； Kubernetes 需要了解待注入的 sidecar 容器本身的配置，如镜像地址、启动参数等； Kubernetes 根据 sidecar 注入模板和以上配置填充 sidecar 的配置参数，将以上配置注入到应用容器的一侧； 使用下面的命令可以手动注入 sidecar。\nistioctl kube-inject -f ${YAML_FILE} | kuebectl apply -f - 该命令会使用 Istio 内置的 sidecar 配置来注入，下面使用 Istio 详细配置请参考 Istio 官网 。\n注入完成后您将看到 Istio 为原有 pod template 注入了 initContainer 及 sidecar proxy 相关的配置。\nInit 容器 Init 容器是一种专用容器，它在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。\n一个 Pod 中可以指定多个 Init 容器，如果指定了多个，那么 Init 容器将会按顺序依次运行。只有当前面的 Init 容器必须运行成功后，才可以运行下一个 Init 容器。当所有的 Init 容器运行完成后，Kubernetes 才初始化 Pod 和运行应用容器。\nInit 容器使用 Linux Namespace，所以相对应用程序容器来说具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用程序容器则不能。\n在 Pod 启动过程中，Init 容器会按顺序在网络和数据卷初始化之后启动。每个容器必须在下一个容器启动之前成功退出。如果由于运行时或失败退出，将导致容器启动失败，它会根据 Pod 的 restartPolicy 指定的策略进行重试。然而，如果 Pod 的 restartPolicy 设置为 Always，Init 容器失败时会使用 RestartPolicy 策略。\n在所有的 Init 容器没有成功之前，Pod 将不会变成 Ready 状态。Init 容器的端口将不会在 Service 中进行聚集。正在初始化中的 Pod 处于 Pending 状态，但应该会将 Initializing 状态设置为 true。Init 容器运行完成以后就会自动终止。\n关于 Init 容器的详细信息请参考 Init 容器 - Kubernetes 中文指南/云原生应用架构实践手册 。\nInit 容器解析 Istio 在 pod 中注入的 Init 容器名为 istio-init，我们在上面 Istio 注入完成后的 YAML 文件中看到了该容器的启动命令是：\nistio-iptables -p 15001 -z 15006 -u 1337 -m REDIRECT -i \u0026#39;*\u0026#39; -x \u0026#34;\u0026#34; -b \u0026#39;*\u0026#39; -d 15090,15020 我们再检查下该容器的 Dockerfile 看看 ENTRYPOINT 是怎么确定启动时执行的命令。\n# 前面的内容省略 # The pilot-agent will bootstrap Envoy. ENTRYPOINT [\u0026#34;/usr/local/bin/pilot-agent\u0026#34;] 我们看到 istio-init 容器的入口是 /usr/local/bin/istio-iptables 命令行，该命令行工具的代码的位置在 Istio 源码仓库的 tools/istio-iptables 目录。\n注意：在 Istio 1.1 版本时还是使用 isito-iptables.sh 命令行来操作 IPtables。\nInit 容器启动入口 Init 容器的启动入口是 istio-iptables 命令行，该命令行工具的用法如下：\n$ istio-iptables [flags] -p: 指定重定向所有 TCP 流量的 sidecar 端口（默认为 $ENVOY_PORT = 15001） -m: 指定入站连接重定向到 sidecar 的模式，“REDIRECT” 或 “TPROXY”（默认为 $ISTIO_INBOUND_INTERCEPTION_MODE) -b: 逗号分隔的入站端口列表，其流量将重定向到 Envoy（可选）。使用通配符 “*” 表示重定向所有端口。为空时表示禁用所有入站重定向（默认为 $ISTIO_INBOUND_PORTS） -d: 指定要从重定向到 sidecar 中排除的入站端口列表（可选），以逗号格式分隔。使用通配符“*” 表示重定向所有入站流量（默认为 $ISTIO_LOCAL_EXCLUDE_PORTS） -o：逗号分隔的出站端口列表，不包括重定向到 Envoy 的端口。 -i: 指定重定向到 sidecar 的 IP 地址范围（可选），以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量。空列表将禁用所有出站重定向（默认为 $ISTIO_SERVICE_CIDR） -x: 指定将从重定向中排除的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量（默认为 $ISTIO_SERVICE_EXCLUDE_CIDR）。 -k：逗号分隔的虚拟接口列表，其入站流量（来自虚拟机的）将被视为出站流量。 -g：指定不应用重定向的用户的 GID。(默认值与 -u param 相同) -u：指定不应用重定向的用户的 UID。通常情况下，这是代理容器的 UID（默认值是 1337，即 istio-proxy 的 UID）。 -z: 所有进入 pod/VM 的 TCP 流量应被重定向到的端口（默认 $INBOUND_CAPTURE_PORT = 15006）。 以上传入的参数都会重新组装成 iptables 规则，关于该命令的详细用法请访问 tools/istio-iptables/pkg/cmd/root.go 。\n该容器存在的意义就是让 sidecar 代理可以拦截所有的进出 pod 的流量，15090 端口（Mixer 使用）和 15092 端口（Ingress Gateway）除外的所有入站（inbound）流量重定向到 15006 端口（sidecar），再拦截应用容器的出站（outbound）流量经过 sidecar 处理（通过 15001 端口监听）后再出站。关于 Istio 中端口用途请参考 Istio 官方文档 。\n命令解析\n这条启动命令的作用是：\n将应用容器的所有流量都转发到 sidecar 的 15006 端口。 使用 istio-proxy 用户身份运行，UID 为 1337，即 sidecar 所处的用户空间，这也是 istio-proxy 容器默认使用的用户，见 YAML 配置中的 runAsUser 字段。 使用默认的 REDIRECT 模式来重定向流量。 将所有出站流量都重定向到 sidecar 代理（通过 15001 端口）。 因为 Init 容器初始化完毕后就会自动终止，因为我们无法登陆到容器中查看 iptables 信息，但是 Init 容器初始化结果会保留到应用容器和 sidecar 容器中。\nPod 启动流程 启用了 Sidecar 自动注入的 Pod 启动流程如下：\nInit 容器先启动，向 Pod 中注入 iptables 规则，进行透明流量拦截。 随后，Kubernetes 会根据 Pod Spec 中容器的声明顺序依次启动容器，但这是非阻塞的，无法保证第一个容器启动完成后才启动下一个。istio-proxy 容器启动时，pilot-agent 将作为 PID 1 号进程，它是 Linux 用户空间的第一个进程，负责拉起其他进程和处理僵尸进程。pilot-agent 将生成 Envoy bootstrap 配置并拉起 envoy 进程；应用容器几乎跟 istio-proxy 容器同时启动，为了防止 Pod 内的容器在还没启动好的情况而接收到外界流量，这时候就绪探针就派上用场了。Kubernetes 会在 istio-proxy 容器的 15021 端口进行就绪检查，直到 isito-proxy 启动完成后 kubelet 才会将流量路由到 Pod 内。 在 Pod 启动完成后，pilot-agent 将变为守护进程监视系统其他进程，除此之外，该进程还为 Envoy 提供 Bootstrap 配置、证书、健康检查、配置热加载、身份支持及进程生命周期管理等。 Pod 内容器启动顺序问题 在 Pod 启动的过程中存在容器启动顺序问题，假设下面这种情况，应用容器先启动，请求其他服务，这时候 istio-proxy 容器还没启动完成，那么该请求将会失败，如果你的应用的健壮性不足，甚至可能导致应用容器崩溃，进而 Pod 重启。对于这种情况的解决方案是：\n修改应用程序，增加超时重试。 增加应用容器中进程的启动延迟，比如增加 sleep 时间。 在应用容器中增加一个 postStart 配置，检测应用进程是否启动完成，只有当检测成功时，Kubernetes 才会将 Pod 的状态标记为 Running。 总结 这篇文章带领大家了解了 Istio 数据平面中的 Pod 启动过程，还有因为 Pod 内容器启动顺序带来的问题。\n参考 istio 常见问题：Sidecar 启动顺序问题 - imroc.cc ","relpermalink":"/blog/istio-pod-process-lifecycle/","summary":"本文将为你讲解 Istio 的 Init 容器、Pod 内部进程及启动过程。","title":"Istio 数据平面 Pod 启动过程详解"},{"content":"iptables 作为 Linux 内核中的重要功能，有着广泛的应用，在 Istio 中默认就是利用 iptables 做透明流量劫持的。理解 iptables，对于我们理解 Istio 的运作有十分重要的作用。本文将为大家简单介绍下 iptbles。\niptables 简介 iptables 是 Linux 内核中的防火墙软件 netfilter 的管理工具，位于用户空间，同时也是 netfilter 的一部分。Netfilter 位于内核空间，不仅有网络地址转换的功能，也具备数据包内容修改、以及数据包过滤等防火墙功能。\n在了解 Init 容器初始化的 iptables 之前，我们先来了解下 iptables 和规则配置。\n下图展示了 iptables 调用链。\niptables 调用链 图片来源 iptables 中的表 Init 容器中使用的的 iptables 版本是 v1.6.0，共包含 5 张表：\nraw 用于配置数据包，raw 中的数据包不会被系统跟踪。 filter 是用于存放所有与防火墙相关操作的默认表。 nat 用于 网络地址转换 （例如：端口转发）。 mangle 用于对特定数据包的修改（参考损坏数据包 ）。 security 用于强制访问控制 网络规则。 注：在本示例中只用到了 nat 表。\n不同的表中的具有的链类型如下表所示：\n规则名称 raw filter nat mangle security PREROUTING ✓ ✓ ✓ INPUT ✓ ✓ ✓ OUTPUT ✓ ✓ ✓ ✓ ✓ POSTROUTING ✓ ✓ FORWARD ✓ ✓ ✓ 理解 iptables 规则 查看 istio-proxy 容器中的默认的 iptables 规则，默认查看的是 filter 表中的规则。\n$ iptables -L -v Chain INPUT (policy ACCEPT 350K packets, 63M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 18M packets, 1916M bytes) pkts bytes target prot opt in out source destination 我们看到三个默认的链，分别是 INPUT、FORWARD 和 OUTPUT，每个链中的第一行输出表示链名称（在本例中为 INPUT/FORWARD/OUTPUT），后跟默认策略（ACCEPT）。\n每条链中都可以添加多条规则，规则是按照顺序从前到后执行的。我们来看下规则的表头定义。\npkts：处理过的匹配的报文数量 bytes：累计处理的报文大小（字节数） target：如果报文与规则匹配，指定目标就会被执行。 prot：协议，例如 tdp、udp、icmp 和 all。 opt：很少使用，这一列用于显示 IP 选项。 in：入站网卡。 out：出站网卡。 source：流量的源 IP 地址或子网，或者是 anywhere。 destination：流量的目的地 IP 地址或子网，或者是 anywhere。 还有一列没有表头，显示在最后，表示规则的选项，作为规则的扩展匹配条件，用来补充前面的几列中的配置。prot、opt、in、out、source 和 destination 和显示在 destination 后面的没有表头的一列扩展条件共同组成匹配规则。当流量匹配这些规则后就会执行 target。\ntarget 支持的类型\ntarget 类型包括 ACCEPT、REJECT、DROP、LOG 、SNAT、MASQUERADE、DNAT、REDIRECT、RETURN 或者跳转到其他规则等。只要执行到某一条链中只有按照顺序有一条规则匹配后就可以确定报文的去向了，除了 RETURN 类型，类似编程语言中的 return 语句，返回到它的调用点，继续执行下一条规则。target 支持的配置详解请参考 iptables 详解（1）：iptables 概念 。\n总结 以上就是对 iptables 的简要介绍，你已经了解了 iptables 是怎样运行的，规则链及其执行顺序。\n","relpermalink":"/blog/understanding-iptables/","summary":"本文将为大家简单介绍下 iptables，其中的表及执行顺序。","title":"理解 iptables"},{"content":"云原生资料库地址：https://lib.jimmysong.io 云原生资料库项目是一个文档项目，使用 Wowchemy 主题构建，开源在 GitHub 上：https://github.com/rootsongjc/cloud-native-library 同时笔者也对网站首页、菜单和目录结构进行了调整，网站上的图书栏目将使用新的主题来维护，发布在云原生资料库 - 书目列表 中。\n云原生资料库定位 云原生资料库收集了笔者自 2017 年以来的发表和翻译的云原生相关图书、资料等，是对已发布的的十几本资料的一个梳理和补充。原有的资料会继续保存在原仓库中，但不再继续维护。\n另外本站活动栏目也进行了改版，迁移到了新的主题页面 。\n","relpermalink":"/notice/cloud-native-public-library/","summary":"一站式云原生资料库，是对已发布资料的梳理。","title":"云原生资料库发布"},{"content":"更新时间：2022 年 03 月 22 日\n我们正在中国招聘 Team Leader 和开发工程师，全球远程办公，让你工作更自由！\nTetrate logo 你想要加入由世界级工程师组成的团队吗？使用 Istio 、Envoy 、Apache SkyWalking 等开源项目来定义下一代云原生网络。下面是我们正在招聘的部分职位。\n分布式系统工程师 - Go 语言（管理平面团队） 我们正在为跨越传统和现代基础设施的关键任务企业应用建立一个安全、强大、高可用的服务网格平台。管理平面团队负责构建平台的主要负责：用户配置、观察和与网格交互的公共 API，为混合和多云服务网格结构提供多租户和安全性的主要工作流。\n在复杂的大型基础设施和灵活但可管理的、有意义的 API 之间架起桥梁是具有挑战性的。我们正在寻找具有强大的分布式系统经验的工程师加入，帮助建立运行时，使平台能够扩展。\n职责\n在平台核心运行时及其 API 的架构方面进行合作。 设计并实现一个分布图的架构，特别关注数据复制、传播和 CAP 定理的挑战。 为 NGAC（下一代访问控制）的生产就绪的参考实施作出贡献。 设计和实施新的 NGAC 访问策略。 针对不同的使用情况进行图的优化，如图的遍历和图的减少。 要求\n基于基本原理的问题解决能力；以职能驱动决策，以第一原则为基础的思维方式。我们不以“职称“为导向，我们重视结果而不是过程。 表现出对行动的偏爱；推动行动，高质量地按时完成。 在寻找最佳创意时，你没有自我意识；你在你的专业之外做出了有效的贡献；你从团队的最佳结果的角度来考虑解决问题。 充满好奇心，善于从模糊不清的地方看到机会。 理解关注细节与注重细节之间的区别。 重视自主性和结果而非过程。 有使用 Go 构建和测试软件的经验。 有编写 gRPC API 的经验。 有分布式系统的经验，了解 CAP 定理。 有分布式数据库的经验者优先。 对分布式系统技术，如复制日志或 CRDs 有深刻的理解。 熟悉 Kubernetes、Istio 和 Envoy 等服务网格技术。 对网络协议、概念、分布式系统的一致性属性、识别和协调配置漂移的技术有良好的理解。 有为开源项目做贡献的经验者优先。 有授权系统（如 RBAC、ABAC）方面的经验为佳。 工作地点\n全球范围远程办公，也可在旧金山、波士顿、东京和印度尼西亚万隆 / 坦噶尔的办公室工作。\n软件工程师（开发人员生产力团队） Tetrate 的可维护性团队负责确保工程组织的许多其他部分不仅拥有成功所需的工具，而且在 5 年后他们将继续热爱他们的代码库。这包括评估我们的构建系统和工具来发展它们。与 Bazel、Rust 中的自定义工具以及用各种语言编写的各种质量的测试一起工作。\n这个角色将用你的想法和工具不断提高 Tetrate 的基础设施的规模效率和开发人员的生产力。如果你关心开发人员的经验，对生产力的热情，以及在测试方面的工作经验，你会发现这个角色很适合你。\n职责\n了解开发人员的工作流程和构建系统，以改善构建时间，使用 Bazel 工作。 设计、开发和提供分布式工程构建工具和平台，用于各种代码库语言。 为代码库设计新的提示器，以帮助执行高质量的 API 使用。 修复脆弱的端到端测试，并确保我们的基础设施能够跟上我们的开发人员的步伐。 要求\n熟悉 Rust、Go 或 JavaScript 任意一种语言。 熟练使用 Bazel，了解其可靠性 / 可追溯性。 你习惯于为 Build、云端缓存创建额外的工具，并帮助开发者使用他们的 IDE。 熟悉测试的挑战，并乐于使测试正确性和可靠性。 重视自主性和结果而非过程。 有系统的解决问题的方法，再加上出色的沟通技巧和主人翁意识。 表现出对行动的偏爱，推动行动，高质量地按时完成任务。 工作地点\n全球范围远程办公，也可在旧金山、波士顿、东京和印度尼西亚万隆 / 坦噶尔的办公室工作。\n软件工程师 / 站点可靠性工程师（云服务团队） Tetrate 的云服务团队负责建立和运营基于 SaaS 的应用程序。包括 SRE 和软件工程的各个方面，如发布工程、消除劳累和建立基础设施抽象；使用现代工具，如 Pulumi 和主要在 AWS 上的无服务器优先方法。\n要求\n与应用团队合作，为关键任务的客户系统设计、开发和提供高可用性、安全的服务 建立变更管理管道 通过自动化识别和消除重复劳动 具有在任何一个公有云供应商上提供高可用性网络服务的经验 熟悉基础设施即代码的工具，如 Terraform 或 Pulumi 熟悉 Go、TypeScript 或 JavaScript 重视自主性和注重结果而非过程 有系统的解决问题的方法，加上优秀的沟通技巧和主人翁意识 表现出对行动的偏爱，推动行动，高质量地按时完成。 工作地点\n全球范围内远程办公。\n软件工程师 - Go 语言 加入一个由世界级工程师组成的团队，使用 Istio、Envoy 和一些开放项目来定义下一代的云原生网络服务。我们正在寻找在使用 Golang 和 gRPC 构建分布式系统方面有经验的后端软件工程师。\n职责\n设计和实现公共 API，用于配置服务网格的所有方面：网络、安全和可观察性。 实施将对客户产生直接影响的新产品功能。 帮助研究不同的新技术，如 NGAC（下一代访问控制）。 要求\n基于基本原理的问题解决能力；以职能驱动决策，以第一原则为基础的思维方式。我们不以“职称“为导向，我们重视结果而不是过程。 表现出对行动的偏爱；推动行动，高质量地按时完成。 在寻找最佳创意时，你没有自我意识；你在你的专业之外做出了有效的贡献；你从团队的最佳结果的角度来考虑解决问题。 充满好奇心，善于从模糊不清的地方看到机会。 理解关注细节与注重细节之间的区别。 重视自主性和结果而非过程。 有使用 Go 构建和测试软件的经验。 有编写 gRPC API 的经验。 有使用 SQL 关系数据库和 Key/Value 数据存储的经验。 熟悉 Kubernetes，服务网格技术，如 Istio 和 Envoy 对网络协议、概念、分布式系统的一致性属性、识别和协调配置漂移的技术有良好的理解。 有为开源项目做贡献的经验者优先。 有认证技术（如 OIDC）和授权系统（如 RBAC、ABAC）的经验者优先。 工作地点\n全球范围远程办公，也可在旧金山、波士顿、东京和印度尼西亚万隆 / 坦噶尔的办公室工作。\nIstio 上游贡献者（Go 语言） 我们的团队正物色在分布式系统方面拥有资深经验的工程师。我们正为众多无论对传统或现代建设来说皆首屈一指的企业级应用系统打造一个安全、稳固，且接触面广的服务网格平台。如果你是 Istio 的支持者，并希望为社区贡献更多力量，你将能通过这份工作定期为 Istio 上游作出重大贡献，机会难得。\n要求\n具备以基本面为基础的解决问题的能力、「第一性原理」思维，并能以功能为首要考虑，推动决策。 能以行动为先，避免分析瘫痪。能在工作结束前时刻保持动力，按时完成工作。 能不囿于自我地进行构想，以优秀的意念为先。 求知若渴，能时刻在不明朗的情境中发掘机遇。 明白「留意细节」和「注重细节」的区别。 注重自主性，结果导向。 能在专长以外作出有效贡献。 拥有运用 Go 语言建立分布式系统平台的经验。 熟悉 Kubernetes，以及 Istio 及 Envoy 等服务网格技术对网络协议、概念、分布式系统一致性的特质有透彻的了解及掌握，拥有分辨及抑制配置漂移的技巧。 如有贡献开源项目的经验则更佳。 如熟悉 WebAssembly 则更佳。 如熟悉 Go 语言、硬件 / 软件负载均衡（F5、NGINX）、HSM module、Active Directory/LDAP 更佳。 分布式系统工程师，企业基础架构（数据平面）Go 或 C++ 开发者 我们正在寻找有使用 Golang 和 gRPC 构建分布式系统经验的后端工程师。我们正在为财富 500 强企业的关键业务构建安全、高可用的服务网格（Service Mesh）平台，横跨传统和现代基础设施。您应具备较强的分布式系统和网络的基础知识。熟悉 Kubernetes、Istio 和 Envoy 等技术，有为开源项目做贡献的经验更佳。\n要求\n有使用 C++、Golang、gRPC 构建分布式系统平台的经验。 熟悉 Kubernetes，Istio、Envoy 等服务网格技术。 对网络协议、概念、分布式系统的一致性、识别和协调配置漂移的技术有很好的理解。 有为开源项目做贡献的经验更佳。 熟悉以下内容更佳：WebAssembly、Authorization、NGAC、RBAC、ABAC。 熟悉硬件 / 软件负载均衡器（F5、NGINX）、HSM 模块、Active Directory/LDAP 者更佳。 站点可靠性工程师（SRE） 站点可靠性工程（SRE）将软件和系统工程结合起来，构建和运行可扩展、大规模分布式、容错系统。作为团队的一员，你将致力于确保 Tetrate 平台具有适合用户需求的可靠性 / 正常运行时间，以及快速的改进速度。此外，我们的工程工作主要集中在建设基础设施，提高平台故障排除能力，并通过自动化减少人工干预。\n要求\n有系统的解决问题的方法，加上优秀的沟通技巧，有主人翁意识 / 完成感和自我导向的动力。 熟悉分布式系统（有状态和 / 或无状态）和网络的操作、调试和故障排除。 熟悉 Kubernetes、服务网格技术（如 Istio 和 Envoy）能够调试、优化代码、自动化日常任务。 至少有以下一种语言的编程经验：C++、Rust、Python、Go。 熟悉使用 SLO 和 SLI 以规范的方式量化故障和可用性的概念。 有性能分析和调优经验者优先。 工作地点 我们的产业遍布全球，在中国、印尼、印度、日本、美国、加拿大、爱尔兰、荷兰、西班牙和乌克兰都有业务。我们支持远程工作，并在旧金山、波士顿、巴塞罗那、万隆 / 坦格朗（印尼）等地设有办公室。\n对英语的要求 我们鼓励口头英语和书面英语的非同步沟通，除 Team Leader 外，受聘者不必操流利英语。\n投递简历 请将展示你代码风格的 GitHub 或在线链接与你的英文简历一起发送至：careers@tetrate.io （并抄送给 jimmy@tetrate.io ）或联系 Jimmy Song 了解详情。\nTetrate 介绍 基于 Istio、Envoy、Apache SkyWalking 等开源项目，Tetrate 的旗舰产品 Tetrate Service Bridge （TSB）可以实现传统和现代工作负载的桥接。在任何环境下，客户都可以为所有工作负载获得一致的内置可观察性、运行时安全性和流量管理。\n除了技术之外，Tetrate 还带来了一个世界级的团队，领导开放的 Istio、Envoy、Apache SkyWalking 等项目，提供企业可以用来实现人员和流程现代化的最佳实践。\nTetrate 的工作氛围 Forbes Tetrate 被认定为 2022 年福布斯美国最佳创业公司雇主\nTetrate 2018 年创立于硅谷，当年获得 1250 万美元 A 轮融资，2021 年获得 4000 万美元 B 轮融资 。我们的团队主要来自加拿大、中国、印度、印尼、爱尔兰、日本、新西兰、新加坡、西班牙、荷兰和乌克兰。我们的招聘目的很简单，我们期望物色最佳人才，不论背景与居住地。我们并不提供一个头衔或角色，反而更着重于要解决的问题。若可透过实时通话，讨论彼此的交集和兴趣领域，则更为理想。你可以自由选择工作地点，弹性的休假时间和工作日程。我们每年会组织 3 到 4 次的线下团队聚会，最近几次分别举办于旧金山、西雅图、巴塞罗那、圣迭戈、华盛顿和万隆（2020 年以来因为疫情原因暂停）。团队建设活动能拉近团员间的距离，有利于你在远程工作时的协作。\nTetrate 公司名称来历 Tetrate 是数学术语 Tetration （迭代幂次）的变体，Tetrate 员工自称 Tetrand。想要了 …","relpermalink":"/notice/tetrate-recruit/","summary":"注册在硅谷，全球化远程办公。","title":"企业级服务网格提供商 Tetrate 公司招聘"},{"content":"在我的前两篇博客中：\nIstio 中的 Sidecar 注入、透明流量劫持及流量路由过程详解 Sidecar 中的流量类型及 iptables 规则详解 我向你详细介绍了 Istio 数据平面中的流量，但数据平面并不能孤立的存在，本文将向你展示 Istio 中的控制平面和数据平面各组件的端口及其功能，有助于你了解这些流量之间的关系及故障排查。\nIstio 中的组件及端口示意图 按照习惯，我们首先展示一个全局示意图。下图展示的是 Istio 数据平面中 sidecar 的组成，以及与其交互的对象。\nIstio sidecar 组成示意图 我们可以使用 nsenter 命令进入 Bookinfo 示例的 productpage Pod 的网络空间，查看其内部监听的端口信息。\nIstio sidecar 中监听的端口信息 从图中我们可以看到除了 productpage 应用本身监听的 9080 端口以外，Sidecar 容器还有监听大量的其他端口，如 15000、15001、15004、15006、15021、15090 等，你可以在 Istio 文档 上了解 Istio 中使用的端口。\n我们再进入 productpage Pod 中，使用 lsof -i 命令查看它打开的端口，如下图所示。\nProductpage Pod 中打开的端口 我们可以看到其中有 pilot-agent 与 istiod 建立了 TCP 连接，上文中所述的监听中的端口，还有在 Pod 内部建立的 TCP 连接，这些连接对应了文章开头的示意图。\nSidecar 容器（istio-proxy ）的根进程是 pilot-agent，启动命令如下图所示：\nSidecar 中的进程 从图中我们可以看到，它 pilot-agent 进程的 PID 是 1，是它拉起了 envoy 进程。\n在 istiod 的 Pod 中查看它打开的端口，如下图所示。\nIstiod 中的端口 我们可以看到其中的监听的端口、进程间和远程通信连接。\nIstio 中各端口的功能概述 这些端口在你进行问题排查时可以起着举足轻重的作用。下面将根据端口所在的组件和功能分类描述。\nIstiod 中的端口 Istiod 中的端口相对比较少且功能单一：\n9876：ControlZ 用户界面，暴露 istiod 的进程信息 8080：istiod 调试端口，通过该端口可以查询网格的配置和状态信息 15010：暴露 xDS API 和颁发纯文本证书 15012：功能同 15010 端口，但使用 TLS 通信 15014：暴露控制平面的指标给 Prometheus 15017：Sidecar 注入和配置校验端口 Sidecar 中的端口 从上文中，我们看到 sidecar 中有众多端口：\n15000：Envoy 管理接口 ，你可以用它来查询和修改 Envoy 代理的的配置，详情请参考 Envoy 文档 。 15001：用于处理出站流量。 15004：调试端口，将在下文中解释。 15006：用于处理入站流量。 15020：汇总统计数据，对 Envoy 和 DNS 代理进行健康检查，调试 pilot-agent 进程，将在下文中详细解释。 15021：用于 sidecar 健康检查，以判断已注入 Pod 是否准备好接收流量。我们在该端口的 /healthz/ready 路径上设置了就绪探针，Istio 把 sidecar 的就绪检测交给了 kubelet，最大化利用 Kubernetes 平台自身的功能。envoy 进程将健康检查路由到 pilot-agent 进程的 15020 端口，实际的健康检查将发生在那里。 15053：本地 DNS 代理，用于解析 Kubernetes DNS 解析不了的集群内部域名的场景。 15090：Envoy Prometheus 查询端口，pilot-agent 将通过此端口收集统计信息。 以上端口可以分为以下几类：\n负责进程间通信，例如 15001、15006、15053 负责健康检查和信息统计，例如 150021、15090 调试：15000、15004 下文将对几个重点端口详解。\n15000 端口 15000 是 Envoy 的 Admin 接口，该接口允许我们修改 Envoy，并获得一个视图和查询指标和配置。\n管理接口由一个具有多个端点的 REST API 和一个简单的用户界面组成，你可以使用下面的命令开启 productpage Pod 中的 Envoy 管理接口视图。\nkubectl -n default port-forward deploy/productpage-v1 15000 在浏览器中访问 http://localhost:15000，你将看到 Envoy Admin 界面如下图所示。\nEnvoy Admin 界面 15004 端口 通过 pilot-agent 代理 istiod 8080 端口上的调试端点，你可以进入数据平面 Pod 中访问 localhost 的 15004 端口查询网格信息，其效果与下面的 8080 端口等同。\n8080 端口 你还可以在本地转发 istiod 8080 端口，请运行下面的命令。\nkubectl -n istio-system port-forward deploy/istiod 8080 在浏览器中访问 http://localhost:8080/debug，你将看到调试端点，如下图所示。\nPilot 调试控制台 当然，这只是一种获取网格信息和调试网格的方式，你还可以使用 istioctl 命令或 Kiali 来调试，那样将更加高效和直观。\n15020 端口 15020 端口有三大功能：\n汇总统计数据：查询 15090 端口获取 envoy 的指标，也可以配置查询应用程序的指标，将 envoy、应用程序和自身的指标汇总以供 Prometheus 收集。对应的调试端点是 /stats/prometheus。 对 Envoy 和 DNS 代理进行健康检查：对应的调试端点是 /healthz/ready 和 /app-health。 调试 pilot-agent 进程：对应的调试端点是 /quitquitquit、debug/ndsz 和 /debug/pprof。 下图展示的是使用本地端口转发后，在浏览器中打开 http://localhost:15020/debug/pprof 看到的调试信息。\npprof 端点 图中信息展示的是 pilot-agent 的堆栈信息。\n总结 通过对 Istio 中各组件端口的了解，你应该对 Istio 中各组件的关系及其内部流量有了更进一步的认识，熟悉这些端口的功能，有助于对网格的故障排除。\n","relpermalink":"/blog/istio-components-and-ports/","summary":"本文将向你介绍 Istio 控制平面和数据平面的各个端口及功能。","title":"Istio 中的各组件端口及功能详解"},{"content":"我在之前的一篇博客中 讲解过 Istio 中 sidecar 的注入、使用 iptables 进行透明流量拦截及流量路由的详细过程，并以 Bookinfo 示例中的 productpage 服务访问 reviews 服务，和 reviews 服务访问 ratings 服务为例绘制了透明流量劫持示意图。在那个示意图中仅展示了 reviews pod 接收流量和对外访问的路由，实际上 sidecar 内的流量远不止于此。\nISTIO_OUTPUT 规则 在所有的 iptables 调用链中最复杂的一个是 ISTIO_OUTPUT，其中共有 9 条规则如下：\nRule Target In Out Source Destination 1 RETURN any lo 127.0.0.6 anywhere 2 ISTIO_IN_REDIRECT any lo anywhere !localhost owner UID match 1337 3 RETURN any lo anywhere anywhere !owner UID match 1337 4 RETURN any any anywhere anywhere owner UID match 1337 5 ISTIO_IN_REDIRECT any lo anywhere !localhost owner GID match 1337 6 RETURN any lo anywhere anywhere !owner GID match 1337 7 RETURN any any anywhere anywhere owner GID match 1337 8 RETURN any any anywhere localhost 9 ISTIO_REDIRECT any any anywhere anywhere 本文将向你展示 Istio sidecar 中的六种流量类型及其 iptables 规则，以示意图的形式带你一览其全貌，其中详细指出了路由具体使用的是 ISTIO_OUTPUT 中的哪一条规则。\nSidecar 中的 iptables 流量路由 Sidecar 中的流量可以划分为以下几类：\n远程服务访问本地服务：Remote Pod -\u0026gt; Local Pod 本地服务访问远程服务：Local Pod -\u0026gt; Remote Pod Prometheus 抓取本地服务的 metrics：Prometheus -\u0026gt; Local Pod 本地 Pod 服务间的流量：Local Pod -\u0026gt; Local Pod Envoy 内部的进程间 TCP 流量 Sidecar 到 Istiod 的流量 下面将依次解释每个场景下 Sidecar 内的 iptables 路由规则。\n类型一：Remote Pod -\u0026gt; Local Pod 以下是远程服务、应用或客户端访问数据平面本地 Pod IP 的 iptables 规则。\nRemote Pod -\u0026gt; RREROUTING -\u0026gt; ISTIO_INBOUND -\u0026gt; ISTIO_IN_REDIRECT -\u0026gt; Envoy 15006（Inbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 1 -\u0026gt; POSTROUTING -\u0026gt; Local Pod\n我们看到流量只经过一次 Envoy 15006 Inbound 端口。这种场景下的 iptables 规则的示意图如下。\nRemote Pod 到 Local Pod 类型二：Local Pod -\u0026gt; Remote Pod 以下是本地 Pod IP 访问远程服务经过的 iptables 规则。\nLocal Pod-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 9 -\u0026gt; ISTIO_REDIRECT -\u0026gt; Envoy 15001 (Outbound)-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 4 -\u0026gt; POSTROUTING -\u0026gt; Remote Pod\n我们看到流量只经过 Envoy 15001 Outbound 端口。\nLocal Pod 到 Remote Pod 以上两种场景中的流量都只经过一次 Envoy，因为该 Pod 中只有发出或接受请求一种场景发生。\n类型三：Prometheus -\u0026gt; Local Pod Prometheus 抓取数据平面 metrics 的流量不会也无须经过 Envoy 代理。\n这些流量通过的 iptables 规则如下。\nPrometheus-\u0026gt; RREROUTING -\u0026gt; ISTIO_INBOUND（对目的地为 15020、15090 端口流量将转到 INPUT）-\u0026gt; INPUT -\u0026gt; Local Pod\n这种场景下的 iptables 规则的示意图如下。\nPrometheus 到 Local Pod 类型四：Local Pod -\u0026gt; Local Pod 一个 Pod 可能同时存在两个或多个服务，如果 Local Pod 访问的服务也在该当前 Pod 上，流量会依次经过 Envoy 15001 和 Envoy 15006 端口最后到达本地 Pod 的服务端口上。\n这些流量通过的 iptables 规则如下。\nLocal Pod-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 9 -\u0026gt; ISTIO_REDIRECT -\u0026gt; Envoy 15001（Outbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 2 -\u0026gt; ISTIO_IN_REDIRECT -\u0026gt; Envoy 15006（Inbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 1 -\u0026gt; POSTROUTING -\u0026gt; Local Pod\nLocal Pod 到 Local Pod 类型五：Envoy 内部的进程间 TCP 流量 Envoy 内部进程的 UID 和 GID 为 1337，它们之间的流量将使用 lo 网卡，使用 localhost 域名来通信。\n这些流量通过的 iptables 规则如下。\nEnvoy 进程（Localhost） -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 8 -\u0026gt; POSTROUTING -\u0026gt; Envoy 进程（Localhost）\nEnvoy 内部的进程间 TCP 流量 类型六：Sidecar 到 Istiod 的流量 Sidecar 需要访问 Istiod 以同步配置，pilot-agent 进程会向 Istiod 发送请求，以同步配置。\n这些流量通过的 iptables 规则如下。\npilot-agent 进程 -\u0026gt; OUTPUT -\u0026gt; Istio_OUTPUT RULE 9 -\u0026gt; Envoy 15001 (Outbound Handler) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 4 -\u0026gt; POSTROUTING -\u0026gt; Istiod\nSidecar 到 Istiod 的流量 总结 Istio 注入在 Pod 内或虚拟机中安装的所有 sidecar 代理组成了服务网格的数据平面，也是 Istio 的主要工作负载所在地，通过 Istio 中的透明流量劫持 及这篇博客，相信你一定对 sidecar 代理中的流量有了一个深刻的了解，但这还只是管中窥豹，略见一斑，在我的下一篇博客 中，我将带你了解 Envoy 中各个组件的端口及其功能，这样可以让我们对 Istio 中的流量有一个更全面的了解。\n","relpermalink":"/blog/istio-sidecar-traffic-types/","summary":"本文将向你展示 Istio sidecar 中的六种流量类型及其 iptables 规则，并以示意图的形式带你一览其全貌。","title":"Istio sidecar 中的流量类型及 iptables 规则详解"},{"content":"在 2022 年 4 月 25 日，IstioCon 2022 开幕的当天，Istio 社区宣布正在申请将项目捐献给 CNCF ，这是 Istio 项目的一个里程碑，企业级服务网格公司 Tetrate 的 CEO/Istio 项目联合创始人 Varun Talwar 对此进行了解读。\n以下是来自 Varun 对 Istio 捐献给 CNCF 的解读 。\n将 Istio 纳入 CNCF，使得 Istio 和 Envoy 的发展更容易同步推进。它还有助于将 Istio 与 Envoy 一起定位为 CNCF 验证的 “云原生技术栈” 的一部分。根据 CNCF 的年度调查 ，到目前为止，Istio 是生产中最受欢迎和使用最多的服务网格。有 20 多家不同的公司在推动 Istio 社区的发展，这一宣布为 CNCF 管理下的持续创新和增长创造了条件。\n2016：Istio 的起源 我想借此机会解释一下 Istio 的起源。Istio 来自谷歌的 API 平台团队，名为 One Platform。(今天，具有讽刺意味的是，Istio 是美国政府项目 Platform One 的一部分，它使用 Tetrate 产品和服务）。一个平台利用了谷歌所有的基础设施优势（stubby、monarch、loas 等），并增加了最初的服务管理经验，并将其全部暴露给应用团队。\n每个团队都会编写他们的方案和方法，并定义他们的 “One Platform API”。一旦与 API 平台团队达成一致，各团队就不必再处理任何跨领域的问题，因为 Istio 处理了这些服务：流量管理、弹性、可观察性（使用具有一致名词的每个服务的预建仪表板）、认证、授权、速率限制等等。\nIstio 的想法来自于此；我们基本上采用了 One Platform 的想法，将 Envoy 加入其中（作为一个更好的数据平面），并将其与 LOAS 服务身份概念相结合，也就是今天世人所知的 Spiffe）。我们把这个想法告诉了 12 家公司，他们都很喜欢这个想法。这些公司包括大型互联网公司、金融服务公司和科技公司，特别是 SaaS 供应商。\n2017：形成核心 2017 年 5 月的，Istio 在 Gluecon 上首次公布 。0.1 展示了 Istio 的潜力，引发了大量的关注和讨论。\n2018-2019：稳定核心，增加能力 接下来的两年里，我们收集了客户的需求，将使用反馈内化，并稳定了核心功能。此外，我们还做出了一些关键的架构决定，如定义多集群模型，并将代码重新架构为一个单一的二进制文件，以方便使用。\n2020：团结社区 随着 Istio 的采用和用户生态系统的发展，人们对管理和商标保护的担忧也越来越大。然而，正如我们在这里 所提到的，作为一个社区保持团结是项目成功的关键。我可以自豪地说，Istio 就是这样做的。因此，今天加入 CNCF 的行动是发展社区和建立最终用户信任的又一步骤。\n2021：向 Wasm 和其他领域发展 人们对加入其他基础设施，如虚拟机、功能和裸机工作负载，以及使用 Wasm 等技术的定制和其他功能作为本地 API 的兴趣越来越大，这样用户就不必再使用 Envoy 过滤器了。2021 年见证了其中一些功能的建立和推广。\n“Varun Talwar 是项目的创始人之一，他一直认为 Istio 是云原生生态系统的一个重要组成部分。今天的公告验证了他对项目的愿景，我要感谢 Tetrate 成为 Istio 和我们社区的有力支持者。\u0026#34;——Louis Ryan（Istio 联合创始人，谷歌工程负责人）\n零信任的基础 关于零信任的话题已经有很多讨论，但很少有明确的说法。正如 Eric Brewer 今天在 IstioCon 的主题演讲 中提到的，Istio 正在成为零信任的一个重要组成部分。其中最主要的是面向身份的控制，而不是面向网络的控制。这方面的核心原则在谷歌白皮书《BeyondProd：云原生安全的新方法》 。\n然而，作为一个行业，这里有更多的事情要做。我们需要确保我们可以把应用用户和数据服务都带进来。如果我们能将身份概念扩展到用户，并为我们提供灵活而丰富的策略机制来指定、监控和跟踪访问控制，我们就能达到一个可操作的零信任结构 —— 一个将用户、服务和数据统一到一个管理层的结构。我在 2020 年为美国国家标准与技术研究院（NIST）举办的围绕信任云原生应用的主题演讲中也提到了这一点。这就是为什么我们在 Tetrate 创建了 Tetrate Service Bridge —— 一个管理平面，使大型组织可操作。\nTetrate Service Bridge 的基础是：\n用户、服务和数据的身份。每个人都有一个加密身份，构成所有政策的骨干。 策略和访问控制。定义 Istio 策略，也包括应用和组织策略，包括用户和设备，以及大规模管理它们的能力。 自动化。在运行时自动化、测量和持续监测策略的能力。 如果我们能让企业以这种方式为云原生工作负载部署和运营安全，我们就能作为一个行业取得巨大进步。\n人才 归根结底，没有高素质、富有创造性的人才，任何项目或技术都不会成为主流。在 Tetrate，我们相信我们需要对社区进行有关这项技术的教育，并为负责任的采用路径做出贡献。因此，我们提供世界级的认证和免费的在线培训课程，使社区中的任何人都可以在 academy.terate.io 轻松参加 Istio 和 Envoy 的初级和高级课程。\n我们 Tetrate 的所有人，特别是我自己，都期待着下一步的发展，我们将始终支持 Istio 项目和社区。\n","relpermalink":"/blog/istio-has-applied-to-join-the-cncf/","summary":"来自 Tetrate CEO、Istio 联合创始人 Varun Talwar 的解读。","title":"Istio 捐献给 CNCF 意味着什么？"},{"content":"Tetrate 是企业级服务网格领域的主要玩家之一，是 Istio、Envoy 和 SkyWalking 开源项目的发起者或主要参与者。本文将向你介绍 Tetrate 发起的几个开源项目：\nTetrate Istio Distro/GetMesh ：Tetrate Istio 发行版 wazero ：使用 Go 语言编写的无需平依赖性的 WebAssembly 运行时 func-e ：Envoy 构建命令行 istio-security-analyzer ：Istio 安全扫描工具 Tetrate Istio Distro/GetMesh Tetrate Istio 发行版，又名 GetMesh，为 Kubernetes 或应用平台安装和管理经过审核的 Istio。\n最简单的安装、操作和升级 Istio 的方法 为您的应用和云平台进行测试和加固 用户、生态系统和合作伙伴的社区中心 GetMesh 是一个命令行工具，你可以用它来：\n强制获取 Istio 的认证版本，并只允许安装 Istio 的兼容版本 允许在多个 istioctl 版本之间无缝切换 符合 FIPS 标准 通过整合多个来源的验证库，提供基于平台的 Istio 配置验证 使用一些云提供商证书管理系统来创建 Istio CA 证书，用于签署服务网格管理工作负载 提供附加的与云提供商多个集成点 使用下面的命令就可以安装 GetMesh：\ncurl -sL https://istio.tetratelabs.io/getmesh/install.sh | bash 注意：如果你位于中国大陆，执行上面的命令需要翻墙。\n想要了解更多关于 Tetrate Istio Distro/GetMesh 的信息请访问 https://istio.tetratelabs.io wazero wazero 是一个用 Go 语言编写的符合 WebAssembly 1.0（20191205） 规范的运行时。\nWebAssembly 是一种安全运行用其他语言编译的代码的方法。运行时 执行 WebAssembly 模块（Wasm），它通常是以 .wasm 为扩展名的二进制文件。\nwazero 仅依赖 Go 语言而无依赖，且不依赖 CGO。你可以运行其他语言的应用程序，但仍然保持交叉编译。也就是说它可以嵌入到应用程序中，而不依赖特定的操作系统。这是 wazero 与其他 WebAssembly 运行时的主要区别。wazero 还可以在 Docker 的 scratch 镜像 中运行。\n想要了解更多关于 wazero 的信息请访问：https://github.com/tetratelabs/wazero func-e func-e 是一个用来安装和运行 Envoy 代理的命令行工具。func-e（发音为 funky）允许你快速查看 Envoy 的可用版本并进行试用。这使得你很容易验证在生产中使用的配置。每次你结束运行时，都会以你的名义获取运行时状态的快照。这使得知识共享和故障排除更加容易，特别是在升级时。\n想要了解更多关于 func-e 的信息请访问：https://github.com/tetratelabs/func-e Istio Security Analyzer Istio Security Analyzer 是一个用于 Istio 安全性分析的命令行工具。该工具可以：\n确保配置遵守 Istio 安全最佳实践。 检查正在运行的 Istio 版本，看是否有任何已知的 CVE 问题。 想要了解更多关于 Istio Security Analyzer 的信息请访问：https://github.com/tetratelabs/istio-security-analyzer 更多 Tetrate 开源的项目请访问：https://github.com/tetratelabs ","relpermalink":"/blog/tetrate-open-source-projects/","summary":"本文介绍了企业级服务网格公司 Tetrate 的几个围绕服务网格领域的开源项目。","title":"Tetrate 公司开源项目介绍"},{"content":"去年第一届 IstioCon 在线上成功举办，今天第二届 IstioCon 也要开始了，今年的会议依旧是线上举办，分为英文场和中文场，注册参会地址：https://events.istio.io/istiocon-2022/ 注册后你可以在大会官网 （需翻墙）上收看直播，也可以在 Istio 社区的 Bililibi 直播间 收看。\n中文场将在北京时间 4 月 26 日（周二）上午 9 点开始，本次大会的中国组织者（徐中虎、Iris Ding 和我）将进行开场，然后在 9:15 分我将和马若飞、赵化冰、刘齐均）有一个关于《Istio 开源生态展望 》的圆桌论坛，欢迎大家收看。\n","relpermalink":"/notice/istiocon-2022-program/","summary":"欢迎注册第二届 IstioCon","title":"IstioCon 2022（4 月 25-29，线上会议）"},{"content":"2022 年 2 月 Istio 发布 1.13.0 和 1.13.1 ，这篇博客将想你介绍这两个版本中有哪些值得注意的新特性。\nIstio 1.13 是 2022 年的第一个版本，不出意外的话，Istio 团队会依然按照每个季度的频率发布新版本。总体来看，这个版本中的新特性包括：\n对 Kubernetes 更新版本的支持 引入了一个新的 API——ProxyConfig，用来配置 sidecar proxy 完善了 Telemetry API 支持多网络网关的基于主机名的负载均衡器 对 Kubernetes 版本的支持 我经常看到有人在社区里问 Istio 支持哪些 Kubernetes 版本，其实 Istio 官网中已经明确列出了支持的 Kubernetes 版本，你可以在这里 看到，Istio 1.13 支持 Kubernetes 1.20、1.21、1.22 和 1.23 版本，并在 Kubernetes 1.16、1.17、1.18、1.19 中测试过，但并得到官方支持。\n在配置 Istio 的时候，其实还有很多检查列表，我将他们都记录到了 Istio cheatsheet 中，这个项目中整理了很多关于配置 Istio、资源对象的使用、常见问题处理等相关的 cheatsheet，将于近期上线，敬请期待。\nIstio cheatsheet 页面截图 引入新的 ProxyConfig API 在 Istio 1.13 版本之前，如果你想自定义 sidecar proxy 的配置，有两种方式。\n方式一：MeshConfig\n使用 MeshConfig，在 Mesh 级别使用 IstioOperator 来修改。例如，使用下面的配置来修改 istiod 的默认发现端口。\napVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: defaultConfig: discoveryAddress: istiod:15012 方式二：Pod 中的 annotation\n你也可以在 Pod 级别使用 annotation 的方式自定义配置，例如在 Pod 中增加下面的配置同样可以修改工作负载所有连接的 istiod 的默认端口。\nanannotations: proxy.istio.io/config: | discoveryAddress: istiod:15012 当你同时使用了以上两种方式配置了 sidecar，annotations 中设置的字段将完全覆盖 MeshConfig 默认的字段。关于 ProxyConfig 的所有配置项请参考 Istio 文档 。\n新方式：ProxyConfig API\n但是在 1.13 版本中，新增了一个顶级自定义资源 ProxyConfig，你可以一站式的在一个地方来自定义 sidecar proxy 的配置，你可以通过指定 namespace、使用 selector 来选择工作负载的范围，就像其他 CRD 一样。目前 Istio 对该 API 的支持有限，关于 ProxyConfig API 的详细信息请参考 Istio 文档 。\n但是不论你用哪种方式自定义 sidecar proxy 的配置，该配置都无法动态生效，需要重启工作负载才可以生效。例如，对于上面的配置，因为你修改了 istiod 的默认端口，mesh 中的所有工作负载都需要重启才可以与 control plane 建立连接。\nTelemetry API 在 Istio 服务网格中，很多扩展和自定义的配置都是通过 MeshConfig 的方式来完成的。可观察性的三种类型 Metric、遥测和日志，分别可以对接不同的提供者，Telemetry API 可以让你有一个一站式的灵活的配置它们。与 ProxyConfig API 类似，Telemetry API 也遵循着工作负载选择器\u0026gt;本地命名空间\u0026gt;根配置命名空间的配置层级关系。该 API 是在 Istio 1.11 中引入，在该版本中得到了进一步完善，增加了 OpenTelemetry 日志、过滤访问日志以及自定义跟踪服务名称的支持。详见 Telemetry 配置 。\n自动解析多网络网关主机名 2021 年 9 月，Istio 社区里有人报告 ，在 AWS EKS 中运行多集群多主的 Istio 时，出现 EKS 的负载均衡器无法解析的问题。对于多集群多网络的网格，跨集群边界的服务负载，需要通过专用的东西向网关，以间接的方式通讯。你可以按照 Istio 官网上的说明 配置多网络的 primary-remote 集群，Istio 会根据主机名自动解析负载均衡器的 IP 地址。\nIstio 1.13.1 修复重大安全漏洞 当月，Istio 1.13.1 发布，修复了一个已知的重大漏洞 ，该漏洞可能导致未经认证的控制平面拒绝服务攻击。\n跨网络的主从集群 在安装多网络的 primary-remote 模式的 Istio 网格时，为了让 remote Kubernetes 集群能够访问控制平面，需要在 primary 集群中安装一个东西向的 Gateway，将控制平面 istiod 的 15012 端口暴露到互联网。攻击者可能向该端口发送特制的消息，导致控制平面崩溃。如果你设置了防火墙，只允许来自部分 IP 的流量访问该端口，将可以缩小该问题的影响范围。建议你立即升级到 Istio 1.13.1 来彻底解决该问题。\nIstioCon 2022 IstioCon 2022 最后，作为上一届和本届 IstioCon 的筹备委员会成员之一，我号召大家报名参加 4 月 25 日在线上举行的 IstioCon 2022 ！IstioCon 2022 是一个以行业为重点的活动，一个连接贡献者和用户的平台，讨论 Istio 在不同架构设置中的用途，有哪些限制，以及项目的下一步发展方向。主要的焦点将是在最终用户公司，因为我们期待着分享多样化的案例研究，展示如何在生产中使用 Istio。\n","relpermalink":"/blog/what-is-new-in-istio-1-13/","summary":"2022 年 2 月 Istio 发布 Istio 1.13.0 和 Istio 1.13.1，这篇博客将想你介绍这两个版本中有哪些值得注意的新特性。","title":"Istio 1.13 有哪些值得注意的更新？"},{"content":"本书译自美国国家标准标准与技术研究院（NIST）Special Publication 800-204C。\n关于本书 作者：Ramaswamy Chandramouli\n审阅\n计算机安全司信息技术实验室 美国商务部 Gina M. Raimondo，秘书 国家标准和技术研究所 James K. Olthoff，履行负责标准和技术的商务部副部长兼国家标准和技术研究所所长的非专属职能和职责 原出版物可在此 免费获取，中文版请在此阅读 。\n","relpermalink":"/notice/service-mesh-devsecops/","summary":"本书译自美国国家标准标准与技术研究院（NIST）SP 800-204C","title":"《利用服务网格为基于微服务的应用程序实施 DevSecOps》电子书发布"},{"content":"每当夜深人静的时候，你是否觉得在桥都做云原生技术很孤单寂寞，没有人懂你，没有人在你需要帮助的时候给你一个拥抱然后说：傻孩子，你不知道重庆有个云原生社区吗，那里是你的归宿。是的，我们的线下交友活动又来了，本次活动由云原生社区重庆站联合中国 DevOps 社区共同举办，邀请了三位重庆地区的云原生实践者分享他们的故事，参加活动你可以学到知识，认识朋友，还有众多礼品可以拿到手软，赶快免费报名吧！点击报名 ！\n时间：2022 年 3 月 12 日 地点：重庆市渝北区两江幸福广场 MK SPACE 以下是详细日程。\n端到端 DevOps：从 GitLab Flow 到 GitOps 时间：14:20 - 15:00\n讲师介绍\n刘剑桥，十余年软件领域从业经验，曾在华为，云从等企业任职。主导和参与过 DevOps 一体化平台、云原生平台、混合云管理平台等领域架构设计和落地，在 DevOps 领域有着丰富的实践经验。目前在极狐 GitLab 任解决方案架构师。\n话题大纲\n通过 GitLab Flow 和 GitOps 搞定从代码提交到 K8S 部署的全流程。帮助企业以代码定义、可追溯、更安全的方式上云。 云原生在 IoT 边缘计算场景应用与实践 时间：15:00 - 15:40\n讲师介绍\n姜仁杰，现就职于中移物联网有限公司，负责 5G+IoT、5G+边缘计算产品及行业解决方案研发，对 IoT、边缘计算、云原生有丰富的经验。\n话题大纲\n物联网边缘计算平台场景介绍 云原生对边缘计算场景支持 应用与实践 DevOps: 从降本增效到业务赋能 时间：16:00 - 16:40\n讲师介绍\n张鹄干，猪八戒网 DevOps 工程师，重庆地区 DevOps 与云原生的先行者，在相关领域具有丰富的实战经验。\n话题大纲\n以猪八戒网为背景，讲解 devops 发展趋势，如何做到从降本增效的原始需求过渡到实现为业务赋能的终极目标。 本次活动赞助商：极狐 GITLAB\n","relpermalink":"/notice/chongqing-meetup/","summary":"3 月 12 日，由云原生社区和中国 DevOps 社区联合主办。","title":"云原生社区重庆站聚会"},{"content":"从 2022 年 2 月 10 日起，我开始以每周一个视频的频率来推出 Istio Workshop/教程，这系列视频每个时长在 2 到 3 分钟，由 Tetrate 、云原生社区 和 Jimmy Song 联合出品，本课程将打破常规，将图文阅读、视频学习、动手实验及社区讨论等模式相结合，适用于有一定 Istio 基础的中高阶人群，关注云原生社区 B 站 、公众号及视频号将及时获得更新。\n","relpermalink":"/notice/istio-workshop/","summary":"由 Tetrate、云原生社区和我共同出品的 Istio 的教程视频","title":"Istio Workshop 系列 Istio 视频教程发布"},{"content":" 封面 Service Mesh Summit 2022 首届服务网格峰会，将于 2022 年 3 月 19 日（星期六），在上海舜元会议中心（近 2 号线淞虹路）举行。请转到活动行 免费报名。本次活动由云原生社区主办，赞助商有 API7 、Tetrate 、Flomesh 。\nService Mesh Summit 2022 服务网格峰会日程表 日程表 以下是详细日程。\n多集群流量调度 ——Flomesh 时间：9:00 - 9:45\n讲师介绍\n张晓辉，Flomesh 高级云原生架构师。资深码农，有多年的微服务和基础架构的实践经验。主要工作涉及微服务、容器、Kubernetes、DevOps 等。目前在 Flomesh 负责流量相关产品和技术布道。\n话题大纲\n背景：集群数量增加带来的跨集群通信问题 Flomesh 的跨集群服务治理方案介绍及原理 详细介绍服务治理的相关功能 听众收获\n了解跨集群网络通信的整体架构及实现原理 了解可编程的统一数据面 Pipy 在方案中的角色 了解跨集群的服务治理方案及相关功能 Apache APISIX 借助 Service Mesh 实现统一技术栈的全流量管理 时间：9:45 - 10:30\n讲师介绍\n张晋涛，API7.ai 云原生技术专家，Apache APISIX PMC，Kubernetes ingress-nginx reviewer，containerd/Docker/Helm/Kubernetes/KIND 等众多开源项目 contributor， 『K8S 生态周报』的维护者，微软 MVP。对 Docker 和 Kubernetes 等容器化技术有大量实践和深入源码的研究，业内多个知名大会讲师，PyCon China 核心组织者，著有《Kubernetes 上手实践》、《Docker 核心知识必知必会》和《Kubernetes 安全原理与实践》等专栏。公众号：MoeLove。\n话题大纲\nService Mesh 领域的现状 实践 Service Mesh 中遇到的问题 Apache APISIX 在 Service Mesh 中实现全流量管理 面向未来的架构演进 听众收获\n了解当前 Service Mesh 领域的现状及一些痛点 了解 Apache APISIX 如何在 Service Mesh 领域中实现统一的全流量管理 Apache APISIX 在 Service Mesh 领域的探索和实践 从开源 Istio 到企业级服务：如何在企业中落地服务网格 时间：10:30 - 11:15\n讲师介绍\n胡渐飞，Tetrate 工程师，之前在 Google 从事 Istio 相关开发。\n话题大纲\nIstio 在企业中的实际落地应用 企业级客户在面对开源 Istio 的时候有什么样的考量 Tetrate 如何解决这些开源 Istio 中遇到的问题 听众收获\nTetrate 作为服务网格的顶级供应商之一，成立 4 年来服务大量的中大企业客户，通过本次分享，你了解到这些大型企业在考虑使用服务网格时候的顾虑和需求。以及作为一个商业化公司，Tetrate 对开源有什么样的经验和看法。\n如何实现 Istio 服务网格自定义扩展功能 时间：11:15 -12:00\n讲师介绍\n曾宇星，阿里云云原生架构师，技术专家，长期从事服务端开发和架构工作，10 多年分布式领域后台开发经验，主要关注于云原生、高性能、高可用分布式架构。有多年 ServiceMesh、Envoy 网关、Kubernetes 容器平台等云原生领域相关开发工作经验。目前在阿里云服务网格团队从事 ServiceMesh 云产品研发和架构设计工作。\n话题大纲\n基于 Istio 的几种服务网格自定义扩展方式介绍，囊括 In-process 和 Out-Of-Process 的几种实现方式，具体包括 wasm (WebAssembly)、lua、golang、native c++、grpc proxyless 等 以上几种扩展方式对比及其背后的实现原理，包括 envoyfilter 和 xds 协议扩展 阿里云服务网格云产品（asm) 是如何将这几种扩展方式集成到产品中的，云产品用户如何使用满足自定义扩展需求 听众收获\n了解 Istio 服务网格的控制面和数据面相关知识和交互原理 了解 Istio 自定义扩展的几种实现方式和对比，如何选型和快速满足自定义扩展需求 基于阿里云服务网格 ASM，如何快速落地服务网格自定义扩展需求 Aeraki Mesh 在冬奥会视频应用中的产品落地实践 时间：13:30 - 14:15\n讲师介绍\n赵化冰，腾讯云工程师，Aeraki Mesh 创始人。\n覃士林，腾讯后台开发，推动腾讯融媒体业务全量上云，主导微服务化体系建设，推动传统运维方式向云原生转型。\n话题大纲\nAeraki Mesh MetaProtocol 协议扩展方案介绍 腾讯融媒体采用 Aeraki Mesh 在冬奥会直播中的服务网格实践 听众收获\n如何基于 Aeraki Mesh 的协议扩展能力在服务网格中管理一个私有协议。\n轻舟服务网格的无侵入增强 Istio 经验 时间：14:15 - 15:00\n讲师介绍\n方志恒，网易数帆架构师，负责轻舟 Service Mesh，先后参与阿里和网易 Service Mesh 建设及相关产品演进。具有多年 Istio 控制面 管理维护、功能拓展和性能优化经验。\n话题大纲\n围绕 slime 的孵化和版本演进，介绍轻舟服务网格的无侵入增强 Istio 的经验，分享 slime 的后续规划。\n听众收获\n了解 Istio 生产落地中可能遇到的问题和应对 了解 Istio 的扩展方式和思路 了解 slime 是如何以无侵入的方式增强 Istio 了解 slime 在工程化方面的改进 了解 slime 的 roadmap 以及最近大版本的功能特性 服务网格中 TLS 的加速和优化 时间：15:15 - 16:00\n讲师介绍\n胡伟，英特尔先进技术与软件部门，负责云计算开源软件开发和合作伙伴技术合作，聚焦云原生软件基于英特尔架构在性能和安全方面的技术创新。\n赵复生，英特尔先进技术与软件部门，负责领导云计算开源软件社区开发和客户技术支持工作，聚焦云原生服务网格软件基于英特尔架构的技术创新。\n话题大纲\n大规模微服务场景下 TLS 的挑战 英特尔加解密加速技术 Crypto Acceleration 介绍 Crypto Acceleration 在服务网格中的实施方案介绍 案例和优化成果分享 听众收获\n听众可以了解到如何利用英特尔的 Crypto Acceleration 技术大幅提升服务网格中 TLS 安全通信的性能。\n字节跳动 Service Mesh 性能优化的实践与思考 时间：16:00 - 16:45\n讲师介绍\n徐佳玮，字节跳动基础架构研发工程师，长期专注于高性能网络、服务治理等领域，目前在字节架构基础架构服务框架团队从事 Service Mesh 研发工作。\n话题大纲\n字节跳动 Service Mesh 落地的现状 字节跳动 Service Mesh 在高性能网络方向的实践和探索 对其他性能优化技术的思考和展望 听众收获\nService Mesh 如何提高网络传输效率，在性能上在哪些方面可以优化，未来可以和哪些技术结合？希望能够引发更多的思考。\n蚂蚁集团 Service Mesh 进展回顾与展望 时间：16:45: 17:30\n讲师介绍\n石建伟，花名卓与，蚂蚁集团高级技术专家，专注服务领域中间件多年，包括微服务、Service Mesh、配置中心、服务注册中心，目前主要负责蚂蚁内部 Service Mesh、SOFARPC、Layotto、SOFAGateway 等产品。\n话题大纲\n练内功：内部集群规模化扩张带来的技术挑战，长连接膨胀，服务治理智能化演进和人工介入说再见 定标准：Mesh 化是终点么？应用运行时标准 建通路：让业务飞速发展的秘诀 看未来：云原生运行时的下一个五年 听众收获\n了解业界前沿 Service Mesh 演进趋势，大规模集群的 Mesh 化优化方向，服务治理自适应限流、自适应流量调度，MOSN 与 Envoy 融合的尝试。 了解跨云无厂商绑定的新技术方案，应用运行时，多云探索。 ","relpermalink":"/notice/service-mesh-summit-2022/","summary":"首届服务网格峰会，由云原生社区主办，正在报名中。","title":"Service Mesh Summit 服务网格峰会 2022 正在报名中"},{"content":" Envoy 基础教程 Envoy 代理是一个开源的边缘和服务代理，是当今现代云原生应用的重要组成部分，Booking.com、Pinterest 和 Airbnb 等大公司都在生产中使用了它。Tetrate 是 Envoy 的顶级贡献者，开发了 Envoy 基础教程 ，这是一个免费的培训，有结业证书，以帮助企业更快地采用该技术。DevOps、SRE、开发人员和其他社区成员能够通过概念文本、实际实验和测验轻松学习 Envoy。Tetrate 也是受欢迎的 Istio 基础教程 和开源项目 Func-e 的创建者，这使得采用 Envoy 更加容易。\n“我对 Tetrate 的 Envoy 基础教程和认证感到兴奋”，Envoy Proxy 的创建者、Lyft 的高级工程师 Matt Klein 说。“该课程组织的很好，有关于 Envoy 应用的信息，以及带有逐步说明和测验的实验。完成全部课程后，你将获得一份证书的奖励。最重要的是，培训是完全免费的。我向想学习和使用 Envoy Proxy 的人强烈推荐这个课程”。\nEnvoy 基础教程结业证书 Envoy 是构建服务网格的默认选择 CNCF 的项目 Envoy Proxy 是最流行的 sidecar 和 ingress 实现。它是多个服务网格项目的默认 sidecar，包括 Istio、Open Service Mesh 和 AppMesh。根据 CNCF 的 2020 年调查 ，Envoy 作为 Ingress 提供者的使用率增加了 116%，共有 37% 的受访者在生产中使用 Envoy Proxy。\nEnvoy 最初是在 Lyft 建立的，作为一个代理，作为大规模微服务服务网格的通用数据平面。其理念是让 Envoy sidecar 在你的应用中的每个服务旁边运行，将网络从应用中抽象出来。它可以作为边缘网关、服务网格和混合网络的桥梁。有了 Envoy，公司可以通过提供更灵活的发布流程和高可用、高弹性的基础设施来扩展其微服务。\nEnvoy 具有丰富的网络相关功能，如重试、超时、流量路由和镜像、TLS 终止、可观察性等。由于所有的网络流量都流经 Envoy 代理，因此可以观察流量和问题区域，对性能进行微调，从中找出任何延迟来源。由于其众多的功能和庞大的 API ，用户可能会对广泛而全面的 文档 感到不知所措，特别是对于不熟悉代理和刚刚开始 Envoy 之旅的初学者。因此，我们决定创建一个课程，介绍 Envoy 的基本概念和内部结构，使用户能够更快地学习。\n“在过去几年中，Envoy 的采用率迅速提高。这意味着获得易于学习的资源，为用户提供快速扩展学习的能力，对保持 Envoy 的采用更加容易至关重要”，Tetrate 联合创始人 Varun Talwar 说。\n关于 Envoy 基础教程 免费的 Envoy 基础教程 由 8 个模块组成，每个模块内有多个视频课程和实验。\nEnvoy 基础教程中的模块 该课程从介绍 Envoy 开始，解释了 HTTP 连接管理器过滤器、集群、监听器、日志、管理界面和扩展 Envoy 等概念。每个模块都包括带有分步说明的动手实验。实验使学习者能够练习所解释的概念，如\nEnvoy 的动态配置 断路器 流量拆分 镜像请求 全局和局部速率限制 HTTP tap filter 使用 Lua 脚本和 Wasm 扩展 Envoy 等 Envoy 基础教程课程大纲 每个模块后的小测验帮助你评估自己对知识的掌握。完成课程和所有测验后，你会收到一份完成证书。在 Tetrate Academy 网站 上注册免费的 Envoy 基础课程 ，开始学习。\n注意：该课程为双语版，见中文版 。\n更多 Envoy 学习资源 5 分钟内开始使用 Envoy（博客） Envoy 和 Envoy 可扩展性的基础知识（博客） Envoy 入门：基于文件的动态配置 (博客) Istio Weekly：Envoy 的基本原理（视频） Istio Weekly：开发 Envoy Wasm 扩展（视频） Envoy 详解（视频） Lyft 的 Envoy：拥抱服务网格（视频） 人人都可以为 Envoy 做贡献（视频） 学员反馈 关于 Tetrate Tetrate 由 Istio 创始人和 Envoy 维护者创办，旨在重新构想应用网络，是一家管理现代混合云应用基础设施复杂性的企业级服务网格公司。其旗舰产品 Tetrate Service Bridge 为多集群、多租户和多云部署提供了一个全面的、企业就绪的服务网格平台。客户在任何环境下都能获得一致的、内嵌的可观察性、运行时的安全性和流量管理。Tetrate 仍然是开源项目 Istio 和 Envoy Proxy 的主要贡献者，其团队包括 Envoy 的高级维护人员。了解更多信息，请访问 tetrate.io 。\n","relpermalink":"/notice/envoy-fundamental-courses/","summary":"本教程由企业级服务网格提供商 Tetrate 出品。","title":"Tetrate 推出免费的 Envoy 基础教程"},{"content":"虎年春节期间，我翻译了 O’Reilly 出品的报告 The Future of Observablity with OpeTelemetry ，作者 Ted Young。现在中文版已翻译完成，可以下载和在线阅读。\n《OpenTelemetry 可观测性的未来》中文版封面 如何阅读本书 您可以使用以下方式阅读：\n在线阅读 下载 PDF 关于本书 本书内容包括：\nOpenTelemetry 如何满足库作者、应用程序拥有者、运维和响应者的需求 应用程序的不同角色如何围绕 OpenTelemetry 来协同和独立工作 关于在组织中采用和管理 OpenTelemetry 的实用建议 关于作者 Ted Young 是 OpenTelemetry 项目的联合创始人之一。在过去的二十年里，他设计并建立了各种大规模的分布式系统，包括可视化 FX 管道和容器调度系统。他目前在 Lightstep 公司担任开发者教育总监，住在俄勒冈州波特兰的一个小农场里。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区 可观测性讨论组（微信群）参与讨论交流，加入前请先填写入群申请问卷 后联系 Jimmy Song 入群。\n","relpermalink":"/notice/opentelemetry-obervability-ebook/","summary":"本书将帮助你理解可观测性、OpenTelemetry 及如何在企业内推广它。","title":"《OpenTelemetry 可观测性的未来》中文版发布"},{"content":"去年第一届 IstioCon 在线上成功举办，今天第二届 IstioCon 演讲议题也征集了，欢迎大家提交。\nIstioCon 2022 IstioCon 接受中文和英文议题，本次活动为线上分享，议题提交截止时间为 2022 年 3 月 3 日（北京时间，下同），提交及要求详见：https://sessionize.com/istiocon-2022/ 其他重要日期有：\n议题开放征集时间：2022 年 1 月 28 日 议题征集截止时间：2022 年 3 月 4 日 议程发布时间：2022 年 3 月 29 日 会议日期：2022 年 4 月 26 - 30 日 本次活动由 Istio 社区主办，我们期待您的精彩分享。\n","relpermalink":"/notice/istiocon-2022/","summary":"第二届 IstioCon 议题正在征集中。","title":"IstioCon 2022 讲师和议题正在征集中"},{"content":"随着服务网格架构理念的深入人心，它的适用场景也慢慢为众人所了解，社区中也不乏争论，甚至是质疑的声音。笔者以在云原生和服务网格社区中多年的观察，将从亲历者的角度总结服务网格在 2021 年的进展。因为当前在国内 Istio 几乎是服务网格的代名词，本文也将主要从 Istio 的技术和生态层面来解读服务网格在 2021 年的发展。\n服务网格：云原生的核心技术之一 作为 CNCF 定义的云原生 关键技术之一，服务网格发展至今已经有五个年头了，其发展经历了以下几个时期：\n探索阶段：2017 年 -2018 年 早期采用者阶段：2019 年 -2020 年 大规模落地及生态发展阶段：2021 年至今 如果根据“跨越鸿沟”理论 ，服务网格已经跨越了“鸿沟”，处于“早期大众”和“晚期大众”阶段之间。根据《Istio 大咖说》 观众中的反馈来看，用户已不再盲从于新技术，开始辩证的考虑是否真的需要引入服务网格 。\n跨越鸿沟理论 云原生的发展方兴未艾，虽然不断有新的技术和产品出现，但作为整个云原生技术栈的一部分，服务网格在过去一年里不断夯实了它作为“云原生网络基础设施”的定位。下图展示了云原生技术栈模型，其中每一层有一些代表性的技术来定义标准。作为新时代的中间件，服务网格与其他云原生技术交相辉映，如 Dapr（分布式应用程序运行时）定义云原生中间件的能力模型，OAM 定义云原生应用程序模型等，而服务网格定义的是云原生七层网络模型。\n云原生技术栈 社区焦点 过去一年中，社区的焦点主要集中在以下几个方面：\n性能优化：服务网格在大规模应用场景下的性能问题； 协议扩展：让服务网格支持任意七层网络协议； 部署模式：Proxyless vs Node 模式 vs Sidecar 模式； 引入 eBPF：将服务网格的部分能力下沉到内核层； 性能优化 Istio 设计之初的目标就是通过“原协议转发”的方式服务于服务间流量，让服务网格尽可能对应用程序“透明”，从而使用了 IPtables 劫持流量 ，根据社区提供的测试结果 ，对于在 16 个连接上具有 1000 RPS 的网格，Istio 1.2 仅增加了 3 毫秒的基准延迟。但是，因为 IPtables conntrack 模块所固有的问题，随着网格规模的扩大，Istio 的性能问题开始显现。关于 Istio sidecar 的资源占用及网络延迟的性能优化，社区给出了以下解决方案：\nSidecar 配置：通过手动或在控制平面增加一个 Operator 的方式来配置服务的依赖项，可以减少向 Sidecar 中下发的服务配置数量，从而降低数据平面的资源占用；为了更加自动和智能地配置 Sidecar，开源项目 Slime 及 Aeraki 都给出了各自的配置懒加载方案； 引入 eBPF：eBPF 可以作为优化服务网格性能的一种可行性方案，有基于 Cilium 的初创公司甚至激进的提出使用 eBPF/Cilium 完全替换 Sidecar 代理 的策略，但事实上 Envoy 代理/xDS 协议已经成为服务网格实现的实际代理，且很好的支持七层协议。eBPF 可用来改善网络性能，但复杂的协议协商、解析和用户扩展在用户侧依然很难实现。 协议扩展 如何扩展 Istio 一直以来就是一个老大难的问题。Istio 的可扩展包含两方面：\n协议层面：让 Istio 支持所有七层协议 生态层面：让 Istio 可以运行更多的插件 Istio 使用的是 Envoy 作为数据平面，扩展 Istio 本质上就是对 Envoy 功能的扩展。Istio 官方目前给出的方案是使用 WebAssembly，并在 Istio 1.12 引入 Wasm 插件配置 API 用于扩展 Istio 生态 ，Istio 的扩展机制使用 Proxy-Wasm 应用二进制接口（ABI） 规范，提供了一套代理无关的流媒体 API 和实用功能，可以用任何有合适 SDK 的语言来实现。截至目前，Proxy-Wasm 的 SDK 有 AssemblyScript（类似 TypeScript）、C++、Rust、Zig 和 Go（使用 TinyGo WebAssembly 系统接口）。\n目前 WebAssembly 扩展应用还比较少，很多企业选择自定义 CRD，基于 Istio 构建服务网格管理平面。另外，让 Istio 支持异构环境，适用于一切工作负载，如虚拟机、容器，这个对于终端用户来说也有很强的需求，因为这可以让用户很方便的从传统负载迁移应用到服务网格中。最后是多集群、多网格的混合云流量管理，这个属于比较高阶的需求了。\n部署模式 在服务网格概念兴起之初就有 Per-node 和 Sidecar 模式之争，他们的代表分别是 Linkerd 和 Istio。后来 eBPF 提出将服务网格下沉的内核，从而演化出了更多的服务网格部署模式，如下图所示。\n服务网格的部署模式 下表中详细对比了这四种部署方式，它们各有优劣，具体选择哪种根据实际情况而定。\n模式 内存开销 安全性 故障域 运维 Sidecar 代理 因为为每个 pod 都注入一个代理，所以开销最大。 由于 sidecar 必须与工作负载一起部署，工作负载有可能绕过 sidecar。 Pod 级别隔离，如果有代理出现故障，只影响到 Pod 中的工作负载。 可以单独升级某个工作负载的 sidecar 而不影响其他工作负载。 节点共享代理 每个节点上只有一个代理，为该节点上的所有工作负载所共享，开销小。 对加密内容和私钥的管理存在安全隐患。 节点级别隔离，如果共享代理升级时出现版本冲突、配置冲突或扩展不兼容等问题，则可能会影响该节点上的所有工作负载。 不需要考虑注入 Sidecar 的问题。 Service Account/节点共享代理 服务账户/身份下的所有工作负载都使用共享代理，开销小。 工作负载和代理之间的连接的认证及安全性无法保障。 节点和服务账号之间级别隔离，故障同“节点共享代理”。 同“节点共享代理”。 带有微代理的共享远程代理 因为为每个 pod 都注入一个微代理，开销比较大。 微代理专门处理 mTLS，不负责 L7 路由，可以保障安全性。 当需要应用 7 层策略时，工作负载实例的流量会被重定向到 L7 代理上，若不需要，则可以直接绕过。该 L7 代理可以采用共享节点代理、每个服务账户代理，或者远程代理的方式运行。 同“Sidecar 代理”。 服务网格的部署模式 生态发展 2021 年，Istio 社区也是精彩纷呈，举办了系列的活动，还发布了系列教程：\n2 月，首个 Istio 发行版， Tetrate Istio Distro（TID） 发布； 2 月，第一届 IstioCon 在线上举办，2000 多人参与了会议； 3 月，首个免费的线上 Istio 基础教程 发布； 5 月，首个 Istio 管理员认证考试（CIAT） 发布； 5 月，ServiceMeshCon Europe 在线上举办； 7 月，Istio Meetup China 在北京举办，100 多人现场参加； 10 月，ServiceMeshCon North America 在洛杉矶举办； 此外还有众多与 Istio 服务网格相关的项目开源，如下表所示。\n项目名称 开源时间 类别 描述 主导公司 Star 数量 与 Istio 的关系 Envoy 2016 年 9 月 网络代理 云原生高性能边缘/中间服务代理 Lyft 18700 默认的数据平面 Istio 2017 年 5 月 服务网格 连接、保护、控制和观察服务。 Google 29100 控制平面 Linkerd2 2017 年 12 月 服务网格 适用于 Kubernetes 的轻量级服务网格。 Buoyant 7900 服务网格的另一种实现 Emissary Gateway 2018 年 2 月 网关 用于微服务的 Kubernetes 原生 API 网关，基于 Envoy 构建 Ambassador 3600 可连接 Istio APISIX 2019 年 6 月 网关 云原生 API 网关 API7 8100 可作为 Istio 的数据平面运行也可以单独作为网关 MOSN 2019 年 12 月 代理 云原生边缘网关及代理 蚂蚁 3500 可作为 Istio 数据平面 Slime 2021 年 1 月 扩展 基于 Istio 的智能服务网格管理器 网易 236 为 Istio 增加一个管理平面 Tetrate Istio Distro 2021 年 2 月 工具 Istio 集成和命令行管理工具 Tetrate 95 第一个 Istio 开源发行版和多版本管理工具 Aeraki 2021 年 3 月 扩展 管理 Istio 的任何七层负载 腾讯 330 扩展多协议支持 Layotto 2021 年 6 月 运行时 云原生应用运行时 蚂蚁 393 可以作为 Istio 的数据平面 Hango Gateway 2021 年 8 月 网关 基于 Envoy 和 Istio 构建的 API 网关 网易 253 可与 Istio 集成 Istio 开源生态 注：数据统计截止到 2022 年 1 月 6 日。 总结 回望 2021 年，我们可以看出用户对服务网格的追求更趋实用，作为云原生网络的基础设施，其地位得到进一步夯实，更重要的是服务网格生态渐起。展望 2022 年，有两个值得关注的技术是 eBPF 和 WebAssembly。我们有理由相信，更多的服务网格实践优秀案例出现，在生态和标准化上更进一步。\n参考 告别 Sidecar——使用 eBPF 解锁内核级服务网格 网易开源 Istio 扩展项目 Slime 简介——基于 Istio 的智能服务网格管理器 Istio 1.12 引入 Wasm 插件配置 API 用于扩展 Istio 生态 基于 GRPC 和 Istio 的无 Sidecar 代理的服务网格 eBPF 如何简化服务网格 使用 Isito 前的考虑要素 ","relpermalink":"/blog/service-mesh-2021/","summary":"本文带您回顾了 2021 年服务网格的发展。","title":"服务网格 2021 年终盘点：实用当先，生态为本"},{"content":"随着跨入 2022 年，我们想分享下 2021 年 Tetrate 在您的支持下创造的一些亮点和达到的里程碑。自 2018 年 3 月成立以来，Tetrate 一直在增长其能力，以实现其设定的目标 —— 重新定义应用网络。今年是我们的第四个年头，由 Sapphire Ventures 领导的 B 轮融资和我们最近被指定为 Gartner 云计算的 Cool Vendor。以下是 Tetrate 在 2021 年完成的主要里程碑。\nTetrate 在 B 轮融资 中筹集了 4000 万美元 ，由 Sapphire Ventures 领投，Scale Venture Partners 、NTTVC 以及之前的投资者 Dell Technologies Capital 、Intel Capital 、8VC 和 Samsung NEXT 跟投。 Tetrate 与 NIST 合作举办了 第二届多云会议（Multi-cloud Conference） ，政府和行业的安全领导人在会上分享了最佳实践、演示和现实世界的使用案例，这些案例利用服务网格提供零信任架构（ZTA）并加速跨多云的 DevSecOps。Tetrate 还继续与 NIST 合作，在 Tetrate Service Bridge 实施下一代访问控制（NGAC），并为微服务定义零信任安全标准 。第三届年会 将于 2022 年 1 月 26-27 日举行。 在开源方面，Tetrate 仍然是 Istio 和 Envoy 的主要贡献者，并且是 Apache SkyWalking APM 和可观察性平台的主要支持者。Tetrate 是 Envoy 的第一大公司贡献者（按提交量计算 ），并对 Wasm 扩展功能、符合 SPIFFE 的 TLS 证书 验证功能以及 Proxy-Wasm C++ host 项目做出了显著贡献。我们还发布了 Func-e CLI （以前称为 GetEnvoy），使运行 Envoy 变得简单，并使开发人员能够快速运行不同的 Envoy 版本。对于 Istio，最流行的开源服务网格，Tetrate 发布了 GetMesh CLI ，作为 Tetrate Istio Distro 的一部分，它允许用户开始使用 Istio，并确保他们使用可信的、支持的 Istio 版本。Tetrate 工程师还帮助建立了 Istio 1.12 中宣布的 alpha 版本的 Wasm 插件功能 。 Tetrate 增强了其旗舰产品 Tetrate Service Bridge 。TSB 在多个集群中抽象出多个计算资源（虚拟机、k8s、云），允许企业使用一个非常性感的管理平面来控制、保护和观察其应用。最新的版本以 API 管理为特色 —— 它将 API 网关和服务网格整合到一个应用连接平台中，并提供一个托管版本的管理平面，因此客户只需带来他们的计算集群。TSB 由生产级和大规模运行的技术驱动，如 Envoy 、Istio 、SkyWalking 、Zipkin 和 下一代访问控制（NGAC） 。 Tetrate 支持世界各地的开源社区，并继续将社区和项目用户的教育作为核心责任。2021 年，我们创建了 Tetrate 学院 ，提供广受好评的免费课程 Istio Fundamentals 英文及中文教程 和 Envoy Fundamentals ，以及 Tetrate 认证 Istio 管理员（CIAT） 认证考试。我们组织了几十个网络研讨会（包括 YouTube 直播的 Istio Weekly 和 Istio 大咖说 （Bilibili 直播），赞助了 KubeCon、EnvoyCon NA、IstioCon 和云原生 Wasm Day，并与 NIST 共同主办了一个会议，以及世界各地的现场和虚拟聚会。 Tetrate 向亚太地区扩张 ，7 月和 12 月分别在新加坡和印度开设了办事处。 Tetrate 扩大了与 AWS 的合作关系 ，加入了 AWS ISV Accelerate 计划，并将其产品放在 Amazon Marketplace 上。Tetrate 现在与 Amazon EKS 和 EKS Anywhere 合作，为企业内部和云中的 Kubernetes 应用带来无缝连接和管理。 2021 年，Tetrate 的团队、客户数量和收入都翻了一番！我们的团队人数超过了 100 人。雇用了不同的职能部门：主要是工程和客户体验，但也有产品、营销和商务体验团队。我们有一个全明星的、分布在全球的团队，致力于客户、卓越技术、开源。Tetrate 与优秀的顾问 、新老客户和合作伙伴一起支持并分享我们的成长和成功。 Tetrate 被选为 Gartner 云计算领域优秀供应商 。Gartner 报告说，Tetrate 被选中是因为它提供了一个跨多云环境的服务网格解决方案，提供全面的应用连接、安全、可观察性和可靠性 —— 所有这些都来自一个平台层。“它的市场差异化源于其统一的网格功能、多云支持、灵活的运营模式和对开源的承诺”。 感谢我们的社区参与这个旅程，并与我们一起期待 2022 年！还有，我们正在招人 ！\n","relpermalink":"/notice/tetrate-2021/","summary":"随着跨入 2022 年，我们想分享下 2021 年 Tetrate 在您的支持下创造的一些亮点和达到的里程碑。","title":"Tetrate 2021 年度发展里程碑回顾"},{"content":"最近为某网站撰写服务网格技术的 2021 年总结，笔者关注该领域也有 4 年时间了，再结合自己最近这几年对云原生行业发展的观察，越发觉得《跨越鸿沟》（Crossing the chasm）这本书中所写的新技术的推广生命周期一一应验了。虽然该理论由 Jeffery Moore 于 1991 年提出，距今已有 30 年时间，但该理论至今依然奏效，另外该理论也在 CNCF 项目 的成熟度划分中得到应用，还有人指出过云原生技术需要跨越的鸿沟 。本文将为读者分享一些关于”鸿沟理论“有关的一些知识，希望能够引发大家对于新技术推广的一些思考。\n什么是鸿沟理论？ 鸿沟理论指的就是高科技产品在市场营销过程中遭遇的最大障碍：高科技企业的早期市场和主流市场之间存在着一条巨大的鸿沟，能否顺利跨越鸿沟并进入主流市场，成功赢得实用主义者的支持，就决定了一项高科技产品的成败。实际上每项新技术都会经历鸿沟。关键在予采取适当的策略令高科技企业成功地“跨越鸿沟”，摩尔在这本书中就告诉了人们一些欠经考验的制胜秘诀。\n前言 基于经典的钟形曲线分布，“跨越鸿沟 \u0026#34; 是一个将新技术的采用随时间推移而可视化的概念：从一小撮早期采用者开始，经过大规模的中端市场，最终进入最抗拒变化的消费者手中。\n1962 年，社会学家 Everett Rogers 出版了 Diffusion of Innovasions （创新扩散）一书。在这本书中，他根据消费者的购买行为，将他们分为不同的群体。他以 500 多项扩散研究的结果为基础进行分类。今天，这个模型被称为“技术采用生命周期”。这个模型全面地描述了新技术产品或创新的采用或接受情况。在《跨越鸿沟》一书中，杰弗里・摩尔根据扩散生命周期中的客户群体，阐述了成功锁定主流消费者的营销技巧。\n客户群体 基于人口学和心理学特征，客户群保护以下五种：\n创新者 早期采用者 早期大众 后期大众 落后者 图：”鸿沟理论“客户分布情况 分布情况 正如可以观察到的，技术采用的生命周期有一个钟形曲线。各个分界线大约相当于标准差的落点。这意味着：\n创新者约占总人口的 2.5% 早期采用者约占 13.5% 早期大众和后期大众均为 34% 落后者占剩余的 16% 每个群体都代表着一个独特的心理特征，即心理和人口特征的组合。因此，针对这些群体的营销需要与其他群体完全不同的策略。营销人员通过更好地了解这些群体之间的差异，可以通过正确的营销技术更好地锁定所有这些消费者。\n创新者 创新者是技术爱好者。这是第一个有可能投资于你产品的消费者群体。创新者积极地追求新的产品和技术。有时，他们甚至在公司启动正式的营销计划之前就开始寻求创新。这是因为技术在他们的生活或业务中占据了核心利益。对于这个客户群体来说，产品功能组合的完整性或性能是次要的。\n不幸的是，在任何特定的细分市场中，都没有很多创新者（大约 2.5%）。通常情况下，他们不愿意为新产品付出很多。尽管如此，赢得他们是很重要的，因为他们的认可为市场上的其他消费者提供了必要的保证。此外，技术爱好者可以作为一个测试小组，在面向主流市场之前进行必要的修改。\n早期采用者 和创新者一样，早期采用者也是有远见的人，他们在新产品的生命周期的早期就接受了新产品的概念。然而，与创新者不同，他们不是技术专家。相反，他们是有远见的人，不只是在寻找一种改进，而且是一种革命性的突破。因此，他们愿意承担高风险，尝试新事物。他们是对价格最不敏感的客户群体，对产品的功能设置和性能要求很高。\n早期采用者在做出购买决定时不依赖成熟的参考资料。相反，他们更愿意依靠自己的直觉和眼光。此外，他们愿意作为其他采用者群体的参考。由于有远见的人善于提醒其他人群，他们是最重要的争取对象。\n早期大众 这个客户群由实用主义者组成。前两个采用者群体属于早期市场。然而，为了获得真正的成功，一个公司必须从早期大众开始，赢得主流市场。这些实用主义者与早期采用者有一些相同的能力，能够与技术产生联系。然而，他们受到强烈的实用意识的驱动。他们知道，很多发明最终会成为过眼云烟。因此，在自己投资之前，他们更期望等待，看看其他客户对该技术的使用情况如何。他们希望在进行大量投资之前看到成熟的参考资料。因为这部分人很多（大约 34%），对于任何努力争取大量利润和增长的企业来说，赢得这些人的支持是最基本的。\n后期大众 这个群体主要由保守派组成。后期大众作为一个群体与早期大众一样大（占总人口的 34%）。他们与早期大众有着同样的担忧。此外，他们对传统的信仰远远多于对进步的信仰。早期大众的顾客如果决定购买新技术产品，他们对自己处理该产品的能力感到满意。相比之下，“后期大众 \u0026#34; 的成员则不然。因此，这些保守派更愿意等到某样东西已经成为一种惯例时才购买。\n落后者 这个群体是由怀疑论者组成的。这一部分人占总数的 16%。这些人根本不希望与新技术有任何关系。他们唯一一次购买技术产品是当它被深埋在另一个产品中时。这些持怀疑态度的人强烈认为，颠覆性的创新很少能实现他们的承诺。他们总是担心意外的后果。从市场发展的角度来看，落后者通常被认为是不值得追求的。然而，他们对产品功能设置和性能的批评为技术公司提供了宝贵的反馈。\n鸿沟 在技术采用生命周期中，你可以看到早期采用者和早期大众群体之间的差距。这个差距代表了技术必须跨越的鸿沟。它标志着将左边的群体作为右边的客户群的参考基础而产生的可信度差距。鸿沟的存在是因为消费者信任属于他们自己的采用者群体的人的推荐。\n当然，这给技术公司带来了一个具有挑战性的困境。\n如果他们还没有从你这里买过东西，你怎么能利用首选参考群体的人呢？\n换句话说，将一个群体的客户作为其他群体的参考是无效的。因此，鸿沟就是这样产生的！\n由于从早期采用者到早期大众的飞跃意味着从早期市场到主流市场的过渡，跨越鸿沟对于新推出的产品 / 技术真正实现市场成功是最重要的。\n总结 根据摩尔的说法，成功跨越鸿沟可以通过首先瞄准早期大众中一个非常具体的利基市场来实现。组织试图跨越鸿沟的唯一目标应该是在主流市场上获得一个桥头堡，以创造一个可供参考的实用主义客户群。在这里，细分就是一切：将你所有的营销资源集中在一个特定的细分市场上，并确保你在这个特定的细分市场上成为领导者，然后再去做下一个细分市场。这就是所谓的“大鱼小池 \u0026#34; 的方法。营销漏斗或 AIDA 模型是一个很好的营销框架，它可以帮助为潜在客户挑选正确的营销技术。此外，确保你的产品提供一个完整的解决方案，并且服务水平高（即整个产品解决方案）。实用主义者对你的产品的用户体验将最终决定他们是否也会激起他们的同行。一旦你在早期大众的不同部分建立了强大的口碑，你就成功地跨越了鸿沟。\n更多 你觉得服务网格目前处于鸿沟理论的哪个阶段呢？你又是何种受众？\n参考 《Crossing the Chasm》丨 NOTES - jianshu.com 灵雀云 CTO 陈恺：从“鸿沟理论”看云原生，哪些技术能够跨越鸿沟？ - infoq.cn ","relpermalink":"/blog/crossing-the-chasm/","summary":"本文将为读者分享一些关于“鸿沟理论”有关的一些知识，希望能够引发大家对于新技术推广的一些思考。","title":"跨越鸿沟：理解鸿沟理论"},{"content":"最近我在研究 Istio 生态中的开源项目，Slime 这个项目开源与 2021 年初，是由网易数帆微服务团队开源的一款基于 Istio 的智能网格管理器。Slime 基于 Kubernetes Operator 实现，可作为 Istio 的 CRD 管理器，无须对 Istio 做任何定制化改造，就可以定义动态的服务治理策略，从而达到自动便捷使用 Istio 和 Envoy 高阶功能的目的。\nSlime 试图解决的问题 Slime 项目的诞生主要为了解决以下问题：\n网格内所有服务配置全量下到所有 Sidecar Proxy，导致其消耗大量资源使得应用性能变差的问题 如何在 Istio 中实现高阶扩展的问题：比如扩展 HTTP 插件；根据服务的资源使用率做到自适应限流 Slime 解决以上问题的答案是构建 Istio 的控制平面，具体做法是：\n构建可拔插控制器 数据平面监控 CRD 转换 通过以上方式 Slime 可以实现配置懒加载和插件管理器。\nSlime 架构 Slime 内部分为三大模块，其架构图如下所示。\nSlime 内部架构图 Slime 内部三大组件为：\nslime-boot：在 Kubernetes 上部署 Slime 模块的 operator。 slime-controller：Slime 的核心组件，监听 Slime CRD 并将其转换为 Istio CRD。 slime-metric：用于获取服务 metrics 信息的组件，slime-controller 会根据其获取的信息动态调整服务治理规则。 目前 Slime 内置了三个控制器子模块：\n配置懒加载（按需加载）：用户无须手动配置 SidecarScope，Istio 可以按需加载服务配置和服务发现信息； HTTP 插件管理：使用新的 CRD——pluginmanager/envoyplugin 包装了可读性，摒弃了可维护性较差的 envoyfilter，使得插件扩展更为便捷； 自适应限流：结合监控信息自动调整限流策略； 什么是 SidecarScope？\nSidecarScope 是在 Istio 1.1 版本中引入的，它并不是一个直接面向用户的配置项，而是 Sidecar 资源的包装器，具体来说就是 Sidecar 资源 中的 egress 选项。通过该配置可以减少 Istio 向 Sidecar 下发的数据量，例如只向某个命名空间中的某些服务下发某些 hosts 的访问配置，从而提高应用提高性能。\n使用 Slime 作为 Istio 的控制平面 为了解决这些问题，Slime 在 Istio 之上构建了更高层次的抽象，相当于为 Istio 构建了一层管理平面，其工作流程图如下所示。\nSlime 工作流程图 具体步骤如下：\nSlime Operator 根据管理员的配置在 Kubernetes 中完成 Slime 组件的初始化； 开发者创建符合 Slime CRD 规范的配置并应用到 Kubernetes 集群中； Slime 查询 Prometheus 中保存的相关服务的监控数据，结合 Slime CRD 中自适应部分的配置，将 Slime CRD 转换为 Istio CRD，同时将其推送到 Global Proxy 中； Istio 监听 Istio CRD 的创建； Istio 将 Sidecar Proxy 的配置信息推送到数据平面相应的 Sidecar Proxy 中； 以上只是一个对 Slime 工作流程的一个笼统的介绍，更多详细信息请参考 Slime GitHub 。\n配置懒加载 为了解决数据平面中 Sidecar Proxy 资源消耗过大及网络延迟问题，Slime 使用了配置懒加载（按需加载 Sidecar 配置）的方案。该方案的核心思想是向每个 Sidecar Proxy 中只下发其所 Pod 中服务所需的配置，而不是将网格中的所有服务信息全量下发。所以 Slime 需要获取每个服务的调用关系这样才能得到其所需的 Sidecar Proxy 配置。\nSlime 实现 Sidecar Proxy 配置懒加载的方法是：\n让数据平面中的所有服务的首次调用都通过一个 Global Proxy，该 Proxy 可以记录所有服务的调用和依赖信息，根据该依赖信息更新 Istio 中 Sidecar 资源的配置； 当某个服务的调用链被 VirtualService 中的路由信息重新定义时，Global Proxy 原有记录就失效了，需要一个新的数据结构来维护该服务的调用关系。Slime 创建了名为 ServiceFence 的 CRD 来维护服务调用关系以解决服务信息缺失问题。 使用 Global Proxy 初始化服务调用拓扑 Slime 在数据平面中部署 Global Proxy（也叫做 Global Sidecar，但其与应用的 Pod 不是一对一的关系，笔者更倾向于称其为 Global Proxy），该代理同样使用 Envoy 构建，在每个需要启动配置懒加载的命名空间中部署一个或在整个网格中只部署一个，所有缺失服务发现信息的调用（你也可以手动配置服务调用关系），都会被兜底路由劫持到 Global Proxy，经过其首次转发后，Slime 便可感知到被调用方的信息，然后根据其对应服务的 VirtualService，找到服务名和真实后端的映射关系，将两者的都加入 SidecarScope，以后该服务的调用就不再需要经过 Global Proxy 了。\n使用 ServiceFence 维护服务调用拓扑 在使用 Global Proxy 初始化服务调用拓扑后，一旦服务调用链有变动的话怎么办？对此 Slime 创建了 ServiceFence 的 CRD。使用 ServiceFence 可以维护服务名和后端服务的映射关系。Slime 根据其对应服务的 VirtualService，找到 Kubernetes 服务名和真实后端（host）的映射关系，将两者的都加入 Sidecar 的配置中。ServiceFence 管理生成的 SidecarScope 的生命周期，自动清理长时间不用的调用关系，从而避免上述问题。\n如何开启配置懒加载 配置懒加载功能对于终端用户是透明的，只需要 Kubernetes Service 上打上 istio.dependency.servicefence/status:\u0026#34;true\u0026#34; 的标签，表明该服务需要开启配置懒加载，剩下的事情交给 Slime Operator 来完成即可。\nHTTP 插件管理 Istio 中的插件扩展只能通过 EnvoyFilter 来实现，因为它是 xDS 层面的配置，管理和维护这样的配置需要耗费大量的精力，也极容易出错。因此，Slime 在 EnvoyFilter 的基础上做了一层面向插件的抽象。\nSlime 共有两个 CRD 用于 HTTP 插件管理，分别是：\nPluginManager：配置为哪些负载开启哪些插件，插件的配置顺序即为执行顺序； EnvoyPlugin：EnvoyPlugin 不关心每个插件的具体配置，具体配置会被放在 EnvoyFilter 资源的 patch.typed_config 结构中透传），EnvoyPlugin 的核心思想是将插件配置在需要的维度中做聚合，从而限定插件的生鲜范围。这样做一方面更加贴合插件使用者的习惯，另一方面也降低了上层配置的冗余， 关于 Slime 中插件管理的详细使用方式请见 Slime GitHub 。\n自适应限流 Envoy 内置的限流组件功能单一，只能以实例维度配置限流值，无法做到根据应用负载的自适应限流。Slime 通过与 Prometheus metric server 对接，实时的获取监控情况，来动态配置限流值。\nSlime 自适应限流的流程图如下所示。\nSlime 的自适应限流流程图 Slime 的自适应限流的流程分为两部分，一部分为 SmartLimiter 到 EnvoyFilter 的转换，另一部分为获取监控数据。目前 Slime 支持从 Kubernetes Metric Server 获取服务的 CPU、内存、副本数等数据。Slime 还对外提供了一套监控数据接口（Metric Discovery Server），通过 MDS，可以将自定义的监控指标同步给限流组件。\nSlime 创建的 CRD SmartLimiter 用于配置自适应限流。其的配置是接近自然语义，例如希望在 CPU 超过 80% 时触发服务 A 的访问限制，限额为 30QPS，对应的 SmartLimiter 定义如下：\napiVersion: microservice.netease.com/v1alpha1 kind: SmartLimiter metadata: name: a namespace: default spec: descriptors: - action: fill_interval: seconds: 1 quota: \u0026#34;30/{pod}\u0026#34; # 30 为该服务的额度，将其均分给每个 pod，加入有 3 个 pod，则每个 pod 的限流为 10 condition: \u0026#34;{cpu}\u0026gt;0.8\u0026#34; # 根据监控项{cpu}的值自动填充该模板 更多 Slime 开源于 2021 年初，本文发稿时该项目仍处于初级阶段，本文大量参考了杨笛航在云原生社区中的分享 Slime：让 Istio 服务网格变得更加高效与智能 及 Slime 的 GitHub 。感兴趣的读者可以关注下这个项目的 GitHub，进一步了解它。\n另外欢迎关注服务网格和 Istio 的朋友加入云原生社区 Istio SIG ，一起参与讨论和交流。\n参考 Slime：让 Istio 服务网格变得更加高效与智能 - cloudnative.to Slime GitHub 文档 - github.com Sidecar - istio.io ","relpermalink":"/blog/slime-intro/","summary":"本文介绍的是由网易数帆微服务团队开源的一款基于 Istio 的智能网格管理器 Slime。","title":"网易开源 Istio 扩展项目 Slime 简介——基于 Istio 的智能服务网格管理器"},{"content":"本文根据 2021 年 11 月 22 日晚我应极客邦邀请在「极客时间训练营」的直播分享《云原生漫谈：聊聊 Service Mesh 的现状》整理而成。\n本来极客时间是想邀请我分享云原生的，但我觉得那个范围太大，在一次分享中只能泛泛而谈，无法聚焦到一个具体的点，因此我想还是先聚焦在服务网格这一个专题上吧。云原生社区最近倒是在做一个云原生系列的分享 ，大家可以关注下。\n这是我今天分享的大纲：\n第一探讨下服务网格跟云原生的关系 第二是给大家陈述下我观察到的目前社区里关于服务网格有哪些争论 第三是给大家介绍几个服务网格的相关的开源项目 最后是畅想下服务网格未来的发展 服务网格与云原生的关系 首先我们将探讨下服务网格与云原生的关系。\n服务网格——容器编排大战后的产物 Docker Swarm vs Kubernetes vs Mesos 如果你关注云原生领域足够早的话，应该还会对 2015 到 2017 年间的容器编排大战记忆犹新。关于服务网格的起源已经无需多言。2017 年 Kubernetes 获得了容器大战的胜利，微服务的理念已经深入人心，容器化的趋势可谓势不可挡。Kubernetes 架构趋向成熟，慢慢变得无聊，以 Linkerd、Istio 为代表的服务网格技术进入了 CNCF 定义的云原生关键技术视野中。\n服务网格将微服务中的通用的功能给下沉到了基础设施层，让开发者可以更加专注于业务逻辑，从而加快服务交付，这与整个云原生的理念的一致的。你不需要再在应用中集成笨重的 SDK，为不同语言开发和维护 SDK，应用部署完后，使用服务网格进行 Day 2 操作即可。\nKubernetes 设计之初就是按照云原生的理念设计的，云原生中有个重要概念就是微服务的架构设计，当将单体应用拆分微服务后，随着服务数量的增多，如何微服务进行管理以保证服务的 SLA 呢？为了从架构层面上解决这个问题，解放程序员的创造性，避免繁琐的服务发现、监控、分布式追踪等事务，服务网格应运而生。\n微服务关注点 来源：https://developers.redhat.com/blog/2016/12/09/spring-cloud-for-microservices-compared-to-kubernetes 服务网格被誉为下一代微服务，从右面这幅图里我们可以看到微服务的一些关注点，这些关注点很多与 Kubernetes 的功能是重合的，既然这些作为平台级的功能 Kubernetes 已经提供了，为什么还要使用服务网格呢？其实 Kubernetes 关注的还是应用的生命周期，它管理的对象是资源和部署，对于服务的管控力度很小。而服务网格正好弥补了这个缺陷。服务网格可以连接、控制、观察和保护微服务。\nKubernetes vs xDS vs Istio 这幅图展示的是 Kubernetes 和 Istio 的分层架构图。\nKubernetes vs Service mesh 从图中我们可以看到 kube-proxy 的设置是全局的，无法对每个服务进行细粒度的控制，Kubernetes 可以做的只有拓扑感知路由、将流量就近路由，为 Pod 设置进出站的网络策略。\n而服务网格通过 sidecar proxy 的方式将 Kubernetes 中的流量控制从服务层中抽离出来，为每个 Pod 中注入代理，并通过一个控制平面来操控这些分布式代理。这样可以实现更大的弹性。\nKube-proxy 实现了一个 Kubernetes 服务的多个 pod 实例之间的流量负载均衡，但如何对这些服务之间的流量进行精细化控制–比如将流量按百分比划分给不同的应用版本（这些应用版本都是同一个服务的一部分，但在不同的部署上），或者做金丝雀发布（灰度发布）和蓝绿发布？\nKubernetes 社区给出了一个使用 Deployment 做金丝雀发布的方法，本质上是通过修改 pod 的标签来给部署的服务分配不同的 pod。\nEnvoy 架构图 目前在中国最流行的服务网格开源实现是 Istio，也有很多公司对 Istio 进行了二次开发，比如蚂蚁、网易、腾讯等，其实 Istio 是在 Envoy 的基础上开发的，从它开源的第一天起就默认使用了 Envoy 作为它的分布式代理。Envoy 开创性的创造了 xDS 协议，用于分布式网关配置，大大简化了大规模分布式网络的配置。2019 年蚂蚁开源的 MOSN 同样支持了 xDS。Envoy 还是 CNCF 中最早毕业的项目之一，经过大规模的生产应用考验。可以说 Istio 的诞生已经有了很好的基础。\n下表是 Kubernetes、xDS、Istio 三者之间的资源抽象对比。\nKubernetes xDS Istio 服务网格 Endpoint Endpoint WorkloadEntry Service Route VirtualService kube-proxy Route DestinationRule kube-proxy Listener EnvoyFilter Ingress Listener Gateway Service Cluster ServiceEntry kube-proxy 组件、xDS 和 Istio 对流量管理的抽象后，现在我们仅从流量管理的角度来看看这三个组件 / 协议的比较。请注意，三者并不完全等同。Kubernetes 更加注重的是应用层面的流量管理，xDS 是更加抽象的协议层面的配置下发，而 Istio 是服务层面的配置。\n服务网格——云原生网络基础设施 在列举过以上 Kubernetes 和服务网格的对比后，我们可以看出服务网格在云原生应用架构中的地位。那就是构建一个云原生网络基础设施，具体来说就是：\n流量管理：控制服务间的流量和 API 调用流，使调用更可靠，增强不同环境下的网络鲁棒性。 可观测性：了解服务之间的依赖关系和它们之间的性质和流量，提供快速识别定位问题的能力。 策略实施：通过配置网格而不是以改变代码的方式来控制服务之间的访问策略。 服务识别与安全：提供在网格里的服务可识别性和安全性保护。 社区里关于 Istio 和服务网格的争论 然而构建基础设施，可谓牵一发而动全身。理想很丰满，现实很骨感。关于服务网格和 Istio，在社区中也不乏争论。我们来看看有这些争论主要有哪些。\n这里列举了我在社区中观察到的关于 Istio 和服务网格最常见的几个问题。\n有人在生产使用 Istio 吗？ 为 pod 注入 sidecar 后带来的大量资源消耗，影响应用性能？ Istio 支持的协议有限，不易扩展？ Istio 太过复杂，老的服务迁移成本太高，业界经验太少，学习曲线陡峭？ 第一个问题，也是很多人刚加入社区和了解这门技术的时候，问的第一个问题，那是有人在生产使用 Istio 吗？\n随着对 Istio 研究的深入，很多人就会抛出第二个问题，为 pod 注入 sidecar 后带来的大量资源消耗，会影响应用性能吗？\n如果能问到第三个问题，说明对 Istio 有比较强的需求了，大多是使用了自定义的 RPC，对 Istio 的协议扩展有需求。 最后一个问题是抱怨 Istio 的概念太过复杂，也没有一个清晰的迁移路径可以使用，学习曲线太过陡峭。\n下面我将一一回答这些问题。\nIstio 架构稳定，生产可用，生态渐起 Istio 发布时间表 首先我们来看下 Istio 的发布时间表，1.12 版本在上周刚刚发布，这里列举了从它开源到 1.8 版本发布的时间表。2018 年可以说是服务网格爆发之年，Tetrate 也在这一年成立。自 1.5 版本起 Istio 正式确立了当前的架构。Istio 社区也也举办了丰富多彩的活动，2021 年 3 月首届 IstioCon 召开，7 月 Istio Meetup China 在北京举行，2022 年 1 月，Service Mesh Summit 2022 也将在上海举行。\nIstio 有着庞大的社区以及供应商和用户群体 。目前主流公有云全都支持了 Istio 服务网格，如阿里云、华为云、腾讯云、网易云等，Istio 的官网上也列举了几十个社区用户，云原生社区 Istio SIG 还陆续举办了八场 Istio 大咖说 ，百度、腾讯、网易、小红书、小电科技都来分享过他们的 Istio 实践。\n还有很多企业基于 Istio 做了二次开发或者适配或者为其开发插件，可以说是 Istio 架构已稳定，生产可用，生态正在萌芽中。\n服务网格对应用性能的影响 服务网格为了做到对应用程序透明，默认采用了 iptables 流量劫持的方式，当服务数量大的时候会有大量的 iptables 规则，影响网络性能，你可以使用 eBPF 这样的技术来提高应用性能，但是该技术对操作系统内核的版本要求比较高，很少有企业能够达到。\nIstio 中的智能 DNS 代理 来源：https://cloudnative.to/blog/istio-dns-proxy/ 还有一种方式，也是小红书使用的方式 ，那就是利用 Istio 1.8 中引入的智能 DNS 代理功能。首先使用 ServiceEntry 定义服务，让所有服务属于一个 VIP 范围，再利用 Istio 的智能 DNS 代理功能，让 sidecar 只拦截 VIP 网段的流量，这样可以减少 iptables 规则，从而提高性能。如果想深入了解这个做法的细节，大家可以去浏览 Istio 大咖说第八期的分享视频 。\nIstio 在初期是将整个网格内的所有服务的路由信息全量下发到所有的 proxy sidecar 中，会导致 sidecar 占用大量资源，后来 Istio 引入了 Sidecar 资源 来精细化控制需要下发的代理配置范围，另外还有企业自己开发了配置懒加载功能，例如腾讯云开源的 Aeraki 、网易开源的 Slime 都可以实现配置懒加载。我们会在 Istio 开源生态中介绍这两个开源项目。\n最后是一个涉及到 Sidecar proxy 运维的问题，如何在保证流量不断的情况下，升级所有 Envoy 代理，这个阿里开源的 OpenKruise 中的 SidecarSet 资源已经给出了解决方案。\n另外 Sidecar 的引入带来的资源消耗以及网络延迟也是在合理的范围内，大家可以参考 Istio 官方博客上的 Service Mesh 基准性能测试 。\n扩展 Istio 服务网格 下一个问题是关于扩展 Istio 服务网格的。目前官方社区给出的方案是使用 WebAssembly，目前这种扩展方式在国内用的还比较少，而且性能也堪忧。我观察到的大部分解决方案都是自定义 CRD，基于 Istio 构建服务网格管理平面。\n另外，让 Istio 支持异构环境，适用于一切工作负载，如虚拟机、容器，这个对于终端用户来说也有很强的需求，因为这可以让用户很方便的从传统负载迁移应用到服务网格中。最后是多集群、多网格的混合云流量管理，这个属于比较高阶的需求了。\n陡峭的学习曲线 以下列举的是 Istio 学习资源：\nIstio 官网中文文档 IstioCon 2021 Istio Meetup China Istio 大咖说/Istio Weekly 云原生社区 Istio SIG Istio 基础教程（中文） Certified Istio Administrator Istio 开源至今已有 4 年时间，2018 年时我和敖小剑一起创建了 ServiceMesher 社区，当时组织过 9 次 Service Mesh Meetup，同其他服务网格爱好者一起翻译了 Istio 的官方文档。我还在今年初参与了 IstioCon 2021 的筹办及首届 Istio Meetup China。可以说是亲眼目睹了国内服务网格技术的应用和发展，在这期间也写过和翻译过大量的文章，加入 Tetrate 后，我还参与发布了 Istio 基础教程，免费提供给大家学习。同时 Tetrate 也推出了 …","relpermalink":"/blog/service-mesh-insight/","summary":"本文探讨了服务网格和云原生的关系，社区发展现状，开源生态，及未来发展。","title":"服务网格现状之我见"},{"content":"业界首个在线的免费中文多媒体的 Istio 教程来了，在 9 月 25 日周六的云原生社区 meetup 第七期深圳站上，Tetrate 布道师、云原生社区创始人宋净超（Jimmy Song）发布了 Istio 中文基础教程。\n该教程由企业级服务网格供应商 Tetrate 出品，该课程已上线，可免费报名学习。报名地址：https://academy.tetrate.io/courses/istio-fundamentals-zh Istio 基础教程页面 课程简介 本课程分为 8 个模块，其中包括理论部分，我们将学习 Istio 的各项功能；实践实验室，我们将在实践中尝试这些功能；以及测试你的这些知识的掌握程度。\n在本课程结束时，您将了解什么是 Istio，使用真实世界的例子，了解服务网格给您的组织带来的价值。你将能够配置流量路由、注入故障、使用弹性功能并保护你的服务。本课程是为了加强你的学习，在课程结束时，如果你能正确回答所有测验的 70%，将提供一份完成证书。\n本课程是为了加强你的学习，在课程结束时，如果你能正确回答所有测验的 70%，将提供一份完成证书。\n本课程有 4 个小时的内容，是自定进度的。你可以按照自己的节奏完成，可以在一天内完成，或者根据你的方便，分散在不同的时间内完成。\n","relpermalink":"/notice/istio-fundamental-courses-zh/","summary":"本教程有企业级服务网格提供商 Tetrate 出品。","title":"首个免费的多媒体 Istio 在线中文教程来了"},{"content":" 时间：2021 年 8 月 22 日（星期日） 地点：大连市甘井子区汇贤街大连腾飞园区腾飞软件园 5 号楼（世达教育） 主持人：马景贺（云原生社区管委会成员，云原生社区大连站长） 报名方式：活动行 关于云原生社区 云原生社区是国内最大的独立第三方云原生终端用户和泛开发者社区，由 CNCF 大使、开源意见领袖共同发起成立于 2020 年 5 月 12 日，提供云原生专业资讯，促进云原生产业发展。\n社区官网：https://cloudnative.to 加入大连站 关注云原生社区微信公众号，微信搜索（CloudNativeCN），在后台回复「大连站」即可加入大连站微信群。\n","relpermalink":"/notice/cloud-native-meetup-dalian/","summary":"8 月 22 日，周日，大连见！","title":"云原生社区 Meetup 第六期大连站报名"},{"content":"近日美国国家安全局（NSA）和网络安全与基础设施安全署（CISA）发布了一份网络安全技术报告 Kubernetes Hardening Guidance（查看英文原版 PDF ）。\nJimmy 翻译的《Kubernetes 加固指南》（或译作《Kubernetes 强化指南》）中文版，点击在线阅读 ，如您发现错误，欢迎在 GitHub 上提交勘误。\n《Kubernetes 加固指南》或（《Kubernetes 强化指南》中文版封面 ","relpermalink":"/notice/kubernetes-hardening-guidance-zh-release/","summary":"《Kubernetes 加固指南》（或译作《Kubernetes 强化指南》中文版发布。","title":"《Kubernetes 加固指南》中文版发布"},{"content":"API 网关作为客户端访问后端的入口，已经存在很长时间了，它主要是用来管理”南北向“的流量；近几年服务网格开始流行，它主要是管理系统内部，即“东西向”流量，而像 Istio 这样的服务网格还内置了网关，从而将系统内外部的流量纳入了统一管控。这经常给初次接触 Istio 的人带来困惑——服务网格与 API 网关之间是什么关系？是不是使用了 Istio 就可以替代了 API 网关？Istio 的 API 网关是如何运作的？有哪些方式暴露 Istio mesh 中的服务？这篇文章给为你解答。\n主要观点 服务网格诞生的初衷是为了解决分布式应用的内部流量的管理问题，而在此之前 API 网关已存在很久了。 虽然 Istio 中内置了 Gateway，但是你仍可以使用自定义的 Ingress Controller 来代理外部流量。 API 网关和服务网格正朝着融合的方向发展。 如何暴露 Istio mesh 中的服务？ 下图展示了使用 Istio Gateway、Kubernetes Ingress、API Gateway 及 NodePort/LB 暴露 Istio mesh 中服务的四种方式。\n暴露 Kubernetes 中服务的几种方式 其中阴影表示的是 Istio mesh，mesh 中的的流量属于集群内部（东西向）流量，而客户端访问 Kubernetes 集群内服务的流量属于外部（南北向）流量。不过因为 Ingress、Gateway 也是部署在 Kubernetes 集群内的，这些节点访问集群内其他服务的流量就难以归属了。\n方式 控制器 功能 NodePort/LoadBalancer Kubernetes 负载均衡 Kubernetes Ingress Ingress Controller 负载均衡、TLS、虚拟主机、流量路由 Istio Gateway Istio 负载均衡、TLS、虚拟主机、高级流量路由、其他 Istio 的高级功能 API 网关 API Gateway 负载均衡、TLS、虚拟主机、流量路由、API 生命周期管理、权限认证、数据聚合、账单和速率限制 由于 NodePort/LoadBalancer 是 Kubernetes 内置的基本的暴露服务的方式，本文就不讨论这种方式了。下文将对其他三种方式分别作出说明。\n使用 Kubernetes Ingress 暴露服务 我们都知道 Kubernetes 集群的客户端是无法直接访问 Pod 的 IP 地址的，因为 Pod 是处于 Kubernetes 内置的一个网络平面中。我们可以将 Kubernetes 内的服务使用 NodePort 或者 LoadBlancer 的方式暴露到集群以外。同时为了支持虚拟主机、隐藏和节省 IP 地址，可以使用 Ingress 来暴露 Kubernetes 中的服务。Kubernetes Ingress 原理如下图所示。\n使用 Kubernetes Ingress 暴露服务 简单的说，Ingress 就是从 Kubernetes 集群外访问集群的入口，将用户的 URL 请求转发到不同的服务上。Ingress 相当于 Nginx、Apache 等负载均衡方向代理服务器，其中还包括规则定义，即 URL 的路由信息，路由信息得的刷新由 Ingress controller 来提供。\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: istio name: ingress spec: rules: - host: httpbin.example.com http: paths: - path: /status/* backend: serviceName: httpbin servicePort: 8000 上面的例子中的 kubernetes.io/ingress.class: istio 注解表明该 Ingress 使用的 Istio Ingress Controller。\n使用 Istio Gateway 暴露服务 我们都知道 Istio 是继承 Kubernetes 之后发展出来的一个流行的服务网格实现，它实现了 Kubernetes 没有的一些功能，请参考什么是 Istio？为什么 Kubernetes 需要 Istio？ 简要来说，正是因为 Istio 补足了 Kubernetes 对于云原生应用的流量管理、可观察性和安全方面的短板，使得流量管理变得对应用程序透明，使这部分功能从应用程序中转移到了平台层，成为了云原生基础设施。\nIstio 0.8 以前版本中使用 Kubernetes Ingress 来作为流量入口，其中使用 Envoy 作为 Ingress Controller。在 Istio 0.8 及以后的版本中，Istio 创建了 Gateway 对象。Gateway 和 VirtualService 用于表示 Istio Ingress 的配置模型，Istio Ingress 的缺省实现则采用了和 sidecar 相同的 Envoy 代理。通过该方式，Istio 控制面用一致的配置模型同时控制了入口网关和内部的 sidecar 代理。这些配置包括路由规则，策略检查、遥测收集以及其他服务管控功能。\nIstio Gateway 的功能与 Kubernetes Ingress 类似，它负责进出集群的南北流量。Istio Gateway 描述了一个负载均衡器，用于承载进出服务网格边缘的连接。该规范描述了一组开放端口和这些端口所使用的协议，以及用于负载均衡的 SNI 配置等。\nIstio Gateway 资源本身只能配置 L4 到 L6 的功能，例如暴露的端口、TLS 设置等；但 Gateway 可与 VirtualService 绑定，在 VirtualService 中可以配置七层路由规则，例如按比例和版本的流量路由，故障注入，HTTP 重定向，HTTP 重写等所有 Mesh 内部支持的路由规则。\n下面是一个 Gateway 与 VirtualService 绑定的示例。拥有 istio: ingressgateway 标签的 pod 将作为 Ingress Gateway 并路由对 httpbin.example.com 虚拟主机的 80 端口的 HTTP 访问，这相当于给 Kubernetes 敞开了一个外部访问的入口。这与使用 Kubernetes Ingress 最大的区别就是，需要我们手动将 VirtualService 与 Gateway 绑定，并指定 Gateway 所在的 pod。\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: httpbin-gateway spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;httpbin.example.com\u0026#34; 下面这个 VirtualService 通过 gateways 与上面的网关绑定在了一起，以接受来自该网关的流量。\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: httpbin spec: hosts: - \u0026#34;httpbin.example.com\u0026#34; gateways: - httpbin-gateway http: - match: - uri: prefix: /status route: - destination: port: number: 8000 host: httpbin 使用 API 网关暴露服务 API 网关是位于客户端和后端服务之间的 API 管理工具，一种将客户端接口与后端实现分离的方式，在微服务中得到了广泛的应用。当客户端发出请求时，API 网关会将其分解为多个请求，然后将它们路由到正确的位置，生成响应，并跟踪所有内容。\nAPI Gateway 是微服务架构体系中的一类型特殊服务，它是所有微服务的入口，它的职责是执行路由请求、协议转换、聚合数据、认证、限流、熔断等。大多数企业 API 都是通过 API 网关部署的。API 网关通常会处理跨 API 服务系统的常见任务，例如用户身份验证、速率限制和统计信息。\n在网格中可以有一个或多个 API Gateway。API 网关的职责有：\n请求路由和版本控制 方便单体应用到微服务的过渡 权限认证 数据聚合：监控和计费 协议转换 消息和缓存 安全和报警 以上很多基本功能比如路由和权限认证通过 Istio Gateway 也可以实现，只是在功能的丰富度和扩展性方面有些成熟的 API Gateway 可能更占优势，不过在 Istio mesh 中再引入 API Gateway 也可能带来一些弊端。\n引入了 API Gateway，需要考虑 API Gateway 本身的部署、运维、负载均衡等场景，增加了后端服务的复杂度 API Gateway 中承载了大量的接口适配，导致难以维护 对于部分场景，增加了一跳可能导致性能的降低 总结 在 Istio mesh 中你可以使用多种 Kubernetes Ingress Controller 来充当入口网关，当然你还可以直接使用 Istio 内置的 Istio 网关，对于策略控制、流量管理和用量监控可以直接通过 Istio 网关来完成，这样做的好处是通过 Istio 的控制平面来直接管理网关，而不需要再借助其他工具。但是对于 API 生命周期管理、复杂的计费、协议转换和认证等功能，传统的 API 网关可能更适合你。所以，你可以根据自己的需求来选择，也可以组合使用。\n目前有些传统的反向代理也在向 Service Mesh 方向发展，如 Nginx 构建了 Nginx Service Mesh ，Traefik 构建了 Traefik Mesh 。还有的 API 网关产品也向 Service Mesh 方向挺进，比如 Kong 发展出了 Kuma 。在未来，我们会看到更多 API 网关、反向代理和服务网格的融合产品出现。\n参考 利用 Gateway API 发展 Kubernetes 网络 如何为服务网格选择入口网关？ Service Mesh 和 API Gateway 关系深度探讨 在 Istio 服务网格中使用 Traefik Ingress Controller ","relpermalink":"/blog/istio-servicemesh-api-gateway/","summary":"服务网格与 API 网关之间是什么关系？是不是使用了 Istio 就可以替代了 API 网关？Istio 的 API 网关是如何运作的？有哪些方式暴露 Istio mesh 中的服务？这篇文章给为你解答。","title":"如何理解 Istio Ingress，它与 API Gateway 有什么区别？"},{"content":" ApacheCon Asia 2021 为了更好地服务于亚太快速增长的 Apache 用户和贡献者，ApacheCon 组委会以及 Apache 软件基金会宣布首次针对亚太地区时区的 ApacheCon 在线会议 ApacheCon Asia 大会将于 2021 年 8 月 6 日至 8 日在线举行。周六（8 月 7 日）下午 1 点半到 5 点半，因为「可观察性」专题的出品人吴晟临时有事无法主持，我将代替主持，欢迎大家观看直播。\n可观察性专题 时间 话题 时长 13:30 - 14:10 如何使用 Apache SkyWalking Log Analysis Language 来进行日志分析，Zhenxu Ke (柯振旭)【中】 40 分钟 14:10 - 14:50 Satellite 如何强化 SkyWalking 生态，Evan【英】 40 分钟 14:50 - 15:30 使用 Skywalking 监控 Dolphinscheduler, 王海林【中】 40 分钟 15:30 - 16:10 如何使用剖析来弥补分布式追踪的盲区，Han Liu【中】 40 分钟 16:10 - 16:50 SkyWalking UI 的低代码模式，Qiuxia Fan【英】 40 分钟 16:50 - 17:30 使用 SkyWalking 和 APISIX 来增强 Nginx 的可观测性，Ming Wen【中】 40 分钟 出品人 以下是本次大会的分论坛和出品人：\nAPI / 微服务 温铭 Incubator 潘娟 Keynote 谭中意 Web Server/Tomcat 张乎兴 中间件 张亮，Jean-Baptiste Onofre 可观测性 吴晟 大数据 张铎，堵俊平，代立冬 工作流和数据治理 郭炜 开源社区 李建盛 数据可视化 羡辙 流处理 李钰 消息系统 翟佳，王殿进 物联网（IoT）和工业物联网（IIoT） 黄向东，Christofer Dutz 集成 姜宁，Zoran Regvart, María Arias de Reyna 详细日程请查看Apache 首次亚洲线上技术峰会 | 议程总览 ，看看有没有你熟悉的开源项目，点击注册参会 。\n","relpermalink":"/notice/apachecon-asia-2021/","summary":"首次针对亚太地区时区的 ApacheCon 在线会议 ApacheCon Asia 大会将在 8 月 6 日至 8 日线上举行。","title":"ApacheCon Asia 2021"},{"content":" 在琼库什台村与哈萨克族村民在一起 摄于新疆维吾尔自治区伊犁哈萨克自治州特克斯县琼库什台村，晓辉和我与哈萨克族村民在一起\n我欲乘风破浪，踏遍黄沙海洋\n与其误会一场，也要不负勇往\n——Jam，《七月上》\n今天是七月的最后的一天，从今天起我的博客将开通旅行 专栏。距离我从新疆回到北京也快一个星期了，这篇博客用来记录我与张晓辉（Addo Zhang 在新疆的七日之旅。\n行程安排 因为晓辉最近处于工作变动空档期，而我又远程工作，今年元旦以来都没有到远游（上一次是元旦到云南大理、丽江），因此我们商议了为其七天的新疆北疆房车自驾之旅。\n我们将分别从北京和广州前往乌鲁木齐汇合然后提车出发，车是出发前就在网上预约好的，现场办手续，进行了 2 个小时的使用培训，然后就上路了。因为晓辉是老司机，而山路崎岖，房车又过于笨重，对于我这个新手难以驾驭，我就作为副驾，同时也会在路上工作。\n行程路线图 我们基本是按照出发前预定的路线走的，整个旅程最精彩的部分应该是：\n赛里木湖牧民家的烧烤晚餐 琼库什台原生态草原 独库公路 寄蜉蝣于天地，渺沧海之一粟。\n——苏轼，《前赤壁赋》\n夜宿在赛里木湖北门附近的牧民家旁边，夜晚可以看到璀璨的星空。\n赛里木湖的星空 如果说有个地方我想再重走一遍的话，我会选择伊犁哈萨克自治州，如果一定要确定一个具体的村庄的话，那我会选择琼库什台。\n琼库什台 我们在草原上工作，到哈萨克族老乡家吃饭一起庆祝古尔邦节，教小朋友玩无人机，一起骑马、打篮球，不亦乐乎。大美新疆，我一定会再来的！\n人生处处知何似，恰似飞鸿踏雪泥。\n——苏轼，《和子由渑池怀旧》\n在琼库什台草原上骑马，这也是我第一次骑马，骑着骑着，马自己就跑起来了。\n琼库什台骑马 视频 下面是我为本次旅程所剪辑的视频 ，希望大家一键三连（点赞、投币、转发）。\nBilibili 点播 贴士 对于去气候适宜或稳定的地域的短途旅行，建议租普通的 SUV 即可，如果想睡在野外可以搭帐篷，没必要租房车，因为在房车体积较大，大部分人没有驾驶过房车的经验，开起来会比较吃力，而且房车对于路况要求也比较高，很多沟沟坎坎、野路就没办法通过了。\n","relpermalink":"/blog/xinjiang-trip/","summary":"记录我与 Addo 一行七天的房车自驾之旅。","title":"新疆北疆房车自驾之旅"},{"content":"这篇文章将带你了解使用 Kubernetes 和 Istio Service Mesh 构建多集群及混合云的过程和需要考虑的问题。\nKubernetes 使用 Kubernetes 可以快速部署一个分布式环境，实现了云的互操作性，统一了云上的控制平面。并提供了 Service、Ingress 和 Gateway 等资源对象来处理应用程序的流量。如下图所示，Kubernetes 中默认使用 Service 做服务注册和发现，服务之间可以使用服务名称来访问。Kubernetes API Server 与集群内的每个节点上的 kube-proxy 组件通信，为节点创建 iptables 规则，并将请求转发到其他 pod 上。\n假定现在客户端要访问 Kubernetes 中的服务，首先请求会发送到 Ingress/Gateway 上，然后根据 Ingress/Gateway 里的路由配置转发到后端服务上（图中是服务 A），接着服务 A 对服务 B 请求的流量转发轮询到服务 B 的实例上。\nKubernetes Kubernetes 多集群管理 多集群管理最常见的使用场景包括服务流量负载均衡、隔离开发和生产环境、解耦数据处理和数据存储、跨云备份和灾难恢复、灵活分配计算资源、跨区域服务的低延迟访问以及避免厂商锁定等。一个企业内部往往有多个 Kubernetes 集群，由 MultiCluster SIG 开发的 KubeFed 实现 Kubernetes 集群联邦可以实现多集群管理的功能，这使得所有 Kubernetes 集群都通过同一个接口来管理。\n在使用集群联邦时需要解决以下几个通用问题：\n配置需要联邦哪些集群 需要在集群中传播的 API 资源 配置 API 资源如何分配到不同的集群 对集群中 DNS 记录注册以实现跨集群的服务发现 下面是 KubeSphere 的多集群架构，也是最常用的一种 Kubernetes 多集群管理架构，其中 Host Cluster 作为控制平面，有两个成员集群，分别是 West 和 East。\nMulticluster Host 集群需要能够访问 Member 集群的 API Server，Member 集群之间的网络连通性没有要求。管理集群 Host Cluster 独立于其所管理的成员集群，Member Cluster 并不知道 Host Cluster 存在，这样做的好处是当控制平面发生故障时不会影响到成员集群，已经部署的负载仍然可以正常运行，不会受到影响。\nHost 集群同时承担着 API 入口的作用，由 Host Cluster 将对 Member 集群的资源请求转发到 Member 集群，这样做的目的是方便聚合，而且也利于做统一的权限认证。我们看到在 Host Cluster 中有联邦控制平面，其中的 Push Reconciler 会将联邦集群中身份、角色及角色绑定传播到所有成员集群中。\nIstio 当我们在 Kubernetes 中运行着多语言、多版本的微服务，并需要更细粒度的金丝雀发布和统一的安全策略管理，实现服务间的可观察性时，可以考虑使用 Istio 服务网格。Istio 通过向应用程序 Pod 中注入 sidecar proxy，缺省使用 IPTables 透明得拦截进出应用程序的所有流量，从而实现了应用层到集群中其他启用服务网格的服务的智能应用感知负载均衡，并绕过了初级的 kube-proxy 负载均衡。Istio 控制平面与 Kubernetes API Server 通信可以获取集群中所有注册的服务信息。\n下图展示了 Istio 的基本原理，其中所有节点属于同一个 Kubernetes 集群。\nIstio Service Mesh 你可能最终会有至少几个 Kubernetes 集群，每个集群都承载着微服务。Istio 的多集群部署根据网络隔离、主备情况存在多种部署模式 ，可以使用 Istio Operator 部署时通过声明来指定。集群中的这些微服务之间的通信可以通过服务网格来加强。在集群内部，Istio 提供通用的通信模式，以提高弹性、安全性和可观察性。\n以上都是关于 Kubernetes 上的应用负载管理，但是对于虚拟机上遗留应用，如何在同一个平面中管理？如何管理多集群中的流量划分、网关和安全性呢？\n管理平面 在 Istio 之上再增加一层抽象，将网关、流量和安全分组管理，并将它们应用到不同的集群和命名空间上。下图展示的是 Tetrate Service Bridge 的多租户模型，利用 NGAC 来管理用户的访问权限，同时也有利于构建零信任网络。\nManagement Plane Istio 提供了工作负载识别，并由强大的 mTLS 加密保护。这种零信任模型比基于源 IP 等拓扑信息来信任工作负载更好。在 Istio 之上构建一个多集群管理的通用控制平面，然后再增加一个管理平面来管理多集群，提供多租户、管理配置、可观察性等功能。\n下图展示的是 Tetrate Service Bridge 的架构图。\nTetrate Service Bridge 总结 使用 Kubernetes 实现了异构集群的互操作性，Istio 将容器化负载和虚拟机负载纳入到一个同一个控制平面内，统一管理集群内的流量、安全和可观察性。但是，随着集群数量、网络环境和用户权限的越发复杂，人们还需要在 Istio 的控制平面至上再构建一层管理平面来进行混合云管理。\n","relpermalink":"/blog/multicluster-management-with-kubernetes-and-istio/","summary":"这篇文章将带你了解使用 Kubernetes 和 Istio Service Mesh 构建多集群及混合云的过程和需要考虑的问题。","title":"服务网格之旅——使用 Kubernetes 和 Istio Service Mesh 构建混合云"},{"content":"Kubernetes 可以说是目前为止用来运行微服务的最佳载体，但是在调试 Kubernetes 环境中的微服务时的体验可能就没那么友好了。本文将带你了解如何调试 Kubernetes 中的微服务，介绍常用的工具，以及 Istio 的引入为微服务的调试带来的变革。\n调试微服务与传统单体应用有巨大的不同 微服务的调试是一直长期困扰软件开发人员的问题，这在传统的单体应用中不存在，因为开发者可以利用 IDE 中的调试器，为应用程序增加断点、修改环境变量，单步执行等，这些都为软件调试提供了巨大帮助。随着 Kubernetes 的流行，微服务的调试就成了一个棘手的问题，其中相比传统单体应用的调试多了以下问题：\n多依赖 一个微服务往往依赖多个其他微服务，在调试某个微服务时，如何部署其他依赖服务以快速搭建一套最新的 stagging 环境？\n从本地机器访问 微服务在开发者的本地电脑上运行时，通常无法直接访问到 Kubernetes 集群中的服务，如何像调试本地服务一样调试部署在 Kubernetes 集群中的微服务？\n开发效率低下 通常情况下，代码从更新到构建成镜像再推送到集群中需要一个漫长的过程，如何加快开发速度？\n我们一起来看下哪些工具能够解决以上问题。\n工具 调试 Kubernetes 中的微服务的主要解决方案有：\nProxy：在 Kubernetes 集群和本地调试终端中部署一个代理，通过构建一个 VPN，使得本地应用可以直接访问到 Kubernetes 中的服务； Sidecar：替换原来应用容器的镜像为开发镜像，可以在这个容器中中对该服务进行调试，同时在要调试的微服务 pod 中注入一个 sidecar 作为辅助工具来同步代码； 服务网格：要想了解应用的整体情况，就需要在所有微服务中注入 sidecar，这样你就可以获得一个监控全局状态的仪表板； 下面是实现以上解决方案的三个典型的开源项目，它们分别从不同的角度可以帮助你调试微服务。\nProxy 模式：Telepresence Telesprence 本质上是一个本地代理，该代理将 Kubernetes 集群中的数据卷、环境变量、网络都代理到了本地。下图展示的是 Teleprence 的主要使用场景。\nProxy 模式：Telepresence 用户需要在本地自主地执行 telepresence 命令，它会自动将代理部署到 Kubernetes 中，有了该代理之后：\n本地的服务就可以完整的访问到 Kubernetes 集群中的其他服务、环境变量、Secret、ConfigMap 等； 集群中的服务还能直接访问到本地暴露出来的端点； 但是这种方式仍然不够连贯，还需要用户在本地调试时运行多次命令，而且在某些网络环境下可能无法与 Kubernetes 集群建立 VPN 连接。\nSidecar 模式：Nocalhost Nocalhost 是一个基于 Kubernetes 的云端开发环境。要想使用它，你只需要在你的 IDE——VS Code 中安装一个插件即可扩展 Kubernetes，并缩短开发反馈周期。通过为不同的用户创建不同的 namespace，并使用 ServiceAccount 绑定到不同用户角身上时，就可以实现开发环境隔离。同时，Nocalhost 还提供了 Web 控制台和 API，方便管理员来管理不同的开发环境。\nSidecar 模式：Nocalhost 测试 参考 Nocalhost 文档 ，我们在 macOS 上安装 Nocalhost，并使用 Minikube 来演示如何调试。\n执行下面的命令安装 Nocalhost 客户端并查看 nhctl 命令行工具的版本。\nbrew install nocalhost/repo/nocalhost nhctl version 我们假设你机的 kubeconfig 文件位于 ~/.kube/config（若不在此位置需要在下面的命令中使用 --kubeconfig 手动指定）并拥有 Kubernetes 集群的 admin 角色，执行下面的命令使用 Helm3 在 Kubernetes 上安装 Nocalhost 服务端。\nnhctl init demo -n nocalhost 执行下面的命令启动 Minikube 隧道并查看 Nocalhost web 端地址。\nminikube tunnel kubectl get service nocalhost-web 在浏览器中访问 http://\u0026lt;EXTERNAL-IP\u0026gt; 即可，用户名/密码为：admin@admin.com/123456。\n要想在 VS Code 中使用，你还想需要创建一个 ServiceAccount 并绑定 admin 角色，然后将该 ServiceAccount 作为 Kubeconfig 文件导出。\nkubectl create serviceaccount my-service-account kubectl create rolebinding admin --clusterrole=admin --serviceaccount=default:my-service-account 只要你有一个 Kubernetes 集群，并有集群的 admin 权限，就可以参考 Nocalhost 的文档快速开始试用。在 VS Code 中使用 Nocalhost 插件时需要先为插件中配置 Kubernetes 集群。选择你刚导出的 Kubeconfig 文件或者直接复制文件中的内容粘贴到配置里。然后选择你需要测试的服务，并选择对应的 Dev Container，VS Code 会自动打开一个新的代码窗口。\n下面是以 Istio 官方提供的 bookinfo 示例 为例，你可以在本地 IDE 中打开克隆下来的代码，然后点击代码文件旁边的锤子即可进入开发模式。选择对应的 DevContainer，nocalhost 会自动向 pod 中注入一个开发容器 sidecar，并在终端中自动进入该容器，如下图所示。\nNocalhost VS code 界面 在开发模式中，本地修改代码，无需重新构建镜像，远端开发环境实时生效，这样可以极大的加快开发速度。同时，Nocalhost 还提供了服务端，可用于开发环境和用户权限进行管理，如下图所示。\nNocalhost web 端 Service Mesh 模式：Istio 以上使用 proxy 和 sidecar 的方式，一次只能对一个服务进行调试，如果想要掌握服务的全局状况，比如获取的服务的指标，以及通过分布式追踪了解服务的依赖和调用流程，对服务的性能进行调试。这些可观察性 的功能，需要为所有服务统一注入 sidecar 来实现。\n而且，当你的服务正处于从虚拟机迁移到 Kubernetes 的过程中时，使用 Istio 可以将虚拟机与 Kubernetes 纳入一个网络平面中（如下图所示），方便开发者调试和做渐进式的迁移。\nSerivce Mesh 模式：Istio 当然要获得这些好处也不是一点“代价”也不没有的，引入 Istio 后，你的 Kubernetes service 需要遵守 Istio 的命名规范 ，学习使用 Istioctl 命令行和日志的方式来调试微服务。\n使用 istioctl analyze 命令来调试集群中的微服务部署情况，可以使用 YAML 文件来检查某个命名空间或整个集群中的资源部署情况。 使用 istioctl proxy-config secret 来调试 service mesh 中的 pod 的 secret 被正确的加载并有效。 Istio 的配置信息在大型的集群部署中传播将会耗时更长并且可能有几秒钟的延迟时间，sidecar 的引入会给服务间调用带来一定延迟。\n总结 在应用微服务化和从虚拟机迁移到 Kubernetes 的过程中，开发者需要很多观念和习惯上的转变。通过 proxy 在本地跟 Kubernetes 间构建 VPN，可以方便开发者像调试本地服务一样调试 Kubernetes 中的服务。通过向 pod 中注入 sidecar，可以实现实时调试，加快开发进度。最后，Istio service mesh 真正实现了全局的可观察性，你还可以使用像 Tetrate Service Bridge 这样的工具来管理异构平台，帮助你渐渐地从单体应用过度到微服务。\n","relpermalink":"/blog/how-to-debug-microservices-in-kubernetes-with-proxy-sidecar-or-service-mesh/","summary":"本文讲解了调试 Kubernetes 中微服务的三种模式/工具，以及 Istio 的引入为微服务的调试带来的变革。","title":"如何调试 Kubernetes 中的微服务 ——proxy、sidecar 还是 service mesh？"},{"content":" 时间：2021 年 07 月 03 日（星期六） 地点：益州大道中段 1999 号银泰城写字楼 - 4 号楼 19F 亚马逊云科技 主持人：粟伟（云原生社区管委会成员，云原生社区成都站长） 报名方式：活动行 关于云原生社区 云原生社区是一个中立的云原生终端用户社区，致力于为开发者提供云原生领域的专业资讯，推动中国云原生产业发展，目标成为中国云原生领域最具影响力的开源社区。\n社区官网：https://cloudnative.to 加入成都站 关注云原生社区微信公众号，微信搜索（CloudNativeCN），在后台回复「成都站」即可加入成都站微信群。\n","relpermalink":"/notice/cloud-native-meetup-chengdu/","summary":"7 月 3 日，周六，成都见！","title":"云原生社区 Meetup 第五期成都站报名"},{"content":" 时间：6 月 23 日（星期三）晚 8 点 - 9 点 直播间：https://live.bilibili.com/23095515 主持人：宋净超（Tetrate） 嘉宾：陈鹏（百度） 话题：如何让 Istio 在大规模生产中落地 提问地址：腾讯文档 嘉宾简介 陈鹏，百度研发工程师，现就职于百度基础架构部云原生团队，主导和参与了服务网格在百度内部多个核心业务的大规模落地，对云原生、Service Mesh、Isito 等方向有深入的研究和实践经验。\n话题介绍 百度服务治理现状 \u0026amp; Istio 落地挑战 深入解读如何让 Isito 在大规模生产环境落地 实践经验总结 \u0026amp; 思考 通过本次分享你将了解当前 Isito 落地的困境和解决思路。\n","relpermalink":"/notice/istio-big-talk-ep4/","summary":"《Istio 大咖说》第 4 期，6 月 23 日，周三晚 8 点将在 B 站直播，欢迎收看。","title":"《Istio 大咖说》第 4 期节目预告"},{"content":"安全问题从互联网诞生之初就存在了，云原生因为涉及到高度分布式，安全问题更加严峻。云原生计算基金会（CNCF）特别就此发布了 Cloud Native Security Whitepaper v1.1，云原生社区将其翻译成了中文版，并合并到了官方仓库，见云原生安全白皮书中文版 。\n云原生安全白皮书（CNCF 出品，云原生社区译） 该白皮书旨在为组织以及技术领导者提供对云原生安全的清晰理解，及其如何在参与整个生命周期流程中使用和评估安全相关的最佳实践。云原生安全是一个多目标和多限制的复杂问题范畴，会跨越许多专业技术和实践领域。软件生命周期中 Day 1、Day 2 的绝大多数操作都会涉及到从身份管理到存储解决方案的安全技术或领域。然而，云原生安全所涵盖的内容远不止这些领域；它是个关于人的广义问题范畴，包含个体、团队和组织。它应该成为人和系统在深度使用甚至改造云原生应用技术过程的一种机理、流程和理念基础。\n","relpermalink":"/notice/cloud-native-security-whitepaper-zh/","summary":"由云原生社区组织翻译的 CNCF 出品的《云原生安全白皮书 v1.1》中文版发布。","title":"CNCF 云原生安全白皮书 v1.1 中文版发布"},{"content":" 时间：6 月 9 日晚 8 点 -9 点 直播间：https://live.bilibili.com/23095515 主持人：宋净超 嘉宾：杨笛航 话题：如何让 Istio 变得更为高效和智能 嘉宾简介\n杨笛航，Istio 社区成员，网易数帆架构师，负责网易轻舟 Service Mesh 配置管理，并主导 Slime 组件设计与研发，参与网易严选和网易传媒的 Service Mesh 建设。具有三年 Istio 控制面功能拓展和性能优化经验。\n话题介绍\nIstio 作为当前最火的 Service Mesh 框架，既有大厂背书，也有优秀的设计，及活跃的社区。但是随着 Mixer 组件的移除，我们无法通过扩展 mixer adapter 的方式实现高阶的流量管理功能，Istio 的接口扩展性成为亟待解决的问题。本次直播将分享本次分享将介绍网易自研的智能网格管理器 Slime，借助它，我们实现了配置懒加载，自适应限流，HTTP 插件管理等扩展功能，从而更为高效的使用 Istio。\n听众收益\n了解在实际业务中驾驭 Istio 框架的挑战 了解 Slime 的设计特点、技术路线及开源进展 了解网易解决 Service Mesh 架构成熟的经验 ","relpermalink":"/notice/istio-big-talk-ep3/","summary":"《Istio 大咖说》第 3 期，6 月 9 日，周三晚 8 点将在 B 站直播，欢迎收看。","title":"《Istio 大咖说》第 3 期节目预告"},{"content":"《Istio 大咖说》第 2 期今晚 8 点将在 B 站开播，欢迎收看。\n时间：2021 年 6 月 2 日晚 8 点 主持人：宋净超（Tetrate） 嘉宾：潘天颖（小电科技） 主题：从微服务架构到 Istio——架构升级实践分享 嘉宾介绍：小电科技工程师，云原生爱好者，Kubernetes contributor，Apache committer 直播间：https://live.bilibili.com/23095515 互动归档：见腾讯文档 详见：第 2 期：从微服务架构到 Istio—— 架构升级实践分享 。\n","relpermalink":"/notice/istio-big-talk-ep2/","summary":"《Istio 大咖说》第 2 期今晚 8 点将在 B 站开播，欢迎收看。","title":"《Istio 大咖说》第 2 期开播提醒"},{"content":" 5 月 30 日云原生社区将在珠海中安广场 A 栋 15 楼举办线下会议，欢迎大家踊跃报名（点击报名 ）！因为广州疫情防控要求，有两位讲师无法按计划来到现场参加，他们会通过直播参与。欢迎珠海的朋友们到现场参加活动。\n活动信息 时间：2020 年 5 月 30 日 14:00 - 17:05 地点：珠海市香洲区拱北夏湾中安广场 A 栋 15 楼 主办方：云原生社区 承办方：云原生社区珠海站 赞助商：红帽公司、亚马逊公司 场地赞助商：天承盛世（珠海）房地产代理有限公司、珠海市钧策商贸发展有限公司 合作伙伴：ServiceMesher、CNCF、Linux Foundation 活动流程 14:00 - 14:30 签到入场 14:30 - 14:35 主持人开场 14:35 - 15:15 漫步云端 —— 华发集团云平台建设经验分享 讲师：胡兆鹏\n个人介绍：华发集团高级经理，高级研发工程师；负责集团 PAAS 云平台运营管理，DevOps 流程构建和应用上云架构优化。\n15:15 - 15:55 云原生应用容器化与编排 讲师：关泽发\n个人介绍：红帽资深架构师。\n15:25 - 15:45 合影、茶歇 15:45 - 16:25 云原生时代下的应用原则 + 12-Factor 讲师：李俊杰\n个人介绍：AWS 解决方案架构师，负责基于 AWS 的云计算方案的咨询与架构设计，同时致力于容器方面研究和推广。\n16:25 - 17:05 企业云原生 PaaS 平台建设 讲师：王志德\n个人介绍：格力软件工程师。\n17:05 - 17:30 自由交流 请到活动行 上报名，或者扫描下面的二维码报名。\n加入云原生社区珠海站 在云原生社区 公众号后台回复「珠海站」即可获取站长联系方式，申请微信好友后即可加入云原生社区珠海站微信群。\n关于云原生社区城市站 为了方便云原生社区成员的线下交流以及后期本地站活动开展，云原生社区城市站计划在全球各地创建城市站长，站长负责本地活动组织，详见云原生社区城市站页面 。\n","relpermalink":"/notice/cloud-native-community-zhuhai/","summary":"5 月 30 日（周日）云原生社区珠海站聚会将按计划举行，欢迎到场参加。","title":"云原生社区珠海站聚会通知"},{"content":"今晚我第一次使用 Zoom + OBS 和马若飞在 B 站上进行了《Istio 大咖说》 栏目的第一期分享——「Istio 开源四周年回顾与展望」。考虑到很多社区、主播、调音台会有在 B 站或其他平台上直播的需求，特别将我的个人经验分享给大家，欢迎大家补充，我会不断优化直播体验。\n下图是我直播时桌面的情况，使用的设备有：\nMacBook Pro，这个自不必说 USB 麦克风博雅 BY-500，作为音频输入麦克风 海康威视外接摄像头，用作第二机位 iPad，作为直播监视器，同时回答观众的弹幕 环形补光灯，直播通常是在晚上，光线太暗需要补光 AirPods Pro，用于监听声音的，不作为音频输入 静音蓝牙键盘，防止键盘敲击声音影响直播的声音体验 iPhone，用于和直播讲师私下沟通，这样不会被直播出去 以上这些不是全部都需要的，只要你有一台电脑和一个耳机就可以直播。\n我的桌面 上面是直播时的桌面（请忽略我杂乱的被各种设备占满空间的桌面），下面是我的配置参考。\n硬件准备 电脑：macOS、Windows 都可以，我是用的是 Macbook Pro 2016 年产，配置如下： 系统配置 外接麦克风：切勿直接使用电脑机身上自带的麦克风，那样会收录电脑风扇的声音，我使用的是博雅 BY-500（400 多块钱）麦克风，电容式麦克风，指向性比较好，基本没有噪音。 摄像头：保证电脑上的摄像头可用，因为会议的时候需要开摄像头，或者用外接摄像头也可以。 网络：确保网速至少 100MB/s 的宽带，因为推流还是比较占用带宽的，而且还需要同时查看直播效果，对下行带宽也有要求。 另一台可联网设备：用来监控直播效果，可以是手机、iPad 等 软件准备 Zoom：需要 Pro 版，这样才可以举行超过 45 分钟的线上会议，否则会在超时后打断再重新加入，需要准备好账号，中国大陆用户貌似不能再注册？如果没有 zoom，换成其他任何一个会议软件都可以，比如腾讯会议。 OBS：用来做推流，到官网 下载最新的版本。 音频插件 Sunflower：点击跳转到下载页面 ，如果安装时遇到系统权限问题，请在命令行中执行 sudo spctl --master-disable 并在电脑的 系统首选项 的 安全与隐私 中批准来自任意途径的软件安装，如果看到有详情页面，点击进去批准软件发行商。 Bilibili：需要一个 B 站账号，并开通直播间，经过实名认证。 OBS 配置 下面是在 Macbook 中安装的 OBS 配置截图。\n需要注意的是输出、音频和视频的配置。请参考图中的配置，尤其注意编码控制、比特率的配置。\nOBS 输出配置 OBS 音频配置 请注意分辨率的配置，同时调整电脑屏幕的分辨率为 1440x900，不要使用太大的分辨率，否则可能导致直播画面黑屏。\nOBS 视频配置 Macbook 显示配置 会议直播 以上场景是仅限于本机画面的直播，还有中场景就是现场会议直播，你还需要录制现场画面，这时候你最好制作一个直播底板，例如下图。\nOBS 直播底板 该底板用于布局 PPT 和摄像头画面，同时底板上也包括了活动 logo 和主办方、赞助商信息。该图最好是 PNG 透明的格式，把图片放在布局最上层，这样就能很好地展示布局。\n音频配置 安装 sunflower 后，在 Midi 设备中创建一个多输出设备，如图。\nMacBook 音频配置 选择 Sunflower（2ch）和你想要用来监听系统声音的设备，我是用的是 AirPods，你也可以选择其他耳机，总之不要让麦克风录到这个系统输出的即可。\n还要在 OBS 的麦克风配置里增加下新创建的这个输出设备，这样直播的时候就可以收录你的系统，也就是你的耳机听到的声音了，比如在视频会议中，所有人讲话的声音都会被直播出去。\n使用独立音频硬件 如果安装 sunflower 有问题的话，你也可以购买一款独立的音频设备，要知道一台电脑是可以安装多块声卡的，这些声卡可以通过 USB 接口转接，而且价格都很便宜（只要几十块钱），一旦有了多个声卡，你就可以为不同的音频源选择不同的输出，而且可以对它们的音量进行单独单独控制。例如下面这款 USB 外置声卡（非利益相关），即插即用，不需要安装任何软件。\nUSB 外置声卡 你也可以用 USB 外置声卡来转接其他的无线麦克风，比如我就转接了 Rode Wireless Go（你还需要买一根转接的音频线，红色的那根，用于连接声卡）。\n多路推流（多渠道同步直播） 因为我们在直播时往往有多个渠道，比如多个 B 站直播间、微信视频号等，如何使用 OBS 同步推流到多个渠道呢？可以使用 sorayuki/obs-multi-rtmp 插件（支持 Windows 和 macOS），注意需要将 OBS 升级到最新版本（至少 27.0.1 版本）。安装完插件，重启 OBS 后就可以看到一个窗口新建多路推流，如下图所示。\nOBS 新建多路推流 如果没有看到该窗口，请点击【视图】-【停靠部件】-【多路推流】即可显示。\nOBS 多路推流选项 直播效果 下面是当晚直播的 zoom 录制的视频直出，已上传到 B 站 ，大家可以感受下画面的清晰度还有声音效果，我还是比较满意的。\nBilibili 其中只有几个小插曲：\n因为我是用的是 AirPods 蓝牙耳机，戴上耳机的时候我无法确定它要连那个设备（我有两个 iPhone、1 个 iPad、1 个 MacBook 都有可能被脸上）活动开始的时候总是连不上 MacBook，一气之下把其他的苹果设备的蓝牙全关掉，只留下 MacBook 的蓝牙开启，这样可以保证连上 MacBook 说话的时候忘记了把麦克风静音了 直播大概进行了 1 个小时的时候，zoom 突然断开了 10 几秒钟后又自动重连，总体来说 zoom 会议还是比较稳定的，1 个小时左右的会议应该不会断连 直播开始前的检查 电脑屏幕分辨率调整为 1440 x 900 关闭与直播无关的 APP，减少系统资源占用 电脑设置为勿扰模式 使用外接麦克风，切勿直接使用电脑内置的麦克风，会收录风扇及键盘杂音，影响音质 使用外接耳机，如 AirPods 音频输出调整为多设备输出，其中包括 Sunflower（2ch）和耳机 Zoom 会议开始前记得点击录像 会议开始后检查 B 站直播间，确保声音和画质没有问题 OBS 推流的时候不用录像，因为 zoom 已经在录了 直播完成后检查 zoom 生成的视频文件并备份 优化项 直接直播屏幕内容也会造成多重布局的问题，以上方案还是有可优化的地方。比如将视频会议中的共享桌面与摄像头画面分开布局到 OBS 上。\n最后 直播是除了在线下面对面交流以外，可以跟社区及开源爱好者交流最直接最友好的方式，我会时常发起，感谢大家的关注我主持的直播间：\n云原生社区 Istio Service Mesh 关注上面的 B 站账号，获取直播推送提醒。关于 B 站直播，如果你有任何问题或者建议请在下面留言。\n","relpermalink":"/blog/zoom-obs-bilibili-broadcast/","summary":"本文将指导你如何配置和使用 OBS + zoom 在 Bilibili 上直播。","title":"Zoom + OBS + B 站直播配置手册"},{"content":"Istio 是由 Tetrate 创始人 Varun Talwar 和谷歌首席工程师 Louis Ryan 命名并在 2017 年 5 月 24 日开源。今天是 Istio 开源四周年，让我们一起来回顾一下 Istio 四年来的发展并展望一下它的未来。\nIstio 的开源历史 2017 年是 Kubernetes 结束容器编排之战的一年，Google 为了巩固在云原生领域的优势，并弥补 Kubernetes 在服务间流量管理方面的劣势，趁势开源了 Istio。下面是截止目前 Istio 历史上最重要的几次版本发布。\n日期 版本 说明 2017-05-24 0.1 正式开源，该版本发布时仅一个命令行工具。确立了功能范围和 sidecar 部署模式，确立的 Envoy 作为默认 sidecar proxy 的地位。 2017-10-10 0.2 支持多运行时环境，如虚拟机。 2018-06-01 0.8 API 重构。 2018-07-31 1.0 生产就绪，此后 Istio 团队被大规模重组。 2019-03-19 1.1 企业就绪，支持多 Kubernetes 集群，性能优化。 2020-03-03 1.5 回归单体架构，支持 WebAssembly 扩展，使得 Istio 的生态更加强大。 2020-11-18 1.8 正式放弃 Mixer，进一步完善对虚拟机的支持。 Istio 开源后经过了一年时间的发展，在 1.0 发布的前两个月发布了 0.8 版本，这是对 API 的一次大规模重构。而在 2018 年 7 月底发布 1.0 时，Istio 达到了生产可用的临界点，此后 Google 对 Istio 团队进行了大规模重组，多家以 Istio 为基础的 Service Mesh 创业公司 诞生，可以说 2018 年是服务网格行业诞生的元年。\n2019 年 3 月 Istio 1.1 发布，而这距离 1.0 发布已经过去了近 9 个月，这已经远远超出一个开源项目的平均发布周期。我们知道迭代和进化速度是基础软件的核心竞争力，此后 Istio 开始以每个季度一个版本的固定发布节奏 ，并在 2019 年成为了 GitHub 增长最快的十大项目中排名第 4 名 ！\nIstio 社区 Istio 开源四年来，已经在 GitHub 上收获了 2.7 万颗星，获得了大量的社区用户 。下图是 Istio 的 GitHub star 数增长情况。\n2020 年 Istio 的项目管理开始走向成熟，治理方式也到了进化的阶段。2020 年，Istio 社区进行了第一次管委会选举 ，还把商标转让给了 Open Usage Commons 。首届 IstioCon 在 2021 年 2 月份成功举办，几千人参加了线上会议。在中国也有大量的 Istio 社区用户，2021 年也会有线下面对面的 Istio 社区 meetup 在中国举办。\n根据 CNCF 2020 年调查，46% 的组织在生产中使用服务网格或计划在未来 12 个月内使用。Istio 是在生产中使用的最多的网格。\n未来 经过 4 年的发展，围绕 Istio 不仅形成了庞大的用户群，还诞生了多家 Istio 供应商，你可以在最近改版的 Istio 的官网首页 中看到。在最近几个版本中，Istio 已经将发展中心转移到了提升 Day 2 Operation 体验上来了。我们还希望看到更多的 Istio 的采纳路径建议、案例研究、学习资料、培训及认证（例如来自 Tetrate 的业界的第一个 Istio 管理员认证 ），这些都将有利于 Istio 的推广和采用。\n","relpermalink":"/blog/istio-4-year-birthday/","summary":"今天是 Istio 开源四周年，让我们一起来回顾一下 Istio 四年来的发展并展望一下它的未来。","title":"Istio 开源四周年回顾与展望"},{"content":"继 4 月 17 日杭州站 meetup 之后，云原生社区的第四次线下 meetup 来了，不仅有来自大厂的云原生工程师现场交流，更有《Knative 实战》、《Kubernetes 实战》、《Quarkus 实战》、《云原生数据中台》、《Kubernetes 进阶实战》书籍等你来拿，5 月 22 日云原生社区将在 广州举行，欢迎大家踊跃报名，报名方式：活动行 ！\n本次活动的图书由机械工业出版社赞助，如下。\n赞助图书 访问 https://cloudnative.to/city/guangzhou/ 了解关于云原生社区广州站的更多信息，扫码关注社区公众号，回复「广州站」即可加入广州站微信群。\n主办方：云原生社区 赞助商：APISIX 承办方：云原生社区广州站 合作伙伴：CNCF、Linux Foundation、机械工业出版社 13:30 - 14:00 签到入场 14:00 - 14:05 主持人开场 14:05 - 14:25 开场致辞 讲师：宋净\n个人介绍：Tetrate 布道师、CNCF Ambassador、云原生社区 创始人、电子工业出版社优秀译者、出品人。Kubernetes、Istio 等技术的早期使用及推广者。曾就职于科大讯飞、TalkingData 和蚂蚁集团。\n14:25 - 15:05 死生之地不可不察：论 API 标准化对 Dapr 的重要性 讲师：敖小剑\n个人介绍：资深码农，十九年软件开发经验，微服务专家，Service Mesh 布道师，Servicemesher 社区联合创始人，Dapr Maintainer。专注于基础架构，Cloud Native 拥护者，敏捷实践者，坚守开发一线打磨匠艺的架构师。曾在亚信、爱立信、唯品会、蚂蚁金服等任职，对基础架构和微服务有过深入研究和实践。目前就职阿里云，在云原生应用平台全职从事 Dapr 开发。\n演讲概要\nDapr 作为新兴的云原生项目，以\u0026#34;应用运行时\u0026#34;之名致力于围绕云原生应用的各种分布式需求打造一个通用而可移植的抽象能力层。这个愿景有着令人兴奋而向往的美好前景：一个受到普通认可和遵循的云原生业界标准，基于此开发的云原生应用可以在不同的厂家的云上自由的部署和迁移，，恍惚间一派云原生下世界大同的美景。然而事情往往没这么简单，API 的标准化之路异常的艰辛而痛苦，Dapr 的分布式能力抽象在实践中会遇到各种挑战和困扰。\n听众收益：\n解 Dapr 的愿景和分布式能力抽象层的重要 了解 Dapr API 在抽象和实现时遇到的实际问题，尤其是取舍之间的艰难 了解目前 Dapr 在 API 抽象上正在进行的努力和新近准备增加的 API 15:05 - 15:45 有了 Nginx 和 Kong，为什么还需要 Apache APISIX？ 讲师：温铭\n个人介绍：Apache member，Apache APISIX PMC 主席，Apache SkyWalking committer，支流科技 CEO\n演讲概要\n在云原生时代，k8s 和微服务已经成为主流，在带来巨大生产力提升的同时，也增加了系统的复杂度。如何发布、管理和可视化服务，成为了一个重要的问题。 每次修改配置都要 reload 的 Nginx、依赖 postgres 才能工作的 Kong，都不是云原生时代的理想之选。 这正是我们创造 Apache APISIX 的原因：没有 reload、毫秒内全集群生效、不依赖数据库、极致性能、支持 Java 和 Go 开发插件。\n听众收益\n更好的理解 API 网关、服务网格，以及各个开源项目的优劣势\n15:45 - 16:05 合影、茶歇 16:05 - 16:45 云原生时代的研发效能 讲师：黄国峰\n个人介绍：腾讯 PCG 工程效能专家。10 多年的软件和互联网从业经验；现任腾讯工程效能部，负责持续集成、研发流程和构建系统等平台；曾任职唯品会高级经理，负责架构团队。在云原生平台下的研发效能方向有丰富的理论知识和实践经验。\n演讲概要\n云原生时代，软件研发的逻辑彻底改变了。传统的软件开发在本机编码/调试、部署到测试环境测试、再发布到生产环境；而云原生时代的开发，基于不可变设施，研发流程从编码、构建、持续测试、持续集成到持续部署，整个过程几乎完全代码化。\n听众收益\n了解云原生开发的新挑战和难点 了解腾讯云原生开发实践的流程和思路 了解腾讯云原生开发中的遇到的坑和解决思路 16:45 - 17:25 37 手游 Go 微服务架构演进和云原生实践 讲师：吴凌峰\n个人介绍：任职于三七互娱集团 37 手游技术部基础架构组，负责平台 golang 基础框架以及 DevOps、CI/CD 生态建设，从业以来一直专注于云原生、DevOps 和容器化等技术应用和推广，在 golang 工程化领域有一定的心得。\n演讲概要\nGolang 微服务应用和云原生的概念近年越来越火热，传统技术栈公司随着业务规模增长，在云原生技术应用落地探索和转型的过程中一定会遇到很多共通的问题以及有各自不同的思考，包括如何更好地提升我们的开发效率、提升服务稳定性、降低运维成本？面对不断增长的服务数量和不断变长变复杂的调用关系网，怎样才能更好地观测、管理和保证核心服务高可用，本次演讲分享将会围绕 37 手游转型为 Go 微服务架构以及建设云原生 DevOps 体系的历程、过程中的领悟和思考展开。\n听众收益\n了解 Golang 云原生微服务框架的关键技术和优化实践经验 了解云原生观测体系如链路追踪、监控等 Golang 微服务落地实践经验 了解混合云混合部署DevOps和CI/CD体系的企业实践经验 ","relpermalink":"/notice/cloud-native-meetup-guangzhou/","summary":"5 月 22 日，周六，广州见！","title":"云原生社区 Meetup 第四期广州站报名"},{"content":" Tetrate 的 Istio 认证管理员考试 企业正在增加对数字化转型的投资，并雇用合适的人才来加速这一旅程。根据 Linux 基金会发布的 2020 年开源工作报告 ，52% 的招聘经理更倾向于雇用有证书的人，而两年前只有 47%。不出所料 93% 的招聘经理表示难以找到足够的人才。Tetrate 今天宣布公开提供 Tetrate 认证 Istio 管理员（CIAT） 考试，该考试评估执行 Istio 服务网格安装和配置以及配置流量管理、弹性和故障注入的技能、知识和能力，以及使用 Istio 服务网格的安全功能。这是继 2 月份推出 Istio 基础知识的 免费培训和认证课程之后的又一举措。自那时起，已有 600 多名 IT 专业人士参加了培训。\n关于 Tetrate 的 Istio 认证管理员（CIAT） CIAT 是由 Tetrate 开发的，以帮助认证个人成为认证 Istio 管理员。该考试是一个在线、监考、基于性能的测试，由一组需要在命令行中解决的问题和一组多选题组成。考试目前基于在 Kubernetes 上运行的 Istio 1.9.1，考生有 2 小时时间完成任务。\nTetrate 的 Istio 认证管理员包括以下内容：\nIstio 基础课程 （培训和认证）：注册 CIAT 的用户会自动加入 Istio 基础课程，以帮助他们准备 Istio 管理员认证考试。然而，对于 CIAT 来说，这并不是必须要经历的。 CIAT 模拟考试 ：模拟考试旨在帮助用户在实际考试前熟悉考试环境。这是一个可选的步骤，不计入 CIAT 的最终得分。 CIAT 考试 ：这是用户必须通过的最后一次（或跳过上述步骤的唯一一次）考试，以获得 Tetrate 认证 Istio 管理员的资格。 谁适合参加？ 该认证适用于 Istio 服务网格管理员、运维和其他负责为 Kubernetes 内外运行的云原生工作负载配置流量路由、安全和其他服务网格功能的 IT 专业人士。该考试假定有基本的容器和 Kubernetes 知识，但不对其进行测试。候选人应该能够熟练使用：\nKubernetes CLI Istio CLI Linux 命令行 评价的范围 该认证项目测试用户在命令行环境中展示其能力的能力。考试测试的领域和能力包括：\nIstio 安装、升级和配置 流量管理 弹性和故障注入 确保工作负载的安全 Kubernetes 场景下的高级测试 经过认证的 Istio 管理员可以为运行在 Kubernetes 集群内外的工作负载安装和配置 Istio 资源。他们可以定义和配置 Istio 资源，控制流量路由、服务目的地、注入故障和使用弹性功能，以及确保工作负载的安全，并将外部和非 Kubernetes 工作负载引入现有的服务网格。\n现已上线 CIAT 考试 现在在 Tetrate 学院 上线，任何人都可以参加，费用为 299 美元。如果你想获得折扣，请与我联系 。\n","relpermalink":"/notice/certified-istio-administrator/","summary":"现已上线 Tetrate 学院，考试费 299 美元。","title":"Tetrate 推出业内首个 Istio 认证管理员考试"},{"content":"Istio 是当前最流行的服务网格实现 ，它是在 Kubernetes 的基础上开发的，它跟 Kubernetes 在云原生应用的生态中拥有着不同的定位。本文不是直接为你介绍 Istio 具有哪些功能，而是先向你介绍 Istio 诞生的历史条件，然后带你从 Kubernetes 与 Istio 的分工开始，了解什么是 Istio。\n要想解释什么是 Istio，还得先了解 Istio 是在什么样的情况下出现的——即为什么会有 Istio？\n容器作为云原生应用的交付物，既解决了环境一致性的问题，又可以更细粒度的限制应用资源，但是随着微服务和 DevOps 的流行，容器作为微服务的载体得以广泛应用。2014 年，Google 开源了 Kubernetes，随后几年得到迅猛发展，在 2017 年奠定了容器编排调度标准的地位。Kubernetes 作为一种容器编排调度工具，解决了分布式应用程序的部署和调度问题。因为一台单机的资源有限，而互联网应用可能因为用户规模的急速扩张，或用户属性的不同在不同时间段会出现流量洪峰，因此对计算资源的弹性要求比较高。而一台单机显然无法满足一个如何规模庞大的应用，反之，对于一个规模很小的应用也没必要占用整台主机，那将导致巨大的浪费。\n简而言之，Kubernetes 定义服务的最终状态，并使系统自动地达到和维持在该状态。那么在应用部署完成后，如何管理服务上的流量呢？下面我们将看下 Kubernetes 中如何做服务管理，及在 Istio 中的变化。\nKubernetes 中如何做服务管理？ 下图展示的是 Kubernetes 中的服务模型。\nKubernetes 服务模型 从上图中我们可以看出：\n同一个服务的的不同示例可能被调度到不同的节点上； Kubernetes 通过 Service 对象将一个服务的多个实例组合在了一起，统一对外服务； Kubernetes 在每个 node 中安装了 kube-proxy 组件来转发流量，它拥有的简单的负载均衡功能； Kubernetes 集群外部流量可以通过 Ingress 进入集群中（Kubernetes 还有其他几种暴露服务的方式，如 NodePort、LoadBalancer 等）； Kubernetes 是用于资源集约管理的工具。但在为应用分配好资源后，如何保证应用的健壮性、冗余性，如何实现更细粒度的流量划分（不是根据服务中实例个数来实现），如何保障服务的安全性，如何进行多集群管理等，这些问题 Kubernetes 都不能很好地解决。\n服务具有多个版本，需要迭代和上线，在新版发布的时候需要切分流量，实现金丝雀发布；同时我们应该假定服务是不可靠的，可能因为各种原因导致请求失败，需要面向失败来编程，如何监控应用程序的指标，了解每个请求的耗时和状态？Istio 的发起这们就想到了在每个 pod 中注入一个代理，将代理的配置通过一个控制平面集中分发，然后将从 pod 中应用容器发起的每个请求都劫持到 sidecar 代理中，然后转发，这样不就可以完美的解决以上问题了吗？Kubernetes 优秀的架构和可扩展性，例如 CRD，pod 内的部署模式，可以完美的解决大量 sidecar 的注入和管理问题，使得 Istio 的实现成为可能。\nIstio 的基本原理 下图是 Istio 中的服务模型，它既可以支持 Kubernetes 中的工作负载，又可以支持虚拟机。\nIstio 从图中我们可以看出：\nIstiod 作为控制平面，将配置下发给所有的 sidecar proxy 和 gateway（为了美观，图中没有画 Istiod 及 sidecar 之间的连接） Istio 不再使用 kube-proxy 组件做流量转发，而是依托在每个 pod 中注入的 sidecar proxy，所有的 proxy 组成了 Istio 的数据平面； 应用程序管理员可以和管理 Kubernetes 中的工作负载一样，通过声明式 API 操作 Istio mesh 中流量的行为； Ingress 被 Gateway 资源所替代，Gateway 是一种特殊的 proxy，实际上也是复用的 Sidecar proxy； 可以在虚拟机中安装 sidecar proxy，将虚拟机引入的 Istio mesh 中； 实际上在 Istio 之前，人们可以使用 SpringCloud、Netflix OSS 等，通过在应用程序中集成 SDK，编程的方式来管理应用程序中的流量。但是这通常会有编程语言限制，而且在 SDK 升级的时候，需要修改代码并重新上线应用，会增大人力负担。Istio 使得流量管理变得对应用程序透明，使这部分功能从应用程序中转移到了平台层，成为了云原生基础设施。\n正是因为 Istio 补足了 Kubernetes 对于云原生应用的流量管理、可观察性和安全方面的短板，在 2017 年由 Google、IBM 和 Lyft 共同发起的这个服务网格开源项目，并在三年来取得了长足的发展。关于 Istio 核心功能的介绍可以参考 Istio 文档 。\n总结 Service Mesh 相当于云原生时代的 TCP/IP，解决应用程序网络通信、安全及可见性问题； Istio 是目前最流行的 service mesh 实现，依托于 Kubernetes，但也可以扩展到虚拟机负载； Istio 的核心由控制平面和数据平面组成，Envoy 是默认的数据平面代理； Istio 作为云原生基础设施的网络层，对应用透明。 ","relpermalink":"/blog/what-is-istio-and-why-does-kubernetes-need-it/","summary":"本文将解释 Istio 是如何产生的，以及它与 Kubernetes 的关系。","title":"什么是 Istio？为什么 Kubernetes 需要 Istio？"},{"content":"很高兴的通知大家，《Kubernetes Handbook——Kubernetes 中文指南/云原生架构实践手册》四周年纪念版发布了，你可以在线浏览 或者下载 PDF 。\nKubernetes Handbook——Kubernetes 中文指南/云原生架构实践手册 by Jimmy Song 封面 感想 最近有两条新闻在朋友圈里刷屏：\n《Mesos 已死，容器永生》 《Kubernetes 1.21: Power to the Community》 当着两篇看似不相关的新闻同时映入我的眼帘，让我感觉恍如隔世。我曾在 2017 年的分享过一篇文章——《Kubernetes 与云原生 2017 年年终总结及 2018 年展望》 ，其中就已经断定 Kubernetes 的大局已定，最终连 Docker 都有可能黯然收场。蓦然回首，四年时间如白驹过隙，Kubernetes 已经在 PaaS 层占据了不可撼动的地位，社区里甚至高呼其为“云原生操作系统”。2019 年后实质上就已经进入了 Kubernetes 次世代 ，一个迈向全面云原生的新世代开启。\n还记得四年前，2017 年 3 月，我开源了 《Kubernetes Handbook——Kubernetes 中文指南/云原生架构实践手册》 ，从开始的 Kubernetes 安装笔记到理论文档，再到工程实践，不断的丰富完善，其间有上百人 参与了本书的工作中，丰富和完善了本分内容，并形成了一个圈子，在其中不断交流碰撞着思维的火花，如今已形成了一个近万人规模的云原生社区 。\n为什么发布四周年纪念版 本书的上个 PDF 版本发布距今已经有两年多时间了，市面上已经流传了关于本书的无数种 PDF 版本，为了防止个别别有用心的人自篡改和传播未经核实的版本，使学习 Kubernetes 和云原生的同学误解，特此编译该四周年纪念版以正视听。\n本书从诞生之初就一直使用 GitBook 来管理，发布在 GitHub Pages 上，社区里时不时会有人问我为什么编译成 PDF 时总出错？本书使用的 GitBook 版本过旧（CLI version: 2.3.2，GitBook version: 3.2.2），官方已经不再维护，且编译时对电脑的要求以及环境配置要求较高，我还特别制作了一个 Docker 镜像用来编译，但是编译依然是一个十分耗时的事情，在我的个人电脑上耗时近 5 分钟。但是因为旧版 GitBook 比较稳定，且可以在本地管理文章和编译发布到 GitHub，而且我已经配置了 GitHub Action，每次有 PR 合并会自动发布到网站上，因此将会继续沿用下去。\n后续 该版本中删除了一些过时章节和与书籍内容关系不大的附录，并重新组织了章节，包括新增服务网格、社区与生态章节，还修复了书中的一些错误。本书会继续通过开源协作的方式来撰写，未来还会在最佳实践、领域应用等章节，跟社区取长补短，汇聚众家之长，也希望广大读者可以在云原生社区中积极投稿 ，将云原生架构的最佳实践共同分享给读者。\n","relpermalink":"/notice/kubernetes-handbook-4-anniversary-edition/","summary":"该版本中删除了一些过时章节和与书籍内容关系不大的附录，并重新组织了章节，包括新增服务网格、社区与生态章节，还修复了书中的一些错误。","title":"Kubernetes Handbook 四周年纪念版发布"},{"content":"在刚刚过去的清明节，我去了趟苏州（视频 ）和南京（视频 ），还在南京跟南京站的朋友们吃了饭，聊了下南京站的发展。\n南京站 下面是我在南京跟云原生社区南京站 成员的合影。\n云原生社区南京站 这周六（4 月 10 日）云原生社区南京站会有一次线下聚会，欢迎大家报名参加，点击报名南京站活动 ，本次会议主要内容为交流云原生落地经验。\n长沙站 在周日（4 月 11 日）云原生社区长沙站会有第一次线下聚会暨长沙站筹办会议，也欢迎大家报名参加，点击报名长沙站聚会 。\n关于云原生社区城市站 为了方便云原生社区成员的线下交流以及后期本地站活动开展，云原生社区城市站计划在全球各地创建城市站长，站长负责本地活动组织，详见云原生社区城市站页面 。\n","relpermalink":"/notice/cloud-naitve-community-nanjing-changsha/","summary":"周末南京站、长沙站聚会通知。","title":"云原生社区南京站及长沙站活动提醒"},{"content":" Tetrate 我们很高兴地宣布 Tetrate Service Bridge 1.0（General Available）发布。Tetrate 的使命是解决应用网络的复杂性，使应用开发者和运维的工作变得简单。今天是我们发展道路上的一个重要里程碑。\n初心 Tetrate 成立的初心是应用网络应与计算无关。我们相信，这是任何组织在构建和交付应用时实现敏捷性和速度的关键。这与当今的应用越来越相关，这些应用在多个云和内部基础设施的异构计算环境（如 Kubernetes 和虚拟机）上运行。如果不加以管理，这种蔓延会导致难以承受的运营成本和复杂性。企业需要一种一致的方式来配置、保护和观察他们的应用，并保持系统的弹性以满足 SLO。\nTetrate Service Bridge 使用一流的开源项目：Istio 、Envoy Proxy 和 Apache SkyWalking （Tetrate 积极参与并维护这些项目），我们开始建立原型，以解决应用网络的挑战。我们与《财富》500 强企业密切合作，并得到他们出色的团队支持，以验证我们的想法和产品。他们一直并将继续帮助我们构建产品，他们所提供的在快速变化的环境中的真实用例为我们的产品构建发挥了重要作用。我们建立了一个平台，以帮助组织在多云和多集群环境的异构计算中管理其应用网络。这个平台能够满足企业的实际需求，使他们能够按照自己的节奏安全地、渐进地进行现代化和 / 或迁移。\n我们将该平台命名为 Tetrate Service Bridge（TSB）。桥梁提供了一种连接方式，而这正是我们所实现的。TSB 是一座桥梁：\n以实现企业应用的现代化。 从任何地方连接任何服务。 将不同的团队聚集在一起，管理和运维应用服务。 从边缘到工作负载的网络 Tetrate Service Bridge 的核心是提供一个单一的管理平面，以便在构成应用网络的三个层面上配置连接性、安全性和可靠性。\n应用程序边缘网关（Application Edge Gateway），作为传入流量的入口点，然后将流量分配到多个集群。 应用程序入口网关（Application Ingress Gateway），作为进入单个集群的流量的入口点，并将流量分配给在该集群中运行的一个或多个工作负载。这还包括 API 网关功能，如认证、授权、速率限制等。 服务网格 Sidecar，作为代理，允许工作负载之间的连接，控制服务访问，并收集指标和追踪，以提供全面的可观察性。 Tetrate Service Brdige 示意图 Tetrate Service Bridge 位于应用边缘、集群入口和集群内，可在 Kubernetes 集群和传统计算之间路由和负载平衡南北流量，并连接集群内的东西向工作负载。\n一个适合多个团队的平台 在任何组织中，多个团队一起构建和交付应用程序，具体来说是：应用程序、平台和安全团队。这些团队中的每一个团队都会有不同的关注点和期望。\n平台团队希望在其基础设施中提供多集群服务网格，并对其进行维护。 应用开发团队希望配置、观察和排除他们的应用程序和 API 的故障。 安全团队希望对用户和服务访问持续应用安全策略和工作流程。 我们建立了 Tetrate Service Bridge，以满足这些团队的要求，加速组织的成功。\n提供和维护多集群服务网格 多集群、多云、混合云 通过 Tetrate Service Bridge，您可以跨集群、云和数据中心连接和管理应用程序。Tetrate Service Bridge 支持来自主要云供应商的任何 Kubernetes 上游合规发行版。这包括但不限于 Google Kubernetes Engine（GKE）、Amazon Elastic Kubernetes Service（EKS）、Azure Kubernetes Service（AKS）、Openshift 和 Mirantis Kubernetes Engine（MKE）。Tetrate Service Bridge 还支持将虚拟机（VM）和裸机工作负载接入网格。\n多集群管理 集群生命周期管理 Tetrate Service Bridge 提供了所有集群中运行的 Istio 和 Envoy 的可视性，包括部署的版本、每个集群的配置状态等信息。此外，您还可以使用 Tetrate Service Bridge 管理网格部署的完整清单，并放心地逐步升级，而不必担心停机。\n配置和观察应用程序和 API 始终如一的安全体验 Tetrate Service Bridge 的管理平面为平台所有者和应用开发者提供了一致的体验，以控制任何环境中任何应用的连接性、安全性和可观察性。\nTetrate Service Bridge 还提供了更安全的配置模型，允许应用团队编写和验证 Istio 配置，通过构造确保正确性。服务级隔离和组织控制保证了只有正确的网格配置才能在运行时到达您的应用。\n单一视图的可观察性 Tetrate Service Bridge 可收集、存储和汇总来自多个集群和环境的指标、追踪和代理日志，并提供单一界面查看服务的拓扑结构及其依赖关系，以便一目了然地了解应用健康状况。\n监控视图 扩展网格功能 Tetrate Service Bridge 为认证和授权、可观察性或通过 WASM 扩展对请求属性进行操作提供了一个可扩展点。这允许应用团队添加可根据组织特定需求定制的功能。\n确保网格的安全 多租户 利用 Tetrate Service Bridge，您可以为您的团队和企业内部的工作空间创建租户，以定义细粒度的访问控制、编辑权限，并应用零信任标准。Tetrate Service Bridge 符合 NIST 微服务安全标准（SP800-204a 和 - 204b，由 Tetrate 工程师与 NIST 共同编写）和下一代访问控制（NGAC）的实现。\n安全策略 Tetrate Service Bridge 允许您在网格中一致地应用安全策略，而不需应用开发人员操心。Tetrate Service Bridge 符合 NIST 微服务安全标准，并实现零信任。\n下一步 要了解有关 Tetrate Service Bridge 的更多信息，请访问我们的页面 ，并注册参加即将举行的网络研讨会，即 TSB 介绍：连接和保护您的应用程序，无论它们在哪里运行 。 联系我们 ，获取 TSB 演示，了解 TSB 如何帮助您管理应用网络的复杂性。通往现代化的道路并不简单，但我们已准备好铺设桥梁，并为您提供每一步支持。\n","relpermalink":"/notice/tetrate-service-bridge-ga/","summary":"Tetrate Service Bridge（TSB）正式可用！","title":"Tetrate Service Bridge 1.0 发布"},{"content":"如果你听说过服务网格，并尝试过 Istio ，你可能有以下问题。\n为什么 Istio 要在 Kubernetes 上运行？ Kubernetes 和服务网格在云原生应用架构中分别扮演什么角色？ Istio 扩展了 Kubernetes 的哪些方面？它解决了哪些问题？ Kubernetes、Envoy 和 Istio 之间是什么关系？ 本文将带大家了解 Kubernetes 和 Istio 的内部工作原理。此外，我会介绍 Kubernetes 中的负载均衡方法，并解释为什么有了 Kubernetes 后还需要 Istio。\nKubernetes 本质上是通过声明式配置来实现应用生命周期管理，而服务网格本质上是提供应用间的流量、安全管理和可观察性。如果你已经使用 Kubernetes 搭建了一个稳定的应用平台，那么如何设置服务间调用的负载均衡和流量控制？是否有这样一个通用的工具或者说平台（非 SDK），可以实现？这就需要用到服务网格了。\nEnvoy 引入了 xDS 协议，这个协议得到了各种开源软件的支持，比如 Istio、MOSN 等。Envoy 将 xDS 贡献给服务网格或云原生基础设施。Envoy 本质上是一个现代版的代理，可以通过 API 进行配置，在此基础上衍生出许多不同的使用场景–比如 API Gateway、服务网格中的 sidecar 代理和边缘代理。\n本文包含以下内容。\nkube-proxy 的作用描述。 Kubernetes 在微服务管理方面的局限性。 Istio 服务网格的功能介绍。 Kubernetes、Envoy 和 Istio 服务网格中一些概念的比较。 Kubernetes vs Service Mesh 下图显示了 Kubernetes 中的服务访问关系和服务网格（每个 pod 模型一个 sidecar）。\nKubernetes vs Service Mesh 流量转发 Kubernetes 集群中的每个节点都部署了一个 kube-proxy 组件，该组件与 Kubernetes API Server 进行通信，获取集群中的服务信息，然后设置 iptables 规则，将服务请求直接发送到对应的 Endpoint（属于同一组服务的 pod）。\n服务发现 Service Discovery Istio 可以跟踪 Kubernetes 中的服务注册，也可以在控制平面中通过平台适配器与其他服务发现系统对接；然后生成数据平面的配置（使用 CRD，这些配置存储在 etcd 中），数据平面的透明代理。数据平面的透明代理以 sidecar 容器的形式部署在每个应用服务的 pod 中，这些代理都需要请求控制平面同步代理配置。代理之所以“透明”，是因为应用容器完全不知道代理的存在。过程中的 kube-proxy 组件也需要拦截流量，只不过 kube-proxy 拦截的是进出 Kubernetes 节点的流量–而 sidecar 代理拦截的是进出 pod 的流量。\n服务网格的劣势 由于 Kubernetes 的每个节点上都运行着很多 pod，所以在每个 pod 中放入原有的 kube-proxy 路由转发功能，会增加响应延迟–由于 sidecar 拦截流量时跳数更多，消耗更多的资源。为了对流量进行精细化管理，将增加一系列新的抽象功能。这将进一步增加用户的学习成本，但随着技术的普及，这种情况会慢慢得到缓解。\n服务网格的优势 kube-proxy 的设置是全局的，无法对每个服务进行细粒度的控制，而 service mesh 通过 sidecar proxy 的方式将 Kubernetes 中的流量控制从服务层中抽离出来–可以实现更大的弹性。\nKube-proxy 的不足之处 首先，如果转发的 pod 不能正常服务，它不会自动尝试其他 pod。每个 pod 都有一个健康检查机制，当一个 pod 出现健康问题时，kubelet 会重启 pod，kube-proxy 会删除相应的转发规则。另外，节点 Port 类型的服务不能添加 TLS 或更复杂的消息路由机制。\nKube-proxy 实现了一个 Kubernetes 服务的多个 pod 实例之间的流量负载均衡，但如何对这些服务之间的流量进行精细化控制–比如将流量按百分比划分给不同的应用版本（这些应用版本都是同一个服务的一部分，但在不同的部署上），或者做金丝雀发布（灰度发布）和蓝绿发布？\nKubernetes 社区给出了一个使用 Deployment 做金丝雀发布 的方法，本质上是通过修改 pod 的标签来给部署的服务分配不同的 pod。\nKubernetes Ingress vs Istio Gateway 如上所述，kube-proxy 只能在 Kubernetes 集群内路由流量。Kubernetes 集群的 pod 位于 CNI 创建的网络中。Ingress 是在 Kubernetes 中创建的资源对象，用于集群外部的通信。它由位于 Kubernetes 边缘节点上的入口控制器驱动，负责管理南北向流量。Ingress 必须与各种 Ingress 控制器对接，比如 nginx ingress 控制器 和 traefik 。Ingress 只适用于 HTTP 流量，使用简单。它只能通过匹配有限的字段来路由流量——如服务、端口、HTTP 路径等。这使得它无法对 TCP 流量进行路由，如 MySQL、Redis 和各种 RPC。这就是为什么你会看到人们在 ingress 资源注释中写 Nginx 配置语言的原因（注：使用 Nginx Ingress Controller 可以通过 配置 ConfigMap 和 Service 的方式 来变通支持 TCP 和 UDP 流量转发）。直接路由南北流量的唯一通行方法是使用服务的 LoadBalancer 或 NodePort，前者需要云厂商支持，后者需要额外的端口管理。\nIstio Gateway 的功能与 Kubernetes Ingress 类似，它负责进出集群的南北流量。Istio Gateway 描述了一个负载均衡器，用于承载进出服务网格边缘的连接。该规范描述了一组开放端口和这些端口所使用的协议，以及用于负载均衡的 SNI 配置等。Gateway 是一个 CRD 扩展，它也重用了 sidecar 代理的功能；详细配置请参见 Istio 网站 。\nEnvoy Envoy 是 Istio 中默认的 sidecar 代理。Istio 基于 Enovy 的 xDS 协议扩展了其控制平面。在讨论 Envoy 的 xDS 协议之前，我们需要先熟悉 Envoy 的基本术语。下面是 Envoy 的架构图。\nEnvoy 架构图 基础概念 以下是 Enovy 中你应该知道的基本术语。\n下游。下游主机连接到 Envoy，发送请求，并接收响应，即发送请求的主机。 上游：上游主机。上游主机接收来自 Envoy 的连接和请求，并返回响应；即接收请求的主机。 Listener：监听器。监听器是一个命名的网络地址（如端口、UNIX 域套接字等）；下游客户端可以连接到这些监听器。Envoy 将一个或多个监听器暴露给下游主机进行连接。 集群。集群是一组逻辑上相同的上游主机，Envoy 连接到它们。Envoy 通过服务发现来发现集群的成员。可以选择通过主动的健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略来决定集群中哪个成员的请求路由。 在 Envoy 中可以设置多个监听器，每个监听器可以设置一个过滤链（过滤链表），而且过滤链是可扩展的，这样我们可以更方便地操纵流量的行为–比如设置加密、私有 RPC 等。\nxDS 协议是由 Envoy 提出的，是 Istio 中默认的 sidecar 代理，但只要实现了 xDS 协议，理论上也可以作为 Istio 中的 sidecar 代理 —— 比如蚂蚁集团开源的 MOSN 。\nimg Istio 是一个功能非常丰富的服务网格，包括以下功能。\n流量管理。这是 Istio 最基本的功能。 策略控制。实现访问控制系统、遥测采集、配额管理、计费等功能。 可观察性。在 sidecar 代理中实现。 安全认证。由 Citadel 组件进行密钥和证书管理。 Istio 中的流量管理 Istio 中定义了以下 CRD 来帮助用户进行流量管理。\n网关。网关描述了一个运行在网络边缘的负载均衡器，用于接收传入或传出的 HTTP/TCP 连接。 虚拟服务（VirtualService）。VirtualService 实际上是将 Kubernetes 服务连接到 Istio 网关。它还可以执行额外的操作，例如定义一组流量路由规则，以便在主机寻址时应用。 DestinationRule。DestinationRule 定义的策略决定了流量被路由后的访问策略。简单来说，它定义了流量的路由方式。其中，这些策略可以定义为负载均衡配置、连接池大小和外部检测（用于识别和驱逐负载均衡池中不健康的主机）配置。 EnvoyFilter。EnvoyFilter 对象描述了代理服务的过滤器，可以自定义 Istio Pilot 生成的代理配置。这种配置一般很少被主用户使用。 ServiceEntry。默认情况下，Istio 服务 Mesh 中的服务无法发现 Mesh 之外的服务。ServiceEntry 可以在 Istio 内部的服务注册表中添加额外的条目，从而允许 Mesh 中自动发现的服务访问并路由到这些手动添加的服务。 Kubernetes vs xDS vs Istio 在回顾了 Kubernetes 的 kube-proxy 组件、xDS 和 Istio 对流量管理的抽象后，现在我们仅从流量管理的角度来看看这三个组件 / 协议的比较（注意，三者并不完全等同）。\nKubernetes xDS Istio service mesh Endpoint Endpoint WorkloadEntry Service Route VirtualService kube-proxy Route DestinationRule kube-proxy Listener EnvoyFilter Ingress Listener Gateway Service Cluster ServiceEntry 核心观点 Kubernetes 的本质是应用生命周期管理，具体来说就是部署和管理（伸缩、自动恢复、发布）。 Kubernetes 为微服务提供了一个可扩展、高弹性的部署和管理平台。 服务网格是基于透明代理，通过 sidecar 代理拦截服务之间的流量，然后通过控制平面配置管理它们的行为。 服务网格将流量管理与 Kubernetes 解耦，不需要 kube-proxy 组件来支持服务网格内的流量；通过提供更接近微服务应用层的抽象来管理服务间的流量、安全性和可观察性。 xDS 是服务网格的协议标准之一。 服务网格是 Kubernetes 中服务的一个更高层次的抽象。 总结 如果说 Kubernetes 管理的对象是一个 pod，那么服务网格管理的对象就是一个服务，所以用 Kubernetes 管理微服务，然后应用服务网格就可以了。如果你连服务都不想管理，那就用 Knative 这样的无服务器平台，不过这是后话。\n","relpermalink":"/blog/why-do-you-need-istio-when-you-already-have-kubernetes/","summary":"本文将带大家了解 Kubernetes 和 Istio 的内部工作原理。此外，我会介绍 Kubernetes 中的负载均衡方法，并解释为什么有了 Kubernetes 后可能还需要 Istio。","title":"为什么在使用了 Kubernetes 后你可能还需要 Istio？"},{"content":"继去年 12 月 20 日北京站 meetup 之后，云原生社区的第三次线下 meetup 来了，不仅有来自大厂的云原生工程师现场交流，更有《混沌工程：NETFLIX 系统稳定性之道》,《云原生服务网格 istio》书籍等你来拿，4 月 17 日杭州站将在西湖区蒋村街道中节能・西溪首座 A1-213 如期举办，欢迎大家踊跃报名！\n报名方式：活动行 云原生社区 meetup 第三期杭州站 本次活动海报模板设计由 UCloud 赞助。\n开场致辞 讲师：宋净超（Tetrate 布道师、云原生社区创始人）\n讲师介绍：Tetrate 云原生布道师，云原生社区创始人，CNCF Ambassador，作家和摄影师，热衷于开源和分享。\n使用 Chaos Mesh 来保障云原生系统的健壮性 讲师：周强\n公司：PingCAP\n讲师介绍：周强，PingCAP 工程效率负责人，Chaos Mesh 负责人，专注稳定性和性能测试平台。在混沌工程领域有 4 年的从业经验，领导开发云原生混沌测试平台 Chaos Mesh。\n演讲概要：\n在云原生的世界中，错误无处不在，混沌工程在提高系统稳定性方面起着至关重要的作用。通过执行混沌工程实验，我们可以了解系统的弱点并主动解决。我们开发了云原生混沌工程平台 Chaos Mesh，并在内部使用 Chaos Mesh 来提升云原生分布式数据库 TiDB 的健壮性。目前 Chaos Mesh 已加入 CNCF Sandbox 项目，该平台依托于 k8s 基础设施，通过对 pod/container 进行诸如杀节点、IO 错误和延时注入、时间回退、内核分配内存失败等等来进行混沌测试。主题大纲：\n在分布式领域会遇到的质量和稳定性问题 混沌工程在提升系统稳定性方面的作用和意义 Chaos Mesh 项目简介 混沌工程主要的测试方式和使用案例 混沌工程平台的构建实践 听众收益：\n了解在构建分布式系统可能出现的问题和风险 了解混沌工程的使用经验和踩过的坑，观众后续可以通过混沌工程来进行相关实践，提升产品质量 通过 case study 可以帮助大家构建分布式混沌测试平台 Envoy 在轻舟微服务中落地实践 讲师：王佰平\n公司：网易\n讲师介绍：网易数帆资深工程师，负责轻舟 Envoy 网关与轻舟 Service Mesh 数据面开发、功能增强、性能优化等工作。对于 Envoy 数据面开发、增强、落地具有较为丰富的经验。\n演讲概要：\nEnvoy 是由 Lyft 开源的高性能数据和服务代理，以其可观察性和高扩展性著称。如何充分利用 Envoy 的特性，为业务构建灵活易扩展、稳定高性能的基础设施（服务网格、API 网关）是 Envoy 落地生产实践必须考虑的问题。本次分享主要介绍 Envoy 在轻舟微服务网关与轻舟微服务网格中落地实践经验和构建此类基础设施时轻舟关注的核心问题。希望能够给大家带来一些帮助。\n听众收益：\n了解 Envoy 本身架构与关键特性； 在生产实践当中微服务网关与服务网格关注的核心问题以及轻舟 Envoy 的解决之道； 为期望实现集群流量全方位治理和观察的听众提供些许借鉴。 KubeVela：阿里巴巴新一代应用交付管理系统实践 讲师：孙健波\n公司：阿里巴巴\n讲师介绍：孙健波 (花名：天元) 阿里云技术专家，云原生应用模型 OAM (Open Application Model) 核心成员和主要制定者，KubeVela 项目作者，致力于推动云原生应用标准化，负责大规模云原生应用交付与应用管理相关工作。曾参与编写《Docker 容器与容器云》技术书籍。\n演讲概要：\n云原生应用交付面临的问题与挑战 社区中常见的应用交付解决方案对比 基于 KubeVela 的标准化应用交付管理核心原理 阿里巴巴基于 KubeVela 的应用交付实践 听众收益：\n随着“云原生”的普及，基础设施逐渐成熟的今天，越来越多的应用开发者们开始追求快速的构建交付应用。然而使用场景的不同往往意味着应用交付的环境会有巨大的差异。就比如，K8s 中不同的工作负载类型需要对接不同的灰度发布、流量管理等实现方案，不同的部署环境（公有云、私有化部署等）也常常需要对接不同的日志监控体系。如何才能将应用管理和交付变得标准化，使得应用研发不再需要花大量精力对接不同的交付平台？这已经逐渐成为云原生应用管理领域的一大痛点。本次分享将针对这些问题为大家介绍如何基于 KubeVela 构建标准化的应用交付管理平台，介绍阿里巴巴在此基础上的实践经验。\nEnvoy 在阿里巴巴内部的落地实践 讲师：张义飞\n公司：阿里巴巴\n讲师介绍：阿里巴巴云原生部门高级工程师，主要负责阿里巴巴内部 ServiceMesh 数据面的落地。Envoy/Istio 社区 Member，给 envoy 社区贡献了 Dubbo Proxy filter，metadata 优化等。\n演讲概要：\n介绍 Envoy 在大规模场景下存在的问题以及如何优化 介绍 Envoy 实现自定义协议的最佳实践 扩展自定义协议 扩展连接池 扩展 Cluster 介绍 Envoy 在阿里巴巴内部落地遇到的一些困难 听众收益：\nEnvoy 在大规模场景下存在的一些问题 机器数量过多导致的内存问题 机器全量下发导致的 CPU 问题 关于云原生社区 云原生社区是一个中立的云原生终端用户社区，致力于推广云原生技术，构建开发者生态。\n社区官网：https://cloudnative.to ","relpermalink":"/notice/cloud-native-meetup-3-hangzhou/","summary":"本次活动特别邀请了云原生社区稳定性 SIG、Envoy SIG、OAM SIG、Istio SIG 的核心贡献者为大家带来精彩演讲！","title":"云原生社区 meetup 第三期杭州站开始报名"},{"content":"致关注 Istio 项目的学习者和网友们：\n近日我们接到社区网友的举报，某教育机构（magedu）在提供 Istio 中文文档的下载，并基于该文档进行商业课程的宣传。以下是相关链接：\n马哥教育网站下载链接 公众号宣传文章 知乎宣传文章（已删除） 下面是相关的网页截图。\n网页截图 首先，ServiceMesher 是 Istio 官方的合作伙伴，是官方指定的唯一的中文文档的翻译组织，Istio 官方有明确的说明，请查看 Istio 官方链接 。\n官方声明 第二，大家在公网上看到的 Istio 官方中文文档是由社区发起，并组织社区成员共同参与完成的，倾注了上百人共同的心血，我们从 Istio1.2 版本就开始翻译，先后组织了 1.2、1.5 和目前正在进行的 1.9 版本的翻译工作，具体可参考 GitHub 。\n另外，Istio 的中文文档是可以直接访问的（具体链接为 https://istio.io/latest/zh/docs/ ），根本不需要下载，这会误导初学者的学习方式，令学习者无法获取到最新的内容。\n我们强烈谴责这种将他人劳动成果据为己有、并用作商业用途（课程宣传）的行为。相信每个参与翻译的成员，以及整个社区都不会答应这种剽窃行为的存在！开源项目、社区都是以非盈利的方式无偿提供产品和交流学习的机会，让学习者共同成长。这种剽窃行为严重危害了所有开发者的利益，彻底违背了开源、开放的技术生态理念。这种行为必须要抵制，必须还技术分享领域一片干净的天空！\n我们要求该机构立即删除其公众号、知乎、百度网盘、教育机构网站上关于所谓「Istio 官方中文文档」的分享，并在「马哥 Linux 运维」公众号上发布声明致歉！\n最后 Istio 官方文档的翻译仍在进行中，欢迎大家参与进来，点击查看详情 。\n—— ServiceMesher，2021 年 3 月 31 日\n更新 在 3 月 31 日，以上公告发布后，马哥教育第一时间联系了我，并承认错误，积极改正，并发布了公告（关于 Istio 官方文档使用声明 ）给马哥教育诚恳的态度点个赞，本次事件并非针对马哥教育，而是希望能够唤醒大家的版权保护意识，以正某些“利用”开源的不正之风。\n","relpermalink":"/notice/istio-doc-announcement/","summary":"将社区上百人的成果据为己有，将一个既非最新版本也没有任何版权声明的 PDF 标称官方中文文档，真的是太不厚道了。","title":"关于 Istio 官方中文文档被恶意使用的声明"},{"content":"近日 Tetrate Academy 发布了 Istio Fundamentals Course（Istio 基础教程），现在可免费学习。赶快到 Tetrate Academy 报名学习吧！\n课程目录 课程安排如下：\nService Mesh and Istio Overview Installing Istio Observability: Telemetry and Logs Traffic Management Security Advanced Features Troubleshooting Real World Examples 每个课程最后都会有自测题目，我已经通过了课程，下面是通过课程后的证书。\nTetrate 学院 Istio 基础教程证书 更多 未来 Tetrate 将发布 Certified Istio Administrator（CIA）考试，欢迎各位 Istio 用户和管理员关注和报名参加。\n","relpermalink":"/notice/tetrate-istio-fundamental-courses/","summary":"近日 Tetrate Academy 发布了 Istio Fundamentals Course（Istio 基础教程），现在可免费学习。","title":"Tetrate 学院发布免费 Istio 基础课程"},{"content":"在 IstioCon 2021 上，Istio 社区确定了 2021 年的社区的工作重点是 Day-2 Operation，很多人问我这个词是什么意思。我查了下中文互联网上，没有对这个词的解释，我在网上找到了一些解释，我发现大部分文章的源头都指向了这篇 Defining Day-2 Operations 。因此，在此我将问翻译一下，同时再加上一些我自己的见解。\n下面是笔者对 Day-2 Operation 的理解。\n假如将开发一个系统比作种下一棵树，那么 Day-2 Operation 就是系统开花后结果的过程。我们要不断改进这颗树的基因，以实现效益最大化。Day-2 Operation 就是对这个系统优化改进的过程。\nDay-2 Operation 的定义 Day-2 Operation 不一定是指第 2 天的行动。一旦“某物“进入行动，“Day-2 Operation“是指在这个“某物“没有被杀死或被“其他东西“取代之前的剩余时间段。如下图中展示的软件的生命周期中，从软件被安装之后到被卸载之前的那段时间。\nDay-2 Operation 当我们审视一个业务流程、应用程序或 IT 基础设施生命中的各个阶段时，有些人喜欢把它们描绘成一个循环过程。我相信这是因为人们倾向于使用“应用程序的生命周期“这个词，并以某种方式陷于相信图中必须循环回到起点。各个阶段通常是在时间上向前推进的，而不是把你带回起点。\n假定“X“称为一个组织或实体所需要的东西，可能是一个业务流程，一个应用程序，或者是一些 IT 基础设施。从技术上讲，每当有人设想 X 的时候，总会有一个起点 —— 我们称它为“零日”（这是高中物理的管理，时间的起点通常是 T0）。Day-Zero 可能不是一天：它是提出并记录一套完整的 X 需求所需的时间段，这些活动可能包括高层设计、记录并向某人推销利益、撰写商业案例、寻求资金等。\n这个过程的下一步是构建和部署。Day-1 包括所有活动，从详细（或底层）设计开始，到构建、测试、提出任何所需的流程和人员，以支持 X，使组织受益。在许多情况下，这里可能还涉及一些采购活动。一旦它被安装、设置、配置和批准（“好的开始”），X 就被认为是“上线“或“开放业务”。\n从这一点开始，直到 X 退役、死亡或被替换，我们有 Day-2 操作。这包括保持 X 运行的一系列活动，照看和支持 X，使其以最佳状态运行，确保 X 的运行和交付结果符合最初的意图和期望。监控利用率、确保可用性和成本优化是在通常的内务管理活动基础上增加的，以保持 X 以“最佳“的方式运行。\n随着我们周围世界的要求发生变化，组织要决定对 X 的调整或升级，这些都是必然需要的，是被称为整个大修还是仅仅是升级。如果是整体大修，我们可以假设 X 已经退役并被新的系统 Y 所取代。如果新的 X 只是比以前的 X 有了更大的改进，那么 Day-2 Operation 将继续进行，并包含了所有的活动，以逐步改进 X。\n一个简短的补充说明：“不可变系统“的概念，即人们倾向于通过不允许变化但总是部署新系统来提高可用性，这与上述概念并不冲突。管理不可变系统的过程成为 Day-2 Operation 的一部分。\n对于大多数企业来说，Day-2 Operation 是重复性的。但这是系统为组织产生结果的地方。因此，在 Day-2 Operation 中不断寻求改进，一个能带来最大效益的改进应该是很自然的。\n评论 Day-2 Operation 目前在中文中暂无统一翻译，我暂且将其翻译为“Day-2 运营”，这样可能会看起来更像是个敏捷词汇，跟“精益运营”比较像。这个命名方式可能来自物理（T0，T1，T2，这样来划分时间段），也可能是来自军事术语。Day 0/Day 1/Day 2 - the software lifecycle in the cloud age 这篇文章中对云时代的软件生命周期 Day0、Day1、Day2 做了比较完整的解释。\n在 IT 领域，Day0、Day1、Day2 指的是软件生命周期的不同阶段。在军事术语中，Day0 是训练的第一天，新兵进入成长阶段。在软件开发中，它代表着设计阶段，在这个阶段，项目需求被指定，解决方案的架构被决定。\nDay1 涉及开发和部署在 Day0 阶段设计的软件。在这个阶段，我们不仅要创建应用程序本身，还要创建它的基础设施、网络、外部服务，并实现这一切的初始配置。\nDay2 是产品发货或提供给客户的时间。在这里，大部分精力都集中在维护、监控和优化系统上。分析系统的行为并做出正确的反应是至关重要的，因为由此产生的反馈循环会一直应用到应用程序的寿命结束。在云时代这三个阶段跟云之前有很大的不同。\n软件准备好后，就开始上线，客户开始使用。Day2 从这里开始，介绍包括软件维护和客户支持在内的内容。软件本身要不断发展，以适应不断变化的需求和客户的要求。在 Day2，主要关注的是建立一个反馈循环。我们监控应用的运行情况，收集用户的反馈意见，并将其发送给开发团队，开发团队将在产品中实现并发布新版本。军事术语 Observe-Orient-Decid-Act 恰好能体现这一阶段的工作内容。\n观察：从监控系统中获取信息（资源使用和指标、应用性能监控）。 定位：对问题进行根本原因分析。 决定：找到解决出现的问题的方法。 行动：实施解决方案。 如同在作战过程中，这个循环不断重复，正如下图中展示的那样。\nDay 2 Operation 流程 监控程序是基于服务水平协议（SLA）中定义的要求。SLA 基于服务水平目标（SLO），它代表了我们的服务水平指标（SLI）的状态。自动化和监控是解决第 2 天责任的关键。\n有几类工具可以帮助完成 Day2 的工作。应用性能监控（APM）类组软件，帮助 IT 管理员监控应用性能，从而提供高质量的用户体验。在这里我们可以说出 Datadog、Dynatrace、SignalFX 或 Nutanix Xi Epoch。还有一些自动化和编排工具，如 Ansible 或 Kubernetes，它们有助于管理应用环境。这些工具的应用与 Day1 的工作相重叠。最后，JIRA 或 GItHub 系统处理客户服务，使用户能够报告与他们正在运行的应用程序有关的问题。\n参考 Defining Day-2 Operations - ozone.com What is “Day-2” - about.gitlab.com Day 0/Day 1/Day 2 - the software lifecycle in the cloud age - codilime.com ","relpermalink":"/blog/what-is-day-2-operation/","summary":"在 IstioCon 2021 上，Istio 社区确定了 2021 年的社区的工作重点是 Day-2 Operation，很多人问我这个词是什么意思。我查了下中文互联网上，没有对这个词的解释，我在网上找到了一些解释，我发现大部分文章的源头都指向了这篇 Defining Day-2 Operations。因此，在此我将问翻译一下，同时再加上一些我自己的见解。","title":"什么是 Day-2 Operation？"},{"content":"各位云原生领域的创业者，你们的机遇来啦！我有幸参与「腾讯云原生加速器 」，担任开源社区及行业导师，共 30 个成员名额，报名截止到 4 月 20 日。云原生社区将与腾讯一起共同推进腾讯云原生加速器。\n近年来，以“云原生”为技术路线，构建信息化平台，已成为企业构建面向未来应用架构的首选。云原生凭借敏捷、开放、标准化的特点，将云计算的优势进一步拓宽，轻量化、松耦合、灵活的技术架构特点，有利于各企业在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。\n3 月 10 日，腾讯正式发布国内首个云原生加速器，面向云原生应用、云原生应用编排及管理、云原生底层技术、云原生安全技术等四大方向开启招募，30 个加速席位虚位以待。\n腾讯云原生加速器是腾讯产业加速器的重要组成部分，依托腾讯领先的科技能力、连接内外部资源、发挥技术创新优势，为入选成员提供技术、资金、品牌等层面的加速赋能与生态合作，加速云原生企业成长，促进“云原生”技术路线落地产业，共建云原生产业生态。\n迎接云计算拐点，邀请合作伙伴共建云原生产业生态\n云原生是面向云应用设计的思想理念，以容器、微服务、DevOps、持续集成与持续交付等技术为基础建立，能够发挥云效能的最佳实践路径，充分提升企业研发运维效率。近年来，随着新型 5G、工业互联网等 IT 基础设施的发展，云原生以其敏捷、开放、标准化的特点将云计算的优势进一步拓宽，迅速成为企业构建面向未来的应用架构的首选。中国信息通信研究院《中国云原生用户调查报告 2020》显示，2019 年中国云原生市场规模已达 350.2 亿元，云计算拐点已至，云原生成为驱动业务增长的重要引擎。\n以产业生态模式，腾讯一直与生态伙伴“共创”跨产业、跨科技边界的融合创新。近年，腾讯不断加速拓展云原生应用场景，推动产业从线下走向云端。目前，腾讯云已打造了完备的云原生产品体系和架构，涵盖软件研发流程、计算资源、架构框架、数据存储和处理、安全等五大领域。截至 2020 年 12 月，腾讯云原生注册用户规模已达 100 + 万，覆盖政府、金融、文体、教育、能源、电商、互联网、游戏、LBS、IM、媒体、交通、影视等主流行业。\n为进一步发挥产业互联网“生态共创”优势，全方位推动云原生生态进阶，腾讯发布云原生加速器，持续挖掘并扶持，具备高成长性、高协同价值的云原生生态合作伙伴，链接内外部资源进行赋能，发挥协同力量寻找企业云原生改造最佳实践路线，共建健康发展的云原生产业生态。\n全链路整合腾讯优势，关键节点赋能入选成员\n此次云原生加速器主要招募四大方向，并将为入选成员提供从热点底层技术，到行业创新应用，再到业务核心价值延展的持续赋能。\n腾讯云原生加速器招募方向 腾讯云原生加速器招募方向\n在技术层面，云原生加速器将对接腾讯云原生、腾讯云计算、腾讯开源联盟、全球顶级开源社区，在关键节点与成员进行联合技术研发，提高存储及计算能力，共同推进云原生技术发展。\n在资金层面，云原生加速器对接腾讯产业生态投资，联合一线 VC 机构，为成员提供多渠道资本扶持，提高资本对接效率。\n在行业层面，腾讯云将提供“商机 + 流量”导入，为成员提供多元合作模式与商业机会，拓展云原生应用场景，加快更多行业向云原生环境迁移。\n在资源层面，入选成员将成为腾讯云原生、腾讯云安全、腾讯 AI 实验室、边缘计算实验室、优图实验室等先进技术合作伙伴，收获更多合作机会。\n跨行业邀请 19 位顶尖导师，全维度解读核心痛点\n为给学员构建一个学习交流、业务合作的生态链接平台，腾讯云原生加速器共邀请来自开源社区、腾讯内部、VC 领域的 19 位顶尖导师，与学员共同探究云原生行业生态版图与业务应用。\n其中，来自开源社区及行业的 3 位导师 ——Linux 基金会和 CNCF 云原生基金会大中华区总裁 Keith Chan, 腾讯开源联盟主席单致豪，云原生社区及 ServiceMesher 创始人宋净超，将针对开源社区及开源生态进行观点输出，助力成员拥抱开源生态，链接云原生技术社群。\n腾讯云原生加速器开源社区及行业导师 腾讯云原生加速器开源社区及行业导师\n此外，腾讯公司高级执行副总裁、云与智慧产业事业群总裁汤道生，腾讯公司副总裁、腾讯云总裁邱跃鹏，腾讯公司副总裁丁珂，腾讯云副总裁穆亦飞，腾讯研究院院长司晓，腾讯公司副总裁王巨宏，腾讯云副总裁陈广域，腾讯云副总裁刘颖将分别就云原生战略布局、生态共享、技术应用等话题进行分享，与学员共同探索云原生新课题与新机遇。\n腾讯云原生加速器腾讯内部导师 腾讯云原生加速器腾讯内部导师\n腾讯云原生加速器获得了来自 VC 领域的多位导师支持。来自 IDG、红点中国、云启资本、斯道资本、晨晖创投、北极光、五源资本、宽带资本的合伙人与董事，将从投资角度出发，详解云原生产业发展的核心痛点与底层逻辑。\n腾讯云原生加速器 VC 导师 腾讯云原生加速器 VC 导师\n除导师辅助外，入选成员还将获得一年期立体孵化，通过 4 次闭门交流 + 1 次海外产业探访 + N 次业务及资源对接，完成技术、资源与合作等全方位持续赋能。\n随着数字化转型的加速，作为基础设施的云原生技术，日益成为构筑商业护城河的新入口。Gartner 曾做出预测：在 2020 年前，50% 的企业将业务工作流放到本地需要作为异常事件进行审批，公司“无云”的策略会和现在“无网络”的策略一样少。\n在云时代，不仅需要夯实云原生技术底座，还需要通过生态合作与社区链接，完成云原生技术共建。腾讯云原生加速器将联结更多生态伙伴，基于云原生实践经验，共创云原生与业务融合的无限可能。\n报名地址：见腾讯问卷 ","relpermalink":"/notice/tencent-cloud-native-accelerator/","summary":"30 个成员席位虚位以待！报名截止到 4 月 20 日。","title":"Jimmy 担任「腾讯云原生加速器」开源社区及行业导师"},{"content":"我所任职的公司 Tetrate，今日宣布获得 4000 万美元 B 轮融资。下面的公告来自 Tetrate 的 CEO Varun Talwar，点击阅读原文 。\n我很高兴地宣布，Tetrate 获得 4000 万美元的 B 轮融资，由 Sapphire Ventures 领投，Scale Venture Partners 、NTTVC 以及之前的投资者 Dell Technologies Capital 、Intel Capital 、8VC 和 Samsung NEXT 跟投。在此，我要感谢我的同事（也就是 Tetrands）、投资人、客户、合作伙伴和朋友们一直以来的支持。\n我们将使用这笔资金扩大我们的全球市场，在工程设计上加倍投入，以进一步支持我们的客户利用 唯一的混合云应用网络平台 Tetrate Service Bridge 构建其连接结构。我们还会利用这笔资金将全新的 SaaS 产品 Tetrate Cloud 推向市场：一个基于 Istio 的完全托管的服务网格平台，以实现在任何云上的一致体验。\n融资理由 当今的现代应用程序是一组相互关联的服务，并以容器的形式部署在多集群，有时甚至是多云环境中。在这种分布式动态应用的组合中，阻碍企业灵活性的是通过硬编码库、定制网关、负载均衡器和单片 API 网关实现应用网络和安全的传统模式。\n这就是 Tetrate Service Bridge 的优势所在。\nTetrate Service Bridge Tetrate Service Bridge 是唯一的边缘到工作负载的服务网格管理平台，它为企业提供了一种统一、一致的方式，在复杂的异构部署环境中管理和保护传统和现代工作负载的服务。它是为多集群、多租户和多云部署而构建的，为客户在任何环境中提供一致的内置可观察性、运行时安全性和流量管理。此外，对于平台所有者来说，它可以管理 Istio 和 Envoy 在内部和云中的生命周期。\n将应用网络和安全集中在一个结构中，便于应用开发者使用，这对企业的生产力，以及应用的可靠性和安全性都是一种变革。\nTetrate 是唯一一家为 Istio 提供商业支持的平台中立和云中立公司，也是唯一一家基于 Istio 和 Envoy 的服务网格平台的公司，其客户包括财富 200 强的金融服务、电信、零售、媒体和美国联邦政府机构。\nTetrate 的发展历程 当我在 2018 年创办 Tetrate 时，我知道行业需要一个与云无关的服务网格平台来使应用具有弹性和安全性。Tetrate 的联合创始人 JJ 之前领导了 Twitter 的云基础设施管理团队，他在这个领域有着深厚的经验，共同的激情促使我们成立了 Tetrate。很快，我们的创始工程师也加入了。Zack Butcher、周礼赞和吴晟 ——Istio、Envoy 和 Apache SkyWalking 背后的三巨头。\n2019 年，我们成功完成了由 Dell Technologies Capital 领投的 A 轮融资 ，在得到早期客户的强烈验证后，我们打造了核心产品 Tetrate Service Bridge。我们还启动了与 NIST 的合作，共同制定服务网格的安全标准，并在我们的平台上使用 NGAC 以实现 ZTA。在 2020 年，我们迅速扩大了我们的客户群和收入；今年，B 轮融资标志着我们旅程中的一个重要里程碑 —— 为创纪录的财年度画上了句号，我们的团队扩大了一倍，客户群和收入增加了 10 倍以上。要了解更多关于我们迄今为止的历程，请查看我们的 时间表 。\n我们很自豪，也很谦虚，因为我们获得了超额认购的融资，并且有这么多的投资者验证了我们在开源生态系统中的深厚根基、我们在服务网格领域的思想领导力，以及我们为客户解决问题的记录。\nTetrate B 轮融资投资者 在此观看 我们的投资者对他们投资 Tetrate 的原因和我们的愿景的看法。我们相信，我们才刚刚起步，最好的还在后面。我们期待着共同打造 Tetrate。\n作者：Varun Talwar（Tetrate CEO）\n","relpermalink":"/notice/tetrate-series-b/","summary":"这笔钱将用于扩大全球市场，增大工程投入，以及基于 Istio 的完全托管的服务网格平台 SaaS 服务 Tetrate Cloud。","title":"Tetrate 宣布 B 轮融资 4000 万美元"},{"content":"注意：该活动已结束，感谢大家的关注。\nEnvoy 中文文档地址：https://cloudnative.to/envoy/ 先给大家拜个晚年 云原生社区 Envoy SIG 再次给众位云原生技术爱好者拜个晚年（毕竟正月十五还没过，也不算晚），祝愿大家牛年牛起来！！！\n当然，技术也要搞起来，因此我们需要接着我们去年的任务 —— Envoy 1.16 版本的翻译，继续撸起袖子加油干。下面会回顾及发布一下翻译进度以及做出突出贡献的志愿者（证书年前已经发放）。\n翻译进度 由云原生社区 Envoy SIG 发起的 Envoy 1.16 版本翻译，在几十位（登记信息的为 64 人）志愿者的共同努力下，经过了以下几个阶段的努力贡献：\n第一阶段：2020 年 11 月 4 日至 2020 年 11 月 28 日，共开放 84 个 issue。 第二阶段：2020 年 11 月 28 日至 2020 年 12 月 6 日，共开放 24 个 issue。 第三阶段：2020 年 12 月 6 日至 2021 年 1 月 10 日，共开放 128 个 issue。 第四阶段：2021 年 1 月 10 日至 2021 年 1 月 17 日，共开放 10 个 issue。 第五阶段：2021 年 1 月 17 日至 2021 年 2 月 9 日，共开放 76 个 issue。 第六阶段：2021 年 2 月 9 日开始，共开放 34 个 issue。 现在已经到了决胜攻坚时刻，最后的翻译任务已经开放。欢迎大家搭载翻译的末班车，为 Envoy 的翻译贡献自己的力量。 突出贡献人员 所有登记参与的人员可以腾讯文档 中查看，做出突出贡献（翻译至少 5 个 issue 或长期从事 Review 工作）已经收到了社区的证书：\n云原生社区证书 收到证书的 Envoy 翻译志愿者如下：\n人员 GitHub 账号 人员 GitHub 账号 许振文 helight 张晓辉 Addo.zhang 刘金欣 scilla0531 梁斌 hzliangbin 李云龙 vgbhfive 王泓智 wiswang 张海立 webup 包仁义 baobaoyeye 黄晓芬 kkfinkkfin 孟显超 smarkm 官余鹏 3ks 申红磊 shenhonglei 除了证书，集社区微薄之力，给突出贡献的部分人员送出了大会门票、云原生书籍等福利。名单会持续更新，大家都有机会。\n小小剧透 在 Envoy 1.16 版本翻译结束后，Envoy SIG 将择一黄道吉日在线上（云原生学院直播平台）开一个发布会。具的流程，暂时保密。\n欢迎感兴趣翻译的小伙伴，赶紧加入我们，一起努力来完成 Envoy 1.16 版本的翻译。\n如何加入 关于如何加入 Envoy SIG 并加入翻译团队，可以查看 Envoy 官方文档翻译工作组成员招募中 ；关于翻译要求及相关福利，可以查看 Envoy 官方文档翻译进度更新及成员招募 。有任何疑问可以直接添加微信（jimmysongio 或者 majinghe11）进行咨询。\n","relpermalink":"/notice/envoy-doc-translation/","summary":"Envoy 文档翻译由云原生社区 Envoy SIG 发起，即将完成，欢迎大家加入！","title":"Envoy 文档翻译进入攻坚决胜时刻！欢迎大家加入"},{"content":"本文为我跟 Ignasi Barrera 共同创作，本文英文版首发于 TheNewStack 。\n不同的公司或软件供应商已经设计了无数种方法来控制用户对功能或资源的访问，如酌情访问控制（DAC）、强制访问控制（MAC）、基于角色的访问控制（RBAC）和基于属性的访问控制（ABAC）。从本质上讲，无论何种类型的访问控制模型，都可以抽象出三个基本要素：用户、系统 / 应用和策略。\n在本文中，我们将介绍 ABAC、RBAC 以及一种新的访问控制模型 —— 下一代访问控制（NGAC），并比较三者之间的异同，以及为什么你应该考虑 NGAC。\n什么是 RBAC？ RBAC，即基于角色的访问控制，采用的方法是根据用户在组织中的角色授予（或拒绝）对资源的访问。每个角色都被分配了一系列的权限和限制，这很好，因为你不需要跟踪每个系统用户和他们的属性。你只需要更新相应的角色，将角色分配给用户，或者删除分配。但这可能很难管理和扩展。使用 RBAC 静态角色模型的企业经历了角色爆炸：大公司可能有数万个相似但不同的角色或用户，他们的角色会随着时间的推移而改变，因此很难跟踪角色或审计不需要的权限。RBAC 具有固定的访问权限，没有规定短暂的权限，也没有考虑位置、时间或设备等属性。使用 RBAC 的企业很难满足复杂的访问控制要求，以满足其他组织需求的监管要求。\nRBAC 示例 下面是 Kubernetes 中 default 命名空间中的一个 Role，可以用来授予 pod 的读取权限。\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [\u0026#34;v1\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] 什么是 ABAC？ ABAC 是“基于属性的访问控制“的缩写。从高层次上讲，NIST 将 ABAC 定义为一种访问控制方法，“在这种方法中，根据分配的主体属性、环境条件以及用这些属性和条件指定的一组策略，批准或拒绝主体对对象进行操作的请求。”ABAC 是一个细粒度的模型，因为你可以给用户分配任何属性，但同时它也成为一种负担，很难管理：\n在定义权限的时候，用户和对象之间的关系无法可视化。 如果规则设计的有点复杂或者混乱，对于管理员来说，维护和跟踪会很麻烦。 当有大量的权限需要处理时，会造成性能问题。\nABAC 示例 Kubernetes 最初使用 ABAC 作为访问控制，并通过 JSON 行配置，例如：\nAlice 可以只读取命名空间 foo 中的 pod。\n{\u0026#34;apiVersion\u0026#34;: \u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: {\u0026#34;user\u0026#34;: \u0026#34;alice\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;pods\u0026#34;, \u0026#34;readonly\u0026#34;: true}} 什么是 NGAC？ NGAC，即下一代访问控制，采用将访问决定数据建模为图的方法。NGAC 可以实现系统化、策略一致的访问控制方法，以高精细度授予或拒绝用户管理能力。NGAC 由 NIST （美国国家标准与技术研究所）开发，目前用于 Tetrate Q 和 Tetrate Service Bridge 。\n有几种类型的实体；它们代表了您要保护的资源、它们之间的关系以及与系统互动的行为者。这些实体是：\n用户 对象 用户属性，如组织单位 对象属性，如文件夹 策略类，如文件系统访问、位置和时间 NIST 的 David Ferraiolo 和 Tetrate 的 Ignasi Barrera 在旧金山举行的 2019 年服务网格日（Service Mesh Day 2019）上发表了关于下一代访问控制的 演讲 ，分享了 NGAC 的工作原理。\nNGAC 是基于这样一个假设：你可以用一个图来表示你要保护的系统，这个图代表了你要保护的资源和你的组织结构，这个图对你有意义，并且符合你的组织语义。在这个对你的组织非常特殊的模型之上，你可以叠加策略。在资源模型和用户模型之间，定义了权限。这样 NGAC 提供了一种优雅的方式来表示你要保护的资源，系统中的不同角色，以及如何用权限把这两个世界联系在一起。\nNGAC 模型中的 DAG 图片来自于 Linear Time Algorithms to Restrict Insider Access using Multi-Policy Access Control Systems NGAC 示例 下面的例子展示了一个简单的 NGAC 图，其中有一个代表组织结构的用户 DAG，一个代表文件系统中的文件和文件夹的对象 DAG，一个文件的分类，以及两个不同的策略 —— 文件系统和范围，可以结合起来做出访问决策。两个 DAG 之间的关联边定义了行为者对目标资源的权限。\nNGAC 示例图 在这张图中，我们可以看到 /hr-docs 文件夹中的两个文件 resume 和 contract 的表示，每个文件都链接到一个类别（public/confidential）。还有两个策略类，File System 和 Scope，图中的对象被连接在这里 —— 需要满足这些条件才能获得对每个文件的访问权。\n在例子中，用户 Allice 对两个文件都有读写访问权限，因为有一个路径将 Allice 链接到每个文件，而且路径授予了两个策略类的权限。但是，用户 Bob 只有对 resume 文件的访问权，因为虽然存在一个从 Bob 到 contract 文件的路径，该路径满足 File System 策略类的“读 \u0026#34; 权限，但没有授予 Scope 策略类权限的路径。所以，Bob 对 contract 文件的访问被拒绝。\n为什么选择 NGAC？ 在 ABAC 的情况下，需要跟踪所有对象的属性，这造成了可管理性的负担。RBAC 减少了负担，因为我们提取了所有角色的访问信息，但是这种模式存在角色爆炸的问题，也会变得不可管理。有了 NGAC，我们在图中就有了我们所需要的一切 —— 以一种紧凑、集中的方式。\n当访问决策很复杂时，ABAC 的处理时间会成倍上升。RBAC 在规模上变得特别难以管理，而 NGAC 则可以线性扩展。\nNGAC 真正出彩的地方在于灵活性。它可以被配置为允许或不允许访问，不仅基于对象属性，而且基于其他条件 —— 时间、位置、月相等。\nNGAC 的其他关键优势包括能够一致地设置策略（以满足合规性要求）和设置历时性策略的能力。例如，NGAC 可以在中断期间授予开发人员一次性的资源访问权，而不会留下不必要的权限，以免日后导致安全漏洞。NGAC 可以在一个访问决策中评估和组合多个策略，同时保持其线性时间的复杂度。\n总结 下表从几个方面对 ABAC、RBAC 和 NGAC 进行了比较。\n权限模型 优点 缺点 ABAC 灵活 性能和审计问题 RBAC 简单 角色爆炸、固定的访问权限、合规需求挑战 NGAC 细粒度、利于审计、灵活、组合权限策略 复杂 总而言之：\nRBAC 比较简单，性能好，但在规模上会受到影响。 ABAC 很灵活，但性能和可审计性是个问题。 NGAC 通过使用一种新颖、优雅的革命性方法来修复这些差距：在用户提供的现有世界表示之上叠加访问策略。你也可以对 RBAC 和 ABAC 策略进行建模。 参考 Guide to Attribute-Based Access Control (ABAC) Definition and Considerations Deploying ABAC policies using RBAC Systems RBAC vs. ABAC: What’s the Difference? Role Explosion: The Unintended Consequence of RBAC Exploring the Next Generation of Access Control Methodologies ","relpermalink":"/blog/why-you-should-choose-ngac-as-your-access-control-model/","summary":"本文将向你介绍下一代权限控制模型——NGAC，并对比 ABAC、RABC，说明为什么要选择 NGAC。","title":"为什么应该选择使用 NGAC 作为权限控制模型"},{"content":" IstioCon 海报（Jimmy Song） 主题：Service Mesh in China 时间：北京时间 2 月 23 日，上午 10:00 - 10:10 参与方式：IstioCon 2021 官网 费用：免费 北京时间 2 月 22 日至 25 日，Istio 社区将在线上举办第一届 IstioCon，免费报名参加，欢迎大家踊跃参加！我将在 2 月 23 日（农历正月十二，星期二）发表闪电演讲，作为一个 Service Mesh 技术在中国的布道者和见证者，我将为大家介绍中国的 Service Mesh 行业及社区在中国的发展。\n我与徐中虎（华为）、丁少君（Intel）都是首届 IstioCon 组委会成员，也是中国区的组织者，考虑到 Istio 在中国有大量的受众，我们特地安排对中国时区友好的中文演讲。本次活动一共有 14 场中文分享，另外还有几十场英文分享。演讲将分为闪电演讲（10 分钟）和 presentation（40 分钟）两种形式。\n加入云原生社区 Istio SIG ，参与本次大会的交流。关于 IstioCon 2021 的时间表，请访问 IstioCon 2021 官网 ，或点击查看详情。\n","relpermalink":"/notice/istiocon-2021/","summary":"IstioCon 2021，我将发表闪电演讲，北京时间 2 月 22 日上午 10 点。","title":"IstioCon 2021 闪电演讲预告"},{"content":"ServiceMesher 网站 因为其代码托管的 GitHub 与网站发布服务器上的 webhook 程序已”失联“，网站托管服务器暂时无法登录，以致于网站无法更新。今天我花了一天的时间，将 ServiceMesher 上的所有博客都迁移到了云原生社区官网 cloudnative.to ，截止今天，云原生社区上一共有 354 篇博客。\n现计划将 ServiceMesher 官网的 GitHub 归档（servicemesher.com 域名下的所有页面），不再接受新的 PR，请大家直接提交到云原生社区 。谢谢大家！\n","relpermalink":"/notice/servicemesher-blog-merged/","summary":"ServiceMesher 网站已停止维护，计划将网站代码归档，博客已迁移到云原生社区，请将新的博客提交到云原生社区。","title":"ServiceMesher 网站停止维护，原有博客已迁移到云原生社区"},{"content":"很荣幸收到 CSDN 的邀请，接受”云原生人物志“专栏采访，其实我从 2017 年起就已经在撰写 Kubernetes 和云原生年度总结和新年展望 ，今天在此聊抒己见，欢迎大家讨论和指正。\n云原生在演进 云原生是一种行为方式和设计理念，究其本质，凡是能够提高云上资源利用率和应用交付效率的行为或方式都是云原生的。云计算的发展史就是一部云原生化的历史。Kubernetes 开启了云原生 1.0 的序幕，服务网格 Istio 的出现，引领了后 Kubernetes 时代的微服务，serverless 的再次兴起，使得云原生从基础设施层不断向应用架构层挺进，我们正处于一个云原生 2.0 的新时代。\n业界动向 最近国内的一些云厂商，如阿里云、腾讯云、华为云陆续发布了各自的云原生相关的架构和实践白皮书。\n2020 年 7，中国信通院发布了《云原生产业白皮书（2020）》。 2020 年 12 月 20 日，在腾讯 2020 Techo Park 开发者大会上，腾讯云正式发布了《云原生最佳实践路线图》，同时发布的还有一份 3 万多字的《腾讯云原生路线图手册》。 2020 年 12 月 23 日，阿里云原生实战峰会上发布了《云原生架构白皮书》。 2020 年 12 月 30 日，华为云在深圳的 TechWave 云原生 2.0 技术峰会上联合 Forrester 发布了《云原生白皮书：拥抱云原生优先战略》。 2021 年初，阿里巴巴达摩院发布 2021 十大科技趋势，其中将“云原生重塑 IT 技术体系”作为 2021 年技术预测之一。 云原生项目的“寒武纪大爆发” 云原生已历经”寒武纪大爆发“，标志是从 2018 年 Kubernetes 毕业 后走向深耕路线。云原生领域的开源项目层出不穷，令人眼花缭乱，见我收集的 Awesome Cloud Native。\n云原生发展阶段 2020 年 CNCF 共接纳了 35 个项目加入基金会，并且有多个项目毕业或晋级，CNCF 托管的项目总数达到了 80 多个。\n图片来自 CNCF 年度报告 2020 云原生之争实际上是标准之争 PC 端操作系统 Windows 占据上风，移动端是 iOS 和 Android，服务器端是 Linux，而云计算商用分布式操作系统呢？答案是 Kubernetes。\n2020 年 Kubernete 宣布将在 v1.20 版本之后弃用 Docker ，实际上 Docker 本来就不是 Kubernetes 中默认和唯一的的容器运行时了，实际上只要是支持 CRI（Container Runtime Interface）或 OCI（Open Container Initiative）标准的容器运行时都可以在 Kubernetes 中运行。如下图所示，容器，英文是 container，也是集装箱的意思，其实集装箱不止一种型号，根据运送的货物的不同特性可以制定了多种集装箱类型。而这个容器类型是标准只能是由 Kubernetes 来定，否则只能是削足适履。\n各种容器类型 Kubernetes 统一了云上的资源对象制定和调度的标准，只要在其标准之上开发 CRD 和 Operator 即可。但是这也仅限于单个应用的管理，如何管理复杂的多集群和混合云环境，如何管理应用间流量，如何如何保证调用链的安全？以 Istio 为代表的服务网格就是为了解决这个问题。\n云原生趋势：云上应用管理 Kubernetes 奠定了云原生基础设施的基础，随着而来的监控、存储、AI、大数据等技术的迁移，从单个应用层面来说已经日趋成熟，而在使用云原生架构尤其是对云上应用的管理，而在异构环境、多集群、混合云等已成为常态的情况下，如何对云上的应用进行管理，成为棘手的事情。\nKubernetes 以其开创新的声明式 API 和调节器模式，奠定了云原生的基础。我们看到 Google 的项目 Anthos，Azure 的 Arc，AWS 最近开源的 EKS-D，它们都是着重在混合云管理，让云无处不在。另外，服务网格（Service Mesh）经过两年的推广和发酵，将会看到越来越多的应用。\n云原生与开源社区 目前企业云原生化转型最缺乏的东西 —— 套路和组合拳。对于基础软件，企业往往会选择开源项目并根据自身需求进行改造，而云原生的开源项目又有很多，企业不是没有选择，而是选择太多，以致于无从下手。就像下面教你如何画猫头鹰的示例。我们可以将企业的云原生化的愿景想象成是这只猫头鹰，这些开源项目就像步骤一中圆，你可能想当然的认为只要用了 Kubernetes 就是云原生了，这就像画了两个圆，而剩余部分没有人教你如何完成。\n如何画猫头鹰 开源社区的核心是面向开发者，就是向开发者灌输如何来画好这只“猫头鹰”的。开源不意味着免费和做慈善，使用开源也是有代价的。开源社区存在的意义是平衡开发者、终端用户及供应商之间的共同利益，而一个中立的开源社区有利于发挥开源的生态优势。\n近年来随着云原生大热，在美国诞生了大量该领域的初创公司，他们基于 AWS、谷歌云、Azure 等提供各种云原生的解决方案，从每次 KubeCon 的赞助商规模上就可以窥知一二。国内该领域的公司目前还不多，而云原生终端用户社区的公司规模上依然跟国外的公司数量有不小的差距。\n云原生社区就是在这样的背景下于 2020 年初由我发起，开始筹备并在 5 月 12 号正式成立，致力于推广云原生技术，构建开发者生态。云原生社区采取 SIG（特别兴趣小组）和 WG（工作组）的组织形式，基于开源项目和不同的专业领域构建研讨组，与厂商合作定期举办线下 meetup，并邀请社区的专家们定期在 B 站的云原生学院进行直播。\n总结 开源应该关注的是终端用户和开发者生态，用 Apache Way 来说就是“社区大于代码”，没有社区的项目是难以长久的。因此我们可以看到国内一些云厂商开源项目之后也会积极投入运营，举行各种各样的活动。我们看到在云原生的推广过程中，CNCF 起到的相当大的作用，2020 年国内也有类似的基金会成立，我们希望看到更多中立的基金会和社区的成立，更多的厂商参与其中，为终端用户提供更佳的解决方案。\n最后感谢 CSDN 宋慧编辑和「CSDN 云计算」的邀请。\n往期报道见：\n梁胜：做开源项目的贡献者没有意义 华为云 CTO 张宇昕：云原生已经进入深水区 APISIX 温铭：开源的本质是要拿开发者的杠杆 个人介绍 在我的职业生涯里先后从事过 Java 开发、大数据运维、DevOps、开源管理等工作，个人爱好是研究并推广开源技术及理念，摄影和旅行。目前在企业级服务网格初创公司 Tetrate 担任 Developer Advocate，同时作为中立的云原生终端用户社区 —— 云原生社区（Cloud Native Community）的负责人。\n我的整个职业生涯都是与开源息息相关的，渊源可以追溯到大学时期。大学时我就开始使用 Linux 系统（Ubuntu）学习，刚进入职场的时候面向的也是 Hadoop 的开源生态及各种开源中间件，2015 起开始接触 Docker，2016 年开始进入云原生领域，2017 年开始写 Kubernetes 领域的第一本开源中文电子书《Kubernetes Handbook——Kubernetes 中文指南 / 云原生应用架构实践手册 》，本书直到如今仍在更新，2018 年在蚂蚁集团做开源管理及服务网格社区 ServiceMesher，2020 年加入基于 Istio、Envoy 和 Apache SkyWalking 等开源项目而构建企业级服务网格的初创公司 Tetrate。\n","relpermalink":"/blog/cloud-native-2021/","summary":"本文为应 CSDN《云原生人物志》栏目约稿，知微见著，窥见云原生价值与趋势。","title":"“寒武纪大爆发”之后的云原生，2021 年走向何处？"},{"content":"AWS 在 2020 年 12 月举行的 re:Invent 大会上发布了 EKS-D ，此举旨在联合合作伙伴，开源 AWS 维护大规模 EKS 集群的经验，帮助用户实现混合云场景下 Kubernetes 的一致性的体验。本文将为你解析 EKS-D 的战略意义，说明它是如何与 Istio 共同保证混合云环境一致性的。\n什么是 EKS-D？ EKS-D 是 Amazon EKS 的一个发行版，可以运行在企业内部、云端或自己的系统上。EKS-D 保持与 Kubernetes 新版本同期发布。在不久的将来，将以 EKS Anywhere（EKS-A）为名，提供 EKS-D 的支持、打包产品和安装方法。\n下图展示了 AWS、EKS-D、Kubernetes 及用户之间的关系。\nEKS-D 对于 AWS、合作伙伴及用户来说具有不同的意义。\nAWS：增加 AWS 的市场拥有率 合作伙伴：整合 AWS 的渠道和客户资源以触达更多用户 用户：保证了异构环境下的 Kubernetes 的一致性，简化运维 如今企业要考虑选择哪个云供应商要考虑很多因素，同时，还有很多企业的 IT 难以跨入云，而是继续依赖究竟考验的传统 IT 架构以开展业务。\n在上云的时候客户希望在企业内部和云端获得一致的体验，以便进行迁移或实现混合云设置。不是所有应用都适合跨云迁移，为了合规、数据安全等种种原因，多集群、混合云的使用场景将很普遍。\n为什么使用多集群和混合云 我们在很多情况下或使用多集群、混合云等部署方式，例如：\n为了避免厂商锁定，便于应用跨集群迁移； 为了实现应用的高可用； 当一个集群的规模过大造成性能瓶颈时； 为了合规和数据安全； 为了就近部署，降低网络延迟，提高用户体验； 为了进行一些测试； 突发业务，需要集群扩容； 以上情况经常发生，对集群的管理造成了挑战。Kubernetes 统一了容器编排的标准，随着其进一步普及，更有望成为云原生应用的底层 API。但是对于如何管理多集群和混合云环境中的 Kubernetes 集群，又为我们带来了新的挑战。\n使用 Istio service mesh 管理混合云 Istio 服务网格作为云原生应用的网络基础设施层，可以同时管理 Kubernetes 及非容器负载，如虚拟机 。Istio 可以在多种平台 中部署，又支持多种部署模式 ，兼具管理多集群和混合云的功能。在部署时需要充分考虑 Region、Zone 的分布、网络隔离、多租户、控制平面的高可用等因素。\n假如我们同时使用 EKS 和部署在私有数据中心中的 EKS-D，那么如何将两个集群使用一个统一的控制平面管理起来呢？如下图所示，cluster1 和 cluster2 分别表示部署在 EKS 和 EKS-D 的 Kubernetes 集群，这两个集群的网络是隔离的，现因为上文所说的适合使用混合云某个场景，现在为了将它们纳入同一个服务网格使用一个控制平面来管理，我们采用了 Primary-Remote 多网络 的部署模式。\n图示：\n图中黑色箭头表示控制平面内获取服务和端点配置的请求； 图中蓝色箭头表示服务 A 访问服务 B 的路由； 图中绿色箭头表示服务 A/B 向控制平面获取服务端点的路由； 使用该模式部署 Istio 时，需要保证控制平面对 Kubernetes 的 API Server 的连接性，具体的安装过程请参考 Istio 文档 。\n总结 EKS-D 保证了在混合云环境下 Kubernetes 集群的一致性，降低了集群的运维成本。Istio 固有的多集群感知能力，进一步从服务层面增强了用户体验的一致性，帮助我们将多集群中的服务纳入统一的控制平面管理。EKS-D 发布的时有众多的合作伙伴的响应，其中 Tetrate 作为 Istio service mesh 的解决方案供应商提供了 Tetrate Service Bridge（TSB） 在 EKS 和 EKS-D 上实现了跨工作负载的统一应用连接和安全性。\n","relpermalink":"/blog/eks-eksd-istio-hybrid-cloud/","summary":"本文将为你解析 EKS-D 的战略意义，说明它是如何与 Istio 共同保证混合云环境一致性的。","title":"使用 EKS-D 和 Istio 保证混合云环境一致性"},{"content":"本文将为你介绍 Istio 历史上对虚拟机负载的支持情况，尤其是 Istio 1.8 中引入的智能 DNS 代理及 WorkloadGroup 使得虚拟机与容器在资源抽象层面可以等同视之。我将为你展现一幅 Istio 支持虚拟机的波澜壮阔的奥德赛。\n前言 在我之前的博客 中谈到 Istio 1.7 如何支持虚拟机，但那时虚拟机仍然无法无缝的集成到 Istio 中，因为还需要做很多手动的操作。现在，Istio 1.8 新增了 WorkloadGroup 及智能 DNS 代理 ，这使得如虚拟机这样的非 Kubernetes 工作负载可以在 Istio 中成为像 Pod 一样的一等公民。\n不论有没有为虚拟机安装 sidecar，虚拟机通常情况下无法直接访问 Kubernetes 集群中的 DNS 服务器以解析 Kubernetes 服务的 Cluster IP 的（虽然你也许可以通过一些黑客的手段做到），这是在 Istio 中集成虚拟的最后一块短板，终于在 Istio 1.8 中完成了突破。\n为什么要支持虚拟机？ 在我们将应用在迁移到云原生架构，不断容器化的过程中，将经历三个阶段，如下图所示。\n云原生应用的三个阶段 阶段一：应用全部部署在虚拟机上 阶段二：应用既部署在虚拟机上也部署在容器里，正在从虚拟机向容器中迁移，并使用 Kubernetes 管理容器 阶段三：所有的应用优先部署在容器里，使用 Kubernetes 管理容器，使用 Istio 管理应用间的通信 上图仅是对以上三个阶段的最简化描述，实际上还会有多混合云、多机房、多集群等情况，且阶段三只是个理想化的阶段，容器和虚拟机将是长期共存的，但是容器化趋势不变。\n在阶段二中，人们通常会将新业务和少量应用率先实现容器化，并部署到 Kubernetes 中，在应用尚未完全实现容器化的时候，处于过度状态时会遇到很多问题，如何让应用与部署在虚拟机中的服务交互？虚拟机如何访问容器中的服务？在服务迁移的过程中如何保证稳定无缝？是否可以将容器和虚拟机纳入一个统一的控制平面来管理？Istio 从开源初期就考虑并着手解决这一问题。\nIstio 支持虚拟机的历史 Istio 对于虚拟机的支持是个漫长的过程，堪称是一部奥德赛。\nIstio mesh 扩张 Istio 从 0.2 版本开始通过 Istio Mesh Expansion 将虚拟机加入的 Mesh 中，但是需要满足以下前提条件：\n虚拟机必须可以通过 IP 地址直接访问到应用的 Pod，这就要求容器与 VM 之间通过 VPC 或者 VPN 建立扁平网络，虚拟机不需要访问 Cluster IP，直接对服务的 Endpoint 端点访问即可。 虚拟机必须可以访问到 Istio 的控制平面服务（Pilot、Mixer、CA，现在已正整合为 Istiod），可以通过在 Istio Mesh 中部署负载均衡器将控制平面端点暴露给虚拟机。 （可选）虚拟机可以访问到 Mesh 内部的（部署在 Kubernetes 中）的 DNS server。 集成虚拟机的步骤如下：\n为 Istio 控制平面服务及 Kubernetes 集群的 DNS 服务创建 Internal 负载均衡器； 生成 Istio Service CIDR、Service Account token、安全证书、Istio 控制平面服务的 IP（通过 Internal 负载均衡器暴露出来的 IP）的配置文件并发送给虚拟机； （可选）在虚拟机中安装、配置并启动 Istio 的组件、dnsmaq（用于 DNS 发现），此时虚拟机可以使用 FQDN 访问 mesh 中的服务了，这一步是为了保证虚拟机可以正确解析出 mesh 中服务的 Cluster IP； 若要在虚拟机中运行服务，需要配置 sidecar，新增需要拦截的 inbound 端口，然后重启 istio，还需要运行 istioctl 为服务注册 下图展示的从集成虚拟机到在 mesh 中访问虚拟机中服务的详细流程。\n图一：从集成虚拟机到在 mesh 中访问虚拟机中服务的详细流程 DNS 被虚拟机中部署的 dnsmasq 劫持，这使得它可以正确的获取 Istio 服务、Kubernetes 内置 DNS 的端点 IP； 访问 Kubernetes 的内置 DNS 服务（该服务已通过 Internal 负载均衡器暴露到集群外，可以直接访问）； 返回 productpage.bookinfo.svc.cluster.local 被解析出来的 Cluster IP，注意该 IP 地址无法直接访问，但是如果无法被 DNS 解析的话将导致 VM 对该服务的请求失败； 虚拟机对 mesh 中服务的访问被 sidecar proxy 劫持； 因为 proxy 已连接 Istio 控制平面，可通过 xDS 查询到该服务的端点，因此流量将被转发到其中的一个端点。关于这一步的详细过程请参考 Istio Handbook 中的 sidecar 流量路由机制分析 一节 ； 要想在 mesh 中访问 VM 中的服务，需要使用 istioctl register 命令手动将 VM 中的服务添加到 mesh 中，这本质上是将 VM 中的服务，注册到 Kubernetes 中的 service 和 endpoint； mesh 中的服务可以使用 VM 注册的服务名称（FQDN，例如 mysql.vm.svc.cluster.local）来访问； 以上 Istio 对虚拟机支持的方式一直延续到 Istio 1.0，在 Istio 1.1 的时候引入了新的 API ServiceEntry ，使用它可以在 Istio 的内部服务注册表中添加额外的条目，这样 mesh 中的服务就可以访问/路由到这些手动指定的服务了，不再需要运行 istioctl register 命令，而且该命令在 Istio 1.9 中将被废弃。\nIstio 1.5 中增加了 istioctl experimental add-to-mesh 命令，可以将虚拟机中的服务添加到 mesh 中，其功能与 istioctl register 一样。\n新增资源抽象 Istio 从 1.6 版本 开始在流量管理 中引入了新的资源类型 WorkloadEntry ，用以将虚拟机进行抽象，使得虚拟机在加入 mesh 后可以作为与 Kubernetes 中的 Pod 等同的负载，具备流量管理、安全管理、可视化等能力。通过 WorkloadEntry 可以简化虚拟机的网格化配置过程。WorkloadEntry 对象可以根据服务条目中指定的标签选择器选择多个工作负载条目和 Kubernetes pod。\nIstio 1.8 中增加了 WorkloadGroup 的资源对象，它提供了一个规范，可以同时包括虚拟机和 Kubernetes 工作负载，旨在模仿现有的用于 Kubernetes 工作负载的 sidecar 注入和部署规范模型来引导 Istio 代理。\n下面是虚拟机与 Kubernetes 中负载的资源抽象层级对比。\n对比项 Kubernetes 虚拟机 基础调度单位 Pod WorkloadEntry 编排组合 Deployment WorkloadGroup 服务注册与发现 Service ServiceEntry 从上面的图表中我们可以看到，对于虚拟机工作负载是可以与 Kubernetes 中的负载一一对对应的。\n此时看似一切都比较完美了，但是直接将 Kubernetes 集群中的 DNS server 暴露出来会带来很大的安全风险 ，因此我们一般手动将虚拟机需要访问的服务的域名和 Cluster IP 对写到本机的 /etc/hosts 中，但是对于一个节点数量庞大的分布式集群来说，这种做法又有些不现实。\n通过配置虚拟机本地 /etc/hosts 访问 mesh 内服务的流程，如下图所示。\n图二：通过配置虚拟机本地 /etc/hosts 访问 mesh 内服务的流程 将虚拟机中的服务注册到 mesh 中； 将要访问的服务的域名、Cluster IP 对手动写入虚拟机本地的 /etc/hosts 文件中； 虚拟机获得访问服务的 Cluster IP； 流量被 sidecar proxy 拦截并解析出要访问的服务的端点地址； 访问服务的指定端点； 在 Kubernetes 中我们一般使用 Service 对象来实现服务的注册和发现，每个服务都有一个独立的 DNS 名称，应用程序可以使用服务名称来互相调用。我们可以使用 ServiceEntry 将虚拟机中的服务注册到 Istio 的服务注册表中，但是在 Kubernetes 集群中的 DNS server 无法对 mesh 外部暴露的情况下，虚拟机无法访问 Kubernetes 集群中的 DNS 服务以获取服务的 Cluster IP，从而导致虚拟机访问 mesh 中的服务失败。如果能在虚拟机中增加一个 sidecar 可以透明地拦截 DNS 请求，可获取 mesh 内所有服务的 ClusterIP，类似于图一中的 dnsmasq 的角色，这样不就可以解决问题了吗？\n智能 DNS 代理 Istio 1.8 中引入了智能 DNS 代理 ，虚拟机访问 mesh 内服务无需再配置 /ect/hosts，如下图所示。\n图三：引入了智能 DNS 代理后虚拟机访问 mesh 内服务的流程 DNS proxy 是用 Go 编写的 Istio sidecar 代理。Sidecar 上的 Istio agent 将附带一个由 Istiod 动态编程的缓存 DNS 代理。来自应用程序的 DNS 查询会被 pod 或 VM 中的 Istio 代理透明地拦截和服务，该代理会智能地响应 DNS 查询请求，可以实现虚拟机到服务网格的无缝多集群访问。\n至此，Istio 1.8 中引入的 WordloadGroup 及智能 DNS 代理，补足了 Istio 对虚拟机支持的最后一块短板，使得部署在虚拟机中的遗留应用可以跟 Kubernetes 中的 Pod 一样完全等同看待。\n总结 在这部 Istio 支持虚拟机的奥德赛中，我们可以看到：从最初的将 mesh 中的 DNS server 暴露给外部，在虚拟机中安装配置 dnsmasq，到最后的使用智能 DNS 代理，并使用 WorkloadEntry、WorkloadGroup 和 ServiceEntry 等资源抽象，逐步实现了虚拟机和 pod 的统一管理。本文仅仅是针对单集群的情况，在实际的生产中使用还远远不够，我们还需要处理安全、多集群、多租户等诸多问题，欢迎关注 Tetrate 的旗舰产品 Tetrate Service Bridge 了解更多关于 Istio 应用在生产上的最佳实践。\n","relpermalink":"/blog/istio-vm-odysssey/","summary":"本文将为你介绍 Istio 历史上对虚拟机负载的支持情况，尤其是 Istio 1.8 中引入的智能 DNS 代理及 WorkloadGroup 使得虚拟机与容器在资源抽象层面可以等同视之。我将为你展现一幅 Istio 支持虚拟机的波澜壮阔的奥德赛。","title":"Istio 对虚拟机支持史话"},{"content":"今天 Istio 1.8 发布了，这是 Istio 在 2020 年发布的最后一个版本，按照 Istio 社区在今年初设定的目标 继续推进，该版本主要有以下更新：\n支持使用 Helm 3 进行安装和升级 正式移除了 Mixer 新增了 Istio DNS proxy，透明地拦截应用程序的 DNS 查询，实现智能应答 新增了 WorkloadGroup 以简化对虚拟机的引入 WorkloadGroup 是一个新的 API 对象，旨在与虚拟机等非 Kubernetes 工作负载一起使用，模仿现有的用于 Kubernetes 工作负载的 sidecar 注入和部署规范模型来引导 Istio 代理。\n安装与升级 Istio 从 1.5 版本开始弃用了 Helm，使用 istioctl manifest 方式安装，后来又改成了 istioctl install，现在又重新回归了 Helm，Helm 作为 Kubernetes 环境下最常用的应用安装管理组件，此次回归也是倾听用户声音，优化安装体验的的反应吧，不过 Istio Operator 依然将是 Istio 安装的最终形式，从 1.8 版本开始 Istio 支持使用 Helm 进行 in-place 升级和 canary 升级。\n增强 Istio 的易用性 istioctl 命令行工具新的了 bug reporting 功能（istioctl bug-report），可以用来收集调试信息和获取集群状态。\n安装 add-on 的方式变了，在 1.7 中已经不推荐使用 istioctl 来安装，在 1.8 中直接被移除了，这样有利于解决 add-on 落后于上游及难以维护的问题。\n正式移除了 Mixer，推荐使用 WebAssembly 通过扩展 Envoy 的方式来扩展 Istio，也推荐大家使用 GetEnvoy Toolkit 来进行 Envoy 的扩展开发。\n对虚拟机的支持 在我之前的博客 中谈到 Istio 1.7 如何支持虚拟机，在 Istio 1.8 中新增了智能 DNS 代理 ，它是由 Go 编写的 Istio sidecar 代理，sidecar 上的 Istio agent 将附带一个由 Istiod 动态编程的缓存 DNS 代理。来自应用程序的 DNS 查询会被 pod 或 VM 中的 Istio 代理透明地拦截和服务，该代理会智能地响应 DNS 查询请求，可以实现虚拟机到服务网格的无缝多集群访问。\n新增了 WorkloadGroup ，它描述了工作负载实例的集合。提供了一个规范，工作负载实例可以用来引导它们的代理，包括元数据和身份。它只打算与虚拟机等非 Kubernetes 工作负载一起使用，旨在模仿现有的用于 Kubernetes 工作负载的 sidecar 注入和部署规范模型来引导 Istio 代理。\n在 Tetrate ，我们在客户的多集群部署中广泛使用这种机制，以使 sidecar 能够为暴露在网格中所有集群的入口网关的主机解析 DNS，并通过 mTLS 访问。\n总结 总而言之，Istio 团队履行了年初的承诺 ，自 2018 年发布 1.1 版本发布起，保持了固定的发布节奏，每 3 个月发布一个版本，在性能、用户体验上持续优化，以满足 brownfiled 应用与 greenfield 应用在 Istio 上的无缝体验。我们期待 Istio 在 2021 年可以给我们带来更多惊喜。\n最后，感谢马若飞 对本文的审阅。\n","relpermalink":"/blog/istio-18-release/","summary":"Istio 信守了年初的承诺，从 1.1 开始，几乎每三个月一个版本，更能体会用户的需求了。此次是 2020 年的最后一个版本，引入了 WorkloadGroup 和 DNS proxy，对如虚拟机的非 Kubernetes 负载的支持更进了一步。","title":"Istio 1.8——用户至上的选择"},{"content":"Istio 是目前最流行的服务网格，用于连接、保护、控制和观察服务。当其 2017 年开源时，Kubernetes 已赢得容器编排之战，Istio 为了满足组织转向微服务的需求。虽然 Istio 声称支持异构环境，如 Nomad、Consul、Eureka、Cloud Foundry、Mesos 等，但实际上，它一直与 Kubernetes 合作得最好–它的服务发现就是基于 Kubernetes。\nIstio 在发展初期就因为一些问题而饱受诟病，比如组件数量多、安装和维护复杂、调试困难、由于引入了太多的新概念和对象（多达 50 个 CRD）而导致学习曲线陡峭，以及 Mixer 组件对性能的影响。但这些问题正在被 Istio 团队逐渐克服。从 2020 年初发布的路线图 中可以看出，Istio 已经取得了长足的进步。\n将基于虚拟机的工作负载更好地集成到服务网格中，是 Istio 团队今年的一大重点。Tetrate 还通过其产品 Tetrate Service Bridge 提供了无缝的多云连接、安全性和可观察性，包括针对虚拟机的。本文将带您了解为什么 Istio 需要与虚拟机整合，以及如何整合。\nIstio 为什么要支持虚拟机？ 虽然现在容器和 Kubernetes 已经被广泛使用，但仍然有很多部署在虚拟机上的服务和 Kubernetes 集群之外的 API 需要由 Istio mesh 来管理。如何将棕地环境与绿地环境统一管理，这是一个巨大的挑战。\n将虚拟机引入到网格中需要具备什么条件？ 在介绍如何集成虚拟机之前，我先介绍一下将虚拟机添加到 Mesh 中需要什么条件。在支持虚拟机流量时，Istio 必须知道几件事：哪些虚拟机的服务要添加到 Mesh 中，以及如何访问虚拟机。每个虚拟机还需要一个身份，以便与服务网格的其他部分安全地通信。这些需求可以和 Kubernetes CRD 一起工作，也可以和 Consul 这样的完整的服务注册表一起工作。而基于服务账户的身份引导机制，为没有平台身份的虚拟机分配工作负载身份。对于有平台身份的虚拟机（如 EC2、GCP、Azure 等），Istio 正在进行这方面的工作，将平台身份与 Kubernetes 身份进行交换，方便设置 mTLS 通信。\nIstio 如何支持虚拟机？ Istio 对虚拟机的支持始于其服务注册表机制。Istio mesh 中的服务和实例信息来自 Istio 的服务注册表，到目前为止，Istio 的服务注册表只关注或跟踪 pod。在新的版本中，Istio 现在有资源类型来跟踪和观察虚拟机。网格内的 sidecar 无法观察和控制网格外服务的流量，因为它们没有任何信息。\nIstio 社区和 Tetrate 在 Istio 对虚拟机的支持上做了很多工作 。1.6 版本中增加了 WorkloadEntry，它允许你像描述 Kubernetes 中运行的主机一样描述虚拟机。在 1.7 版本中，该版本开始增加了通过令牌将虚拟机自动引导到 service mesh 中的基础，Istio 做了大量的工作。Istio 1.8 将首次推出另一个名为 WorkloadGroup 的抽象，它类似于 Kubernetes Deployment 对象 —— 但适用于虚拟机。\n下图显示了 Istio 如何在网格中对服务进行建模。最主要的信息来源来自于 Kubernetes 这样的平台服务注册表，或者 Consul 这样的系统。此外，ServiceEntry 作为用户定义的服务注册表，对虚拟机上的服务或组织外部的服务进行建模。\nIstio 中的服务注册发现模型 为什么不直接使用 ServiceEntry 引入虚拟机中的服务，却还要大费周折在虚拟机中安装 Istio？\n使用 ServiceEntry，你可以让网格内部的服务发现和访问外部服务；此外，还可以管理这些外部服务的流量。结合 VirtualService，你还可以为相应的外部服务配置访问规则，比如请求超时、故障注入等，从而实现对指定外部服务的控制访问。即便如此，它也只能控制客户端的流量，而不能控制引入的外部服务对其他服务的访问。也就是说，它不能控制作为调用发起者的服务的行为。在虚拟机中部署 sidecar，通过工作负载选择器引入虚拟机工作负载，可以像 Kubernetes 中的 pod 一样，对虚拟机进行无差别管理。\nDemo 在下面这个 demo 中我们将使在 GKE 中部署 Istio 并运行 bookinfo 示例，其中 ratings 服务的后端使用的是部署在虚拟机上的 MySQL，该示例可以在 Istio 官方文档 中找到，我作出了部分改动，最终的流量路由如下图所示。\nBookinfo 示例中的流量示意图 安装流程 下面是示例的安装步骤：\n在 Google Cloud 中部署 Kubernetes 集群，Kubernetes 版本是 1.16.13； 在 GKE 中安装 Istio 1.7.1； 在 Google Cloud 中启动一台虚拟机并配置 Istio，将其加入到 Istio Mesh 中，这一步需要很多手动操作，生成证书、创建 token、配置 hosts 等； 在 Istio Mesh 中部署 bookinfo 示例； 在虚拟机中安装 MySQL； 为虚拟机设置 VPC 防火箱规则； 将虚拟机中的 MySQL 服务作为 ServiceEntry 引入到 Mesh 中并作为 rating 服务的后端； 修改 MySQL 表中的数据，验证 bookinfo 中的 rating 相应的行为符合预期； 未来方向 从 bookinfo 的演示中可以看出，在这个过程中涉及到的人工工作太多，很容易出错。在未来，Istio 会改进虚拟机测试的可操作性，根据平台身份自动引导，改进 DNS 支持和 istioctl 调试等。大家可以关注 Istio 环境工作组 ，了解更多关于虚拟机支持的细节。\n参考阅读 Virtual Machine Installation Virtual Machines in Single-Network Meshes Istio: Bringing VMs into the Mesh (with Cynthia Coan) Bridging Traditional and Modern Workloads ","relpermalink":"/blog/how-to-integrate-virtual-machines-into-istio-service-mesh/","summary":"将基于虚拟机的工作负载更好地集成到服务网格中，是 Istio 团队今年的一大重点。Tetrate 还通过其产品 Tetrate Service Bridge 提供了无缝的多云连接、安全性和可观察性，包括针对虚拟机的。本文将带您了解为什么 Istio 需要与虚拟机整合，以及如何整合。","title":"如何在 Istio Service Mesh 中集成虚拟机？"},{"content":"为什么写这篇文章 看到这个标题后，大家可能会问“都已经 2020 年了，Kubernetes 开源有 6 年时间了，为什么还要写一篇 Kubernetes 入门的文章？”我想说的是，Kubernetes 还远远没有达到我们想象的那么普及。众多的开发者，平时忙于各自的业务开发，学习新技术的时间有限；还有大量的学生群体，可能还仅仅停留在“知道有这门技术”的阶段，远远没有入门。这篇文章将助于各位有志于从事云原生领域工作或需要了解该领域背景的人群快速入门 Kubernetes 和云原生。\n因为云原生的知识体系过于庞杂，本文主要讲解容器、Kubernetes 及服务网格的入门概念，关于云原生的更多细节将在后续文章中推出。\n引言 Kubernetes 一词来自希腊语，意思是“飞行员”或“舵手”。这个名字很贴切，Kubernetes 可以帮助你在波涛汹涌的容器海洋中航行。\nKubernetes 是做什么的？什么是 Docker？什么是容器编排？Kubernetes 是如何工作和扩展的？你可能还有很多其他的问题，本文将一一为你解答。\n这篇文章适合初学者，尤其是那些工作忙碌，没有办法抽出太多时间来了解 Kubernetes 和云原生的开发者们，希望本文可以帮助你进入 Kubernetes 的世界。\n简而言之，Kubernetes 提供了一个平台或工具来帮助你快速协调或扩展容器化应用，特别是在 Docker 容器。让我们深入了解一下这些概念。\n容器和容器化 那么什么是容器呢？\n要讨论容器化首先要谈到虚拟机 (VM)，顾名思义，虚拟机就是可以远程连接的虚拟服务器，比如 AWS 的 EC2 或阿里云的 ECS。\n接下来，假如你要在虚拟机上运行一个网络应用——包括一个 MySQL 数据库、一个 Vue 前端和一些 Java 库，在 Ubuntu 操作系统 (OS) 上运行。你不用熟悉其中的每一个技术——你只要记住，一个应用程序由各种组件、服务和库组成，它们运行在操作系统上。\n现在，将应用程序打包成一个虚拟机镜像，这个镜像中包括了 Ubuntu 操作系统。这使得虚拟机变得非常笨重——通常有几个 G 的大小。\n虚拟机镜像包含了整个操作系统及所有的库，对应用程序来说，这个镜像过于臃肿，其中大部分组件并没有被应用程序直接调用。如果你需要重新创建、备份或扩展这个应用程序，就需要复制整个环境（虚拟机镜像），在新环境中启动应用通常需要几十秒甚至几分钟时间。如果你想单独升级应用中的某个组件，比如说 Vue 应用，就需要重建整个虚拟机镜像。另外，如果你的两个应用依赖同一个底层镜像，升级底层镜像会同时影响这两个应用，而有时候，你只需要升级其中一个应用的依赖而已。这就是所谓的“依赖陷阱”。\n解决这个问题的办法就是容器。容器是继虚拟机之后更高层次的抽象，在这层抽象中，整个应用程序的每个组件被单独打包成一个个独立的单元，这个单元就是所谓的容器。通过这种方式，可以将代码和应用服务从底层架构中分离出来，实现了完全的可移植性（在任何操作系统或环境上运行应用的能力）。所以在上面的例子中，Ubuntu 操作系统就是一个单元（容器）。MySQL 数据库是另一个容器，Vue 环境和随之而来的库也是一个容器。\n但是，MySQL 数据库是如何自己“运行”的？数据库本身肯定也要在操作系统上运行吧？没错！\n更高层次的容器，比如 MySQL 容器，实际上会包含必要的库来与底层的操作系统容器通信和集成。所以你可以把容器看成是整个应用堆栈中的一层，每层都依赖于下层的单元。而这就类似于船舶或港口中集装箱的堆叠方式，每个容器的稳定性都依赖于下面的容器的支持。所以应用容器的核心是一个受控的执行环境。它们允许你从头开始定义整个环境，从操作系统开始，到你要使用的各个版本的库，再到你要添加的代码版本。\n与容器相关的一个重要概念是微服务。将应用程序的各个组件拆分并打包成独立的服务，这样每个组件都可以很容易地被替换、升级、调试。上面的例子中，我们会为 Vue 前端创建一个微服务，为 MySQL 数据库创建另一个微服务，为 Java 中间件部分创建另一个微服务，以此类推。很明显，微服务与容器化是相辅相成的。\n从 Docker 开始 现在你已经对容器有一定了解了吧？Docker 是最常用的容器化工具，也是最流行的容器运行时。\nDocker 开源于 2013 年。用于打包和创建容器，管理基于容器的应用。所有 Linux 发行版、Windows 和 macOS 都支持 Docker。\n还有其他的容器化工具，如 CoreOS rkt 、Mesos Containerizer 和 LXC 。但是目前，绝大多数的容器化应用都是在 Docker 上运行的。\n再到 Kubernetes 首先，简单介绍一下历史。Kubernetes 是 Google 基于其内部容器调度平台 Borg 的经验开发的。2014 年开源，并作为 CNCF（云原生计算基金会）的核心发起项目。\n那么 Kubernetes 又跟容器是什么关系呢？让我们再回到上面的例子。假设我们的应用爆火，每天的注册用户越来越多。\n现在，我们需要增加后端资源，使浏览我们网站的用户在浏览页面时加载时间不会过长或者超时。最简单的方式就是增加容器的数量，然后使用负载均衡器将传入的负载（以用户请求的形式）分配给容器。\n这样做虽然行之有效，但也只能在用户规模有限的情况下使用。当用户请求达到几十万或几百万时，这种方法也是不可扩展的。你需要管理几十个也许是几百个负载均衡器，这本身就是另一个令人头疼的问题。如果我们想对网站或应用进行任何升级，也会遇到问题，因为负载均衡不会考虑到应用升级的问题。我们需要单独配置每个负载均衡器，然后升级该均衡器所服务的容器。想象一下，当你有 20 个负载均衡器和每周 5 或 6 个小的更新时，你将不得不进行大量的手工劳动。\n我们需要的是一种可以一次性将变更传递给所有受控容器的方法，同时也需要一种可以轻松地调度可用容器的方法，这个过程还必须要是自动化的，这正是 Kubernetes 所做的事情。\n接下来，我们将探讨 Kubernetes 究竟是如何工作的，它的各种组件和服务，以及更多关于如何使用 Kubernetes 来编排、管理和监控容器化环境。为了简单起见，假设我们使用的是 Docker 容器，尽管如前所述，Kubernetes 除了支持 Docker 之外，还支持其他几种容器平台。\nKubernetes 架构和组件 首先，最重要的是你需要认识到 Kubernetes 利用了“期望状态”原则。就是说，你定义了组件的期望状态，而 Kubernetes 要将它们始终调整到这个状态。\n例如，你想让你的 Web 服务器始终运行在 4 个容器中，以达到负载均衡的目的，你的数据库复制到 3 个不同的容器中，以达到冗余的目的。这就是你想要的状态。如果这 7 个容器中的任何一个出现故障，Kubernetes 引擎会检测到这一点，并自动创建出一个新的容器，以确保维持所需的状态。\n现在我们来定义一些 Kubernetes 的重要组件。\n当你第一次设置 Kubernetes 时，你会创建一个集群。所有其他组件都是集群的一部分。你也可以创建多个虚拟集群，称为命名空间 (namespace)，它们是同一个物理集群的一部分。这与你可以在同一物理服务器上创建多个虚拟机的方式非常相似。如果你不需要，也没有明确定义的命名空间，那么你的集群将在始终存在的默认命名空间中创建。\nKubernetes 运行在节点 (node) 上，节点是集群中的单个机器。如果你有自己的硬件，节点可能对应于物理机器，但更可能对应于在云中运行的虚拟机。节点是部署你的应用或服务的地方，是 Kubernetes 工作的地方。有 2 种类型的节点——master 节点和 worker 节点，所以说 Kubernetes 是主从结构的。\n主节点是一个控制其他所有节点的特殊节点。一方面，它和集群中的任何其他节点一样，这意味着它只是另一台机器或虚拟机。另一方面，它运行着控制集群其他部分的软件。它向集群中的所有其他节点发送消息，将工作分配给它们，工作节点向主节点上的 API Server 汇报。\nMaster 节点本身也包含一个名为 API Server 的组件。这个 API 是节点与控制平面通信的唯一端点。API Server 至关重要，因为这是 worker 节点和 master 节点就 pod、deployment 和所有其他 Kubernetes API 对象的状态进行通信的点。\nWoker 节点是 Kubernetes 中真正干活的节点。当你在应用中部署容器或 pod（稍后定义）时，其实是在将它们部署到 worker 节点上运行。Worker 节点托管和运行一个或多个容器的资源。\nKubernetes 中的逻辑而非物理的工作单位称为 pod。一个 pod 类似于 Docker 中的容器。记得我们在前面讲到，容器可以让你创建独立、隔离的工作单元，可以独立运行。但是要创建复杂的应用程序，比如 Web 服务器，你经常需要结合多个容器，然后在一个 pod 中一起运行和管理。这就是 pod 的设计目的——一个 pod 允许你把多个容器，并指定它们如何组合在一起来创建应用程序。而这也进一步明确了 Docker 和 Kubernetes 之间的关系——一个 Kubernetes pod 通常包含一个或多个 Docker 容器，所有的容器都作为一个单元来管理。\nKubernetes 中的 service 是一组逻辑上的 pod。把一个 service 看成是一个 pod 的逻辑分组，它提供了一个单一的 IP 地址和 DNS 名称，你可以通过它访问服务内的所有 pod。有了服务，就可以非常容易地设置和管理负载均衡，当你需要扩展 Kubernetes pod 时，这对你有很大的帮助，我们很快就会看到。\nReplicationController 或 ReplicaSet 是 Kubernetes 的另一个关键功能。它是负责实际管理 pod 生命周期的组件——当收到指令时或 pod 离线或意外停止时启动 pod，也会在收到指示时杀死 pod，也许是因为用户负载减少。所以换句话说，ReplicationController 有助于实现我们所期望的指定运行的 pod 数量的状态。\n什么是 Kubectl？ kubectl 是一个命令行工具，用于与 Kubernetes 集群和其中的 pod 通信。使用它你可以查看集群的状态，列出集群中的所有 pod，进入 pod 中执行命令等。你还可以使用 YAML 文件定义资源对象，然后使用 kubectl 将其应用到集群中。\nKubernetes 中的自动扩展 请记住，我们使用 Kubernetes 而不是直接使用 Docker 的原因之一，是因为 Kubernetes 能够自动扩展应用实例的数量以满足工作负载的需求。\n自动缩放是通过集群设置来实现的，当服务需求增加时，增加节点数量，当需求减少时，则减少节点数量。但也要记住，节点是“物理”结构——我们把“物理”放在引号里，因为要记住，很多时候，它们实际上是虚拟机。\n无论如何，节点是物理机器的事实意味着我们的云平台必须允许 Kubernetes 引擎创建新机器。各种云提供商对 Kubernetes 支持基本都满足这一点。\n我们再继续说一些概念，这次是和网络有关的。\n什么是 kubernetes Ingress 和 Egress？ 外部用户或应用程序与 Kubernetes pod 交互，就像 pod 是一个真正的服务器一样。我们需要设置安全规则允许哪些流量可以进入和离开“服务器”，就像我们为托管应用程序的服务器定义安全规则一样。\n进入 Kubernetes pod 的流量称为 Ingress，而从 pod 到集群外的出站流量称为 egress。我们创建入口策略和出口策略的目的是限制不需要的流量进入和流出服务。而这些策略也是定义 pod 使用的端口来接受传入和 …","relpermalink":"/blog/must-read-for-cloud-native-beginner/","summary":"这篇文章将助于各位有志于从事云原生领域工作或需要了解该领域背景的人群快速入门 Kubernetes 和云原生。","title":"云原生初学者入门必读"},{"content":"今天是我加入蚂蚁的第 914 天，也是在蚂蚁 的最后一天，明天就是 9 月 1 日了，这一天通常都是学校开学的日子，在阿里巴巴大家都以”同学”相称，明天我将加入 Tetrate ，这也算开始我的新学期吧！\n蚂蚁/阿里巴巴与云原生社区 迄今为止，蚂蚁/阿里巴巴集团对我的职业生涯产生了深远影响，尤其是它的企业文化和价值观，阿里巴巴招聘的理念的“寻找同路人”，在创建云原生社区的过程，不也是寻找同路人的过程吗？云原生社区 就像一个小型社会，我不求它有多大的社会价值，只求它可以对个人、对企业、对社会带来微小而美好的改变。我不断得思考作为个人、员工。尤其是社区的发起人，我的使命到底是什么？我在公司中应该担当什么样的角色？这个社区要走向何方？我在摸索中前进，但是因为有你们的支持，使我更加坚定，致力于云原生技术在中国的普及和应用，以外我一个人可能走得更快，但现在与社区在一起，我们将走得更远！\n2019 年 6 月 24 日，上海，KubeCon China 2019 2019 年 6 月 24 日，上海，KubeCon China 2019\n加入 Tetrate 在过去的两年里，我一直在着力推广 Istio 和 Service Mesh 技术，在蚂蚁集团的资助下，我创办了 ServiceMesher 社区 ，将 Service Mesh 技术带到了中国，接下来我希望将中国实践带到世界。当然还有今年疫情期间成立的云原生社区 ，向开发者和大众普及云原生知识和应用。\n作为 Developer Advocate，最重要的一点是不要停止学习，同时要善于倾听和总结。在过去的两年里，我看到无数人对 Service Mesh 表现出浓厚的兴趣，但因对新技术的风险了解的不足及知识匮乏而无从下手。我十分兴奋加入这家专注于 Service Mesh 的初创公司 Tetrate ，这是一家全球化远程办公的初创公司，公司的产品围绕开源 Istio 、Envoy 和 Apache SkyWalking 等开源项目构建，致力于打造云原生的网络基础设施。这里有这些开源项目的多位 Maintainer，如 吴晟 、Zack Butcher 、周礼赞 等，我相信跟他们一起，可以帮助大家快速、有效的了解和应用 Service Mesh，跨向云原生。\n写在最后 今年年初在筹备云原生社区的时候，我就确定了未来三年内的工作方向——云原生、开源和社区。在追求梦想的道路上充满荆棘，不仅需要勇气和毅力，还需要你们做我坚强的后盾，我一定披荆斩棘，一往无前。开源是世界的，要想让世界更理解我们，我们必须更加主动地融入这个世界。希望中国开源的明天会更好，希望 Service Mesh 技术在中国更好的落地，希望云原生能够普惠大众，希望大家都可以找到自己的使命。\nTetrate 目前也在招聘 中，欢迎投递简历。\n","relpermalink":"/blog/moving-on-from-ant-group/","summary":"今天是我在蚂蚁的最后一天，明天我就要在 Tetrate 开始新的学期了。","title":"新的开始——告别蚂蚁，加入 Tetrate"},{"content":"就在今晚，jimmysong.io 网站迁移到了阿里云香港节点，此举是为了进一步优化用户体验，提升访问速度。我在阿里云香港节点上购买了一台 ECS，现在我拥有一个公网 IP 还可以设置子域名了，之前网站是部署在 GitHub Pages 上，访问速度一般，而且还要承受 GitHub 不稳定带来的影响（近年来 GitHub 的宕机事件时有发生）。\n同时，近日网站博客也做了大量改进，感谢 白俊遥 @baijunyao 的大力支持，为网站改版做了大量工作，包括：\n修改了主题配色，对比加深 使用 aligolia，支持全站搜索 优化了移动端显示 博客中的文章增加了放大功能 在博客文章中增加了目录 本站是基于 educenter 主题构建。\n感谢广大网友几年来对本网站的支持，网站自上线以来已三年有余，拥有几百万的访问量，前后做过两次重大改版，分别是 2020 年 1 月 31 日 和 2017 年 10 月 8 日，更换了网站的主题。将来我会一如既往给大家分享更多云原生内容，欢迎大家收藏，转发，也欢迎加入云原生社区 和广大云原生开发者一起交流。\n","relpermalink":"/notice/migrating-to-alibaba-cloud/","summary":"将网站迁移到阿里云香港节点，提高网站访问速度，并获取公网 IP、子域名的便利。","title":"迁移到阿里云香港节点"},{"content":"Kubernetes 自开源至今已经走过六个年头了，云原生时代 也已到来，我关注云原生领域也四年有余了，最近开始思考云原生的未来走向，特此撰写本文作为《云原生应用白皮书》 的开篇，更多关于云原生应用的介绍请转到白皮书中浏览。\n重点 云原生基础设施已渡过了野蛮生长期，正朝着统一应用标准方向迈进。 Kubernetes 的原语无法完整描述云原生应用体系，且在资源的配置上开发与运维功能耦合严重。 Operator 在扩展了 Kubernetes 生态的同时导致云原生应用碎片化，亟需一个统一的应用定义标准。 OAM 的本质是将云原生应用定义中的研发、运维关注点分离，资源对象进行进一步抽象，化繁为简，包罗万象。 “Kubernetes 次世代”是指在 Kubernetes 成为基础设施层标准之后，云原生生态的关注点正在向应用层过度，近两年来火热的 Service Mesh 正是该过程中的一次有力探索，而基于 Kubernetes 的云原生应用架构的时代即将到来。 Kubernetes 已成为云原生应用的既定运行平台，本文以 Kubernetes 为默认平台展开，包括云原生应用的分层模型。\n云原生的不同发展阶段 Kubernetes 从开源至今已经走过快六个年头 （2014 年 6 月开源）了，可以说是 Kubernetes 的诞生开启了整个云原生的时代。我粗略的将云原生的发展划分为以下几个时期。\n云原生的发展阶段 第一阶段：孵化期（2014 年）\n2014 年，Google 开源 Kubernetes，在此之前的 2013 年，Docker 开源，DevOps、微服务已变得十分流行，云原生的概念已经初出茅庐。在开源了 Kubernetes 之后，Google 联合其他厂商发起成立了 CNCF，并将 Kubernetes 作为初创项目捐献给了 CNCF。CNCF 作为云原生的背后推手，开始推广 Kubernetes。\n第二阶段：高速发展期（2015 年 - 2016 年）\n这几年间，Kubernetes 保持着高速发展，并于 2017 年打败了 Docker Swarm、Mesos，确立了容器编排工具领导者的地位。CRD 和 Operator 模式的诞生，大大增强了 Kubernetes 的扩展性，促进了周边生态的繁荣。\n第三阶段：野蛮生长期（2017 年 - 2018 年）\n2016 年之后的云原生基本都默认运行在 Kubernetes 平台上，2017、2018 年 Google 主导的 Istio、Knative 相继开源，这些开源项目都大量利用了 Kubernetes 的 Operator 进行了扩展，Istio 刚发布时就有 50 多个 CRD 定义。Istio 号称是后 Kubernetes 时代的微服务 ，它的出现第一次使得云原生以服务（应用）为中心。Knative 是 Google 在基于 Kubernetes 之上开源的 Serverless 领域的一次尝试。2018 年 Kubernetes 正式从 CNCF 毕业 ，Prometheus、Envoy 也陆续从 CNCF 毕业。CNCF 也与 2018 年修改了 charter，对云原生进行了重定义，从原来的三要素：”应用容器化；面向微服务架构；应用支持容器的编排调度“，修改为”云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式 API“。这一年，我曾写过两篇 Kubernetes 及云原生发展的年终总结和展望，见 2017 年 和 2018 年 的预测和总结。\n第四阶段：普及推广期（2019 年至今）\n经过几年的发展，Kubernetes 已经得到的大规模的应用，云原生的概念开始深入人心，Kubernetes 号称是云原生的操作系统，基于 Operator 模式的生态大放异彩。整合 Kubernetes 和云基础设施，研发和运维关注点分离。Kubernetes 到 Service Mesh（后 Kubernetes 时代的微服务），基于 Kubernetes 的 Serverless 都在快速发展，OAM 诞生，旨在定义云原生应用标准。\nKubernetes 开辟了云原生时代 Kubernetes 开源之初就继承了 Google 内部调度系统 Borg 的经验，屏蔽掉了底层物理机、虚拟机之间的差异，经过几年时间的发展成为了容器编排标准，进而统一了 PaaS 平台的基础设施层。\n下图是 Kubernetes 原生内置的可以应用到一个 Pod 上的所有控制器、资源对象等。\nKubernetes 概念 图片来自图书 Kubernetes Patterns（O’Reilly） Kubernetes 作为云原生基础设施设计之初遵循了以下原则：\n基础设施即代码（声明式 API） 不可变基础设施 幂等性 调节器模式（Operator 的原理） 其中声明式 API 可谓开创了云原生时代的基调，而调节器模式是 Kubernetes 区别于其他云部署形式 的主要区别之一，这也为后来的 Operator 框架的诞生 打下了基础。\n声明式 API 根据声明式 API 可以做应用编排，定义组件间的依赖，通常使用人类易读的 YAML 文件来表示。但是，YAML 文件声明的字段真的就是最终的状态吗？有没有可能动态改变？\n我们在创建 Deployment 时会指定 Pod 的副本数，但是其实际副本数并不一定是一成不变的。假如集群中还有定义 HPA，那么 Pod 的副本数就可能随着一些外界因素（比如内存、CPU 使用率或者自定义 metric）而改变，而且如果集群中还有运行自定义的控制器话，那么也有可能修改应用的实例数量。在有多个控制器同时控制某个资源对象时，如何确保控制器之间不会发生冲突，资源对象的状态可预期？可以使用动态准入控制 来达到这一点。\nKubernetes 原生应用 我们都知道要想运行一个应用至少需要以下几点：\n应用的业务逻辑（代码）、运行时（可运行的二进制文件、字节码或脚本）。 应用的配置注入（配置文件、环境变量等），身份、路由、服务暴露等满足应用的安全性和可访问性。 应用的生命周期管理（各种 Controller 登场）。 可观察性、可运维、网络和资源及环境依赖、隔离性等。 下图展示了基于 Kubernetes 原语及 PaaS 平台资源的 Kubernetes 原生应用的组成。\nKubernetes 原生应用 我们都知道 Kubernetes 提供了大量的原语 ，用户可以基于这些原语来编排服务，管理应用的生命周期。上图展示的是基于 Kubernetes 原生应用可以使用的 Kubernetes 原语、扩展及平台层资源，从内向外的对象跟应用程序（业务逻辑）的关联度依次降低，到最外层基本只剩下平台资源依赖，已经与 Kubernetes 几乎没有关系了。该图里仅展示了部分资源和对象（包含阿里巴巴开源的 OpenKruise 、Istio），实际上 Operator 资源之丰富，也是 Kubernetes 生态如此繁荣的原因之一。\nKubernetes 本身的原语、资源对象、配置、常用的 CRD 扩展有几十、上百个之多。开发者需要了解这些复杂的概念吗？我只是想部署一个应用而已！不用所对于应用开发者，即使对于基础实施开发和运维人员也需要很陡峭的学习曲线才能完全掌握它。\n我将 Kubernetes 原生应用所需要的定义和资源进行了分层：\n核心层：应用逻辑、服务定义、生命周期控制； 隔离与服务访问层：资源限制与隔离、配置、身份、路由规则等； 调度层：各种调度控制器，这也是 Kubernetes 原生应用的主要扩展层； 资源层：提供网络、存储和其他平台资源； 而这些不同的层，完全可以将其职责分配给相应的人员，比如核心层是由应用程序开发者负责，将其职责分离，可以很大程度上降低开发和运维的复杂度。\n云原生应用落实到 Kubernetes 平台之上，仅仅利用 Kubernetes 的对象原语已很难描述一个复杂的应用程序，所以诞生了各种各样的 Operator，但这也仅仅解决了单个应用的定义，对于应用的打包封装则无能为力。\n同一个资源对象又有多种实现方式，比如 Ingress 就有 10 多种实现 ，PV 就更不用说，对于对于开发者究竟如何选择，平台如何管理，这都是让人很头疼的问题。而且有时候平台所提供的扩展能力还可能会有冲突，这些能力有的可能互不相干，有的可能会有正交，有的可能完全重合。且应用本身与运维特性之间存在太多耦合，不便于复用。\n资源交集动画 上图中不同颜色的方框代表不同的资源类别，红线框代表不能为一个资源同时应用该配置，否则会出现冲突，不同的颜色上面是一个动画，展示的是部分资源组合。图中仅包含了部分 Kubernetes 中的原语和 Istio 中的资源对象组合及自定义扩展，实际上用户可以根据应用的自身特点，基于 Kubernetes 原语和 CRD 创建出千变万化的组合。\n为了管理这些应用诞生出了众多的 Operator 。Kubernetes 1.7 版本以来就引入了自定义控制器 的概念，该功能可以让开发人员扩展添加新功能，更新现有的功能，并且可以自动执行一些管理任务，这些自定义的控制器就像 Kubernetes 原生的组件一样，Operator 直接使用 Kubernetes API 进行开发，也就是说它们可以根据这些控制器内部编写的自定义规则来监控集群、更改 Pods/Services、对正在运行的应用进行扩缩容。\nOperator 的本质是一种调节器模式（Reconciler Pattern）的应用，跟 Kubernetes 本身的实现模式是一样的，用于管理云原生应用，协调应用的实际状态达到预期状态。\n调节器模式的四个原则：\n所有的输入和输出都使用数据结构。 确保数据结构是不可变的。 保持资源映射简单。 使实际状态符合预期状态。 云原生应用走向碎片化 利用声明式 API 及调节器模式，理论上可以在 Kubernetes 上部署任何可声明应用，但是在 Operator 出现之前，管理 Kubernetes 上的有状态应用一直是一个难题，随着 Operator 模式的确立，该难题已得以解决，并促进了 Kubernetes 生态的进一步发展。随着该生态的繁荣，有一种碎片化的特征正在显现。\n云原生应用碎片化的体现\nOperator 模式将运维人员的反应式经验转化成基于 Reconcile 模式的代码，统一了有状态应用的管理模式，极大得扩展了 Kubernetes 应用生态。 开发者在引用 Operator 所提供的能力时没有统一的视图，加大了基础设施运维与开发者之间的沟通成本。 Operator 总体上治理松散，没有统一的管控机制，在同时应用时可能导致互相冲突或无法预期的结果发生。 有状态应用管理难题 Kubernetes 对于无状态应用的管理很出色，但是对于有状态应用就不是那么回事了。虽然 StatefulSet 可以帮助管理有状态应用，但是这还远远不够，有状态应用往往有复杂的依赖。声明式的 API 里往往要加载着大量的配置和启动脚本，才能实现一个复杂应用的 Kubernetes 化。\n例如在 2017 年初，Operator Framework 出现之前，需要使用大量的 ConfigMap、复杂的启动脚本才能在 Kubernetes 上定义 Hadoop YARN 和运行 Spark 。虽然 StatefulSet 号称可以解决有状态应用的部署问题，但是它主要是保证了 Pod 的在启动、伸缩时的顺序和使 Pod 具有稳定的标识。但是很多分布式应用来说并不仅依靠启动顺序就可以保证其状态，根据其在分布式应用中的角色不同（master/worker）而需要有大量的自定义配置，在没有 Operator 之前这些配置通常是通过一些自定义脚本来实现，这些脚本可能存在于应用镜像 …","relpermalink":"/blog/post-kubernetes-era/","summary":"Kubernetes 的次世代在于解决 Kubernetes 生态的碎片化问题。","title":"Kubernetes 次世代的云原生应用"},{"content":" 编译一次，到处运行 就在前几天，Java 刚度过了它的 25 周岁生日 ，从它诞生时就在号称”一次编写，到处运行“，但是 20 多年过去了，从程序编写出来到真正交付生产还是有很深的 gap。IT 的世界从来就不缺概念，如果一个概念无法解决问题，那就再来一层概念。Kubernetes 诞生至今也有 6 年时间了，已经来到了后 Kubernetes 时代了——云原生应用时代！\n云原生应用的发展阶段 这本白皮书将带您探索后 Kubernetes 时代的云原生应用发展路径。\n其中的重点传达的观点包括：\n云原生已渡过了野蛮生长期，正朝着统一应用标准方向迈进。 Kubernetes 的原语无法完整描述云原生应用体系，且在对资源的配置上开发与运维功能耦合严重。 Operator 在扩展了 Kubernetes 生态的同时导致云原生应用碎片化，亟需一个统一的应用定义标准。 OAM 的本质是将云原生应用的定义中的研发、运维关注点分离，资源对象进行进一步抽象，化繁为简，包罗万象。 “Kubernetes 次世代”是指在 Kubernetes 成为基础设施层标准之后，云原生生态的关注点正在向应用层过度，近两年来火热的 Service Mesh 正式该过程中的一次有力探索，而基于 Kubernetes 之上的云原生应用架构的时代即将到来。 Kubernetes 已成为云原生应用的既定运行平台，本白皮书将以 Kubernetes 为默认平台展开，包括基于 OAM 的云原生应用的分层模型的解释。\n","relpermalink":"/notice/guide-to-cloud-native-app/","summary":"带您探索后 Kubernetes 时代的云原生应用发展路径","title":"推出云原生应用白皮书"},{"content":"云原生领域近几年来群英荟萃，但缺乏一个企业中立的有组织有纪律的团体把大家团结在一起。我和我的朋友们在疫情期间，利用业余时间共同发起了这个云原生社区，希望能服务于全球华人群体，共同致力于云原生事业。社区官网 https://cloudnative.to 。\n2020 年伊始，受新冠疫情影响，全球各地的员工开启了在家办公的模式，因此人与人之间的距离感觉被拉远了。但是云原生圈子里有我们这样一群人，因为一个共同的愿景聚集到了一起，组建了社区管理委员会，并在过去的三个月里利用业余时间，齐心协力完成了社区的筹备工作。今天我们要正式宣布云原生社区正式成立了。\n关于云原生社区 云原生社区是一个有技术、有温度、有情怀的开源社区。由一群开源的狂热爱好者自发成立，秉持“共识、共治、共建、共享”的原则。社区的宗旨是：连接、中立、开源。立足中国，面向世界，企业中立，关注开源，回馈开源。\n关于云原生社区管理委员会介绍请见团队介绍页面 。\n加入云原生社区，你将获得：\n更接近源头的知识资讯 更富有价值的人际网络 更专业个性的咨询解答 更亲近意见领袖的机会 更快速高效的个人成长 更多知识分享曝光机会 更多行业人才挖掘发现 ","relpermalink":"/notice/cloud-native-community-announecement/","summary":"我和我的朋友们发起了有技术、有温度、有情怀的云原生社区。","title":"云原生社区成立"},{"content":"OAM（Open Application Model） 是阿里巴巴和微软共同开源的云原生应用规范模型，同时开源了基于 OAM 的实现 Rudr ，自 2019 年 10 月宣布开源以来截止本文发稿已经有快半年时间了。\n当前可能大部分人才刚刚开始了解 OAM，所以这篇文章将从最基础出发，为大家介绍 OAM 的诞生背景和要解决的问题，以及它在云原生生态中的作用。\nTakeaways 如果你没有兴趣或者时间阅读下面的全文，那么建议阅读下面这些核心观点：\nOAM 的本质是根据软件设计的“兴趣点分离”原则对负责的 DevOps 流程的高度抽象和封装，这背后还是“康威定律”在起作用。 OAM 仅定义云原生应用的规范，目前推出的 Rudr 可以看做是 OAM 规范的 Kubernetes 解释器（实验实现），将云原生应用定义翻译成 Kubernetes 的资源对象。 OAM 与 Crossplane 将展开合作，就 Kubernetes 式以 API 为中心的应用定义发扬光大，并深度参与 CNCF SIG App Delivery ，以共同定义云原生应用标准。 康威定律（Conway’s Law）\n康威定律 是马尔文·康威（Melvin Conway）1967 年提出的： “设计系统的架构受制于产生这些设计的组织的沟通结构。”\nOAM 简介 OAM 全称是 Open Application Model，从名称上来看它所定义的就是一种模型，同时也实现了基于 OAM 的我认为这种模型旨在定义了云原生应用的标准。\n开放（Open）：支持异构的平台、容器运行时、调度系统、云供应商、硬件配置等，总之与底层无关 应用（Application）：云原生应用 模型（Model）：定义标准，以使其与底层平台无关 顺便说下 CNCF 中的也有几个定义标准的「开源项目」，其中有的项目都已经毕业。\nSMI（Service Mesh Interface） ：服务网格接口 Cloud Events ：Serverless 中的事件标准 TUF ：更新框架标准 SPIFFE ：身份安全标准 这其中唯独没有应用标准的定义，CNCF SIG App delivery 即是要做这个的。当然既然要制定标准，自然要对不同平台和场景的逻辑做出更高级别的抽象（这也意味着你在掌握了底层逻辑的情况下还要学习更多的概念），这样才能屏蔽底层差异。本文将默认底层平台为 Kubernetes。\n是从管理大量 CRD 中汲取的经验。 业务和研发的沟通成本，比如 YAML 配置中很多字段是开发人员不关心的。 OAM 基本对象 OAM 模型中包含以下基本对象，以本文发稿时的最新 API 版本 core.oam.dev/v1alpha2 为准：\nComponent ：OAM 中最基础的对象，该配置与基础设施无关，定义负载实例的运维特性。例如一个微服务 workload 的定义。 TraitDefinition ：一个组件所需的运维策略与配置，例如环境变量、Ingress、AutoScaler、Volume 等。（注意：该对象在 apiVersion: core.oam.dev/v1alpha1 中的名称为 Trait）。 ScopeDefinition ：多个 Component 的共同边界。可以根据组件的特性或者作用域来划分 Scope，一个 Component 可能同时属于多个 Scope。 ApplicationConfiguration ：将 Component（必须）、Trait（必须）、Scope（非必须）等组合到一起形成一个完整的应用配置。 OAM API 的演变 因为 OAM 还处在发展早起，API 变化较快，以上四个对象在不同的 API 版本中的 kind 名称不同，请大家使用时注意区别。\n名称 core.oam.dev/v1alpha1 core.oam.dev/v1alpha2 Component ComponentSchematic Component Trait Trait TraitDefinition Scope Scope ScopeDefinition Application configuration ApplicationConfiguration ApplicationConfiguration 总的来说，OAM 模型对象的定义格式与 Kubernetes 对象的类型字段 相似。关于 OAM 的基本概念模型的更多信息请访问 Overview and Terminology 。\nOAM 工作原理 下图来自阿里云原生应用平台团队孙健波在 《OAM:云原生时代的应用模型与 下一代 DevOps 技术》 中的分享，OAM 的工作原理如下图所示，OAM Spec 定义了云原生应用的规范（使用一些列 CRD 定义），Rudr 可以看做是 OAM 规范的解析器，将应用定义翻译为 Kubernetes 中的资源对象。\nOAM 的原理 可以将上图分为三个层次：\n汇编层：即人工或者使用工具来根据 OAM 规范定义汇编出一个云原生应用的定义，其中包含了该应用的工作负载和运维能力配置。 转义层：汇编好的文件将打包为 YAML 文件，由 Rudr 或其他 OAM 的实现将其转义为 Kubernetes 或其他云服务（例如 Istio）上可运行的资源对象。 执行层：执行经过转义好的云平台上的资源对象并执行资源配置。 Rudr Rudr 是对 OAM v1alpha1 在 Kubernetes 环境下的实现，OAM 正在与 Crossplane 合作\nCrossplane\n使用 Kubernetes 社区开创的以 API 为中心的声明式配置和自动化方法，使基础设施和应用管理标准化。官方网站：https://crossplane.io/ 。\n安装 Rudr 请参考 Rudr 文档 安装，主要依赖以下组件：\nkubectl helm 3 Kubernetes 1.15+ 执行下面的命令安装 Rudr 和需要的 trait。\n# 克隆项目 git clone https://github.com/oam-dev/rudr.git cd rudr # 创建一个名为 oam 的 namespace kubectl create namespace oam # 安装 Rudr helm install rudr ./charts/rudr --wait -n oam # 要使用 ingress trait，推荐安装 Nginx ingress helm repo add stable https://kubernetes-charts.storage.googleapis.com/ helm install nginx-ingress stable/nginx-ingress # 要使用 autoscaler trait，安装 HorizontalPodAutoscaler helm repo add kedacore https://kedacore.github.io/charts helm repo update helm install keda kedacore/keda -n oam 查看当前 oam namespace 下的所有 pod，你会发现已创建了以下 pod。\n$ kubectl get pod -n oam NAME READY STATUS RESTARTS AGE keda-operator-b6466c989-pn25n 1/1 Running 0 63m keda-operator-metrics-apiserver-6cf88c468-k5wd8 1/1 Running 0 63m nginx-ingress-controller-787bd69d8-n6v8c 1/1 Running 15 7d nginx-ingress-default-backend-7c868597f4-vvddn 1/1 Running 2 7d rudr-c648c9b7b-knj9b 1/1 Running 7 7d 部署示例 我们使用 OAM 官方提供的教程 Tutorial: Deploy, inspect, and update a Rudr application and its components 中的 Python flask 示例，该示例基于 OAM v1alpha1 API，最新版 API 的示例可以参考 crossplane-oam-sample 。\n# 部署 Component kubectl apply -f examples/helloworld-python-component.yaml 此时 get pod 会发现并没有创建任何新的 pod，因为 examples/helloworld-python-component.yaml 文件中只定义了一个名为 helloworld-python-v1 的 ComponentSchematic，但是 ComponentSchematic 是仅仅是定义了一个组件而已，还无法直接创建 pod 的，还需要创建一个 ApplicationConfiguration 将其与 Trait 绑定才可以创建应用的 pod。\n关于该示例的详细信息请参考 Python flask 示例 的创建步骤。\n创建应用配置 在部署了 ComponentSchematic 之后我们还需要创建一个 ApplicationConfiguration 将其与 Trait 资源绑定才可以创建应用。\n当前已有的 Trait\n在安装 Rudr 时已在 oam namespace 中部署了一些 trait，使用下面的命令查看。\n$ kubectl get trait -n oam NAME AGE auto-scaler 7d1h empty 7d1h ingress 7d1h manual-scaler 7d1h volume-mounter 7d1h 在 examples/first-app-config.yaml 中将 ComponentSchematic 与 ingress Trait 联系起来。一个完整的可部署的应用配置 examples/first-app-config.yaml 的内容如下所示：\napiVersion: core.oam.dev/v1alpha1 kind: ApplicationConfiguration metadata: name: first-app spec: components: - componentName: helloworld-python-v1 # 引用了上文中的 Component instanceName: first-app-helloworld-python-v1 parameterValues: - name: target value: Rudr - name: port value: \u0026#39;9999\u0026#39; traits: - name: ingress # Ingress 引用，Rudr 已默认创建 properties: hostname: example.com path: / servicePort: 9999 执行下面的命令部署应用。\nkubectl apply -f examples/first-app-config.yaml -n oam 若此时查看 oam namespace 下的 pod 将发现有一个新的 pod 创建。\n$ kubectl get pod -o oam NAME READY STATUS RESTARTS AGE first-app-helloworld-python-v1-69945684c7-wfd82 1/1 Running 0 16m ... 测试 执行下面的命令可以测试刚安装的应用。\n# 将 Python flask 应用的 pod …","relpermalink":"/blog/oam-intro/","summary":"本文是对 OAM 及 Rudr 的初探，主要介绍了 OAM 诞生的背景和要解决的问题，同时介绍了它在云原生生态中的作用。","title":"OAM（开放应用模型）——定义云原生应用标准的野望"},{"content":"如果你刚听说 Service Mesh 不久，并试用过 Istio 的话，那么你可能都会有下面几个疑问：\n为什么 Istio 要运行在 Kubernetes 上呢？ Kubernetes 和 Service Mesh 分别在云原生中扮演什么角色？ Istio 扩展了 Kubernetes 的哪些方面？解决了哪些问题？ Kubernetes、xDS 协议（Envoy 、MOSN 等）与 Istio 之间又是什么关系？ 到底该不该上 Service Mesh？ 这一节我们将试图带您梳理清楚 Kubernetes、xDS 协议以及 Istio Service Mesh 之间的内在联系。此外，本节还将介绍 Kubernetes 中的负载均衡方式，xDS 协议对于 Service Mesh 的意义以及为什么说及时有了 Kubernetes 还需要 Istio。\n使用 Service Mesh 并不是说与 Kubernetes 决裂，而是水到渠成的事情。Kubernetes 的本质是通过声明式配置对应用进行生命周期管理，而 Service Mesh 的本质是提供应用间的流量和安全性管理以及可观察性。假如你已经使用 Kubernetes 构建了稳定的应用平台，那么如何设置服务间调用的负载均衡和流量控制？\nEnvoy 创造的 xDS 协议被众多开源软件所支持，如 Istio 、MOSN 等。Envoy 对于 Service Mesh 或云原生来说最大的贡献就是定义了 xDS，Envoy 本质上是一个 proxy，是可通过 API 配置的现代版 proxy，基于它衍生出来很多不同的使用场景，如 API Gateway、Service Mesh 中的 Sidecar proxy 和边缘代理。\n本节包含以下内容\n说明 kube-proxy 的作用。 Kubernetes 在微服务管理上的局限性。 介绍 Istio Service Mesh 的功能。 介绍 xDS 包含哪些内容。 比较 Kubernetes、Envoy 和 Istio Service Mesh 中的一些概念。 重要观点 如果你想要提前了解下文的所有内容，那么可以先阅读下面列出的本文中的一些主要观点：\nKubernetes 的本质是应用的生命周期管理，具体来说就是部署和管理（扩缩容、自动恢复、发布）。 Kubernetes 为微服务提供了可扩展、高弹性的部署和管理平台。 Service Mesh 的基础是透明代理，通过 sidecar proxy 拦截到微服务间流量后再通过控制平面配置管理微服务的行为。 Service Mesh 将流量管理从 Kubernetes 中解耦，Service Mesh 内部的流量无需 kube-proxy 组件的支持，通过为更接近微服务应用层的抽象，管理服务间的流量、安全性和可观察性。 xDS 定义了 Service Mesh 配置的协议标准。 Service Mesh 是对 Kubernetes 中的 service 更上层的抽象，它的下一步是 serverless。 Kubernetes vs Service Mesh 下图展示的是 Kubernetes 与 Service Mesh 中的的服务访问关系（每个 pod 一个 sidecar 的模式）。\nkubernetes 对比 service mesh 流量转发\nKubernetes 集群的每个节点都部署了一个 kube-proxy 组件，该组件会与 Kubernetes API Server 通信，获取集群中的 service 信息，然后设置 iptables 规则，直接将对某个 service 的请求发送到对应的 Endpoint（属于同一组 service 的 pod）上。\n服务发现\nService Mesh 中的服务注册 Istio 可以沿用 Kubernetes 中的 service 做服务注册，还可以通过控制平面的平台适配器对接其他服务发现系统，然后生成数据平面的配置（使用 CRD 声明，保存在 etcd 中），数据平面的透明代理（transparent proxy）以 sidecar 容器的形式部署在每个应用服务的 pod 中，这些 proxy 都需要请求控制平面来同步代理配置。之所以说是透明代理，是因为应用程序容器完全无感知代理的存在，该过程 kube-proxy 组件一样需要拦截流量，只不过 kube-proxy 拦截的是进出 Kubernetes 节点的流量，而 sidecar proxy 拦截的是进出该 Pod 的流量，详见理解 Istio Service Mesh 中 Envoy Sidecar 代理的路由转发 。\nService Mesh 的劣势\n因为 Kubernetes 每个节点上都会运行众多的 Pod，将原先 kube-proxy 方式的路由转发功能置于每个 pod 中，因为有 sidecar 拦截流量会多一次跳转时，增加响应延迟，同时大量的配置分发、配置同步，可能会影响应用性能。为了细粒度地进行流量管理，必将添加一系列新的抽象，从而会进一步增加用户的学习成本，但随着技术的普及，这样的情况会慢慢地得到缓解。\nService Mesh 的优势\nkube-proxy 的设置都是全局生效的，无法对每个服务做细粒度的控制，而 Service Mesh 通过 sidecar proxy 的方式将 Kubernetes 中对流量的控制从 service 一层抽离出来，可以做更多的扩展。\nkube-proxy 组件 在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。kube-proxy 负责为 Service 实现了一种 VIP（虚拟 IP）的形式。在 Kubernetes v1.0 版本，代理完全在 userspace 实现。Kubernetes v1.1 版本新增了 iptables 代理模式 ，但并不是默认的运行模式。从 Kubernetes v1.2 起，默认使用 iptables 代理。在 Kubernetes v1.8.0-beta.0 中，添加了 ipvs 代理模式 。关于 kube-proxy 组件的更多介绍请参考 kubernetes 简介：service 和 kube-proxy 原理 和 使用 IPVS 实现 Kubernetes 入口流量负载均衡 。\nkube-proxy 的缺陷 kube-proxy 的不足之处 ：\n首先，如果转发的 pod 不能正常提供服务，它不会自动尝试另一个 pod，每个 pod 都有一个健康检查的机制，当有 pod 健康状况有问题时，kubelet 会重启对应的 pod，kube-proxy 会删除对应的转发规则。另外，nodePort 类型的服务也无法添加 TLS 或者更复杂的报文路由机制。\nKube-proxy 实现了流量在 Kubernetes service 多个 pod 实例间的负载均衡，但是如何对这些 service 间的流量做细粒度的控制，比如按照百分比划分流量到不同的应用版本（这些应用都属于同一个 service，但位于不同的 deployment 上），做金丝雀发布（灰度发布）和蓝绿发布？Kubernetes 社区给出了 使用 Deployment 做金丝雀发布的方法 ，该方法本质上就是通过修改 pod 的 label 来将不同的 pod 划归到 Deployment 的 Service 上。\nKubernetes Ingress vs Istio Gateway 上文说到 kube-proxy 只能路由 Kubernetes 集群内部的流量，而我们知道 Kubernetes 集群的 Pod 位于 CNI 创建的网络中，集群外部是无法直接与其通信的，因此 Kubernetes 中创建了 ingress 这个资源对象，它由位于 Kubernetes 边缘节点 （这样的节点可以是很多个也可以是一组）的 Ingress controller 驱动，负责管理南北向流量，Ingress 必须对接各种 Ingress Controller 才能使用，比如 nginx ingress controller 、traefik 。Ingress 只适用于 HTTP 流量，使用方式也很简单，只能对 service、port、HTTP 路径等有限字段匹配来路由流量，这导致它无法路由如 MySQL、Redis 和各种私有 RPC 等 TCP 流量。要想直接路由南北向的流量，只能使用 Service 的 LoadBalancer 或 NodePort，前者需要云厂商支持，后者需要进行额外的端口管理。有些 Ingress controller 支持暴露 TCP 和 UDP 服务，但是只能使用 Service 来暴露，Ingress 本身是不支持的，例如 nginx ingress controller ，服务暴露的端口是通过创建 ConfigMap 的方式来配置的。\nIstio Gateway 的功能与 Kubernetes Ingress 类似，都是负责集群的南北向流量。Istio Gateway 描述的负载均衡器用于承载进出网格边缘的连接。该规范中描述了一系列开放端口和这些端口所使用的协议、负载均衡的 SNI 配置等内容。Gateway 是一种 CRD 扩展 ，它同时复用了 sidecar proxy 的能力，详细配置请参考 Istio 官网 。\nxDS 协议 下面这张图大家在了解 Service Mesh 的时候可能都看到过，每个方块代表一个服务的实例，例如 Kubernetes 中的一个 Pod（其中包含了 sidecar proxy），xDS 协议控制了 Istio Service Mesh 中所有流量的具体行为，即将下图中的方块链接到了一起。\nService Mesh 示意图 xDS 协议是由 Envoy 提出的，在 Envoy v2 版本 API 中最原始的 xDS 协议指的是 CDS（Cluster Discovery Service）、EDS（Endpoint Discovery service）、LDS（Listener Discovery Service）和 RDS（Route Discovery Service），后来在 v3 版本中又发展出了 Scoped Route Discovery Service（SRDS）、Virtual Host Discovery Service（VHDS）、Secret Discovery Service（SDS）、Runtime Discovery Service（RTDS）等，详见 xDS REST and gRPC protocol 。\n下面我们以各有两个实例的 service，来看下 xDS 协议。\nxDS 协议 上图中的箭头不是流量进入 Proxy 后的路径或路由，也不是实际顺序，而是想象的一种 xDS 接口处理顺序，其实 xDS 之间也是有交叉引用的。\n支持 xDS 协议的代理通过查询文件或管理服务器来动态发现资源。概括地讲，对应的发现服务及其相应的 API 被称作 xDS。Envoy 通过 订阅（subscription） 的方式来获取资源，订阅方式有以下三种：\n文件订阅：监控指定路径下的文件，发现动态资源的最简单方式就是将其保存于文件，并将路径配置在 ConfigSource 中的 path 参数中。 gRPC 流式订阅：每个 xDS API 可以单独配置 ApiConfigSource ，指向对应的上游管理服务器的集群地址。 轮询 REST-JSON 轮询订阅：单个 xDS API 可对 REST 端点进行的同步（长）轮询。 以上的 xDS 订阅方式详情请参考 xDS 协议解析 。Istio 使用 gRPC 流式订阅的方式配置所有的数据平面的 sidecar proxy。\nxDS 协议要点 最后总结下关于 xDS  …","relpermalink":"/blog/service-mesh-the-microservices-in-post-kubernetes-era/","summary":"本文是以前所写内容的重新修订并收录于 ServiceMesher 社区的 Istio Handbook 中，其他章节仍在编纂中。","title":"Service Mesh——后 Kubernetes 时代的微服务"},{"content":" 在这个最孤独的春节里，进行一场最深刻的修行。\n2020 年，的确是开局不利。就在短短不到一个月的时间里，我几乎没听到过一条好消息：\n特朗普暗杀伊朗少将苏莱曼尼；武汉爆发的这次肺炎；我当年的篮球偶像科比因直升机坠毁去世的消息着实让我震惊，湖人队 24 号永别了。这让我在原本就兴致缺缺的春节中，又多了一次精神打击。\n2020 年，必定是要被深重载入全人类记忆的一年。就在这一年的第一个月最后几天，我决定将网站改版，一是因为延长的春节假期又不能出门，在家实在无聊；且网站上很多图片使用的是微博图床保存，因该图床不稳定，导致很多照片已无法挽回；再加上一个每到长假就想整理网站的习惯（网站的上一次改版是 2018 年国庆假期，在家闭关 7 天完成的），因此我决定将网站再次改版，就成了现在的样子。\n特性 这次改版后网站有如下特性：\n重新梳理网站内容，结构更加合理 支持邮件订阅 图片使用 Github 存储 更换 Hugo 主题为 educenter-hugo ，并做了大量修改 响应式静态网站，卡片式设计，用户体验更好 增加了 Google 自定义搜索 增加了 RSS 订阅，见页面底部 ","relpermalink":"/notice/website-revision-notice/","summary":"2020 年的第一个月最后几天，我决定将网站改版。","title":"jimmysong.io 网站改版通知"},{"content":"在前几天中秋节期间我翻译了一遍 Google 于 Github 上开源的 Google’s Engineering Practices documentation ，原文档 Github 地址： https://github.com/google/eng-practices ，目前为止的主要内容为 Google 总结的如何进行 Code Review（代码审查） 指南，根据原 Github 仓库的标题判断以后会追加更多 Google 工程实践的内容。\nGithub：https://github.com/rootsongjc/eng-practices 在线浏览：https://jimmysong.io/eng-practices 译文采用跟原文档同样的样式及目录结构，其实如果原文做了国际化要求的话可以直接合入的，但是根据之前的几个人提交的翻译建议，此项目作者并不建议进行翻译，一是翻译的文档可能会无人维护，再就是无法保证文档的准确定。\n中文合入相关建议见：\nAdd Chinese translation #12 Add the link of Chinese version of code reviewer’s guide #8 ","relpermalink":"/notice/google-engineering-practices-zh/","summary":"翻译自 Google 于 Github 上开源的文档","title":"谷歌工程实践文档中文版"},{"content":"《云原生服务网格 Istio：原理、实践、架构与源码解析（张超盟、章鑫、徐中虎、徐飞编著）》 是 2019 年国内出版的第四本 Istio 相关图书，前三本分别是：\n深入浅出 Istio：Service Mesh 快速入门与实践，崔秀龙 著 Service Mesh 实战：用 Istio 软负载实现服务网格，周遥 著 Istio 入门与实战，毛广献 著 在这四本书刚上市时我都获得了作者的赠书，这本书是由四位华为的同学编写，于 2019 年 7 月第一次印刷，全书共 24 章，606 页，售价 139 元。我是在 KubeCon China 2019 的上海大会现场张超盟亲手赠与我的，张超盟也是 2018 年第三届 Service Mesh Meetup 的讲师。\n右侧是云原生服务网格 Istio（华为云原生技术丛书）作者之一张超盟 本书结构 全书共分四个篇章，24 个章节，606 页，每个章节的页数占比统计如下图所示。\n云原生服务网格 Istio：原理、实践、架构与源码解析》图书章节页数占全书百分比 - 表格 《云原生服务网格 Istio：原理、实践、架构与源码解析》图书章节页数占全书百分比 - 饼图 从统计结果中可以看出书中第 3 章（非侵入的流量治理）、第 14 章（司令官 Pilot）一共占全书的页数百分比为 24%，几乎占了四分之一的篇幅。\n这本书是目前（2019 年 08 月 15 日）市面上能买到的最全的一本 Istio 相关的图书了，话说国外还一本 Istio 的书也出来，国内到现在都出了四本了，是不是有种墙外开花墙内香的感觉？\n建议大家结合 Istio 官方文档 一起来看这本书，Istio 版本更新虽然没有 Kubernetes 那么快，但是在本书发行一个多月后也要发布 1.2 版本了，欢迎大家加入 ServiceMesher 社区 来学习 Istio！\n","relpermalink":"/blog/cloud-native-service-mesh-istio-book/","summary":"云原生服务网格 Istio（华为云原生技术丛书）图书读后感，原理、实践、架构与源码解析（张超盟、章鑫、徐中虎、徐飞编著）。","title":"云原生服务网格 Istio 图书"},{"content":"本文介绍如何为 ServiceMesher.com 网站配置自动化部署的详细说明，通过本文你将了解到：\n如何使用 GitHub Webhook 来自动化发布您的网站 如何配置 Nginx 代理根据 URI 请求转发到本地服务器的指定端口 自动发布脚本 使用名为 deploy.sh 的 Shell 脚本编译 Hugo 生成 HTML 文件，并放到 Nginx 配置的目录下。该脚本位于 ServiceMesher 官网 GitHub 仓库 同级目录下，内容如下：\n#!/bin/bash # 网站的代码仓库目录 input=\u0026#34;website\u0026#34; # Nginx 中配置的网站的 HTML 根目录 output=\u0026#34;/home/admin/servicemesher.com\u0026#34; cd $input git pull hugo cd .. cp -r $input/public/* $output 依赖安装 该网站部署在阿里云上，操作系统为 CentOS 7.6.1810，并配置好了 HTTPS 。\n安装后端服务配置所需的组件。\nyum install -y npm 安装 NPM 包。\nnpm i -S github-webhook-handler npm i -g pm2 创建 webhook 服务后端 我们使用 NodeJS 创建 webhook 服务后端，后端代码保存在 webhook.js文件中，调用 deploy.sh 来发布，因此需要与 deploy.sh 文件在同一级目录中，监听 http://127.0.0.1:6666/webhook：\n当前的所有文件的结构如下：\n$ ls -1 deploy.sh node_modules package.json sofastack.tech webhook.js webhook.js 文件内容如下：\nvar http = require(\u0026#39;http\u0026#39;); var spawn = require(\u0026#39;child_process\u0026#39;).spawn; var createHandler = require(\u0026#39;github-webhook-handler\u0026#39;); // 注意将 secret 修改你自己的 var handler = createHandler({ path: \u0026#39;/webhook\u0026#39;, secret: \u0026#39;yourwebhooksecret\u0026#39; }); http.createServer(function (req, res) { handler(req, res, function (err) { res.statusCode = 404; res.end(\u0026#39;no such location\u0026#39;); }) }).listen(6666); handler.on(\u0026#39;error\u0026#39;, function (err) { console.error(\u0026#39;Error:\u0026#39;, err.message) }); handler.on(\u0026#39;push\u0026#39;, function (event) { console.log(\u0026#39;Received a push event for %s to %s\u0026#39;, event.payload.repository.name, event.payload.ref); runCommand(\u0026#39;sh\u0026#39;, [\u0026#39;./deploy.sh\u0026#39;], function( txt ){ console.log(txt); }); }); function runCommand( cmd, args, callback ){ var child = spawn( cmd, args ); var resp = \u0026#39;Deploy OK\u0026#39;; child.stdout.on(\u0026#39;data\u0026#39;, function( buffer ){ resp += buffer.toString(); }); child.stdout.on(\u0026#39;end\u0026#39;, function(){ callback( resp ) }); } 在 webhook.js 所在目录下启动后端服务：\npm2 start webhook.js 查看服务状态：\n$ pm2 status ┌──────────┬────┬─────────┬──────┬───────┬────────┬─────────┬────────┬─────┬───────────┬──────┬──────────┐ │ App name │ id │ version │ mode │ pid │ status │ restart │ uptime │ cpu │ mem │ user │ watching │ ├──────────┼────┼─────────┼──────┼───────┼────────┼─────────┼────────┼─────┼───────────┼──────┼──────────┤ │ webhook │ 0 │ 1.0.0 │ fork │ 30366 │ online │ 0 │ 6h │ 0% │ 30.8 MB │ root │ disabled │ └──────────┴────┴─────────┴──────┴───────┴────────┴─────────┴────────┴─────┴───────────┴──────┴──────────┘ Use `pm2 show \u0026lt;id|name\u0026gt;` to get more details about an app 使用 pm2 logs webhook 可以查看后端服务日志。\nNginx 配置 在 nginx 配置中增加转发设置，将对网站 /webhook URI 的访问转发到服务器本地的 6666 端口，即 webhook 后端服务商。\n# GitHub auto deploy webhook location /webhook { proxy_pass http://127.0.0.1:6666; } GitHub Webhook 配置 在 GitHub 仓库的 Settings - webhooks 设置中创建一个新的 webhook。\nGitHub Webhook 配置 注意选择 Content Type 为 application/json，secret 设置成与 webhook.js 中的相同。\n配置完成后 GitHub 将自动调用 Webhook 以验证有效性。\nGitHub 自动触发 Webhook 如果看到 200 响应表示成功调用 Webhook 后端服务，这样每次我们的仓库合并后就会触发网站自动部署。\n更多 为了加强 GitHub 自动化，还有更多 GitHub App 可以使用，推荐：\nauto-assigin mergify 这些已经在 servicemesher.com 网站上集成了，感兴趣的读者可以访问 ServiceMesher 官网的代码仓库 查看配置。\n参考 使用 Github 的 webhooks 进行网站自动化部署 ","relpermalink":"/blog/github-webhook-website-auto-deploy/","summary":"通过本文你将了解到如何使用 GitHub Webhook 来自动化发布您的网站。","title":"使用 GitHub Webhook 实现静态网站自动化部署"},{"content":"我的博客从上线第一天起就使用了 HTTPS，用的是 Cloudflare ，直接在其后台配置即可。如果你是用 nginx、apache、haproxy 等服务器来运行自己的网站，给大家推荐 Certbot ，可以自动化来配置 SSL 证书和定时更新。\n下面记录我自己为 servicemesher.com 网站配置 HTTPS 证书的过程，全程不需要 5 分钟。\n环境 网站的托管环境如下：\nOS：CentOS 7.6 阿里云 网站服务器：Nginx，使用 yum 安装，版本 1.12 提前配置好 Nginx，确保使用 HTTP 先可以访问到网站 注意：请使用 yum 命令安装 nginx，这样可以确保 nginx 安装在默认的位置，因为 certbot 会检测 /etc/nginx/ 目录下的配置文件。\n操作步骤 执行下面的步骤可以直接为你的网站配置 HTTPS 证书。\nyum -y install yum-utils yum-config-manager --enable rhui-REGION-rhel-server-extras rhui-REGION-rhel-server-optional yum install certbot python2-certbot-nginx 下图是在 Certbot 中选择服务器和操作系统的页面。\nCertBot 页面 执行下面的命令，根据提示会自动配置 nginx。\ncertbot --nginx Saving debug log to /var/log/letsencrypt/letsencrypt.log Plugins selected: Authenticator nginx, Installer nginx Starting new HTTPS connection (1): acme-v02.api.letsencrypt.org Which names would you like to activate HTTPS for? 1：servicemesher.com 2: www.servicemsher.com # 这里直接回车选择所有的域名 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Select the appropriate numbers separated by commas and/or spaces, or leave input blank to select all options shown (Enter \u0026#39;c\u0026#39; to cancel): - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - You have an existing certificate that contains a portion of the domains you requested (ref: /etc/letsencrypt/renewal/servicemesher.com.conf) It contains these names: servicemesher.com, www.servicemesher.com You requested these names for the new certificate: servicemesher.com, prow.servicemesher.com, www.servicemesher.com. Do you want to expand and replace this existing certificate with the new certificate? - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - (E)xpand/(C)ancel: E Renewing an existing certificate Performing the following challenges: http-01 challenge for prow.servicemesher.com Waiting for verification... Cleaning up challenges Deploying Certificate to VirtualHost /etc/nginx/nginx.conf Deploying Certificate to VirtualHost /etc/nginx/nginx.conf Deploying Certificate to VirtualHost /etc/nginx/nginx.conf Please choose whether or not to redirect HTTP traffic to HTTPS, removing HTTP access. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 1: No redirect - Make no further changes to the webserver configuration. 2: Redirect - Make all requests redirect to secure HTTPS access. Choose this for new sites, or if you\u0026#39;re confident your site works on HTTPS. You can undo this change by editing your web server\u0026#39;s configuration. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Select the appropriate number [1-2] then [enter] (press \u0026#39;c\u0026#39; to cancel): # 这里是为了扩展证书支持更多的域名，所有输入 2 回车 Traffic on port 80 already redirecting to ssl in /etc/nginx/nginx.conf Redirecting all traffic on port 80 to ssl in /etc/nginx/nginx.conf Traffic on port 80 already redirecting to ssl in /etc/nginx/nginx.conf - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Your existing certificate has been successfully renewed, and the new certificate has been installed. 然后重新加载配置。\nnginx -t;nginx -s reload 设置证书自动更新。\necho \u0026#34;0 0,12 * * * root python -c \u0026#39;import random; import time; time.sleep(random.random() * 3600)\u0026#39; \u0026amp;\u0026amp; certbot renew\u0026#34; | sudo tee -a /etc/crontab \u0026gt; /dev/null 好了现在访问你的网站就可以看到 https 头部加了 HTTPS 锁了。\n参考 让网站永久拥有 HTTPS - 申请免费 SSL 证书并自动续期 certbot - 免费的 https 证书 ","relpermalink":"/blog/free-certificates-with-certbot/","summary":"实测推荐使用 Certbot 为网站设置永久免费的 HTTPS 证书，超简单，全程不用五分钟！","title":"使用 Certbot 为网站设置永久免费的 HTTPS 证书"},{"content":"总体概述 开源软件合规（Compliance）实践，从狭义上讲就是企业使用开源软件许可证（License）的合规。Recommended Open Source Compliance Practices for the Enterprise 电子书（共 32 页）由 Ibrahim Haddad 博士撰写，本书从以下几个角度为你的公司的进行开源合规实践以指导：\n创建开源审查委员会（Open Source Review Committee） 代码扫描 软件溯源（Software Sourcing） 开源法务支持（Open Source Legal Support） 流程中的合规检查（Compliance Checkpoints） 开发和部署检查器 合规事项看板 开源合规审查组 企业为了保证自己产品或软件的合规，通常会有一个许可证合规审查组，负责以下几项职责：\n遵守开源许可条款 促进在产品和服务中使用开源 遵守第三方商业软件的许可条款 保护您的产品/服务差异化（知识产权/IP） 开源促进及合规计划 开源合规流程 图 2 是开源合规流程闭环。\n图 2. 开源合规流程闭环 该流程中分为以下三步：确认（Identify）、批准（Approve）和确知（Satisfy）。\n图 3. 开源合规步骤产出 确认（Identify） 此初始步骤的目标是监控软件组合中开源来源，无论该组合是作为独立软件包还是嵌入在第三方或公司开发的软件中。此步骤的输出是详细的软件物料清单（Bill of Materials），用于标识所有开源软件包（Package）和代码片段（Snippet）的来源（Origin）、许可证（License）以及由软件组合分析工具所识别的许可冲突。\n批准（Approve） 这一步的目标是：\n查看上一步的输出，了解管理相关源代码的使用、修改和分发的许可证； 根据其独特的背景（context），确定是否批准使用已识别的开源软件； 确知（Satisfy） 在最后一步中，准备好所有已批准的开源软件（整个组件和片段）的许可证、版权（copyright）和归属声明，并将其交给相关的部门，以包含在产品文档中。同样，已经确知和标记了许可义务的开源软件包，就可以在产品/服务上线时发布了。\n对企业开源合规实践的建议 作者提出了企业可以实施的实践建议，以改进和加强其开源合规性计划：\n成立开源审查委员会（Open Source Review Board，简称 OSRB） 建立自动化系统来识别开源软件 让软件供应商遵守开源许可证 扩展开源法律支持 在业务和开发过程中集成开源合规性检查点 提供各种开源合规性任务的清单（checklist） 开发和部署支持清单 建立开源合规活动基准（benchmark） 参与关键合规性开源合规性计划 成立开源审查委员会 除开源合规官或开源计划办公室的代表外，开源审查委员会（OSRB）由法律和产品/工程团队的代表组成。OSRB 的主要职责是审查和批准计划在产品和服务中使用开源软件。\n下面是 OSRB 中每个参与者职责的宏观概述。\n开源法律顾问（Open Source Legal Counsel）\n审查和批准开源软件的使用、修改和分发。 提供与开源许可相关的法律指导。 为创建合规培训做出贡献。 有助于改进合规计划。 审查和批准与许可证合规性相关的 Web 门户的内容。 审核并批准使用的开源软件的义务列表。 从开源合规性角度签署产品发布。 产品/工程代表（Product / Engineering Representative）\n在其组织内实施合规性政策（policy）和流程（process）。 在软件开发过程中集成合规性实践。 从工程角度为改进合规计划做出贡献。 遵循技术开源合规指南。 与 OSRB 成员协作以响应合规性查询。 进行设计、架构和代码审查。 准备用于出版物的开源软件包和通知。 从开源合规性角度签署产品发布。 开源计划办公室和合规官的代表（Representative of the Open Source Program Office or Compliance Officer）\n推动开源合规活动。 协调源代码扫描和审核。 参与工程审查和分发准备评估。 协调开源软件包和通知的发布。 为培训工作和改进合规计划做出贡献。 有助于促进开源软件的自动化和发现。 从开源合规性角度签署产品发布。 除了 OSRB 的成员之外，实现开源合规是一项跨学科活动，涉及到组织内的各个部门和个人，如图 4 所示。\n图 4. 开源项目办公室组成 下面是对支持团队帮助 OSRB 确保开源合规性的核心职责的描述。\nIT\n创建/获取确保合规性所需的新工具 为合规计划使用的工具和自动化基础设施提供支持和维护 企业发展（Cooperate Development）\n请求/监督开源合规性尽职调查在合并或收购之前完成 从外包开发中心接收源代码时确保合规性记录 文档（Documentation）\n在产品文档中包含开源许可证信息和通知（notice） 开源执行委员会（Open Source Executive Committee）\n审查并批准在开源许可下发布专有源代码的提议 软件采购（Software Procurement）\n授权第三方软件提供商在许可或购买的软件组件中披露开源 协助引入包含（或不包含）开源软件的第三方软件 开源代码审查 开源合规性工作的核心是识别开源代码及其各自的许可证，以便组织可以满足适用的许可证义务。开源策略和流程指导此核心活动。合规性政策和流程管理开源软件的使用、贡献、审核和发布的各个方面。如果我们采用下图 5 所示的基本流程并对其进行扩展，我们将考虑端到端的合规流程。下图显示了这样一个流程，它具有源自多个源的源代码输入。源代码经过一系列步骤，流程的最终输出包括书面报价、通知列表（版权、归属、许可证），以及为履行许可义务而发布的源代码包。\n图 5 提供了端到端合规流程的详细示例，其中包括软件组件被 OSRB 批准在构建系统中与软件产品集成之前经历的各个步骤。\n图 5. 端到端开源合规流程示例 图 6 简要描述了每个步骤中发生的情况。\n图 6. 开源合规代码确认步骤详解 开源代码溯源 让您的软件提供商参与开源合规中至关重要。软件提供商必须披露其可交付成果中包含的开源代码，并提供包括适用源代码在内的所有通知（notice）。\n图 7. 多源开发模型 图 7 描绘了多源开发模型和传入源代码的各种源组合。在此模型下，产品或软件堆栈可以包含专有软件、第三方商业和第三方开源软件的任意组合。例如，除了第三方专有源代码之外，软件组件 A 可以包括专有源代码，而软件组件 B 除了可以包含来自开源项目的源代码之外还可以包括专有源代码。\n当今的公司处于必须更新其供应链（软件采购）程序以解决获取和使用开源软件的状态。通常会有供应链人员参与将软件从供应商转移到贵公司。他们可以通过两种主要方式支持开源合规性活动：\n要求第三方软件提供商披露他们在其可交付成果中使用的任何开源，以及 协助许可与开源软件包捆绑在一起或与之集成的第三方软件。 此领域的推荐做法是强制第三方软件提供商披露其产品中使用的所有开源组件，并声明他们计划如何满足适用的开源许可证义务。如果第三方软件包含开源，供应链必须确保在初始入口后满足开源许可证义务——您作为提供开源产品或服务的分销商将承担这些义务和责任。\n提供便捷的法务支持 大多数组织都会创建开源合规性计划并建立核心团队以确保合规性。大多数公司往往会会遇到开源法律支持的瓶颈，因为您公司里可能有成百上千的使用和集成开源代码的开发人员，而很少有法务人员提供所需的法律支持。扩展开源法律支持需要一些开箱即用的思考，但可以借助以下实用方法实现。\n许可证手册（License Playbooks） 提供面向软件开发人员的易于阅读和摘要的开源许可证摘要。提供有关这些许可证的易于理解的信息，例如许可证授予、限制、义务、专利影响等。使用开源软件许可证手册可以大量减少发送给法律顾问的基本问题的数量，并为开发人员提供了对常见查询的即时指导、信息和答案。\n许可证兼容性矩阵（License Compatibility Matrix） 许可证兼容性是指确定某个许可证是否与另一个许可证兼容。GPL 兼容性是指确定某个许可证是否与 GPL 条款兼容。当合并源自不兼容许可下软件组件的源代码时，开发团队经常会遇到许可兼容性问题。当开发团队将不同许可证下的代码组合在一起时，可以参考许可证兼容性矩阵来验证在单个软件组件中是否存在加入源代码的许可冲突。如果开发团队使用的许可证源不在矩阵中，则可以后续获得法律顾问的建议。\n许可证分类 为了减少开源法律顾问收到的问题数量并增加许可和合规流程教育，一些公司选择在几个类别下对其产品中最常用的许可进行分类。图 11 显示了许可证分类系统的一个简单示例，其中大多数使用的开源许可证分为四类。\n图 8. 开源许可证分类（仅供参考） 上述许可证类别是对许可证进行分类的简单方法，使开发人员在根据这些许可证集成代码时更容易了解操作过程。下面这个例子是开发人员想要使用在以下许可下的开源软件包的：\nLicense A - Action：尽管用，没有什么问题 License E - Action：获得工程经理的批准 License I - Action：获得法律顾问的批准 License M - Action：根据政策禁止适用该 License 其他 - Action：向经理询问行动方案 有关此话题的进一步阅读，我们建议阅读**扩展开源法律支持的实用建议 **。本文探讨了法律顾问在确保开源合规方面的作用，并为法律顾问提供了可以为软件开发团队提供的实用建议。这些实用建议将使软件开发人员能够做出与开源许可相关的日常决策，而无需再去找负责每个问题的法律顾问。\n开源合规流程中的检查点及发布清单 有必要将合规性实践纳入开发流程，以确保开源合规工作的成功。您可以通过多种方式实现这一目标。\n每个内部版本的合规性：更新流程管理，以确保在产品开发周期中尽早包含开源合规性活动，以使组织能够满足其发布时间表。遵循此模型，未来版本的增量合规性也变得简单明了。 更新供应链程序：定制供应链的供应商选择程序，以确保在对供应商及其可交付成果进行尽职调查（Due Diligence）时考虑开源合规性要求。 执行验证：使用验证步骤确保在发生发行外部版本之前满足所有合规性要求。 培训员工：为所有员工提供开源合规培训。 采用 SPDX 报告许可证信息：以 SDPX 格式提供许可证信息，以尽量减少任何可能的错误，并标准化报告信息的方式。 SPDX\nSPDX® （Software Package Data Exchange®）是用于传达软件物料清单信息（包括组件、许可证、版权和安全参考）的开放标准。\nSPDX 通过为公司和社区提供共享格式来共享软件许可、版权和安全参考的重要数据，从而简化和改进合规性，从而减少冗余工作。\nSPDX 规范由 SPDX 工作组开发，该工作组由 Linux 基金会托管。基层工作包括来自 20 多个组织的代表——软件、系统和工具供应商、基金会和系统集成商——都致力于为软件包数据交换格式创建标准。\n开发和部署清单 清单很有用，可确保执行合规性任务的一致性和完整性。强烈建议根据员工职责建立合规里程碑清单和目标清单。\n清单的示例包括：\n批准将传入代码集成到产品的源代码存储库之前的核对表 确保履行义务的清单 开发人员的清单 工程经理的清单 合规人员清单 开源法律人员的清单 软件采购人员清单 为了说明这一点，我们提供了一个示例清单，展示了在组织发布源代码包之前必须检查的各种任务，以履行在交付产品中包含的开源代码的许可义务：\n预发行清单（Pre-Distribution Checklist）\n验证引入开源软件包的修改是否已记录，并作为更改日志的一部分包含在开源发行说明中。 确保每个修改后的源代码文件都包含版权声明，免责声明和通用“更改日志”（Changelog）条目的附加条目。 确认源代码 …","relpermalink":"/blog/open-source-compliance-practices/","summary":"本文是开源软件合规实践的，介绍开源软件合规的流程及建议。","title":"开源软件合规实践"},{"content":"本书的主旨是：如果没有成熟的 DevOps 实践，云原生是玩转不起来的。DevOps 已经不是什么新鲜的话题，但到底什么是 “Cloud Native DevOps”及如何实践 Cloud Native DevOps，这正是本书要探讨的内容。\nDevOps 正在经历一次转型，从自动化构建到声明式基础设施、微服务和 Serverless。大部分人对云原生存在误解，以为云原生就是运行在云上，其实云原生更偏向于一种理念，即应用的定义及架构方式，而不是将应用运行在哪里。而云上的 DevOps 与传统的 DevOps 有什么区别，开发者和运维人员在云原生时代如何转型？也许本书会给你答案。\n关于本书 本书是由 TheNewStack 出品的免费电子书，可以在 TheNewStack 网站 上获取本书的电子版，同时推荐 TheNewStack 的电子书系列 ，囊括了容器、微服务、Kubernetes、云原生诸多主题，可以作为企业决策的参考读物。\n本书是 TheNewStack 编辑集结 DevOps 领域的专家在各种大会上的发言、演讲，有很多观点引用，并结合了一些调查问卷数据展示了一幅云原生 DevOps 的趋势与全景图，下文中我会找一些代表性的观点和图表来说明。\n下面是本书目录，一共分为三大部分：构建、部署和管理，其中前两个部分还给出了参考书目、示例研究等。\n云原生 DevOps 目录 谁适合读这本书 IT 经理、CIO、团队领导者，希望规划自己公司或团队的云原生化 DevOps 的实践路径以面对大规模场景。\n云原生化的 DevOps 云原生是对业务价值和团队功能的重构。\n云原生化的 DevOps 在应用的管理上与原始的 DevOps 最大的区别就是——使用 YAML 文件配置的声明式基础设施（Declarative infrastructure）与应用程序的代码本身放在同一个存储库中，这些 代码 将由开发团队来维护，而运维团队的职能将转变为基础设施的构建者，服务安全性、健壮性、可见性及耐用性的守护者。\nAWS 的 Serverless 布道师 Chris Munns 早已甚至预测到 2025 年非云供应商的运维人员将不复存在，虽然听上去有点危言耸听，但这也是为传统 IT 运维人员的职业生涯敲响的警钟。\n云原生 DevOps 高亮部分 开发接手了原来传统运维的一些职责，如配置和发布，减少了每次发布的成本，而运维的职责向管理整个系统的复杂性转变，例如转变为 SRE（Site Reliability Engineer）。\n工作流自动化的价值 DevOps 的原始教义：DevOps 不是一种工具或流程，而是一种重视整个组织的持续沟通、协作、集成和自动化的实践。\n工作流自动化的五个案例 根据自动化的驱动力及持续时间的长短，将 Workflow Automation 划分为五个类别。\n业务流程自动化 分布式系统通信 分布式事务 编排 决策自动化 运维需要做出的转变 Damon Edwards 提出于运维需要面对的四个灾难（圣经启示录中的四骑士 ）：\nSilos（孤岛） Ticket queues（无尽的低效的工单） Toil（干脏活累活的辛勤） Low trust（低信任度） 要向云原生 DevOps 转变就要克服以上几个问题。\nDevOps 领域的扩展 本书第三章中提到 DevOps 的领域扩展到 Security 和 Networking。\n为了维持合规的编程语言 容器镜像扫描 基于策略的网络安全 金丝雀测试 运行时的威胁检测 日志分析 ","relpermalink":"/blog/cloud-native-devops-book/","summary":"本书是 TheNewStack 编辑集结 DevOps 领域的专家在各种大会上的发言、演讲编纂而成。","title":"TheNewStack 云原生 Devops 报告解读"},{"content":"很多从事开源人可能会注意到有些开源项目要求贡献者在提交 PR 前首先签署 CLA，只有签署了 CLA 之后 PR 才可以合并。\n开源贡献协议简介 下面列举了开源贡献协议的一些简介：\n开源贡献协议有 CLA（Contributor License Agreement）和 DCO （Developer Certificate of Origin）两种； DCO 由 Linux Foundation 提出，是固定的简短条文（只有 4 条），旨在让贡献者保证遵守开源 license； CLA 是对开源 license 的法律性质补充，由法务制定； CLA 可以自定义，不论是个人还是企业级签署的时候都需要提供详细的信息，如姓名、公司、邮箱、地址、电话等； 下表中对比了 CLA 和 DCO 的特性，推荐大型跨公司开源项目使用 CLA，利用项目更加正规和长久发展； 开源社区的贡献者协议一般分为两种 CLA 和 DCO，这两种协议各有优缺点如下。\n特性 CLA DCO 社区属性 弱 强 签署方式 一次性 每次提交时在 commit 信息里追加 Signed-off-by: email 信息 法律责任 明确法律义务 无声明，用来限制提交者遵守开源 license 是否可自定义 公司或组织可自行定义 否 使用案例 Google、Pivotal、CNCF、阿里巴巴、Apache SkyWalking GitLab、Chef、Harbor、TiKV 公司属性 强，可以签署公司级别的 CLA 弱 什么是 CLA CLA 是 Contributor License Agreement 的缩写，CLA 可以看做是对开源软件本身采用的开源协议的补充。一般分为公司级和个人级别的 CLA，所谓公司级即某公司代表签署 CLA 后即可代表该公司所有员工都签署了该 CLA，而个人级别 CLA 只代表个人认可该 CLA。\nCLA 包含哪些内容？ 因为 CLA 是每个公司或组织自定义的，在细节上可能稍有不同，不过总体都包含以下内容：\n关于签署该 CLA 的主体和贡献的定义； 授予著作权给拥有该软件知识产权的公司或组织； 专利许可的授予； 签署者保证依法有权授予上述许可； 签署者确保所有的贡献内容均为原创作品； 签署者为贡献内容支持的免责描述； 说明贡献者提交非原创作品应该采用的方式； 保证在获悉任何方面不准确的事实或情况之时通知签约方； 对于主体在中国的企业，还加入了一些本地化的内容，如 Alibaba Open Source Individual CLA 。\n因为 CLA 分别为个人级和公司级，所以对于不同名义签署时需要提供不同的信息。签署个人级 CLA 的时候需要提供个人信息（姓名、地址、邮箱、电话等），签署公司级 CLA 还需要提供公司信息（名称、地址、联系电话、邮箱、传真等）；\n什么是 DCO DCO 是 Developer Certificate of Origin 的缩写，由 Linux Foundation 于 2004 年制定。DCO 最大的优点是可以减轻开发者贡献的阻碍，不用阅读冗长的 CLA 法律条文，只需要在提交的时候签署邮件地址即可。Chef 和 GitLab 已分别于 2016 年和 2017 年从 CLA 迁移到 DCO。\n如 CNCF 的 Sandbox 项目 harbor 就是使用的 DCO。\nDCO 目前是 1.1 版本，内容很简单，开源项目的贡献者只需要保证以下四点：\n该贡献全部或部分由我创建，我有权根据文件中指明的开源许可提交；要么 该贡献是基于以前的工作，这些工作属于适当的开源许可，无论这些工作全部还是部分由我完成，我有权根据相同的开源许可证（除非我被允许根据不同的许可证提交）提交修改后的工作；要么 该贡献由 1、2、或 3 证明的其他人直接提供给我，而我没有对其进行修改。 我理解并同意该项目和贡献是公开的，并且该贡献的记录（包括我随之提交的所有个人信息，包括我的签字）将无限期保留，并且可以与本项目或涉及的开源许可证保持一致或者重新分配。 CLA vs DCO Kubernetes 社区中有过讨论将 Kubernetes 贡献者从 CLA 迁移到 DCO，最后 TOC 成员 Tim Hockin 觉得签署 CLA 对于贡献者只需要痛苦一次，每次提交都签署 DCO 是持续的痛苦，因此最后还是坚持使用 CLA。参考Move from CLA to DCO #2649 。\n2018 年 CNCF 对其托管的项目的 Maintainer 做了调研，从反馈来看，Maintainer 对 DCO 是存在痛点的，并希望 CNCF 投入更多的 PR 和市场力量来对抗具有全职 PR/marketing 的初创公司。\n如果为了更注重个人贡献者，考虑社区属性，可以使用 DCO，这样对于开源项目的管理者来说就不用指定复杂的 CLA 了，但是对于大型项目由众多合作方的项目，建议使用 CLA。\n阿里巴巴 CLA 阿里巴巴只提供个人级别的 CLA 签署：https://cla-assistant.io/alibaba/weex CLA 内容见：https://github.com/aliyun/cla 阿里巴巴的 CLA 是参照 Apache CLA 撰写的，最后加上两条补充，协议受中国杭州的法院监管，同时提供双语版本，如中引文版本有冲突以英文版本为准。\nGoogle CLA Google 的 CLA 也是仿照 Apache CLA 撰写的，Google 开源的一些列项目如 Istio、TensorFlow、Knative 等都是需要签署 Google CLA 。\n要贡献者授予 Google 以及其他软件用户贡献内容的版权以及内容背后的专利权。贡献者不要因为版权和专利权诉讼 Google 和其他软件用户。 明确贡献的原创性。不要因为贡献者的不适当抄袭行为，导致 Google 和其他软件使用者被诉讼。 签署公司级别 CLA 的人要能代表所在公司的所有贡献者。 维护贡献者列表的不一定是跟签署该协议的是同一个人，签名者可以指定一个人来管理。 参考：解读：Google Software Grant and Corporate Contributor License Agreement Pivotal CLA Pivotal 的 CLA 也是仿照 Apache CLA 撰写的，唯一增加了一点是协议受美国加州法律监管。签署个人级协议的时候需要提供姓名、邮箱、邮寄地址（可选）、国家（可选）、电话（可选），签署公司级别的 CLA 的条款了还增加了一条对于签名者必须有权利代表整个公司，要求的信息也更加详细，包括姓名、邮箱、邮寄地址、国家、电话、公司名称、GitHub 组织、头衔等。参与贡献 Pivotal 主导的 Spring 社区和 CloudFoundry 里的项目需要签署 Pivotal CLA 。\n建议 如果你的开源项目可能会有公司间合作或者要贡献给基金会，为了防范法律风险，请直接使用 CLA；如果更看重社区内的合作，可以使用 DCO。\n参考 Individual Contributor License Agreement (“Agreement”) V2.0 Move from CLA to DCO #2649 - github.com Alphabet CLA Policy and Rationale - opensource.google.com The Apache Software Foundation Software Grant and Corporate Contributor License Agreement (“Agreement”) - apache.org Alibaba Open Source Individual CLA - github.com ","relpermalink":"/blog/open-source-cla/","summary":"很多从事开源人可能会注意到有些开源项目要求贡献者在提交 PR 前首先签署 CLA。","title":"开源社区贡献者协议 CLA 介绍"},{"content":"前段时间看了一篇文章为什么中国没有 Apache 基金会这样的组织？ ，二叉树视频中采访了开源社 的理事长老刘，他的一番话也让我很受启发，在关注和参与 CNCF 基金会这几年来我也有很多收获，有一点就是了解到了一个开源社区（基金会）治理的规则。\n虽然 CNCF 没有 Apache、GNOME、FreeBSD 历史那么悠久，但是它成立的短短几年内就成功的运作了 Kubernetes 这样的超大型开源项目，一定有其可取之处。今天我就来给大家分享下 CNCF 基金会的开源项目治理规则和组织架构，还有如何将一个开源项目加入到 CNCF。\nCNCF 根据“鸿沟理论 ”将其托管的项目分成三个成熟阶段，并设置了项目晋级到更高阶段的标准。\n“鸿沟理论 ”是由 Geoffrey A. Moore 提出的高科技产品的市场营销理论。新技术要想跨越鸿沟，必须能够实现一些跨越式的发展，拥有某一些以前不可能实现的功能，具有某种内在价值并能够赢得非技术人员的青睐。\nCNCF 项目的成熟度分类 图片来自 https://www.cncf.io/projects/ 开源项目如何加入 CNCF 开源项目所支持的公司成为 CNCF 会员 开源项目满足 CNCF 的要求（见后文） 在 GitHub 上提交proposal （GitHub Issue）列举项目介绍、现状、目标、license、用户与社区等 由 Chris Aniszczyk 安排该项目在某个 TOC 双月会议上介绍给 TOC 成员 1.TOC 会将开源项目指定到某个 SIG 中 项目获得两个 TOC 成员的赞成可进入sandbox （也可以直接获得 2/3 多数 TOC 投票进入 Incubating 状态） 知识产权转移给 CNCF CNCF 安排博客撰写、PR 等 每年一次评审，晋升到 incubating需要2/3的 TOC 成员投票赞成；至少 3 家用户成功在生产上使用；通过 TOC 的尽职调查；贡献者数量健康稳定 Sandbox 中的项目没有时效性质，可能永远都无法进入 incubating 状态，被 CNCF 谨慎宣传 CNCF 开源项目成熟度演进 CNCF 的开源项目遵循如下图所示的成熟度演进。\nCNCF 项目成熟度级别 关于上图的一些说明：\n加入 Sandbox 只需要 2 个 TOC 成员赞成 成熟一点的项目可以直接进入 incubating 阶段，但是 CNCF 会控制不同阶段的项目比例 晋级到 Incubating 或 Graduated 需要至少2/3的 TOC 成员（6 名或以上）投票赞成 每年将评审一次 目前处于沙箱、孵化中、已毕业项目的数量比例为 5:16:13，详见 https://cncf.io/projects 。其中沙箱（sandbox）项目因为其处于早期阶段并没有直接在上面的链接页面中列出，而是一个单独的 Sandbox 页面，因为 CNCF 为 sandbox 阶段的项目会谨慎背书。\n纳入 CNCF 开源版图的项目需要符合其对云原生的定义 CNCF 中托管的开源项目要符合云原生定义：\n云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式 API。 这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。 云原生计算基金会（CNCF）致力于培育和维护一个厂商中立的开源生态系统，来推广云原生技术。我们通过将最前沿的模式民主化，让这些创新为大众所用。 项目运作流程 下图演示了开源项目加入 CNCF 后的整个运作流程。\nCNCF 中的项目运作 开源项目如何加入 CNCF 开源项目所支持的公司成为 CNCF 会员 开源项目满足 CNCF 的要求（见后文） 在 GitHub 上提交proposal （GitHub Issue）列举项目介绍、现状、目标、license、用户与社区等 由 Chris Aniszczyk 安排该项目在某个 TOC 双月会议上介绍给 TOC 成员 1.TOC 会将开源项目指定到某个 SIG 中 项目获得两个 TOC 成员的赞成可进入sandbox （也可以直接获得 2/3 多数 TOC 投票进入 Incubating 状态） 知识产权转移给 CNCF CNCF 安排博客撰写、PR 等 每年一次评审，晋升到 incubating需要2/3的 TOC 成员投票赞成；至少 3 家用户成功在生产上使用；通过 TOC 的尽职调查；贡献者数量健康稳定 Sandbox 中的项目没有时效性质，可能永远都无法进入 incubating 状态，被 CNCF 谨慎宣传 开源项目加入 CNCF 的最低要求（Sandbox） 一个开源项目要想加入 CNCF 必须满足以下要求：\n项目名称必须在 CNCF 中唯一 项目描述（用途、价值、起源、历史） 与 CNCF 章程一致的声明 来自 TOC 的 sponsor（项目辅导） license（默认为 Apache 2） 源码控制（Github） 网站（英文） 外部依赖（包括 license） 成熟度模型评估（参考 开源项目加入 CNCF Sandbox 的要求 ） 创始 committer（贡献项目的时长） 基础设施需求（CI/CNCF 集群） 沟通渠道（slack、irc、邮件列表） issue 追踪（GitHub） 发布方法和机制 社交媒体账号 社区规模和已有的赞助商 svg 格式的项目 logo 由 Sandbox 升级到 Incubating 的要求 通过 TOC 的尽职调查 至少有 3 个独立的终端用户在在生产上使用该项目：一般在项目的官网列举实际用户 足够健康数量的贡献者：项目的 GitHub 上有明确的 committer 权限划分、职责说明及成员列表，TOC 将根据项目大小来确认多少 committer 才算健康 展示项目在持续进行、良好的发布节奏、贡献频率十分重要 由 Incubating 升级到 Graduated 的要求 满足 Sandbox 和 Incubating 的所有要求 至少有来自两个组织的贡献者 明确定义的项目治理及 committer 身份、权限管理 接受 CNCF 的行为准则 ，参考Prometheus 获得 CII 最佳实践徽章 在项目主库或项目官网有公开的采用者的 logo 参考归档的 Review：https://github.com/cncf/toc/tree/master/reviews 参考 鸿沟理论 - jianshu.com CNCF Graduation Criteria v1.2 - github.com 为什么中国没有 Apache 基金会这样的组织？ - infoq.cn 开源社首页 - kaiyuanshe.cn ","relpermalink":"/blog/contribute-project-to-cncf/","summary":" CNCF 基金会的开源项目治理规则和组织架构，还有如何将一个开源项目加入到 CNCF。","title":"如何将一个开源项目加入 CNCF？"},{"content":"下面这段是发布说明，来自 Istio 官方博客 https://istio.io/zh/blog/2019/announcing-1.1/ ，是我翻译的。\nIstio 于北京时间今日凌晨 4 点，太平洋时间下午 1 点 Istio 1.1 发布。\n自从去年 7 月份 1.0 发布以来，为了帮助人们将 Istio 投入生产我们做了很多工作。我们不出所料得发布了很多补丁（到目前为止已经发布了 6 个补丁！），但我们也在努力为产品添加新功能。\n1.1 版本的主题是”Enterprise Ready“（企业级就绪）。我们很高兴看到越来越多的公司在生产中使用 Istio，但是随着一些大公司加入进来，Istio 也遇到了一些瓶颈。\n我们关注的主要领域包括性能和可扩展性。随着人们将 Istio 逐步投入生产，使用更大的集群以更高的容量运行更多服务，可能会遇到了一些扩展和性能问题。Sidecar 占用了太多资源增加了太多的延迟。控制平面（尤其是 Pilot）过度耗费资源。\n我们投入了很多精力在使数据平面和控制平面更有效率上。在 1.1 的性能测试中，我们观察到 sidecar 处理 1000 rps 通常需要 0.5 个 vCPU。单个 Pilot 实例能够处理 1000 个服务（以及 2000 个 pod），需要消耗 1.5 个 vCPU 和 2GB 内存。Sidecar 在第 50 百分位增加 5 毫秒，在第 99 百分位增加 10 毫秒（执行策略将增加延迟）。\n我们也完成了命名空间隔离的工作。您可以使用 Kubernetes 命名空间来强制控制边界以确保团队之间不会相互干扰。\n我们还改进了多集群功能和可用性。我们听取了社区的意见，改进了流量控制和策略的默认设置。我们引入了一个名为 Galley 的新组件。Galley 验证 YAML 配置，减少了配置错误的可能性。Galley 还用在多集群设置中——从每个 Kubernetes 集群中收集服务发现信息。我们还支持了其他多集群拓扑，包括单控制平面和多个同步控制平面，而无需扁平网络支持。\n更多信息和详情请查看发布说明 。\n该项目还有更多进展。众所周知 Istio 有许多可移动部件，它们承担了太多工作。为了解决这个问题，我们最近成立了 Usability Working Group（可用性工作组） （可随时加入）。社区会议 （周四上午 11 点）和工作组里也发生了很多事情。您可以使用 GitHub 凭据登录 discuss.istio.io 参与讨论！\n感谢在过去几个月里为 Istio 作出贡献的所有人——修补 1.0，为 1.1 增加功能以及最近在 1.1 上进行的大量测试。特别感谢那些与我们合作安装和升级到早期版本，帮助我们在发布之前发现问题的公司和用户。\n最后，去浏览最新文档，安装 1.1 版本吧！Happy meshing！\n官方网站 ServiceMesher 社区从 Istio 0.6 版本起一直在维护 Istio 官方文档的中文页面 ，截止 2019 年 3 月 19 日晚 12 点已有 596 个 PR 合并，共维护文档 310 余篇，感谢大家的努力！部分文档可能稍微滞后于英文版本，同步工作持续进行中，参与进来请访问 https://github.com/servicemesher/istio-official-translation Istio 官网每个页面右侧都有切换语言按钮，大家可以随时切换中英文版本，还可以提交文档修改，报告网站 Bug 等。\nServiceMesher 社区网站\nServiceMesher 社区网站 http://www.servicemesher.com 上涵盖了所有 Service Mesh 领域的技术文章，并适时发布最新活动，是您一站式了解 Service Mesh 和参与社区的入口。\n","relpermalink":"/notice/istio-11/","summary":"北京时间 3 月 20 日凌晨 4 点 Istio 1.1 发布了，该版本历时 8 个月！ServiceMesher 社区同时推出了 Istio 中文文档。","title":"Istio 1.1 发布"},{"content":"Istio handbook 原是我创作的一本开源电子书（见 https://jimmysong.io/istio-handbook ）在捐献给 ServiceMesher 社区之前已经撰写了 8 个月，为了进一步普及 Istio 和 Service Mesh 技术将次书捐献给社区共同撰写。2019 年 3 月 10 日完成了原书内容的迁移至 https://github.com/servicemesher/istio-handbook ，原书将不再更新。\nGitHub 地址：https://github.com/servicemesher/istio-handbook 在线阅读地址：http://www.servicemesher.com/istio-handbook/ 本书概念图，封面图片上海静安寺夜景 ，Jimmy Song 摄。\n本书发行版权归属于电子工业出版社博文视点，未经授权请勿私自印刷发行。\nIstio 是由 Google、IBM、Lyft 等共同开源的 Service Mesh（服务网格）框架，于 2017 年初开始进入大众视野，作为云原生时代下承 Kubernetes、上接 Serverless 架构的重要基础设施层，地位至关重要。ServiceMesher 社区 作为中国最早的一批在研究和推广 Service Mesh 技术的开源社区决定整合社区资源，合作撰写一本开源电子书以飨读者。\n关于本书 本书起源于 rootsongjc/istio-handbook 及 ServiceMesher 社区创作的 Istio 知识图谱 ，集结社区力量合作创作而成。\n本书基于 Istio 1.0+ 版本编写，包含但不限于 Istio 知识图谱 中的主题。\n参与本书 请参阅本书写作规范 ，加入 ServiceMesher 社区 后进入 Slack channel 讨论。\n","relpermalink":"/notice/istio-handbook-by-servicemesher/","summary":"为了进一步普及 Istio 和 Service Mesh 技术将此书捐献给社区共同撰写。","title":"捐献 Istio Handbook 给 ServiceMesher 社区"},{"content":"2019 年 2 月初，CNCF 发布了 2018 年的年度报告，这是 CNCF 继 2017 年度报告之后，第二次发布年度报告，2017 年度的报告只有区区 14 页，今年的报告长度增长了一倍达 31 页。下面我将带大家一起来深度解读下这份 2018 年的年度报告，一窥 CNCF 过去一年里在推广云原生的道路上取得的进展。\n注：本文最后附上了 2017 年和 2018 年度的报告下载地址。\nCNCF 年度报告涵盖的范围 在解读 CNCF 的 2018 年度报告之前，我们先简单回顾下2017 年度的报告 ，因为 2017 年度报告是 CNCF 的首份年度报告，这样我们也能更好的了解 CNCF 的来龙去脉。\n2017 年度报告已经基本确定了 CNCF 每个年度报告所包含的主题：\n自我定位 会员参与情况 终端用户社区 项目更新 会议和活动 社区 培训和认证 以上为 CNCF 主要的市场活动，2017 年时其成立的第二年，经过一年时间的筹备，这一年里各种市场活动都已经开始确立并有声有色的开展了起来，包括 KubeCon、成员单位、终端用户都已经发展起来了，以后历年里只是对其不断的发展和完善。\n2018 年度报告中又新增了一些主题，这些主题是从 2018 年开始开展的，包括：\n项目更新与满意度调查 给 CNCF 项目的维护者发调查问卷询问满意度 CNCF charter 的修订（2018 年 11 月） 项目更新与发布 项目服务与支援 专项活动、文档、网站与博客支持 本地化、IT 支持和培训 社区拓展 社区奖项 CNCF Meetup CNCF Ambassador 计划 卡通吉祥物 Phippy 生态系统工具 devstats CNCF Landscape 和路线图 项目 logo 物料 测试一致性项目 国际化 进入中国 本地化网站 详情请大家从本文最后的链接下载报告原文以查看详情。\nCNCF 的定位 CNCF（云原生计算基金会）成立于 2015 年 12 月 11 日，每届年度报告的开篇都会阐明 CNCF 的定位，CNCF 的自我定位在 2018 年发生了一次变动，这也说明基金会是跟随市场形势而动，其定位不是一成不变的，其中的变化暗含着 CNCF 战略的转变。\nCNCF 的 2017 年度定位 2017 年度报告 中是这样正式介绍自己的：\nThe Cloud Native Computing Foundation (CNCF) is an open source software foundation dedicated to making cloud-native computing universal and sustainable. Cloud-native computing uses an open source software stack to deploy applications as microservices, packaging each part into its own container, and dynamically orchestrating those containers to optimize resource utilization. Cloud-native technologies enable software developers to build great products faster.\nWe are a community of open source projects, including Kubernetes, Envoy and Prometheus. Kubernetes and other CNCF projects are some of the highest velocity projects in the history of open source.\n可以看到介绍中的重点技术是：微服务、容器、动态编排。而在 2018 年 CNCF 对自己进行了重新的定位和包装，增加了新的内容。\nCNCF 的 2018 年度定位 2018 年度报告 中 CNCF 对自己的定位是：\nThe Cloud Native Computing Foundation (CNCF) is an open source software foundation dedicated to making cloud native computing universal and sustainable. Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach.\nWe are a community of open source projects, including Kubernetes, Prometheus, Envoy, and many others. Kubernetes and other CNCF projects are some of the highest velocity projects in the history of open source.\n我们可以看到其表述中更加注重多云环境，主要涉及的技术比 2017 年多了 Service Mesh（服务网格）、不可变基础设施和声明式 API。\n数读报告 CNCF 年度报告的原文主要是汇报了 CNCF 一年来的所展开的活动和进展，下表示根据 CNCF 2017 和 2018 年度报告整理了关键数据。\nYear 2016 2017 2018 Members 63 170 365 Contributors - 18687 47358 CNCF Meetup Members - 53925 89112 Projects 4 14 32 End User Community Members - 32 69 Conference and Events Participants - 4085 - Certified Kubernetes Partners - 44 - Certified Kubernetes Service Providers - 28 74 CNCF Ambassador - - 65 Kubernetes Training Partners - - 18 注：其中 2016 年是 CNCF 正式开始工作的第一年，大部分数据因为活动尚未开展而缺失。\n从上表中我们可以看到 CNCF 诞生三年来基金会成员规模、托管项目的贡献者、参加 CNCF 名义的 Meetup 的人数取得较大范围的增长，尤其是 2018 年，因为基金会成员的爆发式增长（+130%），CNCF 开始给成员分级，会员级别、费用和权益也在 CNCF 官网 上明码标价。\n2018 年 CNCF 组织的 KubeCon\u0026amp;CloudNativeCon 开始固定每年在西欧、北美和中国举行，且 2018 年是首次进入中国；原来的 Certified Kubernetes Partners 也取消了变成了 Certified Kubernetes Service Providers；CNCF 的 Ambassador 计划拥有了来自 15 个国家的 65 位 Ambassador，在世界各地为云原生布道；CNCF 还首次引入了 Kubernetes Training Partner。\n2018 年 CNCF 又推出了一系列新的认证（CKA 为 2017 年推出），包括：\nCKA （Kubernetes 管理员认证）：这是 CNCF 最早制定的一个证书，顾名思义，通过该认证证明用户具有管理 Kubernetes 集群的技能、知识和能力。虽然该证书在 2017 年即推出，但 2018 年对考试做了更细致的指导。KCSP 要求企业必须有至少三人通过 CKA。 CKAD （Kubernetes 应用开发者认证）：该认证证明用户可以为 Kubernetes 设计、构建、配置和发布云原生应用程序。经过认证的 Kubernetes Application Developer 可以定义应用程序资源并使用核心原语来构建、监控 Kubernetes 中可伸缩应用程序和排除故障。 KCSP （Kubernetes 服务提供商认证）：截止本文发稿时共有 74 家企业通过该认证。该认证的主体是企业或组织，通过 KCSP 的企业意味着可以为其他组织提供 Kubernetes 支持、咨询、专业服务和培训。通过该认证的中国企业有：灵雀云、阿里云、博云、才云、DaoCloud、EasyStack、易建科技、精灵云、谐云科技、华为、时速云、星号科技、睿云智合、沃趣、元鼎科技、ZTE。 Certified Kubernetes Conformance （Kubernetes 一致性认证）：通过该认证的 Kubernetes 提供商所提供的服务，意味着其可以保证 Kubernetes API 的可移植性及跨云的互操作性；及时更新到最新的 Kubernetes 版本；是否一致是可以通过运行开源脚本 验证的。截止本文发稿通过该认证的中国企业的发行版有：灵雀云（ACE、ACP、AKS）、才云 Compass、华为 FusionStage、酷栈科技 CStack MiaoYun、Daocloud Enterprise、新智认知新氦云、浪潮云、京东 TIG、网易云、七牛云、同方有云、睿云智合 WiseCloud；通过认证的中国企业托管平台有：阿里云、百度云、博云、EasyStack、易建科技、谐云科技、华为云 CCE、腾讯云 TKE、时速云、ZTE TECS。 以上是 CNCF 提供的主要证书，一般通过 KCSP 的企业都要先通过 Kubernetes 一致性认证，而通过 Kubernetes 一致性认证不一定要同时通过 KCSP，所以我们看到很多通过 Kubernetes 一致性认证的企业就不一定会通过 KCSP，因为 KCSP 的要求更多，至少要成为 CNCF 会员才可以。\n下面将就 CNCF 会员、托管项目的成熟度等级划分、Kubernetes 服务提供商认证和 Kubernetes 提供商认证做详细说明。\nCNCF 会员 2018 年 CNCF 的会员单位经历了爆发式增长，从 170 家增长到 365 家。CNCF 制定了如下的会员等级：\nSilver Member Gold Member Platinum Member Academic/Nonprofit Member End User Member 不同等级的会员需要交纳的年费与权益不同，详情请见 https://www.cncf.io/about/join/ 。\n成为 CNCF 会员的好处 成为 CNCF 会员包括但不限于如下好处：\n将可以参与 CNCF 市场委员会、CNCF Webinar、在 CNCF 和 Kubernetes 官网发表博客、博客被 KubeWeekly 收录、 获得 KubeCon + CloudNativeCon 的门票折扣和参与大会的市场活动 对于 Kubernetes 系列认证如 KCSP、入选 TOC 也要求必须成为 CNCF 会员才可以获得 End User Case Study 有机会加入 Ambassador 计划 在社区里具有更多的话语权，例如 CNCF 在全球范围内组织的活动 项目成熟度等级 自 2015 …","relpermalink":"/blog/cncf-annual-report-2018-review/","summary":"本文是对 CNCF（云原生计算基金会）2018 年年度报告的解读。","title":"CNCF 年度报告解读（2018 年）"},{"content":"最近在回顾 Service Mesh 技术在 2018 年的发展，想再看看 Linkerd，正好杨彰显的这本《Service Mesh 实战——基于 Linkerd 和 Kubernetes 的微服务实践》上市发售了，机械工业出版社的编辑送了我一本，🙏杨福川编辑，我看了下抽空写了点读后感，我看了下抽空写了点读后感，其实也说不上是读后感，就当是自己的一点感悟吧，就当拿此书借题发挥吧，这个知识爆炸的年代，技术发展如此迅速，可以说是 IT 人员的幸运，也是不幸！有多少写开源软件的书推出一版后能撑过三年的？如果软件红得发紫，持续迭代 N 个版本，例如 Kubernetes，最近两年以每三个月一个版本的速度迭代，之前的书早就跟不上节奏，要么就要不断推出新版，直到软件稳定后不再有大的改动。还有种可能就是软件推广和发展的不理想，无人问津，写这样软件的书就不会有再版了。\n拿到本书后我的第一反应就是看看这本书定稿的时候 Istio 是什么版本，Linkerd 又是什么版本。因为在这一年内两款开源软件都有较大的版本变动，如果书籍定稿的时候基于的软件版本太低，软件架构可能会有较大的变化，影响书中示例和部分章节的时效性。这也是大多技术书籍名短的症结所在，技术发展是在太快，传统的书籍出版流程往往过于繁琐和冗长，等到书籍出版后所介绍的软件都出了好几个版本。例如 Kubernetes 这种的软件，每三个月一个版本，而写一般书从策划到发行少说半年，一般也要一年的时间。\n关于书籍定稿时的软件版本 Istio 0.8\n本书第一章「Service Mesh 简介」对 Service Mesh 相关开源产品介绍时提到本书定稿时 Istio 是 0.8 版本，而 Istio 在 2018 年 7 月 31 日发布了 1.0 版本 。\n这本书定稿时，Istio 的最新版本是 0.8。\nLinkerd 1.3.6\n本书从序言开始一直到第二章结束也没有提及写作时基于的 Linkerd 版本，我在第二章的安装步骤中看到了说明。\n可以看到本书写作时是基于 Linkerd 1.3.6 版本，而 Linkerd 在同年的 9 月 18 日发布了 2.0 GA ，这一版本跟 1.x 版本相比有重大变化——它还将项目从集群范围的 service mesh 转换为可组合的 service sidecar ，旨在为开发人员和服务所有者提供在云原生环境中成功所需的关键工具。\nLinkerd vs Envoy Linkerd 2.0 的 service sidecar 设计使开发人员和服务所有者能够在他们的服务上运行 Linkerd，提供自动可观察性、可靠性和运行时诊断，而无需更改配置或代码。通过提供轻量级的增量路径来获得平台范围的遥测、安全性和可靠性的传统 service mesh 功能，service sidecar 方法还降低了平台所有者和系统架构师的风险。该版本还用 Rust 重写了代理部分，在延迟，吞吐量和资源消耗方面产生了数量级的改进。\n而 Linkerd 1.x 继承自 Twitter 开源的 Finagle 高性能 RPC，所有想要深度学习 Linkerd 1.x 还需要了解 Finagle，这就跟 Istio 将 Envoy 作为默认的数据平面一样，要想深度学习 Istio 必须了解 Envoy。\n二者几乎使用了完全不同的术语，假如你已经了解了 Envoy 想要再切换到 Linkerd 上，那么就要再费很多心力来学习它的概念和原理，例如如下这些术语或配置（Linkerd 中独有的配置）：\ndtab（委托表）：由一系列路由组成，由一系列路由规则组成，以逻辑路径为输入，然后经过路由规则做一系列转换生成具体名字。这是 Linkerd 路由机制的根本，就像 Envoy 中的 xDS 协议 一样，本书的第四章「深入 Linkerd 数据访问流」专门讲解了 dtab 的实现机制。 dentry（委托表记录）：委托表的每条路由规则称为 dentry，如 /consul =\u0026gt; /#/io.l5d.consul/dc1。 namer：配置 Linkerd 支持的服务发现工具。 namerd：Linkerd 的控制平面，相当于 Istio 中的 Pilot，对接各种服务发现。当然 Linkerd 也可以直接与某个服务发现平台对接如 consul，而不使用 namerd 这个集中路由和配置管理组件。 interpreter：interpreter 决定如何解析服务名字和客户端名字。 虽然 Linkerd 也是 CNCF 中的项目 ，但它目前还处于孵化阶段，而 Envoy 的 xDS 协议 已经被众多开源项目所支持，如 Istio 、SOFAMesh 、NginxMesh 等，且 Envoy 已经从 CNCF 中毕业，以后可能成为 Service Mesh 领域的标准协议，Linkerd 的生存状况堪忧。\n关于本书 本书中所有示例都提供了虚拟机的快速上手环境，只要使用 Vagrant 即可创建虚拟机和应用，所以在本书的示例代码 有大量的 Vagrantfile。\n本书第三部分「实战篇」花了大量篇幅（本书一半的页数）来讲解如何使用 Linkerd 和 Kubernetes 来管理微服务，可以参考我 2017 年 8 月 1 日写的这篇微服务管理框架 service mesh——Linkerd 安装试用笔记 ，那时候还是基于 Linkerd 1.1.2，还有 Linkerd 官方示例 ，这些示例基本都不怎么更新了。\n因为该书定稿时所基于的 Linkerd 版本距离本书发售时的 Linkerd 已经落后一个大版本（最新版本是 Linkerd 2.1 ），所以读者一定要注意这一点，老实说我只花了两个夜晚快速过了一下本书，无法对本书内容给出具体评论，所以本书是否是你所需要的就要你自己去思考了。\n","relpermalink":"/blog/service-mesh-in-action-by-yangzhangxian-review/","summary":"顺便对比了下 Linkerd 和 Envoy，给读者一些我自己的建议。","title":"《Service Mesh 实战—基于 Linkerd 和 Kubernetes 的微服务实践》读后感"},{"content":"Envoy 是 Istio Service Mesh 中默认的 Sidecar，Istio 在 Enovy 的基础上按照 Envoy 的 xDS 协议扩展了其控制平面，在讲到 Envoy xDS 协议之前还需要我们先熟悉下 Envoy 的基本术语。下面列举了 Envoy 里的基本术语及其数据结构解析，关于 Envoy 的详细介绍请参考 Envoy 官方文档 ，至于 Envoy 在 Service Mesh（不仅限于 Istio）中是如何作为转发代理工作的请参考网易云刘超的这篇深入解读 Service Mesh 背后的技术细节 以及理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持 ，本文引用其中的一些观点，详细内容不再赘述。\nEnvoy proxy 架构图 基本术语 下面是您应该了解的 Enovy 里的基本术语：\nDownstream（下游）：下游主机连接到 Envoy，发送请求并接收响应，即发送请求的主机。 Upstream（上游）：上游主机接收来自 Envoy 的连接和请求，并返回响应，即接受请求的主机。 Listener（监听器）：监听器是命名网地址（例如，端口、unix domain socket 等)，下游客户端可以连接这些监听器。Envoy 暴露一个或者多个监听器给下游主机连接。 Cluster（集群）：集群是指 Envoy 连接的一组逻辑相同的上游主机。Envoy 通过服务发现 来发现集群的成员。可以选择通过主动健康检查 来确定集群成员的健康状态。Envoy 通过负载均衡策略 决定将请求路由到集群的哪个成员。 我将在本文的后半部分解释以上术语与 Kubernetes、Istio 中概念之间的联系。\n关于 xDS 的版本 有一点需要大家注意，就是 Envoy 的 API 有 v1 和 v2 两个版本，从 Envoy 1.5.0 起 v2 API 就已经生产就绪了，为了能够让用户顺利的向 v2 版本的额 API 过度，Envoy 启动的时候设置了一个 --v2-config-only 的标志，Enovy 不同版本对 v1/v2 API 的支持详情请参考 Envoy v1 配置废弃时间表 。\nEnvoy 的作者 Matt Klein 在 Service Mesh 中的通用数据平面 API 设计 这篇文章中说明了 Envoy API v1 的历史及其缺点，还有 v2 的引入。v2 API 是 v1 的演进，而不是革命，它是 v1 功能的超集。\n在 Istio 1.0 及以上版本中使用的是 Envoy 1.8.0-dev 版本，其支持 v2 的 API，同时在 Envoy 作为 Sidecar proxy 启动的使用使用了例如下面的命令：\n$ /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster ratings --service-node sidecar~172.33.14.2~ratings-v1-8558d4458d-ld8x9.default~default.svc.cluster.local --max-obj-name-len 189 --allow-unknown-fields -l warn --v2-config-only 上面是都 Bookinfo 示例中的 rating pod 中的 sidecar 启动的分析，可以看到其中指定了 --v2-config-only，表明 Istio 1.0+ 只支持 xDS v2 的 API。\nIstio sidecar proxy 配置 假如您使用 kubernetes-vagrant-centos-cluster 部署了 Kubernetes 集群并开启了 Istio Service Mesh ，再部署 bookinfo 示例 ，那么在 default 命名空间下有一个名字类似于 ratings-v1-7c9949d479-dwkr4 的 Pod，使用下面的命令查看该 Pod 的 Envoy sidecar 的全量配置：\nkubectl -n default exec ratings-v1-7c9949d479-dwkr4 -c istio-proxy curl http://localhost:15000/config_dump \u0026gt; dump-rating.json 将 Envoy 的运行时配置 dump 出来之后你将看到一个长 6000 余行的配置文件。关于该配置文件的介绍请参考 Envoy v2 API 概览 。\nIstio 会在为 Service Mesh 中的每个 Pod 注入 Sidecar 的时候同时为 Envoy 注入 Bootstrap 配置，其余的配置是通过 Pilot 下发的，注意整个数据平面即 Service Mesh 中的 Envoy 的动态配置应该是相同的。您也可以使用上面的命令检查其他 sidecar 的 Envoy 配置是否跟最上面的那个相同。\n使用下面的命令检查 Service Mesh 中的所有有 Sidecar 注入的 Pod 中的 proxy 配置是否同步。\n$ istioctl proxy-status PROXY CDS LDS EDS RDS PILOT VERSION details-v1-876bf485f-sx7df.default SYNCED SYNCED SYNCED (100%) SYNCED istio-pilot-5bf6d97f79-6lz4x 1.0.0 ... istioctl 这个命令行工具就像 kubectl 一样有很多神器的魔法，通过它可以高效的管理 Istio 和 debug。\nEnvoy proxy 配置解析 Istio envoy sidecar proxy 配置中包含以下四个部分。\nbootstrap：Envoy proxy 启动时候加载的静态配置。 listeners：监听器配置，使用 LDS 下发。 clusters：集群配置，静态配置中包括 xds-grpc 和 zipkin 地址，动态配置使用 CDS 下发。 routes：路由配置，静态配置中包括了本地监听的服务的集群信息，其中引用了 cluster，动态配置使用 RDS 下发。 每个部分中都包含静态配置与动态配置，其中 bootstrap 配置又是在集群启动的时候通过 sidecar 启动参数注入的，配置文件在 /etc/istio/proxy/envoy-rev0.json。\n由于 bootstrap 中的配置是来自 Envoy 启动时加载的静态文件，主要配置了节点信息、tracing、admin 和统计信息收集等信息，这不是本文的重点，大家可以自行研究。\nBootstrap 是 Envoy 中配置的根本来源，Bootstrap 消息中有一个关键的概念，就是静态和动态资源的之间的区别。例如 Listener 或 Cluster 这些资源既可以从 static_resources 静态的获得也可以从 dynamic_resources 中配置的 LDS 或 CDS 之类的 xDS 服务获取。\nListener Listener 顾名思义，就是监听器，监听 IP 地址和端口，然后根据策略转发。\nListener 的特点\n每个 Envoy 进程中可以有多个 Listener，Envoy 与 Listener 之间是一对多的关系。 每个 Listener 中可以配置一条 filter 链表（filter_chains），Envoy 会根据 filter 顺序执行过滤。 Listener 可以监听下游的端口，也可以接收来自其他 listener 的数据，形成链式处理。 filter 是可扩展的。 可以静态配置，也可以使用 LDS 动态配置。 目前只能监听 TCP，UDP 还未支持。 Listener 的数据结构\nListener 的数据结构如下，除了 name、address 和 filter_chains 为必须配置之外，其他都为可选的。\n{ \u0026#34;name\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;filter_chains\u0026#34;: [], \u0026#34;use_original_dst\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;per_connection_buffer_limit_bytes\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;metadata\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;drain_type\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;listener_filters\u0026#34;: [], \u0026#34;transparent\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;freebind\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;socket_options\u0026#34;: [], \u0026#34;tcp_fast_open_queue_length\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;bugfix_reverse_write_filter_order\u0026#34;: \u0026#34;{...}\u0026#34; } 下面是关于上述数据结构中的常用配置解析。\nname：该 listener 的 UUID，唯一限定名，默认 60 个字符，例如 10.254.74.159_15011，可以使用命令参数指定长度限制。\naddress：监听的逻辑/物理地址和端口号，例如\n\u0026#34;address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;10.254.74.159\u0026#34;, \u0026#34;port_value\u0026#34;: 15011 } } filter_chains：这是一个列表，Envoy 中内置了一些通用的 filter，每种 filter 都有特定的数据结构，Enovy 会根据该配置顺序执行 filter。Envoy 中内置的 filter 有：envoy.client_ssl_auth 、envoy.echo 、enovy.http_connection_manager 、envoy.mongo_proxy 、envoy.rate_limit 、enovy.redis_proxy 、envoy.tcp_proxy 、http_filters 、thrift_filters 等。这些 filter 可以单独使用也可以组合使用，还可以自定义扩展，例如使用 Istio 中的 EnvoyFilter 配置 。\nuse_original_dst：这是一个布尔值，如果使用 iptables 重定向连接，则代理接收的端口可能与原始目的地址 的端口不一样。当此标志设置为 true 时，Listener 将重定向的连接切换到与原始目的地址关联的 Listener。如果没有与原始目的地址关联的 Listener，则连接由接收它的 Listener 处理。默认为 false。注意：该参数将被废弃，请使用原始目的地址 的 Listener filter 替代。该参数的主要用途是：Envoy 通过监听 15001 端口将应用的流量截取后再由其他 Listener 处理而不是直接转发出去，详情见 Virtual Listener 。\n关于 Listener 的详细介绍请参考 Envoy v2 API reference - listener 。\nRoute 我们在这里所说的路由指的是 HTTP 路由 ，这也使得 Envoy 可以用来处理网格边缘的流量。HTTP 路由转发是通过路由过滤器实现的。该过滤器的主要职能就是执行路由表 中的指令。除了可以做重定向和转发，路由过滤器还需要处理重试、统计之类的任务。\nHTTP 路由的特点\n前缀和精确路径匹配规则。 可跨越多个上游集群进行基于权重/百分比的路由 。 基于优先级 的路由。 基于哈希 策略的路由。 Route 的数据结构\n{ \u0026#34;name\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;virtual_hosts\u0026#34;: [], …","relpermalink":"/blog/envoy-proxy-config-deep-dive/","summary":"本文介绍了 Envoy proxy 的概念，对应的 xDS 的版本以及配置的详细解析。","title":"Istio 的数据平面 Envoy Proxy 配置详解"},{"content":"本视频为本人在 2018 年最后一天录制，演示使用 rootsongjc/kubernetes-vagrant-centos-cluster 自动部署 Kubernetes 集群和 Istio Service Mesh。\n前几天天我说到 kubernetes-vagrant-centos-cluster 发布 v1.2.0 版本一键部署云原生实验环境，很久前在 Kubernetes 和 Service Mesh 社区里就有人要求我做一个视频来讲解和演示下如何安装 Kubernetes 和 Istio Service Mesh，因为平时很忙一直某抽出时间，今天我将 demo 视频献上，别看视频就几分钟，为了做这个视频耗费我半个小时录制，两个小时编辑，还有多年的拍摄、剪辑、容器、虚拟机、Kubernetes、服务网格经验。与其说这是一个告别，不如说是一个新的开始。\n因为视频是首发在YouTube 上的，所以使用了英文讲解（只是一些补充说明而已，听不懂也没关系，直接看 GitHub 上的中文文档 即可）。\n跳转到 bilibli 观看 ，如果你对无人机航拍感兴趣，还可以看看Jimmy Song 的航拍作品 ，支持请投币或点赞，谢谢。\n有什么问题反馈可以发弹幕，或者在视频下面评论。\nPS. 有人会问到为什么选择使用 bilibli，因为这个平台看视频没有广告，大多是 Up 主自己上传的，虽然二次元居多，但社区氛围还是比较好的。\n更多精彩视频请访问 Jimmy Song 的 bilibli 主页 。\n","relpermalink":"/notice/cloud-native-kubernetes-service-mesh-local-demo-show/","summary":"本视频为本人在 2018 年最后一天录制，演示使用 rootsongjc/kubernetes-vagrant-centos-cluster 自动部署 Kubernetes 集群和 Istio Service Mesh。","title":"Kubernetes 和 Istio 服务网格（Service Mesh）云原生本地视频 Demo 演示"},{"content":"本文以 Istio 官方的 bookinfo 示例 来讲解在进入 Pod 的流量被 iptables 转交给 Envoy sidecar 后，Envoy 是如何做路由转发的，详述了 Inbound 和 Outbound 处理过程。关于流量拦截的详细分析请参考理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持 。\n下面是 Istio 官方提供的 bookinfo 的请求流程图，假设 bookinfo 应用的所有服务中没有配置 DestinationRule。\nBookinfo 示例 下面是 Istio 自身组件与 Bookinfo 示例的连接关系图，我们可以看到所有的 HTTP 连接都在 9080 端口监听。\nBookinfo 示例与 Istio 组件连接关系图 可以在 Google Drive 上下载原图。\nSidecar 注入及流量劫持步骤概述 下面是从 Sidecar 注入、Pod 启动到 Sidecar proxy 拦截流量及 Envoy 处理路由的步骤概览。\n1. Kubernetes 通过 Admission Controller 自动注入，或者用户使用 istioctl 命令手动注入 sidecar 容器。\n2. 应用 YAML 配置部署应用，此时 Kubernetes API server 接收到的服务创建配置文件中已经包含了 Init 容器及 sidecar proxy。\n3. 在 sidecar proxy 容器和应用容器启动之前，首先运行 Init 容器，Init 容器用于设置 iptables（Istio 中默认的流量拦截方式，还可以使用 BPF、IPVS 等方式）将进入 pod 的流量劫持到 Envoy sidecar proxy。所有 TCP 流量（Envoy 目前只支持 TCP 流量）将被 sidecar 劫持，其他协议的流量将按原来的目的地请求。\n4. 启动 Pod 中的 Envoy sidecar proxy 和应用程序容器。这一步的过程请参考通过管理接口获取完整配置 。\nSidecar proxy 与应用容器的启动顺序问题\n启动 sidecar proxy 和应用容器，究竟哪个容器先启动呢？正常情况是 Envoy Sidecar 和应用程序容器全部启动完成后再开始接收流量请求。但是我们无法预料哪个容器会先启动，那么容器启动顺序是否会对 Envoy 劫持流量有影响呢？答案是肯定的，不过分为以下两种情况。\n情况 1：应用容器先启动，而 sidecar proxy 仍未就绪\n这种情况下，流量被 iptables 转移到 15001 端口，而 Pod 中没有监听该端口，TCP 链接就无法建立，请求失败。\n情况 2：Sidecar 先启动，请求到达而应用程序仍未就绪\n这种情况下请求也肯定会失败，至于是在哪一步开始失败的，留给读者来思考。\n问题：如果为 sidecar proxy 和应用程序容器添加就绪和存活探针 是否可以解决该问题呢？\n5. 不论是进入还是从 Pod 发出的 TCP 请求都会被 iptables 劫持，inbound 流量被劫持后经 Inbound Handler 处理后转交给应用程序容器处理，outbound 流量被 iptables 劫持后转交给 Outbound Handler 处理，并确定转发的 upstream 和 Endpoint。\n6. Sidecar proxy 请求 Pilot 使用 xDS 协议同步 Envoy 配置，其中包括 LDS、EDS、CDS 等，不过为了保证更新的顺序，Envoy 会直接使用 ADS 向 Pilot 请求配置更新。\nEnvoy 如何处理路由转发 下图展示的是 productpage 服务请求访问 http://reviews.default.svc.cluster.local:9080/，当流量进入 reviews 服务内部时，reviews 服务内部的 Envoy Sidecar 是如何做流量拦截和路由转发的。可以在 Google Drive 上下载原图。\nEnvoy sidecar 流量劫持与路由转发示意图 第一步开始时，productpage Pod 中的 Envoy sidecar 已经通过 EDS 选择出了要请求的 reviews 服务的一个 Pod，知晓了其 IP 地址，发送 TCP 连接请求。\nIstio 官网中的 Envoy 配置深度解析 中是以发起 HTTP 请求的一方来详述 Envoy 做流量转发的过程，而本文中考虑的是接受 downstream 的流量的一方，它既要接收 downstream 发来的请求，自己还需要请求其他服务，例如 reviews 服务中的 Pod 还需要请求 ratings 服务。\nreviews 服务有三个版本，每个版本有一个实例，三个版本中的 sidecar 工作步骤类似，下文只以 reviews-v1-cb8655c75-b97zc 这一个 Pod 中的 Sidecar 流量转发步骤来说明。\n理解 Inbound Handler Inbound handler 的作用是将 iptables 拦截到的 downstream 的流量转交给 localhost，与 Pod 内的应用程序容器建立连接。\n查看下 reviews-v1-cb8655c75-b97zc pod 中的 Listener。\n运行 istioctl pc listener reviews-v1-cb8655c75-b97zc 查看该 Pod 中的具有哪些 Listener。\nADDRESS PORT TYPE 172.33.3.3 9080 HTTP \u0026lt;--- 接收所有 Inbound HTTP 流量，该地址即为当前 Pod 的 IP 地址 10.254.0.1 443 TCP \u0026lt;--+ 10.254.4.253 80 TCP | 10.254.4.253 8080 TCP | 10.254.109.182 443 TCP | 10.254.22.50 15011 TCP | 10.254.22.50 853 TCP | 10.254.79.114 443 TCP | 10.254.143.179 15011 TCP | 10.254.0.2 53 TCP | 接收与 0.0.0.0_15001 监听器配对的 Outbound 非 HTTP 流量 10.254.22.50 443 TCP | 10.254.16.64 42422 TCP | 10.254.127.202 16686 TCP | 10.254.22.50 31400 TCP | 10.254.22.50 8060 TCP | 10.254.169.13 14267 TCP | 10.254.169.13 14268 TCP | 10.254.32.134 8443 TCP | 10.254.118.196 443 TCP \u0026lt;--+ 0.0.0.0 15004 HTTP \u0026lt;--+ 0.0.0.0 8080 HTTP | 0.0.0.0 15010 HTTP | 0.0.0.0 8088 HTTP | 0.0.0.0 15031 HTTP | 0.0.0.0 9090 HTTP | 0.0.0.0 9411 HTTP | 接收与 0.0.0.0_15001 配对的 Outbound HTTP 流量 0.0.0.0 80 HTTP | 0.0.0.0 15030 HTTP | 0.0.0.0 9080 HTTP | 0.0.0.0 9093 HTTP | 0.0.0.0 3000 HTTP | 0.0.0.0 8060 HTTP | 0.0.0.0 9091 HTTP \u0026lt;--+ 0.0.0.0 15001 TCP \u0026lt;--- 接收所有经 iptables 拦截的 Inbound 和 Outbound 流量并转交给虚拟监听器处理 当来自 productpage 的流量抵达 reviews Pod 的时候已经，downstream 必须明确知道 Pod 的 IP 地址为 172.33.3.3 所以才会访问该 Pod，所以该请求是 172.33.3.3:9080。\nvirtual Listener\n从该 Pod 的 Listener 列表中可以看到，0.0.0.0:15001/TCP 的 Listener（其实际名字是 virtual）监听所有的 Inbound 流量，下面是该 Listener 的详细配置。\n{ \u0026#34;name\u0026#34;: \u0026#34;virtual\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;socketAddress\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;portValue\u0026#34;: 15001 } }, \u0026#34;filterChains\u0026#34;: [ { \u0026#34;filters\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;envoy.tcp_proxy\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;cluster\u0026#34;: \u0026#34;BlackHoleCluster\u0026#34;, \u0026#34;stat_prefix\u0026#34;: \u0026#34;BlackHoleCluster\u0026#34; } } ] } ], \u0026#34;useOriginalDst\u0026#34;: true } UseOriginalDst：从配置中可以看出 useOriginalDst 配置指定为 true，这是一个布尔值，缺省为 false，使用 iptables 重定向连接时，proxy 接收的端口可能与原始目的地址 的端口不一样，如此处 proxy 接收的端口为 15001，而原始目的地端口为 9080。当此标志设置为 true 时，Listener 将连接重定向到与原始目的地址关联的 Listener，此处为 172.33.3.3:9080。如果没有与原始目的地址关联的 Listener，则连接由接收它的 Listener 处理，即该 virtual Listener，经过 envoy.tcp_proxy 过滤器处理转发给 BlackHoleCluster，这个 Cluster 的作用正如它的名字，当 Envoy 找不到匹配的虚拟监听器时，就会将请求发送给它，并返回 404。这个将于下文提到的 Listener 中设置 bindToPort 相呼应。\n注意：该参数将被废弃，请使用原始目的地址 的 Listener filter 替代。该参数的主要用途是：Envoy 通过监听 15001 端口将 iptables 拦截的流量经由其他 Listener 处理而不是直接转发出去，详情见 Virtual Listener 。\nListener 172.33.3.3_9080\n上文说到进入 Inbound handler 的流量被 virtual Listener 转移到 172.33.3.3_9080 Listener，我们在查看下该 Listener 配置。\n运行 istioctl pc listener reviews-v1-cb8655c75-b97zc --address 172.33.3.3 --port 9080 -o json 查看。\n[{ \u0026#34;name\u0026#34;: \u0026#34;172.33.3.3_9080\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;socketAddress\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;172.33.3.3\u0026#34;, \u0026#34;portValue\u0026#34;: 9080 } }, \u0026#34;filterChains\u0026#34;: [ { \u0026#34;filterChainMatch\u0026#34;: { \u0026#34;transportProtocol\u0026#34;: \u0026#34;raw_buffer\u0026#34; }, \u0026#34;filters\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;envoy.http_connection_manager\u0026#34;, \u0026#34;config\u0026#34;: { ... \u0026#34;route_config\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;inbound|9080||reviews.default.svc.cluster.local\u0026#34;, \u0026#34;validate_clusters\u0026#34;: false, …","relpermalink":"/blog/envoy-sidecar-routing-of-istio-service-mesh-deep-dive/","summary":"Sidecar proxy 中 Indbound/Outbound 流量处理过程详解。","title":"理解 Istio Service Mesh 中 Envoy Sidecar 代理的路由转发"},{"content":"本文介绍了 Istio 和 Kubernetes 中的一些服务和流量的抽象模型。虽然 Istio 一开始确定的抽象模型与对接的底层平台无关，但目前来看基本绑定 Kubernetes，本文仅以 Kubernetes 说明。另外在 ServiceMesher 社区 中最近有很多关于 Istio、Envoy、Kubernetes 之中的服务模型关系的讨论，本文作为一个开篇说明，Kubernetes 和 Isito 之间有哪些共有的服务模型，Istio 在 Kubernetes 的服务模型之上又增加了什么。\n服务具有多个版本。 在 CI/CD 过程中，同一个服务可能同时部署在多个环境中，如开发、生产和测试环境等，这些服务版本不一定具有不同的 API，可能只是一些小的更改导致的迭代版本。在 A/B 测试和灰度发布中经常遇到这种情况。\nKubernetes 与 Istio 中共有的模型 因为 Istio 基本就是绑定在 Kubernetes 上，下面是我们熟知的 Kubernetes 及 Istio 中共有的服务模型。\nKubernetes 中 iptables 代理模式（另外还有 IPVS 模式）下的 service，管理员可以在 kube-proxy 中配置简单的负载均衡，对整个 node 生效，无法配置到单个服务的负载均衡和其他微服务的高级功能，例如熔断、限流、追踪等，这些功能只能在应用中实现了，而在 Istio 的概念模型中完全去掉了 kube-proxy 这个组件，将其分散到每个应用 Pod 中同时部署的 Envoy 中实现。\n下面列举的是 Kubernetes 和 Istio 中共有的模型。\nService 这实际上跟 Kubernetes 中的 service 概念是一致的，请参考 Kubernetes 中的 service 。Istio 推出了比 service 更复杂的模型 VirtualService，这不单纯是定义一个服务了，而是在服务之上定义了路由规则。\n每个服务都有一个完全限定的域名（FQDN），监听一个或多个端口。服务还可以有与其相关联的单个负载均衡器或虚拟 IP 地址。针对 FQDN 的 DNS 查询将解析为该负载均衡器或者虚拟 IP 的地址。\n例如 Kubernetes 中一个服务为 foo.default.svc.cluster.local ，虚拟 IP /ClusterIP 是 10.0.1.1，监听的端口是 80 和 8080。\nEndpoint 这里指的是 Kubernetes 中的 endpoint，一个 endpoint 是实现了某服务的具体实例，一个服务可能有一个或者多个 Endpoint，表示为 IP 地址加端口，也可以为 DNS 名称加端口。\n其实到底哪些实例属于同一个 service，还是需要 通过 label 匹配来选择。\nLabel 服务的版本、对应的引用名称等是通过 label 来标记的，例如下面 Kubernetes 中一个应用的 YAML 配置。\napiVersion: extensions/v1beta1 kind: Deployment metadata: name: ratings-v1 spec: replicas: 1 template: metadata: labels: app: ratings version: v1 spec: containers: - name: ratings image: istio/examples-bookinfo-ratings-v1:1.8.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9080 version: v1 标记该服务是 v1 版本，version 是一个约定俗称的标签，建议大家的服务上都带上该标签。\n当然服务的 label 可以设置任意多个，这样的好处是在做路由的时候可以根据标签匹配来做细粒度的流量划分。\n数据平面 Envoy Envoy 是 Istio 中默认的 sidecar proxy，负责服务间的流量管控、认证与安全加密、可观察性等。\n下面是 Envoy 的架构图。\nEnvoy 架构图 我再给大家介绍 Envoy 中的如下几个重要概念。\nCluster 集群（cluster）是 Envoy 连接到的一组逻辑上相似的上游主机。Envoy 通过服务发现 发现集群中的成员。Envoy 可以通过主动运行状况检查 来确定集群成员的健康状况。Envoy 如何将请求路由到集群成员由负载均衡策略 确定。\n这个与 Kubernetes 中的 Service 概念类似，只不过 Kubernetes 中的服务发现中并不包含健康状况检查，而是通过配置 Pod 的 liveness 和 readiness 探针 来实现，服务发现默认也是通过 DNS 来实现。\nListener 监听器（listener）是可以由下游客户端连接的命名网络位置（例如，端口、unix 域套接字等）。Envoy 公开一个或多个下游主机连接的侦听器。一般是每台主机运行一个 Envoy，使用单进程运行，但是每个进程中可以启动任意数量的 Listener（监听器），目前只监听 TCP，每个监听器都独立配置一定数量的（L3/L4）网络过滤器。Listenter 也可以通过 Listener Discovery Service（LDS）动态获取。\nListener filter Listener 使用 listener filter（监听器过滤器）来操作链接的元数据。它的作用是在不更改 Envoy 的核心功能的情况下添加更多的集成功能。Listener filter 的 API 相对简单，因为这些过滤器最终是在新接受的套接字上运行。在链中可以互相衔接以支持更复杂的场景，例如调用速率限制。Envoy 已经包含了多个监听器过滤器。\nIstio 中增加的流量模型 VirtualService、DestinationRule、Gateway、ServiceEntry 和 EnvoyFilter 都是 Istio 中为流量管理所创建的 CRD，这些概念其实是做路由配置和流量管理的，而 Kubernetes 中的 service 只是用来做服务发现。Service Mesh 中真正的服务模型应该是 Envoy 的 xDS 协议 ，其中包括了服务的流量治理，服务的端点是通过 EDS 来配置的。\nIstio pilot 架构图 上图是 Pilot 设计图，来自Istio Pilot design overview 。\nRouting Kubernetes 中的 service 是没有任何路由属性可以配置的，Istio 在设计之初就通过在同一个 Pod 中，在应用容器旁运行一个 sidecar proxy 来透明得实现细粒度的路由控制。\nVirtualService VirtualService 定义针对指定服务流量的路由规则。每个路由规则都针对特定协议的匹配规则。如果流量符合这些特征，就会根据规则发送到服务注册表中的目标服务（或者目标服务的子集或版本）。对于 A/B 测试和灰度发布等场景，通常需要使用划分 subset，VirtualService 中根据 destination 中的 subset 配置来选择路由，但是这些 subset 究竟对应哪些服务示例，这就需要 DestionationRule。\nDestinationRule DestinationRule 所定义的策略，决定了经过路由处理之后的流量的访问策略。这些策略中可以定义负载均衡配置、连接池尺寸以及外部检测（用于在负载均衡池中对不健康主机进行识别和驱逐）配置。\nGateway Gateway 描述了一个负载均衡器，用于承载网格边缘的进入和发出连接。这一规范中描述了一系列开放端口，以及这些端口所使用的协议、负载均衡的 SNI 配置等内容。\n这个实际上就是定义服务网格的边缘路由。\nServiceEntry ServiceEntry 能够在 Istio 内部的服务注册表中加入额外的条目，从而让网格中自动发现的服务能够访问和路由到这些手工加入的服务。ServiceEntry 描述了服务的属性（DNS 名称、VIP、端口、协议以及端点）。这类服务可能是网格外的 API，或者是处于网格内部但却不存在于平台的服务注册表中的条目（例如需要和 Kubernetes 服务沟通的一组虚拟机服务）。\n如果没有配置 ServiceEntry 的话，Istio 实际上是无法发现服务网格外部的服务的。\nEnvoyFilter EnvoyFilter 对象描述了针对代理服务的过滤器，这些过滤器可以定制由 Istio Pilot 生成的代理配置。这一功能一定要谨慎使用。错误的配置内容一旦完成传播，可能会令整个服务网格进入瘫痪状态。\nEnvoy 中的 listener 可以配置多个 filter ，这也是一种通过 Istio 来扩展 Envoy 的机制。\n参考 Kubernetes 中的 service - jimmysong.io Istio services model - github.com Istio 文档 - istio.io ","relpermalink":"/blog/istio-service-and-traffic-model/","summary":"本文介绍了 Istio 和 Kubernetes 中的一些服务和流量的抽象模型。","title":"Istio 中的服务和流量的抽象模型"},{"content":"北美的 KubeCon\u0026amp;CloudNative 是每年最值得参加的云原生盛会，这次在西雅图举行，为期四天，从 12 月 10 号到 13 号，参考大会官网 。今年 8000 人参加，你应该留意到了 Kubernetes 已经越来越底层，云原生应用开发者不需要过多关注它，各大公司都在发布自己的云原生技术栈布局，包括 IBM、VMware、SAP 等，围绕该生态的创业公司还在不断涌现。本地大会的 PPT 分享地址：https://github.com/warmchang/KubeCon-North-America-2018 ，感谢 William Zhang 对本次会议幻灯片的整理和分享。\n西雅图现场 来自 Google 的 Janet Kuo 介绍云原生技术的采用路径。\nKubeCon\u0026amp;CloudNativeCon 的同场活动，第一届 EnvoyCon 现场。\nKubeCon\u0026amp;CloudNativeCon Seattle 上，IBM、Google、Mastercard、VMware 各种总监、主管、VP 以及 Gartner 分析师正在进行 Scaling with Service Mesh and Istio 话题讨论直播，现场的人可真多，目视也有上百人了，现场有人提问为什么当我们谈起 Service Mesh 大家都在谈 Istio？有什么不适合 Istio 的使用场景。。。\n这些 PPT 中包含基础介绍、使用入门、Deep Dive 还有实际应用等，一共 200 多个，建议大家根据大会官网 上的日程挑选自己感兴趣的话题来看，不然可能看不过来。\n一点感想 KubeCon\u0026amp;CloudNativeCon 一年在办三场，欧洲、中国和北美，中国今年是第一次举办，11 月在上海，据说明年要放在 6 月份举办，虽然大家都说 Kubernetes 变得 boring，但是大会上关于 Kubernetes 的内容还是很多，使用 CRD 来扩展 Kubernetes 的用法也越来越多了。Service Mesh 已经开始变的火起来了，从上面的直播的图片中就可以看到，现场的参与人数众多，相关的话题也越来也多，被誉为是后 Kubernetes 时代的微服务 ,这必将是云原生在 Kubernetes 之后的一个重要发展方向，ServiceMesher 社区 强烈关注。\n","relpermalink":"/notice/kubecon-cloudnativecon-seattle-2018/","summary":"KubeCon\u0026CloudNativeCon 西雅图 2018 资料分享。","title":"Kubecon\u0026CloudNativeCon Seattle 2018"},{"content":"TL;DR\nCannot ssh into a running pod/container – rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused “process_linux.go:110: decoding init error from pipe caused \u0026#34;read parent: connection reset by peer\u0026#34;” command terminated with exit code 126 #21590 Bug 影响 如果你使用的是 CentOS7，需要用到 kubectl exec 或者为 Pod 配置了基于命令返回值的健康检查（非常用的 HTTP Get 方式）的话，该 Bug 将导致命令返回错误，Pod 无法正常启动，引起大规模故障，而且也无法使用 kubectl exec 或者 docker exec 与容器交互。\n例如下面的健康检查配置：\nlivenessProbe: exec: command: - /usr/local/bin/sidecar-injector - probe - --probe-path=/health - --interval=4s failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 4 successThreshold: 1 timeoutSeconds: 1 readinessProbe: exec: command: - /usr/local/bin/sidecar-injector - probe - --probe-path=/health - --interval=4s failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 4 successThreshold: 1 timeoutSeconds: 1 以上 YAML 配置摘自 Istio 发行版中的 istio-demo.yaml 文件。\nBug 成因 根据 RedHat 的 Bug 报告 ，导致该 Bug 的原因是：\nCentOS7 发行版中的 Docker 使用的 docker-runc 二进制文件使用旧版本的 golang 构建的，这里面一些可能导致 FIPS 模式 崩溃的错误。\n至于该 Bug 是如何触发的官方只是说因为某些镜像导致的。\n发现过程 本周 Kubernetes 1.13 发布，想着更新下我的 kubernetes-vagrant-centos-cluster 使用 Vagrant 和 VirtualBox 在本地搭建分布式 Kubernetes 1.13 集群和 Istio Service Mesh 的最新版本 1.0.4，可是在安装 Istio 的时候发现 Istio 有两个 Pod 启动不起来，istio-sidecar-injector 和 istio-galley 这两个 Pod，检查其启动过程，发现它们都是因为 Readiness Probe 和 Liveness Probe 失败导致的。再联想到之前安装较老版本的 Istio 的时候也遇到该问题，见 Increase health probe interval #6610 通过增加健康检查的时间间隔可以解决该问题，可是经过反复的测试后发现还是不行。然后我想到先去掉健康检查，然后我手动使用 kubectl exec 来执行健康检查的命令，解决却遇到下面的错误：\n$ kubectl exec -it istio-sidecar-injector-6fc974b6c8-pts4t -- istio-sidecar-injector-b484dfcbb-9x9l9 probe --probe-path=/health --interval=4s Cannot ssh into a running pod/container -- rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused \u0026#34;process_linux.go:110: decoding init error from pipe caused \u0026#34;read parent: connection reset by peer\u0026#34;\u0026#34; command terminated with exit code 126 然后直接到 Pod 所在的主机使用 docker exec 命令执行，依然报上面的错误，我就确定这不是 Kubernetes 的问题了。更何况前之前 kubernetes-vagrant-centos-cluster 屡试不爽，突然出现问题，有点让人摸不着头脑。知道我搜到了这个四天前才有人提出的 issue 。根据网友反馈，现在 kubernetes-vagrant-centos-cluster 中已经通过降级 Docker 的方式临时修复了该问题，并支持 Kubernetes 1.13 和 Istio 1.0.4，欢迎试用。\n解决方法 有两种解决方法，都需要替换 Docker 版本。\n一、降级到旧的 RedHat CentOS 官方源中的 Docker 版本\n将 RedHat 官方源中的 Docker 版本降级，这样做的好处是所有的配置无需改动，参考 https://github.com/openshift/origin/issues/21590 。\n查看 Docker 版本：\n$ rpm -qa | grep -i docker docker-common-1.13.1-84.git07f3374.el7.centos.x86_64 docker-client-1.13.1-84.git07f3374.el7.centos.x86_64 docker-1.13.1-84.git07f3374.el7.centos.x86_64 降级 Docker 版本。\nyum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64 降级之后再查看 Docker 版本：\n$ rpm -qa | grep -i docker docker-common-1.13.1-75.git8633870.el7.centos.x86_64 docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 此为临时解决方法，RedHat 也在着手解决该问题，为了可能会提供补丁，见 Bug 1655214 - docker exec does not work with registry.access.redhat.com/rhel7:7.3。\n二、更新到 Docker-CE\n众所周知，Docker 自 1.13 版本之后更改了版本的命名方式，也提供了官方的 CentOS 源，替换为 Docker-CE 亦可解决该问题，不过 Docker-CE 的配置可能会与 Docker 1.13 有所不同，所以可能需要修改配置文件。\n参考 配置 Pod 的 liveness 和 readiness 探针 - jimmysong.io Bug 1655214 - docker exec does not work with registry.access.redhat.com/rhel7:7.3 - redhat.com kubernetes-vagrant-centos-cluster - github.com FIPS Mode - an explanation - mozilla.org ","relpermalink":"/blog/docker-exec-bug-on-centos7/","summary":"CentOS7 官方 Docker 1.13 版本 Bug 导致 docker exec 失败，可致 Kubernetes 中的检测探针失败，官方推荐降级 docker 版本解决。","title":"CentOS7 官方 Docker 发行版现重大 Bug"},{"content":"今天给大家分享的是《软件定义交付宣言》，该宣言发出已经有一周多时间了，目前该宣言的官方网站（https://sdd-manifesto.org/ 已停止维护）还在联署签名中。\n云原生通过不可变基础设施与声明式配置，作为了软件定义交付的基础，再假以持续交付工具可以极大的提高软件交付效率，本宣言的起草者中包含众多云原生理念的鉴定拥护者如 Kenny Bastani、Matt Stine 等。\n该宣言通过 GitHub 协作草拟（https://github.com/sdd-manifesto/manifesto ），仍未达到 1.0 版本。下面是《软件定义交付宣言》的中文版。\n软件定义交付宣言 我们从日常生产和实践中认识到软件塑造了我们的世界。我们认识到代码才是指定精确操作的最佳方式。我们认识到代码仅在被交付时才有用。\n开发的软件被能够被交付出去就软件本身存在的目的。现在是时候将我们的核心技能应用到实际的工作中去了。是时候对交付去做出 设计 了。我们将区别人类和计算机在交付工作中承担的作用：人类做决策，计算机来完成自动化任务。\n每一次交付工作本质上都是独一无二的。应用程序、组织、部署环境和团队组合千差万别。我们认识到每个团队都需要能够理解这种交付的独特性和对交付做自动化。我们认识到，虽然持续交付对满足业务需求至关重要，但自动执行所有重复任务也非常重要。\n我们使用与加速应用程序开发类似的方式来加速软件交付：使用现代架构和编程语言，通用功能的框架、库和服务。\n交付基础设施现在是可编程的，我们将对其进行编程。\n软件定义交付是指 核心：交付是每个软件团队和组织的基础和战略能力。\n优先：交付的代码是生产代码。 战略：决定团队和组织层面的策略；在代码中实现精确控制。 不断发展：不断改进交付。 工程设计：强大，可测试的代码。70 年代的脚本语言是不够的。\n现代软件架构：事件驱动和可扩展。 现代编程语言：逻辑最好用代码指定，而不是图片或 GUI。脚本不好扩展。 基于模型：由软件领域的模型支持，具有对代码的理解。 可测试：在生产之前启用小规模应用以发现错误。 协作：\n从群众中来：所有人都可以在代码中表述自己的专业知识，这对大家都有利。 到软件中去：使用最好的工具，但将它们结合起来之后就是独一无二的。 在人与软件之间：协同自动化可以增强我们的感知和帮助我们做决策。将信息落实到行动，使我们能够体察软件的自动化行为。通过代码来区分团队的共享交付目标集及其实现。 加速：\n通过自动化：自动执行重复任务，加快了工作速度还可以避免错误发生。 通过重用：在开发人员、团队和组织之间共享通用功能。 可观察性：通常用于观察和排除作为生产系统的交付过程中发生的情况。\n跟踪：观察系统中的活动并跟踪操作之间的关系。 调试：检查和与交付流程交互。 指标：在整个交付流程的活动中获取指标。 作者：(姓氏按字母顺序排列）：本宣言由 Kenny Bastani、Marc Holmes、Rod Johnson、Jessica Kerr、Mik Kersten、Russ Miles、Erin Schnabel、Matt Stine 及其他社区成员草拟。\n©2018，上述作者和本声明可以任何形式自由复制，但需全文复制本声明。\n","relpermalink":"/blog/software-defined-delivery-manifesto/","summary":"今天给大家分享的是《软件定义交付宣言》，该宣言发出已经有一周多时间了，还在联署签名中。","title":"软件定义交付（SDD）宣言"},{"content":"本次大会是一个难得的机会，让我见到了很多朋友，很多都是第一次在线下见面。\nKubeCon\u0026amp;CloudNativeCon China 上海 2018 图中由上自下的大合影是：ServiceMesher 社区上海聚首合影；中美日的 Kubernetes、Envoy、Istio、Apache Skywalking、ServiceMesher 社区在 KubeCon 上海；Yahoo Japan 与蚂蚁集团团队在上海中心办公室合影。\n活动 Meet the Ambassadors\n参加了一场 Meet the Ambassadors 采访，第一次参加英文的采访，本来准备的英文回答没用上，现场反而还紧张了。。。工作人员的摄像机又没就位，我还充当了摄影师（本色出演）。一共采访了四位中国的 Ambassador。\nJiayao (Julia) Han, Caicloud Jia Xuan, China Mobile Research Institute Jimmy Song, Ant Group Jessie Qian, Alauda 我们的名字不约而同的都是 J 字头。\nService Mesh Roundtable\n然后参加了一场 Service Mesh Roundtable，参加人员有：\nJimmy Song, Developer Advocate on Cloud Native at Ant Group Yulin Son, Principal Architect at Huawei George Miranda, PagerDuty Nic Jackson, Developer Advocate at HashiCorp 我们就 Service Mesh 的现状，存在的问题后未来进行了广泛的探讨。\nPPT KubeCon China 会议的很多 PPT 在大会的官网 上都可以下载，或者通过百度网盘 下载 zip 包，提取码：5vn0。或者通过GitHub 下载单个 PPT。\n飞行 在会场随便一坐，就能遇到熟人。还有很多我都叫不上名字的人来打招呼，不能一一道谢了，感谢晚餐以及收到的 Prometheus 飞行袜。\n11 月 12 日晚在上海静安寺上空飞行，航拍的南京西路夜景。\n","relpermalink":"/blog/kubecon-cloudnativecon-china-2018/","summary":"KubeCon\u0026CloudNativeCon China 2018 参会回顾。","title":"KubeCon\u0026CloudNativeCon China 2018"},{"content":"本文最新更新于 2022 年 3 月 7 日。\n以往有很多文章讲解 Istio 是如何做 Sidecar 注入的，但是没有讲解注入之后 Sidecar 工作的细节。本文将带大家详细了解 Istio 是如何将 Envoy 作为 Sidecar 的方式注入到应用程序 Pod 中，及 Sidecar 是如何做劫持流量的。\n在讲解 Istio 如何将 Envoy 代理注入到应用程序 Pod 中之前，我们需要先了解以下几个概念：\nSidecar 模式：容器应用模式之一，Service Mesh 架构的一种实现方式。 Init 容器：Pod 中的一种专用的容器，在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。 iptables：流量劫持是通过 iptables 转发实现的。 查看目前 reviews-v1-745ffc55b7-2l2lw Pod 中运行的容器：\n$ kubectl -n default get pod reviews-v1-745ffc55b7-2l2lw -o=jsonpath=\u0026#39;{..spec.containers[*].name}\u0026#39; reviews istio-proxy reviews 即应用容器，istio-proxy 即 Envoy 代理的 sidecar 容器。另外该 Pod 中实际上还运行过一个 Init 容器，因为它执行结束就自动终止了，所以我们看不到该容器的存在。关注 jsonpath 的用法请参考 JSONPath Support 。\nSidecar 模式 在了解 Istio 使用 Sidecar 注入之前，需要先说明下什么是 Sidecar 模式。Sidecar 是容器应用模式的一种，也是在 Service Mesh 中发扬光大的一种模式，详见 Service Mesh 架构解析 ，其中详细描述了节点代理和 Sidecar 模式的服务网格架构。\n使用 Sidecar 模式部署服务网格时，无需在节点上运行代理（因此您不需要基础结构的协作），但是集群中将运行多个相同的 Sidecar 副本。从另一个角度看：我可以为一组微服务部署到一个服务网格中，你也可以部署一个有特定实现的服务网格。在 Sidecar 部署方式中，你会为每个应用的容器部署一个伴生容器。Sidecar 接管进出应用容器的所有流量。在 Kubernetes 的 Pod 中，在原有的应用容器旁边运行一个 Sidecar 容器，可以理解为两个容器共享存储、网络等资源，可以广义的将这个注入了 Sidecar 容器的 Pod 理解为一台主机，两个容器共享主机资源。\n下图展示的是 Service Mesh 的架构图，其中的位于每个 Pod 中的 proxy 组成了数据平面，而这些 proxy 正是以 sidecar 模式运行的。\nIstio 架构 注意：下文中所指的 Sidecar 都是指的 Envoy 代理容器。\nInit 容器 Init 容器是一种专用容器，它在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。\n一个 Pod 中可以指定多个 Init 容器，如果指定了多个，那么 Init 容器将会按顺序依次运行。只有当前面的 Init 容器必须运行成功后，才可以运行下一个 Init 容器。当所有的 Init 容器运行完成后，Kubernetes 才初始化 Pod 和运行应用容器。\nInit 容器使用 Linux Namespace，所以相对应用程序容器来说具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用程序容器则不能。\n在 Pod 启动过程中，Init 容器会按顺序在网络和数据卷初始化之后启动。每个容器必须在下一个容器启动之前成功退出。如果由于运行时或失败退出，将导致容器启动失败，它会根据 Pod 的 restartPolicy 指定的策略进行重试。然而，如果 Pod 的 restartPolicy 设置为 Always，Init 容器失败时会使用 RestartPolicy 策略。\n在所有的 Init 容器没有成功之前，Pod 将不会变成 Ready 状态。Init 容器的端口将不会在 Service 中进行聚集。正在初始化中的 Pod 处于 Pending 状态，但应该会将 Initializing 状态设置为 true。Init 容器运行完成以后就会自动终止。\n关于 Init 容器的详细信息请参考 Init 容器 - Kubernetes 中文指南/云原生应用架构实践手册 。\nSidecar 注入示例分析 本文我们将以 Istio 官方示例 bookinfo 中 reivews 服务为例，来接讲解 Sidecar 容器注入的额流程，每个注入了 Sidecar 的 Pod 中除了原先应用的应用本身的容器外，都会多出来这样两个容器：\nistio-init：用于给 Sidecar 容器即 Envoy 代理做初始化，设置 iptables 端口转发 istio-proxy：Envoy 代理容器，运行 Envoy 代理 接下来将分别解析下这两个容器。\nInit 容器解析 Istio 在 Pod 中注入的 Init 容器名为 istio-init，如果你查看 reviews Deployment 配置，你将看到其中 initContaienrs 的启动参数：\ninitContainers: - name: istio-init image: docker.io/istio/proxyv2:1.13.1 args: - istio-iptables - \u0026#39;-p\u0026#39; - \u0026#39;15001\u0026#39; - \u0026#39;-z\u0026#39; - \u0026#39;15006\u0026#39; - \u0026#39;-u\u0026#39; - \u0026#39;1337\u0026#39; - \u0026#39;-m\u0026#39; - REDIRECT - \u0026#39;-i\u0026#39; - \u0026#39;*\u0026#39; - \u0026#39;-x\u0026#39; - \u0026#39;\u0026#39; - \u0026#39;-b\u0026#39; - \u0026#39;*\u0026#39; - \u0026#39;-d\u0026#39; - 15090,15021,15020 我们看到 istio-init 容器的入口是 istio-iptables 命令，该命令是用于初始化路由表的。\nInit 容器启动入口 Init 容器的启动入口是 /usr/local/bin/istio-iptable 命令，该命令的用法如下：\n$ istio-iptables -p PORT -u UID -g GID [-m mode] [-b ports] [-d ports] [-i CIDR] [-x CIDR] [-h] -p: 指定重定向所有 TCP 流量的 Envoy 端口（默认为 $ENVOY_PORT = 15001） -u: 指定未应用重定向的用户的 UID。通常，这是代理容器的 UID（默认为 $ENVOY_USER 的 uid，istio_proxy 的 uid 或 1337） -g: 指定未应用重定向的用户的 GID。（与 -u param 相同的默认值） -m: 指定入站连接重定向到 Envoy 的模式，“REDIRECT” 或 “TPROXY”（默认为 $ISTIO_INBOUND_INTERCEPTION_MODE) -b: 逗号分隔的入站端口列表，其流量将重定向到 Envoy（可选）。使用通配符 “*” 表示重定向所有端口。为空时表示禁用所有入站重定向（默认为 $ISTIO_INBOUND_PORTS） -d: 指定要从重定向到 Envoy 中排除（可选）的入站端口列表，以逗号格式分隔。使用通配符“*” 表示重定向所有入站流量（默认为 $ISTIO_LOCAL_EXCLUDE_PORTS） -i: 指定重定向到 Envoy（可选）的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量。空列表将禁用所有出站重定向（默认为 $ISTIO_SERVICE_CIDR） -x: 指定将从重定向中排除的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量（默认为 $ISTIO_SERVICE_EXCLUDE_CIDR）。 -z: 所有入站 TCP 流量重定向端口（默认为 $INBOUND_CAPTURE_PORT 15006） 关于该命令的详细代码请查看 GitHub：tools/istio-iptables/pkg/cmd/root.go 。\n再参考 istio-init 容器的启动参数，完整的启动命令如下：\n$ /usr/local/bin/istio-iptables -p 15001 -z 15006 -u 1337 -m REDIRECT -i \u0026#39;*\u0026#39; -x \u0026#34;\u0026#34; -b * -d \u0026#34;15090,15201,15020\u0026#34; 该容器存在的意义就是让 Envoy 代理可以拦截所有的进出 Pod 的流量，即将入站流量重定向到 Sidecar，再拦截应用容器的出站流量经过 Sidecar 处理后再出站。\n命令解析\n这条启动命令的作用是：\n将应用容器的所有流量都转发到 Envoy 的 15006 端口。 使用 istio-proxy 用户身份运行，UID 为 1337，即 Envoy 所处的用户空间，这也是 istio-proxy 容器默认使用的用户，见 YAML 配置中的 runAsUser 字段。 使用默认的 REDIRECT 模式来重定向流量。 将所有出站流量都重定向到 Envoy 代理。 将除了 15090、15201、15020 端口以外的所有端口的流量重定向到 Envoy 代理。 因为 Init 容器初始化完毕后就会自动终止，因为我们无法登陆到容器中查看 iptables 信息，但是 Init 容器初始化结果会保留到应用容器和 Sidecar 容器中。\nistio-proxy 容器解析 为了查看 iptables 配置，我们需要登陆到 Sidecar 容器中使用 root 用户来查看，因为 kubectl 无法使用特权模式来远程操作 docker 容器，所以我们需要登陆到 reviews Pod 所在的主机上使用 docker 命令登陆容器中查看。\n查看 reviews Pod 所在的主机。\n$ kubectl -n default get pod -l app=reviews -o wide NAME READY STATUS RESTARTS AGE IP NODE reviews-v1-745ffc55b7-2l2lw 2/2 Running 0 1d 172.33.78.10 node3 从输出结果中可以看到该 Pod 运行在 node3 上，使用 vagrant 命令登陆到 node3 主机中并切换为 root 用户。\n$ vagrant ssh node3 $ sudo -i 查看 iptables 配置，列出 NAT（网络地址转换）表的所有规则，因为在 Init 容器启动的时候选择给 istio-iptables.sh 传递的参数中指定将入站流量重定向到 Envoy 的模式为“REDIRECT”，因此在 iptables 中将只有 NAT 表的规格配置，如果选择 TPROXY 还会有 mangle 表配置。iptables 命令的详细用法请参考 iptables ，规则配置请参考 iptables 规则配置 。\n理解 iptables iptables 是 Linux 内核中的防火墙软件 netfilter 的管理工具，位于用户空间，同时也是 netfilter 的一部分。Netfilter 位于内核空间，不仅有网络地址转换的功能，也具备数据包内容修改、以及数据包过滤等防火墙功能。\n在了解 Init 容器初始化的 iptables 之前，我们先来了解下 iptables 和规则配置。\n下图展示了 iptables 调用链。\niptables 调用链 iptables 中的表 Init 容器中使用的的 iptables 版本是 v1.6.0，共包含 5 张表：\nraw 用于配置数据包，raw 中的数据包不会被系统跟踪。 …","relpermalink":"/blog/envoy-sidecar-injection-in-istio-service-mesh-deep-dive/","summary":"以往有很多文章讲解 Istio 是如何做 Sidecar 注入的，但是没有讲解注入之后 Sidecar 工作的细节。本文将带大家详细了解 Istio 是如何将 Envoy 作为 Sidecar 的方式注入到应用程序 Pod 中，及 Sidecar 是如何做劫持流量的。","title":"理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持"},{"content":"这是一篇后Kubernetes时代的檄文。就在今天傍晚我看到了一篇 Bilgin Ibryam 的文章 Microservices in a Post-Kuberentes Era 有感而发。\n2017 年 4 月 9 日，Kubernetes Handbook - Kubernetes中文指南/云原生应用架构实践手册 第一次提交。在过去的 16 个月的时间里，有 53 位贡献者参与，1088 次 commit，共写了 23,9014 个汉字，同时在Kubernetes\u0026amp;Cloud Native 实战群里也聚集了几千名爱好者。\n距离上一版本发布已经有 4 个多月的时间了，在此期间Kubernetes 和Prometheus 分别从 CNCF 中毕业，已经在商业上成熟，这两个项目基本成型，未来也不会有太大的变动。而当初为容器编排而开发，为了解决微服务部署问题的 Kubernetes 已经深入人心，目前的微服务已经逐步进入后 Kubernetes 时代，Service Mesh 和云原生重新定义微服务和分布式应用。\n本版本发布时 PDF 大小为 108M，共 239,014 个汉字，建议在线浏览 ，或克隆本项目安装 Gitbook 命令后自行编译。\n本版本主要有以下改进：\n增加了Istio Service Mesh 教程 增加了 使用 Vagrant 和 VirtualBox 在本地搭建分布式 Kubernetes 集群和 Istio Service Mesh 增加了对云原生编程语言Ballerina 和Pulumi 的介绍 增加了快速开始指南 增加了对 Kubernetes 1.11 的支持 增加了企业级 Service Mesh 采用路径指南 增加了SOFAMesh 章节 增加了对云原生未来的展望 增加了CNCF 章程 和参与事项 增加了 Docker 镜像仓库的注意事项 增加了Envoy 章节 增加了KCSP（Kubernetes 认证服务提供商） 和CKA（认证 Kubernetes 管理员） 相关说明 更新了一些配置文件、YAML 和参考链接 更新了CRI 章节 删除了过时的描述 改进了 etcdctl 的命令使用教程 修复了一些笔误 浏览与下载 在线浏览：https://jimmysong.io/kubernetes-handbook GitHub 地址：https://github.com/rootsongjc/kubernetes-handbook 为了方便大家下载，我放了一份在微云 上，提供 PDF（108MB）、MOBI（42MB）、EPUB（53MB）格式下载。 感谢 Kubernetes 热心用户对本书的支持，感谢各位Contributors ，在该版本发布之前的几个月内又合作成立了ServiceMesher 社区 ，作为后 Kubernetes 时代的一支生力军，欢迎联系我 加入社区，共同开创云原生新时代。\n目前ServiceMesher 社区微信群也有几千名成员，Kubernete Handbook 仍然会继续下去，但是 Service Mesh 已然是一颗冉冉上升的新星，在对 Kubernetes 已了然于胸的情况下，欢迎加入ServiceMesher 社区 。\n","relpermalink":"/notice/new-kubernetes-handbook-released-and-say-hello-to-post-kubernetes-era/","summary":"这是一篇后 Kubernetes 时代的檄文，Kubernetes handbook by Jimmy Song v1.4 发布，云原生的下一个重心是 Service Mesh！","title":"Kubernetes Handbook v1.4 发布同时后 Kubernetes 时代大幕拉启"},{"content":"今天，我们很高兴地宣布 Istio 1.0 。这距离最初的 0.1 版本发布以来已经过了一年多时间了。从 0.1 起，Istio 就在蓬勃发展的社区、贡献者和用户的帮助下迅速发展。现在已经有许多公司成功将 Istio 应用于生产，并通过 Istio 提供的洞察力和控制力获得了真正的价值。我们帮助大型企业和快速发展的创业公司，如 eBay 、Auto Trader UK 、Descartes Labs 、HP FitStation 、Namely 、PubNub 和 Trulia 使用 Istio 从头开始连接、管理和保护他们的服务。将此版本作为 1.0 发布是对我们构建了一组核心功能的认可，用户们可以依赖这些功能进行生产。\n生态系统 去年，我们看到了 Istio 生态系统的大幅增长。Envoy 继续其令人印象深刻的增长，并增加了许多对生产级别服务网格至关重要的功能。像 Datadog 、 SolarWinds 、 Sysdig 、Google Stackdriver 和 Amazon CloudWatch 这样的可观察性提供商也编写了插件来将 Istio 与他们的产品集成在一起。Tigera 、Aporeto 、Cilium 和 Styra 为我们的策略实施和网络功能构建了扩展。Red Hat 构建的 Kiali 为网格管理和可观察性提供了良好的用户体验。Cloud Foundry 正在为 Istio 建立下一代流量路由堆栈，最近宣布的 Knative 无服务器项目也正在做同样的事情，Apigee 宣布计划在他们的 API 管理解决方案中使用它。这些只是社区去年增加的项目的一些汇总。\n功能 自 0.8 发布以来，我们添加了一些重要的新功能，更重要的是将许多现有的功能标记为 Beta 表明它们可以用于生产。这在发行说明 中有更详细的介绍，但值得一提是：\n现在可以将多个 Kubernetes 集群添加到单个网格中 ，并启用跨集群通信和一致的策略实施。多集群支持现在是 Beta。 通过网格实现对流量的细粒度控制的网络 API 现在是 Beta。使用网关显式建模 ingress 和 egress 问题，允许运维人员控制网络拓扑 并满足边缘的访问安全要求。 现在可以增量上线 双向 TLS，而无需更新服务的所有客户端。这是一项关键功能，可以解除在现有生产上部署采用 Istio 的障碍。 Mixer 现在支持开发进程外适配器 。这将成为在即将发布的版本中扩展 Mixer 的默认方式，这将使构建适配器更加简单。 现在，Envoy 在本地完全评估了控制服务访问的授权策略 ，从而提高了它们的性能和可靠性。 Helm chart 安装 现在是推荐的安装方法，提供丰富的自定义选项，以便根据您的需求配置 Istio。 我们在性能方面投入了大量精力，包括连续回归测试、大规模环境模拟和目标修复。我们对结果非常满意，并将在未来几周内详细分享。 下一步 虽然这是该项目的一个重要里程碑，但还有很多工作要做。在与采用者合作时，我们已经获得了很多关于下一步要关注的重要反馈。我们已经听到了关于支持混合云、安装模块化、更丰富的网络功能和大规模部署可扩展性的一致主题。我们在 1.0 版本中已经考虑到了一些反馈，在未来几个月内我们将继续积极地处理这些工作。\n快速开始 如果您是 Istio 的新手，并希望将其用于部署，我们很乐意听取您的意见。查看我们的文档 ，访问我们的聊天论坛 或访问邮件列表 。如果您想更深入地为该项目做出贡献，请参加我们的社区会议 并打个招呼。\n最后 Istio 团队非常感谢为项目做出贡献的每个人。没有你们的帮助，它不会有今天的成就。去年的成就非常惊人，我们期待未来与我们社区成员一起实现更伟大的成就。\nServiceMesher 社区 负责了 Istio 官网中文内容的翻译和维护工作，目前中文内容还未完全与英文内容同步，需要手动输入 URL 切换为中文（https://istio.io/zh ），还有很多工作要做，欢迎大家加入和参与进来。\n","relpermalink":"/notice/istio-v1-released/","summary":"中文文档同时释出！","title":"Istio 1.0 发布，生态逐步壮大，且可用于生产！"},{"content":" 注意 SOFAMesh 已闭源，本文已过时。 4 月，蚂蚁集团自主研发的分布式中间件（Scalable Open Financial Architecture，以下简称 SOFA）启动开源计划，并开放多个组件，（相关背景请点击链接阅读《开源 | 蚂蚁集团启动分布式中间件开源计划，用于快速构建金融级云原生架构 》、《开源 | 蚂蚁集团分布式中间件开源第二弹：丰富微服务架构体系 》），这一系列的动作受到大家的关注和支持，SOFA 社区也日益壮大。\n在两轮开源之后，蚂蚁集团自主研发的分布式中间件（Scalable Open Financial Architecture，以下简称 SOFA）在今天推出了第三轮的开源产品：SOFAMesh。和前两轮开源的历经多年沉淀和打磨的成熟产品不同，本轮的开源主角 SOFAMesh，将探索一条和以往产品有所不同的开源道路。下面我们就来看看到底有哪些不同吧！\nSOFAMesh 的开源探索之路 SOFAMesh 尝试在以下几个方面进行自我突破和勇敢探索：\n全新的技术领域\nService Mesh 是目前技术社区最为炙手可热的新技术方向，有下一代微服务的明显趋势。但是目前 Service Mesh 技术还处于发展早期，暂时还没有成熟的产品，尤其缺乏大规模的落地实践。\n较早的开源时间\n在上述背景下，我们选择了将启动不久的 Service Mesh 产品开源在开发早期，也就是还未成熟之时，就对社区开放，开放源码并寻求社区合作。\n更加开放的态度\n在 SOFAMesh 上，我们愿意以开源共建的方式来和社区一起推进 Service Mesh 技术的更好发展和实现落地实践，共同打造一个技术先进，功能丰富，具备良好的性能和稳定性，可以实实在在的生产落地的优秀产品。欢迎国内技术社区的朋友们和我们开展不同层面的交流与合作。\n务实的产品路线\nSOFAMesh 在产品路线上，选择了跟随社区主流，我们选择了目前 Service Mesh 中最有影响力和前景的 Istio。SOFAMesh 会在 Istio 的基础上，提升性能，增加扩展性，并在落地实践上做探索和补充，以弥补目前 Istio 的不足，同时保持与 Istio 社区的步骤一致和持续跟进。\nSOFAMesh 介绍 SOFAMesh 将在兼容 Istio 整体架构和协议的基础上，做出部分调整：\n使用 Golang 语言开发全新的 Sidecar，替代 Envoy 为了避免 Mixer 带来的性能瓶颈，合并 Mixer 部分功能进入 Sidecar Pilot 和 Citadel 模块进行了大幅的扩展和增强 我们的目标：打造一个更加务实的 Istio 落地版本！\n备注 以上架构调整的细节以及我们做调整的出发点和原因，请浏览 蚂蚁集团大规模微服务架构下的 Service Mesh 探索之路 一文，有非常详尽的解释。 开源内容 在本轮开源中，我们将推出 SOFAMesh 目前正在开发的两大模块：MOSN 和 SOFAPilot。\nMOSN SOFAMesh 中 Golang 版本的 Sidecar，是一个名为 MOSN (Modular Observable Smart Netstub) 的全新开发的模块，实现 Envoy 的功能，兼容 Envoy 的 API，可以和 Istio 集成。\nMOSN 架构图 此外，我们会增加对 SOFARPC、Dubbo 等通讯协议的支持，以便更好的迎合国内用户包括我们自身的实际需求。\n由于 Sidecar 相对独立，而且我们也预期会有单独使用 MOSN 的场景，因此 MOSN 的代码仓库是独立于 SOFAMesh 的，地址为：https://github.com/mosn/mosn\n欢迎大家使用，提供需求、反馈问题、贡献代码或者合作开发。\nSOFAPilot 我们将大幅扩展和增强 Istio 中的 Pilot 模块：\n增加 SOFARegistry 的 Adapter，提供超大规模服务注册和发现的解决方案 增加数据同步模块，以实现多个服务注册中心之间的数据交换。 增加 Open Service Registry API，提供标准化的服务注册功能 MOSN 和 SOFAPilot 配合，将可以提供让传统侵入式框架（如 Spring Cloud，Dubbo，SOFA RPC 等）和 Service Mesh 产品可以相互通讯的功能，以便可以平滑的向 Service Mesh 产品演进和过渡。\nPilot 和后面会陆续开放的 Mixer，Citadel 等 Istio 模块，会统一存放在同一个从 Istio Fork 出来的代码仓库中。未来会持续更新 Istio 最新代码，以保持和 Istio 的一致。\n附录 本文中提到的链接地址集合：\nMOSN 蚂蚁集团大规模微服务架构下的 Service Mesh 探索之路 ","relpermalink":"/blog/sofamesh-and-mosn-proxy-sidecar-service-mesh-by-ant-financial/","summary":"蚂蚁集团开源 SOFAMesh—一款基于 Istio 改进和扩展而来的 Service Mesh 大规模落地实践方案。","title":"蚂蚁集团开源 SOFAMesh"},{"content":" 2018 年 6 月 18 日 Joe Duffy 在他的博客 中宣布开源了云原生编程语言Pulumi 。这是继Ballerina 之后我看到的另一款云原生编程语言，他们之间有一些共同的特点，例如都是为了支持多种云环境，基于不可变基础设施和基础设施即代码的理念构建，使云原生应用的集成更加方便，但也有一些不同，Ballerina 是直接创建了一个基于 JVM 的语言，而 Pulumi 是为不同编程语言构建了 SDK。\n下文是对\tHello, Pulumi! 一文的翻译。\n今天我们发布了 Pulumi，这是一款开源的云开发平台。有了 Pulumi，您可以使用自己最喜欢的编程语言来开发云应用程序，可以直接跨过底层的基础设施即代码直接开发更高效更具生产力的现代容器和 serverless 应用。一年多前我们发起了 Pulumi，我们都为自己在这一年多取得的成绩感到惊讶。这是我们开源的第一步，也是我们做出的一步重大跨越，我们期望与您分享我们的成就。\nPulumi 支持多语言、混合云环境、完全可扩展。初期支持 JavaScript、TypeScript、Python 和 Go 语言，支持 AWS、Azure、GCP 云平台，另外还支持所有兼容 Kubernetes 的公有云、私有云和混合云。Pulumi 实现了一种单一、一致的编程模型，一组编程工具，可管理所有以上环境，丰富的生态系统支持大量可复用的包。使用真实的语言来改变一切。\nTL;DR 有了 Pulumi，38 页的手动操作说明将变成了 38 行代码。25000 行 YAML 配置变成了使用真实编程语言的 500 行语句。\nPulumi 的整个运行时、CLI、支持的库都可以在 GitHub 上免费下载。我们的团队正急切的等待您的反馈。与此同时，我需要告诉您一些关于 Pulumi 的事情，为什么我们会创造它。\n为什么创造 Pulumi？ 我的背景是 100% 在开发者工具上。我是.NET 的早期工程师，设计了其中的并发和异构部分，领导的分布式操作系统的变成平台，管理微软语言小组，包括开源和使.NET Core 跨平台。因为这些背景，进入云领域我有自己的见解。\n我发现的东西显然无法吸引我。\n2016 年年末的时候我就开始跟我的朋友也是共同创始人 Eric Rudder 开始构思 Pulumi，那时候容器和 serverless 已经甚嚣尘上，但是距离落地还为时尚早。云的能力十分惊人，但是至今将使用它还是十分困难。\n对于每一个 serverless 函数来说，我都要写几十行的 JSON 或者 YAML 配置。要链接到一个 API 端点，我还要学习晦涩的概念，执行一系列复制 - 粘贴的低级工作。如果我想在本机上运行一个小的集群的话，那么 Docker 还是很棒的，但是如果要在生产上使用的话，那么就要手动管理 etcd 集群，配置网络和 iptables 路由表，还有一系列与我的应用程序本身不相干的事情。不过 Kubernetes 的出现至少让我可以配置一次下次就可以跨云平台重用，但这还是会分散开发人员的精力。\n我认为我还算一个经验丰富的工程师，已经在软件行业从业 20 年了，但是当我想要将自己的代码部署到云中的时候，我感觉自己就像是个傻子。真是太令人悲哀了！如果我掌握了这些能力，那么是世界就会出触手可及。我总是在淌这浑水，处理云的复杂性，而我真正想做的是花时间来创造业务价值。\n关于编程的许多方面都经历了类似的转变过程：\n在 80 年代初，我们使用汇编语言对微处理器进行了编程。最终，编译器技术进步了，我们可以同时处理多种常见的架构。像 FORTRAN 和 C 这样的 Low-level 的编程语言开始兴起。 在 90 年代初期，我们直接针对低级别操作系统原语进行编程，无论是 POSIX 系统调用还是 Win32 API，并进行手动内存和资源管理。最终，语言运行时技术和处理器速度提升到了可以使用更高级别语言的状态，如 Java。除了动态语言之外，这种趋势已经加速，如 JavaScript 统治了 Web。 在 21 世纪初期，我们的编程模型中的共享内存并发性最好是原始的（我花了很多时间在这个问题上 ）。现在，我们简单地假设 OS 具有高级线程共享、调度和异步 IO 功能，以及编程到更高级别的 API，例如任务和承诺。 我相信云软件也在进行类似的转变。从构建单一应用程序到构建真正的云优先分布式系统，我们正处在一场巨变中。然而，当海啸发生之前，人们几乎不知道它正在发生。\n从上面的角度来看，使用“配置”情况是有道理的。在虚拟机的早期，我们利用现有的应用程序并将它们扔在栅栏上，以便有人添加一点 INI 或 XML 粘合剂，让它们在虚拟机内部运行，以实现更灵活的管理。随着我们将这些相同的虚拟机“提升并转移到云中”，这种配置方法一直伴随着我们。这将我们带到了大致正确的边界。\n使用这种相同类型的配置表示基于容器的微服务、serverless 和细粒度托管服务之间的关系导致了异常的复杂性。将应用程序转变为分布式系统应该是事后的想法。事实证明，云覆盖了您的架构和设计。表达架构和设计的最好的方式是使用代码，使用真正的编程语言编写抽象，重用和优秀的工具。\n早些时候，Eric 和我采访了几十个客户。我们发现，开发人员和 DevOps 工程师都普遍感到幻灭。我们发现了极端的专业化，即使在同一个团队中，工程师也不会使用同一种语言。最近几周我已经听到了这个消息，我期待有一天会出现 NoYAML 运动。\n专业化是一件好事，我们希望我们最优秀和最聪明的云计算架构师晋升到 DevOps 和 SRE 的高级职位，但团队必须能够在合作时使用相同的语言。因为没有共同的通用语言导致了团队之间的物理隔离，而不是根据策略和环境分工。Pulumi 的目标是为人们提供解决这个问题所需的工具。\nPulumi 是什么？ Pulumi 是一个支持多语言和混合云开发平台。它可以让您使用真实语言和真实代码创建云计算的各个方面，从基础设施到应用程序本身。只需编写程序并运行它们，Pulumi 就能帮你完成出其余部分。\nPulumi 的中心是一个云对象模型，与运行时相结合以了解如何以任何语言编写程序，理解执行它们所需的云资源，然后以强大的方式规划和管理您的云资源。这种云运行时和对象模型本质上是与语言、云中立的，这就是为什么我们能够支持如此多的语言和云平台。更多支持正在路上。\nPulumi 采用了基础设施即代码以及不可变基础设施的概念，并可让您从您最喜欢的语言（而不是 YAML 或 DSL）中获得自动化和可重复性优势。在部署它们之前，您可以对变更进行区分，并且我们会对谁更改了什么以及何时更改进行完善的审计追踪。核心模型因此是陈述性的。\n使用真正的语言可以带来巨大的好处：\n熟悉：不需要学习新的定制 DSL 或基于 YAML 的模板语言 抽象：正如我们喜爱的编程语言那样，我们可以用更小的东西来构建更大的东西 共享和重用：利用现有的语言包管理器共享和重用这些抽象，无论是与社区、团队内部共享 表现力：充分利用您的编程语言，包括异步、循环和条件 工具：通过使用真正的语言，我们可以即时访问 IDE、重构、测试、静态分析和编排等等 生产力：将以上所有好处加在一起，一起将变得更快，我们也会变得更快乐 当提供原始云资源时，这些好处当然最重要，但是我们在团队中发现，您只能使用抽象。这包括在函数中包装事物以消除样板并创建引入更高级别概念的自定义类，通常将它们打包并重复使用。\n例如，此代码在 AWS 中创建一个 DynamoDB 数据库：\nimport * as aws from \u0026#34;@pulumi/aws\u0026#34;; let music = new aws.dynamodb.Table(\u0026#34;music\u0026#34;, { attributes: [ { name: \u0026#34;Album\u0026#34;, type: \u0026#34;S\u0026#34; }, { name: \u0026#34;Artist\u0026#34;, type: \u0026#34;S\u0026#34; }, ], hashKey: \u0026#34;Album\u0026#34;, rangeKey: \u0026#34;Artist\u0026#34;, }); 此代码创建一个基于容器的任务和无服务器功能，由一个存储桶触发：\nimport * as cloud from \u0026#34;@pulumi/cloud\u0026#34;; let bucket = new cloud.Bucket(\u0026#34;bucket\u0026#34;); let task = new cloud.Task(\u0026#34;ffmpegThumbTask\u0026#34;, { build: \u0026#34;./path_to_dockerfile/\u0026#34;, }); bucket.onPut(\u0026#34;onNewVideo\u0026#34;, bucketArgs =\u0026gt; { let file = bucketArgs.key; return task.run({ environment: { \u0026#34;S3_BUCKET\u0026#34;: bucket.id.get(), \u0026#34;INPUT_VIDEO\u0026#34;: file, \u0026#34;TIME_OFFSET\u0026#34;: file.substring(file.indexOf(\u0026#39;_\u0026#39;)+1, file.indexOf(\u0026#39;.\u0026#39;)).replace(\u0026#39;-\u0026#39;,\u0026#39;:\u0026#39;), \u0026#34;OUTPUT_FILE\u0026#34;: file.substring(0, file.indexOf(\u0026#39;_\u0026#39;)) + \u0026#39;.jpg\u0026#39;, }, }); }); 更好的是，这些代码可以根据您的需求部署到任何公共或私有云中。\n最后，这个例子创建了一个 Redis 缓存。我们怎么知道？我们不需要。缓存组件是一个抽象，它封装了我们可以安全忽略的不重要的细节：\nimport {Cache} from \u0026#34;./cache\u0026#34;; let cache = new Cache(\u0026#34;url-cache\u0026#34;); 在使用 Pulumi 之后，你不会再以同样的方式考虑基础设施。你的大脑将不再是一个独立于应用程序的独特“事物”，而是开始将分布式云系统看作是你的程序架构的核心部分，而不是事后的想法。\n由于抽象，我们已经能够提供一些强大的库。该库是提炼和执行最佳实践的绝佳方式。当然，对于我们自己的库来说没有什么特别的，因为它们只是功能、类和代码，我们期待着看到你为自己、你的团队或者社区建立的那些库。\n我们最复杂的库——Pulumi 云框架——提供了一些令人兴奋的正在进行的工作的早期预览，展示如何创建跨越云提供商自己对诸如容器、无服务器功能和存储桶等核心概念的抽象。以同样的方式，您可以用 Node.js、Python、Java、.NET 等语言编写功能强大的应用程序，利用进程、线程和文件系统，无论是在 macOS、Linux 还是 Windows 上，这种方法都可以让您创建针对任何云提供商的现代混合云应用程序。像 Kubernetes 和其他 CNCF 产品组合这样的技术正在帮助推动这一不可避免的结果，因为它们在整个云基板上实现了对基本计算抽象的民主化和共识。\nPulumi 不是 PaaS，尽管它提供类似 PaaS 的生产力；您的程序总是直接针对您选择的云运行，并且始终可以访问该基础云的全部功能。即使您选择使用更高级别的组件，它也会向下兼容，并且您可以随时直接使用原始资源。它就像任何复杂的现代软件：有时，整个事情必须用 C++编写，以便访问底层平台的全部功能，但对于大多数常见情况，70% 到 100％可以是平台独立代码，而只有不到 30% 的专业化才能真正需要直接与操作系统交互。\n接下来我还将发布十几篇博客文章来介绍 Pulumi 所有方面的更多细节。然而，为了保持这篇文章尽量简短，我将首先介绍下 Pulumi 的一些我最喜欢的方面。\n我最喜欢的东西 这很难选择，但这里有一些关于 Pulumi 我最喜欢的东西：\n开源。我坚信所有开发人员工具都应该是开源的。当然，Pulumi 也是一家公司，但是有充足的机会通过增加便利性以建立商业模式。（可以认为是​​Git 与 GitHub 的关系）我们从以前的工作中受益匪浅，其中包括 Docker、Terraform、Kubernetes、TypeScript 以及其他许多明确提及的工作。我们期待 …","relpermalink":"/blog/hello-pulumi-from-jeo-duffy/","summary":"2018 年 6 月 18 日 Joe Duffy 在他的博客中宣布开源了云原生编程语言 Pulumi。","title":"云原生编程语言 Pulumi 开源宣言"},{"content":" 如果有一个可视化的，提供基础设施集群给你操作的学习模式和平台，你会为此买单吗？\n两个月前，我在 Kubernetes 的 Slack channel 中结识了 Jord，后来又在 Taiwan Kubernetes User Group 的 Facebook group 里看到了他（也有可能是其他人）发的 MagicSandbox.io 的链接，我就点击申请试用了，后来又收到了 Jord 发来的邮件，他告诉了我他想打造一个 Kubernetes 学习平台的事情。这就是整件事情的缘起，再后来我们有几次 Zoom 视频聊了很久。\n关于 MagicSandbox MagicSandbox 是一家创业公司，Jord（荷兰人）也是一个连续创业者，他 19 岁起曾在中国的四川大学留学了 4 年，后来回到了德国，他曾在 Boston Consulting Group 做 PM，现就职于 Entrepreneur First（欧洲顶级的风险投资/创业孵化器）德国柏林分公司，也是在那里，他结识了 Mislav（克罗地亚人），Mislav 是一名全栈工程师，也有几次创业经历，他们气味相投，一拍即合，他们决定致力于互联网教育行业，创造世界级的软件工程师教育平台，他们想从 Kubernetes 入手，首先提供基于互联网的 Kubernetes 的理论与实践教学，然后将主题扩大到 ElasticSearch、GraphQL 等等涉及到分布式系统的话题。\nJord 在自己的家中创办了 MagicSandbox，而我成为了 MagicSandbox 在中国的代言人。\n现在我们要发布的是 MagicSandbox Alpha 版本，该版本是一个未成熟的版本，提供给大家免费试用，也欢迎大家积极反馈。\n官方首页：https://magicsandbox.com/ 中文页面：https://cn.magicsandbox.com/ （内容尚未汉化，目前仅提供了中文版首页） 在 Twitter 上关注我们：https://twitter.com/magicsandbox ","relpermalink":"/notice/magicsandbox-alpha-version-annoucement/","summary":"在线实践式软件工程教育平台。","title":"MagicSandbox Alpha 版本发布"},{"content":"众所周知 Kubernetes 并不提供代码构建、发布和部署，所有的这些工作都是由 CI/CD 工作流完成的，最近 TheNewStack 又出了本小册子（117 页）介绍了 Kubernetes 中 CI/CD 的现状，下载本书的 PDF 。\n关于本书 本书的作者有：\nRob Scott：ReactiveOps 公司的 SRE Janakiram MSV：Janakiram \u0026amp; Associates 的首席分析师 Craig Martin：Kenzan 的高级副总裁 Container Solutions 这本小册子里主要主要介绍了以下几点：\nDevOps 模式 云原生应用模式 使用 Spinnaker 做持续交付 云原生时代的监控 DevOps 模式 这一章从一些流行的自动化运维工具讲起，比如 Chef、Puppet 等，引申出 CI/CD 流水线，进而引出 Docker 和 DevOps，将容器如何解除开发和运维之间的隔阂，但同时也带来了一些挑战，比如频繁的发布变更如何控制，如何控制容器集群的行为，如何拆分应用到容器之中等。这是一个专门用于容器编排调度的工具呼之欲出，Kubernetes 的出现彻底改变了局面，可以说它直接改变了应用的基础架构。\nKubernetes 细化的应用程序的分解粒度，同时将服务发现、配置管理、负载均衡和健康检查等作为基础设施的功能，简化了应用程序的开发。\n而Kubernetes这种声明式配置尤其适合CI/CD流程，况且现在还有如Helm、Draft、Spinnaker、Skaffold等开源工具可以帮助我们发布Kuberentes应用。\n有了基于Kubernetes的CI/CD流程后，又诞生了GitOps（WeaveWorks 的博客中有很多相关文章）和 SecOps（Security Operation）。\n云原生应用模式 云原生是通过构建团队、文化和技术，利用自动化和架构来管理系统的复杂性和解放生产力。——Joe Beda，Heptio CTO，联合创始人\n这一章的重点是给出了云原生应用的 10 条关键属性。\n使用轻量级的容器打包 使用最合适的语言和框架开发 以松耦合的微服务方式设计 以 API 为中心的交互和协作 无状态和有状态服务在架构上界限清晰 不依赖于底层操作系统和服务器 部署在自服务、弹性的云基础设施上 通过敏捷的 DevOps 流程管理 自动化能力 通过定义和策略驱动的资源分配 作者然后将应用程序架构中的不同组件映射到云原生的工作负载中。\n这也是 DevOps 需要关注的部分，如何将云原生的组件映射为 Kubernetes 的原语（即 Kubernetes 里的各种资源对象和概念组合）呢？\n总结概括为以下 10 条：\n不要直接部署裸的 Pod。 为工作负载选择合适的 Controller。 使用 Init 容器确保应用程序被正确的初始化。 在应用程序工作负载启动之前先启动 service。 使用 Deployment history 来回滚到历史版本。 使用 ConfigMap 和 Secret 来存储配置。 在 Pod 里增加 Readiness 和 Liveness 探针。 给 Pod 这只 CPU 和内存资源限额。 定义多个 namespace 来限制默认 service 范围的可视性。 配置 HPA 来动态扩展无状态工作负载。 使用 Spinnaker 进行持续交付 作者首先讲到了 Spinnaker 的各种特性，比如面向微服务啦，云原生的交付工具啦，可视化的交付和基础设施啦，支持多个 region，支持容器和 Kubernetes 等等，不一而足，感兴趣大家可以自己看下报告或者登陆Spinnaker 官网 查看。\n总之作者就是想说 Spinnaker 很好很强大啦，足以满足您对云原生应用 CI/CD 的需求。\n云原生时代的监控 监控是为了实现系统的可观察性，不要以为监控就是简单的出个监控页面，监控其实包括以下部分：\n日志收集 监控和指标度量 追踪 告警和可视化 要把其中任何一个方面做好都不容易。作者主要讲述的 Prometheus 和 Grafana 的开源监控方案。这一章我不详述，感兴趣大家可以查看报告原文。\n","relpermalink":"/blog/ci-cd-in-kubernetes/","summary":"TheNewStack 的报告解读，介绍了 Kubernetes 中 CI/CD 的现状。","title":"Kubernetes中的CI/CD"},{"content":"还记得我之前分享的云原生编程语言终于出现！一文带你了解 Ballerina！ 吗？他们准备来参加 KubeCon\u0026amp;CloudNativeCon 中国大会了！\nKubeCon\u0026amp;CloudNativeCon 中国大会将于2018 年 11 月 14-15 日（星期三、星期四）在上海举行。见：https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2018/\n经 Ballerina 公司官方授权，现在我需要帮他们在中国找一位“大使”，负责团队接引、中英文翻译等事务，要求熟悉 Cloud Native 与微服务，在业界具有影响力，英语沟通无障碍。\n大使职责\n负责团队接引 负责品宣、PPT 材料等的中英文翻译 帮助布置展台 对方可以提供\n大会门票 差旅费 大会期间的住宿 其他补偿金 这是他们团队在今年五月份根本哈根的 KubeCon\u0026amp;CloudNativeCon 时在自己展台前的合影。\nP.S 这是我能找到的他们团队最全和拍的最好的一张照片了。（拍照技术亟待加强）\n简单介绍下这家名叫 Ballerina 的初创公司，他们的团队主要来自斯里兰卡，就是这里，位于南亚次大陆印度旁的一个岛国，中国古称其为“狮子国“，盛产宝石💎。\n他们国家的首都是斯里兰卡，用他们国家自己的语言念作：Si li jia ya wa de na pu la ke te\n如有感兴趣者欢迎直接联系我。\n","relpermalink":"/notice/a-ballerina-china-ambassador-required/","summary":"经 Ballerina 公司官方授权，现在我需要帮他们在中国找一位大使。","title":"云原生编程语言 Ballerina 公司征求中国大使一枚"},{"content":"Envoy 是一款由 Lyft 开源的，使用 C++ 编写的 L7 代理和通信总线，目前是 CNCF 旗下的开源项目，代码托管在 GitHub 上，它也是 Istio service mesh 中默认的 data plane。我们发现它有很好的性能，同时也不断有基于 Envoy 的开源项目出现，如 Ambassador 、Gloo 等，而目前 Envoy 的官方文档还没有得到很好的汉化，因此我们 Service Mesh 爱好者们觉得发动社区的力量共同翻译 Enovy 最新的（1.7 版本）的官方文档，并通过 GitHub 组织。\nService Mesh 爱好者们联合翻译了 Envoy 最新版本的官方文档 ，翻译的代码托管在 https://github.com/servicemesher/envoy ，如果你也是 Service Mesh 爱好者可以加入到 SerivceMesher GitHub 组织 里共同参与。\nEnvoy 官方文档排除 v1 API 参考 和 v2 API 参考的两个目录下的所有文章后一共有 120 余篇文档，文档的长短不一，原英文的官方文档都是使用 RST 格式，我手动将它们转成了 Markdown 格式，并使用 Gitbook 编译。按照文档相对于主目录的路径生成了 GitHub Issue，想参与翻译的朋友可以联系我 加入 ServiceMesher 组织，然后可以在 Issue 中选择你想要翻译的文章，然后回复“认领”。\n在这里 可以看到所有的贡献者。未来我们也会创建 Service Mesh 爱好者网站，网站使用静态页面，所有的代码都会托管在 Github 上，欢迎大家参与进来。\n","relpermalink":"/notice/enovy-doc-translation-start/","summary":"SerivceMesher 社区共同参与翻译 Envoy 最新版本的官方文档。","title":"Envoy 最新官方文档翻译工作启动"},{"content":"当我第一眼看到 Ballerina 还真有点惊艳的感觉。Ballerina 这个单词的意思是“芭蕾舞女演员”。我想他们之所以给公司和这们语言起这个名字，可能是希望它成为云原生这个大舞台中，Ballerina 能像一个灵活的芭蕾舞者一样轻松自如吧！\nBallerina 是一款开源的编译式的强类型语言，该语言本身的代码可以通过 GitHub 上获取。我们可以通过 Ballerina 官网上的设计哲学 页面来对这门云原生编程语言一探究竟。\n云原生编程语言 Ballerina 未来的应用程序应该是基于 API 的，而众多 API 之间的通讯和集成就成了关键问题。Ballerina 是一款使用文本和图形语法编译的、事务的、静态和强类型编程语言。Ballerina 包含分布式系统集成到语言的基本概念，并提供类型安全，并发环境下实现的分布式事务，可靠的消息传递，流处理和工作流。\n为什么创建 Ballerina？ 与 ESB 集成仍然是瀑布式开发。你必须部署服务器，配置连接器，使用 XML 编程服务逻辑以及使用 XPath 查询和转换数据。这不是开发者友好的。\n带有 Spring 和 Node.js 等框架的编程语言提供了灵活性，但是它没有使适合于序列并行化、并发模型编程的分布式系统结构变得简单。\nESB、EAI、BPM 和 DSL 需要 XML 和配置来中断迭代开发流程：编辑、构建、运行和测试。这与运行实际应用之间是有一条鸿沟的，而云原生编程语言 Ballerina 的出现就是为了解决这条“集成鸿沟”的。\nBallerina 设计理念 序列图 云原生编程语言 Ballerina 的序列图设计理念 语言灵感\n序列图反映了设计开发人员记录的互联的系统。Ballerina 的语法和高效的编码模式要求开发人员使用强大的交互最佳实践来编码。\n序列图可视化\nBallerina 的语言语义模型旨在定义独立的各方如何通过结构化的交互沟通。接着，每个 Ballerina 程序都可以显示为其流程的序列图。IntelliJ 和 VS Code 的插件中提供了这些可视化。Ballerina Composer 是一款通过序列图创建 Ballerina 服务的工具。\nActor 与 action\n客户端、worker 和远程系统在 Ballerina 的序列图中以不同的 actor 表示。在代码中，远程端点通过连接器进行连接，连接器提供类型安全操作。在图形上，每个连接器在序列图中表示为一个 actor（即一条垂直线），action 表示为与这些 actor 的交互。\n并发 云原生编程语言 Ballerina 的并发理念 序列图和并发\nBallerina 的并发模型是并行优先的，因为与远程方的交互总是涉及多个 worker。Worker 之间的交互作为消息传递进行处理，它们之间没有共享状态。\nWorker 语义\nBallerina 的执行模型由称为 woker 的轻量级并行执行单元组成。Worker 使用非阻塞策略来确保没有函数锁定正在执行的线程，例如等待响应的 HTTP I/O调用。\n编程模型\nWorker 和 fork/join 语义抽象了底层非阻塞方法，以启用更简单的并发编程模型。\n类型系统 下面是 Ballerina 中支持的类型。\nany anything; int integer = 0; float floatingPoint = 0.0; boolean b = true; string hi = \u0026#34;hello\u0026#34;; blob bl = hi.toBlob(\u0026#34;UTF-8\u0026#34;); json jsonNative = { a: \u0026#34;hello\u0026#34;, b: 5 }; xml x = xml `\u0026lt;ballerina\u0026gt; \u0026lt;supports\u0026gt;XML natively\u0026lt;/supports\u0026gt; \u0026lt;/ballerina\u0026gt;`; string[] stringArray = [\u0026#34;hi\u0026#34;, \u0026#34;there\u0026#34;]; int[][] arrayOfArrays = [[1,2],[3,4]]; json | xml | string unionType; (string, int) tuple = (\u0026#34;hello\u0026#34;, 5); () n = (); // the empty tuple acts as \u0026#34;null\u0026#34; string | int stringOrInt = \u0026#34;this is a union type\u0026#34;; int | () intOrNull = 5; var inferred = (\u0026#34;hello\u0026#34;, 5); map\u0026lt;boolean\u0026gt; myMap = {\u0026#34;ballerina\u0026#34;: true}; type myRecord { string a; int b; }; type myObject object { public { string x; } private { string y; } new (string xi, string yi) { x = xi; y = yi; } function getX() returns (string) { return x; } }; 类型安全\nBallerina 有一个结构化的类型系统，包括 primitive、recored、object、tuple 和 union 类型。该类型安全模型在赋值时包含了类型推断，并为连接器、逻辑和网络绑定的有效负载提供了大量的编译时完整性检查。\nUnion 类型和显式 Null\n各个网络端点通常会根据其输入和逻辑返回具有不同有效负载类型消息或 error。Ballerina 的类型系统采用了基于 union 类型的方法。Union 类型明确地采用了这种语义，不需要开发人员创建不必要的“包装”类型。这种方法也增强了对 null 值的处理。默认情况下，类型不支持 null 值。开发人员必须明确创建 union 类型来处理 null 值。结果是 null 的异常不会发生，并且语言语法和编译器会识别是否需要 null 处理逻辑。\n异构数据处理\nBallerina 类型系统内置丰富的对 JSON、XML、流和表格的支持以及对 ProtoBuf 和 gRPC 的直接支持。这样做的结果是可以获得处理网络负载、SQL 编程和流处理的干净可读的代码。数据转换逻辑不受复杂的生成类型、第三方库代码或其他混淆因素的影响——简单明了的可读代码捕捉与异构数据和转换逻辑的交互。\nBallerina 如何工作？ Ballerina 的语法、代码和编译器创建了运行时服务和部署构件，这些工件都是云原生就绪的，您可以选择将其部署在 IaaS、编排系统或 service mesh 中的。开发人员的体验旨在维护流程，包括快速的编辑、构建、调试周期并集成到团队的生命周期工具链中。\n运行时架构 云原生编程语言 ballerina 运行时架构 Ballerina API 网关\n强制执行身份策略并保证性能。通过代码注解（类似于 Spring 中的注解）进行配置和部署。可以运行嵌入式服务、作为管理多个服务的容器代理或者使用 API 管理解决方案（如 WSO2 API Manager）。\nBallerina service\n表示您的 API 和执行逻辑。服务通过不同的协议运行，内部代码结构被编译为支持 OpenAPI 和 Swagger 的 API 接口。服务与端点进行通信，无论它们是调用客户端还是其他服务。\nBallerina bridge\n允许传统代码和服务参与分布式事务中的 Ballerina 服务。Bridge 将您现有服务与本地代理包装起来，通过调用 Ballerina 服务参与和代理分布式事务。\n消息代理、事务协调者和身份代理\n为参与事务、事件驱动的通信和为认证流程的 Ballerina 服务提供代理基础设施功能。这些组件可以嵌入到单个服务部署中或者进行单独部署和扩展以管理多个服务。\n部署架构 云原生编程语言 ballerina 部署架构图 IaaS\n使用代码注解和构建系统，可以打包 Ballerina 服务和其他运行时组件（如 API 网关）以部署到任何云原生环境中。在 IaaS 环境中，Ballerina 服务可以以虚拟机或容器的方式运行，也可以在构建期间将镜像推送到 registry 中。\n编排器\n代码注解会触发编译器扩展，从而为不同的编排器（如 Kubernetes 或 Cloud Foundry）生成 Ballerina 组件的工件包。供应商或 DevOps 可以添加自定义代码注解以生成特定于环境的部署，例如自定义蓝色部署算法。\nService mesh\nBallerina 可以选择断路器和事务流程逻辑委托给像 Istio 或 Envoy 这样的 service mesh（如果有的话）。如果没有 service mesh 的话，Ballerina 服务将嵌入相应的功能。\n生命周期 云原生编程语言 ballerina 生命周期架构图 Ballerina 工具\n使用我们的语言服务器可以在 VS Code 和 IntelliJ 中获取自动补全和调试等智能感知。Ballerina 的关键字和语法结构可以用序列图的方式来表示。使用 Ballerina Composer 可以可视化的编辑 Ballerina 代码。另外它也可以做可视化得运行时和开发环境追踪。\nBallerina 构建\n将服务编译为经过优化的字节码，以便使用内存调优后的 BVM 运行。提供了使用 Testerina 的项目结构、依赖管理、包管理和单元测试。构建锁可以轻松地重新创建服务和部署。生成可执行文件（.balx）或库（.balo）。\nCI/CD\n部署代码注解会触发构建扩展，从而为持续集成、持续交付或编排器环境生成工件。将构建工件推送到您的 CI/CD 系统或完全跳过。\nRegistry\n将端点连接器、自定义注解和代码功能作为可共享软件包组合在一起。可以在全球共享资源库——Ballerina Central 中 pull 或 push 版本化的软件包。\nBallerina 的语言特性 Ballerina 设计为云优先，内置对现代 Web 协议和数据格式的支持，完全支持图灵完备编程语言，以及对微服务架构的原生支持。\nAPI 构造 逻辑语言 异步 Json 和 XML 注解 稳定和强大的类型 stream Ballerina 中集成了哪些内容？ Ballerina 是一种旨在集成简化的语言。基于顺序图的交互，Ballerina 内置了对通用集成模式和连接器的支持，包括分布式事务、补偿和断路器。凭借对 JSON 和 XML 的一流支持，Ballerina 能够简单有效地构建跨网络终端的强大集成。\n类型安全端点集成 类型安全连接器 可靠的消息传递 分布式事务 断路器 注入攻击防护 Docker 和 Kubernetes 关于 Ballerina 中各个功能的示例代码请查阅 ballerina-example 。\n参考 Ballerina 官网 Microservices, Docker, Kubernetes, Serverless, Service Mesh, and Beyond ","relpermalink":"/blog/introducing-cloud-native-programming-language-ballerina/","summary":"编译式强类型基于序列图理念的开源编程语言。","title":"云原生编程语言 Ballerina 介绍"},{"content":"本文是在 Kubernetes 集群中，使用 Envoy 来做 mesh，来为一个简单的使用 Python 编写的 Flask 应用程序做反向代理和负载均衡。\n注：本教程中的示例来自 envoy-steps ，本文中使用的所有的代码和 YAML 配置见 envoy-tutorial 。\nEnvoy Mesh 架构图 前提条件 使用 kubernetes-vagrant-centos-cluster 部署 kubernetes 集群，只要启动集群并安装了 CoreDNS 即可，无须安装其他插件。\n部署应用 我们首先将应用部署到 Kubernetes 中。\n部署 postgres 数据库。\nkubectl apply -f postgres 创建 usersvc 镜像。\ndocker build -t jimmysong/usersvc:step1 . 部署 usersvc。\nkubectl apply -f usersvc 查看 uservc 的 ClusterIP 地址。\n$ kubectl get svc usersvc kubectl get svc usersvc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE usersvc ClusterIP 10.254.176.248 \u0026lt;none\u0026gt; 5000/TCP 11m 进到 node1 中访问该服务，因为我们要访问的是 ClusterIP，在我们自己的电脑上是无法直接访问的，所以进到虚拟机中操作。\n$ vagrant ssh node1 $ curl 10.254.176.248:5000 { \u0026#34;hostname\u0026#34;: \u0026#34;usersvc-7cf5bb9d85-9gx7w\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;user health check OK\u0026#34;, \u0026#34;ok\u0026#34;: true, \u0026#34;resolvedname\u0026#34;: \u0026#34;172.33.10.7\u0026#34; } 尝试添加一个名为 Alice 的用户。\n$ curl -X PUT -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;fullname\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;alicerules\u0026#34; }\u0026#39; \\ 10.254.176.248/user/alice 将会看到类似如下的输出。\n{ \u0026#34;fullname\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;usersvc-7cf5bb9d85-9gx7w\u0026#34;, \u0026#34;ok\u0026#34;: true, \u0026#34;resolvedname\u0026#34;: \u0026#34;172.33.10.7\u0026#34;, \u0026#34;uuid\u0026#34;: \u0026#34;EF43B475F65848C6BE708F436305864B\u0026#34; } 尝试再添加一个名为 Bob 的用户。\n$ curl -X PUT -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;fullname\u0026#34;: \u0026#34;Bob\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;bobrules\u0026#34; }\u0026#39; \\ 10.254.176.248/user/bob 将会看到类似如下的输出。\n{ \u0026#34;fullname\u0026#34;: \u0026#34;Bob\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;usersvc-7cf5bb9d85-9gx7w\u0026#34;, \u0026#34;ok\u0026#34;: true, \u0026#34;resolvedname\u0026#34;: \u0026#34;172.33.10.7\u0026#34;, \u0026#34;uuid\u0026#34;: \u0026#34;6AC944E7D4254D9A811A82C0FDAC3046\u0026#34; } 当应用部署完毕后，我们该部署 edge envoy 了。\n部署 edge envoy 部署 edge envoy 的方式很简单，执行下面的命令。\nkubectl apply -f edge-envoy 现在访问 edge envoy 是就可以路由到 usersvc 上的，当然直接访问 usersvc 也是可以的。\n我们看下 edge-envoy 的 envoy 配置文件定义。\n{ \u0026#34;listeners\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;tcp://0.0.0.0:80\u0026#34;, \u0026#34;filters\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;read\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;http_connection_manager\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;codec_type\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;stat_prefix\u0026#34;: \u0026#34;ingress_http\u0026#34;, \u0026#34;route_config\u0026#34;: { \u0026#34;virtual_hosts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;backend\u0026#34;, \u0026#34;domains\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;routes\u0026#34;: [ { \u0026#34;timeout_ms\u0026#34;: 0, \u0026#34;prefix\u0026#34;: \u0026#34;/user\u0026#34;, \u0026#34;cluster\u0026#34;: \u0026#34;usersvc\u0026#34; } ] } ] }, \u0026#34;filters\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;decoder\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;router\u0026#34;, \u0026#34;config\u0026#34;: {} } ] } } ] } ], \u0026#34;admin\u0026#34;: { \u0026#34;access_log_path\u0026#34;: \u0026#34;/dev/null\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;tcp://127.0.0.1:8001\u0026#34; }, \u0026#34;cluster_manager\u0026#34;: { \u0026#34;clusters\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;usersvc\u0026#34;, \u0026#34;connect_timeout_ms\u0026#34;: 250, \u0026#34;type\u0026#34;: \u0026#34;strict_dns\u0026#34;, \u0026#34;service_name\u0026#34;: \u0026#34;usersvc\u0026#34;, \u0026#34;lb_type\u0026#34;: \u0026#34;round_robin\u0026#34;, \u0026#34;features\u0026#34;: \u0026#34;http2\u0026#34;, \u0026#34;hosts\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;tcp://usersvc:80\u0026#34; } ] } ] } } 客户端访问 edge-envoy 的 ClusterIP:8000/user/health 就可以检查节点的健康状况。\n部署 usersvc2 删除原来的 usersvc，部署第二版 usersvc2，它与原来的 usersvc 唯一不同的地方是在 entrypoint 中集成了 envoy，查看 Dockerfile 中指定的 entrypoint.sh 的内容便可知。\n#!/bin/sh python /application/service.py \u0026amp; /usr/local/bin/envoy -c /application/envoy.json 首先删除老的 usersvc。\nkubectl delete -f usersvc 使用下面的命令部署 usersvc2，它仍然使用 usersvc 这个 service 名称。\nkubectl apply -f usersvc2 Envoy 以 out-of-process 的方式运行，对应用进程没有侵入性，也可以使用 sidecar 的方式运行，让 envoy 与 应用容器运行在同一个 pod 中。\n增加 usersvc2 的实例个数。\nkubectl scale --replicas=3 deployment/usersvc 此时我们有 3 个 usersvc 实例，现在通过 edge-envoy 的 ClusterIP:8000/user/health 检查节点的健康状况时，是不是会轮询的访问到后端的的 usersvc2 的实例呢？\n我们当初在 edge-node 的 envoy.json 中配置过 cluster 的，其中指定了 lb_type 为 round_robin 。\n\u0026#34;cluster_manager\u0026#34;: { \u0026#34;clusters\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;usersvc\u0026#34;, \u0026#34;connect_timeout_ms\u0026#34;: 250, \u0026#34;type\u0026#34;: \u0026#34;strict_dns\u0026#34;, \u0026#34;service_name\u0026#34;: \u0026#34;usersvc\u0026#34;, \u0026#34;lb_type\u0026#34;: \u0026#34;round_robin\u0026#34;, \u0026#34;features\u0026#34;: \u0026#34;http2\u0026#34;, \u0026#34;hosts\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;tcp://usersvc:80\u0026#34; } ] } ] } 而且该 serivce_name 也可以被 DNS 正确解析。\nroot@usersvc-55b6857d44-gcg5c:/application# nslookup usersvc Server: 10.254.0.2 Address: 10.254.0.2#53 Name: usersvc.envoy-tutorial.svc.cluster.local Address: 10.254.123.166 答案是否定的。\n虽然通过 DNS 可以正确的解析出 serivce 的 ClusterIP，但是负载均衡不再通过 kube-proxy 实现，所以不论我们访问多少次 edge-envoy 永远只能访问到一个固定的后端 usersvc。\n服务发现服务 - SDS Kubernetes 中的 DNS 可以发现所有 serivce 的 ClusterIP，但是 DNS 中不包括所有 endpoint 地址，我们需要一个 SDS（服务发现服务）来发现服务的所有的 endpoint，我们将修改 lb_type，使用 sds 替代 strict_dns。\n执行下面的命令部署 SDS。\nkubectl apply -f usersvc-sds 因为在添加了 SDS 之后需要修改 edge-envoy 中的 envoy.josn 配置，在 clusters 字段中增加 sds 信息，我们将所有的配置都写好了，重新打包成了镜像，我们需要先删除之前部署的 edge-envoy。\nkubectl delete -f edge-envoy 部署新的 edge-envoy2。\nkubectl apply -f edge-envoy2 连续访问 usersvc 12 次看看输出结果如何。\nURL=http://172.17.8.101:30800/user/alice for i in `seq 1 12`;do curl -s $URL|grep \u0026#34;resolvedname\u0026#34;|tr -d \u0026#34; \u0026#34;|tr -d \u0026#34;,\u0026#34;|tr -d \u0026#39;\u0026#34;\u0026#39;;done 我们可以看到类似如下的输出：\nresolvedname:172.33.71.2 resolvedname:172.33.88.2 resolvedname:172.33.10.2 resolvedname:172.33.71.2 resolvedname:172.33.88.2 resolvedname:172.33.10.2 resolvedname:172.33.71.2 resolvedname:172.33.88.2 resolvedname:172.33.10.2 resolvedname:172.33.71.2 resolvedname:172.33.88.2 resolvedname:172.33.10.2 再查看下 usersvc 服务的所有 pod 的 IP 地址。\n$ kubectl get pod -l service=usersvc -o wide NAME READY STATUS RESTARTS AGE IP NODE usersvc-55b6857d44-mkfpv 1/1 Running 0 9m 172.33.88.2 node1 usersvc-55b6857d44-q98jg 1/1 Running 0 9m 172.33.71.2 node2 usersvc-55b6857d44-s2znk 1/1 Running 0 9m 172.33.10.2 node3 我们看到 round-robin 负载均衡生效了。\n参考 Part 2: Deploying Envoy with a Python Flask webapp and Kubernetes envoy-steps kubernetes-vagrant-centos-cluster envoy-tutorial ","relpermalink":"/blog/envoy-mesh-in-kubernetes-tutorial/","summary":"本文是在 Kubernetes 集群中，使用 Envoy 来做 mesh，来为一个简单的使用 Python 编写的 Flask 应用程序做反向代理和负载均衡。","title":"在 Kubernetes 中使用 Envoy mesh 教程"},{"content":"在了解一门技术之前一开始就要了解其中的基本概念和术语，只有融入了该语境才能理解这门技术。本文将为大家介绍 Envoy 中的基本术语和重点概念。\n架构 下图是 Envoy proxy 的架构图，显示了 host B 经过 Envoy 访问 host A 的过程。每个 host 上都可能运行多个 service，Envoy 中也可能有多个 Listener，每个 Listener 中可能会有多个 filter 组成了 chain。\nEnvoy proxy 架构图 其中的基本术语将在下面解释。\n基本术语 Host：能够进行网络通信的实体（在手机或服务器等上的应用程序）。在 Envoy 中主机是指逻辑网络应用程序。只要每台主机都可以独立寻址，一块物理硬件上就运行多个主机。\nDownstream：下游（downstream）主机连接到 Envoy，发送请求并或获得响应。\nUpstream：上游（upstream）主机获取来自 Envoy 的链接请求和响应。\nCluster: 集群（cluster）是 Envoy 连接到的一组逻辑上相似的上游主机。Envoy 通过服务发现 发现集群中的成员。Envoy 可以通过主动运行状况检查 来确定集群成员的健康状况。Envoy 如何将请求路由到集群成员由负载均衡策略 确定。\nMesh：一组互相协调以提供一致网络拓扑的主机。Envoy mesh 是指一组 Envoy 代理，它们构成了由多种不同服务和应用程序平台组成的分布式系统的消息传递基础。\n运行时配置：与 Envoy 一起部署的带外实时配置系统。可以在无需重启 Envoy 或 更改 Envoy 主配置的情况下，通过更改设置来影响操作。\nListener: 侦听器（listener）是可以由下游客户端连接的命名网络位置（例如，端口、unix 域套接字等）。Envoy 公开一个或多个下游主机连接的侦听器。一般是每台主机运行一个 Envoy，使用单进程运行，但是每个进程中可以启动任意数量的 Listener（监听器），目前只监听 TCP，每个监听器都独立配置一定数量的（L3/L4）网络过滤器。Listenter 也可以通过 Listener Discovery Service（LDS）动态获取。\nListener filter：Listener 使用 listener filter（监听器过滤器）来操作链接的元数据。它的作用是在不更改 Envoy 的核心功能的情况下添加更多的集成功能。Listener filter 的 API 相对简单，因为这些过滤器最终是在新接受的套接字上运行。在链中可以互相衔接以支持更复杂的场景，例如调用速率限制。Envoy 已经包含了多个监听器过滤器。\nHttp Route Table：HTTP 的路由规则，例如请求的域名，Path 符合什么规则，转发给哪个 Cluster。\nHealth checking：健康检查会与 SDS 服务发现配合使用。但是，即使使用其他服务发现方式，也有相应需要进行主动健康检查的情况。详见 health checking 。\nxDS xDS 是一个关键概念，它是一类发现服务的统称，其包括如下几类：\nCDS: Cluster Discovery Service EDS: Endpoint Discovery Service SDS: Secret Discovery Service RDS: Route Discovery Service LDS: Listener Discovery Service 正是通过对 xDS 的请求来动态更新 Envoy 配置，另外还有个 ADS（Aggregated Discovery Service）通过聚合的方式解决以上 xDS 的更新顺序问题。\nEnvoy Mesh Envoy Mesh 指的是由 envoy 做负载均衡和代理的 mesh。该 Mesh 中会包含两类 envoy：\nEdge envoy：即流量进出 mesh 时候的 envoy，相当于 kubernetes 中的 ingress。 Service envoy：服务 envoy 是跟每个 serivce 实例一起运行的，应用程序无感知的进程外工具，在 kubernetes 中会与应用容器以 sidecar 形式运行在同一个 pod 中。 Envoy 即可以单独作为 edge envoy，也可以仅做 service envoy 使用，也可以两者同时使用。Mesh 中的所有 envoy 会共享路由信息。\nEnvoy 配置 Envoy 中的配置包括两大类：listenner 配置和 cluster 配置。\nListener 配置 我们知道 Envoy 中可以配置一组 listener 以实现复杂的处理逻辑。Listener 中设置监听的 TCP 端口，还有一组 filter 对这些端口上的数据流进行处理。如下所示，该示例来自使用 Envoy 作为前端代理 。\nlisteners: - address: socket_address: address: 0.0.0.0 port_value: 80 filter_chains: - filters: - name: envoy.http_connection_manager config: codec_type: auto stat_prefix: ingress_http route_config: name: local_route virtual_hosts: - name: backend domains: - \u0026#34;*\u0026#34; routes: - match: prefix: \u0026#34;/service/1\u0026#34; route: cluster: service1 - match: prefix: \u0026#34;/service/2\u0026#34; route: cluster: service2 这是一个 http_connection_manager 例子，其中必须包含 virtual_hosts 配置，而 virtual_hosts 配置中必须包含以下几项配置：\nname：服务名称 domains：DNS 域名，必须能跟 virtual_host 的 URL 匹配 routes：路由列表 每个路由中还可以包含以下配置：\nprefix：URL 路径前缀 cluster：处理该请求的 envoy cluster timeout_ms：当出错时的超时时间 如上面的例子中，我们还需要定义 service1 cluster 和 service2 cluster。\nCluster 配置 Cluster 是一组逻辑相似的主机配置，定义哪些主机属于一个服务，cluster 的配置中包含了服务发现和负载均衡方式配置。依然是参考使用 Envoy 作为前端代理 中的配置：\nclusters: - name: service1 connect_timeout: 0.25s type: strict_dns lb_policy: round_robin http2_protocol_options: {} hosts: - socket_address: address: service1 port_value: 80 - name: service2 connect_timeout: 0.25s type: strict_dns lb_policy: round_robin http2_protocol_options: {} hosts: - socket_address: address: service2 port_value: 80 Cluster 的配置中至少包含以下信息：\nname：cluster 名称，就是服务名称 type：该 cluster 怎么知道主机是否启动？即服务发现类型，有以下方式： static：监听 cluster 中的所有主机 strict_dns：envoy 会监听 DNS，每个匹配的 A 记录都会认定为有效 logical_dns：envoy 将使用 DNS 来增加主机，如果 DNS 不再返回该主机也不会删除这些主机信息 sds：即 Serivce Discovery Serivce，envoy 访问外部的 REST 获取 cluster 成员信息 lb_type：cluster 的负载均衡类型，有以下方式： round_robin：轮询主机 weighted_least_request：最近获得最少请求的主机 random：随机 hosts：能够定义 cluster 中主机的 URL 地址，通常是tcp:// URL 参考 Part 1: Getting started with Envoy Proxy for microservices resilience - getambassador.io ","relpermalink":"/blog/envoy-archiecture-and-terminology/","summary":"本文介绍了 Envoy proxy 中的基本概念、配置与架构解析。","title":"Envoy 的架构与基本配置解析"},{"content":"Envoy 是一款由 Lyft 开源的，使用 C++ 编写的 L7 代理和通信总线，目前是 CNCF 旗下的开源项目，代码托管在 GitHub 上，它也是 Istio service mesh 中默认的 data plane。本文将给出使用 Envoy 作为 service mesh 的数据平面的示例，应用使用 docker-compose 编排。\n特性 Envoy 包括如下特性：\n进程外架构，不侵入应用进程 使用现代版 C++11 代码 L3/L4 filter 架构 HTTP L7 filter 架构 支持 HTTP/2 HTTP L7 routing 支持 gRPC 支持 MongoDB L7 动态配置 最佳可观测性 支持 front/edge proxy 高级负载均衡 健康检查 服务发现 支持 DynamoDB L7 Envoy 本身无法构成一个完整的 Service Mesh，但是它可以作为 service mesh 中的应用间流量的代理，负责 service mesh 中的数据层。\n更多信息请参考 Envoy 官网 。\nEnvoy 作为前端代理 本文是使用 Envoy 作为前端代理的介绍，仅使用 docker 容器和 docker-compose 做编排在单机中运行，帮助我们从更底层了解 Envoy，当我们将 Envoy 作为 Istio Service Mesh 的 data panel 的时候将更加游刃有余。\n快速开始 Envoy 中的所有规则配置跟 Kubernetes 一样都是通过 YAML 文件来完成的。在继续下面的步骤之前，首先克隆 Envoy 的 GitHub repo。\ngit clone https://github.com/envoyproxy/envoy.git 运行 sandbox 测试 Envoy 官方提供了以下打包用例：\nFront Proxy Zipkin Tracing Jaeger Tracing gRPC Bridge 全部可以使用 docker-compose 运行，代码可以在 https://github.com/envoyproxy/envoy/tree/master/examples 找到。\nFront proxy Envoy 在 envoymesh 的边缘做反向代理，详细使用方式见 https://www.envoyproxy.io/docs/envoy/latest/start/sandboxes/front_proxy ，在此我将解说下以下问题：\nEnvoy 是如何作为进程外架构运行的？ 为何说 Envoy 是无侵入式架构？ Envoy 作为边缘反向代理能做什么？ 本示例的架构图如下所示，此时 Envoy 将作为一个反向代理，类似于 Nginx，但与 Nginx 不同的是它还会作为一个进程，伴随每个服务一起运行在同一个容器中（在 Kubernetes 中可以作为 Sidecar 与应用容器一起运行在同一个 Pod 中）。\nFront proxy 部署结构图 在此示例中一共有 3 个服务，我们需要为其创建容器编排的 docker-compose.yml 文件。\nversion: \u0026#39;2\u0026#39; services: front-envoy: build: context: . dockerfile: Dockerfile-frontenvoy volumes: - ./front-envoy.yaml:/etc/front-envoy.yaml networks: - envoymesh expose: - \u0026#34;80\u0026#34; - \u0026#34;8001\u0026#34; ports: - \u0026#34;8000:80\u0026#34; - \u0026#34;8001:8001\u0026#34; service1: build: context: . dockerfile: Dockerfile-service volumes: - ./service-envoy.yaml:/etc/service-envoy.yaml networks: envoymesh: aliases: - service1 environment: - SERVICE_NAME=1 expose: - \u0026#34;80\u0026#34; service2: build: context: . dockerfile: Dockerfile-service volumes: - ./service-envoy.yaml:/etc/service-envoy.yaml networks: envoymesh: aliases: - service2 environment: - SERVICE_NAME=2 expose: - \u0026#34;80\u0026#34; networks: envoymesh: {} 使用 docker-compose 启动可以保证三个服务都在同一个网络内，即 frontproxy_envoymesh 网络中。\n其中 front-envoy 是前端（边缘）Envoy 服务，用来做反向代理，它使用的是 Dockerfile-frontenvoy 文件来构建镜像的，我们来看下该 Dockerfile 的内容。\nFROM envoyproxy/envoy:latest RUN apt-get update \u0026amp;\u0026amp; apt-get -q install -y \\ curl CMD /usr/local/bin/envoy -c /etc/front-envoy.yaml --service-cluster front-proxy 其中 /etc/front-envoy.yaml 是本地的 front-envoy.yaml 挂载进去的。我们看下该文件的内容。\nstatic_resources: listeners: - address: socket_address: address: 0.0.0.0 port_value: 80 filter_chains: - filters: - name: envoy.http_connection_manager config: codec_type: auto stat_prefix: ingress_http route_config: name: local_route virtual_hosts: - name: backend domains: - \u0026#34;*\u0026#34; routes: - match: prefix: \u0026#34;/service/1\u0026#34; route: cluster: service1 - match: prefix: \u0026#34;/service/2\u0026#34; route: cluster: service2 http_filters: - name: envoy.router config: {} clusters: - name: service1 connect_timeout: 0.25s type: strict_dns lb_policy: round_robin http2_protocol_options: {} hosts: - socket_address: address: service1 port_value: 80 - name: service2 connect_timeout: 0.25s type: strict_dns lb_policy: round_robin http2_protocol_options: {} hosts: - socket_address: address: service2 port_value: 80 admin: access_log_path: \u0026#34;/dev/null\u0026#34; address: socket_address: address: 0.0.0.0 port_value: 8001 我们看到其中包括了三大配置项：\nstatic_resources：路由配置信息 cluster：envoymesh 的服务注册信息 admin：管理接口，可以通过访问 8001 端口的，访问 /stats 获取当前 envoymesh 的一些统计信息，访问 /server_info 获取 Envoy 的版本信息 使用 docker-compose 启动三个容器。\n$ pwd envoy/examples/front-proxy $ docker-compose up --build -d $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------- example_service1_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp example_service2_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp example_front-envoy_1 /bin/sh -c /usr/local/bin/ ... Up 0.0.0.0:8000-\u0026gt;80/tcp, 0.0.0.0:8001-\u0026gt;8001/tcp 我们下面将过一遍 Envoy 作为前端代理的所有功能，这些功能是通用功能。\n路由 访问 service1 http://localhost:8000/service/1 将看到如下输出。\n$ curl -v localhost:8000/service/1 * Trying ::1... * TCP_NODELAY set * Connected to localhost (::1) port 8000 (#0) \u0026gt; GET /service/1 HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; content-type: text/html; charset=utf-8 \u0026lt; content-length: 89 \u0026lt; server: envoy \u0026lt; date: Fri, 20 Apr 2018 08:26:33 GMT \u0026lt; x-envoy-upstream-service-time: 14 \u0026lt; Hello from behind Envoy (service 1)! hostname: a3e4185a9a49 resolvedhostname: 172.18.0.4 * Connection #0 to host localhost left intact 访问 service2 http://localhost:8000/service/2 将看到如下输出。\n* Trying ::1... * TCP_NODELAY set * Connected to localhost (::1) port 8000 (#0) \u0026gt; GET /service/2 HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; content-type: text/html; charset=utf-8 \u0026lt; content-length: 89 \u0026lt; server: envoy \u0026lt; date: Fri, 20 Apr 2018 08:27:27 GMT \u0026lt; x-envoy-upstream-service-time: 10 \u0026lt; Hello from behind Envoy (service 2)! hostname: f6650e1911a0 resolvedhostname: 172.18.0.3 * Connection #0 to host localhost …","relpermalink":"/blog/envoy-as-front-proxy/","summary":"本文是使用 Envoy 作为前端代理的介绍，仅使用 docker 容器和 docker-compose 做编排在单机中运行，帮助我们从更底层了解 Envoy，当我们将 Envoy 作为 Istio Service Mesh 的 data panel 的时候将更加游刃有余。","title":"使用 Envoy 作为前端代理"},{"content":"TL;DR 点此下载本书 PDF 。\n近日 Nginx 公司的 Michael Hausenblas 发布了一本关于 docker 和 kubernetes 中的容器网络的小册子。这份资料一共 72 页，是大家由浅入深的了解 Docker 和 Kubernetes 中的网络的很好的入门资料。\n目标读者 容器软件开发者 SRE 网络运维工程师 想要把传统软件进行容器化改造的架构师 ","relpermalink":"/notice/container-networking-from-docker-to-kubernetes-nginx/","summary":"资料来自 Nginx，O'Reilly 出版。","title":"从 Docker 到 Kubernetes 中的容器网络图书资料分享"},{"content":"本文是 Istio 管理 Java 微服务的案例教程，使用的所有工具和软件全部基于开源方案，替换了 redhat-developer-demos/istio-tutorial 中的 minishift 环境，使用 kubernetes-vagrant-centos-cluster 替代，沿用了原有的微服务示例，使用 Zipkin 做分布式追踪而不是 Jaeger。\n本文中的代码和 YAML 文件见 https://github.com/rootsongjc/istio-tutorial 。\n准备环境 在进行本教程前需要先准备以下工具和环境。\n8G 以上内存 Vagrant 2.0+ Virtualbox 5.0 + 提前下载 kubernetes1.9.1 的 release 压缩包 docker 1.12+ kubectl 1.9.1+ maven 3.5.2+ istioctl 0.7.1 git curl、gzip、tar kubetail siege 安装 Kubernetes 请参考 kubernetes-vagrant-centos-cluster 在本地启动拥有三个节点的 kubernetes 集群。\ngit clone https://github.com/rootsongjc/kubernetes-vagrant-centos-cluster.git cd kubernetes-vagrant-centos-cluster vagrant up 安装 Istio 在 kubernetes-vagrant-centos-cluster 中的包含 Istio 0.7.1 的安装 YAML 文件，运行下面的命令安装 Istio。\nkubectl apply -f addon/istio/ 运行示例\nkubectl apply -n default -f \u0026lt;(istioctl kube-inject -f yaml/istio-bookinfo/bookinfo.yaml) 在您自己的本地主机的/etc/hosts文件中增加如下配置项。\n172.17.8.102 grafana.istio.jimmysong.io 172.17.8.102 servicegraph.istio.jimmysong.io 172.17.8.102 zipkin.istio.jimmysong.io 我们可以通过下面的 URL 地址访问以上的服务。\nService URL grafana http://grafana.istio.jimmysong.io servicegraph http://servicegraph.istio.jimmysong.io/dotviz，http://servicegraph.istio.jimmysong.io/graph zipkin http://zipkin.istio.jimmysong.io 详细信息请参阅 Istio 文档 。\n部署示例应用 在打包成镜像部署到 kubernetes 集群上运行之前，我们先在本地运行所有示例。\n本教程中三个服务之间的依赖关系如下：\ncustomer → preference → recommendation customer 和 preference 微服务是基于 Spring Boot 构建的，recommendation 微服务是基于 vert.x 构建的。\ncustomer 和 preference 微服务的 pom.xml 文件中都引入了 OpenTracing 和 Jeager 的依赖。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.opentracing.contrib\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;opentracing-spring-cloud-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.1.7\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.uber.jaeger\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jaeger-tracerresolver\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.25.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 本地运行 我们首先在本地确定所有的微服务都可以正常运行，然后再打包镜像在 kubernetes 集群上运行。\n启动 Jaeger\n使用 docker 来运行 jagger。\ndocker run -d \\ --rm \\ -p5775:5775/udp \\ -p6831:6831/udp \\ -p6832:6832/udp \\ -p16686:16686 \\ -p14268:14268 \\ jaegertracing/all-in-one:1.3 Jaeger UI 地址 http://localhost:16686\nCustomer\ncd customer/java/springboot JAEGER_SERVICE_NAME=customer mvn \\ spring-boot:run \\ -Drun.arguments=\u0026#34;--spring.config.location=src/main/resources/application-local.properties\u0026#34; 服务访问地址： http://localhost:8280\nPreference\ncd preference/java/springboot JAEGER_SERVICE_NAME=preference mvn \\ spring-boot:run \\ -Drun.arguments=\u0026#34;--spring.config.location=src/main/resources/application-local.properties\u0026#34; 服务访问地址：http://localhost:8180\nRecommendation\ncd recommendation/java/vertx mvn vertx:run 服务访问地址：http://localhost:8080\n所有服务都启动之后，此时访问 http://localhost:8280 将会看到如下输出。\ncustomer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;unknown\u0026#39;: 1 每访问一次最后的数字就会加 1。\nJaeger\n此时访问 http://localhost:16686 将看到 Jaeger query UI，所有应用将 metrics 发送到 Jeager 中。\n可以在 Jaeger UI 中搜索 customer 和 preference service 的 trace 并查看每次请求的 tracing。\nJaeger query UI 构建镜像 在本地运行测试无误之后就可以构建镜像了。本教程中的容器镜像都是在 fabric8/java-jboss-openjdk8-jdk 的基础上构建的。只要将 Java 应用构建出 Jar 包然后放到 /deployments 目录下基础镜像就可以自动帮我们运行，所以我们看到着几个应用的 Dockerfile 文件中都没有执行入口，真正的执行入口是 run-java.sh 。\nCustomer\n构建 Customer 镜像。\ncd customer/java/springboot mvn clean package docker build -t jimmysong/istio-tutorial-customer:v1 . docker push jimmysong/istio-tutorial-customer:v1 第一次构建和上传需要花费一点时间，下一次构建就会很快。\nPreference\n构建 Preference 镜像。\ncd preference/java/springboot mvn clean package docker build -t jimmysong/istio-tutorial-preference:v1 . docker push jimmysong/istio-tutorial-preference:v1 Recommendation\n构建 Recommendation 镜像。\ncd recommendation/java/vertx mvn clean package docker build -t jimmysong/istio-tutorial-recommendation:v1 . docker push jimmysong/istio-tutorial-recommendation:v1 现在三个 docker 镜像都构建完成了，我们检查一下。\n$ docker images | grep istio-tutorial REPOSITORY TAG IMAGE ID CREATED SIZE jimmysong/istio-tutorial-recommendation v1 d31dd858c300 51 seconds ago 443MB jimmysong/istio-tutorial-preference v1 e5f0be361477 6 minutes ago 459MB jimmysong/istio-tutorial-customer v1 d9601692673e 13 minutes ago 459MB 部署到 Kubernetes 使用下面的命令将以上服务部署到 kubernetes。\n# create new namespace kubectl create ns istio-tutorial # deploy recommendation kubectl apply -f \u0026lt;(istioctl kube-inject -f recommendation/kubernetes/Deployment.yml) -n istio-tutorial kubectl apply -f recommendation/kubernetes/Service.yml # deploy preferrence kubectl apply -f \u0026lt;(istioctl kube-inject -f preference/kubernetes/Deployment.yml) -n istio-tutorial kubectl apply -f preference/kubernetes/Service.yml # deploy customer kubectl apply -f \u0026lt;(istioctl kube-inject -f customer/kubernetes/Deployment.yml) -n istio-tutorial kubectl apply -f customer/kubernetes/Service.yml 注意：preference 和 customer 应用启动速度比较慢，我们将 livenessProb 配置中的 initialDelaySeconds 设置为 20 秒。\n查看 Pod 启动状态：\nkubectl get pod -w -n istio-tutorial 增加 Ingress 配置 为了在 kubernetes 集群外部访问 customer 服务，我们需要增加 ingress 配置。\nkubectl apply -f ingress/ingress.yaml 修改本地的 /etc/hosts 文件，增加一条配置。\n172.17.8.102 customer.istio-tutorial.jimmysong.io 现在访问 http://customer.istio-tutorial.jimmysong.io 将看到如下输出：\ncustomer =\u0026gt; preference =\u0026gt; …","relpermalink":"/blog/istio-tutorial/","summary":"本文是 Istio 管理 Java 微服务的案例教程。","title":"Istio Service Mesh 教程"},{"content":"本文讲述了参与 Istio 社区和进行 Istio 开发时需要注意的事项。\n工作组 绝大多数复杂的开源项目都是以工作组的方式组织的，要想为 Istio 社区做贡献可以加入到以下的工作组（Working Group）：\nAPI Management Config Environments Networking Performance \u0026amp; Scalability Policies \u0026amp; Telemetry Security Test \u0026amp; Release 代码规范 Istio 的代码规范沿用 CNCF 社区的代码规范 。\n开发指南 进行 Istio 开发之前需要做下面几件事情：\n配置基础环境，如 Kubernetes 配置代码库、下载依赖和测试 配置 CircleCI 集成环境 编写参考文档 Git workflow 配置 详见 Dev Guide wiki 。\n设计文档 所有的设计文档都保存在 Google Drive 中，其中包括以下资源：\nTechnical Oversight Committee：ToC 管理的文档 Misc：一些杂项 Working Groups：最重要的部分，各个工作组相关的设计文档 Presentations：Istio 相关的演讲幻灯片，从这些文稿中可以快速了解 Istio Logo：Istio logo Eng：社区相关的维护文档 社区角色划分 根据对开发者和要求和贡献程度的不同，Istio 社区中包含以下角色：\nCollaborator ：非正式贡献者，偶尔贡献，任何人都可以成为该角色 Member ：正式贡献者，经常贡献，必须有 2 个已有的 member 提名 Approver ：老手，可以批准 member 的贡献 Lead ：管理功能、项目和提议，必须由 ToC 提名 Administrator ：管理员，管理和控制权限，必须由 ToC 提名 Vendor ：贡献 Istio 项目的扩展 详见 Istio Community Roles 。\n各种功能的状态 Istio 中的所有 feature 根据是否生产可用、API 兼容性、性能、维护策略分为三种状态：\nAlpha：仅仅可以作为 demo，无法生产上使用，也没有性能保证，随时都可能不维护 Beta：可以在生产上使用了，也有版本化的 API 但是无法保证性能，保证三个月的维护 Stable：可以上生产而且还能保证性能，API 向后兼容，保证一年的维护 Istio 的 feature 分为四大类：\n流量管理：各种协议的支持、路由规则配置、Ingress TLS 等 可观察性：监控、日志、分布式追踪、服务依赖拓扑 安全性：各种 checker 和安全性配置 Core：核心功能 功能划分与各种功能的状态详情请见：https://istio.io/latest/about/feature-stages/ 云原生社区 Istio 讨论组 云原生社区 专门成立里 Istio SIG（微信讨论群），将原来 ServiceMesher 中关注 Istio 的人群专门集中到一个讨论组中，其中包含了百度、阿里巴巴、腾讯、网易、Tetrate、Intel、字节跳动等公司的服务网格专家及众多的终端用户，欢迎大家申请加入群聊 。\n","relpermalink":"/blog/istio-community-tips/","summary":"本文讲述了参与 Istio 社区和进行 Istio 开发时需要注意的事项。","title":"Istio 社区介绍与社区参与注意事项"},{"content":"本文译自 Istio Why do I need it? 我最近没有多少时间去玩 k8s，并承认 Istio 到底给 k8s 带来了什么方面有点迷失了。这是否会增加更多的运营开销？它是否简化了我们通常需要做的事情？这些问题都浮现在我的脑海里。\n我怀疑在发布了这些内容之后，我的团队中比我更懂 k8s 的人可能会想找我谈谈…… 虽然我讲会跟团队中的成员辩论，但那将是我最喜欢的对话。\n那么 Istio 究竟是什么？ Istio 网站 上说，Istio 带给你：\nHTTP、gRPC、WebSocket 和 TCP 流量的自动负载均衡。 通过丰富的路由规则、重试、故障转移和故障注入对流量行为进行细粒度控制。 支持访问控制、速率限制和配额的可拔插策略层和配置 API。 自动指标、日志和集群内所有流量的跟踪，包括集群入口和出口。 通过集群中的服务之间的强身份断言来实现服务间的身份验证。 通过在整个环境中部署一个特殊的 sidecar 代理（辅助容器），您可以将 Istio 支持添加到服务中（这给我留下了深刻的印象，如果您想做到这一点，请参阅后面的内容）。安装了 sidecar 代理之后，（微）服务之间的所有网络通信都通过这个代理。此外，所有的网络通信都是使用 Istio 的控制平面功能进行配置和管理的。\nIstio 是 Service Mesh（服务网格）。我认为的 service mesh 定义就是“它是一个专用的基础设施层，使得服务间的通信安全、高效和可靠”\n然而，如果像我一样，你从概念文档 开始看的话，上面有这样的内容：“术语 service mesh 通常用于描述组成这些应用程序的微服务网络以及它们之间的交互。随着服务网格的大小和复杂程度不断增加，可能会变得难以理解和管理。可能出现包括服务发现、负载平衡、故障恢复、度量和监控，以及更复杂的需求，如 A/B 测试、金丝雀发布、速率限制、访问控制和端到端身份验证。Istio 提供了一个完整的解决方案，通过对整个服务网格提供行为分析和操作控制来满足微服务应用程序的各种需求。“\n读完之后你可能会像我一样困惑！最后在网上查了一圈关于什么是服务网格之后，我终于搞明白了。我最后使用的可能是一个在所有搜索到的样本里一个非代表性的共识，但这是一个合理的选择。不过有个细节确实了，就是如何将它与 k8s 等编排工具分开。Istio 需要跟 k8s 一起使用，没有 k8s 或其他容器编排工具的它就不存在了吗？它没有做编排，实际上它的是为解决管理基于微服务的解决方案中网络和操作复杂性而设计的。它涵盖的范围就像 k8s 一样！现在我真的需要继续这个帖子了。。。\n所以我知道 Istio 是什么，给我们带来了什么，但它实际上解决了什么挑战呢？\n从为什么使用 Istio 页面 中可以看出，它在服务网络中统一提供了许多关键功能：\n流量管理 可观察性 强制策略 服务身份标识和安全 对于我来说，要真正理解 Istio 的价值，所以我使用了 codelab 。编写 code lab 的人真是太棒了！\nCode lab 向我介绍了 Istio 控制平面的四个主要组件：\nPilot：处理代理 sidecar 的配置和编程。 Mixer：为您的流量处理决策并收集遥测数据。 Ingress：处理来自群集外部的传入请求。 CA：证书颁发机构。 查看 Istio 架构概念 页面了解这些组件如何协同工作的。\nCode lab 提供了路由规则 —— 流量管理部分\n我还尝试了 Istio.io 中的一些 task，因为我需要了解它如何处理那些领域的工作。\n提示：如果您在完成 codelab 时也决定在四处看看，那么请将您的群集与应用程序一起启动并运行。无论如何，你会再次使用它。\n所以我对它如何解决这些问题有了一个基本的了解，但是如果我使用像 GKE 这样的托管 K8s（好吧，你知道我会选那个不是吗？）使用 Istio 是否合适？\n注意：是的，这里有更多的细节，但我主要想弄明白为什么需要使用 Istio。\n集群最终用户 / 开发人员访问\nGKE 结合使用 IAM 和 RBAC，是的，这里面有很多东西需要你了解。\n要为您的集群用户授予比 Cloud IAM 更细粒度的权限，您可以使用 namespace 和 RBAC 来限制对特定 pod 的访问或排除对 secret 的访问。\nIstio RBAC 介绍了两个侧重于服务的角色\nServiceRole 定义用于访问网格中的服务的角色。 ServiceRoleBinding 将角色授予主题（例如用户、组、服务）。 它们是 k8s 中的 CustomResourceDefinition（CRD）对象。但您仍然需要了解 IAM。\n服务身份标识 GKE 可以使用 service account 来管理 GKE 上运行的应用程序 可以使用哪些 GCP 服务。这些 service accout 的密钥使用 secret 存储。Pod 中运行的进程的身份标识是由 k8s service account 与 RBAC 一起决定的。Istio 使用 istio-auth ，它使用双向 TLS 提供强大的服务间和最终用户身份验证，内置身份和凭证管理。Istio-auth 使用 Kubernetes service account。\nIstio 不提供任何使用 GCP service account 帮助。这还很早，但是它正在制定未来发展计划的路线图。\nIstio-auth 很好，计划中的增强功能将值得等待。我对安全的复杂性感到厌烦，因为这不可避免地导致配置错误，所以我希望它与 service account 类型之间进行更加无缝的对齐！\n网络控制 GKE（用于 k8s 版本 1.7.6 +）使用 k8s 网络策略 来管理哪些 Pod 可以和服务通信。这是相对简单的配置。Istio 也有网络策略，但他们不是你知道和喜欢的 K8s 策略，为什么会有这样的区别呢？ 这篇文章 很好地解释了这一点，所以我不会在这里重述，但是这个表格总结了不同之处以及为什么会有这样的不同。\n项目 Istio 策略 网络策略 层 Service（7 层） Network（3、4 层） 实现 Userspace Kernel 控制点 Pod Node Istio 使用 envoy 作为 sidecar 代理。Envoy 在 OSI 模型的应用层运行，所以在第 7 层。我的这篇博客将为你详细解释。\n您需要两种策略类型，这是纵深防御的方法。\n多个集群 Istio 有个非常酷的功能是 mixer 适配器 。简而言之，它可以从底层后端进行抽象以提供核心功能，例如日志记录、监控、配额、ACL 检查等。它公开了一个一致的 API，与使用的后端无关。就像 GCS 公开了一个 API，无论您使用什么存储类别！\n我认为 mixer 适配器模型 博客文章中的这张图片解释了 mixer 适配器中的全部要点。\n有一个早期 demo ，我认为它是 istio 最有用的特性之一，它实际上使用虚拟机来承载 codelab 中使用的评分 dbase MySQL 数据库，并将其作为 GKE 集群所属网格的一部分。使用一个网格来管理它们！\n流量管理 如果你使用了 codelab，你会看到使用 istio 来引导使用路由规则的流量是多么容易。使用 K8s，您还可以使用金丝雀部署进行流量管理，并以类似于 istio 的方式将一定比例的流量引导至您的应用的一个版本，但 Istio 在这种情况下更灵活，方法是允许您设置细粒度流量百分比并控制流量使用 code lab 中的其他标准。\n服务发现 服务注册在 k8s 中完成。Istio 抽象出底层的服务发现机制，并将其转换为 envoy sidecar 可消费的标准格式。\n审计记录和监控 如果是超出 GKE 提供的标准日志记录的话，可以将 GKE 与 StackDriver 日志记录 集成来收集，在持久化存储中存储容器日志、系统日志和关于群集中的活动的事件，例如 Pod 的调度。还可以与 StackDriver Monitoring 集成以收集系统度量指标（度量群集的基础设施，例如 CPU 或内存使用情况）和自定义指标（特定于应用程序的指标）。\nIstio 利用 prometheus 与 grafana 一起作为仪表板进行记录和监控。我喜欢 service graph 配置 ，它可以为您提供 service mesh 的图形表示。你也可以用 kibana 和 fluentd 来配合 Elasticsearch 使用。\n那么我需要 Istio 吗？ Istio 的流量管理非常棒，mixer 适配器模型可以轻松管理覆盖多个群集和虚拟机的网格。我喜欢 Istio 是因为它可以让你进中精力思考服务，而不是那么多的 pod 和节点，并不是说你不必担心这些，而是只关注服务就好了！\n如果你需要管理一个分布式集群，那么 Istio 应该在你的选择列表里。如果您需要在流量管理方面有比 k8s 提供的更多的灵活性的化那么 Istio 也很值得关注。\n如果你有足够的资源来处理处于发展早期的事物，那么尽早理解 Istio 是值得的。如果你已经在使用 k8s 的话那么 istio 的学习曲线将很低。\n记住它是一个建立在上层的东西，所以你仍然需要在 k8s 层做些事情，比如配置 k8s 网络策略来补充 istio 网络策略。\nIstio 还处于发展的早期阶段，所以它不会做你期望的所有事情，但我们希望它会。你将无法避免的在提供商 API 和 Istio 之间来回调用才能完成一个完整的工作，所以它不是你希望的那种一站式解决方案。\nDashboard 是可视化网格配置的一种很好的方式，因为编写 YAML 会让人很快疲惫！是的，您可以设置仪表板上的控制面板来可视化度量指标，但我希望看到它与 StackDriver 集成。\n因此，在总体了解 Istio 之后，我实际上很喜欢它所承诺的内容。\n","relpermalink":"/blog/why-do-we-need-istio/","summary":"Istio 为我们带来了什么？","title":"为什么需要 Istio？"},{"content":"CNCF，全称 Cloud Native Computing Foundation（云原生计算基金会），口号是坚持和整合开源技术来编排容器作为微服务架构的一部分，其作为致力于云原生应用推广和普及的一支重要力量，不论您是云原生应用的开发者、管理者还是研究人员都有必要了解。\nCNCF 作为一个厂商中立的基金会，致力于 Github 上的快速成长的开源技术的推广，如 Kubernetes、Prometheus、Envoy 等，帮助开发人员更快更好的构建出色的产品。\nhttp://github.com/cncf/landscape 中维护了一幅 CNCF 的全景图。\n其中包含了 CNCF 中托管的项目，还有很多是非 CNCF 项目，还有个交互式的浏览 CNCF 涵盖的所有的项目的页面：https://i.cncf.io\n关于 CNCF 的使命与组织方式请参考CNCF 章程 ，概括的讲 CNCF 的使命宝库以下三点：\n容器化包装。 通过中心编排系统的动态资源管理。 面向微服务。 以上是 CNCF 最初对云原生特征的定义。\nCNCF 这个角色的作用是推广技术，形成社区，开源项目管理与推进生态系统健康发展。\n另外 CNCF 组织由以下部分组成：\n会员：白金、金牌、银牌、最终用户、学术和非赢利成员，不同级别的会员在治理委员会中的投票权不同。 理事会：负责事务管理 TOC（技术监督委员会）：技术管理 最终用户社区：推动 CNCF 技术的采纳并选举最终用户技术咨询委员会 最终用户技术咨询委员会：为最终用户会议或向理事会提供咨询 营销委员会：市场推广 CNCF 项目成熟度分级与毕业条件 每个 CNCF 项目都需要有个成熟度等级，申请成为 CNCF 项目的时候需要确定项目的成熟度级别。\n成熟度级别（Maturity Level）包括以下三种：\nsandbox（初级）一开始是叫 inception incubating（孵化中） graduated（毕业） 是否可以成为 CNCF 项目需要通过 Technical Oversight Committee（技术监督委员会）简称TOC ，投票采取 fallback 策略，即回退策略，先从最高级别（graduated，目前是从 incubating）开始，如果 2/3 多数投票通过的话则确认为该级别，如果没通过的话，则进行下一低级别的投票，如果一直到 inception 级别都没得到 2/3 多数投票通过的话，则拒绝其进入 CNCF 项目。一般一个项目处于孵化阶段不会超过 2 年。\n当前所有的 CNCF 项目可以访问 https://www.cncf.io/projects/ 在太平洋时间 3 月 6 日，Kubernetes 成为了 CNCF 的第一个毕业项目！\nTOC（技术监督委员会） TOC（Technical Oversight Committee）作为 CNCF 中的一个重要组织，它的作用是：\n定义和维护技术视野 审批新项目加入组织，为项目设定概念架构 接受最终用户的反馈并映射到项目中 调整组件见的访问接口，协调组件之间兼容性 TOC 成员通过选举产生，见选举时间表 。\n参考 CNCF TOC：https://github.com/cncf/toc\nCNCF 章程 CNCF（云原生计算基金会）是 Linux 基金会旗下的一个基金会，加入 CNCF 等于同时加入 Linux 基金会（也意味着你还要交 Linux 基金会的份子钱），对于想加入 CNCF 基金会的企业或者组织首先要做的事情就是要了解 CNCF 的章程（charter），就像是作为一个国家的公民，必须遵守该国家的宪法一样。CNCF 之所以能在短短三年的时间内发展壮大到如此规模，很大程度上是与它出色的社区治理和运作模式有关。了解该章程可以帮助我们理解 CNCF 是如何运作的，也可以当我们自己进行开源项目治理时派上用场。\n该章程最后更新于 2018 年 5 月 15 日，详见https://www.cncf.io/about/charter/ 。下文中关于 CNCF 章程的介绍部分引用自CNCF 是如何工作的 ，有改动。\n1. CNCF 的使命 CNCF 没有偏离自己的主题，核心是解决技术问题：基金会的使命是创建并推动采用新的计算模式，该模式针对现代分布式系统环境进行了优化，能够扩展至数万个自愈式多租户节点。\n所谓的云原生系统须具备下面这些属性：\n应用容器化：将软件容器中的应用程序和进程作为独立的应用程序部署单元运行，并作为实现高级别资源隔离的机制。从总体上改进开发者的体验、促进代码和组件重用，而且要为云元是国内应用简化运维工作。 动态管理：由中心化的编排来进行活跃的调度和频繁的管理，从根本上提高机器效率和资源利用率，同时降低与运维相关的成本。 面向微服务：与显式描述的依赖性松散耦合（例如通过服务端点），可以提高应用程序的整体敏捷性和可维护性。CNCF 将塑造技术的发展，推动应用管理的先进技术发展，并通过可靠的接口使技术无处不在，并且易于使用。 2. CNCF 扮演的角色 CNCF 其实是在开源社区的基础上发挥着作用，应负责：\na) 项目管理\n确保技术可用于社区并且没有杂七杂八的影响 确保技术的品牌（商标和标识）得到社区成员的关注和使用，特别强调统一的用户体验和高水平的应用程序兼容性 b) 促进生态系统的发展和演进\n评估哪些技术可以纳入云原生计算应用的愿景，鼓励社区交付这样的技术，以及集成它们，且要积极的推进总结进度。 提供一种方法来培养各个部分的通用技术标准 c) 推广底层技术和应用定义和管理方法，途径包括：活动和会议、营销（SEM、直接营销）、培训课程和开发人员认证。\nd) 通过使技术可访问和可靠来为社区服务\n旨在通过对参考架构进行明确定义的节奏，为每个组成部分提供完全集成和合格的构建。 3. CNCF 的价值观 CNCF 会极力遵循以下一些原则：\n快速胜过磨叽，基金会的初衷之一就是让项目快速的发展，从而支持用户能够积极的使用。 开放！ CNCF 是以开放和高度透明为最高准则的，而且是独立于任何的其它团体进行运作的。CNCF 根据贡献的内容和优点接受所有的贡献者，且遵循开源的价值观，CNCF 输出的技术是可以让所有人使用和受益的，技术社区及其决策应保持高度透明。 公平：CNCF 会极力避免那些不好的影响、不良行为、以及“按需付费”的决策。 强大的技术身份：CNCF 会实现并保持高度的自身技术认同，并将之同步到所有的共享项目中。 清晰的边界：CNCF 制定明确的目标，并在某些情况下，要确定什么不是基金会的目标，并会帮助整个生态系统的运转，让人们理解新创新的重点所在。 可扩展：能够支持从小型开发人员中心环境到企业和服务提供商规模的所有部署规模。这意味着在某些部署中可能不会部署某些可选组件，但总体设计和体系结构仍应适用。 平台中立：CNCF 所开发的项目并不针对某个特定平台，而是旨在支持各种体系结构和操作系统。 4. 会员制 CNCF 中的会员包括白金、金牌、银牌、最终用户、学术和非赢利成员等级别，不同级别的会员在理事会中的投票权不同。\na) 白金会员：在 CNCF 理事会中任命 1 名代表，在理事会的每个次级委员会和活动中任命 1 名有投票权的代表，在网站可以突出显示；如果也是终端用户成员将继承终端用户成员的所有权利\nb) 金牌会员：基金会中每有 5 个金牌会员，该级别的会员就可以任命 1 名代表，最多任命 3 个；如果也是终端用户成员将继承终端用户成员的所有权利\nc) 银牌会员：基金会中每有 10 个银牌会员，该级别的会员就可以任命 1 名代表，最多任命 3 个；如果也是终端用户成员将继承终端用户成员的所有权利\nd) 终端用户：参加终端用户咨询社区；向终端用户技术咨询委员会中提名 1 名代表\ne) 学术和非赢利会员：学术和非营利会员分别限于学术和非营利机构，需要理事会批准。学术成员和非营利成员有权将其组织认定为支持 CNCF 使命的成员以及理事会确定的任何其他权利或利益。\n5. 理事会 a) CNCF 理事会负责市场营销、业务监督和预算审批，不负责技术方面，除了与 TOC 配合确定 CNCF 工作范围、完成时间表 a)、更新 CNCF 网站\nb) 负责日常事务\n与 TOC 协商 CNCF 的整体范围 商标和版权保护 市场营销、布道和生态系统建设 创建和执行品牌承诺项目，如果需要的话 监督运营，业务发展； 募资和财务管理 c) 理事会投票成员由会员代表和社区代表组成：\n成员代表包括： 每名白金会员任命 1 名代表 黄金和银牌成员当选代表 技术社区代表包括： 技术监督委员会主席 根据当时在任的理事会批准的程序从 CNCF 项目中选出两名提交者。 理事会可能会以白金会员比例的价格扩展白金会员资格，对年收入低于 5000 万美元的创业公司进行长达 5 年的逐年审计，这些公司被视为理事会的战略技术贡献者。 只有来自一组关联公司的人员可以担任会员代表。只有来自一组关联公司的人员可以担任技术社区代表。 d) 职责\n批准预算，指导将所有收入来源筹集的资金用于技术、市场或社区投资，以推动 CNCF 基金的使命； 选举理事会主席主持会议，批准预算批准的支出并管理日常运作； 对理事会的决定或事项进行投票； 界定和执行基金会的知识产权（版权，专利或商标）政策； 通过活动、新闻和分析师宣传、网络、社交媒体以及其他营销活动进行直接营销和布道； 监督运营，业务发展； 建立并监督为推动 CNCF 的使命而创建的任何委员会； 根据 CNCF 要求（可能包括认证测试）建立并执行品牌合规计划（如有），以使用 TOC 建立的品牌标志； 采用商标使用准则或政策； 提供整体财务管理。 e) 基金会的收入用途\n市场营销，用户扩展 CNCF 中的项目的采用 关键设施建设、运行和管理项目的基础设施 促进基于容器的计算使用 CNCF 中的项目实现 6. 技术监督委员会（TOC） a) 要求\nCNCF 技术监督委员会，为了保持中立，则达成了以下共识：\n定义和维护 CNCF 的技术愿景。 批准由理事会制定的 CNCF 范围内的新项目，并为项目创建一个概念架构。 纠正项目的发展方向，决策删除或存档项目。 接受最终用户委员会的反馈并反映在项目中。 在科学管理的情况下调整组件的接口（在代码标准化之前实现参考） 定义在 CNCF 项目中实施的常用做法（如果有的话）。 b) 技术监督委员会的构成\nTOC 最多由 9 名成员组成。 选出的 TOC 成员将涵盖关键的技术领域：容器技术、操作系统、技术运维、分布式系统、用户级应用程序设计等。 理事会将选举 6 名 TOC 成员，最终用户 TAB 将选出 1 名 TOC 成员，最初的 7 名 TOC 成员应另选两名 TOC 成员。 如果超过两名 TOC 成员来自同一组关联公司，无论是在选举时还是来自后来的工作变更，他们将共同决定谁应该下台，或如果没有协商的依据，则应抽签决定。 c) 运营模式\nTOC 会选举出 TOC 的主席来，此角色主要负责 TOC 的议程和召集会议。 TOC 每个季度会面对面讨论重要的热点问题。 TOC 可能会根据需要开会讨论新出现的问题。TOC 审核可能会提出以下问题： 任何的 TOC 成员 任何的理事会成员 建立的 CNCF 项目的维护者或顶级项目负责人 CNCF 执行董事 最终用户技术咨询委员会获得多数票 保持透明：TOC 会议、邮件列表、以及会议记录等均是公开可访问的。 简单的 TOC 问题可以通过简短的讨论和简单的多数表决来解决。TOC 讨论可通过电子邮件或 TOC 会议进行。 在对意见和可选虚拟讨论/辩论选项进行审查后，寻求共识并在必要时进行投票。 目的是让 TOC 在 TOC 和社区内寻找达成共识的途径。满足法定人数要求的会议的 TOC 决定应以超过 TOC 成员出席率的 50％的方式通过。 TOC 会议需要 TOC 总人数的三分之二法定人数进行表决或作出任何决定。如果 TOC 会议未能达到法定人数要求，可以进行讨 …","relpermalink":"/blog/cncf-introduction/","summary":"CNCF 云原生计算基金会简介以及 CNCF 的运作方式与项目成熟度级别标准介绍。","title":"CNCF - 云原生计算基金会简介"},{"content":"在我们安装 Kubernetes 集群的时候就已经安装了 kube-dns 插件，这个插件也是官方推荐安装的。通过将 Service 注册到 DNS 中，Kuberentes 可以为我们提供一种简单的服务注册发现与负载均衡方式。\nCoreDNS 作为 CNCF 中的托管的一个项目，在 Kuberentes1.9 版本中，使用 kubeadm 方式安装的集群可以通过以下命令直接安装 CoreDNS。\nkubeadm init --feature-gates=CoreDNS=true 您也可以使用 CoreDNS 替换 Kubernetes 插件 kube-dns，可以使用 Pod 部署也可以独立部署，请参考Using CoreDNS for Service Discovery ，下文将介绍如何配置 kube-dns。\n本文已归档到kubernetes-handbook 中。\nkube-dns kube-dns 是 Kubernetes 中的一个内置插件，目前作为一个独立的开源项目维护，见 https://github.com/kubernetes/dns 。\n下文中给出了配置 DNS Pod 的提示和定义 DNS 解析过程以及诊断 DNS 问题的指南。\n前提要求 Kubernetes 1.6 及以上版本。 集群必须使用 kube-dns 插件进行配置。 kube-dns 介绍 从 Kubernetes v1.3 版本开始，使用 [cluster add-on 插件管理器回自动启动内置的 DNS。\nKubernetes DNS pod 中包括 3 个容器：\nkubedns：kubedns 进程监视 Kubernetes master 中的 Service 和 Endpoint 的变化，并维护内存查找结构来服务 DNS 请求。 dnsmasq：dnsmasq 容器添加 DNS 缓存以提高性能。 sidecar：sidecar 容器在执行双重健康检查（针对 dnsmasq 和 kubedns）时提供单个健康检查端点（监听在 10054 端口）。 DNS pod 具有静态 IP 并作为 Kubernetes 服务暴露出来。该静态 IP 分配后，kubelet 会将使用 --cluster-dns = \u0026lt;dns-service-ip\u0026gt; 标志配置的 DNS 传递给每个容器。\nDNS 名称也需要域名。本地域可以使用标志 --cluster-domain = \u0026lt;default-local-domain\u0026gt; 在 kubelet 中配置。\nKubernetes 集群 DNS 服务器基于 SkyDNS 库。它支持正向查找（A 记录），服务查找（SRV 记录）和反向 IP 地址查找（PTR 记录）\nkube-dns 支持的 DNS 格式 kube-dns 将分别为 service 和 pod 生成不同格式的 DNS 记录。\nService\nA 记录：生成my-svc.my-namespace.svc.cluster.local域名，解析成 IP 地址，分为两种情况： 普通 Service：解析成 ClusterIP Headless Service：解析为指定 Pod 的 IP 列表 SRV 记录：为命名的端口（普通 Service 或 Headless Service）生成 _my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster.local 的域名 Pod\nA 记录：生成域名 pod-ip.my-namespace.pod.cluster.local kube-dns 存根域名 可以在 Pod 中指定 hostname 和 subdomain：hostname.custom-subdomain.default.svc.cluster.local，例如：\napiVersion: v1 kind: Pod metadata: name: busybox labels: name: busybox spec: hostname: busybox-1 subdomain: busybox-subdomain containers: name: busybox - image: busybox command: - sleep - \u0026#34;3600\u0026#34; 该 Pod 的域名是 busybox-1.busybox-subdomain.default.svc.cluster.local。\n继承节点的 DNS 运行 Pod 时，kubelet 将预先配置集群 DNS 服务器到 Pod 中，并搜索节点自己的 DNS 设置路径。如果节点能够解析特定于较大环境的 DNS 名称，那么 Pod 应该也能够解析。请参阅下面的已知问题 以了解警告。\n如果您不想要这个，或者您想要为 Pod 设置不同的 DNS 配置，您可以给 kubelet 指定 --resolv-conf 标志。将该值设置为 \u0026#34;\u0026#34; 意味着 Pod 不继承 DNS。将其设置为有效的文件路径意味着 kubelet 将使用此文件而不是 /etc/resolv.conf 用于 DNS 继承。\n配置存根域和上游 DNS 服务器 通过为 kube-dns（kube-system:kube-dns）提供一个 ConfigMap，集群管理员能够指定自定义存根域和上游 nameserver。\n例如，下面的 ConfigMap 建立了一个 DNS 配置，它具有一个单独的存根域和两个上游 nameserver：\napiVersion: v1 kind: ConfigMap metadata: name: kube-dns namespace: kube-system data: stubDomains: | {“acme.local”: [“1.2.3.4”]} upstreamNameservers: | [“8.8.8.8”, “8.8.4.4”] 如上面指定的那样，带有“.acme.local”后缀的 DNS 请求被转发到 1.2.3.4 处监听的 DNS。Google Public DNS 为上游查询提供服务。\n下表描述了如何将具有特定域名的查询映射到其目标 DNS 服务器：\n域名 响应查询的服务器 kubernetes.default.svc.cluster.local kube-dns foo.acme.local 自定义 DNS (1.2.3.4) widget.com 上游 DNS (8.8.8.8 或 8.8.4.4) 查看 ConfigMap 选项 获取更多关于配置选项格式的详细信息。\n对 Pod 的影响 自定义的上游名称服务器和存根域不会影响那些将自己的 dnsPolicy 设置为 Default 或者 None 的 Pod。\n如果 Pod 的 dnsPolicy 设置为“ClusterFirst”，则其名称解析将按其他方式处理，具体取决于存根域和上游 DNS 服务器的配置。\n未进行自定义配置：没有匹配上配置的集群域名后缀的任何请求，例如“www.kubernetes.io”，将会被转发到继承自节点的上游 nameserver。\n进行自定义配置：如果配置了存根域和上游 DNS 服务器（和在 前面例子 配置的一样），DNS 查询将根据下面的流程进行路由：\n查询首先被发送到 kube-dns 中的 DNS 缓存层。\n从缓存层，检查请求的后缀，并转发到合适的 DNS 上，基于如下的示例：\n具有集群后缀的名字 （例如“.cluster.local”）：请求被发送到 kube-dns。 具有存根域后缀的名字 （例如“.acme.local”）：请求被发送到配置的自定义 DNS 解析器（例如：监听在 1.2.3.4）。 不具有能匹配上后缀的名字 （例如“widget.com”）：请求被转发到上游 DNS（例如：Google 公共 DNS 服务器，8.8.8.8 和 8.8.4.4）。 DNS lookup flow ConfigMap 选项 kube-dns kube-system:kube-dns ConfigMap 的选项如下所示：\n字段 格式 描述 stubDomains（可选） 使用 DNS 后缀 key 的 JSON map（例如“acme.local”），以及 DNS IP 的 JSON 数组作为 value。 目标 nameserver 可能是一个 Kubernetes Service。例如，可以运行自己的 dnsmasq 副本，将 DNS 名字暴露到 ClusterDNS namespace 中。 upstreamNameservers（可选） DNS IP 的 JSON 数组。 注意：如果指定，则指定的值会替换掉被默认从节点的 /etc/resolv.conf 中获取到的 nameserver。限制：最多可以指定三个上游 nameserver。 示例 示例：存根域 在这个例子中，用户有一个 Consul DNS 服务发现系统，他们希望能够与 kube-dns 集成起来。Consul 域名服务器地址为 10.150.0.1，所有的 Consul 名字具有后缀“.consul.local”。要配置 Kubernetes，集群管理员只需要简单地创建一个 ConfigMap 对象，如下所示：\napiVersion: v1 kind: ConfigMap metadata: name: kube-dns namespace: kube-system data: stubDomains: | {“consul.local”: [“10.150.0.1”]} 注意，集群管理员不希望覆盖节点的上游 nameserver，所以他们不会指定可选的 upstreamNameservers 字段。\n示例：上游 nameserver 在这个示例中，集群管理员不希望显式地强制所有非集群 DNS 查询进入到他们自己的 nameserver 172.16.0.1。而且这很容易实现：他们只需要创建一个 ConfigMap，upstreamNameservers 字段指定期望的 nameserver 即可。\napiVersion: v1 kind: ConfigMap metadata: name: kube-dns namespace: kube-system data: upstreamNameservers: | [“172.16.0.1”] 调试 DNS 解析 创建一个简单的 Pod 用作测试环境 创建一个名为 busybox.yaml 的文件，其中包括以下内容：\napiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - name: busybox image: busybox command: - sleep - \u0026#34;3600\u0026#34; imagePullPolicy: IfNotPresent restartPolicy: Always 使用该文件创建 Pod 并验证其状态：\n$ kubectl create -f busybox.yaml pod \u0026#34;busybox\u0026#34; created $ kubectl get pods busybox NAME READY STATUS RESTARTS AGE busybox 1/1 Running 0 \u0026lt;some-time\u0026gt; 该 Pod 运行后，您可以在它的环境中执行 nslookup。如果您看到类似如下的输出，表示 DNS 正在正确工作。\n$ kubectl exec -ti busybox -- nslookup kubernetes.default Server: 10.0.0.10 Address 1: 10.0.0.10 Name: kubernetes.default Address 1: …","relpermalink":"/blog/configuring-kubernetes-kube-dns/","summary":"配置 Kubernetes DNS Pod 的提示和定义 DNS 解析过程以及诊断 DNS 问题的指南。","title":"配置 Kubernetes DNS 服务 kube-dns"},{"content":"2017 年，我们面临着架构变革的大时代，例如 Kubernetes 结束容器编排之争、Kafka 发布 1.0、Serverless 逐渐发力、边缘计算要取代云计算、Service Mesh 蓄势待发，另外人工智能为业务赋能，也给架构带来了新的挑战。\n我即将参加 InfoQ 在 12 月 8-11 日北京国际会议中心举办ArchSummit 全球架构师峰会，这场大会还邀请了诸如阿里王坚博士等 100+顶尖技术人前来分享总结今年的架构变化和思考。希望各位能凭借这场大会承上启下，总结过去的实践，也展望面向未来的架构，将这个变革的时代转变为我们每人共同的幸运。\n我的演讲 我的演讲内容是从 Kubernetes 到 Cloud Native——云原生应用之路，链接：从 Kubernetes 到 Cloud Native——云原生应用之路 。时间是 12 月 9 日，星期六，上午 9:30，第五会议室。\n云计算经过了十多年的发展，已然进入的云原生的新阶段，企业应用优先考虑部署在云环境，如何顺应云原生的大潮，使用容器和 Kubernetes 构建云原生平台，践行 DevOps 理念和敏捷 IT，开源软件和社区如何助力 IT 转型，所有这些问题的解决方案就是 PaaS 平台，其对于企业的重要性不言而喻。\n我们还为大家准备了赠品：《Cloud Native Go——基于 Go 和 React 构建云原生 Web 应用》 和《智能数据时代——企业大数据战略与实战》，到场的朋友都有机会获得，同时场外还有电子工业出版社博文视点的图书展台，欢迎大家参观。\nArchSummit 大会官网链接：http://bj2017.archsummit.com/ 更多详细信息请参考大会官网：http://bj2017.archsummit.com/ ","relpermalink":"/notice/archsummit-beijing-2017-from-kubernetes-to-cloud-native/","summary":"Jimmy Song 将在 ArchSummit 北京站的演讲，从 Kubernetes 到 Cloud Native，我的云原生应用之路。","title":"ArchSummit Beijing 2017 演讲预告"},{"content":"本文主要讲解访问 kubenretes 中的 Pod 和 Serivce 的几种方式，包括如下几种：\nhostNetwork hostPort NodePort LoadBalancer Ingress 说是暴露 Pod 其实跟暴露 Service 是一回事，因为 Pod 就是 Service 的 backend。\nhostNetwork: true 这是一种直接定义 Pod 网络的方式。\n如果在 Pod 中使用 hostNetwork:true 配置的话，在这种 pod 中运行的应用程序可以直接看到 pod 启动的主机的网络接口。在主机的所有网络接口上都可以访问到该应用程序。以下是使用主机网络的 pod 的示例定义：\napiVersion: v1 kind: Pod metadata: name: influxdb spec: hostNetwork: true containers: - name: influxdb image: influxdb 部署该 Pod：\n$ kubectl create -f influxdb-hostnetwork.yml 访问该 pod 所在主机的 8086 端口：\ncurl -v http://$POD_IP:8086/ping 将看到 204 No Content 的 204 返回码，说明可以正常访问。\n注意每次启动这个 Pod 的时候都可能被调度到不同的节点上，所有外部访问 Pod 的 IP 也是变化的，而且调度 Pod 的时候还需要考虑是否与宿主机上的端口冲突，因此一般情况下除非您知道需要某个特定应用占用特定宿主机上的特定端口时才使用 hostNetwork: true 的方式。\n这种 Pod 的网络模式有一个用处就是可以将网络插件包装在 Pod 中然后部署在每个宿主机上，这样该 Pod 就可以控制该宿主机上的所有网络。\nhostPort 这是一种直接定义 Pod 网络的方式。\nhostPort 是直接将容器的端口与所调度的节点上的端口路由，这样用户就可以通过宿主机的 IP 加上来访问 Pod 了，如:。\napiVersion: v1 kind: Pod metadata: name: influxdb spec: containers: - name: influxdb image: influxdb ports: - containerPort: 8086 hostPort: 8086 这样做有个缺点，因为 Pod 重新调度的时候该 Pod 被调度到的宿主机可能会变动，这样就变化了，用户必须自己维护一个 Pod 与所在宿主机的对应关系。\n这种网络方式可以用来做 nginx Ingress controller 。外部流量都需要通过 kubenretes node 节点的 80 和 443 端口。\nNodePort NodePort 在 kubenretes 里是一个广泛应用的服务暴露方式。Kubernetes 中的 service 默认情况下都是使用的 ClusterIP 这种类型，这样的 service 会产生一个 ClusterIP，这个 IP 只能在集群内部访问，要想让外部能够直接访问 service，需要将 service type 修改为 nodePort。\napiVersion: v1 kind: Pod metadata: name: influxdb labels: name: influxdb spec: containers: - name: influxdb image: influxdb ports: - containerPort: 8086 同时还可以给 service 指定一个 nodePort 值，范围是 30000-32767，这个值在 API server 的配置文件中，用 --service-node-port-range 定义。\nkind: Service apiVersion: v1 metadata: name: influxdb spec: type: NodePort ports: - port: 8086 nodePort: 30000 selector: name: influxdb 集群外就可以使用 kubernetes 任意一个节点的 IP 加上 30000 端口访问该服务了。kube-proxy 会自动将流量以 round-robin 的方式转发给该 service 的每一个 pod。\n这种服务暴露方式，无法让你指定自己想要的应用常用端口，不过可以在集群上再部署一个反向代理作为流量入口。\nLoadBalancer LoadBalancer 只能在 service 上定义。这是公有云提供的负载均衡器，如 AWS、Azure、CloudStack、GCE 等。\nkind: Service apiVersion: v1 metadata: name: influxdb spec: type: LoadBalancer ports: - port: 8086 selector: name: influxdb 查看服务：\n$ kubectl get svc influxdb NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE influxdb 10.97.121.42 10.13.242.236 8086:30051/TCP 39s 内部可以使用 ClusterIP 加端口来访问服务，如 19.97.121.42:8086。\n外部可以用以下两种方式访问该服务：\n使用任一节点的 IP 加 30051 端口访问该服务 使用 EXTERNAL-IP 来访问，这是一个 VIP，是云供应商提供的负载均衡器 IP，如 10.13.242.236:8086。 Ingress Ingress 是自 kubernetes1.1 版本后引入的资源类型。必须要部署 Ingress controller 才能创建 Ingress 资源，Ingress controller 是以一种插件的形式提供。Ingress controller 是部署在 Kubernetes 之上的 Docker 容器。它的 Docker 镜像包含一个像 nginx 或 HAProxy 的负载均衡器和一个控制器守护进程。控制器守护程序从 Kubernetes 接收所需的 Ingress 配置。它会生成一个 nginx 或 HAProxy 配置文件，并重新启动负载平衡器进程以使更改生效。换句话说，Ingress controller 是由 Kubernetes 管理的负载均衡器。\nKubernetes Ingress 提供了负载平衡器的典型特性：HTTP 路由，粘性会话，SSL 终止，SSL 直通，TCP 和 UDP 负载平衡等。目前并不是所有的 Ingress controller 都实现了这些功能，需要查看具体的 Ingress controller 文档。\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: influxdb spec: rules: - host: influxdb.kube.example.com http: paths: - backend: serviceName: influxdb servicePort: 8086 外部访问 URL http://influxdb.kube.example.com/ping 访问该服务，入口就是 80 端口，然后 Ingress controller 直接将流量转发给后端 Pod，不需再经过 kube-proxy 的转发，比 LoadBalancer 方式更高效。\n总结 总的来说 Ingress 是一个非常灵活和越来越得到厂商支持的服务暴露方式，包括 Nginx、HAProxy、Traefik，还有各种 Service Mesh，而其它服务暴露方式可以更适用于服务调试、特殊应用的部署。\n","relpermalink":"/blog/accessing-kubernetes-pods-from-outside-of-the-cluster/","summary":"关于在 Kubenretes 中暴露 Pod 及服务的五种方式。","title":"从外部访问 Kubernetes 中的 Pod"},{"content":"Cloudinary-go is a Go client library and CLI tool to upload static assets to the Cloudinary service.\nInstallation Install the CLI tool and the library with:\ngo get github.com/rootsongjc/cloudinary-go/cloudinary Or download the release binary from release .\n","relpermalink":"/notice/cloudinary-go/","summary":"Cloudinary-go is a Go client library and CLI tool to upload static assets to Cloudinary service","title":"Go 语言编写的 Cloudinary 文件上传工具发布"},{"content":"截止本文发稿时，笔者是以下两本云原生图书的译者：\nCloud Native Go ：已由电子工业出版社出版 Cloud Native Python ：正在翻译中 同时我还参与了 Kubernetes 、Istio 的文档翻译，撰写了开源电子书 kubernetes-handbook ，下面是我本人在翻译过程中的的一些心得。\n说明：本文中使用的方法仅供参考，机器翻译有助您快速了解全书或文章的梗概，请勿直接使用机器翻译结果输出。\n图书引进 1. 联系出版社 假如您看到一本很好的外文书籍想要翻译，首先需要联系出版社，询问该书是否已被引进，因为每年国内引进的外文书籍是有数量控制的，而且有的书也不是你先给引进就可以引进的，每年都有版权引进会议，国内的出版社统一参加确定引进的书籍，哪家引进多少本，哪一本分给哪一家等。可以与出版社编辑沟通，查看该书是否可以引进，是否已经有别的出版社引进且在翻译中，这个过程基本不需要你与原作者沟通。\n2. 取得图书引进的版权 如果很幸运的，这本书可以引进到国内，而且还没有人来翻译，可以跟出版社编辑要求翻译这本书，如果书籍内容适当可以一个人翻译，如果内容较多可以分多个人翻译，建议人数不要超过 4 人。\n环境准备 首先需要准备如下环境：\nGit：用户版本管理，也方便在线查看，我使用 码云 私有代码库管理。 Markdown 编辑器：我推荐使用 typora 。 Gitbook：使用 Gitbook 生成 web 页面便于阅读和查看，注意不要公开发布到 Github 上。 Word：虽然我们使用 markdown 编辑器来编辑，但是 word 还是需要的，因为编辑会在 word 中批注，再返回给你修改。 Translation-shell：命令行翻译工具，见 Github 。 翻译过程 以下是我个人总结的图书翻译流程，仅供参考。\n1. 分析原版压缩包的结构 以 Cloud Native Python 这本书为例，原文的压缩包里包含以下目录：\nCode：书中的代码示例 Cover：本书的封面图片 E-Book：本书的完成 PDF 文档（一个文件） Graphics：书中的图片，按照章节和顺序编号，放在一个目录下，不一定与图片在书中出现的顺序相同，有些后来补充的图片会另外编号 Printers：用于印刷的 PDF 文档，分为封面和正文 2. 初始化翻译项目 我们使用 Git 来管理，使用 Gitbook 来预览，需要先初始化一些目录结构和 gitbook 配置。\n初始化的目录和文件：\nLANGS.md：语言配置文件 README.md：项目说明 book.json：gitbook 配置文件 cn：中文翻译（按章节划分成不同的文件） corrigendum.md：勘误表 cover.jpg：书籍封面 en：英文原文（按章节划分成不同的文件） glossary.md：术语表 images：保存书中的图片 让 Gitbook 支持多语言的 book.json 配置如下：\n{ \u0026#34;title\u0026#34;: \u0026#34;Cloud Native Python\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Cloud Native Python\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;zh-hans\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Jimmy Song\u0026#34;, \u0026#34;links\u0026#34;: { \u0026#34;sidebar\u0026#34;: {\u0026#34;Home\u0026#34;: \u0026#34;https://jimmysong.io\u0026#34;} }, \u0026#34;plugins\u0026#34;: [ \u0026#34;codesnippet\u0026#34;, \u0026#34;splitter\u0026#34;, \u0026#34;page-toc-button\u0026#34;, \u0026#34;back-to-top-button\u0026#34;, \u0026#34;-lunr\u0026#34;, \u0026#34;-search\u0026#34;, \u0026#34;search-plus\u0026#34;, \u0026#34;tbfed-pagefooter@^0.0.1\u0026#34; ], \u0026#34;pluginsConfig\u0026#34;: { \u0026#34;tbfed-pagefooter\u0026#34;: { \u0026#34;copyright\u0026#34;: \u0026#34;Copyright © jimmysong.io 2017\u0026#34;, \u0026#34;modify_label\u0026#34;: \u0026#34;Updated:\u0026#34;, \u0026#34;modify_format\u0026#34;: \u0026#34;YYYY-MM-DD HH:mm:ss\u0026#34; } } } LANG.md 文件中定义不同语言的文件目录：\n# Languages * [中文](cn/) * [English](en/) 3. 原文 Markdown 化 之所以将原文 Markdown 化一是便于我们后续翻译的时候对照英文和引用其中的原文，二是为了生成 gitbook 便于浏览。将每一章的内容都划分成一个 Markdown 文件，按照章节的名字为文档命名，分别在 cn 和 en 目录下都放一份。\n中英文目录 4. 开始正文的翻译 建议从头开始按顺序翻译，如果前后章节联系不大的可以跳跃翻译，翻译的过程中将一些关键的术语，包括翻译不明确的，需要后续参考的数据记录在 glossary.md 文档中。\n格式如下所示：\nEnglish 中文 说明 是否翻译 Cross-Origin Resource Sharing 跨源资源共享 是 HTTP header 否 Observable 观察者 可以不翻译，中文翻译比较模糊 否 cookies 不翻译，保持复数 否 module 模块 是 origin 源 有争议 是 session 会话 否 可以不断向其中追加新的术语。\n翻译的过程中需要用到翻译工具，我使用的是 translation-shell ，一款基于命令行的翻译工具，可以使用 Google、bing 或者 Yandex 翻译，十分方便快捷。也推荐大家使用 DeepL ，翻译效果更好。\n注：使用翻译工具是为了将书籍快速汉化，减少大量的人工输入，但是因为机器翻译比较生硬，而且其中难免有错误，需要译者投入大量心思去优化。\nTranslation-shell 使用 trans :zh -b -shell 进入 translation-shell 交互式界面，拷贝英文段落进去翻译成中文。\nTranslation-shell 注：推荐使用翻译质量更高的工具 DeepL （更新于 2022 年 02 月 22 日）。\n使用 Typora 编辑中文翻译 同时打开 en 和 cn 目录下的同一章节开始翻译。\n中英文翻译界面 在 Gitbook 中查看 使用 gitbook serve 启用 gitbook 服务，在 http://localhost:4000 页面上查看内容。\n首先会出来语言选择页面，我们可以分别选择中文和英文内容浏览，语言是在 LAGNS.md 文件中定义的。\nGitbook 导出为不同格式 使用 typora 编辑完中文翻译后，可以导出为 pdf、word 等其它格式，我们导出为 word 格式后发送给编辑批阅。\n生成的 word 内容格式是这样的：\nword 文档格式 我们可以看到生产的 word 文档仍然保留了代码的高亮，而且可读性也很好。\n5. 审校 每当翻译完一章内容后就发送给编辑，编辑会使用 word 进行审校批注，根据编辑的批注修改后再发回给编辑。\nword review 界面 6. 二审 当所有的章节分别翻译和审校完成后，需要在通读一遍全书，更正前后不一致和翻译中的谬误，然后交给编辑等待排版。这时候还要准备写译者序，还要找人写推荐序。翻译版的图书封面会沿用原书的封面。\n7. 印刷 当正文、译者序、推荐序都完成后就可以交给出版社印刷了，一般初次会印刷几千本。\n8. 后续事宜 书籍印刷后后续事宜主要包括：\n出版社支付稿费：翻译图书稿费 = 图书销量 x 定价 x4%，著作一般为 8% 配合图书宣传：一些 meetup、大会、线上交流时推荐图书 读者交流：可以开设社区、微信群、网站等交流 贴士 图书翻译耗时费力，倾注了原作者和译者的很多心力，打击盗版，维护正版！\n有用的链接 术语在线 非文学翻译理论与实践 - 王长栓 ","relpermalink":"/blog/how-to-translate-a-book/","summary":"如何翻译一本外文书，从图书引进到翻译出版全攻略。","title":"如何翻译一本外文书"},{"content":"很多人问我 jimmysong.io 这个网站是怎么做出来的，我想有必要写本书给大家普及下静态网站构建的知识还有 Hugo 这个利器。\n本手册将指导你如何使用Hugo 构建静态网站用于个人博客或者项目展示。\n手把手教你如何从 0 开始构建一个静态网站，这不需要有太多的编程和开发经验和时间投入，也基本不需要多少成本（除了个性化域名），使用 GitHub 和 Hugo 模板即可快速构建和上线一个网站。\nGithub 地址：https://github.com/rootsongjc/hugo-handbook Gitbook 访问地址：https://jimmysong.io/hugo-handbook 将随着时间的推移，也随着我的网站的改进而不断的完善本书内容，敬请期待。\n","relpermalink":"/notice/building-static-website-with-hugo/","summary":"静态网站构建手册，使用 Hugo 构建个人博客 Gitbook。","title":"Hugo Handbook 发布"},{"content":"今天受 k8smeetup 社区邀请来到杭州，参加 Kubernetes 中国用户大会简称 KEUC ，这已经是我第三次来杭州了，算是再续前缘吧！\n其实今年 6 月 19 日 LinuxCon + ContainerCon + CloudOpen 简称 L3 大会 在北京国家会议中心召开，那是我跟 CNCF 首次相会，也获得了我的首批贴纸，该社区的一系列活动吸引了我浓浓的兴趣，自那以后开始持续关注 CNCF 的社区活动。\n借用孙中山先生在黄埔军校的训词，愿 Kubernetes 携手云原生应用，让 IT 基础设施和软件开发流程进入新的纪元。\n云原生主义歌\n库巴内提，吾辈所宗；携云原生，以进大同。 咨尔多士，为民前锋；夙夜匪懈，主义是从。 创业维艰，矢勤矢勇；同心共德，贯彻始终。\n祝愿明天的大会圆满成功！\n今天不小心获得了 k8smeetup 最佳技术专栏作者和社区最佳译者奖，感谢 CNCF 和 k8smeetup 社区。\nk8smeetup 译者合影 然后跟 CNCF 执行副总裁 Dan Kohn 探讨了下云原生技术在中国的推广，明年在中国会有云原生相关大会。\nJimmy 和 CNCF 执行副总裁 Dan Kohn 在一起 欢迎大家持续关注云原生的发展。\n","relpermalink":"/blog/keuc-china-2017/","summary":"2017 年 10 月 15 日，杭州，一场顶级的 Kubernetes 行业用户落地大会。","title":"记 Kubernetes 中国用户大会 KEUC 2017"},{"content":"Service Mesh 又译作“服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A Service Mesh? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。\n下面是 Willian Morgan 对 Service Mesh 的解释。\nA Service Mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application. In practice, the Service Mesh is typically implemented as an array of lightweight network proxies that are deployed alongside application code, without the application needing to be aware.\n翻译成中文是：\n服务网格（Service Mesh）是处理服务间通信的基础设施层。它负责构成现代云原生应用程序的复杂服务拓扑来可靠地交付请求。在实践中，Service Mesh 通常以轻量级网络代理阵列的形式实现，这些代理与应用程序代码部署在一起，对应用程序来说无需感知代理的存在。\nService Mesh 的特点 Service Mesh 有如下几个特点：\n应用程序间通信的中间层 轻量级网络代理 应用程序无感知 解耦应用程序的重试/超时、监控、追踪和服务发现 目前两款流行的 Service Mesh 开源软件 Istio 和 Linkerd 都可以直接在 Kubernetes 中集成，其中 Linkerd 已经成为 CNCF 中的项目。\n理解 Service Mesh 如果用一句话来解释什么是 Service Mesh，可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用 Service Mesh 也就无须关心服务之间的那些原本通过服务框架实现的事情，比如 Spring Cloud、Netflix OSS 和其他中间件，现在只要交给 Service Mesh 就可以了。\nPhil Calçado 在他的这篇博客 Pattern: Service Mesh 中详细解释了 Service Mesh 的来龙去脉：\n从最原始的主机之间直接使用网线相连 网络层的出现 集成到应用程序内部的控制流 分解到应用程序外部的控制流 应用程序的中集成服务发现和断路器 出现了专门用于服务发现和断路器的软件包/库，如 Twitter 的 Finagle 和 Facebook 的 Proxygen ，这时候还是集成在应用程序内部 出现了专门用于服务发现和断路器的开源软件，如 Netflix OSS 、Airbnb 的 synapse 和 nerve 最后作为微服务的中间层 Service Mesh 出现 Service Mesh 的架构如下图所示：\nService Mesh 架构图 图片来自：Pattern: Service Mesh Service Mesh 作为 sidecar 运行，对应用程序来说是透明，所有应用程序间的流量都会通过它，所以对应用程序流量的控制都可以在 serivce mesh 中实现。\nService Mesh 如何工作？ 下面以 Istio 为例讲解 Service Mesh 如何工作，后续文章将会详解 Istio 如何在 Kubernetes 中工作。\nSidecar（Istio 中使用 Envoy 作为 sidecar 代理）将服务请求路由到目的地址，根据请求中的参数判断是到生产环境、测试环境还是 staging 环境中的服务（服务可能同时部署在这三个环境中），是路由到本地环境还是公有云环境？所有的这些路由信息可以动态配置，可以是全局配置也可以为某些服务单独配置。这些配置是由服务网格的控制平面推送给各个 sidecar 的， 当 sidecar 确认了目的地址后，将流量发送到相应服务发现端点，在 Kubernetes 中是 service，然后 service 会将服务转发给后端的实例。 Sidecar 根据它观测到最近请求的延迟时间，选择出所有应用程序的实例中响应最快的实例。 Sidecar 将请求发送给该实例，同时记录响应类型和延迟数据。 如果该实例挂了、不响应了或者进程不工作了，sidecar 会将把请求发送到其他实例上重试。 如果该实例持续返回 error，sidecar 会将该实例从负载均衡池中移除，稍后再周期性得重试。 如果请求的截止时间已过，sidecar 主动标记该请求为失败，而不是再次尝试添加负载。 SIdecar 以 metric 和分布式追踪的形式捕获上述行为的各个方面，这些追踪信息将发送到集中 metric 系统。 为何使用 Service Mesh？ Service Mesh 并没有给我们带来新功能，它是用于解决其他工具已经解决过的问题，只不过这次是在以 Kubernetes 为基础的云原生生态环境下的实现。\n在传统的 MVC 三层 Web 应用程序架构下，服务之间的通讯并不复杂，在应用程序内部自己管理即可，但是在现今的复杂的大型网站情况下，单体应用被分解为众多的微服务，服务之间的依赖和通讯十分复杂，出现了 twitter 开发的 Finagle 、Netflix 开发的 Hystrix 和 Google 的 Stubby 这样的“胖客户端”库，这些就是早期的 Service Mesh，但是它们都仅适用于特定的环境和特定的开发语言，并不能作为平台级的 Service Mesh 支持。\n在 Cloud Native 架构下，容器的使用赋予了异构应用程序更多的可能性，Kubernetes 增强了应用的横向扩容能力，用户可以快速的编排出复杂环境、复杂依赖关系的应用程序，同时开发者又无须过分关心应用程序的监控、扩展性、服务发现和分布式追踪这些繁琐的事情，进而专注于程序开发，赋予开发者更多的创造性。\n参考 What’s a Service Mesh? And why do I need one? So what even is a Service Mesh? Hot take on Istio and Linkerd linkerd: A Service Mesh for AWS ECS Introducing Istio: A robust Service Mesh for microservices Application Network Functions With ESBs, API Management, and Now.. Service Mesh? Pattern: Service Mesh Envoy 官方文档 Istio 官方文档 Istio Handbook - Istio 服务网格进阶实战 ","relpermalink":"/blog/what-is-a-service-mesh/","summary":"本文介绍了 Service Mesh 是什么，其工作原理并提供了一些有用的链接。","title":"什么是 Service Mesh（服务网格）？"},{"content":"对于没有使用过 kubernetes 的 docker 用户，如何快速掌握 kubectl 命令？kubectl 跟 docker 命令之间有什么区别和联系？\n在本文中，我们将向 docker-cli 用户介绍 Kubernetes 命令行如何与 api 进行交互。该命令行工具——kubectl，被设计成 docker-cli 用户所熟悉的样子，但是它们之间又存在一些必要的差异。该文档将向您展示每个 docker 子命令和 kubectl 与其等效的命令。\n在使用 kubernetes 集群的时候，docker 命令通常情况是不需要用到的，只有在调试程序或者容器的时候用到，我们基本上使用 kubectl 命令即可，所以在操作 kubernetes 的时候我们抛弃原先使用 docker 时的一些观念。\ndocker run 如何运行一个 nginx Deployment 并将其暴露出来？查看 kubectl run 。\n使用 docker 命令：\n$ docker run -d --restart=always -e DOMAIN=cluster --name nginx-app -p 80:80 nginx a9ec34d9878748d2f33dc20cb25c714ff21da8d40558b45bfaec9955859075d0 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a9ec34d98787 nginx \u0026#34;nginx -g \u0026#39;daemon of 2 seconds ago Up 2 seconds 0.0.0.0:80-\u0026gt;80/tcp, 443/tcp nginx-app 使用 kubectl 命令：\n# start the pod running nginx $ kubectl run --image=nginx nginx-app --port=80 --env=\u0026#34;DOMAIN=cluster\u0026#34; deployment \u0026#34;nginx-app\u0026#34; created 在大于等于 1.2 版本 Kubernetes 集群中，使用kubectl run 命令将创建一个名为 “nginx-app” 的 Deployment。如果您运行的是老版本，将会创建一个 replication controller。如果您想沿用旧的行为，使用 --generation=run/v1 参数，这样就会创建 replication controller。查看 kubectl run 获取更多详细信息。\n# expose a port through with a service $ kubectl expose deployment nginx-app --port=80 --name=nginx-http service \u0026#34;nginx-http\u0026#34; exposed 在 kubectl 命令中，我们创建了一个 Deployment ，这将保证有 N 个运行 nginx 的 pod（N 代表 spec 中声明的 replica 数，默认为 1）。我们还创建了一个 service ，使用 selector 匹配具有相应的 selector 的 Deployment。查看 快速开始 获取更多信息。\n默认情况下镜像会在后台运行，与docker run -d ... 类似，如果您想在前台运行，使用：\nkubectl run [-i] [--tty] --attach \u0026lt;name\u0026gt; --image=\u0026lt;image\u0026gt; 与 docker run ... 不同的是，如果指定了 --attach ，我们将连接到 stdin，stdout 和 stderr，而不能控制具体连接到哪个输出流（docker -a ...）。\n因为我们使用 Deployment 启动了容器，如果您终止了连接到的进程（例如 ctrl-c），容器将会重启，这跟 docker run -it不同。如果想销毁该 Deployment（和它的 pod），您需要运行 kubeclt delete deployment \u0026lt;name\u0026gt;。\ndocker ps 如何列出哪些正在运行？查看 kubectl get 。\n使用 docker 命令：\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a9ec34d98787 nginx \u0026#34;nginx -g \u0026#39;daemon of About an hour ago Up About an hour 0.0.0.0:80-\u0026gt;80/tcp, 443/tcp nginx-app 使用 kubectl 命令：\n$ kubectl get po NAME READY STATUS RESTARTS AGE nginx-app-5jyvm 1/1 Running 0 1h docker attach 如何连接到已经运行在容器中的进程？查看 kubectl attach 。\n使用 docker 命令：\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a9ec34d98787 nginx \u0026#34;nginx -g \u0026#39;daemon of 8 minutes ago Up 8 minutes 0.0.0.0:80-\u0026gt;80/tcp, 443/tcp nginx-app $ docker attach a9ec34d98787 ... 使用 kubectl 命令：\n$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-app-5jyvm 1/1 Running 0 10m $ kubectl attach -it nginx-app-5jyvm ... docker exec 如何在容器中执行命令？查看 kubectl exec 。\n使用 docker 命令：\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a9ec34d98787 nginx \u0026#34;nginx -g \u0026#39;daemon of 8 minutes ago Up 8 minutes 0.0.0.0:80-\u0026gt;80/tcp, 443/tcp nginx-app $ docker exec a9ec34d98787 cat /etc/hostname a9ec34d98787 使用 kubectl 命令：\n$ kubectl get po NAME READY STATUS RESTARTS AGE nginx-app-5jyvm 1/1 Running 0 10m $ kubectl exec nginx-app-5jyvm -- cat /etc/hostname nginx-app-5jyvm 执行交互式命令怎么办？\n使用 docker 命令：\n$ docker exec -ti a9ec34d98787 /bin/sh # exit 使用 kubectl 命令：\n$ kubectl exec -ti nginx-app-5jyvm -- /bin/sh # exit 更多信息请查看 获取运行中容器的 Shell 环境 。\ndocker logs 如何查看运行中进程的 stdout/stderr？查看 kubectl logs 。\n使用 docker 命令：\n$ docker logs -f a9e 192.168.9.1 - - [14/Jul/2015:01:04:02 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.35.0\u0026#34; \u0026#34;-\u0026#34; 192.168.9.1 - - [14/Jul/2015:01:04:03 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.35.0\u0026#34; \u0026#34;-\u0026#34; 使用 kubectl 命令：\n$ kubectl logs -f nginx-app-zibvs 10.240.63.110 - - [14/Jul/2015:01:09:01 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.26.0\u0026#34; \u0026#34;-\u0026#34; 10.240.63.110 - - [14/Jul/2015:01:09:02 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.26.0\u0026#34; \u0026#34;-\u0026#34; 现在是时候提一下 pod 和容器之间的细微差别了；默认情况下如果 pod 中的进程退出 pod 也不会终止，相反它将会重启该进程。这类似于 docker run 时的 --restart=always 选项，这是主要差别。在 docker 中，进程的每个调用的输出都是被连接起来的，但是对于 kubernetes，每个调用都是分开的。要查看以前在 kubernetes 中执行的输出，请执行以下操作：\n$ kubectl logs --previous nginx-app-zibvs 10.240.63.110 - - [14/Jul/2015:01:09:01 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.26.0\u0026#34; \u0026#34;-\u0026#34; 10.240.63.110 - - [14/Jul/2015:01:09:02 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.26.0\u0026#34; \u0026#34;-\u0026#34; 查看 记录和监控集群活动 获取更多信息。\ndocker stop 和 docker rm 如何停止和删除运行中的进程？查看 kubectl delete 。\n使用 docker 命令：\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a9ec34d98787 nginx \u0026#34;nginx -g \u0026#39;daemon of 22 hours ago Up 22 hours 0.0.0.0:80-\u0026gt;80/tcp, 443/tcp nginx-app $ docker stop a9ec34d98787 a9ec34d98787 $ docker rm a9ec34d98787 a9ec34d98787 使用 kubectl 命令：\n$ kubectl get deployment nginx-app NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-app 1 1 1 1 2m $ kubectl get po -l run=nginx-app NAME READY STATUS RESTARTS AGE nginx-app-2883164633-aklf7 1/1 Running 0 2m $ kubectl delete deployment nginx-app deployment \u0026#34;nginx-app\u0026#34; deleted $ kubectl get po -l run=nginx-app # Return nothing 请注意，我们不直接删除 pod。使用 kubectl 命令，我们要删除拥有该 pod 的 Deployment。如果我们直接删除 pod，Deployment 将会重新创建该 pod。\ndocker login 在 kubectl 中没有对 docker login 的直接模拟。如果您有兴趣在私有镜像仓库中使用 Kubernetes，请参阅 使用私有镜像仓库 。\ndocker version 如何查看客户端和服务端的版本？查看 kubectl version 。\n使用 docker 命令：\n$ docker version …","relpermalink":"/blog/docker-cli-to-kubectl/","summary":"对于没有使用过 kubernetes 的 docker 用户，如何快速掌握 kubectl 命令？","title":"Docker 用户过渡到 kubectl 命令行指南"},{"content":"Kubernetes 是一个自动发布、扩缩容和管理容器化应用的开源软件。\n尽管 kubernetes 非常强大，有如此多有用的技术特性，但是工具从来都不会被隔离起来单独使用，这要取决与底层基础架构，使用它的团队等等。\n在将 kubernetes 应用到生产环境（如果想成功的引入生产的话）之前，每个 CTO 都应该了解这三件事。\n1. 你需要有坚实的基础设施 大多数组织在运行 kubernetes 时遇到的第一个问题是在其下运行的平台。无论是基于 VMware 的私有云还是像 AWS 这样的公共云，您的平台需要稳定运行一段时间，并且具备以下基础设施基础知识：\n配置：根据需要创建虚拟机，或者直接使用裸机 网络：DNS、负载均衡、VPC/VLAN、防火墙、安全组等 存储：NFS/EFS/EBS/Ceph 等，通过 API 创建 如果这些基础设施建设不到位，那么在尝试部署和运行 kubernetes 群集时，您将会遇到许多问题。\n根据经验，我们通常向客户推荐从像 AWS 这样的公共云提供商开始，然后配备了一些 Hashicorp 工具，如 Terraform 和 Packer，以实现坚实的基础设施。\n2. 您需要打造一支强大的团队 让企业内部实现容器编排的能力很一个很有挑战的事情。\n打造是一个全面的团队，包括一些具有非常强大的 Ops 背景的成员，可以让他们去调试一些底层的东西，还有一些自动化工程师将负责设置和集群管理的日常工作，更多的研究人员将确保 CI/CD 流水线顺利运行以保证开发人员有一个很好的体验。\n下面是构建团队的几个建议：\n找到已经在尝试使用容器的团队，也许他们以前用过 Docker Swarm 或 Rancher。他们可能已经渴望使用 Kubernetes，并且愿意努力实施。 给您的开发和运维团队做关于容器和容器编排方面的培训。 聘请新人才。有时候，您可能会发现，最好的选择是建立一个全新的团队，这样他们就不会被当前流程所淹没，还可以向其他团队展示未来的样子。 3. 依托社区 Kubernetes 能够坐上了容器编排系统的头把交椅的主要原因是社区的支持。\nKubernetes 最初是基于 Google 的 borg，borg 具有非常丰富的功能集，现在已经是一个成熟的框架——这对 kubernetes 来说是非常有利的，但其成功的主要原因是已经形成了积极支持它的社区。\n下面是一些关于如何参与社区的小贴士：\n加入 kubernetes slack channel，现在里面已经有 21,000 人，http://slack.k8s.io 参与一个 SIG（特别兴趣小组），这里面包括从在 AWS 上运行 kubernetes 到管理大数据集群。 参加 meetup 关注#kubernetes 关注那些主流传道者，有个人要特别关注下那就是Kelsey Hightower 走向成功 在拥有了坚如磐石的平台，熟练和多样化的团队，以及与 Kubernetes 社区不断增长的关系后，您将有处理通向成功道路上的遇到的任何问题的资本，克服成长过程中的痛苦。\n原文地址 作者：Marcus Maxwell\n","relpermalink":"/blog/3-things-every-cto-should-know-about-kubernetes/","summary":"使用 Kubernetes 前你需要先了解的问题。","title":"每位 CTO 都该知道的关于 Kubernetes 的三件事"},{"content":"即日起我有了自己的独立域名 jimmysong.io ，网站依然托管在 GitHub 上，原来的网址 https://jimmysong.io 依然可以访问。\n为什么使用 .io 作为后缀呢？因为这是 The First Step to Cloud Native！\n为什么选择在今天？因为今天是 8 月 18 日，日子好记。\nP.S 域名注册于 namecheap ，费用几十美元/年。\nProudly powered by hugo 🎉🎊🎉\n","relpermalink":"/notice/domain-name-jimmysong-io/","summary":"即日起我有了自己的独立域名 jimmysong.io。","title":"即日起更换域名为 jimmysong.io"},{"content":"本文已归档在kubernetes-handbook 中的第 3 章【用户指南】中，一切更新以 kubernetes-handbook 中为准。\n为了详细说明，我特意写了两个示例程序放在 GitHub 中，模拟应用开发流程：\nk8s-app-monitor-test ：生成模拟的监控数据，发送 http 请求，获取 json 返回值 K8s-app-monitor-agent ：获取监控数据并绘图，访问浏览器获取图表 API 文档见k8s-app-monitor-test 中的api.html文件，该文档在 API blueprint 中定义，使用aglio 生成，打开后如图所示：\nAPI 文档 关于服务发现\nK8s-app-monitor-agent服务需要访问k8s-app-monitor-test服务，这就涉及到服务发现的问题，我们在代码中直接写死了要访问的服务的内网 DNS 地址（kubedns 中的地址，即k8s-app-monitor-test.default.svc.cluster.local）。\n我们知道 Kubernetes 在启动 Pod 的时候为容器注入环境变量，这些环境变量在所有的 namespace 中共享（环境变量是不断追加的，新启动的 Pod 中将拥有老的 Pod 中所有的环境变量，而老的 Pod 中的环境变量不变）。但是既然使用这些环境变量就已经可以访问到对应的 service，那么获取应用的地址信息，究竟是使用变量呢？还是直接使用 DNS 解析来发现？\n答案是使用 DNS，详细说明见Kubernetes 中的服务发现与 Docker 容器间的环境变量传递源码探究 。\n打包镜像\n因为我使用 wercker 自动构建，构建完成后自动打包成 docker 镜像并上传到 docker hub 中（需要提前在 docker hub 中创建 repo），如何使用 wercker 做持续构建与发布，并集成 docker hub 插件请参考使用 Wercker 进行持续构建与发布 。\n查看详细构建流程 wercker 生成了如下两个 docker 镜像：\njimmysong/k8s-app-monitor-test:latest jimmysong/k8s-app-monitor-agent:latest 启动服务\n所有的 kubernetes 应用启动所用的 yaml 配置文件都保存在那两个 GitHub 仓库的manifest.yaml文件中。\n分别在两个 GitHub 目录下执行kubectl create -f manifest.yaml即可启动服务。\n外部访问\n服务启动后需要更新 ingress 配置，在ingress.yaml 文件中增加以下几行：\n- host: k8s-app-monitor-agent.jimmysong.io http: paths: - path: / backend: serviceName: k8s-app-monitor-agent servicePort: 8080 保存后，然后执行kubectl replace -f ingress.yaml即可刷新 ingress。\n修改本机的/etc/hosts文件，在其中加入以下一行：\n172.20.0.119 k8s-app-monitor-agent.jimmysong.io 当然你也可以加入到 DNS 中，为了简单起见我使用 hosts。\n详见边缘节点配置 。\n在浏览器中访问 http://k8s-app-monitor-agent.jimmysong.io 图表 刷新页面将获得新的图表。\n参考 使用 Wercker 进行持续构建与发布 示例的项目代码服务器端 示例项目代码前端 kubernetes-handbok 边缘节点配置 ","relpermalink":"/blog/deploy-applications-in-kubernetes/","summary":"本文以在 Kubernetes 中部署两个应用来说明。","title":"适用于 Kubernetes 的应用开发与部署流程详解"},{"content":"下面是这本书的基本信息。\n书名：Kubernetes Management Design Patterns: With Docker, CoreOS Linux, and Other Platforms Amazon 购买链接：链接 作者：Deepak Vohra 发行日期：2017 年 1 月 20 日 出版社：Apress 页数：399 简介 Kubernetes 引领容器集群管理进入一个全新的阶段；学习如何在 CoreOS 上配置和管理 kubernetes 集群；使用适当的管理模式，如 ConfigMaps、Autoscaling、弹性资源使用和高可用性配置。讨论了 kubernetes 的一些其他特性，如日志、调度、滚动升级、volume、服务类型和跨多个云供应商 zone。\nKubernetes 中的最小模块化单位是 Pod，它是拥有共同的文件系统和网络的系列容器的集合。Pod 的抽象层可以对容器使用设计模式，就像面向对象设计模式一样。容器能够提供与软件对象（如模块化或包装，抽象和重用）相同的优势。\n在大多数章节中使用的都是 CoreOS Linux，其他讨论的平台有 CentOS，OpenShift，Debian 8（jessie），AWS 和 Debian 7 for Google Container Engine。\n使用 CoreOS 主要是因为 Docker 已经在 CoreOS 上开箱即用。CoreOS：\n支持大多数云提供商（包括 Amazon AWS EC2 和 Google Cloud Platform）和虚拟化平台（如 VMWare 和 VirtualBox） 提供 Cloud-Config，用于声明式配置 OS，如网络配置（flannel），存储（etcd）和用户帐户 为容器化应用提供生产级基础架构，包括自动化，安全性和可扩展性 引领容器行业标准，并建立了应用程序标准 提供最先进的容器仓库，Quay Docker 于 2013 年 3 月开源，现已称为最流行的容器平台。kubernetes 于 2014 年 6 月开源，现在已经成为最流行的容器集群管理平台。第一个稳定版 CoreOS Linux 于 2014 年 7 月发行，现已成为最流行的容器操作系统之一。\n你将学到什么 使用 docker 和 kubernetes 在 AWS 和 CoreOS 上创建 kubernetes 集群 应用集群管理设计模式 使用多个云供应商 zone 使用 Ansible 管理 kubernetes 基于 kubernetes 的 PAAS 平台 OpenShift 创建高可用网站 构建高可用用的 kubernetes master 集群 使用 volume、configmap、serivce、autoscaling 和 rolling update 管理计算资源 配置日志和调度 谁适合读这本书 Linux 管理员、CoreOS 管理员、应用程序开发者、容器即服务（CAAS）开发者。阅读这本书需要 Linux 和 Docker 的前置知识。介绍 Kubernetes 的知识，例如创建集群，创建 Pod，创建 service 以及创建和缩放 replication controller。还需要一些关于使用 Amazon Web Services（AWS）EC2，CloudFormation 和 VPC 的必备知识。\n关于作者 Deepak Vohra is an Oracle Certified Associate and a Sun Certified Java Programmer. Deepak has published in Oracle Magazine, OTN, IBM developerWorks, ONJava, DevSource, WebLogic Developer’s Journal, XML Journal, Java Developer’s Journal, FTPOnline, and devx.\n目录 第一部分：平台 第 1 章：Kuberentes on AWS 第 2 章：kubernetes on CoreOS on AWS 第 3 章：kubernetes on Google Cloud Platform 第二部分：管理和配置 第 4 章：使用多个可用区 第 5 章：使用 Tectonic Console 第 6 章：使用 volume 第 7 章：使用 service 第 8 章：使用 Rolling updte 第 9 章：在 node 上调度 pod 第 10 章：配置计算资源 第 11 章：使用 ConfigMap 第 12 章：使用资源配额 第 13 章：使用 Autoscaling 第 14 章：配置 logging 第三部分：高可用 第 15 章：在 OpenShift 中使用 HA master 第 16 章：开发高可用网站 个人评价 本书更像是一本参考手册，对于想在公有云中（如 AWS、Google Cloud Platform）中尝试 Kubernetes 的人会有所帮助，而对于想使用 kubernetes 进行自己的私有云建设，或想了解 kubernetes 的实现原理和技术细节的人来说，就不适合了。对我来说，本书中有个别几个章节可以参考，如高可用，但还是使用 OpenShift 实现的。总之，如果你使用 AWS 这样的公有云，对操作系统没有特别要求，可以接受 CoreOS 的话，那么可以看看这本书。本来本书会对 kubernetes 中的各种应用模式能够有个详解，但是从书中我并没有找到。\n本书有两个优点，一个是每个章节都给出了问题的起因和 kubernetes 的解决方案，二是几乎所有的命令和操作都附有截图，说明很详细。\n","relpermalink":"/blog/book-kubernetes-management-design-patterns/","summary":"本书有两个优点，一个是每个章节都给出了问题的起因和 kubernetes 的解决方案，二是几乎所有的命令和操作都附有截图，说明很详细。","title":"记一本关于 kubernetes management design patterns 的书"},{"content":"前言 今天创建了两个 kubernetes 示例应用：\nk8s-app-monitor-test ：启动 server 用来产生 metrics k8s-app-monitor-agent ：获取 metrics 并绘图，显示在 web 上 注：相关的 kubernetes 应用manifest.yaml文件分别见以上两个应用的 GitHub。\n当我查看 Pod 中的环境变量信息时，例如 kubernetes 中的 service k8s-app-monitor-test注入的环境变量时，包括了以下变量：\nK8S_APP_MONITOR_TEST_PORT_3000_TCP_ADDR=10.254.56.68 K8S_APP_MONITOR_TEST_PORT=tcp://10.254.56.68:3000 K8S_APP_MONITOR_TEST_PORT_3000_TCP_PROTO=tcp K8S_APP_MONITOR_TEST_SERVICE_PORT_HTTP=3000 K8S_APP_MONITOR_TEST_PORT_3000_TCP_PORT=3000 K8S_APP_MONITOR_TEST_PORT_3000_TCP=tcp://10.254.56.68:3000 K8S_APP_MONITOR_TEST_SERVICE_HOST=10.254.56.68 K8S_APP_MONITOR_TEST_SERVICE_PORT=3000 我们知道 Kubernetes 在启动 Pod 的时候为容器注入环境变量，这些环境变量将在该 Pod 所在的 namespace 中共享。但是既然使用这些环境变量就已经可以访问到对应的 service，那么获取应用的地址信息，究竟是使用变量呢？还是直接使用 DNS 解析来发现？下面我们从代码中来寻求答案。\n如果不想看下面的文字，可以直接看图。\nkubernetes 中传递 ENV 的探索过程 探索 docker 的docker/engine-api/types/container/config.go中的Config结构体中有对环境变量的定义：\n// Config contains the configuration data about a container. // It should hold only portable information about the container. // Here, \u0026#34;portable\u0026#34; means \u0026#34;independent from the host we are running on\u0026#34;. // Non-portable information *should* appear in HostConfig. // All fields added to this struct must be marked `omitempty` to keep getting // predictable hashes from the old `v1Compatibility` configuration. type Config struct { Hostname string // Hostname Domainname string // Domainname User string // User that will run the command(s) inside the container ... Env []string // List of environment variable to set in the container Cmd strslice.StrSlice // Command to run when starting the container ... } Kubernetes 中在pkg/kubelet/container/runtime.go中的RunContainerOptions结构体中定义：\n// RunContainerOptions specify the options which are necessary for running containers type RunContainerOptions struct { // The environment variables list. Envs []EnvVar // The mounts for the containers. Mounts []Mount // The host devices mapped into the containers. ... } Kubelet 向容器中注入环境变量的配置是在下面的方法中定义：\npkg/kubelet/kuberuntime/kuberuntime_container.go // generateContainerConfig generates container config for kubelet runtime v1. func (m *kubeGenericRuntimeManager) generateContainerConfig(container *v1.Container, pod *v1.Pod, restartCount int, podIP, imageRef string) (*runtimeapi.ContainerConfig, error) { opts, _, err := m.runtimeHelper.GenerateRunContainerOptions(pod, container, podIP) ... // set environment variables envs := make([]*runtimeapi.KeyValue, len(opts.Envs)) for idx := range opts.Envs { e := opts.Envs[idx] envs[idx] = \u0026amp;runtimeapi.KeyValue{ Key: e.Name, Value: e.Value, } } config.Envs = envs return config, nil } kubelet 的pkg/kubelet/kubelet_pods.go的如下方法中生成了RunContainerOptions：\n// GenerateRunContainerOptions generates the RunContainerOptions, which can be used by // the container runtime to set parameters for launching a container. func (kl *Kubelet) GenerateRunContainerOptions(pod *v1.Pod, container *v1.Container, podIP string) (*kubecontainer.RunContainerOptions, bool, error) { ... opts := \u0026amp;kubecontainer.RunContainerOptions{CgroupParent: cgroupParent} ... opts.Envs, err = kl.makeEnvironmentVariables(pod, container, podIP) return opts, useClusterFirstPolicy, nil } 我们再看下makeEnvironmentVariables(pod, container, podIP)方法中又做了什么（该方法也在pkg/kubelet/kubelet_pods.go文件中）。\n// Make the environment variables for a pod in the given namespace. func (kl *Kubelet) makeEnvironmentVariables(pod *v1.Pod, container *v1.Container, podIP string) ([]kubecontainer.EnvVar, error) { var result []kubecontainer.EnvVar // Note: These are added to the docker Config, but are not included in the checksum computed // by dockertools.BuildDockerName(...). That way, we can still determine whether an // v1.Container is already running by its hash. (We don\u0026#39;t want to restart a container just // because some service changed.) // // Note that there is a race between Kubelet seeing the pod and kubelet seeing the service. // To avoid this users can: (1) wait between starting a service and starting; or (2) detect // missing service env var and exit and be restarted; or (3) use DNS instead of env vars // and keep trying to resolve the DNS name of the service (recommended). ... } 该代码段比较长，kubernetes 究竟如何将环境变量注入到 docker 容器中的奥秘就在这里，按图索骥到了这里，从代码注释中已经可以得出结论，使用 DNS 解析而不要使用环境变量来做服务发现，究竟为何这样做，改天我们再详细解读。\n","relpermalink":"/blog/exploring-kubernetes-env-with-docker/","summary":"基于实际应用研究。","title":"Kubernetes 中的服务发现与 docker 容器间的环境变量传递源码探究"},{"content":"这是一份记录关于 Cloud Native 的软件、工具、架构以及参考资料的列表，是我在 GitHub 上开的一个项目 awesome-cloud-native ，同时也可以通过 Web 页面浏览 。\n初步划分成以下这些领域：\nAwesome Cloud Native\nAI API gateway Big Data Container engine CI-CD Database Data Science Fault tolerant Logging Message broker Monitoring Networking Orchestration and scheduler Portability Proxy and load balancer RPC Security and audit Service broker Service mesh Service registry and discovery Serverless Storage Tracing Tools Tutorial 今后会不断更新和完善该列表，不仅是为了本人平时的研究记录，也作为 Cloud Native 业内人士的参考。\n","relpermalink":"/notice/awesome-cloud-native/","summary":"这是一份记录关于 Cloud Native 的软件、工具、架构以及参考资料的列表，是我在 GitHub 上开的一个项目。","title":"Awesome Cloud Native 列表发布"},{"content":"本书是 Migrating to Cloud Native Application Architectures 的中文版，本书英文版发布于 2015 年 2 月，中文版由 Jimmy Song 翻译，发布于 2017 年 7 月。\n本书 GitHub 托管地址：https://github.com/rootsongjc/migrating-to-cloud-native-application-architectures Gitbook 阅读地址：https://jimmysong.io/migrating-to-cloud-native-application-architectures 本书中讨论的应用架构包括：\n十二因素应用程序：云原生应用架构模式的集合 微服务：独立部署的服务，每个服务只做一件事情 自助服务的敏捷基础设施：快速，可重复和一致地提供应用环境和后台服务的平台 基于 API 的协作：发布和版本化的 API，允许在云原生应用架构中的服务之间进行交互 抗压性：根据压力变强的系统 ","relpermalink":"/notice/changes-needed-to-cloud-native-archtecture/","summary":"本书译自 Matt Stine 2015 年 2 月发表的电子书。","title":"《迁移到云原生应用架构》中文译本发布"},{"content":"数据落盘问题的由来 这本质上是数据持久化问题，对于有些应用依赖持久化数据，比如应用自身产生的日志需要持久化存储的情况，需要保证容器里的数据不丢失，在 Pod 挂掉后，其他应用依然可以访问到这些数据，因此我们需要将数据持久化存储起来。\n数据落盘问题解决方案 下面以一个应用的日志收集为例，该日志需要持久化收集到 ElasticSearch 集群中，如果不考虑数据丢失的情形，可以直接使用kubernetes-handbook 中【应用日志收集】一节中的方法，但考虑到 Pod 挂掉时 logstash（或 filebeat）并没有收集完该 pod 内日志的情形，我们想到了如下这种解决方案，示意图如下：\n日志持久化收集解决方案示意图 首先需要给数据落盘的应用划分 node，即这些应用只调用到若干台主机上 给这若干台主机增加 label 使用deamonset方式在这若干台主机上启动 logstash 的 Pod（使用 nodeSelector 来限定在这几台主机上，我们在边缘节点启动的treafik也是这种模式） 将应用的数据通过 volume 挂载到宿主机上 Logstash（或者 filebeat）收集宿主机上的数据，数据持久化不会丢失 Side-effect 首先 kubernetes 本身就提供了数据持久化的解决方案 statefulset，不过需要用到公有云的存储货其他分布式存储，这一点在我们的私有云环境里被否定了。 需要管理主机的 label，增加运维复杂度，但是具体问题具体对待 必须保证应用启动顺序，需要先启动 logstash 为主机打 label 使用 nodeSelector 的方式限制了资源调度的范围 本文已归档到kubernetes-handbook 中的【最佳实践—运维管理】章节中，一切内容以 kubernetes-handbook 为准。\n","relpermalink":"/blog/data-persistence-problem/","summary":"以日志收集问题为例来讨论和解决方案探究。","title":"Kubernetes 中的数据持久化问题"},{"content":"在进行微服务开发的过程中，为了保证最终开发的系统跟最初的设计保持一致，约定 RESTful 接口之间的调用方法，我们需要将 API 设计文档化，因此我们引入了 API Blueprint。\nAPI Blueprint 是什么 API Blueprint 用来编写 API 文档的一种标记语言，类似于 Markdown，因为是纯文本的，所以方便共享编辑，具体的语法规则可以在 API Blueprint documentation 查看，配合一些开源的工具可以把接口文档渲染成 html 再搭配一个静态服务器，方便于分享。\n另外，配合一些工具，可以直接生成一个 mock data 数据，这样只要和后端的同学约定好接口格式，那么前端再开发的时候可以使用 mock data 数据来做测试，等到后端写好接口之后再做联调就可以了。\n我们以Cloud Native Go 书中的gogo-service 示例里的apiary.apib文件为例。\n该文件实际上是一个 Markdown 格式的文件，Github 中原生支持该文件，使用Typora 打开后是这样子。\napiary.apib 文件 在 Visual Studio Code 中有个 API Element extension 对于 API Blueprint 文件的支持也比较好。\n生成静态页面和进行冒烟测试 我们分别使用开源的aglio 和drakov 来生成静态页面和进行冒烟测试。\naglio 是一个可以根据 api-blueprint 的文档生成静态 HTML 页面的工具。\n其生成的工具不是简单的 markdown 到 html 的转换，而是可以生成类似 rdoc 这样的拥有特定格式风格的查询文档。\n在本地安装有 node 环境的情况下，使用下面的命令安装和使用 aglio。\n$ npm install -g aglio $ aglio -i apiary.apib -o api.html 打开 api.html 文件后，如图：\n使用 aglio 生成的 API 文档 安装和使用 drakov。\n$ npm install -g drakov $ drakov -f apiary.apib -p 3000 [INFO] No configuration files found [INFO] Loading configuration from CLI DRAKOV STARTED [LOG] Setup Route: GET /matches List All Matches [LOG] Setup Route: POST /matches Start a New Match [LOG] Setup Route: GET /matches/:match_id Get Match Details [LOG] Setup Route: GET /matches/:match_id Get Current Liberties for Match [LOG] Setup Route: GET /matches/:match_id Get Current Chains for Match [LOG] Setup Route: GET /matches/:match_id/moves Get a Sequential List of All Moves Performed in a Match [LOG] Setup Route: POST /matches/:match_id/moves Make a Move Drakov 1.0.4 Listening on port 3000 通过http://localhost:3000就可以对该应用进行冒烟测试了。\n例如查询有哪些 match：\n$ curl http://localhost:3000/matches [ { \u0026#34;id\u0026#34; : \u0026#34;5a003b78-409e-4452-b456-a6f0dcee05bd\u0026#34;, \u0026#34;started_at\u0026#34;: 13231239123391, \u0026#34;turn\u0026#34; : 27, \u0026#34;gridsize\u0026#34; : 19, \u0026#34;playerWhite\u0026#34; : \u0026#34;bob\u0026#34;, \u0026#34;playerBlack\u0026#34; : \u0026#34;alfred\u0026#34; } ] 另外通过Apiary 这个网站，我们可以直接以上的所有功能，还可以同时在页面上进行 mock test，生成多种语言的 code example。如图：\nApiary 页面 ","relpermalink":"/blog/creating-api-document-with-api-blueprint/","summary":"在进行微服务开发的过程中的 API 设计文档化工具。","title":"使用 API blueprint 创建 API 文档"},{"content":"本文介绍了 wercker 和它的基本用法，并用我 GitHub 上的magpie 应用作为示例，讲解如何给 GitHub 项目增加 wercker 构建流程，并将生成的镜像自动上传到 Docker Hub 上。\n注：本文参考了Cloud Native Go 书中的”持续交付“章节。\nCI 工具 开源项目的构建离不开 CI 工具，你可能经常会在很多 GitHub 的开源项目首页上看到这样的东西：\nwercker status badge 这些图标都是 CI 工具提供的，可以直观的看到当前的构建状态，例如 wercker 中可以在Application-magpie-options中看到：\nwercker status badge 设置 将文本框中的代码复制到你的项目的README文件中，就可以在项目主页上看到这样的标志了。\n现在市面上有很多流行的CI/CD工具和DevOps工具有很多，这些工具提高了软件开发的效率，增加了开发人员的幸福感。这些工具有：\n适用于 GitHub 上的开源项目，可以直接使用 GitHub 账户登陆，对于公开项目可以直接使用：Travis-ci 、CircleCI 、Wercker 。从目前 GitHub 上开源项目的使用情况来看，Travis-ci 的使用率更高一些。\n适用于企业级的：Jenkins 不仅包括CI/CD功能的DevOps平台：JFrog 、Spinnaker 、Fabric8 Wercker 简介 Wercker 是一家为现代云服务提供容器化应用及微服务的快速开发、部署工具的初创企业，成立于 2012 年，总部位于荷兰阿姆斯特丹。其以容器为中心的平台可以对微服务和应用的开发进行自动化。开发者通过利用其命令行工具能够生成容器到桌面，然后自动生成应用并部署到各种云平台上面。其支持的平台包括 Heroku、AWS 以及 Rackspace 等。\nWercker 于 2016 年获得 450 万美元 A 轮融资，此轮融资由 Inkef Capital 领投，Notion Capital 跟投，融资所得将用于商业版产品的开发。此轮融资过后其总融资额为 750 万美元。\nWercker 于 2017 年 4 月被 Oracle 甲骨文于收购。\n为什么使用 Wercker 所有的 CI 工具都可以在市面上获取，但为何要建议使用 Wercker 呢？依据云之道的准则评估了所有工具，发现 Wercker 正是我们需要的。\n首先，无须在工作站中安装 Wecker，仅安装一个命令行客户端即可，构建过程全部在云端进行。\n其次，不用通过信用卡就可使用 Wercker。当我们迫切希望简化流程时，这是一件令人赞叹的事。付款承诺这一条件大大增加了开发者的压力，这通常是不必要的。\n最后，Wercker 使用起来非常简单。它非常容易配置，不需要经过高级培训或拥有持续集成的博士学位，也不用制定专门的流程。\n通过 Wercker 搭建 CI 环境只需经过三个基本步骤。\n在 Wercker 网站中创建一个应用程序。 将 wercker.yml 添加到应用程序的代码库中。 选择打包和部署构建的位置。 如何使用 可以使用 GitHub 帐号直接登录Wercker ，整个创建应用 CI 的流程一共 3 步。\n一旦拥有了账户，那么只需简单地点击位于顶部的应用程序菜单，然后选择创建选项即可。如果系统提示是否要创建组织或应用程序，请选择应用程序。Wercker 组织允许多个 Wercker 用户之间进行协作，而无须提供信用卡。下图为设置新应用程序的向导页面。\n向导页面 选择了 GitHub 中的 repo 之后，第二步配置访问权限，最后一步 Wercker 会尝试生成一个 wercker.yml 文件（后面会讨论）。不过至少对于 Go 应用程序来说，这个配置很少会满足要求，所以我们总是需要创建自己的 Wercker 配置文件。\n安装 Wercker 命令行程序 这一步是可选的，如果你希望在本地进行 wercker 构建的话才需要在本地安装命令行程序。本地构建和云端构建都依赖于 Docker 的使用。基本上，代码会被置于所选择的 docker 镜像中（在 wercker.yml 中定义），然后再选择执行的内容和方法。\n要在本地运行 Wercker 构建，需要使用 Wercker CLI。有关如何安装和测试 CLI 的内容，请查看 http://devcenter.wercker.com/docs/cli 。Wercker 更新文档的频率要比本书更高，所以请在本书中做个标记，然后根据 Wercker 网站的文档安装 Wercker CLI。\n如果已经正确安装了 CLI，应该可以查询到 CLI 的版本，代码如下所示。\nVersion: 1.0.882 Compiled at: 2017-06-02 06:49:39 +0800 CST Git commit: da8bc056ed99e27b4b7a1b608078ddaf025a9dc4 No new version available 本地构建只要在项目的根目录下输入wercker build命令即可，wercker 会自动下载依赖的 docker 镜像在本地运行所有构建流程。\n创建 Wercker 配置文件 wercker.yml Wercker 配置文件是一个 YAML 文件，该文件必须在 GitHub repo 的最顶层目录，该文件主要包含三个部分，对应可用的三个主要管道。\nDev：定义了开发管道的步骤列表。与所有管道一样，可以选定一个box用于构建，也可以全局指定一个 box 应用于所有管道。box 可以是 Wercker 内置的预制 Docker 镜像之一，也可以是 Docker Hub 托管的任何 Docker 镜像。 Build：定义了在 Wercker 构建期间要执行的步骤和脚本的列表。与许多其他服务（如 Jenkins 和 TeamCity）不同，构建步骤位于代码库的配置文件中，而不是隐藏在服务配置里。 Deploy：在这里可以定义构建的部署方式和位置。 Wercker 中还有工作流的概念，通过使用分支、条件构建、多个部署目标和其他高级功能扩展了管道的功能，这些高级功能读着可以自己在 wercker 的网站中探索。\n示例 我们以我用 Go 语言开发的管理 yarn on docker 集群的命令行工具magpie 为例，讲解如何使用 wercker 自动构建，并产生 docker 镜像发布到 Docker Hub 中。\n下面是 magpie 这个项目中使用的wercker.yml文件。\nbox: golang build: steps: # Sets the go workspace and places you package # at the right place in the workspace tree - setup-go-workspace # Gets the dependencies - script: name: go get code: | go get github.com/rootsongjc/magpie # Build the project - script: name: go build code: | go build -o magpie main.go # Test the project - script: name: go test code: | go test ./... - script: name: copy files to wercker output code: | cp -R ./ ${WERCKER_OUTPUT_DIR} deploy: steps: - internal/docker-push: username: $USERNAME password: $PASSWORD cmd: /pipeline/source/magpie tag: latest repository: jimmysong/magpie 此文件包含两个管道：build 和 deploy。在开发流程中，我们使用 Wercker 和 Docker 创建一个干净的 Docker 镜像，然后将它 push 到 Docker Hub 中。Wercker 包含一个叫做Internal/docker-push的 deploy plugin，可以将构建好的 docker 镜像 push 到镜像仓库中，默认是 Docker Hub，也可以配置成私有镜像仓库。\nbox 键的值是 golang。这意味着我们使用的是一个基础的 Docker 镜像，它已经安装了 Go 环境。这一点至关重要，因为执行 Wercker 构建的基准 Docker 镜像需要包含应用程序所需的构建工具。\n这部分存在一些难以理解的概念。当使用 Wercker 进行构建时，其实并没有使用本地工作站的资源（即使在技术层面上，构建也是在本地执行的），相反，使用的是 Docker 镜像中的可用资源。因此，如果要使用 Wercker 编译 Go 应用程序，需要首先运行包含 Go 的 Docker 镜像。如果想要构建唯一的工件，无论它是在本地还是在 Wercker 的云端运行，使用 Docker 镜像都是完全合理的。\n本次构建中运行的第一个脚本是 go get。这一步可以 go get 可能需要的、但不包含在基础镜像中的任何东西。无论为脚本设置什么名称，构建输出都会有所显示，如下图所示。\n构建流程输出 在 build 管道中，接下来的两个脚本执行的构建和测试流程，最后一个脚本是将构建后的文件拷贝到 wercker 的输出目录中，我们将使用该目录构建 docker 镜像。\n我们注意到 deploy 中有两个变量：$USERNAME、$PASSWORD，这是我们自定义的变量，当你不希望将隐私内容直接写在代码中的时候，可以在 wercker 中自定义变量，变量可以只作用于单个 pipeline，也可以是所有 pipeline 共享的。\n在 wercker 中设置环境变量 可以将变量设置成Protected模式，这样只有设置者本人才知道该变量的值是什么，其他人即使有共享访问权限，也看不到该变量的值，但可以重新设置来覆盖原值。\nDeploy 管道中配置的 docker 镜像的 repo、tag 和 cmd 命令，其他容器配置都在代码顶层目录的Dockerfile中定义。当整个构建流程完成后，就可以在 docker 镜像仓库中看到刚构建的镜像jimmysong/magpie:latest了。\n使用 wercker 自动构建的 docker 镜像 magpie 总结 当然以上只是一个很简单的示例，还有很多可以优化的流程，比如我们在示例使用latest作为 docker 镜像的 tag，wercker 本身提供了很多内置和构建时环境变量 ，我们可以在wercker.yml文件里获取这些变量作为命令中的值。\n当比于其他 CI 工具，wercker 配置简单，更易于使用，同时在 wercker 的 registry 中还可以看到很多别人构建的 pipline 可供参考，还有十分友好的workflows 可用于编排构建流程和依赖。\n当然 CI 工具的功能不止这些，利用它可以实现很多自动化流程，节约我们的时间，解放生产力，更多玩法就要大家自己去探索了。\n参考 容器化应用开发部署平台 Wercker 获 450 万美元 A 轮融资 甲骨文收购创业公司 Wercker 为开发人员自动化代码测试部署 Wercker docs Wercker workflow magpie ","relpermalink":"/blog/continuous-integration-with-wercker/","summary":"本文介绍了 wercker 和它的基本用法。","title":"使用 Wercker 进行持续构建与发布"},{"content":"前言 本文档旨在汇总和强调用户指南、快速开始文档和示例中的最佳实践。该文档会很很活跃并持续更新中。如果你觉得很有用的最佳实践但是本文档中没有包含，欢迎给我们提 Pull Request。\n本文已上传到kubernetes-handbook 中的第四章最佳实践章节，本文仅作归档，更新以kubernetes-handbook 为准。\n通用配置建议 定义配置文件的时候，指定最新的稳定 API 版本（目前是 V1）。 在配置文件 push 到集群之前应该保存在版本控制系统中。这样当需要的时候能够快速回滚，必要的时候也可以快速的创建集群。 使用 YAML 格式而不是 JSON 格式的配置文件。在大多数场景下它们都可以作为数据交换格式，但是 YAML 格式比起 JSON 更易读和配置。 尽量将相关的对象放在同一个配置文件里。这样比分成多个文件更容易管理。参考guestbook-all-in-one.yaml 文件中的配置（注意，尽管你可以在使用kubectl命令时指定配置文件目录，你也可以在配置文件目录下执行kubectl create——查看下面的详细信息）。 为了简化和最小化配置，也为了防止错误发生，不要指定不必要的默认配置。例如，省略掉ReplicationController的 selector 和 label，如果你希望它们跟podTemplate中的 label 一样的话，因为那些配置默认是podTemplate的 label 产生的。更多信息请查看 guestbook app 的 yaml 文件和 examples 。 将资源对象的描述放在一个 annotation 中可以更好的内省。 裸奔的 Pods vs Replication Controllers 和 Jobs 如果有其他方式替代“裸奔的 pod”（如没有绑定到replication controller 上的 pod），那么就使用其他选择。在 node 节点出现故障时，裸奔的 pod 不会被重新调度。Replication Controller 总是会重新创建 pod，除了明确指定了restartPolicy: Never 的场景。Job 也许是比较合适的选择。 Services 通常最好在创建相关的replication controllers 之前先创建service （没有这个必要吧？）你也可以在创建 Replication Controller 的时候不指定 replica 数量（默认是 1），创建 service 后，在通过 Replication Controller 来扩容。这样可以在扩容很多个 replica 之前先确认 pod 是正常的。 除非时分必要的情况下（如运行一个 node daemon），不要使用hostPort（用来指定暴露在主机上的端口号）。当你给 Pod 绑定了一个hostPort，该 pod 可被调度到的主机的受限了，因为端口冲突。如果是为了调试目的来通过端口访问的话，你可以使用 kubectl proxy and apiserver proxy 或者 kubectl port-forward 。你可使用 Service 来对外暴露服务。如果你确实需要将 pod 的端口暴露到主机上，考虑使用 NodePort service。 跟hostPort一样的原因，避免使用 hostNetwork。 如果你不需要 kube-proxy 的负载均衡的话，可以考虑使用使用headless services 。 使用 Label 定义 labels 来指定应用或 Deployment 的 semantic attributes 。例如，不是将 label 附加到一组 pod 来显式表示某些服务（例如，service:myservice），或者显式地表示管理 pod 的 replication controller（例如，controller:mycontroller），附加 label 应该是标示语义属性的标签，例如 {app:myapp,tier:frontend,phase:test,deployment:v3} 这将允许您选择适合上下文的对象组——例如，所有的”tier:frontend“pod 的服务或 app 是“myapp”的所有“测试”阶段组件。有关此方法的示例，请参阅guestbook 应用程序。可以通过简单地从其 service 的选择器中省略特定于发行版本的标签，而不是更新服务的选择器来完全匹配 replication controller 的选择器，来实现跨越多个部署的服务，例如滚动更新。\n为了滚动升级的方便，在 Replication Controller 的名字中包含版本信息，例如作为名字的后缀。设置一个version标签页是很有用的。滚动更新创建一个新的 controller 而不是修改现有的 controller。因此，version 含混不清的 controller 名字就可能带来问题。查看Rolling Update Replication Controller 文档获取更多关于滚动升级命令的信息。\n注意 Deployment 对象不需要再管理 replication controller 的版本名。Deployment 中描述了对象的期望状态，如果对 spec 的更改被应用了话，Deployment controller 会以控制的速率来更改实际状态到期望状态。（Deployment 目前是 extensions API Group 的一部分）。\n利用 label 做调试。因为 Kubernetes replication controller 和 service 使用 label 来匹配 pods，这允许你通过移除 pod 中的 label 的方式将其从一个 controller 或者 service 中移除，原来的 controller 会创建一个新的 pod 来取代移除的 pod。这是一个很有用的方式，帮你在一个隔离的环境中调试之前的“活着的”pod。查看 kubectl label 命令。\n容器镜像 默认容器镜像拉取策略 是 IfNotPresent, 当本地已存在该镜像的时候 Kubelet 不会再从镜像仓库拉取。如果你希望总是从镜像仓库中拉取镜像的话，在 yaml 文件中指定镜像拉取策略为Always（ imagePullPolicy: Always）或者指定镜像的 tag 为 :latest 。\n如果你没有将镜像标签指定为:latest，例如指定为myimage:v1，当该标签的镜像进行了更新，kubelet 也不会拉取该镜像。你可以在每次镜像更新后都生成一个新的 tag（例如myimage:v2），在配置文件中明确指定该版本。\n注意： 在生产环境下部署容器应该尽量避免使用:latest标签，因为这样很难追溯到底运行的是哪个版本的容器和回滚。\n使用 kubectl 尽量使用 kubectl create -f \u0026lt;directory\u0026gt; 。kubeclt 会自动查找该目录下的所有后缀名为.yaml、.yml和.json文件并将它们传递给create命令。 使用 kubectl delete 而不是 stop. Delete 是 stop的超集，stop 已经被弃用。 使用 kubectl bulk 操作（通过文件或者 label）来 get 和 delete。查看label selectors 和 using labels effectively 。 使用 kubectl run 和 expose 命令快速创建直有耽搁容器的 Deployment。查看 quick start guide 中的示例。 参考 Configuration Best Practices ","relpermalink":"/blog/configuration-best-practice/","summary":"本文档旨在汇总和强调用户指南、快速开始文档和示例中的最佳实践。","title":"Kubernetes 配置最佳实践"},{"content":"前言 本示例来自 GitHub - distributed-load-testing-using-kubernetes 。\n该教程描述如何在Kubernetes 中进行分布式负载均衡测试，包括一个 web 应用、docker 镜像和 Kubernetes controllers/services。更多资料请查看Distributed Load Testing Using Kubernetes 。\n注意：该测试是在我自己本地搭建的 kubernetes 集群上测试的，不需要使用 Google Cloud Platform。\n准备 不需要 GCE 及其他组件，你只需要有一个 kubernetes 集群即可。\n部署 Web 应用 sample-webapp 目录下包含一个简单的 web 测试应用。我们将其构建为 docker 镜像，在 kubernetes 中运行。你可以自己构建，也可以直接用这个我构建好的镜像index.tenxcloud.com/jimmy/k8s-sample-webapp:latest。\n在 kubernetes 上部署 sample-webapp。\n$ cd kubernetes-config $ kubectl create -f sample-webapp-controller.yaml $ kubectl create -f kubectl create -f sample-webapp-service.yaml 部署 Locust 的 Controller 和 Service locust-master和locust-work使用同样的 docker 镜像，修改 cotnroller 中spec.template.spec.containers.env字段中的 value 为你sample-webapp service 的名字。\n- name: TARGET_HOST value: http://sample-webapp:8000 创建 Controller Docker 镜像（可选） locust-master和locust-work controller 使用的都是locust-tasks docker 镜像。你可以直接下载，也可以自己编译。自己编译大概要花几分钟时间，镜像大小为 820M。\n$ docker build -t index.tenxcloud.com/jimmy/locust-tasks:latest . $ docker push index.tenxcloud.com/jimmy/locust-tasks:latest 注意：我使用的是时速云的镜像仓库。\n每个 controller 的 yaml 的spec.template.spec.containers.image 字段指定的是我的镜像：\nimage: index.tenxcloud.com/jimmy/locust-tasks:latest 部署 locust-master $ kubectl create -f locust-master-controller.yaml $ kubectl create -f locust-master-service.yaml 部署 locust-worker Now deploy locust-worker-controller:\n$ kubectl create -f locust-worker-controller.yaml 你可以很轻易的给 work 扩容，通过命令行方式：\n$ kubectl scale --replicas=20 replicationcontrollers locust-worker 当然你也可以通过 WebUI：Dashboard - Workloads - Replication Controllers - ServiceName - Scale 来扩容。\nDashboard 配置 Traefik 参考kubernetes 的 traefik ingress 安装 ，在ingress.yaml中加入如下配置：\n- host: traefik.locust.io http: paths: - path: / backend: serviceName: locust-master servicePort: 8089 然后执行kubectl replace -f ingress.yaml即可更新 traefik。\n通过 Traefik 的 dashboard 就可以看到刚增加的traefik.locust.io节点。\nTraefik dashboard 执行测试 打开http://traefik.locust.io页面，点击Edit输入伪造的用户数和用户每秒发送的请求个数，点击Start Swarming就可以开始测试了。\n启动 locust 在测试过程中调整sample-webapp的 pod 个数（默认设置了 1 个 pod），观察 pod 的负载变化情况。\n示例 Web 应用 从一段时间的观察中可以看到负载被平均分配给了 3 个 pod。\n在 locust 的页面中可以实时观察也可以下载测试结果。\nLocust dashboard 参考 Distributed Load Testing Using Kubernetes 运用 Kubernetes 进行分布式负载测试 ","relpermalink":"/blog/distributed-load-testing-using-kubernetes/","summary":"该教程描述如何在 Kubernetes 中进行分布式负载均衡测试。","title":"使用 Kubernetes 进行分布式负载测试"},{"content":"Cluster IP 即 Service 的 IP，通常在集群内部使用 Service Name 来访问服务，用户不需要知道该 IP 地址，kubedns 会自动根据 service name 解析到服务的 IP 地址，将流量分发给 Pod。\nService Name 才是对外暴露服务的关键。\n在 kubeapi 的配置中指定该地址范围。\n默认配置\n--service-cluster-ip-range=10.254.0.0/16 --service-node-port-range=30000-32767 Pod IP 通过配置 flannel 的network和subnet来实现。\n默认配置\nFLANNEL_NETWORK=172.30.0.0/16 FLANNEL_SUBNET=172.30.46.1/24 Pod 的 IP 地址不固定，当 pod 重启时 IP 地址会变化。\n该 IP 地址也是用户无需关心的。\n但是 Flannel 会在本地生成相应 IP 段的虚拟网卡，为了防止和集群中的其他 IP 地址冲突，需要规划 IP 段。\n主机/Node IP 物理机的 IP 地址，即 kubernetes 管理的物理机的 IP 地址。\n$ kubectl get nodes NAME STATUS AGE VERSION 172.20.0.113 Ready 12d v1.6.0 172.20.0.114 Ready 12d v1.6.0 172.20.0.115 Ready 12d v1.6.0 服务发现 集群内部的服务发现\n通过 DNS 即可发现，kubends 是 kubernetes 的一个插件，不同服务之间可以直接使用 service name 访问。\n通过sericename:port即可调用服务。\n服务外部的服务发现\n通过 Ingress 来实现，我们是用的Traefik来实现。\n参考 Ingress 解析 Kubernetes Traefik Ingress 安装试用 ","relpermalink":"/blog/ip-and-service-discovry-in-kubernetes/","summary":"几种 IP 的来历。","title":"Kubernetes 中的 IP 和服务发现体系"},{"content":"无意中发现Fabric8 这个对于 Java 友好的开源微服务管理平台。\n其实这在这里发现的Achieving CI/CD with Kubernetes （by Ramit Surana,on February 17, 2017），其实是先在slideshare 上看到的。\n大家可能以前听过一个叫做fabric 的工具，那是一个 Python (2.5-2.7) 库和命令行工具，用来流水线化执行 SSH 以部署应用或系统管理任务。所以大家不要把 fabric8 跟 fabric 搞混，虽然它们之间有一些共同点，但两者完全不是同一个东西，fabric8 不是 fabric 的一个版本。Fabric 是用 python 开发的，fabric8 是 java 开发的。\n如果你想了解简化 Fabric 可以看它的中文官方文档 。\nFabric8 简介 fabric8是一个开源集成开发平台，为基于Kubernetes 和Jenkins 的微服务提供持续发布 。\n使用 fabric 可以很方便的通过Continuous Delivery pipelines 创建、编译、部署和测试微服务，然后通过 Continuous Improvement 和ChatOps 运行和管理他们。\nFabric8 微服务平台 提供：\nDeveloper Console ，是一个富 web 应用 ，提供一个单页面来创建、编辑、编译、部署和测试微服务。 Continuous Integration and Continous Delivery ，使用 Jenkins with a Jenkins Workflow Library 更快和更可靠的交付软件。 Management ，集中式管理Logging 、Metrics , ChatOps 、Chaos Monkey ，使用Hawtio 和Jolokia 管理 Java Containers。 Integration Integration Platform As A Service with deep visualisation of your Apache Camel integration services, an API Registry to view of all your RESTful and SOAP APIs and Fabric8 MQ provides Messaging As A Service based on Apache ActiveMQ 。 Java Tools 帮助 Java 应用使用Kubernetes : Maven Plugin for working with Kubernetes ，这真是极好的 Integration and System Testing of Kubernetes resources easily inside JUnit with Arquillian Java Libraries and support for CDI extensions for working with Kubernetes . Fabric8 微服务平台 Fabric8 提供了一个完全集成的开源微服务平台，可在任何的Kubernetes 和OpenShift 环境中开箱即用。\n整个平台是基于微服务而且是模块化的，你可以按照微服务的方式来使用它。\n微服务平台提供的服务有：\n开发者控制台，这是一个富 Web 应用程序，它提供了一个单一的页面来创建、编辑、编译、部署和测试微服务。 持续集成和持续交付，帮助团队以更快更可靠的方式交付软件，可以使用以下开源软件： Jenkins ：CI／CD pipeline Nexus ：组件库 Gogs ：git 代码库 SonarQube ：代码质量维护平台 Jenkins Workflow Library ：在不同的项目中复用Jenkins Workflow scripts Fabric8.yml ：为每个项目、存储库、聊天室、工作流脚本和问题跟踪器提供一个配置文件 ChatOps ：通过使用hubot 来开发和管理，能够让你的团队拥抱 DevOps，通过聊天和系统通知的方式来approval of release promotion Chaos Monkey ：通过干掉pods 来测试系统健壮性和可靠性 管理 日志 统一集群日志和可视化查看状态 metris 可查看历史 metrics 和可视化 参考 fabric8：容器集成平台——伯乐在线 Kubernetes 部署微服务速成指南——2017-03-09 徐薛彪 容器时代微信公众号 fabric8 官网 fabric8 get started 后记 我在自己笔记本上装了个 minikube，试玩感受将在后续发表。\n试玩时需要科学上网。\n$gofabric8 start using the executable /usr/local/bin/minikube minikube already running using the executable /usr/local/bin/kubectl Switched to context \u0026#34;minikube\u0026#34;. Deploying fabric8 to your Kubernetes installation at https://192.168.99.100:8443 for domain in namespace default Loading fabric8 releases from maven repository:https://repo1.maven.org/maven2/ Deploying package: platform version: 2.4.24 Now about to install package https://repo1.maven.org/maven2/io/fabric8/platform/packages/fabric8-platform/2.4.24/fabric8-platform-2.4.24-kubernetes.yml Processing resource kind: Namespace in namespace default name user-secrets-source-admin Found namespace on kind Secret of user-secrets-source-adminProcessing resource kind: Secret in namespace user-secrets-source-admin name default-gogs-git Processing resource kind: Secret in namespace default name jenkins-docker-cfg Processing resource kind: Secret in namespace default name jenkins-git-ssh Processing resource kind: Secret in namespace default name jenkins-hub-api-token Processing resource kind: Secret in namespace default name jenkins-master-ssh Processing resource kind: Secret in namespace default name jenkins-maven-settings Processing resource kind: Secret in namespace default name jenkins-release-gpg Processing resource kind: Secret in namespace default name jenkins-ssh-config Processing resource kind: ServiceAccount in namespace default name configmapcontroller Processing resource kind: ServiceAccount in namespace default name exposecontroller Processing resource kind: ServiceAccount in namespace default name fabric8 Processing resource kind: ServiceAccount in namespace default name gogs Processing resource kind: ServiceAccount in namespace default name jenkins Processing resource kind: Service in namespace default name fabric8 Processing resource kind: Service in namespace default name fabric8-docker-registry Processing resource kind: Service in namespace default name fabric8-forge Processing resource kind: Service in namespace default name gogs ... ------------------------- Default GOGS admin username/password = gogsadmin/RedHat$1 Checking if PersistentVolumeClaims bind to a PersistentVolume .... Downloading images and waiting to open the fabric8 console... ------------------------- ..................................................... 启动了半天一直是这种状态：\nWaiting, endpoint for service is not ready yet... 我一看下载下来的\nhttps://repo1.maven.org/maven2/io/fabric8/platform/packages/fabric8-platform/2.4.24/fabric8-platform-2.4.24-kubernetes.yml 文件，真是蔚为壮观啊，足足有24712 行(这里面都是实际配置，没有配置充行数)，使用了如下这些 docker 镜像，足足有53 个 docker 镜像：\nfabric8/alpine-caddy:2.2.311 fabric8/apiman-gateway:2.2.168 fabric8/apiman:2.2.168 fabric8/chaos-monkey:2.2.311 fabric8/configmapcontroller:2.3.5 fabric8/eclipse-orion:2.2.311 fabric8/elasticsearch-k8s:2.3.4 fabric8/elasticsearch-logstash-template:2.2.311 fabric8/elasticsearch-v1:2.2.168 fabric8/exposecontroller:2.3.2 fabric8/fabric8-console:2.2.199 …","relpermalink":"/blog/fabric8-introduction/","summary":"本文介绍了开源的微服务管理平台 Fabric8。","title":"开源微服务管理平台 fabric8 简介"},{"content":"前言 我已就该话题已在 2016 年上海 Qcon 上发表过演讲。另外 InfoQ 网站上的文字版数据中心的 YARN on Docker 集群方案 ，即本文。\n项目代码开源在 Github 上：Magpie 当前数据中心存在的问题 数据中心中的应用一般独立部署，为了保证环境隔离与方便管理，保证应用最大资源 数据中心中普遍存在如下问题：\n主机资源利用率低 部署和扩展复杂 资源隔离无法动态调整 无法快速响应业务 为何使用 YARN on Docker 彻底隔离队列\n为了合理利用 Hadoop YARN 的资源，队列间会互相抢占计算资源，造成重要任务阻塞 根据部门申请的机器数量划分 YARN 集群方便财务管理 更细粒度的资源分配 统一的资源分配\n每个 NodeManager 和容器都可以限定 CPU、内存资源 YARN 资源划分精确到 CPU 核数和内存大小 弹性伸缩性服务\n每个容器中运行一个 NodeManager，增减 YARN 资源只需增减容器个数 可以指定每个 NodeManager 拥有的计算资源多少，按需申请资源 给我们带来什么好处？ Swarm 统一集群资源调度\n统一资源 增加 Docker 虚拟化层，降低运维成本 增加 Hadoop 集群资源利用率\n对于数据中心：避免了静态资源隔离\n对于集群：加强集群内部资源隔离\n系统架构 YARN 在 swarm 上运行的架构 比如数据中心中运行的 Hadoop 集群，我们将 HDFS 依然运行在物理机上，即 DataNode 依然部署在实体机器上，将 YARN 计算层运行在 Docker 容器中，整个系统使用二层资源调度，Spark、Flink、MapReduce 等应用运行在 YARN 上。\nSwarm 调度最底层的主机硬件资源，CPU 和内存封装为 Docker 容器，容器中运行 NodeManager，提供给 YARN 集群，一个 Swarm 集群中可以运行多个 YARN 集群，形成圈地式的 YARN 计算集群。\nYARN 在 Swarm 上的架构之资源分配 具体流程\nswarm node 向 swarm master 注册主机资源并加入到 swarm cluster 中 swarm master 向 cluster 申请资源请求启动容器 swarm 根据调度策略选择在某个 node 上启动 docker container swarm node 的 docker daemon 根据容器启动参数启动相应资源大小的 NodeManager NodeManager 自动向 YARN 的 ResourceManager 注册资源一个 NodeManager 资源添加完成。 Swarm 为数据中心做容器即主机资源调度，每个 swarmnode 的节点结构如图：\nYARN 在 swarm 上的架构之单节点资源分配 一个 Swarm node 就是一台物理机，每台主机上可以起多个同类型的 docker container，每个 container 的资源都有限制包括 CPU、内存 NodeManager 容器只需要考虑本身进程占用的资源和需要给主机预留资源。假如主机是 24 核 64G，我们可以分给一个容器 5 核 12G，NodeManager 占用 4 核 10G 的资源提供给 YARN。\nKubernetes VS Swarm\n关于容器集群管理系统的选型，用 Kubernetes 还是 Swarm？我们结合自己的经验和业务需求，对比如下：\nKubernetes vs Swarm 基于以上四点，我们当时选择了 Swarm，它基本满足我们的需求，掌握和开发时常较短。\n镜像制作与发布 镜像制作和发布流程如下图：\nCI 流程 用户从客户端提交代码到 Gitlab 中，需要包含 Dockerfile 文件，通过集成了 docker 插件的 Jenkins 的自动编译发布机制，自动 build 镜像后 push 到 docker 镜像仓库中，同一个项目每提交一次代码都会重新 build 一次镜像，生成不同的 tag 来标识镜像，Swarm 集群使用该镜像仓库就可以直接拉取镜像。\nDockerfile 的编写技巧 Dockerfile 编写技巧 Dockerfile 相当于 docker 镜像的编译打包流程说明，其中也不乏一些技巧。 很多应用需要配置文件，如果想为每次启动容器的时候使用不同的配置参数，可以通过传递环境变量的方式来修改配置文件，前提是需要写一个 bash 脚本，脚本中来处理配置文件，再将这个脚本作为 entrypoint 入口，每当容器启动时就会执行这个脚本从而替换配置文件中的参数，也可以通过 CMD 传递参数给该脚本。\n启动容器的时候通过传递环境变量的方式修改配置文件：\ndocker run -d --net=mynet -e NAMESERVICE=nameservice -e ACTIVE_NAMENODE_ID=namenode29 \\ -e STANDBY_NAMENODE_ID=namenode63 \\ -e HA_ZOOKEEPER_QUORUM=zk1:2181,zk2:2181,zk3:2181 \\ -e YARN_ZK_DIR=rmstore \\ -e YARN_CLUSTER_ID=yarnRM \\ -e YARN_RM1_IP=rm1 \\ -e YARN_RM2_IP=rm2 \\ -e CPU_CORE_NUM=5 -e NODEMANAGER_MEMORY_MB=12288 \\ -e YARN_JOBHISTORY_IP=jobhistory \\ -e ACTIVE_NAMENODE_IP=active-namenode \\ -e STANDBY_NAMENODE_IP=standby-namenode \\ -e HA=yes \\ docker-registry/library/hadoop-yarn:v0.1 resourcemanager 最后传递 Resource Manager 或者 Node Manager 参数指定启动相应的服务。\n集群管理 我开发的命令行工具magpie ，也可以通过其他开源可视化页面来管理集群，比如 shipyard。\nShipyard 自定义网络 Docker 容器跨主机互访一直是一个问题，Docker 官方为了避免网络上带来的诸多麻烦，故将跨主机网络开了比较大的口子，而由用户自己去实现。我们开发并开源了 Shrike 这个 docker 网络插件，大家可以在这里下载到：GitHub - docker-ipam-plugin 目前 Docker 跨主机的网络实现方案也有很多种，主要包括端口映射、ovs、fannel 等。但是这些方案都无法满足我们的需求，端口映射服务内的内网 IP 会映射成外网的 IP，这样会给开发带来困惑，因为他们往往在跨网络交互时是不需要内网 IP 的，而 ovs 与 fannel 则是在基础网络协议上又包装了一层自定义协议，这样当网络流量大时，却又无端的增加了网络负载，最后我们采取了自主研发扁平化网络插件，也就是说让所有的容器统统在大二层上互通。架构如下：\nYARN 网络 我们首先需要创建一个 br0 自定义网桥，这个网桥并不是通过系统命令手动建立的原始 Linux 网桥，而是通过 Docker 的 cerate network 命令来建立的自定义网桥，这样避免了一个很重要的问题就是我们可以通过设置 DefaultGatewayIPv4 参数来设置容器的默认路由，这个解决了原始 Linux 自建网桥不能解决的问题。用 Docker 创建网络时我们可以通过设置 subnet 参数来设置子网 IP 范围，默认我们可以把整个网段给这个子网，后面可以用 ipamdriver（地址管理插件）来进行控制。还有一个参数 gateway 是用来设置 br0 自定义网桥地址的，其实也就是你这台宿主机的地址。\ndocker network create --opt=com.docker.network.bridge.enable_icc=true --opt=com.docker.network.bridge.enable_ip_masquerade=false --opt=com.docker.network.bridge.host_binding_ipv4=0.0.0.0 --opt=com.docker.network.bridge.name=br0 --opt=com.docker.network.driver.mtu=1500 --ipam-driver=talkingdata --subnet=容器IP的子网范围 --gateway=br0网桥使用的IP,也就是宿主机的地址 --aux-address=DefaultGatewayIPv4=容器使用的网关地址 mynet IPAM 插件 IPAM 驱动是专门管理 Docker 容器 IP 的，Docker 每次启停与删除容器都会调用这个驱动提供的 IP 管理接口，然后 IP 接口会对存储 IP 地址的 Etcd 有一个增删改查的操作。此插件运行时会起一个 UnixSocket, 然后会在docker/run/plugins目录下生成一个.sock 文件，Dockerdaemon 之后会和这个 sock 文件进行沟通去调用我们之前实现好的几个接口进行 IP 管理，以此来达到 IP 管理的目的，防止 IP 冲突。 通过 Docker 命令去创建一个自定义的网络起名为mynet，同时会产生一个网桥 br0，之后通过更改网络配置文件（在/etc/sysconfig/network-scripts/下 ifcfg-br0、ifcfg-默认网络接口名）将默认网络接口桥接到 br0 上，重启网络后，桥接网络就会生效。Docker 默认在每次启动容器时都会将容器内的默认网卡桥接到 br0 上，而且宿主机的物理网卡也同样桥接到了 br0 上了。其实桥接的原理就好像是一台交换机，Docker 容器和宿主机物理网络接口都是服务器，通过 vethpair 这个网络设备像一根网线插到交换机上。至此，所有的容器网络已经在同一个网络上可以通信了，每一个 Docker 容器就好比是一台独立的虚拟机，拥有和宿主机同一网段的 IP，可以实现跨主机访问了。\n性能瓶颈与优化 大家可能会担心自定义网络的性能问题，为此我们用 iperf 进行了网络性能测试。我们对比了不同主机容器间的网速，同一主机上的不同容器和不同主机间的网速，结果如下表：\n网络性能对比 从表中我们可以看到，在这一组测试中，容器间的网速与容器是在想通主机还是在不同主机上的差别不大，说明我们的网络插件性能还是很优异的。 Hadoop 配置优化 因为使用 docker 将原来一台机器一个 nodemanager 给细化为了多个，会造成 nodemanager 个数的成倍增加，因此 hadoop 的一些配置需要相应优化。\n- yarn.nodemanager.localizer.fetch.thread-count 随着容器数量增加，需要相应调整该参数 - yarn.resourcemanager.amliveliness-monitor.interval-ms 默认1秒，改为10秒，否则时间太短可能导致有些节点无法注册 - yarn.resourcemanager.resource-tracker.client.thread-count 默认50，改为100，随着容器数量增加，需要相应调整该参数 - yarn.nodemanager.pmem-check-enabled 默认true，改为false，不检查任务正在使用的物理内存量 - 容器中hadoop ulimit值修改，默认4096，改成655350 集群监控 如果使用 shipyard 管理集群会有一个单独的监控页面，可以看到一定时间段内的 CPU、内存、IO、 …","relpermalink":"/blog/yarn-on-docker/","summary":"基于 docker swarm。","title":"容器技术在大数据场景下的应用——YARN on Docker"},{"content":"如果你看过美剧「硅谷」会记得剧中主角们所在的创业公司PiedPipper ，他们就是靠自己发明的视频压缩算法来跟大公司 Hooli 竞争的，这部剧现在已经发展到第 4 季，在腾讯视频 上可以免费观看。\n最近关注了两个图像处理的 Open Source Projects。\nGoogle Guetzli 图像压缩工具 Luan Fujun’s Deep Photo Style Transfer 图像 style 转换工具 另外对于图像处理还处于 Photoshop、Lightroom 这种摄影后期和图像处理命令行工具ImageMagick 的我来说，图像压缩，智能图像风格转换实乃上乘武功，不是我等凡夫俗子驾驭的了，但是乘兴而来，总不能败兴而归吧，下面我们来一探究竟。\nGoogle Guetzli 聊聊架构微信公众号上有一篇介绍Google 开源新算法，可将 JPEG 文件缩小 35% 文章。\n我在 Mac 上试用了一下，安装很简单，只要一条命令：\nbrew install guetzli 但是当我拿一张22M大小的照片使用 guetzli 压缩的时候，我是绝望的，先后三次 kill 掉了进程。\n因为实在是太慢了，也能是我软件对内存和 CPU 的利用率不高，效果你们自己看看。\n原图是这个样子的，拍摄地点在景山上的，俯瞰紫禁城的绝佳位置。\n原图 guetzli --quality 84 --verbose 20160403052.jpg output.jpg 为什么 quality 要设置成 84 呢？因为只能设置为 84+的 quality，如果要设置的更低的话需要自己修改代码。\nprocess 耗时了一个小时，后台进程信息。\n后台进程 这个是使用Squash压缩后的大小效果，压缩每张照片差不多只要 3 秒钟。\nSquash 的 logo 就是个正在被剥皮的🍊，这是下载地址 。\n压缩比分别为70%和30%。\nImg 压缩比 70% 后的细节放大图\n70 压缩比 30% 的细节放大图\n30 你看出什么区别了吗？反正我是没有。\n下面再来看看耗时一个小时，千呼万唤始出来的 guetzli 压缩后的效果和使用 squash 压缩比为 30% 的效果对比。\n对比 左面是使用 guetzli 压缩后（4.1M），右面使用的 squash 压缩后（3.1M）的照片。\n似乎还是没有什么区别啊？你看出来了吗？\nGuetzli 总结 可能是我使用 Guetzli 的方式不对，但是命令行里确实没有设置 CPU 和内存资源的选项啊，为啥压缩照片会这么慢呢？效果也并不出彩，不改代码的话照片质量只能设置成 84 以上，但是这个是Open Source的，使用的 C++开发，可以研究下它的图像压缩算法。\nDeep Photo Style Transfer 来自康奈尔大学的 Luan Fujun 开源的图像 sytle 转换工具，看了README 的介绍，上面有很多图像风格转换的例子，真的很惊艳，市面上好像还没有这种能够在给定任意一张照片的情况下，自动将另一张照片转换成该照片的 style。\n这个工具使用 Matlab 和 Lua 开发，基于Torch 运行的时候需要CUDA ，cudnn ，Matlab ，环境实在太复杂，就没折腾，启动有人发布Docker 镜像 ，已经有人提了 issue。\n如果它能够被商用，绝对是继Prisma后又一人工智能照片处理应用利器。\n后记 是不是有了照片风格转换这个东西就不需要做照片后期了？只要选几张自己喜欢的风格照片，再鼠标点几下就可以完成照片处理了？摄影师要失业了？非也！照片风格东西本来就是很主观性的，每个人都有自己喜欢的风格，照相机发明后就有人说画家要失业了，其实不然，画画依然是创造性地劳动，只能说很多写实风格的画家要失业了。Deep Photo Style Transfer 也许会成为 Lightroom 或者手机上一款 app 的功能，是一个不错的工具。也许还会成为像 Prisma 一样的现象级产品，who knows?🤷‍♂️\n","relpermalink":"/blog/picture-process/","summary":"Google Guetzli 和基于 AI 的 Deep Photo Style Transfer。","title":"两款开源图片处理工具对比"},{"content":"前言 最近研究了下Pivotal的Cloud foundry，CF 本身是一款开源软件，很多 PAAS 厂商都加入了 CF，我们用的是的PCF Dev（PCF Dev 是一款可以在工作站上运行的轻量级 PCF 安装）来试用的，因为它可以部署在自己的环境里，而Pivotal Web Services只免费两个月，之后就要收费。这里 有官方的详细教程。\n开始 根据官网的示例，我们将运行一个 Java 程序示例。\n安装命令行终端\n下载 后双击安装即可，然后执行cf help能够看到帮助。\n安装 PCF Dev\n先下载 ，如果你没有 Pivotal network 账号的话，还需要注册个用户，然后用以下命令安装：\n$./pcfdev-VERSION-osx \u0026amp;\u0026amp; \\ cf dev start Less than 4096 MB of free memory detected, continue (y/N): \u0026gt; y Please sign in with your Pivotal Network account. Need an account? Join Pivotal Network: https://network.pivotal.io Email\u0026gt; 849122844@qq.com Password\u0026gt; Downloading VM... Progress: |+++++++++++++=======\u0026gt;| 100% VM downloaded. Allocating 4096 MB out of 16384 MB total system memory (3514 MB free). Importing VM... Starting VM... Provisioning VM... Waiting for services to start... 8 out of 57 running 8 out of 57 running 8 out of 57 running 46 out of 57 running 57 out of 57 running _______ _______ _______ ______ _______ __ __ | || || | | | | || | | | | _ || || ___| | _ || ___|| |_| | | |_| || || |___ | | | || |___ | | | ___|| _|| ___| | |_| || ___|| | | | | |_ | | | || |___ | | |___| |_______||___| |______| |_______| |___| is now running. To begin using PCF Dev, please run: cf login -a https://api.local.pcfdev.io --skip-ssl-validation Apps Manager URL: https://local.pcfdev.io Admin user =\u0026gt; Email: admin / Password: admin Regular user =\u0026gt; Email: user / Password: pass 启动过程中还需要Sign In，所以注册完后要记住用户名（邮箱地址）和密码（必须超过 8 位要有特殊字符和大写字母）。这个过程中还要下载 VM，对内存要求至少 4G。而且下载速度比较慢，我下载的了大概 3 个多小时吧。\n下面部署一个应用到 PCF Dev 上试一试。\n部署应用 下载代码\n$git clone https://github.com/cloudfoundry-samples/spring-music $cd ./spring-music $cf login -a api.local.pcfdev.io --skip-ssl-validation API endpoint: api.local.pcfdev.io Email\u0026gt; user Password\u0026gt; pass Authenticating... OK Targeted org pcfdev-org Targeted space pcfdev-space API endpoint: https://api.local.pcfdev.io (API version: 2.65.0) User: user Org: pcfdev-org Space: pcfdev-space 编译应用\n使用 gradle 来编译。\n$./gradlew assemble :compileJava UP-TO-DATE :processResources UP-TO-DATE :classes UP-TO-DATE :findMainClass :jar :bootRepackage Download https://repo1.maven.org/maven2/com/sun/jersey/jersey-client/1.13/jersey-client-1.13.jar Download https://repo1.maven.org/maven2/com/sun/jersey/jersey-json/1.13/jersey-json-1.13.jar Download https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.5/httpcore-4.4.5.jar Download https://repo1.maven.org/maven2/com/nimbusds/oauth2-oidc-sdk/4.5/oauth2-oidc-sdk-4.5.jar Download https://repo1.maven.org/maven2/com/google/code/gson/gson/2.3.1/gson-2.3.1.jar Download https://repo1.maven.org/maven2/com/sun/jersey/jersey-core/1.13/jersey-core-1.13.jar Download https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.2/jackson-core-asl-1.9.2.jar Download https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.2/jackson-mapper-asl-1.9.2.jar Download https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-jaxrs/1.9.2/jackson-jaxrs-1.9.2.jar Download https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-xc/1.9.2/jackson-xc-1.9.2.jar Download https://repo1.maven.org/maven2/net/jcip/jcip-annotations/1.0/jcip-annotations-1.0.jar Download https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.3.1/commons-lang3-3.3.1.jar Download https://repo1.maven.org/maven2/net/minidev/json-smart/1.1.1/json-smart-1.1.1.jar Download https://repo1.maven.org/maven2/com/nimbusds/lang-tag/1.4/lang-tag-1.4.jar Download https://repo1.maven.org/maven2/com/nimbusds/nimbus-jose-jwt/3.1.2/nimbus-jose-jwt-3.1.2.jar Download https://repo1.maven.org/maven2/stax/stax-api/1.0.1/stax-api-1.0.1.jar Download https://repo1.maven.org/maven2/org/bouncycastle/bcprov-jdk15on/1.51/bcprov-jdk15on-1.51.jar Download https://repo1.maven.org/maven2/javax/mail/mail/1.4.7/mail-1.4.7.jar :assemble BUILD SUCCESSFUL Total time: 1 mins 25.649 secs This build could be faster, please consider using the Gradle Daemon: https://docs.gradle.org/2.14/userguide/gradle_daemon.html 上传应用\n设置应用的主机名为 spring-music。\n$cf push --hostname spring-music Using manifest file /Users/jimmy/Workspace/github/cloudfoundry-samples/spring-music/manifest.yml Creating app spring-music in org pcfdev-org / space pcfdev-space as user... OK Creating route spring-music.local.pcfdev.io... OK Binding spring-music.local.pcfdev.io to spring-music... OK Uploading spring-music... Uploading app files from: /var/folders/61/f7mqkyjn1nz5mfmfvdztgzjw0000gn/T/unzipped-app139680305 Uploading 38.9M, 234 files Done uploading OK Starting app spring-music in org pcfdev-org / space pcfdev-space as user... Downloading dotnet-core_buildpack... Downloading go_buildpack... Downloading python_buildpack... Downloading php_buildpack... Downloading staticfile_buildpack... Downloaded staticfile_buildpack Downloading binary_buildpack... Downloaded binary_buildpack (9.3K) Downloading java_buildpack... Downloaded java_buildpack (249.1M) Downloaded dotnet-core_buildpack (169.3M) Downloading ruby_buildpack... Downloading nodejs_buildpack... Downloaded …","relpermalink":"/blog/cloud-foundry-tryout/","summary":"最近研究了下 Pivotal 的 Cloud foundry，CF 本身是一款开源软件，很多 PAAS 厂商都加入了 CF。","title":"Pivotal Cloud foundry 快速开始指南"},{"content":"前言 之前陆陆续续看过一点docker的源码，都未成体系，最近在研究Docker-17.03-CE，趁此机会研究下 docker 的源码，在网上找到一些相关资料，都比较过时了，发现孙宏亮写过一本书叫《Docker 源码分析》，而且之前也在InfoQ上陆续发过一些文章，虽然文章都比较老了，基于老的 docker 版本，但我认为依然有阅读的价值。起码能有这三方面收获：\n一是培养阅读源码的思维方式，为自己阅读 docker 源码提供借鉴。 二是可以了解 docker 版本的来龙去脉。 三还可以作为 Go 语言项目开发作为借鉴。 下载地址 鉴于截止本文发稿时这本书已经发行一年半了了，基于的 docker 版本还是1.2.0，而如今都到了1.13.0（docker17.03 的老版本号），应该很少有人买了吧，可以说这本书的纸质版本的生命周期也差不多了吧。如果有人感兴趣可以到网上找找看看，Docker 源码解析 - 机械工业出版社 - 孙宏亮著 -2015 年 8 月（完整文字版，大小 25.86M），Docker 源码解析 - 看云整理版（文字版，有缩略，大小 7.62M）。\nOut-of-date 有一点必须再次强调一下，这本书中的 docker 源码分析是基于docker1.2.0，而这个版本的 docker 源码在 github 上已经无法下载到了，github 上 available 的最低版本的 docker 源码是1.4.1。\n顺便感叹一句，科技行业发展实在太快了，尤其是互联网，一本书能连续用上三年都不过时，如果这样的话那么这门技术恐怕都就要被淘汰了吧？\n总体架构 Docker 总体上是用的是Client/Server模式，所有的命令都可以通过 RESTful 接口传递。\n整个 Docker 软件的架构中可以分成三个角色：\nDaemon：常驻后台运行的进程，接收客户端请求，管理 docker 容器。 Client：命令行终端，包装命令发送 API 请求。 Engine：真正处理客户端请求的后端程序。 代码结构 Docker 的代码结构比较清晰，分成的目录比较多，有以下这些：\napi：定义 API，使用了Swagger2.0这个工具来生成 API，配置文件在api/swagger.yaml builder：用来 build docker 镜像的包，看来历史比较悠久了 bundles：这个包是在进行docker 源码编译和开发环境搭建 的时候用到的，编译生成的二进制文件都在这里。 cli：使用cobra 工具生成的 docker 客户端命令行解析器。 client：接收cli的请求，调用 RESTful API 中的接口，向 server 端发送 http 请求。 cmd：其中包括docker和dockerd两个包，他们分别包含了客户端和服务端的 main 函数入口。 container：容器的配置管理，对不同的 platform 适配。 contrib：这个目录包括一些有用的脚本、镜像和其他非 docker core 中的部分。 daemon：这个包中将 docker deamon 运行时状态 expose 出来。 distribution：负责 docker 镜像的 pull、push 和镜像仓库的维护。 dockerversion：编译的时候自动生成的。 docs：文档。这个目录已经不再维护，文档在另一个仓库里 。 experimental：从 docker1.13.0 版本起开始增加了实验特性。 hack：创建 docker 开发环境和编译打包时用到的脚本和配置文件。 image：用于构建 docker 镜像的。 integration-cli：集成测试 layer：管理 union file system driver 上的 read-only 和 read-write mounts。 libcontainerd：访问内核中的容器系统调用。 man：生成 man pages。 migrate：将老版本的 graph 目录转换成新的 metadata。 oci：Open Container Interface 库 opts：命令行的选项库。 pkg： plugin：docker 插件后端实现包。 profiles：里面有 apparmor 和 seccomp 两个目录。用于内核访问控制。 project：项目管理的一些说明文档。 reference：处理 docker store 中镜像的 reference。 registry：docker registry 的实现。 restartmanager：处理重启后的动作。 runconfig：配置格式解码和校验。 vendor：各种依赖包。 volume：docker volume 的实现。 下一篇将讲解 docker 的各个功能模块和原理。\n","relpermalink":"/blog/docker-source-code-analysis-code-structure/","summary":"之前陆陆续续看过一点 docker 的源码，都未成体系，最近在研究 Docker-17.03-CE，趁此机会研究下 docker 的源码。","title":"Docker 源码分析第一篇——代码结构"},{"content":" 亲，你还在为虚拟主机、域名、空间而发愁吗？你想拥有自己的网站吗？你想拥有一个分享知识、留住感动，为开源事业而奋斗终身吗？那么赶快拿起你手中的📱拨打16899168，不对，是看这篇文章吧，不用 998，也不用 168，这一切都是免费的，是的你没看错，真的不要钱！\n准备 当然还是需要你有一点电脑基础的，会不会编程不要紧，还要会一点英文，你需要先申请一下几个账号和安装一些软件环境：\nGitHub 这是必需的，因为你需要使用Github Pages 来托管你的网站。而且你还需要安装 git 工具。创建一个以自己用户名命名的 username.github.io 的 project。 七牛云存储 非必需，为了存储文件方便，建议申请一个，免费 10G 的存储空间，存储照片和一些小文件是足够的，可以用来做外链，方便存储和管理，这样你就不用把图片也托管到 Github 上了。流量也是不限的。我没有收七牛的一点好处，以为是我自己用的，所以推荐给大家，七牛还有命令行客户端，方便你上传和同步文件。如上的题图都是存储在七牛云中的。 百度统计 非必需，基本的网站数据分析，免费的，质量还行。还有微信公众号可以查看，这一点我发现腾讯分析居然都没有微信公众号，自家的产品咋都不推出微信客户端呢。顺便提一下，这个统计账号跟你的百度账号不是同一个东西，两者是两套体系，当然你可以和自己的百度账号关联。只需要在 Web 的 Header 中植入一段 JS 代码即可。 Hugo 必需的，静态网站生成工具，用来编译静态网站的。跟 Hexo 比起来我更喜欢这个工具。 Typro 非必需，但是强烈推荐，我最喜欢的免费的 Markdown 编辑器，hugo 可以编译 markdown 格式为 HTML，所以用它来写博客是最合适不过了。 好了注册好 Github 后你现在可以尽情的玩耍了！😄\nLet’s rock\u0026amp;roll! 首先介绍下 Hugo\nHugo 是一种通用的网站框架。严格来说，Hugo 应该被称作静态网站生成器。\n静态网站生成器从字面上来理解，就是将你的内容生成静态网站。所谓“静态”的含义其实反映在网站页面的生成的时间。一般的 web 服务器（WordPress, Ghost, Drupal 等等）在收到页面请求时，需要调用数据库生成页面（也就是 HTML 代码），再返回给用户请求。而静态网站则不需要在收到请求后生成页面，而是在整个网站建立起之前就将所有的页面全部生成完成，页面一经生成便称为静态文件，访问时直接返回现成的静态页面，不需要数据库的参与。\n采用静态网站的维护也相当简单，实际上你根本不需要什么维护，完全不用考虑复杂的运行时间，依赖和数据库的问题。再有也不用担心安全性的问题，没有数据库，网站注入什么的也无从下手。\n静态网站最大好处就是访问快速，不用每次重新生成页面。当然，一旦网站有任何更改，静态网站生成器需要重新生成所有的与更改相关的页面。然而对于小型的个人网站，项目主页等等，网站规模很小，重新生成整个网站也是非常快的。Hugo 在速度方面做得非常好，Dan Hersam 在他这个Hugo 教程 里提到，5000 篇文章的博客，Hugo 生成整个网站只花了 6 秒，而很多其他的静态网站生成器则需要几分钟的时间。我的博客目前文章只有几十篇，用 Hugo 生成整个网站只需要 0.1 秒。官方文档提供的数据是每篇页面的生成时间不到 1ms。\n认为对于个人博客来说，应该将时间花在内容上而不是各种折腾网站。Hugo 会将 Markdown 格式的内容和设置好模版一起，生成漂亮干净的页面。挑选折腾好一个喜爱的模版，在 Sublime Text 里用 Markdown 写博客，再敲一行命令生成同步到服务器就 OK 了。整个体验是不是非常优雅简单还有点 geek 的味道呢？\n了解 Hugo 首先建立自己的网站，mysite 是网站的路径\n$ hugo new site mysite 然后进入该路径\n$ cd mysite 在该目录下你可以看到以下几个目录和config.toml文件\n▸ archetypes/ ▸ content/ ▸ layouts/ ▸ static/ config.toml config.toml是网站的配置文件，包括baseurl, title, copyright等等网站参数。\n这几个文件夹的作用分别是：\narchetypes：包括内容类型，在创建新内容时自动生成内容的配置 content：包括网站内容，全部使用 markdown 格式 layouts：包括了网站的模版，决定内容如何呈现 static：包括了 css, js, fonts, media 等，决定网站的外观 Hugo 提供了一些完整的主题可以使用，下载这些主题：\n$ git clone --recursive https://github.com/spf13/hugoThemes themes 此时现成的主题存放在themes/文件夹中。\n现在我们先熟悉一下 Hugo，创建新页面：\n$ hugo new about.md 进入content/文件夹可以看到，此时多了一个 markdown 格式的文件about.md，打开文件可以看到时间和文件名等信息已经自动加到文件开头，包括创建时间，页面名，是否为草稿等。\n--- date: \u0026#34;2015-02-01T18:19:54+08:00\u0026#34; draft: true title: \u0026#34;about\u0026#34; categories: \u0026#34;github-pages\u0026#34; tag: [\u0026#34;blog\u0026#34;,\u0026#34;post\u0026#34;] --- # About me - Jimmy Song - rootsongjc@gmail.com 我在页面中加入了一些内容，然后运行 Hugo:\n$ hugo server -t hyde --buildDrafts -t参数的意思是使用 hyde 主题渲染我们的页面，注意到about.md目前是作为草稿，即draft参数设置为true，运行 Hugo 时要加上--buildDrafts参数才会生成被标记为草稿的页面。在浏览器输入 localhost:1313，就可以看到我们刚刚创建的页面。\n注意观察当前目录下多了一个文件夹public/，这里面是 Hugo 生成的整个静态网站，如果使用 Github pages 来作为博客的 Host，你只需要将public/里的文件上传就可以，这相当于是 Hugo 的输出。\n详细说明请看这位朋友的文章：Nanshu Wang - Hugo 静态网站生成器中文教程 说明\n使用hugo new命令生成的文章前面的加号中包括的那几行，是用来设置文章属性的，这些属性使用的是 yaml 语法。\ndate 自动增加时间标签，页面上默认显示 n 篇最新的文章。 draft 设置为 false 的时候会被编译为 HTML，true 则不会编译和发表，在本地修改文章时候用 true。 title 设置文章标题 tags 数组，可以设置多个标签，都好隔开，hugo 会自动在你博客主页下生成标签的子 URL，通过这个 URL 可以看到所有具有该标签的文章。 categories 文章分类，跟 Tag 功能差不多，只能设置一个字符串。 今天先说到这里，再次声明下，Jimmy Song’s blog 就是用👆的步骤建立的。\nJimmy Song’s blog 的页面比较简陋，你可以在这里 找到更多可爱的模版。另外我给自己翻译的书Cloude Native Go 做一个静态页面，点此查看 ，欢迎大家关注。🙏\n以下 2017 年 8 月 31 日更新\n如果你对 GitHub 的域名不满意，想要用自己的域名，那么申请域名的地方有很多，比如万网、GoDaddy、Namecheap，我的域名 jimmysong.io 就是在 Namecheap 上申请的，申请完域名后还需要做域名解析，我使用的是 DNSPod，免费的，然后在 GitHub 中配置下 CNAME 即可。\n","relpermalink":"/blog/building-github-pages-with-hugo/","summary":"Hugo 是一种通用的网站框架，本问教你如何使用 Hugo 来构建静态网站。","title":"零基础使用 Hugo 和 GitHub Pages 创建自己的博客"},{"content":"前几天写的几篇关于 Contiv 的文章 已经把引入坑了😂\n今天这篇文章将带领大家用正确的姿势编译和打包一个contiv netplugin。\n请一定要在Linux环境中编译。docker 中编译也会报错，最好还是搞个虚拟🐔吧，最好还有 VPN 能翻墙。\n环境准备 我使用的是 docker17.03-CE、安装了 open vSwitch(这个包 redhat 的源里没有，需要自己的编译安装)。\n编译 这一步是很容易失败的，有人提过issue-779 具体步骤\n创建一个 link /go链接到你的 GOPATH 目录，下面编译的时候要用。 将源码的vender目录下的文件拷贝到$GOPATH/src 目录。 执行编译 在 netplugin 目录下执行以下命令能够编译出二进制文件。\nNET_CONTAINER_BUILD=1 make build 在你的**/$GOPATH/bin**目录下应该会有如下几个文件：\ncontivk8s github-release godep golint misspell modelgen netcontiv netctl netmaster netplugin ⚠️编译过程中可能会遇到 有些包不存在或者需要翻墙下载。\n打包 我们将其打包为 docker plugin。\nMakefile 里用于创建 plugin rootfs 的命令是：\nhost-pluginfs-create: @echo dev: creating a docker v2plugin rootfs ... sh scripts/v2plugin_rootfs.sh v2plugin_rootfs.sh这个脚本的内容：\n#!/bin/bash # Script to create the docker v2 plugin # run this script from contiv/netplugin directory echo \u0026#34;Creating rootfs for v2plugin \u0026#34;, ${CONTIV_V2PLUGIN_NAME} cat install/v2plugin/config.template | grep -v \u0026#34;##\u0026#34; \u0026gt; install/v2plugin/config.json sed -i \u0026#34;s%PluginName%${CONTIV_V2PLUGIN_NAME}%\u0026#34; install/v2plugin/config.json cp bin/netplugin bin/netmaster bin/netctl install/v2plugin docker build -t contivrootfs install/v2plugin id=$(docker create contivrootfs true) mkdir -p install/v2plugin/rootfs sudo docker export \u0026#34;${id}\u0026#34; | sudo tar -x -C install/v2plugin/rootfs docker rm -vf \u0026#34;${id}\u0026#34; docker rmi contivrootfs rm install/v2plugin/netplugin install/v2plugin/netmaster install/v2plugin/netctl 先把$GOPATH/bin下生成的\nnetplugin\nnetmaster\nnetctl\nnetplugin\n这几个二进制文件拷贝到 netplugin 源码的 bin 目录下。\n这里面用语创建 contivrootfs 镜像的 Dockerfile 内容：\n# Docker v2plugin container with OVS / netplugin / netmaster FROM alpine:3.5 MAINTAINER Cisco Contiv (http://contiv.github.io/) RUN mkdir -p /run/docker/plugins /etc/openvswitch /var/run/contiv/log \\ \u0026amp;\u0026amp; echo \u0026#39;http://dl-cdn.alpinelinux.org/alpine/v3.4/main\u0026#39; \u0026gt;\u0026gt; /etc/apk/repositories \\ \u0026amp;\u0026amp; apk update \u0026amp;\u0026amp; apk add openvswitch=2.5.0-r0 iptables COPY netplugin netmaster netctl startcontiv.sh / ENTRYPOINT [\u0026#34;/startcontiv.sh\u0026#34;] 执行make host-pluginfs-create创建 rootfs。\n创建出了 rootfs 后，然后执行\ndocker plugin create localhost:5000/contiv/netplugin . docker push localhost:5000/contiv/netplugin 注：我们将插件 push 到 docker registry 的镜像仓库中，当前Harbor 还不支持 docker 插件的 push。\nInstall plugin\n下面是编译和安装我自己生成 v2plugin 的过程。\n修改config.json文件中的plugin_name字段的值为插件的名称。\n$docker plugin install localhost:5000/contiv/v2plugin Plugin \u0026#34;localhost:5000/contiv/v2plugin\u0026#34; is requesting the following privileges: - network: [host] - mount: [/etc/openvswitch] - mount: [/var/log/openvswitch] - mount: [/var/run] - mount: [/lib/modules] - capabilities: [CAP_SYS_ADMIN CAP_NET_ADMIN CAP_SYS_MODULE] Do you grant the above permissions? [y/N] y latest: Pulling from contiv/v2plugin fd87a71d9090: Download complete Digest: sha256:b13ad7930f771c9602acf562c2ae147482466f4d94e708692a215935663215a6 Status: Downloaded newer image for localhost:5000/contiv/v2plugin:latest Installed plugin localhost:5000/contiv/v2plugin 自己 create 的插件 enable 的时候从 docker daemon 的日志中依然可以看到之前看到找不到 socket 的错误，实际上也确实是没有生成。如果直接使用docker plugin install store/contiv/v2plugin:1.0.0-beta.3 的方式安装插件是没有问题的。\nDocker17.03-CE 中插件机制存在的问题 Docker17.03 的插件机制是为了 docker 公司的商业化策略而实行的，所有的 docker 插件都运行在自己的 namespace 和 rootfs 中，插件接口\nPlugin backend 接口\n// Backend for Plugin type Backend interface { Disable(name string, config *enginetypes.PluginDisableConfig) error Enable(name string, config *enginetypes.PluginEnableConfig) error List(filters.Args) ([]enginetypes.Plugin, error) Inspect(name string) (*enginetypes.Plugin, error) Remove(name string, config *enginetypes.PluginRmConfig) error Set(name string, args []string) error Privileges(ctx context.Context, ref reference.Named, metaHeaders http.Header, authConfig *enginetypes.AuthConfig) (enginetypes.PluginPrivileges, error) Pull(ctx context.Context, ref reference.Named, name string, metaHeaders http.Header, authConfig *enginetypes.AuthConfig, privileges enginetypes.PluginPrivileges, outStream io.Writer) error Push(ctx context.Context, name string, metaHeaders http.Header, authConfig *enginetypes.AuthConfig, outStream io.Writer) error Upgrade(ctx context.Context, ref reference.Named, name string, metaHeaders http.Header, authConfig *enginetypes.AuthConfig, privileges enginetypes.PluginPrivileges, outStream io.Writer) error CreateFromContext(ctx context.Context, tarCtx io.ReadCloser, options *enginetypes.PluginCreateOptions) error } 从 Plugin 的后端接口中可以看到，没有像镜像一样的两个常用方法：\n没有修改 plugin 名字的方法，因为没有这个方法，就无法 push plugin 到自己的镜像仓库，另外Harbor还是不支持docker plugin push Issue-1532 。 没有导出 plugin 的方法，这样就只能在联网的主机上安装 docker plugin 了，对于无法联网的主机只好束手无策了。 估计 docker 官方也不会开放这两个接口吧。毕竟这是Docker EE 的一个重要卖点：\nDocker EE’s Certified Plugins provide networking and volume plugins and easy to download and install containers to the Docker EE environment.\n疑问 为什么一定要使用 docker plugin install\n因为docker plugin install的时候会申请一些访问权限。\n这一块在上面的步骤中可以看到。\n为什么 docker plugin 不能改名字？\n我们看下 Plugin 的结构体（在 api/types/plugin.go 中定义）：\n// Plugin A plugin for the Engine API // swagger:model Plugin type Plugin struct { // config // Required: …","relpermalink":"/blog/contiv-ultimate/","summary":"本文将带领大家用正确的姿势编译和打包一个 contiv netplugin。","title":"Docker 17.03CE 下思科 Docker 网络插件 contiv 趟坑终极版"},{"content":" 当你看到这篇文章时，如果你也正在进行 docker1.13+版本下的 plugin 开发，恭喜你也入坑了，如果你趟出坑，麻烦告诉你的方法，感恩不尽🙏\n看了文章后你可能会觉得，官网上的可能是个假🌰。虽然官网上的文档写的有点不对，不过你使用 docker-ssh-volume 的开源代码自己去构建 plugin 的还是可以成功的！\nDocker plugin 开发文档 首先 docker 官方给出了一个docker legacy plugin 文档 ，这篇文章基本就是告诉你 docker 目前支持哪些插件，罗列了一系列连接，不过对不起，这些不是 docker 官方插件，有问题去找它们的开发者去吧😂\nDocker plugin 貌似开始使用了新的 v2 plugin 了，legacy 版本的 plugin 可以能在后期被废弃。\n从 docker 的源码plugin/store.go中可以看到：\n/* allowV1PluginsFallback determines daemon\u0026#39;s support for V1 plugins. * When the time comes to remove support for V1 plugins, flipping * this bool is all that will be needed. */ const allowV1PluginsFallback bool = true /* defaultAPIVersion is the version of the plugin API for volume, network, IPAM and authz. This is a very stable API. When we update this API, then pluginType should include a version. e.g. \u0026#34;networkdriver/2.0\u0026#34;. */ const defaultAPIVersion string = \u0026#34;1.0\u0026#34; 随着 docker 公司是的战略调整，推出了 docker-CE 和 docker-EE 之后，未来有些插件就可能要收费了，v2 版本的插件都是在 docker store 中下载了，而这种插件在创建的时候都是打包成 docker image，如果不开放源码的话，你即使 pull 下来插件也无法修改和导出的，docker plugin 目前没有导出接口。\n真正要开发一个 docker plugin 还是得看docker plugin API ，这篇文档告诉我们：\n插件发现 当你开发好一个插件docker engine怎么才能发现它们呢？有三种方式：\n- **.sock**，linux 下放在/run/docker/plugins 目录下，或该目录下的子目录比如[flocker](https://github.com/ClusterHQ/flocker)插件的`.sock`文件放在`/run/docker/plugins/flocker/flocker.sock`下 - **.spec**，比如**convoy**插件在`/etc/docker/plugins/convoy.spec `定义，内容为`unix:///var/run/convoy/convoy.sock` - **.json**，比如**infinit**插件在`/usr/lib/docker/plugins/infinit.json `定义，内容为`{\u0026#34;Addr\u0026#34;:\u0026#34;https://infinit.sh\u0026#34;,\u0026#34;Name\u0026#34;:\u0026#34;infinit\u0026#34;}` 文章中的其它部分貌似都过时了，新的插件不是作为systemd进程运行的，而是完全通过docker plugin命令来管理的。\n当你使用**docker plugin enable \u0026lt;plugin_name\u0026gt;**来激活了插件后，理应在/run/docker/plugins目录下生成插件的.sock文件，但是现在只有一个以 runc ID 命名的目录，这个问题下面有详细的叙述过程，你也可以跳过，直接看issue-31723 docker plugin 管理 创建 sshfs volume plugin 官方示例文档 （这个文档有问题）docker-issue29886 官方以开发一个sshfs的 volume plugin 为例。\n执行docker plugin create命令的目录下必须包含以下内容：\nconfig.json文件，里面是插件的配置信息，plugin config 参考文档 rootfs目录，插件镜像解压后的目录。v2 版本的 docker plugin 都是以 docker 镜像的方式包装的。 $ git clone https://github.com/vieux/docker-volume-sshfs $ cd docker-volume-sshfs $ go get github.com/docker/go-plugins-helpers/volume $ go build -o docker-volume-sshfs main.go $ docker build -t rootfsimage . $ id=$(docker create rootfsimage true) # id was cd851ce43a403 when the image was created $ sudo mkdir -p myplugin/rootfs $ sudo docker export \u0026#34;$id\u0026#34; | sudo tar -x -C myplugin/rootfs $ docker rm -vf \u0026#34;$id\u0026#34; $ docker rmi rootfsimage 我们可以看到sshfs的 Dockerfile 是这样的：\nFROM alpine RUN apk update \u0026amp;\u0026amp; apk add sshfs RUN mkdir -p /run/docker/plugins /mnt/state /mnt/volumes COPY docker-volume-sshfs docker-volume-sshfs CMD [\u0026#34;docker-volume-sshfs\u0026#34;] 实际上是编译好的可执行文件复制到 alpine linux 容器中运行。\n编译 rootfsimage 镜像的过程。\ndocker build -t rootfsimage . Sending build context to Docker daemon 11.71 MB Step 1/5 : FROM alpine ---\u0026gt; 4a415e366388 Step 2/5 : RUN apk update \u0026amp;\u0026amp; apk add sshfs ---\u0026gt; Running in 1551ecc1c847 fetch http://dl-cdn.alpinelinux.org/alpine/v3.5/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.5/community/x86_64/APKINDEX.tar.gz v3.5.2-2-ge626ce8c3c [http://dl-cdn.alpinelinux.org/alpine/v3.5/main] v3.5.1-71-gc7bb9a04f0 [http://dl-cdn.alpinelinux.org/alpine/v3.5/community] OK: 7959 distinct packages available (1/10) Installing openssh-client (7.4_p1-r0) (2/10) Installing fuse (2.9.7-r0) (3/10) Installing libffi (3.2.1-r2) (4/10) Installing libintl (0.19.8.1-r0) (5/10) Installing libuuid (2.28.2-r1) (6/10) Installing libblkid (2.28.2-r1) (7/10) Installing libmount (2.28.2-r1) (8/10) Installing pcre (8.39-r0) (9/10) Installing glib (2.50.2-r0) (10/10) Installing sshfs (2.8-r0) Executing busybox-1.25.1-r0.trigger Executing glib-2.50.2-r0.trigger OK: 11 MiB in 21 packages ---\u0026gt; 1a73c501f431 Removing intermediate container 1551ecc1c847 Step 3/5 : RUN mkdir -p /run/docker/plugins /mnt/state /mnt/volumes ---\u0026gt; Running in 032af3b2595a ---\u0026gt; 30c7e8463e96 Removing intermediate container 032af3b2595a Step 4/5 : COPY docker-volume-sshfs docker-volume-sshfs ---\u0026gt; a924c6fcc1e4 Removing intermediate container ffc5e3c97707 Step 5/5 : CMD docker-volume-sshfs ---\u0026gt; Running in 0dc938fe4f4e ---\u0026gt; 0fd2e3d94860 Removing intermediate container 0dc938fe4f4e Successfully built 0fd2e3d94860 编写config.json文档\n{ \u0026#34;description\u0026#34;: \u0026#34;sshFS plugin for Docker\u0026#34;, \u0026#34;documentation\u0026#34;: \u0026#34;https://docs.docker.com/engine/extend/plugins/\u0026#34;, \u0026#34;entrypoint\u0026#34;: [ \u0026#34;/docker-volume-sshfs\u0026#34; ], \u0026#34;env\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;DEBUG\u0026#34;, \u0026#34;settable\u0026#34;: [ \u0026#34;value\u0026#34; ], \u0026#34;value\u0026#34;: \u0026#34;0\u0026#34; } ], \u0026#34;interface\u0026#34;: { \u0026#34;socket\u0026#34;: \u0026#34;sshfs.sock\u0026#34;, \u0026#34;types\u0026#34;: [ \u0026#34;docker.volumedriver/1.0\u0026#34; ] }, \u0026#34;linux\u0026#34;: { \u0026#34;capabilities\u0026#34;: [ \u0026#34;CAP_SYS_ADMIN\u0026#34; ], \u0026#34;devices\u0026#34;: [ { \u0026#34;path\u0026#34;: \u0026#34;/dev/fuse\u0026#34; } ] }, \u0026#34;mounts\u0026#34;: [ { \u0026#34;destination\u0026#34;: \u0026#34;/mnt/state\u0026#34;, \u0026#34;options\u0026#34;: [ \u0026#34;rbind\u0026#34; ], \u0026#34;source\u0026#34;: \u0026#34;/var/lib/docker/plugins/\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bind\u0026#34; } ], \u0026#34;network\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host\u0026#34; }, \u0026#34;propagatedmount\u0026#34;: \u0026#34;/mnt/volumes\u0026#34; } 该插件使用 host 网络类型，使用/run/docker/plugins/sshfs.sock 接口与 docker engine 通信。\n注意官网上的这个文档有问题，config.json 与代码里的不符，尤其是 Entrypoint 的二进制文件的位置不对。\n注意socket配置的地址不要写详细地址，默认会在/run/docker/plugins 目录下生成 socket 文件。 …","relpermalink":"/blog/docker-plugin-develop/","summary":"看了文章后你可能会觉得，官网上的可能是个假例子。","title":"Docker 17.03-CE 插件开发案例"},{"content":"继续上一篇Docker17.03-CE 插件开发的🌰 ，今天来看下docker create plugin的源码。\ncli/command/plugin/create.go\nDocker 命令行docker plugin create调用的，使用的是cobra ，这个命令行工具开发包很好用，推荐下。\n执行这两个函数\nfunc newCreateCommand(dockerCli *command.DockerCli) *cobra.Command //调用下面的函数，拼装成 URL 调用 RESTful API 接口 func runCreate(dockerCli *command.DockerCli, options pluginCreateOptions) error { ... if err = dockerCli.Client().PluginCreate(ctx, createCtx, createOptions); err != nil { return err } ... } 我们再看下下面的这个文件：\napi/server/router/plugin/plugin_routes.go func (pr *pluginRouter) createPlugin(ctx context.Context, w http.ResponseWriter, r *http.Request, vars map[string]string) error { ... if err := pr.backend.CreateFromContext(ctx, r.Body, options); err != nil { return err } ... } createPlugin这个方法定义在api/server/route/plugin/backen.go的Backend接口中。\nPluginCreate这个方法定义在docker/docker/client/Interface.go的PluginAPIClient接口中。\ndocker/client/plugin_create.go\n// PluginCreate creates a plugin func (cli *Client) PluginCreate(ctx context.Context, createContext io.Reader, createOptions types.PluginCreateOptions) error { headers := http.Header(make(map[string][]string)) headers.Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/x-tar\u0026#34;) query := url.Values{} query.Set(\u0026#34;name\u0026#34;, createOptions.RepoName) resp, err := cli.postRaw(ctx, \u0026#34;/plugins/create\u0026#34;, query, createContext, headers) if err != nil { return err } ensureReaderClosed(resp) return err } plugin 在后端接收到请求后会执行下面的方法。最终create plugin的实现在plugin/backend_linux.go下：\n// CreateFromContext creates a plugin from the given pluginDir which contains // both the rootfs and the config.json and a repoName with optional tag. func (pm *Manager) CreateFromContext(ctx context.Context, tarCtx io.ReadCloser, options *types.PluginCreateOptions) (err error) {} 至于 docker create plugin 时 docker 后台究竟做了什么，就看👆那个文件。\n","relpermalink":"/blog/docker-create-plugin/","summary":"今天来看下 docker create plugin 的源码。","title":"Docker 17.03-CE create plugin 源码解析"},{"content":"最近在看 《微服务设计（Sam Newman 著）》 这本书。作者是 ThoughtWorks 的 Sam Newman。这本书中包括很多业界是用案例，比如 Netflix 和 亚马逊。有兴趣的话大家一起看看讨论一下。😄\nP.S 这本书比较偏理论，另外还有一本中国人写的书，《微服务架构与实践，王磊著，电子工业出版社》 。这个人同样也是 ThoughtWorks 的，两个人的观点不谋而合，依然是便理论的东西。\nCloud Native Go - 基于 Go 和 React 的 web 云服务构建指南\n这本书是我最近在翻译的，将由 电子工业出版社 出版，本书根据实际案例教你如何构建一个 web 微服务，是实践为服务架构的很好的参考。查看本书介绍 。\n1.微服务初探 什么是微服务？ 微服务（Microservices）这个词比较新颖，但是其实这种架构设计理念早就有了。微服务是一种分布式架构设计理念，为了推动细粒度服务的使用，这些服务要能协同工作，每个服务都有自己的生命周期。一个微服务就是一个独立的实体，可以独立的部署在 PAAS 平台上，也可以作为一个独立的进程在主机中运行。服务之间通过 API 访问，修改一个服务不会影响其它服务。\n微服务的好处 微服务的好处有很多，包括：\n帮助你更快的采用新技术 解决技术异构的问题，因为是用 API 网络通信，可以使用不同的语言和技术开发不同的服务 增强系统弹性，服务的边界比较清晰，便于故障处理 方便扩展，比如使用容器技术，可以很方便的一次性启动很多个微服务 方便部署，因为微服务之间彼此独立，所以能够独立的部署单个服务而不影响其它服务，如果部署失败的话还可以回滚 别忘了康威定律，微服务可以很好契合解决组织架构问题 可重用，可随意组合 便于维护，可以随时重写服务，不必担心历史遗留问题 与面向服务架构 SOA 的关系 可以说微服务架构师 SOA 的一种，但是目前的大多数 SOA 做的都不好，在通信协议的选择、第三方中间件的选择、服务力度如何划分方面做的都不够好。\n微服务与 SOA 的共同点\n都使用共享库，比如可重用的代码库 模块化，比如 Java 中的 OSGI(Open Source Gateway Initiative)、Erlang 中的模块化 2.架构师的职责 架构师应该关心是什么 架构师（Architect）在英文中和建筑师是同一个词，他们之间也有很多相同之处，架构师构建的是软件，而建筑师构建的是建筑。\n终于看到了我翻译的Cloud Native Go第 14 章中引用的这本书的原话了。\n原话 软件的需求变更是来的那么快来的那么直接，不像建筑那样可以在设计好后按照设计图纸一步步的去建设。\n架构师应该关心的是什么呢？\n保证系统适合开发人员在上面工作 关注服务之间的交互，不需要过于关注各个服务内部发生的事情，比如服务之间互相调用的接口，是使用protocol buffer呢，还是使用RESTful API，还是使用Java RMI，这个才是架构师需要关注的问题，至于服务内部究竟使用什么，那就看开发人员自己了，架构师更需要关注系统的边界和分区。 架构师应该与团队在一起，结对编程 🤓🤓 了解普通工作，知道普通的工作是什么样子，做一个代码架构师 😂 架构师应该做什么 提供原则指导实践，比如 Heroku 的12 因素法则 用来指导 SAAS 应用架构一样，微服务架构设计也要有一套原则。 提供要求标准，通过日志功能和监控对服务进行集中式管理，明确接口标准，提供安全性建议。 代码治理。为开发人员提供范例和服务代码模板。 解决技术债务。 集中治理和领导。维持良好的团队关系，当团队跑偏的时候及时纠正。 3.服务建模 以MusicCorp这家公司的服务为例子讲解。\n服务建模的两个指导原则：\n高内聚：关键是找出问题的边界，把相关的问题放在同一个服务中。 松耦合：修改一个服务不需要修改另一个。 使用限定上下文（一个由显示边界限定的特定指责）的方法将服务拆分，比如 MusicCorp 的服务可以拆分为：\n财务部门 仓库 他们都不需要知道各自的具体实现，只要给它们提供特定的输入就会有你想要的产出。\n过早的将一个系统划分成微服务的代价非常高，尤其是在面对新领域时，将一个已有的代码库划分成微服务会比葱头开始建设微服务要简单的多。\n4.集成 使用共享数据库，为用户创建好接口，可以使用 RPC（protocol buffer、thrift）或者 REST。服务端和客户端消息格式可以用 Json 或 XML。当然每种技术都有各自的适用场景，结合自己的业务选择。\n微服务的协作方式是什么样的呢？基于事件的异步通信，使用消息中间件来实现事件发布和消费者接收机制。比如用 Kafka 或 RabbitMQ。\n5.分解单块系统 分解巨大无比没人感动的单块系统，首先要做的是理清代码库，找到接缝。\n分解系统带来的好处：\n加快以后系统开发速度 划清了团队结构（又是康威定律） 增加安全审计功能后，保障安全性 利于开展新技术 6. 部署 这一块跟传统服务的部署并没有太大的不同，无非是微服务的短平快，加快了 CI（持续集成）的速度。如果将微服务打包为 docker 镜像，使用 Jenkins、ansible、puppet 等技术来部署微服务可以实现部署自动和效率的显著提高。\n其它 该书的后面还讲了测试、监控、安全、康威定律、最后还上升到人本，给予广大的软件开发人员强烈的人文关怀，可见提倡架构师要融入团队，最一个代码架构师和结对编程的作者是多么博爱❤️。\n该书的核心部分是第 11 章规模化微服务，为将在下篇中来探讨一下。\n","relpermalink":"/blog/microservice-reading-notes/","summary":"这本书中包括很多业界是用案例，比如 Netflix 和亚马逊。有兴趣的话大家一起看看讨论一下。","title":"微服务设计读书笔记"},{"content":"本文是Docker v.s Kubernetes 系列第二篇，续接上文Docker 对比 Kuberntes 第一部分 。\nKubernetes 是典型的Master/Slave架构模式，本文简要的介绍 kubenetes 的架构和组件构成。\nKubernetes 核心架构 master 节点 apiserver：作为 kubernetes 系统的入口，封装了核心对象的增删改查操作，以 RESTFul 接口方式提供给外部客户和内部组件调用。它维护的 REST 对象将持久化到 etcd（一个分布式强一致性的 key/value 存储）。 scheduler：负责集群的资源调度，为新建的 Pod 分配机器。这部分工作分出来变成一个组件，意味着可以很方便地替换成其他的调度器。 controller-manager：负责执行各种控制器，目前有两类： endpoint-controller：定期关联 service 和 Pod(关联信息由 endpoint 对象维护)，保证 service 到 Pod 的映射总是最新的。 replication-controller：定期关联 replicationController 和 Pod，保证 replicationController 定义的复制数量与实际运行 Pod 的数量总是一致的。 node 节点 kubelet：负责管控 docker 容器，如启动/停止、监控运行状态等。它会定期从 etcd 获取分配到本机的 Pod，并根据 Pod 信息启动或停止相应的容器。同时，它也会接收 apiserver 的 HTTP 请求，汇报 Pod 的运行状态。 proxy：负责为 Pod 提供代理。它会定期从 etcd 获取所有的 service，并根据 service 信息创建代理。当某个客户 Pod 要访问其他 Pod 时，访问请求会经过本机 proxy 做转发。 master slave 架构 Kubernetes 组件详细介绍 etcd 虽然不是 Kubernetes 的组件但是有必要提一下，etcd 是一个分布式协同数据库，基于 Go 语言开发，CoreOS公司出品，使用 raft 一致性算法协同。Kubernetes 的主数据库，在安装 kubernetes 之前就要先安装它，很多开源下项目都用到，老版本的docker swarm也用到了它。目前主要使用的是2.7.x版本，3.0+版本的 API 变化太大。\nAPIServer APIServer 负责对外提供 kubernetes API 服务，它运行在 master 节点上。任何对资源的增删改查都要交给 APIServer 处理后才能提交给 etcd。APIServer 总体上由两部分组成：HTTP/HTTPS 服务和一些功能性插件。这些功能性插件又分为两种：一部分与底层 IaaS 平台（Cloud Provide）相关；另一部分与资源管理控制（Admission Control）相关。\nScheduler Scheduler 的作用是根据特定的调度算法将 pod 调度到 node 节点上，这一过程也被称为绑定。Scheduler 调度器的输入是待调度的 pod 和可用的工作节点列表，输出则是一个已经绑定了 pod 的节点，这个节点是通过调度算法在工作节点列表中选择的最优节点。\n工作节点从哪里来？工作节点并不是由 Kubernetes 创建，它是由 IaaS 平台创建，或者就是由用户管理的物理机或者虚拟机。但是 Kubernetes 会创建一个 Node 对象，用来描述这个工作节点。描述的具体信息由创建 Node 对象的配置文件给出。一旦用户创建节点的请求被成功处理，Kubernetes 又会立即在内部创建一个 node 对象，再去检查该节点的健康状况。只有那些当前可用的 node 才会被认为是一个有效的节点并允许 pod 调度到上面运行。\n工作节点可以通过资源配置文件或者 kubectl 命令行工具来创建。Kubernetes 主要维护工作节点的两个属性：spec 和 status 来描述一个工作节点的期望状态和当前状态。其中，所谓的当前状态信息由 3 个信息组成：HostIp、NodePhase和Node Condition。\n工作节点的动态维护过程依靠Node Controller来完成，它是Kubernetes Controller Manager下属的一个控制器。它会一直不断的检查 Kubernetes 已知的每台 node 节点是否正常工作，如果一个之前已经失败的节点在这个检查循环中被检查为可以工作的，那么 Node Controller 会把这个节点添加到工作节点中，Node Controller 会从工作节点中删除这个节点。\nController Manager Controller Manager 运行在集群的 Master 节点上，是基于 pod API 的一个独立服务，它重点实现 service Endpoint（服务端点）的动态更新。管理着 Kubernetes 集群中各种控制节点，包括replication Controller和node Controller。\n与 APIServer 相比，APIServer 负责接受用户请求并创建对应的资源，而 Controller Manager 在系统中扮演的角色是在一旁旁默默的管控这些资源，确保他们永远保持在预期的状态。它采用各种管理器定时的对 pod、节点等资源进行预设的检查，然后判断出于预期的是否一致，若不一致，则通知 APIServer 采取行动，比如重启、迁移、删除等。\nkubelet kubelet 组件工作在 Kubernetes 的 node 上，负责管理和维护在这台主机上运行着的所有容器。kubelet 与 cAdvisor 交互来抓取 docker 容器和主机的资源信息。kubelet 垃圾回收机制，包括容器垃圾回收和镜像垃圾回收。kubelet 工作节点状态同步。\nkube-proxy kube-proxy 提供两种功能：\n提供算法将客服端流量负载均衡到 service 对应的一组后端 pod。 使用 etcd 的 watch 机制，实现服务发现功能，维护一张从 service 到 endpoint 的映射关系，从而保证后端 pod 的 IP 变化不会对访问者的访问造成影响。 ","relpermalink":"/blog/docker-vs-kubernetes-part2/","summary":"这一系列文章是对比 kubernetes 和 docker 两者之间的差异。","title":"Docker 对比 Kubernetes 第二部分"},{"content":"前言 这一系列文章是对比 kubernetes 和 docker 两者之间的差异，鉴于我之前从 docker1.10.3 起开始使用 docker，对原生 docker 的了解比较多，最近又正在看《Kunernetes 权威指南（第二版）》这本书（P.S 感谢电子工业出版社的编辑朋友赠送此书）。这系列文章不是为了比较孰优孰劣，适合自己的才是最好的。\n此系列文章中所说的 Docker 指的是 17.03-ce 版本。\n概念性的差别 Kubernetes\n了解一样东西首先要高屋建瓴的了解它的概念，kubernetes 包括以下几种资源对象：\nPod Service Volume Namespace ReplicaSet Deployment StatefulSet DaemonSet Job Docker\nDocker 的资源对象相对于 kubernetes 来说就简单多了，只有以下几个：\nService Node Stack Docker 就这么简单，使用一个 docker-compose.yml 即可以启动一系列服务。当然简单的好处是便于理解和管理，但是在功能方面就没有 kubernetes 那么强大了。\n功能性差别 Kubernetes 资源限制 CPU 100m 千分之一核为单位，绝对值，requests 和 limits，超过这个值可能被杀掉，资源限制力度比 docker 更细。 Pod 中有个最底层的 pause 容器，其他业务容器共用他的 IP，docker 因为没有这层概念，所以没法共用 IP，而是使用 overlay 网络同处于一个网络里来通信。 Kubernetes 在 rc 中使用环境变量传递配置（1.3 版本是这样的，后续版本还没有研究过） Kuberentes Label 可以在开始和动态的添加修改，所有的资源对象都有，这一点 docker 也有，但是资源调度因为没有 kubernetes 那么层级，所有还是相对比较弱一些。 Kubernetes 对象选择机制继续通过 label selector，用于对象调度。 Kubernetes 中有一个比较特别的镜像，叫做 google_containers/pause，这个镜像是用来实现 Pod 概念的。 HPA horizontal pod autoscaling 横向移动扩容，也是一种资源对象，根据负载变化情况针对性的调整 pod 目标副本数。 Kubernetes 中有三个 IP，Node,Pod,Cluster IP 的关系比较复杂，docker 中没有 Cluster IP 的概念。 持久化存储，在 Kubernetes 中有 Persistent volume 只能是网络存储，不属于任何 node，独立于 pod 之外，而 docker 只能使用 volume plugin。 多租户管理，kubernetes 中有 `Namespace，docker 暂时没有多租户管理功能。 总体来说 Docker 架构更加简单，使用起来也没有那么多的配置，只需要每个结点都安装 docker 即可，调度和管理功能没 kubernetes 那么复杂。但是 kubernetes 本身就是一个通用的数据中心管理工具，不仅可以用来管理 docker，pod 这个概念里就可以运行不仅是 docker 了。\n以后的文章中将结合 docker 着重讲 Kubernetes，基于 1.3 版本。\n","relpermalink":"/blog/docker-vs-kubernetes-part1/","summary":"这一系列文章是对比 kubernetes 和 docker 两者之间的差异。","title":"Docker 对比 Kubernetes 第一部分"},{"content":"继续趟昨天挖的坑。\n昨天的issue-776 已经得到@gkvijay 的回复，原来是因为没有安装 contiv/v2plugin 的缘故，所以 create contiv network 失败，我需要自己 build 一个docker plugin。\n查看下这个commit 里面有 build v2plugin的脚本更改，所以直接调用以下命令就可以 build 自己的 v2plugin。\n前提你需要先 build 出netctl、netmaster、netplugin三个二进制文件并保存到bin目录下，如果你没自己 build 直接下载release里面的文件保存进去也行。\n编译 v2plugin 插件 修改 config.json 插件配置文件\n{ \u0026#34;manifestVersion\u0026#34;: \u0026#34;v0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Contiv network plugin for Docker\u0026#34;, \u0026#34;documentation\u0026#34;: \u0026#34;https://contiv.github.io\u0026#34;, \u0026#34;entrypoint\u0026#34;: [\u0026#34;/startcontiv.sh\u0026#34;], \u0026#34;network\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host\u0026#34; }, \u0026#34;env\u0026#34;: [ { \u0026#34;Description\u0026#34;: \u0026#34;To enable debug mode, set to \u0026#39;-debug\u0026#39;\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;dbg_flag\u0026#34;, \u0026#34;Settable\u0026#34;: [ \u0026#34;value\u0026#34; ], \u0026#34;Value\u0026#34;: \u0026#34;-debug\u0026#34; }, { \u0026#34;Description\u0026#34;: \u0026#34;VLAN uplink interface used by OVS\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;iflist\u0026#34;, \u0026#34;Settable\u0026#34;: [ \u0026#34;value\u0026#34; ], \u0026#34;Value\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;Description\u0026#34;: \u0026#34;Etcd or Consul cluster store url\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;cluster_store\u0026#34;, \u0026#34;Settable\u0026#34;: [ \u0026#34;value\u0026#34; ], \u0026#34;Value\u0026#34;: \u0026#34;etcd://172.20.0.113:2379\u0026#34; }, { \u0026#34;Description\u0026#34;: \u0026#34;Local IP address to be used by netplugin for control communication\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;ctrl_ip\u0026#34;, \u0026#34;Settable\u0026#34;: [ \u0026#34;value\u0026#34; ], \u0026#34;Value\u0026#34;: \u0026#34;none\u0026#34; }, { \u0026#34;Description\u0026#34;: \u0026#34;Local VTEP IP address to be used by netplugin\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;vtep_ip\u0026#34;, \u0026#34;Settable\u0026#34;: [ \u0026#34;value\u0026#34; ], \u0026#34;Value\u0026#34;: \u0026#34;none\u0026#34; }, { \u0026#34;Description\u0026#34;: \u0026#34;In \u0026#39;master\u0026#39; role, plugin runs netmaster and netplugin\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;plugin_role\u0026#34;, \u0026#34;Settable\u0026#34;: [ \u0026#34;value\u0026#34; ], \u0026#34;Value\u0026#34;: \u0026#34;master\u0026#34; }, { \u0026#34;Description\u0026#34;: \u0026#34;Netmaster url to listen http requests on\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;listen_url\u0026#34;, \u0026#34;Settable\u0026#34;: [ \u0026#34;value\u0026#34; ], \u0026#34;Value\u0026#34;: \u0026#34;172.20.0.113:9999\u0026#34; }, { \u0026#34;Description\u0026#34;: \u0026#34;Network Driver name for requests to dockerd. Should be same as name:tag of the plugin\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;plugin_name\u0026#34;, \u0026#34;Settable\u0026#34;: [ \u0026#34;value\u0026#34; ], \u0026#34;Value\u0026#34;: \u0026#34;contiv/v2plugin:latest\u0026#34; } ], \u0026#34;mounts\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;bind\u0026#34;, \u0026#34;options\u0026#34;: [\u0026#34;rbind\u0026#34;], \u0026#34;source\u0026#34;: \u0026#34;/etc/openvswitch\u0026#34;, \u0026#34;destination\u0026#34;: \u0026#34;/etc/openvswitch\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;bind\u0026#34;, \u0026#34;options\u0026#34;: [\u0026#34;rbind\u0026#34;], \u0026#34;source\u0026#34;: \u0026#34;/var/log/openvswitch\u0026#34;, \u0026#34;destination\u0026#34;: \u0026#34;/var/log/openvswitch\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;bind\u0026#34;, \u0026#34;options\u0026#34;: [\u0026#34;rbind\u0026#34;], \u0026#34;source\u0026#34;: \u0026#34;/var/run\u0026#34;, \u0026#34;destination\u0026#34;: \u0026#34;/var/run\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;bind\u0026#34;, \u0026#34;options\u0026#34;: [\u0026#34;rbind\u0026#34;], \u0026#34;source\u0026#34;: \u0026#34;/lib/modules\u0026#34;, \u0026#34;destination\u0026#34;: \u0026#34;/lib/modules\u0026#34; } ], \u0026#34;interface\u0026#34; : { \u0026#34;types\u0026#34;: [\u0026#34;docker.networkdriver/1.0\u0026#34;, \u0026#34;docker.ipamdriver/1.0\u0026#34;], \u0026#34;socket\u0026#34;: \u0026#34;netplugin.sock\u0026#34; }, \u0026#34;Linux\u0026#34;: { \u0026#34;Capabilities\u0026#34;: [\u0026#34;CAP_SYS_ADMIN\u0026#34;, \u0026#34;CAP_NET_ADMIN\u0026#34;, \u0026#34;CAP_SYS_MODULE\u0026#34;] } } 关于docker plugin v2配置文件的说明 方法一\n自动化 make\n$make host-pluginfs-create 方法二\n直接调用 Makefile 里指定的那个 shell 脚本scripts/v2plugin_rootfs.sh。\n$bash scripts/v2plugin_rootfs Creating rootfs for v2plugin , sed: 1: \u0026#34;install/v2plugin/config ...\u0026#34;: command i expects \\ followed by text Sending build context to Docker daemon 73.94 MB Step 1/5 : FROM alpine:3.5 ---\u0026gt; 4a415e366388 Step 2/5 : MAINTAINER Cisco Contiv (http://contiv.github.io/) ---\u0026gt; Running in fada1677341b ---\u0026gt; f0440792dff6 Removing intermediate container fada1677341b Step 3/5 : RUN mkdir -p /run/docker/plugins /etc/openvswitch /var/run/contiv/log \u0026amp;\u0026amp; echo \u0026#39;http://dl-cdn.alpinelinux.org/alpine/v3.4/main\u0026#39; \u0026gt;\u0026gt; /etc/apk/repositories \u0026amp;\u0026amp; apk update \u0026amp;\u0026amp; apk add openvswitch=2.5.0-r0 iptables ---\u0026gt; Running in 2ae2fbee6834 fetch http://dl-cdn.alpinelinux.org/alpine/v3.5/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.5/community/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.4/main/x86_64/APKINDEX.tar.gz v3.5.2-3-g3649125268 [http://dl-cdn.alpinelinux.org/alpine/v3.5/main] v3.5.1-71-gc7bb9a04f0 [http://dl-cdn.alpinelinux.org/alpine/v3.5/community] v3.4.6-81-g1f1f409 [http://dl-cdn.alpinelinux.org/alpine/v3.4/main] OK: 13194 distinct packages available (1/6) Installing libmnl (1.0.4-r0) (2/6) Installing libnftnl-libs (1.0.7-r0) (3/6) Installing iptables (1.6.0-r0) (4/6) Installing libcrypto1.0 (1.0.2k-r0) (5/6) Installing libssl1.0 (1.0.2k-r0) (6/6) Installing openvswitch (2.5.0-r0) Executing busybox-1.25.1-r0.trigger OK: 19 MiB in 17 packages ---\u0026gt; b130141ad660 Removing intermediate container 2ae2fbee6834 Step 4/5 : COPY netplugin netmaster netctl startcontiv.sh / ---\u0026gt; 2b88b2f8e5e7 Removing intermediate container d7580a394c64 Step 5/5 : ENTRYPOINT /startcontiv.sh ---\u0026gt; Running in e6fc5c887cb3 ---\u0026gt; 1c569e4c633d Removing intermediate container e6fc5c887cb3 Successfully built 1c569e4c633d Password: 03d60dc01488362156f98a062d17af7a34e4b17569c2fe4f5d2048d619860314 Untagged: contivrootfs:latest Deleted: sha256:1c569e4c633d27bd3e79d9d30b2825ce57452d30f90a3452304b932835331b13 Deleted: sha256:2b88b2f8e5e7bae348bf296f6254662c1d444760db5acd1764b9c955b106adad Deleted: sha256:b60594671dc9312bf7ba73bf17abb9704d2b0d0e802c0d990315c5b4a5ca11fe Deleted: sha256:b130141ad660d4ee291d9eb9a1e0704c4bc009fc91a73de28e8fd110aa45c481 Deleted: sha256:ab3c02d5a171681ba00d27f2c456cf8b63eeeaf408161dc84d9d89526d0399de Deleted: sha256:f0440792dff6a89e321cc5d34ecaa21b4cb993f0c4e4df6c2b04eef8878bb471 创建镜像这一步需要输入你的 docker hub 密码。而且 alpine 下载软件需要翻墙的。打包 v2plugin 目录需要使用 sudo，不然会报一个错。\n整个插件打包压缩后的大小是 91M。现在rootfs和config.json都已经有了，就可以在你自己的系统上 create docker plugin 了。\n启动 contiv plugin 创建 docker network plugin 并 enable。\n$docker plugin create …","relpermalink":"/blog/contiv-v2plugin/","summary":"继续趟昨天挖的坑。","title":"Contiv 入坑指南-v2plugin"},{"content":"Contiv是思科开发的 docker 网络插件，从 2015 年就开源了，业界通常拿它和 Calico 比较。貌似 Contiv 以前还开发过 volume plugin，现在销声匿迹了，只有 netplugin 仍在活跃开发。\n容器网络插件 Calico 与 Contiv Netplugin 深入比较 。\n还有篇文章讲解了docker 网络方案的改进 。\nContiv Netplugin 简介 Contiv Netplugin 是来自思科的解决方案。编程语言为 Go。它基于 OpenvSwitch，以插件化的形式支持容器访问网络，支持 VLAN，Vxlan，多租户，主机访问控制策略等。作为思科整体支持容器基础设施 contiv 项目的网络部分，最大的亮点在于容器被赋予了 SDN 能力，实现对容器更细粒度，更丰富的访问控制功能。另外，对 Docker CNM 网络模型的支持，并内置了 IPAM 接口，不仅仅提供了一容器一 IP，而且容器的网络信息被记录的容器配置中，伴随着容器的整个生命周期，减低因为状态不同步造成网络信息丢失的风险。有别于 CNI，这种内聚化的设计有利于减少对第三方模块的依赖。随着项目的发展，除了 Docker，还提供了对 Kubernetes 以及 Mesos 的支持，即 CNI 接口。\nnetplugin 架构 Netmaster 后台进程负责记录所有节点状态，保存网络信息，分配 IP 地址 Netplugin 后台进程作为每个宿主机上的 Agent 与 Docker 及 OVS 通信，处理来自 Docker 的请求，管理 OVS。Docker 方面接口为 remote driver，包括一系列 Docker 定义的 JSON-RPC(POST) 消息。OVS 方面接口为 remote ovsdb，也是 JSON-RPC 消息。以上消息都在 localhost 上处理。 集群管理依赖 etcd/serf netplugin 代码结构 Netplugin 的优势 较早支持 CNM 模型。与已有的网络基础设施兼容性较高，改造影响小。基于 VLAN 的平行扩展与现有网络结构地位对等 SDN 能力，能够对容器的网络访问做更精细的控制 多租户支持，具备未来向混合云/公有云迁移的潜力 代码规模不大，逻辑结构清晰，并发好，VLAN 在公司内部有开发部署运维实践经验，稳定性经过生产环境验证 京东基于相同的技术栈（OVS + VLAN）已支持 10w+ 容器的运行。 Next 后续文章会讲解 contiv netplugin 的环境配置和开发。目前还在 1.0-beta 版本。Docker store上提供了 contiv 插件的下载地址 。\n","relpermalink":"/blog/contiv-guide/","summary":"Contiv 是思科开发的 docker 网络插件，从 2015 年就开源了，业界通常拿它和 Calico 比较。","title":"思科开源 docker 网络插件 Contiv 简介"},{"content":"起源 久闻Vagrant大名，之前经常看到有开源项目使用它作为分布式开发的环境配置。\n因为今天在看contiv 正好里面使用 vagrant 搭建的开发测试环境，所以顺便了解下。它的Vagrantfile 文件中定义了三台主机。并安装了很多依赖软件，如 consul、etcd、docker、go 等，整的比较复杂。\n➜ netplugin git:(master) ✗ vagrant status Current machine states: netplugin-node1 running (virtualbox) netplugin-node2 running (virtualbox) netplugin-node3 running (virtualbox) This environment represents multiple VMs. The VMs are all listed above with their current state. For more information about a specific VM, run `vagrant status NAME`. Vagrant 是hashicorp 这家公司的产品，这家公司主要做数据中心 PAAS 和虚拟化，其名下大名鼎鼎的产品有Consul、Vault、Nomad、Terraform。他们的产品都是基于Open Source的Github 地址 。\n用途 Vagrant 是用来管理虚拟机的，如 VirtualBox、VMware、AWS 等，主要好处是可以提供一个可配置、可移植和复用的软件环境，可以使用 shell、chef、puppet 等工具部署。所以 vagrant 不能单独使用，如果你用它来管理自己的开发环境的话，必须在自己的电脑里安装了虚拟机软件，我使用的是virtualbox。\nVagrant 提供一个命令行工具vagrant，通过这个命令行工具可以直接启动一个虚拟机，当然你需要提前定义一个 Vagrantfile 文件，这有点类似 Dockerfile 之于 docker 了。\n跟 docker 类比这来看 vagrant 就比较好理解了，vagrant 也是用来提供一致性环境的，vagrant 本身也提供一个镜像源，使用vagrant init hashicorp/precise64就可以初始化一个 Ubuntu 12.04 的镜像。\n用法 你可以下载安装文件来安装 vagrant，也可以使用 RubyGem 安装，它是用 Ruby 开发的。\nVagrantfile\nVagrantfile 是用来定义 vagrant project 的，使用 ruby 语法，不过你不必了解 ruby 就可以写一个 Vagrantfile。\n看个例子，选自 https://github.com/fenbox/Vagrantfile # -*- mode: ruby -*- # vi: set ft=ruby : # All Vagrant configuration is done below. The \u0026#34;2\u0026#34; in Vagrant.configure # configures the configuration version (we support older styles for # backwards compatibility). Please don\u0026#39;t change it unless you know what # you\u0026#39;re doing. Vagrant.configure(\u0026#34;2\u0026#34;) do |config| # The most common configuration options are documented and commented below. # For a complete reference, please see the online documentation at # https://docs.vagrantup.com. # Every Vagrant development environment requires a box. You can search for # boxes at https://atlas.hashicorp.com/search. config.vm.box = \u0026#34;ubuntu/trusty64\u0026#34; # Disable automatic box update checking. If you disable this, then # boxes will only be checked for updates when the user runs # `vagrant box outdated`. This is not recommended. # config.vm.box_check_update = false # Create a forwarded port mapping which allows access to a specific port # within the machine from a port on the host machine. In the example below, # accessing \u0026#34;localhost:8080\u0026#34; will access port 80 on the guest machine. # config.vm.network \u0026#34;forwarded_port\u0026#34;, guest: 80, host: 8080 # Create a private network, which allows host-only access to the machine # using a specific IP. config.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.10\u0026#34; # Create a public network, which generally matched to bridged network. # Bridged networks make the machine appear as another physical device on # your network. # config.vm.network \u0026#34;public_network\u0026#34; # Share an additional folder to the guest VM. The first argument is # the path on the host to the actual folder. The second argument is # the path on the guest to mount the folder. And the optional third # argument is a set of non-required options. # config.vm.synced_folder \u0026#34;../data\u0026#34;, \u0026#34;/vagrant_data\u0026#34; # Provider-specific configuration so you can fine-tune various # backing providers for Vagrant. These expose provider-specific options. # Example for VirtualBox: # # config.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| # # Display the VirtualBox GUI when booting the machine # vb.gui = true # # # Customize the amount of memory on the VM: # vb.memory = \u0026#34;1024\u0026#34; # end # # View the documentation for the provider you are using for more # information on available options. # Define a Vagrant Push strategy for pushing to Atlas. Other push strategies # such as FTP and Heroku are also available. See the documentation at # https://docs.vagrantup.com/v2/push/atlas.html for more information. # config.push.define \u0026#34;atlas\u0026#34; do |push| # push.app = \u0026#34;YOUR_ATLAS_USERNAME/YOUR_APPLICATION_NAME\u0026#34; # end # Enable provisioning with a shell script. Additional provisioners such as # Puppet, Chef, Ansible, Salt, and Docker are also available. Please see the # documentation for more information about their specific syntax and use. # config.vm.provision \u0026#34;shell\u0026#34;, inline: \u0026lt;\u0026lt;-SHELL # apt-get update # apt-get install -y apache2 # SHELL config.vm.provision :shell, path: \u0026#34;bootstrap.sh\u0026#34; end Boxes\nVagrant 的基础镜像，相当于 docker images。可以在这些基础镜像的基础上制作自己的虚拟机镜像。\n添加一个 box\n$ vagrant box add hashicorp/precise64 在 Vagrantfile 中指定 box\nVagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.box = \u0026#34;hashicorp/precise64\u0026#34; config.vm.box_version = \u0026#34;1.1.0\u0026#34; end 使用 ssh 进入 vagrant\nvagrant up后就可以用vagrant ssh $name进入虚拟机内，如果主机上就一个 vagrant 可以不指定名字。默认进入的用户是 vagrant。\n文件同步\nvagrant up后在虚拟机中会有一个/vagrant目录，这跟你定义Vagrantfile是同一级目录。\n这个目录跟你宿主机上的目录文件是同步的。\n软件安装\n在 Vagrantfile 中定义要安装的软件和操作。\n例如安装 apache\n在与 Vagrantfile 同级的目录下创建一个bootstrap.sh文件。\n#!/usr/bin/env bash apt-get update apt-get install -y apache2 if ! [ -L /var/www ]; then rm -rf /var/www ln -fs /vagrant /var/www fi 然后在 Vagrantfile 中使用它。\nVagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.box = \u0026#34;hashicorp/precise64\u0026#34; config.vm.box_version = \u0026#34;1.1.0\u0026#34; end 网络\n端口转发\nVagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.box = …","relpermalink":"/blog/vagrant-intro/","summary":"我在一年内使用 Vagrant 的心路历程。","title":"Vagrant 从使用到放弃再到掌握完全指南"},{"content":"回顾历史 多少次我回过头看看走过的路，你还在小村旁。\n去年基于 docker1.11 对 Hadoop yarn 进行了 docker 化改造，我将这个项目命名为 magpie ，因为它就像是喜鹊一样收集着各种各样的资源搭建自己的小窝。magpie 还是有很多事情可以做的，大数据集群的虚拟化也不会止步，它仅仅是对其做了初步的探索，对于资源利用率和管理方面的优化还有很长的路要走，Yarn 本身就是做为大数据集群的资源管理调度角色出现的，一开始是为调度 MapReduce，后来的 spark、hive、tensrflow、slide 等等不一而足陆续出现。但是用它来管理 docker 似乎还是有点过重，还不如用 kubernetes、marathon、nomad、swarm 等。\n但是在微服务方面 docker1.11 的很多弊端或者说缺点就暴露了出来，首先 docker1.11 原生并不带 cluster 管理，需要配合 docker swarm、kubernetes、marathon 等才能管理 docker 集群。之前的对于 docker 的使用方式基本就是按照虚拟机的方式使用的，固定 IP 有悖于微服务的原则。\n我们基于 docker1.11 和 shrike 二层网络模式，还有 shipyard 来做集群管理，shipyard 只是一个简单的 docker 集群管理的 WebUI，基本都是调用 docker API，唯一做了一点 docker 原生没有的功能就是 scale 容器，而且只支持到 docker1.11，早已停止开发。我抛弃了 shipyard，它的页面功能基本可有可无，我自己开发的 magpie 一样可以管理 yarn on docker 集群。\nDocker Swarm 有如下几个缺点\n对于大规模集群的管理效率太低，当管理上百个 node 的时候经常出现有节点状态不同步的问题，比如主机重启后容器已经 Exited 了，但是 master 让然认为是 Running 状态，必须重启所有 master 节点才行。 没有中心化 Node 管理功能，必须登录到每台 node 上手动启停 swarm-agent。 集群管理功能实在太太太简陋，查看所有 node 状态只能用 docker info 而且那个格式就不提了，shipyard 里有处理这个格式的代码，我 copy 到了 magpie 里，彻底抛弃 shipyard 了。 Docker swarm 的集群管理概念缺失，因为 docker 一开始设计的时候就不是用来管理集群的，所以出现了 swarm，但是只能使用 docker-compose 来编排服务，但是无法在 swarm 集群中使用我们自定义的 mynet 网络，compose issue-4233 ，compose 也已经被 docker 官方废弃（最近一年 docker 发展的太快了，原来用 python 写的 compose 已经被用 go 重构为 libcompose 直接集成到 swarm mode 里了），而且 docker1.11 里也没有像 kubernetes 那样 service 的单位，在 docker1.11 所有的管理都是基于 docker 容器的。 Docker Swarm 的问题也是 shipyard 的问题，谁让 shipyard 直接调用 docker 的 API 呢。当然，在后续版本的 docker 里以上问题都已经不是问题，docker 已经越来越像 kubernetes，不论是在设计理念上还是在功能上，甚至还发行了企业版，以后每个月发布一个版本。\n技术选型 主要对比 Docker1.11 和 Docker17.03-ce 版本。\n首先有一点需要了解的是，docker1.12 + 带来的 swarm mode ，你可以使用一个命令直接启动一个复杂的 stack ，其中包括了服务编排和所有的服务配置，这是一个投票应用的例子 。\n下表对比了 docker1.11 和 docker17.03-ce\n版本 docker1.11 docker17.03-ce 基本单位 docker 容器 docker 容器、service、stack 服务编排 compose，不支持 docker swarm 的 mynet 网络 改造后的 compose，支持 stack 中完整的服务编排 网络模型 Host、bridge、overlay、mynet 默认支持跨主机的 overlay 网络，创建单个容器时也可以 attach 到已有的 overla 网络中 插件 没有插件管理命令，但是可以手动创建和管理 有插件管理命令，可以手动创建和从 docker hub 中下载，上传插件到自己的私有镜像仓库 升级 不支持平滑升级，重启 docker 原来的容器也会停掉 可以停止 docker engine 但不影响已启动的容器 弹性伸缩 不支持 service 内置功能 服务发现 监听 docker event 增删 DNS 内置服务发现，根据 DNS 负载均衡 节点管理 手动启停 中心化管理 node 节点 服务升级 手动升级 service 内置功能 负载均衡 本身不支持 Swarm mode 内部 DNS 轮寻 基于以上对比，使用 docker17.03-ce 不仅可以兼容以前的 mynet 网络模式，只需要重构以前的 shrike 为 docker plugin，在创建 service 的时候指定为 mynet 即可。也可以同时使用 docker mode 的 overlay 网络，而且还可以安装其它 docker plugin 首先更高级网络和 volume 功能。\nDocker17.03-ce 借鉴了很多 kubernetes 的设计理念，docker 发力企业级市场，相信新版的才符合微服务的方向，既能兼容以前的虚拟机式的使用模式，也能兼容微服务架构。\n下一步 之前考虑过使用 docker1.11 + compose + shipyard + eureka + nginx 等做微服务架构，但是考虑到最新版 docker 的重大升级，从长远的眼光来看，不能一直限定于之前的那一套，我更倾向于新版本。\n调研 Docker17.03-ce 的新特性，尤其是服务治理方面 结合具体业务试用 重构 shrike 为 docker plugin Don’t speak, I’ll try to save us from ourselves\nIf were going down, we’re going down in flames\n","relpermalink":"/blog/docker-tech-selection/","summary":"本文讲述如何进行 Docker 的版本选择。","title":"如何选择 Docker 版本？"},{"content":"看了下网上其他人写的 docker 开发环境搭建，要么是在 ubuntu 下搭建，要么就是使用官方说明的 build docker-dev 镜像的方式一步步搭建的，甚是繁琐，docker hub 上有一个 docker 官方推出的 dockercore/docker 镜像，其实这就是官网上所说的 docker-dev 镜像，不过以前的那个 deprecated 了，使用目前这个镜像搭建 docker 开发环境是最快捷的了。\n想要修改 docker 源码和做 docker 定制开发的同学可以参考下。\n官方指导文档 设置 docker 开发环境 docker 的编译实质上是在 docker 容器中运行 docker。\n因此在本地编译 docker 的前提是需要安装了 docker，还需要用 git 把代码 pull 下来。\n创建分支 为了方便以后给 docker 提交更改，我们从 docker 官方 fork 一个分支。\ngit clone https://github.com/rootsongjc/docker.git git config --local user.name \u0026#34;Jimmy Song\u0026#34; git config --local user.email \u0026#34;rootsongjc@gmail.com\u0026#34; git remote add upstream https://github.com/docker/docker.git git config --local -l git remote -v git checkout -b dry-run-test touch TEST.md vim TEST.md git status git add TEST.md git commit -am \u0026#34;Making a dry run test.\u0026#34; git push --set-upstream origin dry-run-test 然后就可以在 dry-run-test 这个分支下工作了。\n配置 docker 开发环境 官网 上说需要先清空自己电脑上已有的容器和镜像。\ndocker 开发环境本质上是创建一个 docker 镜像，镜像里包含了 docker 的所有开发运行环境，本地代码通过挂载的方式放到容器中运行，下面这条命令会自动创建这样一个镜像。\n在 dry-run-test 分支下执行\nmake BIND_DIR=. shell 该命令会自动编译一个 docker 镜像，From debian:jessie。这一步会上网下载很多依赖包，速度比较慢。如果翻不了墙的话肯定都会失败。因为需要下载的软件和安装包都是在国外服务器上，不翻墙根本就下载不下来，为了不用这么麻烦，推荐直接使用 docker 官方的 dockercore/docker 镜像，也不用以前的 docker-dev 镜像，那个造就废弃了。这个镜像大小有 2.31G。\ndocker pull dockercore/docker 使用方法见 docker hub 然后就可以进入到容器里\ndocker run --rm -i --privileged -e BUILDFLAGS -e KEEPBUNDLE -e DOCKER_BUILD_GOGC -e DOCKER_BUILD_PKGS -e DOCKER_CLIENTONLY -e DOCKER_DEBUG -e DOCKER_EXPERIMENTAL -e DOCKER_GITCOMMIT -e DOCKER_GRAPHDRIVER=devicemapper -e DOCKER_INCREMENTAL_BINARY -e DOCKER_REMAP_ROOT -e DOCKER_STORAGE_OPTS -e DOCKER_USERLANDPROXY -e TESTDIRS -e TESTFLAGS -e TIMEOUT -v \u0026#34;/Users/jimmy/Workspace/github/rootsongjc/docker/bundles:/go/src/github.com/docker/docker/bundles\u0026#34; -t \u0026#34;dockercore/docker:latest\u0026#34; bash 按照官网的说明 make 会报错\nroot@f2753f78bb6d:/go/src/github.com/docker/docker# ./hack/make.sh binary error: .git directory missing and DOCKER_GITCOMMIT not specified Please either build with the .git directory accessible, or specify the exact (--short) commit hash you are building using DOCKER_GITCOMMIT for future accountability in diagnosing build issues. Thanks! 这是一个 issue-27581 ，解决方式就是在 make 的时候手动指定 DOCKER_GITCOMMIT。\nroot@f2753f78bb6d:/go/src/github.com/docker/docker# DOCKER_GITCOMMIT=3385658 ./hack/make.sh binary ---\u0026gt; Making bundle: binary (in bundles/17.04.0-dev/binary) Building: bundles/17.04.0-dev/binary-client/docker-17.04.0-dev Created binary: bundles/17.04.0-dev/binary-client/docker-17.04.0-dev Building: bundles/17.04.0-dev/binary-daemon/dockerd-17.04.0-dev Created binary: bundles/17.04.0-dev/binary-daemon/dockerd-17.04.0-dev Copying nested executables into bundles/17.04.0-dev/binary-daemon bundles 目录下会生成如下文件结构\n. ├── 17.04.0-dev │ ├── binary-client │ │ ├── docker -\u0026gt; docker-17.04.0-dev │ │ ├── docker-17.04.0-dev │ │ ├── docker-17.04.0-dev.md5 │ │ └── docker-17.04.0-dev.sha256 │ └── binary-daemon │ ├── docker-containerd │ ├── docker-containerd-ctr │ ├── docker-containerd-ctr.md5 │ ├── docker-containerd-ctr.sha256 │ ├── docker-containerd-shim │ ├── docker-containerd-shim.md5 │ ├── docker-containerd-shim.sha256 │ ├── docker-containerd.md5 │ ├── docker-containerd.sha256 │ ├── docker-init │ ├── docker-init.md5 │ ├── docker-init.sha256 │ ├── docker-proxy │ ├── docker-proxy.md5 │ ├── docker-proxy.sha256 │ ├── docker-runc │ ├── docker-runc.md5 │ ├── docker-runc.sha256 │ ├── dockerd -\u0026gt; dockerd-17.04.0-dev │ ├── dockerd-17.04.0-dev │ ├── dockerd-17.04.0-dev.md5 │ └── dockerd-17.04.0-dev.sha256 └── latest -\u0026gt; 17.04.0-dev 4 directories, 26 files 现在可以将 docker-daemon 和 docker-client 目录下的 docker 可以执行文件复制到容器的 /usr/bin/ 目录下了。\n启动 docker deamon\ndocker daemon -D\u0026amp; 检查下 docker 是否可用\nroot@f2753f78bb6d:/go/src/github.com/docker/docker/bundles/17.04.0-dev# docker version DEBU[0048] Calling GET /_ping DEBU[0048] Calling GET /v1.27/version Client: Version: 17.04.0-dev API version: 1.27 Go version: go1.7.5 Git commit: 3385658 Built: Mon Mar 6 08:39:06 2017 OS/Arch: linux/amd64 Server: Version: 17.04.0-dev API version: 1.27 (minimum version 1.12) Go version: go1.7.5 Git commit: 3385658 Built: Mon Mar 6 08:39:06 2017 OS/Arch: linux/amd64 Experimental: false 到此 docker 源码编译和开发环境都已经搭建好了。\n如果想要修改 docker 源码，只要在你的 IDE、容器里或者你本机上修改 docker 代码后，再执行上面的 hack/make.sh binary 命令就可以生成新的 docker 二进制文件，再替换原来的 /usr/bin/ 目录下的 docker 二进制文件即可。\n","relpermalink":"/blog/docker-dev-env/","summary":"本文将讲解如何进行 Docker 源码编译及开发环境搭建。","title":"Docker 源码编译和开发环境搭建"}]