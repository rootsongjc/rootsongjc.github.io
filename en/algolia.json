[{"content":"In this article, I will explain how to use GraphQL to query data from SkyWalking with Postman . It includes steps to obtain the bearer token, construct a query to retrieve load metrics for a specific service, and use GraphQL introspection to see the schema of SkyWalking GraphQL APIs. The article also provides references for further information.\nWhat Is GraphQL? GraphQL is a query language and runtime for APIs developed by Facebook. It provides a more efficient, powerful, and flexible alternative to traditional REST APIs by allowing clients to specify exactly what data they need and receive only that data in response. With GraphQL, clients can query multiple resources in a single request, reducing the number of roundtrips to the server and improving performance.\nWhat’s the Difference between GraphQL and REST APIs? GraphQL allows clients to request only the data they need, while REST APIs require clients to retrieve everything in a resource regardless of whether they need it or not. Additionally, GraphQL allows clients to query multiple resources in a single request, making it more efficient and less chatty than REST APIs.\nHow Do I Query Data from SkyWalking? SkyWalking defines the communication protocol for the query stage. The SkyWalking native UI and CLI use this protocol to consistently fetch data from the backend, without needing to worry about backend updates.\nThere are two methods for querying metrics from SkyWalking:\nGraphQL APIs PromQL APIs This article provides a guide on how to use GraphQL to query metrics from SkyWalking. If you are interested in the PromQL APIs, you can refer to the article Build Grafana dashboards for Apache SkyWalking — Native PromQL Support .Continuing with the following steps requires a TSB installation. If you don’t have one and still want to experience using GraphQL to query data in SkyWalking, you can use the free demo environment (username/password: skywalking/skywalking) provided by SkyWalking. Log in to the demo website and get a token for queries. Endpoint address for GraphQL queries is http://demo.skywalking.apache.org/graphql . The steps to construct the query are the same as described below.\nObserve GraphQL Queries in TSB Before we use Postman to construct our own GraphQL query, let’s first observe how TSB obtains data from SkyWalking.\nOpen Chrome DevTools and switch to the Network tab. Visit the Organization – Services tab on the website. Watch the network request list and right-click on the one of the graphql requests, like in the following image:\nFigure 1: Chrome DevTool The curl commands you see will look like this. Execute the command in your terminal, and you will get a list of services managed by TSB from SkyWalking.\ncurl \u0026#39;\u0026lt;https://saturn.tetrate.work/ui/graphql\u0026gt;\u0026#39; \\ -H \u0026#39;Accept-Language: en,zh-CN;q=0.9,zh;q=0.8,en-US;q=0.7,zh-TW;q=0.6\u0026#39; \\ -H \u0026#39;Cache-Control: no-cache\u0026#39; \\ -H \u0026#39;Client-Timestamp: 1686104776136\u0026#39; \\ -H \u0026#39;Connection: keep-alive\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -H \u0026#39;Cookie: ...\u0026#39; \\ -H \u0026#39;Origin: \u0026lt;https://saturn.tetrate.work\u0026gt;\u0026#39; \\ -H \u0026#39;Pragma: no-cache\u0026#39; \\ -H \u0026#39;Referer: \u0026lt;https://saturn.tetrate.work/mp/services\u0026gt;\u0026#39; \\ -H \u0026#39;Request-Id: ...\u0026#39; \\ -H \u0026#39;Sec-Fetch-Dest: empty\u0026#39; \\ -H \u0026#39;Sec-Fetch-Mode: cors\u0026#39; \\ -H \u0026#39;Sec-Fetch-Site: same-origin\u0026#39; \\ -H \u0026#39;User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\u0026#39; \\ -H \u0026#39;X-Bridge-Csrf-Token: IOmJszLAqY3TRIUNhTuGu7vQgnfQY1FtgYFm+l/+Mu4EmVQU5T8EaQ7bngkCv4hQ12ZGids+I21pHMdepE9/qQ==\u0026#39; \\ -H \u0026#39;X-Csrf-Token: xTbxZerD3t8N3PaS7nbjKCfxk1Q9dtvvrx4D+IJohHicb0VfB4iAZaP0zh1eXDWctQyCYZWaKLhAYT3M6Drk3A==\u0026#39; \\ -H \u0026#39;accept: application/json\u0026#39; \\ -H \u0026#39;sec-ch-ua: \u0026#34;Not.A/Brand\u0026#34;;v=\u0026#34;8\u0026#34;, \u0026#34;Chromium\u0026#34;;v=\u0026#34;114\u0026#34;, \u0026#34;Google Chrome\u0026#34;;v=\u0026#34;114\u0026#34;\u0026#39; \\ -H \u0026#39;sec-ch-ua-mobile: ?0\u0026#39; \\ -H \u0026#39;sec-ch-ua-platform: \u0026#34;macOS\u0026#34;\u0026#39; \\ --data-raw $\u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;query ServiceRegistryListMetrics(...)}\u0026#39; \\ --compressed Note: Some fields in the above example are too long and replaced with dots (…).\nNext, I will guide you through constructing a query to retrieve the load metrics for a specific service.\nObtain the Bearer Token Firstly, you need to obtain the bearer of the website. Log in to TSB UI, click on the user button in the upper right corner, and then click “Show token information”. In the pop-up window, you will see the Bearer Token, as shown in the following image.\nFigure 2: Get the bearer token from the TSB UI Note: The validity period of the bearer token is relatively short. When it expires, you need to log in to TSB again to obtain a new token.\nWe have already deployed the bookinfo application in advance and sent some test traffic. To query the load metrics of reviews using GraphQL in the Postman client, follow these steps:\nCreate a new GraphQL request and enter the request URL: $TSB_ADDRESS/graphql Add the Authorization header with the value Bearer $TOKEN Use GraphQL Introspection to see the schema of SkyWalking GraphQL APIs. Find and click the readMetricsValues item. You will see the variables on the right side. Fill in the …","relpermalink":"/en/blog/how-to-use-graphql-to-query-observability-data-from-skywalking-with-postman/","summary":"This article explains how to use GraphQL to query observability data from SkyWalking with Postman. It first introduces GraphQL and SkyWalking, then explains how to set up Postman to send GraphQL queries, and finally provides some example GraphQL queries that can be used to query observability data from SkyWalking.","title":"How to Use GraphQL to Query Observability Data from SkyWalking with Postman"},{"content":"In June, Istio 1.18 was released , marking the second release of Istio in 2023 and the first to offer official support for ambient mode . Tetrate’s Paul Merrison was one of the release managers for this version, and Tetrate’s contributions to this release included various customer-driven usability enhancements and important work in the underlying Envoy Proxy upon which Istio depends. When asked about the experience of working on Istio 1.18, Paul said “working as the lead release manager for Istio 1.18 gave me a fascinating insight into how a group of super talented people from around the world come together, organize themselves and ship software. There was a steep learning curve, but the Istio community is awesome and I was supported brilliantly. The biggest challenge was definitely learning and executing all the steps that are needed to bring a release to life, but the feeling of achievement when it finally made its way out into the world will stay with me for a while!” Istio first announced the introduction of Ambient mode in September last year, which was covered in detail by Zack in this blog post , where he explains the differences between ambient mode and sidecar mode.\nThis release introduces many new features and changes in addition to ambient mode, including enhanced Kubernetes Gateway API support, health checks for virtual machines that are not automatically registered, support for expired metrics, enhanced istioctl analyze, and more. See the release blog for details. The most significant of these are the ambient mode and Gateway API enhancements, detailed below.\n“Working as the lead release manager for Istio 1.18 gave me a fascinating insight into how a group of super talented people from around the world come together, organize themselves and ship software. There was a steep learning curve, but the Istio community is awesome and I was supported brilliantly. The biggest challenge was definitely learning and executing all the steps that are needed to bring a release to life, but the feeling of achievement when it finally made its way out into the world will stay with me for a while!”\nPaul Merrison, Tetrate Engineering and Istio Release Manager\nWhat Is Ambient Mode? Before discussing ambient mode, it is essential to understand the current “sidecar mode” used by Istio. Sidecar mode is the default data plane mode used by Istio, where each application pod comes equipped with a sidecar proxy (usually Envoy) that handles all network traffic in and out of the pod, providing Istio’s core functionality such as Zero Trust security, telemetry and traffic management. While sidecar mode is suitable for most users, ambient mode offers some advantages in specific circumstances. For more information on the differences between ambient mode and the standard sidecar mode, see our article on Ambient Mesh: What You Need to Know about This Experimental New Deployment Model for Istio .\nWhat Are the Design Goals of Ambient Mode? Non-intrusive: Ambient mode does not require injecting sidecar proxies into the application’s pods and only requires the application to be tagged to automatically join the mesh, potentially reducing the mesh’s impact on the application. Efficient resource utilization: Ambient mode can optimize resource utilization for some use cases by sharing the ztunnel proxy across the mesh. If certain L7 functionality of Istio is required, it can also be addressed by deploying Waypoints precisely for a ServiceAccount or Namespace, providing more control over resource consumption. Performance parity with sidecar mode: Ambient mode initially adopted a shared proxy model based on Envoy, but during development, issues such as complex Envoy configuration were discovered, leading the Istio community to develop its shared proxy (ztunnel ) based on Rust. In the future, ambient mode is expected to have comparable performance to traditional sidecar mode. Security: Ambient mode provides TLS support by running a shared proxy ztunnel on each node, and when users require the same security as sidecar mode, they also need to deploy one or more Waypoints in each namespace to handle L7 traffic in that namespace. Users can use istioctl x waypoint for Waypoint configuration management. For example, running the istioctl x waypoint generate command generates a Kubernetes Gateway API resource managed by Istio.\nOverall, ambient mode promises to offer additional flexibility to Istio’s deployment model which may prove helpful to some users. It should be noted that the ambient mode is still in the alpha stage and has yet to achieve production-level stability.\nEnhanced Kubernetes Gateway API Support Istio 1.18 introduces several vital improvements and modifications to its Kubernetes Gateway API support:\nSupport for v1beta1: When upgrading to the new version of Istio, Gateway API version greater than 0.6.0+ is required. Use the istioctl x precheck command to check for upgrade issues.\nGateway API automated deployment management upgrades: …","relpermalink":"/en/blog/istio-1-18-released-now-with-ambient-mode-available/","summary":"This article introduces Istio 1.18, the latest release of the service mesh platform. It highlights the new features and improvements, such as ambient mode, which allows Istio to run on any Kubernetes cluster without requiring a dedicated control plane. It also explains how to get started with Istio 1.18 using Tetrate’s distribution and support.","title":"Istio 1.18 Released, Now with Ambient Mode Available"},{"content":"In a previous blog post , I discussed how Istio Ambient Mesh uses iptables and Geneve tunnels to intercept traffic from application pods into Ztunnel. Many readers may not be familiar with this tunneling protocol, so this article will introduce the definition, packet structure and advantages of Geneve tunnels compared with the VXLAN protocol. Finally, this article will introduce how Istio Ambient Mesh applies Geneve tunnels to implement traffic interception and the new eBPF mode introduced in Istio 1.18.\nIntroduction to Geneve Tunnels In order to address the lack of flexibility and security in current data transmissions, the Geneve (Generic Network Virtualization Encapsulation) network virtualization encapsulation (tunneling) protocol was created. Geneve only defines a data encapsulation format, excluding control plane information. The key advantage of Geneve over VXLAN encapsulation is that it extends the types of encapsulated protocols by adding TLV format options.\nGeneve vs. VXLAN VXLAN and Geneve are both network virtualization protocols and they have many similarities. Virtualization protocols are technologies that separate virtual networks from physical networks. They allow network administrators to create multiple virtual networks in a virtual environment, each of which can have its own VLAN identifiers, IP addresses and routing. In addition, VXLAN and Geneve use UDP encapsulation, which enables them to be extended through existing network infrastructure. VXLAN and Geneve protocols are also flexible, can be used in different network topologies and are compatible with different virtualization platforms.\nFigure 1 shows the message structure of VXLAN and Geneve tunnels and the differences in their respective headers.\nFigure 1: VXLAN and Geneve packet format schematic diagram. From the figure, we can see that the message structure of VXLAN and Geneve tunneling protocols is similar, with the main difference being the use of different UDP port numbers and protocol headers. VXLAN uses port 4789, while Geneve uses port 6081. The Geneve protocol header is more extendable than VXLAN.\nThe Geneve tunneling protocol adds variable-length options that can contain zero or more option data in TLV format, making it more scalable than VXLAN. TLV stands for Type-Length-Value, which is a format for parsing and transmitting metadata in network packets. Each metadata information in the Geneve protocol is composed of a TLV format field, making it simple to flexibly add, delete and modify these metadata.\nThe TLV format field contains the following data:\nType: 8-bit type field. Length: 5-bit option length field, represented in multiples of 4 bytes, excluding the option header. Data: Variable-length option data field, which may not exist or may be between 4 and 128 bytes. The Geneve protocol can easily modify and extend metadata information while maintaining compatibility and flexibility by using the TLV format.\nPlease refer to RFC 7348 Virtual eXtensible Local Area Network (VXLAN): A Framework for Overlaying Virtualized Layer 2 Networks over Layer 3 Networks for more information about VXLAN. For more information about the Geneve tunnel packet format, please refer to RFC 8926 Geneve: Generic Network Virtualization Encapsulation .\nHow it Works The Geneve tunnel is mainly used in cloud computing and virtualization scenarios, and it can encapsulate packets in a new packet for transmission in a virtual network. The Geneve tunnel uses a 24-bit VNI (Virtual Network Identifier) to transmit packets from one physical network to another. The Geneve tunnel can also use security protocols such as IPsec and TLS to protect the transmission of packets.\nWhen a packet reaches the destination host, the Geneve tunnel protocol will de-encapsulate the packet from the Geneve protocol header and deliver it to the destination in the virtual network. During the de-encapsulation process, the VNI information in the Geneve protocol header is used to determine the destination of the packet, ensuring that the packet is correctly routed to the destination in the virtual network.\nAssuming there is a virtual network with a VNI of 1001. When a packet is transmitted from one physical network to another, a tunnel can be used to track the packet during transmission by setting the VNI between the source and target physical networks to 1001. When the packet reaches the target physical network, the VNI is removed from the packet and the packet is delivered to the target physical network.\nSecurity The Geneve tunnel protocol itself does not provide any security mechanisms, so packets transmitted in the Geneve tunnel can be subject to threats such as packet tampering, interception and replay.\nTo ensure the security of packets transmitted in the Geneve tunnel, some security protocols can be used. The following are some common security protocols:\nIPsec (Internet Protocol Security): IPsec is a network layer security protocol that can encrypt, authenticate and provide …","relpermalink":"/en/blog/traffic-interception-with-geneve-tunnel-with-istio-ambient-mesh/","summary":"This article introduces Geneve tunnels, a network virtualization protocol that can intercept Istio ambient mesh traffic more flexibly and securely than VXLAN. It also explains how Istio Ambient Mesh uses Geneve tunnels and the new eBPF mode in Istio 1.18.","title":"Using Geneve Tunnels to Implement Istio Ambient Mesh Traffic Interception"},{"content":"Artificial Intelligence (AI) is changing our lives, making our work more efficient and intelligent. In this rapidly developing field, there are many practical AI tools that can help us better accomplish our work. In the future, mastering various AI tools to optimize your workflow and improve work efficiency will be a necessary skill for everyone. It’s time to gather some cheap and practical AI tools. Below are some recommended practical AI tools that are worth collecting. You can directly use these tools without additional programming knowledge. Most of these tools are free to use, or provide free usage quotas, or have low usage costs.\n1. ChatGPT ChatGPT ChatGPT is an intelligent chatbot based on GPT technology, which can interact with users in natural language. It uses deep learning technology and large-scale trained language models to understand users’ questions and provide useful answers.\nChatGPT can answer various questions, and users can directly input questions or topics on the website and get quick and accurate answers. It should be noted that ChatGPT is an online chatbot, and its answers may not be 100% accurate. In addition, the data that ChatGPT model is trained on is up to 2021, and it is still in the early development stage, and is constantly improving and optimizing.\nRecommendation Reasons\nChatGPT’s response speed is super fast, and you can get an answer in a few seconds, which is especially suitable for situations where quick replies are needed. You can also use OpenAI’s API to create your own tools. However, the free version of ChatGPT’s response speed is sometimes slow, and frequently refreshing the page is required when following up, and there are also limits on the number of characters for questions and answers.\nThe commonly used functions include:\nWriting code Translation Proofreading articles Summarizing an article (you can output a URL) Learning a knowledge area that you are not familiar with In addition, a ChatGPT desktop application is recommended: https://github.com/lencx/ChatGPT , as well as the ChatGPT prompt project https://github.com/f/awesome-chatgpt-prompts .\nChatGPT Plus ChatGPT Plus is an enhanced version of ChatGPT that utilizes GPT-3 technology to provide more powerful and intelligent natural language processing capabilities. It can assist users in completing various tasks such as generating articles, translation, Q\u0026amp;A, speech conversion, and chatbots.\nChatGPT Plus has a sleek and beautiful interface that is very intuitive and user-friendly. Users can input text using various methods such as keyboard input, voice input, and image input. ChatGPT Plus also offers multiple language and style options, making it easy for users to generate high-quality text and speech.\n2. Smodin Smodin Smodin.io is an online tool based on artificial intelligence and natural language processing technology that can help users generate content such as articles, press releases, blog posts, and social media posts. The website uses deep learning technology and language models to automatically generate high-quality text and offers a variety of language and style options.\nUsing Smodin.io is very simple. Simply enter the topic, keywords, desired language, and style you wish to generate, then click the “Generate” button to obtain a high-quality article. You can also make adjustments and edits as needed to meet your specific needs.\nIn addition to generating text, Smodin.io also offers other useful features such as grammar and spell check, SEO optimization suggestions, and real-time translation. The website is very flexible and convenient to use, making it suitable for individuals and businesses who require high-quality text, such as marketers, editors, writers, and bloggers.\nIt’s important to note that while Smodin.io can save you a lot of time and effort, since the text is generated automatically, there may be some syntax or logic errors. Therefore, appropriate review and editing are necessary when using it.\nRecommended Reasons\nThe website supports over 40 languages, limits input to 1000 characters per session, and returns rewritten results quickly. Free users have a limit on the number of calls they can make, while paid users have a limit. There are two payment tiers, both of which are relatively inexpensive.\n3. Bing Bing chat Bing is a search engine developed by Microsoft that integrates artificial intelligence technology to provide more intelligent search results. Recently, Microsoft announced a new feature called “Chat” added to Bing, where users can input questions in the chat box and Bing will automatically answer them, just like an intelligent chatbot. Bing also provides multiple conversation styles to choose from.\nIt should be noted that the chat function is currently in the testing phase and may have some issues and limitations. Additionally, Bing’s availability may be restricted in some countries and regions. However, it is still a very useful search engine that can help users quickly find the …","relpermalink":"/en/blog/ai-tools-collection/","summary":"Are you looking for ways to optimize your workflow and increase productivity? Here are some useful AI tools for you.","title":"Useful AI Tools to Optimize Your Workflow"},{"content":"In the previous blog post , I introduced how Istio manages certificates, and in this article, I will guide you on how to use an external certificate authority (CA) to achieve fine-grained certificate management and automatic certificate rotation through the integration of SPIRE and cert-manager .\nIf you are not familiar with SPIRE and what it’s used for, we recommend reading the following articles:\nWhy Would You Need SPIRE for Authentication with Istio? How to integrate SPIRE in Istio Introduction to the Certificate Issuance and Management Process Figure 1 shows the certificate trust chain used in this article based on cert-manager and SPIRE:\nFigure 1: Certificate trust chain based on cert-manager and SPIRE. cert-manager acts as the root CA to issue certificates to istiod and SPIRE. We use a self-signed issuer , but you can also configure it to use built-in issuers such as Let’s Encrypt, Vault, Venafi, or other external issuers. You can also choose to use other UpstreamAuthorities , such as Vault, SPIRE Federation, etc. SPIRE issues SVID certificates to the workloads and ingress Gateway and egress Gateway in the Istio mesh for mTLS between services. The certificates used when accessing the ingress Gateway from outside the mesh and when the egress Gateway access services outside the mesh need to be configured separately. Figure 2 shows the certificate issuance and update process after integrating SPIRE and cert-manager in Istio.\nFigure 2: Certificate issuance and update process after integrating SPIRE and \u0026lt;em\u0026gt;cert-manager\u0026lt;/em\u0026gt; in Istio. The Kubernetes Workload Registrar in the SPIRE Server automatically registers the workloads in Kubernetes and generates SPIFFE standard identities for all workloads. cert-manager issues and manages the CA certificates for istiod. The Envoy proxies in the workloads send certificate signing request (CSR) requests to the SPIRE Agent on the same node through the SDS API via a Unix domain socket (UDS). The SPIRE Agent sends the CSR to the SPIRE Server. The SPIRE Server returns the signed certificate to the SPIRE Agent. The SPIRE Agent returns the signed certificate to the workloads. SPIRE is responsible for the certificate management and updates for the workloads. Now that we have a general understanding of the process, let’s install the components manually.\nInstall cert-manager Run the following command to install cert-manager, which we will use for automatic certificate rotation:\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.10.1/cert-manager.yaml The root CA is a self-signed certificate. Run the following command to configure the root CA:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: selfsigned namespace: cert-manager spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: selfsigned-ca namespace: cert-manager spec: isCA: true duration: 21600h secretName: selfsigned-ca commonName: certmanager-ca subject: organizations: - cert-manager issuerRef: name: selfsigned kind: Issuer group: cert-manager.io --- apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: selfsigned-ca spec: ca: secretName: selfsigned-ca EOF Then configure certificates for istiod:\nkubectl create namespace istio-system cat \u0026lt;\u0026lt; EOF | kubectl apply -f - --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: cacerts namespace: istio-system spec: secretName: cacerts duration: 1440h renewBefore: 360h commonName: istiod.istio-system.svc isCA: true usages: - digital signature - key encipherment - cert sign dnsNames: - istiod.istio-system.svc issuerRef: name: selfsigned-ca kind: ClusterIssuer group: cert-manager.io EOF Now that we have installed cert-manager and created a ClusterIssuer named selfsigned-ca, let’s install SPIRE and use cert-manager as the UpstreamAuthority for SPIRE.\nInstalling SPIRE Quickly install SPIRE by running the following command:\nkubectl apply -f https://gist.githubusercontent.com/rootsongjc/5dac0518cc432cbf844114faca74aa40/raw/814587f94bbef8fb1dd376282249dcb2a8f7fa1b/spire-with-cert-manager-upstream-authority-quick-start.yaml This YAML file adapts to cert-manager compared to the samples/security/spire/spire-quickstart.yaml file in the Istio 1.16 installation package, such as:\nAdding permissions for the cert-manager.io API group to the spire-server-trust-role ClusterRole; Adding an UpstreamAuthority “cert-manager” configuration in the SPIRE Server configuration. Note: The trust_domain in the SPIRE Server configuration should be consistent with the TRUST_DOMAIN environment variable specified when installing Istio.\nThis command installs the Kubernetes Workload Registrar , automatically registering the workloads in Kubernetes. All workloads will be registered with the SPIFFE standard service identity format spiffe://\u0026lt;trust-domain\u0026gt;/ns/\u0026lt;namespace\u0026gt;/sa/\u0026lt;service-account\u0026gt; based on their service accounts.\nIf you want to adjust the TTL of the SPIRE CA and SVID …","relpermalink":"/en/blog/cert-manager-spire-istio/","summary":"In this blog, I will guide you on how to use an external certificate authority (CA) to achieve fine-grained certificate management and automatic certificate rotation through the integration of SPIRE and cert-manager.","title":"Managing Certificates in Istio with cert-manager and SPIRE"},{"content":"I mentioned in my last article on understanding mTLS traffic encryption in Istio that the key to traffic encryption is certificate management. We can use the built-in certificate authority (CA) in Istio or a custom CA to manage certificates within the mesh. This blog post will explain how Istio handles certificate management.\nWhat Is a Certificate? There are many types of certificates, but in this article, I am explicitly referring to X.509 V3 certificates. X.509 certificates are a standard digital format used to identify entities in computer networks. X.509 is the international standard for public key infrastructure (PKI) and is primarily used for identity authentication and information encryption, such as TLS.\nThe contents of a certificate are hashed using a hash function and then signed with the issuer’s private key. This way, when a certificate is received, the recipient can use the issuer’s public key to verify the certificate’s validity.\nA hash function is a function that maps an input of any length (also called a message) to a fixed-length output, also called a hash value or message digest. There are many hash functions, such as MD5, SHA-1, etc.\nA certificate is like a business card issued by an authoritative agency for the user to prove their identity, and it can also be used to encrypt information to ensure the security and integrity of communication. The following diagram shows the general steps of TLS communication, where the certificate proves the server’s identity and encrypts the communication.\nFigure 1 shows an example of an HTTP call to a website, issuing a digital certificate, authenticating, and encrypting the communication.\nFigure 1. TLS certificate issuance and validation process Here are the detailed steps:\nThe server (website owner) submits a certificate signing request to the CA; The CA verifies the server’s identity and the authenticity of the website and issues a digital certificate to the server, which the server installs so that visitors can verify the website’s security; The user sends a request to the website through a browser (client); The server returns the TLS certificate to the client; The client verifies the certificate’s validity with the CA and establishes the connection if the certificate is valid, or prompts the user to reject the connection if it is invalid; The client generates a pair of random public and private keys; The client sends its public key to the server; The server encrypts the message using the client’s public key; The server sends the encrypted data to the client; The client decrypts the data sent by the server using its private key. At this point, both parties have established a secure channel and can transmit encrypted data in both directions.\nHow to Generate Certificates You can generate X.509 certificates with the following open-source tools:\nEasy-RSA : A simple command-line tool maintained by the OpenVPN project, EasyRSA can easily generate secure certificates and keys for the OpenVPN network. OpenSSL : Originated by an individual in 1995 and now maintained by an independent organization, OpenSSL provides only a command-line tool. CFSSL : Developed and maintained by CloudFlare, CFSSL is not just a command-line tool for generating certificates but also can serve as a PKI server. BoringSSL : A branch of OpenSSL developed and maintained by Google, BoringSSL is used in the Chrome browser and Android operating system. Since most people are probably familiar with OpenSSL, we will use OpenSSL to create certificates in the following text.\nCertificate Trust Chain The validation of a certificate requires using a certificate trust chain (Certificate Trust Chain). A certificate trust chain refers to a series of certificates used for identity verification, which form a chain starting from a trusted root certificate issuing agency, connecting downward step by step, until a specific intermediate or terminal certificate is used for verifying a particular certificate. The trustworthiness of a digital certificate increases as the certificate level increases in the certificate trust chain.\nIn Figure 2, you can see four trust chains.\nFigure 2. Certificate trust chains Certificate trust chains are tree-like structures, where each CA can have one or more child CAs. There are three roles:\nRoot CA: The top-level CA can issue certificates to intermediate CAs. Intermediate CA: They can issue end-entity certificates. End entity: A device or service that holds an end-entity certificate. Root CAs are the top-level issuing authorities for digital certificates, so the certificates they issue are the most trustworthy. Root certificate authorities are usually operated and regulated by government agencies or other authoritative organizations such as the International Infrastructure Security Organization. Common root CAs include:\nSymantec/VeriSign Comodo DigiCert GlobalSign GoDaddy Entrust GeoTrust RapidSSL Baltimore CyberTrust Root Please note that the above list is just a sample. …","relpermalink":"/en/blog/istio-certificates-management/","summary":"This blog post will explain how Istio handles certificate management.","title":"How Are Certificates Managed in Istio?"},{"content":"In my last blog , I introduced transparent traffic intercepting and L4 routing in Ambient mode. In this blog, I will show you how L7 traffic is routed.\nThe figure below shows the L7 network traffic path in ambient mode.\nFigure 1: L7 network traffic in ambient mesh Note: The Waypoint proxy can be located on the same node as the application, and even all of the service and the Waypoint proxy can be on the same node. I draw them on three nodes for display purposes, but it has no significant impact on the actual traffic path, except that it is no longer sent to another node via eth0.\nIn the following section, we will explore the process in Figure 1 from a hands-on perspective.\nEnvironments for Waypoint Proxy Let’s continue to view the environment description using the ambient mode Istio deployed in the previous blog. To illustrate the L7 network routing, we need to create a Gateway on top of this.\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: gateway.networking.k8s.io/v1alpha2 kind: Gateway metadata: name: productpage annotations: istio.io/service-account: bookinfo-productpage spec: gatewayClassName: istio-mesh EOF After executing this command, a Waypoint proxy is created under the default namespace; in my environment, this pod is named bookinfo-productpage-waypoint-proxy-6f88c55d59-4dzdx and is specifically used to handle L7 traffic to the productpage service ( Service B), which I will call Waypoint proxy B.\nThe Waypoint proxy may be deployed in a different namespace, on a different node from the workload, or both. No matter where the Waypoint proxy is situated, the L7 traffic path is unaffected.\nWe will omit the sections of this blog dealing with intercepting inbound and outbound traffic because the way transparent traffic is handled in ambient mesh in L4 and L7 networks is similar. Details are available in the prior blog .\nWe will start directly with the traffic intercepted at Ztunnel A and then forward it to Envoy port 15006.\nOutbound Traffic Routing on Ztunnel A Use the following command to dump the Envoy proxy configuration on Ztunnel A:\nkubectl exec -n istio-system ztunnel-hptxk -c istio-proxy -- curl \u0026#34;127.0.0.1:15000/config_dump?include_eds\u0026#34;\u0026gt;ztunnel-a-all-include-eds.json 10.8.14.226 is the Cluster IP of the target service, and the service port is 9080. The traffic will be routed to the spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage cluster. Let’s look at the configuration of that cluster.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \u0026#34;10.8.14.226\u0026#34;: { \u0026#34;matcher\u0026#34;: { \u0026#34;matcher_tree\u0026#34;: { \u0026#34;input\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;port\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.matching.common_inputs.network.v3.DestinationPortInput\u0026#34; } }, \u0026#34;exact_match_map\u0026#34;: { \u0026#34;map\u0026#34;: { \u0026#34;9080\u0026#34;: { \u0026#34;action\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/google.protobuf.StringValue\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage\u0026#34; } } } } } } } } } 10.8.14.226 is the Cluster IP of the target service, and the service port is 9080. The traffic will be routed to the spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage cluster. Let’s look at the configuration of that cluster.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;version_info\u0026#34;: \u0026#34;2022-11-17T03:27:45Z/82\u0026#34;, \u0026#34;cluster\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.cluster.v3.Cluster\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;EDS\u0026#34;, \u0026#34;eds_cluster_config\u0026#34;: { \u0026#34;eds_config\u0026#34;: { \u0026#34;ads\u0026#34;: {}, \u0026#34;initial_fetch_timeout\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;resource_api_version\u0026#34;: \u0026#34;V3\u0026#34; } }, /* omit */ } The cluster is discovered using the EDS service. To view the EDS information for this cluster:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.endpoint.v3.ClusterLoadAssignment\u0026#34;, \u0026#34;endpoints\u0026#34;: [ { \u0026#34;locality\u0026#34;: {}, \u0026#34;lb_endpoints\u0026#34;: [ { \u0026#34;endpoint\u0026#34;: { \u0026#34;address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;10.4.3.14\u0026#34;, \u0026#34;port_value\u0026#34;: 15006 } }, \u0026#34;health_check_config\u0026#34;: {} }, \u0026#34;health_status\u0026#34;: \u0026#34;HEALTHY\u0026#34;, \u0026#34;load_balancing_weight\u0026#34;: 1 } ] } ], \u0026#34;policy\u0026#34;: { \u0026#34;overprovisioning_factor\u0026#34;: 140 } } Note: The output cluster_name field is still missing here. See the GitHub issue .\nTraffic is forwarded here directly to the Waypoint Proxy endpoint at 10.4.3.14:15006.\nTraffic Routing Using Waypoint Proxy Let’s dump the Envoy configuration into Waypoint Proxy B again.\nkubectl exec -n default bookinfo-productpage-waypoint-proxy-6f88c55d59-4dzdx -c istio-proxy -- curl \u0026#34;127.0.0.1:15000/config_dump?include_eds\u0026#34;\u0026gt;waypoint-a-all-include-eds.json Look into the configuration of inbound_CONNECT_terminate listener:\n1 2 3 4 5 6 …","relpermalink":"/en/blog/ambient-mesh-l7-traffic-path/","summary":"This article describes in detail the L7 traffic path in Ambient Mesh in both diagrammatic and hands-on form.","title":"L7 Traffic Path in Ambient Mesh"},{"content":"Istio’s new “ambient mode” is an experimental, “sidecar-less” deployment model for Istio . Instead of a sidecar proxy in front of every workload, ambient mode uses tproxy and HTTP Based Overlay Network Environment (HBONE) as key technologies for transparent traffic intercepting and routing that we covered in our recent article on transparent traffic intercepting and routing in the L4 network of Istio Ambient Mesh . In this article, we’ll take a closer look at tproxy and how it’s used.\nWhat Is a Proxy For? Proxies have a wide range of uses on the Internet, such as:\nRequest caching: to speed up network response, acting similarly to a CDN. Traffic filtering: used for network supervision, blocking or allowing access to specific hosts and websites. Traffic forwarding: used for load balancing or as a network relay. Traffic management: fine-grained management of traffic to and from the proxy, such as publishing to different backends by percentage, timeout and retry settings, circuit breaking, etc. Security auditing: logging and limiting client requests for billing or auditing purposes. Proxy Types There are a number of ways to classify proxies based on how they’re used. You can see two categories based on the location of the proxy in Figure 1:\nFigure 1: Forward proxy and reverse proxy. Forward proxies (like shadowsocks ) run on the client side and send requests to the server on behalf of the client. Reverse proxies (often in the form of a web server) accept Internet or external requests on behalf of the server and route them to the corresponding backends. Proxies may be located on the same node as the client or server or on a different node. We can classify them as transparent or non-transparent based on whether the client or server can see them. Figure 2 (below) shows the process of a client (A) sending a request to a server (C) through a proxy (B).\nFigure 2: Transparent vs. non-transparent proxies To use a non-transparent proxy, the client needs to explicitly change the destination address to that of the proxy server and use the proxy protocol to connect to the proxy server. To use a transparent proxy, the client and the server do not know the proxy is there, which means the client does not need to modify the destination address, and does not need to use the proxy protocol to connect to the proxy server; all the destination address conversion is done in the transparent proxy. Using the tproxy Transparent Proxy tproxy is a Linux kernel module (since Linux 2.2) that implements transparent proxies. To use tproxy, you must first use iptables to intercept the required packets at the required NIC, then listen for and forward the packet on that NIC.\nFollow these steps to use tproxy to implement a transparent proxy:\nFirst, you need to implement traffic interception: create a rule in the mangle table of the PREROUTING chain of iptables to intercept traffic and send it to tproxy for processing, for example, iptables -t mangle -A PREROUTING -p tcp -dport 9080 -j TPROXY --on-port 15001 --on-ip 127.0.0.1 --tproxy-mark 0x1/0x1, marking all TCP packets destined for port 9080 with a mark 1. You can specify the source IP address or IP set to further narrow the marking, with tproxy listening on port 15001. Create a routing rule that looks up all packets with mark 1 in a specific routing table: for example, add ip rule add fwmark 1 lookup 100, so that all packets with fwmark 1 look up to the routing table 100. Mapping packets to specific local addresses: for example, ip rule add local 0.0.0.0/0 dev lo table 100, which declares all IPv4 addresses as local in the routing table 100, but of course, this is just an example. In practice, you will need to forward packets with specific IPs to the local lo loopback NIC. The traffic has been intercepted on tproxy’s listening port 15001 (enter from Linux kernel space into user space). You can write a web application to process the packets or use tproxy-enabled software such as Squid or Envoy to process the packets.\nPros and Cons of Transparent Proxies Transparent proxies have the following advantages:\nHigher bandwidth and reduced transmission latency, thereby improving the quality of service. No need for users to configure networks and hosts. Control access to network services. Transparent proxies have the following disadvantages:\nIncorrectly configured, the transparent proxy may prevent connection to the Internet, leaving users unable to troubleshoot and fix errors. Security cannot be guaranteed, as intercepted user traffic may be tampered with by transparent proxies. The risk that transparent proxies may cache user information, leading to privacy leaks. Summary As a vital type of proxy, transparent proxies are used in a wide range of applications, whether in proxy software such as shadowsocks, Xray, or in the Istio service mesh. Understanding how they work helps us use proxies correctly, and whether or not you use a transparent proxy depends on how much you trust and know about it.\nThis …","relpermalink":"/en/blog/what-is-tproxy/","summary":"Tproxy is used to intercept traffic in Istio Ambient mode.","title":"How Istio's Ambient Mode Transparent Proxy - tproxy Works Under the Hood"},{"content":"In cloud native applications, a request often needs to be processed through a series of APIs or backend services, some of which are parallel and some serial and located on different platforms or nodes. How do we determine the service paths and nodes a call goes through to help us troubleshoot the problem? This is where distributed tracing comes into play.\nThis article covers:\nHow distributed tracing works How to choose distributed tracing software How to use distributed tracing in Istio How to view distributed tracing data using Bookinfo and SkyWalking as examples Distributed Tracing Basics Distributed tracing is a method for tracing requests in a distributed system to help users better understand, control, and optimize distributed systems. There are two concepts used in distributed tracing: TraceID and SpanID. You can see them in Figure 1 below.\nTraceID is a globally unique ID that identifies the trace information of a request. All traces of a request belong to the same TraceID, and the TraceID remains constant throughout the trace of the request. SpanID is a locally unique ID that identifies a request’s trace information at a certain time. A request generates different SpanIDs at different periods, and SpanIDs are used to distinguish trace information for a request at different periods. TraceID and SpanID are the basis of distributed tracing. They provide a uniform identifier for request tracing in distributed systems and facilitate users’ ability to query, manage, and analyze the trace information of requests.\nFigure 1: Trace and span The following is the process of distributed tracing:\nWhen a system receives a request, the distributed tracing system assigns a TraceID to the request, which is used to chain together the entire chain of invocations. The distributed trace system generates a SpanID and ParentID for each service call within the system for the request, which is used to record the parent-child relationship of the call; a Span without a ParentID is used as the entry point of the call chain. TraceID and SpanID are to be passed during each service call. When viewing a distributed trace, query the full process of a particular request by TraceID. How Istio Implements Distributed Tracing Istio’s distributed tracing is based on information collected by the Envoy proxy in the data plane. After a service request is intercepted by Envoy, Envoy adds tracing information as headers to the request forwarded to the destination workload. The following headers are relevant for distributed tracing:\nAs TraceID: x-request-id Used to establish parent-child relationships for Span in the LightStep trace: x-ot-span-context\u0026lt;/li Used for Zipkin, also for Jaeger, SkyWalking, see b3-propagation : x-b3-traceid x-b3-traceid x-b3-spanid x-b3-parentspanid x-b3-sampled x-b3-flags b3 For Datadog: x-datadog-trace-id x-datadog-parent-id x-datadog-sampling-priority For SkyWalking: sw8 For AWS X-Ray: x-amzn-trace-id For more information on how to use these headers, please see the Envoy documentation .\nRegardless of the language of your application, Envoy will generate the appropriate tracing headers for you at the Ingress Gateway and forward these headers to the upstream cluster. However, in order to utilize the distributed tracing feature, you must modify your application code to attach the tracing headers to upstream requests. Since neither the service mesh nor the application can automatically propagate these headers, you can integrate the agent for distributed tracing into the application or manually propagate these headers in the application code itself. Once the tracing headers are propagated to all upstream requests, Envoy will send the tracing data to the tracer’s back-end processing, and then you can view the tracing data in the UI.\nFor example, look at the code of the Productpage service in the Bookinfo application . You can see that it integrates the Jaeger client library and synchronizes the header generated by Envoy with the HTTP requests to the Details and Reviews services in the getForwardHeaders (request) function.\ndef getForwardHeaders(request): headers = {} # Using Jaeger agent to get the x-b3-* headers span = get_current_span() carrier = {} tracer.inject( span_context=span.context, format=Format.HTTP_HEADERS, carrier=carrier) headers.update(carrier) # Dealing with the non x-b3-* header manually if \u0026#39;user\u0026#39; in session: headers[\u0026#39;end-user\u0026#39;] = session[\u0026#39;user\u0026#39;] incoming_headers = [ \u0026#39;x-request-id\u0026#39;, \u0026#39;x-ot-span-context\u0026#39;, \u0026#39;x-datadog-trace-id\u0026#39;, \u0026#39;x-datadog-parent-id\u0026#39;, \u0026#39;x-datadog-sampling-priority\u0026#39;, \u0026#39;traceparent\u0026#39;, \u0026#39;tracestate\u0026#39;, \u0026#39;x-cloud-trace-context\u0026#39;, \u0026#39;grpc-trace-bin\u0026#39;, \u0026#39;sw8\u0026#39;, \u0026#39;user-agent\u0026#39;, \u0026#39;cookie\u0026#39;, \u0026#39;authorization\u0026#39;, \u0026#39;jwt\u0026#39;, ] for ihdr in incoming_headers: val = request.headers.get(ihdr) if val is not None: headers[ihdr] = val return headers For more information, the Istio documentation provides answers to frequently asked questions about distributed tracing in Istio.\nHow to Choose A Distributed Tracing System Distributed …","relpermalink":"/en/blog/distributed-tracing-with-skywalking-in-istio/","summary":"This blog will guide you to use SkyWalking for distributed tracing with Istio.","title":"How to Use SkyWalking for Distributed Tracing in Istio?"},{"content":"The Istio service mesh offers cloud native deployments a standard way to implement automatic mutual transport layer security (mTLS) . This reduces the attack surface of network communication by using strong identities to establish encrypted channels between workloads within the mesh that are both confidential and tamper-resistant. mTLS is a key component for building zero-trust application networks. To understand mTLS traffic encryption in Istio, this article will cover the following:\nAn overview of TLS, mTLS, and TLS termination An introduction to howTLS encryption works in Istio How to use Istio to implement mTLS in Kubernetes A discussion of when you do and don’t need mTLS What Is TLS and mTLS? TLS, the successor to Secure Sockets Layer (SSL), is a widely adopted security protocol used to create authenticated and encrypted connections between networked computers. For this reason, people often use the terms TLS and SSL interchangeably. In this article, we will refer to them collectively as TLS. TLS 1.0 was released in 1999, and the latest version is 1.3 (released in August 2018); versions 1.0 and 1.1 are deprecated.\nThe HTTPS we see when browsing the web uses TLS, as shown in Figure 1, which is built on top of TCP as the session layer in the OSI model. To ensure compatibility, TLS usually uses port 443, but you can use any port you want.\nFigure 1: HTTP vs. HTTPS TLS encryption is required when a client needs to confirm the identity of the server in order to guard against man-in-the-middle attacks and ensure communication security. Figure 2 shows how TLS-encrypted communication proceeds.\nFigure 2: simplified TLS handshake flow The server applies for and obtains a certificate (X.509 certificate) from a trusted certificate authority (CA). A request from the client to the server containing information such as the TLS version and password combination supported by the client. The server responds to the client request and attaches a digital certificate. The client verifies the status, validity, and digital signature of the certificate and confirms the identity of the server. Encrypted communication commences between the client and the server using a shared private key. The above is only an outline description of the TLS communication flow. If you’re interested in the details, please see this in-depth discussion of the complete TLS handshake process. From the above process, you will find that the certificate is the critical element representing the server’s identity. The server must use a certificate issued by an authoritatively certified CA in order to provide public services over the Internet. In contrast, you can manage certificates using your own public key infrastructure (PKI) for services inside of a private environment.\nMutual TLS, also referred to as mTLS, is the use of a two-way encrypted channel between a server and a client that necessitates certificate exchange and identity authentication between the parties.\nWhat Is TLS Termination? TLS termination is the process of decrypting TLS-encrypted traffic before it is forwarded to a web server. Offloading TLS traffic to an ingress gateway or specialized device improves web application performance while securing encrypted traffic. TLS termination is typically implemented at cluster ingress. All communication between the ingress and servers in the cluster will be conducted directly over HTTP in plaintext, enhancing service performance.\nFigure 3: TLS termination By default, Istio enables mTLS for mesh-based services and ends TLS at the ingress gateway. Furthermore, you can pass through traffic to back-end services for processing.\napiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: sample-gateway spec: servers: - port: number: 443 name: https protocol: HTTPS tls: mode: PASSTHROUGH See Gateway TLS Configuration for details.\nHow to Implement Automatic mTLS in Istio Figure 4 depicts the security architecture of Istio. This figure clearly shows that at the entry point, JSON Web Token (JWT) + TLS authentication and encryption are used, and that mTLS is enabled between all services within the Istio mesh.\nIstio 安全架构图 Istio includes a built-in CA, and Secret Discovery Service (SDS) —one of the discovery services in Envoy xDS —enables the issuance and rotation of SVID certificates. The mTLS flow in the Istio mesh is as follows:\nThe sidecar of every service requests a certificate from Istiod on behalf of the workload at startup, and Istiod issues the SVID certificate (the process is more complex, and I will explain it in a future blog). The sidecar of every workload intercepts all client requests within the pod. The client sidecar starts an mTLS handshake with the server sidecar. During the handshake, the JWT and authentication filter in the client sidecar will authenticate the identity of the request, and store the identity in the filter metadata after the authentication. Then the request will go through the authorization filter to determine if the …","relpermalink":"/en/blog/understanding-the-tls-encryption-in-istio/","summary":"This article introduces TLS and mTLS, and describes how to enable mTLS in Istio and its application scenarios.","title":"How Istio’s mTLS Traffic Encryption Works as Part of a Zero Trust Security Posture"},{"content":"Ambient mesh is an experimental new deployment model recently introduced to Istio. It splits the duties currently performed by the Envoy sidecar into two separate components: a node-level component for encryption (called “ztunnel”) and an L7 Envoy instance deployed per service for all other processing (called “waypoint”). The ambient mesh model is an attempt to gain some efficiencies in potentially improved lifecycle and resource management. You can learn more about what ambient mesh is and how it differs from the Sidecar pattern here .\nThis article takes you step-by-step through a hands-on approach to the transparent traffic intercepting and routing of L4 traffic paths in the Istio’s Ambient mode. If you don’t know what Ambient mode is, this article can help you understand.\nIf you want to skip the actual hands-on steps and just want to know the L4 traffic path in Ambient mode, please see the figure below, it shows a Pod of Service A calling a Pod of Service B on a different node below.\nFigure 1: Transparent traffic intercepting and routing in the L4 network of Istio Ambient Mesh Principles Ambient mode uses tproxy and HTTP Based Overlay Network Environment (HBONE) as key technologies for transparent traffic intercepting and routing:\nUsing tproxy to intercept the traffic from the host Pod into the Ztunnel (Envoy Proxy). Using HBONE to establish a tunnel for passing TCP traffic between Ztunnels. What Is tproxy? tproxy is a transparent proxy supported by the Linux kernel since version 2.2, where the t stands for transparent. You need to enable NETFILTER_TPROXY and policy routing in the kernel configuration. With tproxy, the Linux kernel can act as a router and redirect packets to user space. See the tproxy documentation for details.\nWhat Is HBONE? HBONE is a method of providing tunneling capabilities using the HTTP protocol. A client sends an HTTP CONNECT request (which contains the destination address) to an HTTP proxy server to establish a tunnel, and the proxy server establishes a TCP connection to the destination on behalf of the client, which can then transparently transport TCP data streams to the destination server through the proxy. In Ambient mode, Ztunnel (Envoy inside) acts as a transparent proxy, using Envoy Internal Listener to receive HTTP CONNECT requests and pass TCP streams to the upstream cluster.\nEnvironment Before starting the hands-on, it is necessary to explain the demo environment, and the corresponding object names in this article:\nItems Name IP Service A Pod sleep-5644bdc767-2dfg7 10.4.4.19 Service B Pod productpage-v1-5586c4d4ff-qxz9f 10.4.3.20 Ztunnel A Pod ztunnel-rts54 10.4.4.18 Ztunnel B Pod ztunnel-z4qmh 10.4.3.14 Node A gke-jimmy-cluster-default-pool-d5041909-d10i 10.168.15.222 Node B gke-jimmy-cluster-default-pool-d5041909-c1da 10.168.15.224 Service B Cluster productpage 10.8.14.226 Because these names will be used in subsequent command lines, the text will use pronouns, so that you can experiment in your own environment.\nFor the tutorial, I installed Istio Ambient mode in GKE. You can refer to this Istio blog post for installation instructions. Be careful not to install the Gateway, so as not to enable the L7 functionality; otherwise, the traffic path will be different from the descriptions in this blog.\nIn the following, we will experiment and dive into the L4 traffic path of a pod of sleep service to a pod of productpage service on different nodes. We will look at the outbound and inbound traffic of the Pods separately.\nOutbound Traffic Intercepting The transparent traffic intercepting process for outbound traffic from a pod in Ambient mesh is as follows:\nIstio CNI creates the istioout NIC and iptables rules on the node, adds the Pods’ IP in Ambient mesh to the IP set , and transparently intercepts outbound traffic from Ambient mesh to pistioout virtual NIC through Geneve (Generic Network Virtualization Encapsulation) tunnels with netfilter nfmark tags and routing rules. The init container in Ztunnel creates iptables rules that forward all traffic from the pistioout NIC to port 15001 of the Envoy proxy in Ztunnel. Envoy processes the packets and establishes an HBONE tunnel (HTTP CONNECT) with the upstream endpoints to forward the packets upstream. Check The Routing Rules On Node A Log in to Node A, where Service A is located, and use iptables-save to check the rules.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 $ iptables-save /* omit */ -A PREROUTING -j ztunnel-PREROUTING -A PREROUTING -m comment --comment \u0026#34;kubernetes service portals\u0026#34; -j KUBE-SERVICES -A ztunnel-POSTROUTING -m mark --mark 0x100/0x100 -j ACCEPT -A ztunnel-PREROUTING -m mark --mark 0x100/0x100 -j ACCEPT /* omit */ *mangle /* omit */ -A PREROUTING -j ztunnel-PREROUTING -A INPUT -j ztunnel-INPUT -A FORWARD -j ztunnel-FORWARD -A OUTPUT -j ztunnel-OUTPUT -A OUTPUT -s 169.254.169.254/32 -j DROP -A POSTROUTING -j ztunnel-POSTROUTING -A ztunnel-FORWARD -m mark …","relpermalink":"/en/blog/ambient-mesh-l4-traffic-path/","summary":"This article details transparent traffic intercepting and L4 traffic paths in Ambient Mesh in both diagrammatic and hands-on form.","title":"Transparent Traffic Intercepting and Routing in the L4 Network of Istio Ambient Mesh"},{"content":"In September 2022, Istio became a CNCF incubation project and launched the new Ambient Mesh . With CNCF’s strong community and marketing resources, and Ambient Mesh further lowering the barrier to trying Istio, the five year old open source project has been revitalized.\nIf you don’t know about service mesh and Istio, or are curious about the future of Istio, this eBook—The Current State and Future of the Istio Service Mesh will give you the answers. The following is an excerpt from the book. In my view, the future of Istio lies in being the infrastructure for zero-trust network and hybrid cloud.\nZero Trust Zero trust is an important topic, including when I spoke at IstioCon 2022. Istio is becoming an important part of zero trust, the most important element of which is identity-oriented control rather than network-oriented control.\nWhat Is Zero Trust? Zero trust is a security philosophy, not a best practice that all security teams follow in the same way. The concept of zero trust was proposed to bring a more secure network to the cloud-native world. Zero trust is a theoretical state where all consumers within a network not only have no authority but also have no awareness of the surrounding network. The main challenges of zero trust are the increasingly granular authorization and the need for a time limit for user authorization.\nAuthentication Istio 1.14 adds support for the SPIFFE Runtime Environment (SPIRE). SPIRE, a CNCF incubation project, is an implementation of the Secure Production Identity Framework for Everyone (SPIFFE), also a CNCF Incubation Project. In Kubernetes, we use ServiceAccount to provide identity information for workloads in Pods, and its core is based on Token (using Secret resource storage) to represent workload identity. A token is a resource in a Kubernetes cluster. How to unify the identities of multiple clusters and workloads running in non-Kubernetes environments (such as virtual machines)? That’s what SPIFFE is trying to solve.\nThe purpose of SPIFFE is to establish an open and unified workload identity standard based on the concept of zero trust, which helps to establish a fully identifiable data center network with zero trust. The core of SPIFFE is to define a short-lived encrypted identity document—SPIFFE Verifiable Identity Document (SVID)—through a simple API, which is used as an identity document (based on an X.509 certificate or JWT token) for workload authentication. SPIRE can automatically rotate SVID certificates and keys according to administrator-defined policies, dynamically provide workload identities, and Istio can dynamically consume these workload identities through SPIRE.\nThe Kubernetes-based SPIRE architecture diagram is shown below.\nFigure 1: SPIRE deployed in Kubernetes Istio originally used the Citadel service in Istiod to be responsible for certificate management in the service mesh, and issued the certificate to the data plane through the xDS (to be precise, SDS API) protocol. With SPIRE, the work of certificate management is handed over to SPIRE Server. SPIRE also supports the Envoy SDS API. After we enable SPIRE in Istio, the traffic entering the workload pod will be authenticated once after being transparently intercepted into the sidecar. The purpose of authentication is to compare the identity of the workload with the environment information it runs on (node, Pod’s ServiceAccount and Namespace, etc.) to prevent identity forgery. Please refer to How to Integrate SPIRE in Istio to learn how to use SPIRE for authentication in Istio.\nWe can deploy SPIRE in Kubernetes using the Kubernetes Workload Registrar, which automatically registers the workload in Kubernetes for us and generates an SVID. The registration machine is a Server-Agent architecture, which deploys a SPIRE Agent on each node, and the Agent communicates with the workload through a shared UNIX Domain Socket. The following diagram shows the process of using SPIRE for authentication in Istio.\nFigure 2: SPIRE-based workload authentication process in Istio The steps to using SPIRE for workload authentication in Istio are as follows:\nTo obtain the SIVD, the SPIRE Agent is referred to as a pilot-agent via shared UDS. The SPIRE Agent asks Kubernetes (to be precise, the kubelet on the node) for load information. The kubelet returns the information queried from the API server to the workload validator. The validator compares the result returned by the kubelet with the identity information shared by the sidecar. If it is the same, it returns the correct SVID cache to the workload. If it is different, the authentication fails. Please refer to the SPIRE documentation for the detailed process of registering and authenticating workloads.\nNGAC When each workload has an accurate identity, how can the permissions of these identities be restricted? Role-based access control (RBAC) is used by default in Kubernetes for access control. As the name suggests, this access control is based on roles. Although it is …","relpermalink":"/en/blog/the-future-of-istio/","summary":"The future of Istio lies in being the infrastructure for zero-trust network and hybrid cloud.","title":"The Future of Istio: the Path to Zero Trust Security"},{"content":"In this blog, you will learn about the Kubernetes Ingress Gateway, the Gateway API, and the emerging Gateway API trend, which enables the convergence of Kubernetes and service mesh.\nTakeaways Ingress, the original gateway for Kubernetes, has a resource model that is too simple to fit into today’s programmable networks. The Gateway API , the latest addition to the Kubernetes portal gateway, separates concerns through role delineation and provides cross-namespace support to make it more adaptable to multi-cloud environments. Most API gateways already support it. The Gateway API provides a new reference model for the convergence of ingress gateways (north-south) and service mesh (east-west, cross-cluster routing), where there is a partial functional overlap. History of the Kubernetes ingress gateway When Kubernetes was launched in June 2014, only NodePort and LoadBalancer-type Service objects were available to expose services within the cluster to the outside world. Later, Ingress was introduced to offer more control over incoming traffic.. To preserve its portability and lightweight design, the Ingress API matured more slowly than other Kubernetes APIs; it was not upgraded to GA until Kubernetes 1.19.\nIngress’ primary objective is to expose HTTP applications using a straightforward declarative syntax. When creating an Ingress or setting a default IngressClass in Kubernetes, you can deploy several Ingress Controllers and define the controller the gateway uses via IngressClass. Kubernetes currently supports only AWS, GCE, and Nginx Ingress controllers by default; many third-party ingress controllers are also supported.\nThe following diagram illustrates the workflow of Kubernetes Ingress.\nFigure 1: Kubernetes ingress workflow The detailed process is as follows:\nKubernetes cluster administrators deploy an Ingress Controller in Kubernetes. The Ingress Controller continuously monitors changes to IngressClass and Ingress objects in the Kubernetes API Server. Administrators apply IngressClass and Ingress to deploy the gateway. Ingress Controller creates the corresponding ingress gateway and configures the routing rules according to the administrator’s configuration. If in the cloud, the client accesses the load balancer for that ingress gateway. The gateway will route the traffic to the corresponding back-end service based on the host and path in the HTTP request. Istio supports both the Ingress and Gateway APIs. Below is an example configuration using the Istio Ingress Gateway, which will be created later using the Gateway API:\napiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: istio spec: controller: istio.io/ingress-controller --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress spec: ingressClassName: istio rules: - host: httpbin.example.com http: paths: - path: / pathType: Prefix backend: service: name: httpbin port: 8000 Note: You must specify the IngressClass in the ingressClassName field in the Ingress spec. Otherwise, the ingress gateway will not be created.\nLimitations of Kubernetes Ingress Although IngressClass decouples the ingress gateway from the back-end implementation, it still has significant limitations.\nIngress is too simple for most real-world use and it only supports HTTP protocol routing. It only supports host and path matching, and there is no standard configuration for advanced routing features, which can only be achieved through annotation, such as URL redirection using Nginx Ingress Controller, which requires configuration of nginx.ingress.kubernetes.io/rewrite-target annotation, which is no longer adaptable to the needs of a programmable proxy. The situation where services in different namespaces must be bound to the same gateway often arises in practical situations where the ingress gateway cannot be shared across multiple namespaces. No delineation of responsibilities for creating and managing ingress gateways, resulting in developers having to not only configure gateway routes but also create and manage gateways themselves. Kubernetes Gateway API The Gateway API is a collection of API resources: GatewayClass, Gateway, HTTPRoute, TCPRoute, ReferenceGrant, etc. The Gateway API exposes a more generic proxy API that can be used for more protocols than HTTP and models more infrastructure components, providing better deployment and management options for cluster operations.\nIn addition, the Gateway API achieves configuration decoupling by separating resource objects that people can manage in different roles. The following diagram shows the roles and objects in the Gateway API.\nFigure: Roles and componentes in Kubernetes Gateway API The following is an example of using the Gateway API in Istio.\napiVersion: gateway.networking.k8s.io/v1alpha2 kind: Gateway metadata: name: gateway namespace: istio-ingress spec: gatewayClassName: istio listeners: - name: default hostname: \u0026#34;*.example.com\u0026#34; port: 80 protocol: HTTP allowedRoutes: namespaces: from: All --- …","relpermalink":"/en/blog/why-gateway-api-is-the-future-of-ingress-and-mesh/","summary":"This article introduces the ingress gateway and Gateway API in Kubernetes, the new trend of them with service mesh.","title":"Why the Gateway API Is the Unified Future of Ingress for Kubernetes and Service Mesh"},{"content":"Istio 1.14 was released in June of this year, and one of the most notable features of this release is support for SPIRE , which is one of the implementations of SPIFFE , a CNCF incubation project. This article explains what SPIRE means for zero-trust architectures and why you would need SPIRE for authentication in Istio.\nAuthentication in Kubernetes We all know that Istio was built for and typically runs on Kubernetes, so before we talk about how to use SPIRE for authentication in Istio, let’s take a look at how Kubernetes handles authentication.\nLet’s look at an example of a pod’s token. Whenever a pod gets created in Kubernetes, it gets assigned a default service account from the namespace, assuming we didn’t explicitly assign a service account to it. Here is an example:\napiVersion: v1 data: ca.crt: {CA_CRT} namespace: ZGVmYXVsdA== token: {TOKEN_STRING} kind: Secret metadata: annotations: kubernetes.io/service-account.name: sleep kubernetes.io/service-account.uid: 2c0d00e8-13a2-48d0-9ff8-f987f3325ecf creationTimestamp: \u0026#34;2022-06-14T03:01:35Z\u0026#34; name: sleep-token-gwhwd namespace: default resourceVersion: \u0026#34;244535398\u0026#34; uid: b8822ceb-9553-4a17-96dc-d525bbaed0e0 type: kubernetes.io/service-account-token Kubernetes manages the identity of Pods with Service Accounts and then specifies the permissions of Pods with a Service Account to the Kubernetes API using RBAC. A service account’s token is stored in a secret, which does not include a declaration of the node or pod where the workload is running. When a malicious actor steals a token, they gain full access to the account and can steal information or carry out sabotage under the guise of that user.\nA token can only be used to identify a workload in one cluster, but Istio supports multiple clusters and meshes, as well as Kubernetes environments and virtual machines. A unified workload identity standard can help here.\nAn Introduction to SPIFFE and SPIRE SPIFFE’s (Secure Production Identity Framework for Everyone) goal is to create a zero-trust, fully-identified data center network by establishing an open, unified workload identity standard based on the concept of zero-trust. SPIRE can rotate X.509 SVID certificates and secret keys on a regular basis. Based on administrator-defined policies, SPIRE can dynamically provision workload certificates and Istio can consume them. I’ll go over some of the terms associated with SPIFFE in a little more detail below.\nSPIRE (SPIFFE Runtime Environment) is a SPIFFE implementation that is ready for production. SVID (SPIFFE Verifiable Identity Document) is the document that a workload uses to prove its identity to a resource or caller. SVID contains a SPIFFE ID that represents the service’s identity. It uses an X.509 certificate or a JWT token to encode the SPIFFE ID in a cryptographically verifiable document. The SPIFFE ID is a URI that looks like this: spiffe://trust_domain/workload_identifier.\nSPIFFE and Zero Trust Security The essence of Zero Trust is identity-centric dynamic access control. SPIFFE addresses the problem of identifying workloads.\nWe might identify a workload using an IP address and port in the era of virtual machines, but IP address-based identification is vulnerable to multiple services sharing an IP address, IP address forgery, and oversized access control lists. Because containers have a short lifecycle in the Kubernetes era, instead of an IP address, we rely on a pod or service name. However, different clouds and software platforms approach workload identity differently, and there are compatibility issues. This is especially true in heterogeneous hybrid clouds, where workloads run on both virtual machines and Kubernetes. It is critical to establish a fine-grained, interoperable identification system at this point.\nUsing SPIRE for Authentication in Istio With the introduction of SPIRE to Istio, we can give each workload a unique identity, which is used by workloads in the service mesh for peer authentication, request authentication, and authorization policies. The SPIRE Agent issues SVIDs for workloads by communicating with a shared UNIX Domain Socket in the workload. The Envoy proxy and the SPIRE agent communicate through the Envoy SDS (Secret Discovery Service) API. Whenever an Envoy proxy needs to access secrets (certificates, keys, or anything else needed to do secure communication), it will talk to the SPIRE agent through Envoy’s SDS API.\nThe most significant advantage of SDS is the ease with which certificates can be managed. Without this feature, certificates would have to be created as a secret and then mounted into the agent container in a Kubernetes deployment. The secret must be updated, and the proxy container must be re-deployed if the certificate expires. Using SDS, Istio can push the certificates to all Envoy instances in the service mesh. If the certificate expires, the server only needs to push the new certificate to the Envoy instance; Envoy will use the new certificate right away, and the …","relpermalink":"/en/blog/why-istio-need-spire/","summary":"This article explains what SPIRE means for zero-trust architectures and why you would need SPIRE for authentication in Istio.","title":"Why Would You Need Spire for Authentication With Istio?"},{"content":" It’s been more than 5 years since Google, IBM, and Lyft unveiled the Istio open source project in May 2017. The Istio project has developed from a seed to a tall tree in these years. Many domestic books on the Istio service mesh were launched in the two years following the release of Istio 1.0 in 2018. My country is at the forefront of the world in the field of Istio book publishing.\nService mesh: one of the core technologies of cloud native Today, Istio is nearly synonymous with service mesh in China. The development of service mesh, as one of the core cloud-native technologies described by CNCF (Cloud Native Computing Foundation), has gone through the following stages.\n2017-2018: Exploratory Phase 2019-2020: Early Adopter Phase 2021 to present: Implementation on a large scale and ecological development stage Cloud native technology enables enterprises to design and deploy elastically scalable applications in new dynamic settings such as public, private, and hybrid clouds, according to the CNCF. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs are examples of cloud native technology.\nService mesh has been included to the CNCF definition of cloud native, indicating that it is one of the representative technologies of cloud native. Google is donating Istio to CNCF today, and we have reason to expect that as a CNCF project, Istio’s community will be more open, and its future development will be more smooth.\nService mesh and cloud native applications Cloud-native development is gaining traction. Despite the frequent emergence of new technologies and products, service mesh has maintained its place as “cloud-native network infrastructure” as part of the overall cloud-native technology stack throughout the past year. The cloud-native technology stack model is depicted in the diagram below, with representative technologies for each layer to define the standard. Service mesh and other cloud-native technologies complement each other as a new era of middleware emerges. Dapr (Distributed Application Runtime) defines the cloud-native middleware capability model, OAM defines the cloud-native application model, and so on, whereas service mesh Lattice defines a cloud-native seven-layer network model.\nCloud Native Application Model Why you need a service mesh Using a service mesh isn’t tantamount to abandoning Kubernetes; it just makes sense. The goal of Kubernetes is to manage application lifecycles through declarative configuration, whereas the goal of service mesh is to provide traffic control, security management, and observability amongst apps. How do you set up load balancing and flow management for calls between services after a robust microservice platform has been developed with Kubernetes?\nMany open source tools, including Istio, Linkerd, MOSN, and others, support Envoy’s xDS protocol. The specification of xDS is Envoy’s most significant contribution to service mesh or cloud native. Many various usage cases, such as API gateways, sidecar proxies in service meshes, and edge proxies, are derived from Envoy, which is simply a network proxy, a modern version of the proxy configured through the API.\nIn a nutshell, the move from Kubernetes to Istio was made for the following reasons.\nApplication life cycle management, specifically application deployment and management, is at the heart of Kubernetes (scaling, automatic recovery, and release). Kubernetes is a microservices deployment and management platform that is scalable and extremely elastic. Transparent proxy is the cornerstone of service mesh, which intercepts communication between microservices via sidecar proxy and then regulates microservice behavior via control plane settings. The deployment mode of service meshes has introduced new issues today. For service meshes, sidecar is no longer required, and an agentless service mesh based on gRPC is also being tested. xDS is a protocol standard for configuring service meshes, and a gRPC-based xDS is currently being developed. Kubernetes traffic management is decoupled with the service mesh. The kube-proxy component is not required to support traffic within the service mesh. The traffic between services is controlled by an abstraction close to the microservice application layer to achieve security and observability features. In Kubernetes, service mesh is an upper-level abstraction of service, and Serverless is the next stage, which is why Google released Knative based on Kubernetes and Istio following Istio. Open source in the name of the community The ServiceMesher community was founded in May 2018 with the help of Ant Financial. Following that, a tornado of service meshes erupted in China, and the community-led translation of Istio’s official documentation reached a fever pitch.\nI became aware of a dearth of Chinese resources for systematically teaching Istio over time, so in September 2018, I began to plan and create an Istio book, launching the Istio Handbook open source …","relpermalink":"/en/blog/istio-service-mesh-book/","summary":"By the Cloud Native Community(China)","title":"In-Depth Understanding of Istio: Announcing the Publication of a New Istio Book"},{"content":"This article will guide you on how to compile the Istio binaries and Docker images on macOS.\nBefore you begin Before we start, refer to the Istio Wiki , here is the information about my build environment.\nmacOS 12.3.1 Darwin AMD64 Docker Desktop 4.8.1(78998) Docker Engine v20.10.14 Start to compile First, download the Istio code from GitHub to the $GOPATH/src/istio.io/istio directory, and execute the commands below in that root directory.\nCompile into binaries Execute the following command to download the Istio dependent packages, which will be downloaded to the vendor directory.\ngo mod vendor Run the following command to build Istio:\nsudo make build If you do not run the command with sudo, you may encounter the following error.\nfatal: unsafe repository (\u0026#39;/work\u0026#39; is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work fatal: unsafe repository (\u0026#39;/work\u0026#39; is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work Makefile.core.mk:170: *** \u0026#34;TAG cannot be empty\u0026#34;. Stop. make: *** [build] Error 2 Even if you follow the prompts and run git config --global --add safe.directory /work, you will still get errors during compilation.\nThe compiled binary will be saved in out directory with the following directory structure.\nout ├── darwin_amd64 │ ├── bug-report │ ├── client │ ├── envoy │ ├── extauthz │ ├── install-cni │ ├── istio-cni │ ├── istio-cni-taint │ ├── istio-iptables │ ├── istio_is_init │ ├── istioctl │ ├── logs │ ├── operator │ ├── pilot-agent │ ├── pilot-discovery │ ├── release │ └── server └── linux_amd64 ├── envoy ├── envoy-centos ├── logs └── release It will build both the linux_amd64 and darwin_amd64 architectures binaries at the same time.\nCompile into Docker images Run the following command to compile Istio into a Docker image.\nsudo make docker The compilation will take about 3 to 5 minutes depending on your network. Once the compilation is complete, you will see the Docker image of Istio by running the following command.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE localhost:5000/app_sidecar_centos_7 latest 2044037df94b 51 seconds ago 524MB localhost:5000/app_sidecar_ubuntu_jammy latest 5d8ae5ed55b7 About a minute ago 362MB localhost:5000/proxyv2 latest d4679412385f About a minute ago 243MB localhost:5000/install-cni latest 78f46d5771d2 About a minute ago 270MB localhost:5000/istioctl latest c38130a5adc8 About a minute ago 190MB localhost:5000/pilot latest 2aa9185ec202 About a minute ago 190MB localhost:5000/app latest 473adafaeb8d About a minute ago 188MB localhost:5000/operator latest 9ac1fedcdd12 About a minute ago 191MB localhost:5000/ext-authz latest 1fb5aaf20791 About a minute ago 117MB localhost:5000/app_sidecar_debian_11 latest 61376a02b95d 2 minutes ago 407MB localhost:5000/app_sidecar_ubuntu_xenial latest 7e8efe666611 2 minutes ago 418MB You can change the image name and push it into your own container registry.\nSummary This is how to build Istio on macOS. If you have already downloaded the Docker image you need to build, the build will take less than a minute. It also takes only a few minutes to build Docker images.\nReference Using the Code Base - github.com ","relpermalink":"/en/blog/how-to-build-istio/","summary":"This article will guide you on how to compile the Istio binaries on macOS.","title":"How to Build Istio?"},{"content":"Updated on May 6, 2022\nBased on Istio version 1.13, this article will present the following.\nWhat is the sidecar pattern and what advantages does it have? How are the sidecar injections done in Istio? How does the sidecar proxy do transparent traffic intercepting? How is the traffic routed to upstream? The figure below shows how the productpage service requests access to http://reviews.default.svc.cluster.local:9080/ and how the sidecar proxy inside the reviews service does traffic blocking and routing forwarding when traffic goes inside the reviews service.\nIstio transparent traffic intercepting and traffic routing diagram At the beginning of the first step, the sidecar in the productpage pod has selected a pod of the reviews service to be requested via EDS, knows its IP address, and sends a TCP connection request.\nThere are three versions of the reviews service, each with an instance, and the sidecar work steps in the three versions are similar, as illustrated below only by the sidecar traffic forwarding step in one of the Pods.\nSidecar pattern Dividing the functionality of an application into separate processes running in the same minimal scheduling unit (e.g. Pod in Kubernetes) can be considered sidecar mode. As shown in the figure below, the sidecar pattern allows you to add more features next to your application without additional third-party component configuration or modifications to the application code.\nSidecar pattern The Sidecar application is loosely coupled to the main application. It can shield the differences between different programming languages and unify the functions of microservices such as observability, monitoring, logging, configuration, circuit breaker, etc.\nAdvantages of using the Sidecar pattern When deploying a service mesh using the sidecar model, there is no need to run an agent on the node, but multiple copies of the same sidecar will run in the cluster. In the sidecar deployment model, a companion container (such as Envoy or MOSN) is deployed next to each application’s container, which is called a sidecar container. The sidecar takes overall traffic in and out of the application container. In Kubernetes’ Pod, a sidecar container is injected next to the original application container, and the two containers share storage, networking, and other resources.\nDue to its unique deployment architecture, the sidecar model offers the following advantages.\nAbstracting functions unrelated to application business logic into a common infrastructure reduces the complexity of microservice code. Reduce code duplication in microservices architectures because it is no longer necessary to write the same third-party component profiles and code. The sidecar can be independently upgraded to reduce the coupling of application code to the underlying platform. iptables manipulation analysis In order to view the iptables configuration, we need to nsenter the sidecar container using the root user to view it, because kubectl cannot use privileged mode to remotely manipulate the docker container, so we need to log on to the host where the productpage pod is located.\nIf you use Kubernetes deployed by minikube, you can log directly into the minikube’s virtual machine and switch to root. View the iptables configuration that lists all the rules for the NAT (Network Address Translation) table because the mode for redirecting inbound traffic to the sidecar is REDIRECT in the parameters passed to the istio-iptables when the Init container is selected for the startup, so there will only be NAT table specifications in the iptables and mangle table configurations if TPROXY is selected. See the iptables command for detailed usage.\nWe only look at the iptables rules related to productpage below.\n# login to minikube, change user to root $ minikube ssh $ sudo -i # See the processes in the productpage pod\u0026#39;s istio-proxy container $ docker top `docker ps|grep \u0026#34;istio-proxy_productpage\u0026#34;|cut -d \u0026#34; \u0026#34; -f1` UID PID PPID C STIME TTY TIME CMD 1337 10576 10517 0 08:09 ? 00:00:07 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage.default --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istiod.istio-system.svc:15012 --zipkinAddress zipkin.istio-system:9411 --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --connectTimeout 10s --proxyAdminPort 15000 --concurrency 2 --controlPlaneAuthPolicy NONE --dnsRefreshRate 300s --statusPort 15020 --trust-domain=cluster.local --controlPlaneBootstrap=false 1337 10660 10576 0 08:09 ? 00:00:33 /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage.default --service-node sidecar~172.17.0.16~productpage-v1-7f44c4d57c-ksf9b.default~default.svc.cluster.local --max-obj-name-len 189 --local-address-ip-version v4 --log-format [Envoy (Epoch 0)] [%Y-%m-%d …","relpermalink":"/en/blog/sidecar-injection-iptables-and-traffic-routing/","summary":"Learn the sidecar pattern, transparent traffic intercepting and routing in Istio.","title":"Understanding the Sidecar Injection, Traffic Intercepting \u0026 Routing Process in Istio"},{"content":"This article will explain:\nThe sidecar auto-injection process in Istio The init container startup process in Istio The startup process of a Pod with Sidecar auto-injection enabled The following figure shows the components of a Pod in the Istio data plane after it has been started.\nIstio data plane pod Sidecar injection in Istio The following two sidecar injection methods are available in Istio.\nManual injection using istioctl. Kubernetes-based mutating webhook admission controller automatic sidecar injection method. Whether injected manually or automatically, SIDECAR’s injection process follows the following steps.\nKubernetes needs to know the Istio cluster to which the sidecar to be injected is connected and its configuration. Kubernetes needs to know the configuration of the sidecar container itself to be injected, such as the image address, boot parameters, etc. Kubernetes injects the above configuration into the side of the application container by the sidecar injection template and the configuration parameters of the above configuration-filled sidecar. The sidecar can be injected manually using the following command.\nistioctl kube-inject -f ${YAML_FILE} | kuebectl apply -f - This command is injected using Istio’s built-in sidecar configuration, see the Istio official website for details on how to use Istio below.\nWhen the injection is complete you will see that Istio has injected initContainer and sidecar proxy-related configurations into the original pod template.\nInit container The Init container is a dedicated container that runs before the application container is launched and is used to contain some utilities or installation scripts that do not exist in the application image.\nMultiple Init containers can be specified in a Pod, and if more than one is specified, the Init containers will run sequentially. The next Init container can only be run if the previous Init container must run successfully. Kubernetes only initializes the Pod and runs the application container when all the Init containers have been run.\nThe Init container uses Linux Namespace, so it has a different view of the file system than the application container. As a result, they can have access to Secret in a way that application containers cannot.\nDuring Pod startup, the Init container starts sequentially after the network and data volumes are initialized. Each container must be successfully exited before the next container can be started. If exiting due to an error will result in a container startup failure, it will retry according to the policy specified in the Pod’s restartPolicy. However, if the Pod’s restartPolicy is set to Always, the restartPolicy is used when the Init container failed.\nThe Pod will not become Ready until all Init containers are successful. The ports of the Init containers will not be aggregated in the Service. The Pod that is being initialized is in the Pending state but should set the Initializing state to true. The Init container will automatically terminate once it is run.\nSidecar injection example analysis For a detailed YAML configuration for the bookinfo applications, see bookinfo.yaml for the official Istio YAML of productpage in the bookinfo sample.\nThe following will be explained in the following terms.\nInjection of Sidecar containers Creation of iptables rules The detailed process of routing apiVersion: apps/v1 kind: Deployment metadata: name: productpage-v1 labels: app: productpage version: v1 spec: replicas: 1 selector: matchLabels: app: productpage version: v1 template: metadata: labels: app: productpage version: v1 spec: serviceAccountName: bookinfo-productpage containers: - name: productpage image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9080 volumeMounts: - name: tmp mountPath: /tmp volumes: - name: tmp emptyDir: {} Let’s see the productpage container’s Dockerfile .\nFROM python:3.7.4-slim COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt COPY test-requirements.txt ./ RUN pip install --no-cache-dir -r test-requirements.txt COPY productpage.py /opt/microservices/ COPY tests/unit/* /opt/microservices/ COPY templates /opt/microservices/templates COPY static /opt/microservices/static COPY requirements.txt /opt/microservices/ ARG flood_factor ENV FLOOD_FACTOR ${flood_factor:-0} EXPOSE 9080 WORKDIR /opt/microservices RUN python -m unittest discover USER 1 CMD [\u0026#34;python\u0026#34;, \u0026#34;productpage.py\u0026#34;, \u0026#34;9080\u0026#34;] We see that ENTRYPOINT is not configured in Dockerfile, so CMD’s configuration python productpage.py 9080 will be the default ENTRYPOINT, keep that in mind and look at the configuration after the sidecar injection.\n$ istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml We intercept only a portion of the YAML configuration that is part of the Deployment configuration associated with productpage.\ncontainers: - image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 # application image name: …","relpermalink":"/en/blog/istio-pod-process-lifecycle/","summary":"This article will explain Istio's Init container, Pod internal processes and the startup process.","title":"Istio Data Plane Pod Startup Process Explained"},{"content":"iptables is an important feature in the Linux kernel and has a wide range of applications. iptables is used by default in Istio for transparent traffic hijacking. Understanding iptables is very important for us to understand how Istio works. This article will give you a brief introduction to iptbles.\niptables introduction iptables is a management tool for netfilter, the firewall software in the Linux kernel. netfilter is located in the user space and is part of netfilter. netfilter is located in the kernel space and has not only network address conversion, but also packet content modification and packet filtering firewall functions.\nBefore learning about iptables for Init container initialization, let’s go over iptables and rule configuration.\nThe following figure shows the iptables call chain.\niptables 调用链 iptables The iptables version used in the Init container is v1.6.0 and contains 5 tables.\nRAW is used to configure packets. Packets in RAW are not tracked by the system. The filter is the default table used to house all firewall-related operations. NAT is used for network address translation (e.g., port forwarding). Mangle is used for modifications to specific packets (refer to corrupted packets). Security is used to force access to control network rules. Note: In this example, only the NAT table is used.\nThe chain types in the different tables are as follows.\nRule name raw filter nat mangle security PREROUTING ✓ ✓ ✓ INPUT ✓ ✓ ✓ OUTPUT ✓ ✓ ✓ ✓ ✓ POSTROUTING ✓ ✓ FORWARD ✓ ✓ ✓ Understand iptables rules View the default iptables rules in the istio-proxy container, the default view is the rules in the filter table.\n$ iptables -L -v Chain INPUT (policy ACCEPT 350K packets, 63M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 18M packets, 1916M bytes) pkts bytes target prot opt in out source destination We see three default chains, INPUT, FORWARD, and OUTPUT, with the first line of output in each chain indicating the chain name (INPUT/FORWARD/OUTPUT in this case), followed by the default policy (ACCEPT).\nThe following is a proposed structure diagram of iptables, where traffic passes through the INPUT chain and then enters the upper protocol stack, such as:\niptables chains Multiple rules can be added to each chain and the rules are executed in order from front to back. Let’s look at the table header definition of the rule.\nPKTS: Number of matched messages processed bytes: cumulative packet size processed (bytes) Target: If the message matches the rule, the specified target is executed. PROT: Protocols such as TDP, UDP, ICMP, and ALL. opt: Rarely used, this column is used to display IP options. IN: Inbound network interface. OUT: Outbound network interface. source: the source IP address or subnet of the traffic, the latter being anywhere. destination: the destination IP address or subnet of the traffic, or anywhere. There is also a column without a header, shown at the end, which represents the options of the rule, and is used as an extended match condition for the rule to complement the configuration in the previous columns. prot, opt, in, out, source and destination and the column without a header shown after destination together form the match rule. TARGET is executed when traffic matches these rules.\nTypes supported by TARGET\nTarget types include ACCEPT, REJECT, DROP, LOG, SNAT, MASQUERADE, DNAT, REDIRECT, RETURN or jump to other rules, etc. You can determine where the telegram is going by executing only one rule in a chain that matches in order, except for the RETURN type, which is similar to the return statement in programming languages, which returns to its call point and continues to execute the next rule.\nFrom the output, you can see that the Init container does not create any rules in the default link of iptables, but instead creates a new link.\nSummary With the above brief introduction to iptables, you have understood how iptables works, the rule chain and its execution order.\n","relpermalink":"/en/blog/understanding-iptables/","summary":"This article will give you a brief introduction to iptables, its tables and the order of execution.","title":"Understanding IPTables"},{"content":"See the cloud native public library at: https://lib.jimmysong.io/ The cloud native public library project is a documentation project built using the Wowchemy theme, open sourced on GitHub .\nI have also adjusted the home page, menu and directory structure of the site, and the books section of the site will be maintained using the new theme.\nCloud native library positioning The cloud native public library is a collection of cloud native related books and materials published and translated by the author since 2017, and is a compendium and supplement to the dozen or so books already published. The original materials will continue to be published in the form of GitBooks, and the essence and related content will be sorted into the cloud native public library through this project.\nIn addition, the events section of this site has been revamped and moved to a new page .\n","relpermalink":"/en/notice/cloud-native-public-library/","summary":"A one-stop cloud native library that is a compendium of published materials.","title":"Announcement of Cloud Native Library"},{"content":"In my last two blogs:\nSidecar injection, transparent traffic hijacking , and routing process in Istio explained in detail Traffic types and iptables rules in Istio sidecar explained I gave you a detailed overview of the traffic in the Istio data plane, but the data plane does not exist in isolation. This article will show you the ports and their usages for each component of both the control plane and data plane in Istio, which will help you understand the relationship between these flows and troubleshoot them.\nOverview Firstly, I will show you a global schematic. The following figure shows the components of a sidecar in the Istio data plane, and the objects that interact with it.\nIstio components We can use the nsenter command to enter the namespace of the productpage Pod of the Bookinfo example and see the information about the ports it is listening on internally.\nIstio sidecar ports From the figure, we can see that besides the port 9080 that the productpage application listens to, the Sidecar container also listens to a large number of other ports, such as 15000, 15001, 15004, 15006, 15021, 15090, etc. You can learn about the ports used in Istio in the Istio documentation .\nLet’s go back into the productpage Pod and use the lsof -i command to see the ports it has open, as shown in the following figure.\nProductpage Pod ports We can see that there is a TCP connection established between the pilot-agent and istiod, the port in the listening described above, and the TCP connection established inside the Pod, which corresponds to the figure at the beginning of the article.\nThe root process of the Sidecar container (istio-proxy) is pilot-agent, and the startup command is shown below.\nInternal procecces in Sidecar As we can see from the figure, the PID of its pilot-agent process is 1, and it forked the Envoy process.\nCheck the ports it opens in Istiod, as shown in the figure below.\nIstiod ports We can see the ports that are listened to, the inter-process and remote communication connections.\nPorts usage overview These ports can play a pivotal role when you are troubleshooting. They are described below according to the component and function in which the port is located.\nPorts in Istiod The ports in Istiod are relatively few and single-function.\n9876: ControlZ user interface, exposing information about Istiod’s processes 8080: Istiod debugging port, through which the configuration and status information of the grid can be queried 15010: Exposes the xDS API and issues plain text certificates 15012: Same functionality as port 15010, but uses TLS communication 15014: Exposes control plane metrics to Prometheus 15017: Sidecar injection and configuration validation port Ports in sidecar From the above, we see that there are numerous ports in the sidecar.\n15000: Envoy admin interface, which you can use to query and modify the configuration of Envoy Proxy. Please refer to Envoy documentation for details. 15001: Used to handle outbound traffic. 15004: Debug port (explained further below). 15006: Used to handle inbound traffic. 15020: Summarizes statistics, perform health checks on Envoy and DNS agents, and debugs pilot-agent processes, as explained in detail below. 15021: Used for sidecar health checks to determine if the injected Pod is ready to receive traffic. We set up the readiness probe on the /healthz/ready path on this port, and Istio hands off the sidecar readiness checks to kubelet. 15053: Local DNS proxy for scenarios where the cluster’s internal domain names are not resolved by Kubernetes DNS. 15090: Envoy Prometheus query port, through which the pilot-agent will scratch metrics. The above ports can be divided into the following categories.\nResponsible for inter-process communication, such as 15001, 15006, 15053 Health check and information statistics, e.g. 150021, 15090 Debugging: 15000, 15004 Let’s look at the key ports in detail.\n15000 15000 is Envoy’s Admin interface, which allows us to modify Envoy and get a view and query metrics and configurations.\nThe Admin interface consists of a REST API with multiple endpoints and a simple user interface. You can enable the Envoy Admin interface view in the productpage Pod using the following command:\nkubectl -n default port-forward deploy/productpage-v1 15000 Visit http://localhost:15000 in your browser and you will see the Envoy Admin interface as shown below.\nEnvoy Admin interface 15004 With the pilot-agent proxy istiod debug endpoint on port 8080, you can access localhost’s port 15004 in the data plane Pod to query the grid information, which has the same effect as port 8080 below.\n8080 You can also forward istiod port 8080 locally by running the following command:\nkubectl -n istio-system port-forward deploy/istiod 8080 Visit http://localhost:8080/debug in your browser and you will see the debug endpoint as shown in the figure below.\nPilot Debug Console Of course, this is only one way to get the mesh information and debug the mesh, you can also use istioctl …","relpermalink":"/en/blog/istio-components-and-ports/","summary":"This article will introduce you to the various ports and functions of the Istio control plane and data plane.","title":"Istio Component Ports and Functions in Details"},{"content":"As we know that Istio uses iptables for traffic hijacking, where the iptables rule chains has one called ISTIO_OUTPUT, which contains the following rules.\nRule target in out source destination 1 RETURN any lo 127.0.0.6 anywhere 2 ISTIO_IN_REDIRECT any lo anywhere !localhost owner UID match 1337 3 RETURN any lo anywhere anywhere !owner UID match 1337 4 RETURN any any anywhere anywhere owner UID match 1337 5 ISTIO_IN_REDIRECT any lo anywhere !localhost owner GID match 1337 6 RETURN any lo anywhere anywhere !owner GID match 1337 7 RETURN any any anywhere anywhere owner GID match 1337 8 RETURN any any anywhere localhost 9 ISTIO_REDIRECT any any anywhere anywhere The sidecar applies these rules to deal with different types of traffic. This article will show you the six types of traffic and their iptables rules in Istio sidecar.\niptables Traffic Routing in Sidecar The following list summarizes the six types of traffic in Sidecar.\nRemote service accessing local service: Remote Pod -\u0026gt; Local Pod Local service accessing remote service: Local Pod -\u0026gt; Remote Pod Prometheus crawling metrics of local service: Prometheus -\u0026gt; Local Pod Traffic between Local Pod service: Local Pod -\u0026gt; Local Pod Inter-process TCP traffic within Envoy Sidecar to Istiod traffic The following will explain the iptables routing rules within Sidecar for each scenario, which specifies which rule in ISTIO_OUTPUT is used for routing.\nType 1: Remote Pod -\u0026gt; Local Pod The following are the iptables rules for remote services, applications or clients accessing the local pod IP of the data plane.\nRemote Pod -\u0026gt; RREROUTING -\u0026gt; ISTIO_INBOUND -\u0026gt; ISTIO_IN_REDIRECT -\u0026gt; Envoy 15006 (Inbound) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 1 -\u0026gt; POSTROUTING -\u0026gt; Local Pod\nWe see that the traffic only passes through the Envoy 15006 Inbound port once. The following diagram shows this scenario of the iptables rules.\nRemote Pod to Local Pod Type 2: Local Pod -\u0026gt; Remote Pod The following are the iptables rules that the local pod IP goes through to access the remote service.\nLocal Pod-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 9 -\u0026gt; ISTIO_REDIRECT -\u0026gt; Envoy 15001 (Outbound) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 4 -\u0026gt; POSTROUTING -\u0026gt; Remote Pod\nWe see that the traffic only goes through the Envoy 15001 Outbound port.\nLocal Pod to Remote Pod The traffic in both scenarios above passes through Envoy only once because only one scenario occurs in that Pod, sending or receiving requests.\nType 3: Prometheus -\u0026gt; Local Pod Prometheus traffic that grabs data plane metrics does not have to go through the Envoy proxy.\nThese traffic pass through the following iptables rules.\nPrometheus-\u0026gt; RREROUTING -\u0026gt; ISTIO_INBOUND (traffic destined for ports 15020, 15090 will go to INPUT) -\u0026gt; INPUT -\u0026gt; Local Pod\nPrometheus to Local Pod Type 4: Local Pod -\u0026gt; Local Pod A Pod may simultaneously have two or more services. If the Local Pod accesses a service on the current Pod, the traffic will go through Envoy 15001 and Envoy 15006 ports to reach the service port of the Local Pod.\nThe iptables rules for this traffic are as follows.\nLocal Pod-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 9 -\u0026gt; ISTIO_REDIRECT -\u0026gt; Envoy 15001（Outbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 2 -\u0026gt; ISTIO_IN_REDIRECT -\u0026gt; Envoy 15006（Inbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 1 -\u0026gt; POSTROUTING -\u0026gt; Local Pod\nLocal Pod to Local Pod Type 5: Inter-process TCP traffic within Envoy Envoy internal processes with UID and GID 1337 will communicate with each other using lo NICs and localhost domains.\nThe iptables rules that these flows pass through are as follows.\nEnvoy process (Localhost) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 8 -\u0026gt; POSTROUTING -\u0026gt; Envoy process (Localhost)\nEnvoy inter-process TCP traffic Type 6: Sidecar to Istiod traffic Sidecar needs access to Istiod to synchronize its configuration so that Envoy will have traffic sent to Istiod.\nThe iptables rules that this traffic passes through are as follows.\npilot-agent process -\u0026gt; OUTPUT -\u0026gt; Istio_OUTPUT RULE 9 -\u0026gt; Envoy 15001 (Outbound Handler) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 4 -\u0026gt; POSTROUTING -\u0026gt; Istiod\nSidecar to Istiod Summary All the sidecar proxies that Istio injects into the Pod or installed in the virtual machine form the data plane of the service mesh, which is also the main workload location of Istio. In my next blog, I will take you through the ports of each component in Envoy and their functions, so that we can have a more comprehensive understanding of the traffic in Istio.\n","relpermalink":"/en/blog/istio-sidecar-traffic-types/","summary":"This article will show you the six traffic types and their iptables rules in Istio sidecar, and take you through the whole diagram in a schematic format.","title":"Traffic Types and Iptables Rules in Istio Sidecar Explained"},{"content":"Istio 1.13 is the first release of 2022, and, not surprisingly, the Istio team will continue to release new versions every quarter. Overall, the new features in this release include:\nSupport for newer versions of Kubernetes New API – ProxyConfig, for configuring sidecar proxies Improved Telemetry API Support for hostname-based load balancers with multiple network gateways Support for Kubernetes Versions I often see people asking in the community which Istio supports Kubernetes versions. Istio’s website has a clear list of supported Kubernetes versions. You can see here that Istio 1.13 supports Kubernetes versions 1.20, 1.21, 1.22, and 1.23, and has been tested but not officially supported in Kubernetes 1.16, 1.17, 1.18, 1.19.\nWhen configuring Istio, there are a lot of checklists. I noted them all in the Istio cheatsheet . There are a lot of cheat sheets about configuring Istio, using resources, dealing with everyday problems, etc., in this project, which will be online soon, so stay tuned.\nThe following screenshot is from the Istio cheatsheet website, it shows the basic cheat sheet for setting up Istio.\nIstio cheatsheet Introducing the new ProxyConfig API Before Istio version 1.13, if you wanted to customize the configuration of the sidecar proxy, there were two ways to do it.\nMeshConfig\nUse MeshConfig and use IstioOperator to modify it at the Mesh level. For example, use the following configuration to alter the default discovery port for istiod.\napVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: defaultConfig: discoveryAddress: istiod:15012 Annotation in the Pods\nYou can also use annotation at the Pod level to customize the configuration. For example, you can add the following annotations to Pod to modify the default port for istiod of the workload:\nanannotations: proxy.istio.io/config: | discoveryAddress: istiod:15012 When you configure sidecar in either of these ways, the fields set in annotations will completely override the default fields in MeshConfig. Please refer to the Istio documentation for all configuration items of ProxyConfig.\nThe new API – ProxyConfig\nBut in 1.13, a new top-level custom resource, ProxyConfig, has been added, allowing you to customize the configuration of your sidecar proxy in one place by specifying a namespace and using a selector to select the scope of the workload, just like any other CRD. Istio currently has limited support for this API, so please refer to the Istio documentation for more information on the ProxyConfig API.\nHowever, no matter which way you customize the configuration of the sidecar proxy, it does not take effect dynamically and requires a workload restart to take effect. For example, for the above configuration, because you changed the default port of istiod, all the workloads in the mesh need to be restarted before connecting to the control plane.\nTelemetry API MeshConfig customized extensions and configurations in the Istio mesh. The three pillars of observability– Metrics, Telemetry, and Logging– can each be docked to different providers. The Telemetry API gives you a one-stop place for flexible configuration of them. Like the ProxyConfig API, the Telemetry API follows the configuration hierarchy of Workload Selector \u0026gt; Local Namespace \u0026gt; Root Configuration Namespace. The API was introduced in Istio 1.11 and has been further refined in that release to add support for OpenTelemetry logs, filtered access logs, and custom tracing service names. See Telemetry Configuration for details.\nAutomatic resolution of multi-network gateway hostnames In September 2021, a member of the Istio community reported an issue with the EKS load balancer failing to resolve when running multi-cluster Istio in AWS EKS. Workloads that cross cluster boundaries need to be communicated indirectly through a dedicated east-west gateway for a multi-cluster, multi-network mesh. You can follow the instructions on Istio’s website to configure a multi-network, primary-remote cluster, and Istio will automatically resolve the IP address of the load balancer based on the hostname.\nIstio 1.13.1 fixing the critical security vulnerabilities Istio 1.13.1 was released to fix a known critical vulnerability that could lead to an unauthenticated control plane denial of service attack.\nThe figure below shows a multi-cluster primary-remote mesh where istiod exposes port 15012 to the public Internet via a gateway so that a pod on another network can connect to it.\nMulti-network Mesh When installing a multi-network, primary-remote mode Istio mesh, for a remote Kubernetes cluster to access the control plane, an east-west Gateway needs to be installed in the Primary cluster, exposing port 15012 of the control plane istiod to the Internet. An attacker could send specially crafted messages to that port, causing the control plane to crash. If you set up a firewall to allow traffic from only some IPs to access this port, you will be able to reduce the impact of the problem. It is …","relpermalink":"/en/blog/what-is-new-in-istio-1-13/","summary":"In February 2022, Istio released 1.13.0 and 1.13.1. This blog will give you an overview of what’s new in these two releases.","title":"What's New in Istio 1.13?"},{"content":"Join a team of world-class engineers working on the next generation of networking services using Istio, Envoy, Apache SkyWalking and a few of the open projects to define the next generation of cloud native network service.\nIstio upstream contributor: Golang We are looking for engineers with strong distributed systems experience to join our team. We are building a secure, robust, and highly available service mesh platform for mission critical enterprise applications spanning both legacy and modern infrastructure. This is an opportunity to dedicate a significant amount of contribution to Istio upstream on a regular basis. If you are a fan of Istio and would like to increase your contribution on a dedicated basis, this would be an opportunity for you.\nRequirements\nFundamentals-based problem solving skills; Drive decision by function, first principles based mindset. Demonstrate bias-to-action and avoid analysis-paralysis; Drive action to the finish line and on time. You are ego-less when searching for the best ideasIntellectually curious with a penchant for seeing opportunities in ambiguity Understands the difference between attention to detail vs. detailed - oriented Values autonomy and results over process You contribute effectively outside of your specialty Experience building distributed system platforms using Golang Familiarity with Kubernetes, service mesh technologies such as Istio and Envoy Excellent understanding of networking protocols, concepts, consistency properties of distributed systems, techniques to identify and reconcile configuration drifts Experience contributing to open source projects is a plus. Familiarity with WebAssembly is a plus. Familiarity with Golang, hardware/software load balancers (F5, NGINX), HSM modules, Active Directory/LDAP is a plus. We encourage written and asynchronous communication in English, and proficient oral English is not required.\nDistributed Systems Engineer, Enterprise Infrastructure (Data plane) GoLang or C++ Developers Seeking backend software engineers experienced in building distributed systems using Golang and gRPC. We are building a secure, and highly available service mesh platform for mission-critical enterprise applications for Fortune 500 companies, spanning both the legacy and modern infrastructure. Should possess strong fundamentals in distributed systems and networking. Familiarity with technologies like Kubernetes, Istio, and Envoy, as well as open contributions would be a plus.\nRequirements\nExperience building distributed system platforms using C++, Golang, and gRPC. Familiarity with Kubernetes, service mesh technologies such as Istio and Envoy. Excellent understanding of networking protocols, concepts, consistency properties of distributed systems, techniques to identify and reconcile configuration drifts. Experience contributing to open source projects is a plus. Familiarity with the following is a plus: WebAssembly, Authorization: NGAC, RBAC, ABAC Familiarity with hardware/software load balancers (F5, NGINX), HSM modules, Active Directory/LDAP is a plus. Site Reliability Engineer, SRE Site Reliability Engineering (SRE) combines software and systems engineering to build and run scalable, massively distributed, fault-tolerant systems. As part of the team, you will be working on ensuring that Tetrate’s platform has reliability/uptime appropriate to users’ needs as well as a fast rate of improvement. Additionally, much of our engineering effort focuses on building infrastructure, improving the platform troubleshooting abilities, and eliminating toil through automation.\nRequirements\nSystematic problem-solving approach, coupled with excellent communication skills and a sense of ownership/finish and self-directed drive. Strong fundamentals in operating, debugging, and troubleshooting distributed systems(stateful and/or stateless) and networking. Familiarity with Kubernetes, service mesh technologies such as Istio and EnvoyAbility to debug, optimize code, and automate routine tasks. Experience programming in at least one of the following languages: C++, Rust, Python, Go. Familiarity with the concepts of quantifying failure and availability in a prescriptive manner using SLOs and SLIs. Experience in performance analysis and tuning is a plus. Location Worldwide\nWe are remote with presence in China, Indonesia, India, Japan, U.S., Canada, Ireland, the Netherlands, Spain, and Ukraine.\nPlease send GitHub or online links that showcase your code style along with your resume to careers@tetrate.io .\nAbout Tetrate Powered by Envoy and Istio, its ﬂagship product, Tetrate Service Bridge, enables bridging traditional and modern workloads. Customers can get consistent baked-in observability, runtime security and traffic management for all their workloads, in any environment.\nIn addition to the technology, Tetrate brings a world-class team that leads the open Envoy and Istio projects, providing best practices and playbooks that enterprises can use to modernize their …","relpermalink":"/en/notice/tetrate-recruit/","summary":"Remotely worldwide","title":"The Enterprise Service Mesh company Tetrate is hiring"},{"content":"As the service mesh architecture concept gains traction and the scenarios for its applications emerge, there is no shortage of discussions about it in the community. I have worked on service mesh with the community for 4 years now, and will summarize the development of service mesh in 2021 from this perspective. Since Istio is the most popular service mesh, this article will focus on the technical and ecological aspects of Istio.\nService mesh: a critical tech for Cloud Native Infrastructure As one of the vital technologies defined by CNCF for cloud native, Istio has been around for five years now. Their development has gone through the following periods.\nExploration phase: 2017-2018 Early adopter phase: 2019-2020 Large-scale landing and ecological development phase: 2021-present Service mesh has crossed the “chasm”(refer Crossing the Chasm theory) and is in between the “early majority” and “late majority” phases of adoption. Based on feedback from the audience of Istio Weekly, users are no longer blindly following new technologies for experimentation and are starting to consider whether they need them in their organization dialectically.\nCross the chasm While new technologies and products continue to emerge, the service mesh, as part of the cloud native technology stack, has continued to solidify its position as the “cloud native network infrastructure” over the past year. The diagram below illustrates the cloud native technology stack model, where each layer has several representative technologies that define the standard. As new-age middleware, the service mesh mirrors other cloud native technologies, such as Dapr (Distributed Application Runtime), which represents the capability model for cloud native middleware, OAM , which defines the cloud native application model, and the service mesh, which defines the L7 network model.\nCloud Native Stack A layered view of the cloud native application platform technology stack\nOptimizing the mesh for large scale production applications with different deployment models Over the past year, the community focused on the following areas.\nPerformance optimization: performance issues of service mesh in large-scale application scenarios. Protocol and extensions: enabling service mesh to support arbitrary L7 network protocols. Deployment models: Proxyless vs. Node model vs. Sidecar model. eBPF: putting some of the service mesh’s capabilities to the kernel layer. Performance optimization Istio was designed to serve service to service traffic by “proto-protocol forwarding”. The goal is making the service mesh as “transparent” as possible to applications. Thus using IPtables to hijack the traffic, according to the community-provided test results Istio 1.2 adds only 3 ms to the baseline latency for a mesh with 1000 RPS on 16 connections. However, because of issues inherent in the IPtables conntrack module, Istio’s performance issues begin to emerge as the mesh size increases. To optimize the performance of the Istio sidecar for resource usage and network latency, the community gave the following solutions.\nSidecar configuration: By configuring service dependencies manually or by adding an Operator to the control plane, the number of service configurations sent to Sidecar can be reduced, thus reducing the resource footprint of the data plane; for more automatic and intelligent configuration of Sidecar, the open source projects Slime and Aeraki both offer their innovative configuration loading solutions. The introduction of eBPF: eBPF can be a viable solution to optimize the performance of the service mesh. Some Cilium-based startups even radically propose to use eBPF to replace the Sidecar proxy completely. Still, the Envoy proxy/xDS protocol has become the proxy for the service mesh implementation and supports the Layer 7 protocol very well. We can use eBPF to improve network performance, but complex protocol negotiation, parsing, and user scaling remain challenging to implement on the user side. Protocol and extensions Extensibility of Istio has always been a significant problem, and there are two aspects to Istio’s extensibility.\nProtocol level: allowing Istio to support all L7 protocols Ecological: allowing Istio to run more extensions Istio uses Envoy as its data plane. Extending Istio is essentially an extension of Envoy’s functionality. Istio’s official solution is to use WebAssembly, and in Istio 1.12, the Wasm plugin configuration API was introduced to extend the Istio ecosystem. Istio’s extension mechanism uses the Proxy-Wasm Application Binary Interface (ABI) specification to provide a set of proxy-independent streaming APIs and utilities that can be implemented in any language with an appropriate SDK. Today, Proxy-Wasm’s SDKs are AssemblyScript (similar to TypeScript), C++, Rust, Zig, and Go (using the TinyGo WebAssembly System Interface).\nThere are still relatively few WebAssembly extensions available, and many enterprises choose to customize their CRD and build a …","relpermalink":"/en/blog/service-mesh-in-2021/","summary":"A review of the development of Service Mesh in 2021.","title":"Service Mesh in 2021: The Ecosystem Is Emerging"},{"content":"It’s been more than four years since Istio launched in May 2017, and while the project has had a strong following on GitHub and 10+ releases, its growing open-source ecosystem is still in its infancy.\nRecently added support for WebAssembly extensions has made the most popular open source service mesh more extensible than ever. This table lists the open-source projects in the Istio ecosystem as of November 11, 2021, sorted by open-source date. These projects enhance the Istio service mesh with gateways, extensions, utilities, and more. In this article, I’ll highlight the two new projects in the category of extensions.\nProject Value Relationship with Istio Category Launch Date Dominant company Number of stars Envoy Cloud native high-performance edge/middle-service proxy The default data plane proxy September 2016 Lyft 18700 Istio Connection, secure, control, and observation services. Control plane service mesh May 2017 Google 29100 Emissary Gateway Kubernetes native API gateway for microservices, built on Envoy Connectable to Istio gateway February 2018 Ambassador 3600 APISIX Cloud native API gateways It can run as a data plane for Istio or as a gateway on its own gateway June 2019 API7 8100 MOSN Cloud native edge gateways \u0026amp; agents Available as Istio data plane proxy December 2019 Ant 3500 Slime Intelligent service mesh manager based on Istio Adding a management plane to Istio extensions January 2021 NetEase 236 GetMesh Istio integration and command-line management tools Utility for Istio multi-version management tools February 2021 Tetrate 95 Aeraki Manage any of Istio’s seven layers of load Extended multi-protocol support extensions March 2021 Tencent 330 Layotto Cloud native application runtime Using as a data plane for Istio runtime June 2021 Ant 393 Hango Gateway API gateways built on Envoy and Istio Integrates with Istio gateway August 2021 NetEase 253 Slime: an intelligent service mesh manager for Istio Slime is an Istio-based, intelligent mesh manager open-sourced by NetEase’s microservices team. Based on the Kubernetes Operator implementation, Slime can be used as a CRD manager that seamlessly interfaces with Istio without needing any customization or definition of dynamic service governance policies. This achieves automatic and convenient use of Istio and Envoy’s advanced features.\nSlime addresses the following issues:\nImplementing higher-level extensions in Istio. For example, extending the HTTP plugin; adaptive traffic limiting based on the resource usage of the service. Poor performance arising from Istio sending all the configurations within the mesh to each sidecar proxy. Slime solves these problems by building an Istio management plane. Its main purpose are\nto build a pluggable controller to facilitate the extension of new functions. to obtain data by listening to the data plane to intelligently generate the configuration for Istio. to build a higher-level CRD for the user to configure, which Slime converts into an Istio configuration. The following diagram shows the flow chart of Istio as an Istio management plane.\nSlime architecture The specific steps for Slime to manage Istio are as follows.\nSlime operator completes the initialization of Slime components in Kubernetes based on the administrator’s configuration. Developers create configurations that conform to the Slime CRD specification and apply them to Kubernetes clusters. Slime queries the monitoring data of the relevant service stored in Prometheus and converts the Slime CRD into an Istio CRD, in conjunction with the configuration of the adaptive part of the Slime CRD while pushing it to the Global Proxy. Istio listens for the creation of Istio CRDs. Istio pushes the configuration information of the Sidecar Proxy to the corresponding Sidecar Proxy in the data plane. The diagram below shows the internal architecture of Slime.\nSlime Internal We can divide Slime internally into three main components.\nslime-boot: operator for deploying Slime modules on Kubernetes. slime-controller: the core component of Slime that listens to the Slime CRD and converts it to an Istio CRD. slime-metric: the component used to obtain service metrics information. slime-controller dynamically adjusts service governance rules based on the information it receives. The following diagram shows the architecture of Slime Adaptive Traffic Limiting. Slime smart limiter Slime dynamically configures traffic limits by interfacing with the Prometheus metric server to obtain real-time monitoring.\nSlime’s adaptive traffic limitation process has two parts: one that converts SmartLimiter to EnvoyFilter and the other that monitors the data. Slime also provides an external monitoring data interface (Metric Discovery Server) that allows you to sync custom monitoring metrics to the traffic limiting component via MDS.\nThe CRD SmartLimiter created by Slime is used to configure adaptive traffic limiting. Its configuration is close to natural semantics, e.g., if you want to trigger an …","relpermalink":"/en/blog/istio-extensions-slime-and-aeraki/","summary":"In this article, I’ll introduce you two Istio extension projects: Aeraki and Slime.","title":"Introducing Slime and Aeraki in the Evolution of Istio Open-Source Ecosystem"},{"content":"You can use Istio to do multi-cluster management , API Gateway , and manage applications on Kubernetes or virtual machines . In my last blog , I talked about how service mesh is an integral part of cloud native applications. However, building infrastructure can be a big deal. There is no shortage of debate in the community about the practicability of service mesh and Istio– here’s a list of common questions and concerns, and how to address them.\nIs anyone using Istio in production? What is the impact on application performance due to the many resources consumed by injecting sidecar into the pod? Istio supports a limited number of protocols; is it scalable? Will Istio be manageable? – Or is it too complex, old services too costly to migrate, and the learning curve too steep? I will answer each of these questions below.\nIstio is architecturally stable, production-ready, and ecologically emerging Istio 1.12 was just released in November – and has evolved significantly since the explosion of service mesh in 2018 (the year Istio co-founders established Tetrate). Istio has a large community of providers and users . The Istio SIG of Cloud Native Community has held eight Istio Big Talk (Istio 大咖说) , with Baidu, Tencent, NetEase, Xiaohongshu(小红书), and Xiaodian Technology(小电科技) sharing their Istio practices. According to CNCF Survey Report 2020 , about 50% of the companies surveyed are using a service mesh in production or planning to in the next year, and about half (47%) of organizations using a service mesh in production are using Istio.\nMany companies have developed extensions or plugins for Istio, such as Ant, NetEase, eBay, and Airbnb. Istio’s architecture has been stable since the 1.5 release, and the release cycle is fixed quarterly, with the current project’s main task being Day-2 Operations.\nThe Istio community has also hosted various events, with the first IstioCon in March 2021, the Istio Meetup China in Beijing in July, and the Service Mesh Summit 2022 in Shanghai in January 2022.\nSo we can say that the Istio architecture is stable and production-ready, and the ecosystem is budding.\nThe impact of service mesh on application performance A service mesh uses iptables to do traffic hijacking by default to be transparent to applications. When the number of services is large, there are a lot of iptables rules that affect network performance. You can use techniques like eBPF to provide application performance, but the method requires a high version of the operating system kernel, which few enterprises can achieve.\nIstio DNS In the early days, Istio distributed the routing information of all services in the mesh to all proxy sidecars, which caused sidecar s to take up a lot of resources. Aeraki and Slime can achieve configuration lazy loading. We will introduce these two open-source projects in the Istio open-source ecosystem.\nFinally, there is a problem related to Sidecar proxy operation and maintenance: upgrading all Envoy proxies while ensuring constant traffic. A solution is using the SidecarSet resource in the open-source project OpenKruise .\nThe resource consumption and network latency associated with the introduction of Sidecar are also within reasonable limits, as you can see from the service mesh benchmark performance tests .\nExtending the Istio service mesh The next question is about extending the Istio service mesh. The current solution given by the Istio community is to use WebAssembly , an extension that is still relatively little used in production by now and has performance concerns. Most of the answers I’ve observed are CRDs that build a service mesh management plane based on Istio.\nAlso, making Istio support heterogeneous environments for all workloads, such as virtual machines and containers, is in strong demand for end-users. It allows them to migrate applications from traditional loads to cloud native easily. Finally, hybrid cloud traffic management for multiple clusters and meshes is a more advanced requirement.\nSteep learning curve Many people complain that Istio has too little learning material. Istio has been open source for four years, and there are a lot of learning resources now:\nIstio Documentation IstioCon 2021 Istio Big Talk/Istio Weekly Istio Fundamentals Course Certified Istio Administrator Yes, Istio is complex, but it’s been getting more and more manageable with every release. In my next blog, I will introduce you to two open source projects that extend Istio and give you some insight into what’s going on in the Istio community.\n","relpermalink":"/en/blog/the-debate-in-the-community-about-istio-and-service-mesh/","summary":"There is no shortage of debate in the community about the practicability of service mesh and Istio – here’s a list of common questions and concerns, and how to address them.","title":"The Debate in the Community About Istio and Service Mesh"},{"content":"If you don’t know what Istio is, you can read my previous articles below:\nWhat Is Istio and Why Does Kubernetes Need it? Why do you need Istio when you already have Kubernetes? This article will explore the relationship between service mesh and cloud native.\nService mesh – the product of the container orchestration war If you’ve been following the cloud-native space since its early days, you’ll remember the container orchestration wars of 2015 to 2017. Kubernetes won the container wars in 2017, the idea of microservices had taken hold, and the trend toward containerization was unstoppable. Kubernetes architecture matured and slowly became boring, and service mesh technologies, represented by Linkerd and Istio, entered the CNCF-defined cloud-native critical technologies on the horizon.\nKubernetes was designed with the concept of cloud-native in mind. A critical idea in cloud-native is the architectural design of microservices. When a single application is split into microservices, how can microservices be managed to ensure the SLA of the service as the number of services increases? The service mesh was born to solve this problem at the architectural level, free programmers’ creativity, and avoid tedious service discovery, monitoring, distributed tracing, and other matters.\nThe service mesh takes the standard functionality of microservices down to the infrastructure layer, allowing developers to focus more on business logic and thus speed up service delivery, which is consistent with the whole idea of cloud-native. You no longer need to integrate bulky SDKs in your application, develop and maintain SDKs for different languages, and just use the service mesh for Day 2 operations after the application is deployed.\nThe service mesh is regarded as the next generation of microservices. In the diagram, we can see that many of the concerns of microservices overlap with the functionality of Kubernetes. Kubernetes focuses on the application lifecycle, managing resources and deployments with little control over services. The service mesh fills this gap. The service mesh can connect, control, observe and protect microservices.\nKubernetes vs. xDS vs. Istio This diagram shows the layered architecture of Kubernetes and Istio.\nKubernetes vs xDS vs Istio The diagram indicates that the kube-proxy settings are global and cannot be controlled at a granular level for each service. All Kubernetes can do is topology-aware routing, routing traffic closer to the Pod, and setting network policies in and out of the Pod.\nIn contrast, the service mesh takes traffic control out of the service layer in Kubernetes through sidecar proxies, injects proxies into each Pod, and manipulates these distributed proxies through a control plane. It allows for more excellent resiliency.\nKube-proxy implements traffic load balancing between multiple pod instances of a Kubernetes service. But how do you finely control the traffic between these services — such as dividing the traffic by percentage to different application versions (which are all part of the same service, but on other deployments), or doing canary releases and blue-green releases?\nThe Kubernetes community gives a way to do canary releases using Deployment, assigning different pods to deployed services by modifying the pod’s label.\nEnvoy Architecture Currently, the most popular open-source implementation of service mesh in the world is Istio. From the CNCF Survey Report 2020 , we know that Istio is the most used service mesh in production today. Many companies have built their service mesh based on Istio, such as Ant, Airbnb, eBay, NetEase, Tencent, etc.\nCNCF Survey Report 2020 Figure from CNCF Survey Report 2020 Istio is developed based on Envoy, which has been used by default as its distributed proxy since the first day it was open-sourced. Envoy pioneered the creation of the xDS protocol for distributed gateway configuration, greatly simplifying the configuration of large-scale distributed networks. Ant Group open source MOSN also supported xDS In 2019. Envoy was also one of the first projects to graduate from CNCF, tested by large-scale production applications.\nService mesh – the cloud-native networking infrastructure With the above comparison between Kubernetes and service mesh in mind, we can see the place of service mesh in the cloud-native application architecture. That is, building a cloud-native network infrastructure specifically provides:\nTraffic management: controlling the flow of traffic and API calls between services, making calls more reliable, and enhancing network robustness in different environments. Observability: understanding the dependencies between services and the nature and flow of traffic between them provides the ability to identify problems quickly. Policy enforcement: controlling access policies between services by configuring the mesh rather than by changing the code. Service Identification and Security: providing service identifiability and security …","relpermalink":"/en/blog/service-mesh-an-integral-part-of-cloud-native-apps/","summary":"This article will explore the relationship between service mesh and cloud native.","title":"Service Mesh - An Integral Part of Cloud-Native Applications"},{"content":"API gateways have been around for a long time as the entry point for clients to access the back-end, mainly to manage “north-south” traffic, In recent years, service mesh architectures have become popular, mainly for managing internal systems,(i.e. “east-west” traffic), while a service mesh like Istio also has built-in gateways that bring traffic inside and outside the system under unified control. This often creates confusion for first-time users of Istio. What is the relationship between the service mesh and the API gateway? How does Istio’s gateway work? What are the ways to expose the services in the Istio mesh? This article gives you the answer.\nKey Insights The service mesh was originally created to solve the problem of managing internal traffic for distributed systems, but API gateways existed long before it. While the Gateway is built into Istio, you can still use a custom Ingress Controller to proxy external traffic. API gateways and service mesh are converging. How do I expose services in the Istio mesh? The following diagram shows four approaches to expose services in the Istio mesh using Istio Gateway, Kubernetes Ingress, API Gateway, and NodePort/LB.\nExposing services through Istio Ingress Gateway The Istio mesh is shaded, and the traffic in the mesh is internal (east-west) traffic, while the traffic from clients accessing services within the Kubernetes cluster is external (north-south) traffic.\nApproach Controller Features NodePort/LoadBalancer Kubernetes Load balancing Kubernetes Ingress Ingress controller Load balancing, TLS, virtual host, traffic routing Istio Gateway Istio Load balancing, TLS, virtual host, advanced traffic routing, other advanced Istio features API Gateway API Gateway Load balancing, TLS, virtual host, advanced traffic routing, API lifecycle management, billing, rate limiting, policy enforcement, data aggregation Since NodePort/LoadBalancer is a basic way to expose services built into Kubernetes, this article will not discuss that option. Each of the other three approaches will be described below.\nUsing Kubernetes Ingress to expose traffic We all know that clients of a Kubernetes cluster cannot directly access the IP address of a pod because the pod is in a network plane built into Kubernetes. We can expose services inside Kubernetes outside the cluster using NodePort or Load Balancer Kubernetes service type. To support virtual hosting, hiding and saving IP addresses, you can use Ingress resources to expose services in Kubernetes.\nKubernetes Ingress to expose services Ingress is a Kubernetes resource that controls the behavior of an ingress controller that does the traffic touring, which is the equivalent of a load-balanced directional proxy server such as Nginx, Apache, etc., which also includes rule definitions, i.e., routing information for URLs, which is provided by the Ingress controller .\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: istio name: ingress spec: rules: - host: httpbin.example.com http: paths: - path: /status/* backend: serviceName: httpbin servicePort: 8000 The kubernetes.io/ingress.class: istio annotation in the example above indicates that the Ingress uses the Istio Ingress Controller which in fact uses Envoy proxy.\nUsing Istio Gateway to expose services Istio is a popular service mesh implementation that has evolved from Kubernetes that implements some features that Kubernetes doesn’t. (See What is Istio and why does Kubernetes need Istio? ) It makes traffic management transparent to the application, moving this functionality from the application to the platform layer and becoming a cloud-native infrastructure.\nIstio used Kubernetes Ingress as the traffic portal in versions prior to Istio 0.8, where Envoy was used as the Ingress Controller. From Istio 0.8 and later, Istio created the Gateway object. Gateway and VirtualService are used to represent the configuration model of Istio Ingress, and the default implementation of Istio Ingress uses the same Envoy proxy. In this way, the Istio control plane controls both the ingress gateway and the internal sidecar proxy with a consistent configuration model. These configurations include routing rules, policy enforcement, telemetry, and other service control functions.\nThe Istio Gateway resources function similarly to the Kubernetes Ingress in that it is responsible for north-south traffic to and from the cluster. The Istio Gateway acts as a load balancer to carry connections to and from the edge of the service mesh. The specification describes a set of open ports and the protocols used by those ports, as well as the SNI configuration for load balancing, etc.\nThe Istio Gateway resource itself can only be configured for L4 through L6, such as exposed ports, TLS settings, etc.; however, the Gateway can be bound to a VirtualService, where routing rules can be configured on L7, such as versioned traffic routing, fault injection, HTTP redirects, HTTP …","relpermalink":"/en/blog/istio-servicemesh-api-gateway/","summary":"What is the relationship between the service mesh and the API gateway? How does Istio’s gateway work? What are the ways to expose the services in the Istio mesh? This article gives you the answer.","title":"Using Istio Service Mesh as API Gateway"},{"content":"Do you have multiple Kubernetes clusters and a service mesh? Do your virtual machines and services in a Kubernetes cluster need to interact? This article will take you through the process and considerations of building a hybrid cloud using Kubernetes and an Istio Service Mesh. Together, Kubernetes and Istio can be used to bring hybrid workloads into a mesh and achieve interoperability for multicluster. But another layer of infrastructure — a management plane — is helpful for managing multicluster or multimesh deployments.\nKubernetes Using Kubernetes enables rapid deployment of a distributed environment that enables cloud interoperability and unifies the control plane on the cloud. It also provides resource objects, such as Service, Ingress and Gateway , to handle application traffic. The Kubernetes API Server communicates with the kube-proxy component on each node in the cluster, creates iptables rules for the node, and forwards requests to other pods.\nAssuming that a client now wants to access a service in Kubernetes, the request is first sent to the Ingress/Gateway, then forwarded to the backend service (Service A in the diagram below) based on the routing configuration in the Ingress/Gateway. Then Service A polls an instance of Service B for the traffic requested by Service B. Lastly, the traffic requested by Service A for Service B is polled forward to Service B’s instance.\nKubernetes Kubernetes Multicluster The most common usage scenarios for multicluster management include:\nservice traffic load balancing isolating development and production environments decoupling data processing and data storage cross-cloud backup and disaster recovery flexible allocation of compute resources low-latency access to services across regions avoiding vendor lock-in There are often multiple Kubernetes clusters within an enterprise; and the KubeFed implementation of Kubernetes cluster federation developed by Multicluster SIG enables multicluster management capabilities, which allows all Kubernetes clusters to be managed through the same interface.\nThere are several general issues that need to be addressed when using cluster federation:\nConfiguring which clusters need to be federated API resources need to be propagated across the clusters Configuring how API resources are distributed to different clusters Registering DNS records in clusters to enable service discovery across clusters The following is a multicluster architecture for KubeSphere — one of the most commonly used Kubernetes multicluster management architectures — where the Host Cluster serves as the control plane with two member clusters, West and East.\nMulticluster The Host Cluster needs to be able to access the API Server of the Member Cluster, but the network connectivity between Member Clusters is not required. The Host Cluster is independent of the Member Cluster it manages and the Member Cluster is not aware of the existence of the Host Cluster. The advantage of this is that when the control plane fails, the Member Cluster will not be affected and the deployed load can still operate normally without being affected.\nThe Host Cluster also assumes the role of API portal, and the Host Cluster forwards the resource requests to the Member Cluster — which is convenient for aggregation and also facilitates unified authority authentication. We see that there is a Federation Control Plane in the Host Cluster, where the Push Reconciler propagates the identity, role, and role binding from the Federation Cluster to all Member Clusters.\nIstio Service Mesh Consider using the Istio service mesh when we have multilingual, multiversion microservices running in Kubernetes and need finer-grained canary publishing and unified security policy management for inter-service observability. Istio enables intelligent application-aware load balancing from the application layer to other Service Mesh-enabled services in the cluster, by transparently intercepting all traffic to and from the application using IPTables, and bypassing the primary kube-proxy load balancing. The Istio control plane communicates with the Kubernetes API Server to obtain information about all registered services in the cluster.\nThe following diagram illustrates the basics of Istio, where all nodes belong to the same Kubernetes cluster.\nIstio Service Mesh You may end up with at least a few Kubernetes clusters, each hosting microservices. Multiple deployment models exist for Istio’s multicluster deployments — depending on network isolation, primary and backup — which can be specified by declaration when deploying using Istio Operator. Communication between these microservices in a cluster can be enhanced by a service mesh. Within the cluster, Istio provides common communication patterns to improve resiliency, security and observability.\nAll of the above is about application load management on Kubernetes, but for legacy applications on virtual machines: how can they be managed in the same plane? Istio supports …","relpermalink":"/en/blog/multicluster-management-with-kubernetes-and-istio/","summary":"This article explains three patterns/tools for debugging microservices in Kubernetes and the changes brought by the introduction of Istio for debugging microservices.","title":"Multicluster Management With Kubernetes and Istio"},{"content":"Kubernetes is arguably the best environment for running microservices so far, but the experience of debugging microservices in a Kubernetes environment may not be as user-friendly. This article will show you how to debug microservices in Kubernetes, introduce common tools, and explain how the introduction of Istio impacts debugging microservices.\nDebugging microservices is vastly different from traditional monolithic applications The debugging of microservices has been a long-standing problem for software developers. This challenge does not exist in traditional monolithic applications because developers can leverage the debugger in IDEs to add breakpoints, modify environment variables, single-step execution, etc. for their applications, all of which provide great help in software debugging. With the popularity of Kubernetes, the debugging of microservices becomes a thorny issue, where the following issues are more complicated than the debugging of traditional monolithic applications.\nMultiple dependencies A microservice often depends on multiple other microservices, some shared volumes across multiple microservices, and authorizations based on service accounts. When debugging a microservice, how do you deploy other dependent services to quickly build a latest set of staging environments?\nAccess from a local machine When microservices are running on a developer’s local computer, there is usually no direct access to the services in a Kubernetes cluster. How can you debug microservices deployed in a Kubernetes cluster as if they were local services?\nSlow development loop Usually, it takes a long process to update the code and build it into an image before pushing it to the cluster. How do you speed up the development cycle? Let’s look at the tools that address those challenges.\nTools The main solutions for debugging microservices in Kubernetes are:\nProxy: by building a VPN, deploying a proxy in the Kubernetes cluster, and adding local debug endpoints to make the services in Kubernetes directly accessible to local applications, your architecture will look like [ local service ] \u0026lt;-\u0026gt; [ proxy ] \u0026lt;-\u0026gt; [ app in Kubernetes ]. Sidecar: Inject a sidecar into the pod of the microservice to be debugged to intercept all traffic to and from the service, so that the service can be tracked and monitored, and the service can also be debugged in this sidecar. Service Mesh: To get an overall picture of the application, inject sidecars into all microservices so that you can get a dashboard that monitors global status. Here are three typical open source projects that implement the above solutions, each of which can help you debug microservices from a different perspective. You can apply them at different stages of software development and they can be said to be complementary to each other.\nProxy – debugging microservices with Telepresence Telepresence is essentially a local proxy that proxies data volumes, environment variables, and networks in a Kubernetes cluster locally. The following diagram shows the main usage scenarios for Telepresence.\nProxy mode: Telepresence Users need to manually execute the telepresence command locally, which will automatically deploy the agent to Kubernetes. Once the agent has been deployed,\nLocal services will have complete access to other services in the Kubernetes cluster, environment variables, Secret, ConfigMap, etc. Services in the cluster also have direct access to the locally exposed endpoints. However, this approach requires users to run multiple commands while debugging locally, and in some network environments it may not be possible to establish a VPN connection to the Kubernetes cluster.\nSidecar – debugging microservices with Nocalhost Nocalhost is a Kubernetes-based cloud development environment. To use it, you just need to install a plugin in your IDE – VS Code to extend Kubernetes and shorten the development feedback cycle. The development environment can be isolated by creating different namespaces for different users and using ServiceAccount when binding to different user corners. Nocalhost also provides a web console and API for administrators to manage different development environments.\nSidecar mode: Nocalhost As long as you have a Kubernetes cluster and have admin rights to the cluster, you can refer to the Nocalhost documentation to quickly start trying it out. To use the Nocalhost plugin in VS Code, you need to configure the Kubernetes cluster in the plugin first.\nSelect the Kubeconfig file you just exported or copy and paste the contents of the file directly into the configuration. Then select the service you need to test and select the corresponding Dev Container. VS Code will automatically open a new code window. Here is an example of the bookinfo sample provided by Istio. You can open the cloned code in your local IDE and click the hammer next to the code file to enter development mode. Selecting the corresponding DevContainer and Nocalhost will automatically inject a …","relpermalink":"/en/blog/how-to-debug-microservices-in-kubernetes-with-proxy-sidecar-or-service-mesh/","summary":"This article explains three patterns/tools for debugging microservices in Kubernetes and the changes brought by the introduction of Istio for debugging microservices.","title":"How to Debug Microservices in Kubernetes With Proxy, Sidecar or Service Mesh?"},{"content":"Istio was named by Tetrate founder Varun Talwar and Google lead engineer Louis Ryan in 2017 and was open sourced on May 24, 2017. Today is the fourth anniversary of Istio’s open source arrival. Let’s take a look back at Istio’s four years of development — and look forward to Istio’s future.\nIstio’s open source history In 2017, the year Kubernetes ended the container orchestration battle, Google took the opportunity to consolidate its dominance in the cloud native space and compensate for Kubernetes’ disadvantage in service-to-service traffic management by open-sourcing Istio. Istio released its 1.10 last week — but here are some of the most important releases in Istio’s history to date.\nDate Version Note May 24, 2017 0.1 Officially open source; established the architectural foundation of Control Plane, Data Plane and sidecar proxy. October 10, 2017 0.2 Started to support multiple runtime environments, such as virtual machines. June 1, 2018 0.8 API refactoring July 31, 2018 1.0 Production-ready, after which the Istio team underwent a massive reorganization. March 19, 2019 1.1 Enterprise-ready. Support for multiple Kubernetes clusters, with performance optimizations. March 3, 2020 1.5 Back to monolith, with microservice components merged into istiod, making Istio’s architecture cleaner and easier to maintain. Support for WebAssembly extension, making Istio’s ecology much stronger. November 18, 2020 1.8 Officially deprecated Mixer and focused on adding support for virtual machines. A year after its inception– and two months before the 1.0 release, version 0.8 was released with a massive refactoring of the API. In late July 2018, when 1.0 was released, Istio reached a production-ready tipping point. Since then, Google has massively reorganized the Istio team and several Istio-based service mesh startups were born, making 2018 the booming year of the service mesh industry.\nIstio 1.1 was released in March 2019, almost 9 months after 1.0 was released, which is far beyond the average release cycle of an open-source project. We know that the speed of iteration and evolution is a core competency of basic software. Since then, Istio has started a regular release cadence of one version per quarter and has become the #4 fastest growing project in GitHub’s top 10 in 2019 !\nThe Istio community In 2020, Istio’s project management began to mature and its governance reached a stage of evolution. We saw the first election of a steering committee for the Istio community and the transfer of the trademark to Open Usage Commons . The first IstioCon was successfully held in February 2021, with thousands of people attending the online conference. There is also a large Istio community in China , and face-to-face Istio community meetups will be held there in 2021. Stay tuned for more.\nAccording to the CNCF 2020 Survey, 46% of organizations were either using a service mesh in production or planning to use it in the next 12 months. Istio was the top used mesh among those using a mesh in production.\nThe future After 4 years of development, there is not only a large user base around Istio, but also several Istio vendors, as you can see on the homepage of the recently revamped Istio website. In the last few releases, Istio has shifted its development focus to improving the Day 2 Operation experience. We also expect to see more Istio adoption path recommendations, case studies, learning materials, training, and certifications (such as the industry’s first Certified Istio Administrator from Tetrate) that will facilitate the adoption of Istio.\n","relpermalink":"/en/blog/istio-4-year-birthday/","summary":"Today is Istio's 4 year birthday, let’s take a look back at Istio’s four years of development — and look forward to Istio’s future.","title":"Happy Istio 4th Anniversary -- Retrospect and Outlook"},{"content":"Istio, the most popular service mesh implementation , was developed on top of Kubernetes and has a different niche in the cloud native application ecosystem than Kubernetes. Rather than introduce you directly to what Istio has to offer, this article will explain how Istio came about and what it is in relation to Kubernetes.\nWhy Is There an Istio? To explain what Istio is, it’s also important to understand the context in which Istio came into being — i.e., why is there an Istio?\nMicroservices are a technical solution to an organizational problem. And Kubernetes/Istio are a technical solution to deal with the issues created by moving to microservices. As a deliverable for microservices, containers solve the problem of environmental consistency and allow for more granularity in limiting application resources. They are widely used as a vehicle for microservices.\nGoogle open-sourced Kubernetes in 2014, which grew exponentially over the next few years. It became a container scheduling tool to solve the deployment and scheduling problems of distributed applications — allowing you to treat many computers as though they were one computer. Because the resources of a single machine are limited and Internet applications may have traffic floods at different times (due to rapid expansion of user scale or different user attributes), the elasticity of computing resources needs to be high. A single machine obviously can’t meet the needs of a large-scale application; and conversely, it would be a huge waste for a very small-scale application to occupy the whole host.\nIn short, Kubernetes defines the final state of the service and enables the system to reach and stay in that state automatically. So how do you manage the traffic on the service after the application has been deployed? Below we will look at how service management is done in Kubernetes and how it has changed in Istio.\nHow Do You Do Service Management in Kubernetes? The following diagram shows the service model in Kubernetes:\nKubernetes Service Model From the above figure we can see that:\nDifferent instances of the same service may be scheduled to different nodes. Kubernetes combines multiple instances of a service through Service objects to unify external services. Kubernetes installs a kube-proxy component in each node to forward traffic, which has simple load balancing capabilities. Traffic from outside the Kubernetes cluster can enter the cluster via Ingress (Kubernetes has several other ways of exposing services; such as NodePort, LoadBalancer, etc.). Kubernetes is used as a tool for intensive resource management. However, after allocating resources to the application, Kubernetes doesn’t fully solve the problems of how to ensure the robustness and redundancy of the application, how to achieve finer-grained traffic division (not based on the number of instances of the service), how to guarantee the security of the service, or how to manage multiple clusters, etc.\nThe Basics of Istio The following diagram shows the service model in Istio, which supports both workloads and virtual machines in Kubernetes.\nIstio From the diagram we can see that:\nIstiod acts as the control plane, distributing the configuration to all sidecar proxies and gateways. (Note: for simplification, the connections between Istiod and sidecar are not drawn in the diagram.) Istio enables intelligent application-aware load balancing from the application layer to other mesh enabled services in the cluster, and bypasses the rudimentary kube-proxy load balancing. Application administrators can manipulate the behavior of traffic in the Istio mesh through a declarative API, in the same way they manage workloads in Kubernetes. It can take effects within seconds and they can do this without needing to redeploy. Ingress is replaced by Gateway resources, a special kind of proxy that is also a reused Sidecar proxy. A sidecar proxy can be installed in a virtual machine to bring the virtual machine into the Istio mesh. In fact, before Istio one could use SpringCloud, Netflix OSS, and other tools to programmatically manage the traffic in an application, by integrating the SDK in the application. Istio makes traffic management transparent to the application, moving this functionality out of the application and into the platform layer as a cloud native infrastructure.\nIstio complements Kubernetes, by enhancing its traffic management, observability and security for cloud native applications. The service mesh open source project — launched in 2017 by Google, IBM and Lyft — has come a long way in three years. A description of Istio’s core features can be found in the Istio documentation .\nSummary Service Mesh is the cloud native equivalent of TCP/IP, addressing application network communication, security and visibility issues. Istio is currently the most popular service mesh implementation, relying on Kubernetes but also scalable to virtual machine loads. Istio’s core consists of a control plane and a data …","relpermalink":"/en/blog/what-is-istio-and-why-does-kubernetes-need-it/","summary":"This article will explain how Istio came about and what it is in relation to Kubernetes.","title":"What Is Istio and Why Does Kubernetes Need it?"},{"content":"If you’ve heard of service mesh and tried Istio , you may have the following questions:\nWhy is Istio running on Kubernetes? What is the role of Kubernetes and a service mesh in the cloud native application architecture, respectively? What aspects of Kubernetes does Istio extend? What problems does it solve? What is the relationship between Kubernetes, Envoy, and Istio? This article will take you through the inner workings of Kubernetes and Istio. In addition, I will introduce the load balancing approach in Kubernetes, and explain why you need Istio when you have Kubernetes.\nKubernetes is essentially about application lifecycle management through declarative configuration, while a service mesh is essentially about providing inter-application traffic, security management and observability. If you have already built a stable application platform using Kubernetes, how do you set up load balancing and traffic control for calls between services? This is where a service mesh comes into the picture.\nEnvoy introduces the xDS protocol, which is supported by various open source software, such as Istio , MOSN , etc. Envoy contributes xDS to a service mesh or cloud native infrastructure. Envoy is essentially a modern version of a proxy that can be configured through APIs, based on which many different usage scenarios are derived — such as API Gateway, sidecar proxy in service mesh, and edge proxy.\nThis article contains the following:\nA description of the role of kube-proxy. The limitations of Kubernetes for microservice management. An introduction to the capabilities of Istio service mesh. A comparison of some of the concepts in Kubernetes, Envoy, and the Istio service mesh. Kubernetes vs Service Mesh The following diagram shows the service access relationship in Kubernetes and service mesh (one sidecar per pod model).\nKubernetes vs Service Mesh Traffic Forwarding Each node in a Kubernetes cluster deploys a kube-proxy component that communicates with the Kubernetes API Server, gets information about the services in the cluster, and then sets iptables rules to send requests for service directly to the corresponding Endpoint (a pod belonging to the same group of services).\nService Discovery Service Discovery Istio can follow the service registration in Kubernetes and can also interface with other service discovery systems via platform adapters in the control plane; and then generate data plane configurations (using CRD, which are stored in etcd) with transparent proxies for the data plane. The transparent proxy of the data plane is deployed as a sidecar container in the pod of each application service, and all these proxies need to request the control plane to synchronize the proxy configuration. The proxy is “transparent” because the application container is completely unaware of the presence of the proxy. The kube-proxy component in the process needs to intercept traffic as well, except that the kube-proxy intercepts traffic to and from the Kubernetes node — while the sidecar proxy intercepts traffic to and from the pod.\nDisadvantages of a Service Mesh Since Kubernetes has many pods running on each node, putting the original kube-proxy route forwarding function in each pod will increase the response latency — due to more hops when the sidecar intercepts the traffic — and consume more resources. In order to manage traffic in a fine-grained manner, a series of new abstractions will be added. This will further increase the learning cost for users, but as the technology becomes more popular this situation will be slowly alleviated.\nAdvantages of a Service Mesh The kube-proxy settings are global and cannot be controlled at a granular level for each service, while service mesh takes the traffic control out of the service layer in Kubernetes by means of sidecar proxy — allowing for more elasticity.\nShortcomings of Kube-Proxy First, it does not automatically try another pod if the forwarded pod is not serving properly. Each pod has a health check mechanism and when a pod has health problems, kubelet will restart the pod and kube-proxy will remove the corresponding forwarding rules. Also, nodePort-type services cannot add TLS or more complex message routing mechanisms.\nKube-proxy implements load balancing of traffic across multiple pod instances of a Kubernetes service, but how do you do fine-grained control of traffic between these services — such as dividing traffic by percentage to different application versions (which are all part of the same service but on different deployments), or doing canary releases (grayscale releases) and blue-green releases?\nThe Kubernetes community gives a way to do canary releases using Deployment , which is essentially a way to assign different pods to a deployment’s service by modifying the pod’s label.\nKubernetes Ingress vs. Istio Gateway As mentioned above, kube-proxy can only route traffic within a Kubernetes cluster. The pods of a Kubernetes cluster are located in a network created by CNI. …","relpermalink":"/en/blog/why-do-you-need-istio-when-you-already-have-kubernetes/","summary":"This article will take you through the inner workings of Kubernetes and Istio. In addition, I will introduce the load balancing approach in Kubernetes, and explain why you need Istio when you have Kubernetes.","title":"Why Do You Need Istio When You Already Have Kubernetes?"},{"content":"Tetrate Academy has recently released the Istio Fundamentals Course, which is now available for free. Sign up at Tetrate Academy now!\nCourse curriculum Here is the curriculum:\nService Mesh and Istio Overview Installing Istio Observability: Telemetry and Logs Traffic Management Security Advanced Features Troubleshooting Real World Examples There are self-assessment questions at the end of each course. I have passed the course, and here is the certificate after passing the course.\nTetrate Academy Istio Fundamentals Course More In the future, Tetrate will release the Certified Istio Administrator (CIA) exam and welcome all Istio users and administrators to follow and register for it.\n","relpermalink":"/en/notice/tetrate-istio-fundamental-courses/","summary":"Tetrate Academy has recently released the Istio Fundamentals Course, which is now available for free.","title":"Tetrate Academy Releases Free Istio Fundamentals Course"},{"content":"Different companies or software providers have devised countless ways to control user access to functions or resources, such as Discretionary Access Control (DAC), Mandatory Access Control (MAC), Role-Based Access Control (RBAC), and Attribute-Based Access Control (ABAC). In essence, whatever the type of access control model, three basic elements can be abstracted: user, system/application, and policy.\nIn this article, we will introduce ABAC, RBAC, and a new access control model — Next Generation Access Control (NGAC) — and compare the similarities and differences between the three, as well as why you should consider NGAC.\nWhat Is RBAC? RBAC, or Role-Based Access Control, takes an approach whereby users are granted (or denied) access to resources based on their role in the organization. Every role is assigned a collection of permissions and restrictions, which is great because you don’t need to keep track of every system user and their attributes. You just need to update appropriate roles, assign roles to users, or remove assignments. But this can be difficult to manage and scale. Enterprises that use the RBAC static role-based model have experienced role explosion: large companies may have tens of thousands of similar but distinct roles or users whose roles change over time, making it difficult to track roles or audit unneeded permissions. RBAC has fixed access rights, with no provision for ephemeral permissions or for considering attributes like location, time, or device. Enterprises using RBAC have had difficulty meeting the complex access control requirements to meet regulatory requirements of other organizational needs.\nRBAC Example Here’s an example Role in the “default” namespace in Kubernetes that can be used to grant read access to pods:\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [\u0026#34;v1\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] What Is ABAC? ABAC stands for Attribute-Based Access Control. At a high level, NIST defines ABAC as an access control method “where subject requests to perform operations on objects are granted or denied based on assigned attributes of the subject, environment conditions, and a set of policies that are specified in terms of those attributes and conditions.” ABAC is a fine-grained model since you can assign any attributes to the user, but at the same time it becomes a burden and hard to manage:\nWhen defining permissions, the relationship between users and objects cannot be visualized. If the rules are a little complex or confusingly designed, it will be troublesome for the administrator to maintain and trace. This can cause performance problems when there is a large number of permissions to process.\nABAC Example Kubernetes initially uses ABAC as access control and is configured via JSON Lines, for example:\nAlice can just read pods in namespace “foo”:\n{\u0026#34;apiVersion\u0026#34;: \u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: {\u0026#34;user\u0026#34;: \u0026#34;alice\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;pods\u0026#34;, \u0026#34;readonly\u0026#34;: true}} What Is NGAC? NGAC, or Next Generation Access Control, takes the approach of modeling access decision data as a graph. NGAC enables a systematic, policy-consistent approach to access control, granting or denying users administrative capabilities with a high level of granularity. NGAC was developed by NIST (National Institute of Standards and Technology) and is currently used in Tetrate Q and Tetrate Service Bridge .\nThere are several types of entities; they represent the resources you want to protect, the relationships between them, and the actors that interact with the system. The entities are:\nUsers Objects User attributes, such as organization unit Object attributes, such as folders Policy classes, such as file system access, location, and time NIST’s David Ferraiolo and Tetrate ‘s Ignasi Barrera shared how NGAC works at their presentation on Next Generation Access Control at Service Mesh Day 2019 in San Francisco.\nNGAC is based on the assumption that you can represent the system you want to protect in a graph that represents the resources you want to protect and your organizational structure, in a way that has meaning to you and that adheres to your organization semantics. On top of this model that is very particular to your organization, you can overlay policies. Between the resource model and the user model, the permissions are defined. This way NGAC provides an elegant way of representing the resources you want to protect, the different actors in the system, and how both worlds are tied together with permissions.\nNGAC DAG Image via Linear Time Algorithms to Restrict Insider Access using Multi-Policy Access Control Systems NGAC Example The following example shows a simple NGAC graph with a User DAG representing an organization structure, an Object DAG representing files and folders in a filesystem, a categorization of the files, and two different policies — file system and scope — …","relpermalink":"/en/blog/why-you-should-choose-ngac-as-your-access-control-model/","summary":"This article will introduce you to the next generation permission control model, NGAC, and compare ABAC, RABC, and explain why you should choose NGAC.","title":"Why You Should Choose NGAC as Your Access Control Model"},{"content":" IstioCon 2021 poster (Jimmy Song) Topic: Service Mesh in China Time: February 23rd, 10:00 - 10:10 am Beijing time How to participate: IstioCon 2021 website Cost: Free From February 22-25, Beijing time, the Istio community will be hosting the first IstioCon online, and registration is free to attend! I will be giving a lightning talk on Tuesday, February 23rd (the 12th day of the first month of the lunar calendar), as an evangelist and witness of Service Mesh technology in China, I will introduce the Service Mesh industry and community in China.\nI am a member of the inaugural IstioCon organizing committee with Zhonghu Xu (Huawei) and Shaojun Ding (Intel), as well as the organizer of the China region. Considering Istio’s large audience in China, we have arranged for Chinese presentations that are friendly to the Chinese time zone. There will be a total of 14 sharing sessions in Chinese, plus dozens more in English. The presentations will be in both lightning talk (10 minutes) and presentation (40 minutes) formats.\nJoin the Cloud Native Community Istio SIG to participate in the networking at this conference. For the schedule of IstioCon 2021, please visit IstioCon 2021 official website , or click for details.\n","relpermalink":"/en/notice/istiocon-2021/","summary":"IstioCon 2021, I'll be giving a lightning talk, February 22nd at 10am BST.","title":"IstioCon 2021 Lightning Talk Preview"},{"content":"The ServiceMesher website has lost connection with the webhook program on the web publishing server because the GitHub where the code is hosted has has been “lost” and the hosting server is temporarily unable to log in, so the site cannot be updated. Today I spent a day migrating all the blogs on ServiceMesher to the Cloud Native Community website cloudnative.to , and as of today, there are 354 blogs on the Cloud Native Community.\nServiceMesher blogs Now we plan to archive ServiceMesher official GitHub (all pages under the servicemesher.com domain) We are no longer accepting new PRs, so please submit them directly to the Cloud Native Community . Thank you all!\n","relpermalink":"/en/notice/servicemesher-blog-merged/","summary":"ServiceMesher website is no longer maintained, plan to archive the website code, the blog has been migrated to Cloud Native Community, please submit the new blog to Cloud Native Community.","title":"ServiceMesher website is no longer maintained, the original blog has been migrated to the cloud native community"},{"content":"In this article, I’ll give you an overview of Istio ‘s history of virtual machine integration support. In particular, the introduction of the smart DNS proxy and WorkloadGroup in Istio 1.8, which makes virtual machines and containers equivalent at the resource abstraction level.\nI will show you a tumultuous odyssey of Istio’s virtual machine integration. Tetrate, the enterprise service mesh company that made pushing Istio to run everywhere part of its founding mission, has used VM features extensively in customer deployments and has been instrumental in pushing VMs to Istio upstream.\nPreface In my previous article , I talked about how Istio 1.7 supported virtual machines. But at that time, late October, virtual machines were still not seamlessly integrated into Istio — there was still a lot of manual work required. Now, Istio 1.8 has added WorkloadGroup and smart DNS proxy, which allows non-Kubernetes workloads like VMs to become first-class citizens in Istio — just like pods.\nWith or without a sidecar installed for virtual machines, until 1.7 you could not resolve the DNS name of a Kubernetes service unless a kube-external DNS was configured — which is the last piece of virtual machine integration in Istio. This shortcoming has finally been fixed in Istio 1.8.\nWhy Is Virtual Machine Support Important? In the process of migrating our applications to cloud native architectures and continuously containerizing them, we will go through three phases as shown in the figure below.\nCloud Native Stages Stage 1: All applications are deployed on virtual machines Stage 2: Applications are deployed on both virtual machines and containers, are migrating from virtual machines to containers, and are using Kubernetes to manage containers. Stage 3: All applications are deployed in containers first, using Kubernetes to manage containers and Istio to manage service-to-service communication. The above diagram is artificially simplified: in reality, there might be multiple hybrid clouds, multiple regions, multiple clusters, etc. Plus, at stage 3 containers and virtual machines may remain in long-term coexistence, but the trend of containerization remains unchanged.\nIstio’s History of Virtual Machine Support Istio’s support for virtual machines is a long process, an odyssey of sorts.\n0.2: Istio Mesh Expansion As of version 0.2, Istio added virtual machines to the Mesh via Istio Mesh Expansion , provided that the following prerequisites were met.\nVirtual machines must have direct access to the application’s pods via IP address, which requires a flat network between the container and the VM via VPC or VPN; and virtual machines do not need access to the Cluster IP, but rather direct access to the service’s endpoints. Virtual machines must have access to Istio’s control plane services (Pilot, Mixer, CA, now being integrated as Istiod), which can expose the control plane endpoints to virtual machines by deploying load balancers in the Istio Mesh. (optional) the virtual machine has access to the DNS server inside the Mesh (deployed in Kubernetes). The steps to integrate a virtual machine are as follows.\nCreate an internal load balancer for the Istio control plane service and the DNS service for the Kubernetes cluster. Generate a configuration file for the Istio Service CIDR, Service Account token, security certificate, and IP of the Istio Control Plane Service (the IP exposed through the Internal Load Balancer) and send it to the virtual machine. Setup the Istio component, dnsmaq (for DNS discovery), in the virtual machine; so that the virtual machine can access the services in the mesh using FQDN, to ensure that the virtual machine can correctly resolve the Cluster IP of the services in the mesh. To run the service in a virtual machine, you need to configure the sidecar, add inbound ports to be intercepted, then restart Istio and also run istioctl to register the service. The following figure shows the detailed flow from integrating a virtual machine to accessing services in the virtual machine in a mesh.\nFigure 1 Figure 1\nThe DNS is hijacked by dnsmasq deployed in the virtual machine, which allows it to correctly obtain the Cluster IP of the Istio service (Kubernetes’ built-in DNS). Access to Kubernetes’ built-in DNS service (which is exposed outside the cluster via the Internal Load Balancer and can be accessed directly). Return the Cluster IP resolved by productpage.bookinfo.svc.cluster.local, noting that the IP address is not directly accessible, but failure to be DNS resolved will result in a failed VM request for the service. The virtual machine’s call to services in a mesh is hijacked by the sidecar proxy. Since the proxy is connected to the Istio control plane, the endpoints of the service can be queried via xDS, so traffic will be forwarded to one of the endpoints. To access VM services in mesh, you need to manually add VM services to mesh using the istioctl register command, which essentially registers the VM services to the …","relpermalink":"/en/blog/istio-18-a-virtual-machine-integration-odyssey/","summary":"In this article, I’ll give you an overview of Istio‘s history of virtual machine integration support. In particular, the introduction of the smart DNS proxy and WorkloadGroup in Istio 1.8, which makes virtual machines and containers equivalent at the resource abstraction level.","title":"Istio 1.8: A Virtual Machine Integration Odyssey"},{"content":"A service mesh is a relatively simple concept, consisting of a bunch of network proxies paired with each service in an application, plus a set of task management processes. The proxies are called the data plane and the management processes are called the control plane in the Service Mesh. The data plane intercepts calls between different services and “processes” them; the control plane is the brain of the mesh that coordinates the behavior of proxies and provides APIs for operations and maintenance personnel to manipulate and observe the entire network.\nThe diagram below shows the architecture of a service mesh.\nService Mesh Architecture Further, the service mesh is a dedicated infrastructure layer designed to enable reliable, fast, and secure inter-service invocation in microservices architectures. It is not a mesh of “services” but rather a mesh of “proxies” that services can plug into, thus abstracting the network from the application code. In a typical service mesh, these proxies are injected into each service deployment as a sidecar (and also may be deployed at the edge of the mesh). Instead of invoking services directly over the network, services invoke their local sidecar proxy, which in turn manages requests on behalf of the service, pushing the complexities of inter-service communications into a networking layer that can resolve them at scale. The set of interconnected sidecar proxies implements a so-called data plane, while on the other hand the service mesh control plane is used to configure proxies. The infrastructure introduced by a service mesh provides an opportunity, too, to collect metrics about the traffic that is flowing through the application.\nThe architecture of a service mesh The infrastructure layer of a service mesh is divided into two main parts: the control plane and the data plane.\nCharacteristics of the control plane\nDo not parse packets directly. Communicates with proxies in the control plane to issue policies and configurations. Visualizes network behavior. Typically provides APIs or command-line tools for configuration versioning and management for continuous integration and deployment. Characteristics of the data plane\nIs usually designed with the goal of statelessness (though in practice some data needs to be cached to improve traffic forwarding performance). Directly handles inbound and outbound packets, forwarding, routing, health checking, load balancing, authentication, authentication, generating monitoring data, etc. Is transparent to the application, i.e., can be deployed senselessly. Changes brought by the service mesh Decoupling of microservice governance from business logic\nA service mesh takes most of the capabilities in the SDK out of the application, disassembles them into separate processes, and deploys them in a sidecar model. By separating service communication and related control functions from the business process and synching them to the infrastructure layer, a service mesh mostly decouples them from the business logic, allowing application developers to focus more on the business itself.\nNote that the word “mostly” is mentioned here and that the SDK often needs to retain protocol coding and decoding logic, or even a lightweight SDK to implement fine-grained governance and monitoring policies in some scenarios. For example, to implement method-level call distributed tracing, the service mesh requires the business application to implement trace ID passing, and this part of the implementation logic can also be implemented through a lightweight SDK. Therefore, the service mesh is not zero-intrusive from a code level.\nUnified governance of heterogeneous environments\nWith the development of new technologies and staff turnover, there are often applications and services in different languages and frameworks in the same company, and in order to control these services uniformly, the previous practice was to develop a complete set of SDKs for each language and framework, which is very costly to maintain. With a service mesh, multilingual support is much easier by synching the main service governance capabilities to the infrastructure. By providing a very lightweight SDK, and in many cases, not even a separate SDK, it is easy to achieve unified traffic control and monitoring requirements for multiple languages and protocols.\nFeatures of service mesh Service mesh also has three major technical advantages over traditional microservice frameworks.\nObservability\nBecause the service mesh is a dedicated infrastructure layer through which all inter-service communication passes, it is uniquely positioned in the technology stack to provide uniform telemetry at the service invocation level. This means that all services are monitored as “black boxes.” The service mesh captures route data such as source, destination, protocol, URL, status codes, latency, duration, etc. This is essentially the same data that web server logs can provide, but the service mesh captures this data for …","relpermalink":"/en/blog/what-is-a-service-mesh/","summary":"This article will take you through what a service mesh is, as well as its architecture, features, and advantages and disadvantages.","title":"What Is a Service Mesh?"},{"content":"1.8 is the last version of Istio to be released in 2020 and it has the following major updates:\nSupports installation and upgrades using Helm 3. Mixer was officially removed. Added Istio DNS proxy to transparently intercept DNS queries from applications. WorkloadGroup has been added to simplify the integration of virtual machines. WorkloadGroup is a new API object. It is intended to be used with non-Kubernetes workloads like Virtual Machines and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies.\nInstallation and Upgrades Istio starts to officially support the use of Helm v3 for installations and upgrades. In previous versions, the installation was done with the istioctl command-line tool or Operator. With version 1.8, Istio supports in-place and canary upgrades with Helm.\nEnhancing Istio’s Usability The istioctl command-line tool has a new bug reporting feature (istioctl bug-report ), which can be used to collect debugging information and get cluster status.\nThe way to install the add-on has changed: 1.7 istioctl is no longer recommended and has been removed in 1.8, to help solve the problem of add-on lagging upstream and to make it easier to maintain.\nTetrate is an enterprise service mesh company. Our flagship product, TSB, enables customers to bridge their workloads across bare metal, VMs, K8s, \u0026amp; cloud at the application layer and provide a resilient, feature-rich service mesh fabric powered by Istio, Envoy, and Apache SkyWalking.\nMixer, the Istio component that had been responsible for policy controls and telemetry collection, has been removed. Its functionalities are now being served by the Envoy proxies. For extensibility, service mesh experts recommend using WebAssembly (Wasm) to extend Envoy; and you can also try the GetEnvoy Toolkit , which makes it easier for developers to create Wasm extensions for Envoy. If you still want to use Mixer, you must use version 1.7 or older. Mixer continued receiving bug fixes and security fixes until Istio 1.7. Many features supported by Mixer have alternatives as specified in the Mixer Deprecation document, including the in-proxy extensions based on the Wasm sandbox API.\nSupport for Virtual Machines Istio’s recent upgrades have steadily focused on making virtual machines first-class citizens in the mesh. Istio 1.7 made progress to support virtual machines and Istio 1.8 adds a smart DNS proxy , which is an Istio sidecar agent written in Go. The Istio agent on the sidecar will come with a cache that is dynamically programmed by Istiod DNS Proxy. DNS queries from applications are transparently intercepted and served by an Istio proxy in a pod or VM that intelligently responds to DNS query requests, enabling seamless multicluster access from virtual machines to the service mesh.\nIstio 1.8 adds a WorkloadGroup , which describes a collection of workload instances. It provides a specification that the workload instances can use to bootstrap their proxies, including the metadata and identity. It is only intended to be used with non-k8s workloads like Virtual Machines, and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies. Using WorkloadGroups, Istio has started to help automate VM registration with istioctl experimental workload group .\nTetrate , the enterprise service mesh company, uses these VM features extensively in customers’ multicluster deployments, to enable sidecars to resolve DNS for hosts exposed at ingress gateways of all the clusters in a mesh; and to access them over mutual TLS.\nConclusion All in all, the Istio team has kept the promise made at the beginning of the year to maintain a regular release cadence of one release every three months since the 1.1 release in 2018, with continuous optimizations in performance and user experience for a seamless experience of brownfield and greenfield apps on Istio. We look forward to more progress from Istio in 2021.\n","relpermalink":"/en/blog/istio-1-8-a-smart-dns-proxy-takes-support-for-virtual-machines-a-step-further/","summary":"WorkloadGroup is a new API object. It is intended to be used with non-Kubernetes workloads like Virtual Machines and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies.","title":"Istio 1.8: A Smart DNS Proxy Takes Support For Virtual Machines A Step Further"},{"content":"Istio is a popular service mesh to connect, secure, control, and observe services. When it was first introduced as open source in 2017, Kubernetes was winning the container orchestration battle and Istio answered the needs of organizations moving to microservices. Although Istio claims to support heterogeneous environments such as Nomad, Consul, Eureka, Cloud Foundry, Mesos, etc., in reality, it has always worked best with Kubernetes — on which its service discovery is based.\nIstio was criticized for a number of issues early in its development, for the large number of components, the complexity of installation and maintenance, the difficulty of debugging, a steep learning curve due to the introduction of too many new concepts and objects (up to 50 CRDs), and the impact of Mixer components on performance. But these issues are gradually being overcome by the Istio team. As you can see from the roadmap released in early 2020, Istio has come a long way.\nBetter integration of VM-based workloads into the mesh is a major focus for the Istio team this year. Tetrate also offers seamless multicloud connectivity, security, and observability, including for VMs, via its product Tetrate Service Bridge . This article will take you through why Istio needs to integrate with virtual machines and how you can do so.\nWhy Should Istio Support Virtual Machines? Although containers and Kubernetes are now widely used, there are still many services deployed on virtual machines and APIs outside of the Kubernetes cluster that needs to be managed by Istio mesh. It’s a huge challenge to unify the management of the brownfield environment with the greenfield.\nWhat Is Needed to Add VMs to the Mesh? Before the “how,” I’ll describe what is needed to add virtual machines to the mesh. There are a couple of things that Istio must know when supporting virtual machine traffic: which VMs have services that should be part of the mesh, and how to reach the VMs. Each VM also needs an identity, in order to communicate securely with the rest of the mesh. These requirements could work with Kubernetes CRDs, as well as a full-blown Service Registry like Consul. And the service account based identity bootstrapping could work as a mechanism for assigning workload identities to VMs that do not have a platform identity. For VMs that do have a platform identity (like EC2, GCP, Azure, etc.), work is underway in Istio to exchange the platform identity with a Kubernetes identity for ease of setting up mTLS communication.\nHow Does Istio Support Virtual Machines? Istio’s support for virtual machines starts with its service registry mechanism. The information about services and instances in the Istio mesh comes from Istio’s service registries, which up to this point have only looked at or tracked pods. In newer versions, Istio now has resource types to track and watch VMs. The sidecars inside the mesh cannot observe and control traffic to services outside the mesh, because they do not have any information about them.\nThe Istio community and Tetrate have done a lot of work on Istio’s support for virtual machines. The 1.6 release included the addition of WorkloadEntry, which allows you to describe a VM exactly as you would a host running in Kubernetes. In 1.7, the release started to add the foundations for bootstrapping VMs into the mesh automatically through tokens, with Istio doing the heavy lifting. Istio 1.8 will debut another abstraction called WorkloadGroup, which is similar to a Kubernetes Deployment object — but for VMs.\nThe following diagram shows how Istio models services in the mesh. The predominant source of information comes from a platform service registry like Kubernetes, or a system like Consul. In addition, the ServiceEntry serves as a user-defined service registry, modeling services on VMs or external services outside the organization.\nWhy install Istio in a virtual machine when you can just use ServiceEntry to bring in the services in the VMs?\nUsing ServiceEntry, you can enable services inside the mesh to discover and access external services; and in addition, manage the traffic to those external services. In conjunction with VirtualService, you can also configure access rules for the corresponding external service — such as request timeouts, fault injection, etc. — to enable controlled access to the specified external service.\nEven so, it only controls the traffic on the client-side, not access to the introduced external service to other services. That is, it cannot control the behavior of the service as the call initiator. Deploying sidecars in a virtual machine and introducing the virtual machine workload via workload selector allows the virtual machine to be managed indiscriminately, like a pod in Kubernetes.\nFuture As you can see from the bookinfo demo , there is too much manual work involved in the process and it’s easy to go wrong. In the future, Istio will improve VM testing to be realistic, automate bootstrapping based on platform identity, …","relpermalink":"/en/blog/how-to-integrate-virtual-machines-into-istio-service-mesh/","summary":"Better integration of virtual machine-based workloads into the service mesh is a major focus for the Istio team this year, and Tetrate also provides seamless multi-cloud connectivity, security and observability, including for virtual machines, through its product Tetrate Service Bridge. This article will show you why Istio needs to integrate with virtual machines and how.","title":"How to Integrate Virtual Machines Into Istio Service Mesh"},{"content":"Today is my 914th day and also the last day with Ant Group , tomorrow is September 1st, which is usually the day school starts, and everyone at Alibaba is known as “classmate”, tomorrow I will join Tetrate , and that’s kind of starting my new semester!\nAnt/Alibaba and the Cloud Native Community To date, Ant/Alibaba Group has had a profound impact on my career, especially its corporate culture and values, and the Alibaba recruiting philosophy of “finding like-minded people”, and isn’t the process of creating the Cloud Native Community also a process of finding like-minded people? Cloud Native Community is like a small society, I don’t want it to have much social value, but only want it to make a small but beautiful change to individuals, to enterprises and to society. I constantly think about myself as an individual and as an employee, especially as an initiator of the community. What is my mission as an individual, an employee, and especially as an initiator of a community? What role should I play in the company? Where is this community going? I’m fumbling along, but because of your support, it makes me stronger and more committed to the adoption and application of cloud native technology in China, outside of me I may have gone faster, but now with the community together we will go further!\n24 June 2019, Shanghai, KubeCon China 2019 June 24, 2019, Shanghai, KubeCon China 2019\nJoining Tetrate Over the past two years, I’ve been working hard to promote Istio and Service Mesh technology, and with funding from Ant Group, I started the ServiceMesher Community to bring Service Mesh technology to China. Next I want to bring Chinese practice to the world.\nAs a Developer Advocate, the most important thing is not to stop learning, but to listen and take stock. Over the past two years, I’ve seen a lot of people show interest in Service Mesh, but not enough to understand the risks and lack of knowledge about the new technology. I’m excited to join this Service Mesh-focused startup Tetrate , a global telecommuting startup with products built around open source Istio , [Envoy](https:/ /envoyproxy.io) and Apache SkyWalking , it aims to make it to be the cloud native network infrastructure. Here are several maintainers of these open source projects, such as Sheng Wu , Zack Butcher , Lizan Zhou , etc., and I believe that working with them can help you understand and apply Service Mesh quickly and effectively across cloud native.\nMore Earlier this year as I was preparing for the Cloud Native community, I set the course for the next three years - cloud native, open source and community. The road to pursue my dream is full of thorns, not only need courage and perseverance, but also need you to be my strong backing, I will overcome the thorns and move forward. Open source belongs to the world, to let the world understand us better, we must be more active into the world. I hope that China’s open source tomorrow will be better, I hope that Service Mesh technology will be better applied by the enterprises in China, I hope that cloud native can benefit the public, and I hope that we can all find our own mission.\nWe are hiring now, if you are interested with Tetrate , please send your resume to careers@tetrate.io .\n","relpermalink":"/en/blog/moving-on-from-ant-group/","summary":"Today is my last day at Ant and tomorrow I'm starting a new career at Tetrate.","title":"New Beginning - Goodbye Ant, Hello Tetrate"},{"content":"Just tonight, the jimmysong.io website was moved to the Alibaba Cloud Hong Kong node. This is to further optimize the user experience and increase access speed. I purchased an ECS on the Alibaba Cloud Hong Kong node, and now I have a public IP and can set subdomains. The website was previously deployed on GitHub Pages, the access speed is average, and it has to withstand GitHub instability. Impact (In recent years, GitHub downtime has occurred).\nMeanwhile, the blog has also done a lot to improve the site, thanks to Bai Jun away @baijunyao strong support, a lot of work for the revision of the site, including:\nChanged the theme color scheme and deepened the contrast Use aligolia to support full site search Optimized mobile display Articles in the blog have added zoom function Added table of contents to blog post This site is built on the theme of educenter .\nThanks to the majority of netizens who have supported this website for several years. The website has been in use for more than three years and has millions of visits. It has undergone two major revisions before and after, on January 31, 2020 and October 8, 2017, respectively. And changed the theme of the website. In the future, I will share more cloud-native content with you as always, welcome to collect, forward, and join the cloud-native community to communicate with the majority of cloud-native developers.\n","relpermalink":"/en/notice/migrating-to-alibaba-cloud/","summary":"Move the website to the Alibaba Cloud Hong Kong node to increase the speed of website access and the convenience of obtaining public IP and subdomain names.","title":"Move to Alibaba Cloud Hong Kong node"},{"content":" Just the other day, Java just celebrated its 25th birthday , and from the time of its birth it was called “write once, run everywhere”, but more than 20 years later, there is still a deep gap between programming and actual production delivery. the world of IT is never short of concepts, and if a concept doesn’t solve the problem, then it’s time for another layer of concepts. it’s been 6 years since Kubernetes was born, and it’s time for the post-Kubernetes era - the era of cloud-native applications!\nCloud Native Stage This white paper will take you on a journey to explore the development path of cloud-native applications in the post-Kubernetes era.\nHighlights of the ideas conveyed include.\nCloud-native has passed through a savage growth period and is moving towards uniform application of standards. Kubernetes’ native language does not fully describe the cloud-native application architecture, and the development and operation functions are heavily coupled in the configuration of resources. Operator’s expansion of the Kubernetes ecosystem has led to the fragmentation of cloud-native applications, and there is an urgent need for a unified application definition standard. The essence of OAM is to separate the R\u0026amp;D and O\u0026amp;M concerns in the definition of cloud-native applications, and to further abstract resource objects, simplify and encompass everything. “Kubernetes Next Generation” refers to the fact that after Kubernetes became the infrastructure layer standard, the focus of the cloud-native ecology is being overtaken by the application layer, and the last two years have been a powerful exploration of the hot Service Mesh process, and the era of cloud-native application architecture based on Kubernetes is coming. Kubernetes has become an established operating platform for cloud-native applications, and this white paper will expand with Kubernetes as the default platform, including an explanation of the OAM-based hierarchical model for cloud-native applications.\n","relpermalink":"/en/notice/guide-to-cloud-native-app/","summary":"Take you on a journey through the post-Kubernetes era of cloud-native applications.","title":"Guide to Cloud Native Application"},{"content":"At the beginning of 2020, due to the outbreak of the Crona-19 pandemic, employees around the world began to work at home. Though the distance between people grew farer, there was a group of people, who were us working in the cloud native area, gathered together for a common vision. During the past three months, we have set up the community management committee and used our spare time working together to complete the preparatory work for the community. Today we are here to announce the establishment of the Cloud Native Community.\nBackground Software is eating the world. —— Marc Andreessen\nThis sentence has been quoted countless times, and with the rise of Cloud Native, we’d like to talk about “Cloud Native is eating the software.” As more and more enterprises migrate their services to the cloud, the original development mode of enterprises cannot adapt to the application scenarios in the cloud, and it is being reshaped to conform to the cloud native standard.\nSo what is cloud native? Cloud native is a collection of best practices in architecture, r\u0026amp;d process and team culture to support faster innovation, superior user experience, stable and reliable user service and efficient r\u0026amp;d. The relationship between the open source community and the cloud native is inseparable. It is the existence of the open source community, especially the end user community, that greatly promotes the continuous evolution of cloud native technologies represented by container, service mesh and microservices.\nCNCF (Cloud Native Computing Foundation) holds Cloud Native conference every year in the international community, which has a wide audience and great influence. But it was not held in China for the first time until 2018, after several successful international events. However, there are no independent foundations or neutral open source communities in China. In recent years, many cloud native enthusiasts in China have set up many communication groups and held many meetups, which are very popular. Many excellent open source projects have emerged in the cloud native field, but there is no organized neutral community for overall management. Under this background, the Cloud Native Community emerges at the right moment.\nAbout Cloud Native Community is an open source community with technology, temperature and passion. It was founded spontaneously by a group of industry elites who love open source and uphold the principle of consensus, co-governance, co-construction and sharing. The aim of the community is: connection, neutral, open source. We are based in China, facing the world, enterprise neutrality, focusing on open source, and giving feedback to open source.\nIntroduction for the Steering Community:https://cloudnative.to/en/team/ .\nYou will gain the followings after joining the community:\nKnowledge and news closer to the source A more valuable network More professional and characteristic consultation Opportunities to get closer to opinion leaders Faster and more efficient personal growth More knowledge sharing and exposure opportunities More industry talent to be found Contact Contact with us.\nEmail: mailto:contact@cloudnative.to Twitter: https://twitter.com/CloudNativeTo ","relpermalink":"/en/notice/cloud-native-community-announecement/","summary":"Today the Community Steering Committee announced the official formation of the Cloud Native Community.","title":"Establishment of the Cloud Native Community"},{"content":"This article is a rework of previously written content and is included in the Istio Handbook of the ServiceMesher community . Other chapters are still being compiled.\nPeople who have just heard of Service Mesh and tried Istio may have the following questions:\nWhy does Istio bind Kubernetes? What roles do Kubernetes and Service Mesh play in cloud native? What aspects of Kubernetes has Istio extended? What problems have been solved? What is the relationship between Kubernetes, xDS protocols (Envoy , MOSN, etc) and Istio? Should I use Service Mesh? In this section, we will try to guide you through the internal connections between Kubernetes, the xDS protocol, and Istio Service Mesh. In addition, this section will also introduce the load balancing methods in Kubernetes, the significance of the xDS protocol for Service Mesh, and why Istio is needed in time for Kubernetes.\nUsing Service Mesh is not to say that it will break with Kubernetes, but that it will happen naturally. The essence of Kubernetes is to perform application lifecycle management through declarative configuration, while the essence of Service Mesh is to provide traffic and security management and observability between applications. If you have built a stable microservice platform using Kubernetes, how do you set up load balancing and flow control for calls between services?\nThe xDS protocol created by Envoy is supported by many open source software, such as Istio , Linkerd , MOSN, etc. Envoy’s biggest contribution to Service Mesh or cloud native is the definition of xDS. Envoy is essentially a proxy. It is a modern version of proxy that can be configured through APIs. Based on it, many different usage scenarios are derived, such as API Gateway, Service Mesh. Sidecar proxy and Edge proxy in.\nThis section contains the following\nExplain the role of kube-proxy. Kubernetes’ limitations in microservice management. Describe the features of Istio Service Mesh. Describe what xDS includes. Compare some concepts in Kubernetes, Envoy and Istio Service Mesh. Key takeaways If you want to know everything in advance, here are some of the key points from this article:\nThe essence of Kubernetes is application lifecycle management, specifically deployment and management (scaling, scaling, automatic recovery, release). Kubernetes provides a scalable and highly resilient deployment and management platform for microservices. The foundation of Service Mesh is a transparent proxy. After the traffic between microservices is intercepted through sidecar proxy, the behavior of microservices is managed through the control plane configuration. Service Mesh decoupled from Kubernetes traffic management, the internal flow without the need of Service Mesh kube-proxy supporting components, micro-services closer to abstract the application layer by, for traffic between management services, security and observability. xDS defines the protocol standards for Service Mesh configuration. Service Mesh is a higher-level abstraction of services in Kubernetes. Its next step is serverless. Kubernetes vs Service Mesh The following figure shows the service access relationship between Kubernetes and Service Mesh (one sidecar per pod mode).\nkubernetes vs service mesh Traffic forwarding\nEach node of the cluster Kubernetes a deployed kube-proxy assembly Kubernetes API Server may communicate with the cluster acquired service information, and then set iptables rules, sends a request for a service directly to the corresponding Endpoint (belonging to the same group service pod).\nService discovery\nService registration in Service Mesh Istio Service Mesh can use the service in Kubernetes for service registration. It can also connect to other service discovery systems through the platform adapter of the control plane, and then generate the configuration of the data plane (using CRD statements, stored in etcd), a transparent proxy for the data plane. (Transparent proxy) is deployed in the sidecar container in each application service pod. These proxy need to request the control plane to synchronize the proxy configuration. The reason why is a transparent proxy, because there is no application container fully aware agent, the process kube-proxy components like the need to block traffic, but kube-proxythat blocks traffic to Kubernetes node and sidecar proxy that blocks out of the Pod For more information, see Understanding Route Forwarding by the Envoy Sidecar Proxy in Istio Service Mesh .\nDisadvantages of Service Mesh\nBecause each node on Kubernetes many runs Pod, the original kube-proxyrouting forwarding placed in each pod, the distribution will lead to a lot of configuration, synchronization, and eventual consistency problems. In order to perform fine-grained traffic management, a series of new abstractions will be added, which will further increase the user’s learning costs. However, with the popularization of technology, this situation will gradually ease.\nAdvantages of Service Mesh\nkube-proxy The …","relpermalink":"/en/blog/service-mesh-the-microservices-in-post-kubernetes-era/","summary":"This article is a rework of previously written content and is included in the Istio Handbook of the ServiceMesher community . Other chapters are still being compiled.","title":"Service Mesh - The Microservices in Post Kubernetes Era"},{"content":"2020 is indeed a bad start. In less than a month, I hardly heard any good news:\nTrump assassinated Major General Sulaymani of Iran; this pneumonia outbreak in Wuhan; the news that my basketball icon Kobe died of a helicopter crash really shocked me, and the Lakers said goodbye on the 24th. This gave me another spiritual blow during the Spring Festival, which was originally lacking in interest.\n2020 is bound to be a year deeply remembered in all humankind. In the last few days of the first month of the year, I decided to revise the website. The first reason was that I could n’t go out during the extended Chinese New Year holiday, and it was boring at home. And many pictures on the website were saved on Weibo map beds. The picture bed is unstable, causing many photos to be irretrievable; coupled with a habit of organizing the website every long holiday (the last revision of the website was completed during the National Day holiday in 2018, completed at home for 7 days), so I decided to The website has been revised again and it has become what it is now.\nFeatures The website has the following features after this revision:\nReorganize the website content, the structure is more reasonable Support email subscription Images are stored on Github Responsive static website, card design, better user experience Everyone is welcome to enter your email address in the email input box at the bottom of the page. Once there is an important content update on this site, we will push you through the email as soon as possible.\n","relpermalink":"/en/notice/website-revision-notice/","summary":"In the last days of the first month of 2020, I decided to revamp the website.","title":"jimmysong.io website revision notice"},{"content":"A few days ago during the Mid-Autumn Festival, I translated Google’s Engineering Practices documentation , which is open source on Github. The original document Github address: https://github.com/google/eng-practices , the main content so far is summarized by Google How to conduct a Code Review guide, based on the title of the original Github repository, we will add more Google engineering practices in the future.\nGithub:https://github.com/rootsongjc/eng-practices Browse online: https://jimmysong.io/eng-practices The translation uses the same style and directory structure as the original document. In fact, if the original text has international requirements, it can be directly incorporated, but according to the translation suggestions submitted by several previous people, the author of this project does not recommend translation. The first is translation. The documents may be unmaintained, and the accuracy of the documents cannot be guaranteed.\nFor suggestions on Chinese integration, see:\nAdd Chinese translation #12 Add the link of Chinese version of code reviewer’s guide #8 ","relpermalink":"/en/notice/google-engineering-practices-zh/","summary":"Translated from Google's open source documentation on Github","title":"Chinese version of Google Engineering Practice Documents"},{"content":"The following paragraph is a release note from the Istio official blog https://istio.io/zh/blog/2019/announcing-1.1/ , which I translated.\nIstio was released at 4 a.m. Beijing time today and 1 p.m. Pacific time.\nSince the 1.0 release last July, we have done a lot to help people get Istio into production. We expected to release a lot of patches (six patches have been released so far!), But we are also working hard to add new features to the product.\nThe theme for version 1.1 is “Enterprise Ready”. We are happy to see more and more companies using Istio in production, but as some big companies join in, Istio also encounters some bottlenecks.\nThe main areas we focus on include performance and scalability. As people gradually put Istio into production and use larger clusters to run more services with higher capacity, there may be some scaling and performance issues. Sidecar takes up too much resources and adds too much latency. The control plane (especially Pilot) consumes excessive resources.\nWe put a lot of effort into making the data plane and control plane more efficient. In the 1.1 performance test, we observed that sidecars typically require 0.5 vCPU to process 1000 rps. A single Pilot instance can handle 1000 services (and 2000 pods) and consumes 1.5 vCPUs and 2GB of memory. Sidecar adds 5 milliseconds at the 50th percentile and 10 milliseconds at the 99th percentile (the execution strategy will increase latency).\nWe have also completed the work of namespace isolation. You can use the Kubernetes namespace to enforce control boundaries to ensure that teams do not interfere with each other.\nWe have also improved multi-cluster functionality and usability. We listened to the community and improved the default settings for flow control and policies. We introduced a new component called Galley. Galley validates YAML configuration, reducing the possibility of configuration errors. Galley is also used in multi-cluster setups-collecting service discovery information from each Kubernetes cluster. We also support other multi-cluster topologies, including single control planes and multiple synchronous control planes, without the need for flat network support.\nSee the release notes for more information and details .\nThere is more progress on this project. As we all know, Istio has many moving parts, and they take on too much work. To address this, we have recently established the Usability Working Group (available at any time). A lot happened in the community meeting (Thursday at 11 am) and in the working group. You can log in to discuss.istio.io with GitHub credentials to participate in the discussion!\nThanks to everyone who has contributed to Istio over the past few months-patching 1.0, adding features to 1.1, and extensive testing recently on 1.1. Special thanks to companies and users who work with us to install and upgrade to earlier versions to help us identify issues before they are released.\nFinally, go to the latest documentation and install version 1.1! Happy meshing!\nOfficial website The ServiceMesher community has been maintaining the Chinese page of the official Istio documentation since the 0.6 release of Istio . As of March 19, 2019, there have been 596 PR merges, and more than 310 documents have been maintained. Thank you for your efforts! Some documents may lag slightly behind the English version. The synchronization work is ongoing. For participation, please visit https://github.com/servicemesher/istio-official-translation. Istio official website has a language switch button on the right side of each page. You can always Switch between Chinese and English versions, you can also submit document modifications, report website bugs, etc.\nServiceMesher Community Website\nThe ServiceMesher community website http://www.servicemesher.com covers all technical articles in the Service Mesh field and releases the latest activities in a timely manner. It is your one-stop portal to learn about Service Mesh and participate in the community.\n","relpermalink":"/en/notice/istio-11/","summary":"Istio 1.1 was released at 4 am on March 20th, Beijing time. This version took 8 months! The ServiceMesher community also launched the Istio Chinese documentation.","title":"Istio 1.1 released"},{"content":"Istio handbook was originally an open source e-book I created (see https://jimmysong.io/istio-handbook ). It has been written for 8 months before donating to the ServiceMesher community. In order to further popularize Istio and Service Mesh technology, this book Donate to the community for co-authoring. The content of the original book was migrated to https://github.com/servicemesher/istio-handbook on March 10, 2019. The original book will no longer be updated.\nGitHub address: https://github.com/servicemesher/istio-handbook Reading address online: http://www.servicemesher.com/istio-handbook/ Conceptual picture of this book, cover photo of Shanghai Jing’an Temple at night , photo by Jimmy Song .\nThe publishing copyright of this book belongs to the blog post of Electronic Industry Press. Please do not print and distribute it without authorization.\nIstio is a service mesh framework jointly developed by Google, IBM, Lyft, etc., and began to enter the public vision in early 2017. As an important infrastructure layer that inherits Kubernetes and connects to the serverless architecture in the cloud-native era, Istio is of crucial importance important. The ServiceMesher community, as one of the earliest open source communities in China that is researching and promoting Service Mesh technology, decided to integrate community resources and co-author an open source e-book for readers.\nAbout this book This book originates from the rootsongjc / istio-handbook and the Istio knowledge map created by the ServiceMesher community .\nThis book is based on Istio 1.0+ and includes, but is not limited to , topics in the Istio Knowledge Graph .\nParticipate in the book Please refer to the writing guidelines of this book and join the Slack channel discussion after joining the ServiceMesher community .\n","relpermalink":"/en/notice/istio-handbook-by-servicemesher/","summary":"To further popularize Istio and Service Mesh technology, donate this book to the community for co-authoring.","title":"Donate Istio Handbook to the ServiceMesher community"},{"content":"Github: https://github.com/rootsongjc/cloud-native-sandbox Cloud Native Sandbox can help you setup a standalone Kubernetes and istio environment with Docker on you own laptop.\nThe sandbox integrated with the following components:\nKubernetes v1.10.3 Istio v1.0.4 Kubernetes dashboard v1.8.3 Differences with kubernetes-vagrant-centos-cluster As I have created the kubernetes-vagrant-centos-cluster to set up a Kubernetes cluster and istio service mesh with vagrantfile which consists of 1 master(also as node) and 3 nodes, but there is a big problem that it is so high weight and consume resources. So I made this light weight sandbox.\nFeatures\nNo VirtualBox or Vagrantfile required Light weight High speed, low drag Easy to operate Services\nAs the sandbox setup, you will get the following services.\nRecord with termtosvg .\nPrerequisite You only need a laptop with Docker Desktop installed and Kubernetes enabled .\nNote: Leave enough resources for Docker Desktop. At least 2 CPU, 4G memory.\nInstall To start the sandbox, you have to run the following steps.\nKubernetes dashboard(Optional) Install Kubernetes dashboard.\nkubectl apply -f install/dashbaord/ Get the dashboard token.\nkubectl -n kube-system describe secret default| awk \u0026#39;$1==\u0026#34;token:\u0026#34;{print $2}\u0026#39; Expose kubernetes-dashboard service.\nkubectl -n kube-system get pod -l k8s-app=kubernetes-dashboard -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39; Login to Kubernetes dashboard on http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login with the above token.\nIstio(Required) Install istio service mesh with the default add-ons.\n# Install istio kubectl apply -f install/istio/ To expose service grafana on http://localhost:3000 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 3000:3000 \u0026amp; To expose service prometheus on http://localhost:9090 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=prometheus -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 9090:9090 \u0026amp; To expose service jaeger on http://localhost:16686 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=jaeger -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 16686:16686 \u0026amp; To expose service servicegraph on http://localhost:8088/dotviz , http://localhost:8088/force/forcegraph.html .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=servicegraph -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 8088:8088 \u0026amp; Kiali Install kiali .\nkubectl -n istio-system apply -f install/kiali To expose service kiali on http://localhost:20001 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=kiali -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 20001:20001 \u0026amp; Bookinfo sample Deploy bookinfo sample .\n# Enable sidecar auto injection kubectl label namespace default istio-injection=enabled # Deploy bookinfo sample kubectl -n default apply -f sample/bookinfo Visit productpage on http://localhost/productpage .\nLet’s generate some loads.\nfor ((i=0;i\u0026lt;1000;i=i+1));do echo \u0026#34;Step-\u0026gt;$i\u0026#34;;curl http://localhost/productpage;done You can watch the service status through http://localhost:3000 .\nClient tools To operate the applications on Kubernetes, you should install the following tools.\nRequired\nkubectl - Deploy and manage applications on Kubernetes. istioctl - Istio configuration command line utility. Optional\nkubectx - Switch faster between clusters and namespaces in kubectl kube-ps1 - Kubernetes prompt info for bash and zsh ","relpermalink":"/en/blog/cloud-native-sandbox/","summary":"A standalone Kubernetes and Istio environment with Docker on you own laptop","title":"Cloud Native Sandbox"},{"content":"This video was recorded on the last day of 2018 and demonstrates the use of rootsongjc/kubernetes-vagrant-centos-cluster to automatically deploy a Kubernetes cluster and Istio Service Mesh.\nA few days ago I mentioned that kubernetes-vagrant-centos-cluster released v1.2.0 version to deploy a cloud-native experimental environment with one click. Someone in the Kubernetes and Service Mesh community asked me a long time ago to make a video to explain and demonstrate how to install Kubernetes and Istio Service Mesh, because I’m always busy, I’ve always made some time. Today, I will give a demo video, just don’t watch the video for a few minutes. In order to make this video, it took me half an hour to record, two hours to edit, and many years of shooting. , Editing, containers, virtual machines, Kubernetes, service grid experience. This is not so much a farewell as a new beginning.\nBecause the video was first posted on YouTube , it was explained in English (just a few supplementary instructions, it does n’t matter if you do n’t understand, just read the Chinese documentation on GitHub ).\nSkip to bilibli to watch . If you are interested in drone aerial photography, you can also take a look at Jimmy Song’s aerial photography works . Please support the coin or like, thank you.\nIf you have any questions, you can send a barrage or comment below the video.\nPS. Some people will ask why you chose to use bilibli, because there are no ads for watching videos on this platform, and most of them are uploaded by the Up master. Although the two-dimensional elements are mostly, the community atmosphere is still good.\nFor more exciting videos, visit Jimmy Song’s bibli homepage .\n","relpermalink":"/en/notice/cloud-native-kubernetes-service-mesh-local-demo-show/","summary":"This video was recorded by me on the last day of 2018 and demonstrates the use of rootsongjc/kubernetes-vagrant-centos-cluster to automatically deploy Kubernetes clusters and Istio Service Mesh.","title":"Kubernetes and Istio Service Mesh Cloud Native Local Video Demo Show"},{"content":"Updated at Mar 8, 2022\nThis article uses Istio’s official bookinfo sample to explain how Envoy performs routing forwarding after the traffic entering the Pod and forwarded to Envoy sidecar by iptables, detailing the inbound and outbound processing. For a detailed analysis of traffic interception, see Understanding Envoy Sidecar Proxy Injection and Traffic Interception in Istio Service Mesh .\nOverview of Sidecar Injection and Traffic Interception Steps Below is an overview of the steps from Sidecar injection, Pod startup to Sidecar proxy interception traffic and Envoy processing routing.\nKubernetes automatically injected through Admission Controller, or the user run istioctl command to manually inject sidecar container. Apply the YAML configuration deployment application. At this time, the service creation configuration file received by the Kubernetes API server already includes the Init container and the sidecar proxy. Before the sidecar proxy container and application container are started, the Init container started firstly. The Init container is used to set iptables (the default traffic interception method in Istio, and can also use BPF, IPVS, etc.) to Intercept traffic entering the pod to Envoy sidecar Proxy. All TCP traffic (Envoy currently only supports TCP traffic) will be Intercepted by sidecar, and traffic from other protocols will be requested as originally. Launch the Envoy sidecar proxy and application container in the Pod. Sidecar proxy and application container startup order issues\nStart the sidecar proxy and the application container. Which container is started first? Normally, Envoy Sidecar and the application container are all started up before receiving traffic requests. But we can’t predict which container will start first, so does the container startup order have an impact on Envoy intercepting traffic? The answer is yes, but it is divided into the following two situations.\nCase 1: The application container starts first, and the sidecar proxy is still not ready\nIn this case, the traffic is transferred to the 15001 port by iptables, and the port is not monitored in the Pod. The TCP link cannot be established and the request fails.\nCase 2: Sidecar starts first, the request arrives and the application is still not ready\nIn this case, the request will certainly fail. As for the step at which the failure begins, the reader is left to think.\nQuestion : If adding a readiness and living probe for the sidecar proxy and application container can solve the problem?\nTCP requests that are sent or received from the Pod will be intercepted by iptables. After the inbound traffic is intercepted, it is processed by the Inbound Handler and then forwarded to the application container for processing. The outbound traffic is intercepted by iptables and then forwarded to the Outbound Handler for processing. Upstream and Endpoint. Sidecar proxy requests Pilot to use the xDS protocol to synchronize Envoy configurations, including LDS, EDS, CDS, etc., but to ensure the order of updates, Envoy will use ADS to request configuration updates from Pilot directly. How Envoy handles route forwarding The following figure shows a productpageservice access request http://reviews.default.svc.cluster.local:9080/, when traffic enters reviews the internal services, reviews internal services Envoy Sidecar is how to do traffic blocked the route forward.\nIstio transparent traffic intercepting and traffic routing schematic Before the first step, productpage Envoy Sidecar Pod has been selected by EDS of a request to reviews a Pod service of its IP address, it sends a TCP connection request.\nThe Envoy configuration in the official website of Istio is to describe the process of Envoy doing traffic forwarding. The party considering the traffic of the downstream is to receive the request sent by the downstream. You need to request additional services, such as reviews service requests need Pod ratings service.\nreviews, there are three versions of the service, there is one instance of each version, three versions sidecar similar working steps, only to later reviews-v1-cb8655c75-b97zc Sidecar flow Pod forwarding this step will be described.\nUnderstanding the Inbound Handler The role of the inbound handler is to transfer the traffic from the downstream intercepted by iptables to localhost to establish a connection with the application container inside the Pod.\nLook reviews-v1-cb8655c75-b97zc at the Listener in the pod.\nRun istioctl pc listener reviews-v1-cb8655c75-b97zc to see what the Pod has a Listener.\nADDRESS PORT TYPE 172.33.3.3 9080 HTTP \u0026lt;--- Receives all inbound traffic on 9080 from listener 0.0.0.0_15006 10.254.0.1 443 TCP \u0026lt;--+ 10.254.4.253 80 TCP | 10.254.4.253 8080 TCP | 10.254.109.182 443 TCP | 10.254.22.50 15011 TCP | 10.254.22.50 853 TCP | 10.254.79.114 443 TCP | 10.254.143.179 15011 TCP | 10.254.0.2 53 TCP | Receives outbound non-HTTP traffic for relevant IP:PORT pair from listener 0.0.0.0_15001 10.254.22.50 443 TCP | …","relpermalink":"/en/blog/understanding-how-envoy-sidecar-intercept-and-route-traffic-in-istio-service-mesh/","summary":"Details about Envoy sidecar with iptables rules.","title":"Understanding How Envoy Sidecar Intercept and Route Traffic in Istio Service Mesh"},{"content":"KubeCon \u0026amp; CloudNative in North America is the most worthy cloud-native event every year. This time it will be held in Seattle for four days, from December 10th to 13th, refer to the official website of the conference . This year, 8,000 people participated. You should notice that Kubernetes has become more and more low-level. Cloud-native application developers do not need to pay much attention to it. Major companies are publishing their own cloud-native technology stack layouts, including IBM, VMware, SAP, Startups around this ecosystem are still emerging. The PPT sharing address of the local conference: https://github.com/warmchang/KubeCon-North-America-2018 , thank you William Zhang for finishing and sharing the slides of this conference .\nSeattle scene Janet Kuo from Google describes the path to cloud-native technology adoption.\nThe same event of KubeCon \u0026amp; CloudNativeCon, the scene of the first EnvoyCon.\nAt KubeCon \u0026amp; CloudNativeCon Seattle, various directors, directors, directors, VPs, and Gartner analysts from IBM, Google, Mastercard, VMware, and Gartner are conducting live discussions on the topic of Scaling with Service Mesh and Istio. Why do we talk about Istio when we talk about Service Mesh? What is not suitable for Istio use case. . .\nThe PPT contains basic introduction, getting started, a total of more than 200 Deep Dive as well as practical application, we recommend you according to the General Assembly’s official website to choose topics of interest to look at the schedule, otherwise I might see, however.\nA little impression KubeCon \u0026amp; CloudNativeCon is held three times a year, Europe, China and North America. China is the first time this year, and it will be held in Shanghai in November. It is said that it will be held in June next year. Although everyone said that Kubernetes has become boring, the conference about Kubernetes There is still a lot of content, and the use of CRD to extend Kubernetes usage is increasing. Service Mesh has begun to become hot. As can be seen from the live pictures above, there are a large number of participants on the site and related topics are also increasing. It is known as a microservice in the post-Kubernetes era . This must be It will be an important development direction of cloud native after Kubernetes, and the ServiceMesher community pays close attention to it.\n","relpermalink":"/en/notice/kubecon-cloudnativecon-seattle-2018/","summary":"KubeCon \u0026 CloudNativeCon Seattle 2018 Data Sharing.","title":"Kubecon\u0026CloudNativeCon Seattle 2018"},{"content":"This is a postscript from the post- Kubernetes era. Just this evening I saw a post by Bilgin Ibryam Microservices in a Post-Kuberentes Era .\nOn April 9, 2017, the Kubernetes Handbook-Kubernetes Chinese Guide / Cloud Native Application Architecture Practice Manual was first submitted. In the past 16 months, 53 contributors participated, 1,088 commits, and a total of 23,9014 Chinese characters were written. At the same time , thousands of enthusiasts have gathered in the Kubernetes \u0026amp; Cloud Native combat group .\nIt has been more than 4 months since the previous version was released. During this period, Kubernetes and Prometheus graduated from CNCF respectively and have matured commercially. These two projects have basically taken shape and will not change much in the future. Kubernetes was originally developed for container orchestration. In order to solve the problem of microservice deployment, Kubernetes has gained popularity. The current microservices have gradually entered the post-Kubernetes era . Service Mesh and cloud native redefine microservices and distributed applications.\nWhen this version was released, the PDF size was 108M with a total of 239,014 Chinese characters. It is recommended to browse online , or clone the project and install the Gitbook command to compile it yourself.\nThis version has the following improvements:\nAdded Istio Service Mesh tutorial Increased use VirtualBox and Vagrant set up in a local cluster and distributed Kubernetes Istio Service Mesh Added cloud native programming language Ballerina and Pulumi introduced Added Quick Start Guide Added support for Kubernetes 1.11 Added enterprise-level service mesh adoption path guide Added SOFAMesh chapter Added vision for the cloud-native future Added CNCF charter and participation Added notes for Docker image repositories Added Envoy chapter Increased KCSP (Kubernetes certification service providers) and CKA (Certified Kubernetes administrator) instructions Updated some configuration files, YAML and reference links Updated CRI chapter Removed obsolete description Improved etcdctl command usage tutorial Fixed some typos Browse and download Browse online https://jimmysong.io/kubernetes-handbook To make it easy for everyone to download, I put a copy on Weiyun , which is available in PDF (108MB), MOBI (42MB), and EPUB (53MB). In this book, there are more practical tutorials. In order to better understand the principles of Kubernetes, I recommend studying ** In- depth analysis of Kubernetes by Zhang Lei, produced by Geek Time **.\nThank you Kubernetes for your support of this book. Thank you Contributors . In the months before this version was released, the ServiceMesher community was co-founded . As a force in the post-Kubernetes era , welcome to contact me to join the community and create cloud native New era .\nAt present , the WeChat group of the ServiceMesher community also has thousands of members. The Kubernete Handbook will continue, but Service Mesh is already a rising star. With Kubernetes in mind, welcome to join the ServiceMesher community and follow us. The public account of the community (also the one I manage).\n","relpermalink":"/en/notice/new-kubernetes-handbook-released-and-say-hello-to-post-kubernetes-era/","summary":"This is an obituary post-Kubernetes era. Kubernetes handbook by Jimmy Song v1.4 is released. The next focus of cloud native is Service Mesh!","title":"Kubernetes Handbook v1.4 is released"},{"content":"Today I am honored to announce that I have become a CNCF Ambassador . Here is my story with Cloud Native.\nOrigin The first time to attend the Cloud Native Computing Foundation is at the LC3 in Beijing 2017. I attended the meeting again this year, and in November of this year, CNCF will hold the KubeCon \u0026amp; CloudNativeCon for the first time in Shanghai, China. I’ll be there too.\nCloud Native Books My origins with the Cloud Native is originated from Kevin Hoffman’s book Cloud Native Go . I translated this book at the end of 2016. Since then, in China, the translation of the word Cloud Native has not been determined, we introduced it with 云原生 to China.\nAnd then I begin to write the kubernetes-handbook on GitHub. So far, it has more than 2000 stars. This book has written more than 200,000 Chinese characters, the first commit happened on April 14, 2017.\nSince the the book Cloud Native Go completed, the publisher recommended another Cloud Native book to me - Cloud Native Python by Manish Sethi.\nAnd the book Cloud Native Java by Josh Long and Kenny Bastani.\nIn March 2018, with the hope that Bring the world equal opportunities and Building a Financial Cloud Native Infrastructure, I joined the Ant Group .\nServiceMesher Community By the time of May 2018, I start to organize the ServiceMesher community.\nIn the last few months, we work with other open source communities in China, such as k8smeetup , Sharding-Sphere , Apache SkyWalking . Our community has grown to have 1,700 members and two round meetups in Hangzhou and Beijing till now.\nMore than 300 people participated in the scene and more than 20,000 people watched it live by IT 大咖说 。\nFuture Here are some hopes of mine:\nOpen source culture become popular in China More and more people would like to be involved in open source projects Host one open source project into the CNCF A book related to Cloud Native or Service Mesh Strengthen cultural exchanges between China and the global Finally, welcome to China for traveling or share your topic with us on Cloud Native, and in the mean while we will share our experience on large scale web apps to the world. Hope to hear your voice!\n","relpermalink":"/en/blog/cloud-native-and-me-the-past-current-and-future/","summary":"Today I am honored to announce that I have become a CNCF Ambassador.","title":"Cloud Native With Me - The Past, Current and Future"},{"content":"Today, we are pleased to announce Istio 1.0 . It’s been over a year since the original 0.1 release. Since 0.1, Istio has grown rapidly with the help of a thriving community, contributors, and users. Many companies have successfully applied Istio to production today and have gained real value through the insight and control provided by Istio. We help large businesses and fast-growing startups such as eBay , Auto Trader UK , Descartes Labs , HP FitStation , Namely , PubNub and Trulia to connect, manage and protect their services from scratch with Istio. The release of this version as 1.0 recognizes that we have built a core set of features that users can rely on for their production.\nEcosystem Last year we saw a significant increase in the Istio ecosystem. Envoy continues its impressive growth and adds many features that are critical to a production-level service grid. Observability providers like Datadog , SolarWinds , Sysdig , Google Stackdriver, and Amazon CloudWatch have also written plugins to integrate Istio with their products. Tigera , Aporeto , Cilium and Styra have built extensions for our strategy implementation and network capabilities. Kiali built by Red Hat provides a good user experience for grid management and observability. Cloud Foundry is building the next-generation traffic routing stack for Istio, the recently announced Knative serverless project is doing the same, and Apigee has announced plans to use it in their API management solution. These are just a few of the projects that the community added last year.\nFeatures Since the 0.8 release, we have added some important new features, and more importantly, marked many existing features as Beta to indicate that they can be used in production. This is covered in more detail in the release notes , but it is worth mentioning:\nMultiple Kubernetes clusters can now be added to a single grid , enabling cross-cluster communication and consistent policy enforcement. Multi-cluster support is now Beta. The network API for fine-grained control of traffic through the grid is now Beta. Explicitly modeling ingress and egress issues with gateways allows operations personnel to control the network topology and meet access security requirements at the edge. Two-way TLS can now be launched incrementally without updating all clients of the service. This is a key feature that removes the barriers to deploying Istio on existing production. Mixer now supports developing out-of-process adapters . This will be the default way to extend Mixer in an upcoming release, which will make it easier to build adapters. Envoy now fully evaluates the authorization policies that control service access locally , improving their performance and reliability. Helm chart installation is now the recommended installation method, with a wealth of customization options to configure Istio to your needs. We put a lot of effort into performance, including continuous regression testing, large-scale environmental simulation, and target repair. We are very happy with the results and will share details in the coming weeks. Next step Although this is an important milestone for the project, much work remains to be done. When working with adopters, we’ve received a lot of important feedback about what to focus next. We’ve heard consistent topics about supporting hybrid clouds, installing modularity, richer network capabilities, and scalability for large-scale deployments. We have considered some feedback in the 1.0 release and we will continue to actively work on it in the coming months.\nQuick start If you are new to Istio and want to use it for deployment, we would love to hear from you. Check out our documentation , visit our chat forum or visit the mailing list . If you want to contribute more to the project, please join our community meeting and say hello.\nAt last The Istio team is grateful to everyone who contributed to the project. Without your help, it won’t have what it is today. Last year’s achievements were amazing, and we look forward to achieving even greater achievements with our community members in the future.\nThe ServiceMesher community is responsible for the translation and maintenance of Chinese content on Istio’s official website. At present, the Chinese content is not yet synchronized with the English content. You need to manually enter the URL to switch to Chinese ( https://istio.io/zh ). There is still a lot of work to do , Welcome everyone to join and participate.\n","relpermalink":"/en/notice/istio-v1-released/","summary":"Chinese documentation is released at the same time!","title":"Istio 1.0 is released"},{"content":" If there is a visual learning model and platform that provides infrastructure clusters for your operation, would you pay for it?\nTwo months ago, I met Jord in Kubernetes’s Slack channel, and later saw the link of MagicSandbox.io he (and possibly others) sent in the Facebook group of Taiwan Kubernetes User Group, and I clicked to apply for a trial Then, I received an email from Jord later, and he told me that he wanted to build a Kubernetes learning platform. That’s where the whole thing started, and then we had a couple of Zoom video chats for a long time.\nAbout MagicSandbox MagicSandbox is a startup company. Jord (Dutch) is also a serial entrepreneur. He has studied at Sichuan University in China for 4 years since he was 19, and then returned to Germany. He worked as a PM at Boston Consulting Group and now works at Entrepreneur. First (Europe’s top venture capital / enterprise incubator) is also located in Berlin, Germany. He met Mislav (Croatian). Mislav is a full-stack engineer and has several entrepreneurial experiences. They have similar odors, and they hit it off. Decided to be committed to the Internet education industry and create a world-class software engineer education platform. They want to start with Kubernetes, first provide Internet-based Kubernetes theory and practice teaching, and then expand the topic to ElasticSearch, GraphQL, and so on. topic.\nJord founded MagicSandbox in his home, and I became the face of MagicSandbox in China.\nNow we are going to release the MagicSandbox Alpha version. This version is an immature version and is provided for everyone to try for free. Positive feedback is also welcome.\nOfficial homepage: https://magicsandbox.com/ Chinese page: https://cn.magicsandbox.com/ (The content has not been finished yet, only the Chinese version homepage is currently provided) Follow us on Twitter: https://twitter.com/magicsandbox ","relpermalink":"/en/notice/magicsandbox-alpha-version-annoucement/","summary":"Online practical software engineering education platform.","title":"MagicSandbox Alpha released"},{"content":"Remember the cloud-native programming language I shared before finally appeared! Learn about Ballerina in one article! ? They are ready to attend the KubeCon \u0026amp; CloudNativeCon China Conference!\nKubeCon \u0026amp; CloudNativeCon China Conference will be held on November 14-15, 2018 (Wednesday, Thursday) in Shanghai . See: https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2018/ With Ballerina’s official authorization, I now need to help them find an “ambassador” in China, responsible for team guidance, Chinese and English translation, and familiarity with Cloud Native and microservices. It has influence in the industry and has no barriers to English communication.\nAmbassador duties\nTeam Leadership Responsible for Chinese and English translation of product declaration, PPT materials, etc. Help to arrange the booth The other party can provide\nConference tickets Travel expenses Accommodation during the conference Other compensation This is a photo of their team in front of their booth during the KubeCon \u0026amp; CloudNativeCon in Hagen in May this year.\nPS This is the most complete and best picture I have ever found of their team. (Photography needs to be strengthened)\nLet’s briefly introduce this startup called Ballerina. Their team is mainly from Sri Lanka. This is an island country next to India in the South Asian subcontinent. In ancient China, it was called “Lion Country” and rich in gemstones.\nThe capital of their country is Sri Lanka, which is pronounced in their own language: Si li jia ya wa de na pu la ke te\nIf you are interested, please contact me directly.\n","relpermalink":"/en/notice/a-ballerina-china-ambassador-required/","summary":"With official authorization from Ballerina, I now need to help them find an ambassador in China.","title":"Ballerina seeks Chinese ambassador"},{"content":"Envoy-Designed for cloud-native applications, open source edge and service proxy, Istio Service Mesh default data plane, Chinese version of the latest official document, dedicated by the ServiceMesher community, welcome everyone to learn and share together.\nTL;DR: http://www.servicemesher.com/envoy/ PDF download address: servicemesher/envoy This is the first time Community Service Mesh enthusiasts group activities, the document is based on Envoy latest (version 1.7) Official documents https://www.envoyproxy.io/docs/envoy/latest/ . A total of 120 articles with 26 participants took 13 days and 65,148 Chinese characters.\nVisit online address: http://www.servicemesher.com/envoy/ Note : This book does not include the v1 API reference and v2 API reference sections in the official documentation. Any links to API references in the book will jump directly to the official page.\nContributor See the contributor page: https://github.com/servicemesher/envoy/graphs/contributors Thanks to the above contributors for their efforts! Because the level of translators is limited, there are inevitably inadequacies in the text. I also ask readers to correct them. Also welcome more friends to join our GitHub organization: https://github.com/servicemesher ","relpermalink":"/en/notice/envoyproxy-docs-cn-17-release/","summary":"Translated by ServiceMesher community.","title":"Chinese version of the latest official document of Envoy released"},{"content":"Envoy is an open source L7 proxy and communication bus written in C ++ by Lyft. It is currently an open source project under CNCF . The code is hosted on GitHub. It is also the default data plane in the Istio service mesh. We found that it has very good performance, and there are also continuous open source projects based on Envoy, such as Ambassador , Gloo, etc. At present, the official documentation of Envoy has not been well finished, so we Service Service enthusiasts feel that they are launching the community The power of the co-translation of the latest (version 1.7) official documentation of Enovy and organization through GitHub.\nService Mesh enthusiasts have jointly translated the latest version of the official document of Envoy . The translated code is hosted at https://github.com/servicemesher/envoy . If you are also a Service Mesh enthusiast, you can join the SerivceMesher GitHub organization and participate together.\nThe official Envoy document excludes all articles in the two directories of the v1 API reference and the v2 API reference. There are more than 120 documents. The length of the documents varies. The original English official documents use the RST format. I manually converted them into Markdown format and compiled using Gitbook. A GitHub Issue was generated according to the path of the document relative to the home directory. Friends who want to participate in translation can contact me to join the ServiceMesher organization, and then select the article you want to translate in Issue , and then reply “Claim”.\nHere you can see all the contributors. In the future, we will also create a Service Mesh enthusiast website. The website uses static pages. All code will be hosted on Github. Welcome everyone to participate.\n","relpermalink":"/en/notice/enovy-doc-translation-start/","summary":"The SerivceMesher community is involved in translating the official documentation for the latest version of Envoy.","title":"Envoy's latest official document translation work started"},{"content":"TL; DR Click here to download the PDF of this book .\nRecently, Michael Hausenblas of Nginx released a booklet on container networks in docker and kubernetes. This 72-page material is a good introduction for everyone to understand the network in Docker and Kubernetes from shallow to deep.\nTarget audience Container Software Developer SRE Network Operation and Maintenance Engineer Architects who want to containerize traditional software ","relpermalink":"/en/notice/container-networking-from-docker-to-kubernetes-nginx/","summary":"Source from Nginx, published by O’Reilly.","title":"Docker container network book sharing"},{"content":"In 2017, we are facing a big era of architectural changes, such as Kubernetes ending the battle for container orchestration, Kafka release 1.0, serverless gradually gaining momentum, edge computing to replace cloud computing, Service Mesh ready to go, and artificial intelligence to empower business , Also brings new challenges to the architecture.\nI am about to participate in InfoQ’s ArchSummit Global Architects Summit on December 8-11 in Beijing . This conference also invited 100+ top technologists such as Dr. Ali Wangjian to share and summarize the architectural changes and reflections this year. I hope that you can build on this conference, summarize past practices, and look forward to a future-oriented architecture to transform this era of change into the common fortune of each of us.\nMy speech The content of my speech is from Kubernetes to Cloud Native-the road to cloud native applications . Link: From Kubernetes to Cloud Native-the road to cloud native applications . The time is Saturday, December 9, 9:30 am, in the fifth meeting room.\nAfter more than ten years of development of cloud computing, the new phase of cloud native has entered. Enterprise applications are preferentially deployed in cloud environments. How to adapt to the cloud native tide, use containers and Kubernetes to build cloud native platforms, and practice DevOps concepts and agility How IT, open source software, and the community can help IT transform, the solution to all these problems is the PaaS platform, which is self-evident to the enterprise.\nWe also prepared gifts for everyone: “Cloud Native Go-Building Cloud Native Web Applications Based on Go and React” and “Intelligent Data Era-Enterprise Big Data Strategy and Practice” There is also a book stand for the blog post of the Electronic Industry Press. Welcome to visit.\nArchSummit conference official website link: http://bj2017.archsummit.com/ For more details, please refer to the official website of the conference: http://bj2017.archsummit.com/ ","relpermalink":"/en/notice/archsummit-beijing-2017-from-kubernetes-to-cloud-native/","summary":"I will give a lecture at ArchSummit Beijing. From Kubernetes to Cloud Native, my path to cloud native applications.","title":"ArchSummit Beijing 2017 speech preview"},{"content":"Cloudinary-go is a Go client library and CLI tool to upload static assets to the Cloudinary service.\nInstallation Install the CLI tool and the library with:\ngo get github.com/rootsongjc/cloudinary-go/cloudinary Or download the release binary from release .\n","relpermalink":"/en/notice/cloudinary-go/","summary":"Cloudinary-go is a Go client library and CLI tool to upload static assets to Cloudinary service","title":"Cloudinary file upload tool written in Go released"},{"content":"Many people asked me how jimmysong.io made this website. I think it is necessary to write a book to popularize the knowledge of static website construction and Hugo as a tool.\nThis manual will guide you how to use Hugo to build a static website for personal blog or project display.\nTeach you how to build a static website from scratch. This does not require much programming and development experience and time investment, and basically does not require much cost (except for personalized domain names). You can quickly build and Launch a website.\nGithub address: https://github.com/rootsongjc/hugo-handbook Gitbook access address: https://jimmysong.io/hugo-handbook The content of this book will continue to improve over time and as my site improves, so stay tuned.\n","relpermalink":"/en/notice/building-static-website-with-hugo/","summary":"A manual for static website building, and a personal blog Gitbook using Hugo.","title":"Hugo Handbook is released"},{"content":"Kevin Hoffman(From Capital One, twitter @KevinHoffman ) was making a speech on TalkingData T11 Smart Data Summit.\nHe addressed that 15 Factors of Cloud Native which based on Heroku’s original Twelve-Factor App , but he add more 3 another factors on it.\nLet’s have a look at the 15 factors of Cloud Native.\n1. One codebase, one App Single version-controlled codebase, many deploys Multiple apps should not share code Microservices need separate release schedules Upgrade, deploy one without impacting others Tie build and deploy pipelines to single codebase 2. API first Service ecosystem requires a contract Public API Multiple teams on different schedulers Code to contract/API, not code dependencies Use well-documented contract standards Protobuf IDL, Swagger, Apiary, etc API First != REST first RPC can be more appropriate in some situations 3. Dependency Management Explicitly declare dependencies Include all dependencies with app release Create immutable build artifact (e.g. docker image) Rely on smallest docker image Base on scratch if possible App cannot rely on host for system tools or libraries 4. Design, Build, Release, Run Design part of iterative cycle Agile doesn’t mean random or undesigned Mature CI/CD pipeline and teams Design to production in days not months Build immutable artifacts Release automatically deploys to environment Environments contains config, not release artifact 5. Configuration, Credentials, Code “3 Cs” volatile substances that explode when combinded Password in a config file is as bad as password in code App must accept “3 Cs” from environment and only use harmless defaults Test - Could you expose code on Github and not reveal passwords, URLs, credentials? 6. Logs Emit formatted logs to stdout Code should not know about destination or purpose of log emissions Use downstream log aggregator collect, store, process, expose logs ELK, Splunk, Sumo, etc Use structured logs to allow query and analysis JSON, csv, KV, etc Logs are not metrics 7. Disposability App must start as quickly as possible App must stop quickly and gracefully Processes start and stop all the time in the cloud Every scale up/down disposes of processes Slow dispose == slow scale Slow dispose or startup can cause availability gaps 8. Backing Services Assume all resources supplied by backingservices Cannotassume mutable file system “Disk as a Service” (e.g. S3, virtual mounts, etc) Every backing service is bound resource URL, credentials, etc-\u0026gt; environment config Host does not satisfy NFRs Backing services and cloud infrastructure 9. Environment Parity “Works on my machine” Cloud-native anti-pattern. Must work everywhere Every commit is candidate for deployment Automated acceptance tests Provide no confidence if environments don’t match 10. Administrative Processes Database migrations Run-once scripts or jobs Avoid using for batch operations, consider instead: Event sourcing Schedulers Triggers from queues, etc Lambdas/functions 11. Port Binding In cloud, infrastructure determines port App must accept port assigned by platform Containers have internal/external ports App design must embrace this Never use reserved ports Beware of container “host mode” networking 12. Stateless Processes What is stateless? Long-term state handled by a backing service In-memory state lives onlyas long as request Requests from same client routed to different instances “Sticky sessions” cloud native anti-pattern 13. Concurency Scale horizontally using the process model Build disposable, stateless, share-nothing processes Avoid adding CPU/RAM to increase scale/throughput Where possible, let platform/libraries do threading Many single-threaded services \u0026gt; 1 multi-threaded monolith 14. Telemetry Monitor apps in the cloud like satellite in orbit No tether, no live debugger Application Perf Monitoring (APM) Domain Telemetry Health and system logs 15. Authentication \u0026amp; Authorization Security should never be an afterthought Auth should be explicit, documented decision Even if anonymous access is allowed Don’t allow anonymous access Bearer tokens/OAuth/OIDC best practices Audit all attempts to access Migrating Monoliths to the Cloud After this 15 factors, he also gave us some tips about how to migrate monoliths to the Cloud:\nMake a rule - stop adding to the monolith All new code must be cloud native Prioritize features Where will you get most benefit from cloud native? Come up with a plan Decompose monolith over time Fast, agile iterations toward ultimate goal Use multiple strategies and patterns Go - the Best Language for Building Cloud Native App At last, he advise us the programming language Go is the best language to build Cloud Native applications for these reasons below:\nLightweight Easily learning curve Compiles to native binaries Very fast Large, thriving, engaged community http://gopherize.me Kevin also wrote a book Cloud Native Go to show how to Building Web Applications and Microservices for the Cloud with Go and React. This book has been …","relpermalink":"/en/blog/high-level-cloud-native-from-kevin-hoffman/","summary":"Kevin Hoffman address that 15 Factors of Cloud Native.","title":"High Level Cloud Native From Kevin Hoffman"},{"content":"From now on I have my own independent domain name jimmysong.io , the website is still hosted on GitHub, the original URL https://jimmysong.io is still accessible.\nWhy use .io as the suffix? Because this is The First Step to Cloud Native!\nWhy choose today? Because today is August 18, the days are easy to remember.\nPS domain names are registered in namecheap and cost tens of dollars / year.\nProudly powered by hugo 🎉🎊🎉\n","relpermalink":"/en/notice/domain-name-jimmysong-io/","summary":"From now on I have my own independent domain name jimmysong.io.","title":"New domain name jimmysong.io"},{"content":"This is a list of software, tools, architecture and reference materials about Cloud Native. It is awesome-cloud-native , a project I opened on GitHub , and can also be browsed through the web page .\nIt is divided into the following areas:\nAwesome Cloud Native\nAI API gateway Big Data Container engine CI-CD Database Data Science Fault tolerant Logging Message broker Monitoring Networking Orchestration and scheduler Portability Proxy and load balancer RPC Security and audit Service broker Service mesh Service registry and discovery Serverless Storage Tracing Tools Tutorial This list will be continuously updated and improved in the future, not only for my usual research records, but also as a reference for the Cloud Native industry.\n","relpermalink":"/en/notice/awesome-cloud-native/","summary":"This is a list of software, tools, architecture, and reference materials about Cloud Native. It is a project I started on GitHub.","title":"Awesome Cloud Native list is released"},{"content":"This book is the Chinese version of Migrating to Cloud Native Application Architectures. The English version of this book was released in February 2015. The Chinese version was translated by Jimmy Song and published in July 2017.\nGitHub hosting address for this book: https://github.com/rootsongjc/migrating-to-cloud-native-application-architectures Gitbook reading address: https://jimmysong.io/migrating-to-cloud-native-application-architectures The application architectures discussed in this book include:\nTwelve-factor application: A collection of cloud-native application architecture patterns Microservices: independently deployed services, each service does one thing Self-service agile infrastructure: a platform that provides application environments and back-office services quickly, repeatably, and consistently API-based collaboration: published and versioned APIs that allow interactions between services in a cloud-native application architecture Pressure resistance: a system that becomes stronger under pressure ","relpermalink":"/en/notice/changes-needed-to-cloud-native-archtecture/","summary":"This book is translated from an eBook published by Matt Stine in February 2015.","title":"Migrating to Cloud Native Chinese version released"},{"content":"In this article, I will explain how to use GraphQL to query data from SkyWalking with Postman . It includes steps to obtain the bearer token, construct a query to retrieve load metrics for a specific service, and use GraphQL introspection to see the schema of SkyWalking GraphQL APIs. The article also provides references for further information.\nWhat Is GraphQL? GraphQL is a query language and runtime for APIs developed by Facebook. It provides a more efficient, powerful, and flexible alternative to traditional REST APIs by allowing clients to specify exactly what data they need and receive only that data in response. With GraphQL, clients can query multiple resources in a single request, reducing the number of roundtrips to the server and improving performance.\nWhat’s the Difference between GraphQL and REST APIs? GraphQL allows clients to request only the data they need, while REST APIs require clients to retrieve everything in a resource regardless of whether they need it or not. Additionally, GraphQL allows clients to query multiple resources in a single request, making it more efficient and less chatty than REST APIs.\nHow Do I Query Data from SkyWalking? SkyWalking defines the communication protocol for the query stage. The SkyWalking native UI and CLI use this protocol to consistently fetch data from the backend, without needing to worry about backend updates.\nThere are two methods for querying metrics from SkyWalking:\nGraphQL APIs PromQL APIs This article provides a guide on how to use GraphQL to query metrics from SkyWalking. If you are interested in the PromQL APIs, you can refer to the article Build Grafana dashboards for Apache SkyWalking — Native PromQL Support .Continuing with the following steps requires a TSB installation. If you don’t have one and still want to experience using GraphQL to query data in SkyWalking, you can use the free demo environment (username/password: skywalking/skywalking) provided by SkyWalking. Log in to the demo website and get a token for queries. Endpoint address for GraphQL queries is http://demo.skywalking.apache.org/graphql . The steps to construct the query are the same as described below.\nObserve GraphQL Queries in TSB Before we use Postman to construct our own GraphQL query, let’s first observe how TSB obtains data from SkyWalking.\nOpen Chrome DevTools and switch to the Network tab. Visit the Organization – Services tab on the website. Watch the network request list and right-click on the one of the graphql requests, like in the following image:\nFigure 1: Chrome DevTool The curl commands you see will look like this. Execute the command in your terminal, and you will get a list of services managed by TSB from SkyWalking.\ncurl \u0026#39;\u0026lt;https://saturn.tetrate.work/ui/graphql\u0026gt;\u0026#39; \\ -H \u0026#39;Accept-Language: en,zh-CN;q=0.9,zh;q=0.8,en-US;q=0.7,zh-TW;q=0.6\u0026#39; \\ -H \u0026#39;Cache-Control: no-cache\u0026#39; \\ -H \u0026#39;Client-Timestamp: 1686104776136\u0026#39; \\ -H \u0026#39;Connection: keep-alive\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -H \u0026#39;Cookie: ...\u0026#39; \\ -H \u0026#39;Origin: \u0026lt;https://saturn.tetrate.work\u0026gt;\u0026#39; \\ -H \u0026#39;Pragma: no-cache\u0026#39; \\ -H \u0026#39;Referer: \u0026lt;https://saturn.tetrate.work/mp/services\u0026gt;\u0026#39; \\ -H \u0026#39;Request-Id: ...\u0026#39; \\ -H \u0026#39;Sec-Fetch-Dest: empty\u0026#39; \\ -H \u0026#39;Sec-Fetch-Mode: cors\u0026#39; \\ -H \u0026#39;Sec-Fetch-Site: same-origin\u0026#39; \\ -H \u0026#39;User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\u0026#39; \\ -H \u0026#39;X-Bridge-Csrf-Token: IOmJszLAqY3TRIUNhTuGu7vQgnfQY1FtgYFm+l/+Mu4EmVQU5T8EaQ7bngkCv4hQ12ZGids+I21pHMdepE9/qQ==\u0026#39; \\ -H \u0026#39;X-Csrf-Token: xTbxZerD3t8N3PaS7nbjKCfxk1Q9dtvvrx4D+IJohHicb0VfB4iAZaP0zh1eXDWctQyCYZWaKLhAYT3M6Drk3A==\u0026#39; \\ -H \u0026#39;accept: application/json\u0026#39; \\ -H \u0026#39;sec-ch-ua: \u0026#34;Not.A/Brand\u0026#34;;v=\u0026#34;8\u0026#34;, \u0026#34;Chromium\u0026#34;;v=\u0026#34;114\u0026#34;, \u0026#34;Google Chrome\u0026#34;;v=\u0026#34;114\u0026#34;\u0026#39; \\ -H \u0026#39;sec-ch-ua-mobile: ?0\u0026#39; \\ -H \u0026#39;sec-ch-ua-platform: \u0026#34;macOS\u0026#34;\u0026#39; \\ --data-raw $\u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;query ServiceRegistryListMetrics(...)}\u0026#39; \\ --compressed Note: Some fields in the above example are too long and replaced with dots (…).\nNext, I will guide you through constructing a query to retrieve the load metrics for a specific service.\nObtain the Bearer Token Firstly, you need to obtain the bearer of the website. Log in to TSB UI, click on the user button in the upper right corner, and then click “Show token information”. In the pop-up window, you will see the Bearer Token, as shown in the following image.\nFigure 2: Get the bearer token from the TSB UI Note: The validity period of the bearer token is relatively short. When it expires, you need to log in to TSB again to obtain a new token.\nWe have already deployed the bookinfo application in advance and sent some test traffic. To query the load metrics of reviews using GraphQL in the Postman client, follow these steps:\nCreate a new GraphQL request and enter the request URL: $TSB_ADDRESS/graphql Add the Authorization header with the value Bearer $TOKEN Use GraphQL Introspection to see the schema of SkyWalking GraphQL APIs. Find and click the readMetricsValues item. You will see the variables on the right side. Fill in the …","relpermalink":"/en/blog/how-to-use-graphql-to-query-observability-data-from-skywalking-with-postman/","summary":"This article explains how to use GraphQL to query observability data from SkyWalking with Postman. It first introduces GraphQL and SkyWalking, then explains how to set up Postman to send GraphQL queries, and finally provides some example GraphQL queries that can be used to query observability data from SkyWalking.","title":"How to Use GraphQL to Query Observability Data from SkyWalking with Postman"},{"content":"In June, Istio 1.18 was released , marking the second release of Istio in 2023 and the first to offer official support for ambient mode . Tetrate’s Paul Merrison was one of the release managers for this version, and Tetrate’s contributions to this release included various customer-driven usability enhancements and important work in the underlying Envoy Proxy upon which Istio depends. When asked about the experience of working on Istio 1.18, Paul said “working as the lead release manager for Istio 1.18 gave me a fascinating insight into how a group of super talented people from around the world come together, organize themselves and ship software. There was a steep learning curve, but the Istio community is awesome and I was supported brilliantly. The biggest challenge was definitely learning and executing all the steps that are needed to bring a release to life, but the feeling of achievement when it finally made its way out into the world will stay with me for a while!” Istio first announced the introduction of Ambient mode in September last year, which was covered in detail by Zack in this blog post , where he explains the differences between ambient mode and sidecar mode.\nThis release introduces many new features and changes in addition to ambient mode, including enhanced Kubernetes Gateway API support, health checks for virtual machines that are not automatically registered, support for expired metrics, enhanced istioctl analyze, and more. See the release blog for details. The most significant of these are the ambient mode and Gateway API enhancements, detailed below.\n“Working as the lead release manager for Istio 1.18 gave me a fascinating insight into how a group of super talented people from around the world come together, organize themselves and ship software. There was a steep learning curve, but the Istio community is awesome and I was supported brilliantly. The biggest challenge was definitely learning and executing all the steps that are needed to bring a release to life, but the feeling of achievement when it finally made its way out into the world will stay with me for a while!”\nPaul Merrison, Tetrate Engineering and Istio Release Manager\nWhat Is Ambient Mode? Before discussing ambient mode, it is essential to understand the current “sidecar mode” used by Istio. Sidecar mode is the default data plane mode used by Istio, where each application pod comes equipped with a sidecar proxy (usually Envoy) that handles all network traffic in and out of the pod, providing Istio’s core functionality such as Zero Trust security, telemetry and traffic management. While sidecar mode is suitable for most users, ambient mode offers some advantages in specific circumstances. For more information on the differences between ambient mode and the standard sidecar mode, see our article on Ambient Mesh: What You Need to Know about This Experimental New Deployment Model for Istio .\nWhat Are the Design Goals of Ambient Mode? Non-intrusive: Ambient mode does not require injecting sidecar proxies into the application’s pods and only requires the application to be tagged to automatically join the mesh, potentially reducing the mesh’s impact on the application. Efficient resource utilization: Ambient mode can optimize resource utilization for some use cases by sharing the ztunnel proxy across the mesh. If certain L7 functionality of Istio is required, it can also be addressed by deploying Waypoints precisely for a ServiceAccount or Namespace, providing more control over resource consumption. Performance parity with sidecar mode: Ambient mode initially adopted a shared proxy model based on Envoy, but during development, issues such as complex Envoy configuration were discovered, leading the Istio community to develop its shared proxy (ztunnel ) based on Rust. In the future, ambient mode is expected to have comparable performance to traditional sidecar mode. Security: Ambient mode provides TLS support by running a shared proxy ztunnel on each node, and when users require the same security as sidecar mode, they also need to deploy one or more Waypoints in each namespace to handle L7 traffic in that namespace. Users can use istioctl x waypoint for Waypoint configuration management. For example, running the istioctl x waypoint generate command generates a Kubernetes Gateway API resource managed by Istio.\nOverall, ambient mode promises to offer additional flexibility to Istio’s deployment model which may prove helpful to some users. It should be noted that the ambient mode is still in the alpha stage and has yet to achieve production-level stability.\nEnhanced Kubernetes Gateway API Support Istio 1.18 introduces several vital improvements and modifications to its Kubernetes Gateway API support:\nSupport for v1beta1: When upgrading to the new version of Istio, Gateway API version greater than 0.6.0+ is required. Use the istioctl x precheck command to check for upgrade issues.\nGateway API automated deployment management upgrades: …","relpermalink":"/en/blog/istio-1-18-released-now-with-ambient-mode-available/","summary":"This article introduces Istio 1.18, the latest release of the service mesh platform. It highlights the new features and improvements, such as ambient mode, which allows Istio to run on any Kubernetes cluster without requiring a dedicated control plane. It also explains how to get started with Istio 1.18 using Tetrate’s distribution and support.","title":"Istio 1.18 Released, Now with Ambient Mode Available"},{"content":"In a previous blog post , I discussed how Istio Ambient Mesh uses iptables and Geneve tunnels to intercept traffic from application pods into Ztunnel. Many readers may not be familiar with this tunneling protocol, so this article will introduce the definition, packet structure and advantages of Geneve tunnels compared with the VXLAN protocol. Finally, this article will introduce how Istio Ambient Mesh applies Geneve tunnels to implement traffic interception and the new eBPF mode introduced in Istio 1.18.\nIntroduction to Geneve Tunnels In order to address the lack of flexibility and security in current data transmissions, the Geneve (Generic Network Virtualization Encapsulation) network virtualization encapsulation (tunneling) protocol was created. Geneve only defines a data encapsulation format, excluding control plane information. The key advantage of Geneve over VXLAN encapsulation is that it extends the types of encapsulated protocols by adding TLV format options.\nGeneve vs. VXLAN VXLAN and Geneve are both network virtualization protocols and they have many similarities. Virtualization protocols are technologies that separate virtual networks from physical networks. They allow network administrators to create multiple virtual networks in a virtual environment, each of which can have its own VLAN identifiers, IP addresses and routing. In addition, VXLAN and Geneve use UDP encapsulation, which enables them to be extended through existing network infrastructure. VXLAN and Geneve protocols are also flexible, can be used in different network topologies and are compatible with different virtualization platforms.\nFigure 1 shows the message structure of VXLAN and Geneve tunnels and the differences in their respective headers.\nFigure 1: VXLAN and Geneve packet format schematic diagram. From the figure, we can see that the message structure of VXLAN and Geneve tunneling protocols is similar, with the main difference being the use of different UDP port numbers and protocol headers. VXLAN uses port 4789, while Geneve uses port 6081. The Geneve protocol header is more extendable than VXLAN.\nThe Geneve tunneling protocol adds variable-length options that can contain zero or more option data in TLV format, making it more scalable than VXLAN. TLV stands for Type-Length-Value, which is a format for parsing and transmitting metadata in network packets. Each metadata information in the Geneve protocol is composed of a TLV format field, making it simple to flexibly add, delete and modify these metadata.\nThe TLV format field contains the following data:\nType: 8-bit type field. Length: 5-bit option length field, represented in multiples of 4 bytes, excluding the option header. Data: Variable-length option data field, which may not exist or may be between 4 and 128 bytes. The Geneve protocol can easily modify and extend metadata information while maintaining compatibility and flexibility by using the TLV format.\nPlease refer to RFC 7348 Virtual eXtensible Local Area Network (VXLAN): A Framework for Overlaying Virtualized Layer 2 Networks over Layer 3 Networks for more information about VXLAN. For more information about the Geneve tunnel packet format, please refer to RFC 8926 Geneve: Generic Network Virtualization Encapsulation .\nHow it Works The Geneve tunnel is mainly used in cloud computing and virtualization scenarios, and it can encapsulate packets in a new packet for transmission in a virtual network. The Geneve tunnel uses a 24-bit VNI (Virtual Network Identifier) to transmit packets from one physical network to another. The Geneve tunnel can also use security protocols such as IPsec and TLS to protect the transmission of packets.\nWhen a packet reaches the destination host, the Geneve tunnel protocol will de-encapsulate the packet from the Geneve protocol header and deliver it to the destination in the virtual network. During the de-encapsulation process, the VNI information in the Geneve protocol header is used to determine the destination of the packet, ensuring that the packet is correctly routed to the destination in the virtual network.\nAssuming there is a virtual network with a VNI of 1001. When a packet is transmitted from one physical network to another, a tunnel can be used to track the packet during transmission by setting the VNI between the source and target physical networks to 1001. When the packet reaches the target physical network, the VNI is removed from the packet and the packet is delivered to the target physical network.\nSecurity The Geneve tunnel protocol itself does not provide any security mechanisms, so packets transmitted in the Geneve tunnel can be subject to threats such as packet tampering, interception and replay.\nTo ensure the security of packets transmitted in the Geneve tunnel, some security protocols can be used. The following are some common security protocols:\nIPsec (Internet Protocol Security): IPsec is a network layer security protocol that can encrypt, authenticate and provide …","relpermalink":"/en/blog/traffic-interception-with-geneve-tunnel-with-istio-ambient-mesh/","summary":"This article introduces Geneve tunnels, a network virtualization protocol that can intercept Istio ambient mesh traffic more flexibly and securely than VXLAN. It also explains how Istio Ambient Mesh uses Geneve tunnels and the new eBPF mode in Istio 1.18.","title":"Using Geneve Tunnels to Implement Istio Ambient Mesh Traffic Interception"},{"content":"Artificial Intelligence (AI) is changing our lives, making our work more efficient and intelligent. In this rapidly developing field, there are many practical AI tools that can help us better accomplish our work. In the future, mastering various AI tools to optimize your workflow and improve work efficiency will be a necessary skill for everyone. It’s time to gather some cheap and practical AI tools. Below are some recommended practical AI tools that are worth collecting. You can directly use these tools without additional programming knowledge. Most of these tools are free to use, or provide free usage quotas, or have low usage costs.\n1. ChatGPT ChatGPT ChatGPT is an intelligent chatbot based on GPT technology, which can interact with users in natural language. It uses deep learning technology and large-scale trained language models to understand users’ questions and provide useful answers.\nChatGPT can answer various questions, and users can directly input questions or topics on the website and get quick and accurate answers. It should be noted that ChatGPT is an online chatbot, and its answers may not be 100% accurate. In addition, the data that ChatGPT model is trained on is up to 2021, and it is still in the early development stage, and is constantly improving and optimizing.\nRecommendation Reasons\nChatGPT’s response speed is super fast, and you can get an answer in a few seconds, which is especially suitable for situations where quick replies are needed. You can also use OpenAI’s API to create your own tools. However, the free version of ChatGPT’s response speed is sometimes slow, and frequently refreshing the page is required when following up, and there are also limits on the number of characters for questions and answers.\nThe commonly used functions include:\nWriting code Translation Proofreading articles Summarizing an article (you can output a URL) Learning a knowledge area that you are not familiar with In addition, a ChatGPT desktop application is recommended: https://github.com/lencx/ChatGPT , as well as the ChatGPT prompt project https://github.com/f/awesome-chatgpt-prompts .\nChatGPT Plus ChatGPT Plus is an enhanced version of ChatGPT that utilizes GPT-3 technology to provide more powerful and intelligent natural language processing capabilities. It can assist users in completing various tasks such as generating articles, translation, Q\u0026amp;A, speech conversion, and chatbots.\nChatGPT Plus has a sleek and beautiful interface that is very intuitive and user-friendly. Users can input text using various methods such as keyboard input, voice input, and image input. ChatGPT Plus also offers multiple language and style options, making it easy for users to generate high-quality text and speech.\n2. Smodin Smodin Smodin.io is an online tool based on artificial intelligence and natural language processing technology that can help users generate content such as articles, press releases, blog posts, and social media posts. The website uses deep learning technology and language models to automatically generate high-quality text and offers a variety of language and style options.\nUsing Smodin.io is very simple. Simply enter the topic, keywords, desired language, and style you wish to generate, then click the “Generate” button to obtain a high-quality article. You can also make adjustments and edits as needed to meet your specific needs.\nIn addition to generating text, Smodin.io also offers other useful features such as grammar and spell check, SEO optimization suggestions, and real-time translation. The website is very flexible and convenient to use, making it suitable for individuals and businesses who require high-quality text, such as marketers, editors, writers, and bloggers.\nIt’s important to note that while Smodin.io can save you a lot of time and effort, since the text is generated automatically, there may be some syntax or logic errors. Therefore, appropriate review and editing are necessary when using it.\nRecommended Reasons\nThe website supports over 40 languages, limits input to 1000 characters per session, and returns rewritten results quickly. Free users have a limit on the number of calls they can make, while paid users have a limit. There are two payment tiers, both of which are relatively inexpensive.\n3. Bing Bing chat Bing is a search engine developed by Microsoft that integrates artificial intelligence technology to provide more intelligent search results. Recently, Microsoft announced a new feature called “Chat” added to Bing, where users can input questions in the chat box and Bing will automatically answer them, just like an intelligent chatbot. Bing also provides multiple conversation styles to choose from.\nIt should be noted that the chat function is currently in the testing phase and may have some issues and limitations. Additionally, Bing’s availability may be restricted in some countries and regions. However, it is still a very useful search engine that can help users quickly find the …","relpermalink":"/en/blog/ai-tools-collection/","summary":"Are you looking for ways to optimize your workflow and increase productivity? Here are some useful AI tools for you.","title":"Useful AI Tools to Optimize Your Workflow"},{"content":"In the previous blog post , I introduced how Istio manages certificates, and in this article, I will guide you on how to use an external certificate authority (CA) to achieve fine-grained certificate management and automatic certificate rotation through the integration of SPIRE and cert-manager .\nIf you are not familiar with SPIRE and what it’s used for, we recommend reading the following articles:\nWhy Would You Need SPIRE for Authentication with Istio? How to integrate SPIRE in Istio Introduction to the Certificate Issuance and Management Process Figure 1 shows the certificate trust chain used in this article based on cert-manager and SPIRE:\nFigure 1: Certificate trust chain based on cert-manager and SPIRE. cert-manager acts as the root CA to issue certificates to istiod and SPIRE. We use a self-signed issuer , but you can also configure it to use built-in issuers such as Let’s Encrypt, Vault, Venafi, or other external issuers. You can also choose to use other UpstreamAuthorities , such as Vault, SPIRE Federation, etc. SPIRE issues SVID certificates to the workloads and ingress Gateway and egress Gateway in the Istio mesh for mTLS between services. The certificates used when accessing the ingress Gateway from outside the mesh and when the egress Gateway access services outside the mesh need to be configured separately. Figure 2 shows the certificate issuance and update process after integrating SPIRE and cert-manager in Istio.\nFigure 2: Certificate issuance and update process after integrating SPIRE and \u0026lt;em\u0026gt;cert-manager\u0026lt;/em\u0026gt; in Istio. The Kubernetes Workload Registrar in the SPIRE Server automatically registers the workloads in Kubernetes and generates SPIFFE standard identities for all workloads. cert-manager issues and manages the CA certificates for istiod. The Envoy proxies in the workloads send certificate signing request (CSR) requests to the SPIRE Agent on the same node through the SDS API via a Unix domain socket (UDS). The SPIRE Agent sends the CSR to the SPIRE Server. The SPIRE Server returns the signed certificate to the SPIRE Agent. The SPIRE Agent returns the signed certificate to the workloads. SPIRE is responsible for the certificate management and updates for the workloads. Now that we have a general understanding of the process, let’s install the components manually.\nInstall cert-manager Run the following command to install cert-manager, which we will use for automatic certificate rotation:\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.10.1/cert-manager.yaml The root CA is a self-signed certificate. Run the following command to configure the root CA:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: selfsigned namespace: cert-manager spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: selfsigned-ca namespace: cert-manager spec: isCA: true duration: 21600h secretName: selfsigned-ca commonName: certmanager-ca subject: organizations: - cert-manager issuerRef: name: selfsigned kind: Issuer group: cert-manager.io --- apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: selfsigned-ca spec: ca: secretName: selfsigned-ca EOF Then configure certificates for istiod:\nkubectl create namespace istio-system cat \u0026lt;\u0026lt; EOF | kubectl apply -f - --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: cacerts namespace: istio-system spec: secretName: cacerts duration: 1440h renewBefore: 360h commonName: istiod.istio-system.svc isCA: true usages: - digital signature - key encipherment - cert sign dnsNames: - istiod.istio-system.svc issuerRef: name: selfsigned-ca kind: ClusterIssuer group: cert-manager.io EOF Now that we have installed cert-manager and created a ClusterIssuer named selfsigned-ca, let’s install SPIRE and use cert-manager as the UpstreamAuthority for SPIRE.\nInstalling SPIRE Quickly install SPIRE by running the following command:\nkubectl apply -f https://gist.githubusercontent.com/rootsongjc/5dac0518cc432cbf844114faca74aa40/raw/814587f94bbef8fb1dd376282249dcb2a8f7fa1b/spire-with-cert-manager-upstream-authority-quick-start.yaml This YAML file adapts to cert-manager compared to the samples/security/spire/spire-quickstart.yaml file in the Istio 1.16 installation package, such as:\nAdding permissions for the cert-manager.io API group to the spire-server-trust-role ClusterRole; Adding an UpstreamAuthority “cert-manager” configuration in the SPIRE Server configuration. Note: The trust_domain in the SPIRE Server configuration should be consistent with the TRUST_DOMAIN environment variable specified when installing Istio.\nThis command installs the Kubernetes Workload Registrar , automatically registering the workloads in Kubernetes. All workloads will be registered with the SPIFFE standard service identity format spiffe://\u0026lt;trust-domain\u0026gt;/ns/\u0026lt;namespace\u0026gt;/sa/\u0026lt;service-account\u0026gt; based on their service accounts.\nIf you want to adjust the TTL of the SPIRE CA and SVID …","relpermalink":"/en/blog/cert-manager-spire-istio/","summary":"In this blog, I will guide you on how to use an external certificate authority (CA) to achieve fine-grained certificate management and automatic certificate rotation through the integration of SPIRE and cert-manager.","title":"Managing Certificates in Istio with cert-manager and SPIRE"},{"content":"I mentioned in my last article on understanding mTLS traffic encryption in Istio that the key to traffic encryption is certificate management. We can use the built-in certificate authority (CA) in Istio or a custom CA to manage certificates within the mesh. This blog post will explain how Istio handles certificate management.\nWhat Is a Certificate? There are many types of certificates, but in this article, I am explicitly referring to X.509 V3 certificates. X.509 certificates are a standard digital format used to identify entities in computer networks. X.509 is the international standard for public key infrastructure (PKI) and is primarily used for identity authentication and information encryption, such as TLS.\nThe contents of a certificate are hashed using a hash function and then signed with the issuer’s private key. This way, when a certificate is received, the recipient can use the issuer’s public key to verify the certificate’s validity.\nA hash function is a function that maps an input of any length (also called a message) to a fixed-length output, also called a hash value or message digest. There are many hash functions, such as MD5, SHA-1, etc.\nA certificate is like a business card issued by an authoritative agency for the user to prove their identity, and it can also be used to encrypt information to ensure the security and integrity of communication. The following diagram shows the general steps of TLS communication, where the certificate proves the server’s identity and encrypts the communication.\nFigure 1 shows an example of an HTTP call to a website, issuing a digital certificate, authenticating, and encrypting the communication.\nFigure 1. TLS certificate issuance and validation process Here are the detailed steps:\nThe server (website owner) submits a certificate signing request to the CA; The CA verifies the server’s identity and the authenticity of the website and issues a digital certificate to the server, which the server installs so that visitors can verify the website’s security; The user sends a request to the website through a browser (client); The server returns the TLS certificate to the client; The client verifies the certificate’s validity with the CA and establishes the connection if the certificate is valid, or prompts the user to reject the connection if it is invalid; The client generates a pair of random public and private keys; The client sends its public key to the server; The server encrypts the message using the client’s public key; The server sends the encrypted data to the client; The client decrypts the data sent by the server using its private key. At this point, both parties have established a secure channel and can transmit encrypted data in both directions.\nHow to Generate Certificates You can generate X.509 certificates with the following open-source tools:\nEasy-RSA : A simple command-line tool maintained by the OpenVPN project, EasyRSA can easily generate secure certificates and keys for the OpenVPN network. OpenSSL : Originated by an individual in 1995 and now maintained by an independent organization, OpenSSL provides only a command-line tool. CFSSL : Developed and maintained by CloudFlare, CFSSL is not just a command-line tool for generating certificates but also can serve as a PKI server. BoringSSL : A branch of OpenSSL developed and maintained by Google, BoringSSL is used in the Chrome browser and Android operating system. Since most people are probably familiar with OpenSSL, we will use OpenSSL to create certificates in the following text.\nCertificate Trust Chain The validation of a certificate requires using a certificate trust chain (Certificate Trust Chain). A certificate trust chain refers to a series of certificates used for identity verification, which form a chain starting from a trusted root certificate issuing agency, connecting downward step by step, until a specific intermediate or terminal certificate is used for verifying a particular certificate. The trustworthiness of a digital certificate increases as the certificate level increases in the certificate trust chain.\nIn Figure 2, you can see four trust chains.\nFigure 2. Certificate trust chains Certificate trust chains are tree-like structures, where each CA can have one or more child CAs. There are three roles:\nRoot CA: The top-level CA can issue certificates to intermediate CAs. Intermediate CA: They can issue end-entity certificates. End entity: A device or service that holds an end-entity certificate. Root CAs are the top-level issuing authorities for digital certificates, so the certificates they issue are the most trustworthy. Root certificate authorities are usually operated and regulated by government agencies or other authoritative organizations such as the International Infrastructure Security Organization. Common root CAs include:\nSymantec/VeriSign Comodo DigiCert GlobalSign GoDaddy Entrust GeoTrust RapidSSL Baltimore CyberTrust Root Please note that the above list is just a sample. …","relpermalink":"/en/blog/istio-certificates-management/","summary":"This blog post will explain how Istio handles certificate management.","title":"How Are Certificates Managed in Istio?"},{"content":"In my last blog , I introduced transparent traffic intercepting and L4 routing in Ambient mode. In this blog, I will show you how L7 traffic is routed.\nThe figure below shows the L7 network traffic path in ambient mode.\nFigure 1: L7 network traffic in ambient mesh Note: The Waypoint proxy can be located on the same node as the application, and even all of the service and the Waypoint proxy can be on the same node. I draw them on three nodes for display purposes, but it has no significant impact on the actual traffic path, except that it is no longer sent to another node via eth0.\nIn the following section, we will explore the process in Figure 1 from a hands-on perspective.\nEnvironments for Waypoint Proxy Let’s continue to view the environment description using the ambient mode Istio deployed in the previous blog. To illustrate the L7 network routing, we need to create a Gateway on top of this.\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: gateway.networking.k8s.io/v1alpha2 kind: Gateway metadata: name: productpage annotations: istio.io/service-account: bookinfo-productpage spec: gatewayClassName: istio-mesh EOF After executing this command, a Waypoint proxy is created under the default namespace; in my environment, this pod is named bookinfo-productpage-waypoint-proxy-6f88c55d59-4dzdx and is specifically used to handle L7 traffic to the productpage service ( Service B), which I will call Waypoint proxy B.\nThe Waypoint proxy may be deployed in a different namespace, on a different node from the workload, or both. No matter where the Waypoint proxy is situated, the L7 traffic path is unaffected.\nWe will omit the sections of this blog dealing with intercepting inbound and outbound traffic because the way transparent traffic is handled in ambient mesh in L4 and L7 networks is similar. Details are available in the prior blog .\nWe will start directly with the traffic intercepted at Ztunnel A and then forward it to Envoy port 15006.\nOutbound Traffic Routing on Ztunnel A Use the following command to dump the Envoy proxy configuration on Ztunnel A:\nkubectl exec -n istio-system ztunnel-hptxk -c istio-proxy -- curl \u0026#34;127.0.0.1:15000/config_dump?include_eds\u0026#34;\u0026gt;ztunnel-a-all-include-eds.json 10.8.14.226 is the Cluster IP of the target service, and the service port is 9080. The traffic will be routed to the spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage cluster. Let’s look at the configuration of that cluster.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \u0026#34;10.8.14.226\u0026#34;: { \u0026#34;matcher\u0026#34;: { \u0026#34;matcher_tree\u0026#34;: { \u0026#34;input\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;port\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.matching.common_inputs.network.v3.DestinationPortInput\u0026#34; } }, \u0026#34;exact_match_map\u0026#34;: { \u0026#34;map\u0026#34;: { \u0026#34;9080\u0026#34;: { \u0026#34;action\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/google.protobuf.StringValue\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage\u0026#34; } } } } } } } } } 10.8.14.226 is the Cluster IP of the target service, and the service port is 9080. The traffic will be routed to the spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage cluster. Let’s look at the configuration of that cluster.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;version_info\u0026#34;: \u0026#34;2022-11-17T03:27:45Z/82\u0026#34;, \u0026#34;cluster\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.cluster.v3.Cluster\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;EDS\u0026#34;, \u0026#34;eds_cluster_config\u0026#34;: { \u0026#34;eds_config\u0026#34;: { \u0026#34;ads\u0026#34;: {}, \u0026#34;initial_fetch_timeout\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;resource_api_version\u0026#34;: \u0026#34;V3\u0026#34; } }, /* omit */ } The cluster is discovered using the EDS service. To view the EDS information for this cluster:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.endpoint.v3.ClusterLoadAssignment\u0026#34;, \u0026#34;endpoints\u0026#34;: [ { \u0026#34;locality\u0026#34;: {}, \u0026#34;lb_endpoints\u0026#34;: [ { \u0026#34;endpoint\u0026#34;: { \u0026#34;address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;10.4.3.14\u0026#34;, \u0026#34;port_value\u0026#34;: 15006 } }, \u0026#34;health_check_config\u0026#34;: {} }, \u0026#34;health_status\u0026#34;: \u0026#34;HEALTHY\u0026#34;, \u0026#34;load_balancing_weight\u0026#34;: 1 } ] } ], \u0026#34;policy\u0026#34;: { \u0026#34;overprovisioning_factor\u0026#34;: 140 } } Note: The output cluster_name field is still missing here. See the GitHub issue .\nTraffic is forwarded here directly to the Waypoint Proxy endpoint at 10.4.3.14:15006.\nTraffic Routing Using Waypoint Proxy Let’s dump the Envoy configuration into Waypoint Proxy B again.\nkubectl exec -n default bookinfo-productpage-waypoint-proxy-6f88c55d59-4dzdx -c istio-proxy -- curl \u0026#34;127.0.0.1:15000/config_dump?include_eds\u0026#34;\u0026gt;waypoint-a-all-include-eds.json Look into the configuration of inbound_CONNECT_terminate listener:\n1 2 3 4 5 6 …","relpermalink":"/en/blog/ambient-mesh-l7-traffic-path/","summary":"This article describes in detail the L7 traffic path in Ambient Mesh in both diagrammatic and hands-on form.","title":"L7 Traffic Path in Ambient Mesh"},{"content":"Istio’s new “ambient mode” is an experimental, “sidecar-less” deployment model for Istio . Instead of a sidecar proxy in front of every workload, ambient mode uses tproxy and HTTP Based Overlay Network Environment (HBONE) as key technologies for transparent traffic intercepting and routing that we covered in our recent article on transparent traffic intercepting and routing in the L4 network of Istio Ambient Mesh . In this article, we’ll take a closer look at tproxy and how it’s used.\nWhat Is a Proxy For? Proxies have a wide range of uses on the Internet, such as:\nRequest caching: to speed up network response, acting similarly to a CDN. Traffic filtering: used for network supervision, blocking or allowing access to specific hosts and websites. Traffic forwarding: used for load balancing or as a network relay. Traffic management: fine-grained management of traffic to and from the proxy, such as publishing to different backends by percentage, timeout and retry settings, circuit breaking, etc. Security auditing: logging and limiting client requests for billing or auditing purposes. Proxy Types There are a number of ways to classify proxies based on how they’re used. You can see two categories based on the location of the proxy in Figure 1:\nFigure 1: Forward proxy and reverse proxy. Forward proxies (like shadowsocks ) run on the client side and send requests to the server on behalf of the client. Reverse proxies (often in the form of a web server) accept Internet or external requests on behalf of the server and route them to the corresponding backends. Proxies may be located on the same node as the client or server or on a different node. We can classify them as transparent or non-transparent based on whether the client or server can see them. Figure 2 (below) shows the process of a client (A) sending a request to a server (C) through a proxy (B).\nFigure 2: Transparent vs. non-transparent proxies To use a non-transparent proxy, the client needs to explicitly change the destination address to that of the proxy server and use the proxy protocol to connect to the proxy server. To use a transparent proxy, the client and the server do not know the proxy is there, which means the client does not need to modify the destination address, and does not need to use the proxy protocol to connect to the proxy server; all the destination address conversion is done in the transparent proxy. Using the tproxy Transparent Proxy tproxy is a Linux kernel module (since Linux 2.2) that implements transparent proxies. To use tproxy, you must first use iptables to intercept the required packets at the required NIC, then listen for and forward the packet on that NIC.\nFollow these steps to use tproxy to implement a transparent proxy:\nFirst, you need to implement traffic interception: create a rule in the mangle table of the PREROUTING chain of iptables to intercept traffic and send it to tproxy for processing, for example, iptables -t mangle -A PREROUTING -p tcp -dport 9080 -j TPROXY --on-port 15001 --on-ip 127.0.0.1 --tproxy-mark 0x1/0x1, marking all TCP packets destined for port 9080 with a mark 1. You can specify the source IP address or IP set to further narrow the marking, with tproxy listening on port 15001. Create a routing rule that looks up all packets with mark 1 in a specific routing table: for example, add ip rule add fwmark 1 lookup 100, so that all packets with fwmark 1 look up to the routing table 100. Mapping packets to specific local addresses: for example, ip rule add local 0.0.0.0/0 dev lo table 100, which declares all IPv4 addresses as local in the routing table 100, but of course, this is just an example. In practice, you will need to forward packets with specific IPs to the local lo loopback NIC. The traffic has been intercepted on tproxy’s listening port 15001 (enter from Linux kernel space into user space). You can write a web application to process the packets or use tproxy-enabled software such as Squid or Envoy to process the packets.\nPros and Cons of Transparent Proxies Transparent proxies have the following advantages:\nHigher bandwidth and reduced transmission latency, thereby improving the quality of service. No need for users to configure networks and hosts. Control access to network services. Transparent proxies have the following disadvantages:\nIncorrectly configured, the transparent proxy may prevent connection to the Internet, leaving users unable to troubleshoot and fix errors. Security cannot be guaranteed, as intercepted user traffic may be tampered with by transparent proxies. The risk that transparent proxies may cache user information, leading to privacy leaks. Summary As a vital type of proxy, transparent proxies are used in a wide range of applications, whether in proxy software such as shadowsocks, Xray, or in the Istio service mesh. Understanding how they work helps us use proxies correctly, and whether or not you use a transparent proxy depends on how much you trust and know about it.\nThis …","relpermalink":"/en/blog/what-is-tproxy/","summary":"Tproxy is used to intercept traffic in Istio Ambient mode.","title":"How Istio's Ambient Mode Transparent Proxy - tproxy Works Under the Hood"},{"content":"In cloud native applications, a request often needs to be processed through a series of APIs or backend services, some of which are parallel and some serial and located on different platforms or nodes. How do we determine the service paths and nodes a call goes through to help us troubleshoot the problem? This is where distributed tracing comes into play.\nThis article covers:\nHow distributed tracing works How to choose distributed tracing software How to use distributed tracing in Istio How to view distributed tracing data using Bookinfo and SkyWalking as examples Distributed Tracing Basics Distributed tracing is a method for tracing requests in a distributed system to help users better understand, control, and optimize distributed systems. There are two concepts used in distributed tracing: TraceID and SpanID. You can see them in Figure 1 below.\nTraceID is a globally unique ID that identifies the trace information of a request. All traces of a request belong to the same TraceID, and the TraceID remains constant throughout the trace of the request. SpanID is a locally unique ID that identifies a request’s trace information at a certain time. A request generates different SpanIDs at different periods, and SpanIDs are used to distinguish trace information for a request at different periods. TraceID and SpanID are the basis of distributed tracing. They provide a uniform identifier for request tracing in distributed systems and facilitate users’ ability to query, manage, and analyze the trace information of requests.\nFigure 1: Trace and span The following is the process of distributed tracing:\nWhen a system receives a request, the distributed tracing system assigns a TraceID to the request, which is used to chain together the entire chain of invocations. The distributed trace system generates a SpanID and ParentID for each service call within the system for the request, which is used to record the parent-child relationship of the call; a Span without a ParentID is used as the entry point of the call chain. TraceID and SpanID are to be passed during each service call. When viewing a distributed trace, query the full process of a particular request by TraceID. How Istio Implements Distributed Tracing Istio’s distributed tracing is based on information collected by the Envoy proxy in the data plane. After a service request is intercepted by Envoy, Envoy adds tracing information as headers to the request forwarded to the destination workload. The following headers are relevant for distributed tracing:\nAs TraceID: x-request-id Used to establish parent-child relationships for Span in the LightStep trace: x-ot-span-context\u0026lt;/li Used for Zipkin, also for Jaeger, SkyWalking, see b3-propagation : x-b3-traceid x-b3-traceid x-b3-spanid x-b3-parentspanid x-b3-sampled x-b3-flags b3 For Datadog: x-datadog-trace-id x-datadog-parent-id x-datadog-sampling-priority For SkyWalking: sw8 For AWS X-Ray: x-amzn-trace-id For more information on how to use these headers, please see the Envoy documentation .\nRegardless of the language of your application, Envoy will generate the appropriate tracing headers for you at the Ingress Gateway and forward these headers to the upstream cluster. However, in order to utilize the distributed tracing feature, you must modify your application code to attach the tracing headers to upstream requests. Since neither the service mesh nor the application can automatically propagate these headers, you can integrate the agent for distributed tracing into the application or manually propagate these headers in the application code itself. Once the tracing headers are propagated to all upstream requests, Envoy will send the tracing data to the tracer’s back-end processing, and then you can view the tracing data in the UI.\nFor example, look at the code of the Productpage service in the Bookinfo application . You can see that it integrates the Jaeger client library and synchronizes the header generated by Envoy with the HTTP requests to the Details and Reviews services in the getForwardHeaders (request) function.\ndef getForwardHeaders(request): headers = {} # Using Jaeger agent to get the x-b3-* headers span = get_current_span() carrier = {} tracer.inject( span_context=span.context, format=Format.HTTP_HEADERS, carrier=carrier) headers.update(carrier) # Dealing with the non x-b3-* header manually if \u0026#39;user\u0026#39; in session: headers[\u0026#39;end-user\u0026#39;] = session[\u0026#39;user\u0026#39;] incoming_headers = [ \u0026#39;x-request-id\u0026#39;, \u0026#39;x-ot-span-context\u0026#39;, \u0026#39;x-datadog-trace-id\u0026#39;, \u0026#39;x-datadog-parent-id\u0026#39;, \u0026#39;x-datadog-sampling-priority\u0026#39;, \u0026#39;traceparent\u0026#39;, \u0026#39;tracestate\u0026#39;, \u0026#39;x-cloud-trace-context\u0026#39;, \u0026#39;grpc-trace-bin\u0026#39;, \u0026#39;sw8\u0026#39;, \u0026#39;user-agent\u0026#39;, \u0026#39;cookie\u0026#39;, \u0026#39;authorization\u0026#39;, \u0026#39;jwt\u0026#39;, ] for ihdr in incoming_headers: val = request.headers.get(ihdr) if val is not None: headers[ihdr] = val return headers For more information, the Istio documentation provides answers to frequently asked questions about distributed tracing in Istio.\nHow to Choose A Distributed Tracing System Distributed …","relpermalink":"/en/blog/distributed-tracing-with-skywalking-in-istio/","summary":"This blog will guide you to use SkyWalking for distributed tracing with Istio.","title":"How to Use SkyWalking for Distributed Tracing in Istio?"},{"content":"The Istio service mesh offers cloud native deployments a standard way to implement automatic mutual transport layer security (mTLS) . This reduces the attack surface of network communication by using strong identities to establish encrypted channels between workloads within the mesh that are both confidential and tamper-resistant. mTLS is a key component for building zero-trust application networks. To understand mTLS traffic encryption in Istio, this article will cover the following:\nAn overview of TLS, mTLS, and TLS termination An introduction to howTLS encryption works in Istio How to use Istio to implement mTLS in Kubernetes A discussion of when you do and don’t need mTLS What Is TLS and mTLS? TLS, the successor to Secure Sockets Layer (SSL), is a widely adopted security protocol used to create authenticated and encrypted connections between networked computers. For this reason, people often use the terms TLS and SSL interchangeably. In this article, we will refer to them collectively as TLS. TLS 1.0 was released in 1999, and the latest version is 1.3 (released in August 2018); versions 1.0 and 1.1 are deprecated.\nThe HTTPS we see when browsing the web uses TLS, as shown in Figure 1, which is built on top of TCP as the session layer in the OSI model. To ensure compatibility, TLS usually uses port 443, but you can use any port you want.\nFigure 1: HTTP vs. HTTPS TLS encryption is required when a client needs to confirm the identity of the server in order to guard against man-in-the-middle attacks and ensure communication security. Figure 2 shows how TLS-encrypted communication proceeds.\nFigure 2: simplified TLS handshake flow The server applies for and obtains a certificate (X.509 certificate) from a trusted certificate authority (CA). A request from the client to the server containing information such as the TLS version and password combination supported by the client. The server responds to the client request and attaches a digital certificate. The client verifies the status, validity, and digital signature of the certificate and confirms the identity of the server. Encrypted communication commences between the client and the server using a shared private key. The above is only an outline description of the TLS communication flow. If you’re interested in the details, please see this in-depth discussion of the complete TLS handshake process. From the above process, you will find that the certificate is the critical element representing the server’s identity. The server must use a certificate issued by an authoritatively certified CA in order to provide public services over the Internet. In contrast, you can manage certificates using your own public key infrastructure (PKI) for services inside of a private environment.\nMutual TLS, also referred to as mTLS, is the use of a two-way encrypted channel between a server and a client that necessitates certificate exchange and identity authentication between the parties.\nWhat Is TLS Termination? TLS termination is the process of decrypting TLS-encrypted traffic before it is forwarded to a web server. Offloading TLS traffic to an ingress gateway or specialized device improves web application performance while securing encrypted traffic. TLS termination is typically implemented at cluster ingress. All communication between the ingress and servers in the cluster will be conducted directly over HTTP in plaintext, enhancing service performance.\nFigure 3: TLS termination By default, Istio enables mTLS for mesh-based services and ends TLS at the ingress gateway. Furthermore, you can pass through traffic to back-end services for processing.\napiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: sample-gateway spec: servers: - port: number: 443 name: https protocol: HTTPS tls: mode: PASSTHROUGH See Gateway TLS Configuration for details.\nHow to Implement Automatic mTLS in Istio Figure 4 depicts the security architecture of Istio. This figure clearly shows that at the entry point, JSON Web Token (JWT) + TLS authentication and encryption are used, and that mTLS is enabled between all services within the Istio mesh.\nIstio 安全架构图 Istio includes a built-in CA, and Secret Discovery Service (SDS) —one of the discovery services in Envoy xDS —enables the issuance and rotation of SVID certificates. The mTLS flow in the Istio mesh is as follows:\nThe sidecar of every service requests a certificate from Istiod on behalf of the workload at startup, and Istiod issues the SVID certificate (the process is more complex, and I will explain it in a future blog). The sidecar of every workload intercepts all client requests within the pod. The client sidecar starts an mTLS handshake with the server sidecar. During the handshake, the JWT and authentication filter in the client sidecar will authenticate the identity of the request, and store the identity in the filter metadata after the authentication. Then the request will go through the authorization filter to determine if the …","relpermalink":"/en/blog/understanding-the-tls-encryption-in-istio/","summary":"This article introduces TLS and mTLS, and describes how to enable mTLS in Istio and its application scenarios.","title":"How Istio’s mTLS Traffic Encryption Works as Part of a Zero Trust Security Posture"},{"content":"Ambient mesh is an experimental new deployment model recently introduced to Istio. It splits the duties currently performed by the Envoy sidecar into two separate components: a node-level component for encryption (called “ztunnel”) and an L7 Envoy instance deployed per service for all other processing (called “waypoint”). The ambient mesh model is an attempt to gain some efficiencies in potentially improved lifecycle and resource management. You can learn more about what ambient mesh is and how it differs from the Sidecar pattern here .\nThis article takes you step-by-step through a hands-on approach to the transparent traffic intercepting and routing of L4 traffic paths in the Istio’s Ambient mode. If you don’t know what Ambient mode is, this article can help you understand.\nIf you want to skip the actual hands-on steps and just want to know the L4 traffic path in Ambient mode, please see the figure below, it shows a Pod of Service A calling a Pod of Service B on a different node below.\nFigure 1: Transparent traffic intercepting and routing in the L4 network of Istio Ambient Mesh Principles Ambient mode uses tproxy and HTTP Based Overlay Network Environment (HBONE) as key technologies for transparent traffic intercepting and routing:\nUsing tproxy to intercept the traffic from the host Pod into the Ztunnel (Envoy Proxy). Using HBONE to establish a tunnel for passing TCP traffic between Ztunnels. What Is tproxy? tproxy is a transparent proxy supported by the Linux kernel since version 2.2, where the t stands for transparent. You need to enable NETFILTER_TPROXY and policy routing in the kernel configuration. With tproxy, the Linux kernel can act as a router and redirect packets to user space. See the tproxy documentation for details.\nWhat Is HBONE? HBONE is a method of providing tunneling capabilities using the HTTP protocol. A client sends an HTTP CONNECT request (which contains the destination address) to an HTTP proxy server to establish a tunnel, and the proxy server establishes a TCP connection to the destination on behalf of the client, which can then transparently transport TCP data streams to the destination server through the proxy. In Ambient mode, Ztunnel (Envoy inside) acts as a transparent proxy, using Envoy Internal Listener to receive HTTP CONNECT requests and pass TCP streams to the upstream cluster.\nEnvironment Before starting the hands-on, it is necessary to explain the demo environment, and the corresponding object names in this article:\nItems Name IP Service A Pod sleep-5644bdc767-2dfg7 10.4.4.19 Service B Pod productpage-v1-5586c4d4ff-qxz9f 10.4.3.20 Ztunnel A Pod ztunnel-rts54 10.4.4.18 Ztunnel B Pod ztunnel-z4qmh 10.4.3.14 Node A gke-jimmy-cluster-default-pool-d5041909-d10i 10.168.15.222 Node B gke-jimmy-cluster-default-pool-d5041909-c1da 10.168.15.224 Service B Cluster productpage 10.8.14.226 Because these names will be used in subsequent command lines, the text will use pronouns, so that you can experiment in your own environment.\nFor the tutorial, I installed Istio Ambient mode in GKE. You can refer to this Istio blog post for installation instructions. Be careful not to install the Gateway, so as not to enable the L7 functionality; otherwise, the traffic path will be different from the descriptions in this blog.\nIn the following, we will experiment and dive into the L4 traffic path of a pod of sleep service to a pod of productpage service on different nodes. We will look at the outbound and inbound traffic of the Pods separately.\nOutbound Traffic Intercepting The transparent traffic intercepting process for outbound traffic from a pod in Ambient mesh is as follows:\nIstio CNI creates the istioout NIC and iptables rules on the node, adds the Pods’ IP in Ambient mesh to the IP set , and transparently intercepts outbound traffic from Ambient mesh to pistioout virtual NIC through Geneve (Generic Network Virtualization Encapsulation) tunnels with netfilter nfmark tags and routing rules. The init container in Ztunnel creates iptables rules that forward all traffic from the pistioout NIC to port 15001 of the Envoy proxy in Ztunnel. Envoy processes the packets and establishes an HBONE tunnel (HTTP CONNECT) with the upstream endpoints to forward the packets upstream. Check The Routing Rules On Node A Log in to Node A, where Service A is located, and use iptables-save to check the rules.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 $ iptables-save /* omit */ -A PREROUTING -j ztunnel-PREROUTING -A PREROUTING -m comment --comment \u0026#34;kubernetes service portals\u0026#34; -j KUBE-SERVICES -A ztunnel-POSTROUTING -m mark --mark 0x100/0x100 -j ACCEPT -A ztunnel-PREROUTING -m mark --mark 0x100/0x100 -j ACCEPT /* omit */ *mangle /* omit */ -A PREROUTING -j ztunnel-PREROUTING -A INPUT -j ztunnel-INPUT -A FORWARD -j ztunnel-FORWARD -A OUTPUT -j ztunnel-OUTPUT -A OUTPUT -s 169.254.169.254/32 -j DROP -A POSTROUTING -j ztunnel-POSTROUTING -A ztunnel-FORWARD -m mark …","relpermalink":"/en/blog/ambient-mesh-l4-traffic-path/","summary":"This article details transparent traffic intercepting and L4 traffic paths in Ambient Mesh in both diagrammatic and hands-on form.","title":"Transparent Traffic Intercepting and Routing in the L4 Network of Istio Ambient Mesh"},{"content":"In September 2022, Istio became a CNCF incubation project and launched the new Ambient Mesh . With CNCF’s strong community and marketing resources, and Ambient Mesh further lowering the barrier to trying Istio, the five year old open source project has been revitalized.\nIf you don’t know about service mesh and Istio, or are curious about the future of Istio, this eBook—The Current State and Future of the Istio Service Mesh will give you the answers. The following is an excerpt from the book. In my view, the future of Istio lies in being the infrastructure for zero-trust network and hybrid cloud.\nZero Trust Zero trust is an important topic, including when I spoke at IstioCon 2022. Istio is becoming an important part of zero trust, the most important element of which is identity-oriented control rather than network-oriented control.\nWhat Is Zero Trust? Zero trust is a security philosophy, not a best practice that all security teams follow in the same way. The concept of zero trust was proposed to bring a more secure network to the cloud-native world. Zero trust is a theoretical state where all consumers within a network not only have no authority but also have no awareness of the surrounding network. The main challenges of zero trust are the increasingly granular authorization and the need for a time limit for user authorization.\nAuthentication Istio 1.14 adds support for the SPIFFE Runtime Environment (SPIRE). SPIRE, a CNCF incubation project, is an implementation of the Secure Production Identity Framework for Everyone (SPIFFE), also a CNCF Incubation Project. In Kubernetes, we use ServiceAccount to provide identity information for workloads in Pods, and its core is based on Token (using Secret resource storage) to represent workload identity. A token is a resource in a Kubernetes cluster. How to unify the identities of multiple clusters and workloads running in non-Kubernetes environments (such as virtual machines)? That’s what SPIFFE is trying to solve.\nThe purpose of SPIFFE is to establish an open and unified workload identity standard based on the concept of zero trust, which helps to establish a fully identifiable data center network with zero trust. The core of SPIFFE is to define a short-lived encrypted identity document—SPIFFE Verifiable Identity Document (SVID)—through a simple API, which is used as an identity document (based on an X.509 certificate or JWT token) for workload authentication. SPIRE can automatically rotate SVID certificates and keys according to administrator-defined policies, dynamically provide workload identities, and Istio can dynamically consume these workload identities through SPIRE.\nThe Kubernetes-based SPIRE architecture diagram is shown below.\nFigure 1: SPIRE deployed in Kubernetes Istio originally used the Citadel service in Istiod to be responsible for certificate management in the service mesh, and issued the certificate to the data plane through the xDS (to be precise, SDS API) protocol. With SPIRE, the work of certificate management is handed over to SPIRE Server. SPIRE also supports the Envoy SDS API. After we enable SPIRE in Istio, the traffic entering the workload pod will be authenticated once after being transparently intercepted into the sidecar. The purpose of authentication is to compare the identity of the workload with the environment information it runs on (node, Pod’s ServiceAccount and Namespace, etc.) to prevent identity forgery. Please refer to How to Integrate SPIRE in Istio to learn how to use SPIRE for authentication in Istio.\nWe can deploy SPIRE in Kubernetes using the Kubernetes Workload Registrar, which automatically registers the workload in Kubernetes for us and generates an SVID. The registration machine is a Server-Agent architecture, which deploys a SPIRE Agent on each node, and the Agent communicates with the workload through a shared UNIX Domain Socket. The following diagram shows the process of using SPIRE for authentication in Istio.\nFigure 2: SPIRE-based workload authentication process in Istio The steps to using SPIRE for workload authentication in Istio are as follows:\nTo obtain the SIVD, the SPIRE Agent is referred to as a pilot-agent via shared UDS. The SPIRE Agent asks Kubernetes (to be precise, the kubelet on the node) for load information. The kubelet returns the information queried from the API server to the workload validator. The validator compares the result returned by the kubelet with the identity information shared by the sidecar. If it is the same, it returns the correct SVID cache to the workload. If it is different, the authentication fails. Please refer to the SPIRE documentation for the detailed process of registering and authenticating workloads.\nNGAC When each workload has an accurate identity, how can the permissions of these identities be restricted? Role-based access control (RBAC) is used by default in Kubernetes for access control. As the name suggests, this access control is based on roles. Although it is …","relpermalink":"/en/blog/the-future-of-istio/","summary":"The future of Istio lies in being the infrastructure for zero-trust network and hybrid cloud.","title":"The Future of Istio: the Path to Zero Trust Security"},{"content":"In this blog, you will learn about the Kubernetes Ingress Gateway, the Gateway API, and the emerging Gateway API trend, which enables the convergence of Kubernetes and service mesh.\nTakeaways Ingress, the original gateway for Kubernetes, has a resource model that is too simple to fit into today’s programmable networks. The Gateway API , the latest addition to the Kubernetes portal gateway, separates concerns through role delineation and provides cross-namespace support to make it more adaptable to multi-cloud environments. Most API gateways already support it. The Gateway API provides a new reference model for the convergence of ingress gateways (north-south) and service mesh (east-west, cross-cluster routing), where there is a partial functional overlap. History of the Kubernetes ingress gateway When Kubernetes was launched in June 2014, only NodePort and LoadBalancer-type Service objects were available to expose services within the cluster to the outside world. Later, Ingress was introduced to offer more control over incoming traffic.. To preserve its portability and lightweight design, the Ingress API matured more slowly than other Kubernetes APIs; it was not upgraded to GA until Kubernetes 1.19.\nIngress’ primary objective is to expose HTTP applications using a straightforward declarative syntax. When creating an Ingress or setting a default IngressClass in Kubernetes, you can deploy several Ingress Controllers and define the controller the gateway uses via IngressClass. Kubernetes currently supports only AWS, GCE, and Nginx Ingress controllers by default; many third-party ingress controllers are also supported.\nThe following diagram illustrates the workflow of Kubernetes Ingress.\nFigure 1: Kubernetes ingress workflow The detailed process is as follows:\nKubernetes cluster administrators deploy an Ingress Controller in Kubernetes. The Ingress Controller continuously monitors changes to IngressClass and Ingress objects in the Kubernetes API Server. Administrators apply IngressClass and Ingress to deploy the gateway. Ingress Controller creates the corresponding ingress gateway and configures the routing rules according to the administrator’s configuration. If in the cloud, the client accesses the load balancer for that ingress gateway. The gateway will route the traffic to the corresponding back-end service based on the host and path in the HTTP request. Istio supports both the Ingress and Gateway APIs. Below is an example configuration using the Istio Ingress Gateway, which will be created later using the Gateway API:\napiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: istio spec: controller: istio.io/ingress-controller --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress spec: ingressClassName: istio rules: - host: httpbin.example.com http: paths: - path: / pathType: Prefix backend: service: name: httpbin port: 8000 Note: You must specify the IngressClass in the ingressClassName field in the Ingress spec. Otherwise, the ingress gateway will not be created.\nLimitations of Kubernetes Ingress Although IngressClass decouples the ingress gateway from the back-end implementation, it still has significant limitations.\nIngress is too simple for most real-world use and it only supports HTTP protocol routing. It only supports host and path matching, and there is no standard configuration for advanced routing features, which can only be achieved through annotation, such as URL redirection using Nginx Ingress Controller, which requires configuration of nginx.ingress.kubernetes.io/rewrite-target annotation, which is no longer adaptable to the needs of a programmable proxy. The situation where services in different namespaces must be bound to the same gateway often arises in practical situations where the ingress gateway cannot be shared across multiple namespaces. No delineation of responsibilities for creating and managing ingress gateways, resulting in developers having to not only configure gateway routes but also create and manage gateways themselves. Kubernetes Gateway API The Gateway API is a collection of API resources: GatewayClass, Gateway, HTTPRoute, TCPRoute, ReferenceGrant, etc. The Gateway API exposes a more generic proxy API that can be used for more protocols than HTTP and models more infrastructure components, providing better deployment and management options for cluster operations.\nIn addition, the Gateway API achieves configuration decoupling by separating resource objects that people can manage in different roles. The following diagram shows the roles and objects in the Gateway API.\nFigure: Roles and componentes in Kubernetes Gateway API The following is an example of using the Gateway API in Istio.\napiVersion: gateway.networking.k8s.io/v1alpha2 kind: Gateway metadata: name: gateway namespace: istio-ingress spec: gatewayClassName: istio listeners: - name: default hostname: \u0026#34;*.example.com\u0026#34; port: 80 protocol: HTTP allowedRoutes: namespaces: from: All --- …","relpermalink":"/en/blog/why-gateway-api-is-the-future-of-ingress-and-mesh/","summary":"This article introduces the ingress gateway and Gateway API in Kubernetes, the new trend of them with service mesh.","title":"Why the Gateway API Is the Unified Future of Ingress for Kubernetes and Service Mesh"},{"content":"Istio 1.14 was released in June of this year, and one of the most notable features of this release is support for SPIRE , which is one of the implementations of SPIFFE , a CNCF incubation project. This article explains what SPIRE means for zero-trust architectures and why you would need SPIRE for authentication in Istio.\nAuthentication in Kubernetes We all know that Istio was built for and typically runs on Kubernetes, so before we talk about how to use SPIRE for authentication in Istio, let’s take a look at how Kubernetes handles authentication.\nLet’s look at an example of a pod’s token. Whenever a pod gets created in Kubernetes, it gets assigned a default service account from the namespace, assuming we didn’t explicitly assign a service account to it. Here is an example:\napiVersion: v1 data: ca.crt: {CA_CRT} namespace: ZGVmYXVsdA== token: {TOKEN_STRING} kind: Secret metadata: annotations: kubernetes.io/service-account.name: sleep kubernetes.io/service-account.uid: 2c0d00e8-13a2-48d0-9ff8-f987f3325ecf creationTimestamp: \u0026#34;2022-06-14T03:01:35Z\u0026#34; name: sleep-token-gwhwd namespace: default resourceVersion: \u0026#34;244535398\u0026#34; uid: b8822ceb-9553-4a17-96dc-d525bbaed0e0 type: kubernetes.io/service-account-token Kubernetes manages the identity of Pods with Service Accounts and then specifies the permissions of Pods with a Service Account to the Kubernetes API using RBAC. A service account’s token is stored in a secret, which does not include a declaration of the node or pod where the workload is running. When a malicious actor steals a token, they gain full access to the account and can steal information or carry out sabotage under the guise of that user.\nA token can only be used to identify a workload in one cluster, but Istio supports multiple clusters and meshes, as well as Kubernetes environments and virtual machines. A unified workload identity standard can help here.\nAn Introduction to SPIFFE and SPIRE SPIFFE’s (Secure Production Identity Framework for Everyone) goal is to create a zero-trust, fully-identified data center network by establishing an open, unified workload identity standard based on the concept of zero-trust. SPIRE can rotate X.509 SVID certificates and secret keys on a regular basis. Based on administrator-defined policies, SPIRE can dynamically provision workload certificates and Istio can consume them. I’ll go over some of the terms associated with SPIFFE in a little more detail below.\nSPIRE (SPIFFE Runtime Environment) is a SPIFFE implementation that is ready for production. SVID (SPIFFE Verifiable Identity Document) is the document that a workload uses to prove its identity to a resource or caller. SVID contains a SPIFFE ID that represents the service’s identity. It uses an X.509 certificate or a JWT token to encode the SPIFFE ID in a cryptographically verifiable document. The SPIFFE ID is a URI that looks like this: spiffe://trust_domain/workload_identifier.\nSPIFFE and Zero Trust Security The essence of Zero Trust is identity-centric dynamic access control. SPIFFE addresses the problem of identifying workloads.\nWe might identify a workload using an IP address and port in the era of virtual machines, but IP address-based identification is vulnerable to multiple services sharing an IP address, IP address forgery, and oversized access control lists. Because containers have a short lifecycle in the Kubernetes era, instead of an IP address, we rely on a pod or service name. However, different clouds and software platforms approach workload identity differently, and there are compatibility issues. This is especially true in heterogeneous hybrid clouds, where workloads run on both virtual machines and Kubernetes. It is critical to establish a fine-grained, interoperable identification system at this point.\nUsing SPIRE for Authentication in Istio With the introduction of SPIRE to Istio, we can give each workload a unique identity, which is used by workloads in the service mesh for peer authentication, request authentication, and authorization policies. The SPIRE Agent issues SVIDs for workloads by communicating with a shared UNIX Domain Socket in the workload. The Envoy proxy and the SPIRE agent communicate through the Envoy SDS (Secret Discovery Service) API. Whenever an Envoy proxy needs to access secrets (certificates, keys, or anything else needed to do secure communication), it will talk to the SPIRE agent through Envoy’s SDS API.\nThe most significant advantage of SDS is the ease with which certificates can be managed. Without this feature, certificates would have to be created as a secret and then mounted into the agent container in a Kubernetes deployment. The secret must be updated, and the proxy container must be re-deployed if the certificate expires. Using SDS, Istio can push the certificates to all Envoy instances in the service mesh. If the certificate expires, the server only needs to push the new certificate to the Envoy instance; Envoy will use the new certificate right away, and the …","relpermalink":"/en/blog/why-istio-need-spire/","summary":"This article explains what SPIRE means for zero-trust architectures and why you would need SPIRE for authentication in Istio.","title":"Why Would You Need Spire for Authentication With Istio?"},{"content":" It’s been more than 5 years since Google, IBM, and Lyft unveiled the Istio open source project in May 2017. The Istio project has developed from a seed to a tall tree in these years. Many domestic books on the Istio service mesh were launched in the two years following the release of Istio 1.0 in 2018. My country is at the forefront of the world in the field of Istio book publishing.\nService mesh: one of the core technologies of cloud native Today, Istio is nearly synonymous with service mesh in China. The development of service mesh, as one of the core cloud-native technologies described by CNCF (Cloud Native Computing Foundation), has gone through the following stages.\n2017-2018: Exploratory Phase 2019-2020: Early Adopter Phase 2021 to present: Implementation on a large scale and ecological development stage Cloud native technology enables enterprises to design and deploy elastically scalable applications in new dynamic settings such as public, private, and hybrid clouds, according to the CNCF. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs are examples of cloud native technology.\nService mesh has been included to the CNCF definition of cloud native, indicating that it is one of the representative technologies of cloud native. Google is donating Istio to CNCF today, and we have reason to expect that as a CNCF project, Istio’s community will be more open, and its future development will be more smooth.\nService mesh and cloud native applications Cloud-native development is gaining traction. Despite the frequent emergence of new technologies and products, service mesh has maintained its place as “cloud-native network infrastructure” as part of the overall cloud-native technology stack throughout the past year. The cloud-native technology stack model is depicted in the diagram below, with representative technologies for each layer to define the standard. Service mesh and other cloud-native technologies complement each other as a new era of middleware emerges. Dapr (Distributed Application Runtime) defines the cloud-native middleware capability model, OAM defines the cloud-native application model, and so on, whereas service mesh Lattice defines a cloud-native seven-layer network model.\nCloud Native Application Model Why you need a service mesh Using a service mesh isn’t tantamount to abandoning Kubernetes; it just makes sense. The goal of Kubernetes is to manage application lifecycles through declarative configuration, whereas the goal of service mesh is to provide traffic control, security management, and observability amongst apps. How do you set up load balancing and flow management for calls between services after a robust microservice platform has been developed with Kubernetes?\nMany open source tools, including Istio, Linkerd, MOSN, and others, support Envoy’s xDS protocol. The specification of xDS is Envoy’s most significant contribution to service mesh or cloud native. Many various usage cases, such as API gateways, sidecar proxies in service meshes, and edge proxies, are derived from Envoy, which is simply a network proxy, a modern version of the proxy configured through the API.\nIn a nutshell, the move from Kubernetes to Istio was made for the following reasons.\nApplication life cycle management, specifically application deployment and management, is at the heart of Kubernetes (scaling, automatic recovery, and release). Kubernetes is a microservices deployment and management platform that is scalable and extremely elastic. Transparent proxy is the cornerstone of service mesh, which intercepts communication between microservices via sidecar proxy and then regulates microservice behavior via control plane settings. The deployment mode of service meshes has introduced new issues today. For service meshes, sidecar is no longer required, and an agentless service mesh based on gRPC is also being tested. xDS is a protocol standard for configuring service meshes, and a gRPC-based xDS is currently being developed. Kubernetes traffic management is decoupled with the service mesh. The kube-proxy component is not required to support traffic within the service mesh. The traffic between services is controlled by an abstraction close to the microservice application layer to achieve security and observability features. In Kubernetes, service mesh is an upper-level abstraction of service, and Serverless is the next stage, which is why Google released Knative based on Kubernetes and Istio following Istio. Open source in the name of the community The ServiceMesher community was founded in May 2018 with the help of Ant Financial. Following that, a tornado of service meshes erupted in China, and the community-led translation of Istio’s official documentation reached a fever pitch.\nI became aware of a dearth of Chinese resources for systematically teaching Istio over time, so in September 2018, I began to plan and create an Istio book, launching the Istio Handbook open source …","relpermalink":"/en/blog/istio-service-mesh-book/","summary":"By the Cloud Native Community(China)","title":"In-Depth Understanding of Istio: Announcing the Publication of a New Istio Book"},{"content":"This article will guide you on how to compile the Istio binaries and Docker images on macOS.\nBefore you begin Before we start, refer to the Istio Wiki , here is the information about my build environment.\nmacOS 12.3.1 Darwin AMD64 Docker Desktop 4.8.1(78998) Docker Engine v20.10.14 Start to compile First, download the Istio code from GitHub to the $GOPATH/src/istio.io/istio directory, and execute the commands below in that root directory.\nCompile into binaries Execute the following command to download the Istio dependent packages, which will be downloaded to the vendor directory.\ngo mod vendor Run the following command to build Istio:\nsudo make build If you do not run the command with sudo, you may encounter the following error.\nfatal: unsafe repository (\u0026#39;/work\u0026#39; is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work fatal: unsafe repository (\u0026#39;/work\u0026#39; is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work Makefile.core.mk:170: *** \u0026#34;TAG cannot be empty\u0026#34;. Stop. make: *** [build] Error 2 Even if you follow the prompts and run git config --global --add safe.directory /work, you will still get errors during compilation.\nThe compiled binary will be saved in out directory with the following directory structure.\nout ├── darwin_amd64 │ ├── bug-report │ ├── client │ ├── envoy │ ├── extauthz │ ├── install-cni │ ├── istio-cni │ ├── istio-cni-taint │ ├── istio-iptables │ ├── istio_is_init │ ├── istioctl │ ├── logs │ ├── operator │ ├── pilot-agent │ ├── pilot-discovery │ ├── release │ └── server └── linux_amd64 ├── envoy ├── envoy-centos ├── logs └── release It will build both the linux_amd64 and darwin_amd64 architectures binaries at the same time.\nCompile into Docker images Run the following command to compile Istio into a Docker image.\nsudo make docker The compilation will take about 3 to 5 minutes depending on your network. Once the compilation is complete, you will see the Docker image of Istio by running the following command.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE localhost:5000/app_sidecar_centos_7 latest 2044037df94b 51 seconds ago 524MB localhost:5000/app_sidecar_ubuntu_jammy latest 5d8ae5ed55b7 About a minute ago 362MB localhost:5000/proxyv2 latest d4679412385f About a minute ago 243MB localhost:5000/install-cni latest 78f46d5771d2 About a minute ago 270MB localhost:5000/istioctl latest c38130a5adc8 About a minute ago 190MB localhost:5000/pilot latest 2aa9185ec202 About a minute ago 190MB localhost:5000/app latest 473adafaeb8d About a minute ago 188MB localhost:5000/operator latest 9ac1fedcdd12 About a minute ago 191MB localhost:5000/ext-authz latest 1fb5aaf20791 About a minute ago 117MB localhost:5000/app_sidecar_debian_11 latest 61376a02b95d 2 minutes ago 407MB localhost:5000/app_sidecar_ubuntu_xenial latest 7e8efe666611 2 minutes ago 418MB You can change the image name and push it into your own container registry.\nSummary This is how to build Istio on macOS. If you have already downloaded the Docker image you need to build, the build will take less than a minute. It also takes only a few minutes to build Docker images.\nReference Using the Code Base - github.com ","relpermalink":"/en/blog/how-to-build-istio/","summary":"This article will guide you on how to compile the Istio binaries on macOS.","title":"How to Build Istio?"},{"content":"Updated on May 6, 2022\nBased on Istio version 1.13, this article will present the following.\nWhat is the sidecar pattern and what advantages does it have? How are the sidecar injections done in Istio? How does the sidecar proxy do transparent traffic intercepting? How is the traffic routed to upstream? The figure below shows how the productpage service requests access to http://reviews.default.svc.cluster.local:9080/ and how the sidecar proxy inside the reviews service does traffic blocking and routing forwarding when traffic goes inside the reviews service.\nIstio transparent traffic intercepting and traffic routing diagram At the beginning of the first step, the sidecar in the productpage pod has selected a pod of the reviews service to be requested via EDS, knows its IP address, and sends a TCP connection request.\nThere are three versions of the reviews service, each with an instance, and the sidecar work steps in the three versions are similar, as illustrated below only by the sidecar traffic forwarding step in one of the Pods.\nSidecar pattern Dividing the functionality of an application into separate processes running in the same minimal scheduling unit (e.g. Pod in Kubernetes) can be considered sidecar mode. As shown in the figure below, the sidecar pattern allows you to add more features next to your application without additional third-party component configuration or modifications to the application code.\nSidecar pattern The Sidecar application is loosely coupled to the main application. It can shield the differences between different programming languages and unify the functions of microservices such as observability, monitoring, logging, configuration, circuit breaker, etc.\nAdvantages of using the Sidecar pattern When deploying a service mesh using the sidecar model, there is no need to run an agent on the node, but multiple copies of the same sidecar will run in the cluster. In the sidecar deployment model, a companion container (such as Envoy or MOSN) is deployed next to each application’s container, which is called a sidecar container. The sidecar takes overall traffic in and out of the application container. In Kubernetes’ Pod, a sidecar container is injected next to the original application container, and the two containers share storage, networking, and other resources.\nDue to its unique deployment architecture, the sidecar model offers the following advantages.\nAbstracting functions unrelated to application business logic into a common infrastructure reduces the complexity of microservice code. Reduce code duplication in microservices architectures because it is no longer necessary to write the same third-party component profiles and code. The sidecar can be independently upgraded to reduce the coupling of application code to the underlying platform. iptables manipulation analysis In order to view the iptables configuration, we need to nsenter the sidecar container using the root user to view it, because kubectl cannot use privileged mode to remotely manipulate the docker container, so we need to log on to the host where the productpage pod is located.\nIf you use Kubernetes deployed by minikube, you can log directly into the minikube’s virtual machine and switch to root. View the iptables configuration that lists all the rules for the NAT (Network Address Translation) table because the mode for redirecting inbound traffic to the sidecar is REDIRECT in the parameters passed to the istio-iptables when the Init container is selected for the startup, so there will only be NAT table specifications in the iptables and mangle table configurations if TPROXY is selected. See the iptables command for detailed usage.\nWe only look at the iptables rules related to productpage below.\n# login to minikube, change user to root $ minikube ssh $ sudo -i # See the processes in the productpage pod\u0026#39;s istio-proxy container $ docker top `docker ps|grep \u0026#34;istio-proxy_productpage\u0026#34;|cut -d \u0026#34; \u0026#34; -f1` UID PID PPID C STIME TTY TIME CMD 1337 10576 10517 0 08:09 ? 00:00:07 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage.default --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istiod.istio-system.svc:15012 --zipkinAddress zipkin.istio-system:9411 --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --connectTimeout 10s --proxyAdminPort 15000 --concurrency 2 --controlPlaneAuthPolicy NONE --dnsRefreshRate 300s --statusPort 15020 --trust-domain=cluster.local --controlPlaneBootstrap=false 1337 10660 10576 0 08:09 ? 00:00:33 /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage.default --service-node sidecar~172.17.0.16~productpage-v1-7f44c4d57c-ksf9b.default~default.svc.cluster.local --max-obj-name-len 189 --local-address-ip-version v4 --log-format [Envoy (Epoch 0)] [%Y-%m-%d …","relpermalink":"/en/blog/sidecar-injection-iptables-and-traffic-routing/","summary":"Learn the sidecar pattern, transparent traffic intercepting and routing in Istio.","title":"Understanding the Sidecar Injection, Traffic Intercepting \u0026 Routing Process in Istio"},{"content":"This article will explain:\nThe sidecar auto-injection process in Istio The init container startup process in Istio The startup process of a Pod with Sidecar auto-injection enabled The following figure shows the components of a Pod in the Istio data plane after it has been started.\nIstio data plane pod Sidecar injection in Istio The following two sidecar injection methods are available in Istio.\nManual injection using istioctl. Kubernetes-based mutating webhook admission controller automatic sidecar injection method. Whether injected manually or automatically, SIDECAR’s injection process follows the following steps.\nKubernetes needs to know the Istio cluster to which the sidecar to be injected is connected and its configuration. Kubernetes needs to know the configuration of the sidecar container itself to be injected, such as the image address, boot parameters, etc. Kubernetes injects the above configuration into the side of the application container by the sidecar injection template and the configuration parameters of the above configuration-filled sidecar. The sidecar can be injected manually using the following command.\nistioctl kube-inject -f ${YAML_FILE} | kuebectl apply -f - This command is injected using Istio’s built-in sidecar configuration, see the Istio official website for details on how to use Istio below.\nWhen the injection is complete you will see that Istio has injected initContainer and sidecar proxy-related configurations into the original pod template.\nInit container The Init container is a dedicated container that runs before the application container is launched and is used to contain some utilities or installation scripts that do not exist in the application image.\nMultiple Init containers can be specified in a Pod, and if more than one is specified, the Init containers will run sequentially. The next Init container can only be run if the previous Init container must run successfully. Kubernetes only initializes the Pod and runs the application container when all the Init containers have been run.\nThe Init container uses Linux Namespace, so it has a different view of the file system than the application container. As a result, they can have access to Secret in a way that application containers cannot.\nDuring Pod startup, the Init container starts sequentially after the network and data volumes are initialized. Each container must be successfully exited before the next container can be started. If exiting due to an error will result in a container startup failure, it will retry according to the policy specified in the Pod’s restartPolicy. However, if the Pod’s restartPolicy is set to Always, the restartPolicy is used when the Init container failed.\nThe Pod will not become Ready until all Init containers are successful. The ports of the Init containers will not be aggregated in the Service. The Pod that is being initialized is in the Pending state but should set the Initializing state to true. The Init container will automatically terminate once it is run.\nSidecar injection example analysis For a detailed YAML configuration for the bookinfo applications, see bookinfo.yaml for the official Istio YAML of productpage in the bookinfo sample.\nThe following will be explained in the following terms.\nInjection of Sidecar containers Creation of iptables rules The detailed process of routing apiVersion: apps/v1 kind: Deployment metadata: name: productpage-v1 labels: app: productpage version: v1 spec: replicas: 1 selector: matchLabels: app: productpage version: v1 template: metadata: labels: app: productpage version: v1 spec: serviceAccountName: bookinfo-productpage containers: - name: productpage image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9080 volumeMounts: - name: tmp mountPath: /tmp volumes: - name: tmp emptyDir: {} Let’s see the productpage container’s Dockerfile .\nFROM python:3.7.4-slim COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt COPY test-requirements.txt ./ RUN pip install --no-cache-dir -r test-requirements.txt COPY productpage.py /opt/microservices/ COPY tests/unit/* /opt/microservices/ COPY templates /opt/microservices/templates COPY static /opt/microservices/static COPY requirements.txt /opt/microservices/ ARG flood_factor ENV FLOOD_FACTOR ${flood_factor:-0} EXPOSE 9080 WORKDIR /opt/microservices RUN python -m unittest discover USER 1 CMD [\u0026#34;python\u0026#34;, \u0026#34;productpage.py\u0026#34;, \u0026#34;9080\u0026#34;] We see that ENTRYPOINT is not configured in Dockerfile, so CMD’s configuration python productpage.py 9080 will be the default ENTRYPOINT, keep that in mind and look at the configuration after the sidecar injection.\n$ istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml We intercept only a portion of the YAML configuration that is part of the Deployment configuration associated with productpage.\ncontainers: - image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 # application image name: …","relpermalink":"/en/blog/istio-pod-process-lifecycle/","summary":"This article will explain Istio's Init container, Pod internal processes and the startup process.","title":"Istio Data Plane Pod Startup Process Explained"},{"content":"iptables is an important feature in the Linux kernel and has a wide range of applications. iptables is used by default in Istio for transparent traffic hijacking. Understanding iptables is very important for us to understand how Istio works. This article will give you a brief introduction to iptbles.\niptables introduction iptables is a management tool for netfilter, the firewall software in the Linux kernel. netfilter is located in the user space and is part of netfilter. netfilter is located in the kernel space and has not only network address conversion, but also packet content modification and packet filtering firewall functions.\nBefore learning about iptables for Init container initialization, let’s go over iptables and rule configuration.\nThe following figure shows the iptables call chain.\niptables 调用链 iptables The iptables version used in the Init container is v1.6.0 and contains 5 tables.\nRAW is used to configure packets. Packets in RAW are not tracked by the system. The filter is the default table used to house all firewall-related operations. NAT is used for network address translation (e.g., port forwarding). Mangle is used for modifications to specific packets (refer to corrupted packets). Security is used to force access to control network rules. Note: In this example, only the NAT table is used.\nThe chain types in the different tables are as follows.\nRule name raw filter nat mangle security PREROUTING ✓ ✓ ✓ INPUT ✓ ✓ ✓ OUTPUT ✓ ✓ ✓ ✓ ✓ POSTROUTING ✓ ✓ FORWARD ✓ ✓ ✓ Understand iptables rules View the default iptables rules in the istio-proxy container, the default view is the rules in the filter table.\n$ iptables -L -v Chain INPUT (policy ACCEPT 350K packets, 63M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 18M packets, 1916M bytes) pkts bytes target prot opt in out source destination We see three default chains, INPUT, FORWARD, and OUTPUT, with the first line of output in each chain indicating the chain name (INPUT/FORWARD/OUTPUT in this case), followed by the default policy (ACCEPT).\nThe following is a proposed structure diagram of iptables, where traffic passes through the INPUT chain and then enters the upper protocol stack, such as:\niptables chains Multiple rules can be added to each chain and the rules are executed in order from front to back. Let’s look at the table header definition of the rule.\nPKTS: Number of matched messages processed bytes: cumulative packet size processed (bytes) Target: If the message matches the rule, the specified target is executed. PROT: Protocols such as TDP, UDP, ICMP, and ALL. opt: Rarely used, this column is used to display IP options. IN: Inbound network interface. OUT: Outbound network interface. source: the source IP address or subnet of the traffic, the latter being anywhere. destination: the destination IP address or subnet of the traffic, or anywhere. There is also a column without a header, shown at the end, which represents the options of the rule, and is used as an extended match condition for the rule to complement the configuration in the previous columns. prot, opt, in, out, source and destination and the column without a header shown after destination together form the match rule. TARGET is executed when traffic matches these rules.\nTypes supported by TARGET\nTarget types include ACCEPT, REJECT, DROP, LOG, SNAT, MASQUERADE, DNAT, REDIRECT, RETURN or jump to other rules, etc. You can determine where the telegram is going by executing only one rule in a chain that matches in order, except for the RETURN type, which is similar to the return statement in programming languages, which returns to its call point and continues to execute the next rule.\nFrom the output, you can see that the Init container does not create any rules in the default link of iptables, but instead creates a new link.\nSummary With the above brief introduction to iptables, you have understood how iptables works, the rule chain and its execution order.\n","relpermalink":"/en/blog/understanding-iptables/","summary":"This article will give you a brief introduction to iptables, its tables and the order of execution.","title":"Understanding IPTables"},{"content":"See the cloud native public library at: https://lib.jimmysong.io/ The cloud native public library project is a documentation project built using the Wowchemy theme, open sourced on GitHub .\nI have also adjusted the home page, menu and directory structure of the site, and the books section of the site will be maintained using the new theme.\nCloud native library positioning The cloud native public library is a collection of cloud native related books and materials published and translated by the author since 2017, and is a compendium and supplement to the dozen or so books already published. The original materials will continue to be published in the form of GitBooks, and the essence and related content will be sorted into the cloud native public library through this project.\nIn addition, the events section of this site has been revamped and moved to a new page .\n","relpermalink":"/en/notice/cloud-native-public-library/","summary":"A one-stop cloud native library that is a compendium of published materials.","title":"Announcement of Cloud Native Library"},{"content":"In my last two blogs:\nSidecar injection, transparent traffic hijacking , and routing process in Istio explained in detail Traffic types and iptables rules in Istio sidecar explained I gave you a detailed overview of the traffic in the Istio data plane, but the data plane does not exist in isolation. This article will show you the ports and their usages for each component of both the control plane and data plane in Istio, which will help you understand the relationship between these flows and troubleshoot them.\nOverview Firstly, I will show you a global schematic. The following figure shows the components of a sidecar in the Istio data plane, and the objects that interact with it.\nIstio components We can use the nsenter command to enter the namespace of the productpage Pod of the Bookinfo example and see the information about the ports it is listening on internally.\nIstio sidecar ports From the figure, we can see that besides the port 9080 that the productpage application listens to, the Sidecar container also listens to a large number of other ports, such as 15000, 15001, 15004, 15006, 15021, 15090, etc. You can learn about the ports used in Istio in the Istio documentation .\nLet’s go back into the productpage Pod and use the lsof -i command to see the ports it has open, as shown in the following figure.\nProductpage Pod ports We can see that there is a TCP connection established between the pilot-agent and istiod, the port in the listening described above, and the TCP connection established inside the Pod, which corresponds to the figure at the beginning of the article.\nThe root process of the Sidecar container (istio-proxy) is pilot-agent, and the startup command is shown below.\nInternal procecces in Sidecar As we can see from the figure, the PID of its pilot-agent process is 1, and it forked the Envoy process.\nCheck the ports it opens in Istiod, as shown in the figure below.\nIstiod ports We can see the ports that are listened to, the inter-process and remote communication connections.\nPorts usage overview These ports can play a pivotal role when you are troubleshooting. They are described below according to the component and function in which the port is located.\nPorts in Istiod The ports in Istiod are relatively few and single-function.\n9876: ControlZ user interface, exposing information about Istiod’s processes 8080: Istiod debugging port, through which the configuration and status information of the grid can be queried 15010: Exposes the xDS API and issues plain text certificates 15012: Same functionality as port 15010, but uses TLS communication 15014: Exposes control plane metrics to Prometheus 15017: Sidecar injection and configuration validation port Ports in sidecar From the above, we see that there are numerous ports in the sidecar.\n15000: Envoy admin interface, which you can use to query and modify the configuration of Envoy Proxy. Please refer to Envoy documentation for details. 15001: Used to handle outbound traffic. 15004: Debug port (explained further below). 15006: Used to handle inbound traffic. 15020: Summarizes statistics, perform health checks on Envoy and DNS agents, and debugs pilot-agent processes, as explained in detail below. 15021: Used for sidecar health checks to determine if the injected Pod is ready to receive traffic. We set up the readiness probe on the /healthz/ready path on this port, and Istio hands off the sidecar readiness checks to kubelet. 15053: Local DNS proxy for scenarios where the cluster’s internal domain names are not resolved by Kubernetes DNS. 15090: Envoy Prometheus query port, through which the pilot-agent will scratch metrics. The above ports can be divided into the following categories.\nResponsible for inter-process communication, such as 15001, 15006, 15053 Health check and information statistics, e.g. 150021, 15090 Debugging: 15000, 15004 Let’s look at the key ports in detail.\n15000 15000 is Envoy’s Admin interface, which allows us to modify Envoy and get a view and query metrics and configurations.\nThe Admin interface consists of a REST API with multiple endpoints and a simple user interface. You can enable the Envoy Admin interface view in the productpage Pod using the following command:\nkubectl -n default port-forward deploy/productpage-v1 15000 Visit http://localhost:15000 in your browser and you will see the Envoy Admin interface as shown below.\nEnvoy Admin interface 15004 With the pilot-agent proxy istiod debug endpoint on port 8080, you can access localhost’s port 15004 in the data plane Pod to query the grid information, which has the same effect as port 8080 below.\n8080 You can also forward istiod port 8080 locally by running the following command:\nkubectl -n istio-system port-forward deploy/istiod 8080 Visit http://localhost:8080/debug in your browser and you will see the debug endpoint as shown in the figure below.\nPilot Debug Console Of course, this is only one way to get the mesh information and debug the mesh, you can also use istioctl …","relpermalink":"/en/blog/istio-components-and-ports/","summary":"This article will introduce you to the various ports and functions of the Istio control plane and data plane.","title":"Istio Component Ports and Functions in Details"},{"content":"As we know that Istio uses iptables for traffic hijacking, where the iptables rule chains has one called ISTIO_OUTPUT, which contains the following rules.\nRule target in out source destination 1 RETURN any lo 127.0.0.6 anywhere 2 ISTIO_IN_REDIRECT any lo anywhere !localhost owner UID match 1337 3 RETURN any lo anywhere anywhere !owner UID match 1337 4 RETURN any any anywhere anywhere owner UID match 1337 5 ISTIO_IN_REDIRECT any lo anywhere !localhost owner GID match 1337 6 RETURN any lo anywhere anywhere !owner GID match 1337 7 RETURN any any anywhere anywhere owner GID match 1337 8 RETURN any any anywhere localhost 9 ISTIO_REDIRECT any any anywhere anywhere The sidecar applies these rules to deal with different types of traffic. This article will show you the six types of traffic and their iptables rules in Istio sidecar.\niptables Traffic Routing in Sidecar The following list summarizes the six types of traffic in Sidecar.\nRemote service accessing local service: Remote Pod -\u0026gt; Local Pod Local service accessing remote service: Local Pod -\u0026gt; Remote Pod Prometheus crawling metrics of local service: Prometheus -\u0026gt; Local Pod Traffic between Local Pod service: Local Pod -\u0026gt; Local Pod Inter-process TCP traffic within Envoy Sidecar to Istiod traffic The following will explain the iptables routing rules within Sidecar for each scenario, which specifies which rule in ISTIO_OUTPUT is used for routing.\nType 1: Remote Pod -\u0026gt; Local Pod The following are the iptables rules for remote services, applications or clients accessing the local pod IP of the data plane.\nRemote Pod -\u0026gt; RREROUTING -\u0026gt; ISTIO_INBOUND -\u0026gt; ISTIO_IN_REDIRECT -\u0026gt; Envoy 15006 (Inbound) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 1 -\u0026gt; POSTROUTING -\u0026gt; Local Pod\nWe see that the traffic only passes through the Envoy 15006 Inbound port once. The following diagram shows this scenario of the iptables rules.\nRemote Pod to Local Pod Type 2: Local Pod -\u0026gt; Remote Pod The following are the iptables rules that the local pod IP goes through to access the remote service.\nLocal Pod-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 9 -\u0026gt; ISTIO_REDIRECT -\u0026gt; Envoy 15001 (Outbound) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 4 -\u0026gt; POSTROUTING -\u0026gt; Remote Pod\nWe see that the traffic only goes through the Envoy 15001 Outbound port.\nLocal Pod to Remote Pod The traffic in both scenarios above passes through Envoy only once because only one scenario occurs in that Pod, sending or receiving requests.\nType 3: Prometheus -\u0026gt; Local Pod Prometheus traffic that grabs data plane metrics does not have to go through the Envoy proxy.\nThese traffic pass through the following iptables rules.\nPrometheus-\u0026gt; RREROUTING -\u0026gt; ISTIO_INBOUND (traffic destined for ports 15020, 15090 will go to INPUT) -\u0026gt; INPUT -\u0026gt; Local Pod\nPrometheus to Local Pod Type 4: Local Pod -\u0026gt; Local Pod A Pod may simultaneously have two or more services. If the Local Pod accesses a service on the current Pod, the traffic will go through Envoy 15001 and Envoy 15006 ports to reach the service port of the Local Pod.\nThe iptables rules for this traffic are as follows.\nLocal Pod-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 9 -\u0026gt; ISTIO_REDIRECT -\u0026gt; Envoy 15001（Outbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 2 -\u0026gt; ISTIO_IN_REDIRECT -\u0026gt; Envoy 15006（Inbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 1 -\u0026gt; POSTROUTING -\u0026gt; Local Pod\nLocal Pod to Local Pod Type 5: Inter-process TCP traffic within Envoy Envoy internal processes with UID and GID 1337 will communicate with each other using lo NICs and localhost domains.\nThe iptables rules that these flows pass through are as follows.\nEnvoy process (Localhost) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 8 -\u0026gt; POSTROUTING -\u0026gt; Envoy process (Localhost)\nEnvoy inter-process TCP traffic Type 6: Sidecar to Istiod traffic Sidecar needs access to Istiod to synchronize its configuration so that Envoy will have traffic sent to Istiod.\nThe iptables rules that this traffic passes through are as follows.\npilot-agent process -\u0026gt; OUTPUT -\u0026gt; Istio_OUTPUT RULE 9 -\u0026gt; Envoy 15001 (Outbound Handler) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 4 -\u0026gt; POSTROUTING -\u0026gt; Istiod\nSidecar to Istiod Summary All the sidecar proxies that Istio injects into the Pod or installed in the virtual machine form the data plane of the service mesh, which is also the main workload location of Istio. In my next blog, I will take you through the ports of each component in Envoy and their functions, so that we can have a more comprehensive understanding of the traffic in Istio.\n","relpermalink":"/en/blog/istio-sidecar-traffic-types/","summary":"This article will show you the six traffic types and their iptables rules in Istio sidecar, and take you through the whole diagram in a schematic format.","title":"Traffic Types and Iptables Rules in Istio Sidecar Explained"},{"content":"Istio 1.13 is the first release of 2022, and, not surprisingly, the Istio team will continue to release new versions every quarter. Overall, the new features in this release include:\nSupport for newer versions of Kubernetes New API – ProxyConfig, for configuring sidecar proxies Improved Telemetry API Support for hostname-based load balancers with multiple network gateways Support for Kubernetes Versions I often see people asking in the community which Istio supports Kubernetes versions. Istio’s website has a clear list of supported Kubernetes versions. You can see here that Istio 1.13 supports Kubernetes versions 1.20, 1.21, 1.22, and 1.23, and has been tested but not officially supported in Kubernetes 1.16, 1.17, 1.18, 1.19.\nWhen configuring Istio, there are a lot of checklists. I noted them all in the Istio cheatsheet . There are a lot of cheat sheets about configuring Istio, using resources, dealing with everyday problems, etc., in this project, which will be online soon, so stay tuned.\nThe following screenshot is from the Istio cheatsheet website, it shows the basic cheat sheet for setting up Istio.\nIstio cheatsheet Introducing the new ProxyConfig API Before Istio version 1.13, if you wanted to customize the configuration of the sidecar proxy, there were two ways to do it.\nMeshConfig\nUse MeshConfig and use IstioOperator to modify it at the Mesh level. For example, use the following configuration to alter the default discovery port for istiod.\napVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: defaultConfig: discoveryAddress: istiod:15012 Annotation in the Pods\nYou can also use annotation at the Pod level to customize the configuration. For example, you can add the following annotations to Pod to modify the default port for istiod of the workload:\nanannotations: proxy.istio.io/config: | discoveryAddress: istiod:15012 When you configure sidecar in either of these ways, the fields set in annotations will completely override the default fields in MeshConfig. Please refer to the Istio documentation for all configuration items of ProxyConfig.\nThe new API – ProxyConfig\nBut in 1.13, a new top-level custom resource, ProxyConfig, has been added, allowing you to customize the configuration of your sidecar proxy in one place by specifying a namespace and using a selector to select the scope of the workload, just like any other CRD. Istio currently has limited support for this API, so please refer to the Istio documentation for more information on the ProxyConfig API.\nHowever, no matter which way you customize the configuration of the sidecar proxy, it does not take effect dynamically and requires a workload restart to take effect. For example, for the above configuration, because you changed the default port of istiod, all the workloads in the mesh need to be restarted before connecting to the control plane.\nTelemetry API MeshConfig customized extensions and configurations in the Istio mesh. The three pillars of observability– Metrics, Telemetry, and Logging– can each be docked to different providers. The Telemetry API gives you a one-stop place for flexible configuration of them. Like the ProxyConfig API, the Telemetry API follows the configuration hierarchy of Workload Selector \u0026gt; Local Namespace \u0026gt; Root Configuration Namespace. The API was introduced in Istio 1.11 and has been further refined in that release to add support for OpenTelemetry logs, filtered access logs, and custom tracing service names. See Telemetry Configuration for details.\nAutomatic resolution of multi-network gateway hostnames In September 2021, a member of the Istio community reported an issue with the EKS load balancer failing to resolve when running multi-cluster Istio in AWS EKS. Workloads that cross cluster boundaries need to be communicated indirectly through a dedicated east-west gateway for a multi-cluster, multi-network mesh. You can follow the instructions on Istio’s website to configure a multi-network, primary-remote cluster, and Istio will automatically resolve the IP address of the load balancer based on the hostname.\nIstio 1.13.1 fixing the critical security vulnerabilities Istio 1.13.1 was released to fix a known critical vulnerability that could lead to an unauthenticated control plane denial of service attack.\nThe figure below shows a multi-cluster primary-remote mesh where istiod exposes port 15012 to the public Internet via a gateway so that a pod on another network can connect to it.\nMulti-network Mesh When installing a multi-network, primary-remote mode Istio mesh, for a remote Kubernetes cluster to access the control plane, an east-west Gateway needs to be installed in the Primary cluster, exposing port 15012 of the control plane istiod to the Internet. An attacker could send specially crafted messages to that port, causing the control plane to crash. If you set up a firewall to allow traffic from only some IPs to access this port, you will be able to reduce the impact of the problem. It is …","relpermalink":"/en/blog/what-is-new-in-istio-1-13/","summary":"In February 2022, Istio released 1.13.0 and 1.13.1. This blog will give you an overview of what’s new in these two releases.","title":"What's New in Istio 1.13?"},{"content":"Join a team of world-class engineers working on the next generation of networking services using Istio, Envoy, Apache SkyWalking and a few of the open projects to define the next generation of cloud native network service.\nIstio upstream contributor: Golang We are looking for engineers with strong distributed systems experience to join our team. We are building a secure, robust, and highly available service mesh platform for mission critical enterprise applications spanning both legacy and modern infrastructure. This is an opportunity to dedicate a significant amount of contribution to Istio upstream on a regular basis. If you are a fan of Istio and would like to increase your contribution on a dedicated basis, this would be an opportunity for you.\nRequirements\nFundamentals-based problem solving skills; Drive decision by function, first principles based mindset. Demonstrate bias-to-action and avoid analysis-paralysis; Drive action to the finish line and on time. You are ego-less when searching for the best ideasIntellectually curious with a penchant for seeing opportunities in ambiguity Understands the difference between attention to detail vs. detailed - oriented Values autonomy and results over process You contribute effectively outside of your specialty Experience building distributed system platforms using Golang Familiarity with Kubernetes, service mesh technologies such as Istio and Envoy Excellent understanding of networking protocols, concepts, consistency properties of distributed systems, techniques to identify and reconcile configuration drifts Experience contributing to open source projects is a plus. Familiarity with WebAssembly is a plus. Familiarity with Golang, hardware/software load balancers (F5, NGINX), HSM modules, Active Directory/LDAP is a plus. We encourage written and asynchronous communication in English, and proficient oral English is not required.\nDistributed Systems Engineer, Enterprise Infrastructure (Data plane) GoLang or C++ Developers Seeking backend software engineers experienced in building distributed systems using Golang and gRPC. We are building a secure, and highly available service mesh platform for mission-critical enterprise applications for Fortune 500 companies, spanning both the legacy and modern infrastructure. Should possess strong fundamentals in distributed systems and networking. Familiarity with technologies like Kubernetes, Istio, and Envoy, as well as open contributions would be a plus.\nRequirements\nExperience building distributed system platforms using C++, Golang, and gRPC. Familiarity with Kubernetes, service mesh technologies such as Istio and Envoy. Excellent understanding of networking protocols, concepts, consistency properties of distributed systems, techniques to identify and reconcile configuration drifts. Experience contributing to open source projects is a plus. Familiarity with the following is a plus: WebAssembly, Authorization: NGAC, RBAC, ABAC Familiarity with hardware/software load balancers (F5, NGINX), HSM modules, Active Directory/LDAP is a plus. Site Reliability Engineer, SRE Site Reliability Engineering (SRE) combines software and systems engineering to build and run scalable, massively distributed, fault-tolerant systems. As part of the team, you will be working on ensuring that Tetrate’s platform has reliability/uptime appropriate to users’ needs as well as a fast rate of improvement. Additionally, much of our engineering effort focuses on building infrastructure, improving the platform troubleshooting abilities, and eliminating toil through automation.\nRequirements\nSystematic problem-solving approach, coupled with excellent communication skills and a sense of ownership/finish and self-directed drive. Strong fundamentals in operating, debugging, and troubleshooting distributed systems(stateful and/or stateless) and networking. Familiarity with Kubernetes, service mesh technologies such as Istio and EnvoyAbility to debug, optimize code, and automate routine tasks. Experience programming in at least one of the following languages: C++, Rust, Python, Go. Familiarity with the concepts of quantifying failure and availability in a prescriptive manner using SLOs and SLIs. Experience in performance analysis and tuning is a plus. Location Worldwide\nWe are remote with presence in China, Indonesia, India, Japan, U.S., Canada, Ireland, the Netherlands, Spain, and Ukraine.\nPlease send GitHub or online links that showcase your code style along with your resume to careers@tetrate.io .\nAbout Tetrate Powered by Envoy and Istio, its ﬂagship product, Tetrate Service Bridge, enables bridging traditional and modern workloads. Customers can get consistent baked-in observability, runtime security and traffic management for all their workloads, in any environment.\nIn addition to the technology, Tetrate brings a world-class team that leads the open Envoy and Istio projects, providing best practices and playbooks that enterprises can use to modernize their …","relpermalink":"/en/notice/tetrate-recruit/","summary":"Remotely worldwide","title":"The Enterprise Service Mesh company Tetrate is hiring"},{"content":"As the service mesh architecture concept gains traction and the scenarios for its applications emerge, there is no shortage of discussions about it in the community. I have worked on service mesh with the community for 4 years now, and will summarize the development of service mesh in 2021 from this perspective. Since Istio is the most popular service mesh, this article will focus on the technical and ecological aspects of Istio.\nService mesh: a critical tech for Cloud Native Infrastructure As one of the vital technologies defined by CNCF for cloud native, Istio has been around for five years now. Their development has gone through the following periods.\nExploration phase: 2017-2018 Early adopter phase: 2019-2020 Large-scale landing and ecological development phase: 2021-present Service mesh has crossed the “chasm”(refer Crossing the Chasm theory) and is in between the “early majority” and “late majority” phases of adoption. Based on feedback from the audience of Istio Weekly, users are no longer blindly following new technologies for experimentation and are starting to consider whether they need them in their organization dialectically.\nCross the chasm While new technologies and products continue to emerge, the service mesh, as part of the cloud native technology stack, has continued to solidify its position as the “cloud native network infrastructure” over the past year. The diagram below illustrates the cloud native technology stack model, where each layer has several representative technologies that define the standard. As new-age middleware, the service mesh mirrors other cloud native technologies, such as Dapr (Distributed Application Runtime), which represents the capability model for cloud native middleware, OAM , which defines the cloud native application model, and the service mesh, which defines the L7 network model.\nCloud Native Stack A layered view of the cloud native application platform technology stack\nOptimizing the mesh for large scale production applications with different deployment models Over the past year, the community focused on the following areas.\nPerformance optimization: performance issues of service mesh in large-scale application scenarios. Protocol and extensions: enabling service mesh to support arbitrary L7 network protocols. Deployment models: Proxyless vs. Node model vs. Sidecar model. eBPF: putting some of the service mesh’s capabilities to the kernel layer. Performance optimization Istio was designed to serve service to service traffic by “proto-protocol forwarding”. The goal is making the service mesh as “transparent” as possible to applications. Thus using IPtables to hijack the traffic, according to the community-provided test results Istio 1.2 adds only 3 ms to the baseline latency for a mesh with 1000 RPS on 16 connections. However, because of issues inherent in the IPtables conntrack module, Istio’s performance issues begin to emerge as the mesh size increases. To optimize the performance of the Istio sidecar for resource usage and network latency, the community gave the following solutions.\nSidecar configuration: By configuring service dependencies manually or by adding an Operator to the control plane, the number of service configurations sent to Sidecar can be reduced, thus reducing the resource footprint of the data plane; for more automatic and intelligent configuration of Sidecar, the open source projects Slime and Aeraki both offer their innovative configuration loading solutions. The introduction of eBPF: eBPF can be a viable solution to optimize the performance of the service mesh. Some Cilium-based startups even radically propose to use eBPF to replace the Sidecar proxy completely. Still, the Envoy proxy/xDS protocol has become the proxy for the service mesh implementation and supports the Layer 7 protocol very well. We can use eBPF to improve network performance, but complex protocol negotiation, parsing, and user scaling remain challenging to implement on the user side. Protocol and extensions Extensibility of Istio has always been a significant problem, and there are two aspects to Istio’s extensibility.\nProtocol level: allowing Istio to support all L7 protocols Ecological: allowing Istio to run more extensions Istio uses Envoy as its data plane. Extending Istio is essentially an extension of Envoy’s functionality. Istio’s official solution is to use WebAssembly, and in Istio 1.12, the Wasm plugin configuration API was introduced to extend the Istio ecosystem. Istio’s extension mechanism uses the Proxy-Wasm Application Binary Interface (ABI) specification to provide a set of proxy-independent streaming APIs and utilities that can be implemented in any language with an appropriate SDK. Today, Proxy-Wasm’s SDKs are AssemblyScript (similar to TypeScript), C++, Rust, Zig, and Go (using the TinyGo WebAssembly System Interface).\nThere are still relatively few WebAssembly extensions available, and many enterprises choose to customize their CRD and build a …","relpermalink":"/en/blog/service-mesh-in-2021/","summary":"A review of the development of Service Mesh in 2021.","title":"Service Mesh in 2021: The Ecosystem Is Emerging"},{"content":"It’s been more than four years since Istio launched in May 2017, and while the project has had a strong following on GitHub and 10+ releases, its growing open-source ecosystem is still in its infancy.\nRecently added support for WebAssembly extensions has made the most popular open source service mesh more extensible than ever. This table lists the open-source projects in the Istio ecosystem as of November 11, 2021, sorted by open-source date. These projects enhance the Istio service mesh with gateways, extensions, utilities, and more. In this article, I’ll highlight the two new projects in the category of extensions.\nProject Value Relationship with Istio Category Launch Date Dominant company Number of stars Envoy Cloud native high-performance edge/middle-service proxy The default data plane proxy September 2016 Lyft 18700 Istio Connection, secure, control, and observation services. Control plane service mesh May 2017 Google 29100 Emissary Gateway Kubernetes native API gateway for microservices, built on Envoy Connectable to Istio gateway February 2018 Ambassador 3600 APISIX Cloud native API gateways It can run as a data plane for Istio or as a gateway on its own gateway June 2019 API7 8100 MOSN Cloud native edge gateways \u0026amp; agents Available as Istio data plane proxy December 2019 Ant 3500 Slime Intelligent service mesh manager based on Istio Adding a management plane to Istio extensions January 2021 NetEase 236 GetMesh Istio integration and command-line management tools Utility for Istio multi-version management tools February 2021 Tetrate 95 Aeraki Manage any of Istio’s seven layers of load Extended multi-protocol support extensions March 2021 Tencent 330 Layotto Cloud native application runtime Using as a data plane for Istio runtime June 2021 Ant 393 Hango Gateway API gateways built on Envoy and Istio Integrates with Istio gateway August 2021 NetEase 253 Slime: an intelligent service mesh manager for Istio Slime is an Istio-based, intelligent mesh manager open-sourced by NetEase’s microservices team. Based on the Kubernetes Operator implementation, Slime can be used as a CRD manager that seamlessly interfaces with Istio without needing any customization or definition of dynamic service governance policies. This achieves automatic and convenient use of Istio and Envoy’s advanced features.\nSlime addresses the following issues:\nImplementing higher-level extensions in Istio. For example, extending the HTTP plugin; adaptive traffic limiting based on the resource usage of the service. Poor performance arising from Istio sending all the configurations within the mesh to each sidecar proxy. Slime solves these problems by building an Istio management plane. Its main purpose are\nto build a pluggable controller to facilitate the extension of new functions. to obtain data by listening to the data plane to intelligently generate the configuration for Istio. to build a higher-level CRD for the user to configure, which Slime converts into an Istio configuration. The following diagram shows the flow chart of Istio as an Istio management plane.\nSlime architecture The specific steps for Slime to manage Istio are as follows.\nSlime operator completes the initialization of Slime components in Kubernetes based on the administrator’s configuration. Developers create configurations that conform to the Slime CRD specification and apply them to Kubernetes clusters. Slime queries the monitoring data of the relevant service stored in Prometheus and converts the Slime CRD into an Istio CRD, in conjunction with the configuration of the adaptive part of the Slime CRD while pushing it to the Global Proxy. Istio listens for the creation of Istio CRDs. Istio pushes the configuration information of the Sidecar Proxy to the corresponding Sidecar Proxy in the data plane. The diagram below shows the internal architecture of Slime.\nSlime Internal We can divide Slime internally into three main components.\nslime-boot: operator for deploying Slime modules on Kubernetes. slime-controller: the core component of Slime that listens to the Slime CRD and converts it to an Istio CRD. slime-metric: the component used to obtain service metrics information. slime-controller dynamically adjusts service governance rules based on the information it receives. The following diagram shows the architecture of Slime Adaptive Traffic Limiting. Slime smart limiter Slime dynamically configures traffic limits by interfacing with the Prometheus metric server to obtain real-time monitoring.\nSlime’s adaptive traffic limitation process has two parts: one that converts SmartLimiter to EnvoyFilter and the other that monitors the data. Slime also provides an external monitoring data interface (Metric Discovery Server) that allows you to sync custom monitoring metrics to the traffic limiting component via MDS.\nThe CRD SmartLimiter created by Slime is used to configure adaptive traffic limiting. Its configuration is close to natural semantics, e.g., if you want to trigger an …","relpermalink":"/en/blog/istio-extensions-slime-and-aeraki/","summary":"In this article, I’ll introduce you two Istio extension projects: Aeraki and Slime.","title":"Introducing Slime and Aeraki in the Evolution of Istio Open-Source Ecosystem"},{"content":"You can use Istio to do multi-cluster management , API Gateway , and manage applications on Kubernetes or virtual machines . In my last blog , I talked about how service mesh is an integral part of cloud native applications. However, building infrastructure can be a big deal. There is no shortage of debate in the community about the practicability of service mesh and Istio– here’s a list of common questions and concerns, and how to address them.\nIs anyone using Istio in production? What is the impact on application performance due to the many resources consumed by injecting sidecar into the pod? Istio supports a limited number of protocols; is it scalable? Will Istio be manageable? – Or is it too complex, old services too costly to migrate, and the learning curve too steep? I will answer each of these questions below.\nIstio is architecturally stable, production-ready, and ecologically emerging Istio 1.12 was just released in November – and has evolved significantly since the explosion of service mesh in 2018 (the year Istio co-founders established Tetrate). Istio has a large community of providers and users . The Istio SIG of Cloud Native Community has held eight Istio Big Talk (Istio 大咖说) , with Baidu, Tencent, NetEase, Xiaohongshu(小红书), and Xiaodian Technology(小电科技) sharing their Istio practices. According to CNCF Survey Report 2020 , about 50% of the companies surveyed are using a service mesh in production or planning to in the next year, and about half (47%) of organizations using a service mesh in production are using Istio.\nMany companies have developed extensions or plugins for Istio, such as Ant, NetEase, eBay, and Airbnb. Istio’s architecture has been stable since the 1.5 release, and the release cycle is fixed quarterly, with the current project’s main task being Day-2 Operations.\nThe Istio community has also hosted various events, with the first IstioCon in March 2021, the Istio Meetup China in Beijing in July, and the Service Mesh Summit 2022 in Shanghai in January 2022.\nSo we can say that the Istio architecture is stable and production-ready, and the ecosystem is budding.\nThe impact of service mesh on application performance A service mesh uses iptables to do traffic hijacking by default to be transparent to applications. When the number of services is large, there are a lot of iptables rules that affect network performance. You can use techniques like eBPF to provide application performance, but the method requires a high version of the operating system kernel, which few enterprises can achieve.\nIstio DNS In the early days, Istio distributed the routing information of all services in the mesh to all proxy sidecars, which caused sidecar s to take up a lot of resources. Aeraki and Slime can achieve configuration lazy loading. We will introduce these two open-source projects in the Istio open-source ecosystem.\nFinally, there is a problem related to Sidecar proxy operation and maintenance: upgrading all Envoy proxies while ensuring constant traffic. A solution is using the SidecarSet resource in the open-source project OpenKruise .\nThe resource consumption and network latency associated with the introduction of Sidecar are also within reasonable limits, as you can see from the service mesh benchmark performance tests .\nExtending the Istio service mesh The next question is about extending the Istio service mesh. The current solution given by the Istio community is to use WebAssembly , an extension that is still relatively little used in production by now and has performance concerns. Most of the answers I’ve observed are CRDs that build a service mesh management plane based on Istio.\nAlso, making Istio support heterogeneous environments for all workloads, such as virtual machines and containers, is in strong demand for end-users. It allows them to migrate applications from traditional loads to cloud native easily. Finally, hybrid cloud traffic management for multiple clusters and meshes is a more advanced requirement.\nSteep learning curve Many people complain that Istio has too little learning material. Istio has been open source for four years, and there are a lot of learning resources now:\nIstio Documentation IstioCon 2021 Istio Big Talk/Istio Weekly Istio Fundamentals Course Certified Istio Administrator Yes, Istio is complex, but it’s been getting more and more manageable with every release. In my next blog, I will introduce you to two open source projects that extend Istio and give you some insight into what’s going on in the Istio community.\n","relpermalink":"/en/blog/the-debate-in-the-community-about-istio-and-service-mesh/","summary":"There is no shortage of debate in the community about the practicability of service mesh and Istio – here’s a list of common questions and concerns, and how to address them.","title":"The Debate in the Community About Istio and Service Mesh"},{"content":"If you don’t know what Istio is, you can read my previous articles below:\nWhat Is Istio and Why Does Kubernetes Need it? Why do you need Istio when you already have Kubernetes? This article will explore the relationship between service mesh and cloud native.\nService mesh – the product of the container orchestration war If you’ve been following the cloud-native space since its early days, you’ll remember the container orchestration wars of 2015 to 2017. Kubernetes won the container wars in 2017, the idea of microservices had taken hold, and the trend toward containerization was unstoppable. Kubernetes architecture matured and slowly became boring, and service mesh technologies, represented by Linkerd and Istio, entered the CNCF-defined cloud-native critical technologies on the horizon.\nKubernetes was designed with the concept of cloud-native in mind. A critical idea in cloud-native is the architectural design of microservices. When a single application is split into microservices, how can microservices be managed to ensure the SLA of the service as the number of services increases? The service mesh was born to solve this problem at the architectural level, free programmers’ creativity, and avoid tedious service discovery, monitoring, distributed tracing, and other matters.\nThe service mesh takes the standard functionality of microservices down to the infrastructure layer, allowing developers to focus more on business logic and thus speed up service delivery, which is consistent with the whole idea of cloud-native. You no longer need to integrate bulky SDKs in your application, develop and maintain SDKs for different languages, and just use the service mesh for Day 2 operations after the application is deployed.\nThe service mesh is regarded as the next generation of microservices. In the diagram, we can see that many of the concerns of microservices overlap with the functionality of Kubernetes. Kubernetes focuses on the application lifecycle, managing resources and deployments with little control over services. The service mesh fills this gap. The service mesh can connect, control, observe and protect microservices.\nKubernetes vs. xDS vs. Istio This diagram shows the layered architecture of Kubernetes and Istio.\nKubernetes vs xDS vs Istio The diagram indicates that the kube-proxy settings are global and cannot be controlled at a granular level for each service. All Kubernetes can do is topology-aware routing, routing traffic closer to the Pod, and setting network policies in and out of the Pod.\nIn contrast, the service mesh takes traffic control out of the service layer in Kubernetes through sidecar proxies, injects proxies into each Pod, and manipulates these distributed proxies through a control plane. It allows for more excellent resiliency.\nKube-proxy implements traffic load balancing between multiple pod instances of a Kubernetes service. But how do you finely control the traffic between these services — such as dividing the traffic by percentage to different application versions (which are all part of the same service, but on other deployments), or doing canary releases and blue-green releases?\nThe Kubernetes community gives a way to do canary releases using Deployment, assigning different pods to deployed services by modifying the pod’s label.\nEnvoy Architecture Currently, the most popular open-source implementation of service mesh in the world is Istio. From the CNCF Survey Report 2020 , we know that Istio is the most used service mesh in production today. Many companies have built their service mesh based on Istio, such as Ant, Airbnb, eBay, NetEase, Tencent, etc.\nCNCF Survey Report 2020 Figure from CNCF Survey Report 2020 Istio is developed based on Envoy, which has been used by default as its distributed proxy since the first day it was open-sourced. Envoy pioneered the creation of the xDS protocol for distributed gateway configuration, greatly simplifying the configuration of large-scale distributed networks. Ant Group open source MOSN also supported xDS In 2019. Envoy was also one of the first projects to graduate from CNCF, tested by large-scale production applications.\nService mesh – the cloud-native networking infrastructure With the above comparison between Kubernetes and service mesh in mind, we can see the place of service mesh in the cloud-native application architecture. That is, building a cloud-native network infrastructure specifically provides:\nTraffic management: controlling the flow of traffic and API calls between services, making calls more reliable, and enhancing network robustness in different environments. Observability: understanding the dependencies between services and the nature and flow of traffic between them provides the ability to identify problems quickly. Policy enforcement: controlling access policies between services by configuring the mesh rather than by changing the code. Service Identification and Security: providing service identifiability and security …","relpermalink":"/en/blog/service-mesh-an-integral-part-of-cloud-native-apps/","summary":"This article will explore the relationship between service mesh and cloud native.","title":"Service Mesh - An Integral Part of Cloud-Native Applications"},{"content":"API gateways have been around for a long time as the entry point for clients to access the back-end, mainly to manage “north-south” traffic, In recent years, service mesh architectures have become popular, mainly for managing internal systems,(i.e. “east-west” traffic), while a service mesh like Istio also has built-in gateways that bring traffic inside and outside the system under unified control. This often creates confusion for first-time users of Istio. What is the relationship between the service mesh and the API gateway? How does Istio’s gateway work? What are the ways to expose the services in the Istio mesh? This article gives you the answer.\nKey Insights The service mesh was originally created to solve the problem of managing internal traffic for distributed systems, but API gateways existed long before it. While the Gateway is built into Istio, you can still use a custom Ingress Controller to proxy external traffic. API gateways and service mesh are converging. How do I expose services in the Istio mesh? The following diagram shows four approaches to expose services in the Istio mesh using Istio Gateway, Kubernetes Ingress, API Gateway, and NodePort/LB.\nExposing services through Istio Ingress Gateway The Istio mesh is shaded, and the traffic in the mesh is internal (east-west) traffic, while the traffic from clients accessing services within the Kubernetes cluster is external (north-south) traffic.\nApproach Controller Features NodePort/LoadBalancer Kubernetes Load balancing Kubernetes Ingress Ingress controller Load balancing, TLS, virtual host, traffic routing Istio Gateway Istio Load balancing, TLS, virtual host, advanced traffic routing, other advanced Istio features API Gateway API Gateway Load balancing, TLS, virtual host, advanced traffic routing, API lifecycle management, billing, rate limiting, policy enforcement, data aggregation Since NodePort/LoadBalancer is a basic way to expose services built into Kubernetes, this article will not discuss that option. Each of the other three approaches will be described below.\nUsing Kubernetes Ingress to expose traffic We all know that clients of a Kubernetes cluster cannot directly access the IP address of a pod because the pod is in a network plane built into Kubernetes. We can expose services inside Kubernetes outside the cluster using NodePort or Load Balancer Kubernetes service type. To support virtual hosting, hiding and saving IP addresses, you can use Ingress resources to expose services in Kubernetes.\nKubernetes Ingress to expose services Ingress is a Kubernetes resource that controls the behavior of an ingress controller that does the traffic touring, which is the equivalent of a load-balanced directional proxy server such as Nginx, Apache, etc., which also includes rule definitions, i.e., routing information for URLs, which is provided by the Ingress controller .\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: istio name: ingress spec: rules: - host: httpbin.example.com http: paths: - path: /status/* backend: serviceName: httpbin servicePort: 8000 The kubernetes.io/ingress.class: istio annotation in the example above indicates that the Ingress uses the Istio Ingress Controller which in fact uses Envoy proxy.\nUsing Istio Gateway to expose services Istio is a popular service mesh implementation that has evolved from Kubernetes that implements some features that Kubernetes doesn’t. (See What is Istio and why does Kubernetes need Istio? ) It makes traffic management transparent to the application, moving this functionality from the application to the platform layer and becoming a cloud-native infrastructure.\nIstio used Kubernetes Ingress as the traffic portal in versions prior to Istio 0.8, where Envoy was used as the Ingress Controller. From Istio 0.8 and later, Istio created the Gateway object. Gateway and VirtualService are used to represent the configuration model of Istio Ingress, and the default implementation of Istio Ingress uses the same Envoy proxy. In this way, the Istio control plane controls both the ingress gateway and the internal sidecar proxy with a consistent configuration model. These configurations include routing rules, policy enforcement, telemetry, and other service control functions.\nThe Istio Gateway resources function similarly to the Kubernetes Ingress in that it is responsible for north-south traffic to and from the cluster. The Istio Gateway acts as a load balancer to carry connections to and from the edge of the service mesh. The specification describes a set of open ports and the protocols used by those ports, as well as the SNI configuration for load balancing, etc.\nThe Istio Gateway resource itself can only be configured for L4 through L6, such as exposed ports, TLS settings, etc.; however, the Gateway can be bound to a VirtualService, where routing rules can be configured on L7, such as versioned traffic routing, fault injection, HTTP redirects, HTTP …","relpermalink":"/en/blog/istio-servicemesh-api-gateway/","summary":"What is the relationship between the service mesh and the API gateway? How does Istio’s gateway work? What are the ways to expose the services in the Istio mesh? This article gives you the answer.","title":"Using Istio Service Mesh as API Gateway"},{"content":"Do you have multiple Kubernetes clusters and a service mesh? Do your virtual machines and services in a Kubernetes cluster need to interact? This article will take you through the process and considerations of building a hybrid cloud using Kubernetes and an Istio Service Mesh. Together, Kubernetes and Istio can be used to bring hybrid workloads into a mesh and achieve interoperability for multicluster. But another layer of infrastructure — a management plane — is helpful for managing multicluster or multimesh deployments.\nKubernetes Using Kubernetes enables rapid deployment of a distributed environment that enables cloud interoperability and unifies the control plane on the cloud. It also provides resource objects, such as Service, Ingress and Gateway , to handle application traffic. The Kubernetes API Server communicates with the kube-proxy component on each node in the cluster, creates iptables rules for the node, and forwards requests to other pods.\nAssuming that a client now wants to access a service in Kubernetes, the request is first sent to the Ingress/Gateway, then forwarded to the backend service (Service A in the diagram below) based on the routing configuration in the Ingress/Gateway. Then Service A polls an instance of Service B for the traffic requested by Service B. Lastly, the traffic requested by Service A for Service B is polled forward to Service B’s instance.\nKubernetes Kubernetes Multicluster The most common usage scenarios for multicluster management include:\nservice traffic load balancing isolating development and production environments decoupling data processing and data storage cross-cloud backup and disaster recovery flexible allocation of compute resources low-latency access to services across regions avoiding vendor lock-in There are often multiple Kubernetes clusters within an enterprise; and the KubeFed implementation of Kubernetes cluster federation developed by Multicluster SIG enables multicluster management capabilities, which allows all Kubernetes clusters to be managed through the same interface.\nThere are several general issues that need to be addressed when using cluster federation:\nConfiguring which clusters need to be federated API resources need to be propagated across the clusters Configuring how API resources are distributed to different clusters Registering DNS records in clusters to enable service discovery across clusters The following is a multicluster architecture for KubeSphere — one of the most commonly used Kubernetes multicluster management architectures — where the Host Cluster serves as the control plane with two member clusters, West and East.\nMulticluster The Host Cluster needs to be able to access the API Server of the Member Cluster, but the network connectivity between Member Clusters is not required. The Host Cluster is independent of the Member Cluster it manages and the Member Cluster is not aware of the existence of the Host Cluster. The advantage of this is that when the control plane fails, the Member Cluster will not be affected and the deployed load can still operate normally without being affected.\nThe Host Cluster also assumes the role of API portal, and the Host Cluster forwards the resource requests to the Member Cluster — which is convenient for aggregation and also facilitates unified authority authentication. We see that there is a Federation Control Plane in the Host Cluster, where the Push Reconciler propagates the identity, role, and role binding from the Federation Cluster to all Member Clusters.\nIstio Service Mesh Consider using the Istio service mesh when we have multilingual, multiversion microservices running in Kubernetes and need finer-grained canary publishing and unified security policy management for inter-service observability. Istio enables intelligent application-aware load balancing from the application layer to other Service Mesh-enabled services in the cluster, by transparently intercepting all traffic to and from the application using IPTables, and bypassing the primary kube-proxy load balancing. The Istio control plane communicates with the Kubernetes API Server to obtain information about all registered services in the cluster.\nThe following diagram illustrates the basics of Istio, where all nodes belong to the same Kubernetes cluster.\nIstio Service Mesh You may end up with at least a few Kubernetes clusters, each hosting microservices. Multiple deployment models exist for Istio’s multicluster deployments — depending on network isolation, primary and backup — which can be specified by declaration when deploying using Istio Operator. Communication between these microservices in a cluster can be enhanced by a service mesh. Within the cluster, Istio provides common communication patterns to improve resiliency, security and observability.\nAll of the above is about application load management on Kubernetes, but for legacy applications on virtual machines: how can they be managed in the same plane? Istio supports …","relpermalink":"/en/blog/multicluster-management-with-kubernetes-and-istio/","summary":"This article explains three patterns/tools for debugging microservices in Kubernetes and the changes brought by the introduction of Istio for debugging microservices.","title":"Multicluster Management With Kubernetes and Istio"},{"content":"Kubernetes is arguably the best environment for running microservices so far, but the experience of debugging microservices in a Kubernetes environment may not be as user-friendly. This article will show you how to debug microservices in Kubernetes, introduce common tools, and explain how the introduction of Istio impacts debugging microservices.\nDebugging microservices is vastly different from traditional monolithic applications The debugging of microservices has been a long-standing problem for software developers. This challenge does not exist in traditional monolithic applications because developers can leverage the debugger in IDEs to add breakpoints, modify environment variables, single-step execution, etc. for their applications, all of which provide great help in software debugging. With the popularity of Kubernetes, the debugging of microservices becomes a thorny issue, where the following issues are more complicated than the debugging of traditional monolithic applications.\nMultiple dependencies A microservice often depends on multiple other microservices, some shared volumes across multiple microservices, and authorizations based on service accounts. When debugging a microservice, how do you deploy other dependent services to quickly build a latest set of staging environments?\nAccess from a local machine When microservices are running on a developer’s local computer, there is usually no direct access to the services in a Kubernetes cluster. How can you debug microservices deployed in a Kubernetes cluster as if they were local services?\nSlow development loop Usually, it takes a long process to update the code and build it into an image before pushing it to the cluster. How do you speed up the development cycle? Let’s look at the tools that address those challenges.\nTools The main solutions for debugging microservices in Kubernetes are:\nProxy: by building a VPN, deploying a proxy in the Kubernetes cluster, and adding local debug endpoints to make the services in Kubernetes directly accessible to local applications, your architecture will look like [ local service ] \u0026lt;-\u0026gt; [ proxy ] \u0026lt;-\u0026gt; [ app in Kubernetes ]. Sidecar: Inject a sidecar into the pod of the microservice to be debugged to intercept all traffic to and from the service, so that the service can be tracked and monitored, and the service can also be debugged in this sidecar. Service Mesh: To get an overall picture of the application, inject sidecars into all microservices so that you can get a dashboard that monitors global status. Here are three typical open source projects that implement the above solutions, each of which can help you debug microservices from a different perspective. You can apply them at different stages of software development and they can be said to be complementary to each other.\nProxy – debugging microservices with Telepresence Telepresence is essentially a local proxy that proxies data volumes, environment variables, and networks in a Kubernetes cluster locally. The following diagram shows the main usage scenarios for Telepresence.\nProxy mode: Telepresence Users need to manually execute the telepresence command locally, which will automatically deploy the agent to Kubernetes. Once the agent has been deployed,\nLocal services will have complete access to other services in the Kubernetes cluster, environment variables, Secret, ConfigMap, etc. Services in the cluster also have direct access to the locally exposed endpoints. However, this approach requires users to run multiple commands while debugging locally, and in some network environments it may not be possible to establish a VPN connection to the Kubernetes cluster.\nSidecar – debugging microservices with Nocalhost Nocalhost is a Kubernetes-based cloud development environment. To use it, you just need to install a plugin in your IDE – VS Code to extend Kubernetes and shorten the development feedback cycle. The development environment can be isolated by creating different namespaces for different users and using ServiceAccount when binding to different user corners. Nocalhost also provides a web console and API for administrators to manage different development environments.\nSidecar mode: Nocalhost As long as you have a Kubernetes cluster and have admin rights to the cluster, you can refer to the Nocalhost documentation to quickly start trying it out. To use the Nocalhost plugin in VS Code, you need to configure the Kubernetes cluster in the plugin first.\nSelect the Kubeconfig file you just exported or copy and paste the contents of the file directly into the configuration. Then select the service you need to test and select the corresponding Dev Container. VS Code will automatically open a new code window. Here is an example of the bookinfo sample provided by Istio. You can open the cloned code in your local IDE and click the hammer next to the code file to enter development mode. Selecting the corresponding DevContainer and Nocalhost will automatically inject a …","relpermalink":"/en/blog/how-to-debug-microservices-in-kubernetes-with-proxy-sidecar-or-service-mesh/","summary":"This article explains three patterns/tools for debugging microservices in Kubernetes and the changes brought by the introduction of Istio for debugging microservices.","title":"How to Debug Microservices in Kubernetes With Proxy, Sidecar or Service Mesh?"},{"content":"Istio was named by Tetrate founder Varun Talwar and Google lead engineer Louis Ryan in 2017 and was open sourced on May 24, 2017. Today is the fourth anniversary of Istio’s open source arrival. Let’s take a look back at Istio’s four years of development — and look forward to Istio’s future.\nIstio’s open source history In 2017, the year Kubernetes ended the container orchestration battle, Google took the opportunity to consolidate its dominance in the cloud native space and compensate for Kubernetes’ disadvantage in service-to-service traffic management by open-sourcing Istio. Istio released its 1.10 last week — but here are some of the most important releases in Istio’s history to date.\nDate Version Note May 24, 2017 0.1 Officially open source; established the architectural foundation of Control Plane, Data Plane and sidecar proxy. October 10, 2017 0.2 Started to support multiple runtime environments, such as virtual machines. June 1, 2018 0.8 API refactoring July 31, 2018 1.0 Production-ready, after which the Istio team underwent a massive reorganization. March 19, 2019 1.1 Enterprise-ready. Support for multiple Kubernetes clusters, with performance optimizations. March 3, 2020 1.5 Back to monolith, with microservice components merged into istiod, making Istio’s architecture cleaner and easier to maintain. Support for WebAssembly extension, making Istio’s ecology much stronger. November 18, 2020 1.8 Officially deprecated Mixer and focused on adding support for virtual machines. A year after its inception– and two months before the 1.0 release, version 0.8 was released with a massive refactoring of the API. In late July 2018, when 1.0 was released, Istio reached a production-ready tipping point. Since then, Google has massively reorganized the Istio team and several Istio-based service mesh startups were born, making 2018 the booming year of the service mesh industry.\nIstio 1.1 was released in March 2019, almost 9 months after 1.0 was released, which is far beyond the average release cycle of an open-source project. We know that the speed of iteration and evolution is a core competency of basic software. Since then, Istio has started a regular release cadence of one version per quarter and has become the #4 fastest growing project in GitHub’s top 10 in 2019 !\nThe Istio community In 2020, Istio’s project management began to mature and its governance reached a stage of evolution. We saw the first election of a steering committee for the Istio community and the transfer of the trademark to Open Usage Commons . The first IstioCon was successfully held in February 2021, with thousands of people attending the online conference. There is also a large Istio community in China , and face-to-face Istio community meetups will be held there in 2021. Stay tuned for more.\nAccording to the CNCF 2020 Survey, 46% of organizations were either using a service mesh in production or planning to use it in the next 12 months. Istio was the top used mesh among those using a mesh in production.\nThe future After 4 years of development, there is not only a large user base around Istio, but also several Istio vendors, as you can see on the homepage of the recently revamped Istio website. In the last few releases, Istio has shifted its development focus to improving the Day 2 Operation experience. We also expect to see more Istio adoption path recommendations, case studies, learning materials, training, and certifications (such as the industry’s first Certified Istio Administrator from Tetrate) that will facilitate the adoption of Istio.\n","relpermalink":"/en/blog/istio-4-year-birthday/","summary":"Today is Istio's 4 year birthday, let’s take a look back at Istio’s four years of development — and look forward to Istio’s future.","title":"Happy Istio 4th Anniversary -- Retrospect and Outlook"},{"content":"Istio, the most popular service mesh implementation , was developed on top of Kubernetes and has a different niche in the cloud native application ecosystem than Kubernetes. Rather than introduce you directly to what Istio has to offer, this article will explain how Istio came about and what it is in relation to Kubernetes.\nWhy Is There an Istio? To explain what Istio is, it’s also important to understand the context in which Istio came into being — i.e., why is there an Istio?\nMicroservices are a technical solution to an organizational problem. And Kubernetes/Istio are a technical solution to deal with the issues created by moving to microservices. As a deliverable for microservices, containers solve the problem of environmental consistency and allow for more granularity in limiting application resources. They are widely used as a vehicle for microservices.\nGoogle open-sourced Kubernetes in 2014, which grew exponentially over the next few years. It became a container scheduling tool to solve the deployment and scheduling problems of distributed applications — allowing you to treat many computers as though they were one computer. Because the resources of a single machine are limited and Internet applications may have traffic floods at different times (due to rapid expansion of user scale or different user attributes), the elasticity of computing resources needs to be high. A single machine obviously can’t meet the needs of a large-scale application; and conversely, it would be a huge waste for a very small-scale application to occupy the whole host.\nIn short, Kubernetes defines the final state of the service and enables the system to reach and stay in that state automatically. So how do you manage the traffic on the service after the application has been deployed? Below we will look at how service management is done in Kubernetes and how it has changed in Istio.\nHow Do You Do Service Management in Kubernetes? The following diagram shows the service model in Kubernetes:\nKubernetes Service Model From the above figure we can see that:\nDifferent instances of the same service may be scheduled to different nodes. Kubernetes combines multiple instances of a service through Service objects to unify external services. Kubernetes installs a kube-proxy component in each node to forward traffic, which has simple load balancing capabilities. Traffic from outside the Kubernetes cluster can enter the cluster via Ingress (Kubernetes has several other ways of exposing services; such as NodePort, LoadBalancer, etc.). Kubernetes is used as a tool for intensive resource management. However, after allocating resources to the application, Kubernetes doesn’t fully solve the problems of how to ensure the robustness and redundancy of the application, how to achieve finer-grained traffic division (not based on the number of instances of the service), how to guarantee the security of the service, or how to manage multiple clusters, etc.\nThe Basics of Istio The following diagram shows the service model in Istio, which supports both workloads and virtual machines in Kubernetes.\nIstio From the diagram we can see that:\nIstiod acts as the control plane, distributing the configuration to all sidecar proxies and gateways. (Note: for simplification, the connections between Istiod and sidecar are not drawn in the diagram.) Istio enables intelligent application-aware load balancing from the application layer to other mesh enabled services in the cluster, and bypasses the rudimentary kube-proxy load balancing. Application administrators can manipulate the behavior of traffic in the Istio mesh through a declarative API, in the same way they manage workloads in Kubernetes. It can take effects within seconds and they can do this without needing to redeploy. Ingress is replaced by Gateway resources, a special kind of proxy that is also a reused Sidecar proxy. A sidecar proxy can be installed in a virtual machine to bring the virtual machine into the Istio mesh. In fact, before Istio one could use SpringCloud, Netflix OSS, and other tools to programmatically manage the traffic in an application, by integrating the SDK in the application. Istio makes traffic management transparent to the application, moving this functionality out of the application and into the platform layer as a cloud native infrastructure.\nIstio complements Kubernetes, by enhancing its traffic management, observability and security for cloud native applications. The service mesh open source project — launched in 2017 by Google, IBM and Lyft — has come a long way in three years. A description of Istio’s core features can be found in the Istio documentation .\nSummary Service Mesh is the cloud native equivalent of TCP/IP, addressing application network communication, security and visibility issues. Istio is currently the most popular service mesh implementation, relying on Kubernetes but also scalable to virtual machine loads. Istio’s core consists of a control plane and a data …","relpermalink":"/en/blog/what-is-istio-and-why-does-kubernetes-need-it/","summary":"This article will explain how Istio came about and what it is in relation to Kubernetes.","title":"What Is Istio and Why Does Kubernetes Need it?"},{"content":"If you’ve heard of service mesh and tried Istio , you may have the following questions:\nWhy is Istio running on Kubernetes? What is the role of Kubernetes and a service mesh in the cloud native application architecture, respectively? What aspects of Kubernetes does Istio extend? What problems does it solve? What is the relationship between Kubernetes, Envoy, and Istio? This article will take you through the inner workings of Kubernetes and Istio. In addition, I will introduce the load balancing approach in Kubernetes, and explain why you need Istio when you have Kubernetes.\nKubernetes is essentially about application lifecycle management through declarative configuration, while a service mesh is essentially about providing inter-application traffic, security management and observability. If you have already built a stable application platform using Kubernetes, how do you set up load balancing and traffic control for calls between services? This is where a service mesh comes into the picture.\nEnvoy introduces the xDS protocol, which is supported by various open source software, such as Istio , MOSN , etc. Envoy contributes xDS to a service mesh or cloud native infrastructure. Envoy is essentially a modern version of a proxy that can be configured through APIs, based on which many different usage scenarios are derived — such as API Gateway, sidecar proxy in service mesh, and edge proxy.\nThis article contains the following:\nA description of the role of kube-proxy. The limitations of Kubernetes for microservice management. An introduction to the capabilities of Istio service mesh. A comparison of some of the concepts in Kubernetes, Envoy, and the Istio service mesh. Kubernetes vs Service Mesh The following diagram shows the service access relationship in Kubernetes and service mesh (one sidecar per pod model).\nKubernetes vs Service Mesh Traffic Forwarding Each node in a Kubernetes cluster deploys a kube-proxy component that communicates with the Kubernetes API Server, gets information about the services in the cluster, and then sets iptables rules to send requests for service directly to the corresponding Endpoint (a pod belonging to the same group of services).\nService Discovery Service Discovery Istio can follow the service registration in Kubernetes and can also interface with other service discovery systems via platform adapters in the control plane; and then generate data plane configurations (using CRD, which are stored in etcd) with transparent proxies for the data plane. The transparent proxy of the data plane is deployed as a sidecar container in the pod of each application service, and all these proxies need to request the control plane to synchronize the proxy configuration. The proxy is “transparent” because the application container is completely unaware of the presence of the proxy. The kube-proxy component in the process needs to intercept traffic as well, except that the kube-proxy intercepts traffic to and from the Kubernetes node — while the sidecar proxy intercepts traffic to and from the pod.\nDisadvantages of a Service Mesh Since Kubernetes has many pods running on each node, putting the original kube-proxy route forwarding function in each pod will increase the response latency — due to more hops when the sidecar intercepts the traffic — and consume more resources. In order to manage traffic in a fine-grained manner, a series of new abstractions will be added. This will further increase the learning cost for users, but as the technology becomes more popular this situation will be slowly alleviated.\nAdvantages of a Service Mesh The kube-proxy settings are global and cannot be controlled at a granular level for each service, while service mesh takes the traffic control out of the service layer in Kubernetes by means of sidecar proxy — allowing for more elasticity.\nShortcomings of Kube-Proxy First, it does not automatically try another pod if the forwarded pod is not serving properly. Each pod has a health check mechanism and when a pod has health problems, kubelet will restart the pod and kube-proxy will remove the corresponding forwarding rules. Also, nodePort-type services cannot add TLS or more complex message routing mechanisms.\nKube-proxy implements load balancing of traffic across multiple pod instances of a Kubernetes service, but how do you do fine-grained control of traffic between these services — such as dividing traffic by percentage to different application versions (which are all part of the same service but on different deployments), or doing canary releases (grayscale releases) and blue-green releases?\nThe Kubernetes community gives a way to do canary releases using Deployment , which is essentially a way to assign different pods to a deployment’s service by modifying the pod’s label.\nKubernetes Ingress vs. Istio Gateway As mentioned above, kube-proxy can only route traffic within a Kubernetes cluster. The pods of a Kubernetes cluster are located in a network created by CNI. …","relpermalink":"/en/blog/why-do-you-need-istio-when-you-already-have-kubernetes/","summary":"This article will take you through the inner workings of Kubernetes and Istio. In addition, I will introduce the load balancing approach in Kubernetes, and explain why you need Istio when you have Kubernetes.","title":"Why Do You Need Istio When You Already Have Kubernetes?"},{"content":"Tetrate Academy has recently released the Istio Fundamentals Course, which is now available for free. Sign up at Tetrate Academy now!\nCourse curriculum Here is the curriculum:\nService Mesh and Istio Overview Installing Istio Observability: Telemetry and Logs Traffic Management Security Advanced Features Troubleshooting Real World Examples There are self-assessment questions at the end of each course. I have passed the course, and here is the certificate after passing the course.\nTetrate Academy Istio Fundamentals Course More In the future, Tetrate will release the Certified Istio Administrator (CIA) exam and welcome all Istio users and administrators to follow and register for it.\n","relpermalink":"/en/notice/tetrate-istio-fundamental-courses/","summary":"Tetrate Academy has recently released the Istio Fundamentals Course, which is now available for free.","title":"Tetrate Academy Releases Free Istio Fundamentals Course"},{"content":"Different companies or software providers have devised countless ways to control user access to functions or resources, such as Discretionary Access Control (DAC), Mandatory Access Control (MAC), Role-Based Access Control (RBAC), and Attribute-Based Access Control (ABAC). In essence, whatever the type of access control model, three basic elements can be abstracted: user, system/application, and policy.\nIn this article, we will introduce ABAC, RBAC, and a new access control model — Next Generation Access Control (NGAC) — and compare the similarities and differences between the three, as well as why you should consider NGAC.\nWhat Is RBAC? RBAC, or Role-Based Access Control, takes an approach whereby users are granted (or denied) access to resources based on their role in the organization. Every role is assigned a collection of permissions and restrictions, which is great because you don’t need to keep track of every system user and their attributes. You just need to update appropriate roles, assign roles to users, or remove assignments. But this can be difficult to manage and scale. Enterprises that use the RBAC static role-based model have experienced role explosion: large companies may have tens of thousands of similar but distinct roles or users whose roles change over time, making it difficult to track roles or audit unneeded permissions. RBAC has fixed access rights, with no provision for ephemeral permissions or for considering attributes like location, time, or device. Enterprises using RBAC have had difficulty meeting the complex access control requirements to meet regulatory requirements of other organizational needs.\nRBAC Example Here’s an example Role in the “default” namespace in Kubernetes that can be used to grant read access to pods:\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [\u0026#34;v1\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] What Is ABAC? ABAC stands for Attribute-Based Access Control. At a high level, NIST defines ABAC as an access control method “where subject requests to perform operations on objects are granted or denied based on assigned attributes of the subject, environment conditions, and a set of policies that are specified in terms of those attributes and conditions.” ABAC is a fine-grained model since you can assign any attributes to the user, but at the same time it becomes a burden and hard to manage:\nWhen defining permissions, the relationship between users and objects cannot be visualized. If the rules are a little complex or confusingly designed, it will be troublesome for the administrator to maintain and trace. This can cause performance problems when there is a large number of permissions to process.\nABAC Example Kubernetes initially uses ABAC as access control and is configured via JSON Lines, for example:\nAlice can just read pods in namespace “foo”:\n{\u0026#34;apiVersion\u0026#34;: \u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: {\u0026#34;user\u0026#34;: \u0026#34;alice\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;pods\u0026#34;, \u0026#34;readonly\u0026#34;: true}} What Is NGAC? NGAC, or Next Generation Access Control, takes the approach of modeling access decision data as a graph. NGAC enables a systematic, policy-consistent approach to access control, granting or denying users administrative capabilities with a high level of granularity. NGAC was developed by NIST (National Institute of Standards and Technology) and is currently used in Tetrate Q and Tetrate Service Bridge .\nThere are several types of entities; they represent the resources you want to protect, the relationships between them, and the actors that interact with the system. The entities are:\nUsers Objects User attributes, such as organization unit Object attributes, such as folders Policy classes, such as file system access, location, and time NIST’s David Ferraiolo and Tetrate ‘s Ignasi Barrera shared how NGAC works at their presentation on Next Generation Access Control at Service Mesh Day 2019 in San Francisco.\nNGAC is based on the assumption that you can represent the system you want to protect in a graph that represents the resources you want to protect and your organizational structure, in a way that has meaning to you and that adheres to your organization semantics. On top of this model that is very particular to your organization, you can overlay policies. Between the resource model and the user model, the permissions are defined. This way NGAC provides an elegant way of representing the resources you want to protect, the different actors in the system, and how both worlds are tied together with permissions.\nNGAC DAG Image via Linear Time Algorithms to Restrict Insider Access using Multi-Policy Access Control Systems NGAC Example The following example shows a simple NGAC graph with a User DAG representing an organization structure, an Object DAG representing files and folders in a filesystem, a categorization of the files, and two different policies — file system and scope — …","relpermalink":"/en/blog/why-you-should-choose-ngac-as-your-access-control-model/","summary":"This article will introduce you to the next generation permission control model, NGAC, and compare ABAC, RABC, and explain why you should choose NGAC.","title":"Why You Should Choose NGAC as Your Access Control Model"},{"content":" IstioCon 2021 poster (Jimmy Song) Topic: Service Mesh in China Time: February 23rd, 10:00 - 10:10 am Beijing time How to participate: IstioCon 2021 website Cost: Free From February 22-25, Beijing time, the Istio community will be hosting the first IstioCon online, and registration is free to attend! I will be giving a lightning talk on Tuesday, February 23rd (the 12th day of the first month of the lunar calendar), as an evangelist and witness of Service Mesh technology in China, I will introduce the Service Mesh industry and community in China.\nI am a member of the inaugural IstioCon organizing committee with Zhonghu Xu (Huawei) and Shaojun Ding (Intel), as well as the organizer of the China region. Considering Istio’s large audience in China, we have arranged for Chinese presentations that are friendly to the Chinese time zone. There will be a total of 14 sharing sessions in Chinese, plus dozens more in English. The presentations will be in both lightning talk (10 minutes) and presentation (40 minutes) formats.\nJoin the Cloud Native Community Istio SIG to participate in the networking at this conference. For the schedule of IstioCon 2021, please visit IstioCon 2021 official website , or click for details.\n","relpermalink":"/en/notice/istiocon-2021/","summary":"IstioCon 2021, I'll be giving a lightning talk, February 22nd at 10am BST.","title":"IstioCon 2021 Lightning Talk Preview"},{"content":"The ServiceMesher website has lost connection with the webhook program on the web publishing server because the GitHub where the code is hosted has has been “lost” and the hosting server is temporarily unable to log in, so the site cannot be updated. Today I spent a day migrating all the blogs on ServiceMesher to the Cloud Native Community website cloudnative.to , and as of today, there are 354 blogs on the Cloud Native Community.\nServiceMesher blogs Now we plan to archive ServiceMesher official GitHub (all pages under the servicemesher.com domain) We are no longer accepting new PRs, so please submit them directly to the Cloud Native Community . Thank you all!\n","relpermalink":"/en/notice/servicemesher-blog-merged/","summary":"ServiceMesher website is no longer maintained, plan to archive the website code, the blog has been migrated to Cloud Native Community, please submit the new blog to Cloud Native Community.","title":"ServiceMesher website is no longer maintained, the original blog has been migrated to the cloud native community"},{"content":"In this article, I’ll give you an overview of Istio ‘s history of virtual machine integration support. In particular, the introduction of the smart DNS proxy and WorkloadGroup in Istio 1.8, which makes virtual machines and containers equivalent at the resource abstraction level.\nI will show you a tumultuous odyssey of Istio’s virtual machine integration. Tetrate, the enterprise service mesh company that made pushing Istio to run everywhere part of its founding mission, has used VM features extensively in customer deployments and has been instrumental in pushing VMs to Istio upstream.\nPreface In my previous article , I talked about how Istio 1.7 supported virtual machines. But at that time, late October, virtual machines were still not seamlessly integrated into Istio — there was still a lot of manual work required. Now, Istio 1.8 has added WorkloadGroup and smart DNS proxy, which allows non-Kubernetes workloads like VMs to become first-class citizens in Istio — just like pods.\nWith or without a sidecar installed for virtual machines, until 1.7 you could not resolve the DNS name of a Kubernetes service unless a kube-external DNS was configured — which is the last piece of virtual machine integration in Istio. This shortcoming has finally been fixed in Istio 1.8.\nWhy Is Virtual Machine Support Important? In the process of migrating our applications to cloud native architectures and continuously containerizing them, we will go through three phases as shown in the figure below.\nCloud Native Stages Stage 1: All applications are deployed on virtual machines Stage 2: Applications are deployed on both virtual machines and containers, are migrating from virtual machines to containers, and are using Kubernetes to manage containers. Stage 3: All applications are deployed in containers first, using Kubernetes to manage containers and Istio to manage service-to-service communication. The above diagram is artificially simplified: in reality, there might be multiple hybrid clouds, multiple regions, multiple clusters, etc. Plus, at stage 3 containers and virtual machines may remain in long-term coexistence, but the trend of containerization remains unchanged.\nIstio’s History of Virtual Machine Support Istio’s support for virtual machines is a long process, an odyssey of sorts.\n0.2: Istio Mesh Expansion As of version 0.2, Istio added virtual machines to the Mesh via Istio Mesh Expansion , provided that the following prerequisites were met.\nVirtual machines must have direct access to the application’s pods via IP address, which requires a flat network between the container and the VM via VPC or VPN; and virtual machines do not need access to the Cluster IP, but rather direct access to the service’s endpoints. Virtual machines must have access to Istio’s control plane services (Pilot, Mixer, CA, now being integrated as Istiod), which can expose the control plane endpoints to virtual machines by deploying load balancers in the Istio Mesh. (optional) the virtual machine has access to the DNS server inside the Mesh (deployed in Kubernetes). The steps to integrate a virtual machine are as follows.\nCreate an internal load balancer for the Istio control plane service and the DNS service for the Kubernetes cluster. Generate a configuration file for the Istio Service CIDR, Service Account token, security certificate, and IP of the Istio Control Plane Service (the IP exposed through the Internal Load Balancer) and send it to the virtual machine. Setup the Istio component, dnsmaq (for DNS discovery), in the virtual machine; so that the virtual machine can access the services in the mesh using FQDN, to ensure that the virtual machine can correctly resolve the Cluster IP of the services in the mesh. To run the service in a virtual machine, you need to configure the sidecar, add inbound ports to be intercepted, then restart Istio and also run istioctl to register the service. The following figure shows the detailed flow from integrating a virtual machine to accessing services in the virtual machine in a mesh.\nFigure 1 Figure 1\nThe DNS is hijacked by dnsmasq deployed in the virtual machine, which allows it to correctly obtain the Cluster IP of the Istio service (Kubernetes’ built-in DNS). Access to Kubernetes’ built-in DNS service (which is exposed outside the cluster via the Internal Load Balancer and can be accessed directly). Return the Cluster IP resolved by productpage.bookinfo.svc.cluster.local, noting that the IP address is not directly accessible, but failure to be DNS resolved will result in a failed VM request for the service. The virtual machine’s call to services in a mesh is hijacked by the sidecar proxy. Since the proxy is connected to the Istio control plane, the endpoints of the service can be queried via xDS, so traffic will be forwarded to one of the endpoints. To access VM services in mesh, you need to manually add VM services to mesh using the istioctl register command, which essentially registers the VM services to the …","relpermalink":"/en/blog/istio-18-a-virtual-machine-integration-odyssey/","summary":"In this article, I’ll give you an overview of Istio‘s history of virtual machine integration support. In particular, the introduction of the smart DNS proxy and WorkloadGroup in Istio 1.8, which makes virtual machines and containers equivalent at the resource abstraction level.","title":"Istio 1.8: A Virtual Machine Integration Odyssey"},{"content":"A service mesh is a relatively simple concept, consisting of a bunch of network proxies paired with each service in an application, plus a set of task management processes. The proxies are called the data plane and the management processes are called the control plane in the Service Mesh. The data plane intercepts calls between different services and “processes” them; the control plane is the brain of the mesh that coordinates the behavior of proxies and provides APIs for operations and maintenance personnel to manipulate and observe the entire network.\nThe diagram below shows the architecture of a service mesh.\nService Mesh Architecture Further, the service mesh is a dedicated infrastructure layer designed to enable reliable, fast, and secure inter-service invocation in microservices architectures. It is not a mesh of “services” but rather a mesh of “proxies” that services can plug into, thus abstracting the network from the application code. In a typical service mesh, these proxies are injected into each service deployment as a sidecar (and also may be deployed at the edge of the mesh). Instead of invoking services directly over the network, services invoke their local sidecar proxy, which in turn manages requests on behalf of the service, pushing the complexities of inter-service communications into a networking layer that can resolve them at scale. The set of interconnected sidecar proxies implements a so-called data plane, while on the other hand the service mesh control plane is used to configure proxies. The infrastructure introduced by a service mesh provides an opportunity, too, to collect metrics about the traffic that is flowing through the application.\nThe architecture of a service mesh The infrastructure layer of a service mesh is divided into two main parts: the control plane and the data plane.\nCharacteristics of the control plane\nDo not parse packets directly. Communicates with proxies in the control plane to issue policies and configurations. Visualizes network behavior. Typically provides APIs or command-line tools for configuration versioning and management for continuous integration and deployment. Characteristics of the data plane\nIs usually designed with the goal of statelessness (though in practice some data needs to be cached to improve traffic forwarding performance). Directly handles inbound and outbound packets, forwarding, routing, health checking, load balancing, authentication, authentication, generating monitoring data, etc. Is transparent to the application, i.e., can be deployed senselessly. Changes brought by the service mesh Decoupling of microservice governance from business logic\nA service mesh takes most of the capabilities in the SDK out of the application, disassembles them into separate processes, and deploys them in a sidecar model. By separating service communication and related control functions from the business process and synching them to the infrastructure layer, a service mesh mostly decouples them from the business logic, allowing application developers to focus more on the business itself.\nNote that the word “mostly” is mentioned here and that the SDK often needs to retain protocol coding and decoding logic, or even a lightweight SDK to implement fine-grained governance and monitoring policies in some scenarios. For example, to implement method-level call distributed tracing, the service mesh requires the business application to implement trace ID passing, and this part of the implementation logic can also be implemented through a lightweight SDK. Therefore, the service mesh is not zero-intrusive from a code level.\nUnified governance of heterogeneous environments\nWith the development of new technologies and staff turnover, there are often applications and services in different languages and frameworks in the same company, and in order to control these services uniformly, the previous practice was to develop a complete set of SDKs for each language and framework, which is very costly to maintain. With a service mesh, multilingual support is much easier by synching the main service governance capabilities to the infrastructure. By providing a very lightweight SDK, and in many cases, not even a separate SDK, it is easy to achieve unified traffic control and monitoring requirements for multiple languages and protocols.\nFeatures of service mesh Service mesh also has three major technical advantages over traditional microservice frameworks.\nObservability\nBecause the service mesh is a dedicated infrastructure layer through which all inter-service communication passes, it is uniquely positioned in the technology stack to provide uniform telemetry at the service invocation level. This means that all services are monitored as “black boxes.” The service mesh captures route data such as source, destination, protocol, URL, status codes, latency, duration, etc. This is essentially the same data that web server logs can provide, but the service mesh captures this data for …","relpermalink":"/en/blog/what-is-a-service-mesh/","summary":"This article will take you through what a service mesh is, as well as its architecture, features, and advantages and disadvantages.","title":"What Is a Service Mesh?"},{"content":"1.8 is the last version of Istio to be released in 2020 and it has the following major updates:\nSupports installation and upgrades using Helm 3. Mixer was officially removed. Added Istio DNS proxy to transparently intercept DNS queries from applications. WorkloadGroup has been added to simplify the integration of virtual machines. WorkloadGroup is a new API object. It is intended to be used with non-Kubernetes workloads like Virtual Machines and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies.\nInstallation and Upgrades Istio starts to officially support the use of Helm v3 for installations and upgrades. In previous versions, the installation was done with the istioctl command-line tool or Operator. With version 1.8, Istio supports in-place and canary upgrades with Helm.\nEnhancing Istio’s Usability The istioctl command-line tool has a new bug reporting feature (istioctl bug-report ), which can be used to collect debugging information and get cluster status.\nThe way to install the add-on has changed: 1.7 istioctl is no longer recommended and has been removed in 1.8, to help solve the problem of add-on lagging upstream and to make it easier to maintain.\nTetrate is an enterprise service mesh company. Our flagship product, TSB, enables customers to bridge their workloads across bare metal, VMs, K8s, \u0026amp; cloud at the application layer and provide a resilient, feature-rich service mesh fabric powered by Istio, Envoy, and Apache SkyWalking.\nMixer, the Istio component that had been responsible for policy controls and telemetry collection, has been removed. Its functionalities are now being served by the Envoy proxies. For extensibility, service mesh experts recommend using WebAssembly (Wasm) to extend Envoy; and you can also try the GetEnvoy Toolkit , which makes it easier for developers to create Wasm extensions for Envoy. If you still want to use Mixer, you must use version 1.7 or older. Mixer continued receiving bug fixes and security fixes until Istio 1.7. Many features supported by Mixer have alternatives as specified in the Mixer Deprecation document, including the in-proxy extensions based on the Wasm sandbox API.\nSupport for Virtual Machines Istio’s recent upgrades have steadily focused on making virtual machines first-class citizens in the mesh. Istio 1.7 made progress to support virtual machines and Istio 1.8 adds a smart DNS proxy , which is an Istio sidecar agent written in Go. The Istio agent on the sidecar will come with a cache that is dynamically programmed by Istiod DNS Proxy. DNS queries from applications are transparently intercepted and served by an Istio proxy in a pod or VM that intelligently responds to DNS query requests, enabling seamless multicluster access from virtual machines to the service mesh.\nIstio 1.8 adds a WorkloadGroup , which describes a collection of workload instances. It provides a specification that the workload instances can use to bootstrap their proxies, including the metadata and identity. It is only intended to be used with non-k8s workloads like Virtual Machines, and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies. Using WorkloadGroups, Istio has started to help automate VM registration with istioctl experimental workload group .\nTetrate , the enterprise service mesh company, uses these VM features extensively in customers’ multicluster deployments, to enable sidecars to resolve DNS for hosts exposed at ingress gateways of all the clusters in a mesh; and to access them over mutual TLS.\nConclusion All in all, the Istio team has kept the promise made at the beginning of the year to maintain a regular release cadence of one release every three months since the 1.1 release in 2018, with continuous optimizations in performance and user experience for a seamless experience of brownfield and greenfield apps on Istio. We look forward to more progress from Istio in 2021.\n","relpermalink":"/en/blog/istio-1-8-a-smart-dns-proxy-takes-support-for-virtual-machines-a-step-further/","summary":"WorkloadGroup is a new API object. It is intended to be used with non-Kubernetes workloads like Virtual Machines and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies.","title":"Istio 1.8: A Smart DNS Proxy Takes Support For Virtual Machines A Step Further"},{"content":"Istio is a popular service mesh to connect, secure, control, and observe services. When it was first introduced as open source in 2017, Kubernetes was winning the container orchestration battle and Istio answered the needs of organizations moving to microservices. Although Istio claims to support heterogeneous environments such as Nomad, Consul, Eureka, Cloud Foundry, Mesos, etc., in reality, it has always worked best with Kubernetes — on which its service discovery is based.\nIstio was criticized for a number of issues early in its development, for the large number of components, the complexity of installation and maintenance, the difficulty of debugging, a steep learning curve due to the introduction of too many new concepts and objects (up to 50 CRDs), and the impact of Mixer components on performance. But these issues are gradually being overcome by the Istio team. As you can see from the roadmap released in early 2020, Istio has come a long way.\nBetter integration of VM-based workloads into the mesh is a major focus for the Istio team this year. Tetrate also offers seamless multicloud connectivity, security, and observability, including for VMs, via its product Tetrate Service Bridge . This article will take you through why Istio needs to integrate with virtual machines and how you can do so.\nWhy Should Istio Support Virtual Machines? Although containers and Kubernetes are now widely used, there are still many services deployed on virtual machines and APIs outside of the Kubernetes cluster that needs to be managed by Istio mesh. It’s a huge challenge to unify the management of the brownfield environment with the greenfield.\nWhat Is Needed to Add VMs to the Mesh? Before the “how,” I’ll describe what is needed to add virtual machines to the mesh. There are a couple of things that Istio must know when supporting virtual machine traffic: which VMs have services that should be part of the mesh, and how to reach the VMs. Each VM also needs an identity, in order to communicate securely with the rest of the mesh. These requirements could work with Kubernetes CRDs, as well as a full-blown Service Registry like Consul. And the service account based identity bootstrapping could work as a mechanism for assigning workload identities to VMs that do not have a platform identity. For VMs that do have a platform identity (like EC2, GCP, Azure, etc.), work is underway in Istio to exchange the platform identity with a Kubernetes identity for ease of setting up mTLS communication.\nHow Does Istio Support Virtual Machines? Istio’s support for virtual machines starts with its service registry mechanism. The information about services and instances in the Istio mesh comes from Istio’s service registries, which up to this point have only looked at or tracked pods. In newer versions, Istio now has resource types to track and watch VMs. The sidecars inside the mesh cannot observe and control traffic to services outside the mesh, because they do not have any information about them.\nThe Istio community and Tetrate have done a lot of work on Istio’s support for virtual machines. The 1.6 release included the addition of WorkloadEntry, which allows you to describe a VM exactly as you would a host running in Kubernetes. In 1.7, the release started to add the foundations for bootstrapping VMs into the mesh automatically through tokens, with Istio doing the heavy lifting. Istio 1.8 will debut another abstraction called WorkloadGroup, which is similar to a Kubernetes Deployment object — but for VMs.\nThe following diagram shows how Istio models services in the mesh. The predominant source of information comes from a platform service registry like Kubernetes, or a system like Consul. In addition, the ServiceEntry serves as a user-defined service registry, modeling services on VMs or external services outside the organization.\nWhy install Istio in a virtual machine when you can just use ServiceEntry to bring in the services in the VMs?\nUsing ServiceEntry, you can enable services inside the mesh to discover and access external services; and in addition, manage the traffic to those external services. In conjunction with VirtualService, you can also configure access rules for the corresponding external service — such as request timeouts, fault injection, etc. — to enable controlled access to the specified external service.\nEven so, it only controls the traffic on the client-side, not access to the introduced external service to other services. That is, it cannot control the behavior of the service as the call initiator. Deploying sidecars in a virtual machine and introducing the virtual machine workload via workload selector allows the virtual machine to be managed indiscriminately, like a pod in Kubernetes.\nFuture As you can see from the bookinfo demo , there is too much manual work involved in the process and it’s easy to go wrong. In the future, Istio will improve VM testing to be realistic, automate bootstrapping based on platform identity, …","relpermalink":"/en/blog/how-to-integrate-virtual-machines-into-istio-service-mesh/","summary":"Better integration of virtual machine-based workloads into the service mesh is a major focus for the Istio team this year, and Tetrate also provides seamless multi-cloud connectivity, security and observability, including for virtual machines, through its product Tetrate Service Bridge. This article will show you why Istio needs to integrate with virtual machines and how.","title":"How to Integrate Virtual Machines Into Istio Service Mesh"},{"content":"Today is my 914th day and also the last day with Ant Group , tomorrow is September 1st, which is usually the day school starts, and everyone at Alibaba is known as “classmate”, tomorrow I will join Tetrate , and that’s kind of starting my new semester!\nAnt/Alibaba and the Cloud Native Community To date, Ant/Alibaba Group has had a profound impact on my career, especially its corporate culture and values, and the Alibaba recruiting philosophy of “finding like-minded people”, and isn’t the process of creating the Cloud Native Community also a process of finding like-minded people? Cloud Native Community is like a small society, I don’t want it to have much social value, but only want it to make a small but beautiful change to individuals, to enterprises and to society. I constantly think about myself as an individual and as an employee, especially as an initiator of the community. What is my mission as an individual, an employee, and especially as an initiator of a community? What role should I play in the company? Where is this community going? I’m fumbling along, but because of your support, it makes me stronger and more committed to the adoption and application of cloud native technology in China, outside of me I may have gone faster, but now with the community together we will go further!\n24 June 2019, Shanghai, KubeCon China 2019 June 24, 2019, Shanghai, KubeCon China 2019\nJoining Tetrate Over the past two years, I’ve been working hard to promote Istio and Service Mesh technology, and with funding from Ant Group, I started the ServiceMesher Community to bring Service Mesh technology to China. Next I want to bring Chinese practice to the world.\nAs a Developer Advocate, the most important thing is not to stop learning, but to listen and take stock. Over the past two years, I’ve seen a lot of people show interest in Service Mesh, but not enough to understand the risks and lack of knowledge about the new technology. I’m excited to join this Service Mesh-focused startup Tetrate , a global telecommuting startup with products built around open source Istio , [Envoy](https:/ /envoyproxy.io) and Apache SkyWalking , it aims to make it to be the cloud native network infrastructure. Here are several maintainers of these open source projects, such as Sheng Wu , Zack Butcher , Lizan Zhou , etc., and I believe that working with them can help you understand and apply Service Mesh quickly and effectively across cloud native.\nMore Earlier this year as I was preparing for the Cloud Native community, I set the course for the next three years - cloud native, open source and community. The road to pursue my dream is full of thorns, not only need courage and perseverance, but also need you to be my strong backing, I will overcome the thorns and move forward. Open source belongs to the world, to let the world understand us better, we must be more active into the world. I hope that China’s open source tomorrow will be better, I hope that Service Mesh technology will be better applied by the enterprises in China, I hope that cloud native can benefit the public, and I hope that we can all find our own mission.\nWe are hiring now, if you are interested with Tetrate , please send your resume to careers@tetrate.io .\n","relpermalink":"/en/blog/moving-on-from-ant-group/","summary":"Today is my last day at Ant and tomorrow I'm starting a new career at Tetrate.","title":"New Beginning - Goodbye Ant, Hello Tetrate"},{"content":"Just tonight, the jimmysong.io website was moved to the Alibaba Cloud Hong Kong node. This is to further optimize the user experience and increase access speed. I purchased an ECS on the Alibaba Cloud Hong Kong node, and now I have a public IP and can set subdomains. The website was previously deployed on GitHub Pages, the access speed is average, and it has to withstand GitHub instability. Impact (In recent years, GitHub downtime has occurred).\nMeanwhile, the blog has also done a lot to improve the site, thanks to Bai Jun away @baijunyao strong support, a lot of work for the revision of the site, including:\nChanged the theme color scheme and deepened the contrast Use aligolia to support full site search Optimized mobile display Articles in the blog have added zoom function Added table of contents to blog post This site is built on the theme of educenter .\nThanks to the majority of netizens who have supported this website for several years. The website has been in use for more than three years and has millions of visits. It has undergone two major revisions before and after, on January 31, 2020 and October 8, 2017, respectively. And changed the theme of the website. In the future, I will share more cloud-native content with you as always, welcome to collect, forward, and join the cloud-native community to communicate with the majority of cloud-native developers.\n","relpermalink":"/en/notice/migrating-to-alibaba-cloud/","summary":"Move the website to the Alibaba Cloud Hong Kong node to increase the speed of website access and the convenience of obtaining public IP and subdomain names.","title":"Move to Alibaba Cloud Hong Kong node"},{"content":" Just the other day, Java just celebrated its 25th birthday , and from the time of its birth it was called “write once, run everywhere”, but more than 20 years later, there is still a deep gap between programming and actual production delivery. the world of IT is never short of concepts, and if a concept doesn’t solve the problem, then it’s time for another layer of concepts. it’s been 6 years since Kubernetes was born, and it’s time for the post-Kubernetes era - the era of cloud-native applications!\nCloud Native Stage This white paper will take you on a journey to explore the development path of cloud-native applications in the post-Kubernetes era.\nHighlights of the ideas conveyed include.\nCloud-native has passed through a savage growth period and is moving towards uniform application of standards. Kubernetes’ native language does not fully describe the cloud-native application architecture, and the development and operation functions are heavily coupled in the configuration of resources. Operator’s expansion of the Kubernetes ecosystem has led to the fragmentation of cloud-native applications, and there is an urgent need for a unified application definition standard. The essence of OAM is to separate the R\u0026amp;D and O\u0026amp;M concerns in the definition of cloud-native applications, and to further abstract resource objects, simplify and encompass everything. “Kubernetes Next Generation” refers to the fact that after Kubernetes became the infrastructure layer standard, the focus of the cloud-native ecology is being overtaken by the application layer, and the last two years have been a powerful exploration of the hot Service Mesh process, and the era of cloud-native application architecture based on Kubernetes is coming. Kubernetes has become an established operating platform for cloud-native applications, and this white paper will expand with Kubernetes as the default platform, including an explanation of the OAM-based hierarchical model for cloud-native applications.\n","relpermalink":"/en/notice/guide-to-cloud-native-app/","summary":"Take you on a journey through the post-Kubernetes era of cloud-native applications.","title":"Guide to Cloud Native Application"},{"content":"At the beginning of 2020, due to the outbreak of the Crona-19 pandemic, employees around the world began to work at home. Though the distance between people grew farer, there was a group of people, who were us working in the cloud native area, gathered together for a common vision. During the past three months, we have set up the community management committee and used our spare time working together to complete the preparatory work for the community. Today we are here to announce the establishment of the Cloud Native Community.\nBackground Software is eating the world. —— Marc Andreessen\nThis sentence has been quoted countless times, and with the rise of Cloud Native, we’d like to talk about “Cloud Native is eating the software.” As more and more enterprises migrate their services to the cloud, the original development mode of enterprises cannot adapt to the application scenarios in the cloud, and it is being reshaped to conform to the cloud native standard.\nSo what is cloud native? Cloud native is a collection of best practices in architecture, r\u0026amp;d process and team culture to support faster innovation, superior user experience, stable and reliable user service and efficient r\u0026amp;d. The relationship between the open source community and the cloud native is inseparable. It is the existence of the open source community, especially the end user community, that greatly promotes the continuous evolution of cloud native technologies represented by container, service mesh and microservices.\nCNCF (Cloud Native Computing Foundation) holds Cloud Native conference every year in the international community, which has a wide audience and great influence. But it was not held in China for the first time until 2018, after several successful international events. However, there are no independent foundations or neutral open source communities in China. In recent years, many cloud native enthusiasts in China have set up many communication groups and held many meetups, which are very popular. Many excellent open source projects have emerged in the cloud native field, but there is no organized neutral community for overall management. Under this background, the Cloud Native Community emerges at the right moment.\nAbout Cloud Native Community is an open source community with technology, temperature and passion. It was founded spontaneously by a group of industry elites who love open source and uphold the principle of consensus, co-governance, co-construction and sharing. The aim of the community is: connection, neutral, open source. We are based in China, facing the world, enterprise neutrality, focusing on open source, and giving feedback to open source.\nIntroduction for the Steering Community:https://cloudnative.to/en/team/ .\nYou will gain the followings after joining the community:\nKnowledge and news closer to the source A more valuable network More professional and characteristic consultation Opportunities to get closer to opinion leaders Faster and more efficient personal growth More knowledge sharing and exposure opportunities More industry talent to be found Contact Contact with us.\nEmail: mailto:contact@cloudnative.to Twitter: https://twitter.com/CloudNativeTo ","relpermalink":"/en/notice/cloud-native-community-announecement/","summary":"Today the Community Steering Committee announced the official formation of the Cloud Native Community.","title":"Establishment of the Cloud Native Community"},{"content":"This article is a rework of previously written content and is included in the Istio Handbook of the ServiceMesher community . Other chapters are still being compiled.\nPeople who have just heard of Service Mesh and tried Istio may have the following questions:\nWhy does Istio bind Kubernetes? What roles do Kubernetes and Service Mesh play in cloud native? What aspects of Kubernetes has Istio extended? What problems have been solved? What is the relationship between Kubernetes, xDS protocols (Envoy , MOSN, etc) and Istio? Should I use Service Mesh? In this section, we will try to guide you through the internal connections between Kubernetes, the xDS protocol, and Istio Service Mesh. In addition, this section will also introduce the load balancing methods in Kubernetes, the significance of the xDS protocol for Service Mesh, and why Istio is needed in time for Kubernetes.\nUsing Service Mesh is not to say that it will break with Kubernetes, but that it will happen naturally. The essence of Kubernetes is to perform application lifecycle management through declarative configuration, while the essence of Service Mesh is to provide traffic and security management and observability between applications. If you have built a stable microservice platform using Kubernetes, how do you set up load balancing and flow control for calls between services?\nThe xDS protocol created by Envoy is supported by many open source software, such as Istio , Linkerd , MOSN, etc. Envoy’s biggest contribution to Service Mesh or cloud native is the definition of xDS. Envoy is essentially a proxy. It is a modern version of proxy that can be configured through APIs. Based on it, many different usage scenarios are derived, such as API Gateway, Service Mesh. Sidecar proxy and Edge proxy in.\nThis section contains the following\nExplain the role of kube-proxy. Kubernetes’ limitations in microservice management. Describe the features of Istio Service Mesh. Describe what xDS includes. Compare some concepts in Kubernetes, Envoy and Istio Service Mesh. Key takeaways If you want to know everything in advance, here are some of the key points from this article:\nThe essence of Kubernetes is application lifecycle management, specifically deployment and management (scaling, scaling, automatic recovery, release). Kubernetes provides a scalable and highly resilient deployment and management platform for microservices. The foundation of Service Mesh is a transparent proxy. After the traffic between microservices is intercepted through sidecar proxy, the behavior of microservices is managed through the control plane configuration. Service Mesh decoupled from Kubernetes traffic management, the internal flow without the need of Service Mesh kube-proxy supporting components, micro-services closer to abstract the application layer by, for traffic between management services, security and observability. xDS defines the protocol standards for Service Mesh configuration. Service Mesh is a higher-level abstraction of services in Kubernetes. Its next step is serverless. Kubernetes vs Service Mesh The following figure shows the service access relationship between Kubernetes and Service Mesh (one sidecar per pod mode).\nkubernetes vs service mesh Traffic forwarding\nEach node of the cluster Kubernetes a deployed kube-proxy assembly Kubernetes API Server may communicate with the cluster acquired service information, and then set iptables rules, sends a request for a service directly to the corresponding Endpoint (belonging to the same group service pod).\nService discovery\nService registration in Service Mesh Istio Service Mesh can use the service in Kubernetes for service registration. It can also connect to other service discovery systems through the platform adapter of the control plane, and then generate the configuration of the data plane (using CRD statements, stored in etcd), a transparent proxy for the data plane. (Transparent proxy) is deployed in the sidecar container in each application service pod. These proxy need to request the control plane to synchronize the proxy configuration. The reason why is a transparent proxy, because there is no application container fully aware agent, the process kube-proxy components like the need to block traffic, but kube-proxythat blocks traffic to Kubernetes node and sidecar proxy that blocks out of the Pod For more information, see Understanding Route Forwarding by the Envoy Sidecar Proxy in Istio Service Mesh .\nDisadvantages of Service Mesh\nBecause each node on Kubernetes many runs Pod, the original kube-proxyrouting forwarding placed in each pod, the distribution will lead to a lot of configuration, synchronization, and eventual consistency problems. In order to perform fine-grained traffic management, a series of new abstractions will be added, which will further increase the user’s learning costs. However, with the popularization of technology, this situation will gradually ease.\nAdvantages of Service Mesh\nkube-proxy The …","relpermalink":"/en/blog/service-mesh-the-microservices-in-post-kubernetes-era/","summary":"This article is a rework of previously written content and is included in the Istio Handbook of the ServiceMesher community . Other chapters are still being compiled.","title":"Service Mesh - The Microservices in Post Kubernetes Era"},{"content":"2020 is indeed a bad start. In less than a month, I hardly heard any good news:\nTrump assassinated Major General Sulaymani of Iran; this pneumonia outbreak in Wuhan; the news that my basketball icon Kobe died of a helicopter crash really shocked me, and the Lakers said goodbye on the 24th. This gave me another spiritual blow during the Spring Festival, which was originally lacking in interest.\n2020 is bound to be a year deeply remembered in all humankind. In the last few days of the first month of the year, I decided to revise the website. The first reason was that I could n’t go out during the extended Chinese New Year holiday, and it was boring at home. And many pictures on the website were saved on Weibo map beds. The picture bed is unstable, causing many photos to be irretrievable; coupled with a habit of organizing the website every long holiday (the last revision of the website was completed during the National Day holiday in 2018, completed at home for 7 days), so I decided to The website has been revised again and it has become what it is now.\nFeatures The website has the following features after this revision:\nReorganize the website content, the structure is more reasonable Support email subscription Images are stored on Github Responsive static website, card design, better user experience Everyone is welcome to enter your email address in the email input box at the bottom of the page. Once there is an important content update on this site, we will push you through the email as soon as possible.\n","relpermalink":"/en/notice/website-revision-notice/","summary":"In the last days of the first month of 2020, I decided to revamp the website.","title":"jimmysong.io website revision notice"},{"content":"A few days ago during the Mid-Autumn Festival, I translated Google’s Engineering Practices documentation , which is open source on Github. The original document Github address: https://github.com/google/eng-practices , the main content so far is summarized by Google How to conduct a Code Review guide, based on the title of the original Github repository, we will add more Google engineering practices in the future.\nGithub:https://github.com/rootsongjc/eng-practices Browse online: https://jimmysong.io/eng-practices The translation uses the same style and directory structure as the original document. In fact, if the original text has international requirements, it can be directly incorporated, but according to the translation suggestions submitted by several previous people, the author of this project does not recommend translation. The first is translation. The documents may be unmaintained, and the accuracy of the documents cannot be guaranteed.\nFor suggestions on Chinese integration, see:\nAdd Chinese translation #12 Add the link of Chinese version of code reviewer’s guide #8 ","relpermalink":"/en/notice/google-engineering-practices-zh/","summary":"Translated from Google's open source documentation on Github","title":"Chinese version of Google Engineering Practice Documents"},{"content":"The following paragraph is a release note from the Istio official blog https://istio.io/zh/blog/2019/announcing-1.1/ , which I translated.\nIstio was released at 4 a.m. Beijing time today and 1 p.m. Pacific time.\nSince the 1.0 release last July, we have done a lot to help people get Istio into production. We expected to release a lot of patches (six patches have been released so far!), But we are also working hard to add new features to the product.\nThe theme for version 1.1 is “Enterprise Ready”. We are happy to see more and more companies using Istio in production, but as some big companies join in, Istio also encounters some bottlenecks.\nThe main areas we focus on include performance and scalability. As people gradually put Istio into production and use larger clusters to run more services with higher capacity, there may be some scaling and performance issues. Sidecar takes up too much resources and adds too much latency. The control plane (especially Pilot) consumes excessive resources.\nWe put a lot of effort into making the data plane and control plane more efficient. In the 1.1 performance test, we observed that sidecars typically require 0.5 vCPU to process 1000 rps. A single Pilot instance can handle 1000 services (and 2000 pods) and consumes 1.5 vCPUs and 2GB of memory. Sidecar adds 5 milliseconds at the 50th percentile and 10 milliseconds at the 99th percentile (the execution strategy will increase latency).\nWe have also completed the work of namespace isolation. You can use the Kubernetes namespace to enforce control boundaries to ensure that teams do not interfere with each other.\nWe have also improved multi-cluster functionality and usability. We listened to the community and improved the default settings for flow control and policies. We introduced a new component called Galley. Galley validates YAML configuration, reducing the possibility of configuration errors. Galley is also used in multi-cluster setups-collecting service discovery information from each Kubernetes cluster. We also support other multi-cluster topologies, including single control planes and multiple synchronous control planes, without the need for flat network support.\nSee the release notes for more information and details .\nThere is more progress on this project. As we all know, Istio has many moving parts, and they take on too much work. To address this, we have recently established the Usability Working Group (available at any time). A lot happened in the community meeting (Thursday at 11 am) and in the working group. You can log in to discuss.istio.io with GitHub credentials to participate in the discussion!\nThanks to everyone who has contributed to Istio over the past few months-patching 1.0, adding features to 1.1, and extensive testing recently on 1.1. Special thanks to companies and users who work with us to install and upgrade to earlier versions to help us identify issues before they are released.\nFinally, go to the latest documentation and install version 1.1! Happy meshing!\nOfficial website The ServiceMesher community has been maintaining the Chinese page of the official Istio documentation since the 0.6 release of Istio . As of March 19, 2019, there have been 596 PR merges, and more than 310 documents have been maintained. Thank you for your efforts! Some documents may lag slightly behind the English version. The synchronization work is ongoing. For participation, please visit https://github.com/servicemesher/istio-official-translation. Istio official website has a language switch button on the right side of each page. You can always Switch between Chinese and English versions, you can also submit document modifications, report website bugs, etc.\nServiceMesher Community Website\nThe ServiceMesher community website http://www.servicemesher.com covers all technical articles in the Service Mesh field and releases the latest activities in a timely manner. It is your one-stop portal to learn about Service Mesh and participate in the community.\n","relpermalink":"/en/notice/istio-11/","summary":"Istio 1.1 was released at 4 am on March 20th, Beijing time. This version took 8 months! The ServiceMesher community also launched the Istio Chinese documentation.","title":"Istio 1.1 released"},{"content":"Istio handbook was originally an open source e-book I created (see https://jimmysong.io/istio-handbook ). It has been written for 8 months before donating to the ServiceMesher community. In order to further popularize Istio and Service Mesh technology, this book Donate to the community for co-authoring. The content of the original book was migrated to https://github.com/servicemesher/istio-handbook on March 10, 2019. The original book will no longer be updated.\nGitHub address: https://github.com/servicemesher/istio-handbook Reading address online: http://www.servicemesher.com/istio-handbook/ Conceptual picture of this book, cover photo of Shanghai Jing’an Temple at night , photo by Jimmy Song .\nThe publishing copyright of this book belongs to the blog post of Electronic Industry Press. Please do not print and distribute it without authorization.\nIstio is a service mesh framework jointly developed by Google, IBM, Lyft, etc., and began to enter the public vision in early 2017. As an important infrastructure layer that inherits Kubernetes and connects to the serverless architecture in the cloud-native era, Istio is of crucial importance important. The ServiceMesher community, as one of the earliest open source communities in China that is researching and promoting Service Mesh technology, decided to integrate community resources and co-author an open source e-book for readers.\nAbout this book This book originates from the rootsongjc / istio-handbook and the Istio knowledge map created by the ServiceMesher community .\nThis book is based on Istio 1.0+ and includes, but is not limited to , topics in the Istio Knowledge Graph .\nParticipate in the book Please refer to the writing guidelines of this book and join the Slack channel discussion after joining the ServiceMesher community .\n","relpermalink":"/en/notice/istio-handbook-by-servicemesher/","summary":"To further popularize Istio and Service Mesh technology, donate this book to the community for co-authoring.","title":"Donate Istio Handbook to the ServiceMesher community"},{"content":"Github: https://github.com/rootsongjc/cloud-native-sandbox Cloud Native Sandbox can help you setup a standalone Kubernetes and istio environment with Docker on you own laptop.\nThe sandbox integrated with the following components:\nKubernetes v1.10.3 Istio v1.0.4 Kubernetes dashboard v1.8.3 Differences with kubernetes-vagrant-centos-cluster As I have created the kubernetes-vagrant-centos-cluster to set up a Kubernetes cluster and istio service mesh with vagrantfile which consists of 1 master(also as node) and 3 nodes, but there is a big problem that it is so high weight and consume resources. So I made this light weight sandbox.\nFeatures\nNo VirtualBox or Vagrantfile required Light weight High speed, low drag Easy to operate Services\nAs the sandbox setup, you will get the following services.\nRecord with termtosvg .\nPrerequisite You only need a laptop with Docker Desktop installed and Kubernetes enabled .\nNote: Leave enough resources for Docker Desktop. At least 2 CPU, 4G memory.\nInstall To start the sandbox, you have to run the following steps.\nKubernetes dashboard(Optional) Install Kubernetes dashboard.\nkubectl apply -f install/dashbaord/ Get the dashboard token.\nkubectl -n kube-system describe secret default| awk \u0026#39;$1==\u0026#34;token:\u0026#34;{print $2}\u0026#39; Expose kubernetes-dashboard service.\nkubectl -n kube-system get pod -l k8s-app=kubernetes-dashboard -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39; Login to Kubernetes dashboard on http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login with the above token.\nIstio(Required) Install istio service mesh with the default add-ons.\n# Install istio kubectl apply -f install/istio/ To expose service grafana on http://localhost:3000 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 3000:3000 \u0026amp; To expose service prometheus on http://localhost:9090 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=prometheus -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 9090:9090 \u0026amp; To expose service jaeger on http://localhost:16686 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=jaeger -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 16686:16686 \u0026amp; To expose service servicegraph on http://localhost:8088/dotviz , http://localhost:8088/force/forcegraph.html .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=servicegraph -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 8088:8088 \u0026amp; Kiali Install kiali .\nkubectl -n istio-system apply -f install/kiali To expose service kiali on http://localhost:20001 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=kiali -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 20001:20001 \u0026amp; Bookinfo sample Deploy bookinfo sample .\n# Enable sidecar auto injection kubectl label namespace default istio-injection=enabled # Deploy bookinfo sample kubectl -n default apply -f sample/bookinfo Visit productpage on http://localhost/productpage .\nLet’s generate some loads.\nfor ((i=0;i\u0026lt;1000;i=i+1));do echo \u0026#34;Step-\u0026gt;$i\u0026#34;;curl http://localhost/productpage;done You can watch the service status through http://localhost:3000 .\nClient tools To operate the applications on Kubernetes, you should install the following tools.\nRequired\nkubectl - Deploy and manage applications on Kubernetes. istioctl - Istio configuration command line utility. Optional\nkubectx - Switch faster between clusters and namespaces in kubectl kube-ps1 - Kubernetes prompt info for bash and zsh ","relpermalink":"/en/blog/cloud-native-sandbox/","summary":"A standalone Kubernetes and Istio environment with Docker on you own laptop","title":"Cloud Native Sandbox"},{"content":"This video was recorded on the last day of 2018 and demonstrates the use of rootsongjc/kubernetes-vagrant-centos-cluster to automatically deploy a Kubernetes cluster and Istio Service Mesh.\nA few days ago I mentioned that kubernetes-vagrant-centos-cluster released v1.2.0 version to deploy a cloud-native experimental environment with one click. Someone in the Kubernetes and Service Mesh community asked me a long time ago to make a video to explain and demonstrate how to install Kubernetes and Istio Service Mesh, because I’m always busy, I’ve always made some time. Today, I will give a demo video, just don’t watch the video for a few minutes. In order to make this video, it took me half an hour to record, two hours to edit, and many years of shooting. , Editing, containers, virtual machines, Kubernetes, service grid experience. This is not so much a farewell as a new beginning.\nBecause the video was first posted on YouTube , it was explained in English (just a few supplementary instructions, it does n’t matter if you do n’t understand, just read the Chinese documentation on GitHub ).\nSkip to bilibli to watch . If you are interested in drone aerial photography, you can also take a look at Jimmy Song’s aerial photography works . Please support the coin or like, thank you.\nIf you have any questions, you can send a barrage or comment below the video.\nPS. Some people will ask why you chose to use bilibli, because there are no ads for watching videos on this platform, and most of them are uploaded by the Up master. Although the two-dimensional elements are mostly, the community atmosphere is still good.\nFor more exciting videos, visit Jimmy Song’s bibli homepage .\n","relpermalink":"/en/notice/cloud-native-kubernetes-service-mesh-local-demo-show/","summary":"This video was recorded by me on the last day of 2018 and demonstrates the use of rootsongjc/kubernetes-vagrant-centos-cluster to automatically deploy Kubernetes clusters and Istio Service Mesh.","title":"Kubernetes and Istio Service Mesh Cloud Native Local Video Demo Show"},{"content":"Updated at Mar 8, 2022\nThis article uses Istio’s official bookinfo sample to explain how Envoy performs routing forwarding after the traffic entering the Pod and forwarded to Envoy sidecar by iptables, detailing the inbound and outbound processing. For a detailed analysis of traffic interception, see Understanding Envoy Sidecar Proxy Injection and Traffic Interception in Istio Service Mesh .\nOverview of Sidecar Injection and Traffic Interception Steps Below is an overview of the steps from Sidecar injection, Pod startup to Sidecar proxy interception traffic and Envoy processing routing.\nKubernetes automatically injected through Admission Controller, or the user run istioctl command to manually inject sidecar container. Apply the YAML configuration deployment application. At this time, the service creation configuration file received by the Kubernetes API server already includes the Init container and the sidecar proxy. Before the sidecar proxy container and application container are started, the Init container started firstly. The Init container is used to set iptables (the default traffic interception method in Istio, and can also use BPF, IPVS, etc.) to Intercept traffic entering the pod to Envoy sidecar Proxy. All TCP traffic (Envoy currently only supports TCP traffic) will be Intercepted by sidecar, and traffic from other protocols will be requested as originally. Launch the Envoy sidecar proxy and application container in the Pod. Sidecar proxy and application container startup order issues\nStart the sidecar proxy and the application container. Which container is started first? Normally, Envoy Sidecar and the application container are all started up before receiving traffic requests. But we can’t predict which container will start first, so does the container startup order have an impact on Envoy intercepting traffic? The answer is yes, but it is divided into the following two situations.\nCase 1: The application container starts first, and the sidecar proxy is still not ready\nIn this case, the traffic is transferred to the 15001 port by iptables, and the port is not monitored in the Pod. The TCP link cannot be established and the request fails.\nCase 2: Sidecar starts first, the request arrives and the application is still not ready\nIn this case, the request will certainly fail. As for the step at which the failure begins, the reader is left to think.\nQuestion : If adding a readiness and living probe for the sidecar proxy and application container can solve the problem?\nTCP requests that are sent or received from the Pod will be intercepted by iptables. After the inbound traffic is intercepted, it is processed by the Inbound Handler and then forwarded to the application container for processing. The outbound traffic is intercepted by iptables and then forwarded to the Outbound Handler for processing. Upstream and Endpoint. Sidecar proxy requests Pilot to use the xDS protocol to synchronize Envoy configurations, including LDS, EDS, CDS, etc., but to ensure the order of updates, Envoy will use ADS to request configuration updates from Pilot directly. How Envoy handles route forwarding The following figure shows a productpageservice access request http://reviews.default.svc.cluster.local:9080/, when traffic enters reviews the internal services, reviews internal services Envoy Sidecar is how to do traffic blocked the route forward.\nIstio transparent traffic intercepting and traffic routing schematic Before the first step, productpage Envoy Sidecar Pod has been selected by EDS of a request to reviews a Pod service of its IP address, it sends a TCP connection request.\nThe Envoy configuration in the official website of Istio is to describe the process of Envoy doing traffic forwarding. The party considering the traffic of the downstream is to receive the request sent by the downstream. You need to request additional services, such as reviews service requests need Pod ratings service.\nreviews, there are three versions of the service, there is one instance of each version, three versions sidecar similar working steps, only to later reviews-v1-cb8655c75-b97zc Sidecar flow Pod forwarding this step will be described.\nUnderstanding the Inbound Handler The role of the inbound handler is to transfer the traffic from the downstream intercepted by iptables to localhost to establish a connection with the application container inside the Pod.\nLook reviews-v1-cb8655c75-b97zc at the Listener in the pod.\nRun istioctl pc listener reviews-v1-cb8655c75-b97zc to see what the Pod has a Listener.\nADDRESS PORT TYPE 172.33.3.3 9080 HTTP \u0026lt;--- Receives all inbound traffic on 9080 from listener 0.0.0.0_15006 10.254.0.1 443 TCP \u0026lt;--+ 10.254.4.253 80 TCP | 10.254.4.253 8080 TCP | 10.254.109.182 443 TCP | 10.254.22.50 15011 TCP | 10.254.22.50 853 TCP | 10.254.79.114 443 TCP | 10.254.143.179 15011 TCP | 10.254.0.2 53 TCP | Receives outbound non-HTTP traffic for relevant IP:PORT pair from listener 0.0.0.0_15001 10.254.22.50 443 TCP | …","relpermalink":"/en/blog/understanding-how-envoy-sidecar-intercept-and-route-traffic-in-istio-service-mesh/","summary":"Details about Envoy sidecar with iptables rules.","title":"Understanding How Envoy Sidecar Intercept and Route Traffic in Istio Service Mesh"},{"content":"KubeCon \u0026amp; CloudNative in North America is the most worthy cloud-native event every year. This time it will be held in Seattle for four days, from December 10th to 13th, refer to the official website of the conference . This year, 8,000 people participated. You should notice that Kubernetes has become more and more low-level. Cloud-native application developers do not need to pay much attention to it. Major companies are publishing their own cloud-native technology stack layouts, including IBM, VMware, SAP, Startups around this ecosystem are still emerging. The PPT sharing address of the local conference: https://github.com/warmchang/KubeCon-North-America-2018 , thank you William Zhang for finishing and sharing the slides of this conference .\nSeattle scene Janet Kuo from Google describes the path to cloud-native technology adoption.\nThe same event of KubeCon \u0026amp; CloudNativeCon, the scene of the first EnvoyCon.\nAt KubeCon \u0026amp; CloudNativeCon Seattle, various directors, directors, directors, VPs, and Gartner analysts from IBM, Google, Mastercard, VMware, and Gartner are conducting live discussions on the topic of Scaling with Service Mesh and Istio. Why do we talk about Istio when we talk about Service Mesh? What is not suitable for Istio use case. . .\nThe PPT contains basic introduction, getting started, a total of more than 200 Deep Dive as well as practical application, we recommend you according to the General Assembly’s official website to choose topics of interest to look at the schedule, otherwise I might see, however.\nA little impression KubeCon \u0026amp; CloudNativeCon is held three times a year, Europe, China and North America. China is the first time this year, and it will be held in Shanghai in November. It is said that it will be held in June next year. Although everyone said that Kubernetes has become boring, the conference about Kubernetes There is still a lot of content, and the use of CRD to extend Kubernetes usage is increasing. Service Mesh has begun to become hot. As can be seen from the live pictures above, there are a large number of participants on the site and related topics are also increasing. It is known as a microservice in the post-Kubernetes era . This must be It will be an important development direction of cloud native after Kubernetes, and the ServiceMesher community pays close attention to it.\n","relpermalink":"/en/notice/kubecon-cloudnativecon-seattle-2018/","summary":"KubeCon \u0026 CloudNativeCon Seattle 2018 Data Sharing.","title":"Kubecon\u0026CloudNativeCon Seattle 2018"},{"content":"This is a postscript from the post- Kubernetes era. Just this evening I saw a post by Bilgin Ibryam Microservices in a Post-Kuberentes Era .\nOn April 9, 2017, the Kubernetes Handbook-Kubernetes Chinese Guide / Cloud Native Application Architecture Practice Manual was first submitted. In the past 16 months, 53 contributors participated, 1,088 commits, and a total of 23,9014 Chinese characters were written. At the same time , thousands of enthusiasts have gathered in the Kubernetes \u0026amp; Cloud Native combat group .\nIt has been more than 4 months since the previous version was released. During this period, Kubernetes and Prometheus graduated from CNCF respectively and have matured commercially. These two projects have basically taken shape and will not change much in the future. Kubernetes was originally developed for container orchestration. In order to solve the problem of microservice deployment, Kubernetes has gained popularity. The current microservices have gradually entered the post-Kubernetes era . Service Mesh and cloud native redefine microservices and distributed applications.\nWhen this version was released, the PDF size was 108M with a total of 239,014 Chinese characters. It is recommended to browse online , or clone the project and install the Gitbook command to compile it yourself.\nThis version has the following improvements:\nAdded Istio Service Mesh tutorial Increased use VirtualBox and Vagrant set up in a local cluster and distributed Kubernetes Istio Service Mesh Added cloud native programming language Ballerina and Pulumi introduced Added Quick Start Guide Added support for Kubernetes 1.11 Added enterprise-level service mesh adoption path guide Added SOFAMesh chapter Added vision for the cloud-native future Added CNCF charter and participation Added notes for Docker image repositories Added Envoy chapter Increased KCSP (Kubernetes certification service providers) and CKA (Certified Kubernetes administrator) instructions Updated some configuration files, YAML and reference links Updated CRI chapter Removed obsolete description Improved etcdctl command usage tutorial Fixed some typos Browse and download Browse online https://jimmysong.io/kubernetes-handbook To make it easy for everyone to download, I put a copy on Weiyun , which is available in PDF (108MB), MOBI (42MB), and EPUB (53MB). In this book, there are more practical tutorials. In order to better understand the principles of Kubernetes, I recommend studying ** In- depth analysis of Kubernetes by Zhang Lei, produced by Geek Time **.\nThank you Kubernetes for your support of this book. Thank you Contributors . In the months before this version was released, the ServiceMesher community was co-founded . As a force in the post-Kubernetes era , welcome to contact me to join the community and create cloud native New era .\nAt present , the WeChat group of the ServiceMesher community also has thousands of members. The Kubernete Handbook will continue, but Service Mesh is already a rising star. With Kubernetes in mind, welcome to join the ServiceMesher community and follow us. The public account of the community (also the one I manage).\n","relpermalink":"/en/notice/new-kubernetes-handbook-released-and-say-hello-to-post-kubernetes-era/","summary":"This is an obituary post-Kubernetes era. Kubernetes handbook by Jimmy Song v1.4 is released. The next focus of cloud native is Service Mesh!","title":"Kubernetes Handbook v1.4 is released"},{"content":"Today I am honored to announce that I have become a CNCF Ambassador . Here is my story with Cloud Native.\nOrigin The first time to attend the Cloud Native Computing Foundation is at the LC3 in Beijing 2017. I attended the meeting again this year, and in November of this year, CNCF will hold the KubeCon \u0026amp; CloudNativeCon for the first time in Shanghai, China. I’ll be there too.\nCloud Native Books My origins with the Cloud Native is originated from Kevin Hoffman’s book Cloud Native Go . I translated this book at the end of 2016. Since then, in China, the translation of the word Cloud Native has not been determined, we introduced it with 云原生 to China.\nAnd then I begin to write the kubernetes-handbook on GitHub. So far, it has more than 2000 stars. This book has written more than 200,000 Chinese characters, the first commit happened on April 14, 2017.\nSince the the book Cloud Native Go completed, the publisher recommended another Cloud Native book to me - Cloud Native Python by Manish Sethi.\nAnd the book Cloud Native Java by Josh Long and Kenny Bastani.\nIn March 2018, with the hope that Bring the world equal opportunities and Building a Financial Cloud Native Infrastructure, I joined the Ant Group .\nServiceMesher Community By the time of May 2018, I start to organize the ServiceMesher community.\nIn the last few months, we work with other open source communities in China, such as k8smeetup , Sharding-Sphere , Apache SkyWalking . Our community has grown to have 1,700 members and two round meetups in Hangzhou and Beijing till now.\nMore than 300 people participated in the scene and more than 20,000 people watched it live by IT 大咖说 。\nFuture Here are some hopes of mine:\nOpen source culture become popular in China More and more people would like to be involved in open source projects Host one open source project into the CNCF A book related to Cloud Native or Service Mesh Strengthen cultural exchanges between China and the global Finally, welcome to China for traveling or share your topic with us on Cloud Native, and in the mean while we will share our experience on large scale web apps to the world. Hope to hear your voice!\n","relpermalink":"/en/blog/cloud-native-and-me-the-past-current-and-future/","summary":"Today I am honored to announce that I have become a CNCF Ambassador.","title":"Cloud Native With Me - The Past, Current and Future"},{"content":"Today, we are pleased to announce Istio 1.0 . It’s been over a year since the original 0.1 release. Since 0.1, Istio has grown rapidly with the help of a thriving community, contributors, and users. Many companies have successfully applied Istio to production today and have gained real value through the insight and control provided by Istio. We help large businesses and fast-growing startups such as eBay , Auto Trader UK , Descartes Labs , HP FitStation , Namely , PubNub and Trulia to connect, manage and protect their services from scratch with Istio. The release of this version as 1.0 recognizes that we have built a core set of features that users can rely on for their production.\nEcosystem Last year we saw a significant increase in the Istio ecosystem. Envoy continues its impressive growth and adds many features that are critical to a production-level service grid. Observability providers like Datadog , SolarWinds , Sysdig , Google Stackdriver, and Amazon CloudWatch have also written plugins to integrate Istio with their products. Tigera , Aporeto , Cilium and Styra have built extensions for our strategy implementation and network capabilities. Kiali built by Red Hat provides a good user experience for grid management and observability. Cloud Foundry is building the next-generation traffic routing stack for Istio, the recently announced Knative serverless project is doing the same, and Apigee has announced plans to use it in their API management solution. These are just a few of the projects that the community added last year.\nFeatures Since the 0.8 release, we have added some important new features, and more importantly, marked many existing features as Beta to indicate that they can be used in production. This is covered in more detail in the release notes , but it is worth mentioning:\nMultiple Kubernetes clusters can now be added to a single grid , enabling cross-cluster communication and consistent policy enforcement. Multi-cluster support is now Beta. The network API for fine-grained control of traffic through the grid is now Beta. Explicitly modeling ingress and egress issues with gateways allows operations personnel to control the network topology and meet access security requirements at the edge. Two-way TLS can now be launched incrementally without updating all clients of the service. This is a key feature that removes the barriers to deploying Istio on existing production. Mixer now supports developing out-of-process adapters . This will be the default way to extend Mixer in an upcoming release, which will make it easier to build adapters. Envoy now fully evaluates the authorization policies that control service access locally , improving their performance and reliability. Helm chart installation is now the recommended installation method, with a wealth of customization options to configure Istio to your needs. We put a lot of effort into performance, including continuous regression testing, large-scale environmental simulation, and target repair. We are very happy with the results and will share details in the coming weeks. Next step Although this is an important milestone for the project, much work remains to be done. When working with adopters, we’ve received a lot of important feedback about what to focus next. We’ve heard consistent topics about supporting hybrid clouds, installing modularity, richer network capabilities, and scalability for large-scale deployments. We have considered some feedback in the 1.0 release and we will continue to actively work on it in the coming months.\nQuick start If you are new to Istio and want to use it for deployment, we would love to hear from you. Check out our documentation , visit our chat forum or visit the mailing list . If you want to contribute more to the project, please join our community meeting and say hello.\nAt last The Istio team is grateful to everyone who contributed to the project. Without your help, it won’t have what it is today. Last year’s achievements were amazing, and we look forward to achieving even greater achievements with our community members in the future.\nThe ServiceMesher community is responsible for the translation and maintenance of Chinese content on Istio’s official website. At present, the Chinese content is not yet synchronized with the English content. You need to manually enter the URL to switch to Chinese ( https://istio.io/zh ). There is still a lot of work to do , Welcome everyone to join and participate.\n","relpermalink":"/en/notice/istio-v1-released/","summary":"Chinese documentation is released at the same time!","title":"Istio 1.0 is released"},{"content":" If there is a visual learning model and platform that provides infrastructure clusters for your operation, would you pay for it?\nTwo months ago, I met Jord in Kubernetes’s Slack channel, and later saw the link of MagicSandbox.io he (and possibly others) sent in the Facebook group of Taiwan Kubernetes User Group, and I clicked to apply for a trial Then, I received an email from Jord later, and he told me that he wanted to build a Kubernetes learning platform. That’s where the whole thing started, and then we had a couple of Zoom video chats for a long time.\nAbout MagicSandbox MagicSandbox is a startup company. Jord (Dutch) is also a serial entrepreneur. He has studied at Sichuan University in China for 4 years since he was 19, and then returned to Germany. He worked as a PM at Boston Consulting Group and now works at Entrepreneur. First (Europe’s top venture capital / enterprise incubator) is also located in Berlin, Germany. He met Mislav (Croatian). Mislav is a full-stack engineer and has several entrepreneurial experiences. They have similar odors, and they hit it off. Decided to be committed to the Internet education industry and create a world-class software engineer education platform. They want to start with Kubernetes, first provide Internet-based Kubernetes theory and practice teaching, and then expand the topic to ElasticSearch, GraphQL, and so on. topic.\nJord founded MagicSandbox in his home, and I became the face of MagicSandbox in China.\nNow we are going to release the MagicSandbox Alpha version. This version is an immature version and is provided for everyone to try for free. Positive feedback is also welcome.\nOfficial homepage: https://magicsandbox.com/ Chinese page: https://cn.magicsandbox.com/ (The content has not been finished yet, only the Chinese version homepage is currently provided) Follow us on Twitter: https://twitter.com/magicsandbox ","relpermalink":"/en/notice/magicsandbox-alpha-version-annoucement/","summary":"Online practical software engineering education platform.","title":"MagicSandbox Alpha released"},{"content":"Remember the cloud-native programming language I shared before finally appeared! Learn about Ballerina in one article! ? They are ready to attend the KubeCon \u0026amp; CloudNativeCon China Conference!\nKubeCon \u0026amp; CloudNativeCon China Conference will be held on November 14-15, 2018 (Wednesday, Thursday) in Shanghai . See: https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2018/ With Ballerina’s official authorization, I now need to help them find an “ambassador” in China, responsible for team guidance, Chinese and English translation, and familiarity with Cloud Native and microservices. It has influence in the industry and has no barriers to English communication.\nAmbassador duties\nTeam Leadership Responsible for Chinese and English translation of product declaration, PPT materials, etc. Help to arrange the booth The other party can provide\nConference tickets Travel expenses Accommodation during the conference Other compensation This is a photo of their team in front of their booth during the KubeCon \u0026amp; CloudNativeCon in Hagen in May this year.\nPS This is the most complete and best picture I have ever found of their team. (Photography needs to be strengthened)\nLet’s briefly introduce this startup called Ballerina. Their team is mainly from Sri Lanka. This is an island country next to India in the South Asian subcontinent. In ancient China, it was called “Lion Country” and rich in gemstones.\nThe capital of their country is Sri Lanka, which is pronounced in their own language: Si li jia ya wa de na pu la ke te\nIf you are interested, please contact me directly.\n","relpermalink":"/en/notice/a-ballerina-china-ambassador-required/","summary":"With official authorization from Ballerina, I now need to help them find an ambassador in China.","title":"Ballerina seeks Chinese ambassador"},{"content":"Envoy-Designed for cloud-native applications, open source edge and service proxy, Istio Service Mesh default data plane, Chinese version of the latest official document, dedicated by the ServiceMesher community, welcome everyone to learn and share together.\nTL;DR: http://www.servicemesher.com/envoy/ PDF download address: servicemesher/envoy This is the first time Community Service Mesh enthusiasts group activities, the document is based on Envoy latest (version 1.7) Official documents https://www.envoyproxy.io/docs/envoy/latest/ . A total of 120 articles with 26 participants took 13 days and 65,148 Chinese characters.\nVisit online address: http://www.servicemesher.com/envoy/ Note : This book does not include the v1 API reference and v2 API reference sections in the official documentation. Any links to API references in the book will jump directly to the official page.\nContributor See the contributor page: https://github.com/servicemesher/envoy/graphs/contributors Thanks to the above contributors for their efforts! Because the level of translators is limited, there are inevitably inadequacies in the text. I also ask readers to correct them. Also welcome more friends to join our GitHub organization: https://github.com/servicemesher ","relpermalink":"/en/notice/envoyproxy-docs-cn-17-release/","summary":"Translated by ServiceMesher community.","title":"Chinese version of the latest official document of Envoy released"},{"content":"Envoy is an open source L7 proxy and communication bus written in C ++ by Lyft. It is currently an open source project under CNCF . The code is hosted on GitHub. It is also the default data plane in the Istio service mesh. We found that it has very good performance, and there are also continuous open source projects based on Envoy, such as Ambassador , Gloo, etc. At present, the official documentation of Envoy has not been well finished, so we Service Service enthusiasts feel that they are launching the community The power of the co-translation of the latest (version 1.7) official documentation of Enovy and organization through GitHub.\nService Mesh enthusiasts have jointly translated the latest version of the official document of Envoy . The translated code is hosted at https://github.com/servicemesher/envoy . If you are also a Service Mesh enthusiast, you can join the SerivceMesher GitHub organization and participate together.\nThe official Envoy document excludes all articles in the two directories of the v1 API reference and the v2 API reference. There are more than 120 documents. The length of the documents varies. The original English official documents use the RST format. I manually converted them into Markdown format and compiled using Gitbook. A GitHub Issue was generated according to the path of the document relative to the home directory. Friends who want to participate in translation can contact me to join the ServiceMesher organization, and then select the article you want to translate in Issue , and then reply “Claim”.\nHere you can see all the contributors. In the future, we will also create a Service Mesh enthusiast website. The website uses static pages. All code will be hosted on Github. Welcome everyone to participate.\n","relpermalink":"/en/notice/enovy-doc-translation-start/","summary":"The SerivceMesher community is involved in translating the official documentation for the latest version of Envoy.","title":"Envoy's latest official document translation work started"},{"content":"TL; DR Click here to download the PDF of this book .\nRecently, Michael Hausenblas of Nginx released a booklet on container networks in docker and kubernetes. This 72-page material is a good introduction for everyone to understand the network in Docker and Kubernetes from shallow to deep.\nTarget audience Container Software Developer SRE Network Operation and Maintenance Engineer Architects who want to containerize traditional software ","relpermalink":"/en/notice/container-networking-from-docker-to-kubernetes-nginx/","summary":"Source from Nginx, published by O’Reilly.","title":"Docker container network book sharing"},{"content":"In 2017, we are facing a big era of architectural changes, such as Kubernetes ending the battle for container orchestration, Kafka release 1.0, serverless gradually gaining momentum, edge computing to replace cloud computing, Service Mesh ready to go, and artificial intelligence to empower business , Also brings new challenges to the architecture.\nI am about to participate in InfoQ’s ArchSummit Global Architects Summit on December 8-11 in Beijing . This conference also invited 100+ top technologists such as Dr. Ali Wangjian to share and summarize the architectural changes and reflections this year. I hope that you can build on this conference, summarize past practices, and look forward to a future-oriented architecture to transform this era of change into the common fortune of each of us.\nMy speech The content of my speech is from Kubernetes to Cloud Native-the road to cloud native applications . Link: From Kubernetes to Cloud Native-the road to cloud native applications . The time is Saturday, December 9, 9:30 am, in the fifth meeting room.\nAfter more than ten years of development of cloud computing, the new phase of cloud native has entered. Enterprise applications are preferentially deployed in cloud environments. How to adapt to the cloud native tide, use containers and Kubernetes to build cloud native platforms, and practice DevOps concepts and agility How IT, open source software, and the community can help IT transform, the solution to all these problems is the PaaS platform, which is self-evident to the enterprise.\nWe also prepared gifts for everyone: “Cloud Native Go-Building Cloud Native Web Applications Based on Go and React” and “Intelligent Data Era-Enterprise Big Data Strategy and Practice” There is also a book stand for the blog post of the Electronic Industry Press. Welcome to visit.\nArchSummit conference official website link: http://bj2017.archsummit.com/ For more details, please refer to the official website of the conference: http://bj2017.archsummit.com/ ","relpermalink":"/en/notice/archsummit-beijing-2017-from-kubernetes-to-cloud-native/","summary":"I will give a lecture at ArchSummit Beijing. From Kubernetes to Cloud Native, my path to cloud native applications.","title":"ArchSummit Beijing 2017 speech preview"},{"content":"Cloudinary-go is a Go client library and CLI tool to upload static assets to the Cloudinary service.\nInstallation Install the CLI tool and the library with:\ngo get github.com/rootsongjc/cloudinary-go/cloudinary Or download the release binary from release .\n","relpermalink":"/en/notice/cloudinary-go/","summary":"Cloudinary-go is a Go client library and CLI tool to upload static assets to Cloudinary service","title":"Cloudinary file upload tool written in Go released"},{"content":"Many people asked me how jimmysong.io made this website. I think it is necessary to write a book to popularize the knowledge of static website construction and Hugo as a tool.\nThis manual will guide you how to use Hugo to build a static website for personal blog or project display.\nTeach you how to build a static website from scratch. This does not require much programming and development experience and time investment, and basically does not require much cost (except for personalized domain names). You can quickly build and Launch a website.\nGithub address: https://github.com/rootsongjc/hugo-handbook Gitbook access address: https://jimmysong.io/hugo-handbook The content of this book will continue to improve over time and as my site improves, so stay tuned.\n","relpermalink":"/en/notice/building-static-website-with-hugo/","summary":"A manual for static website building, and a personal blog Gitbook using Hugo.","title":"Hugo Handbook is released"},{"content":"Kevin Hoffman(From Capital One, twitter @KevinHoffman ) was making a speech on TalkingData T11 Smart Data Summit.\nHe addressed that 15 Factors of Cloud Native which based on Heroku’s original Twelve-Factor App , but he add more 3 another factors on it.\nLet’s have a look at the 15 factors of Cloud Native.\n1. One codebase, one App Single version-controlled codebase, many deploys Multiple apps should not share code Microservices need separate release schedules Upgrade, deploy one without impacting others Tie build and deploy pipelines to single codebase 2. API first Service ecosystem requires a contract Public API Multiple teams on different schedulers Code to contract/API, not code dependencies Use well-documented contract standards Protobuf IDL, Swagger, Apiary, etc API First != REST first RPC can be more appropriate in some situations 3. Dependency Management Explicitly declare dependencies Include all dependencies with app release Create immutable build artifact (e.g. docker image) Rely on smallest docker image Base on scratch if possible App cannot rely on host for system tools or libraries 4. Design, Build, Release, Run Design part of iterative cycle Agile doesn’t mean random or undesigned Mature CI/CD pipeline and teams Design to production in days not months Build immutable artifacts Release automatically deploys to environment Environments contains config, not release artifact 5. Configuration, Credentials, Code “3 Cs” volatile substances that explode when combinded Password in a config file is as bad as password in code App must accept “3 Cs” from environment and only use harmless defaults Test - Could you expose code on Github and not reveal passwords, URLs, credentials? 6. Logs Emit formatted logs to stdout Code should not know about destination or purpose of log emissions Use downstream log aggregator collect, store, process, expose logs ELK, Splunk, Sumo, etc Use structured logs to allow query and analysis JSON, csv, KV, etc Logs are not metrics 7. Disposability App must start as quickly as possible App must stop quickly and gracefully Processes start and stop all the time in the cloud Every scale up/down disposes of processes Slow dispose == slow scale Slow dispose or startup can cause availability gaps 8. Backing Services Assume all resources supplied by backingservices Cannotassume mutable file system “Disk as a Service” (e.g. S3, virtual mounts, etc) Every backing service is bound resource URL, credentials, etc-\u0026gt; environment config Host does not satisfy NFRs Backing services and cloud infrastructure 9. Environment Parity “Works on my machine” Cloud-native anti-pattern. Must work everywhere Every commit is candidate for deployment Automated acceptance tests Provide no confidence if environments don’t match 10. Administrative Processes Database migrations Run-once scripts or jobs Avoid using for batch operations, consider instead: Event sourcing Schedulers Triggers from queues, etc Lambdas/functions 11. Port Binding In cloud, infrastructure determines port App must accept port assigned by platform Containers have internal/external ports App design must embrace this Never use reserved ports Beware of container “host mode” networking 12. Stateless Processes What is stateless? Long-term state handled by a backing service In-memory state lives onlyas long as request Requests from same client routed to different instances “Sticky sessions” cloud native anti-pattern 13. Concurency Scale horizontally using the process model Build disposable, stateless, share-nothing processes Avoid adding CPU/RAM to increase scale/throughput Where possible, let platform/libraries do threading Many single-threaded services \u0026gt; 1 multi-threaded monolith 14. Telemetry Monitor apps in the cloud like satellite in orbit No tether, no live debugger Application Perf Monitoring (APM) Domain Telemetry Health and system logs 15. Authentication \u0026amp; Authorization Security should never be an afterthought Auth should be explicit, documented decision Even if anonymous access is allowed Don’t allow anonymous access Bearer tokens/OAuth/OIDC best practices Audit all attempts to access Migrating Monoliths to the Cloud After this 15 factors, he also gave us some tips about how to migrate monoliths to the Cloud:\nMake a rule - stop adding to the monolith All new code must be cloud native Prioritize features Where will you get most benefit from cloud native? Come up with a plan Decompose monolith over time Fast, agile iterations toward ultimate goal Use multiple strategies and patterns Go - the Best Language for Building Cloud Native App At last, he advise us the programming language Go is the best language to build Cloud Native applications for these reasons below:\nLightweight Easily learning curve Compiles to native binaries Very fast Large, thriving, engaged community http://gopherize.me Kevin also wrote a book Cloud Native Go to show how to Building Web Applications and Microservices for the Cloud with Go and React. This book has been …","relpermalink":"/en/blog/high-level-cloud-native-from-kevin-hoffman/","summary":"Kevin Hoffman address that 15 Factors of Cloud Native.","title":"High Level Cloud Native From Kevin Hoffman"},{"content":"From now on I have my own independent domain name jimmysong.io , the website is still hosted on GitHub, the original URL https://jimmysong.io is still accessible.\nWhy use .io as the suffix? Because this is The First Step to Cloud Native!\nWhy choose today? Because today is August 18, the days are easy to remember.\nPS domain names are registered in namecheap and cost tens of dollars / year.\nProudly powered by hugo 🎉🎊🎉\n","relpermalink":"/en/notice/domain-name-jimmysong-io/","summary":"From now on I have my own independent domain name jimmysong.io.","title":"New domain name jimmysong.io"},{"content":"This is a list of software, tools, architecture and reference materials about Cloud Native. It is awesome-cloud-native , a project I opened on GitHub , and can also be browsed through the web page .\nIt is divided into the following areas:\nAwesome Cloud Native\nAI API gateway Big Data Container engine CI-CD Database Data Science Fault tolerant Logging Message broker Monitoring Networking Orchestration and scheduler Portability Proxy and load balancer RPC Security and audit Service broker Service mesh Service registry and discovery Serverless Storage Tracing Tools Tutorial This list will be continuously updated and improved in the future, not only for my usual research records, but also as a reference for the Cloud Native industry.\n","relpermalink":"/en/notice/awesome-cloud-native/","summary":"This is a list of software, tools, architecture, and reference materials about Cloud Native. It is a project I started on GitHub.","title":"Awesome Cloud Native list is released"},{"content":"This book is the Chinese version of Migrating to Cloud Native Application Architectures. The English version of this book was released in February 2015. The Chinese version was translated by Jimmy Song and published in July 2017.\nGitHub hosting address for this book: https://github.com/rootsongjc/migrating-to-cloud-native-application-architectures Gitbook reading address: https://jimmysong.io/migrating-to-cloud-native-application-architectures The application architectures discussed in this book include:\nTwelve-factor application: A collection of cloud-native application architecture patterns Microservices: independently deployed services, each service does one thing Self-service agile infrastructure: a platform that provides application environments and back-office services quickly, repeatably, and consistently API-based collaboration: published and versioned APIs that allow interactions between services in a cloud-native application architecture Pressure resistance: a system that becomes stronger under pressure ","relpermalink":"/en/notice/changes-needed-to-cloud-native-archtecture/","summary":"This book is translated from an eBook published by Matt Stine in February 2015.","title":"Migrating to Cloud Native Chinese version released"}]