[{"content":"Istio 1.14 was released in June of this year, and one of the most notable features of this release is support for SPIRE , which is one of the implementations of SPIFFE , a CNCF incubation project. This article explains what SPIRE means for zero-trust architectures and why you would need SPIRE for authentication in Istio.\nAuthentication in Kubernetes We all know that Istio was built for and typically runs on Kubernetes, so before we talk about how to use SPIRE for authentication in Istio, let’s take a look at how Kubernetes handles authentication.\nLet’s look at an example of a pod’s token. Whenever a pod gets created in Kubernetes, it gets assigned a default service account from the namespace, assuming we didn’t explicitly assign a service account to it. Here is an example:\napiVersion: v1 data: ca.crt: {CA_CRT} namespace: ZGVmYXVsdA== token: {TOKEN_STRING} kind: Secret metadata: annotations: kubernetes.io/service-account.name: sleep kubernetes.io/service-account.uid: 2c0d00e8-13a2-48d0-9ff8-f987f3325ecf creationTimestamp: \"2022-06-14T03:01:35Z\" name: sleep-token-gwhwd namespace: default resourceVersion: \"244535398\" uid: b8822ceb-9553-4a17-96dc-d525bbaed0e0 type: kubernetes.io/service-account-token Kubernetes manages the identity of Pods with Service Accounts and then specifies the permissions of Pods with a Service Account to the Kubernetes API using RBAC. A service account’s token is stored in a secret, which does not include a declaration of the node or pod where the workload is running. When a malicious actor steals a token, they gain full access to the account and can steal information or carry out sabotage under the guise of that user.\nA token can only be used to identify a workload in one cluster, but Istio supports multiple clusters and meshes, as well as Kubernetes environments and virtual machines. A unified workload identity standard can help here.\nAn Introduction to SPIFFE and SPIRE SPIFFE’s (Secure Production Identity Framework for Everyone) goal is to create a zero-trust, fully-identified data center network by establishing an open, unified workload identity standard based on the concept of zero-trust. SPIRE can rotate X.509 SVID certificates and secret keys on a regular basis. Based on administrator-defined policies, SPIRE can dynamically provision workload certificates and Istio can consume them. I’ll go over some of the terms associated with SPIFFE in a little more detail below.\nSPIRE (SPIFFE Runtime Environment) is a SPIFFE implementation that is ready for production. SVID (SPIFFE Verifiable Identity Document) is the document that a workload uses to prove its identity to a resource or caller. SVID contains a SPIFFE ID that represents the service’s identity. It uses an X.509 certificate or a JWT token to encode the SPIFFE ID in a cryptographically verifiable document. The SPIFFE ID is a URI that looks like this: spiffe://trust_domain/workload_identifier.\nSPIFFE and Zero Trust Security The essence of Zero Trust is identity-centric dynamic access control. SPIFFE addresses the problem of identifying workloads.\nWe might identify a workload using an IP address and port in the era of virtual machines, but IP address-based identification is vulnerable to multiple services sharing an IP address, IP address forgery, and oversized access control lists. Because containers have a short lifecycle in the Kubernetes era, instead of an IP address, we rely on a pod or service name. However, different clouds and software platforms approach workload identity differently, and there are compatibility issues. This is especially true in heterogeneous hybrid clouds, where workloads run on both virtual machines and Kubernetes. It is critical to establish a fine-grained, interoperable identification system at this point.\nUsing SPIRE for Authentication in Istio With the introduction of SPIRE to Istio, we can give each workload a unique identity, which is used by workloads in the service mesh for peer authentication, request authentication, and authorization policies. The SPIRE Agent issues SVIDs for workloads by communicating with a shared UNIX Domain Socket in the workload. The Envoy proxy and the SPIRE agent communicate through the Envoy SDS (Secret Discovery Service) API. Whenever an Envoy proxy needs to access secrets (certificates, keys, or anything else needed to do secure communication), it will talk to the SPIRE agent through Envoy’s SDS API.\nThe most significant advantage of SDS is the ease with which certificates can be managed. Without this feature, certificates would have to be created as a secret and then mounted into the agent container in a Kubernetes deployment. The secret must be updated, and the proxy container must be re-deployed if the certificate expires. Using SDS, Istio can push the certificates to all Envoy instances in the service mesh. If the certificate expires, the server only needs to push the new certificate to the Envoy instance; Envoy will use the new certificate right away, and the proxy container will not need to be re-deployed.\nThe architecture of using SPIRE for authentication in Istio is depicted in the diagram below.\nSPIRE Architecture with Istio  Use StatefulSet resources to deploy the SPIRE Server and Kubernetes Workload Registrar in the spire namespace of the Kubernetes cluster, and DaemonSet resources to deploy a SPIRE Agent for each node. Assuming that you used the default DNS name cluster.local when you install Kubernetes, Kubernetes Workload Registrar creates identities for the workloads in the Istio mesh in the following format:\n SPRRE Server：spiffe://cluster.local/ns/spire/sa/server SPIRE Agent：spiffe://cluster.local/ns/spire/sa/spire-agent Kubernetes Node：spiffe://cluster.local/k8s-workload-registrar/demo-cluster/node/ Kubernetes Workload Pod：spiffe://cluster.local/{namespace}/spire/sa/{service_acount}  This way, both the nodes and each workload have their own globally unique identity and can be scaled according to the cluster (trust domain).\nThe workload authentication process in Istio mesh is shown in the figure below.\n The workload authentication process in the Istio mesh   The detailed process is as follows:\n The pilot-agent in the sidecar of the workload calls the SPIRE agent via the shared UDS to get the SVID. SPIRE Agent asks Kubernetes (kubelet on the node to be precise) for information about the workload. The kubelet returns the information queries from the Kubernetes API server to the workload attestor. The attestor compares the results returned by the kubelet with the identity information shared by the sidecar. If they match, returns the SVID to the workload and caches it, if not, the attestation failed.  Please refer to the Istio documentation to learn how to use SPIRE for authentication in Istio.\nSummary SPIFFE unifies identity standards in heterogeneous environments, which is the foundation of zero-trust networks. In Istio, whether we use SPIRE or not, authentication is not perceptible to workloads. By using SPIRE to provide authentication for workloads, we can effectively manage workload identity and lay the foundation for a zero-trust network.\n","permalink":"https://jimmysong.io/en/blog/why-istio-need-spire/","summary":"This article explains what SPIRE means for zero-trust architectures and why you would need SPIRE for authentication in Istio.","title":"Why would you need SPIRE for authentication with Istio?"},{"content":"  It’s been more than 5 years since Google, IBM, and Lyft unveiled the Istio open source project in May 2017. The Istio project has developed from a seed to a tall tree in these years. Many domestic books on the Istio service mesh were launched in the two years following the release of Istio 1.0 in 2018. My country is at the forefront of the world in the field of Istio book publishing.\nService mesh: one of the core technologies of cloud native Today, Istio is nearly synonymous with service mesh in China. The development of service mesh, as one of the core cloud-native technologies described by CNCF (Cloud Native Computing Foundation), has gone through the following stages.\n 2017-2018: Exploratory Phase 2019-2020: Early Adopter Phase 2021 to present: Implementation on a large scale and ecological development stage  Cloud native technology enables enterprises to design and deploy elastically scalable applications in new dynamic settings such as public, private, and hybrid clouds, according to the CNCF. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs are examples of cloud native technology.\nService mesh has been included to the CNCF definition of cloud native, indicating that it is one of the representative technologies of cloud native. Google is donating Istio to CNCF today, and we have reason to expect that as a CNCF project, Istio’s community will be more open, and its future development will be more smooth.\nService mesh and cloud native applications Cloud-native development is gaining traction. Despite the frequent emergence of new technologies and products, service mesh has maintained its place as “cloud-native network infrastructure” as part of the overall cloud-native technology stack throughout the past year. The cloud-native technology stack model is depicted in the diagram below, with representative technologies for each layer to define the standard. Service mesh and other cloud-native technologies complement each other as a new era of middleware emerges. Dapr (Distributed Application Runtime) defines the cloud-native middleware capability model, OAM defines the cloud-native application model, and so on, whereas service mesh Lattice defines a cloud-native seven-layer network model.\n Cloud Native Application Model   Why you need a service mesh Using a service mesh isn’t tantamount to abandoning Kubernetes; it just makes sense. The goal of Kubernetes is to manage application lifecycles through declarative configuration, whereas the goal of service mesh is to provide traffic control, security management, and observability amongst apps. How do you set up load balancing and flow management for calls between services after a robust microservice platform has been developed with Kubernetes?\nMany open source tools, including Istio, Linkerd, MOSN, and others, support Envoy’s xDS protocol. The specification of xDS is Envoy’s most significant contribution to service mesh or cloud native. Many various usage cases, such as API gateways, sidecar proxies in service meshes, and edge proxies, are derived from Envoy, which is simply a network proxy, a modern version of the proxy configured through the API.\nIn a nutshell, the move from Kubernetes to Istio was made for the following reasons.\n Application life cycle management, specifically application deployment and management, is at the heart of Kubernetes (scaling, automatic recovery, and release). Kubernetes is a microservices deployment and management platform that is scalable and extremely elastic. Transparent proxy is the cornerstone of service mesh, which intercepts communication between microservices via sidecar proxy and then regulates microservice behavior via control plane settings. The deployment mode of service meshes has introduced new issues today. For service meshes, sidecar is no longer required, and an agentless service mesh based on gRPC is also being tested. xDS is a protocol standard for configuring service meshes, and a gRPC-based xDS is currently being developed. Kubernetes traffic management is decoupled with the service mesh. The kube-proxy component is not required to support traffic within the service mesh. The traffic between services is controlled by an abstraction close to the microservice application layer to achieve security and observability features. In Kubernetes, service mesh is an upper-level abstraction of service, and Serverless is the next stage, which is why Google released Knative based on Kubernetes and Istio following Istio.  Open source in the name of the community The ServiceMesher community was founded in May 2018 with the help of Ant Financial. Following that, a tornado of service meshes erupted in China, and the community-led translation of Istio’s official documentation reached a fever pitch.\nI became aware of a dearth of Chinese resources for systematically teaching Istio over time, so in September 2018, I began to plan and create an Istio book, launching the Istio Handbook open source e-book project on GitHub. I met many friends who are also interested in Istio and service mesh technology in the online and offline events of the community a few months later, with the promotion of service mesh technology and the expansion of the ServiceMesher community. We unanimously agreed to collaborate on an open source Istio e-book, which will compile the community’s important writings and experience into a logical text and make it available to the majority of developers.\nHundreds of people volunteered and began co-authoring the book in March 2019 under the auspices of the Community Stewardship Council. In May 2020, we created a cloud-native community that incorporated the original ServiceMesher community in order to further promote cloud-native technology and expand the technical knowledge supplied by the community. The scope of community operations has also widened, moving away from service mesh to more extensive cloud-native tools.\nThe editorial board for this book, which includes me, Ma Ruofei, Wang Baiping, Wang Wei, Luo Guangming, Zhao Huabing, Zhong Hua, and Guo Xudong, was founded in October 2020. We performed further version updates, improvements, and optimizations to this book under the supervision and assistance of the publishing business. This book, “In-depth Understanding of Isito: Advanced Practice of Cloud Native Service Mesh,” finally met you after many iterations.\n The book cover   About this book After version 1.5, Istio underwent considerable architectural modifications, and various new or better features were added, including the addition of a smart DNS proxy, additional resource objects, increased support for virtual machines, and more.\nThis book is based on the new edition of Istio, and it aims to provide readers with the most up-to-date and comprehensive content possible by following the newest trends in the Istio community. Furthermore, several of the book’s authors are front-line development or operation and maintenance engineers with extensive Istio expertise, offering detailed and useful reference cases for the book.\nThis book is currently available on the JD.com . Please read “In-depth Understanding of Isito: Advanced Practice of Cloud Native Service Mesh” if you want to learn more about Istio!\n Buy now\n  ","permalink":"https://jimmysong.io/en/blog/istio-service-mesh-book/","summary":"By the Cloud Native Community(China)","title":"In-Depth Understanding of Istio: Announcing the Publication of a New Istio Book"},{"content":"This article will guide you on how to compile the Istio binaries and Docker images on macOS.\nBefore you begin Before we start, refer to the Istio Wiki , here is the information about my build environment.\n macOS 12.3.1 Darwin AMD64 Docker Desktop 4.8.1(78998) Docker Engine v20.10.14  Start to compile First, download the Istio code from GitHub to the $GOPATH/src/istio.io/istio directory, and execute the commands below in that root directory.\nCompile into binaries Execute the following command to download the Istio dependent packages, which will be downloaded to the vendor directory.\ngo mod vendor Run the following command to build Istio:\nsudo make build If you do not run the command with sudo, you may encounter the following error.\nfatal: unsafe repository ('/work' is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work fatal: unsafe repository ('/work' is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work Makefile.core.mk:170: *** \"TAG cannot be empty\". Stop. make: *** [build] Error 2 Even if you follow the prompts and run git config --global --add safe.directory /work, you will still get errors during compilation.\nThe compiled binary will be saved in out directory with the following directory structure.\nout ├── darwin_amd64 │ ├── bug-report │ ├── client │ ├── envoy │ ├── extauthz │ ├── install-cni │ ├── istio-cni │ ├── istio-cni-taint │ ├── istio-iptables │ ├── istio_is_init │ ├── istioctl │ ├── logs │ ├── operator │ ├── pilot-agent │ ├── pilot-discovery │ ├── release │ └── server └── linux_amd64 ├── envoy ├── envoy-centos ├── logs └── release It will build both the linux_amd64 and darwin_amd64 architectures binaries at the same time.\nCompile into Docker images Run the following command to compile Istio into a Docker image.\nsudo make build The compilation will take about 3 to 5 minutes depending on your network. Once the compilation is complete, you will see the Docker image of Istio by running the following command.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE localhost:5000/app_sidecar_centos_7 latest 2044037df94b 51 seconds ago 524MB localhost:5000/app_sidecar_ubuntu_jammy latest 5d8ae5ed55b7 About a minute ago 362MB localhost:5000/proxyv2 latest d4679412385f About a minute ago 243MB localhost:5000/install-cni latest 78f46d5771d2 About a minute ago 270MB localhost:5000/istioctl latest c38130a5adc8 About a minute ago 190MB localhost:5000/pilot latest 2aa9185ec202 About a minute ago 190MB localhost:5000/app latest 473adafaeb8d About a minute ago 188MB localhost:5000/operator latest 9ac1fedcdd12 About a minute ago 191MB localhost:5000/ext-authz latest 1fb5aaf20791 About a minute ago 117MB localhost:5000/app_sidecar_debian_11 latest 61376a02b95d 2 minutes ago 407MB localhost:5000/app_sidecar_ubuntu_xenial latest 7e8efe666611 2 minutes ago 418MB You can change the image name and push it into your own container registry.\nSummary This is how to build Istio on macOS. If you have already downloaded the Docker image you need to build, the build will take less than a minute. It also takes only a few minutes to build Docker images.\nReference  Using the Code Base - github.com   ","permalink":"https://jimmysong.io/en/blog/how-to-build-istio/","summary":"This article will guide you on how to compile the Istio binaries on macOS.","title":"How to build Istio?"},{"content":"Updated on May 6, 2022\nBased on Istio version 1.13, this article will present the following.\n What is the sidecar pattern and what advantages does it have? How are the sidecar injections done in Istio? How does the sidecar proxy do transparent traffic hijacking? How is the traffic routed to upstream?  The figure below shows how the productpage service requests access to http://reviews.default.svc.cluster.local:9080/ and how the sidecar proxy inside the reviews service does traffic blocking and routing forwarding when traffic goes inside the reviews service.\nIstio transparent traffic hijacking and traffic routing diagram  At the beginning of the first step, the sidecar in the productpage pod has selected a pod of the reviews service to be requested via EDS, knows its IP address, and sends a TCP connection request.\nThere are three versions of the reviews service, each with an instance, and the sidecar work steps in the three versions are similar, as illustrated below only by the sidecar traffic forwarding step in one of the Pods.\nSidecar pattern Dividing the functionality of an application into separate processes running in the same minimal scheduling unit (e.g. Pod in Kubernetes) can be considered sidecar mode. As shown in the figure below, the sidecar pattern allows you to add more features next to your application without additional third-party component configuration or modifications to the application code.\nSidecar pattern  The Sidecar application is loosely coupled to the main application. It can shield the differences between different programming languages and unify the functions of microservices such as observability, monitoring, logging, configuration, circuit breaker, etc.\nAdvantages of using the Sidecar pattern When deploying a service mesh using the sidecar model, there is no need to run an agent on the node, but multiple copies of the same sidecar will run in the cluster. In the sidecar deployment model, a companion container (such as Envoy or MOSN) is deployed next to each application’s container, which is called a sidecar container. The sidecar takes overall traffic in and out of the application container. In Kubernetes’ Pod, a sidecar container is injected next to the original application container, and the two containers share storage, networking, and other resources.\nDue to its unique deployment architecture, the sidecar model offers the following advantages.\n Abstracting functions unrelated to application business logic into a common infrastructure reduces the complexity of microservice code. Reduce code duplication in microservices architectures because it is no longer necessary to write the same third-party component profiles and code. The sidecar can be independently upgraded to reduce the coupling of application code to the underlying platform.  iptables manipulation analysis In order to view the iptables configuration, we need to nsenter the sidecar container using the root user to view it, because kubectl cannot use privileged mode to remotely manipulate the docker container, so we need to log on to the host where the productpage pod is located.\nIf you use Kubernetes deployed by minikube, you can log directly into the minikube’s virtual machine and switch to root. View the iptables configuration that lists all the rules for the NAT (Network Address Translation) table because the mode for redirecting inbound traffic to the sidecar is REDIRECT in the parameters passed to the istio-iptables when the Init container is selected for the startup, so there will only be NAT table specifications in the iptables and mangle table configurations if TPROXY is selected. See the iptables command for detailed usage.\nWe only look at the iptables rules related to productpage below.\n# login to minikube, change user to root $ minikube ssh $ sudo -i # See the processes in the productpage pod's istio-proxy container $ docker top `docker ps|grep \"istio-proxy_productpage\"|cut -d \" \" -f1` UID PID PPID C STIME TTY TIME CMD 1337 10576 10517 0 08:09 ? 00:00:07 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage.default --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istiod.istio-system.svc:15012 --zipkinAddress zipkin.istio-system:9411 --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --connectTimeout 10s --proxyAdminPort 15000 --concurrency 2 --controlPlaneAuthPolicy NONE --dnsRefreshRate 300s --statusPort 15020 --trust-domain=cluster.local --controlPlaneBootstrap=false 1337 10660 10576 0 08:09 ? 00:00:33 /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage.default --service-node sidecar~172.17.0.16~productpage-v1-7f44c4d57c-ksf9b.default~default.svc.cluster.local --max-obj-name-len 189 --local-address-ip-version v4 --log-format [Envoy (Epoch 0)] [%Y-%m-%d %T.%e][%t][%l][%n] %v -l warning --component-log-level misc:error --concurrency 2 # Enter the nsenter into the namespace of the sidecar container (any of the above is ok) $ nsenter -n --target 10660 View the process’s iptables rule chain under its namespace.\n# View the details of the rule configuration in the NAT table. $ iptables -t nat -L -v # PREROUTING chain: Used for Destination Address Translation (DNAT) to jump all incoming TCP traffic to the ISTIO_INBOUND chain. Chain PREROUTING (policy ACCEPT 2701 packets, 162K bytes) pkts bytes target prot opt in out source destination 2701 162K ISTIO_INBOUND tcp -- any any anywhere anywhere # INPUT chain: Processes incoming packets and non-TCP traffic will continue on the OUTPUT chain. Chain INPUT (policy ACCEPT 2701 packets, 162K bytes) pkts bytes target prot opt in out source destination # OUTPUT chain: jumps all outbound packets to the ISTIO_OUTPUT chain. Chain OUTPUT (policy ACCEPT 79 packets, 6761 bytes) pkts bytes target prot opt in out source destination 15 900 ISTIO_OUTPUT tcp -- any any anywhere anywhere # POSTROUTING CHAIN: All packets must first enter the POSTROUTING chain when they leave the network card, and the kernel determines whether they need to be forwarded out according to the packet destination. Chain POSTROUTING (policy ACCEPT 79 packets, 6761 bytes) pkts bytes target prot opt in out source destination # ISTIO_INBOUND CHAIN: Redirects all inbound traffic to the ISTIO_IN_REDIRECT chain, except for traffic destined for ports 15090 (used by Prometheus) and 15020 (used by Ingress gateway for Pilot health checks), and traffic sent to these two ports will return to the call point of the iptables rule chain, the successor POSTROUTING to the PREROUTING chain. Chain ISTIO_INBOUND (1 references) pkts bytes target prot opt in out source destination 0 0 RETURN tcp -- any any anywhere anywhere tcp dpt:ssh 2 120 RETURN tcp -- any any anywhere anywhere tcp dpt:15090 2699 162K RETURN tcp -- any any anywhere anywhere tcp dpt:15020 0 0 ISTIO_IN_REDIRECT tcp -- any any anywhere anywhere # ISTIO_IN_REDIRECT chain: jumps all inbound traffic to the local 15006 port, thus successfully blocking traffic to the sidecar. Chain ISTIO_IN_REDIRECT (3 references) pkts bytes target prot opt in out source destination 0 0 REDIRECT tcp -- any any anywhere anywhere redir ports 15006 # ISTIO_OUTPUT chain: see the details bellow Chain ISTIO_OUTPUT (1 references) pkts bytes target prot opt in out source destination 0 0 RETURN all -- any lo 127.0.0.6 anywhere 0 0 ISTIO_IN_REDIRECT all -- any lo anywhere !localhost owner UID match 1337 0 0 RETURN all -- any lo anywhere anywhere ! owner UID match 1337 15 900 RETURN all -- any any anywhere anywhere owner UID match 1337 0 0 ISTIO_IN_REDIRECT all -- any lo anywhere !localhost owner GID match 1337 0 0 RETURN all -- any lo anywhere anywhere ! owner GID match 1337 0 0 RETURN all -- any any anywhere anywhere owner GID match 1337 0 0 RETURN all -- any any anywhere localhost 0 0 ISTIO_REDIRECT all -- any any anywhere anywhere # ISTIO_REDIRECT chain: redirects all traffic to Sidecar (i.e. local) port 15001. Chain ISTIO_REDIRECT (1 references) pkts bytes target prot opt in out source destination 0 0 REDIRECT tcp -- any any anywhere anywhere redir ports 15001 The focus here is on the 9 rules in the ISTIO_OUTPUT chain. For ease of reading, I will show some of the above rules in the form of a table as follows.\n   Rule target in out source destination     1 RETURN any lo 127.0.0.6 anywhere   2 ISTIO_IN_REDIRECT any lo anywhere !localhost owner UID match 1337   3 RETURN any lo anywhere anywhere !owner UID match 1337   4 RETURN any any anywhere anywhere owner UID match 1337   5 ISTIO_IN_REDIRECT any lo anywhere !localhost owner GID match 1337   6 RETURN any lo anywhere anywhere !owner GID match 1337   7 RETURN any any anywhere anywhere owner GID match 1337   8 RETURN any any anywhere localhost   9 ISTIO_REDIRECT any any anywhere anywhere    The following diagram shows the detailed flow of the ISTIO_ROUTE rule.\nISTIO_ROUTE iptables rules  I will explain the purpose of each rule, corresponding to the steps and details in the illustration at the beginning of the article, in the order in which they appear. Where rules 5, 6, and 7 are extensions of the application of rules 2, 3, and 4 respectively (from UID to GID), which serve similar purposes and will be explained together. Note that the rules therein are executed in order, meaning that the rule with the next highest order will be used as the default. When the outbound NIC (out) is lo (local loopback address, loopback interface), it means that the destination of the traffic is the local Pod, and traffic sent from the Pod to the outside, will not go through this interface. Only rules 4, 7, 8, and 9 apply to all outbound traffic from the review Pod.\nRule 1\n Purpose: To pass through traffic sent by the Envoy proxy to the local application container, so that it bypasses the Envoy proxy and goes directly to the application container. Corresponds to steps 6 through 7 in the illustration. Details: This rule causes all requests from 127.0.0.6 (this IP address will be explained below) to jump out of the chain, return to the point of invocation of iptables (i.e. OUTPUT) and continue with the rest of the routing rules, i.e. the POSTROUTING rule, which sends traffic to an arbitrary destination, such as the application container within the local Pod. Without this rule, traffic from the Envoy proxy within the Pod to the Pod container will execute the next rule, rule 2, and the traffic will enter the Inbound Handler again, creating a dead loop. Putting this rule in the first place can avoid the problem of traffic dead-ending in the Inbound Handler.  Rule 2, 5\n Purpose: Handle inbound traffic (traffic inside the Pod) from the Envoy proxy, but not requests to the localhost, and forward it to the Envoy proxy’s Inbound Handler via a subsequent rule. This rule applies to scenarios where the Pod invokes its own IP address, i.e., traffic between services within the Pod. Details: If the destination of the traffic is not localhost and the packet is sent by 1337 UID (i.e. istio-proxy user, Envoy proxy), the traffic will be forwarded to Envoy’s Inbound Handler through ISTIO_IN_REDIRECT eventually.  Rule 3, 6\n Purpose: To pass through the internal traffic of the application container within the Pod. This rule applies to traffic within the container. For example, access to Pod IP or localhost within a Pod. Corresponds to steps 6 through 7 in the illustration. Details: If the traffic is not sent by an Envoy user, then jump out of the chain and return to OUTPUT to call POSTROUTING and go straight to the destination.  Rule 4, 7\n Purpose: To pass through outbound requests sent by Envoy proxy. Corresponds to steps 14 through 15 in the illustration. Details: If the request was made by the Envoy proxy, return OUTPUT to continue invoking the POSTROUTING rule and eventually access the destination directly.  Rule 8\n Purpose: Passes requests from within the Pod to the localhost. Details: If the destination of the request is localhost, return OUTPUT and call POSTROUTING to access localhost directly.  Rule 9\n Purpose: All other traffic will be forwarded to ISTIO_REDIRECT after finally reaching the Outbound Handler of Envoy proxy. Corresponds to steps 10 through 11 in the illustration.  The above rule avoids dead loops in the iptables rules for Envoy proxy to application routing, and guarantees that traffic can be routed correctly to the Envoy proxy, and that real outbound requests can be made.\nAbout RETURN target\nYou may notice that there are many RETURN targets in the above rules, which means that when this rule is specified, it jumps out of the rule chain, returns to the call point of iptables (in our case OUTPUT) and continues to execute the rest of the routing rules, in our case the POSTROUTING rule, which sends traffic to any destination address, you can think of This is intuitively understood as pass-through.\nAbout the 127.0.0.6 IP address\nThe IP 127.0.0.6 is the default InboundPassthroughClusterIpv4 in Istio and is specified in the code of Istio. This is the IP address to which traffic is bound after entering the Envoy proxy, and serves to allow Outbound traffic to be re-sent to the application container in the Pod, i.e. Passthought, bypassing the Outbound Handler. this traffic is access to the Pod itself, and not real outbound traffic. See Istio Issue-29603 for more information on why this IP was chosen as the traffic passthrough.\nThe traffic routing process explained Traffic routing is divided into two processes, Inbound and Outbound, which will be analyzed in detail for the reader below based on the example above and the configuration of the sidecar.\nUnderstand Inbound Handler The role of the Inbound handler is to pass traffic from the downstream blocked by iptables to the localhost and establish a connection to the application container within the Pod. Assuming the name of one of the Pods is reviews-v1-545db77b95-jkgv2, run istioctl proxy-config listener reviews-v1-545db77b95-jkgv2 --port 15006 to see which Listener is in that Pod.\nADDRESS PORT MATCH DESTINATION 0.0.0.0 15006 Addr: *:15006 Non-HTTP/Non-TCP 0.0.0.0 15006 Trans: tls; App: istio-http/1.0,istio-http/1.1,istio-h2; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: raw_buffer; App: http/1.1,h2c; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: tls; App: TCP TLS; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: raw_buffer; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: tls; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: tls; App: istio,istio-peer-exchange,istio-http/1.0,istio-http/1.1,istio-h2; Addr: *:9080 Cluster: inbound|9080|| 0.0.0.0 15006 Trans: raw_buffer; Addr: *:9080 Cluster: inbound|9080|| The following lists the meanings of the fields in the above output.\n ADDRESS: downstream address PORT: The port the Envoy listener is listening on MATCH: The transport protocol used by the request or the matching downstream address DESTINATION: Route destination  The Iptables in the reviews Pod hijack inbound traffic to port 15006, and from the above output we can see that Envoy’s Inbound Handler is listening on port 15006, and requests to port 9080 destined for any IP will be routed to the inbound|9080|| Cluster.\nAs you can see in the last two rows of the Pod’s Listener list, the Listener for 0.0.0.0:15006/TCP (whose actual name is virtualInbound) listens for all Inbound traffic, which contains matching rules, and traffic to port 9080 from any IP will be routed. If you want to see the detailed configuration of this Listener in Json format, you can execute the istioctl proxy-config listeners reviews-v1-545db77b95-jkgv2 --port 15006 -o json command. You will get an output similar to the following.\n[ /*omit*/ { \"name\": \"virtualInbound\", \"address\": { \"socketAddress\": { \"address\": \"0.0.0.0\", \"portValue\": 15006 } }, \"filterChains\": [ /*omit*/ { \"filterChainMatch\": { \"destinationPort\": 9080, \"transportProtocol\": \"tls\", \"applicationProtocols\": [ \"istio\", \"istio-peer-exchange\", \"istio-http/1.0\", \"istio-http/1.1\", \"istio-h2\" ] }, \"filters\": [ /*omit*/ { \"name\": \"envoy.filters.network.http_connection_manager\", \"typedConfig\": { \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\", \"statPrefix\": \"inbound_0.0.0.0_9080\", \"routeConfig\": { \"name\": \"inbound|9080||\", \"virtualHosts\": [ { \"name\": \"inbound|http|9080\", \"domains\": [ \"*\" ], \"routes\": [ { \"name\": \"default\", \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"inbound|9080||\", \"timeout\": \"0s\", \"maxStreamDuration\": { \"maxStreamDuration\": \"0s\", \"grpcTimeoutHeaderMax\": \"0s\" } }, \"decorator\": { \"operation\": \"reviews.default.svc.cluster.local:9080/*\" } } ] } ], \"validateClusters\": false }, /*omit*/ } } ], /*omit*/ ], \"listenerFilters\": [ /*omit*/ ], \"listenerFiltersTimeout\": \"0s\", \"continueOnListenerFiltersTimeout\": true, \"trafficDirection\": \"INBOUND\" } ] Since the Inbound Handler traffic routes traffic from any address to this Pod port 9080 to the inbound|9080|| Cluster, let’s run istioctl pc cluster reviews-v1-545db77b95-jkgv2 --port 9080 --direction inbound -o json to see the Cluster configuration and you will get something like the following output.\n[ { \"name\": \"inbound|9080||\", \"type\": \"ORIGINAL_DST\", \"connectTimeout\": \"10s\", \"lbPolicy\": \"CLUSTER_PROVIDED\", \"circuitBreakers\": { \"thresholds\": [ { \"maxConnections\": 4294967295, \"maxPendingRequests\": 4294967295, \"maxRequests\": 4294967295, \"maxRetries\": 4294967295, \"trackRemaining\": true } ] }, \"cleanupInterval\": \"60s\", \"upstreamBindConfig\": { \"sourceAddress\": { \"address\": \"127.0.0.6\", \"portValue\": 0 } }, \"metadata\": { \"filterMetadata\": { \"istio\": { \"services\": [ { \"host\": \"reviews.default.svc.cluster.local\", \"name\": \"reviews\", \"namespace\": \"default\" } ] } } } } ] We see that the TYPE is ORIGINAL_DST, which sends the traffic to the original destination address (Pod IP), because the original destination address is the current Pod, you should also notice that the value of upstreamBindConfig.sourceAddress.address is rewritten to 127.0.0.6, and for Pod This echoes the first rule in the iptables ISTIO_OUTPUT the chain above, according to which traffic will be passed through to the application container inside the Pod.\nUnderstand Outbound Handler Because reviews send an HTTP request to the ratings service at http://ratings.default.svc.cluster.local:9080/, the role of the Outbound handler is to intercept traffic from the local application to which iptables has intercepted, and determine how to route it to the upstream via the sidecar.\nRequests from application containers are Outbound traffic, hijacked by iptables and transferred to the Outbound handler for processing, which then passes through the virtualOutbound Listener, the 0.0.0.0_9080 Listener, and then finds the upstream cluster via Route 9080, which in turn finds the Endpoint via EDS to perform the routing action.\nRoute ratings.default.svc.cluster.local:9080\nreviews requests the ratings service and runs istioctl proxy-config routes reviews-v1-545db77b95-jkgv2 --name 9080 -o json. View the route configuration because the sidecar matches VirtualHost based on domains in the HTTP header, so only ratings.default.svc.cluster.local:9080 is listed below for this VirtualHost.\n[{ { \"name\": \"ratings.default.svc.cluster.local:9080\", \"domains\": [ \"ratings.default.svc.cluster.local\", \"ratings.default.svc.cluster.local:9080\", \"ratings\", \"ratings:9080\", \"ratings.default.svc.cluster\", \"ratings.default.svc.cluster:9080\", \"ratings.default.svc\", \"ratings.default.svc:9080\", \"ratings.default\", \"ratings.default:9080\", \"10.98.49.62\", \"10.98.49.62:9080\" ], \"routes\": [ { \"name\": \"default\", \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"outbound|9080||ratings.default.svc.cluster.local\", \"timeout\": \"0s\", \"retryPolicy\": { \"retryOn\": \"connect-failure,refused-stream,unavailable,cancelled,resource-exhausted,retriable-status-codes\", \"numRetries\": 2, \"retryHostPredicate\": [ { \"name\": \"envoy.retry_host_predicates.previous_hosts\" } ], \"hostSelectionRetryMaxAttempts\": \"5\", \"retriableStatusCodes\": [ 503 ] }, \"maxGrpcTimeout\": \"0s\" }, \"decorator\": { \"operation\": \"ratings.default.svc.cluster.local:9080/*\" } } ] }, ..] From this VirtualHost configuration, you can see routing traffic to the cluster outbound|9080||ratings.default.svc.cluster.local.\nEndpoint outbound|9080||ratings.default.svc.cluster.local\nRunning istioctl proxy-config endpoint reviews-v1-545db77b95-jkgv2 --port 9080 -o json --cluster \"outbound|9080||ratings.default.svc.cluster.local\" to view the Endpoint configuration, the results are as follows.\n{ \"clusterName\": \"outbound|9080||ratings.default.svc.cluster.local\", \"endpoints\": [ { \"locality\": { }, \"lbEndpoints\": [ { \"endpoint\": { \"address\": { \"socketAddress\": { \"address\": \"172.33.100.2\", \"portValue\": 9080 } } }, \"metadata\": { \"filterMetadata\": { \"istio\": { \"uid\": \"kubernetes://ratings-v1-8558d4458d-ns6lk.default\" } } } } ] } ] } We see that the endpoint address is 10.4.1.12. In fact, the Endpoint can be one or more, and the sidecar will select the appropriate Endpoint to route based on certain rules. At this point the review Pod has found the Endpoint for its upstream service rating.\nSummary This article uses the bookinfo example provided by Istio to guide readers through the implementation details behind the sidecar injection, iptables transparent traffic hijacking, and traffic routing in the sidecar. The sidecar mode and traffic transparent hijacking are the features and basic functions of Istio service mesh, understanding the process behind this function and the implementation details will help you understand the principle of service mesh and the content in the later chapters of the Istio Handbook , so I hope readers can try it from scratch in their own environment to deepen their understanding.\nUsing iptables for traffic hijacking is just one of the ways to do traffic hijacking in the data plane of a service mesh, and there are many more traffic hijacking scenarios, quoted below from the description of the traffic hijacking section given in the MOSN official network of the cloud-native network proxy.\nProblems with using iptables for traffic hijacking Currently, Istio uses iptables for transparent hijacking and there are three main problems.\n The need to use the conntrack module for connection tracking, in the case of a large number of connections, will cause a large consumption and may cause the track table to be full, in order to avoid this problem, the industry has a practice of closing conntrack. iptables is a common module with global effect and cannot explicitly prohibit associated changes, which is less controllable. iptables redirect traffic is essentially exchanging data via a loopback. The outbound traffic will traverse the protocol stack twice and lose forwarding performance in a large concurrency scenario.  Several of the above problems are not present in all scenarios, let’s say some scenarios where the number of connections is not large and the NAT table is not used, iptables is a simple solution that meets the requirements. In order to adapt to a wider range of scenarios, transparent hijacking needs to address all three of these issues.\nTransparent hijacking optimization In order to optimize the performance of transparent traffic hijacking in Istio, the following solutions have been proposed by the industry.\nTraffic Hijacking with eBPF using the Merbridge Open Source Project\nMerbridge is a plug-in that leverages eBPF to accelerate the Istio service mesh, which was open sourced by DaoCloud in early 2022. Using Merbridge can optimize network performance in the data plane to some extent.\nMerbridge leverages the sockops and redir capabilities of eBPF to transfer packets directly from inbound sockets to outbound sockets. eBPF provides the bpf_msg_redirect_hash function to forward application packets directly.\nHandling inbound traffic with tproxy\ntproxy can be used for redirection of inbound traffic without changing the destination IP/port in the packet, without performing connection tracking, and without the problem of conntrack modules creating a large number of connections. Restricted to the kernel version, tproxy’s application to outbound is flawed. Istio currently supports handling inbound traffic via tproxy.\nUse hook connect to handle outbound traffic\nIn order to adapt to more application scenarios, the outbound direction is implemented by hook connect, which is implemented as follows.\n Hook Connect Diagram   Whichever transparent hijacking scheme is used, the problem of obtaining the real destination IP/port needs to be solved, using the iptables scheme through getsockopt, tproxy can read the destination address directly, by modifying the call interface, hook connect scheme reads in a similar way to tproxy.\nAfter the transparent hijacking, the sockmap can shorten the packet traversal path and improve forwarding performance in the outbound direction, provided that the kernel version meets the requirements (4.16 and above).\nReferences  Debugging Envoy and Istiod - istio.io  Demystifying Istio’s Sidecar Injection Model - istio.io  The traffic hijacking solution when MOSN is used as a sidecar - mosn.io   ","permalink":"https://jimmysong.io/en/blog/sidecar-injection-iptables-and-traffic-routing/","summary":"This blog describes the sidecar pattern, transparent traffic hijacking and routing.","title":"Sidecar injection, transparent traffic hijacking , and routing process in Istio explained in detail"},{"content":"This article will explain:\n The sidecar auto-injection process in Istio The init container startup process in Istio The startup process of a Pod with Sidecar auto-injection enabled  The following figure shows the components of a Pod in the Istio data plane after it has been started.\nIstio data plane pod  Sidecar injection in Istio The following two sidecar injection methods are available in Istio.\n Manual injection using istioctl. Kubernetes-based mutating webhook admission controller automatic sidecar injection method.  Whether injected manually or automatically, SIDECAR’s injection process follows the following steps.\n Kubernetes needs to know the Istio cluster to which the sidecar to be injected is connected and its configuration. Kubernetes needs to know the configuration of the sidecar container itself to be injected, such as the image address, boot parameters, etc. Kubernetes injects the above configuration into the side of the application container by the sidecar injection template and the configuration parameters of the above configuration-filled sidecar.  The sidecar can be injected manually using the following command.\nistioctl kube-inject -f ${YAML_FILE} | kuebectl apply -f - This command is injected using Istio’s built-in sidecar configuration, see the Istio official website for details on how to use Istio below.\nWhen the injection is complete you will see that Istio has injected initContainer and sidecar proxy-related configurations into the original pod template.\nInit container The Init container is a dedicated container that runs before the application container is launched and is used to contain some utilities or installation scripts that do not exist in the application image.\nMultiple Init containers can be specified in a Pod, and if more than one is specified, the Init containers will run sequentially. The next Init container can only be run if the previous Init container must run successfully. Kubernetes only initializes the Pod and runs the application container when all the Init containers have been run.\nThe Init container uses Linux Namespace, so it has a different view of the file system than the application container. As a result, they can have access to Secret in a way that application containers cannot.\nDuring Pod startup, the Init container starts sequentially after the network and data volumes are initialized. Each container must be successfully exited before the next container can be started. If exiting due to an error will result in a container startup failure, it will retry according to the policy specified in the Pod’s restartPolicy. However, if the Pod’s restartPolicy is set to Always, the restartPolicy is used when the Init container failed.\nThe Pod will not become Ready until all Init containers are successful. The ports of the Init containers will not be aggregated in the Service. The Pod that is being initialized is in the Pending state but should set the Initializing state to true. The Init container will automatically terminate once it is run.\nSidecar injection example analysis For a detailed YAML configuration for the bookinfo applications, see bookinfo.yaml for the official Istio YAML of productpage in the bookinfo sample.\nThe following will be explained in the following terms.\n Injection of Sidecar containers Creation of iptables rules The detailed process of routing  apiVersion:apps/v1kind:Deploymentmetadata:name:productpage-v1labels:app:productpageversion:v1spec:replicas:1selector:matchLabels:app:productpageversion:v1template:metadata:labels:app:productpageversion:v1spec:serviceAccountName:bookinfo-productpagecontainers:- name:productpageimage:docker.io/istio/examples-bookinfo-productpage-v1:1.15.0imagePullPolicy:IfNotPresentports:- containerPort:9080volumeMounts:- name:tmpmountPath:/tmpvolumes:- name:tmpemptyDir:{}Let’s see the productpage container’s Dockerfile .\nFROMpython:3.7.4-slimCOPY requirements.txt ./RUN pip install --no-cache-dir -r requirements.txtCOPY test-requirements.txt ./RUN pip install --no-cache-dir -r test-requirements.txtCOPY productpage.py /opt/microservices/COPY tests/unit/* /opt/microservices/COPY templates /opt/microservices/templatesCOPY static /opt/microservices/staticCOPY requirements.txt /opt/microservices/ARG flood_factorENV FLOOD_FACTOR ${flood_factor:-0}EXPOSE9080WORKDIR/opt/microservicesRUN python -m unittest discoverUSER1CMD [\"python\", \"productpage.py\", \"9080\"]We see that ENTRYPOINT is not configured in Dockerfile, so CMD’s configuration python productpage.py 9080 will be the default ENTRYPOINT, keep that in mind and look at the configuration after the sidecar injection.\n$ istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml We intercept only a portion of the YAML configuration that is part of the Deployment configuration associated with productpage.\ncontainers:- image:docker.io/istio/examples-bookinfo-productpage-v1:1.15.0# application imagename:productpageports:- containerPort:9080- args:- proxy- sidecar- --domain- $(POD_NAMESPACE).svc.cluster.local- --configPath- /etc/istio/proxy- --binaryPath- /usr/local/bin/envoy- --serviceCluster- productpage.$(POD_NAMESPACE)- --drainDuration- 45s- --parentShutdownDuration- 1m0s- --discoveryAddress- istiod.istio-system.svc:15012- --zipkinAddress- zipkin.istio-system:9411- --proxyLogLevel=warning- --proxyComponentLogLevel=misc:error- --connectTimeout- 10s- --proxyAdminPort- \"15000\"- --concurrency- \"2\"- --controlPlaneAuthPolicy- NONE- --dnsRefreshRate- 300s- --statusPort- \"15020\"- --trust-domain=cluster.local- --controlPlaneBootstrap=falseimage:docker.io/istio/proxyv2:1.5.1# sidecar proxyname:istio-proxyports:- containerPort:15090name:http-envoy-promprotocol:TCPinitContainers:- command:- istio-iptables- -p- \"15001\"- -z- \"15006\"- -u- \"1337\"- -m- REDIRECT- -i- '*'- -x- \"\"- -b- '*'- -d- 15090,15020image:docker.io/istio/proxyv2:1.5.1# init containername:istio-initIstio’s configuration for application Pod injection mainly includes:\n Init container istio-init: for setting iptables port forwarding in the pod Sidecar container istio-proxy: running a sidecar proxy, such as Envoy or MOSN  The two containers will be parsed separately.\nInit container analysis The Init container that Istio injects into the pod is named istio-init, and we see in the YAML file above after Istio’s injection is complete that the init command for this container is.\nistio-iptables -p 15001 -z 15006 -u 1337 -m REDIRECT -i '*' -x \"\" -b '*' -d 15090,15020 Let’s check the container’s Dockerfile again to see how ENTRYPOINT determines what commands are executed at startup.\n# ommit# The pilot-agent will bootstrap Envoy.ENTRYPOINT [\"/usr/local/bin/pilot-agent\"]We see that the entrypoint of the istio-init container is the /usr/local/bin/istio-iptables command line, and the location of the code for this command-line tool is in the tools/istio-iptables directory of the Istio source code repository.\nInit container initiation The Init container’s entrypoint is the istio-iptables command line, which is used as follows.\nUsage: istio-iptables [flags] Flags: -n, --dry-run Do not call any external dependencies like iptables -p, --envoy-port string Specify the envoy port to which redirect all TCP traffic (default $ENVOY_PORT = 15001) -h, --help help for istio-iptables -z, --inbound-capture-port string Port to which all inbound TCP traffic to the pod/VM should be redirected to (default $INBOUND_CAPTURE_PORT = 15006) --iptables-probe-port string set listen port for failure detection (default \"15002\") -m, --istio-inbound-interception-mode string The mode used to redirect inbound connections to Envoy, either \"REDIRECT\" or \"TPROXY\" -b, --istio-inbound-ports string Comma separated list of inbound ports for which traffic is to be redirected to Envoy (optional). The wildcard character \"*\" can be used to configure redirection for all ports. An empty list will disable -t, --istio-inbound-tproxy-mark string -r, --istio-inbound-tproxy-route-table string -d, --istio-local-exclude-ports string Comma separated list of inbound ports to be excluded from redirection to Envoy (optional). Only applies when all inbound traffic (i.e. \"*\") is being redirected (default to $ISTIO_LOCAL_EXCLUDE_PORTS) -o, --istio-local-outbound-ports-exclude string Comma separated list of outbound ports to be excluded from redirection to Envoy -i, --istio-service-cidr string Comma separated list of IP ranges in CIDR form to redirect to envoy (optional). The wildcard character \"*\" can be used to redirect all outbound traffic. An empty list will disable all outbound -x, --istio-service-exclude-cidr string Comma separated list of IP ranges in CIDR form to be excluded from redirection. Only applies when all outbound traffic (i.e. \"*\") is being redirected (default to $ISTIO_SERVICE_EXCLUDE_CIDR) -k, --kube-virt-interfaces string Comma separated list of virtual interfaces whose inbound traffic (from VM) will be treated as outbound --probe-timeout duration failure detection timeout (default 5s) -g, --proxy-gid string Specify the GID of the user for which the redirection is not applied. (same default value as -u param) -u, --proxy-uid string Specify the UID of the user for which the redirection is not applied. Typically, this is the UID of the proxy container -f, --restore-format Print iptables rules in iptables-restore interpretable format (default true) --run-validation Validate iptables --skip-rule-apply Skip iptables apply The above incoming parameters are reassembled into iptables rules. For more information on how to use this command, visit tools/istio-iptables/pkg/cmd/root.go.\nThe significance of the container’s existence is that it allows the sidecar agent to intercept all inbound and outbound traffic to the pod, redirect all inbound traffic to port 15006 (sidecar) except port 15090 (used by Prometheus) and port 15092 (Ingress Gateway), and then intercept outbound traffic from the application container which is processed by sidecar (listening through port 15001) and then outbound. See the official Istio documentation for port usage in Istio.\nCommand analysis\nHere is the purpose of this start-up command.\n Forward all traffic from the application container to port 15006 of the sidecar. Run with the istio-proxy user identity, with a UID of 1337, the userspace where the sidecar is located, which is the default user used by the istio-proxy container, see the runAsUser field of the YAML configuration. Use the default REDIRECT mode to redirect traffic. Redirect all outbound traffic to the sidecar proxy (via port 15001).  Because the Init container is automatically terminated after initialization, since we cannot log into the container to view the iptables information, the Init container initialization results are retained in the application container and sidecar container.\nPod Startup Sequence The startup process of a Pod with Sidecar auto-injection enabled is as follows.\n The Init container starts first, injecting iptables rules into the Pod for transparent traffic interception. Subsequently, Kubernetes starts the containers in the order in which they are declared in the Pod Spec, but this is non-blocking and there is no guarantee that the first container will be started before the next one is started. istio-proxy container starts, pilot-agent will be the PID 1 process, which is the first process in the Linux user space and is responsible for pulling up other processes and handling zombie processes. The pilot-agent generates the Envoy bootstrap configuration and fork the envoy process; the application container is started almost simultaneously with the istio-proxy container, and the readiness probe comes in handy to prevent the container inside the Pod from receiving outside traffic before it is ready to start. Kubernetes will perform a readiness check on port 15021 of the istio-proxy container, and the kubelet will not route traffic to the Pod until the isito-proxy has finished booting. After the Pod is started, the pilot-agent becomes a daemon that monitors the rest of the system and provides Envoy with Bootstrap configuration, certificates, health checks, configuration hot reloading, identity support, and process lifecycle management, among other things.  Pod container startup order problem In the process of Pod startup there is a container startup order problem. Suppose the following situation, the application container starts first and requests other services, when the istio-proxy container has not finished starting, then the request will fail, and if your application is not robust enough, it may even cause the application container to crash and the Pod to restart. The solution for this situation is to\n Modify the application to add timeout retries. Increase the start delay of the process in the application container, for example by increasing the sleep time. Add a postStart configuration to the application container to detect if the application process has finished starting, and Kubernetes will only mark the Pod’s state as Running if the detection is successful.  Summary This article walks you through the process of starting Pods in the Istio data plane, and the issues that arise because of the order in which Pod contenters are started.\n","permalink":"https://jimmysong.io/en/blog/istio-pod-process-lifecycle/","summary":"This article will explain Istio's Init container, Pod internal processes and the startup process.","title":"Istio data plane pod startup process explained"},{"content":"iptables is an important feature in the Linux kernel and has a wide range of applications. iptables is used by default in Istio for transparent traffic hijacking. Understanding iptables is very important for us to understand how Istio works. This article will give you a brief introduction to iptbles.\niptables introduction iptables is a management tool for netfilter, the firewall software in the Linux kernel. netfilter is located in the user space and is part of netfilter. netfilter is located in the kernel space and has not only network address conversion, but also packet content modification and packet filtering firewall functions.\nBefore learning about iptables for Init container initialization, let’s go over iptables and rule configuration.\nThe following figure shows the iptables call chain.\niptables 调用链  iptables The iptables version used in the Init container is v1.6.0 and contains 5 tables.\n RAW is used to configure packets. Packets in RAW are not tracked by the system. The filter is the default table used to house all firewall-related operations. NAT is used for network address translation (e.g., port forwarding). Mangle is used for modifications to specific packets (refer to corrupted packets). Security is used to force access to control network rules.  Note: In this example, only the NAT table is used.\nThe chain types in the different tables are as follows.\n   Rule name raw filter nat mangle security     PREROUTING ✓  ✓ ✓    INPUT  ✓  ✓ ✓   OUTPUT ✓ ✓ ✓ ✓ ✓   POSTROUTING   ✓ ✓    FORWARD  ✓  ✓ ✓    Understand iptables rules View the default iptables rules in the istio-proxy container, the default view is the rules in the filter table.\n$ iptables -L -v Chain INPUT (policy ACCEPT 350K packets, 63M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 18M packets, 1916M bytes) pkts bytes target prot opt in out source destination We see three default chains, INPUT, FORWARD, and OUTPUT, with the first line of output in each chain indicating the chain name (INPUT/FORWARD/OUTPUT in this case), followed by the default policy (ACCEPT).\nThe following is a proposed structure diagram of iptables, where traffic passes through the INPUT chain and then enters the upper protocol stack, such as:\niptables chains  Multiple rules can be added to each chain and the rules are executed in order from front to back. Let’s look at the table header definition of the rule.\n PKTS: Number of matched messages processed bytes: cumulative packet size processed (bytes) Target: If the message matches the rule, the specified target is executed. PROT: Protocols such as TDP, UDP, ICMP, and ALL. opt: Rarely used, this column is used to display IP options. IN: Inbound network interface. OUT: Outbound network interface. source: the source IP address or subnet of the traffic, the latter being anywhere. destination: the destination IP address or subnet of the traffic, or anywhere.  There is also a column without a header, shown at the end, which represents the options of the rule, and is used as an extended match condition for the rule to complement the configuration in the previous columns. prot, opt, in, out, source and destination and the column without a header shown after destination together form the match rule. TARGET is executed when traffic matches these rules.\nTypes supported by TARGET\nTarget types include ACCEPT, REJECT, DROP, LOG, SNAT, MASQUERADE, DNAT, REDIRECT, RETURN or jump to other rules, etc. You can determine where the telegram is going by executing only one rule in a chain that matches in order, except for the RETURN type, which is similar to the return statement in programming languages, which returns to its call point and continues to execute the next rule.\nFrom the output, you can see that the Init container does not create any rules in the default link of iptables, but instead creates a new link.\nSummary With the above brief introduction to iptables, you have understood how iptables works, the rule chain and its execution order.\n","permalink":"https://jimmysong.io/en/blog/understanding-iptables/","summary":"This article will give you a brief introduction to iptbles, its tables and the order of execution.","title":"Understanding iptalbes"},{"content":"See the cloud native public library at: https://jimmysong.io/docs/ The cloud native public library project is a documentation project built using the Wowchemy theme, open sourced on GitHub .\nI have also adjusted the home page, menu and directory structure of the site, and the books section of the site will be maintained using the new theme.\nCloud native library positioning The cloud native public library is a collection of cloud native related books and materials published and translated by the author since 2017, and is a compendium and supplement to the dozen or so books already published. The original materials will continue to be published in the form of GitBooks, and the essence and related content will be sorted into the cloud native public library through this project.\nIn addition, the events section of this site has been revamped and moved to a new page .\n","permalink":"https://jimmysong.io/en/notice/cloud-native-public-library/","summary":"A one-stop cloud native library that is a compendium of published materials.","title":"Cloud Native library launch"},{"content":"In my last two blogs:\n Sidecar injection, transparent traffic hijacking , and routing process in Istio explained in detail  Traffic types and iptables rules in Istio sidecar explained   I gave you a detailed overview of the traffic in the Istio data plane, but the data plane does not exist in isolation. This article will show you the ports and their usages for each component of both the control plane and data plane in Istio, which will help you understand the relationship between these flows and troubleshoot them.\nOverview Firstly, I will show you a global schematic. The following figure shows the components of a sidecar in the Istio data plane, and the objects that interact with it.\nIstio components  We can use the nsenter command to enter the namespace of the productpage Pod of the Bookinfo example and see the information about the ports it is listening on internally.\nIstio sidecar ports  From the figure, we can see that besides the port 9080 that the productpage application listens to, the Sidecar container also listens to a large number of other ports, such as 15000, 15001, 15004, 15006, 15021, 15090, etc. You can learn about the ports used in Istio in the Istio documentation .\nLet’s go back into the productpage Pod and use the lsof -i command to see the ports it has open, as shown in the following figure.\nProductpage Pod ports  We can see that there is a TCP connection established between the pilot-agent and istiod, the port in the listening described above, and the TCP connection established inside the Pod, which corresponds to the figure at the beginning of the article.\nThe root process of the Sidecar container (istio-proxy) is pilot-agent, and the startup command is shown below.\nInternal procecces in Sidecar  As we can see from the figure, the PID of its pilot-agent process is 1, and it forked the Envoy process.\nCheck the ports it opens in Istiod, as shown in the figure below.\nIstiod ports  We can see the ports that are listened to, the inter-process and remote communication connections.\nPorts usage overview These ports can play a pivotal role when you are troubleshooting. They are described below according to the component and function in which the port is located.\nPorts in Istiod The ports in Istiod are relatively few and single-function.\n 9876: ControlZ user interface, exposing information about Istiod’s processes 8080: Istiod debugging port, through which the configuration and status information of the grid can be queried 15010: Exposes the xDS API and issues plain text certificates 15012: Same functionality as port 15010, but uses TLS communication 15014: Exposes control plane metrics to Prometheus 15017: Sidecar injection and configuration validation port  Ports in sidecar From the above, we see that there are numerous ports in the sidecar.\n 15000: Envoy admin interface, which you can use to query and modify the configuration of Envoy Proxy. Please refer toEnvoy documentation for details. 15001: Used to handle outbound traffic. 15004: Debug port (explained further below). 15006: Used to handle inbound traffic. 15020: Summarizes statistics, perform health checks on Envoy and DNS agents, and debugs pilot-agent processes, as explained in detail below. 15021: Used for sidecar health checks to determine if the injected Pod is ready to receive traffic. We set up the readiness probe on the /healthz/ready path on this port, and Istio hands off the sidecar readiness checks to kubelet. 15053: Local DNS proxy for scenarios where the cluster’s internal domain names are not resolved by Kubernetes DNS. 15090: Envoy Prometheus query port, through which the pilot-agent will scratch metrics.  The above ports can be divided into the following categories.\n Responsible for inter-process communication, such as 15001, 15006, 15053 Health check and information statistics, e.g. 150021, 15090 Debugging: 15000, 15004  Let’s look at the key ports in detail.\n15000 15000 is Envoy’s Admin interface, which allows us to modify Envoy and get a view and query metrics and configurations.\nThe Admin interface consists of a REST API with multiple endpoints and a simple user interface. You can enable the Envoy Admin interface view in the productpage Pod using the following command:\nkubectl -n default port-forward deploy/productpage-v1 15000 Visit http://localhost:15000 in your browser and you will see the Envoy Admin interface as shown below.\nEnvoy Admin interface  15004 With the pilot-agent proxy istiod debug endpoint on port 8080, you can access localhost’s port 15004 in the data plane Pod to query the grid information, which has the same effect as port 8080 below.\n8080 You can also forward istiod port 8080 locally by running the following command:\nkubectl -n istio-system port-forward deploy/istiod 8080 Visit http://localhost:8080/debug in your browser and you will see the debug endpoint as shown in the figure below.\nPilot Debug Console  Of course, this is only one way to get the mesh information and debug the mesh, you can also use istioctl command or Kiali to debug it, which will be more efficient and intuitive.\n15020 Port 15020 has three main usages.\n Aggregating metrics: You can query port 15090 for Envoy’s metrics, or you can configure it to query the application’s metrics, aggregating Envoy, application, and its own metrics for Prometheus to collect. The corresponding debug endpoint is /stats/prometheus. Performing health checks on Envoy and DNS agent: the corresponding debug endpoints are /healthz/ready and /app-health. Debugging pilot-agent processes: the corresponding debug endpoints are /quitquitquit, debug/ndsz and /debug/pprof.  The following figure shows the debugging information you see when you open http://localhost:15020/debug/pprof in your browser.\npprof endpoint  The information in the figure shows the stack information of the pilot-agent.\nSummary By understanding the component ports in Istio, you should have a better understanding of the relationship between the components in Istio and their internal traffic. Being familiar with the functions of these ports will help in troubleshooting the mesh.\n","permalink":"https://jimmysong.io/en/blog/istio-components-and-ports/","summary":"This article will introduce you to the various ports and functions of the Istio control plane and data plane.","title":" Istio component ports and functions in detail"},{"content":"As we know that Istio uses iptables for traffic hijacking, where the iptables rule chains has one called ISTIO_OUTPUT, which contains the following rules.\n   Rule target in out source destination     1 RETURN any lo 127.0.0.6 anywhere   2 ISTIO_IN_REDIRECT any lo anywhere !localhost owner UID match 1337   3 RETURN any lo anywhere anywhere !owner UID match 1337   4 RETURN any any anywhere anywhere owner UID match 1337   5 ISTIO_IN_REDIRECT any lo anywhere !localhost owner GID match 1337   6 RETURN any lo anywhere anywhere !owner GID match 1337   7 RETURN any any anywhere anywhere owner GID match 1337   8 RETURN any any anywhere localhost   9 ISTIO_REDIRECT any any anywhere anywhere    The sidecar applies these rules to deal with different types of traffic. This article will show you the six types of traffic and their iptables rules in Istio sidecar.\niptables Traffic Routing in Sidecar The following list summarizes the six types of traffic in Sidecar.\n Remote service accessing local service: Remote Pod -\u003e Local Pod Local service accessing remote service: Local Pod -\u003e Remote Pod Prometheus crawling metrics of local service: Prometheus -\u003e Local Pod Traffic between Local Pod service: Local Pod -\u003e Local Pod Inter-process TCP traffic within Envoy Sidecar to Istiod traffic  The following will explain the iptables routing rules within Sidecar for each scenario, which specifies which rule in ISTIO_OUTPUT is used for routing.\nType 1: Remote Pod -\u003e Local Pod The following are the iptables rules for remote services, applications or clients accessing the local pod IP of the data plane.\nRemote Pod -\u003e RREROUTING -\u003e ISTIO_INBOUND -\u003e ISTIO_IN_REDIRECT -\u003e Envoy 15006 (Inbound) -\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 1 -\u003e  POSTROUTING -\u003e Local Pod\nWe see that the traffic only passes through the Envoy 15006 Inbound port once. The following diagram shows this scenario of the iptables rules.\nRemote Pod to Local Pod  Type 2: Local Pod -\u003e Remote Pod The following are the iptables rules that the local pod IP goes through to access the remote service.\nLocal Pod-\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 9 -\u003e ISTIO_REDIRECT -\u003e Envoy 15001 (Outbound) -\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 4 -\u003e POSTROUTING -\u003e Remote Pod\nWe see that the traffic only goes through the Envoy 15001 Outbound port.\nLocal Pod to Remote Pod  The traffic in both scenarios above passes through Envoy only once because only one scenario occurs in that Pod, sending or receiving requests.\nType 3: Prometheus -\u003e Local Pod Prometheus traffic that grabs data plane metrics does not have to go through the Envoy proxy.\nThese traffic pass through the following iptables rules.\nPrometheus-\u003e RREROUTING -\u003e ISTIO_INBOUND (traffic destined for ports 15002, 15090 will go to INPUT) -\u003e INPUT -\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 3 -\u003e POSTROUTING -\u003e Local Pod\nPrometheus to Local Pod  Type 4: Local Pod -\u003e Local Pod A Pod may simultaneously have two or more services. If the Local Pod accesses a service on the current Pod, the traffic will go through Envoy 15001 and Envoy 15006 ports to reach the service port of the Local Pod.\nThe iptables rules for this traffic are as follows.\nLocal Pod-\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 9 -\u003e ISTIO_REDIRECT -\u003e Envoy 15001（Outbound）-\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 2 -\u003e ISTIO_IN_REDIRECT -\u003e Envoy 15006（Inbound）-\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 1 -\u003e POSTROUTING -\u003e Local Pod\nLocal Pod to Local Pod  Type 5: Inter-process TCP traffic within Envoy Envoy internal processes with UID and GID 1337 will communicate with each other using lo NICs and localhost domains.\nThe iptables rules that these flows pass through are as follows.\nEnvoy process (Localhost) -\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 8 -\u003e POSTROUTING -\u003e Envoy process (Localhost)\nEnvoy inter-process TCP traffic  Type 6: Sidecar to Istiod traffic Sidecar needs access to Istiod to synchronize its configuration so that Envoy will have traffic sent to Istiod.\nThe iptables rules that this traffic passes through are as follows.\npilot-agent process -\u003e OUTPUT -\u003e Istio_OUTPUT RULE 9 -\u003e Envoy 15001 (Outbound Handler) -\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 4 -\u003e POSTROUTING -\u003e Istiod\nSidecar to Istiod  Summary All the sidecar proxies that Istio injects into the Pod or installed in the virtual machine form the data plane of the service mesh, which is also the main workload location of Istio. In my next blog, I will take you through the ports of each component in Envoy and their functions, so that we can have a more comprehensive understanding of the traffic in Istio.\n","permalink":"https://jimmysong.io/en/blog/istio-sidecar-traffic-types/","summary":"This article will show you the six traffic types and their iptables rules in Istio sidecar, and take you through the whole diagram in a schematic format.","title":"Traffic types and iptables rules in Istio sidecar explained"},{"content":"Istio 1.13 is the first release of 2022, and, not surprisingly, the Istio team will continue to release new versions every quarter. Overall, the new features in this release include:\n Support for newer versions of Kubernetes New API – ProxyConfig, for configuring sidecar proxies Improved Telemetry API Support for hostname-based load balancers with multiple network gateways  Support for Kubernetes Versions I often see people asking in the community which Istio supports Kubernetes versions. Istio’s website has a clear list of supported Kubernetes versions. You can see here that Istio 1.13 supports Kubernetes versions 1.20, 1.21, 1.22, and 1.23, and has been tested but not officially supported in Kubernetes 1.16, 1.17, 1.18, 1.19.\nWhen configuring Istio, there are a lot of checklists. I noted them all in the Istio cheatsheet . There are a lot of cheat sheets about configuring Istio, using resources, dealing with everyday problems, etc., in this project, which will be online soon, so stay tuned.\nThe following screenshot is from the Istio cheatsheet website, it shows the basic cheat sheet for setting up Istio.\nIstio cheatsheet  Introducing the new ProxyConfig API Before Istio version 1.13, if you wanted to customize the configuration of the sidecar proxy, there were two ways to do it.\nMeshConfig\nUse MeshConfig and use IstioOperator to modify it at the Mesh level. For example, use the following configuration to alter the default discovery port for istiod.\napVersion:install.istio.io/v1alpha1kind:IstioOperatorspec:meshConfig:defaultConfig:discoveryAddress:istiod:15012Annotation in the Pods\nYou can also use annotation at the Pod level to customize the configuration. For example, you can add the following annotations to Pod to modify the default port for istiod of the workload:\nanannotations:proxy.istio.io/config:|discoveryAddress:istiod:15012When you configure sidecar in either of these ways, the fields set in annotations will completely override the default fields in MeshConfig. Please refer to the Istio documentation for all configuration items of ProxyConfig.\nThe new API – ProxyConfig\nBut in 1.13, a new top-level custom resource, ProxyConfig, has been added, allowing you to customize the configuration of your sidecar proxy in one place by specifying a namespace and using a selector to select the scope of the workload, just like any other CRD. Istio currently has limited support for this API, so please refer to the Istio documentation for more information on the ProxyConfig API.\nHowever, no matter which way you customize the configuration of the sidecar proxy, it does not take effect dynamically and requires a workload restart to take effect. For example, for the above configuration, because you changed the default port of istiod, all the workloads in the mesh need to be restarted before connecting to the control plane.\nTelemetry API MeshConfig customized extensions and configurations in the Istio mesh. The three pillars of observability– Metrics, Telemetry, and Logging– can each be docked to different providers. The Telemetry API gives you a one-stop place for flexible configuration of them. Like the ProxyConfig API, the Telemetry API follows the configuration hierarchy of Workload Selector \u003e Local Namespace \u003e Root Configuration Namespace. The API was introduced in Istio 1.11 and has been further refined in that release to add support for OpenTelemetry logs, filtered access logs, and custom tracing service names. See Telemetry Configuration for details.\nAutomatic resolution of multi-network gateway hostnames In September 2021, a member of the Istio community reported an issue with the EKS load balancer failing to resolve when running multi-cluster Istio in AWS EKS. Workloads that cross cluster boundaries need to be communicated indirectly through a dedicated east-west gateway for a multi-cluster, multi-network mesh. You can follow the instructions on Istio’s website to configure a multi-network, primary-remote cluster, and Istio will automatically resolve the IP address of the load balancer based on the hostname.\nIstio 1.13.1 fixing the critical security vulnerabilities Istio 1.13.1 was released to fix a known critical vulnerability that could lead to an unauthenticated control plane denial of service attack.\nThe figure below shows a multi-cluster primary-remote mesh where istiod exposes port 15012 to the public Internet via a gateway so that a pod on another network can connect to it.\nMulti-network Mesh  When installing a multi-network, primary-remote mode Istio mesh, for a remote Kubernetes cluster to access the control plane, an east-west Gateway needs to be installed in the Primary cluster, exposing port 15012 of the control plane istiod to the Internet. An attacker could send specially crafted messages to that port, causing the control plane to crash. If you set up a firewall to allow traffic from only some IPs to access this port, you will be able to reduce the impact of the problem. It is recommended that you upgrade to Istio 1.13.1 immediately to resolve the issue completely.\nIstioCon 2022 IstioCon 2022  Finally, as a committee member for the last and current IstioCon, I call on everyone to register for IstioCon 2022 , which will be held online on April 25! It will be an industry-focused event, a platform to connect contributors and users to discuss the uses of Istio in different architectural setups, its limitations, and where to take the project next. The main focus on end-user companies, as we look forward to sharing a diversity of case studies showing how to use Istio in production.\n","permalink":"https://jimmysong.io/en/blog/what-is-new-in-istio-1-13/","summary":"In February 2022, Istio released 1.13.0 and 1.13.1. This blog will give you an overview of what’s new in these two releases.","title":"What's new in Istio 1.13?"},{"content":"Join a team of world-class engineers working on the next generation of networking services using Istio, Envoy, Apache SkyWalking and a few of the open projects to define the next generation of cloud native network service.\nIstio upstream contributor: Golang We are looking for engineers with strong distributed systems experience to join our team. We are building a secure, robust, and highly available service mesh platform for mission critical enterprise applications spanning both legacy and modern infrastructure. This is an opportunity to dedicate a significant amount of contribution to Istio upstream on a regular basis. If you are a fan of Istio and would like to increase your contribution on a dedicated basis, this would be an opportunity for you.\nRequirements\n Fundamentals-based problem solving skills; Drive decision by function, first principles based mindset. Demonstrate bias-to-action and avoid analysis-paralysis; Drive action to the finish line and on time. You are ego-less when searching for the best ideasIntellectually curious with a penchant for seeing opportunities in ambiguity Understands the difference between attention to detail vs. detailed - oriented Values autonomy and results over process You contribute effectively outside of your specialty Experience building distributed system platforms using Golang Familiarity with Kubernetes, service mesh technologies such as Istio and Envoy Excellent understanding of networking protocols, concepts, consistency properties of distributed systems, techniques to identify and reconcile configuration drifts Experience contributing to open source projects is a plus. Familiarity with WebAssembly is a plus. Familiarity with Golang, hardware/software load balancers (F5, NGINX), HSM modules, Active Directory/LDAP is a plus.  We encourage written and asynchronous communication in English, and proficient oral English is not required.\nDistributed Systems Engineer, Enterprise Infrastructure (Data plane) GoLang or C++ Developers Seeking backend software engineers experienced in building distributed systems using Golang and gRPC. We are building a secure, and highly available service mesh platform for mission-critical enterprise applications for Fortune 500 companies, spanning both the legacy and modern infrastructure. Should possess strong fundamentals in distributed systems and networking. Familiarity with technologies like Kubernetes, Istio, and Envoy, as well as open contributions would be a plus.\nRequirements\n Experience building distributed system platforms using C++, Golang, and gRPC. Familiarity with Kubernetes, service mesh technologies such as Istio and Envoy. Excellent understanding of networking protocols, concepts, consistency properties of distributed systems, techniques to identify and reconcile configuration drifts. Experience contributing to open source projects is a plus. Familiarity with the following is a plus: WebAssembly, Authorization: NGAC, RBAC, ABAC Familiarity with hardware/software load balancers (F5, NGINX), HSM modules, Active Directory/LDAP is a plus.  Site Reliability Engineer, SRE Site Reliability Engineering (SRE) combines software and systems engineering to build and run scalable, massively distributed, fault-tolerant systems. As part of the team, you will be working on ensuring that Tetrate’s platform has reliability/uptime appropriate to users’ needs as well as a fast rate of improvement. Additionally, much of our engineering effort focuses on building infrastructure, improving the platform troubleshooting abilities, and eliminating toil through automation.\nRequirements\n Systematic problem-solving approach, coupled with excellent communication skills and a sense of ownership/finish and self-directed drive. Strong fundamentals in operating, debugging, and troubleshooting distributed systems(stateful and/or stateless) and networking. Familiarity with Kubernetes, service mesh technologies such as Istio and EnvoyAbility to debug, optimize code, and automate routine tasks. Experience programming in at least one of the following languages: C++, Rust, Python, Go. Familiarity with the concepts of quantifying failure and availability in a prescriptive manner using SLOs and SLIs. Experience in performance analysis and tuning is a plus.  Location Worldwide\nWe are remote with presence in China, Indonesia, India, Japan, U.S., Canada, Ireland, the Netherlands, Spain, and Ukraine.\nPlease send GitHub or online links that showcase your code style along with your resume to careers@tetrate.io .\nAbout Tetrate Powered by Envoy and Istio, its ﬂagship product, Tetrate Service Bridge, enables bridging traditional and modern workloads. Customers can get consistent baked-in observability, runtime security and traffic management for all their workloads, in any environment.\nIn addition to the technology, Tetrate brings a world-class team that leads the open Envoy and Istio projects, providing best practices and playbooks that enterprises can use to modernize their people and processes.\nVarun , co-founder was the initial founder of Istio and gRPC back in Google. We are a two-year-old start-up building compelling network products and services that we believe will result in a step function change in the industry. JJ , co-founder, started the cloud infrastructure team (working alongside Mesos, VM, OS and Provisioning infrastructure teams – under platform team) at Twitter. What got him excited to leave Twitter to start this company is the ability to create a change in how service development happens in enterprises.\nTetrate was founded in Silicon Valley with a large number of our team represented in Canada,, China, India, Indonesia, Ireland, Japan, New Zealand, Singapore, Spain, the Netherlands and Ukraine. Our recruiting goal is simple: find the best talent no matter the background and location. We don’t recruit to a title or a role, instead, we focus on problems that need to be solved. It would be great to discuss overlaps and interests over a live call. You choose where you want to work, flexible time away and work schedule. We have teams get-togethers about 3-4 times a year, most recently in SF, Seattle, Barcelona, San Diego, Washington D.C. and Bandung. This helps with building rapport which then helps with collaboration when you are working virtually.\nTo learn more, pleases visit tetrate.io .\n","permalink":"https://jimmysong.io/en/notice/tetrate-recruit/","summary":"Remotely worldwide","title":"The Enterprise Service Mesh company Tetrate is hiring"},{"content":"As the service mesh architecture concept gains traction and the scenarios for its applications emerge, there is no shortage of discussions about it in the community. I have worked on service mesh with the community for 4 years now, and will summarize the development of service mesh in 2021 from this perspective. Since Istio is the most popular service mesh, this article will focus on the technical and ecological aspects of Istio.\nService mesh: a critical tech for Cloud Native Infrastructure As one of the vital technologies defined by CNCF for cloud native, Istio has been around for five years now. Their development has gone through the following periods.\n Exploration phase: 2017-2018 Early adopter phase: 2019-2020 Large-scale landing and ecological development phase: 2021-present  Service mesh has crossed the “chasm”(refer Crossing the Chasm theory) and is in between the “early majority” and “late majority” phases of adoption. Based on feedback from the audience of Istio Weekly, users are no longer blindly following new technologies for experimentation and are starting to consider whether they need them in their organization dialectically.\nCross the chasm  While new technologies and products continue to emerge, the service mesh, as part of the cloud native technology stack, has continued to solidify its position as the “cloud native network infrastructure” over the past year. The diagram below illustrates the cloud native technology stack model, where each layer has several representative technologies that define the standard. As new-age middleware, the service mesh mirrors other cloud native technologies, such as Dapr (Distributed Application Runtime), which represents the capability model for cloud native middleware, OAM , which defines the cloud native application model, and the service mesh, which defines the L7 network model.\nCloud Native Stack  A layered view of the cloud native application platform technology stack\nOptimizing the mesh for large scale production applications with different deployment models Over the past year, the community focused on the following areas.\n Performance optimization: performance issues of service mesh in large-scale application scenarios. Protocol and extensions: enabling service mesh to support arbitrary L7 network protocols. Deployment models: Proxyless vs. Node model vs. Sidecar model. eBPF: putting some of the service mesh’s capabilities to the kernel layer.  Performance optimization Istio was designed to serve service to service traffic by “proto-protocol forwarding”. The goal is making the service mesh as “transparent” as possible to applications. Thus using IPtables to hijack the traffic, according to the community-provided test results Istio 1.2 adds only 3 ms to the baseline latency for a mesh with 1000 RPS on 16 connections. However, because of issues inherent in the IPtables conntrack module, Istio’s performance issues begin to emerge as the mesh size increases. To optimize the performance of the Istio sidecar for resource usage and network latency, the community gave the following solutions.\n Sidecar configuration: By configuring service dependencies manually or by adding an Operator to the control plane, the number of service configurations sent to Sidecar can be reduced, thus reducing the resource footprint of the data plane; for more automatic and intelligent configuration of Sidecar, the open source projects Slime and Aeraki both offer their innovative configuration loading solutions. The introduction of eBPF: eBPF can be a viable solution to optimize the performance of the service mesh. Some Cilium-based startups even radically propose to use eBPF to replace the Sidecar proxy completely. Still, the Envoy proxy/xDS protocol has become the proxy for the service mesh implementation and supports the Layer 7 protocol very well. We can use eBPF to improve network performance, but complex protocol negotiation, parsing, and user scaling remain challenging to implement on the user side.  Protocol and extensions Extensibility of Istio has always been a significant problem, and there are two aspects to Istio’s extensibility.\n Protocol level: allowing Istio to support all L7 protocols Ecological: allowing Istio to run more extensions  Istio uses Envoy as its data plane. Extending Istio is essentially an extension of Envoy’s functionality. Istio’s official solution is to use WebAssembly, and in Istio 1.12, the Wasm plugin configuration API was introduced to extend the Istio ecosystem. Istio’s extension mechanism uses the Proxy-Wasm Application Binary Interface (ABI) specification to provide a set of proxy-independent streaming APIs and utilities that can be implemented in any language with an appropriate SDK. Today, Proxy-Wasm’s SDKs are AssemblyScript (similar to TypeScript), C++, Rust, Zig, and Go (using the TinyGo WebAssembly System Interface).\nThere are still relatively few WebAssembly extensions available, and many enterprises choose to customize their CRD and build a service mesh management plane based on Istio. In addition, making Istio support heterogeneous environments for all workloads, such as virtual machines and containers, is also in strong demand for end-users. It allows them to migrate applications from traditional loads to service mesh easily. Finally, there is the hybrid cloud traffic management with multiple clusters and mesh, which is a more advanced requirement.\nDeployment models When the service mesh concept first emerged, there was a debate between the Per-node and Sidecar models, represented by Linkerd and Istio. eBPF later proposed a kernel to sink the service mesh, which led to more service mesh deployment models, as shown in the figure below.\nService Mesh Deployment Models  These four deployment methods have their own advantages and disadvantages, the specific choice of which depends on the actual situation.\nDevelopment of the Istio ecosystem and the projects that support Istio 2021 was also an exciting year for the Istio community, with a series of events and tutorials.\n February, the first Istio distribution, Tetrate Istio Distro (TID) . February, the first IstioCon was held online, with over 2,000 participants. March, the first free online Istio Fundamentals Course is released. May, the first Certification Istio Administrator exam be released. May, ServiceMeshCon Europe was held online. July, Istio Meetup China was held in Beijing with more than 100 attendees. October, ServiceMeshCon North America was held in Los Angeles.  There are also numerous open source projects related to Istio Service Mesh, as shown in the table below.\n   Project Value Relationship with Istio Category Launch Date Dominant company Number of stars     Envoy  Cloud native high-performance edge/middle-service proxy The default data plane proxy September 2016 Lyft 18700   Istio  Connection, secure, control, and observation services. Control plane service mesh May 2017 Google 29100   Emissary Gateway  Kubernetes native API gateway for microservices, built on Envoy Connectable to Istio gateway February 2018 Ambassador 3600   APISIX  Cloud native API gateways It can run as a data plane for Istio or as a gateway on its own gateway June 2019 API7 8100   MOSN  Cloud native edge gateways \u0026 agents Available as Istio data plane proxy December 2019 Ant 3500   Slime  Intelligent service mesh manager based on Istio Adding a management plane to Istio extensions January 2021 NetEase 236   GetMesh  Istio integration and command-line management tools Utility for Istio multi-version management tools February 2021 Tetrate 95   Aeraki  Manage any of Istio’s seven layers of load Extended multi-protocol support extensions March 2021 Tencent 330   Layotto  Cloud native application runtime Using as a data plane for Istio runtime June 2021 Ant 393   Hango Gateway  API gateways built on Envoy and Istio Integrates with Istio gateway August 2021 NetEase 253    Note: Data is as of January 6, 2022\nSummary Looking back, we can see that, unlike previous years where users were experimenting, users in 2021 looked for more practical uses for service mesh before implementing them. Their position as the infrastructure of cloud native networks is further strengthened, and more importantly, the service mesh ecosystem is emerging. Looking ahead, in 2022, two technologies to watch are eBPF and WebAssembly(Wasm). We believe that more good examples of service mesh practices will emerge, taking the ecology and standardization a step further.\n","permalink":"https://jimmysong.io/en/blog/service-mesh-in-2021/","summary":"A review of the development of Service Mesh in 2021.","title":"Service Mesh in 2021: the ecosystem is emerging"},{"content":"It’s been more than four years since Istio launched in May 2017, and while the project has had a strong following on GitHub and 10+ releases, its growing open-source ecosystem is still in its infancy.\nRecently added support for WebAssembly extensions has made the most popular open source service mesh more extensible than ever. This table lists the open-source projects in the Istio ecosystem as of November 11, 2021, sorted by open-source date. These projects enhance the Istio service mesh with gateways, extensions, utilities, and more. In this article, I’ll highlight the two new projects in the category of extensions.\n   Project Value Relationship with Istio Category Launch Date Dominant company Number of stars     Envoy  Cloud native high-performance edge/middle-service proxy The default data plane proxy September 2016 Lyft 18700   Istio  Connection, secure, control, and observation services. Control plane service mesh May 2017 Google 29100   Emissary Gateway  Kubernetes native API gateway for microservices, built on Envoy Connectable to Istio gateway February 2018 Ambassador 3600   APISIX  Cloud native API gateways It can run as a data plane for Istio or as a gateway on its own gateway June 2019 API7 8100   MOSN  Cloud native edge gateways \u0026 agents Available as Istio data plane proxy December 2019 Ant 3500   Slime  Intelligent service mesh manager based on Istio Adding a management plane to Istio extensions January 2021 NetEase 236   GetMesh  Istio integration and command-line management tools Utility for Istio multi-version management tools February 2021 Tetrate 95   Aeraki  Manage any of Istio’s seven layers of load Extended multi-protocol support extensions March 2021 Tencent 330   Layotto  Cloud native application runtime Using as a data plane for Istio runtime June 2021 Ant 393   Hango Gateway  API gateways built on Envoy and Istio Integrates with Istio gateway August 2021 NetEase 253    Slime: an intelligent service mesh manager for Istio Slime is an Istio-based, intelligent mesh manager open-sourced by NetEase’s microservices team. Based on the Kubernetes Operator implementation, Slime can be used as a CRD manager that seamlessly interfaces with Istio without needing any customization or definition of dynamic service governance policies. This achieves automatic and convenient use of Istio and Envoy’s advanced features.\nSlime addresses the following issues:\n Implementing higher-level extensions in Istio. For example, extending the HTTP plugin; adaptive traffic limiting based on the resource usage of the service. Poor performance arising from Istio sending all the configurations within the mesh to each sidecar proxy.  Slime solves these problems by building an Istio management plane. Its main purpose are\n to build a pluggable controller to facilitate the extension of new functions. to obtain data by listening to the data plane to intelligently generate the configuration for Istio. to build a higher-level CRD for the user to configure, which Slime converts into an Istio configuration.  The following diagram shows the flow chart of Istio as an Istio management plane.\nSlime architecture  The specific steps for Slime to manage Istio are as follows.\n Slime operator completes the initialization of Slime components in Kubernetes based on the administrator’s configuration. Developers create configurations that conform to the Slime CRD specification and apply them to Kubernetes clusters. Slime queries the monitoring data of the relevant service stored in Prometheus and converts the Slime CRD into an Istio CRD, in conjunction with the configuration of the adaptive part of the Slime CRD while pushing it to the Global Proxy. Istio listens for the creation of Istio CRDs. Istio pushes the configuration information of the Sidecar Proxy to the corresponding Sidecar Proxy in the data plane.  The diagram below shows the internal architecture of Slime.\nSlime Internal  We can divide Slime internally into three main components.\n slime-boot: operator for deploying Slime modules on Kubernetes. slime-controller: the core component of Slime that listens to the Slime CRD and converts it to an Istio CRD. slime-metric: the component used to obtain service metrics information. slime-controller dynamically adjusts service governance rules based on the information it receives.  The following diagram shows the architecture of Slime Adaptive Traffic Limiting. Slime smart limiter  Slime dynamically configures traffic limits by interfacing with the Prometheus metric server to obtain real-time monitoring.\nSlime’s adaptive traffic limitation process has two parts: one that converts SmartLimiter toEnvoyFilter and the other that monitors the data. Slime also provides an external monitoring data interface (Metric Discovery Server) that allows you to sync custom monitoring metrics to the traffic limiting component via MDS.\nThe CRD SmartLimiter created by Slime is used to configure adaptive traffic limiting. Its configuration is close to natural semantics, e.g., if you want to trigger an access limit for Service A with a limit of 30QPS when the CPU exceeds 80%, the corresponding SmartLimiter is defined as follows.\napiVersion:microservice.netease.com/v1alpha1kind:SmartLimitermetadata:name:anamespace:defaultspec:descriptors:- action:fill_interval:seconds:1quota:\"30/{pod}\"# 30 is the quota for this service. If there are three pods, the limit is 10 per pod.condition:\"{cpu}\u003e0.8\"# Auto-fill the template based on the value of the monitor {cpu}Aeraki: A Non-Invasive Istio Extension Toolset Aeraki is a service mesh project open sourced by Tencent Cloud in March 2021. Aeraki provides an end-to-end cloud-native service mesh protocol extension solution that provides Istio with powerful third-party protocol extension capabilities in a non-intrusive way, supporting traffic management for Dubbo, Thrift, Redis, and private protocols in Istio. Aeraki’s architecture is shown in the following diagram.\nAeraki architecture  Aeraki architecture, source Istio blog .\nAs seen in the Aeraki architecture diagram, the Aeraki protocol extension solution consists of two components.\n Aeraki: Aeraki runs as an Istio enhancement component on the control plane, providing user-friendly traffic rule configurations to operations via CRDs. Aeraki translates these traffic rule configurations into Envoy configurations distributed via Istio to sidecar proxies on the data plane. Aeraki also acts as an RDS server providing dynamic routing to the MetaProtocol Proxy on the data plane. The RDS provided by Aeraki differs from Envoy’s RDS in that Envoy RDS primarily offers dynamic routing for the HTTP protocol, while Aeraki RDS is designed to provide dynamic routing capabilities for all L7 protocols developed on the MetaProtocol framework. MetaProtocol Proxy: A generic L7 protocol proxy based on Envoy implementation. MetaProtocol Proxy is an extension of Envoy. It unifies the basic capabilities of service discovery, load balancing, RDS dynamic routing, traffic mirroring, fault injection, local/global traffic limiting, etc. for L7 protocols, which greatly reduces the difficulty of developing third-party protocols on Envoy and allows you to quickly create a third-party protocol plug-in based on MetaProtocol by only implementing the codec interface.  Before the introduction of MetaProtocol Proxy, if you wanted to use Envoy to implement an L7 protocol to implement routing, traffic limiting, telemetry, etc., you needed to write a complete TCP filter, which would have required a lot of work. For most L7 protocols, the required traffic management capabilities are similar, so there is no need to duplicate this work in each L7 filter implementation. The Aeraki project uses a MetaProtocol Proxy to implement these unified capabilities, as shown in the following figure.\nMetaProtocol proxy  MetaProtocol proxy, source Istio blog .\nBased on MetaProtocol Proxy, we only need to implement the codec interface part of the code to write a new L7 protocol Envoy Filter. In addition, without adding a single line of code, Aeraki can provide configuration distribution and RDS dynamic routing configuration for this L7 protocol at the control plane.\nMake Istio work for all environments and workloads We have seen that NetEase and Tencent are scaling Istio mainly by building Operator. However, this scaling is not enough for multi-cluster management. We know that much of our current infrastructure is transitioning to cloud native or containerized, which means containers, virtual machines, and other environments co-exist. How do we unify traffic management of these different environments? It is possible to do so using Istio.\nYou have to again build a management plane on top of Istio and add an abstraction layer to add CRDs that apply to cluster management, such as cluster traffic configuration, policy configuration, etc. Additionally, you have to deploy a Gateway in each cluster that connects uniformly to an edge proxy that interconnects all the groups.\nTo learn more about Tetrate Service Bridge (TSB), which provides this layer of infrastructure, you can go here . TSB is built on the open source Istio with enhancements, it follows the concept of the above two open source projects, and also builds a management plane to support heterogeneous environments.\nAs we can see, the Istio-based projects and the open source environment are booming and companies like Tetrate are doing useful jobs of productizing and making Istio available to all workloads.\n","permalink":"https://jimmysong.io/en/blog/istio-extensions-slime-and-aeraki/","summary":"In this article, I’ll introduce you two Istio extension projects: Aeraki and Slime.","title":"Introducing Slime and Aeraki in the evolution of Istio open-source ecosystem"},{"content":"You can use Istio to do multi-cluster management , API Gateway , and manage applications on Kubernetes or virtual machines . In my last blog , I talked about how service mesh is an integral part of cloud native applications. However, building infrastructure can be a big deal. There is no shortage of debate in the community about the practicability of service mesh and Istio– here’s a list of common questions and concerns, and how to address them.\n Is anyone using Istio in production? What is the impact on application performance due to the many resources consumed by injecting sidecar into the pod? Istio supports a limited number of protocols; is it scalable? Will Istio be manageable? – Or is it too complex, old services too costly to migrate, and the learning curve too steep?  I will answer each of these questions below.\nIstio is architecturally stable, production-ready, and ecologically emerging Istio 1.12 was just released in November – and has evolved significantly since the explosion of service mesh in 2018 (the year Istio co-founders established Tetrate). Istio has a large community of providers and users . The Istio SIG of Cloud Native Community has held eight Istio Big Talk (Istio 大咖说) , with Baidu, Tencent, NetEase, Xiaohongshu(小红书), and Xiaodian Technology(小电科技) sharing their Istio practices. According to CNCF Survey Report 2020 , about 50% of the companies surveyed are using a service mesh in production or planning to in the next year, and about half (47%) of organizations using a service mesh in production are using Istio.\nMany companies have developed extensions or plugins for Istio, such as Ant, NetEase, eBay, and Airbnb. Istio’s architecture has been stable since the 1.5 release, and the release cycle is fixed quarterly, with the current project’s main task being Day-2 Operations.\nThe Istio community has also hosted various events, with the first IstioCon in March 2021, the Istio Meetup China in Beijing in July, and the Service Mesh Summit 2022 in Shanghai in January 2022.\nSo we can say that the Istio architecture is stable and production-ready, and the ecosystem is budding.\nThe impact of service mesh on application performance A service mesh uses iptables to do traffic hijacking by default to be transparent to applications. When the number of services is large, there are a lot of iptables rules that affect network performance. You can use techniques like eBPF to provide application performance, but the method requires a high version of the operating system kernel, which few enterprises can achieve.\nIstio DNS  In the early days, Istio distributed the routing information of all services in the mesh to all proxy sidecars, which caused sidecar s to take up a lot of resources. Aeraki and Slime can achieve configuration lazy loading. We will introduce these two open-source projects in the Istio open-source ecosystem.\nFinally, there is a problem related to Sidecar proxy operation and maintenance: upgrading all Envoy proxies while ensuring constant traffic. A solution is using the SidecarSet resource in the open-source project OpenKruise .\nThe resource consumption and network latency associated with the introduction of Sidecar are also within reasonable limits, as you can see from the service mesh benchmark performance tests .\nExtending the Istio service mesh The next question is about extending the Istio service mesh. The current solution given by the Istio community is to use WebAssembly , an extension that is still relatively little used in production by now and has performance concerns. Most of the answers I’ve observed are CRDs that build a service mesh management plane based on Istio.\nAlso, making Istio support heterogeneous environments for all workloads, such as virtual machines and containers, is in strong demand for end-users. It allows them to migrate applications from traditional loads to cloud native easily. Finally, hybrid cloud traffic management for multiple clusters and meshes is a more advanced requirement.\nSteep learning curve Many people complain that Istio has too little learning material. Istio has been open source for four years, and there are a lot of learning resources now:\n Istio Documentation  IstioCon 2021  Istio Big Talk/Istio Weekly  Istio Fundamentals Course  Certified Istio Administrator   Yes, Istio is complex, but it’s been getting more and more manageable with every release. In my next blog, I will introduce you to two open source projects that extend Istio and give you some insight into what’s going on in the Istio community.\n","permalink":"https://jimmysong.io/en/blog/the-debate-in-the-community-about-istio-and-service-mesh/","summary":"There is no shortage of debate in the community about the practicability of service mesh and Istio – here’s a list of common questions and concerns, and how to address them.","title":"The debate in the community about Istio and service mesh"},{"content":"If you don’t know what Istio is, you can read my previous articles below:\n What Is Istio and Why Does Kubernetes Need it?  Why do you need Istio when you already have Kubernetes?   This article will explore the relationship between service mesh and cloud native.\nService mesh – the product of the container orchestration war If you’ve been following the cloud-native space since its early days, you’ll remember the container orchestration wars of 2015 to 2017. Kubernetes won the container wars in 2017, the idea of microservices had taken hold, and the trend toward containerization was unstoppable. Kubernetes architecture matured and slowly became boring, and service mesh technologies, represented by Linkerd and Istio, entered the CNCF-defined cloud-native critical technologies on the horizon.\nKubernetes was designed with the concept of cloud-native in mind. A critical idea in cloud-native is the architectural design of microservices. When a single application is split into microservices, how can microservices be managed to ensure the SLA of the service as the number of services increases? The service mesh was born to solve this problem at the architectural level, free programmers’ creativity, and avoid tedious service discovery, monitoring, distributed tracing, and other matters.\nThe service mesh takes the standard functionality of microservices down to the infrastructure layer, allowing developers to focus more on business logic and thus speed up service delivery, which is consistent with the whole idea of cloud-native. You no longer need to integrate bulky SDKs in your application, develop and maintain SDKs for different languages, and just use the service mesh for Day 2 operations after the application is deployed.\nThe service mesh is regarded as the next generation of microservices. In the diagram, we can see that many of the concerns of microservices overlap with the functionality of Kubernetes. Kubernetes focuses on the application lifecycle, managing resources and deployments with little control over services. The service mesh fills this gap. The service mesh can connect, control, observe and protect microservices.\nKubernetes vs. xDS vs. Istio This diagram shows the layered architecture of Kubernetes and Istio.\nimg  The diagram indicates that the kube-proxy settings are global and cannot be controlled at a granular level for each service. All Kubernetes can do is topology-aware routing, routing traffic closer to the Pod, and setting network policies in and out of the Pod.\nIn contrast, the service mesh takes traffic control out of the service layer in Kubernetes through sidecar proxies, injects proxies into each Pod, and manipulates these distributed proxies through a control plane. It allows for more excellent resiliency.\nKube-proxy implements traffic load balancing between multiple pod instances of a Kubernetes service. But how do you finely control the traffic between these services — such as dividing the traffic by percentage to different application versions (which are all part of the same service, but on other deployments), or doing canary releases and blue-green releases?\nThe Kubernetes community gives a way to do canary releases using Deployment, assigning different pods to deployed services by modifying the pod’s label.\nEnvoy Architecture  Currently, the most popular open-source implementation of service mesh in the world is Istio. From the CNCF Survey Report 2020 , we know that Istio is the most used service mesh in production today. Many companies have built their service mesh based on Istio, such as Ant, Airbnb, eBay, NetEase, Tencent, etc.\nCNCF Survey Report 2020  Figure from CNCF Survey Report 2020 Istio is developed based on Envoy, which has been used by default as its distributed proxy since the first day it was open-sourced. Envoy pioneered the creation of the xDS protocol for distributed gateway configuration, greatly simplifying the configuration of large-scale distributed networks. Ant Group open source MOSN also supported xDS In 2019. Envoy was also one of the first projects to graduate from CNCF, tested by large-scale production applications.\nService mesh – the cloud-native networking infrastructure With the above comparison between Kubernetes and service mesh in mind, we can see the place of service mesh in the cloud-native application architecture. That is, building a cloud-native network infrastructure specifically provides:\n Traffic management: controlling the flow of traffic and API calls between services, making calls more reliable, and enhancing network robustness in different environments. Observability: understanding the dependencies between services and the nature and flow of traffic between them provides the ability to identify problems quickly. Policy enforcement: controlling access policies between services by configuring the mesh rather than by changing the code. Service Identification and Security: providing service identifiability and security protection in the mesh.  ","permalink":"https://jimmysong.io/en/blog/service-mesh-an-integral-part-of-cloud-native-apps/","summary":"This article will explore the relationship between service mesh and cloud native.","title":"Service Mesh - an integral part of cloud-native applications"},{"content":"API gateways have been around for a long time as the entry point for clients to access the back-end, mainly to manage “north-south” traffic, In recent years, service mesh architectures have become popular, mainly for managing internal systems,(i.e. “east-west” traffic), while a service mesh like Istio also has built-in gateways that bring traffic inside and outside the system under unified control. This often creates confusion for first-time users of Istio. What is the relationship between the service mesh and the API gateway? How does Istio’s gateway work? What are the ways to expose the services in the Istio mesh? This article gives you the answer.\nKey Insights  The service mesh was originally created to solve the problem of managing internal traffic for distributed systems, but API gateways existed long before it. While the Gateway is built into Istio, you can still use a custom Ingress Controller to proxy external traffic. API gateways and service mesh are converging.  How do I expose services in the Istio mesh? The following diagram shows four approaches to expose services in the Istio mesh using Istio Gateway, Kubernetes Ingress, API Gateway, and NodePort/LB.\nExposing services through Istio Ingress Gateway  The Istio mesh is shaded, and the traffic in the mesh is internal (east-west) traffic, while the traffic from clients accessing services within the Kubernetes cluster is external (north-south) traffic.\n   Approach Controller Features     NodePort/LoadBalancer Kubernetes Load balancing   Kubernetes Ingress Ingress controller Load balancing, TLS, virtual host, traffic routing   Istio Gateway Istio Load balancing, TLS, virtual host, advanced traffic routing, other advanced Istio features   API Gateway API Gateway Load balancing, TLS, virtual host, advanced traffic routing, API lifecycle management, billing, rate limiting, policy enforcement, data aggregation    Since NodePort/LoadBalancer is a basic way to expose services built into Kubernetes, this article will not discuss that option. Each of the other three approaches will be described below.\nUsing Kubernetes Ingress to expose traffic We all know that clients of a Kubernetes cluster cannot directly access the IP address of a pod because the pod is in a network plane built into Kubernetes. We can expose services inside Kubernetes outside the cluster using NodePort or Load Balancer Kubernetes service type. To support virtual hosting, hiding and saving IP addresses, you can use Ingress resources to expose services in Kubernetes.\nKubernetes Ingress to expose services  Ingress is a Kubernetes resource that controls the behavior of an ingress controller that does the traffic touring, which is the equivalent of a load-balanced directional proxy server such as Nginx, Apache, etc., which also includes rule definitions, i.e., routing information for URLs, which is provided by the Ingress controller .\napiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:annotations:kubernetes.io/ingress.class:istioname:ingressspec:rules:- host:httpbin.example.comhttp:paths:- path:/status/*backend:serviceName:httpbinservicePort:8000The kubernetes.io/ingress.class: istio annotation in the example above indicates that the Ingress uses the Istio Ingress Controller which in fact uses Envoy proxy.\nUsing Istio Gateway to expose services Istio is a popular service mesh implementation that has evolved from Kubernetes that implements some features that Kubernetes doesn’t. (See What is Istio and why does Kubernetes need Istio? ) It makes traffic management transparent to the application, moving this functionality from the application to the platform layer and becoming a cloud-native infrastructure.\nIstio used Kubernetes Ingress as the traffic portal in versions prior to Istio 0.8, where Envoy was used as the Ingress Controller. From Istio 0.8 and later, Istio created the Gateway object. Gateway and VirtualService are used to represent the configuration model of Istio Ingress, and the default implementation of Istio Ingress uses the same Envoy proxy. In this way, the Istio control plane controls both the ingress gateway and the internal sidecar proxy with a consistent configuration model. These configurations include routing rules, policy enforcement, telemetry, and other service control functions.\nThe Istio Gateway resources function similarly to the Kubernetes Ingress in that it is responsible for north-south traffic to and from the cluster. The Istio Gateway acts as a load balancer to carry connections to and from the edge of the service mesh. The specification describes a set of open ports and the protocols used by those ports, as well as the SNI configuration for load balancing, etc.\nThe Istio Gateway resource itself can only be configured for L4 through L6, such as exposed ports, TLS settings, etc.; however, the Gateway can be bound to a VirtualService, where routing rules can be configured on L7, such as versioned traffic routing, fault injection, HTTP redirects, HTTP rewrites, and all other routing rules supported within the mesh.\nBelow is an example of a Gateway binding to a VirtualService. The pod with the “istio: ingressgateway” label will act as the Ingress controller and route HTTP traffic to port 80 of the httpbin.example.com virtual host. The biggest difference between this and using Kubernetes Ingress is that it requires us to manually bind the VirtualService to the Gateway and specify the pod where the Gateway is located. This configuration is equivalent to opening up an entry point to Kubernetes for external access.\napiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:httpbin-gatewayspec:selector:istio:ingressgatewayservers:- port:number:80name:httpprotocol:HTTPhosts:- \"httpbin.example.com\"The VirtualService below is bound to the gateway above via gateways to accept traffic from that gateway.\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:httpbinspec:hosts:- \"httpbin.example.com\"gateways:- httpbin-gatewayhttp:- match:- uri:prefix:/statusroute:- destination:port:number:8000host:httpbinUsing an API Gateway API gateways are API management tools that sit between the client and the back-end service and are widely used in microservices as a way to separate the client interface from the back-end implementation. When a client makes a request, the API gateway breaks it down into multiple requests, then routes them to the correct location, generates a response, and keeps track of everything.\nThe API Gateway is a special type of service in the microservices architecture that serves as the entry point for all microservices and is responsible for performing routing requests, protocol conversions, aggregating data, authentication, rate limiting, circuit breaking, and more. Most enterprise APIs are deployed through API Gateways, which typically handle common tasks across API service systems, such as TLS termination, authentication and authorization, rate limiting, and statistical information.\nThere can be one or more API Gateways in the mesh. The responsibilities of the API Gateway are\n Request routing and version control Facilitating the transition of monolithic applications to microservices Permission authentication Data aggregation: monitoring and billing Protocol conversion Messaging and caching Security and alerting  Many of the above basic functions such as routing and permission authentication can also be achieved through Istio Gateway, but some mature API gateways may be more advantageous in terms of feature richness and scalability.\n The introduction of API Gateway requires consideration of the deployment, operation and maintenance, load balancing, and other scenarios of API Gateway itself, which increases the complexity of back-end services. An API Gateway carries a large number of interface adaptations, which makes it difficult to maintain. For some scenarios, the addition of a hop may lead to a reduction in performance.  Currently, some API Gateway imitations are building their own service mesh by deploying them in the sidecar.\nSummary In the Istio mesh, you can use a variety of Kubernetes Ingress Controllers to act as entry gateways, but of course, you can also use Istio’s built-in Istio Gateway directly, for policy control, traffic management, and usage monitoring. The advantage of this is that the gateway can be managed directly through Istio’s control plane, without the need for additional tools. But for functions such as API statement cycle management, complex billing, protocol conversion, and authentication, a traditional API gateway may be a better fit for you. So, you can choose according to your needs, or you can use a combination.\nSome traditional reverse proxies are also moving towards Service Mesh, such as Nginx with Nginx Service Mesh and Traefik with Traefik Mesh, and some API gateway products are also moving towards Service Mesh, such as Kong with Kuma, and in the future, we will see more convergence of API gateways, reverse proxies, and service meshes.\n","permalink":"https://jimmysong.io/en/blog/istio-servicemesh-api-gateway/","summary":"What is the relationship between the service mesh and the API gateway? How does Istio’s gateway work? What are the ways to expose the services in the Istio mesh? This article gives you the answer.","title":"Using Istio service mesh as API Gateway"},{"content":"Do you have multiple Kubernetes clusters and a service mesh? Do your virtual machines and services in a Kubernetes cluster need to interact? This article will take you through the process and considerations of building a hybrid cloud using Kubernetes and an Istio Service Mesh. Together, Kubernetes and Istio can be used to bring hybrid workloads into a mesh and achieve interoperability for multicluster. But another layer of infrastructure — a management plane — is helpful for managing multicluster or multimesh deployments.\nKubernetes Using Kubernetes enables rapid deployment of a distributed environment that enables cloud interoperability and unifies the control plane on the cloud. It also provides resource objects, such as Service, Ingress and Gateway , to handle application traffic. The Kubernetes API Server communicates with the kube-proxy component on each node in the cluster, creates iptables rules for the node, and forwards requests to other pods.\nAssuming that a client now wants to access a service in Kubernetes, the request is first sent to the Ingress/Gateway, then forwarded to the backend service (Service A in the diagram below) based on the routing configuration in the Ingress/Gateway. Then Service A polls an instance of Service B for the traffic requested by Service B. Lastly, the traffic requested by Service A for Service B is polled forward to Service B’s instance.\nKubernetes  Kubernetes Multicluster The most common usage scenarios for multicluster management include:\n service traffic load balancing isolating development and production environments decoupling data processing and data storage cross-cloud backup and disaster recovery flexible allocation of compute resources low-latency access to services across regions avoiding vendor lock-in  There are often multiple Kubernetes clusters within an enterprise; and the KubeFed implementation of Kubernetes cluster federation developed by Multicluster SIG enables multicluster management capabilities, which allows all Kubernetes clusters to be managed through the same interface.\nThere are several general issues that need to be addressed when using cluster federation:\n Configuring which clusters need to be federated API resources need to be propagated across the clusters Configuring how API resources are distributed to different clusters Registering DNS records in clusters to enable service discovery across clusters  The following is a multicluster architecture for KubeSphere — one of the most commonly used Kubernetes multicluster management architectures — where the Host Cluster serves as the control plane with two member clusters, West and East.\nMulticluster  The Host Cluster needs to be able to access the API Server of the Member Cluster, but the network connectivity between Member Clusters is not required. The Host Cluster is independent of the Member Cluster it manages and the Member Cluster is not aware of the existence of the Host Cluster. The advantage of this is that when the control plane fails, the Member Cluster will not be affected and the deployed load can still operate normally without being affected.\nThe Host Cluster also assumes the role of API portal, and the Host Cluster forwards the resource requests to the Member Cluster — which is convenient for aggregation and also facilitates unified authority authentication. We see that there is a Federation Control Plane in the Host Cluster, where the Push Reconciler propagates the identity, role, and role binding from the Federation Cluster to all Member Clusters.\nIstio Service Mesh Consider using the Istio service mesh when we have multilingual, multiversion microservices running in Kubernetes and need finer-grained canary publishing and unified security policy management for inter-service observability. Istio enables intelligent application-aware load balancing from the application layer to other Service Mesh-enabled services in the cluster, by transparently intercepting all traffic to and from the application using IPTables, and bypassing the primary kube-proxy load balancing. The Istio control plane communicates with the Kubernetes API Server to obtain information about all registered services in the cluster.\nThe following diagram illustrates the basics of Istio, where all nodes belong to the same Kubernetes cluster.\nIstio Service Mesh  You may end up with at least a few Kubernetes clusters, each hosting microservices. Multiple deployment models exist for Istio’s multicluster deployments — depending on network isolation, primary and backup — which can be specified by declaration when deploying using Istio Operator. Communication between these microservices in a cluster can be enhanced by a service mesh. Within the cluster, Istio provides common communication patterns to improve resiliency, security and observability.\nAll of the above is about application load management on Kubernetes, but for legacy applications on virtual machines: how can they be managed in the same plane? Istio supports applications on virtual machines, so why do we need a management plane?\nManagement Plane To manage gateways, traffic and security groupings, and apply them to different clusters and namespaces, you’ll need to add another layer of abstraction on top of Istio: a management plane. The diagram below shows the multitenant model of Tetrate Service Bridge (TSB). TSB uses Next Generation Access Control (NGAC) — a fine-grained authorization framework — to manage user access and also facilitate the construction of a zero-trust network.\nManagement Plane  Istio provides workload identification, protected by strong mTLS encryption. This zero-trust model is better than trusting workloads based on topology information, such as source IP. A common control plane for multicluster management is built on top of Istio. Then a management plane is added to manage multiple clusters — providing multitenancy, management configuration, observability, and more.\nThe diagram below shows the architecture of Tetrate Service Bridge.\nTetrate Service Bridge  Summary Interoperability of heterogeneous clusters is achieved with Kubernetes. Istio brings containerized and virtual machine loads into a single control plane, to unify traffic, security and observability within the clusters. However, as the number of clusters, network environments and user permissions become more complex, there is a need to build another management plane above Istio’s control plane (for example, Tetrate Service Bridge ) for hybrid cloud management.\n","permalink":"https://jimmysong.io/en/blog/multicluster-management-with-kubernetes-and-istio/","summary":"This article explains three patterns/tools for debugging microservices in Kubernetes and the changes brought by the introduction of Istio for debugging microservices.","title":"Multicluster Management with Kubernetes and Istio"},{"content":"Kubernetes is arguably the best environment for running microservices so far, but the experience of debugging microservices in a Kubernetes environment may not be as user-friendly. This article will show you how to debug microservices in Kubernetes, introduce common tools, and explain how the introduction of Istio impacts debugging microservices.\nDebugging microservices is vastly different from traditional monolithic applications The debugging of microservices has been a long-standing problem for software developers. This challenge does not exist in traditional monolithic applications because developers can leverage the debugger in IDEs to add breakpoints, modify environment variables, single-step execution, etc. for their applications, all of which provide great help in software debugging. With the popularity of Kubernetes, the debugging of microservices becomes a thorny issue, where the following issues are more complicated than the debugging of traditional monolithic applications.\nMultiple dependencies A microservice often depends on multiple other microservices, some shared volumes across multiple microservices, and authorizations based on service accounts. When debugging a microservice, how do you deploy other dependent services to quickly build a latest set of staging environments?\nAccess from a local machine When microservices are running on a developer’s local computer, there is usually no direct access to the services in a Kubernetes cluster. How can you debug microservices deployed in a Kubernetes cluster as if they were local services?\nSlow development loop Usually, it takes a long process to update the code and build it into an image before pushing it to the cluster. How do you speed up the development cycle? Let’s look at the tools that address those challenges.\nTools The main solutions for debugging microservices in Kubernetes are:\n Proxy: by building a VPN, deploying a proxy in the Kubernetes cluster, and adding local debug endpoints to make the services in Kubernetes directly accessible to local applications, your architecture will look like [ local service ] \u003c-\u003e [ proxy ] \u003c-\u003e [ app in Kubernetes ]. Sidecar: Inject a sidecar into the pod of the microservice to be debugged to intercept all traffic to and from the service, so that the service can be tracked and monitored, and the service can also be debugged in this sidecar. Service Mesh: To get an overall picture of the application, inject sidecars into all microservices so that you can get a dashboard that monitors global status.  Here are three typical open source projects that implement the above solutions, each of which can help you debug microservices from a different perspective. You can apply them at different stages of software development and they can be said to be complementary to each other.\nProxy – debugging microservices with Telepresence Telepresence is essentially a local proxy that proxies data volumes, environment variables, and networks in a Kubernetes cluster locally. The following diagram shows the main usage scenarios for Telepresence.\nProxy mode: Telepresence  Users need to manually execute the telepresence command locally, which will automatically deploy the agent to Kubernetes. Once the agent has been deployed,\n Local services will have complete access to other services in the Kubernetes cluster, environment variables, Secret, ConfigMap, etc. Services in the cluster also have direct access to the locally exposed endpoints.  However, this approach requires users to run multiple commands while debugging locally, and in some network environments it may not be possible to establish a VPN connection to the Kubernetes cluster.\nSidecar – debugging microservices with Nocalhost Nocalhost is a Kubernetes-based cloud development environment. To use it, you just need to install a plugin in your IDE – VS Code to extend Kubernetes and shorten the development feedback cycle. The development environment can be isolated by creating different namespaces for different users and using ServiceAccount when binding to different user corners. Nocalhost also provides a web console and API for administrators to manage different development environments.\nSidecar mode: Nocalhost  As long as you have a Kubernetes cluster and have admin rights to the cluster, you can refer to the Nocalhost documentation to quickly start trying it out. To use the Nocalhost plugin in VS Code, you need to configure the Kubernetes cluster in the plugin first.\n Select the Kubeconfig file you just exported or copy and paste the contents of the file directly into the configuration. Then select the service you need to test and select the corresponding Dev Container. VS Code will automatically open a new code window.  Here is an example of the bookinfo sample provided by Istio. You can open the cloned code in your local IDE and click the hammer next to the code file to enter development mode. Selecting the corresponding DevContainer and Nocalhost will automatically inject a development container sidecar into the pod and automatically enter the container in the terminal, as shown in the following figure.\nNocalhost VS code  In development mode, the code is modified locally without rebuilding the image, and the remote development environment takes effect in real time, which can greatly accelerate the development speed. At the same time, Nocalhost also provides a server for managing the development environment and user rights, as shown in the following figure.\nNocalhost Web  Service Mesh – debugging microservices with Istio The above method of using proxy and sidecar can only debug one service at a time. You’ll need a mesh to get the global status of the application, such as the metrics of the service obtained, and debug the performance of the service by understanding the dependency and invocation process of the service through distributed tracing. These observability features need to be implemented by injecting sidecar uniformly for all services. And, when your services are in the process of migrating from VMs to Kubernetes, using Istio can bring VMs and Kubernetes into a single network plane (as shown below), making it easy for developers to debug and do incremental migrations.\nSerivce Mesh mode: Istio  Of course, these benefits do not come without a “cost.” With the introduction of Istio, your Kubernetes services will need to adhere to the Istio naming convention and you’ll need to know how to debug microservices using the Istioctl command line and logging.\n Use the istioctl analyze command to debug the deployment of microservices in your cluster, and you can use YAML files to examine the deployment of resources in a namespace or across your cluster. Use istioctl proxy-config secret to ensure that the secret of a pod in a service mesh is loaded correctly and is valid.  Summary In the process of microservicing applications and migrating from virtual machines to Kubernetes, developers need to make a lot of changes in their mindset and habits. By building a VPN between local and Kubernetes via proxy, developers can easily debug services in Kubernetes as if they were local services. By injecting a sidecar into the pod, you can achieve real-time debugging and speed up the development process. Finally, the Istio service mesh truly enables global observability, and you can also use tools like Tetrate Service Bridge to manage heterogeneous platforms, helping you gradually move from monolithic applications to microservices.\n","permalink":"https://jimmysong.io/en/blog/how-to-debug-microservices-in-kubernetes-with-proxy-sidecar-or-service-mesh/","summary":"This article explains three patterns/tools for debugging microservices in Kubernetes and the changes brought by the introduction of Istio for debugging microservices.","title":"How to debug microservices in Kubernetes with proxy, sidecar or service mesh?"},{"content":"Istio was named by Tetrate founder Varun Talwar and Google lead engineer Louis Ryan in 2017 and was open sourced on May 24, 2017. Today is the fourth anniversary of Istio’s open source arrival. Let’s take a look back at Istio’s four years of development — and look forward to Istio’s future.\nIstio’s open source history In 2017, the year Kubernetes ended the container orchestration battle, Google took the opportunity to consolidate its dominance in the cloud native space and compensate for Kubernetes’ disadvantage in service-to-service traffic management by open-sourcing Istio. Istio released its 1.10 last week — but here are some of the most important releases in Istio’s history to date.\n   Date Version Note     May 24, 2017 0.1 Officially open source; established the architectural foundation of Control Plane, Data Plane and sidecar proxy.   October 10, 2017 0.2 Started to support multiple runtime environments, such as virtual machines.   June 1, 2018 0.8 API refactoring   July 31, 2018 1.0 Production-ready, after which the Istio team underwent a massive reorganization.   March 19, 2019 1.1 Enterprise-ready. Support for multiple Kubernetes clusters, with performance optimizations.   March 3, 2020 1.5 Back to monolith, with microservice components merged into istiod, making Istio’s architecture cleaner and easier to maintain. Support for WebAssembly extension, making Istio’s ecology much stronger.   November 18, 2020 1.8 Officially deprecated Mixer and focused on adding support for virtual machines.    A year after its inception– and two months before the 1.0 release, version 0.8 was released with a massive refactoring of the API. In late July 2018, when 1.0 was released, Istio reached a production-ready tipping point. Since then, Google has massively reorganized the Istio team and several Istio-based service mesh startups were born, making 2018 the booming year of the service mesh industry.\nIstio 1.1 was released in March 2019, almost 9 months after 1.0 was released, which is far beyond the average release cycle of an open-source project. We know that the speed of iteration and evolution is a core competency of basic software. Since then, Istio has started a regular release cadence of one version per quarter and has become the #4 fastest growing project in GitHub’s top 10 in 2019 !\nThe Istio community In 2020, Istio’s project management began to mature and its governance reached a stage of evolution. We saw the first election of a steering committee for the Istio community and the transfer of the trademark to Open Usage Commons . The first IstioCon was successfully held in February 2021, with thousands of people attending the online conference. There is also a large Istio community in China , and face-to-face Istio community meetups will be held there in 2021. Stay tuned for more.\n  According to the CNCF 2020 Survey, 46% of organizations were either using a service mesh in production or planning to use it in the next 12 months. Istio was the top used mesh among those using a mesh in production.\nThe future After 4 years of development, there is not only a large user base around Istio, but also several Istio vendors, as you can see on the homepage of the recently revamped Istio website. In the last few releases, Istio has shifted its development focus to improving the Day 2 Operation experience. We also expect to see more Istio adoption path recommendations, case studies, learning materials, training, and certifications (such as the industry’s first Certified Istio Administrator from Tetrate) that will facilitate the adoption of Istio.\n","permalink":"https://jimmysong.io/en/blog/istio-4-year-birthday/","summary":"Today is Istio's 4 year birthday, let’s take a look back at Istio’s four years of development — and look forward to Istio’s future.","title":"Happy Istio 4th Anniversary -- Retrospect and Outlook"},{"content":"Istio, the most popular service mesh implementation , was developed on top of Kubernetes and has a different niche in the cloud native application ecosystem than Kubernetes. Rather than introduce you directly to what Istio has to offer, this article will explain how Istio came about and what it is in relation to Kubernetes.\nWhy Is There an Istio? To explain what Istio is, it’s also important to understand the context in which Istio came into being — i.e., why is there an Istio?\nMicroservices are a technical solution to an organizational problem. And Kubernetes/Istio are a technical solution to deal with the issues created by moving to microservices. As a deliverable for microservices, containers solve the problem of environmental consistency and allow for more granularity in limiting application resources. They are widely used as a vehicle for microservices.\nGoogle open-sourced Kubernetes in 2014, which grew exponentially over the next few years. It became a container scheduling tool to solve the deployment and scheduling problems of distributed applications — allowing you to treat many computers as though they were one computer. Because the resources of a single machine are limited and Internet applications may have traffic floods at different times (due to rapid expansion of user scale or different user attributes), the elasticity of computing resources needs to be high. A single machine obviously can’t meet the needs of a large-scale application; and conversely, it would be a huge waste for a very small-scale application to occupy the whole host.\nIn short, Kubernetes defines the final state of the service and enables the system to reach and stay in that state automatically. So how do you manage the traffic on the service after the application has been deployed? Below we will look at how service management is done in Kubernetes and how it has changed in Istio.\nHow Do You Do Service Management in Kubernetes? The following diagram shows the service model in Kubernetes:\nKubernetes Service Model  From the above figure we can see that:\n Different instances of the same service may be scheduled to different nodes. Kubernetes combines multiple instances of a service through Service objects to unify external services. Kubernetes installs a kube-proxy component in each node to forward traffic, which has simple load balancing capabilities. Traffic from outside the Kubernetes cluster can enter the cluster via Ingress (Kubernetes has several other ways of exposing services; such as NodePort, LoadBalancer, etc.).  Kubernetes is used as a tool for intensive resource management. However, after allocating resources to the application, Kubernetes doesn’t fully solve the problems of how to ensure the robustness and redundancy of the application, how to achieve finer-grained traffic division (not based on the number of instances of the service), how to guarantee the security of the service, or how to manage multiple clusters, etc.\nThe Basics of Istio The following diagram shows the service model in Istio, which supports both workloads and virtual machines in Kubernetes.\nIstio  From the diagram we can see that:\n Istiod acts as the control plane, distributing the configuration to all sidecar proxies and gateways. (Note: for simplification, the connections between Istiod and sidecar are not drawn in the diagram.) Istio enables intelligent application-aware load balancing from the application layer to other mesh enabled services in the cluster, and bypasses the rudimentary kube-proxy load balancing. Application administrators can manipulate the behavior of traffic in the Istio mesh through a declarative API, in the same way they manage workloads in Kubernetes. It can take effects within seconds and they can do this without needing to redeploy. Ingress is replaced by Gateway resources, a special kind of proxy that is also a reused Sidecar proxy. A sidecar proxy can be installed in a virtual machine to bring the virtual machine into the Istio mesh.  In fact, before Istio one could use SpringCloud, Netflix OSS, and other tools to programmatically manage the traffic in an application, by integrating the SDK in the application. Istio makes traffic management transparent to the application, moving this functionality out of the application and into the platform layer as a cloud native infrastructure.\nIstio complements Kubernetes, by enhancing its traffic management, observability and security for cloud native applications. The service mesh open source project — launched in 2017 by Google, IBM and Lyft — has come a long way in three years. A description of Istio’s core features can be found in the Istio documentation .\nSummary  Service Mesh is the cloud native equivalent of TCP/IP, addressing application network communication, security and visibility issues. Istio is currently the most popular service mesh implementation, relying on Kubernetes but also scalable to virtual machine loads. Istio’s core consists of a control plane and a data plane, with Envoy as the default data-plane agent. Istio acts as the network layer of the cloud native infrastructure and is transparent to applications.  ","permalink":"https://jimmysong.io/en/blog/what-is-istio-and-why-does-kubernetes-need-it/","summary":"This article will explain how Istio came about and what it is in relation to Kubernetes.","title":"What Is Istio and Why Does Kubernetes Need it?"},{"content":"If you’ve heard of service mesh and tried Istio , you may have the following questions:\n Why is Istio running on Kubernetes? What is the role of Kubernetes and a service mesh in the cloud native application architecture, respectively? What aspects of Kubernetes does Istio extend? What problems does it solve? What is the relationship between Kubernetes, Envoy, and Istio?  This article will take you through the inner workings of Kubernetes and Istio. In addition, I will introduce the load balancing approach in Kubernetes, and explain why you need Istio when you have Kubernetes.\nKubernetes is essentially about application lifecycle management through declarative configuration, while a service mesh is essentially about providing inter-application traffic, security management and observability. If you have already built a stable application platform using Kubernetes, how do you set up load balancing and traffic control for calls between services? This is where a service mesh comes into the picture.\nEnvoy introduces the xDS protocol, which is supported by various open source software, such as Istio , MOSN , etc. Envoy contributes xDS to a service mesh or cloud native infrastructure. Envoy is essentially a modern version of a proxy that can be configured through APIs, based on which many different usage scenarios are derived — such as API Gateway, sidecar proxy in service mesh, and edge proxy.\nThis article contains the following:\n A description of the role of kube-proxy. The limitations of Kubernetes for microservice management. An introduction to the capabilities of Istio service mesh. A comparison of some of the concepts in Kubernetes, Envoy, and the Istio service mesh.  Kubernetes vs Service Mesh The following diagram shows the service access relationship in Kubernetes and service mesh (one sidecar per pod model).\nKubernetes vs Service Mesh  Traffic Forwarding Each node in a Kubernetes cluster deploys a kube-proxy component that communicates with the Kubernetes API Server, gets information about the services in the cluster, and then sets iptables rules to send requests for service directly to the corresponding Endpoint (a pod belonging to the same group of services).\nService Discovery Service Discovery  Istio can follow the service registration in Kubernetes and can also interface with other service discovery systems via platform adapters in the control plane; and then generate data plane configurations (using CRD, which are stored in etcd) with transparent proxies for the data plane. The transparent proxy of the data plane is deployed as a sidecar container in the pod of each application service, and all these proxies need to request the control plane to synchronize the proxy configuration. The proxy is “transparent” because the application container is completely unaware of the presence of the proxy. The kube-proxy component in the process needs to intercept traffic as well, except that the kube-proxy intercepts traffic to and from the Kubernetes node — while the sidecar proxy intercepts traffic to and from the pod.\nDisadvantages of a Service Mesh Since Kubernetes has many pods running on each node, putting the original kube-proxy route forwarding function in each pod will increase the response latency — due to more hops when the sidecar intercepts the traffic — and consume more resources. In order to manage traffic in a fine-grained manner, a series of new abstractions will be added. This will further increase the learning cost for users, but as the technology becomes more popular this situation will be slowly alleviated.\nAdvantages of a Service Mesh The kube-proxy settings are global and cannot be controlled at a granular level for each service, while service mesh takes the traffic control out of the service layer in Kubernetes by means of sidecar proxy — allowing for more elasticity.\nShortcomings of Kube-Proxy First, it does not automatically try another pod if the forwarded pod is not serving properly. Each pod has a health check mechanism and when a pod has health problems, kubelet will restart the pod and kube-proxy will remove the corresponding forwarding rules. Also, nodePort-type services cannot add TLS or more complex message routing mechanisms.\nKube-proxy implements load balancing of traffic across multiple pod instances of a Kubernetes service, but how do you do fine-grained control of traffic between these services — such as dividing traffic by percentage to different application versions (which are all part of the same service but on different deployments), or doing canary releases (grayscale releases) and blue-green releases?\nThe Kubernetes community gives a way to do canary releases using Deployment , which is essentially a way to assign different pods to a deployment’s service by modifying the pod’s label.\nKubernetes Ingress vs. Istio Gateway As mentioned above, kube-proxy can only route traffic within a Kubernetes cluster. The pods of a Kubernetes cluster are located in a network created by CNI. An ingress — a resource object created in Kubernetes — is created for communication outside the cluster. It’s driven by an ingress controller located on Kubernetes edge nodes responsible for managing north-south traffic. Ingress must be docked to various Ingress Controllers, such as the nginx ingress controller and traefik . Ingress is only applicable to HTTP traffic and is simple to use. It can only route traffic by matching a limited number of fields — such as service, port, HTTP path, etc. This makes it impossible to route TCP traffic such as MySQL, Redis, and various RPCs. This is why you see people writing nginx config language in ingress resource annotations.The only way to directly route north-south traffic is to use the service’s LoadBalancer or NodePort, the former requiring cloud vendor support and the latter requiring additional port management.\nIstio Gateway functions similarly to Kubernetes Ingress, in that it is responsible for north-south traffic to and from the cluster. Istio Gateway describes a load balancer for carrying connections to and from the edge of the mesh. The specification describes a set of open ports and the protocols used by those ports, the SNI configuration for load balancing, etc. Gateway is a CRD extension that also reuses the capabilities of the sidecar proxy; see the Istio website for detailed configuration.\nEnvoy Envoy is the default sidecar proxy in Istio. Istio extends its control plane based on Enovy’s xDS protocol. We need to familiarize ourselves with Envoy’s basic terminology before talking about Envoy’s xDS protocol. The following is a list of basic terms and their data structures in Envoy; please refer to the Envoy documentation for more details.\nEnvoy  Basic Terminology The following are the basic terms in Enovy that you should know.\n Downstream: The downstream host connects to Envoy, sends the request, and receives the response; i.e., the host that sent the request. Upstream: The upstream host receives connections and requests from Envoy and returns responses; i.e., the host that receives the requests. Listener: Listener is a named network address (for example, port, UNIX domain socket, etc.); downstream clients can connect to these listeners. Envoy exposes one or more listeners to the downstream hosts to connect. Cluster: A cluster is a group of logically identical upstream hosts to which Envoy connects. Envoy discovers the members of a cluster through service discovery. Optionally, the health status of cluster members can be determined through proactive health checks. Envoy decides which member of the cluster to route requests through a load balancing policy.  Multiple listeners can be set in Envoy, each listener can set a filter chain (filter chain table), and the filter is scalable so that we can more easily manipulate the behavior of traffic — such as setting encryption, private RPC, etc.\nThe xDS protocol was proposed by Envoy and is the default sidecar proxy in Istio, but as long as the xDS protocol is implemented, it can theoretically be used as a sidecar proxy in Istio — such as the MOSN open source by Ant Group.\nimg   Istio is a very feature-rich service mesh that includes the following capabilities.\n Traffic Management: This is the most basic feature of Istio. Policy Control: Enables access control systems, telemetry capture, quota management, billing, etc. Observability: Implemented in the sidecar proxy. Security Authentication: The Citadel component does key and certificate management.  Traffic Management in Istio The following CRDs are defined in Istio to help users with traffic management.\n Gateway: Gateway describes a load balancer that runs at the edge of the network and is used to receive incoming or outgoing HTTP/TCP connections. VirtualService: VirtualService actually connects the Kubernetes service to the Istio Gateway. It can also perform additional operations, such as defining a set of traffic routing rules to be applied when a host is addressed. DestinationRule: The policy defined by the DestinationRule determines the access policy for the traffic after it has been routed. Simply put, it defines how traffic is routed. Among others, these policies can be defined as load balancing configurations, connection pool sizes, and external detection (for identifying and expelling unhealthy hosts in the load balancing pool) configurations. EnvoyFilter: The EnvoyFilter object describes filters for proxy services that can customize the proxy configuration generated by Istio Pilot. This configuration is generally rarely used by primary users. ServiceEntry: By default, services in the Istio service mesh are unable to discover services outside of the Mesh. ServiceEntry enables additional entries to be added to the service registry inside Istio, thus allowing automatically discovered services in the mesh to access and route to these manually added services.  Kubernetes vs. xDS vs. Istio Having reviewed the abstraction of traffic management in Kubernetes’ kube-proxy component, xDS, and Istio, let’s look now at a comparison of the three components/protocols in terms of traffic management only (note that the three are not exactly equivalent).\n   Kubernetes xDS Istio service mesh     Endpoint Endpoint WorkloadEntry   Service Route VirtualService   kube-proxy Route DestinationRule   kube-proxy Listener EnvoyFilter   Ingress Listener Gateway   Service Cluster ServiceEntry    Takeaways  The essence of Kubernetes is application lifecycle management, specifically deployment and management (scaling up and down, auto-recovery, release). Kubernetes provides a scalable and highly resilient deployment and management platform for microservices. A service mesh is based on transparent proxies that intercept traffic between services through sidecar proxies, and then manage the behavior of them through control plane configuration. A service mesh decouples traffic management from Kubernetes, eliminating the need for a kube-proxy component to support traffic within service mesh; and managing inter-service traffic, security and observability by providing an abstraction closer to the microservice application layer. xDS is one of the protocol standards for service mesh configuration. A service mesh is a higher-level abstraction of service in Kubernetes.  Summary If the object managed by Kubernetes is a pod, then the object managed in service mesh is a service, so it’s just a matter of using Kubernetes to manage microservices and then applying service mesh. If you don’t even want to manage a service, then use a serverless platform like Knative — but that’s an afterthought.\n","permalink":"https://jimmysong.io/en/blog/why-do-you-need-istio-when-you-already-have-kubernetes/","summary":"This article will take you through the inner workings of Kubernetes and Istio. In addition, I will introduce the load balancing approach in Kubernetes, and explain why you need Istio when you have Kubernetes.","title":"Why Do You Need Istio When You Already Have Kubernetes?"},{"content":"Tetrate Academy has recently released the Istio Fundamentals Course, which is now available for free. Sign up at Tetrate Academy now!\nCourse curriculum Here is the curriculum:\n Service Mesh and Istio Overview Installing Istio Observability: Telemetry and Logs Traffic Management Security Advanced Features Troubleshooting Real World Examples  There are self-assessment questions at the end of each course. I have passed the course, and here is the certificate after passing the course.\nTetrate Academy Istio Fundamentals Course  More In the future, Tetrate will release the Certified Istio Administrator (CIA) exam and welcome all Istio users and administrators to follow and register for it.\n","permalink":"https://jimmysong.io/en/notice/tetrate-istio-fundamental-courses/","summary":"Tetrate Academy has recently released the Istio Fundamentals Course, which is now available for free。","title":"Tetrate Academy Releases Free Istio Fundamentals Course"},{"content":"Different companies or software providers have devised countless ways to control user access to functions or resources, such as Discretionary Access Control (DAC), Mandatory Access Control (MAC), Role-Based Access Control (RBAC), and Attribute-Based Access Control (ABAC). In essence, whatever the type of access control model, three basic elements can be abstracted: user, system/application, and policy.\nIn this article, we will introduce ABAC, RBAC, and a new access control model — Next Generation Access Control (NGAC) — and compare the similarities and differences between the three, as well as why you should consider NGAC.\nWhat Is RBAC? RBAC, or Role-Based Access Control, takes an approach whereby users are granted (or denied) access to resources based on their role in the organization. Every role is assigned a collection of permissions and restrictions, which is great because you don’t need to keep track of every system user and their attributes. You just need to update appropriate roles, assign roles to users, or remove assignments. But this can be difficult to manage and scale. Enterprises that use the RBAC static role-based model have experienced role explosion: large companies may have tens of thousands of similar but distinct roles or users whose roles change over time, making it difficult to track roles or audit unneeded permissions. RBAC has fixed access rights, with no provision for ephemeral permissions or for considering attributes like location, time, or device. Enterprises using RBAC have had difficulty meeting the complex access control requirements to meet regulatory requirements of other organizational needs.\nRBAC Example Here’s an example Role in the “default” namespace in Kubernetes that can be used to grant read access to pods:\napiVersion:rbac.authorization.k8s.io/v1kind:Rolemetadata:namespace:defaultname:pod-readerrules:- apiGroups:[\"v1\"]resources:[\"pods\"]verbs:[\"get\",\"watch\",\"list\"]What Is ABAC? ABAC stands for Attribute-Based Access Control. At a high level, NIST defines ABAC as an access control method “where subject requests to perform operations on objects are granted or denied based on assigned attributes of the subject, environment conditions, and a set of policies that are specified in terms of those attributes and conditions.” ABAC is a fine-grained model since you can assign any attributes to the user, but at the same time it becomes a burden and hard to manage:\n When defining permissions, the relationship between users and objects cannot be visualized. If the rules are a little complex or confusingly designed, it will be troublesome for the administrator to maintain and trace.  This can cause performance problems when there is a large number of permissions to process.\nABAC Example Kubernetes initially uses ABAC as access control and is configured via JSON Lines, for example:\nAlice can just read pods in namespace “foo”:\n{\"apiVersion\": \"abac.authorization.kubernetes.io/v1beta1\", \"kind\": \"Policy\", \"spec\": {\"user\": \"alice\", \"namespace\": \"foo\", \"resource\": \"pods\", \"readonly\": true}} What Is NGAC? NGAC, or Next Generation Access Control, takes the approach of modeling access decision data as a graph. NGAC enables a systematic, policy-consistent approach to access control, granting or denying users administrative capabilities with a high level of granularity. NGAC was developed by NIST (National Institute of Standards and Technology) and is currently used in Tetrate Q and Tetrate Service Bridge .\nThere are several types of entities; they represent the resources you want to protect, the relationships between them, and the actors that interact with the system. The entities are:\n Users Objects User attributes, such as organization unit Object attributes, such as folders Policy classes, such as file system access, location, and time  NIST’s David Ferraiolo and Tetrate ‘s Ignasi Barrera shared how NGAC works at their presentation on Next Generation Access Control at Service Mesh Day 2019 in San Francisco.\nNGAC is based on the assumption that you can represent the system you want to protect in a graph that represents the resources you want to protect and your organizational structure, in a way that has meaning to you and that adheres to your organization semantics. On top of this model that is very particular to your organization, you can overlay policies. Between the resource model and the user model, the permissions are defined. This way NGAC provides an elegant way of representing the resources you want to protect, the different actors in the system, and how both worlds are tied together with permissions.\nNGAC DAG  Image via Linear Time Algorithms to Restrict Insider Access using Multi-Policy Access Control Systems NGAC Example The following example shows a simple NGAC graph with a User DAG representing an organization structure, an Object DAG representing files and folders in a filesystem, a categorization of the files, and two different policies — file system and scope — that can be combined to make access decisions. The association edges between the two DAGs define the permissions the actors have on the target resources.\nNGAC  In this graph we can see a representation of two files, “resume” and “contract” in the “/hr-docs” folder, each linked to a category (“public/confidential”). There are also two policy classes, “File System” and “Scope,” where the objects in the graph are attached — these need to be satisfied in order to get access to each file.\nUser Allice has read and write access to both files in the example, because a path links Allice to each of the files and the paths grant permissions on both policy classes. However, user Bob only has access to the “resume” file, because although there exists a path from Bob to the “contract” file that satisfies the “File System” policy class with “read” permissions, there is no path granting permissions on the “Scope” policy class. So, access to the “contract” file is denied to Bob.\nWhy Choose NGAC? The need to keep track of attributes of all objects creates a manageability burden in the case of ABAC. RBAC reduces the burden since we extract all access information to roles, but this paradigm suffers from role explosion problems and can also become unmanageable. With NGAC we have everything we need in graphs — in a compact, centralized fashion.\nWhen access decisions are complex, processing times of ABAC can rise exponentially. RBAC becomes especially hard to manage at scale, while NGAC scales linearly.\nWhere NGAC really shines is in flexibility. It can be configured to allow or disallow access based not only on object attributes, but also on other conditions — time, location, phase of the moon, and so on.\nOther key advantages of NGAC include the ability to set policies consistently (to meet compliance requirements) and the ability to set ephemeral policies. For example, NGAC could grant a developer one-time access to resources during an outage, without leaving unnecessary permissions in place that could later lead to a security breach. NGAC can evaluate and combine multiple policies in a single access decision, while keeping its linear time complexity.\nSummary The following table compares ABAC, RBAC, and NGAC in several aspects.\nNGAC vs RBAC vs ABAC  In conclusion:\n RBAC is simpler and has good performance, but can suffer at scale. ABAC is flexible, but performance and auditability are a problem. NGAC fixes those gaps by using a novel, elegant revolutionary approach: overlay access policies on top of an existing representation of the world, provided by the user. You can model RBAC and ABAC policies as well.  References  Guide to Attribute-Based Access Control (ABAC) Definition and Considerations  Deploying ABAC policies using RBAC Systems  RBAC vs. ABAC: What’s the Difference?  Role Explosion: The Unintended Consequence of RBAC  Exploring the Next Generation of Access Control Methodologies   ","permalink":"https://jimmysong.io/en/blog/why-you-should-choose-ngac-as-your-access-control-model/","summary":"This article will introduce you to the next generation permission control model, NGAC, and compare ABAC, RABC, and explain why you should choose NGAC.","title":"Why You Should Choose NGAC as Your Access Control Model"},{"content":"IstioCon 2021 poster（Jimmy Song）   Topic: Service Mesh in China Time: February 23rd, 10:00 - 10:10 am Beijing time How to participate: IstioCon 2021 website  Cost: Free  From February 22-25, Beijing time, the Istio community will be hosting the first IstioCon online, and registration is free to attend! I will be giving a lightning talk on Tuesday, February 23rd (the 12th day of the first month of the lunar calendar), as an evangelist and witness of Service Mesh technology in China, I will introduce the Service Mesh industry and community in China.\nI am a member of the inaugural IstioCon organizing committee with Zhonghu Xu (Huawei) and Shaojun Ding (Intel), as well as the organizer of the China region. Considering Istio’s large audience in China, we have arranged for Chinese presentations that are friendly to the Chinese time zone. There will be a total of 14 sharing sessions in Chinese, plus dozens more in English. The presentations will be in both lightning talk (10 minutes) and presentation (40 minutes) formats.\nJoin the Cloud Native Community Istio SIG to participate in the networking at this conference. For the schedule of IstioCon 2021, please visit IstioCon 2021 official website , or click for details.\n","permalink":"https://jimmysong.io/en/notice/istiocon-2021/","summary":"IstioCon 2021, I'll be giving a lightning talk, February 22nd at 10am BST.","title":"IstioCon 2021 Lightning Talk Preview"},{"content":"The ServiceMesher website has lost connection with the webhook program on the web publishing server because the GitHub where the code is hosted has has been “lost” and the hosting server is temporarily unable to log in, so the site cannot be updated. Today I spent a day migrating all the blogs on ServiceMesher to the Cloud Native Community website cloudnative.to , and as of today, there are 354 blogs on the Cloud Native Community.\nServiceMesher blogs  Now we plan to archive ServiceMesher official GitHub (all pages under the servicemesher.com domain) We are no longer accepting new PRs, so please submit them directly to the Cloud Native Community . Thank you all!\n","permalink":"https://jimmysong.io/en/notice/servicemesher-blog-merged/","summary":"ServiceMesher website is no longer maintained, plan to archive the website code, the blog has been migrated to Cloud Native Community, please submit the new blog to Cloud Native Community.","title":"ServiceMesher website is no longer maintained, the original blog has been migrated to the cloud native community"},{"content":"In this article, I’ll give you an overview of Istio ‘s history of virtual machine integration support. In particular, the introduction of the smart DNS proxy and WorkloadGroup in Istio 1.8, which makes virtual machines and containers equivalent at the resource abstraction level.\nI will show you a tumultuous odyssey of Istio’s virtual machine integration. Tetrate, the enterprise service mesh company that made pushing Istio to run everywhere part of its founding mission, has used VM features extensively in customer deployments and has been instrumental in pushing VMs to Istio upstream.\nPreface In my previous article , I talked about how Istio 1.7 supported virtual machines. But at that time, late October, virtual machines were still not seamlessly integrated into Istio — there was still a lot of manual work required. Now, Istio 1.8 has added WorkloadGroup and smart DNS proxy, which allows non-Kubernetes workloads like VMs to become first-class citizens in Istio — just like pods.\nWith or without a sidecar installed for virtual machines, until 1.7 you could not resolve the DNS name of a Kubernetes service unless a kube-external DNS was configured — which is the last piece of virtual machine integration in Istio. This shortcoming has finally been fixed in Istio 1.8.\nWhy Is Virtual Machine Support Important? In the process of migrating our applications to cloud native architectures and continuously containerizing them, we will go through three phases as shown in the figure below.\nCloud Native Stages   Stage 1: All applications are deployed on virtual machines Stage 2: Applications are deployed on both virtual machines and containers, are migrating from virtual machines to containers, and are using Kubernetes to manage containers. Stage 3: All applications are deployed in containers first, using Kubernetes to manage containers and Istio to manage service-to-service communication.  The above diagram is artificially simplified: in reality, there might be multiple hybrid clouds, multiple regions, multiple clusters, etc. Plus, at stage 3 containers and virtual machines may remain in long-term coexistence, but the trend of containerization remains unchanged.\nIstio’s History of Virtual Machine Support Istio’s support for virtual machines is a long process, an odyssey of sorts.\n0.2: Istio Mesh Expansion As of version 0.2, Istio added virtual machines to the Mesh via Istio Mesh Expansion , provided that the following prerequisites were met.\n Virtual machines must have direct access to the application’s pods via IP address, which requires a flat network between the container and the VM via VPC or VPN; and virtual machines do not need access to the Cluster IP, but rather direct access to the service’s endpoints. Virtual machines must have access to Istio’s control plane services (Pilot, Mixer, CA, now being integrated as Istiod), which can expose the control plane endpoints to virtual machines by deploying load balancers in the Istio Mesh. (optional) the virtual machine has access to the DNS server inside the Mesh (deployed in Kubernetes).  The steps to integrate a virtual machine are as follows.\n Create an internal load balancer for the Istio control plane service and the DNS service for the Kubernetes cluster. Generate a configuration file for the Istio Service CIDR, Service Account token, security certificate, and IP of the Istio Control Plane Service (the IP exposed through the Internal Load Balancer) and send it to the virtual machine. Setup the Istio component, dnsmaq (for DNS discovery), in the virtual machine; so that the virtual machine can access the services in the mesh using FQDN, to ensure that the virtual machine can correctly resolve the Cluster IP of the services in the mesh. To run the service in a virtual machine, you need to configure the sidecar, add inbound ports to be intercepted, then restart Istio and also run istioctl to register the service.  The following figure shows the detailed flow from integrating a virtual machine to accessing services in the virtual machine in a mesh.\nFigure 1  Figure 1\n The DNS is hijacked by dnsmasq deployed in the virtual machine, which allows it to correctly obtain the Cluster IP of the Istio service (Kubernetes’ built-in DNS). Access to Kubernetes’ built-in DNS service (which is exposed outside the cluster via the Internal Load Balancer and can be accessed directly). Return the Cluster IP resolved by productpage.bookinfo.svc.cluster.local, noting that the IP address is not directly accessible, but failure to be DNS resolved will result in a failed VM request for the service. The virtual machine’s call to services in a mesh is hijacked by the sidecar proxy. Since the proxy is connected to the Istio control plane, the endpoints of the service can be queried via xDS, so traffic will be forwarded to one of the endpoints. To access VM services in mesh, you need to manually add VM services to mesh using the istioctl register command, which essentially registers the VM services to the service and endpoint in Kubernetes. Services in the mesh can be accessed using the VM-registered service name (FQDN, e.g. mysql.vm.svc.cluster.local).  The above Istio support for virtual machines continued with Istio 1.0, which introduced a new API ServiceEntry with Istio 1.1, that allows additional entries to be added to Istio’s internal service registry so that services in the mesh can access/route to these manually specified services. The istioctl register command is no longer needed and will be deprecated in Istio 1.9.\nThe istioctl experimental add-to-mesh command has been added to Istio 1.5 to add services from a virtual machine to a mesh, and it works just like the istioctl register.\n1.6 to 1.7: New Resource Abstractions Istio introduced a new resource type, WorkloadEntry , in traffic management from version 1.6 , to abstract virtual machines so that they can be added to the mesh as equivalent loads to the pods in Kubernetes; with traffic management, security management, observability, etc. The mesh configuration process for virtual machines is simplified with WorkloadEntry, which selects multiple workload entries and Kubernetes pods based on the label selector specified in the service entry.\nIstio 1.8 adds a resource object for WorkloadGroup that provides a specification that can include both virtual machines and Kubernetes workloads, designed to mimic the existing sidecar injection and deployment specification model for Kubernetes workloads to bootstrap Istio agents on the VMs.\nBelow is a comparison of resource abstraction levels for virtual machines versus workloads in Kubernetes.\n   Item Kubernetes Virtual Machine     Basic schedule unit Pod WorkloadEntry   Component Deployment WorkloadGroup   Service register and discovery Service ServiceEntry    From the above diagram, we can see that for virtual machine workloads there is a one-to-one correspondence with the workloads in Kubernetes.\nEverything seems perfect at this point. However, exposing the DNS server in the Kubernetes cluster directly is a big security risk , so we usually manually write the domain name and Cluster IP pair of the service the virtual machine needs to access to the local /etc/hosts — but this is not practical for a distributed cluster with a large number of nodes.\nThe process of accessing the services inside mesh by configuring the local /etc/hosts of the virtual machine is shown in the following figure.\nFigure 2  Figure 2\n Registration of services in the virtual machine into the mesh. Manually write the domain name and Cluster IP pairs of the service to be accessed to the local /etc/hosts file in the virtual machine. Cluster IP where the virtual machine gets access to the service. The traffic is intercepted by the sidecar proxy and the endpoint address of the service to be accessed is resolved by Envoy. Access to designated endpoints of the service.  In Kubernetes, we generally use the Service object for service registration and discovery; each service has a separate DNS name that allows applications to call each other by using the service name. We can use ServiceEntry to register a service in a virtual machine into Istio’s service registry, but a virtual machine cannot access a DNS server in a Kubernetes cluster to get the Cluster IP if the DNS server is not exposed externally to the mesh, which causes the virtual machine to fail to access the services in the mesh. Wouldn’t the problem be solved if we could add a sidecar to the virtual machine that would transparently intercept DNS requests and get the Cluster IP of all services in the mesh, similar to the role of dnsmasq in Figure 1?\nAs of Istio 1.8 — Smart DNS Proxy With the introduction of smart DNS proxy in Istio 1.8, virtual machines can access services within the mesh without the need to configure /etc/hosts, as shown in the following figure.\nFigure 3  Figure 3\nThe Istio agent on the sidecar will come with a cached DNS proxy dynamically programmed by Istiod. DNS queries from the application are transparently intercepted and served by the Istio proxy in the pod or VM, with the response to DNS query requests, enabling seamless access from the virtual machine to the service mesh.\nThe WorkloadGroup and smart DNS proxy introduced in Istio 1.8 provide powerful support for virtual machine workloads, making legacy applications deployed in virtual machines fully equivalent to pods in Kubernetes.\nSummary In this odyssey of Istio’s virtual machine support, we can see the gradual realization of unified management of virtual machines and pods — starting with exposing the DNS server in the mesh and setting up dnsmasq in the virtual machine, and ending with using smart DNS proxies and abstracting resources such as WorkloadEntry, WorkloadGroup and ServiceEntry. This article only focuses on the single cluster situation, which is not enough to be used in real production. We also need to deal with security, multicluster, multitenancy, etc.\nReferenced resources  Tetrate Service Bridge — Across all compute bridging Kubernetes clusters, VMs, and bare metal  Expanding into New Frontiers — Smart DNS Proxying in Istio  Virtual Machine Installation — Istio documentation  How to Integrate Virtual Machines into Istio Service Mesh   ","permalink":"https://jimmysong.io/en/blog/istio-18-a-virtual-machine-integration-odyssey/","summary":"In this article, I’ll give you an overview of Istio‘s history of virtual machine integration support. In particular, the introduction of the smart DNS proxy and WorkloadGroup in Istio 1.8, which makes virtual machines and containers equivalent at the resource abstraction level.","title":"Istio 1.8: A Virtual Machine Integration Odyssey"},{"content":"A service mesh is a relatively simple concept, consisting of a bunch of network proxies paired with each service in an application, plus a set of task management processes. The proxies are called the data plane and the management processes are called the control plane in the Service Mesh. The data plane intercepts calls between different services and “processes” them; the control plane is the brain of the mesh that coordinates the behavior of proxies and provides APIs for operations and maintenance personnel to manipulate and observe the entire network.\nThe diagram below shows the architecture of a service mesh.\nService Mesh Architecture  Further, the service mesh is a dedicated infrastructure layer designed to enable reliable, fast, and secure inter-service invocation in microservices architectures. It is not a mesh of “services” but rather a mesh of “proxies” that services can plug into, thus abstracting the network from the application code. In a typical service mesh, these proxies are injected into each service deployment as a sidecar (and also may be deployed at the edge of the mesh). Instead of invoking services directly over the network, services invoke their local sidecar proxy, which in turn manages requests on behalf of the service, pushing the complexities of inter-service communications into a networking layer that can resolve them at scale. The set of interconnected sidecar proxies implements a so-called data plane, while on the other hand the service mesh control plane is used to configure proxies. The infrastructure introduced by a service mesh provides an opportunity, too, to collect metrics about the traffic that is flowing through the application.\nThe architecture of a service mesh The infrastructure layer of a service mesh is divided into two main parts: the control plane and the data plane.\nCharacteristics of the control plane\n Do not parse packets directly. Communicates with proxies in the control plane to issue policies and configurations. Visualizes network behavior. Typically provides APIs or command-line tools for configuration versioning and management for continuous integration and deployment.  Characteristics of the data plane\n Is usually designed with the goal of statelessness (though in practice some data needs to be cached to improve traffic forwarding performance). Directly handles inbound and outbound packets, forwarding, routing, health checking, load balancing, authentication, authentication, generating monitoring data, etc. Is transparent to the application, i.e., can be deployed senselessly.  Changes brought by the service mesh Decoupling of microservice governance from business logic\nA service mesh takes most of the capabilities in the SDK out of the application, disassembles them into separate processes, and deploys them in a sidecar model. By separating service communication and related control functions from the business process and synching them to the infrastructure layer, a service mesh mostly decouples them from the business logic, allowing application developers to focus more on the business itself.\nNote that the word “mostly” is mentioned here and that the SDK often needs to retain protocol coding and decoding logic, or even a lightweight SDK to implement fine-grained governance and monitoring policies in some scenarios. For example, to implement method-level call distributed tracing, the service mesh requires the business application to implement trace ID passing, and this part of the implementation logic can also be implemented through a lightweight SDK. Therefore, the service mesh is not zero-intrusive from a code level.\nUnified governance of heterogeneous environments\nWith the development of new technologies and staff turnover, there are often applications and services in different languages and frameworks in the same company, and in order to control these services uniformly, the previous practice was to develop a complete set of SDKs for each language and framework, which is very costly to maintain. With a service mesh, multilingual support is much easier by synching the main service governance capabilities to the infrastructure. By providing a very lightweight SDK, and in many cases, not even a separate SDK, it is easy to achieve unified traffic control and monitoring requirements for multiple languages and protocols.\nFeatures of service mesh Service mesh also has three major technical advantages over traditional microservice frameworks.\nObservability\nBecause the service mesh is a dedicated infrastructure layer through which all inter-service communication passes, it is uniquely positioned in the technology stack to provide uniform telemetry at the service invocation level. This means that all services are monitored as “black boxes.” The service mesh captures route data such as source, destination, protocol, URL, status codes, latency, duration, etc. This is essentially the same data that web server logs can provide, but the service mesh captures this data for all services, not just the web layer of individual services. It is important to note that collecting data is only part of the solution to the observability problem in microservice applications. Storing and analyzing this data needs to be complemented by mechanisms for additional capabilities, which then act as alerts or automatic instance scaling, for example.\nTraffic control\nWith a service mesh, services can be provided with various control capabilities such as intelligent routing (blue-green deployment, canary release, A/B test), timeout retries, circuit breaking, fault injection, traffic mirroring, etc. These are often features that are not available in traditional microservices frameworks but are critical to the system. For example, the service mesh carries the communication traffic between microservices, so it is possible to test the robustness of the whole application by simulating the failure of some microservices through rules for fault injection in the grid. Since the service mesh is designed to efficiently connect source request calls to their optimal destination service instances, these traffic control features are “destination-oriented.” This is a key feature of the service mesh’s traffic control capabilities.\nSecurity\nTo some extent, monolithic applications are protected by their single address space. However, once a monolithic application is broken down into multiple microservices, the network becomes a significant attack surface. More services mean more network traffic, which means more opportunities for hackers to attack the information flow. And service mesh provides the capabilities and infrastructure to protect network calls. The security-related benefits of service mesh are in three core areas: authentication of services, encryption of inter-service communications, and enforcement of security-related policies.\nService mesh has brought about tremendous change and has strong technical advantages, and has been called the second generation of “microservice architecture.” However, there is no silver bullet in software development. Traditional microservices architecture has many pain points, and service mesh is no exception. It has its limitations.\nIncreased complexity\nService mesh introduces sidecar proxies and other components into an already complex, distributed environment, which can greatly increase the overall chain and operational O\u0026M complexity. Ops needs to be more specialized. Adding a service mesh such as Istio to a container orchestrator such as Kubernetes often requires Ops to become an expert in both technologies in order to fully utilize the capabilities of both and to troubleshoot the problems encountered in the environment.\nLatency\nAt the link level, a service mesh is an invasive, complex technology that can add significant latency to system calls. This latency is on the millisecond level, but it can also be intolerable in special business scenarios.\nPlatform adaptation\nThe intrusive nature of service mesh forces developers and operators to adapt to highly autonomous platforms and adhere to the platform’s rules.\nThe relationship between service mesh and Kubernetes Kubernetes is essentially application lifecycle management, specifically the deployment and management (scaling, auto-recovery, publishing) of containerized applications. Service mesh decouples traffic management from Kubernetes, eliminating the need for a kube-proxy component for internal traffic, and manages inter-service and ingress traffic, security, and observability through an abstraction closer to the microservice application layer. The xDS used by Istio and Envoy is one of the protocol standards for service mesh configuration.\nOrganizations that use Kubernetes often turn to a service mesh to address the networking issues that arise with containerization — but notably, a service mesh can work with a legacy or a modern workload, and can be put in place prior to containerization for a faster, safer path to modernization.\nSummary Readers should look dialectically at the advantages and disadvantages of a service mesh compared with traditional microservices architecture. A service mesh can be a critical part of the evolutionary path of application architecture, from the earliest monolith to distributed, to microservices, containerization, container orchestration, to hybrid workloads and multi-cloud.\nLooking ahead, Kubernetes is exploding, and it has become the container orchestration of choice for enterprise greenfield applications. If Kubernetes has completely won the market and the size and complexity of Kubernetes-based applications continue to grow, there will be a tipping point, and service mesh will be necessary to effectively manage these applications. As service mesh technology continues to evolve and the architecture and functionality of its implementation products, such as Istio, continue to be optimized, service mesh will completely replace traditional microservice architectures as the architecture of choice for microservices and transformation to the cloud for enterprises.\nThis article was co-authored by Guangming Luo, a member of the ServiceMesher community and the CNC steering community.\n","permalink":"https://jimmysong.io/en/blog/what-is-a-service-mesh/","summary":"This article will take you through what a service mesh is, as well as its architecture, features, and advantages and disadvantages.","title":"What is a service mesh?"},{"content":"1.8 is the last version of Istio to be released in 2020 and it has the following major updates:\n Supports installation and upgrades using Helm 3. Mixer was officially removed. Added Istio DNS proxy to transparently intercept DNS queries from applications. WorkloadGroup has been added to simplify the integration of virtual machines.  WorkloadGroup is a new API object. It is intended to be used with non-Kubernetes workloads like Virtual Machines and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies.\nInstallation and Upgrades Istio starts to officially support the use of Helm v3 for installations and upgrades. In previous versions, the installation was done with the istioctl command-line tool or Operator. With version 1.8, Istio supports in-place and canary upgrades with Helm.\nEnhancing Istio’s Usability The istioctl command-line tool has a new bug reporting feature (istioctl bug-report ), which can be used to collect debugging information and get cluster status.\nThe way to install the add-on has changed: 1.7 istioctl is no longer recommended and has been removed in 1.8, to help solve the problem of add-on lagging upstream and to make it easier to maintain.\nTetrate is an enterprise service mesh company. Our flagship product, TSB, enables customers to bridge their workloads across bare metal, VMs, K8s, \u0026 cloud at the application layer and provide a resilient, feature-rich service mesh fabric powered by Istio, Envoy, and Apache SkyWalking.\nMixer, the Istio component that had been responsible for policy controls and telemetry collection, has been removed. Its functionalities are now being served by the Envoy proxies. For extensibility, service mesh experts recommend using WebAssembly (Wasm) to extend Envoy; and you can also try the GetEnvoy Toolkit , which makes it easier for developers to create Wasm extensions for Envoy. If you still want to use Mixer, you must use version 1.7 or older. Mixer continued receiving bug fixes and security fixes until Istio 1.7. Many features supported by Mixer have alternatives as specified in the Mixer Deprecation document, including the in-proxy extensions based on the Wasm sandbox API.\nSupport for Virtual Machines Istio’s recent upgrades have steadily focused on making virtual machines first-class citizens in the mesh. Istio 1.7 made progress to support virtual machines and Istio 1.8 adds a smart DNS proxy , which is an Istio sidecar agent written in Go. The Istio agent on the sidecar will come with a cache that is dynamically programmed by Istiod DNS Proxy. DNS queries from applications are transparently intercepted and served by an Istio proxy in a pod or VM that intelligently responds to DNS query requests, enabling seamless multicluster access from virtual machines to the service mesh.\nIstio 1.8 adds a WorkloadGroup , which describes a collection of workload instances. It provides a specification that the workload instances can use to bootstrap their proxies, including the metadata and identity. It is only intended to be used with non-k8s workloads like Virtual Machines, and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies. Using WorkloadGroups, Istio has started to help automate VM registration with istioctl experimental workload group .\nTetrate , the enterprise service mesh company, uses these VM features extensively in customers’ multicluster deployments, to enable sidecars to resolve DNS for hosts exposed at ingress gateways of all the clusters in a mesh; and to access them over mutual TLS.\nConclusion All in all, the Istio team has kept the promise made at the beginning of the year to maintain a regular release cadence of one release every three months since the 1.1 release in 2018, with continuous optimizations in performance and user experience for a seamless experience of brownfield and greenfield apps on Istio. We look forward to more progress from Istio in 2021.\n","permalink":"https://jimmysong.io/en/blog/istio-1-8-a-smart-dns-proxy-takes-support-for-virtual-machines-a-step-further/","summary":"WorkloadGroup is a new API object. It is intended to be used with non-Kubernetes workloads like Virtual Machines and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies.","title":"Istio 1.8: A Smart DNS Proxy Takes Support for Virtual Machines a Step Further"},{"content":"Istio is a popular service mesh to connect, secure, control, and observe services. When it was first introduced as open source in 2017, Kubernetes was winning the container orchestration battle and Istio answered the needs of organizations moving to microservices. Although Istio claims to support heterogeneous environments such as Nomad, Consul, Eureka, Cloud Foundry, Mesos, etc., in reality, it has always worked best with Kubernetes — on which its service discovery is based.\nIstio was criticized for a number of issues early in its development, for the large number of components, the complexity of installation and maintenance, the difficulty of debugging, a steep learning curve due to the introduction of too many new concepts and objects (up to 50 CRDs), and the impact of Mixer components on performance. But these issues are gradually being overcome by the Istio team. As you can see from the roadmap released in early 2020, Istio has come a long way.\nBetter integration of VM-based workloads into the mesh is a major focus for the Istio team this year. Tetrate also offers seamless multicloud connectivity, security, and observability, including for VMs, via its product Tetrate Service Bridge . This article will take you through why Istio needs to integrate with virtual machines and how you can do so.\nWhy Should Istio Support Virtual Machines? Although containers and Kubernetes are now widely used, there are still many services deployed on virtual machines and APIs outside of the Kubernetes cluster that needs to be managed by Istio mesh. It’s a huge challenge to unify the management of the brownfield environment with the greenfield.\nWhat Is Needed to Add VMs to the Mesh? Before the “how,” I’ll describe what is needed to add virtual machines to the mesh. There are a couple of things that Istio must know when supporting virtual machine traffic: which VMs have services that should be part of the mesh, and how to reach the VMs. Each VM also needs an identity, in order to communicate securely with the rest of the mesh. These requirements could work with Kubernetes CRDs, as well as a full-blown Service Registry like Consul. And the service account based identity bootstrapping could work as a mechanism for assigning workload identities to VMs that do not have a platform identity. For VMs that do have a platform identity (like EC2, GCP, Azure, etc.), work is underway in Istio to exchange the platform identity with a Kubernetes identity for ease of setting up mTLS communication.\nHow Does Istio Support Virtual Machines? Istio’s support for virtual machines starts with its service registry mechanism. The information about services and instances in the Istio mesh comes from Istio’s service registries, which up to this point have only looked at or tracked pods. In newer versions, Istio now has resource types to track and watch VMs. The sidecars inside the mesh cannot observe and control traffic to services outside the mesh, because they do not have any information about them.\nThe Istio community and Tetrate have done a lot of work on Istio’s support for virtual machines. The 1.6 release included the addition of WorkloadEntry, which allows you to describe a VM exactly as you would a host running in Kubernetes. In 1.7, the release started to add the foundations for bootstrapping VMs into the mesh automatically through tokens, with Istio doing the heavy lifting. Istio 1.8 will debut another abstraction called WorkloadGroup, which is similar to a Kubernetes Deployment object — but for VMs.\nThe following diagram shows how Istio models services in the mesh. The predominant source of information comes from a platform service registry like Kubernetes, or a system like Consul. In addition, the ServiceEntry serves as a user-defined service registry, modeling services on VMs or external services outside the organization.\n  Why install Istio in a virtual machine when you can just use ServiceEntry to bring in the services in the VMs?\nUsing ServiceEntry, you can enable services inside the mesh to discover and access external services; and in addition, manage the traffic to those external services. In conjunction with VirtualService, you can also configure access rules for the corresponding external service — such as request timeouts, fault injection, etc. — to enable controlled access to the specified external service.\nEven so, it only controls the traffic on the client-side, not access to the introduced external service to other services. That is, it cannot control the behavior of the service as the call initiator. Deploying sidecars in a virtual machine and introducing the virtual machine workload via workload selector allows the virtual machine to be managed indiscriminately, like a pod in Kubernetes.\nFuture As you can see from the bookinfo demo , there is too much manual work involved in the process and it’s easy to go wrong. In the future, Istio will improve VM testing to be realistic, automate bootstrapping based on platform identity, improve DNS support and istioctl debugging, and more. You can follow the Istio Environment Working Group for more details about virtual machine support.\nReferences  Virtual Machine Installation  Virtual Machines in Single-Network Meshes  Istio: Bringing VMs into the Mesh (with Cynthia Coan)  Bridging Traditional and Modern Workloads   ","permalink":"https://jimmysong.io/en/blog/how-to-integrate-virtual-machines-into-istio-service-mesh/","summary":"Better integration of virtual machine-based workloads into the service mesh is a major focus for the Istio team this year, and Tetrate also provides seamless multi-cloud connectivity, security and observability, including for virtual machines, through its product Tetrate Service Bridge. This article will show you why Istio needs to integrate with virtual machines and how.","title":"How to Integrate Virtual Machines into Istio Service Mesh"},{"content":"Today is my 914th day and also the last day with Ant Group , tomorrow is September 1st, which is usually the day school starts, and everyone at Alibaba is known as “classmate”, tomorrow I will join Tetrate , and that’s kind of starting my new semester!\nAnt/Alibaba and the Cloud Native Community To date, Ant/Alibaba Group has had a profound impact on my career, especially its corporate culture and values, and the Alibaba recruiting philosophy of “finding like-minded people”, and isn’t the process of creating the Cloud Native Community also a process of finding like-minded people? Cloud Native Community is like a small society, I don’t want it to have much social value, but only want it to make a small but beautiful change to individuals, to enterprises and to society. I constantly think about myself as an individual and as an employee, especially as an initiator of the community. What is my mission as an individual, an employee, and especially as an initiator of a community? What role should I play in the company? Where is this community going? I’m fumbling along, but because of your support, it makes me stronger and more committed to the adoption and application of cloud native technology in China, outside of me I may have gone faster, but now with the community together we will go further!\n24 June 2019, Shanghai, KubeCon China 2019  June 24, 2019, Shanghai, KubeCon China 2019\nJoining Tetrate Over the past two years, I’ve been working hard to promote Istio and Service Mesh technology, and with funding from Ant Group, I started the ServiceMesher Community to bring Service Mesh technology to China. Next I want to bring Chinese practice to the world.\nAs a Developer Advocate, the most important thing is not to stop learning, but to listen and take stock. Over the past two years, I’ve seen a lot of people show interest in Service Mesh, but not enough to understand the risks and lack of knowledge about the new technology. I’m excited to join this Service Mesh-focused startup Tetrate , a global telecommuting startup with products built around open source Istio , [Envoy](https:/ /envoyproxy.io) and Apache SkyWalking , it aims to make it to be the cloud native network infrastructure. Here are several maintainers of these open source projects, such as Sheng Wu , Zack Butcher , Lizan Zhou , etc., and I believe that working with them can help you understand and apply Service Mesh quickly and effectively across cloud native.\nMore Earlier this year as I was preparing for the Cloud Native community, I set the course for the next three years - cloud native, open source and community. The road to pursue my dream is full of thorns, not only need courage and perseverance, but also need you to be my strong backing, I will overcome the thorns and move forward. Open source belongs to the world, to let the world understand us better, we must be more active into the world. I hope that China’s open source tomorrow will be better, I hope that Service Mesh technology will be better applied by the enterprises in China, I hope that cloud native can benefit the public, and I hope that we can all find our own mission.\nWe are hiring now, if you are interested with Tetrate , please send your resume to careers@tetrate.io .\n","permalink":"https://jimmysong.io/en/blog/moving-on-from-ant-group/","summary":"Today is my last day at Ant and tomorrow I'm starting a new career at Tetrate.","title":"New Beginning - Goodbye Ant, Hello Tetrate"},{"content":"Just tonight, the jimmysong.io website was moved to the Alibaba Cloud Hong Kong node. This is to further optimize the user experience and increase access speed. I purchased an ECS on the Alibaba Cloud Hong Kong node, and now I have a public IP and can set subdomains. The website was previously deployed on GitHub Pages, the access speed is average, and it has to withstand GitHub instability. Impact (In recent years, GitHub downtime has occurred).\nMeanwhile, the blog has also done a lot to improve the site, thanks to Bai Jun away @baijunyao strong support, a lot of work for the revision of the site, including:\n Changed the theme color scheme and deepened the contrast Use aligolia to support full site search Optimized mobile display Articles in the blog have added zoom function Added table of contents to blog post  This site is built on the theme of educenter .\nThanks to the majority of netizens who have supported this website for several years. The website has been in use for more than three years and has millions of visits. It has undergone two major revisions before and after, on January 31, 2020 and October 8, 2017, respectively. And changed the theme of the website. In the future, I will share more cloud-native content with you as always, welcome to collect, forward, and join the cloud-native community to communicate with the majority of cloud-native developers.\n","permalink":"https://jimmysong.io/en/notice/migrating-to-alibaba-cloud/","summary":"Move the website to the Alibaba Cloud Hong Kong node to increase the speed of website access and the convenience of obtaining public IP and subdomain names.","title":"Move to Alibaba Cloud Hong Kong node"},{"content":"  Just the other day, Java just celebrated its 25th birthday , and from the time of its birth it was called “write once, run everywhere”, but more than 20 years later, there is still a deep gap between programming and actual production delivery. the world of IT is never short of concepts, and if a concept doesn’t solve the problem, then it’s time for another layer of concepts. it’s been 6 years since Kubernetes was born, and it’s time for the post-Kubernetes era - the era of cloud-native applications!\nCloud Native Stage  This white paper will take you on a journey to explore the development path of cloud-native applications in the post-Kubernetes era.\nHighlights of the ideas conveyed include.\n Cloud-native has passed through a savage growth period and is moving towards uniform application of standards. Kubernetes’ native language does not fully describe the cloud-native application architecture, and the development and operation functions are heavily coupled in the configuration of resources. Operator’s expansion of the Kubernetes ecosystem has led to the fragmentation of cloud-native applications, and there is an urgent need for a unified application definition standard. The essence of OAM is to separate the R\u0026D and O\u0026M concerns in the definition of cloud-native applications, and to further abstract resource objects, simplify and encompass everything. “Kubernetes Next Generation” refers to the fact that after Kubernetes became the infrastructure layer standard, the focus of the cloud-native ecology is being overtaken by the application layer, and the last two years have been a powerful exploration of the hot Service Mesh process, and the era of cloud-native application architecture based on Kubernetes is coming.  Kubernetes has become an established operating platform for cloud-native applications, and this white paper will expand with Kubernetes as the default platform, including an explanation of the OAM-based hierarchical model for cloud-native applications.\n","permalink":"https://jimmysong.io/en/notice/guide-to-cloud-native-app/","summary":"Take you on a journey through the post-Kubernetes era of cloud-native applications.","title":"Guide to Cloud Native Application"},{"content":"At the beginning of 2020, due to the outbreak of the Crona-19 pandemic, employees around the world began to work at home. Though the distance between people grew farer, there was a group of people, who were us working in the cloud native area, gathered together for a common vision. During the past three months, we have set up the community management committee and used our spare time working together to complete the preparatory work for the community. Today we are here to announce the establishment of the Cloud Native Community.\nBackground Software is eating the world. —— Marc Andreessen\nThis sentence has been quoted countless times, and with the rise of Cloud Native, we’d like to talk about “Cloud Native is eating the software.” As more and more enterprises migrate their services to the cloud, the original development mode of enterprises cannot adapt to the application scenarios in the cloud, and it is being reshaped to conform to the cloud native standard.\nSo what is cloud native? Cloud native is a collection of best practices in architecture, r\u0026d process and team culture to support faster innovation, superior user experience, stable and reliable user service and efficient r\u0026d. The relationship between the open source community and the cloud native is inseparable. It is the existence of the open source community, especially the end user community, that greatly promotes the continuous evolution of cloud native technologies represented by container, service mesh and microservices.\nCNCF (Cloud Native Computing Foundation) holds Cloud Native conference every year in the international community, which has a wide audience and great influence. But it was not held in China for the first time until 2018, after several successful international events. However, there are no independent foundations or neutral open source communities in China. In recent years, many cloud native enthusiasts in China have set up many communication groups and held many meetups, which are very popular. Many excellent open source projects have emerged in the cloud native field, but there is no organized neutral community for overall management. Under this background, the Cloud Native Community emerges at the right moment.\nAbout Cloud Native Community is an open source community with technology, temperature and passion. It was founded spontaneously by a group of industry elites who love open source and uphold the principle of consensus, co-governance, co-construction and sharing. The aim of the community is: connection, neutral, open source. We are based in China, facing the world, enterprise neutrality, focusing on open source, and giving feedback to open source.\nIntroduction for the Steering Community：https://cloudnative.to/en/team/ .\nYou will gain the followings after joining the community:\n Knowledge and news closer to the source A more valuable network More professional and characteristic consultation Opportunities to get closer to opinion leaders Faster and more efficient personal growth More knowledge sharing and exposure opportunities More industry talent to be found  Contact Contact with us.\n Email: mailto:contact@cloudnative.to  Twitter: https://twitter.com/CloudNativeTo   ","permalink":"https://jimmysong.io/en/notice/cloud-native-community-announecement/","summary":"Today the Community Steering Committee announced the official formation of the Cloud Native Community.","title":"Establishment of the Cloud Native Community"},{"content":"This article is a rework of previously written content and is included in the Istio Handbook of the ServiceMesher community . Other chapters are still being compiled.\nPeople who have just heard of Service Mesh and tried Istio may have the following questions:\n Why does Istio bind Kubernetes? What roles do Kubernetes and Service Mesh play in cloud native? What aspects of Kubernetes has Istio extended? What problems have been solved? What is the relationship between Kubernetes, xDS protocols (Envoy , MOSN, etc) and Istio? Should I use Service Mesh?  In this section, we will try to guide you through the internal connections between Kubernetes, the xDS protocol, and Istio Service Mesh. In addition, this section will also introduce the load balancing methods in Kubernetes, the significance of the xDS protocol for Service Mesh, and why Istio is needed in time for Kubernetes.\nUsing Service Mesh is not to say that it will break with Kubernetes, but that it will happen naturally. The essence of Kubernetes is to perform application lifecycle management through declarative configuration, while the essence of Service Mesh is to provide traffic and security management and observability between applications. If you have built a stable microservice platform using Kubernetes, how do you set up load balancing and flow control for calls between services?\nThe xDS protocol created by Envoy is supported by many open source software, such as Istio , Linkerd , MOSN, etc. Envoy’s biggest contribution to Service Mesh or cloud native is the definition of xDS. Envoy is essentially a proxy. It is a modern version of proxy that can be configured through APIs. Based on it, many different usage scenarios are derived, such as API Gateway, Service Mesh. Sidecar proxy and Edge proxy in.\nThis section contains the following\n Explain the role of kube-proxy. Kubernetes’ limitations in microservice management. Describe the features of Istio Service Mesh. Describe what xDS includes. Compare some concepts in Kubernetes, Envoy and Istio Service Mesh.  Key takeaways If you want to know everything in advance, here are some of the key points from this article:\n The essence of Kubernetes is application lifecycle management, specifically deployment and management (scaling, scaling, automatic recovery, release). Kubernetes provides a scalable and highly resilient deployment and management platform for microservices. The foundation of Service Mesh is a transparent proxy. After the traffic between microservices is intercepted through sidecar proxy, the behavior of microservices is managed through the control plane configuration. Service Mesh decoupled from Kubernetes traffic management, the internal flow without the need of Service Mesh kube-proxy supporting components, micro-services closer to abstract the application layer by, for traffic between management services, security and observability. xDS defines the protocol standards for Service Mesh configuration. Service Mesh is a higher-level abstraction of services in Kubernetes. Its next step is serverless.  Kubernetes vs Service Mesh The following figure shows the service access relationship between Kubernetes and Service Mesh (one sidecar per pod mode).\nkubernetes vs service mesh  Traffic forwarding\nEach node of the cluster Kubernetes a deployed kube-proxy assembly Kubernetes API Server may communicate with the cluster acquired service information, and then set iptables rules, sends a request for a service directly to the corresponding Endpoint (belonging to the same group service pod).\nService discovery\nService registration in Service Mesh  Istio Service Mesh can use the service in Kubernetes for service registration. It can also connect to other service discovery systems through the platform adapter of the control plane, and then generate the configuration of the data plane (using CRD statements, stored in etcd), a transparent proxy for the data plane. (Transparent proxy) is deployed in the sidecar container in each application service pod. These proxy need to request the control plane to synchronize the proxy configuration. The reason why is a transparent proxy, because there is no application container fully aware agent, the process kube-proxy components like the need to block traffic, but kube-proxythat blocks traffic to Kubernetes node and sidecar proxy that blocks out of the Pod For more information, see Understanding Route Forwarding by the Envoy Sidecar Proxy in Istio Service Mesh .\nDisadvantages of Service Mesh\nBecause each node on Kubernetes many runs Pod, the original kube-proxyrouting forwarding placed in each pod, the distribution will lead to a lot of configuration, synchronization, and eventual consistency problems. In order to perform fine-grained traffic management, a series of new abstractions will be added, which will further increase the user’s learning costs. However, with the popularization of technology, this situation will gradually ease.\nAdvantages of Service Mesh\nkube-proxy The settings are globally effective, and fine-grained control of each service cannot be performed. Service Mesh uses sidecar proxy to extract the control of traffic in Kubernetes from the service layer, which can be further expanded.\nkube-proxy component In Kubernetes cluster, each Node to run a kube-proxy  process. kube-proxy Responsible for the Service realization of a VIP (virtual IP) form. In Kubernetes v1.0, the proxy is implemented entirely in userspace. Kubernetes v1.1 adds the iptables proxy mode , but it is not the default operating mode. As of Kubernetes v1.2, the iptables proxy is used by default. In Kubernetes v1.8.0-beta.0, the ipvs proxy mode was added . More about kube-proxy component description please refer kubernetes Description: service and kube-proxy principle and use IPVS achieve Kubernetes inlet flow load balancing .\nkube-proxy flaws The disadvantages of kube-proxy :\n First, if forwarded pod can not provide normal service, it does not automatically try another pod, of course, this can liveness probes be solved. Each pod has a health check mechanism. When there is a problem with the health of the pod, kube-proxy will delete the corresponding forwarding rule. In addition, nodePorttypes of services cannot add TLS or more sophisticated message routing mechanisms.\n Kube-proxy implements load balancing of traffic among multiple pod instances of the Kubernetes service, but how to fine-grained control the traffic between these services, such as dividing the traffic into different application versions by percentage (these applications belong to the same service , But on a different deployment), do canary release and blue-green release? Kubernetes community gives the method using the Deployment do canary release , essentially by modifying the pod of the method label different pod to be classified into the Deployment of Service.\nKubernetes Ingress vs. Istio Gateway Speaking above kube-proxythe flow inside the only route Kubernetes clusters, and we know that Pod Kubernetes cluster located CNI outside the network created, external cluster is unable to communicate directly with, so Kubernetes created in the ingress of this resource object, which is located by the Kubernetes edge nodes (such nodes can be many or a group) are driven by the Ingress controller, which is responsible for managing north-south traffic . Ingress must be connected to various ingress controllers, such as nginx ingress controller and traefik . Ingress is only applicable to HTTP traffic, and its usage is also very simple. It can only route traffic by matching limited fields such as service, port, and HTTP path, which makes it unable to route TCP traffic such as MySQL, Redis, and various private RPCs. To directly route north-south traffic, you can only use Service’s LoadBalancer or NodePort. The former requires cloud vendor support, while the latter requires additional port management. Some Ingress controllers support exposing TCP and UDP services, but they can only be exposed using Services. Ingress itself does not support it, such as the nginx ingress controller . The exposed port of the service is configured by creating a ConfigMap.\nIstio Gateway is similar to Kubernetes Ingress in that it is responsible for north-south traffic to the cluster. GatewayThe load balancer described by Istio is used to carry connections in and out of the edge of the mesh. The specification describes a series of open ports and the protocols used by these ports, SNI configuration for load balancing, and so on. Gateway is a CRD extension . It also reuses the capability of sidecar proxy. For detailed configuration, please refer to Istio official website .\nxDS protocol You may have seen the following picture when you understand Service Mesh. Each block represents an instance of a service, such as a Pod in Kubernetes (which contains a sidecar proxy). The xDS protocol controls all traffic in Istio Service Mesh. The specific behavior is to link the squares in the figure below.\nService Mesh diagram  The xDS protocol was proposed by Envoy . The original xDS protocols in the Envoy v2 API refer to CDS (Cluster Discovery Service), EDS (Endpoint Discovery Service), LDS (Listener Discovery Service), and RDS (Route Discovery Service). Later, in the v3 version, Scoped Route Discovery Service (SRDS), Virtual Host Discovery Service (VHDS), Secret Discovery Service (SDS), and Runtime Discovery Service (RTDS) were developed. See the xDS REST and gRPC protocol for details .\nLet’s take a look at the xDS protocol with a service with two instances each.\nxDS protocol  The arrow in the figure above is not the path or route after the traffic enters the proxy, nor is it the actual sequence. It is an imagined xDS interface processing sequence. In fact, there are cross references between xDS.\nAgents that support the xDS protocol dynamically discover resources by querying files or managing servers. In summary, the corresponding discovery service and its corresponding API are called xDS. Envoy by subscription (subscription) to get the resources the way, there are three ways to subscribe:\n File subscription : Monitor files in the specified path, the easiest way to find dynamic resource is to save it in a file and path configuration in ConfigSource the pathparameter. gRPC streaming subscription : Each xDS API can be individually configured ApiConfigSource to point to the cluster address of the corresponding upstream management server. Polling REST-JSON polling subscription : A single xDS API can perform synchronous (long) polling of REST endpoints.  For details of the above xDS subscription methods, please refer to the xDS protocol analysis . Istio uses gRPC streaming subscriptions to configure sidecar proxy for all data planes.\nThe article introduces the overall architecture of the Istio pilot, the generation of proxy configuration, the function of the pilot-discovery module, and the CDS, EDS, and ADS in the xDS protocol. For details on ADS, please refer to the official Envoy documentation .\nxDS protocol highlights Finally, summarize the main points about the xDS protocol:\n CDS, EDS, LDS, and RDS are the most basic xDS protocols, and they can be updated independently. All Discovery Services can connect to different Management Servers, which means that there can be multiple servers managing xDS. Envoy has made a series of extensions based on the original xDS protocol, adding SDS (Key Discovery Service), ADS (Aggregated Discovery Service), HDS (Health Discovery Service), MS (Metric Service), RLS (Rate Limiting Service) Wait for the API. To ensure data consistency, if used directly xDS original API, it needs to ensure that such sequential update: CDS -\u003e EDS -\u003e LDS -\u003e RDS, which is to follow the electronic engineering before-break (Make-Before-Break) The principle is to establish a new connection before disconnecting the original connection. The application in routing is to prevent the situation where the upstream cluster cannot be found and the traffic is dropped when a new routing rule is set, similar to the circuit Open circuit. CDS sets which services are in the service mesh. EDS sets which instances (Endpoints) belong to these services (Cluster). LDS sets the listening port on the instance to configure routing. The routing relationship between RDS final services should ensure that RDS is updated last.  Envoy Envoy is the default sidecar in Istio Service Mesh. Based on Enovy, Istio has extended its control plane in accordance with Envoy’s xDS protocol. Before talking about the Envoy xDS protocol, we need to be familiar with the basic terms of Envoy. The following lists the basic terms and data structure analysis in Envoy. For a detailed introduction to Envoy , please refer to the official Envoy document . As for how Envoy works as a forwarding proxy in Service Mesh (not limited to Istio), please refer to NetEase Cloud Liu Chao this in-depth interpretation of the technical details behind the Service Mesh and understanding Istio Service Mesh Envoy agent in Sidecar injection and traffic hijacking , in which the article refers to some of the points, the details will not be repeated.\nEnvoy proxy architecture diagram  Basic terminology Here are the basic terms in Enovy you should know:\n Downstream : The downstream host connects to Envoy, sends a request and receives a response, that is, the host sending the request. Upstream : The upstream host receives the connection and request from Envoy and returns a response, that is, the host that accepted the request. Listener : The listener is a named network address (for example, port, unix domain socket, etc.), and downstream clients can connect to these listeners. Envoy exposes one or more listeners to connect to downstream hosts. Cluster : A cluster is a group of logically identical upstream hosts connected to Envoy. Envoy discovers members of the cluster through service discovery . You can choose to determine the health status of cluster members through active health checks . Envoy uses load balancing policies to decide which member of the cluster to route requests to.  Envoy can set multiple Listeners, and each Listener can also set a filter chain, and the filters are extensible, which can make it easier for us to manipulate traffic behavior, such as setting encryption, private RPC, and so on.\nThe xDS protocol was proposed by Envoy and is now the default sidecar proxy in Istio. However, as long as the xDS protocol is implemented, it can theoretically be used as a sidecar proxy in Istio, such as the open source proxy MOSN by Ant Group .\nIstio Service Mesh Istio service mesh architecture diagram  Istio is a very feature-rich Service Mesh, which includes the following functions:\n Traffic Management: This is the most basic feature of Istio. Policy control: Implemented through Mixer components and various adapters to implement access control systems, telemetry capture, quota management, and billing. Observability: Achieved through Mixer. Security certification: Citadel components do key and certificate management.  Traffic Management in Istio Istio defined as the CRD to help users perform traffic management:\n Gateway : Gateway describes a load balancer running at the edge of the network for receiving incoming or outgoing HTTP / TCP connections. VirtualService : VirtualService actually connects Kubernetes services to Istio Gateway. It can also do more, such as defining a set of traffic routing rules to apply when a host is addressed. DestinationRule : DestinationRule The defined policy determines the access policy of the traffic after routing processing. Simply put, it defines how the traffic is routed. These policies can define load balancing configurations, connection pool sizes, and external detection (used to identify and evict unhealthy hosts in a load balancing pool) configuration. EnvoyFilter : The EnvoyFilter object describes filters for proxy services that can customize the proxy configuration generated by Istio Pilot. This configuration is rarely used by beginning users. ServiceEntry : By default, services in Istio Service Mesh cannot discover services outside Mesh. It ServiceEntry can add additional entries to the service registry inside Istio, so that services automatically discovered in the mesh can access and route to these manual Joined services.  Kubernetes vs xDS vs Istio After the reading of the above Kubernetes kube-proxyafter abstraction component, and XDS Istio in traffic management, we will take you far as the traffic management aspect of comparison components corresponding to the three / protocol (note, not completely three equivalents).\n   Governors xDS Istio Service Mesh     Endpoint Endpoint -   Service Route VirtualService   kube-proxy Route DestinationRule   kube-proxy Listener EnvoyFilter   Ingress Listener Gateway   Service Cluster ServiceEntry    Conclusion If you say that the objects managed by Kubernetes are Pods, then the objects managed by Service Mesh are Service. Therefore, it is a natural thing to apply Service Mesh after using Kubernetes to manage microservices. If you do n’t want to manage even the Service, use serverless platforms like knative, but that’s what comes next.\nThe function of Envoy/MOSN is not just for traffic forwarding. The above concepts are just the tip of the iceberg in Istio’s new layer of abstraction over Kubernetes. This will be the beginning of the book.\nReference  In-depth interpretation of the technical details behind Service Mesh-cnblogs.com  Understanding Envoy Proxy Sidecar Injection and Traffic Hijacking in Istio Service Mesh - jimmysong.io  Introduction to kubernetes: service and kube-proxy principles - cizixs.com  Kubernetes Ingress Traffic Load Balancing Using IPVS - jishu.io  xDS REST and gRPC protocol - envoyproxy.io   ","permalink":"https://jimmysong.io/en/blog/service-mesh-the-microservices-in-post-kubernetes-era/","summary":"This article is a rework of previously written content and is included in the Istio Handbook of the ServiceMesher community . Other chapters are still being compiled.","title":"Service Mesh - the Microservices in Post Kubernetes Era"},{"content":"2020 is indeed a bad start. In less than a month, I hardly heard any good news:\nTrump assassinated Major General Sulaymani of Iran; this pneumonia outbreak in Wuhan; the news that my basketball icon Kobe died of a helicopter crash really shocked me, and the Lakers said goodbye on the 24th. This gave me another spiritual blow during the Spring Festival, which was originally lacking in interest.\n2020 is bound to be a year deeply remembered in all humankind. In the last few days of the first month of the year, I decided to revise the website. The first reason was that I could n’t go out during the extended Chinese New Year holiday, and it was boring at home. And many pictures on the website were saved on Weibo map beds. The picture bed is unstable, causing many photos to be irretrievable; coupled with a habit of organizing the website every long holiday (the last revision of the website was completed during the National Day holiday in 2018, completed at home for 7 days), so I decided to The website has been revised again and it has become what it is now.\nFeatures The website has the following features after this revision:\n Reorganize the website content, the structure is more reasonable Support email subscription Images are stored on Github Responsive static website, card design, better user experience  Everyone is welcome to enter your email address in the email input box at the bottom of the page. Once there is an important content update on this site, we will push you through the email as soon as possible.\n","permalink":"https://jimmysong.io/en/notice/website-revision-notice/","summary":"In the last days of the first month of 2020, I decided to revamp the website.","title":"jimmysong.io website revision notice"},{"content":"A few days ago during the Mid-Autumn Festival, I translated Google’s Engineering Practices documentation , which is open source on Github. The original document Github address: https://github.com/google/eng-practices , the main content so far is summarized by Google How to conduct a Code Review guide, based on the title of the original Github repository, we will add more Google engineering practices in the future.\n Github：https://github.com/rootsongjc/eng-practices  Browse online: https://jimmysong.io/eng-practices   The translation uses the same style and directory structure as the original document. In fact, if the original text has international requirements, it can be directly incorporated, but according to the translation suggestions submitted by several previous people, the author of this project does not recommend translation. The first is translation. The documents may be unmaintained, and the accuracy of the documents cannot be guaranteed.\nFor suggestions on Chinese integration, see:\n Add Chinese translation #12  Add the link of Chinese version of code reviewer’s guide #8   ","permalink":"https://jimmysong.io/en/notice/google-engineering-practices-zh/","summary":"Translated from Google's open source documentation on Github","title":"Chinese version of Google Engineering Practice Documents"},{"content":"The following paragraph is a release note from the Istio official blog https://istio.io/zh/blog/2019/announcing-1.1/ , which I translated.\nIstio was released at 4 a.m. Beijing time today and 1 p.m. Pacific time.\nSince the 1.0 release last July, we have done a lot to help people get Istio into production. We expected to release a lot of patches (six patches have been released so far!), But we are also working hard to add new features to the product.\nThe theme for version 1.1 is “Enterprise Ready”. We are happy to see more and more companies using Istio in production, but as some big companies join in, Istio also encounters some bottlenecks.\nThe main areas we focus on include performance and scalability. As people gradually put Istio into production and use larger clusters to run more services with higher capacity, there may be some scaling and performance issues. Sidecar takes up too much resources and adds too much latency. The control plane (especially Pilot) consumes excessive resources.\nWe put a lot of effort into making the data plane and control plane more efficient. In the 1.1 performance test, we observed that sidecars typically require 0.5 vCPU to process 1000 rps. A single Pilot instance can handle 1000 services (and 2000 pods) and consumes 1.5 vCPUs and 2GB of memory. Sidecar adds 5 milliseconds at the 50th percentile and 10 milliseconds at the 99th percentile (the execution strategy will increase latency).\nWe have also completed the work of namespace isolation. You can use the Kubernetes namespace to enforce control boundaries to ensure that teams do not interfere with each other.\nWe have also improved multi-cluster functionality and usability. We listened to the community and improved the default settings for flow control and policies. We introduced a new component called Galley. Galley validates YAML configuration, reducing the possibility of configuration errors. Galley is also used in multi-cluster setups-collecting service discovery information from each Kubernetes cluster. We also support other multi-cluster topologies, including single control planes and multiple synchronous control planes, without the need for flat network support.\nSee the release notes for more information and details .\nThere is more progress on this project. As we all know, Istio has many moving parts, and they take on too much work. To address this, we have recently established the Usability Working Group (available at any time). A lot happened in the community meeting (Thursday at 11 am) and in the working group. You can log in to discuss.istio.io with GitHub credentials to participate in the discussion!\nThanks to everyone who has contributed to Istio over the past few months-patching 1.0, adding features to 1.1, and extensive testing recently on 1.1. Special thanks to companies and users who work with us to install and upgrade to earlier versions to help us identify issues before they are released.\nFinally, go to the latest documentation and install version 1.1! Happy meshing!\nOfficial website The ServiceMesher community has been maintaining the Chinese page of the official Istio documentation since the 0.6 release of Istio . As of March 19, 2019, there have been 596 PR merges, and more than 310 documents have been maintained. Thank you for your efforts! Some documents may lag slightly behind the English version. The synchronization work is ongoing. For participation, please visit https://github.com/servicemesher/istio-official-translation. Istio official website has a language switch button on the right side of each page. You can always Switch between Chinese and English versions, you can also submit document modifications, report website bugs, etc.\nServiceMesher Community Website\nThe ServiceMesher community website http://www.servicemesher.com covers all technical articles in the Service Mesh field and releases the latest activities in a timely manner. It is your one-stop portal to learn about Service Mesh and participate in the community.\n","permalink":"https://jimmysong.io/en/notice/istio-11/","summary":"Istio 1.1 was released at 4 am on March 20th, Beijing time. This version took 8 months! The ServiceMesher community also launched the Istio Chinese documentation.","title":"Istio 1.1 released"},{"content":"Istio handbook was originally an open source e-book I created (see https://jimmysong.io/istio-handbook ). It has been written for 8 months before donating to the ServiceMesher community. In order to further popularize Istio and Service Mesh technology, this book Donate to the community for co-authoring. The content of the original book was migrated to https://github.com/servicemesher/istio-handbook on March 10, 2019. The original book will no longer be updated.\n GitHub address: https://github.com/servicemesher/istio-handbook  Reading address online: http://www.servicemesher.com/istio-handbook/   Conceptual picture of this book, cover photo of Shanghai Jing’an Temple at night , photo by Jimmy Song .\nThe publishing copyright of this book belongs to the blog post of Electronic Industry Press. Please do not print and distribute it without authorization.\nIstio is a service mesh framework jointly developed by Google, IBM, Lyft, etc., and began to enter the public vision in early 2017. As an important infrastructure layer that inherits Kubernetes and connects to the serverless architecture in the cloud-native era, Istio is of crucial importance important. The ServiceMesher community, as one of the earliest open source communities in China that is researching and promoting Service Mesh technology, decided to integrate community resources and co-author an open source e-book for readers.\nAbout this book This book originates from the rootsongjc / istio-handbook and the Istio knowledge map created by the ServiceMesher community .\nThis book is based on Istio 1.0+ and includes, but is not limited to , topics in the Istio Knowledge Graph .\nParticipate in the book Please refer to the writing guidelines of this book and join the Slack channel discussion after joining the ServiceMesher community .\n","permalink":"https://jimmysong.io/en/notice/istio-handbook-by-servicemesher/","summary":"To further popularize Istio and Service Mesh technology, donate this book to the community for co-authoring.","title":"Donate Istio Handbook to the ServiceMesher community"},{"content":"Github: https://github.com/rootsongjc/cloud-native-sandbox Cloud Native Sandbox can help you setup a standalone Kubernetes and istio environment with Docker on you own laptop.\nThe sandbox integrated with the following components:\n Kubernetes v1.10.3 Istio v1.0.4 Kubernetes dashboard v1.8.3  Differences with kubernetes-vagrant-centos-cluster As I have created the kubernetes-vagrant-centos-cluster to set up a Kubernetes cluster and istio service mesh with vagrantfile which consists of 1 master(also as node) and 3 nodes, but there is a big problem that it is so high weight and consume resources. So I made this light weight sandbox.\nFeatures\n No VirtualBox or Vagrantfile required Light weight High speed, low drag Easy to operate  Services\nAs the sandbox setup, you will get the following services.\nRecord with termtosvg .\nPrerequisite You only need a laptop with Docker Desktop installed and Kubernetes enabled .\nNote: Leave enough resources for Docker Desktop. At least 2 CPU, 4G memory.\nInstall To start the sandbox, you have to run the following steps.\nKubernetes dashboard(Optional) Install Kubernetes dashboard.\nkubectl apply -f install/dashbaord/ Get the dashboard token.\nkubectl -n kube-system describe secret default| awk '$1==\"token:\"{print $2}' Expose kubernetes-dashboard service.\nkubectl -n kube-system get pod -l k8s-app=kubernetes-dashboard -o jsonpath='{.items[0].metadata.name}' Login to Kubernetes dashboard on http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login with the above token.\nIstio(Required) Install istio service mesh with the default add-ons.\n# Install istio kubectl apply -f install/istio/ To expose service grafana on http://localhost:3000 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 3000:3000 \u0026 To expose service prometheus on http://localhost:9090 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=prometheus -o jsonpath='{.items[0].metadata.name}') 9090:9090 \u0026 To expose service jaeger on http://localhost:16686 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=jaeger -o jsonpath='{.items[0].metadata.name}') 16686:16686 \u0026 To expose service servicegraph on http://localhost:8088/dotviz , http://localhost:8088/force/forcegraph.html .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=servicegraph -o jsonpath='{.items[0].metadata.name}') 8088:8088 \u0026 Kiali Install kiali .\nkubectl -n istio-system apply -f install/kiali To expose service kiali on http://localhost:20001 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=kiali -o jsonpath='{.items[0].metadata.name}') 20001:20001 \u0026 Bookinfo sample Deploy bookinfo sample .\n# Enable sidecar auto injection kubectl label namespace default istio-injection=enabled # Deploy bookinfo sample kubectl -n default apply -f sample/bookinfo Visit productpage on http://localhost/productpage .\nLet’s generate some loads.\nfor ((i=0;i\u003c1000;i=i+1));do echo \"Step-\u003e$i\";curl http://localhost/productpage;done You can watch the service status through http://localhost:3000 .\nClient tools To operate the applications on Kubernetes, you should install the following tools.\nRequired\n kubectl - Deploy and manage applications on Kubernetes. istioctl - Istio configuration command line utility.  Optional\n kubectx - Switch faster between clusters and namespaces in kubectl kube-ps1 - Kubernetes prompt info for bash and zsh  ","permalink":"https://jimmysong.io/en/blog/cloud-native-sandbox/","summary":"A standalone Kubernetes and Istio environment with Docker on you own laptop","title":"Cloud Native Sandbox"},{"content":"This video was recorded on the last day of 2018 and demonstrates the use of rootsongjc/kubernetes-vagrant-centos-cluster to automatically deploy a Kubernetes cluster and Istio Service Mesh.\nA few days ago I mentioned that kubernetes-vagrant-centos-cluster released v1.2.0 version to deploy a cloud-native experimental environment with one click. Someone in the Kubernetes and Service Mesh community asked me a long time ago to make a video to explain and demonstrate how to install Kubernetes and Istio Service Mesh, because I’m always busy, I’ve always made some time. Today, I will give a demo video, just don’t watch the video for a few minutes. In order to make this video, it took me half an hour to record, two hours to edit, and many years of shooting. , Editing, containers, virtual machines, Kubernetes, service grid experience. This is not so much a farewell as a new beginning.\nBecause the video was first posted on YouTube , it was explained in English (just a few supplementary instructions, it does n’t matter if you do n’t understand, just read the Chinese documentation on GitHub ).\nSkip to bilibli to watch . If you are interested in drone aerial photography, you can also take a look at Jimmy Song’s aerial photography works . Please support the coin or like, thank you.\nIf you have any questions, you can send a barrage or comment below the video.\nPS. Some people will ask why you chose to use bilibli, because there are no ads for watching videos on this platform, and most of them are uploaded by the Up master. Although the two-dimensional elements are mostly, the community atmosphere is still good.\nFor more exciting videos, visit Jimmy Song’s bibli homepage .\n","permalink":"https://jimmysong.io/en/notice/cloud-native-kubernetes-service-mesh-local-demo-show/","summary":"This video was recorded by me on the last day of 2018 and demonstrates the use of rootsongjc/kubernetes-vagrant-centos-cluster to automatically deploy Kubernetes clusters and Istio Service Mesh.","title":"Kubernetes and Istio Service Mesh Cloud Native Local Video Demo Show"},{"content":"Updated at Mar 8, 2022\nThis article uses Istio’s official bookinfo sample to explain how Envoy performs routing forwarding after the traffic entering the Pod and forwarded to Envoy sidecar by iptables, detailing the inbound and outbound processing. For a detailed analysis of traffic interception, see Understanding Envoy Sidecar Proxy Injection and Traffic Interception in Istio Service Mesh .\nOverview of Sidecar Injection and Traffic Interception Steps Below is an overview of the steps from Sidecar injection, Pod startup to Sidecar proxy interception traffic and Envoy processing routing.\n Kubernetes automatically injected through Admission Controller, or the user run istioctl command to manually inject sidecar container. Apply the YAML configuration deployment application. At this time, the service creation configuration file received by the Kubernetes API server already includes the Init container and the sidecar proxy. Before the sidecar proxy container and application container are started, the Init container started firstly. The Init container is used to set iptables (the default traffic interception method in Istio, and can also use BPF, IPVS, etc.) to Intercept traffic entering the pod to Envoy sidecar Proxy. All TCP traffic (Envoy currently only supports TCP traffic) will be Intercepted by sidecar, and traffic from other protocols will be requested as originally. Launch the Envoy sidecar proxy and application container in the Pod.  Sidecar proxy and application container startup order issues\nStart the sidecar proxy and the application container. Which container is started first? Normally, Envoy Sidecar and the application container are all started up before receiving traffic requests. But we can’t predict which container will start first, so does the container startup order have an impact on Envoy hijacking traffic? The answer is yes, but it is divided into the following two situations.\nCase 1: The application container starts first, and the sidecar proxy is still not ready\nIn this case, the traffic is transferred to the 15001 port by iptables, and the port is not monitored in the Pod. The TCP link cannot be established and the request fails.\nCase 2: Sidecar starts first, the request arrives and the application is still not ready\nIn this case, the request will certainly fail. As for the step at which the failure begins, the reader is left to think.\nQuestion : If adding a readiness and living probe for the sidecar proxy and application container can solve the problem?\n  TCP requests that are sent or received from the Pod will be hijacked by iptables. After the inbound traffic is hijacked, it is processed by the Inbound Handler and then forwarded to the application container for processing. The outbound traffic is hijacked by iptables and then forwarded to the Outbound Handler for processing. Upstream and Endpoint. Sidecar proxy requests Pilot to use the xDS protocol to synchronize Envoy configurations, including LDS, EDS, CDS, etc., but to ensure the order of updates, Envoy will use ADS to request configuration updates from Pilot directly.  How Envoy handles route forwarding The following figure shows a productpageservice access request http://reviews.default.svc.cluster.local:9080/, when traffic enters reviews the internal services, reviews internal services Envoy Sidecar is how to do traffic blocked the route forward.\nIstio transparent traffic hijacking and traffic routing schematic  Before the first step, productpage Envoy Sidecar Pod has been selected by EDS of a request to reviews a Pod service of its IP address, it sends a TCP connection request.\nThe Envoy configuration in the official website of Istio is to describe the process of Envoy doing traffic forwarding. The party considering the traffic of the downstream is to receive the request sent by the downstream. You need to request additional services, such as reviews service requests need Pod ratings service.\nreviews, there are three versions of the service, there is one instance of each version, three versions sidecar similar working steps, only to later reviews-v1-cb8655c75-b97zc Sidecar flow Pod forwarding this step will be described.\nUnderstanding the Inbound Handler The role of the inbound handler is to transfer the traffic from the downstream intercepted by iptables to localhost to establish a connection with the application container inside the Pod.\nLook reviews-v1-cb8655c75-b97zc at the Listener in the pod.\nRun istioctl pc listener reviews-v1-cb8655c75-b97zc to see what the Pod has a Listener.\nADDRESS PORT TYPE  172.33.3.3 9080 HTTP \u003c--- Receives all inbound traffic on 9080 from listener 0.0.0.0_15006 10.254.0.1 443 TCP \u003c--+ 10.254.4.253 80 TCP | 10.254.4.253 8080 TCP | 10.254.109.182 443 TCP | 10.254.22.50 15011 TCP | 10.254.22.50 853 TCP | 10.254.79.114 443 TCP |  10.254.143.179 15011 TCP | 10.254.0.2 53 TCP | Receives outbound non-HTTP traffic for relevant IP:PORT pair from listener 0.0.0.0_15001 10.254.22.50 443 TCP | 10.254.16.64 42422 TCP | 10.254.127.202 16686 TCP | 10.254.22.50 31400 TCP | 10.254.22.50 8060 TCP | 10.254.169.13 14267 TCP | 10.254.169.13 14268 TCP | 10.254.32.134 8443 TCP | 10.254.118.196 443 TCP \u003c--+ 0.0.0.0 15004 HTTP \u003c--+ 0.0.0.0 8080 HTTP | 0.0.0.0 15010 HTTP |  0.0.0.0 8088 HTTP | 0.0.0.0 15031 HTTP | 0.0.0.0 9090 HTTP |  0.0.0.0 9411 HTTP | Receives outbound HTTP traffic for relevant port from listener 0.0.0.0_15001 0.0.0.0 80 HTTP | 0.0.0.0 15030 HTTP | 0.0.0.0 9080 HTTP | 0.0.0.0 9093 HTTP | 0.0.0.0 3000 HTTP | 0.0.0.0 8060 HTTP | 0.0.0.0 9091 HTTP \u003c--+  0.0.0.0 15006 TCP \u003c--- Receives all inbound and outbound traffic to the pod from IP tables and hands over to virtual listener As from productpage traffic arriving reviews Pods, downstream must clearly know the IP address of the Pod which is 172.33.3.3, so the request is 172.33.3.3:9080.\nVirtual Listener\nAs you can see from the Pod’s Listener list, the 0.0.0.0:15001/TCP Listener (the actual name is virtual) listens for all inbound traffic, and the following is the detailed configuration of the Listener.\n{ \"name\": \"virtual\", \"address\": { \"socketAddress\": { \"address\": \"0.0.0.0\", \"portValue\": 15006 } }, \"filterChains\": [ { \"filters\": [ { \"name\": \"envoy.tcp_proxy\", \"config\": { \"cluster\": \"BlackHoleCluster\", \"stat_prefix\": \"BlackHoleCluster\" } } ] } ], \"useOriginalDst\": true } UseOriginalDst : As can be seen from the configuration in useOriginalDstthe configuration as specified true, which is a Boolean value, the default is false, using iptables redirect connections, the proxy may receive port original destination address is not the same port, thus received at the proxy port It is 15001 and the original destination port is 9080. When this flag is set to true, the Listener redirects the connection to the Listener associated with the original destination address, here 172.33.3.3:9080. Listener If no relationship to the original destination address, the connection processing by the Listener to receive it, i.e. the virtualListener, after envoy.tcp_proxyforwarded to a filter process BlackHoleCluster, as the name implies, when no matching Envoy virtual listener when the effect of Cluster , will send the request to it and return 404. This will be referred to below Listener provided bindToPort echoes.\nNote : This parameter will be discarded, please use the Listener filter of the original destination address instead. The main purpose of this parameter is: Envoy listens to the 15201 port to intercept the traffic intercepted by iptables via other Listeners instead of directly forwarding it. See the Virtual Listener for details .\nListener 172.33.3.3_9080\nAs mentioned above, the traffic entering the inbound handler is virtual transferred to the 172.33.3.3_9080 Listener by the Listener. We are looking at the Listener configuration.\nRun istioctl pc listener reviews-v1-cb8655c75-b97zc --address 172.33.3.3 --port 9080 -o json view.\n[{ \"name\": \"172.33.3.3_9080\", \"address\": { \"socketAddress\": { \"address\": \"172.33.3.3\", \"portValue\": 9080 } }, \"filterChains\": [ { \"filterChainMatch\": { \"transportProtocol\": \"raw_buffer\" }, \"filters\": [ { \"name\": \"envoy.http_connection_manager\", \"config\": { ... \"route_config\": { \"name\": \"inbound|9080||reviews.default.svc.cluster.local\", \"validate_clusters\": false, \"virtual_hosts\": [ { \"domains\": [ \"*\" ], \"name\": \"inbound|http|9080\", \"routes\": [ { ... \"route\": { \"cluster\": \"inbound|9080||reviews.default.svc.cluster.local\", \"max_grpc_timeout\": \"0.000s\", \"timeout\": \"0.000s\" } } ] } ] }, \"use_remote_address\": false, ... } } ]， \"deprecatedV1\": { \"bindToPort\": false } ... }, { \"filterChainMatch\": { \"transportProtocol\": \"tls\" }, \"tlsContext\": {... }, \"filters\": [... ] } ], ... }] bindToPort : Note that there are a bindToPort configuration that is false, the default value of the configuration true, showing Listener bind to the port, set here to false the process flow can Listener Listener transferred from the other, i.e., above said virtual Listener, where we see filterChains.filters in the envoy.http_connection_manager configuration section:\n\"route_config\": { \"name\": \"inbound|9080||reviews.default.svc.cluster.local\", \"validate_clusters\": false, \"virtual_hosts\": [ { \"domains\": [ \"*\" ], \"name\": \"inbound|http|9080\", \"routes\": [ { ... \"route\": { \"cluster\": \"inbound|9080||reviews.default.svc.cluster.local\", \"max_grpc_timeout\": \"0.000s\", \"timeout\": \"0.000s\" } } ] } ] } This configuration indicates that traffic will be handed off to the Cluster for inbound|9080||reviews.default.svc.cluster.local processing.\nCluster inbound|9080||reviews.default.svc.cluster.local\nRun istioctl pc cluster reviews-v1-cb8655c75-b97zc --fqdn reviews.default.svc.cluster.local --direction inbound -o json to see the Cluster configuration is as follows.\n[ { \"name\": \"inbound|9080||reviews.default.svc.cluster.local\", \"connectTimeout\": \"1.000s\", \"hosts\": [ { \"socketAddress\": { \"address\": \"127.0.0.1\", \"portValue\": 9080 } } ], \"circuitBreakers\": { \"thresholds\": [ {} ] } } ] You can see that the Endpoint of the Cluster directly corresponds to localhost, and then the traffic is forwarded by the application container after iptables.\nUnderstanding the Outbound Handler Because the reviews will to ratings send an HTTP request service, request address are: http://ratings.default.svc.cluster.local:9080/ the role of Outbound handler is to intercept traffic to iptables to native applications sent via Envoy to determine how to route to the upstream.\nThe request sent by the application container is outbound traffic. After being hijacked by iptables, it is transferred to the Envoy Outbound handler for processing, then passed through virtual Listener and 0.0.0.0_9080 Listener, and then finds the cluster of upstream through Route 9080, and then finds Endpoint through EDS to perform routing action.\nRoute 9080\nreviews requests ratings service, run istioctl proxy-config routes reviews-v1-cb8655c75-b97zc --name 9080 -o json view route configuration because Envoy VirtualHost will be matched according to HTTP header of domains, so the following list only ratings.default.svc.cluster.local:9080 this one VirtualHost.\n[{ \"name\": \"ratings.default.svc.cluster.local:9080\", \"domains\": [ \"ratings.default.svc.cluster.local\", \"ratings.default.svc.cluster.local:9080\", \"ratings\", \"ratings:9080\", \"ratings.default.svc.cluster\", \"ratings.default.svc.cluster:9080\", \"ratings.default.svc\", \"ratings.default.svc:9080\", \"ratings.default\", \"ratings.default:9080\", \"10.254.234.130\", \"10.254.234.130:9080\" ], \"routes\": [ { \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"outbound|9080||ratings.default.svc.cluster.local\", \"timeout\": \"0.000s\", \"maxGrpcTimeout\": \"0.000s\" }, \"decorator\": { \"operation\": \"ratings.default.svc.cluster.local:9080/*\" }, \"perFilterConfig\": {... } } ] }, ..] You can see the routing of traffic to the Cluster from this Virtual Host configuration outbound|9080||ratings.default.svc.cluster.local.\nEndpoint outbound|9080||ratings.default.svc.cluster.local\nIstio 1.1 previous versions do not support the use of istioctl commands to directly query Endpoint Cluster, you can use the debug queries Pilot endpoint way compromise.\nkubectl exec reviews-v1-cb8655c75-b97zc -c istio-proxy curl http://istio-pilot.istio-system.svc.cluster.local:9093/debug/edsz \u003e endpoints.json endpoints.json file contains all the Endpoint information of the Cluster, and we only select outbound|9080||ratings.default.svc.cluster.local the results of the Cluster as follows.\n{ \"clusterName\": \"outbound|9080||ratings.default.svc.cluster.local\", \"endpoints\": [ { \"locality\": { }, \"lbEndpoints\": [ { \"endpoint\": { \"address\": { \"socketAddress\": { \"address\": \"172.33.100.2\", \"portValue\": 9080 } } }, \"metadata\": { \"filterMetadata\": { \"istio\": { \"uid\": \"kubernetes://ratings-v1-8558d4458d-ns6lk.default\" } } } } ] } ] } The Endpoint can be one or more, and Envoy will route it according to certain rules by selecting the appropriate Endpoint.\nNote : Istio 1.1 will support the istioctl pc endpoint command to query Endpoint.\nReference  Debugging Envoy and Pilot - istio.io  Understanding Envoy Agent Sidecar Injection and Traffic Interception in Istio Service Mesh - jimmysong.io  Istio traffic management implementation mechanism deep analysis - zhaohuabing.com   ","permalink":"https://jimmysong.io/en/blog/understanding-how-envoy-sidecar-intercept-and-route-traffic-in-istio-service-mesh/","summary":"Details about Envoy sidecar with iptables rules.","title":"Understanding How Envoy Sidecar Intercept and Route Traffic in Istio Service Mesh"},{"content":"KubeCon \u0026 CloudNative in North America is the most worthy cloud-native event every year. This time it will be held in Seattle for four days, from December 10th to 13th, refer to the official website of the conference . This year, 8,000 people participated. You should notice that Kubernetes has become more and more low-level. Cloud-native application developers do not need to pay much attention to it. Major companies are publishing their own cloud-native technology stack layouts, including IBM, VMware, SAP, Startups around this ecosystem are still emerging. The PPT sharing address of the local conference: https://github.com/warmchang/KubeCon-North-America-2018 , thank you William Zhang for finishing and sharing the slides of this conference .\nSeattle scene Janet Kuo from Google describes the path to cloud-native technology adoption.\nThe same event of KubeCon \u0026 CloudNativeCon, the scene of the first EnvoyCon.\nAt KubeCon \u0026 CloudNativeCon Seattle, various directors, directors, directors, VPs, and Gartner analysts from IBM, Google, Mastercard, VMware, and Gartner are conducting live discussions on the topic of Scaling with Service Mesh and Istio. Why do we talk about Istio when we talk about Service Mesh? What is not suitable for Istio use case. . .\nThe PPT contains basic introduction, getting started, a total of more than 200 Deep Dive as well as practical application, we recommend you according to the General Assembly’s official website to choose topics of interest to look at the schedule, otherwise I might see, however.\nA little impression KubeCon \u0026 CloudNativeCon is held three times a year, Europe, China and North America. China is the first time this year, and it will be held in Shanghai in November. It is said that it will be held in June next year. Although everyone said that Kubernetes has become boring, the conference about Kubernetes There is still a lot of content, and the use of CRD to extend Kubernetes usage is increasing. Service Mesh has begun to become hot. As can be seen from the live pictures above, there are a large number of participants on the site and related topics are also increasing. It is known as a microservice in the post-Kubernetes era . This must be It will be an important development direction of cloud native after Kubernetes, and the ServiceMesher community pays close attention to it.\n","permalink":"https://jimmysong.io/en/notice/kubecon-cloudnativecon-seattle-2018/","summary":"KubeCon \u0026 CloudNativeCon Seattle 2018 Data Sharing.","title":"Kubecon\u0026CloudNativeCon Seattle 2018"},{"content":"This is a postscript from the post- Kubernetes era. Just this evening I saw a post by Bilgin Ibryam Microservices in a Post-Kuberentes Era  .\nOn April 9, 2017, the Kubernetes Handbook-Kubernetes Chinese Guide / Cloud Native Application Architecture Practice Manual was first submitted. In the past 16 months, 53 contributors participated, 1,088 commits, and a total of 23,9014 Chinese characters were written. At the same time , thousands of enthusiasts have gathered in the Kubernetes \u0026 Cloud Native combat group .\nIt has been more than 4 months since the previous version was released. During this period, Kubernetes and Prometheus graduated from CNCF respectively and have matured commercially. These two projects have basically taken shape and will not change much in the future. Kubernetes was originally developed for container orchestration. In order to solve the problem of microservice deployment, Kubernetes has gained popularity. The current microservices have gradually entered the post-Kubernetes era . Service Mesh and cloud native redefine microservices and distributed applications.\nWhen this version was released, the PDF size was 108M with a total of 239,014 Chinese characters. It is recommended to browse online , or clone the project and install the Gitbook command to compile it yourself.\nThis version has the following improvements:\n Added Istio Service Mesh tutorial  Increased use VirtualBox and Vagrant set up in a local cluster and distributed Kubernetes Istio Service Mesh  Added cloud native programming language Ballerina and Pulumi introduced Added Quick Start Guide  Added support for Kubernetes 1.11 Added enterprise-level service mesh adoption path guide  Added SOFAMesh chapter  Added vision for the cloud-native future  Added CNCF charter and participation Added notes for Docker image repositories Added Envoy chapter  Increased KCSP (Kubernetes certification service providers) and CKA (Certified Kubernetes administrator) instructions Updated some configuration files, YAML and reference links Updated CRI chapter  Removed obsolete description Improved etcdctl command usage tutorial Fixed some typos  Browse and download  Browse online https://jimmysong.io/kubernetes-handbook  To make it easy for everyone to download, I put a copy on Weiyun , which is available in PDF (108MB), MOBI (42MB), and EPUB (53MB).  In this book, there are more practical tutorials. In order to better understand the principles of Kubernetes, I recommend studying ** In- depth analysis of Kubernetes by Zhang Lei, produced by Geek Time **.\nThank you Kubernetes for your support of this book. Thank you Contributors . In the months before this version was released, the ServiceMesher community was co-founded . As a force in the post-Kubernetes era , welcome to contact me to join the community and create cloud native New era .\nAt present , the WeChat group of the ServiceMesher community also has thousands of members. The Kubernete Handbook will continue, but Service Mesh is already a rising star. With Kubernetes in mind, welcome to join the ServiceMesher community and follow us. The public account of the community (also the one I manage).\n","permalink":"https://jimmysong.io/en/notice/new-kubernetes-handbook-released-and-say-hello-to-post-kubernetes-era/","summary":"This is an obituary post-Kubernetes era. Kubernetes handbook by Jimmy Song v1.4 is released. The next focus of cloud native is Service Mesh!","title":"Kubernetes Handbook v1.4 is released"},{"content":"Today I am honored to announce that I have become a CNCF Ambassador . Here is my story with Cloud Native.\nOrigin The first time to attend the Cloud Native Computing Foundation is at the LC3 in Beijing 2017. I attended the meeting again this year, and in November of this year, CNCF will hold the KubeCon \u0026 CloudNativeCon for the first time in Shanghai, China. I’ll be there too.\nCloud Native Books My origins with the Cloud Native is originated from Kevin Hoffman’s book Cloud Native Go . I translated this book at the end of 2016. Since then, in China, the translation of the word Cloud Native has not been determined, we introduced it with 云原生 to China.\nAnd then I begin to write the kubernetes-handbook on GitHub. So far, it has more than 2000 stars. This book has written more than 200,000 Chinese characters, the first commit happened on April 14, 2017.\nSince the the book Cloud Native Go completed, the publisher recommended another Cloud Native book to me - Cloud Native Python by Manish Sethi.\nAnd the book Cloud Native Java by Josh Long and Kenny Bastani.\nIn March 2018, with the hope that Bring the world equal opportunities and Building a Financial Cloud Native Infrastructure, I joined the Ant Group .\nServiceMesher Community By the time of May 2018, I start to organize the ServiceMesher community.\nIn the last few months, we work with other open source communities in China, such as k8smeetup , Sharding-Sphere , Apache SkyWalking . Our community has grown to have 1,700 members and two round meetups in Hangzhou and Beijing till now.\nMore than 300 people participated in the scene and more than 20,000 people watched it live by IT大咖说 。\nFuture Here are some hopes of mine:\n Open source culture become popular in China More and more people would like to be involved in open source projects Host one open source project into the CNCF A book related to Cloud Native or Service Mesh Strengthen cultural exchanges between China and the global  Finally, welcome to China for traveling or share your topic with us on Cloud Native, and in the mean while we will share our experience on large scale web apps to the world. Hope to hear your voice!\n","permalink":"https://jimmysong.io/en/blog/cloud-native-and-me-the-past-current-and-future/","summary":"Today I am honored to announce that I have become a CNCF Ambassador.","title":"Cloud Native and me - the past, current and future"},{"content":"Today, we are pleased to announce Istio 1.0 . It’s been over a year since the original 0.1 release. Since 0.1, Istio has grown rapidly with the help of a thriving community, contributors, and users. Many companies have successfully applied Istio to production today and have gained real value through the insight and control provided by Istio. We help large businesses and fast-growing startups such as eBay , Auto Trader UK , Descartes Labs , HP FitStation , Namely , PubNub and Trulia to connect, manage and protect their services from scratch with Istio. The release of this version as 1.0 recognizes that we have built a core set of features that users can rely on for their production.\nEcosystem Last year we saw a significant increase in the Istio ecosystem. Envoy continues its impressive growth and adds many features that are critical to a production-level service grid. Observability providers like Datadog , SolarWinds , Sysdig , Google Stackdriver, and Amazon CloudWatch have also written plugins to integrate Istio with their products. Tigera , Aporeto , Cilium and Styra have built extensions for our strategy implementation and network capabilities. Kiali built by Red Hat provides a good user experience for grid management and observability. Cloud Foundry is building the next-generation traffic routing stack for Istio, the recently announced Knative serverless project is doing the same, and Apigee has announced plans to use it in their API management solution. These are just a few of the projects that the community added last year.\nFeatures Since the 0.8 release, we have added some important new features, and more importantly, marked many existing features as Beta to indicate that they can be used in production. This is covered in more detail in the release notes , but it is worth mentioning:\n Multiple Kubernetes clusters can now be added to a single grid , enabling cross-cluster communication and consistent policy enforcement. Multi-cluster support is now Beta. The network API for fine-grained control of traffic through the grid is now Beta. Explicitly modeling ingress and egress issues with gateways allows operations personnel to control the network topology and meet access security requirements at the edge. Two-way TLS can now be launched incrementally without updating all clients of the service. This is a key feature that removes the barriers to deploying Istio on existing production. Mixer now supports developing out-of-process adapters . This will be the default way to extend Mixer in an upcoming release, which will make it easier to build adapters. Envoy now fully evaluates the authorization policies that control service access locally , improving their performance and reliability. Helm chart installation is now the recommended installation method, with a wealth of customization options to configure Istio to your needs. We put a lot of effort into performance, including continuous regression testing, large-scale environmental simulation, and target repair. We are very happy with the results and will share details in the coming weeks.  Next step Although this is an important milestone for the project, much work remains to be done. When working with adopters, we’ve received a lot of important feedback about what to focus next. We’ve heard consistent topics about supporting hybrid clouds, installing modularity, richer network capabilities, and scalability for large-scale deployments. We have considered some feedback in the 1.0 release and we will continue to actively work on it in the coming months.\nQuick start If you are new to Istio and want to use it for deployment, we would love to hear from you. Check out our documentation , visit our chat forum or visit the mailing list . If you want to contribute more to the project, please join our community meeting and say hello.\nAt last The Istio team is grateful to everyone who contributed to the project. Without your help, it won’t have what it is today. Last year’s achievements were amazing, and we look forward to achieving even greater achievements with our community members in the future.\n The ServiceMesher community is responsible for the translation and maintenance of Chinese content on Istio’s official website. At present, the Chinese content is not yet synchronized with the English content. You need to manually enter the URL to switch to Chinese ( https://istio.io/zh ). There is still a lot of work to do , Welcome everyone to join and participate.\n","permalink":"https://jimmysong.io/en/notice/istio-v1-released/","summary":"Chinese documentation is released at the same time!","title":"Istio 1.0 is released"},{"content":" If there is a visual learning model and platform that provides infrastructure clusters for your operation, would you pay for it?\n Two months ago, I met Jord in Kubernetes’s Slack channel, and later saw the link of MagicSandbox.io he (and possibly others) sent in the Facebook group of Taiwan Kubernetes User Group, and I clicked to apply for a trial Then, I received an email from Jord later, and he told me that he wanted to build a Kubernetes learning platform. That’s where the whole thing started, and then we had a couple of Zoom video chats for a long time.\nAbout MagicSandbox MagicSandbox is a startup company. Jord (Dutch) is also a serial entrepreneur. He has studied at Sichuan University in China for 4 years since he was 19, and then returned to Germany. He worked as a PM at Boston Consulting Group and now works at Entrepreneur. First (Europe’s top venture capital / enterprise incubator) is also located in Berlin, Germany. He met Mislav (Croatian). Mislav is a full-stack engineer and has several entrepreneurial experiences. They have similar odors, and they hit it off. Decided to be committed to the Internet education industry and create a world-class software engineer education platform. They want to start with Kubernetes, first provide Internet-based Kubernetes theory and practice teaching, and then expand the topic to ElasticSearch, GraphQL, and so on. topic.\nJord founded MagicSandbox in his home, and I became the face of MagicSandbox in China.\nNow we are going to release the MagicSandbox Alpha version. This version is an immature version and is provided for everyone to try for free. Positive feedback is also welcome.\n Official homepage: https://magicsandbox.com/  Chinese page: https://cn.magicsandbox.com/ (The content has not been finished yet, only the Chinese version homepage is currently provided) Follow us on Twitter: https://twitter.com/magicsandbox   ","permalink":"https://jimmysong.io/en/notice/magicsandbox-alpha-version-annoucement/","summary":"Online practical software engineering education platform.","title":"MagicSandbox Alpha released"},{"content":"Remember the cloud-native programming language I shared before finally appeared!  Learn about Ballerina in one article!  ? They are ready to attend the KubeCon \u0026 CloudNativeCon China Conference!\nKubeCon \u0026 CloudNativeCon China Conference will be held on November 14-15, 2018 (Wednesday, Thursday) in Shanghai . See: https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2018/ With Ballerina’s official authorization, I now need to help them find an “ambassador” in China, responsible for team guidance, Chinese and English translation, and familiarity with Cloud Native and microservices. It has influence in the industry and has no barriers to English communication.\nAmbassador duties\n Team Leadership Responsible for Chinese and English translation of product declaration, PPT materials, etc. Help to arrange the booth  The other party can provide\n Conference tickets Travel expenses Accommodation during the conference Other compensation  This is a photo of their team in front of their booth during the KubeCon \u0026 CloudNativeCon in Hagen in May this year.\nPS This is the most complete and best picture I have ever found of their team. (Photography needs to be strengthened)\nLet’s briefly introduce this startup called Ballerina. Their team is mainly from Sri Lanka. This is an island country next to India in the South Asian subcontinent. In ancient China, it was called “Lion Country” and rich in gemstones.\nThe capital of their country is Sri Lanka, which is pronounced in their own language: Si li jia ya wa de na pu la ke te\nIf you are interested, please contact me directly.\n","permalink":"https://jimmysong.io/en/notice/a-ballerina-china-ambassador-required/","summary":"With official authorization from Ballerina, I now need to help them find an ambassador in China.","title":"Ballerina seeks Chinese ambassador"},{"content":"Envoy-Designed for cloud-native applications, open source edge and service proxy, Istio Service Mesh default data plane, Chinese version of the latest official document, dedicated by the ServiceMesher community, welcome everyone to learn and share together.\n TL;DR: http://www.servicemesher.com/envoy/  PDF download address: servicemesher/envoy   This is the first time Community Service Mesh enthusiasts group activities, the document is based on Envoy latest (version 1.7) Official documents https://www.envoyproxy.io/docs/envoy/latest/ . A total of 120 articles with 26 participants took 13 days and 65,148 Chinese characters.\nVisit online address: http://www.servicemesher.com/envoy/ Note : This book does not include the v1 API reference and v2 API reference sections in the official documentation. Any links to API references in the book will jump directly to the official page.\nContributor See the contributor page: https://github.com/servicemesher/envoy/graphs/contributors Thanks to the above contributors for their efforts! Because the level of translators is limited, there are inevitably inadequacies in the text. I also ask readers to correct them. Also welcome more friends to join our GitHub organization: https://github.com/servicemesher ","permalink":"https://jimmysong.io/en/notice/envoyproxy-docs-cn-17-release/","summary":"Translated by ServiceMesher community.","title":"Chinese version of the latest official document of Envoy released"},{"content":"Envoy is an open source L7 proxy and communication bus written in C ++ by Lyft. It is currently an open source project under CNCF . The code is hosted on GitHub. It is also the default data plane in the Istio service mesh. We found that it has very good performance, and there are also continuous open source projects based on Envoy, such as Ambassador , Gloo, etc. At present, the official documentation of Envoy has not been well finished, so we Service Service enthusiasts feel that they are launching the community The power of the co-translation of the latest (version 1.7) official documentation of Enovy and organization through GitHub.\nService Mesh enthusiasts have jointly translated the latest version of the official document of Envoy . The translated code is hosted at https://github.com/servicemesher/envoy . If you are also a Service Mesh enthusiast, you can join the SerivceMesher GitHub organization and participate together.\nThe official Envoy document excludes all articles in the two directories of the v1 API reference and the v2 API reference. There are more than 120 documents. The length of the documents varies. The original English official documents use the RST format. I manually converted them into Markdown format and compiled using Gitbook. A GitHub Issue was generated according to the path of the document relative to the home directory. Friends who want to participate in translation can contact me to join the ServiceMesher organization, and then select the article you want to translate in Issue , and then reply “Claim”.\nHere you can see all the contributors. In the future, we will also create a Service Mesh enthusiast website. The website uses static pages. All code will be hosted on Github. Welcome everyone to participate.\n","permalink":"https://jimmysong.io/en/notice/enovy-doc-translation-start/","summary":"The SerivceMesher community is involved in translating the official documentation for the latest version of Envoy.","title":"Envoy's latest official document translation work started"},{"content":"TL; DR Click here to download the PDF of this book .\nRecently, Michael Hausenblas of Nginx released a booklet on container networks in docker and kubernetes. This 72-page material is a good introduction for everyone to understand the network in Docker and Kubernetes from shallow to deep.\nTarget audience  Container Software Developer SRE Network Operation and Maintenance Engineer Architects who want to containerize traditional software  ","permalink":"https://jimmysong.io/en/notice/container-networking-from-docker-to-kubernetes-nginx/","summary":"Source from Nginx, published by O’Reilly.","title":"Docker container network book sharing"},{"content":"In 2017, we are facing a big era of architectural changes, such as Kubernetes ending the battle for container orchestration, Kafka release 1.0, serverless gradually gaining momentum, edge computing to replace cloud computing, Service Mesh ready to go, and artificial intelligence to empower business , Also brings new challenges to the architecture.\nI am about to participate in InfoQ’s ArchSummit Global Architects Summit on December 8-11 in Beijing . This conference also invited 100+ top technologists such as Dr. Ali Wangjian to share and summarize the architectural changes and reflections this year. I hope that you can build on this conference, summarize past practices, and look forward to a future-oriented architecture to transform this era of change into the common fortune of each of us.\nMy speech The content of my speech is from Kubernetes to Cloud Native-the road to cloud native applications . Link: From Kubernetes to Cloud Native-the road to cloud native applications . The time is Saturday, December 9, 9:30 am, in the fifth meeting room.\nAfter more than ten years of development of cloud computing, the new phase of cloud native has entered. Enterprise applications are preferentially deployed in cloud environments. How to adapt to the cloud native tide, use containers and Kubernetes to build cloud native platforms, and practice DevOps concepts and agility How IT, open source software, and the community can help IT transform, the solution to all these problems is the PaaS platform, which is self-evident to the enterprise.\nWe also prepared gifts for everyone: “Cloud Native Go-Building Cloud Native Web Applications Based on Go and React” and “Intelligent Data Era-Enterprise Big Data Strategy and Practice” There is also a book stand for the blog post of the Electronic Industry Press. Welcome to visit.\nArchSummit conference official website link: http://bj2017.archsummit.com/ For more details, please refer to the official website of the conference: http://bj2017.archsummit.com/ ","permalink":"https://jimmysong.io/en/notice/archsummit-beijing-2017-from-kubernetes-to-cloud-native/","summary":"I will give a lecture at ArchSummit Beijing. From Kubernetes to Cloud Native, my path to cloud native applications.","title":"ArchSummit Beijing 2017 speech preview"},{"content":"Cloudinary-go is a Go client library and CLI tool to upload static assets to the Cloudinary service.\nInstallation Install the CLI tool and the library with:\ngo get github.com/rootsongjc/cloudinary-go/cloudinary Or download the release binary from release .\n","permalink":"https://jimmysong.io/en/notice/cloudinary-go/","summary":"Cloudinary-go is a Go client library and CLI tool to upload static assets to Cloudinary service","title":"Cloudinary file upload tool written in Go released"},{"content":"Many people asked me how jimmysong.io made this website. I think it is necessary to write a book to popularize the knowledge of static website construction and Hugo as a tool.\nThis manual will guide you how to use Hugo to build a static website for personal blog or project display.\nTeach you how to build a static website from scratch. This does not require much programming and development experience and time investment, and basically does not require much cost (except for personalized domain names). You can quickly build and Launch a website.\nGithub address: https://github.com/rootsongjc/hugo-handbook Gitbook access address: https://jimmysong.io/hugo-handbook The content of this book will continue to improve over time and as my site improves, so stay tuned.\n","permalink":"https://jimmysong.io/en/notice/building-static-website-with-hugo/","summary":"A manual for static website building, and a personal blog Gitbook using Hugo.","title":"Hugo Handbook is released"},{"content":"Kevin Hoffman(From Capital One, twitter @KevinHoffman ) was making a speech on TalkingData T11 Smart Data Summit.\nHe addressed that 15 Factors of Cloud Native which based on Heroku’s original Twelve-Factor App , but he add more 3 another factors on it.\nLet’s have a look at the 15 factors of Cloud Native.\n1. One codebase, one App  Single version-controlled codebase, many deploys Multiple apps should not share code  Microservices need separate release schedules Upgrade, deploy one without impacting others   Tie build and deploy pipelines to single codebase  2. API first  Service ecosystem requires a contract  Public API   Multiple teams on different schedulers  Code to contract/API, not code dependencies   Use well-documented contract standards  Protobuf IDL, Swagger, Apiary, etc   API First != REST first  RPC can be more appropriate in some situations    3. Dependency Management  Explicitly declare dependencies Include all dependencies with app release Create immutable build artifact (e.g. docker image) Rely on smallest docker image  Base on scratch if possible   App cannot rely on host for system tools or libraries  4. Design, Build, Release, Run  Design part of iterative cycle  Agile doesn’t mean random or undesigned   Mature CI/CD pipeline and teams  Design to production in days not months   Build immutable artifacts Release automatically deploys to environment  Environments contains config, not release artifact    5. Configuration, Credentials, Code  “3 Cs” volatile substances that explode when combinded Password in a config file is as bad as password in code App must accept “3 Cs” from environment and only use harmless defaults Test - Could you expose code on Github and not reveal passwords, URLs, credentials?  6. Logs  Emit formatted logs to stdout Code should not know about destination or purpose of log emissions Use downstream log aggregator  collect, store, process, expose logs ELK, Splunk, Sumo, etc   Use structured logs to allow query and analysis  JSON, csv, KV, etc   Logs are not metrics  7. Disposability  App must start as quickly as possible App must stop quickly and gracefully Processes start and stop all the time in the cloud Every scale up/down disposes of processes Slow dispose == slow scale Slow dispose or startup can cause availability gaps  8. Backing Services  Assume all resources supplied by backingservices Cannotassume mutable file system  “Disk as a Service” (e.g. S3, virtual mounts, etc)   Every backing service is bound resource  URL, credentials, etc-\u003e environment config   Host does not satisfy NFRs  Backing services and cloud infrastructure    9. Environment Parity  “Works on my machine”  Cloud-native anti-pattern. Must work everywhere   Every commit is candidate for deployment Automated acceptance tests  Provide no confidence if environments don’t match    10. Administrative Processes  Database migrations Run-once scripts or jobs Avoid using for batch operations, consider instead:  Event sourcing Schedulers Triggers from queues, etc Lambdas/functions    11. Port Binding  In cloud, infrastructure determines port App must accept port assigned by platform Containers have internal/external ports  App design must embrace this   Never use reserved ports Beware of container “host mode” networking  12. Stateless Processes  What is stateless? Long-term state handled by a backing service In-memory state lives onlyas long as request Requests from same client routed to different instances  “Sticky sessions” cloud native anti-pattern    13. Concurency  Scale horizontally using the process model Build disposable, stateless, share-nothing processes Avoid adding CPU/RAM to increase scale/throughput Where possible, let platform/libraries do threading  Many single-threaded services \u003e 1 multi-threaded monolith    14. Telemetry  Monitor apps in the cloud like satellite in orbit No tether, no live debugger Application Perf Monitoring (APM) Domain Telemetry Health and system logs  15. Authentication \u0026 Authorization  Security should never be an afterthought Auth should be explicit, documented decision  Even if anonymous access is allowed Don’t allow anonymous access   Bearer tokens/OAuth/OIDC best practices Audit all attempts to access  Migrating Monoliths to the Cloud After this 15 factors, he also gave us some tips about how to migrate monoliths to the Cloud:\n Make a rule - stop adding to the monolith  All new code must be cloud native   Prioritize features  Where will you get most benefit from cloud native?   Come up with a plan  Decompose monolith over time Fast, agile iterations toward ultimate goal   Use multiple strategies and patterns  Go - the Best Language for Building Cloud Native App At last, he advise us the programming language Go is the best language to build Cloud Native applications for these reasons below:\n Lightweight Easily learning curve Compiles to native binaries Very fast Large, thriving, engaged community  http://gopherize.me     Kevin also wrote a book Cloud Native Go to show how to Building Web Applications and Microservices for the Cloud with Go and React. This book has been translated to Chinese by four guys from TalkingData with ❤️. 《Cloud Native Go 构建基于Go和React的云原生Web应用与微服务》published by PHEI publisher house.\nKevin was signing his name on the book\nkevin siging on the book  This is his first visit to China, as a main translator of this book I an honored to be with him to take this photo.\nkevin hoffman with me  ","permalink":"https://jimmysong.io/en/blog/high-level-cloud-native-from-kevin-hoffman/","summary":"Kevin Hoffman address that 15 Factors of Cloud Native.","title":"High Level Cloud Native From Kevin Hoffman"},{"content":"From now on I have my own independent domain name jimmysong.io , the website is still hosted on GitHub, the original URL https://jimmysong.io is still accessible.\nWhy use .io as the suffix? Because this is The First Step to Cloud Native!\nWhy choose today? Because today is August 18, the days are easy to remember.\nPS domain names are registered in namecheap and cost tens of dollars / year.\nProudly powered by hugo 🎉🎊🎉\n","permalink":"https://jimmysong.io/en/notice/domain-name-jimmysong-io/","summary":"From now on I have my own independent domain name jimmysong.io.","title":"New domain name jimmysong.io"},{"content":"This is a list of software, tools, architecture and reference materials about Cloud Native. It is awesome-cloud-native , a project I opened on GitHub , and can also be browsed through the web page .\nIt is divided into the following areas:\nAwesome Cloud Native\n AI  API gateway  Big Data  Container engine  CI-CD  Database  Data Science  Fault tolerant  Logging  Message broker  Monitoring  Networking  Orchestration and scheduler  Portability  Proxy and load balancer  RPC  Security and audit  Service broker  Service mesh  Service registry and discovery  Serverless  Storage  Tracing  Tools  Tutorial   This list will be continuously updated and improved in the future, not only for my usual research records, but also as a reference for the Cloud Native industry.\n","permalink":"https://jimmysong.io/en/notice/awesome-cloud-native/","summary":"This is a list of software, tools, architecture, and reference materials about Cloud Native. It is a project I started on GitHub.","title":"Awesome Cloud Native list is released"},{"content":"This book is the Chinese version of Migrating to Cloud Native Application Architectures. The English version of this book was released in February 2015. The Chinese version was translated by Jimmy Song and published in July 2017.\n GitHub hosting address for this book: https://github.com/rootsongjc/migrating-to-cloud-native-application-architectures  Gitbook reading address: https://jimmysong.io/migrating-to-cloud-native-application-architectures   The application architectures discussed in this book include:\n Twelve-factor application: A collection of cloud-native application architecture patterns Microservices: independently deployed services, each service does one thing Self-service agile infrastructure: a platform that provides application environments and back-office services quickly, repeatably, and consistently API-based collaboration: published and versioned APIs that allow interactions between services in a cloud-native application architecture Pressure resistance: a system that becomes stronger under pressure  ","permalink":"https://jimmysong.io/en/notice/changes-needed-to-cloud-native-archtecture/","summary":"This book is translated from an eBook published by Matt Stine in February 2015.","title":"Migrating to Cloud Native Chinese version released"},{"content":"Istio 1.14 was released in June of this year, and one of the most notable features of this release is support for SPIRE , which is one of the implementations of SPIFFE , a CNCF incubation project. This article explains what SPIRE means for zero-trust architectures and why you would need SPIRE for authentication in Istio.\nAuthentication in Kubernetes We all know that Istio was built for and typically runs on Kubernetes, so before we talk about how to use SPIRE for authentication in Istio, let’s take a look at how Kubernetes handles authentication.\nLet’s look at an example of a pod’s token. Whenever a pod gets created in Kubernetes, it gets assigned a default service account from the namespace, assuming we didn’t explicitly assign a service account to it. Here is an example:\napiVersion: v1 data: ca.crt: {CA_CRT} namespace: ZGVmYXVsdA== token: {TOKEN_STRING} kind: Secret metadata: annotations: kubernetes.io/service-account.name: sleep kubernetes.io/service-account.uid: 2c0d00e8-13a2-48d0-9ff8-f987f3325ecf creationTimestamp: \"2022-06-14T03:01:35Z\" name: sleep-token-gwhwd namespace: default resourceVersion: \"244535398\" uid: b8822ceb-9553-4a17-96dc-d525bbaed0e0 type: kubernetes.io/service-account-token Kubernetes manages the identity of Pods with Service Accounts and then specifies the permissions of Pods with a Service Account to the Kubernetes API using RBAC. A service account’s token is stored in a secret, which does not include a declaration of the node or pod where the workload is running. When a malicious actor steals a token, they gain full access to the account and can steal information or carry out sabotage under the guise of that user.\nA token can only be used to identify a workload in one cluster, but Istio supports multiple clusters and meshes, as well as Kubernetes environments and virtual machines. A unified workload identity standard can help here.\nAn Introduction to SPIFFE and SPIRE SPIFFE’s (Secure Production Identity Framework for Everyone) goal is to create a zero-trust, fully-identified data center network by establishing an open, unified workload identity standard based on the concept of zero-trust. SPIRE can rotate X.509 SVID certificates and secret keys on a regular basis. Based on administrator-defined policies, SPIRE can dynamically provision workload certificates and Istio can consume them. I’ll go over some of the terms associated with SPIFFE in a little more detail below.\nSPIRE (SPIFFE Runtime Environment) is a SPIFFE implementation that is ready for production. SVID (SPIFFE Verifiable Identity Document) is the document that a workload uses to prove its identity to a resource or caller. SVID contains a SPIFFE ID that represents the service’s identity. It uses an X.509 certificate or a JWT token to encode the SPIFFE ID in a cryptographically verifiable document. The SPIFFE ID is a URI that looks like this: spiffe://trust_domain/workload_identifier.\nSPIFFE and Zero Trust Security The essence of Zero Trust is identity-centric dynamic access control. SPIFFE addresses the problem of identifying workloads.\nWe might identify a workload using an IP address and port in the era of virtual machines, but IP address-based identification is vulnerable to multiple services sharing an IP address, IP address forgery, and oversized access control lists. Because containers have a short lifecycle in the Kubernetes era, instead of an IP address, we rely on a pod or service name. However, different clouds and software platforms approach workload identity differently, and there are compatibility issues. This is especially true in heterogeneous hybrid clouds, where workloads run on both virtual machines and Kubernetes. It is critical to establish a fine-grained, interoperable identification system at this point.\nUsing SPIRE for Authentication in Istio With the introduction of SPIRE to Istio, we can give each workload a unique identity, which is used by workloads in the service mesh for peer authentication, request authentication, and authorization policies. The SPIRE Agent issues SVIDs for workloads by communicating with a shared UNIX Domain Socket in the workload. The Envoy proxy and the SPIRE agent communicate through the Envoy SDS (Secret Discovery Service) API. Whenever an Envoy proxy needs to access secrets (certificates, keys, or anything else needed to do secure communication), it will talk to the SPIRE agent through Envoy’s SDS API.\nThe most significant advantage of SDS is the ease with which certificates can be managed. Without this feature, certificates would have to be created as a secret and then mounted into the agent container in a Kubernetes deployment. The secret must be updated, and the proxy container must be re-deployed if the certificate expires. Using SDS, Istio can push the certificates to all Envoy instances in the service mesh. If the certificate expires, the server only needs to push the new certificate to the Envoy instance; Envoy will use the new certificate right away, and the proxy container will not need to be re-deployed.\nThe architecture of using SPIRE for authentication in Istio is depicted in the diagram below.\nSPIRE Architecture with Istio  Use StatefulSet resources to deploy the SPIRE Server and Kubernetes Workload Registrar in the spire namespace of the Kubernetes cluster, and DaemonSet resources to deploy a SPIRE Agent for each node. Assuming that you used the default DNS name cluster.local when you install Kubernetes, Kubernetes Workload Registrar creates identities for the workloads in the Istio mesh in the following format:\n SPRRE Server：spiffe://cluster.local/ns/spire/sa/server SPIRE Agent：spiffe://cluster.local/ns/spire/sa/spire-agent Kubernetes Node：spiffe://cluster.local/k8s-workload-registrar/demo-cluster/node/ Kubernetes Workload Pod：spiffe://cluster.local/{namespace}/spire/sa/{service_acount}  This way, both the nodes and each workload have their own globally unique identity and can be scaled according to the cluster (trust domain).\nThe workload authentication process in Istio mesh is shown in the figure below.\n The workload authentication process in the Istio mesh   The detailed process is as follows:\n The pilot-agent in the sidecar of the workload calls the SPIRE agent via the shared UDS to get the SVID. SPIRE Agent asks Kubernetes (kubelet on the node to be precise) for information about the workload. The kubelet returns the information queries from the Kubernetes API server to the workload attestor. The attestor compares the results returned by the kubelet with the identity information shared by the sidecar. If they match, returns the SVID to the workload and caches it, if not, the attestation failed.  Please refer to the Istio documentation to learn how to use SPIRE for authentication in Istio.\nSummary SPIFFE unifies identity standards in heterogeneous environments, which is the foundation of zero-trust networks. In Istio, whether we use SPIRE or not, authentication is not perceptible to workloads. By using SPIRE to provide authentication for workloads, we can effectively manage workload identity and lay the foundation for a zero-trust network.\n","permalink":"https://jimmysong.io/en/blog/why-istio-need-spire/","summary":"This article explains what SPIRE means for zero-trust architectures and why you would need SPIRE for authentication in Istio.","title":"Why would you need SPIRE for authentication with Istio?"},{"content":"  It’s been more than 5 years since Google, IBM, and Lyft unveiled the Istio open source project in May 2017. The Istio project has developed from a seed to a tall tree in these years. Many domestic books on the Istio service mesh were launched in the two years following the release of Istio 1.0 in 2018. My country is at the forefront of the world in the field of Istio book publishing.\nService mesh: one of the core technologies of cloud native Today, Istio is nearly synonymous with service mesh in China. The development of service mesh, as one of the core cloud-native technologies described by CNCF (Cloud Native Computing Foundation), has gone through the following stages.\n 2017-2018: Exploratory Phase 2019-2020: Early Adopter Phase 2021 to present: Implementation on a large scale and ecological development stage  Cloud native technology enables enterprises to design and deploy elastically scalable applications in new dynamic settings such as public, private, and hybrid clouds, according to the CNCF. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs are examples of cloud native technology.\nService mesh has been included to the CNCF definition of cloud native, indicating that it is one of the representative technologies of cloud native. Google is donating Istio to CNCF today, and we have reason to expect that as a CNCF project, Istio’s community will be more open, and its future development will be more smooth.\nService mesh and cloud native applications Cloud-native development is gaining traction. Despite the frequent emergence of new technologies and products, service mesh has maintained its place as “cloud-native network infrastructure” as part of the overall cloud-native technology stack throughout the past year. The cloud-native technology stack model is depicted in the diagram below, with representative technologies for each layer to define the standard. Service mesh and other cloud-native technologies complement each other as a new era of middleware emerges. Dapr (Distributed Application Runtime) defines the cloud-native middleware capability model, OAM defines the cloud-native application model, and so on, whereas service mesh Lattice defines a cloud-native seven-layer network model.\n Cloud Native Application Model   Why you need a service mesh Using a service mesh isn’t tantamount to abandoning Kubernetes; it just makes sense. The goal of Kubernetes is to manage application lifecycles through declarative configuration, whereas the goal of service mesh is to provide traffic control, security management, and observability amongst apps. How do you set up load balancing and flow management for calls between services after a robust microservice platform has been developed with Kubernetes?\nMany open source tools, including Istio, Linkerd, MOSN, and others, support Envoy’s xDS protocol. The specification of xDS is Envoy’s most significant contribution to service mesh or cloud native. Many various usage cases, such as API gateways, sidecar proxies in service meshes, and edge proxies, are derived from Envoy, which is simply a network proxy, a modern version of the proxy configured through the API.\nIn a nutshell, the move from Kubernetes to Istio was made for the following reasons.\n Application life cycle management, specifically application deployment and management, is at the heart of Kubernetes (scaling, automatic recovery, and release). Kubernetes is a microservices deployment and management platform that is scalable and extremely elastic. Transparent proxy is the cornerstone of service mesh, which intercepts communication between microservices via sidecar proxy and then regulates microservice behavior via control plane settings. The deployment mode of service meshes has introduced new issues today. For service meshes, sidecar is no longer required, and an agentless service mesh based on gRPC is also being tested. xDS is a protocol standard for configuring service meshes, and a gRPC-based xDS is currently being developed. Kubernetes traffic management is decoupled with the service mesh. The kube-proxy component is not required to support traffic within the service mesh. The traffic between services is controlled by an abstraction close to the microservice application layer to achieve security and observability features. In Kubernetes, service mesh is an upper-level abstraction of service, and Serverless is the next stage, which is why Google released Knative based on Kubernetes and Istio following Istio.  Open source in the name of the community The ServiceMesher community was founded in May 2018 with the help of Ant Financial. Following that, a tornado of service meshes erupted in China, and the community-led translation of Istio’s official documentation reached a fever pitch.\nI became aware of a dearth of Chinese resources for systematically teaching Istio over time, so in September 2018, I began to plan and create an Istio book, launching the Istio Handbook open source e-book project on GitHub. I met many friends who are also interested in Istio and service mesh technology in the online and offline events of the community a few months later, with the promotion of service mesh technology and the expansion of the ServiceMesher community. We unanimously agreed to collaborate on an open source Istio e-book, which will compile the community’s important writings and experience into a logical text and make it available to the majority of developers.\nHundreds of people volunteered and began co-authoring the book in March 2019 under the auspices of the Community Stewardship Council. In May 2020, we created a cloud-native community that incorporated the original ServiceMesher community in order to further promote cloud-native technology and expand the technical knowledge supplied by the community. The scope of community operations has also widened, moving away from service mesh to more extensive cloud-native tools.\nThe editorial board for this book, which includes me, Ma Ruofei, Wang Baiping, Wang Wei, Luo Guangming, Zhao Huabing, Zhong Hua, and Guo Xudong, was founded in October 2020. We performed further version updates, improvements, and optimizations to this book under the supervision and assistance of the publishing business. This book, “In-depth Understanding of Isito: Advanced Practice of Cloud Native Service Mesh,” finally met you after many iterations.\n The book cover   About this book After version 1.5, Istio underwent considerable architectural modifications, and various new or better features were added, including the addition of a smart DNS proxy, additional resource objects, increased support for virtual machines, and more.\nThis book is based on the new edition of Istio, and it aims to provide readers with the most up-to-date and comprehensive content possible by following the newest trends in the Istio community. Furthermore, several of the book’s authors are front-line development or operation and maintenance engineers with extensive Istio expertise, offering detailed and useful reference cases for the book.\nThis book is currently available on the JD.com . Please read “In-depth Understanding of Isito: Advanced Practice of Cloud Native Service Mesh” if you want to learn more about Istio!\n Buy now\n  ","permalink":"https://jimmysong.io/en/blog/istio-service-mesh-book/","summary":"By the Cloud Native Community(China)","title":"In-Depth Understanding of Istio: Announcing the Publication of a New Istio Book"},{"content":"This article will guide you on how to compile the Istio binaries and Docker images on macOS.\nBefore you begin Before we start, refer to the Istio Wiki , here is the information about my build environment.\n macOS 12.3.1 Darwin AMD64 Docker Desktop 4.8.1(78998) Docker Engine v20.10.14  Start to compile First, download the Istio code from GitHub to the $GOPATH/src/istio.io/istio directory, and execute the commands below in that root directory.\nCompile into binaries Execute the following command to download the Istio dependent packages, which will be downloaded to the vendor directory.\ngo mod vendor Run the following command to build Istio:\nsudo make build If you do not run the command with sudo, you may encounter the following error.\nfatal: unsafe repository ('/work' is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work fatal: unsafe repository ('/work' is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work Makefile.core.mk:170: *** \"TAG cannot be empty\". Stop. make: *** [build] Error 2 Even if you follow the prompts and run git config --global --add safe.directory /work, you will still get errors during compilation.\nThe compiled binary will be saved in out directory with the following directory structure.\nout ├── darwin_amd64 │ ├── bug-report │ ├── client │ ├── envoy │ ├── extauthz │ ├── install-cni │ ├── istio-cni │ ├── istio-cni-taint │ ├── istio-iptables │ ├── istio_is_init │ ├── istioctl │ ├── logs │ ├── operator │ ├── pilot-agent │ ├── pilot-discovery │ ├── release │ └── server └── linux_amd64 ├── envoy ├── envoy-centos ├── logs └── release It will build both the linux_amd64 and darwin_amd64 architectures binaries at the same time.\nCompile into Docker images Run the following command to compile Istio into a Docker image.\nsudo make build The compilation will take about 3 to 5 minutes depending on your network. Once the compilation is complete, you will see the Docker image of Istio by running the following command.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE localhost:5000/app_sidecar_centos_7 latest 2044037df94b 51 seconds ago 524MB localhost:5000/app_sidecar_ubuntu_jammy latest 5d8ae5ed55b7 About a minute ago 362MB localhost:5000/proxyv2 latest d4679412385f About a minute ago 243MB localhost:5000/install-cni latest 78f46d5771d2 About a minute ago 270MB localhost:5000/istioctl latest c38130a5adc8 About a minute ago 190MB localhost:5000/pilot latest 2aa9185ec202 About a minute ago 190MB localhost:5000/app latest 473adafaeb8d About a minute ago 188MB localhost:5000/operator latest 9ac1fedcdd12 About a minute ago 191MB localhost:5000/ext-authz latest 1fb5aaf20791 About a minute ago 117MB localhost:5000/app_sidecar_debian_11 latest 61376a02b95d 2 minutes ago 407MB localhost:5000/app_sidecar_ubuntu_xenial latest 7e8efe666611 2 minutes ago 418MB You can change the image name and push it into your own container registry.\nSummary This is how to build Istio on macOS. If you have already downloaded the Docker image you need to build, the build will take less than a minute. It also takes only a few minutes to build Docker images.\nReference  Using the Code Base - github.com   ","permalink":"https://jimmysong.io/en/blog/how-to-build-istio/","summary":"This article will guide you on how to compile the Istio binaries on macOS.","title":"How to build Istio?"},{"content":"Updated on May 6, 2022\nBased on Istio version 1.13, this article will present the following.\n What is the sidecar pattern and what advantages does it have? How are the sidecar injections done in Istio? How does the sidecar proxy do transparent traffic hijacking? How is the traffic routed to upstream?  The figure below shows how the productpage service requests access to http://reviews.default.svc.cluster.local:9080/ and how the sidecar proxy inside the reviews service does traffic blocking and routing forwarding when traffic goes inside the reviews service.\nIstio transparent traffic hijacking and traffic routing diagram  At the beginning of the first step, the sidecar in the productpage pod has selected a pod of the reviews service to be requested via EDS, knows its IP address, and sends a TCP connection request.\nThere are three versions of the reviews service, each with an instance, and the sidecar work steps in the three versions are similar, as illustrated below only by the sidecar traffic forwarding step in one of the Pods.\nSidecar pattern Dividing the functionality of an application into separate processes running in the same minimal scheduling unit (e.g. Pod in Kubernetes) can be considered sidecar mode. As shown in the figure below, the sidecar pattern allows you to add more features next to your application without additional third-party component configuration or modifications to the application code.\nSidecar pattern  The Sidecar application is loosely coupled to the main application. It can shield the differences between different programming languages and unify the functions of microservices such as observability, monitoring, logging, configuration, circuit breaker, etc.\nAdvantages of using the Sidecar pattern When deploying a service mesh using the sidecar model, there is no need to run an agent on the node, but multiple copies of the same sidecar will run in the cluster. In the sidecar deployment model, a companion container (such as Envoy or MOSN) is deployed next to each application’s container, which is called a sidecar container. The sidecar takes overall traffic in and out of the application container. In Kubernetes’ Pod, a sidecar container is injected next to the original application container, and the two containers share storage, networking, and other resources.\nDue to its unique deployment architecture, the sidecar model offers the following advantages.\n Abstracting functions unrelated to application business logic into a common infrastructure reduces the complexity of microservice code. Reduce code duplication in microservices architectures because it is no longer necessary to write the same third-party component profiles and code. The sidecar can be independently upgraded to reduce the coupling of application code to the underlying platform.  iptables manipulation analysis In order to view the iptables configuration, we need to nsenter the sidecar container using the root user to view it, because kubectl cannot use privileged mode to remotely manipulate the docker container, so we need to log on to the host where the productpage pod is located.\nIf you use Kubernetes deployed by minikube, you can log directly into the minikube’s virtual machine and switch to root. View the iptables configuration that lists all the rules for the NAT (Network Address Translation) table because the mode for redirecting inbound traffic to the sidecar is REDIRECT in the parameters passed to the istio-iptables when the Init container is selected for the startup, so there will only be NAT table specifications in the iptables and mangle table configurations if TPROXY is selected. See the iptables command for detailed usage.\nWe only look at the iptables rules related to productpage below.\n# login to minikube, change user to root $ minikube ssh $ sudo -i # See the processes in the productpage pod's istio-proxy container $ docker top `docker ps|grep \"istio-proxy_productpage\"|cut -d \" \" -f1` UID PID PPID C STIME TTY TIME CMD 1337 10576 10517 0 08:09 ? 00:00:07 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage.default --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istiod.istio-system.svc:15012 --zipkinAddress zipkin.istio-system:9411 --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --connectTimeout 10s --proxyAdminPort 15000 --concurrency 2 --controlPlaneAuthPolicy NONE --dnsRefreshRate 300s --statusPort 15020 --trust-domain=cluster.local --controlPlaneBootstrap=false 1337 10660 10576 0 08:09 ? 00:00:33 /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage.default --service-node sidecar~172.17.0.16~productpage-v1-7f44c4d57c-ksf9b.default~default.svc.cluster.local --max-obj-name-len 189 --local-address-ip-version v4 --log-format [Envoy (Epoch 0)] [%Y-%m-%d %T.%e][%t][%l][%n] %v -l warning --component-log-level misc:error --concurrency 2 # Enter the nsenter into the namespace of the sidecar container (any of the above is ok) $ nsenter -n --target 10660 View the process’s iptables rule chain under its namespace.\n# View the details of the rule configuration in the NAT table. $ iptables -t nat -L -v # PREROUTING chain: Used for Destination Address Translation (DNAT) to jump all incoming TCP traffic to the ISTIO_INBOUND chain. Chain PREROUTING (policy ACCEPT 2701 packets, 162K bytes) pkts bytes target prot opt in out source destination 2701 162K ISTIO_INBOUND tcp -- any any anywhere anywhere # INPUT chain: Processes incoming packets and non-TCP traffic will continue on the OUTPUT chain. Chain INPUT (policy ACCEPT 2701 packets, 162K bytes) pkts bytes target prot opt in out source destination # OUTPUT chain: jumps all outbound packets to the ISTIO_OUTPUT chain. Chain OUTPUT (policy ACCEPT 79 packets, 6761 bytes) pkts bytes target prot opt in out source destination 15 900 ISTIO_OUTPUT tcp -- any any anywhere anywhere # POSTROUTING CHAIN: All packets must first enter the POSTROUTING chain when they leave the network card, and the kernel determines whether they need to be forwarded out according to the packet destination. Chain POSTROUTING (policy ACCEPT 79 packets, 6761 bytes) pkts bytes target prot opt in out source destination # ISTIO_INBOUND CHAIN: Redirects all inbound traffic to the ISTIO_IN_REDIRECT chain, except for traffic destined for ports 15090 (used by Prometheus) and 15020 (used by Ingress gateway for Pilot health checks), and traffic sent to these two ports will return to the call point of the iptables rule chain, the successor POSTROUTING to the PREROUTING chain. Chain ISTIO_INBOUND (1 references) pkts bytes target prot opt in out source destination 0 0 RETURN tcp -- any any anywhere anywhere tcp dpt:ssh 2 120 RETURN tcp -- any any anywhere anywhere tcp dpt:15090 2699 162K RETURN tcp -- any any anywhere anywhere tcp dpt:15020 0 0 ISTIO_IN_REDIRECT tcp -- any any anywhere anywhere # ISTIO_IN_REDIRECT chain: jumps all inbound traffic to the local 15006 port, thus successfully blocking traffic to the sidecar. Chain ISTIO_IN_REDIRECT (3 references) pkts bytes target prot opt in out source destination 0 0 REDIRECT tcp -- any any anywhere anywhere redir ports 15006 # ISTIO_OUTPUT chain: see the details bellow Chain ISTIO_OUTPUT (1 references) pkts bytes target prot opt in out source destination 0 0 RETURN all -- any lo 127.0.0.6 anywhere 0 0 ISTIO_IN_REDIRECT all -- any lo anywhere !localhost owner UID match 1337 0 0 RETURN all -- any lo anywhere anywhere ! owner UID match 1337 15 900 RETURN all -- any any anywhere anywhere owner UID match 1337 0 0 ISTIO_IN_REDIRECT all -- any lo anywhere !localhost owner GID match 1337 0 0 RETURN all -- any lo anywhere anywhere ! owner GID match 1337 0 0 RETURN all -- any any anywhere anywhere owner GID match 1337 0 0 RETURN all -- any any anywhere localhost 0 0 ISTIO_REDIRECT all -- any any anywhere anywhere # ISTIO_REDIRECT chain: redirects all traffic to Sidecar (i.e. local) port 15001. Chain ISTIO_REDIRECT (1 references) pkts bytes target prot opt in out source destination 0 0 REDIRECT tcp -- any any anywhere anywhere redir ports 15001 The focus here is on the 9 rules in the ISTIO_OUTPUT chain. For ease of reading, I will show some of the above rules in the form of a table as follows.\n   Rule target in out source destination     1 RETURN any lo 127.0.0.6 anywhere   2 ISTIO_IN_REDIRECT any lo anywhere !localhost owner UID match 1337   3 RETURN any lo anywhere anywhere !owner UID match 1337   4 RETURN any any anywhere anywhere owner UID match 1337   5 ISTIO_IN_REDIRECT any lo anywhere !localhost owner GID match 1337   6 RETURN any lo anywhere anywhere !owner GID match 1337   7 RETURN any any anywhere anywhere owner GID match 1337   8 RETURN any any anywhere localhost   9 ISTIO_REDIRECT any any anywhere anywhere    The following diagram shows the detailed flow of the ISTIO_ROUTE rule.\nISTIO_ROUTE iptables rules  I will explain the purpose of each rule, corresponding to the steps and details in the illustration at the beginning of the article, in the order in which they appear. Where rules 5, 6, and 7 are extensions of the application of rules 2, 3, and 4 respectively (from UID to GID), which serve similar purposes and will be explained together. Note that the rules therein are executed in order, meaning that the rule with the next highest order will be used as the default. When the outbound NIC (out) is lo (local loopback address, loopback interface), it means that the destination of the traffic is the local Pod, and traffic sent from the Pod to the outside, will not go through this interface. Only rules 4, 7, 8, and 9 apply to all outbound traffic from the review Pod.\nRule 1\n Purpose: To pass through traffic sent by the Envoy proxy to the local application container, so that it bypasses the Envoy proxy and goes directly to the application container. Corresponds to steps 6 through 7 in the illustration. Details: This rule causes all requests from 127.0.0.6 (this IP address will be explained below) to jump out of the chain, return to the point of invocation of iptables (i.e. OUTPUT) and continue with the rest of the routing rules, i.e. the POSTROUTING rule, which sends traffic to an arbitrary destination, such as the application container within the local Pod. Without this rule, traffic from the Envoy proxy within the Pod to the Pod container will execute the next rule, rule 2, and the traffic will enter the Inbound Handler again, creating a dead loop. Putting this rule in the first place can avoid the problem of traffic dead-ending in the Inbound Handler.  Rule 2, 5\n Purpose: Handle inbound traffic (traffic inside the Pod) from the Envoy proxy, but not requests to the localhost, and forward it to the Envoy proxy’s Inbound Handler via a subsequent rule. This rule applies to scenarios where the Pod invokes its own IP address, i.e., traffic between services within the Pod. Details: If the destination of the traffic is not localhost and the packet is sent by 1337 UID (i.e. istio-proxy user, Envoy proxy), the traffic will be forwarded to Envoy’s Inbound Handler through ISTIO_IN_REDIRECT eventually.  Rule 3, 6\n Purpose: To pass through the internal traffic of the application container within the Pod. This rule applies to traffic within the container. For example, access to Pod IP or localhost within a Pod. Corresponds to steps 6 through 7 in the illustration. Details: If the traffic is not sent by an Envoy user, then jump out of the chain and return to OUTPUT to call POSTROUTING and go straight to the destination.  Rule 4, 7\n Purpose: To pass through outbound requests sent by Envoy proxy. Corresponds to steps 14 through 15 in the illustration. Details: If the request was made by the Envoy proxy, return OUTPUT to continue invoking the POSTROUTING rule and eventually access the destination directly.  Rule 8\n Purpose: Passes requests from within the Pod to the localhost. Details: If the destination of the request is localhost, return OUTPUT and call POSTROUTING to access localhost directly.  Rule 9\n Purpose: All other traffic will be forwarded to ISTIO_REDIRECT after finally reaching the Outbound Handler of Envoy proxy. Corresponds to steps 10 through 11 in the illustration.  The above rule avoids dead loops in the iptables rules for Envoy proxy to application routing, and guarantees that traffic can be routed correctly to the Envoy proxy, and that real outbound requests can be made.\nAbout RETURN target\nYou may notice that there are many RETURN targets in the above rules, which means that when this rule is specified, it jumps out of the rule chain, returns to the call point of iptables (in our case OUTPUT) and continues to execute the rest of the routing rules, in our case the POSTROUTING rule, which sends traffic to any destination address, you can think of This is intuitively understood as pass-through.\nAbout the 127.0.0.6 IP address\nThe IP 127.0.0.6 is the default InboundPassthroughClusterIpv4 in Istio and is specified in the code of Istio. This is the IP address to which traffic is bound after entering the Envoy proxy, and serves to allow Outbound traffic to be re-sent to the application container in the Pod, i.e. Passthought, bypassing the Outbound Handler. this traffic is access to the Pod itself, and not real outbound traffic. See Istio Issue-29603 for more information on why this IP was chosen as the traffic passthrough.\nThe traffic routing process explained Traffic routing is divided into two processes, Inbound and Outbound, which will be analyzed in detail for the reader below based on the example above and the configuration of the sidecar.\nUnderstand Inbound Handler The role of the Inbound handler is to pass traffic from the downstream blocked by iptables to the localhost and establish a connection to the application container within the Pod. Assuming the name of one of the Pods is reviews-v1-545db77b95-jkgv2, run istioctl proxy-config listener reviews-v1-545db77b95-jkgv2 --port 15006 to see which Listener is in that Pod.\nADDRESS PORT MATCH DESTINATION 0.0.0.0 15006 Addr: *:15006 Non-HTTP/Non-TCP 0.0.0.0 15006 Trans: tls; App: istio-http/1.0,istio-http/1.1,istio-h2; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: raw_buffer; App: http/1.1,h2c; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: tls; App: TCP TLS; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: raw_buffer; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: tls; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: tls; App: istio,istio-peer-exchange,istio-http/1.0,istio-http/1.1,istio-h2; Addr: *:9080 Cluster: inbound|9080|| 0.0.0.0 15006 Trans: raw_buffer; Addr: *:9080 Cluster: inbound|9080|| The following lists the meanings of the fields in the above output.\n ADDRESS: downstream address PORT: The port the Envoy listener is listening on MATCH: The transport protocol used by the request or the matching downstream address DESTINATION: Route destination  The Iptables in the reviews Pod hijack inbound traffic to port 15006, and from the above output we can see that Envoy’s Inbound Handler is listening on port 15006, and requests to port 9080 destined for any IP will be routed to the inbound|9080|| Cluster.\nAs you can see in the last two rows of the Pod’s Listener list, the Listener for 0.0.0.0:15006/TCP (whose actual name is virtualInbound) listens for all Inbound traffic, which contains matching rules, and traffic to port 9080 from any IP will be routed. If you want to see the detailed configuration of this Listener in Json format, you can execute the istioctl proxy-config listeners reviews-v1-545db77b95-jkgv2 --port 15006 -o json command. You will get an output similar to the following.\n[ /*omit*/ { \"name\": \"virtualInbound\", \"address\": { \"socketAddress\": { \"address\": \"0.0.0.0\", \"portValue\": 15006 } }, \"filterChains\": [ /*omit*/ { \"filterChainMatch\": { \"destinationPort\": 9080, \"transportProtocol\": \"tls\", \"applicationProtocols\": [ \"istio\", \"istio-peer-exchange\", \"istio-http/1.0\", \"istio-http/1.1\", \"istio-h2\" ] }, \"filters\": [ /*omit*/ { \"name\": \"envoy.filters.network.http_connection_manager\", \"typedConfig\": { \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\", \"statPrefix\": \"inbound_0.0.0.0_9080\", \"routeConfig\": { \"name\": \"inbound|9080||\", \"virtualHosts\": [ { \"name\": \"inbound|http|9080\", \"domains\": [ \"*\" ], \"routes\": [ { \"name\": \"default\", \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"inbound|9080||\", \"timeout\": \"0s\", \"maxStreamDuration\": { \"maxStreamDuration\": \"0s\", \"grpcTimeoutHeaderMax\": \"0s\" } }, \"decorator\": { \"operation\": \"reviews.default.svc.cluster.local:9080/*\" } } ] } ], \"validateClusters\": false }, /*omit*/ } } ], /*omit*/ ], \"listenerFilters\": [ /*omit*/ ], \"listenerFiltersTimeout\": \"0s\", \"continueOnListenerFiltersTimeout\": true, \"trafficDirection\": \"INBOUND\" } ] Since the Inbound Handler traffic routes traffic from any address to this Pod port 9080 to the inbound|9080|| Cluster, let’s run istioctl pc cluster reviews-v1-545db77b95-jkgv2 --port 9080 --direction inbound -o json to see the Cluster configuration and you will get something like the following output.\n[ { \"name\": \"inbound|9080||\", \"type\": \"ORIGINAL_DST\", \"connectTimeout\": \"10s\", \"lbPolicy\": \"CLUSTER_PROVIDED\", \"circuitBreakers\": { \"thresholds\": [ { \"maxConnections\": 4294967295, \"maxPendingRequests\": 4294967295, \"maxRequests\": 4294967295, \"maxRetries\": 4294967295, \"trackRemaining\": true } ] }, \"cleanupInterval\": \"60s\", \"upstreamBindConfig\": { \"sourceAddress\": { \"address\": \"127.0.0.6\", \"portValue\": 0 } }, \"metadata\": { \"filterMetadata\": { \"istio\": { \"services\": [ { \"host\": \"reviews.default.svc.cluster.local\", \"name\": \"reviews\", \"namespace\": \"default\" } ] } } } } ] We see that the TYPE is ORIGINAL_DST, which sends the traffic to the original destination address (Pod IP), because the original destination address is the current Pod, you should also notice that the value of upstreamBindConfig.sourceAddress.address is rewritten to 127.0.0.6, and for Pod This echoes the first rule in the iptables ISTIO_OUTPUT the chain above, according to which traffic will be passed through to the application container inside the Pod.\nUnderstand Outbound Handler Because reviews send an HTTP request to the ratings service at http://ratings.default.svc.cluster.local:9080/, the role of the Outbound handler is to intercept traffic from the local application to which iptables has intercepted, and determine how to route it to the upstream via the sidecar.\nRequests from application containers are Outbound traffic, hijacked by iptables and transferred to the Outbound handler for processing, which then passes through the virtualOutbound Listener, the 0.0.0.0_9080 Listener, and then finds the upstream cluster via Route 9080, which in turn finds the Endpoint via EDS to perform the routing action.\nRoute ratings.default.svc.cluster.local:9080\nreviews requests the ratings service and runs istioctl proxy-config routes reviews-v1-545db77b95-jkgv2 --name 9080 -o json. View the route configuration because the sidecar matches VirtualHost based on domains in the HTTP header, so only ratings.default.svc.cluster.local:9080 is listed below for this VirtualHost.\n[{ { \"name\": \"ratings.default.svc.cluster.local:9080\", \"domains\": [ \"ratings.default.svc.cluster.local\", \"ratings.default.svc.cluster.local:9080\", \"ratings\", \"ratings:9080\", \"ratings.default.svc.cluster\", \"ratings.default.svc.cluster:9080\", \"ratings.default.svc\", \"ratings.default.svc:9080\", \"ratings.default\", \"ratings.default:9080\", \"10.98.49.62\", \"10.98.49.62:9080\" ], \"routes\": [ { \"name\": \"default\", \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"outbound|9080||ratings.default.svc.cluster.local\", \"timeout\": \"0s\", \"retryPolicy\": { \"retryOn\": \"connect-failure,refused-stream,unavailable,cancelled,resource-exhausted,retriable-status-codes\", \"numRetries\": 2, \"retryHostPredicate\": [ { \"name\": \"envoy.retry_host_predicates.previous_hosts\" } ], \"hostSelectionRetryMaxAttempts\": \"5\", \"retriableStatusCodes\": [ 503 ] }, \"maxGrpcTimeout\": \"0s\" }, \"decorator\": { \"operation\": \"ratings.default.svc.cluster.local:9080/*\" } } ] }, ..] From this VirtualHost configuration, you can see routing traffic to the cluster outbound|9080||ratings.default.svc.cluster.local.\nEndpoint outbound|9080||ratings.default.svc.cluster.local\nRunning istioctl proxy-config endpoint reviews-v1-545db77b95-jkgv2 --port 9080 -o json --cluster \"outbound|9080||ratings.default.svc.cluster.local\" to view the Endpoint configuration, the results are as follows.\n{ \"clusterName\": \"outbound|9080||ratings.default.svc.cluster.local\", \"endpoints\": [ { \"locality\": { }, \"lbEndpoints\": [ { \"endpoint\": { \"address\": { \"socketAddress\": { \"address\": \"172.33.100.2\", \"portValue\": 9080 } } }, \"metadata\": { \"filterMetadata\": { \"istio\": { \"uid\": \"kubernetes://ratings-v1-8558d4458d-ns6lk.default\" } } } } ] } ] } We see that the endpoint address is 10.4.1.12. In fact, the Endpoint can be one or more, and the sidecar will select the appropriate Endpoint to route based on certain rules. At this point the review Pod has found the Endpoint for its upstream service rating.\nSummary This article uses the bookinfo example provided by Istio to guide readers through the implementation details behind the sidecar injection, iptables transparent traffic hijacking, and traffic routing in the sidecar. The sidecar mode and traffic transparent hijacking are the features and basic functions of Istio service mesh, understanding the process behind this function and the implementation details will help you understand the principle of service mesh and the content in the later chapters of the Istio Handbook , so I hope readers can try it from scratch in their own environment to deepen their understanding.\nUsing iptables for traffic hijacking is just one of the ways to do traffic hijacking in the data plane of a service mesh, and there are many more traffic hijacking scenarios, quoted below from the description of the traffic hijacking section given in the MOSN official network of the cloud-native network proxy.\nProblems with using iptables for traffic hijacking Currently, Istio uses iptables for transparent hijacking and there are three main problems.\n The need to use the conntrack module for connection tracking, in the case of a large number of connections, will cause a large consumption and may cause the track table to be full, in order to avoid this problem, the industry has a practice of closing conntrack. iptables is a common module with global effect and cannot explicitly prohibit associated changes, which is less controllable. iptables redirect traffic is essentially exchanging data via a loopback. The outbound traffic will traverse the protocol stack twice and lose forwarding performance in a large concurrency scenario.  Several of the above problems are not present in all scenarios, let’s say some scenarios where the number of connections is not large and the NAT table is not used, iptables is a simple solution that meets the requirements. In order to adapt to a wider range of scenarios, transparent hijacking needs to address all three of these issues.\nTransparent hijacking optimization In order to optimize the performance of transparent traffic hijacking in Istio, the following solutions have been proposed by the industry.\nTraffic Hijacking with eBPF using the Merbridge Open Source Project\nMerbridge is a plug-in that leverages eBPF to accelerate the Istio service mesh, which was open sourced by DaoCloud in early 2022. Using Merbridge can optimize network performance in the data plane to some extent.\nMerbridge leverages the sockops and redir capabilities of eBPF to transfer packets directly from inbound sockets to outbound sockets. eBPF provides the bpf_msg_redirect_hash function to forward application packets directly.\nHandling inbound traffic with tproxy\ntproxy can be used for redirection of inbound traffic without changing the destination IP/port in the packet, without performing connection tracking, and without the problem of conntrack modules creating a large number of connections. Restricted to the kernel version, tproxy’s application to outbound is flawed. Istio currently supports handling inbound traffic via tproxy.\nUse hook connect to handle outbound traffic\nIn order to adapt to more application scenarios, the outbound direction is implemented by hook connect, which is implemented as follows.\n Hook Connect Diagram   Whichever transparent hijacking scheme is used, the problem of obtaining the real destination IP/port needs to be solved, using the iptables scheme through getsockopt, tproxy can read the destination address directly, by modifying the call interface, hook connect scheme reads in a similar way to tproxy.\nAfter the transparent hijacking, the sockmap can shorten the packet traversal path and improve forwarding performance in the outbound direction, provided that the kernel version meets the requirements (4.16 and above).\nReferences  Debugging Envoy and Istiod - istio.io  Demystifying Istio’s Sidecar Injection Model - istio.io  The traffic hijacking solution when MOSN is used as a sidecar - mosn.io   ","permalink":"https://jimmysong.io/en/blog/sidecar-injection-iptables-and-traffic-routing/","summary":"This blog describes the sidecar pattern, transparent traffic hijacking and routing.","title":"Sidecar injection, transparent traffic hijacking , and routing process in Istio explained in detail"},{"content":"This article will explain:\n The sidecar auto-injection process in Istio The init container startup process in Istio The startup process of a Pod with Sidecar auto-injection enabled  The following figure shows the components of a Pod in the Istio data plane after it has been started.\nIstio data plane pod  Sidecar injection in Istio The following two sidecar injection methods are available in Istio.\n Manual injection using istioctl. Kubernetes-based mutating webhook admission controller automatic sidecar injection method.  Whether injected manually or automatically, SIDECAR’s injection process follows the following steps.\n Kubernetes needs to know the Istio cluster to which the sidecar to be injected is connected and its configuration. Kubernetes needs to know the configuration of the sidecar container itself to be injected, such as the image address, boot parameters, etc. Kubernetes injects the above configuration into the side of the application container by the sidecar injection template and the configuration parameters of the above configuration-filled sidecar.  The sidecar can be injected manually using the following command.\nistioctl kube-inject -f ${YAML_FILE} | kuebectl apply -f - This command is injected using Istio’s built-in sidecar configuration, see the Istio official website for details on how to use Istio below.\nWhen the injection is complete you will see that Istio has injected initContainer and sidecar proxy-related configurations into the original pod template.\nInit container The Init container is a dedicated container that runs before the application container is launched and is used to contain some utilities or installation scripts that do not exist in the application image.\nMultiple Init containers can be specified in a Pod, and if more than one is specified, the Init containers will run sequentially. The next Init container can only be run if the previous Init container must run successfully. Kubernetes only initializes the Pod and runs the application container when all the Init containers have been run.\nThe Init container uses Linux Namespace, so it has a different view of the file system than the application container. As a result, they can have access to Secret in a way that application containers cannot.\nDuring Pod startup, the Init container starts sequentially after the network and data volumes are initialized. Each container must be successfully exited before the next container can be started. If exiting due to an error will result in a container startup failure, it will retry according to the policy specified in the Pod’s restartPolicy. However, if the Pod’s restartPolicy is set to Always, the restartPolicy is used when the Init container failed.\nThe Pod will not become Ready until all Init containers are successful. The ports of the Init containers will not be aggregated in the Service. The Pod that is being initialized is in the Pending state but should set the Initializing state to true. The Init container will automatically terminate once it is run.\nSidecar injection example analysis For a detailed YAML configuration for the bookinfo applications, see bookinfo.yaml for the official Istio YAML of productpage in the bookinfo sample.\nThe following will be explained in the following terms.\n Injection of Sidecar containers Creation of iptables rules The detailed process of routing  apiVersion:apps/v1kind:Deploymentmetadata:name:productpage-v1labels:app:productpageversion:v1spec:replicas:1selector:matchLabels:app:productpageversion:v1template:metadata:labels:app:productpageversion:v1spec:serviceAccountName:bookinfo-productpagecontainers:- name:productpageimage:docker.io/istio/examples-bookinfo-productpage-v1:1.15.0imagePullPolicy:IfNotPresentports:- containerPort:9080volumeMounts:- name:tmpmountPath:/tmpvolumes:- name:tmpemptyDir:{}Let’s see the productpage container’s Dockerfile .\nFROMpython:3.7.4-slimCOPY requirements.txt ./RUN pip install --no-cache-dir -r requirements.txtCOPY test-requirements.txt ./RUN pip install --no-cache-dir -r test-requirements.txtCOPY productpage.py /opt/microservices/COPY tests/unit/* /opt/microservices/COPY templates /opt/microservices/templatesCOPY static /opt/microservices/staticCOPY requirements.txt /opt/microservices/ARG flood_factorENV FLOOD_FACTOR ${flood_factor:-0}EXPOSE9080WORKDIR/opt/microservicesRUN python -m unittest discoverUSER1CMD [\"python\", \"productpage.py\", \"9080\"]We see that ENTRYPOINT is not configured in Dockerfile, so CMD’s configuration python productpage.py 9080 will be the default ENTRYPOINT, keep that in mind and look at the configuration after the sidecar injection.\n$ istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml We intercept only a portion of the YAML configuration that is part of the Deployment configuration associated with productpage.\ncontainers:- image:docker.io/istio/examples-bookinfo-productpage-v1:1.15.0# application imagename:productpageports:- containerPort:9080- args:- proxy- sidecar- --domain- $(POD_NAMESPACE).svc.cluster.local- --configPath- /etc/istio/proxy- --binaryPath- /usr/local/bin/envoy- --serviceCluster- productpage.$(POD_NAMESPACE)- --drainDuration- 45s- --parentShutdownDuration- 1m0s- --discoveryAddress- istiod.istio-system.svc:15012- --zipkinAddress- zipkin.istio-system:9411- --proxyLogLevel=warning- --proxyComponentLogLevel=misc:error- --connectTimeout- 10s- --proxyAdminPort- \"15000\"- --concurrency- \"2\"- --controlPlaneAuthPolicy- NONE- --dnsRefreshRate- 300s- --statusPort- \"15020\"- --trust-domain=cluster.local- --controlPlaneBootstrap=falseimage:docker.io/istio/proxyv2:1.5.1# sidecar proxyname:istio-proxyports:- containerPort:15090name:http-envoy-promprotocol:TCPinitContainers:- command:- istio-iptables- -p- \"15001\"- -z- \"15006\"- -u- \"1337\"- -m- REDIRECT- -i- '*'- -x- \"\"- -b- '*'- -d- 15090,15020image:docker.io/istio/proxyv2:1.5.1# init containername:istio-initIstio’s configuration for application Pod injection mainly includes:\n Init container istio-init: for setting iptables port forwarding in the pod Sidecar container istio-proxy: running a sidecar proxy, such as Envoy or MOSN  The two containers will be parsed separately.\nInit container analysis The Init container that Istio injects into the pod is named istio-init, and we see in the YAML file above after Istio’s injection is complete that the init command for this container is.\nistio-iptables -p 15001 -z 15006 -u 1337 -m REDIRECT -i '*' -x \"\" -b '*' -d 15090,15020 Let’s check the container’s Dockerfile again to see how ENTRYPOINT determines what commands are executed at startup.\n# ommit# The pilot-agent will bootstrap Envoy.ENTRYPOINT [\"/usr/local/bin/pilot-agent\"]We see that the entrypoint of the istio-init container is the /usr/local/bin/istio-iptables command line, and the location of the code for this command-line tool is in the tools/istio-iptables directory of the Istio source code repository.\nInit container initiation The Init container’s entrypoint is the istio-iptables command line, which is used as follows.\nUsage: istio-iptables [flags] Flags: -n, --dry-run Do not call any external dependencies like iptables -p, --envoy-port string Specify the envoy port to which redirect all TCP traffic (default $ENVOY_PORT = 15001) -h, --help help for istio-iptables -z, --inbound-capture-port string Port to which all inbound TCP traffic to the pod/VM should be redirected to (default $INBOUND_CAPTURE_PORT = 15006) --iptables-probe-port string set listen port for failure detection (default \"15002\") -m, --istio-inbound-interception-mode string The mode used to redirect inbound connections to Envoy, either \"REDIRECT\" or \"TPROXY\" -b, --istio-inbound-ports string Comma separated list of inbound ports for which traffic is to be redirected to Envoy (optional). The wildcard character \"*\" can be used to configure redirection for all ports. An empty list will disable -t, --istio-inbound-tproxy-mark string -r, --istio-inbound-tproxy-route-table string -d, --istio-local-exclude-ports string Comma separated list of inbound ports to be excluded from redirection to Envoy (optional). Only applies when all inbound traffic (i.e. \"*\") is being redirected (default to $ISTIO_LOCAL_EXCLUDE_PORTS) -o, --istio-local-outbound-ports-exclude string Comma separated list of outbound ports to be excluded from redirection to Envoy -i, --istio-service-cidr string Comma separated list of IP ranges in CIDR form to redirect to envoy (optional). The wildcard character \"*\" can be used to redirect all outbound traffic. An empty list will disable all outbound -x, --istio-service-exclude-cidr string Comma separated list of IP ranges in CIDR form to be excluded from redirection. Only applies when all outbound traffic (i.e. \"*\") is being redirected (default to $ISTIO_SERVICE_EXCLUDE_CIDR) -k, --kube-virt-interfaces string Comma separated list of virtual interfaces whose inbound traffic (from VM) will be treated as outbound --probe-timeout duration failure detection timeout (default 5s) -g, --proxy-gid string Specify the GID of the user for which the redirection is not applied. (same default value as -u param) -u, --proxy-uid string Specify the UID of the user for which the redirection is not applied. Typically, this is the UID of the proxy container -f, --restore-format Print iptables rules in iptables-restore interpretable format (default true) --run-validation Validate iptables --skip-rule-apply Skip iptables apply The above incoming parameters are reassembled into iptables rules. For more information on how to use this command, visit tools/istio-iptables/pkg/cmd/root.go.\nThe significance of the container’s existence is that it allows the sidecar agent to intercept all inbound and outbound traffic to the pod, redirect all inbound traffic to port 15006 (sidecar) except port 15090 (used by Prometheus) and port 15092 (Ingress Gateway), and then intercept outbound traffic from the application container which is processed by sidecar (listening through port 15001) and then outbound. See the official Istio documentation for port usage in Istio.\nCommand analysis\nHere is the purpose of this start-up command.\n Forward all traffic from the application container to port 15006 of the sidecar. Run with the istio-proxy user identity, with a UID of 1337, the userspace where the sidecar is located, which is the default user used by the istio-proxy container, see the runAsUser field of the YAML configuration. Use the default REDIRECT mode to redirect traffic. Redirect all outbound traffic to the sidecar proxy (via port 15001).  Because the Init container is automatically terminated after initialization, since we cannot log into the container to view the iptables information, the Init container initialization results are retained in the application container and sidecar container.\nPod Startup Sequence The startup process of a Pod with Sidecar auto-injection enabled is as follows.\n The Init container starts first, injecting iptables rules into the Pod for transparent traffic interception. Subsequently, Kubernetes starts the containers in the order in which they are declared in the Pod Spec, but this is non-blocking and there is no guarantee that the first container will be started before the next one is started. istio-proxy container starts, pilot-agent will be the PID 1 process, which is the first process in the Linux user space and is responsible for pulling up other processes and handling zombie processes. The pilot-agent generates the Envoy bootstrap configuration and fork the envoy process; the application container is started almost simultaneously with the istio-proxy container, and the readiness probe comes in handy to prevent the container inside the Pod from receiving outside traffic before it is ready to start. Kubernetes will perform a readiness check on port 15021 of the istio-proxy container, and the kubelet will not route traffic to the Pod until the isito-proxy has finished booting. After the Pod is started, the pilot-agent becomes a daemon that monitors the rest of the system and provides Envoy with Bootstrap configuration, certificates, health checks, configuration hot reloading, identity support, and process lifecycle management, among other things.  Pod container startup order problem In the process of Pod startup there is a container startup order problem. Suppose the following situation, the application container starts first and requests other services, when the istio-proxy container has not finished starting, then the request will fail, and if your application is not robust enough, it may even cause the application container to crash and the Pod to restart. The solution for this situation is to\n Modify the application to add timeout retries. Increase the start delay of the process in the application container, for example by increasing the sleep time. Add a postStart configuration to the application container to detect if the application process has finished starting, and Kubernetes will only mark the Pod’s state as Running if the detection is successful.  Summary This article walks you through the process of starting Pods in the Istio data plane, and the issues that arise because of the order in which Pod contenters are started.\n","permalink":"https://jimmysong.io/en/blog/istio-pod-process-lifecycle/","summary":"This article will explain Istio's Init container, Pod internal processes and the startup process.","title":"Istio data plane pod startup process explained"},{"content":"iptables is an important feature in the Linux kernel and has a wide range of applications. iptables is used by default in Istio for transparent traffic hijacking. Understanding iptables is very important for us to understand how Istio works. This article will give you a brief introduction to iptbles.\niptables introduction iptables is a management tool for netfilter, the firewall software in the Linux kernel. netfilter is located in the user space and is part of netfilter. netfilter is located in the kernel space and has not only network address conversion, but also packet content modification and packet filtering firewall functions.\nBefore learning about iptables for Init container initialization, let’s go over iptables and rule configuration.\nThe following figure shows the iptables call chain.\niptables 调用链  iptables The iptables version used in the Init container is v1.6.0 and contains 5 tables.\n RAW is used to configure packets. Packets in RAW are not tracked by the system. The filter is the default table used to house all firewall-related operations. NAT is used for network address translation (e.g., port forwarding). Mangle is used for modifications to specific packets (refer to corrupted packets). Security is used to force access to control network rules.  Note: In this example, only the NAT table is used.\nThe chain types in the different tables are as follows.\n   Rule name raw filter nat mangle security     PREROUTING ✓  ✓ ✓    INPUT  ✓  ✓ ✓   OUTPUT ✓ ✓ ✓ ✓ ✓   POSTROUTING   ✓ ✓    FORWARD  ✓  ✓ ✓    Understand iptables rules View the default iptables rules in the istio-proxy container, the default view is the rules in the filter table.\n$ iptables -L -v Chain INPUT (policy ACCEPT 350K packets, 63M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 18M packets, 1916M bytes) pkts bytes target prot opt in out source destination We see three default chains, INPUT, FORWARD, and OUTPUT, with the first line of output in each chain indicating the chain name (INPUT/FORWARD/OUTPUT in this case), followed by the default policy (ACCEPT).\nThe following is a proposed structure diagram of iptables, where traffic passes through the INPUT chain and then enters the upper protocol stack, such as:\niptables chains  Multiple rules can be added to each chain and the rules are executed in order from front to back. Let’s look at the table header definition of the rule.\n PKTS: Number of matched messages processed bytes: cumulative packet size processed (bytes) Target: If the message matches the rule, the specified target is executed. PROT: Protocols such as TDP, UDP, ICMP, and ALL. opt: Rarely used, this column is used to display IP options. IN: Inbound network interface. OUT: Outbound network interface. source: the source IP address or subnet of the traffic, the latter being anywhere. destination: the destination IP address or subnet of the traffic, or anywhere.  There is also a column without a header, shown at the end, which represents the options of the rule, and is used as an extended match condition for the rule to complement the configuration in the previous columns. prot, opt, in, out, source and destination and the column without a header shown after destination together form the match rule. TARGET is executed when traffic matches these rules.\nTypes supported by TARGET\nTarget types include ACCEPT, REJECT, DROP, LOG, SNAT, MASQUERADE, DNAT, REDIRECT, RETURN or jump to other rules, etc. You can determine where the telegram is going by executing only one rule in a chain that matches in order, except for the RETURN type, which is similar to the return statement in programming languages, which returns to its call point and continues to execute the next rule.\nFrom the output, you can see that the Init container does not create any rules in the default link of iptables, but instead creates a new link.\nSummary With the above brief introduction to iptables, you have understood how iptables works, the rule chain and its execution order.\n","permalink":"https://jimmysong.io/en/blog/understanding-iptables/","summary":"This article will give you a brief introduction to iptbles, its tables and the order of execution.","title":"Understanding iptalbes"},{"content":"See the cloud native public library at: https://jimmysong.io/docs/ The cloud native public library project is a documentation project built using the Wowchemy theme, open sourced on GitHub .\nI have also adjusted the home page, menu and directory structure of the site, and the books section of the site will be maintained using the new theme.\nCloud native library positioning The cloud native public library is a collection of cloud native related books and materials published and translated by the author since 2017, and is a compendium and supplement to the dozen or so books already published. The original materials will continue to be published in the form of GitBooks, and the essence and related content will be sorted into the cloud native public library through this project.\nIn addition, the events section of this site has been revamped and moved to a new page .\n","permalink":"https://jimmysong.io/en/notice/cloud-native-public-library/","summary":"A one-stop cloud native library that is a compendium of published materials.","title":"Cloud Native library launch"},{"content":"In my last two blogs:\n Sidecar injection, transparent traffic hijacking , and routing process in Istio explained in detail  Traffic types and iptables rules in Istio sidecar explained   I gave you a detailed overview of the traffic in the Istio data plane, but the data plane does not exist in isolation. This article will show you the ports and their usages for each component of both the control plane and data plane in Istio, which will help you understand the relationship between these flows and troubleshoot them.\nOverview Firstly, I will show you a global schematic. The following figure shows the components of a sidecar in the Istio data plane, and the objects that interact with it.\nIstio components  We can use the nsenter command to enter the namespace of the productpage Pod of the Bookinfo example and see the information about the ports it is listening on internally.\nIstio sidecar ports  From the figure, we can see that besides the port 9080 that the productpage application listens to, the Sidecar container also listens to a large number of other ports, such as 15000, 15001, 15004, 15006, 15021, 15090, etc. You can learn about the ports used in Istio in the Istio documentation .\nLet’s go back into the productpage Pod and use the lsof -i command to see the ports it has open, as shown in the following figure.\nProductpage Pod ports  We can see that there is a TCP connection established between the pilot-agent and istiod, the port in the listening described above, and the TCP connection established inside the Pod, which corresponds to the figure at the beginning of the article.\nThe root process of the Sidecar container (istio-proxy) is pilot-agent, and the startup command is shown below.\nInternal procecces in Sidecar  As we can see from the figure, the PID of its pilot-agent process is 1, and it forked the Envoy process.\nCheck the ports it opens in Istiod, as shown in the figure below.\nIstiod ports  We can see the ports that are listened to, the inter-process and remote communication connections.\nPorts usage overview These ports can play a pivotal role when you are troubleshooting. They are described below according to the component and function in which the port is located.\nPorts in Istiod The ports in Istiod are relatively few and single-function.\n 9876: ControlZ user interface, exposing information about Istiod’s processes 8080: Istiod debugging port, through which the configuration and status information of the grid can be queried 15010: Exposes the xDS API and issues plain text certificates 15012: Same functionality as port 15010, but uses TLS communication 15014: Exposes control plane metrics to Prometheus 15017: Sidecar injection and configuration validation port  Ports in sidecar From the above, we see that there are numerous ports in the sidecar.\n 15000: Envoy admin interface, which you can use to query and modify the configuration of Envoy Proxy. Please refer toEnvoy documentation for details. 15001: Used to handle outbound traffic. 15004: Debug port (explained further below). 15006: Used to handle inbound traffic. 15020: Summarizes statistics, perform health checks on Envoy and DNS agents, and debugs pilot-agent processes, as explained in detail below. 15021: Used for sidecar health checks to determine if the injected Pod is ready to receive traffic. We set up the readiness probe on the /healthz/ready path on this port, and Istio hands off the sidecar readiness checks to kubelet. 15053: Local DNS proxy for scenarios where the cluster’s internal domain names are not resolved by Kubernetes DNS. 15090: Envoy Prometheus query port, through which the pilot-agent will scratch metrics.  The above ports can be divided into the following categories.\n Responsible for inter-process communication, such as 15001, 15006, 15053 Health check and information statistics, e.g. 150021, 15090 Debugging: 15000, 15004  Let’s look at the key ports in detail.\n15000 15000 is Envoy’s Admin interface, which allows us to modify Envoy and get a view and query metrics and configurations.\nThe Admin interface consists of a REST API with multiple endpoints and a simple user interface. You can enable the Envoy Admin interface view in the productpage Pod using the following command:\nkubectl -n default port-forward deploy/productpage-v1 15000 Visit http://localhost:15000 in your browser and you will see the Envoy Admin interface as shown below.\nEnvoy Admin interface  15004 With the pilot-agent proxy istiod debug endpoint on port 8080, you can access localhost’s port 15004 in the data plane Pod to query the grid information, which has the same effect as port 8080 below.\n8080 You can also forward istiod port 8080 locally by running the following command:\nkubectl -n istio-system port-forward deploy/istiod 8080 Visit http://localhost:8080/debug in your browser and you will see the debug endpoint as shown in the figure below.\nPilot Debug Console  Of course, this is only one way to get the mesh information and debug the mesh, you can also use istioctl command or Kiali to debug it, which will be more efficient and intuitive.\n15020 Port 15020 has three main usages.\n Aggregating metrics: You can query port 15090 for Envoy’s metrics, or you can configure it to query the application’s metrics, aggregating Envoy, application, and its own metrics for Prometheus to collect. The corresponding debug endpoint is /stats/prometheus. Performing health checks on Envoy and DNS agent: the corresponding debug endpoints are /healthz/ready and /app-health. Debugging pilot-agent processes: the corresponding debug endpoints are /quitquitquit, debug/ndsz and /debug/pprof.  The following figure shows the debugging information you see when you open http://localhost:15020/debug/pprof in your browser.\npprof endpoint  The information in the figure shows the stack information of the pilot-agent.\nSummary By understanding the component ports in Istio, you should have a better understanding of the relationship between the components in Istio and their internal traffic. Being familiar with the functions of these ports will help in troubleshooting the mesh.\n","permalink":"https://jimmysong.io/en/blog/istio-components-and-ports/","summary":"This article will introduce you to the various ports and functions of the Istio control plane and data plane.","title":" Istio component ports and functions in detail"},{"content":"As we know that Istio uses iptables for traffic hijacking, where the iptables rule chains has one called ISTIO_OUTPUT, which contains the following rules.\n   Rule target in out source destination     1 RETURN any lo 127.0.0.6 anywhere   2 ISTIO_IN_REDIRECT any lo anywhere !localhost owner UID match 1337   3 RETURN any lo anywhere anywhere !owner UID match 1337   4 RETURN any any anywhere anywhere owner UID match 1337   5 ISTIO_IN_REDIRECT any lo anywhere !localhost owner GID match 1337   6 RETURN any lo anywhere anywhere !owner GID match 1337   7 RETURN any any anywhere anywhere owner GID match 1337   8 RETURN any any anywhere localhost   9 ISTIO_REDIRECT any any anywhere anywhere    The sidecar applies these rules to deal with different types of traffic. This article will show you the six types of traffic and their iptables rules in Istio sidecar.\niptables Traffic Routing in Sidecar The following list summarizes the six types of traffic in Sidecar.\n Remote service accessing local service: Remote Pod -\u003e Local Pod Local service accessing remote service: Local Pod -\u003e Remote Pod Prometheus crawling metrics of local service: Prometheus -\u003e Local Pod Traffic between Local Pod service: Local Pod -\u003e Local Pod Inter-process TCP traffic within Envoy Sidecar to Istiod traffic  The following will explain the iptables routing rules within Sidecar for each scenario, which specifies which rule in ISTIO_OUTPUT is used for routing.\nType 1: Remote Pod -\u003e Local Pod The following are the iptables rules for remote services, applications or clients accessing the local pod IP of the data plane.\nRemote Pod -\u003e RREROUTING -\u003e ISTIO_INBOUND -\u003e ISTIO_IN_REDIRECT -\u003e Envoy 15006 (Inbound) -\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 1 -\u003e  POSTROUTING -\u003e Local Pod\nWe see that the traffic only passes through the Envoy 15006 Inbound port once. The following diagram shows this scenario of the iptables rules.\nRemote Pod to Local Pod  Type 2: Local Pod -\u003e Remote Pod The following are the iptables rules that the local pod IP goes through to access the remote service.\nLocal Pod-\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 9 -\u003e ISTIO_REDIRECT -\u003e Envoy 15001 (Outbound) -\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 4 -\u003e POSTROUTING -\u003e Remote Pod\nWe see that the traffic only goes through the Envoy 15001 Outbound port.\nLocal Pod to Remote Pod  The traffic in both scenarios above passes through Envoy only once because only one scenario occurs in that Pod, sending or receiving requests.\nType 3: Prometheus -\u003e Local Pod Prometheus traffic that grabs data plane metrics does not have to go through the Envoy proxy.\nThese traffic pass through the following iptables rules.\nPrometheus-\u003e RREROUTING -\u003e ISTIO_INBOUND (traffic destined for ports 15002, 15090 will go to INPUT) -\u003e INPUT -\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 3 -\u003e POSTROUTING -\u003e Local Pod\nPrometheus to Local Pod  Type 4: Local Pod -\u003e Local Pod A Pod may simultaneously have two or more services. If the Local Pod accesses a service on the current Pod, the traffic will go through Envoy 15001 and Envoy 15006 ports to reach the service port of the Local Pod.\nThe iptables rules for this traffic are as follows.\nLocal Pod-\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 9 -\u003e ISTIO_REDIRECT -\u003e Envoy 15001（Outbound）-\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 2 -\u003e ISTIO_IN_REDIRECT -\u003e Envoy 15006（Inbound）-\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 1 -\u003e POSTROUTING -\u003e Local Pod\nLocal Pod to Local Pod  Type 5: Inter-process TCP traffic within Envoy Envoy internal processes with UID and GID 1337 will communicate with each other using lo NICs and localhost domains.\nThe iptables rules that these flows pass through are as follows.\nEnvoy process (Localhost) -\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 8 -\u003e POSTROUTING -\u003e Envoy process (Localhost)\nEnvoy inter-process TCP traffic  Type 6: Sidecar to Istiod traffic Sidecar needs access to Istiod to synchronize its configuration so that Envoy will have traffic sent to Istiod.\nThe iptables rules that this traffic passes through are as follows.\npilot-agent process -\u003e OUTPUT -\u003e Istio_OUTPUT RULE 9 -\u003e Envoy 15001 (Outbound Handler) -\u003e OUTPUT -\u003e ISTIO_OUTPUT RULE 4 -\u003e POSTROUTING -\u003e Istiod\nSidecar to Istiod  Summary All the sidecar proxies that Istio injects into the Pod or installed in the virtual machine form the data plane of the service mesh, which is also the main workload location of Istio. In my next blog, I will take you through the ports of each component in Envoy and their functions, so that we can have a more comprehensive understanding of the traffic in Istio.\n","permalink":"https://jimmysong.io/en/blog/istio-sidecar-traffic-types/","summary":"This article will show you the six traffic types and their iptables rules in Istio sidecar, and take you through the whole diagram in a schematic format.","title":"Traffic types and iptables rules in Istio sidecar explained"},{"content":"Istio 1.13 is the first release of 2022, and, not surprisingly, the Istio team will continue to release new versions every quarter. Overall, the new features in this release include:\n Support for newer versions of Kubernetes New API – ProxyConfig, for configuring sidecar proxies Improved Telemetry API Support for hostname-based load balancers with multiple network gateways  Support for Kubernetes Versions I often see people asking in the community which Istio supports Kubernetes versions. Istio’s website has a clear list of supported Kubernetes versions. You can see here that Istio 1.13 supports Kubernetes versions 1.20, 1.21, 1.22, and 1.23, and has been tested but not officially supported in Kubernetes 1.16, 1.17, 1.18, 1.19.\nWhen configuring Istio, there are a lot of checklists. I noted them all in the Istio cheatsheet . There are a lot of cheat sheets about configuring Istio, using resources, dealing with everyday problems, etc., in this project, which will be online soon, so stay tuned.\nThe following screenshot is from the Istio cheatsheet website, it shows the basic cheat sheet for setting up Istio.\nIstio cheatsheet  Introducing the new ProxyConfig API Before Istio version 1.13, if you wanted to customize the configuration of the sidecar proxy, there were two ways to do it.\nMeshConfig\nUse MeshConfig and use IstioOperator to modify it at the Mesh level. For example, use the following configuration to alter the default discovery port for istiod.\napVersion:install.istio.io/v1alpha1kind:IstioOperatorspec:meshConfig:defaultConfig:discoveryAddress:istiod:15012Annotation in the Pods\nYou can also use annotation at the Pod level to customize the configuration. For example, you can add the following annotations to Pod to modify the default port for istiod of the workload:\nanannotations:proxy.istio.io/config:|discoveryAddress:istiod:15012When you configure sidecar in either of these ways, the fields set in annotations will completely override the default fields in MeshConfig. Please refer to the Istio documentation for all configuration items of ProxyConfig.\nThe new API – ProxyConfig\nBut in 1.13, a new top-level custom resource, ProxyConfig, has been added, allowing you to customize the configuration of your sidecar proxy in one place by specifying a namespace and using a selector to select the scope of the workload, just like any other CRD. Istio currently has limited support for this API, so please refer to the Istio documentation for more information on the ProxyConfig API.\nHowever, no matter which way you customize the configuration of the sidecar proxy, it does not take effect dynamically and requires a workload restart to take effect. For example, for the above configuration, because you changed the default port of istiod, all the workloads in the mesh need to be restarted before connecting to the control plane.\nTelemetry API MeshConfig customized extensions and configurations in the Istio mesh. The three pillars of observability– Metrics, Telemetry, and Logging– can each be docked to different providers. The Telemetry API gives you a one-stop place for flexible configuration of them. Like the ProxyConfig API, the Telemetry API follows the configuration hierarchy of Workload Selector \u003e Local Namespace \u003e Root Configuration Namespace. The API was introduced in Istio 1.11 and has been further refined in that release to add support for OpenTelemetry logs, filtered access logs, and custom tracing service names. See Telemetry Configuration for details.\nAutomatic resolution of multi-network gateway hostnames In September 2021, a member of the Istio community reported an issue with the EKS load balancer failing to resolve when running multi-cluster Istio in AWS EKS. Workloads that cross cluster boundaries need to be communicated indirectly through a dedicated east-west gateway for a multi-cluster, multi-network mesh. You can follow the instructions on Istio’s website to configure a multi-network, primary-remote cluster, and Istio will automatically resolve the IP address of the load balancer based on the hostname.\nIstio 1.13.1 fixing the critical security vulnerabilities Istio 1.13.1 was released to fix a known critical vulnerability that could lead to an unauthenticated control plane denial of service attack.\nThe figure below shows a multi-cluster primary-remote mesh where istiod exposes port 15012 to the public Internet via a gateway so that a pod on another network can connect to it.\nMulti-network Mesh  When installing a multi-network, primary-remote mode Istio mesh, for a remote Kubernetes cluster to access the control plane, an east-west Gateway needs to be installed in the Primary cluster, exposing port 15012 of the control plane istiod to the Internet. An attacker could send specially crafted messages to that port, causing the control plane to crash. If you set up a firewall to allow traffic from only some IPs to access this port, you will be able to reduce the impact of the problem. It is recommended that you upgrade to Istio 1.13.1 immediately to resolve the issue completely.\nIstioCon 2022 IstioCon 2022  Finally, as a committee member for the last and current IstioCon, I call on everyone to register for IstioCon 2022 , which will be held online on April 25! It will be an industry-focused event, a platform to connect contributors and users to discuss the uses of Istio in different architectural setups, its limitations, and where to take the project next. The main focus on end-user companies, as we look forward to sharing a diversity of case studies showing how to use Istio in production.\n","permalink":"https://jimmysong.io/en/blog/what-is-new-in-istio-1-13/","summary":"In February 2022, Istio released 1.13.0 and 1.13.1. This blog will give you an overview of what’s new in these two releases.","title":"What's new in Istio 1.13?"},{"content":"Join a team of world-class engineers working on the next generation of networking services using Istio, Envoy, Apache SkyWalking and a few of the open projects to define the next generation of cloud native network service.\nIstio upstream contributor: Golang We are looking for engineers with strong distributed systems experience to join our team. We are building a secure, robust, and highly available service mesh platform for mission critical enterprise applications spanning both legacy and modern infrastructure. This is an opportunity to dedicate a significant amount of contribution to Istio upstream on a regular basis. If you are a fan of Istio and would like to increase your contribution on a dedicated basis, this would be an opportunity for you.\nRequirements\n Fundamentals-based problem solving skills; Drive decision by function, first principles based mindset. Demonstrate bias-to-action and avoid analysis-paralysis; Drive action to the finish line and on time. You are ego-less when searching for the best ideasIntellectually curious with a penchant for seeing opportunities in ambiguity Understands the difference between attention to detail vs. detailed - oriented Values autonomy and results over process You contribute effectively outside of your specialty Experience building distributed system platforms using Golang Familiarity with Kubernetes, service mesh technologies such as Istio and Envoy Excellent understanding of networking protocols, concepts, consistency properties of distributed systems, techniques to identify and reconcile configuration drifts Experience contributing to open source projects is a plus. Familiarity with WebAssembly is a plus. Familiarity with Golang, hardware/software load balancers (F5, NGINX), HSM modules, Active Directory/LDAP is a plus.  We encourage written and asynchronous communication in English, and proficient oral English is not required.\nDistributed Systems Engineer, Enterprise Infrastructure (Data plane) GoLang or C++ Developers Seeking backend software engineers experienced in building distributed systems using Golang and gRPC. We are building a secure, and highly available service mesh platform for mission-critical enterprise applications for Fortune 500 companies, spanning both the legacy and modern infrastructure. Should possess strong fundamentals in distributed systems and networking. Familiarity with technologies like Kubernetes, Istio, and Envoy, as well as open contributions would be a plus.\nRequirements\n Experience building distributed system platforms using C++, Golang, and gRPC. Familiarity with Kubernetes, service mesh technologies such as Istio and Envoy. Excellent understanding of networking protocols, concepts, consistency properties of distributed systems, techniques to identify and reconcile configuration drifts. Experience contributing to open source projects is a plus. Familiarity with the following is a plus: WebAssembly, Authorization: NGAC, RBAC, ABAC Familiarity with hardware/software load balancers (F5, NGINX), HSM modules, Active Directory/LDAP is a plus.  Site Reliability Engineer, SRE Site Reliability Engineering (SRE) combines software and systems engineering to build and run scalable, massively distributed, fault-tolerant systems. As part of the team, you will be working on ensuring that Tetrate’s platform has reliability/uptime appropriate to users’ needs as well as a fast rate of improvement. Additionally, much of our engineering effort focuses on building infrastructure, improving the platform troubleshooting abilities, and eliminating toil through automation.\nRequirements\n Systematic problem-solving approach, coupled with excellent communication skills and a sense of ownership/finish and self-directed drive. Strong fundamentals in operating, debugging, and troubleshooting distributed systems(stateful and/or stateless) and networking. Familiarity with Kubernetes, service mesh technologies such as Istio and EnvoyAbility to debug, optimize code, and automate routine tasks. Experience programming in at least one of the following languages: C++, Rust, Python, Go. Familiarity with the concepts of quantifying failure and availability in a prescriptive manner using SLOs and SLIs. Experience in performance analysis and tuning is a plus.  Location Worldwide\nWe are remote with presence in China, Indonesia, India, Japan, U.S., Canada, Ireland, the Netherlands, Spain, and Ukraine.\nPlease send GitHub or online links that showcase your code style along with your resume to careers@tetrate.io .\nAbout Tetrate Powered by Envoy and Istio, its ﬂagship product, Tetrate Service Bridge, enables bridging traditional and modern workloads. Customers can get consistent baked-in observability, runtime security and traffic management for all their workloads, in any environment.\nIn addition to the technology, Tetrate brings a world-class team that leads the open Envoy and Istio projects, providing best practices and playbooks that enterprises can use to modernize their people and processes.\nVarun , co-founder was the initial founder of Istio and gRPC back in Google. We are a two-year-old start-up building compelling network products and services that we believe will result in a step function change in the industry. JJ , co-founder, started the cloud infrastructure team (working alongside Mesos, VM, OS and Provisioning infrastructure teams – under platform team) at Twitter. What got him excited to leave Twitter to start this company is the ability to create a change in how service development happens in enterprises.\nTetrate was founded in Silicon Valley with a large number of our team represented in Canada,, China, India, Indonesia, Ireland, Japan, New Zealand, Singapore, Spain, the Netherlands and Ukraine. Our recruiting goal is simple: find the best talent no matter the background and location. We don’t recruit to a title or a role, instead, we focus on problems that need to be solved. It would be great to discuss overlaps and interests over a live call. You choose where you want to work, flexible time away and work schedule. We have teams get-togethers about 3-4 times a year, most recently in SF, Seattle, Barcelona, San Diego, Washington D.C. and Bandung. This helps with building rapport which then helps with collaboration when you are working virtually.\nTo learn more, pleases visit tetrate.io .\n","permalink":"https://jimmysong.io/en/notice/tetrate-recruit/","summary":"Remotely worldwide","title":"The Enterprise Service Mesh company Tetrate is hiring"},{"content":"As the service mesh architecture concept gains traction and the scenarios for its applications emerge, there is no shortage of discussions about it in the community. I have worked on service mesh with the community for 4 years now, and will summarize the development of service mesh in 2021 from this perspective. Since Istio is the most popular service mesh, this article will focus on the technical and ecological aspects of Istio.\nService mesh: a critical tech for Cloud Native Infrastructure As one of the vital technologies defined by CNCF for cloud native, Istio has been around for five years now. Their development has gone through the following periods.\n Exploration phase: 2017-2018 Early adopter phase: 2019-2020 Large-scale landing and ecological development phase: 2021-present  Service mesh has crossed the “chasm”(refer Crossing the Chasm theory) and is in between the “early majority” and “late majority” phases of adoption. Based on feedback from the audience of Istio Weekly, users are no longer blindly following new technologies for experimentation and are starting to consider whether they need them in their organization dialectically.\nCross the chasm  While new technologies and products continue to emerge, the service mesh, as part of the cloud native technology stack, has continued to solidify its position as the “cloud native network infrastructure” over the past year. The diagram below illustrates the cloud native technology stack model, where each layer has several representative technologies that define the standard. As new-age middleware, the service mesh mirrors other cloud native technologies, such as Dapr (Distributed Application Runtime), which represents the capability model for cloud native middleware, OAM , which defines the cloud native application model, and the service mesh, which defines the L7 network model.\nCloud Native Stack  A layered view of the cloud native application platform technology stack\nOptimizing the mesh for large scale production applications with different deployment models Over the past year, the community focused on the following areas.\n Performance optimization: performance issues of service mesh in large-scale application scenarios. Protocol and extensions: enabling service mesh to support arbitrary L7 network protocols. Deployment models: Proxyless vs. Node model vs. Sidecar model. eBPF: putting some of the service mesh’s capabilities to the kernel layer.  Performance optimization Istio was designed to serve service to service traffic by “proto-protocol forwarding”. The goal is making the service mesh as “transparent” as possible to applications. Thus using IPtables to hijack the traffic, according to the community-provided test results Istio 1.2 adds only 3 ms to the baseline latency for a mesh with 1000 RPS on 16 connections. However, because of issues inherent in the IPtables conntrack module, Istio’s performance issues begin to emerge as the mesh size increases. To optimize the performance of the Istio sidecar for resource usage and network latency, the community gave the following solutions.\n Sidecar configuration: By configuring service dependencies manually or by adding an Operator to the control plane, the number of service configurations sent to Sidecar can be reduced, thus reducing the resource footprint of the data plane; for more automatic and intelligent configuration of Sidecar, the open source projects Slime and Aeraki both offer their innovative configuration loading solutions. The introduction of eBPF: eBPF can be a viable solution to optimize the performance of the service mesh. Some Cilium-based startups even radically propose to use eBPF to replace the Sidecar proxy completely. Still, the Envoy proxy/xDS protocol has become the proxy for the service mesh implementation and supports the Layer 7 protocol very well. We can use eBPF to improve network performance, but complex protocol negotiation, parsing, and user scaling remain challenging to implement on the user side.  Protocol and extensions Extensibility of Istio has always been a significant problem, and there are two aspects to Istio’s extensibility.\n Protocol level: allowing Istio to support all L7 protocols Ecological: allowing Istio to run more extensions  Istio uses Envoy as its data plane. Extending Istio is essentially an extension of Envoy’s functionality. Istio’s official solution is to use WebAssembly, and in Istio 1.12, the Wasm plugin configuration API was introduced to extend the Istio ecosystem. Istio’s extension mechanism uses the Proxy-Wasm Application Binary Interface (ABI) specification to provide a set of proxy-independent streaming APIs and utilities that can be implemented in any language with an appropriate SDK. Today, Proxy-Wasm’s SDKs are AssemblyScript (similar to TypeScript), C++, Rust, Zig, and Go (using the TinyGo WebAssembly System Interface).\nThere are still relatively few WebAssembly extensions available, and many enterprises choose to customize their CRD and build a service mesh management plane based on Istio. In addition, making Istio support heterogeneous environments for all workloads, such as virtual machines and containers, is also in strong demand for end-users. It allows them to migrate applications from traditional loads to service mesh easily. Finally, there is the hybrid cloud traffic management with multiple clusters and mesh, which is a more advanced requirement.\nDeployment models When the service mesh concept first emerged, there was a debate between the Per-node and Sidecar models, represented by Linkerd and Istio. eBPF later proposed a kernel to sink the service mesh, which led to more service mesh deployment models, as shown in the figure below.\nService Mesh Deployment Models  These four deployment methods have their own advantages and disadvantages, the specific choice of which depends on the actual situation.\nDevelopment of the Istio ecosystem and the projects that support Istio 2021 was also an exciting year for the Istio community, with a series of events and tutorials.\n February, the first Istio distribution, Tetrate Istio Distro (TID) . February, the first IstioCon was held online, with over 2,000 participants. March, the first free online Istio Fundamentals Course is released. May, the first Certification Istio Administrator exam be released. May, ServiceMeshCon Europe was held online. July, Istio Meetup China was held in Beijing with more than 100 attendees. October, ServiceMeshCon North America was held in Los Angeles.  There are also numerous open source projects related to Istio Service Mesh, as shown in the table below.\n   Project Value Relationship with Istio Category Launch Date Dominant company Number of stars     Envoy  Cloud native high-performance edge/middle-service proxy The default data plane proxy September 2016 Lyft 18700   Istio  Connection, secure, control, and observation services. Control plane service mesh May 2017 Google 29100   Emissary Gateway  Kubernetes native API gateway for microservices, built on Envoy Connectable to Istio gateway February 2018 Ambassador 3600   APISIX  Cloud native API gateways It can run as a data plane for Istio or as a gateway on its own gateway June 2019 API7 8100   MOSN  Cloud native edge gateways \u0026 agents Available as Istio data plane proxy December 2019 Ant 3500   Slime  Intelligent service mesh manager based on Istio Adding a management plane to Istio extensions January 2021 NetEase 236   GetMesh  Istio integration and command-line management tools Utility for Istio multi-version management tools February 2021 Tetrate 95   Aeraki  Manage any of Istio’s seven layers of load Extended multi-protocol support extensions March 2021 Tencent 330   Layotto  Cloud native application runtime Using as a data plane for Istio runtime June 2021 Ant 393   Hango Gateway  API gateways built on Envoy and Istio Integrates with Istio gateway August 2021 NetEase 253    Note: Data is as of January 6, 2022\nSummary Looking back, we can see that, unlike previous years where users were experimenting, users in 2021 looked for more practical uses for service mesh before implementing them. Their position as the infrastructure of cloud native networks is further strengthened, and more importantly, the service mesh ecosystem is emerging. Looking ahead, in 2022, two technologies to watch are eBPF and WebAssembly(Wasm). We believe that more good examples of service mesh practices will emerge, taking the ecology and standardization a step further.\n","permalink":"https://jimmysong.io/en/blog/service-mesh-in-2021/","summary":"A review of the development of Service Mesh in 2021.","title":"Service Mesh in 2021: the ecosystem is emerging"},{"content":"It’s been more than four years since Istio launched in May 2017, and while the project has had a strong following on GitHub and 10+ releases, its growing open-source ecosystem is still in its infancy.\nRecently added support for WebAssembly extensions has made the most popular open source service mesh more extensible than ever. This table lists the open-source projects in the Istio ecosystem as of November 11, 2021, sorted by open-source date. These projects enhance the Istio service mesh with gateways, extensions, utilities, and more. In this article, I’ll highlight the two new projects in the category of extensions.\n   Project Value Relationship with Istio Category Launch Date Dominant company Number of stars     Envoy  Cloud native high-performance edge/middle-service proxy The default data plane proxy September 2016 Lyft 18700   Istio  Connection, secure, control, and observation services. Control plane service mesh May 2017 Google 29100   Emissary Gateway  Kubernetes native API gateway for microservices, built on Envoy Connectable to Istio gateway February 2018 Ambassador 3600   APISIX  Cloud native API gateways It can run as a data plane for Istio or as a gateway on its own gateway June 2019 API7 8100   MOSN  Cloud native edge gateways \u0026 agents Available as Istio data plane proxy December 2019 Ant 3500   Slime  Intelligent service mesh manager based on Istio Adding a management plane to Istio extensions January 2021 NetEase 236   GetMesh  Istio integration and command-line management tools Utility for Istio multi-version management tools February 2021 Tetrate 95   Aeraki  Manage any of Istio’s seven layers of load Extended multi-protocol support extensions March 2021 Tencent 330   Layotto  Cloud native application runtime Using as a data plane for Istio runtime June 2021 Ant 393   Hango Gateway  API gateways built on Envoy and Istio Integrates with Istio gateway August 2021 NetEase 253    Slime: an intelligent service mesh manager for Istio Slime is an Istio-based, intelligent mesh manager open-sourced by NetEase’s microservices team. Based on the Kubernetes Operator implementation, Slime can be used as a CRD manager that seamlessly interfaces with Istio without needing any customization or definition of dynamic service governance policies. This achieves automatic and convenient use of Istio and Envoy’s advanced features.\nSlime addresses the following issues:\n Implementing higher-level extensions in Istio. For example, extending the HTTP plugin; adaptive traffic limiting based on the resource usage of the service. Poor performance arising from Istio sending all the configurations within the mesh to each sidecar proxy.  Slime solves these problems by building an Istio management plane. Its main purpose are\n to build a pluggable controller to facilitate the extension of new functions. to obtain data by listening to the data plane to intelligently generate the configuration for Istio. to build a higher-level CRD for the user to configure, which Slime converts into an Istio configuration.  The following diagram shows the flow chart of Istio as an Istio management plane.\nSlime architecture  The specific steps for Slime to manage Istio are as follows.\n Slime operator completes the initialization of Slime components in Kubernetes based on the administrator’s configuration. Developers create configurations that conform to the Slime CRD specification and apply them to Kubernetes clusters. Slime queries the monitoring data of the relevant service stored in Prometheus and converts the Slime CRD into an Istio CRD, in conjunction with the configuration of the adaptive part of the Slime CRD while pushing it to the Global Proxy. Istio listens for the creation of Istio CRDs. Istio pushes the configuration information of the Sidecar Proxy to the corresponding Sidecar Proxy in the data plane.  The diagram below shows the internal architecture of Slime.\nSlime Internal  We can divide Slime internally into three main components.\n slime-boot: operator for deploying Slime modules on Kubernetes. slime-controller: the core component of Slime that listens to the Slime CRD and converts it to an Istio CRD. slime-metric: the component used to obtain service metrics information. slime-controller dynamically adjusts service governance rules based on the information it receives.  The following diagram shows the architecture of Slime Adaptive Traffic Limiting. Slime smart limiter  Slime dynamically configures traffic limits by interfacing with the Prometheus metric server to obtain real-time monitoring.\nSlime’s adaptive traffic limitation process has two parts: one that converts SmartLimiter toEnvoyFilter and the other that monitors the data. Slime also provides an external monitoring data interface (Metric Discovery Server) that allows you to sync custom monitoring metrics to the traffic limiting component via MDS.\nThe CRD SmartLimiter created by Slime is used to configure adaptive traffic limiting. Its configuration is close to natural semantics, e.g., if you want to trigger an access limit for Service A with a limit of 30QPS when the CPU exceeds 80%, the corresponding SmartLimiter is defined as follows.\napiVersion:microservice.netease.com/v1alpha1kind:SmartLimitermetadata:name:anamespace:defaultspec:descriptors:- action:fill_interval:seconds:1quota:\"30/{pod}\"# 30 is the quota for this service. If there are three pods, the limit is 10 per pod.condition:\"{cpu}\u003e0.8\"# Auto-fill the template based on the value of the monitor {cpu}Aeraki: A Non-Invasive Istio Extension Toolset Aeraki is a service mesh project open sourced by Tencent Cloud in March 2021. Aeraki provides an end-to-end cloud-native service mesh protocol extension solution that provides Istio with powerful third-party protocol extension capabilities in a non-intrusive way, supporting traffic management for Dubbo, Thrift, Redis, and private protocols in Istio. Aeraki’s architecture is shown in the following diagram.\nAeraki architecture  Aeraki architecture, source Istio blog .\nAs seen in the Aeraki architecture diagram, the Aeraki protocol extension solution consists of two components.\n Aeraki: Aeraki runs as an Istio enhancement component on the control plane, providing user-friendly traffic rule configurations to operations via CRDs. Aeraki translates these traffic rule configurations into Envoy configurations distributed via Istio to sidecar proxies on the data plane. Aeraki also acts as an RDS server providing dynamic routing to the MetaProtocol Proxy on the data plane. The RDS provided by Aeraki differs from Envoy’s RDS in that Envoy RDS primarily offers dynamic routing for the HTTP protocol, while Aeraki RDS is designed to provide dynamic routing capabilities for all L7 protocols developed on the MetaProtocol framework. MetaProtocol Proxy: A generic L7 protocol proxy based on Envoy implementation. MetaProtocol Proxy is an extension of Envoy. It unifies the basic capabilities of service discovery, load balancing, RDS dynamic routing, traffic mirroring, fault injection, local/global traffic limiting, etc. for L7 protocols, which greatly reduces the difficulty of developing third-party protocols on Envoy and allows you to quickly create a third-party protocol plug-in based on MetaProtocol by only implementing the codec interface.  Before the introduction of MetaProtocol Proxy, if you wanted to use Envoy to implement an L7 protocol to implement routing, traffic limiting, telemetry, etc., you needed to write a complete TCP filter, which would have required a lot of work. For most L7 protocols, the required traffic management capabilities are similar, so there is no need to duplicate this work in each L7 filter implementation. The Aeraki project uses a MetaProtocol Proxy to implement these unified capabilities, as shown in the following figure.\nMetaProtocol proxy  MetaProtocol proxy, source Istio blog .\nBased on MetaProtocol Proxy, we only need to implement the codec interface part of the code to write a new L7 protocol Envoy Filter. In addition, without adding a single line of code, Aeraki can provide configuration distribution and RDS dynamic routing configuration for this L7 protocol at the control plane.\nMake Istio work for all environments and workloads We have seen that NetEase and Tencent are scaling Istio mainly by building Operator. However, this scaling is not enough for multi-cluster management. We know that much of our current infrastructure is transitioning to cloud native or containerized, which means containers, virtual machines, and other environments co-exist. How do we unify traffic management of these different environments? It is possible to do so using Istio.\nYou have to again build a management plane on top of Istio and add an abstraction layer to add CRDs that apply to cluster management, such as cluster traffic configuration, policy configuration, etc. Additionally, you have to deploy a Gateway in each cluster that connects uniformly to an edge proxy that interconnects all the groups.\nTo learn more about Tetrate Service Bridge (TSB), which provides this layer of infrastructure, you can go here . TSB is built on the open source Istio with enhancements, it follows the concept of the above two open source projects, and also builds a management plane to support heterogeneous environments.\nAs we can see, the Istio-based projects and the open source environment are booming and companies like Tetrate are doing useful jobs of productizing and making Istio available to all workloads.\n","permalink":"https://jimmysong.io/en/blog/istio-extensions-slime-and-aeraki/","summary":"In this article, I’ll introduce you two Istio extension projects: Aeraki and Slime.","title":"Introducing Slime and Aeraki in the evolution of Istio open-source ecosystem"},{"content":"You can use Istio to do multi-cluster management , API Gateway , and manage applications on Kubernetes or virtual machines . In my last blog , I talked about how service mesh is an integral part of cloud native applications. However, building infrastructure can be a big deal. There is no shortage of debate in the community about the practicability of service mesh and Istio– here’s a list of common questions and concerns, and how to address them.\n Is anyone using Istio in production? What is the impact on application performance due to the many resources consumed by injecting sidecar into the pod? Istio supports a limited number of protocols; is it scalable? Will Istio be manageable? – Or is it too complex, old services too costly to migrate, and the learning curve too steep?  I will answer each of these questions below.\nIstio is architecturally stable, production-ready, and ecologically emerging Istio 1.12 was just released in November – and has evolved significantly since the explosion of service mesh in 2018 (the year Istio co-founders established Tetrate). Istio has a large community of providers and users . The Istio SIG of Cloud Native Community has held eight Istio Big Talk (Istio 大咖说) , with Baidu, Tencent, NetEase, Xiaohongshu(小红书), and Xiaodian Technology(小电科技) sharing their Istio practices. According to CNCF Survey Report 2020 , about 50% of the companies surveyed are using a service mesh in production or planning to in the next year, and about half (47%) of organizations using a service mesh in production are using Istio.\nMany companies have developed extensions or plugins for Istio, such as Ant, NetEase, eBay, and Airbnb. Istio’s architecture has been stable since the 1.5 release, and the release cycle is fixed quarterly, with the current project’s main task being Day-2 Operations.\nThe Istio community has also hosted various events, with the first IstioCon in March 2021, the Istio Meetup China in Beijing in July, and the Service Mesh Summit 2022 in Shanghai in January 2022.\nSo we can say that the Istio architecture is stable and production-ready, and the ecosystem is budding.\nThe impact of service mesh on application performance A service mesh uses iptables to do traffic hijacking by default to be transparent to applications. When the number of services is large, there are a lot of iptables rules that affect network performance. You can use techniques like eBPF to provide application performance, but the method requires a high version of the operating system kernel, which few enterprises can achieve.\nIstio DNS  In the early days, Istio distributed the routing information of all services in the mesh to all proxy sidecars, which caused sidecar s to take up a lot of resources. Aeraki and Slime can achieve configuration lazy loading. We will introduce these two open-source projects in the Istio open-source ecosystem.\nFinally, there is a problem related to Sidecar proxy operation and maintenance: upgrading all Envoy proxies while ensuring constant traffic. A solution is using the SidecarSet resource in the open-source project OpenKruise .\nThe resource consumption and network latency associated with the introduction of Sidecar are also within reasonable limits, as you can see from the service mesh benchmark performance tests .\nExtending the Istio service mesh The next question is about extending the Istio service mesh. The current solution given by the Istio community is to use WebAssembly , an extension that is still relatively little used in production by now and has performance concerns. Most of the answers I’ve observed are CRDs that build a service mesh management plane based on Istio.\nAlso, making Istio support heterogeneous environments for all workloads, such as virtual machines and containers, is in strong demand for end-users. It allows them to migrate applications from traditional loads to cloud native easily. Finally, hybrid cloud traffic management for multiple clusters and meshes is a more advanced requirement.\nSteep learning curve Many people complain that Istio has too little learning material. Istio has been open source for four years, and there are a lot of learning resources now:\n Istio Documentation  IstioCon 2021  Istio Big Talk/Istio Weekly  Istio Fundamentals Course  Certified Istio Administrator   Yes, Istio is complex, but it’s been getting more and more manageable with every release. In my next blog, I will introduce you to two open source projects that extend Istio and give you some insight into what’s going on in the Istio community.\n","permalink":"https://jimmysong.io/en/blog/the-debate-in-the-community-about-istio-and-service-mesh/","summary":"There is no shortage of debate in the community about the practicability of service mesh and Istio – here’s a list of common questions and concerns, and how to address them.","title":"The debate in the community about Istio and service mesh"},{"content":"If you don’t know what Istio is, you can read my previous articles below:\n What Is Istio and Why Does Kubernetes Need it?  Why do you need Istio when you already have Kubernetes?   This article will explore the relationship between service mesh and cloud native.\nService mesh – the product of the container orchestration war If you’ve been following the cloud-native space since its early days, you’ll remember the container orchestration wars of 2015 to 2017. Kubernetes won the container wars in 2017, the idea of microservices had taken hold, and the trend toward containerization was unstoppable. Kubernetes architecture matured and slowly became boring, and service mesh technologies, represented by Linkerd and Istio, entered the CNCF-defined cloud-native critical technologies on the horizon.\nKubernetes was designed with the concept of cloud-native in mind. A critical idea in cloud-native is the architectural design of microservices. When a single application is split into microservices, how can microservices be managed to ensure the SLA of the service as the number of services increases? The service mesh was born to solve this problem at the architectural level, free programmers’ creativity, and avoid tedious service discovery, monitoring, distributed tracing, and other matters.\nThe service mesh takes the standard functionality of microservices down to the infrastructure layer, allowing developers to focus more on business logic and thus speed up service delivery, which is consistent with the whole idea of cloud-native. You no longer need to integrate bulky SDKs in your application, develop and maintain SDKs for different languages, and just use the service mesh for Day 2 operations after the application is deployed.\nThe service mesh is regarded as the next generation of microservices. In the diagram, we can see that many of the concerns of microservices overlap with the functionality of Kubernetes. Kubernetes focuses on the application lifecycle, managing resources and deployments with little control over services. The service mesh fills this gap. The service mesh can connect, control, observe and protect microservices.\nKubernetes vs. xDS vs. Istio This diagram shows the layered architecture of Kubernetes and Istio.\nimg  The diagram indicates that the kube-proxy settings are global and cannot be controlled at a granular level for each service. All Kubernetes can do is topology-aware routing, routing traffic closer to the Pod, and setting network policies in and out of the Pod.\nIn contrast, the service mesh takes traffic control out of the service layer in Kubernetes through sidecar proxies, injects proxies into each Pod, and manipulates these distributed proxies through a control plane. It allows for more excellent resiliency.\nKube-proxy implements traffic load balancing between multiple pod instances of a Kubernetes service. But how do you finely control the traffic between these services — such as dividing the traffic by percentage to different application versions (which are all part of the same service, but on other deployments), or doing canary releases and blue-green releases?\nThe Kubernetes community gives a way to do canary releases using Deployment, assigning different pods to deployed services by modifying the pod’s label.\nEnvoy Architecture  Currently, the most popular open-source implementation of service mesh in the world is Istio. From the CNCF Survey Report 2020 , we know that Istio is the most used service mesh in production today. Many companies have built their service mesh based on Istio, such as Ant, Airbnb, eBay, NetEase, Tencent, etc.\nCNCF Survey Report 2020  Figure from CNCF Survey Report 2020 Istio is developed based on Envoy, which has been used by default as its distributed proxy since the first day it was open-sourced. Envoy pioneered the creation of the xDS protocol for distributed gateway configuration, greatly simplifying the configuration of large-scale distributed networks. Ant Group open source MOSN also supported xDS In 2019. Envoy was also one of the first projects to graduate from CNCF, tested by large-scale production applications.\nService mesh – the cloud-native networking infrastructure With the above comparison between Kubernetes and service mesh in mind, we can see the place of service mesh in the cloud-native application architecture. That is, building a cloud-native network infrastructure specifically provides:\n Traffic management: controlling the flow of traffic and API calls between services, making calls more reliable, and enhancing network robustness in different environments. Observability: understanding the dependencies between services and the nature and flow of traffic between them provides the ability to identify problems quickly. Policy enforcement: controlling access policies between services by configuring the mesh rather than by changing the code. Service Identification and Security: providing service identifiability and security protection in the mesh.  ","permalink":"https://jimmysong.io/en/blog/service-mesh-an-integral-part-of-cloud-native-apps/","summary":"This article will explore the relationship between service mesh and cloud native.","title":"Service Mesh - an integral part of cloud-native applications"},{"content":"API gateways have been around for a long time as the entry point for clients to access the back-end, mainly to manage “north-south” traffic, In recent years, service mesh architectures have become popular, mainly for managing internal systems,(i.e. “east-west” traffic), while a service mesh like Istio also has built-in gateways that bring traffic inside and outside the system under unified control. This often creates confusion for first-time users of Istio. What is the relationship between the service mesh and the API gateway? How does Istio’s gateway work? What are the ways to expose the services in the Istio mesh? This article gives you the answer.\nKey Insights  The service mesh was originally created to solve the problem of managing internal traffic for distributed systems, but API gateways existed long before it. While the Gateway is built into Istio, you can still use a custom Ingress Controller to proxy external traffic. API gateways and service mesh are converging.  How do I expose services in the Istio mesh? The following diagram shows four approaches to expose services in the Istio mesh using Istio Gateway, Kubernetes Ingress, API Gateway, and NodePort/LB.\nExposing services through Istio Ingress Gateway  The Istio mesh is shaded, and the traffic in the mesh is internal (east-west) traffic, while the traffic from clients accessing services within the Kubernetes cluster is external (north-south) traffic.\n   Approach Controller Features     NodePort/LoadBalancer Kubernetes Load balancing   Kubernetes Ingress Ingress controller Load balancing, TLS, virtual host, traffic routing   Istio Gateway Istio Load balancing, TLS, virtual host, advanced traffic routing, other advanced Istio features   API Gateway API Gateway Load balancing, TLS, virtual host, advanced traffic routing, API lifecycle management, billing, rate limiting, policy enforcement, data aggregation    Since NodePort/LoadBalancer is a basic way to expose services built into Kubernetes, this article will not discuss that option. Each of the other three approaches will be described below.\nUsing Kubernetes Ingress to expose traffic We all know that clients of a Kubernetes cluster cannot directly access the IP address of a pod because the pod is in a network plane built into Kubernetes. We can expose services inside Kubernetes outside the cluster using NodePort or Load Balancer Kubernetes service type. To support virtual hosting, hiding and saving IP addresses, you can use Ingress resources to expose services in Kubernetes.\nKubernetes Ingress to expose services  Ingress is a Kubernetes resource that controls the behavior of an ingress controller that does the traffic touring, which is the equivalent of a load-balanced directional proxy server such as Nginx, Apache, etc., which also includes rule definitions, i.e., routing information for URLs, which is provided by the Ingress controller .\napiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:annotations:kubernetes.io/ingress.class:istioname:ingressspec:rules:- host:httpbin.example.comhttp:paths:- path:/status/*backend:serviceName:httpbinservicePort:8000The kubernetes.io/ingress.class: istio annotation in the example above indicates that the Ingress uses the Istio Ingress Controller which in fact uses Envoy proxy.\nUsing Istio Gateway to expose services Istio is a popular service mesh implementation that has evolved from Kubernetes that implements some features that Kubernetes doesn’t. (See What is Istio and why does Kubernetes need Istio? ) It makes traffic management transparent to the application, moving this functionality from the application to the platform layer and becoming a cloud-native infrastructure.\nIstio used Kubernetes Ingress as the traffic portal in versions prior to Istio 0.8, where Envoy was used as the Ingress Controller. From Istio 0.8 and later, Istio created the Gateway object. Gateway and VirtualService are used to represent the configuration model of Istio Ingress, and the default implementation of Istio Ingress uses the same Envoy proxy. In this way, the Istio control plane controls both the ingress gateway and the internal sidecar proxy with a consistent configuration model. These configurations include routing rules, policy enforcement, telemetry, and other service control functions.\nThe Istio Gateway resources function similarly to the Kubernetes Ingress in that it is responsible for north-south traffic to and from the cluster. The Istio Gateway acts as a load balancer to carry connections to and from the edge of the service mesh. The specification describes a set of open ports and the protocols used by those ports, as well as the SNI configuration for load balancing, etc.\nThe Istio Gateway resource itself can only be configured for L4 through L6, such as exposed ports, TLS settings, etc.; however, the Gateway can be bound to a VirtualService, where routing rules can be configured on L7, such as versioned traffic routing, fault injection, HTTP redirects, HTTP rewrites, and all other routing rules supported within the mesh.\nBelow is an example of a Gateway binding to a VirtualService. The pod with the “istio: ingressgateway” label will act as the Ingress controller and route HTTP traffic to port 80 of the httpbin.example.com virtual host. The biggest difference between this and using Kubernetes Ingress is that it requires us to manually bind the VirtualService to the Gateway and specify the pod where the Gateway is located. This configuration is equivalent to opening up an entry point to Kubernetes for external access.\napiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:httpbin-gatewayspec:selector:istio:ingressgatewayservers:- port:number:80name:httpprotocol:HTTPhosts:- \"httpbin.example.com\"The VirtualService below is bound to the gateway above via gateways to accept traffic from that gateway.\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:httpbinspec:hosts:- \"httpbin.example.com\"gateways:- httpbin-gatewayhttp:- match:- uri:prefix:/statusroute:- destination:port:number:8000host:httpbinUsing an API Gateway API gateways are API management tools that sit between the client and the back-end service and are widely used in microservices as a way to separate the client interface from the back-end implementation. When a client makes a request, the API gateway breaks it down into multiple requests, then routes them to the correct location, generates a response, and keeps track of everything.\nThe API Gateway is a special type of service in the microservices architecture that serves as the entry point for all microservices and is responsible for performing routing requests, protocol conversions, aggregating data, authentication, rate limiting, circuit breaking, and more. Most enterprise APIs are deployed through API Gateways, which typically handle common tasks across API service systems, such as TLS termination, authentication and authorization, rate limiting, and statistical information.\nThere can be one or more API Gateways in the mesh. The responsibilities of the API Gateway are\n Request routing and version control Facilitating the transition of monolithic applications to microservices Permission authentication Data aggregation: monitoring and billing Protocol conversion Messaging and caching Security and alerting  Many of the above basic functions such as routing and permission authentication can also be achieved through Istio Gateway, but some mature API gateways may be more advantageous in terms of feature richness and scalability.\n The introduction of API Gateway requires consideration of the deployment, operation and maintenance, load balancing, and other scenarios of API Gateway itself, which increases the complexity of back-end services. An API Gateway carries a large number of interface adaptations, which makes it difficult to maintain. For some scenarios, the addition of a hop may lead to a reduction in performance.  Currently, some API Gateway imitations are building their own service mesh by deploying them in the sidecar.\nSummary In the Istio mesh, you can use a variety of Kubernetes Ingress Controllers to act as entry gateways, but of course, you can also use Istio’s built-in Istio Gateway directly, for policy control, traffic management, and usage monitoring. The advantage of this is that the gateway can be managed directly through Istio’s control plane, without the need for additional tools. But for functions such as API statement cycle management, complex billing, protocol conversion, and authentication, a traditional API gateway may be a better fit for you. So, you can choose according to your needs, or you can use a combination.\nSome traditional reverse proxies are also moving towards Service Mesh, such as Nginx with Nginx Service Mesh and Traefik with Traefik Mesh, and some API gateway products are also moving towards Service Mesh, such as Kong with Kuma, and in the future, we will see more convergence of API gateways, reverse proxies, and service meshes.\n","permalink":"https://jimmysong.io/en/blog/istio-servicemesh-api-gateway/","summary":"What is the relationship between the service mesh and the API gateway? How does Istio’s gateway work? What are the ways to expose the services in the Istio mesh? This article gives you the answer.","title":"Using Istio service mesh as API Gateway"},{"content":"Do you have multiple Kubernetes clusters and a service mesh? Do your virtual machines and services in a Kubernetes cluster need to interact? This article will take you through the process and considerations of building a hybrid cloud using Kubernetes and an Istio Service Mesh. Together, Kubernetes and Istio can be used to bring hybrid workloads into a mesh and achieve interoperability for multicluster. But another layer of infrastructure — a management plane — is helpful for managing multicluster or multimesh deployments.\nKubernetes Using Kubernetes enables rapid deployment of a distributed environment that enables cloud interoperability and unifies the control plane on the cloud. It also provides resource objects, such as Service, Ingress and Gateway , to handle application traffic. The Kubernetes API Server communicates with the kube-proxy component on each node in the cluster, creates iptables rules for the node, and forwards requests to other pods.\nAssuming that a client now wants to access a service in Kubernetes, the request is first sent to the Ingress/Gateway, then forwarded to the backend service (Service A in the diagram below) based on the routing configuration in the Ingress/Gateway. Then Service A polls an instance of Service B for the traffic requested by Service B. Lastly, the traffic requested by Service A for Service B is polled forward to Service B’s instance.\nKubernetes  Kubernetes Multicluster The most common usage scenarios for multicluster management include:\n service traffic load balancing isolating development and production environments decoupling data processing and data storage cross-cloud backup and disaster recovery flexible allocation of compute resources low-latency access to services across regions avoiding vendor lock-in  There are often multiple Kubernetes clusters within an enterprise; and the KubeFed implementation of Kubernetes cluster federation developed by Multicluster SIG enables multicluster management capabilities, which allows all Kubernetes clusters to be managed through the same interface.\nThere are several general issues that need to be addressed when using cluster federation:\n Configuring which clusters need to be federated API resources need to be propagated across the clusters Configuring how API resources are distributed to different clusters Registering DNS records in clusters to enable service discovery across clusters  The following is a multicluster architecture for KubeSphere — one of the most commonly used Kubernetes multicluster management architectures — where the Host Cluster serves as the control plane with two member clusters, West and East.\nMulticluster  The Host Cluster needs to be able to access the API Server of the Member Cluster, but the network connectivity between Member Clusters is not required. The Host Cluster is independent of the Member Cluster it manages and the Member Cluster is not aware of the existence of the Host Cluster. The advantage of this is that when the control plane fails, the Member Cluster will not be affected and the deployed load can still operate normally without being affected.\nThe Host Cluster also assumes the role of API portal, and the Host Cluster forwards the resource requests to the Member Cluster — which is convenient for aggregation and also facilitates unified authority authentication. We see that there is a Federation Control Plane in the Host Cluster, where the Push Reconciler propagates the identity, role, and role binding from the Federation Cluster to all Member Clusters.\nIstio Service Mesh Consider using the Istio service mesh when we have multilingual, multiversion microservices running in Kubernetes and need finer-grained canary publishing and unified security policy management for inter-service observability. Istio enables intelligent application-aware load balancing from the application layer to other Service Mesh-enabled services in the cluster, by transparently intercepting all traffic to and from the application using IPTables, and bypassing the primary kube-proxy load balancing. The Istio control plane communicates with the Kubernetes API Server to obtain information about all registered services in the cluster.\nThe following diagram illustrates the basics of Istio, where all nodes belong to the same Kubernetes cluster.\nIstio Service Mesh  You may end up with at least a few Kubernetes clusters, each hosting microservices. Multiple deployment models exist for Istio’s multicluster deployments — depending on network isolation, primary and backup — which can be specified by declaration when deploying using Istio Operator. Communication between these microservices in a cluster can be enhanced by a service mesh. Within the cluster, Istio provides common communication patterns to improve resiliency, security and observability.\nAll of the above is about application load management on Kubernetes, but for legacy applications on virtual machines: how can they be managed in the same plane? Istio supports applications on virtual machines, so why do we need a management plane?\nManagement Plane To manage gateways, traffic and security groupings, and apply them to different clusters and namespaces, you’ll need to add another layer of abstraction on top of Istio: a management plane. The diagram below shows the multitenant model of Tetrate Service Bridge (TSB). TSB uses Next Generation Access Control (NGAC) — a fine-grained authorization framework — to manage user access and also facilitate the construction of a zero-trust network.\nManagement Plane  Istio provides workload identification, protected by strong mTLS encryption. This zero-trust model is better than trusting workloads based on topology information, such as source IP. A common control plane for multicluster management is built on top of Istio. Then a management plane is added to manage multiple clusters — providing multitenancy, management configuration, observability, and more.\nThe diagram below shows the architecture of Tetrate Service Bridge.\nTetrate Service Bridge  Summary Interoperability of heterogeneous clusters is achieved with Kubernetes. Istio brings containerized and virtual machine loads into a single control plane, to unify traffic, security and observability within the clusters. However, as the number of clusters, network environments and user permissions become more complex, there is a need to build another management plane above Istio’s control plane (for example, Tetrate Service Bridge ) for hybrid cloud management.\n","permalink":"https://jimmysong.io/en/blog/multicluster-management-with-kubernetes-and-istio/","summary":"This article explains three patterns/tools for debugging microservices in Kubernetes and the changes brought by the introduction of Istio for debugging microservices.","title":"Multicluster Management with Kubernetes and Istio"},{"content":"Kubernetes is arguably the best environment for running microservices so far, but the experience of debugging microservices in a Kubernetes environment may not be as user-friendly. This article will show you how to debug microservices in Kubernetes, introduce common tools, and explain how the introduction of Istio impacts debugging microservices.\nDebugging microservices is vastly different from traditional monolithic applications The debugging of microservices has been a long-standing problem for software developers. This challenge does not exist in traditional monolithic applications because developers can leverage the debugger in IDEs to add breakpoints, modify environment variables, single-step execution, etc. for their applications, all of which provide great help in software debugging. With the popularity of Kubernetes, the debugging of microservices becomes a thorny issue, where the following issues are more complicated than the debugging of traditional monolithic applications.\nMultiple dependencies A microservice often depends on multiple other microservices, some shared volumes across multiple microservices, and authorizations based on service accounts. When debugging a microservice, how do you deploy other dependent services to quickly build a latest set of staging environments?\nAccess from a local machine When microservices are running on a developer’s local computer, there is usually no direct access to the services in a Kubernetes cluster. How can you debug microservices deployed in a Kubernetes cluster as if they were local services?\nSlow development loop Usually, it takes a long process to update the code and build it into an image before pushing it to the cluster. How do you speed up the development cycle? Let’s look at the tools that address those challenges.\nTools The main solutions for debugging microservices in Kubernetes are:\n Proxy: by building a VPN, deploying a proxy in the Kubernetes cluster, and adding local debug endpoints to make the services in Kubernetes directly accessible to local applications, your architecture will look like [ local service ] \u003c-\u003e [ proxy ] \u003c-\u003e [ app in Kubernetes ]. Sidecar: Inject a sidecar into the pod of the microservice to be debugged to intercept all traffic to and from the service, so that the service can be tracked and monitored, and the service can also be debugged in this sidecar. Service Mesh: To get an overall picture of the application, inject sidecars into all microservices so that you can get a dashboard that monitors global status.  Here are three typical open source projects that implement the above solutions, each of which can help you debug microservices from a different perspective. You can apply them at different stages of software development and they can be said to be complementary to each other.\nProxy – debugging microservices with Telepresence Telepresence is essentially a local proxy that proxies data volumes, environment variables, and networks in a Kubernetes cluster locally. The following diagram shows the main usage scenarios for Telepresence.\nProxy mode: Telepresence  Users need to manually execute the telepresence command locally, which will automatically deploy the agent to Kubernetes. Once the agent has been deployed,\n Local services will have complete access to other services in the Kubernetes cluster, environment variables, Secret, ConfigMap, etc. Services in the cluster also have direct access to the locally exposed endpoints.  However, this approach requires users to run multiple commands while debugging locally, and in some network environments it may not be possible to establish a VPN connection to the Kubernetes cluster.\nSidecar – debugging microservices with Nocalhost Nocalhost is a Kubernetes-based cloud development environment. To use it, you just need to install a plugin in your IDE – VS Code to extend Kubernetes and shorten the development feedback cycle. The development environment can be isolated by creating different namespaces for different users and using ServiceAccount when binding to different user corners. Nocalhost also provides a web console and API for administrators to manage different development environments.\nSidecar mode: Nocalhost  As long as you have a Kubernetes cluster and have admin rights to the cluster, you can refer to the Nocalhost documentation to quickly start trying it out. To use the Nocalhost plugin in VS Code, you need to configure the Kubernetes cluster in the plugin first.\n Select the Kubeconfig file you just exported or copy and paste the contents of the file directly into the configuration. Then select the service you need to test and select the corresponding Dev Container. VS Code will automatically open a new code window.  Here is an example of the bookinfo sample provided by Istio. You can open the cloned code in your local IDE and click the hammer next to the code file to enter development mode. Selecting the corresponding DevContainer and Nocalhost will automatically inject a development container sidecar into the pod and automatically enter the container in the terminal, as shown in the following figure.\nNocalhost VS code  In development mode, the code is modified locally without rebuilding the image, and the remote development environment takes effect in real time, which can greatly accelerate the development speed. At the same time, Nocalhost also provides a server for managing the development environment and user rights, as shown in the following figure.\nNocalhost Web  Service Mesh – debugging microservices with Istio The above method of using proxy and sidecar can only debug one service at a time. You’ll need a mesh to get the global status of the application, such as the metrics of the service obtained, and debug the performance of the service by understanding the dependency and invocation process of the service through distributed tracing. These observability features need to be implemented by injecting sidecar uniformly for all services. And, when your services are in the process of migrating from VMs to Kubernetes, using Istio can bring VMs and Kubernetes into a single network plane (as shown below), making it easy for developers to debug and do incremental migrations.\nSerivce Mesh mode: Istio  Of course, these benefits do not come without a “cost.” With the introduction of Istio, your Kubernetes services will need to adhere to the Istio naming convention and you’ll need to know how to debug microservices using the Istioctl command line and logging.\n Use the istioctl analyze command to debug the deployment of microservices in your cluster, and you can use YAML files to examine the deployment of resources in a namespace or across your cluster. Use istioctl proxy-config secret to ensure that the secret of a pod in a service mesh is loaded correctly and is valid.  Summary In the process of microservicing applications and migrating from virtual machines to Kubernetes, developers need to make a lot of changes in their mindset and habits. By building a VPN between local and Kubernetes via proxy, developers can easily debug services in Kubernetes as if they were local services. By injecting a sidecar into the pod, you can achieve real-time debugging and speed up the development process. Finally, the Istio service mesh truly enables global observability, and you can also use tools like Tetrate Service Bridge to manage heterogeneous platforms, helping you gradually move from monolithic applications to microservices.\n","permalink":"https://jimmysong.io/en/blog/how-to-debug-microservices-in-kubernetes-with-proxy-sidecar-or-service-mesh/","summary":"This article explains three patterns/tools for debugging microservices in Kubernetes and the changes brought by the introduction of Istio for debugging microservices.","title":"How to debug microservices in Kubernetes with proxy, sidecar or service mesh?"},{"content":"Istio was named by Tetrate founder Varun Talwar and Google lead engineer Louis Ryan in 2017 and was open sourced on May 24, 2017. Today is the fourth anniversary of Istio’s open source arrival. Let’s take a look back at Istio’s four years of development — and look forward to Istio’s future.\nIstio’s open source history In 2017, the year Kubernetes ended the container orchestration battle, Google took the opportunity to consolidate its dominance in the cloud native space and compensate for Kubernetes’ disadvantage in service-to-service traffic management by open-sourcing Istio. Istio released its 1.10 last week — but here are some of the most important releases in Istio’s history to date.\n   Date Version Note     May 24, 2017 0.1 Officially open source; established the architectural foundation of Control Plane, Data Plane and sidecar proxy.   October 10, 2017 0.2 Started to support multiple runtime environments, such as virtual machines.   June 1, 2018 0.8 API refactoring   July 31, 2018 1.0 Production-ready, after which the Istio team underwent a massive reorganization.   March 19, 2019 1.1 Enterprise-ready. Support for multiple Kubernetes clusters, with performance optimizations.   March 3, 2020 1.5 Back to monolith, with microservice components merged into istiod, making Istio’s architecture cleaner and easier to maintain. Support for WebAssembly extension, making Istio’s ecology much stronger.   November 18, 2020 1.8 Officially deprecated Mixer and focused on adding support for virtual machines.    A year after its inception– and two months before the 1.0 release, version 0.8 was released with a massive refactoring of the API. In late July 2018, when 1.0 was released, Istio reached a production-ready tipping point. Since then, Google has massively reorganized the Istio team and several Istio-based service mesh startups were born, making 2018 the booming year of the service mesh industry.\nIstio 1.1 was released in March 2019, almost 9 months after 1.0 was released, which is far beyond the average release cycle of an open-source project. We know that the speed of iteration and evolution is a core competency of basic software. Since then, Istio has started a regular release cadence of one version per quarter and has become the #4 fastest growing project in GitHub’s top 10 in 2019 !\nThe Istio community In 2020, Istio’s project management began to mature and its governance reached a stage of evolution. We saw the first election of a steering committee for the Istio community and the transfer of the trademark to Open Usage Commons . The first IstioCon was successfully held in February 2021, with thousands of people attending the online conference. There is also a large Istio community in China , and face-to-face Istio community meetups will be held there in 2021. Stay tuned for more.\n  According to the CNCF 2020 Survey, 46% of organizations were either using a service mesh in production or planning to use it in the next 12 months. Istio was the top used mesh among those using a mesh in production.\nThe future After 4 years of development, there is not only a large user base around Istio, but also several Istio vendors, as you can see on the homepage of the recently revamped Istio website. In the last few releases, Istio has shifted its development focus to improving the Day 2 Operation experience. We also expect to see more Istio adoption path recommendations, case studies, learning materials, training, and certifications (such as the industry’s first Certified Istio Administrator from Tetrate) that will facilitate the adoption of Istio.\n","permalink":"https://jimmysong.io/en/blog/istio-4-year-birthday/","summary":"Today is Istio's 4 year birthday, let’s take a look back at Istio’s four years of development — and look forward to Istio’s future.","title":"Happy Istio 4th Anniversary -- Retrospect and Outlook"},{"content":"Istio, the most popular service mesh implementation , was developed on top of Kubernetes and has a different niche in the cloud native application ecosystem than Kubernetes. Rather than introduce you directly to what Istio has to offer, this article will explain how Istio came about and what it is in relation to Kubernetes.\nWhy Is There an Istio? To explain what Istio is, it’s also important to understand the context in which Istio came into being — i.e., why is there an Istio?\nMicroservices are a technical solution to an organizational problem. And Kubernetes/Istio are a technical solution to deal with the issues created by moving to microservices. As a deliverable for microservices, containers solve the problem of environmental consistency and allow for more granularity in limiting application resources. They are widely used as a vehicle for microservices.\nGoogle open-sourced Kubernetes in 2014, which grew exponentially over the next few years. It became a container scheduling tool to solve the deployment and scheduling problems of distributed applications — allowing you to treat many computers as though they were one computer. Because the resources of a single machine are limited and Internet applications may have traffic floods at different times (due to rapid expansion of user scale or different user attributes), the elasticity of computing resources needs to be high. A single machine obviously can’t meet the needs of a large-scale application; and conversely, it would be a huge waste for a very small-scale application to occupy the whole host.\nIn short, Kubernetes defines the final state of the service and enables the system to reach and stay in that state automatically. So how do you manage the traffic on the service after the application has been deployed? Below we will look at how service management is done in Kubernetes and how it has changed in Istio.\nHow Do You Do Service Management in Kubernetes? The following diagram shows the service model in Kubernetes:\nKubernetes Service Model  From the above figure we can see that:\n Different instances of the same service may be scheduled to different nodes. Kubernetes combines multiple instances of a service through Service objects to unify external services. Kubernetes installs a kube-proxy component in each node to forward traffic, which has simple load balancing capabilities. Traffic from outside the Kubernetes cluster can enter the cluster via Ingress (Kubernetes has several other ways of exposing services; such as NodePort, LoadBalancer, etc.).  Kubernetes is used as a tool for intensive resource management. However, after allocating resources to the application, Kubernetes doesn’t fully solve the problems of how to ensure the robustness and redundancy of the application, how to achieve finer-grained traffic division (not based on the number of instances of the service), how to guarantee the security of the service, or how to manage multiple clusters, etc.\nThe Basics of Istio The following diagram shows the service model in Istio, which supports both workloads and virtual machines in Kubernetes.\nIstio  From the diagram we can see that:\n Istiod acts as the control plane, distributing the configuration to all sidecar proxies and gateways. (Note: for simplification, the connections between Istiod and sidecar are not drawn in the diagram.) Istio enables intelligent application-aware load balancing from the application layer to other mesh enabled services in the cluster, and bypasses the rudimentary kube-proxy load balancing. Application administrators can manipulate the behavior of traffic in the Istio mesh through a declarative API, in the same way they manage workloads in Kubernetes. It can take effects within seconds and they can do this without needing to redeploy. Ingress is replaced by Gateway resources, a special kind of proxy that is also a reused Sidecar proxy. A sidecar proxy can be installed in a virtual machine to bring the virtual machine into the Istio mesh.  In fact, before Istio one could use SpringCloud, Netflix OSS, and other tools to programmatically manage the traffic in an application, by integrating the SDK in the application. Istio makes traffic management transparent to the application, moving this functionality out of the application and into the platform layer as a cloud native infrastructure.\nIstio complements Kubernetes, by enhancing its traffic management, observability and security for cloud native applications. The service mesh open source project — launched in 2017 by Google, IBM and Lyft — has come a long way in three years. A description of Istio’s core features can be found in the Istio documentation .\nSummary  Service Mesh is the cloud native equivalent of TCP/IP, addressing application network communication, security and visibility issues. Istio is currently the most popular service mesh implementation, relying on Kubernetes but also scalable to virtual machine loads. Istio’s core consists of a control plane and a data plane, with Envoy as the default data-plane agent. Istio acts as the network layer of the cloud native infrastructure and is transparent to applications.  ","permalink":"https://jimmysong.io/en/blog/what-is-istio-and-why-does-kubernetes-need-it/","summary":"This article will explain how Istio came about and what it is in relation to Kubernetes.","title":"What Is Istio and Why Does Kubernetes Need it?"},{"content":"If you’ve heard of service mesh and tried Istio , you may have the following questions:\n Why is Istio running on Kubernetes? What is the role of Kubernetes and a service mesh in the cloud native application architecture, respectively? What aspects of Kubernetes does Istio extend? What problems does it solve? What is the relationship between Kubernetes, Envoy, and Istio?  This article will take you through the inner workings of Kubernetes and Istio. In addition, I will introduce the load balancing approach in Kubernetes, and explain why you need Istio when you have Kubernetes.\nKubernetes is essentially about application lifecycle management through declarative configuration, while a service mesh is essentially about providing inter-application traffic, security management and observability. If you have already built a stable application platform using Kubernetes, how do you set up load balancing and traffic control for calls between services? This is where a service mesh comes into the picture.\nEnvoy introduces the xDS protocol, which is supported by various open source software, such as Istio , MOSN , etc. Envoy contributes xDS to a service mesh or cloud native infrastructure. Envoy is essentially a modern version of a proxy that can be configured through APIs, based on which many different usage scenarios are derived — such as API Gateway, sidecar proxy in service mesh, and edge proxy.\nThis article contains the following:\n A description of the role of kube-proxy. The limitations of Kubernetes for microservice management. An introduction to the capabilities of Istio service mesh. A comparison of some of the concepts in Kubernetes, Envoy, and the Istio service mesh.  Kubernetes vs Service Mesh The following diagram shows the service access relationship in Kubernetes and service mesh (one sidecar per pod model).\nKubernetes vs Service Mesh  Traffic Forwarding Each node in a Kubernetes cluster deploys a kube-proxy component that communicates with the Kubernetes API Server, gets information about the services in the cluster, and then sets iptables rules to send requests for service directly to the corresponding Endpoint (a pod belonging to the same group of services).\nService Discovery Service Discovery  Istio can follow the service registration in Kubernetes and can also interface with other service discovery systems via platform adapters in the control plane; and then generate data plane configurations (using CRD, which are stored in etcd) with transparent proxies for the data plane. The transparent proxy of the data plane is deployed as a sidecar container in the pod of each application service, and all these proxies need to request the control plane to synchronize the proxy configuration. The proxy is “transparent” because the application container is completely unaware of the presence of the proxy. The kube-proxy component in the process needs to intercept traffic as well, except that the kube-proxy intercepts traffic to and from the Kubernetes node — while the sidecar proxy intercepts traffic to and from the pod.\nDisadvantages of a Service Mesh Since Kubernetes has many pods running on each node, putting the original kube-proxy route forwarding function in each pod will increase the response latency — due to more hops when the sidecar intercepts the traffic — and consume more resources. In order to manage traffic in a fine-grained manner, a series of new abstractions will be added. This will further increase the learning cost for users, but as the technology becomes more popular this situation will be slowly alleviated.\nAdvantages of a Service Mesh The kube-proxy settings are global and cannot be controlled at a granular level for each service, while service mesh takes the traffic control out of the service layer in Kubernetes by means of sidecar proxy — allowing for more elasticity.\nShortcomings of Kube-Proxy First, it does not automatically try another pod if the forwarded pod is not serving properly. Each pod has a health check mechanism and when a pod has health problems, kubelet will restart the pod and kube-proxy will remove the corresponding forwarding rules. Also, nodePort-type services cannot add TLS or more complex message routing mechanisms.\nKube-proxy implements load balancing of traffic across multiple pod instances of a Kubernetes service, but how do you do fine-grained control of traffic between these services — such as dividing traffic by percentage to different application versions (which are all part of the same service but on different deployments), or doing canary releases (grayscale releases) and blue-green releases?\nThe Kubernetes community gives a way to do canary releases using Deployment , which is essentially a way to assign different pods to a deployment’s service by modifying the pod’s label.\nKubernetes Ingress vs. Istio Gateway As mentioned above, kube-proxy can only route traffic within a Kubernetes cluster. The pods of a Kubernetes cluster are located in a network created by CNI. An ingress — a resource object created in Kubernetes — is created for communication outside the cluster. It’s driven by an ingress controller located on Kubernetes edge nodes responsible for managing north-south traffic. Ingress must be docked to various Ingress Controllers, such as the nginx ingress controller and traefik . Ingress is only applicable to HTTP traffic and is simple to use. It can only route traffic by matching a limited number of fields — such as service, port, HTTP path, etc. This makes it impossible to route TCP traffic such as MySQL, Redis, and various RPCs. This is why you see people writing nginx config language in ingress resource annotations.The only way to directly route north-south traffic is to use the service’s LoadBalancer or NodePort, the former requiring cloud vendor support and the latter requiring additional port management.\nIstio Gateway functions similarly to Kubernetes Ingress, in that it is responsible for north-south traffic to and from the cluster. Istio Gateway describes a load balancer for carrying connections to and from the edge of the mesh. The specification describes a set of open ports and the protocols used by those ports, the SNI configuration for load balancing, etc. Gateway is a CRD extension that also reuses the capabilities of the sidecar proxy; see the Istio website for detailed configuration.\nEnvoy Envoy is the default sidecar proxy in Istio. Istio extends its control plane based on Enovy’s xDS protocol. We need to familiarize ourselves with Envoy’s basic terminology before talking about Envoy’s xDS protocol. The following is a list of basic terms and their data structures in Envoy; please refer to the Envoy documentation for more details.\nEnvoy  Basic Terminology The following are the basic terms in Enovy that you should know.\n Downstream: The downstream host connects to Envoy, sends the request, and receives the response; i.e., the host that sent the request. Upstream: The upstream host receives connections and requests from Envoy and returns responses; i.e., the host that receives the requests. Listener: Listener is a named network address (for example, port, UNIX domain socket, etc.); downstream clients can connect to these listeners. Envoy exposes one or more listeners to the downstream hosts to connect. Cluster: A cluster is a group of logically identical upstream hosts to which Envoy connects. Envoy discovers the members of a cluster through service discovery. Optionally, the health status of cluster members can be determined through proactive health checks. Envoy decides which member of the cluster to route requests through a load balancing policy.  Multiple listeners can be set in Envoy, each listener can set a filter chain (filter chain table), and the filter is scalable so that we can more easily manipulate the behavior of traffic — such as setting encryption, private RPC, etc.\nThe xDS protocol was proposed by Envoy and is the default sidecar proxy in Istio, but as long as the xDS protocol is implemented, it can theoretically be used as a sidecar proxy in Istio — such as the MOSN open source by Ant Group.\nimg   Istio is a very feature-rich service mesh that includes the following capabilities.\n Traffic Management: This is the most basic feature of Istio. Policy Control: Enables access control systems, telemetry capture, quota management, billing, etc. Observability: Implemented in the sidecar proxy. Security Authentication: The Citadel component does key and certificate management.  Traffic Management in Istio The following CRDs are defined in Istio to help users with traffic management.\n Gateway: Gateway describes a load balancer that runs at the edge of the network and is used to receive incoming or outgoing HTTP/TCP connections. VirtualService: VirtualService actually connects the Kubernetes service to the Istio Gateway. It can also perform additional operations, such as defining a set of traffic routing rules to be applied when a host is addressed. DestinationRule: The policy defined by the DestinationRule determines the access policy for the traffic after it has been routed. Simply put, it defines how traffic is routed. Among others, these policies can be defined as load balancing configurations, connection pool sizes, and external detection (for identifying and expelling unhealthy hosts in the load balancing pool) configurations. EnvoyFilter: The EnvoyFilter object describes filters for proxy services that can customize the proxy configuration generated by Istio Pilot. This configuration is generally rarely used by primary users. ServiceEntry: By default, services in the Istio service mesh are unable to discover services outside of the Mesh. ServiceEntry enables additional entries to be added to the service registry inside Istio, thus allowing automatically discovered services in the mesh to access and route to these manually added services.  Kubernetes vs. xDS vs. Istio Having reviewed the abstraction of traffic management in Kubernetes’ kube-proxy component, xDS, and Istio, let’s look now at a comparison of the three components/protocols in terms of traffic management only (note that the three are not exactly equivalent).\n   Kubernetes xDS Istio service mesh     Endpoint Endpoint WorkloadEntry   Service Route VirtualService   kube-proxy Route DestinationRule   kube-proxy Listener EnvoyFilter   Ingress Listener Gateway   Service Cluster ServiceEntry    Takeaways  The essence of Kubernetes is application lifecycle management, specifically deployment and management (scaling up and down, auto-recovery, release). Kubernetes provides a scalable and highly resilient deployment and management platform for microservices. A service mesh is based on transparent proxies that intercept traffic between services through sidecar proxies, and then manage the behavior of them through control plane configuration. A service mesh decouples traffic management from Kubernetes, eliminating the need for a kube-proxy component to support traffic within service mesh; and managing inter-service traffic, security and observability by providing an abstraction closer to the microservice application layer. xDS is one of the protocol standards for service mesh configuration. A service mesh is a higher-level abstraction of service in Kubernetes.  Summary If the object managed by Kubernetes is a pod, then the object managed in service mesh is a service, so it’s just a matter of using Kubernetes to manage microservices and then applying service mesh. If you don’t even want to manage a service, then use a serverless platform like Knative — but that’s an afterthought.\n","permalink":"https://jimmysong.io/en/blog/why-do-you-need-istio-when-you-already-have-kubernetes/","summary":"This article will take you through the inner workings of Kubernetes and Istio. In addition, I will introduce the load balancing approach in Kubernetes, and explain why you need Istio when you have Kubernetes.","title":"Why Do You Need Istio When You Already Have Kubernetes?"},{"content":"Tetrate Academy has recently released the Istio Fundamentals Course, which is now available for free. Sign up at Tetrate Academy now!\nCourse curriculum Here is the curriculum:\n Service Mesh and Istio Overview Installing Istio Observability: Telemetry and Logs Traffic Management Security Advanced Features Troubleshooting Real World Examples  There are self-assessment questions at the end of each course. I have passed the course, and here is the certificate after passing the course.\nTetrate Academy Istio Fundamentals Course  More In the future, Tetrate will release the Certified Istio Administrator (CIA) exam and welcome all Istio users and administrators to follow and register for it.\n","permalink":"https://jimmysong.io/en/notice/tetrate-istio-fundamental-courses/","summary":"Tetrate Academy has recently released the Istio Fundamentals Course, which is now available for free。","title":"Tetrate Academy Releases Free Istio Fundamentals Course"},{"content":"Different companies or software providers have devised countless ways to control user access to functions or resources, such as Discretionary Access Control (DAC), Mandatory Access Control (MAC), Role-Based Access Control (RBAC), and Attribute-Based Access Control (ABAC). In essence, whatever the type of access control model, three basic elements can be abstracted: user, system/application, and policy.\nIn this article, we will introduce ABAC, RBAC, and a new access control model — Next Generation Access Control (NGAC) — and compare the similarities and differences between the three, as well as why you should consider NGAC.\nWhat Is RBAC? RBAC, or Role-Based Access Control, takes an approach whereby users are granted (or denied) access to resources based on their role in the organization. Every role is assigned a collection of permissions and restrictions, which is great because you don’t need to keep track of every system user and their attributes. You just need to update appropriate roles, assign roles to users, or remove assignments. But this can be difficult to manage and scale. Enterprises that use the RBAC static role-based model have experienced role explosion: large companies may have tens of thousands of similar but distinct roles or users whose roles change over time, making it difficult to track roles or audit unneeded permissions. RBAC has fixed access rights, with no provision for ephemeral permissions or for considering attributes like location, time, or device. Enterprises using RBAC have had difficulty meeting the complex access control requirements to meet regulatory requirements of other organizational needs.\nRBAC Example Here’s an example Role in the “default” namespace in Kubernetes that can be used to grant read access to pods:\napiVersion:rbac.authorization.k8s.io/v1kind:Rolemetadata:namespace:defaultname:pod-readerrules:- apiGroups:[\"v1\"]resources:[\"pods\"]verbs:[\"get\",\"watch\",\"list\"]What Is ABAC? ABAC stands for Attribute-Based Access Control. At a high level, NIST defines ABAC as an access control method “where subject requests to perform operations on objects are granted or denied based on assigned attributes of the subject, environment conditions, and a set of policies that are specified in terms of those attributes and conditions.” ABAC is a fine-grained model since you can assign any attributes to the user, but at the same time it becomes a burden and hard to manage:\n When defining permissions, the relationship between users and objects cannot be visualized. If the rules are a little complex or confusingly designed, it will be troublesome for the administrator to maintain and trace.  This can cause performance problems when there is a large number of permissions to process.\nABAC Example Kubernetes initially uses ABAC as access control and is configured via JSON Lines, for example:\nAlice can just read pods in namespace “foo”:\n{\"apiVersion\": \"abac.authorization.kubernetes.io/v1beta1\", \"kind\": \"Policy\", \"spec\": {\"user\": \"alice\", \"namespace\": \"foo\", \"resource\": \"pods\", \"readonly\": true}} What Is NGAC? NGAC, or Next Generation Access Control, takes the approach of modeling access decision data as a graph. NGAC enables a systematic, policy-consistent approach to access control, granting or denying users administrative capabilities with a high level of granularity. NGAC was developed by NIST (National Institute of Standards and Technology) and is currently used in Tetrate Q and Tetrate Service Bridge .\nThere are several types of entities; they represent the resources you want to protect, the relationships between them, and the actors that interact with the system. The entities are:\n Users Objects User attributes, such as organization unit Object attributes, such as folders Policy classes, such as file system access, location, and time  NIST’s David Ferraiolo and Tetrate ‘s Ignasi Barrera shared how NGAC works at their presentation on Next Generation Access Control at Service Mesh Day 2019 in San Francisco.\nNGAC is based on the assumption that you can represent the system you want to protect in a graph that represents the resources you want to protect and your organizational structure, in a way that has meaning to you and that adheres to your organization semantics. On top of this model that is very particular to your organization, you can overlay policies. Between the resource model and the user model, the permissions are defined. This way NGAC provides an elegant way of representing the resources you want to protect, the different actors in the system, and how both worlds are tied together with permissions.\nNGAC DAG  Image via Linear Time Algorithms to Restrict Insider Access using Multi-Policy Access Control Systems NGAC Example The following example shows a simple NGAC graph with a User DAG representing an organization structure, an Object DAG representing files and folders in a filesystem, a categorization of the files, and two different policies — file system and scope — that can be combined to make access decisions. The association edges between the two DAGs define the permissions the actors have on the target resources.\nNGAC  In this graph we can see a representation of two files, “resume” and “contract” in the “/hr-docs” folder, each linked to a category (“public/confidential”). There are also two policy classes, “File System” and “Scope,” where the objects in the graph are attached — these need to be satisfied in order to get access to each file.\nUser Allice has read and write access to both files in the example, because a path links Allice to each of the files and the paths grant permissions on both policy classes. However, user Bob only has access to the “resume” file, because although there exists a path from Bob to the “contract” file that satisfies the “File System” policy class with “read” permissions, there is no path granting permissions on the “Scope” policy class. So, access to the “contract” file is denied to Bob.\nWhy Choose NGAC? The need to keep track of attributes of all objects creates a manageability burden in the case of ABAC. RBAC reduces the burden since we extract all access information to roles, but this paradigm suffers from role explosion problems and can also become unmanageable. With NGAC we have everything we need in graphs — in a compact, centralized fashion.\nWhen access decisions are complex, processing times of ABAC can rise exponentially. RBAC becomes especially hard to manage at scale, while NGAC scales linearly.\nWhere NGAC really shines is in flexibility. It can be configured to allow or disallow access based not only on object attributes, but also on other conditions — time, location, phase of the moon, and so on.\nOther key advantages of NGAC include the ability to set policies consistently (to meet compliance requirements) and the ability to set ephemeral policies. For example, NGAC could grant a developer one-time access to resources during an outage, without leaving unnecessary permissions in place that could later lead to a security breach. NGAC can evaluate and combine multiple policies in a single access decision, while keeping its linear time complexity.\nSummary The following table compares ABAC, RBAC, and NGAC in several aspects.\nNGAC vs RBAC vs ABAC  In conclusion:\n RBAC is simpler and has good performance, but can suffer at scale. ABAC is flexible, but performance and auditability are a problem. NGAC fixes those gaps by using a novel, elegant revolutionary approach: overlay access policies on top of an existing representation of the world, provided by the user. You can model RBAC and ABAC policies as well.  References  Guide to Attribute-Based Access Control (ABAC) Definition and Considerations  Deploying ABAC policies using RBAC Systems  RBAC vs. ABAC: What’s the Difference?  Role Explosion: The Unintended Consequence of RBAC  Exploring the Next Generation of Access Control Methodologies   ","permalink":"https://jimmysong.io/en/blog/why-you-should-choose-ngac-as-your-access-control-model/","summary":"This article will introduce you to the next generation permission control model, NGAC, and compare ABAC, RABC, and explain why you should choose NGAC.","title":"Why You Should Choose NGAC as Your Access Control Model"},{"content":"IstioCon 2021 poster（Jimmy Song）   Topic: Service Mesh in China Time: February 23rd, 10:00 - 10:10 am Beijing time How to participate: IstioCon 2021 website  Cost: Free  From February 22-25, Beijing time, the Istio community will be hosting the first IstioCon online, and registration is free to attend! I will be giving a lightning talk on Tuesday, February 23rd (the 12th day of the first month of the lunar calendar), as an evangelist and witness of Service Mesh technology in China, I will introduce the Service Mesh industry and community in China.\nI am a member of the inaugural IstioCon organizing committee with Zhonghu Xu (Huawei) and Shaojun Ding (Intel), as well as the organizer of the China region. Considering Istio’s large audience in China, we have arranged for Chinese presentations that are friendly to the Chinese time zone. There will be a total of 14 sharing sessions in Chinese, plus dozens more in English. The presentations will be in both lightning talk (10 minutes) and presentation (40 minutes) formats.\nJoin the Cloud Native Community Istio SIG to participate in the networking at this conference. For the schedule of IstioCon 2021, please visit IstioCon 2021 official website , or click for details.\n","permalink":"https://jimmysong.io/en/notice/istiocon-2021/","summary":"IstioCon 2021, I'll be giving a lightning talk, February 22nd at 10am BST.","title":"IstioCon 2021 Lightning Talk Preview"},{"content":"The ServiceMesher website has lost connection with the webhook program on the web publishing server because the GitHub where the code is hosted has has been “lost” and the hosting server is temporarily unable to log in, so the site cannot be updated. Today I spent a day migrating all the blogs on ServiceMesher to the Cloud Native Community website cloudnative.to , and as of today, there are 354 blogs on the Cloud Native Community.\nServiceMesher blogs  Now we plan to archive ServiceMesher official GitHub (all pages under the servicemesher.com domain) We are no longer accepting new PRs, so please submit them directly to the Cloud Native Community . Thank you all!\n","permalink":"https://jimmysong.io/en/notice/servicemesher-blog-merged/","summary":"ServiceMesher website is no longer maintained, plan to archive the website code, the blog has been migrated to Cloud Native Community, please submit the new blog to Cloud Native Community.","title":"ServiceMesher website is no longer maintained, the original blog has been migrated to the cloud native community"},{"content":"In this article, I’ll give you an overview of Istio ‘s history of virtual machine integration support. In particular, the introduction of the smart DNS proxy and WorkloadGroup in Istio 1.8, which makes virtual machines and containers equivalent at the resource abstraction level.\nI will show you a tumultuous odyssey of Istio’s virtual machine integration. Tetrate, the enterprise service mesh company that made pushing Istio to run everywhere part of its founding mission, has used VM features extensively in customer deployments and has been instrumental in pushing VMs to Istio upstream.\nPreface In my previous article , I talked about how Istio 1.7 supported virtual machines. But at that time, late October, virtual machines were still not seamlessly integrated into Istio — there was still a lot of manual work required. Now, Istio 1.8 has added WorkloadGroup and smart DNS proxy, which allows non-Kubernetes workloads like VMs to become first-class citizens in Istio — just like pods.\nWith or without a sidecar installed for virtual machines, until 1.7 you could not resolve the DNS name of a Kubernetes service unless a kube-external DNS was configured — which is the last piece of virtual machine integration in Istio. This shortcoming has finally been fixed in Istio 1.8.\nWhy Is Virtual Machine Support Important? In the process of migrating our applications to cloud native architectures and continuously containerizing them, we will go through three phases as shown in the figure below.\nCloud Native Stages   Stage 1: All applications are deployed on virtual machines Stage 2: Applications are deployed on both virtual machines and containers, are migrating from virtual machines to containers, and are using Kubernetes to manage containers. Stage 3: All applications are deployed in containers first, using Kubernetes to manage containers and Istio to manage service-to-service communication.  The above diagram is artificially simplified: in reality, there might be multiple hybrid clouds, multiple regions, multiple clusters, etc. Plus, at stage 3 containers and virtual machines may remain in long-term coexistence, but the trend of containerization remains unchanged.\nIstio’s History of Virtual Machine Support Istio’s support for virtual machines is a long process, an odyssey of sorts.\n0.2: Istio Mesh Expansion As of version 0.2, Istio added virtual machines to the Mesh via Istio Mesh Expansion , provided that the following prerequisites were met.\n Virtual machines must have direct access to the application’s pods via IP address, which requires a flat network between the container and the VM via VPC or VPN; and virtual machines do not need access to the Cluster IP, but rather direct access to the service’s endpoints. Virtual machines must have access to Istio’s control plane services (Pilot, Mixer, CA, now being integrated as Istiod), which can expose the control plane endpoints to virtual machines by deploying load balancers in the Istio Mesh. (optional) the virtual machine has access to the DNS server inside the Mesh (deployed in Kubernetes).  The steps to integrate a virtual machine are as follows.\n Create an internal load balancer for the Istio control plane service and the DNS service for the Kubernetes cluster. Generate a configuration file for the Istio Service CIDR, Service Account token, security certificate, and IP of the Istio Control Plane Service (the IP exposed through the Internal Load Balancer) and send it to the virtual machine. Setup the Istio component, dnsmaq (for DNS discovery), in the virtual machine; so that the virtual machine can access the services in the mesh using FQDN, to ensure that the virtual machine can correctly resolve the Cluster IP of the services in the mesh. To run the service in a virtual machine, you need to configure the sidecar, add inbound ports to be intercepted, then restart Istio and also run istioctl to register the service.  The following figure shows the detailed flow from integrating a virtual machine to accessing services in the virtual machine in a mesh.\nFigure 1  Figure 1\n The DNS is hijacked by dnsmasq deployed in the virtual machine, which allows it to correctly obtain the Cluster IP of the Istio service (Kubernetes’ built-in DNS). Access to Kubernetes’ built-in DNS service (which is exposed outside the cluster via the Internal Load Balancer and can be accessed directly). Return the Cluster IP resolved by productpage.bookinfo.svc.cluster.local, noting that the IP address is not directly accessible, but failure to be DNS resolved will result in a failed VM request for the service. The virtual machine’s call to services in a mesh is hijacked by the sidecar proxy. Since the proxy is connected to the Istio control plane, the endpoints of the service can be queried via xDS, so traffic will be forwarded to one of the endpoints. To access VM services in mesh, you need to manually add VM services to mesh using the istioctl register command, which essentially registers the VM services to the service and endpoint in Kubernetes. Services in the mesh can be accessed using the VM-registered service name (FQDN, e.g. mysql.vm.svc.cluster.local).  The above Istio support for virtual machines continued with Istio 1.0, which introduced a new API ServiceEntry with Istio 1.1, that allows additional entries to be added to Istio’s internal service registry so that services in the mesh can access/route to these manually specified services. The istioctl register command is no longer needed and will be deprecated in Istio 1.9.\nThe istioctl experimental add-to-mesh command has been added to Istio 1.5 to add services from a virtual machine to a mesh, and it works just like the istioctl register.\n1.6 to 1.7: New Resource Abstractions Istio introduced a new resource type, WorkloadEntry , in traffic management from version 1.6 , to abstract virtual machines so that they can be added to the mesh as equivalent loads to the pods in Kubernetes; with traffic management, security management, observability, etc. The mesh configuration process for virtual machines is simplified with WorkloadEntry, which selects multiple workload entries and Kubernetes pods based on the label selector specified in the service entry.\nIstio 1.8 adds a resource object for WorkloadGroup that provides a specification that can include both virtual machines and Kubernetes workloads, designed to mimic the existing sidecar injection and deployment specification model for Kubernetes workloads to bootstrap Istio agents on the VMs.\nBelow is a comparison of resource abstraction levels for virtual machines versus workloads in Kubernetes.\n   Item Kubernetes Virtual Machine     Basic schedule unit Pod WorkloadEntry   Component Deployment WorkloadGroup   Service register and discovery Service ServiceEntry    From the above diagram, we can see that for virtual machine workloads there is a one-to-one correspondence with the workloads in Kubernetes.\nEverything seems perfect at this point. However, exposing the DNS server in the Kubernetes cluster directly is a big security risk , so we usually manually write the domain name and Cluster IP pair of the service the virtual machine needs to access to the local /etc/hosts — but this is not practical for a distributed cluster with a large number of nodes.\nThe process of accessing the services inside mesh by configuring the local /etc/hosts of the virtual machine is shown in the following figure.\nFigure 2  Figure 2\n Registration of services in the virtual machine into the mesh. Manually write the domain name and Cluster IP pairs of the service to be accessed to the local /etc/hosts file in the virtual machine. Cluster IP where the virtual machine gets access to the service. The traffic is intercepted by the sidecar proxy and the endpoint address of the service to be accessed is resolved by Envoy. Access to designated endpoints of the service.  In Kubernetes, we generally use the Service object for service registration and discovery; each service has a separate DNS name that allows applications to call each other by using the service name. We can use ServiceEntry to register a service in a virtual machine into Istio’s service registry, but a virtual machine cannot access a DNS server in a Kubernetes cluster to get the Cluster IP if the DNS server is not exposed externally to the mesh, which causes the virtual machine to fail to access the services in the mesh. Wouldn’t the problem be solved if we could add a sidecar to the virtual machine that would transparently intercept DNS requests and get the Cluster IP of all services in the mesh, similar to the role of dnsmasq in Figure 1?\nAs of Istio 1.8 — Smart DNS Proxy With the introduction of smart DNS proxy in Istio 1.8, virtual machines can access services within the mesh without the need to configure /etc/hosts, as shown in the following figure.\nFigure 3  Figure 3\nThe Istio agent on the sidecar will come with a cached DNS proxy dynamically programmed by Istiod. DNS queries from the application are transparently intercepted and served by the Istio proxy in the pod or VM, with the response to DNS query requests, enabling seamless access from the virtual machine to the service mesh.\nThe WorkloadGroup and smart DNS proxy introduced in Istio 1.8 provide powerful support for virtual machine workloads, making legacy applications deployed in virtual machines fully equivalent to pods in Kubernetes.\nSummary In this odyssey of Istio’s virtual machine support, we can see the gradual realization of unified management of virtual machines and pods — starting with exposing the DNS server in the mesh and setting up dnsmasq in the virtual machine, and ending with using smart DNS proxies and abstracting resources such as WorkloadEntry, WorkloadGroup and ServiceEntry. This article only focuses on the single cluster situation, which is not enough to be used in real production. We also need to deal with security, multicluster, multitenancy, etc.\nReferenced resources  Tetrate Service Bridge — Across all compute bridging Kubernetes clusters, VMs, and bare metal  Expanding into New Frontiers — Smart DNS Proxying in Istio  Virtual Machine Installation — Istio documentation  How to Integrate Virtual Machines into Istio Service Mesh   ","permalink":"https://jimmysong.io/en/blog/istio-18-a-virtual-machine-integration-odyssey/","summary":"In this article, I’ll give you an overview of Istio‘s history of virtual machine integration support. In particular, the introduction of the smart DNS proxy and WorkloadGroup in Istio 1.8, which makes virtual machines and containers equivalent at the resource abstraction level.","title":"Istio 1.8: A Virtual Machine Integration Odyssey"},{"content":"A service mesh is a relatively simple concept, consisting of a bunch of network proxies paired with each service in an application, plus a set of task management processes. The proxies are called the data plane and the management processes are called the control plane in the Service Mesh. The data plane intercepts calls between different services and “processes” them; the control plane is the brain of the mesh that coordinates the behavior of proxies and provides APIs for operations and maintenance personnel to manipulate and observe the entire network.\nThe diagram below shows the architecture of a service mesh.\nService Mesh Architecture  Further, the service mesh is a dedicated infrastructure layer designed to enable reliable, fast, and secure inter-service invocation in microservices architectures. It is not a mesh of “services” but rather a mesh of “proxies” that services can plug into, thus abstracting the network from the application code. In a typical service mesh, these proxies are injected into each service deployment as a sidecar (and also may be deployed at the edge of the mesh). Instead of invoking services directly over the network, services invoke their local sidecar proxy, which in turn manages requests on behalf of the service, pushing the complexities of inter-service communications into a networking layer that can resolve them at scale. The set of interconnected sidecar proxies implements a so-called data plane, while on the other hand the service mesh control plane is used to configure proxies. The infrastructure introduced by a service mesh provides an opportunity, too, to collect metrics about the traffic that is flowing through the application.\nThe architecture of a service mesh The infrastructure layer of a service mesh is divided into two main parts: the control plane and the data plane.\nCharacteristics of the control plane\n Do not parse packets directly. Communicates with proxies in the control plane to issue policies and configurations. Visualizes network behavior. Typically provides APIs or command-line tools for configuration versioning and management for continuous integration and deployment.  Characteristics of the data plane\n Is usually designed with the goal of statelessness (though in practice some data needs to be cached to improve traffic forwarding performance). Directly handles inbound and outbound packets, forwarding, routing, health checking, load balancing, authentication, authentication, generating monitoring data, etc. Is transparent to the application, i.e., can be deployed senselessly.  Changes brought by the service mesh Decoupling of microservice governance from business logic\nA service mesh takes most of the capabilities in the SDK out of the application, disassembles them into separate processes, and deploys them in a sidecar model. By separating service communication and related control functions from the business process and synching them to the infrastructure layer, a service mesh mostly decouples them from the business logic, allowing application developers to focus more on the business itself.\nNote that the word “mostly” is mentioned here and that the SDK often needs to retain protocol coding and decoding logic, or even a lightweight SDK to implement fine-grained governance and monitoring policies in some scenarios. For example, to implement method-level call distributed tracing, the service mesh requires the business application to implement trace ID passing, and this part of the implementation logic can also be implemented through a lightweight SDK. Therefore, the service mesh is not zero-intrusive from a code level.\nUnified governance of heterogeneous environments\nWith the development of new technologies and staff turnover, there are often applications and services in different languages and frameworks in the same company, and in order to control these services uniformly, the previous practice was to develop a complete set of SDKs for each language and framework, which is very costly to maintain. With a service mesh, multilingual support is much easier by synching the main service governance capabilities to the infrastructure. By providing a very lightweight SDK, and in many cases, not even a separate SDK, it is easy to achieve unified traffic control and monitoring requirements for multiple languages and protocols.\nFeatures of service mesh Service mesh also has three major technical advantages over traditional microservice frameworks.\nObservability\nBecause the service mesh is a dedicated infrastructure layer through which all inter-service communication passes, it is uniquely positioned in the technology stack to provide uniform telemetry at the service invocation level. This means that all services are monitored as “black boxes.” The service mesh captures route data such as source, destination, protocol, URL, status codes, latency, duration, etc. This is essentially the same data that web server logs can provide, but the service mesh captures this data for all services, not just the web layer of individual services. It is important to note that collecting data is only part of the solution to the observability problem in microservice applications. Storing and analyzing this data needs to be complemented by mechanisms for additional capabilities, which then act as alerts or automatic instance scaling, for example.\nTraffic control\nWith a service mesh, services can be provided with various control capabilities such as intelligent routing (blue-green deployment, canary release, A/B test), timeout retries, circuit breaking, fault injection, traffic mirroring, etc. These are often features that are not available in traditional microservices frameworks but are critical to the system. For example, the service mesh carries the communication traffic between microservices, so it is possible to test the robustness of the whole application by simulating the failure of some microservices through rules for fault injection in the grid. Since the service mesh is designed to efficiently connect source request calls to their optimal destination service instances, these traffic control features are “destination-oriented.” This is a key feature of the service mesh’s traffic control capabilities.\nSecurity\nTo some extent, monolithic applications are protected by their single address space. However, once a monolithic application is broken down into multiple microservices, the network becomes a significant attack surface. More services mean more network traffic, which means more opportunities for hackers to attack the information flow. And service mesh provides the capabilities and infrastructure to protect network calls. The security-related benefits of service mesh are in three core areas: authentication of services, encryption of inter-service communications, and enforcement of security-related policies.\nService mesh has brought about tremendous change and has strong technical advantages, and has been called the second generation of “microservice architecture.” However, there is no silver bullet in software development. Traditional microservices architecture has many pain points, and service mesh is no exception. It has its limitations.\nIncreased complexity\nService mesh introduces sidecar proxies and other components into an already complex, distributed environment, which can greatly increase the overall chain and operational O\u0026M complexity. Ops needs to be more specialized. Adding a service mesh such as Istio to a container orchestrator such as Kubernetes often requires Ops to become an expert in both technologies in order to fully utilize the capabilities of both and to troubleshoot the problems encountered in the environment.\nLatency\nAt the link level, a service mesh is an invasive, complex technology that can add significant latency to system calls. This latency is on the millisecond level, but it can also be intolerable in special business scenarios.\nPlatform adaptation\nThe intrusive nature of service mesh forces developers and operators to adapt to highly autonomous platforms and adhere to the platform’s rules.\nThe relationship between service mesh and Kubernetes Kubernetes is essentially application lifecycle management, specifically the deployment and management (scaling, auto-recovery, publishing) of containerized applications. Service mesh decouples traffic management from Kubernetes, eliminating the need for a kube-proxy component for internal traffic, and manages inter-service and ingress traffic, security, and observability through an abstraction closer to the microservice application layer. The xDS used by Istio and Envoy is one of the protocol standards for service mesh configuration.\nOrganizations that use Kubernetes often turn to a service mesh to address the networking issues that arise with containerization — but notably, a service mesh can work with a legacy or a modern workload, and can be put in place prior to containerization for a faster, safer path to modernization.\nSummary Readers should look dialectically at the advantages and disadvantages of a service mesh compared with traditional microservices architecture. A service mesh can be a critical part of the evolutionary path of application architecture, from the earliest monolith to distributed, to microservices, containerization, container orchestration, to hybrid workloads and multi-cloud.\nLooking ahead, Kubernetes is exploding, and it has become the container orchestration of choice for enterprise greenfield applications. If Kubernetes has completely won the market and the size and complexity of Kubernetes-based applications continue to grow, there will be a tipping point, and service mesh will be necessary to effectively manage these applications. As service mesh technology continues to evolve and the architecture and functionality of its implementation products, such as Istio, continue to be optimized, service mesh will completely replace traditional microservice architectures as the architecture of choice for microservices and transformation to the cloud for enterprises.\nThis article was co-authored by Guangming Luo, a member of the ServiceMesher community and the CNC steering community.\n","permalink":"https://jimmysong.io/en/blog/what-is-a-service-mesh/","summary":"This article will take you through what a service mesh is, as well as its architecture, features, and advantages and disadvantages.","title":"What is a service mesh?"},{"content":"1.8 is the last version of Istio to be released in 2020 and it has the following major updates:\n Supports installation and upgrades using Helm 3. Mixer was officially removed. Added Istio DNS proxy to transparently intercept DNS queries from applications. WorkloadGroup has been added to simplify the integration of virtual machines.  WorkloadGroup is a new API object. It is intended to be used with non-Kubernetes workloads like Virtual Machines and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies.\nInstallation and Upgrades Istio starts to officially support the use of Helm v3 for installations and upgrades. In previous versions, the installation was done with the istioctl command-line tool or Operator. With version 1.8, Istio supports in-place and canary upgrades with Helm.\nEnhancing Istio’s Usability The istioctl command-line tool has a new bug reporting feature (istioctl bug-report ), which can be used to collect debugging information and get cluster status.\nThe way to install the add-on has changed: 1.7 istioctl is no longer recommended and has been removed in 1.8, to help solve the problem of add-on lagging upstream and to make it easier to maintain.\nTetrate is an enterprise service mesh company. Our flagship product, TSB, enables customers to bridge their workloads across bare metal, VMs, K8s, \u0026 cloud at the application layer and provide a resilient, feature-rich service mesh fabric powered by Istio, Envoy, and Apache SkyWalking.\nMixer, the Istio component that had been responsible for policy controls and telemetry collection, has been removed. Its functionalities are now being served by the Envoy proxies. For extensibility, service mesh experts recommend using WebAssembly (Wasm) to extend Envoy; and you can also try the GetEnvoy Toolkit , which makes it easier for developers to create Wasm extensions for Envoy. If you still want to use Mixer, you must use version 1.7 or older. Mixer continued receiving bug fixes and security fixes until Istio 1.7. Many features supported by Mixer have alternatives as specified in the Mixer Deprecation document, including the in-proxy extensions based on the Wasm sandbox API.\nSupport for Virtual Machines Istio’s recent upgrades have steadily focused on making virtual machines first-class citizens in the mesh. Istio 1.7 made progress to support virtual machines and Istio 1.8 adds a smart DNS proxy , which is an Istio sidecar agent written in Go. The Istio agent on the sidecar will come with a cache that is dynamically programmed by Istiod DNS Proxy. DNS queries from applications are transparently intercepted and served by an Istio proxy in a pod or VM that intelligently responds to DNS query requests, enabling seamless multicluster access from virtual machines to the service mesh.\nIstio 1.8 adds a WorkloadGroup , which describes a collection of workload instances. It provides a specification that the workload instances can use to bootstrap their proxies, including the metadata and identity. It is only intended to be used with non-k8s workloads like Virtual Machines, and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies. Using WorkloadGroups, Istio has started to help automate VM registration with istioctl experimental workload group .\nTetrate , the enterprise service mesh company, uses these VM features extensively in customers’ multicluster deployments, to enable sidecars to resolve DNS for hosts exposed at ingress gateways of all the clusters in a mesh; and to access them over mutual TLS.\nConclusion All in all, the Istio team has kept the promise made at the beginning of the year to maintain a regular release cadence of one release every three months since the 1.1 release in 2018, with continuous optimizations in performance and user experience for a seamless experience of brownfield and greenfield apps on Istio. We look forward to more progress from Istio in 2021.\n","permalink":"https://jimmysong.io/en/blog/istio-1-8-a-smart-dns-proxy-takes-support-for-virtual-machines-a-step-further/","summary":"WorkloadGroup is a new API object. It is intended to be used with non-Kubernetes workloads like Virtual Machines and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies.","title":"Istio 1.8: A Smart DNS Proxy Takes Support for Virtual Machines a Step Further"},{"content":"Istio is a popular service mesh to connect, secure, control, and observe services. When it was first introduced as open source in 2017, Kubernetes was winning the container orchestration battle and Istio answered the needs of organizations moving to microservices. Although Istio claims to support heterogeneous environments such as Nomad, Consul, Eureka, Cloud Foundry, Mesos, etc., in reality, it has always worked best with Kubernetes — on which its service discovery is based.\nIstio was criticized for a number of issues early in its development, for the large number of components, the complexity of installation and maintenance, the difficulty of debugging, a steep learning curve due to the introduction of too many new concepts and objects (up to 50 CRDs), and the impact of Mixer components on performance. But these issues are gradually being overcome by the Istio team. As you can see from the roadmap released in early 2020, Istio has come a long way.\nBetter integration of VM-based workloads into the mesh is a major focus for the Istio team this year. Tetrate also offers seamless multicloud connectivity, security, and observability, including for VMs, via its product Tetrate Service Bridge . This article will take you through why Istio needs to integrate with virtual machines and how you can do so.\nWhy Should Istio Support Virtual Machines? Although containers and Kubernetes are now widely used, there are still many services deployed on virtual machines and APIs outside of the Kubernetes cluster that needs to be managed by Istio mesh. It’s a huge challenge to unify the management of the brownfield environment with the greenfield.\nWhat Is Needed to Add VMs to the Mesh? Before the “how,” I’ll describe what is needed to add virtual machines to the mesh. There are a couple of things that Istio must know when supporting virtual machine traffic: which VMs have services that should be part of the mesh, and how to reach the VMs. Each VM also needs an identity, in order to communicate securely with the rest of the mesh. These requirements could work with Kubernetes CRDs, as well as a full-blown Service Registry like Consul. And the service account based identity bootstrapping could work as a mechanism for assigning workload identities to VMs that do not have a platform identity. For VMs that do have a platform identity (like EC2, GCP, Azure, etc.), work is underway in Istio to exchange the platform identity with a Kubernetes identity for ease of setting up mTLS communication.\nHow Does Istio Support Virtual Machines? Istio’s support for virtual machines starts with its service registry mechanism. The information about services and instances in the Istio mesh comes from Istio’s service registries, which up to this point have only looked at or tracked pods. In newer versions, Istio now has resource types to track and watch VMs. The sidecars inside the mesh cannot observe and control traffic to services outside the mesh, because they do not have any information about them.\nThe Istio community and Tetrate have done a lot of work on Istio’s support for virtual machines. The 1.6 release included the addition of WorkloadEntry, which allows you to describe a VM exactly as you would a host running in Kubernetes. In 1.7, the release started to add the foundations for bootstrapping VMs into the mesh automatically through tokens, with Istio doing the heavy lifting. Istio 1.8 will debut another abstraction called WorkloadGroup, which is similar to a Kubernetes Deployment object — but for VMs.\nThe following diagram shows how Istio models services in the mesh. The predominant source of information comes from a platform service registry like Kubernetes, or a system like Consul. In addition, the ServiceEntry serves as a user-defined service registry, modeling services on VMs or external services outside the organization.\n  Why install Istio in a virtual machine when you can just use ServiceEntry to bring in the services in the VMs?\nUsing ServiceEntry, you can enable services inside the mesh to discover and access external services; and in addition, manage the traffic to those external services. In conjunction with VirtualService, you can also configure access rules for the corresponding external service — such as request timeouts, fault injection, etc. — to enable controlled access to the specified external service.\nEven so, it only controls the traffic on the client-side, not access to the introduced external service to other services. That is, it cannot control the behavior of the service as the call initiator. Deploying sidecars in a virtual machine and introducing the virtual machine workload via workload selector allows the virtual machine to be managed indiscriminately, like a pod in Kubernetes.\nFuture As you can see from the bookinfo demo , there is too much manual work involved in the process and it’s easy to go wrong. In the future, Istio will improve VM testing to be realistic, automate bootstrapping based on platform identity, improve DNS support and istioctl debugging, and more. You can follow the Istio Environment Working Group for more details about virtual machine support.\nReferences  Virtual Machine Installation  Virtual Machines in Single-Network Meshes  Istio: Bringing VMs into the Mesh (with Cynthia Coan)  Bridging Traditional and Modern Workloads   ","permalink":"https://jimmysong.io/en/blog/how-to-integrate-virtual-machines-into-istio-service-mesh/","summary":"Better integration of virtual machine-based workloads into the service mesh is a major focus for the Istio team this year, and Tetrate also provides seamless multi-cloud connectivity, security and observability, including for virtual machines, through its product Tetrate Service Bridge. This article will show you why Istio needs to integrate with virtual machines and how.","title":"How to Integrate Virtual Machines into Istio Service Mesh"},{"content":"Today is my 914th day and also the last day with Ant Group , tomorrow is September 1st, which is usually the day school starts, and everyone at Alibaba is known as “classmate”, tomorrow I will join Tetrate , and that’s kind of starting my new semester!\nAnt/Alibaba and the Cloud Native Community To date, Ant/Alibaba Group has had a profound impact on my career, especially its corporate culture and values, and the Alibaba recruiting philosophy of “finding like-minded people”, and isn’t the process of creating the Cloud Native Community also a process of finding like-minded people? Cloud Native Community is like a small society, I don’t want it to have much social value, but only want it to make a small but beautiful change to individuals, to enterprises and to society. I constantly think about myself as an individual and as an employee, especially as an initiator of the community. What is my mission as an individual, an employee, and especially as an initiator of a community? What role should I play in the company? Where is this community going? I’m fumbling along, but because of your support, it makes me stronger and more committed to the adoption and application of cloud native technology in China, outside of me I may have gone faster, but now with the community together we will go further!\n24 June 2019, Shanghai, KubeCon China 2019  June 24, 2019, Shanghai, KubeCon China 2019\nJoining Tetrate Over the past two years, I’ve been working hard to promote Istio and Service Mesh technology, and with funding from Ant Group, I started the ServiceMesher Community to bring Service Mesh technology to China. Next I want to bring Chinese practice to the world.\nAs a Developer Advocate, the most important thing is not to stop learning, but to listen and take stock. Over the past two years, I’ve seen a lot of people show interest in Service Mesh, but not enough to understand the risks and lack of knowledge about the new technology. I’m excited to join this Service Mesh-focused startup Tetrate , a global telecommuting startup with products built around open source Istio , [Envoy](https:/ /envoyproxy.io) and Apache SkyWalking , it aims to make it to be the cloud native network infrastructure. Here are several maintainers of these open source projects, such as Sheng Wu , Zack Butcher , Lizan Zhou , etc., and I believe that working with them can help you understand and apply Service Mesh quickly and effectively across cloud native.\nMore Earlier this year as I was preparing for the Cloud Native community, I set the course for the next three years - cloud native, open source and community. The road to pursue my dream is full of thorns, not only need courage and perseverance, but also need you to be my strong backing, I will overcome the thorns and move forward. Open source belongs to the world, to let the world understand us better, we must be more active into the world. I hope that China’s open source tomorrow will be better, I hope that Service Mesh technology will be better applied by the enterprises in China, I hope that cloud native can benefit the public, and I hope that we can all find our own mission.\nWe are hiring now, if you are interested with Tetrate , please send your resume to careers@tetrate.io .\n","permalink":"https://jimmysong.io/en/blog/moving-on-from-ant-group/","summary":"Today is my last day at Ant and tomorrow I'm starting a new career at Tetrate.","title":"New Beginning - Goodbye Ant, Hello Tetrate"},{"content":"Just tonight, the jimmysong.io website was moved to the Alibaba Cloud Hong Kong node. This is to further optimize the user experience and increase access speed. I purchased an ECS on the Alibaba Cloud Hong Kong node, and now I have a public IP and can set subdomains. The website was previously deployed on GitHub Pages, the access speed is average, and it has to withstand GitHub instability. Impact (In recent years, GitHub downtime has occurred).\nMeanwhile, the blog has also done a lot to improve the site, thanks to Bai Jun away @baijunyao strong support, a lot of work for the revision of the site, including:\n Changed the theme color scheme and deepened the contrast Use aligolia to support full site search Optimized mobile display Articles in the blog have added zoom function Added table of contents to blog post  This site is built on the theme of educenter .\nThanks to the majority of netizens who have supported this website for several years. The website has been in use for more than three years and has millions of visits. It has undergone two major revisions before and after, on January 31, 2020 and October 8, 2017, respectively. And changed the theme of the website. In the future, I will share more cloud-native content with you as always, welcome to collect, forward, and join the cloud-native community to communicate with the majority of cloud-native developers.\n","permalink":"https://jimmysong.io/en/notice/migrating-to-alibaba-cloud/","summary":"Move the website to the Alibaba Cloud Hong Kong node to increase the speed of website access and the convenience of obtaining public IP and subdomain names.","title":"Move to Alibaba Cloud Hong Kong node"},{"content":"  Just the other day, Java just celebrated its 25th birthday , and from the time of its birth it was called “write once, run everywhere”, but more than 20 years later, there is still a deep gap between programming and actual production delivery. the world of IT is never short of concepts, and if a concept doesn’t solve the problem, then it’s time for another layer of concepts. it’s been 6 years since Kubernetes was born, and it’s time for the post-Kubernetes era - the era of cloud-native applications!\nCloud Native Stage  This white paper will take you on a journey to explore the development path of cloud-native applications in the post-Kubernetes era.\nHighlights of the ideas conveyed include.\n Cloud-native has passed through a savage growth period and is moving towards uniform application of standards. Kubernetes’ native language does not fully describe the cloud-native application architecture, and the development and operation functions are heavily coupled in the configuration of resources. Operator’s expansion of the Kubernetes ecosystem has led to the fragmentation of cloud-native applications, and there is an urgent need for a unified application definition standard. The essence of OAM is to separate the R\u0026D and O\u0026M concerns in the definition of cloud-native applications, and to further abstract resource objects, simplify and encompass everything. “Kubernetes Next Generation” refers to the fact that after Kubernetes became the infrastructure layer standard, the focus of the cloud-native ecology is being overtaken by the application layer, and the last two years have been a powerful exploration of the hot Service Mesh process, and the era of cloud-native application architecture based on Kubernetes is coming.  Kubernetes has become an established operating platform for cloud-native applications, and this white paper will expand with Kubernetes as the default platform, including an explanation of the OAM-based hierarchical model for cloud-native applications.\n","permalink":"https://jimmysong.io/en/notice/guide-to-cloud-native-app/","summary":"Take you on a journey through the post-Kubernetes era of cloud-native applications.","title":"Guide to Cloud Native Application"},{"content":"At the beginning of 2020, due to the outbreak of the Crona-19 pandemic, employees around the world began to work at home. Though the distance between people grew farer, there was a group of people, who were us working in the cloud native area, gathered together for a common vision. During the past three months, we have set up the community management committee and used our spare time working together to complete the preparatory work for the community. Today we are here to announce the establishment of the Cloud Native Community.\nBackground Software is eating the world. —— Marc Andreessen\nThis sentence has been quoted countless times, and with the rise of Cloud Native, we’d like to talk about “Cloud Native is eating the software.” As more and more enterprises migrate their services to the cloud, the original development mode of enterprises cannot adapt to the application scenarios in the cloud, and it is being reshaped to conform to the cloud native standard.\nSo what is cloud native? Cloud native is a collection of best practices in architecture, r\u0026d process and team culture to support faster innovation, superior user experience, stable and reliable user service and efficient r\u0026d. The relationship between the open source community and the cloud native is inseparable. It is the existence of the open source community, especially the end user community, that greatly promotes the continuous evolution of cloud native technologies represented by container, service mesh and microservices.\nCNCF (Cloud Native Computing Foundation) holds Cloud Native conference every year in the international community, which has a wide audience and great influence. But it was not held in China for the first time until 2018, after several successful international events. However, there are no independent foundations or neutral open source communities in China. In recent years, many cloud native enthusiasts in China have set up many communication groups and held many meetups, which are very popular. Many excellent open source projects have emerged in the cloud native field, but there is no organized neutral community for overall management. Under this background, the Cloud Native Community emerges at the right moment.\nAbout Cloud Native Community is an open source community with technology, temperature and passion. It was founded spontaneously by a group of industry elites who love open source and uphold the principle of consensus, co-governance, co-construction and sharing. The aim of the community is: connection, neutral, open source. We are based in China, facing the world, enterprise neutrality, focusing on open source, and giving feedback to open source.\nIntroduction for the Steering Community：https://cloudnative.to/en/team/ .\nYou will gain the followings after joining the community:\n Knowledge and news closer to the source A more valuable network More professional and characteristic consultation Opportunities to get closer to opinion leaders Faster and more efficient personal growth More knowledge sharing and exposure opportunities More industry talent to be found  Contact Contact with us.\n Email: mailto:contact@cloudnative.to  Twitter: https://twitter.com/CloudNativeTo   ","permalink":"https://jimmysong.io/en/notice/cloud-native-community-announecement/","summary":"Today the Community Steering Committee announced the official formation of the Cloud Native Community.","title":"Establishment of the Cloud Native Community"},{"content":"This article is a rework of previously written content and is included in the Istio Handbook of the ServiceMesher community . Other chapters are still being compiled.\nPeople who have just heard of Service Mesh and tried Istio may have the following questions:\n Why does Istio bind Kubernetes? What roles do Kubernetes and Service Mesh play in cloud native? What aspects of Kubernetes has Istio extended? What problems have been solved? What is the relationship between Kubernetes, xDS protocols (Envoy , MOSN, etc) and Istio? Should I use Service Mesh?  In this section, we will try to guide you through the internal connections between Kubernetes, the xDS protocol, and Istio Service Mesh. In addition, this section will also introduce the load balancing methods in Kubernetes, the significance of the xDS protocol for Service Mesh, and why Istio is needed in time for Kubernetes.\nUsing Service Mesh is not to say that it will break with Kubernetes, but that it will happen naturally. The essence of Kubernetes is to perform application lifecycle management through declarative configuration, while the essence of Service Mesh is to provide traffic and security management and observability between applications. If you have built a stable microservice platform using Kubernetes, how do you set up load balancing and flow control for calls between services?\nThe xDS protocol created by Envoy is supported by many open source software, such as Istio , Linkerd , MOSN, etc. Envoy’s biggest contribution to Service Mesh or cloud native is the definition of xDS. Envoy is essentially a proxy. It is a modern version of proxy that can be configured through APIs. Based on it, many different usage scenarios are derived, such as API Gateway, Service Mesh. Sidecar proxy and Edge proxy in.\nThis section contains the following\n Explain the role of kube-proxy. Kubernetes’ limitations in microservice management. Describe the features of Istio Service Mesh. Describe what xDS includes. Compare some concepts in Kubernetes, Envoy and Istio Service Mesh.  Key takeaways If you want to know everything in advance, here are some of the key points from this article:\n The essence of Kubernetes is application lifecycle management, specifically deployment and management (scaling, scaling, automatic recovery, release). Kubernetes provides a scalable and highly resilient deployment and management platform for microservices. The foundation of Service Mesh is a transparent proxy. After the traffic between microservices is intercepted through sidecar proxy, the behavior of microservices is managed through the control plane configuration. Service Mesh decoupled from Kubernetes traffic management, the internal flow without the need of Service Mesh kube-proxy supporting components, micro-services closer to abstract the application layer by, for traffic between management services, security and observability. xDS defines the protocol standards for Service Mesh configuration. Service Mesh is a higher-level abstraction of services in Kubernetes. Its next step is serverless.  Kubernetes vs Service Mesh The following figure shows the service access relationship between Kubernetes and Service Mesh (one sidecar per pod mode).\nkubernetes vs service mesh  Traffic forwarding\nEach node of the cluster Kubernetes a deployed kube-proxy assembly Kubernetes API Server may communicate with the cluster acquired service information, and then set iptables rules, sends a request for a service directly to the corresponding Endpoint (belonging to the same group service pod).\nService discovery\nService registration in Service Mesh  Istio Service Mesh can use the service in Kubernetes for service registration. It can also connect to other service discovery systems through the platform adapter of the control plane, and then generate the configuration of the data plane (using CRD statements, stored in etcd), a transparent proxy for the data plane. (Transparent proxy) is deployed in the sidecar container in each application service pod. These proxy need to request the control plane to synchronize the proxy configuration. The reason why is a transparent proxy, because there is no application container fully aware agent, the process kube-proxy components like the need to block traffic, but kube-proxythat blocks traffic to Kubernetes node and sidecar proxy that blocks out of the Pod For more information, see Understanding Route Forwarding by the Envoy Sidecar Proxy in Istio Service Mesh .\nDisadvantages of Service Mesh\nBecause each node on Kubernetes many runs Pod, the original kube-proxyrouting forwarding placed in each pod, the distribution will lead to a lot of configuration, synchronization, and eventual consistency problems. In order to perform fine-grained traffic management, a series of new abstractions will be added, which will further increase the user’s learning costs. However, with the popularization of technology, this situation will gradually ease.\nAdvantages of Service Mesh\nkube-proxy The settings are globally effective, and fine-grained control of each service cannot be performed. Service Mesh uses sidecar proxy to extract the control of traffic in Kubernetes from the service layer, which can be further expanded.\nkube-proxy component In Kubernetes cluster, each Node to run a kube-proxy  process. kube-proxy Responsible for the Service realization of a VIP (virtual IP) form. In Kubernetes v1.0, the proxy is implemented entirely in userspace. Kubernetes v1.1 adds the iptables proxy mode , but it is not the default operating mode. As of Kubernetes v1.2, the iptables proxy is used by default. In Kubernetes v1.8.0-beta.0, the ipvs proxy mode was added . More about kube-proxy component description please refer kubernetes Description: service and kube-proxy principle and use IPVS achieve Kubernetes inlet flow load balancing .\nkube-proxy flaws The disadvantages of kube-proxy :\n First, if forwarded pod can not provide normal service, it does not automatically try another pod, of course, this can liveness probes be solved. Each pod has a health check mechanism. When there is a problem with the health of the pod, kube-proxy will delete the corresponding forwarding rule. In addition, nodePorttypes of services cannot add TLS or more sophisticated message routing mechanisms.\n Kube-proxy implements load balancing of traffic among multiple pod instances of the Kubernetes service, but how to fine-grained control the traffic between these services, such as dividing the traffic into different application versions by percentage (these applications belong to the same service , But on a different deployment), do canary release and blue-green release? Kubernetes community gives the method using the Deployment do canary release , essentially by modifying the pod of the method label different pod to be classified into the Deployment of Service.\nKubernetes Ingress vs. Istio Gateway Speaking above kube-proxythe flow inside the only route Kubernetes clusters, and we know that Pod Kubernetes cluster located CNI outside the network created, external cluster is unable to communicate directly with, so Kubernetes created in the ingress of this resource object, which is located by the Kubernetes edge nodes (such nodes can be many or a group) are driven by the Ingress controller, which is responsible for managing north-south traffic . Ingress must be connected to various ingress controllers, such as nginx ingress controller and traefik . Ingress is only applicable to HTTP traffic, and its usage is also very simple. It can only route traffic by matching limited fields such as service, port, and HTTP path, which makes it unable to route TCP traffic such as MySQL, Redis, and various private RPCs. To directly route north-south traffic, you can only use Service’s LoadBalancer or NodePort. The former requires cloud vendor support, while the latter requires additional port management. Some Ingress controllers support exposing TCP and UDP services, but they can only be exposed using Services. Ingress itself does not support it, such as the nginx ingress controller . The exposed port of the service is configured by creating a ConfigMap.\nIstio Gateway is similar to Kubernetes Ingress in that it is responsible for north-south traffic to the cluster. GatewayThe load balancer described by Istio is used to carry connections in and out of the edge of the mesh. The specification describes a series of open ports and the protocols used by these ports, SNI configuration for load balancing, and so on. Gateway is a CRD extension . It also reuses the capability of sidecar proxy. For detailed configuration, please refer to Istio official website .\nxDS protocol You may have seen the following picture when you understand Service Mesh. Each block represents an instance of a service, such as a Pod in Kubernetes (which contains a sidecar proxy). The xDS protocol controls all traffic in Istio Service Mesh. The specific behavior is to link the squares in the figure below.\nService Mesh diagram  The xDS protocol was proposed by Envoy . The original xDS protocols in the Envoy v2 API refer to CDS (Cluster Discovery Service), EDS (Endpoint Discovery Service), LDS (Listener Discovery Service), and RDS (Route Discovery Service). Later, in the v3 version, Scoped Route Discovery Service (SRDS), Virtual Host Discovery Service (VHDS), Secret Discovery Service (SDS), and Runtime Discovery Service (RTDS) were developed. See the xDS REST and gRPC protocol for details .\nLet’s take a look at the xDS protocol with a service with two instances each.\nxDS protocol  The arrow in the figure above is not the path or route after the traffic enters the proxy, nor is it the actual sequence. It is an imagined xDS interface processing sequence. In fact, there are cross references between xDS.\nAgents that support the xDS protocol dynamically discover resources by querying files or managing servers. In summary, the corresponding discovery service and its corresponding API are called xDS. Envoy by subscription (subscription) to get the resources the way, there are three ways to subscribe:\n File subscription : Monitor files in the specified path, the easiest way to find dynamic resource is to save it in a file and path configuration in ConfigSource the pathparameter. gRPC streaming subscription : Each xDS API can be individually configured ApiConfigSource to point to the cluster address of the corresponding upstream management server. Polling REST-JSON polling subscription : A single xDS API can perform synchronous (long) polling of REST endpoints.  For details of the above xDS subscription methods, please refer to the xDS protocol analysis . Istio uses gRPC streaming subscriptions to configure sidecar proxy for all data planes.\nThe article introduces the overall architecture of the Istio pilot, the generation of proxy configuration, the function of the pilot-discovery module, and the CDS, EDS, and ADS in the xDS protocol. For details on ADS, please refer to the official Envoy documentation .\nxDS protocol highlights Finally, summarize the main points about the xDS protocol:\n CDS, EDS, LDS, and RDS are the most basic xDS protocols, and they can be updated independently. All Discovery Services can connect to different Management Servers, which means that there can be multiple servers managing xDS. Envoy has made a series of extensions based on the original xDS protocol, adding SDS (Key Discovery Service), ADS (Aggregated Discovery Service), HDS (Health Discovery Service), MS (Metric Service), RLS (Rate Limiting Service) Wait for the API. To ensure data consistency, if used directly xDS original API, it needs to ensure that such sequential update: CDS -\u003e EDS -\u003e LDS -\u003e RDS, which is to follow the electronic engineering before-break (Make-Before-Break) The principle is to establish a new connection before disconnecting the original connection. The application in routing is to prevent the situation where the upstream cluster cannot be found and the traffic is dropped when a new routing rule is set, similar to the circuit Open circuit. CDS sets which services are in the service mesh. EDS sets which instances (Endpoints) belong to these services (Cluster). LDS sets the listening port on the instance to configure routing. The routing relationship between RDS final services should ensure that RDS is updated last.  Envoy Envoy is the default sidecar in Istio Service Mesh. Based on Enovy, Istio has extended its control plane in accordance with Envoy’s xDS protocol. Before talking about the Envoy xDS protocol, we need to be familiar with the basic terms of Envoy. The following lists the basic terms and data structure analysis in Envoy. For a detailed introduction to Envoy , please refer to the official Envoy document . As for how Envoy works as a forwarding proxy in Service Mesh (not limited to Istio), please refer to NetEase Cloud Liu Chao this in-depth interpretation of the technical details behind the Service Mesh and understanding Istio Service Mesh Envoy agent in Sidecar injection and traffic hijacking , in which the article refers to some of the points, the details will not be repeated.\nEnvoy proxy architecture diagram  Basic terminology Here are the basic terms in Enovy you should know:\n Downstream : The downstream host connects to Envoy, sends a request and receives a response, that is, the host sending the request. Upstream : The upstream host receives the connection and request from Envoy and returns a response, that is, the host that accepted the request. Listener : The listener is a named network address (for example, port, unix domain socket, etc.), and downstream clients can connect to these listeners. Envoy exposes one or more listeners to connect to downstream hosts. Cluster : A cluster is a group of logically identical upstream hosts connected to Envoy. Envoy discovers members of the cluster through service discovery . You can choose to determine the health status of cluster members through active health checks . Envoy uses load balancing policies to decide which member of the cluster to route requests to.  Envoy can set multiple Listeners, and each Listener can also set a filter chain, and the filters are extensible, which can make it easier for us to manipulate traffic behavior, such as setting encryption, private RPC, and so on.\nThe xDS protocol was proposed by Envoy and is now the default sidecar proxy in Istio. However, as long as the xDS protocol is implemented, it can theoretically be used as a sidecar proxy in Istio, such as the open source proxy MOSN by Ant Group .\nIstio Service Mesh Istio service mesh architecture diagram  Istio is a very feature-rich Service Mesh, which includes the following functions:\n Traffic Management: This is the most basic feature of Istio. Policy control: Implemented through Mixer components and various adapters to implement access control systems, telemetry capture, quota management, and billing. Observability: Achieved through Mixer. Security certification: Citadel components do key and certificate management.  Traffic Management in Istio Istio defined as the CRD to help users perform traffic management:\n Gateway : Gateway describes a load balancer running at the edge of the network for receiving incoming or outgoing HTTP / TCP connections. VirtualService : VirtualService actually connects Kubernetes services to Istio Gateway. It can also do more, such as defining a set of traffic routing rules to apply when a host is addressed. DestinationRule : DestinationRule The defined policy determines the access policy of the traffic after routing processing. Simply put, it defines how the traffic is routed. These policies can define load balancing configurations, connection pool sizes, and external detection (used to identify and evict unhealthy hosts in a load balancing pool) configuration. EnvoyFilter : The EnvoyFilter object describes filters for proxy services that can customize the proxy configuration generated by Istio Pilot. This configuration is rarely used by beginning users. ServiceEntry : By default, services in Istio Service Mesh cannot discover services outside Mesh. It ServiceEntry can add additional entries to the service registry inside Istio, so that services automatically discovered in the mesh can access and route to these manual Joined services.  Kubernetes vs xDS vs Istio After the reading of the above Kubernetes kube-proxyafter abstraction component, and XDS Istio in traffic management, we will take you far as the traffic management aspect of comparison components corresponding to the three / protocol (note, not completely three equivalents).\n   Governors xDS Istio Service Mesh     Endpoint Endpoint -   Service Route VirtualService   kube-proxy Route DestinationRule   kube-proxy Listener EnvoyFilter   Ingress Listener Gateway   Service Cluster ServiceEntry    Conclusion If you say that the objects managed by Kubernetes are Pods, then the objects managed by Service Mesh are Service. Therefore, it is a natural thing to apply Service Mesh after using Kubernetes to manage microservices. If you do n’t want to manage even the Service, use serverless platforms like knative, but that’s what comes next.\nThe function of Envoy/MOSN is not just for traffic forwarding. The above concepts are just the tip of the iceberg in Istio’s new layer of abstraction over Kubernetes. This will be the beginning of the book.\nReference  In-depth interpretation of the technical details behind Service Mesh-cnblogs.com  Understanding Envoy Proxy Sidecar Injection and Traffic Hijacking in Istio Service Mesh - jimmysong.io  Introduction to kubernetes: service and kube-proxy principles - cizixs.com  Kubernetes Ingress Traffic Load Balancing Using IPVS - jishu.io  xDS REST and gRPC protocol - envoyproxy.io   ","permalink":"https://jimmysong.io/en/blog/service-mesh-the-microservices-in-post-kubernetes-era/","summary":"This article is a rework of previously written content and is included in the Istio Handbook of the ServiceMesher community . Other chapters are still being compiled.","title":"Service Mesh - the Microservices in Post Kubernetes Era"},{"content":"2020 is indeed a bad start. In less than a month, I hardly heard any good news:\nTrump assassinated Major General Sulaymani of Iran; this pneumonia outbreak in Wuhan; the news that my basketball icon Kobe died of a helicopter crash really shocked me, and the Lakers said goodbye on the 24th. This gave me another spiritual blow during the Spring Festival, which was originally lacking in interest.\n2020 is bound to be a year deeply remembered in all humankind. In the last few days of the first month of the year, I decided to revise the website. The first reason was that I could n’t go out during the extended Chinese New Year holiday, and it was boring at home. And many pictures on the website were saved on Weibo map beds. The picture bed is unstable, causing many photos to be irretrievable; coupled with a habit of organizing the website every long holiday (the last revision of the website was completed during the National Day holiday in 2018, completed at home for 7 days), so I decided to The website has been revised again and it has become what it is now.\nFeatures The website has the following features after this revision:\n Reorganize the website content, the structure is more reasonable Support email subscription Images are stored on Github Responsive static website, card design, better user experience  Everyone is welcome to enter your email address in the email input box at the bottom of the page. Once there is an important content update on this site, we will push you through the email as soon as possible.\n","permalink":"https://jimmysong.io/en/notice/website-revision-notice/","summary":"In the last days of the first month of 2020, I decided to revamp the website.","title":"jimmysong.io website revision notice"},{"content":"A few days ago during the Mid-Autumn Festival, I translated Google’s Engineering Practices documentation , which is open source on Github. The original document Github address: https://github.com/google/eng-practices , the main content so far is summarized by Google How to conduct a Code Review guide, based on the title of the original Github repository, we will add more Google engineering practices in the future.\n Github：https://github.com/rootsongjc/eng-practices  Browse online: https://jimmysong.io/eng-practices   The translation uses the same style and directory structure as the original document. In fact, if the original text has international requirements, it can be directly incorporated, but according to the translation suggestions submitted by several previous people, the author of this project does not recommend translation. The first is translation. The documents may be unmaintained, and the accuracy of the documents cannot be guaranteed.\nFor suggestions on Chinese integration, see:\n Add Chinese translation #12  Add the link of Chinese version of code reviewer’s guide #8   ","permalink":"https://jimmysong.io/en/notice/google-engineering-practices-zh/","summary":"Translated from Google's open source documentation on Github","title":"Chinese version of Google Engineering Practice Documents"},{"content":"The following paragraph is a release note from the Istio official blog https://istio.io/zh/blog/2019/announcing-1.1/ , which I translated.\nIstio was released at 4 a.m. Beijing time today and 1 p.m. Pacific time.\nSince the 1.0 release last July, we have done a lot to help people get Istio into production. We expected to release a lot of patches (six patches have been released so far!), But we are also working hard to add new features to the product.\nThe theme for version 1.1 is “Enterprise Ready”. We are happy to see more and more companies using Istio in production, but as some big companies join in, Istio also encounters some bottlenecks.\nThe main areas we focus on include performance and scalability. As people gradually put Istio into production and use larger clusters to run more services with higher capacity, there may be some scaling and performance issues. Sidecar takes up too much resources and adds too much latency. The control plane (especially Pilot) consumes excessive resources.\nWe put a lot of effort into making the data plane and control plane more efficient. In the 1.1 performance test, we observed that sidecars typically require 0.5 vCPU to process 1000 rps. A single Pilot instance can handle 1000 services (and 2000 pods) and consumes 1.5 vCPUs and 2GB of memory. Sidecar adds 5 milliseconds at the 50th percentile and 10 milliseconds at the 99th percentile (the execution strategy will increase latency).\nWe have also completed the work of namespace isolation. You can use the Kubernetes namespace to enforce control boundaries to ensure that teams do not interfere with each other.\nWe have also improved multi-cluster functionality and usability. We listened to the community and improved the default settings for flow control and policies. We introduced a new component called Galley. Galley validates YAML configuration, reducing the possibility of configuration errors. Galley is also used in multi-cluster setups-collecting service discovery information from each Kubernetes cluster. We also support other multi-cluster topologies, including single control planes and multiple synchronous control planes, without the need for flat network support.\nSee the release notes for more information and details .\nThere is more progress on this project. As we all know, Istio has many moving parts, and they take on too much work. To address this, we have recently established the Usability Working Group (available at any time). A lot happened in the community meeting (Thursday at 11 am) and in the working group. You can log in to discuss.istio.io with GitHub credentials to participate in the discussion!\nThanks to everyone who has contributed to Istio over the past few months-patching 1.0, adding features to 1.1, and extensive testing recently on 1.1. Special thanks to companies and users who work with us to install and upgrade to earlier versions to help us identify issues before they are released.\nFinally, go to the latest documentation and install version 1.1! Happy meshing!\nOfficial website The ServiceMesher community has been maintaining the Chinese page of the official Istio documentation since the 0.6 release of Istio . As of March 19, 2019, there have been 596 PR merges, and more than 310 documents have been maintained. Thank you for your efforts! Some documents may lag slightly behind the English version. The synchronization work is ongoing. For participation, please visit https://github.com/servicemesher/istio-official-translation. Istio official website has a language switch button on the right side of each page. You can always Switch between Chinese and English versions, you can also submit document modifications, report website bugs, etc.\nServiceMesher Community Website\nThe ServiceMesher community website http://www.servicemesher.com covers all technical articles in the Service Mesh field and releases the latest activities in a timely manner. It is your one-stop portal to learn about Service Mesh and participate in the community.\n","permalink":"https://jimmysong.io/en/notice/istio-11/","summary":"Istio 1.1 was released at 4 am on March 20th, Beijing time. This version took 8 months! The ServiceMesher community also launched the Istio Chinese documentation.","title":"Istio 1.1 released"},{"content":"Istio handbook was originally an open source e-book I created (see https://jimmysong.io/istio-handbook ). It has been written for 8 months before donating to the ServiceMesher community. In order to further popularize Istio and Service Mesh technology, this book Donate to the community for co-authoring. The content of the original book was migrated to https://github.com/servicemesher/istio-handbook on March 10, 2019. The original book will no longer be updated.\n GitHub address: https://github.com/servicemesher/istio-handbook  Reading address online: http://www.servicemesher.com/istio-handbook/   Conceptual picture of this book, cover photo of Shanghai Jing’an Temple at night , photo by Jimmy Song .\nThe publishing copyright of this book belongs to the blog post of Electronic Industry Press. Please do not print and distribute it without authorization.\nIstio is a service mesh framework jointly developed by Google, IBM, Lyft, etc., and began to enter the public vision in early 2017. As an important infrastructure layer that inherits Kubernetes and connects to the serverless architecture in the cloud-native era, Istio is of crucial importance important. The ServiceMesher community, as one of the earliest open source communities in China that is researching and promoting Service Mesh technology, decided to integrate community resources and co-author an open source e-book for readers.\nAbout this book This book originates from the rootsongjc / istio-handbook and the Istio knowledge map created by the ServiceMesher community .\nThis book is based on Istio 1.0+ and includes, but is not limited to , topics in the Istio Knowledge Graph .\nParticipate in the book Please refer to the writing guidelines of this book and join the Slack channel discussion after joining the ServiceMesher community .\n","permalink":"https://jimmysong.io/en/notice/istio-handbook-by-servicemesher/","summary":"To further popularize Istio and Service Mesh technology, donate this book to the community for co-authoring.","title":"Donate Istio Handbook to the ServiceMesher community"},{"content":"Github: https://github.com/rootsongjc/cloud-native-sandbox Cloud Native Sandbox can help you setup a standalone Kubernetes and istio environment with Docker on you own laptop.\nThe sandbox integrated with the following components:\n Kubernetes v1.10.3 Istio v1.0.4 Kubernetes dashboard v1.8.3  Differences with kubernetes-vagrant-centos-cluster As I have created the kubernetes-vagrant-centos-cluster to set up a Kubernetes cluster and istio service mesh with vagrantfile which consists of 1 master(also as node) and 3 nodes, but there is a big problem that it is so high weight and consume resources. So I made this light weight sandbox.\nFeatures\n No VirtualBox or Vagrantfile required Light weight High speed, low drag Easy to operate  Services\nAs the sandbox setup, you will get the following services.\nRecord with termtosvg .\nPrerequisite You only need a laptop with Docker Desktop installed and Kubernetes enabled .\nNote: Leave enough resources for Docker Desktop. At least 2 CPU, 4G memory.\nInstall To start the sandbox, you have to run the following steps.\nKubernetes dashboard(Optional) Install Kubernetes dashboard.\nkubectl apply -f install/dashbaord/ Get the dashboard token.\nkubectl -n kube-system describe secret default| awk '$1==\"token:\"{print $2}' Expose kubernetes-dashboard service.\nkubectl -n kube-system get pod -l k8s-app=kubernetes-dashboard -o jsonpath='{.items[0].metadata.name}' Login to Kubernetes dashboard on http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login with the above token.\nIstio(Required) Install istio service mesh with the default add-ons.\n# Install istio kubectl apply -f install/istio/ To expose service grafana on http://localhost:3000 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 3000:3000 \u0026 To expose service prometheus on http://localhost:9090 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=prometheus -o jsonpath='{.items[0].metadata.name}') 9090:9090 \u0026 To expose service jaeger on http://localhost:16686 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=jaeger -o jsonpath='{.items[0].metadata.name}') 16686:16686 \u0026 To expose service servicegraph on http://localhost:8088/dotviz , http://localhost:8088/force/forcegraph.html .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=servicegraph -o jsonpath='{.items[0].metadata.name}') 8088:8088 \u0026 Kiali Install kiali .\nkubectl -n istio-system apply -f install/kiali To expose service kiali on http://localhost:20001 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=kiali -o jsonpath='{.items[0].metadata.name}') 20001:20001 \u0026 Bookinfo sample Deploy bookinfo sample .\n# Enable sidecar auto injection kubectl label namespace default istio-injection=enabled # Deploy bookinfo sample kubectl -n default apply -f sample/bookinfo Visit productpage on http://localhost/productpage .\nLet’s generate some loads.\nfor ((i=0;i\u003c1000;i=i+1));do echo \"Step-\u003e$i\";curl http://localhost/productpage;done You can watch the service status through http://localhost:3000 .\nClient tools To operate the applications on Kubernetes, you should install the following tools.\nRequired\n kubectl - Deploy and manage applications on Kubernetes. istioctl - Istio configuration command line utility.  Optional\n kubectx - Switch faster between clusters and namespaces in kubectl kube-ps1 - Kubernetes prompt info for bash and zsh  ","permalink":"https://jimmysong.io/en/blog/cloud-native-sandbox/","summary":"A standalone Kubernetes and Istio environment with Docker on you own laptop","title":"Cloud Native Sandbox"},{"content":"This video was recorded on the last day of 2018 and demonstrates the use of rootsongjc/kubernetes-vagrant-centos-cluster to automatically deploy a Kubernetes cluster and Istio Service Mesh.\nA few days ago I mentioned that kubernetes-vagrant-centos-cluster released v1.2.0 version to deploy a cloud-native experimental environment with one click. Someone in the Kubernetes and Service Mesh community asked me a long time ago to make a video to explain and demonstrate how to install Kubernetes and Istio Service Mesh, because I’m always busy, I’ve always made some time. Today, I will give a demo video, just don’t watch the video for a few minutes. In order to make this video, it took me half an hour to record, two hours to edit, and many years of shooting. , Editing, containers, virtual machines, Kubernetes, service grid experience. This is not so much a farewell as a new beginning.\nBecause the video was first posted on YouTube , it was explained in English (just a few supplementary instructions, it does n’t matter if you do n’t understand, just read the Chinese documentation on GitHub ).\nSkip to bilibli to watch . If you are interested in drone aerial photography, you can also take a look at Jimmy Song’s aerial photography works . Please support the coin or like, thank you.\nIf you have any questions, you can send a barrage or comment below the video.\nPS. Some people will ask why you chose to use bilibli, because there are no ads for watching videos on this platform, and most of them are uploaded by the Up master. Although the two-dimensional elements are mostly, the community atmosphere is still good.\nFor more exciting videos, visit Jimmy Song’s bibli homepage .\n","permalink":"https://jimmysong.io/en/notice/cloud-native-kubernetes-service-mesh-local-demo-show/","summary":"This video was recorded by me on the last day of 2018 and demonstrates the use of rootsongjc/kubernetes-vagrant-centos-cluster to automatically deploy Kubernetes clusters and Istio Service Mesh.","title":"Kubernetes and Istio Service Mesh Cloud Native Local Video Demo Show"},{"content":"Updated at Mar 8, 2022\nThis article uses Istio’s official bookinfo sample to explain how Envoy performs routing forwarding after the traffic entering the Pod and forwarded to Envoy sidecar by iptables, detailing the inbound and outbound processing. For a detailed analysis of traffic interception, see Understanding Envoy Sidecar Proxy Injection and Traffic Interception in Istio Service Mesh .\nOverview of Sidecar Injection and Traffic Interception Steps Below is an overview of the steps from Sidecar injection, Pod startup to Sidecar proxy interception traffic and Envoy processing routing.\n Kubernetes automatically injected through Admission Controller, or the user run istioctl command to manually inject sidecar container. Apply the YAML configuration deployment application. At this time, the service creation configuration file received by the Kubernetes API server already includes the Init container and the sidecar proxy. Before the sidecar proxy container and application container are started, the Init container started firstly. The Init container is used to set iptables (the default traffic interception method in Istio, and can also use BPF, IPVS, etc.) to Intercept traffic entering the pod to Envoy sidecar Proxy. All TCP traffic (Envoy currently only supports TCP traffic) will be Intercepted by sidecar, and traffic from other protocols will be requested as originally. Launch the Envoy sidecar proxy and application container in the Pod.  Sidecar proxy and application container startup order issues\nStart the sidecar proxy and the application container. Which container is started first? Normally, Envoy Sidecar and the application container are all started up before receiving traffic requests. But we can’t predict which container will start first, so does the container startup order have an impact on Envoy hijacking traffic? The answer is yes, but it is divided into the following two situations.\nCase 1: The application container starts first, and the sidecar proxy is still not ready\nIn this case, the traffic is transferred to the 15001 port by iptables, and the port is not monitored in the Pod. The TCP link cannot be established and the request fails.\nCase 2: Sidecar starts first, the request arrives and the application is still not ready\nIn this case, the request will certainly fail. As for the step at which the failure begins, the reader is left to think.\nQuestion : If adding a readiness and living probe for the sidecar proxy and application container can solve the problem?\n  TCP requests that are sent or received from the Pod will be hijacked by iptables. After the inbound traffic is hijacked, it is processed by the Inbound Handler and then forwarded to the application container for processing. The outbound traffic is hijacked by iptables and then forwarded to the Outbound Handler for processing. Upstream and Endpoint. Sidecar proxy requests Pilot to use the xDS protocol to synchronize Envoy configurations, including LDS, EDS, CDS, etc., but to ensure the order of updates, Envoy will use ADS to request configuration updates from Pilot directly.  How Envoy handles route forwarding The following figure shows a productpageservice access request http://reviews.default.svc.cluster.local:9080/, when traffic enters reviews the internal services, reviews internal services Envoy Sidecar is how to do traffic blocked the route forward.\nIstio transparent traffic hijacking and traffic routing schematic  Before the first step, productpage Envoy Sidecar Pod has been selected by EDS of a request to reviews a Pod service of its IP address, it sends a TCP connection request.\nThe Envoy configuration in the official website of Istio is to describe the process of Envoy doing traffic forwarding. The party considering the traffic of the downstream is to receive the request sent by the downstream. You need to request additional services, such as reviews service requests need Pod ratings service.\nreviews, there are three versions of the service, there is one instance of each version, three versions sidecar similar working steps, only to later reviews-v1-cb8655c75-b97zc Sidecar flow Pod forwarding this step will be described.\nUnderstanding the Inbound Handler The role of the inbound handler is to transfer the traffic from the downstream intercepted by iptables to localhost to establish a connection with the application container inside the Pod.\nLook reviews-v1-cb8655c75-b97zc at the Listener in the pod.\nRun istioctl pc listener reviews-v1-cb8655c75-b97zc to see what the Pod has a Listener.\nADDRESS PORT TYPE  172.33.3.3 9080 HTTP \u003c--- Receives all inbound traffic on 9080 from listener 0.0.0.0_15006 10.254.0.1 443 TCP \u003c--+ 10.254.4.253 80 TCP | 10.254.4.253 8080 TCP | 10.254.109.182 443 TCP | 10.254.22.50 15011 TCP | 10.254.22.50 853 TCP | 10.254.79.114 443 TCP |  10.254.143.179 15011 TCP | 10.254.0.2 53 TCP | Receives outbound non-HTTP traffic for relevant IP:PORT pair from listener 0.0.0.0_15001 10.254.22.50 443 TCP | 10.254.16.64 42422 TCP | 10.254.127.202 16686 TCP | 10.254.22.50 31400 TCP | 10.254.22.50 8060 TCP | 10.254.169.13 14267 TCP | 10.254.169.13 14268 TCP | 10.254.32.134 8443 TCP | 10.254.118.196 443 TCP \u003c--+ 0.0.0.0 15004 HTTP \u003c--+ 0.0.0.0 8080 HTTP | 0.0.0.0 15010 HTTP |  0.0.0.0 8088 HTTP | 0.0.0.0 15031 HTTP | 0.0.0.0 9090 HTTP |  0.0.0.0 9411 HTTP | Receives outbound HTTP traffic for relevant port from listener 0.0.0.0_15001 0.0.0.0 80 HTTP | 0.0.0.0 15030 HTTP | 0.0.0.0 9080 HTTP | 0.0.0.0 9093 HTTP | 0.0.0.0 3000 HTTP | 0.0.0.0 8060 HTTP | 0.0.0.0 9091 HTTP \u003c--+  0.0.0.0 15006 TCP \u003c--- Receives all inbound and outbound traffic to the pod from IP tables and hands over to virtual listener As from productpage traffic arriving reviews Pods, downstream must clearly know the IP address of the Pod which is 172.33.3.3, so the request is 172.33.3.3:9080.\nVirtual Listener\nAs you can see from the Pod’s Listener list, the 0.0.0.0:15001/TCP Listener (the actual name is virtual) listens for all inbound traffic, and the following is the detailed configuration of the Listener.\n{ \"name\": \"virtual\", \"address\": { \"socketAddress\": { \"address\": \"0.0.0.0\", \"portValue\": 15006 } }, \"filterChains\": [ { \"filters\": [ { \"name\": \"envoy.tcp_proxy\", \"config\": { \"cluster\": \"BlackHoleCluster\", \"stat_prefix\": \"BlackHoleCluster\" } } ] } ], \"useOriginalDst\": true } UseOriginalDst : As can be seen from the configuration in useOriginalDstthe configuration as specified true, which is a Boolean value, the default is false, using iptables redirect connections, the proxy may receive port original destination address is not the same port, thus received at the proxy port It is 15001 and the original destination port is 9080. When this flag is set to true, the Listener redirects the connection to the Listener associated with the original destination address, here 172.33.3.3:9080. Listener If no relationship to the original destination address, the connection processing by the Listener to receive it, i.e. the virtualListener, after envoy.tcp_proxyforwarded to a filter process BlackHoleCluster, as the name implies, when no matching Envoy virtual listener when the effect of Cluster , will send the request to it and return 404. This will be referred to below Listener provided bindToPort echoes.\nNote : This parameter will be discarded, please use the Listener filter of the original destination address instead. The main purpose of this parameter is: Envoy listens to the 15201 port to intercept the traffic intercepted by iptables via other Listeners instead of directly forwarding it. See the Virtual Listener for details .\nListener 172.33.3.3_9080\nAs mentioned above, the traffic entering the inbound handler is virtual transferred to the 172.33.3.3_9080 Listener by the Listener. We are looking at the Listener configuration.\nRun istioctl pc listener reviews-v1-cb8655c75-b97zc --address 172.33.3.3 --port 9080 -o json view.\n[{ \"name\": \"172.33.3.3_9080\", \"address\": { \"socketAddress\": { \"address\": \"172.33.3.3\", \"portValue\": 9080 } }, \"filterChains\": [ { \"filterChainMatch\": { \"transportProtocol\": \"raw_buffer\" }, \"filters\": [ { \"name\": \"envoy.http_connection_manager\", \"config\": { ... \"route_config\": { \"name\": \"inbound|9080||reviews.default.svc.cluster.local\", \"validate_clusters\": false, \"virtual_hosts\": [ { \"domains\": [ \"*\" ], \"name\": \"inbound|http|9080\", \"routes\": [ { ... \"route\": { \"cluster\": \"inbound|9080||reviews.default.svc.cluster.local\", \"max_grpc_timeout\": \"0.000s\", \"timeout\": \"0.000s\" } } ] } ] }, \"use_remote_address\": false, ... } } ]， \"deprecatedV1\": { \"bindToPort\": false } ... }, { \"filterChainMatch\": { \"transportProtocol\": \"tls\" }, \"tlsContext\": {... }, \"filters\": [... ] } ], ... }] bindToPort : Note that there are a bindToPort configuration that is false, the default value of the configuration true, showing Listener bind to the port, set here to false the process flow can Listener Listener transferred from the other, i.e., above said virtual Listener, where we see filterChains.filters in the envoy.http_connection_manager configuration section:\n\"route_config\": { \"name\": \"inbound|9080||reviews.default.svc.cluster.local\", \"validate_clusters\": false, \"virtual_hosts\": [ { \"domains\": [ \"*\" ], \"name\": \"inbound|http|9080\", \"routes\": [ { ... \"route\": { \"cluster\": \"inbound|9080||reviews.default.svc.cluster.local\", \"max_grpc_timeout\": \"0.000s\", \"timeout\": \"0.000s\" } } ] } ] } This configuration indicates that traffic will be handed off to the Cluster for inbound|9080||reviews.default.svc.cluster.local processing.\nCluster inbound|9080||reviews.default.svc.cluster.local\nRun istioctl pc cluster reviews-v1-cb8655c75-b97zc --fqdn reviews.default.svc.cluster.local --direction inbound -o json to see the Cluster configuration is as follows.\n[ { \"name\": \"inbound|9080||reviews.default.svc.cluster.local\", \"connectTimeout\": \"1.000s\", \"hosts\": [ { \"socketAddress\": { \"address\": \"127.0.0.1\", \"portValue\": 9080 } } ], \"circuitBreakers\": { \"thresholds\": [ {} ] } } ] You can see that the Endpoint of the Cluster directly corresponds to localhost, and then the traffic is forwarded by the application container after iptables.\nUnderstanding the Outbound Handler Because the reviews will to ratings send an HTTP request service, request address are: http://ratings.default.svc.cluster.local:9080/ the role of Outbound handler is to intercept traffic to iptables to native applications sent via Envoy to determine how to route to the upstream.\nThe request sent by the application container is outbound traffic. After being hijacked by iptables, it is transferred to the Envoy Outbound handler for processing, then passed through virtual Listener and 0.0.0.0_9080 Listener, and then finds the cluster of upstream through Route 9080, and then finds Endpoint through EDS to perform routing action.\nRoute 9080\nreviews requests ratings service, run istioctl proxy-config routes reviews-v1-cb8655c75-b97zc --name 9080 -o json view route configuration because Envoy VirtualHost will be matched according to HTTP header of domains, so the following list only ratings.default.svc.cluster.local:9080 this one VirtualHost.\n[{ \"name\": \"ratings.default.svc.cluster.local:9080\", \"domains\": [ \"ratings.default.svc.cluster.local\", \"ratings.default.svc.cluster.local:9080\", \"ratings\", \"ratings:9080\", \"ratings.default.svc.cluster\", \"ratings.default.svc.cluster:9080\", \"ratings.default.svc\", \"ratings.default.svc:9080\", \"ratings.default\", \"ratings.default:9080\", \"10.254.234.130\", \"10.254.234.130:9080\" ], \"routes\": [ { \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"outbound|9080||ratings.default.svc.cluster.local\", \"timeout\": \"0.000s\", \"maxGrpcTimeout\": \"0.000s\" }, \"decorator\": { \"operation\": \"ratings.default.svc.cluster.local:9080/*\" }, \"perFilterConfig\": {... } } ] }, ..] You can see the routing of traffic to the Cluster from this Virtual Host configuration outbound|9080||ratings.default.svc.cluster.local.\nEndpoint outbound|9080||ratings.default.svc.cluster.local\nIstio 1.1 previous versions do not support the use of istioctl commands to directly query Endpoint Cluster, you can use the debug queries Pilot endpoint way compromise.\nkubectl exec reviews-v1-cb8655c75-b97zc -c istio-proxy curl http://istio-pilot.istio-system.svc.cluster.local:9093/debug/edsz \u003e endpoints.json endpoints.json file contains all the Endpoint information of the Cluster, and we only select outbound|9080||ratings.default.svc.cluster.local the results of the Cluster as follows.\n{ \"clusterName\": \"outbound|9080||ratings.default.svc.cluster.local\", \"endpoints\": [ { \"locality\": { }, \"lbEndpoints\": [ { \"endpoint\": { \"address\": { \"socketAddress\": { \"address\": \"172.33.100.2\", \"portValue\": 9080 } } }, \"metadata\": { \"filterMetadata\": { \"istio\": { \"uid\": \"kubernetes://ratings-v1-8558d4458d-ns6lk.default\" } } } } ] } ] } The Endpoint can be one or more, and Envoy will route it according to certain rules by selecting the appropriate Endpoint.\nNote : Istio 1.1 will support the istioctl pc endpoint command to query Endpoint.\nReference  Debugging Envoy and Pilot - istio.io  Understanding Envoy Agent Sidecar Injection and Traffic Interception in Istio Service Mesh - jimmysong.io  Istio traffic management implementation mechanism deep analysis - zhaohuabing.com   ","permalink":"https://jimmysong.io/en/blog/understanding-how-envoy-sidecar-intercept-and-route-traffic-in-istio-service-mesh/","summary":"Details about Envoy sidecar with iptables rules.","title":"Understanding How Envoy Sidecar Intercept and Route Traffic in Istio Service Mesh"},{"content":"KubeCon \u0026 CloudNative in North America is the most worthy cloud-native event every year. This time it will be held in Seattle for four days, from December 10th to 13th, refer to the official website of the conference . This year, 8,000 people participated. You should notice that Kubernetes has become more and more low-level. Cloud-native application developers do not need to pay much attention to it. Major companies are publishing their own cloud-native technology stack layouts, including IBM, VMware, SAP, Startups around this ecosystem are still emerging. The PPT sharing address of the local conference: https://github.com/warmchang/KubeCon-North-America-2018 , thank you William Zhang for finishing and sharing the slides of this conference .\nSeattle scene Janet Kuo from Google describes the path to cloud-native technology adoption.\nThe same event of KubeCon \u0026 CloudNativeCon, the scene of the first EnvoyCon.\nAt KubeCon \u0026 CloudNativeCon Seattle, various directors, directors, directors, VPs, and Gartner analysts from IBM, Google, Mastercard, VMware, and Gartner are conducting live discussions on the topic of Scaling with Service Mesh and Istio. Why do we talk about Istio when we talk about Service Mesh? What is not suitable for Istio use case. . .\nThe PPT contains basic introduction, getting started, a total of more than 200 Deep Dive as well as practical application, we recommend you according to the General Assembly’s official website to choose topics of interest to look at the schedule, otherwise I might see, however.\nA little impression KubeCon \u0026 CloudNativeCon is held three times a year, Europe, China and North America. China is the first time this year, and it will be held in Shanghai in November. It is said that it will be held in June next year. Although everyone said that Kubernetes has become boring, the conference about Kubernetes There is still a lot of content, and the use of CRD to extend Kubernetes usage is increasing. Service Mesh has begun to become hot. As can be seen from the live pictures above, there are a large number of participants on the site and related topics are also increasing. It is known as a microservice in the post-Kubernetes era . This must be It will be an important development direction of cloud native after Kubernetes, and the ServiceMesher community pays close attention to it.\n","permalink":"https://jimmysong.io/en/notice/kubecon-cloudnativecon-seattle-2018/","summary":"KubeCon \u0026 CloudNativeCon Seattle 2018 Data Sharing.","title":"Kubecon\u0026CloudNativeCon Seattle 2018"},{"content":"This is a postscript from the post- Kubernetes era. Just this evening I saw a post by Bilgin Ibryam Microservices in a Post-Kuberentes Era  .\nOn April 9, 2017, the Kubernetes Handbook-Kubernetes Chinese Guide / Cloud Native Application Architecture Practice Manual was first submitted. In the past 16 months, 53 contributors participated, 1,088 commits, and a total of 23,9014 Chinese characters were written. At the same time , thousands of enthusiasts have gathered in the Kubernetes \u0026 Cloud Native combat group .\nIt has been more than 4 months since the previous version was released. During this period, Kubernetes and Prometheus graduated from CNCF respectively and have matured commercially. These two projects have basically taken shape and will not change much in the future. Kubernetes was originally developed for container orchestration. In order to solve the problem of microservice deployment, Kubernetes has gained popularity. The current microservices have gradually entered the post-Kubernetes era . Service Mesh and cloud native redefine microservices and distributed applications.\nWhen this version was released, the PDF size was 108M with a total of 239,014 Chinese characters. It is recommended to browse online , or clone the project and install the Gitbook command to compile it yourself.\nThis version has the following improvements:\n Added Istio Service Mesh tutorial  Increased use VirtualBox and Vagrant set up in a local cluster and distributed Kubernetes Istio Service Mesh  Added cloud native programming language Ballerina and Pulumi introduced Added Quick Start Guide  Added support for Kubernetes 1.11 Added enterprise-level service mesh adoption path guide  Added SOFAMesh chapter  Added vision for the cloud-native future  Added CNCF charter and participation Added notes for Docker image repositories Added Envoy chapter  Increased KCSP (Kubernetes certification service providers) and CKA (Certified Kubernetes administrator) instructions Updated some configuration files, YAML and reference links Updated CRI chapter  Removed obsolete description Improved etcdctl command usage tutorial Fixed some typos  Browse and download  Browse online https://jimmysong.io/kubernetes-handbook  To make it easy for everyone to download, I put a copy on Weiyun , which is available in PDF (108MB), MOBI (42MB), and EPUB (53MB).  In this book, there are more practical tutorials. In order to better understand the principles of Kubernetes, I recommend studying ** In- depth analysis of Kubernetes by Zhang Lei, produced by Geek Time **.\nThank you Kubernetes for your support of this book. Thank you Contributors . In the months before this version was released, the ServiceMesher community was co-founded . As a force in the post-Kubernetes era , welcome to contact me to join the community and create cloud native New era .\nAt present , the WeChat group of the ServiceMesher community also has thousands of members. The Kubernete Handbook will continue, but Service Mesh is already a rising star. With Kubernetes in mind, welcome to join the ServiceMesher community and follow us. The public account of the community (also the one I manage).\n","permalink":"https://jimmysong.io/en/notice/new-kubernetes-handbook-released-and-say-hello-to-post-kubernetes-era/","summary":"This is an obituary post-Kubernetes era. Kubernetes handbook by Jimmy Song v1.4 is released. The next focus of cloud native is Service Mesh!","title":"Kubernetes Handbook v1.4 is released"},{"content":"Today I am honored to announce that I have become a CNCF Ambassador . Here is my story with Cloud Native.\nOrigin The first time to attend the Cloud Native Computing Foundation is at the LC3 in Beijing 2017. I attended the meeting again this year, and in November of this year, CNCF will hold the KubeCon \u0026 CloudNativeCon for the first time in Shanghai, China. I’ll be there too.\nCloud Native Books My origins with the Cloud Native is originated from Kevin Hoffman’s book Cloud Native Go . I translated this book at the end of 2016. Since then, in China, the translation of the word Cloud Native has not been determined, we introduced it with 云原生 to China.\nAnd then I begin to write the kubernetes-handbook on GitHub. So far, it has more than 2000 stars. This book has written more than 200,000 Chinese characters, the first commit happened on April 14, 2017.\nSince the the book Cloud Native Go completed, the publisher recommended another Cloud Native book to me - Cloud Native Python by Manish Sethi.\nAnd the book Cloud Native Java by Josh Long and Kenny Bastani.\nIn March 2018, with the hope that Bring the world equal opportunities and Building a Financial Cloud Native Infrastructure, I joined the Ant Group .\nServiceMesher Community By the time of May 2018, I start to organize the ServiceMesher community.\nIn the last few months, we work with other open source communities in China, such as k8smeetup , Sharding-Sphere , Apache SkyWalking . Our community has grown to have 1,700 members and two round meetups in Hangzhou and Beijing till now.\nMore than 300 people participated in the scene and more than 20,000 people watched it live by IT大咖说 。\nFuture Here are some hopes of mine:\n Open source culture become popular in China More and more people would like to be involved in open source projects Host one open source project into the CNCF A book related to Cloud Native or Service Mesh Strengthen cultural exchanges between China and the global  Finally, welcome to China for traveling or share your topic with us on Cloud Native, and in the mean while we will share our experience on large scale web apps to the world. Hope to hear your voice!\n","permalink":"https://jimmysong.io/en/blog/cloud-native-and-me-the-past-current-and-future/","summary":"Today I am honored to announce that I have become a CNCF Ambassador.","title":"Cloud Native and me - the past, current and future"},{"content":"Today, we are pleased to announce Istio 1.0 . It’s been over a year since the original 0.1 release. Since 0.1, Istio has grown rapidly with the help of a thriving community, contributors, and users. Many companies have successfully applied Istio to production today and have gained real value through the insight and control provided by Istio. We help large businesses and fast-growing startups such as eBay , Auto Trader UK , Descartes Labs , HP FitStation , Namely , PubNub and Trulia to connect, manage and protect their services from scratch with Istio. The release of this version as 1.0 recognizes that we have built a core set of features that users can rely on for their production.\nEcosystem Last year we saw a significant increase in the Istio ecosystem. Envoy continues its impressive growth and adds many features that are critical to a production-level service grid. Observability providers like Datadog , SolarWinds , Sysdig , Google Stackdriver, and Amazon CloudWatch have also written plugins to integrate Istio with their products. Tigera , Aporeto , Cilium and Styra have built extensions for our strategy implementation and network capabilities. Kiali built by Red Hat provides a good user experience for grid management and observability. Cloud Foundry is building the next-generation traffic routing stack for Istio, the recently announced Knative serverless project is doing the same, and Apigee has announced plans to use it in their API management solution. These are just a few of the projects that the community added last year.\nFeatures Since the 0.8 release, we have added some important new features, and more importantly, marked many existing features as Beta to indicate that they can be used in production. This is covered in more detail in the release notes , but it is worth mentioning:\n Multiple Kubernetes clusters can now be added to a single grid , enabling cross-cluster communication and consistent policy enforcement. Multi-cluster support is now Beta. The network API for fine-grained control of traffic through the grid is now Beta. Explicitly modeling ingress and egress issues with gateways allows operations personnel to control the network topology and meet access security requirements at the edge. Two-way TLS can now be launched incrementally without updating all clients of the service. This is a key feature that removes the barriers to deploying Istio on existing production. Mixer now supports developing out-of-process adapters . This will be the default way to extend Mixer in an upcoming release, which will make it easier to build adapters. Envoy now fully evaluates the authorization policies that control service access locally , improving their performance and reliability. Helm chart installation is now the recommended installation method, with a wealth of customization options to configure Istio to your needs. We put a lot of effort into performance, including continuous regression testing, large-scale environmental simulation, and target repair. We are very happy with the results and will share details in the coming weeks.  Next step Although this is an important milestone for the project, much work remains to be done. When working with adopters, we’ve received a lot of important feedback about what to focus next. We’ve heard consistent topics about supporting hybrid clouds, installing modularity, richer network capabilities, and scalability for large-scale deployments. We have considered some feedback in the 1.0 release and we will continue to actively work on it in the coming months.\nQuick start If you are new to Istio and want to use it for deployment, we would love to hear from you. Check out our documentation , visit our chat forum or visit the mailing list . If you want to contribute more to the project, please join our community meeting and say hello.\nAt last The Istio team is grateful to everyone who contributed to the project. Without your help, it won’t have what it is today. Last year’s achievements were amazing, and we look forward to achieving even greater achievements with our community members in the future.\n The ServiceMesher community is responsible for the translation and maintenance of Chinese content on Istio’s official website. At present, the Chinese content is not yet synchronized with the English content. You need to manually enter the URL to switch to Chinese ( https://istio.io/zh ). There is still a lot of work to do , Welcome everyone to join and participate.\n","permalink":"https://jimmysong.io/en/notice/istio-v1-released/","summary":"Chinese documentation is released at the same time!","title":"Istio 1.0 is released"},{"content":" If there is a visual learning model and platform that provides infrastructure clusters for your operation, would you pay for it?\n Two months ago, I met Jord in Kubernetes’s Slack channel, and later saw the link of MagicSandbox.io he (and possibly others) sent in the Facebook group of Taiwan Kubernetes User Group, and I clicked to apply for a trial Then, I received an email from Jord later, and he told me that he wanted to build a Kubernetes learning platform. That’s where the whole thing started, and then we had a couple of Zoom video chats for a long time.\nAbout MagicSandbox MagicSandbox is a startup company. Jord (Dutch) is also a serial entrepreneur. He has studied at Sichuan University in China for 4 years since he was 19, and then returned to Germany. He worked as a PM at Boston Consulting Group and now works at Entrepreneur. First (Europe’s top venture capital / enterprise incubator) is also located in Berlin, Germany. He met Mislav (Croatian). Mislav is a full-stack engineer and has several entrepreneurial experiences. They have similar odors, and they hit it off. Decided to be committed to the Internet education industry and create a world-class software engineer education platform. They want to start with Kubernetes, first provide Internet-based Kubernetes theory and practice teaching, and then expand the topic to ElasticSearch, GraphQL, and so on. topic.\nJord founded MagicSandbox in his home, and I became the face of MagicSandbox in China.\nNow we are going to release the MagicSandbox Alpha version. This version is an immature version and is provided for everyone to try for free. Positive feedback is also welcome.\n Official homepage: https://magicsandbox.com/  Chinese page: https://cn.magicsandbox.com/ (The content has not been finished yet, only the Chinese version homepage is currently provided) Follow us on Twitter: https://twitter.com/magicsandbox   ","permalink":"https://jimmysong.io/en/notice/magicsandbox-alpha-version-annoucement/","summary":"Online practical software engineering education platform.","title":"MagicSandbox Alpha released"},{"content":"Remember the cloud-native programming language I shared before finally appeared!  Learn about Ballerina in one article!  ? They are ready to attend the KubeCon \u0026 CloudNativeCon China Conference!\nKubeCon \u0026 CloudNativeCon China Conference will be held on November 14-15, 2018 (Wednesday, Thursday) in Shanghai . See: https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2018/ With Ballerina’s official authorization, I now need to help them find an “ambassador” in China, responsible for team guidance, Chinese and English translation, and familiarity with Cloud Native and microservices. It has influence in the industry and has no barriers to English communication.\nAmbassador duties\n Team Leadership Responsible for Chinese and English translation of product declaration, PPT materials, etc. Help to arrange the booth  The other party can provide\n Conference tickets Travel expenses Accommodation during the conference Other compensation  This is a photo of their team in front of their booth during the KubeCon \u0026 CloudNativeCon in Hagen in May this year.\nPS This is the most complete and best picture I have ever found of their team. (Photography needs to be strengthened)\nLet’s briefly introduce this startup called Ballerina. Their team is mainly from Sri Lanka. This is an island country next to India in the South Asian subcontinent. In ancient China, it was called “Lion Country” and rich in gemstones.\nThe capital of their country is Sri Lanka, which is pronounced in their own language: Si li jia ya wa de na pu la ke te\nIf you are interested, please contact me directly.\n","permalink":"https://jimmysong.io/en/notice/a-ballerina-china-ambassador-required/","summary":"With official authorization from Ballerina, I now need to help them find an ambassador in China.","title":"Ballerina seeks Chinese ambassador"},{"content":"Envoy-Designed for cloud-native applications, open source edge and service proxy, Istio Service Mesh default data plane, Chinese version of the latest official document, dedicated by the ServiceMesher community, welcome everyone to learn and share together.\n TL;DR: http://www.servicemesher.com/envoy/  PDF download address: servicemesher/envoy   This is the first time Community Service Mesh enthusiasts group activities, the document is based on Envoy latest (version 1.7) Official documents https://www.envoyproxy.io/docs/envoy/latest/ . A total of 120 articles with 26 participants took 13 days and 65,148 Chinese characters.\nVisit online address: http://www.servicemesher.com/envoy/ Note : This book does not include the v1 API reference and v2 API reference sections in the official documentation. Any links to API references in the book will jump directly to the official page.\nContributor See the contributor page: https://github.com/servicemesher/envoy/graphs/contributors Thanks to the above contributors for their efforts! Because the level of translators is limited, there are inevitably inadequacies in the text. I also ask readers to correct them. Also welcome more friends to join our GitHub organization: https://github.com/servicemesher ","permalink":"https://jimmysong.io/en/notice/envoyproxy-docs-cn-17-release/","summary":"Translated by ServiceMesher community.","title":"Chinese version of the latest official document of Envoy released"},{"content":"Envoy is an open source L7 proxy and communication bus written in C ++ by Lyft. It is currently an open source project under CNCF . The code is hosted on GitHub. It is also the default data plane in the Istio service mesh. We found that it has very good performance, and there are also continuous open source projects based on Envoy, such as Ambassador , Gloo, etc. At present, the official documentation of Envoy has not been well finished, so we Service Service enthusiasts feel that they are launching the community The power of the co-translation of the latest (version 1.7) official documentation of Enovy and organization through GitHub.\nService Mesh enthusiasts have jointly translated the latest version of the official document of Envoy . The translated code is hosted at https://github.com/servicemesher/envoy . If you are also a Service Mesh enthusiast, you can join the SerivceMesher GitHub organization and participate together.\nThe official Envoy document excludes all articles in the two directories of the v1 API reference and the v2 API reference. There are more than 120 documents. The length of the documents varies. The original English official documents use the RST format. I manually converted them into Markdown format and compiled using Gitbook. A GitHub Issue was generated according to the path of the document relative to the home directory. Friends who want to participate in translation can contact me to join the ServiceMesher organization, and then select the article you want to translate in Issue , and then reply “Claim”.\nHere you can see all the contributors. In the future, we will also create a Service Mesh enthusiast website. The website uses static pages. All code will be hosted on Github. Welcome everyone to participate.\n","permalink":"https://jimmysong.io/en/notice/enovy-doc-translation-start/","summary":"The SerivceMesher community is involved in translating the official documentation for the latest version of Envoy.","title":"Envoy's latest official document translation work started"},{"content":"TL; DR Click here to download the PDF of this book .\nRecently, Michael Hausenblas of Nginx released a booklet on container networks in docker and kubernetes. This 72-page material is a good introduction for everyone to understand the network in Docker and Kubernetes from shallow to deep.\nTarget audience  Container Software Developer SRE Network Operation and Maintenance Engineer Architects who want to containerize traditional software  ","permalink":"https://jimmysong.io/en/notice/container-networking-from-docker-to-kubernetes-nginx/","summary":"Source from Nginx, published by O’Reilly.","title":"Docker container network book sharing"},{"content":"In 2017, we are facing a big era of architectural changes, such as Kubernetes ending the battle for container orchestration, Kafka release 1.0, serverless gradually gaining momentum, edge computing to replace cloud computing, Service Mesh ready to go, and artificial intelligence to empower business , Also brings new challenges to the architecture.\nI am about to participate in InfoQ’s ArchSummit Global Architects Summit on December 8-11 in Beijing . This conference also invited 100+ top technologists such as Dr. Ali Wangjian to share and summarize the architectural changes and reflections this year. I hope that you can build on this conference, summarize past practices, and look forward to a future-oriented architecture to transform this era of change into the common fortune of each of us.\nMy speech The content of my speech is from Kubernetes to Cloud Native-the road to cloud native applications . Link: From Kubernetes to Cloud Native-the road to cloud native applications . The time is Saturday, December 9, 9:30 am, in the fifth meeting room.\nAfter more than ten years of development of cloud computing, the new phase of cloud native has entered. Enterprise applications are preferentially deployed in cloud environments. How to adapt to the cloud native tide, use containers and Kubernetes to build cloud native platforms, and practice DevOps concepts and agility How IT, open source software, and the community can help IT transform, the solution to all these problems is the PaaS platform, which is self-evident to the enterprise.\nWe also prepared gifts for everyone: “Cloud Native Go-Building Cloud Native Web Applications Based on Go and React” and “Intelligent Data Era-Enterprise Big Data Strategy and Practice” There is also a book stand for the blog post of the Electronic Industry Press. Welcome to visit.\nArchSummit conference official website link: http://bj2017.archsummit.com/ For more details, please refer to the official website of the conference: http://bj2017.archsummit.com/ ","permalink":"https://jimmysong.io/en/notice/archsummit-beijing-2017-from-kubernetes-to-cloud-native/","summary":"I will give a lecture at ArchSummit Beijing. From Kubernetes to Cloud Native, my path to cloud native applications.","title":"ArchSummit Beijing 2017 speech preview"},{"content":"Cloudinary-go is a Go client library and CLI tool to upload static assets to the Cloudinary service.\nInstallation Install the CLI tool and the library with:\ngo get github.com/rootsongjc/cloudinary-go/cloudinary Or download the release binary from release .\n","permalink":"https://jimmysong.io/en/notice/cloudinary-go/","summary":"Cloudinary-go is a Go client library and CLI tool to upload static assets to Cloudinary service","title":"Cloudinary file upload tool written in Go released"},{"content":"Many people asked me how jimmysong.io made this website. I think it is necessary to write a book to popularize the knowledge of static website construction and Hugo as a tool.\nThis manual will guide you how to use Hugo to build a static website for personal blog or project display.\nTeach you how to build a static website from scratch. This does not require much programming and development experience and time investment, and basically does not require much cost (except for personalized domain names). You can quickly build and Launch a website.\nGithub address: https://github.com/rootsongjc/hugo-handbook Gitbook access address: https://jimmysong.io/hugo-handbook The content of this book will continue to improve over time and as my site improves, so stay tuned.\n","permalink":"https://jimmysong.io/en/notice/building-static-website-with-hugo/","summary":"A manual for static website building, and a personal blog Gitbook using Hugo.","title":"Hugo Handbook is released"},{"content":"Kevin Hoffman(From Capital One, twitter @KevinHoffman ) was making a speech on TalkingData T11 Smart Data Summit.\nHe addressed that 15 Factors of Cloud Native which based on Heroku’s original Twelve-Factor App , but he add more 3 another factors on it.\nLet’s have a look at the 15 factors of Cloud Native.\n1. One codebase, one App  Single version-controlled codebase, many deploys Multiple apps should not share code  Microservices need separate release schedules Upgrade, deploy one without impacting others   Tie build and deploy pipelines to single codebase  2. API first  Service ecosystem requires a contract  Public API   Multiple teams on different schedulers  Code to contract/API, not code dependencies   Use well-documented contract standards  Protobuf IDL, Swagger, Apiary, etc   API First != REST first  RPC can be more appropriate in some situations    3. Dependency Management  Explicitly declare dependencies Include all dependencies with app release Create immutable build artifact (e.g. docker image) Rely on smallest docker image  Base on scratch if possible   App cannot rely on host for system tools or libraries  4. Design, Build, Release, Run  Design part of iterative cycle  Agile doesn’t mean random or undesigned   Mature CI/CD pipeline and teams  Design to production in days not months   Build immutable artifacts Release automatically deploys to environment  Environments contains config, not release artifact    5. Configuration, Credentials, Code  “3 Cs” volatile substances that explode when combinded Password in a config file is as bad as password in code App must accept “3 Cs” from environment and only use harmless defaults Test - Could you expose code on Github and not reveal passwords, URLs, credentials?  6. Logs  Emit formatted logs to stdout Code should not know about destination or purpose of log emissions Use downstream log aggregator  collect, store, process, expose logs ELK, Splunk, Sumo, etc   Use structured logs to allow query and analysis  JSON, csv, KV, etc   Logs are not metrics  7. Disposability  App must start as quickly as possible App must stop quickly and gracefully Processes start and stop all the time in the cloud Every scale up/down disposes of processes Slow dispose == slow scale Slow dispose or startup can cause availability gaps  8. Backing Services  Assume all resources supplied by backingservices Cannotassume mutable file system  “Disk as a Service” (e.g. S3, virtual mounts, etc)   Every backing service is bound resource  URL, credentials, etc-\u003e environment config   Host does not satisfy NFRs  Backing services and cloud infrastructure    9. Environment Parity  “Works on my machine”  Cloud-native anti-pattern. Must work everywhere   Every commit is candidate for deployment Automated acceptance tests  Provide no confidence if environments don’t match    10. Administrative Processes  Database migrations Run-once scripts or jobs Avoid using for batch operations, consider instead:  Event sourcing Schedulers Triggers from queues, etc Lambdas/functions    11. Port Binding  In cloud, infrastructure determines port App must accept port assigned by platform Containers have internal/external ports  App design must embrace this   Never use reserved ports Beware of container “host mode” networking  12. Stateless Processes  What is stateless? Long-term state handled by a backing service In-memory state lives onlyas long as request Requests from same client routed to different instances  “Sticky sessions” cloud native anti-pattern    13. Concurency  Scale horizontally using the process model Build disposable, stateless, share-nothing processes Avoid adding CPU/RAM to increase scale/throughput Where possible, let platform/libraries do threading  Many single-threaded services \u003e 1 multi-threaded monolith    14. Telemetry  Monitor apps in the cloud like satellite in orbit No tether, no live debugger Application Perf Monitoring (APM) Domain Telemetry Health and system logs  15. Authentication \u0026 Authorization  Security should never be an afterthought Auth should be explicit, documented decision  Even if anonymous access is allowed Don’t allow anonymous access   Bearer tokens/OAuth/OIDC best practices Audit all attempts to access  Migrating Monoliths to the Cloud After this 15 factors, he also gave us some tips about how to migrate monoliths to the Cloud:\n Make a rule - stop adding to the monolith  All new code must be cloud native   Prioritize features  Where will you get most benefit from cloud native?   Come up with a plan  Decompose monolith over time Fast, agile iterations toward ultimate goal   Use multiple strategies and patterns  Go - the Best Language for Building Cloud Native App At last, he advise us the programming language Go is the best language to build Cloud Native applications for these reasons below:\n Lightweight Easily learning curve Compiles to native binaries Very fast Large, thriving, engaged community  http://gopherize.me     Kevin also wrote a book Cloud Native Go to show how to Building Web Applications and Microservices for the Cloud with Go and React. This book has been translated to Chinese by four guys from TalkingData with ❤️. 《Cloud Native Go 构建基于Go和React的云原生Web应用与微服务》published by PHEI publisher house.\nKevin was signing his name on the book\nkevin siging on the book  This is his first visit to China, as a main translator of this book I an honored to be with him to take this photo.\nkevin hoffman with me  ","permalink":"https://jimmysong.io/en/blog/high-level-cloud-native-from-kevin-hoffman/","summary":"Kevin Hoffman address that 15 Factors of Cloud Native.","title":"High Level Cloud Native From Kevin Hoffman"},{"content":"From now on I have my own independent domain name jimmysong.io , the website is still hosted on GitHub, the original URL https://jimmysong.io is still accessible.\nWhy use .io as the suffix? Because this is The First Step to Cloud Native!\nWhy choose today? Because today is August 18, the days are easy to remember.\nPS domain names are registered in namecheap and cost tens of dollars / year.\nProudly powered by hugo 🎉🎊🎉\n","permalink":"https://jimmysong.io/en/notice/domain-name-jimmysong-io/","summary":"From now on I have my own independent domain name jimmysong.io.","title":"New domain name jimmysong.io"},{"content":"This is a list of software, tools, architecture and reference materials about Cloud Native. It is awesome-cloud-native , a project I opened on GitHub , and can also be browsed through the web page .\nIt is divided into the following areas:\nAwesome Cloud Native\n AI  API gateway  Big Data  Container engine  CI-CD  Database  Data Science  Fault tolerant  Logging  Message broker  Monitoring  Networking  Orchestration and scheduler  Portability  Proxy and load balancer  RPC  Security and audit  Service broker  Service mesh  Service registry and discovery  Serverless  Storage  Tracing  Tools  Tutorial   This list will be continuously updated and improved in the future, not only for my usual research records, but also as a reference for the Cloud Native industry.\n","permalink":"https://jimmysong.io/en/notice/awesome-cloud-native/","summary":"This is a list of software, tools, architecture, and reference materials about Cloud Native. It is a project I started on GitHub.","title":"Awesome Cloud Native list is released"},{"content":"This book is the Chinese version of Migrating to Cloud Native Application Architectures. The English version of this book was released in February 2015. The Chinese version was translated by Jimmy Song and published in July 2017.\n GitHub hosting address for this book: https://github.com/rootsongjc/migrating-to-cloud-native-application-architectures  Gitbook reading address: https://jimmysong.io/migrating-to-cloud-native-application-architectures   The application architectures discussed in this book include:\n Twelve-factor application: A collection of cloud-native application architecture patterns Microservices: independently deployed services, each service does one thing Self-service agile infrastructure: a platform that provides application environments and back-office services quickly, repeatably, and consistently API-based collaboration: published and versioned APIs that allow interactions between services in a cloud-native application architecture Pressure resistance: a system that becomes stronger under pressure  ","permalink":"https://jimmysong.io/en/notice/changes-needed-to-cloud-native-archtecture/","summary":"This book is translated from an eBook published by Matt Stine in February 2015.","title":"Migrating to Cloud Native Chinese version released"}]