[{"content":"Istio 1.14 was released in June of this year, and one of the most notable features of this release is support for SPIRE , which is one of the implementations of SPIFFE , a CNCF incubation project. This article explains what SPIRE means for zero-trust architectures and why you would need SPIRE for authentication in Istio.\nAuthentication in Kubernetes We all know that Istio was built for and typically runs on Kubernetes, so before we talk about how to use SPIRE for authentication in Istio, let’s take a look at how Kubernetes handles authentication.\nLet’s look at an example of a pod’s token. Whenever a pod gets created in Kubernetes, it gets assigned a default service account from the namespace, assuming we didn’t explicitly assign a service account to it. Here is an example:\napiVersion: v1 data: ca.crt: {CA_CRT} namespace: ZGVmYXVsdA== token: {TOKEN_STRING} kind: Secret metadata: annotations: kubernetes.io/service-account.name: sleep kubernetes.io/service-account.uid: 2c0d00e8-13a2-48d0-9ff8-f987f3325ecf creationTimestamp: \u0026#34;2022-06-14T03:01:35Z\u0026#34; name: sleep-token-gwhwd namespace: default resourceVersion: \u0026#34;244535398\u0026#34; uid: b8822ceb-9553-4a17-96dc-d525bbaed0e0 type: kubernetes.io/service-account-token Kubernetes manages the identity of Pods with Service Accounts and then specifies the permissions of Pods with a Service Account to the Kubernetes API using RBAC. A service account’s token is stored in a secret, which does not include a declaration of the node or pod where the workload is running. When a malicious actor steals a token, they gain full access to the account and can steal information or carry out sabotage under the guise of that user.\nA token can only be used to identify a workload in one cluster, but Istio supports multiple clusters and meshes, as well as Kubernetes environments and virtual machines. A unified workload identity standard can help here.\nAn Introduction to SPIFFE and SPIRE SPIFFE’s (Secure Production Identity Framework for Everyone) goal is to create a zero-trust, fully-identified data center network by establishing an open, unified workload identity standard based on the concept of zero-trust. SPIRE can rotate X.509 SVID certificates and secret keys on a regular basis. Based on administrator-defined policies, SPIRE can dynamically provision workload certificates and Istio can consume them. I’ll go over some of the terms associated with SPIFFE in a little more detail below.\nSPIRE (SPIFFE Runtime Environment) is a SPIFFE implementation that is ready for production. SVID (SPIFFE Verifiable Identity Document) is the document that a workload uses to prove its identity to a resource or caller. SVID contains a SPIFFE ID that represents the service’s identity. It uses an X.509 certificate or a JWT token to encode the SPIFFE ID in a cryptographically verifiable document. The SPIFFE ID is a URI that looks like this: spiffe://trust_domain/workload_identifier.\nSPIFFE and Zero Trust Security The essence of Zero Trust is identity-centric dynamic access control. SPIFFE addresses the problem of identifying workloads.\nWe might identify a workload using an IP address and port in the era of virtual machines, but IP address-based identification is vulnerable to multiple services sharing an IP address, IP address forgery, and oversized access control lists. Because containers have a short lifecycle in the Kubernetes era, instead of an IP address, we rely on a pod or service name. However, different clouds and software platforms approach workload identity differently, and there are compatibility issues. This is especially true in heterogeneous hybrid clouds, where workloads run on both virtual machines and Kubernetes. It is critical to establish a fine-grained, interoperable identification system at this point.\nUsing SPIRE for Authentication in Istio With the introduction of SPIRE to Istio, we can give each workload a unique identity, which is used by workloads in the service mesh for peer authentication, request authentication, and authorization policies. The SPIRE Agent issues SVIDs for workloads by communicating with a shared UNIX Domain Socket in the workload. The Envoy proxy and the SPIRE agent communicate through the Envoy SDS (Secret Discovery Service) API. Whenever an Envoy proxy needs to access secrets (certificates, keys, or anything else needed to do secure communication), it will talk to the SPIRE agent through Envoy’s SDS API.\nThe most significant advantage of SDS is the ease with which certificates can be managed. Without this feature, certificates would have to be created as a secret and then mounted into the agent container in a Kubernetes deployment. The secret must be updated, and the proxy container must be re-deployed if the certificate expires. Using SDS, Istio can push the certificates to all Envoy instances in the service mesh. If the certificate expires, the server only needs to push the new certificate to the Envoy instance; Envoy will use the new certificate right away, and the …","relpermalink":"/en/blog/why-istio-need-spire/","summary":"This article explains what SPIRE means for zero-trust architectures and why you would need SPIRE for authentication in Istio.","title":"Why would you need SPIRE for authentication with Istio?"},{"content":" It’s been more than 5 years since Google, IBM, and Lyft unveiled the Istio open source project in May 2017. The Istio project has developed from a seed to a tall tree in these years. Many domestic books on the Istio service mesh were launched in the two years following the release of Istio 1.0 in 2018. My country is at the forefront of the world in the field of Istio book publishing.\nService mesh: one of the core technologies of cloud native Today, Istio is nearly synonymous with service mesh in China. The development of service mesh, as one of the core cloud-native technologies described by CNCF (Cloud Native Computing Foundation), has gone through the following stages.\n2017-2018: Exploratory Phase 2019-2020: Early Adopter Phase 2021 to present: Implementation on a large scale and ecological development stage Cloud native technology enables enterprises to design and deploy elastically scalable applications in new dynamic settings such as public, private, and hybrid clouds, according to the CNCF. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs are examples of cloud native technology.\nService mesh has been included to the CNCF definition of cloud native, indicating that it is one of the representative technologies of cloud native. Google is donating Istio to CNCF today, and we have reason to expect that as a CNCF project, Istio’s community will be more open, and its future development will be more smooth.\nService mesh and cloud native applications Cloud-native development is gaining traction. Despite the frequent emergence of new technologies and products, service mesh has maintained its place as “cloud-native network infrastructure” as part of the overall cloud-native technology stack throughout the past year. The cloud-native technology stack model is depicted in the diagram below, with representative technologies for each layer to define the standard. Service mesh and other cloud-native technologies complement each other as a new era of middleware emerges. Dapr (Distributed Application Runtime) defines the cloud-native middleware capability model, OAM defines the cloud-native application model, and so on, whereas service mesh Lattice defines a cloud-native seven-layer network model.\nCloud Native Application Model Why you need a service mesh Using a service mesh isn’t tantamount to abandoning Kubernetes; it just makes sense. The goal of Kubernetes is to manage application lifecycles through declarative configuration, whereas the goal of service mesh is to provide traffic control, security management, and observability amongst apps. How do you set up load balancing and flow management for calls between services after a robust microservice platform has been developed with Kubernetes?\nMany open source tools, including Istio, Linkerd, MOSN, and others, support Envoy’s xDS protocol. The specification of xDS is Envoy’s most significant contribution to service mesh or cloud native. Many various usage cases, such as API gateways, sidecar proxies in service meshes, and edge proxies, are derived from Envoy, which is simply a network proxy, a modern version of the proxy configured through the API.\nIn a nutshell, the move from Kubernetes to Istio was made for the following reasons.\nApplication life cycle management, specifically application deployment and management, is at the heart of Kubernetes (scaling, automatic recovery, and release). Kubernetes is a microservices deployment and management platform that is scalable and extremely elastic. Transparent proxy is the cornerstone of service mesh, which intercepts communication between microservices via sidecar proxy and then regulates microservice behavior via control plane settings. The deployment mode of service meshes has introduced new issues today. For service meshes, sidecar is no longer required, and an agentless service mesh based on gRPC is also being tested. xDS is a protocol standard for configuring service meshes, and a gRPC-based xDS is currently being developed. Kubernetes traffic management is decoupled with the service mesh. The kube-proxy component is not required to support traffic within the service mesh. The traffic between services is controlled by an abstraction close to the microservice application layer to achieve security and observability features. In Kubernetes, service mesh is an upper-level abstraction of service, and Serverless is the next stage, which is why Google released Knative based on Kubernetes and Istio following Istio. Open source in the name of the community The ServiceMesher community was founded in May 2018 with the help of Ant Financial. Following that, a tornado of service meshes erupted in China, and the community-led translation of Istio’s official documentation reached a fever pitch.\nI became aware of a dearth of Chinese resources for systematically teaching Istio over time, so in September 2018, I began to plan and create an Istio book, launching the Istio Handbook open source …","relpermalink":"/en/blog/istio-service-mesh-book/","summary":"By the Cloud Native Community(China)","title":"In-Depth Understanding of Istio: Announcing the Publication of a New Istio Book"},{"content":"This article will guide you on how to compile the Istio binaries and Docker images on macOS.\nBefore you begin Before we start, refer to the Istio Wiki , here is the information about my build environment.\nmacOS 12.3.1 Darwin AMD64 Docker Desktop 4.8.1(78998) Docker Engine v20.10.14 Start to compile First, download the Istio code from GitHub to the $GOPATH/src/istio.io/istio directory, and execute the commands below in that root directory.\nCompile into binaries Execute the following command to download the Istio dependent packages, which will be downloaded to the vendor directory.\ngo mod vendor Run the following command to build Istio:\nsudo make build If you do not run the command with sudo, you may encounter the following error.\nfatal: unsafe repository (\u0026#39;/work\u0026#39; is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work fatal: unsafe repository (\u0026#39;/work\u0026#39; is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work Makefile.core.mk:170: *** \u0026#34;TAG cannot be empty\u0026#34;. Stop. make: *** [build] Error 2 Even if you follow the prompts and run git config --global --add safe.directory /work, you will still get errors during compilation.\nThe compiled binary will be saved in out directory with the following directory structure.\nout ├── darwin_amd64 │ ├── bug-report │ ├── client │ ├── envoy │ ├── extauthz │ ├── install-cni │ ├── istio-cni │ ├── istio-cni-taint │ ├── istio-iptables │ ├── istio_is_init │ ├── istioctl │ ├── logs │ ├── operator │ ├── pilot-agent │ ├── pilot-discovery │ ├── release │ └── server └── linux_amd64 ├── envoy ├── envoy-centos ├── logs └── release It will build both the linux_amd64 and darwin_amd64 architectures binaries at the same time.\nCompile into Docker images Run the following command to compile Istio into a Docker image.\nsudo make build The compilation will take about 3 to 5 minutes depending on your network. Once the compilation is complete, you will see the Docker image of Istio by running the following command.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE localhost:5000/app_sidecar_centos_7 latest 2044037df94b 51 seconds ago 524MB localhost:5000/app_sidecar_ubuntu_jammy latest 5d8ae5ed55b7 About a minute ago 362MB localhost:5000/proxyv2 latest d4679412385f About a minute ago 243MB localhost:5000/install-cni latest 78f46d5771d2 About a minute ago 270MB localhost:5000/istioctl latest c38130a5adc8 About a minute ago 190MB localhost:5000/pilot latest 2aa9185ec202 About a minute ago 190MB localhost:5000/app latest 473adafaeb8d About a minute ago 188MB localhost:5000/operator latest 9ac1fedcdd12 About a minute ago 191MB localhost:5000/ext-authz latest 1fb5aaf20791 About a minute ago 117MB localhost:5000/app_sidecar_debian_11 latest 61376a02b95d 2 minutes ago 407MB localhost:5000/app_sidecar_ubuntu_xenial latest 7e8efe666611 2 minutes ago 418MB You can change the image name and push it into your own container registry.\nSummary This is how to build Istio on macOS. If you have already downloaded the Docker image you need to build, the build will take less than a minute. It also takes only a few minutes to build Docker images.\nReference Using the Code Base - github.com ","relpermalink":"/en/blog/how-to-build-istio/","summary":"This article will guide you on how to compile the Istio binaries on macOS.","title":"How to build Istio?"},{"content":"Updated on May 6, 2022\nBased on Istio version 1.13, this article will present the following.\nWhat is the sidecar pattern and what advantages does it have? How are the sidecar injections done in Istio? How does the sidecar proxy do transparent traffic hijacking? How is the traffic routed to upstream? The figure below shows how the productpage service requests access to http://reviews.default.svc.cluster.local:9080/ and how the sidecar proxy inside the reviews service does traffic blocking and routing forwarding when traffic goes inside the reviews service.\nIstio transparent traffic hijacking and traffic routing diagram At the beginning of the first step, the sidecar in the productpage pod has selected a pod of the reviews service to be requested via EDS, knows its IP address, and sends a TCP connection request.\nThere are three versions of the reviews service, each with an instance, and the sidecar work steps in the three versions are similar, as illustrated below only by the sidecar traffic forwarding step in one of the Pods.\nSidecar pattern Dividing the functionality of an application into separate processes running in the same minimal scheduling unit (e.g. Pod in Kubernetes) can be considered sidecar mode. As shown in the figure below, the sidecar pattern allows you to add more features next to your application without additional third-party component configuration or modifications to the application code.\nSidecar pattern The Sidecar application is loosely coupled to the main application. It can shield the differences between different programming languages and unify the functions of microservices such as observability, monitoring, logging, configuration, circuit breaker, etc.\nAdvantages of using the Sidecar pattern When deploying a service mesh using the sidecar model, there is no need to run an agent on the node, but multiple copies of the same sidecar will run in the cluster. In the sidecar deployment model, a companion container (such as Envoy or MOSN) is deployed next to each application’s container, which is called a sidecar container. The sidecar takes overall traffic in and out of the application container. In Kubernetes’ Pod, a sidecar container is injected next to the original application container, and the two containers share storage, networking, and other resources.\nDue to its unique deployment architecture, the sidecar model offers the following advantages.\nAbstracting functions unrelated to application business logic into a common infrastructure reduces the complexity of microservice code. Reduce code duplication in microservices architectures because it is no longer necessary to write the same third-party component profiles and code. The sidecar can be independently upgraded to reduce the coupling of application code to the underlying platform. iptables manipulation analysis In order to view the iptables configuration, we need to nsenter the sidecar container using the root user to view it, because kubectl cannot use privileged mode to remotely manipulate the docker container, so we need to log on to the host where the productpage pod is located.\nIf you use Kubernetes deployed by minikube, you can log directly into the minikube’s virtual machine and switch to root. View the iptables configuration that lists all the rules for the NAT (Network Address Translation) table because the mode for redirecting inbound traffic to the sidecar is REDIRECT in the parameters passed to the istio-iptables when the Init container is selected for the startup, so there will only be NAT table specifications in the iptables and mangle table configurations if TPROXY is selected. See the iptables command for detailed usage.\nWe only look at the iptables rules related to productpage below.\n# login to minikube, change user to root $ minikube ssh $ sudo -i # See the processes in the productpage pod\u0026#39;s istio-proxy container $ docker top `docker ps|grep \u0026#34;istio-proxy_productpage\u0026#34;|cut -d \u0026#34; \u0026#34; -f1` UID PID PPID C STIME TTY TIME CMD 1337 10576 10517 0 08:09 ? 00:00:07 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage.default --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istiod.istio-system.svc:15012 --zipkinAddress zipkin.istio-system:9411 --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --connectTimeout 10s --proxyAdminPort 15000 --concurrency 2 --controlPlaneAuthPolicy NONE --dnsRefreshRate 300s --statusPort 15020 --trust-domain=cluster.local --controlPlaneBootstrap=false 1337 10660 10576 0 08:09 ? 00:00:33 /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage.default --service-node sidecar~172.17.0.16~productpage-v1-7f44c4d57c-ksf9b.default~default.svc.cluster.local --max-obj-name-len 189 --local-address-ip-version v4 --log-format [Envoy (Epoch 0)] [%Y-%m-%d %T.%e][%t][%l][%n] …","relpermalink":"/en/blog/sidecar-injection-iptables-and-traffic-routing/","summary":"This blog describes the sidecar pattern, transparent traffic hijacking and routing.","title":"Sidecar injection, transparent traffic hijacking , and routing process in Istio explained in detail"},{"content":"This article will explain:\nThe sidecar auto-injection process in Istio The init container startup process in Istio The startup process of a Pod with Sidecar auto-injection enabled The following figure shows the components of a Pod in the Istio data plane after it has been started.\nIstio data plane pod Sidecar injection in Istio The following two sidecar injection methods are available in Istio.\nManual injection using istioctl. Kubernetes-based mutating webhook admission controller automatic sidecar injection method. Whether injected manually or automatically, SIDECAR’s injection process follows the following steps.\nKubernetes needs to know the Istio cluster to which the sidecar to be injected is connected and its configuration. Kubernetes needs to know the configuration of the sidecar container itself to be injected, such as the image address, boot parameters, etc. Kubernetes injects the above configuration into the side of the application container by the sidecar injection template and the configuration parameters of the above configuration-filled sidecar. The sidecar can be injected manually using the following command.\nistioctl kube-inject -f ${YAML_FILE} | kuebectl apply -f - This command is injected using Istio’s built-in sidecar configuration, see the Istio official website for details on how to use Istio below.\nWhen the injection is complete you will see that Istio has injected initContainer and sidecar proxy-related configurations into the original pod template.\nInit container The Init container is a dedicated container that runs before the application container is launched and is used to contain some utilities or installation scripts that do not exist in the application image.\nMultiple Init containers can be specified in a Pod, and if more than one is specified, the Init containers will run sequentially. The next Init container can only be run if the previous Init container must run successfully. Kubernetes only initializes the Pod and runs the application container when all the Init containers have been run.\nThe Init container uses Linux Namespace, so it has a different view of the file system than the application container. As a result, they can have access to Secret in a way that application containers cannot.\nDuring Pod startup, the Init container starts sequentially after the network and data volumes are initialized. Each container must be successfully exited before the next container can be started. If exiting due to an error will result in a container startup failure, it will retry according to the policy specified in the Pod’s restartPolicy. However, if the Pod’s restartPolicy is set to Always, the restartPolicy is used when the Init container failed.\nThe Pod will not become Ready until all Init containers are successful. The ports of the Init containers will not be aggregated in the Service. The Pod that is being initialized is in the Pending state but should set the Initializing state to true. The Init container will automatically terminate once it is run.\nSidecar injection example analysis For a detailed YAML configuration for the bookinfo applications, see bookinfo.yaml for the official Istio YAML of productpage in the bookinfo sample.\nThe following will be explained in the following terms.\nInjection of Sidecar containers Creation of iptables rules The detailed process of routing apiVersion: apps/v1 kind: Deployment metadata: name: productpage-v1 labels: app: productpage version: v1 spec: replicas: 1 selector: matchLabels: app: productpage version: v1 template: metadata: labels: app: productpage version: v1 spec: serviceAccountName: bookinfo-productpage containers: - name: productpage image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9080 volumeMounts: - name: tmp mountPath: /tmp volumes: - name: tmp emptyDir: {} Let’s see the productpage container’s Dockerfile .\nFROM python:3.7.4-slim COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt COPY test-requirements.txt ./ RUN pip install --no-cache-dir -r test-requirements.txt COPY productpage.py /opt/microservices/ COPY tests/unit/* /opt/microservices/ COPY templates /opt/microservices/templates COPY static /opt/microservices/static COPY requirements.txt /opt/microservices/ ARG flood_factor ENV FLOOD_FACTOR ${flood_factor:-0} EXPOSE 9080 WORKDIR /opt/microservices RUN python -m unittest discover USER 1 CMD [\u0026#34;python\u0026#34;, \u0026#34;productpage.py\u0026#34;, \u0026#34;9080\u0026#34;] We see that ENTRYPOINT is not configured in Dockerfile, so CMD’s configuration python productpage.py 9080 will be the default ENTRYPOINT, keep that in mind and look at the configuration after the sidecar injection.\n$ istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml We intercept only a portion of the YAML configuration that is part of the Deployment configuration associated with productpage.\ncontainers: - image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 # application image name: …","relpermalink":"/en/blog/istio-pod-process-lifecycle/","summary":"This article will explain Istio's Init container, Pod internal processes and the startup process.","title":"Istio data plane pod startup process explained"},{"content":"iptables is an important feature in the Linux kernel and has a wide range of applications. iptables is used by default in Istio for transparent traffic hijacking. Understanding iptables is very important for us to understand how Istio works. This article will give you a brief introduction to iptbles.\niptables introduction iptables is a management tool for netfilter, the firewall software in the Linux kernel. netfilter is located in the user space and is part of netfilter. netfilter is located in the kernel space and has not only network address conversion, but also packet content modification and packet filtering firewall functions.\nBefore learning about iptables for Init container initialization, let’s go over iptables and rule configuration.\nThe following figure shows the iptables call chain.\niptables 调用链 iptables The iptables version used in the Init container is v1.6.0 and contains 5 tables.\nRAW is used to configure packets. Packets in RAW are not tracked by the system. The filter is the default table used to house all firewall-related operations. NAT is used for network address translation (e.g., port forwarding). Mangle is used for modifications to specific packets (refer to corrupted packets). Security is used to force access to control network rules. Note: In this example, only the NAT table is used.\nThe chain types in the different tables are as follows.\nRule name raw filter nat mangle security PREROUTING ✓ ✓ ✓ INPUT ✓ ✓ ✓ OUTPUT ✓ ✓ ✓ ✓ ✓ POSTROUTING ✓ ✓ FORWARD ✓ ✓ ✓ Understand iptables rules View the default iptables rules in the istio-proxy container, the default view is the rules in the filter table.\n$ iptables -L -v Chain INPUT (policy ACCEPT 350K packets, 63M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 18M packets, 1916M bytes) pkts bytes target prot opt in out source destination We see three default chains, INPUT, FORWARD, and OUTPUT, with the first line of output in each chain indicating the chain name (INPUT/FORWARD/OUTPUT in this case), followed by the default policy (ACCEPT).\nThe following is a proposed structure diagram of iptables, where traffic passes through the INPUT chain and then enters the upper protocol stack, such as:\niptables chains Multiple rules can be added to each chain and the rules are executed in order from front to back. Let’s look at the table header definition of the rule.\nPKTS: Number of matched messages processed bytes: cumulative packet size processed (bytes) Target: If the message matches the rule, the specified target is executed. PROT: Protocols such as TDP, UDP, ICMP, and ALL. opt: Rarely used, this column is used to display IP options. IN: Inbound network interface. OUT: Outbound network interface. source: the source IP address or subnet of the traffic, the latter being anywhere. destination: the destination IP address or subnet of the traffic, or anywhere. There is also a column without a header, shown at the end, which represents the options of the rule, and is used as an extended match condition for the rule to complement the configuration in the previous columns. prot, opt, in, out, source and destination and the column without a header shown after destination together form the match rule. TARGET is executed when traffic matches these rules.\nTypes supported by TARGET\nTarget types include ACCEPT, REJECT, DROP, LOG, SNAT, MASQUERADE, DNAT, REDIRECT, RETURN or jump to other rules, etc. You can determine where the telegram is going by executing only one rule in a chain that matches in order, except for the RETURN type, which is similar to the return statement in programming languages, which returns to its call point and continues to execute the next rule.\nFrom the output, you can see that the Init container does not create any rules in the default link of iptables, but instead creates a new link.\nSummary With the above brief introduction to iptables, you have understood how iptables works, the rule chain and its execution order.\n","relpermalink":"/en/blog/understanding-iptables/","summary":"This article will give you a brief introduction to iptables, its tables and the order of execution.","title":"Understanding iptables"},{"content":"See the cloud native public library at: https://jimmysong.io/docs/ The cloud native public library project is a documentation project built using the Wowchemy theme, open sourced on GitHub .\nI have also adjusted the home page, menu and directory structure of the site, and the books section of the site will be maintained using the new theme.\nCloud native library positioning The cloud native public library is a collection of cloud native related books and materials published and translated by the author since 2017, and is a compendium and supplement to the dozen or so books already published. The original materials will continue to be published in the form of GitBooks, and the essence and related content will be sorted into the cloud native public library through this project.\nIn addition, the events section of this site has been revamped and moved to a new page .\n","relpermalink":"/en/notice/cloud-native-public-library/","summary":"A one-stop cloud native library that is a compendium of published materials.","title":"Cloud Native library launch"},{"content":"In my last two blogs:\nSidecar injection, transparent traffic hijacking , and routing process in Istio explained in detail Traffic types and iptables rules in Istio sidecar explained I gave you a detailed overview of the traffic in the Istio data plane, but the data plane does not exist in isolation. This article will show you the ports and their usages for each component of both the control plane and data plane in Istio, which will help you understand the relationship between these flows and troubleshoot them.\nOverview Firstly, I will show you a global schematic. The following figure shows the components of a sidecar in the Istio data plane, and the objects that interact with it.\nIstio components We can use the nsenter command to enter the namespace of the productpage Pod of the Bookinfo example and see the information about the ports it is listening on internally.\nIstio sidecar ports From the figure, we can see that besides the port 9080 that the productpage application listens to, the Sidecar container also listens to a large number of other ports, such as 15000, 15001, 15004, 15006, 15021, 15090, etc. You can learn about the ports used in Istio in the Istio documentation .\nLet’s go back into the productpage Pod and use the lsof -i command to see the ports it has open, as shown in the following figure.\nProductpage Pod ports We can see that there is a TCP connection established between the pilot-agent and istiod, the port in the listening described above, and the TCP connection established inside the Pod, which corresponds to the figure at the beginning of the article.\nThe root process of the Sidecar container (istio-proxy) is pilot-agent, and the startup command is shown below.\nInternal procecces in Sidecar As we can see from the figure, the PID of its pilot-agent process is 1, and it forked the Envoy process.\nCheck the ports it opens in Istiod, as shown in the figure below.\nIstiod ports We can see the ports that are listened to, the inter-process and remote communication connections.\nPorts usage overview These ports can play a pivotal role when you are troubleshooting. They are described below according to the component and function in which the port is located.\nPorts in Istiod The ports in Istiod are relatively few and single-function.\n9876: ControlZ user interface, exposing information about Istiod’s processes 8080: Istiod debugging port, through which the configuration and status information of the grid can be queried 15010: Exposes the xDS API and issues plain text certificates 15012: Same functionality as port 15010, but uses TLS communication 15014: Exposes control plane metrics to Prometheus 15017: Sidecar injection and configuration validation port Ports in sidecar From the above, we see that there are numerous ports in the sidecar.\n15000: Envoy admin interface, which you can use to query and modify the configuration of Envoy Proxy. Please refer to Envoy documentation for details. 15001: Used to handle outbound traffic. 15004: Debug port (explained further below). 15006: Used to handle inbound traffic. 15020: Summarizes statistics, perform health checks on Envoy and DNS agents, and debugs pilot-agent processes, as explained in detail below. 15021: Used for sidecar health checks to determine if the injected Pod is ready to receive traffic. We set up the readiness probe on the /healthz/ready path on this port, and Istio hands off the sidecar readiness checks to kubelet. 15053: Local DNS proxy for scenarios where the cluster’s internal domain names are not resolved by Kubernetes DNS. 15090: Envoy Prometheus query port, through which the pilot-agent will scratch metrics. The above ports can be divided into the following categories.\nResponsible for inter-process communication, such as 15001, 15006, 15053 Health check and information statistics, e.g. 150021, 15090 Debugging: 15000, 15004 Let’s look at the key ports in detail.\n15000 15000 is Envoy’s Admin interface, which allows us to modify Envoy and get a view and query metrics and configurations.\nThe Admin interface consists of a REST API with multiple endpoints and a simple user interface. You can enable the Envoy Admin interface view in the productpage Pod using the following command:\nkubectl -n default port-forward deploy/productpage-v1 15000 Visit http://localhost:15000 in your browser and you will see the Envoy Admin interface as shown below.\nEnvoy Admin interface 15004 With the pilot-agent proxy istiod debug endpoint on port 8080, you can access localhost’s port 15004 in the data plane Pod to query the grid information, which has the same effect as port 8080 below.\n8080 You can also forward istiod port 8080 locally by running the following command:\nkubectl -n istio-system port-forward deploy/istiod 8080 Visit http://localhost:8080/debug in your browser and you will see the debug endpoint as shown in the figure below.\nPilot Debug Console Of course, this is only one way to get the mesh information and debug the mesh, you can also use istioctl …","relpermalink":"/en/blog/istio-components-and-ports/","summary":"This article will introduce you to the various ports and functions of the Istio control plane and data plane.","title":" Istio component ports and functions in detail"},{"content":"As we know that Istio uses iptables for traffic hijacking, where the iptables rule chains has one called ISTIO_OUTPUT, which contains the following rules.\nRule target in out source destination 1 RETURN any lo 127.0.0.6 anywhere 2 ISTIO_IN_REDIRECT any lo anywhere !localhost owner UID match 1337 3 RETURN any lo anywhere anywhere !owner UID match 1337 4 RETURN any any anywhere anywhere owner UID match 1337 5 ISTIO_IN_REDIRECT any lo anywhere !localhost owner GID match 1337 6 RETURN any lo anywhere anywhere !owner GID match 1337 7 RETURN any any anywhere anywhere owner GID match 1337 8 RETURN any any anywhere localhost 9 ISTIO_REDIRECT any any anywhere anywhere The sidecar applies these rules to deal with different types of traffic. This article will show you the six types of traffic and their iptables rules in Istio sidecar.\niptables Traffic Routing in Sidecar The following list summarizes the six types of traffic in Sidecar.\nRemote service accessing local service: Remote Pod -\u0026gt; Local Pod Local service accessing remote service: Local Pod -\u0026gt; Remote Pod Prometheus crawling metrics of local service: Prometheus -\u0026gt; Local Pod Traffic between Local Pod service: Local Pod -\u0026gt; Local Pod Inter-process TCP traffic within Envoy Sidecar to Istiod traffic The following will explain the iptables routing rules within Sidecar for each scenario, which specifies which rule in ISTIO_OUTPUT is used for routing.\nType 1: Remote Pod -\u0026gt; Local Pod The following are the iptables rules for remote services, applications or clients accessing the local pod IP of the data plane.\nRemote Pod -\u0026gt; RREROUTING -\u0026gt; ISTIO_INBOUND -\u0026gt; ISTIO_IN_REDIRECT -\u0026gt; Envoy 15006 (Inbound) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 1 -\u0026gt; POSTROUTING -\u0026gt; Local Pod\nWe see that the traffic only passes through the Envoy 15006 Inbound port once. The following diagram shows this scenario of the iptables rules.\nRemote Pod to Local Pod Type 2: Local Pod -\u0026gt; Remote Pod The following are the iptables rules that the local pod IP goes through to access the remote service.\nLocal Pod-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 9 -\u0026gt; ISTIO_REDIRECT -\u0026gt; Envoy 15001 (Outbound) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 4 -\u0026gt; POSTROUTING -\u0026gt; Remote Pod\nWe see that the traffic only goes through the Envoy 15001 Outbound port.\nLocal Pod to Remote Pod The traffic in both scenarios above passes through Envoy only once because only one scenario occurs in that Pod, sending or receiving requests.\nType 3: Prometheus -\u0026gt; Local Pod Prometheus traffic that grabs data plane metrics does not have to go through the Envoy proxy.\nThese traffic pass through the following iptables rules.\nPrometheus-\u0026gt; RREROUTING -\u0026gt; ISTIO_INBOUND (traffic destined for ports 15020, 15090 will go to INPUT) -\u0026gt; INPUT -\u0026gt; Local Pod\nPrometheus to Local Pod Type 4: Local Pod -\u0026gt; Local Pod A Pod may simultaneously have two or more services. If the Local Pod accesses a service on the current Pod, the traffic will go through Envoy 15001 and Envoy 15006 ports to reach the service port of the Local Pod.\nThe iptables rules for this traffic are as follows.\nLocal Pod-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 9 -\u0026gt; ISTIO_REDIRECT -\u0026gt; Envoy 15001（Outbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 2 -\u0026gt; ISTIO_IN_REDIRECT -\u0026gt; Envoy 15006（Inbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 1 -\u0026gt; POSTROUTING -\u0026gt; Local Pod\nLocal Pod to Local Pod Type 5: Inter-process TCP traffic within Envoy Envoy internal processes with UID and GID 1337 will communicate with each other using lo NICs and localhost domains.\nThe iptables rules that these flows pass through are as follows.\nEnvoy process (Localhost) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 8 -\u0026gt; POSTROUTING -\u0026gt; Envoy process (Localhost)\nEnvoy inter-process TCP traffic Type 6: Sidecar to Istiod traffic Sidecar needs access to Istiod to synchronize its configuration so that Envoy will have traffic sent to Istiod.\nThe iptables rules that this traffic passes through are as follows.\npilot-agent process -\u0026gt; OUTPUT -\u0026gt; Istio_OUTPUT RULE 9 -\u0026gt; Envoy 15001 (Outbound Handler) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 4 -\u0026gt; POSTROUTING -\u0026gt; Istiod\nSidecar to Istiod Summary All the sidecar proxies that Istio injects into the Pod or installed in the virtual machine form the data plane of the service mesh, which is also the main workload location of Istio. In my next blog, I will take you through the ports of each component in Envoy and their functions, so that we can have a more comprehensive understanding of the traffic in Istio.\n","relpermalink":"/en/blog/istio-sidecar-traffic-types/","summary":"This article will show you the six traffic types and their iptables rules in Istio sidecar, and take you through the whole diagram in a schematic format.","title":"Traffic types and iptables rules in Istio sidecar explained"},{"content":"Istio 1.13 is the first release of 2022, and, not surprisingly, the Istio team will continue to release new versions every quarter. Overall, the new features in this release include:\nSupport for newer versions of Kubernetes New API – ProxyConfig, for configuring sidecar proxies Improved Telemetry API Support for hostname-based load balancers with multiple network gateways Support for Kubernetes Versions I often see people asking in the community which Istio supports Kubernetes versions. Istio’s website has a clear list of supported Kubernetes versions. You can see here that Istio 1.13 supports Kubernetes versions 1.20, 1.21, 1.22, and 1.23, and has been tested but not officially supported in Kubernetes 1.16, 1.17, 1.18, 1.19.\nWhen configuring Istio, there are a lot of checklists. I noted them all in the Istio cheatsheet . There are a lot of cheat sheets about configuring Istio, using resources, dealing with everyday problems, etc., in this project, which will be online soon, so stay tuned.\nThe following screenshot is from the Istio cheatsheet website, it shows the basic cheat sheet for setting up Istio.\nIstio cheatsheet Introducing the new ProxyConfig API Before Istio version 1.13, if you wanted to customize the configuration of the sidecar proxy, there were two ways to do it.\nMeshConfig\nUse MeshConfig and use IstioOperator to modify it at the Mesh level. For example, use the following configuration to alter the default discovery port for istiod.\napVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: defaultConfig: discoveryAddress: istiod:15012 Annotation in the Pods\nYou can also use annotation at the Pod level to customize the configuration. For example, you can add the following annotations to Pod to modify the default port for istiod of the workload:\nanannotations: proxy.istio.io/config: | discoveryAddress: istiod:15012 When you configure sidecar in either of these ways, the fields set in annotations will completely override the default fields in MeshConfig. Please refer to the Istio documentation for all configuration items of ProxyConfig.\nThe new API – ProxyConfig\nBut in 1.13, a new top-level custom resource, ProxyConfig, has been added, allowing you to customize the configuration of your sidecar proxy in one place by specifying a namespace and using a selector to select the scope of the workload, just like any other CRD. Istio currently has limited support for this API, so please refer to the Istio documentation for more information on the ProxyConfig API.\nHowever, no matter which way you customize the configuration of the sidecar proxy, it does not take effect dynamically and requires a workload restart to take effect. For example, for the above configuration, because you changed the default port of istiod, all the workloads in the mesh need to be restarted before connecting to the control plane.\nTelemetry API MeshConfig customized extensions and configurations in the Istio mesh. The three pillars of observability– Metrics, Telemetry, and Logging– can each be docked to different providers. The Telemetry API gives you a one-stop place for flexible configuration of them. Like the ProxyConfig API, the Telemetry API follows the configuration hierarchy of Workload Selector \u0026gt; Local Namespace \u0026gt; Root Configuration Namespace. The API was introduced in Istio 1.11 and has been further refined in that release to add support for OpenTelemetry logs, filtered access logs, and custom tracing service names. See Telemetry Configuration for details.\nAutomatic resolution of multi-network gateway hostnames In September 2021, a member of the Istio community reported an issue with the EKS load balancer failing to resolve when running multi-cluster Istio in AWS EKS. Workloads that cross cluster boundaries need to be communicated indirectly through a dedicated east-west gateway for a multi-cluster, multi-network mesh. You can follow the instructions on Istio’s website to configure a multi-network, primary-remote cluster, and Istio will automatically resolve the IP address of the load balancer based on the hostname.\nIstio 1.13.1 fixing the critical security vulnerabilities Istio 1.13.1 was released to fix a known critical vulnerability that could lead to an unauthenticated control plane denial of service attack.\nThe figure below shows a multi-cluster primary-remote mesh where istiod exposes port 15012 to the public Internet via a gateway so that a pod on another network can connect to it.\nMulti-network Mesh When installing a multi-network, primary-remote mode Istio mesh, for a remote Kubernetes cluster to access the control plane, an east-west Gateway needs to be installed in the Primary cluster, exposing port 15012 of the control plane istiod to the Internet. An attacker could send specially crafted messages to that port, causing the control plane to crash. If you set up a firewall to allow traffic from only some IPs to access this port, you will be able to reduce the impact of the problem. It is …","relpermalink":"/en/blog/what-is-new-in-istio-1-13/","summary":"In February 2022, Istio released 1.13.0 and 1.13.1. This blog will give you an overview of what’s new in these two releases.","title":"What's new in Istio 1.13?"},{"content":"Join a team of world-class engineers working on the next generation of networking services using Istio, Envoy, Apache SkyWalking and a few of the open projects to define the next generation of cloud native network service.\nIstio upstream contributor: Golang We are looking for engineers with strong distributed systems experience to join our team. We are building a secure, robust, and highly available service mesh platform for mission critical enterprise applications spanning both legacy and modern infrastructure. This is an opportunity to dedicate a significant amount of contribution to Istio upstream on a regular basis. If you are a fan of Istio and would like to increase your contribution on a dedicated basis, this would be an opportunity for you.\nRequirements\nFundamentals-based problem solving skills; Drive decision by function, first principles based mindset. Demonstrate bias-to-action and avoid analysis-paralysis; Drive action to the finish line and on time. You are ego-less when searching for the best ideasIntellectually curious with a penchant for seeing opportunities in ambiguity Understands the difference between attention to detail vs. detailed - oriented Values autonomy and results over process You contribute effectively outside of your specialty Experience building distributed system platforms using Golang Familiarity with Kubernetes, service mesh technologies such as Istio and Envoy Excellent understanding of networking protocols, concepts, consistency properties of distributed systems, techniques to identify and reconcile configuration drifts Experience contributing to open source projects is a plus. Familiarity with WebAssembly is a plus. Familiarity with Golang, hardware/software load balancers (F5, NGINX), HSM modules, Active Directory/LDAP is a plus. We encourage written and asynchronous communication in English, and proficient oral English is not required.\nDistributed Systems Engineer, Enterprise Infrastructure (Data plane) GoLang or C++ Developers Seeking backend software engineers experienced in building distributed systems using Golang and gRPC. We are building a secure, and highly available service mesh platform for mission-critical enterprise applications for Fortune 500 companies, spanning both the legacy and modern infrastructure. Should possess strong fundamentals in distributed systems and networking. Familiarity with technologies like Kubernetes, Istio, and Envoy, as well as open contributions would be a plus.\nRequirements\nExperience building distributed system platforms using C++, Golang, and gRPC. Familiarity with Kubernetes, service mesh technologies such as Istio and Envoy. Excellent understanding of networking protocols, concepts, consistency properties of distributed systems, techniques to identify and reconcile configuration drifts. Experience contributing to open source projects is a plus. Familiarity with the following is a plus: WebAssembly, Authorization: NGAC, RBAC, ABAC Familiarity with hardware/software load balancers (F5, NGINX), HSM modules, Active Directory/LDAP is a plus. Site Reliability Engineer, SRE Site Reliability Engineering (SRE) combines software and systems engineering to build and run scalable, massively distributed, fault-tolerant systems. As part of the team, you will be working on ensuring that Tetrate’s platform has reliability/uptime appropriate to users’ needs as well as a fast rate of improvement. Additionally, much of our engineering effort focuses on building infrastructure, improving the platform troubleshooting abilities, and eliminating toil through automation.\nRequirements\nSystematic problem-solving approach, coupled with excellent communication skills and a sense of ownership/finish and self-directed drive. Strong fundamentals in operating, debugging, and troubleshooting distributed systems(stateful and/or stateless) and networking. Familiarity with Kubernetes, service mesh technologies such as Istio and EnvoyAbility to debug, optimize code, and automate routine tasks. Experience programming in at least one of the following languages: C++, Rust, Python, Go. Familiarity with the concepts of quantifying failure and availability in a prescriptive manner using SLOs and SLIs. Experience in performance analysis and tuning is a plus. Location Worldwide\nWe are remote with presence in China, Indonesia, India, Japan, U.S., Canada, Ireland, the Netherlands, Spain, and Ukraine.\nPlease send GitHub or online links that showcase your code style along with your resume to careers@tetrate.io .\nAbout Tetrate Powered by Envoy and Istio, its ﬂagship product, Tetrate Service Bridge, enables bridging traditional and modern workloads. Customers can get consistent baked-in observability, runtime security and traffic management for all their workloads, in any environment.\nIn addition to the technology, Tetrate brings a world-class team that leads the open Envoy and Istio projects, providing best practices and playbooks that enterprises can use to modernize their …","relpermalink":"/en/notice/tetrate-recruit/","summary":"Remotely worldwide","title":"The Enterprise Service Mesh company Tetrate is hiring"},{"content":"As the service mesh architecture concept gains traction and the scenarios for its applications emerge, there is no shortage of discussions about it in the community. I have worked on service mesh with the community for 4 years now, and will summarize the development of service mesh in 2021 from this perspective. Since Istio is the most popular service mesh, this article will focus on the technical and ecological aspects of Istio.\nService mesh: a critical tech for Cloud Native Infrastructure As one of the vital technologies defined by CNCF for cloud native, Istio has been around for five years now. Their development has gone through the following periods.\nExploration phase: 2017-2018 Early adopter phase: 2019-2020 Large-scale landing and ecological development phase: 2021-present Service mesh has crossed the “chasm”(refer Crossing the Chasm theory) and is in between the “early majority” and “late majority” phases of adoption. Based on feedback from the audience of Istio Weekly, users are no longer blindly following new technologies for experimentation and are starting to consider whether they need them in their organization dialectically.\nCross the chasm While new technologies and products continue to emerge, the service mesh, as part of the cloud native technology stack, has continued to solidify its position as the “cloud native network infrastructure” over the past year. The diagram below illustrates the cloud native technology stack model, where each layer has several representative technologies that define the standard. As new-age middleware, the service mesh mirrors other cloud native technologies, such as Dapr (Distributed Application Runtime), which represents the capability model for cloud native middleware, OAM , which defines the cloud native application model, and the service mesh, which defines the L7 network model.\nCloud Native Stack A layered view of the cloud native application platform technology stack\nOptimizing the mesh for large scale production applications with different deployment models Over the past year, the community focused on the following areas.\nPerformance optimization: performance issues of service mesh in large-scale application scenarios. Protocol and extensions: enabling service mesh to support arbitrary L7 network protocols. Deployment models: Proxyless vs. Node model vs. Sidecar model. eBPF: putting some of the service mesh’s capabilities to the kernel layer. Performance optimization Istio was designed to serve service to service traffic by “proto-protocol forwarding”. The goal is making the service mesh as “transparent” as possible to applications. Thus using IPtables to hijack the traffic, according to the community-provided test results Istio 1.2 adds only 3 ms to the baseline latency for a mesh with 1000 RPS on 16 connections. However, because of issues inherent in the IPtables conntrack module, Istio’s performance issues begin to emerge as the mesh size increases. To optimize the performance of the Istio sidecar for resource usage and network latency, the community gave the following solutions.\nSidecar configuration: By configuring service dependencies manually or by adding an Operator to the control plane, the number of service configurations sent to Sidecar can be reduced, thus reducing the resource footprint of the data plane; for more automatic and intelligent configuration of Sidecar, the open source projects Slime and Aeraki both offer their innovative configuration loading solutions. The introduction of eBPF: eBPF can be a viable solution to optimize the performance of the service mesh. Some Cilium-based startups even radically propose to use eBPF to replace the Sidecar proxy completely. Still, the Envoy proxy/xDS protocol has become the proxy for the service mesh implementation and supports the Layer 7 protocol very well. We can use eBPF to improve network performance, but complex protocol negotiation, parsing, and user scaling remain challenging to implement on the user side. Protocol and extensions Extensibility of Istio has always been a significant problem, and there are two aspects to Istio’s extensibility.\nProtocol level: allowing Istio to support all L7 protocols Ecological: allowing Istio to run more extensions Istio uses Envoy as its data plane. Extending Istio is essentially an extension of Envoy’s functionality. Istio’s official solution is to use WebAssembly, and in Istio 1.12, the Wasm plugin configuration API was introduced to extend the Istio ecosystem. Istio’s extension mechanism uses the Proxy-Wasm Application Binary Interface (ABI) specification to provide a set of proxy-independent streaming APIs and utilities that can be implemented in any language with an appropriate SDK. Today, Proxy-Wasm’s SDKs are AssemblyScript (similar to TypeScript), C++, Rust, Zig, and Go (using the TinyGo WebAssembly System Interface).\nThere are still relatively few WebAssembly extensions available, and many enterprises choose to customize their CRD and build a …","relpermalink":"/en/blog/service-mesh-in-2021/","summary":"A review of the development of Service Mesh in 2021.","title":"Service Mesh in 2021: the ecosystem is emerging"},{"content":"It’s been more than four years since Istio launched in May 2017, and while the project has had a strong following on GitHub and 10+ releases, its growing open-source ecosystem is still in its infancy.\nRecently added support for WebAssembly extensions has made the most popular open source service mesh more extensible than ever. This table lists the open-source projects in the Istio ecosystem as of November 11, 2021, sorted by open-source date. These projects enhance the Istio service mesh with gateways, extensions, utilities, and more. In this article, I’ll highlight the two new projects in the category of extensions.\nProject Value Relationship with Istio Category Launch Date Dominant company Number of stars Envoy Cloud native high-performance edge/middle-service proxy The default data plane proxy September 2016 Lyft 18700 Istio Connection, secure, control, and observation services. Control plane service mesh May 2017 Google 29100 Emissary Gateway Kubernetes native API gateway for microservices, built on Envoy Connectable to Istio gateway February 2018 Ambassador 3600 APISIX Cloud native API gateways It can run as a data plane for Istio or as a gateway on its own gateway June 2019 API7 8100 MOSN Cloud native edge gateways \u0026amp; agents Available as Istio data plane proxy December 2019 Ant 3500 Slime Intelligent service mesh manager based on Istio Adding a management plane to Istio extensions January 2021 NetEase 236 GetMesh Istio integration and command-line management tools Utility for Istio multi-version management tools February 2021 Tetrate 95 Aeraki Manage any of Istio’s seven layers of load Extended multi-protocol support extensions March 2021 Tencent 330 Layotto Cloud native application runtime Using as a data plane for Istio runtime June 2021 Ant 393 Hango Gateway API gateways built on Envoy and Istio Integrates with Istio gateway August 2021 NetEase 253 Slime: an intelligent service mesh manager for Istio Slime is an Istio-based, intelligent mesh manager open-sourced by NetEase’s microservices team. Based on the Kubernetes Operator implementation, Slime can be used as a CRD manager that seamlessly interfaces with Istio without needing any customization or definition of dynamic service governance policies. This achieves automatic and convenient use of Istio and Envoy’s advanced features.\nSlime addresses the following issues:\nImplementing higher-level extensions in Istio. For example, extending the HTTP plugin; adaptive traffic limiting based on the resource usage of the service. Poor performance arising from Istio sending all the configurations within the mesh to each sidecar proxy. Slime solves these problems by building an Istio management plane. Its main purpose are\nto build a pluggable controller to facilitate the extension of new functions. to obtain data by listening to the data plane to intelligently generate the configuration for Istio. to build a higher-level CRD for the user to configure, which Slime converts into an Istio configuration. The following diagram shows the flow chart of Istio as an Istio management plane.\nSlime architecture The specific steps for Slime to manage Istio are as follows.\nSlime operator completes the initialization of Slime components in Kubernetes based on the administrator’s configuration. Developers create configurations that conform to the Slime CRD specification and apply them to Kubernetes clusters. Slime queries the monitoring data of the relevant service stored in Prometheus and converts the Slime CRD into an Istio CRD, in conjunction with the configuration of the adaptive part of the Slime CRD while pushing it to the Global Proxy. Istio listens for the creation of Istio CRDs. Istio pushes the configuration information of the Sidecar Proxy to the corresponding Sidecar Proxy in the data plane. The diagram below shows the internal architecture of Slime.\nSlime Internal We can divide Slime internally into three main components.\nslime-boot: operator for deploying Slime modules on Kubernetes. slime-controller: the core component of Slime that listens to the Slime CRD and converts it to an Istio CRD. slime-metric: the component used to obtain service metrics information. slime-controller dynamically adjusts service governance rules based on the information it receives. The following diagram shows the architecture of Slime Adaptive Traffic Limiting. Slime smart limiter Slime dynamically configures traffic limits by interfacing with the Prometheus metric server to obtain real-time monitoring.\nSlime’s adaptive traffic limitation process has two parts: one that converts SmartLimiter to EnvoyFilter and the other that monitors the data. Slime also provides an external monitoring data interface (Metric Discovery Server) that allows you to sync custom monitoring metrics to the traffic limiting component via MDS.\nThe CRD SmartLimiter created by Slime is used to configure adaptive traffic limiting. Its configuration is close to natural semantics, e.g., if you want to trigger an …","relpermalink":"/en/blog/istio-extensions-slime-and-aeraki/","summary":"In this article, I’ll introduce you two Istio extension projects: Aeraki and Slime.","title":"Introducing Slime and Aeraki in the evolution of Istio open-source ecosystem"},{"content":"You can use Istio to do multi-cluster management , API Gateway , and manage applications on Kubernetes or virtual machines . In my last blog , I talked about how service mesh is an integral part of cloud native applications. However, building infrastructure can be a big deal. There is no shortage of debate in the community about the practicability of service mesh and Istio– here’s a list of common questions and concerns, and how to address them.\nIs anyone using Istio in production? What is the impact on application performance due to the many resources consumed by injecting sidecar into the pod? Istio supports a limited number of protocols; is it scalable? Will Istio be manageable? – Or is it too complex, old services too costly to migrate, and the learning curve too steep? I will answer each of these questions below.\nIstio is architecturally stable, production-ready, and ecologically emerging Istio 1.12 was just released in November – and has evolved significantly since the explosion of service mesh in 2018 (the year Istio co-founders established Tetrate). Istio has a large community of providers and users . The Istio SIG of Cloud Native Community has held eight Istio Big Talk (Istio 大咖说) , with Baidu, Tencent, NetEase, Xiaohongshu(小红书), and Xiaodian Technology(小电科技) sharing their Istio practices. According to CNCF Survey Report 2020 , about 50% of the companies surveyed are using a service mesh in production or planning to in the next year, and about half (47%) of organizations using a service mesh in production are using Istio.\nMany companies have developed extensions or plugins for Istio, such as Ant, NetEase, eBay, and Airbnb. Istio’s architecture has been stable since the 1.5 release, and the release cycle is fixed quarterly, with the current project’s main task being Day-2 Operations.\nThe Istio community has also hosted various events, with the first IstioCon in March 2021, the Istio Meetup China in Beijing in July, and the Service Mesh Summit 2022 in Shanghai in January 2022.\nSo we can say that the Istio architecture is stable and production-ready, and the ecosystem is budding.\nThe impact of service mesh on application performance A service mesh uses iptables to do traffic hijacking by default to be transparent to applications. When the number of services is large, there are a lot of iptables rules that affect network performance. You can use techniques like eBPF to provide application performance, but the method requires a high version of the operating system kernel, which few enterprises can achieve.\nIstio DNS In the early days, Istio distributed the routing information of all services in the mesh to all proxy sidecars, which caused sidecar s to take up a lot of resources. Aeraki and Slime can achieve configuration lazy loading. We will introduce these two open-source projects in the Istio open-source ecosystem.\nFinally, there is a problem related to Sidecar proxy operation and maintenance: upgrading all Envoy proxies while ensuring constant traffic. A solution is using the SidecarSet resource in the open-source project OpenKruise .\nThe resource consumption and network latency associated with the introduction of Sidecar are also within reasonable limits, as you can see from the service mesh benchmark performance tests .\nExtending the Istio service mesh The next question is about extending the Istio service mesh. The current solution given by the Istio community is to use WebAssembly , an extension that is still relatively little used in production by now and has performance concerns. Most of the answers I’ve observed are CRDs that build a service mesh management plane based on Istio.\nAlso, making Istio support heterogeneous environments for all workloads, such as virtual machines and containers, is in strong demand for end-users. It allows them to migrate applications from traditional loads to cloud native easily. Finally, hybrid cloud traffic management for multiple clusters and meshes is a more advanced requirement.\nSteep learning curve Many people complain that Istio has too little learning material. Istio has been open source for four years, and there are a lot of learning resources now:\nIstio Documentation IstioCon 2021 Istio Big Talk/Istio Weekly Istio Fundamentals Course Certified Istio Administrator Yes, Istio is complex, but it’s been getting more and more manageable with every release. In my next blog, I will introduce you to two open source projects that extend Istio and give you some insight into what’s going on in the Istio community.\n","relpermalink":"/en/blog/the-debate-in-the-community-about-istio-and-service-mesh/","summary":"There is no shortage of debate in the community about the practicability of service mesh and Istio – here’s a list of common questions and concerns, and how to address them.","title":"The debate in the community about Istio and service mesh"},{"content":"If you don’t know what Istio is, you can read my previous articles below:\nWhat Is Istio and Why Does Kubernetes Need it? Why do you need Istio when you already have Kubernetes? This article will explore the relationship between service mesh and cloud native.\nService mesh – the product of the container orchestration war If you’ve been following the cloud-native space since its early days, you’ll remember the container orchestration wars of 2015 to 2017. Kubernetes won the container wars in 2017, the idea of microservices had taken hold, and the trend toward containerization was unstoppable. Kubernetes architecture matured and slowly became boring, and service mesh technologies, represented by Linkerd and Istio, entered the CNCF-defined cloud-native critical technologies on the horizon.\nKubernetes was designed with the concept of cloud-native in mind. A critical idea in cloud-native is the architectural design of microservices. When a single application is split into microservices, how can microservices be managed to ensure the SLA of the service as the number of services increases? The service mesh was born to solve this problem at the architectural level, free programmers’ creativity, and avoid tedious service discovery, monitoring, distributed tracing, and other matters.\nThe service mesh takes the standard functionality of microservices down to the infrastructure layer, allowing developers to focus more on business logic and thus speed up service delivery, which is consistent with the whole idea of cloud-native. You no longer need to integrate bulky SDKs in your application, develop and maintain SDKs for different languages, and just use the service mesh for Day 2 operations after the application is deployed.\nThe service mesh is regarded as the next generation of microservices. In the diagram, we can see that many of the concerns of microservices overlap with the functionality of Kubernetes. Kubernetes focuses on the application lifecycle, managing resources and deployments with little control over services. The service mesh fills this gap. The service mesh can connect, control, observe and protect microservices.\nKubernetes vs. xDS vs. Istio This diagram shows the layered architecture of Kubernetes and Istio.\nKubernetes vs xDS vs Istio The diagram indicates that the kube-proxy settings are global and cannot be controlled at a granular level for each service. All Kubernetes can do is topology-aware routing, routing traffic closer to the Pod, and setting network policies in and out of the Pod.\nIn contrast, the service mesh takes traffic control out of the service layer in Kubernetes through sidecar proxies, injects proxies into each Pod, and manipulates these distributed proxies through a control plane. It allows for more excellent resiliency.\nKube-proxy implements traffic load balancing between multiple pod instances of a Kubernetes service. But how do you finely control the traffic between these services — such as dividing the traffic by percentage to different application versions (which are all part of the same service, but on other deployments), or doing canary releases and blue-green releases?\nThe Kubernetes community gives a way to do canary releases using Deployment, assigning different pods to deployed services by modifying the pod’s label.\nEnvoy Architecture Currently, the most popular open-source implementation of service mesh in the world is Istio. From the CNCF Survey Report 2020 , we know that Istio is the most used service mesh in production today. Many companies have built their service mesh based on Istio, such as Ant, Airbnb, eBay, NetEase, Tencent, etc.\nCNCF Survey Report 2020 Figure from CNCF Survey Report 2020 Istio is developed based on Envoy, which has been used by default as its distributed proxy since the first day it was open-sourced. Envoy pioneered the creation of the xDS protocol for distributed gateway configuration, greatly simplifying the configuration of large-scale distributed networks. Ant Group open source MOSN also supported xDS In 2019. Envoy was also one of the first projects to graduate from CNCF, tested by large-scale production applications.\nService mesh – the cloud-native networking infrastructure With the above comparison between Kubernetes and service mesh in mind, we can see the place of service mesh in the cloud-native application architecture. That is, building a cloud-native network infrastructure specifically provides:\nTraffic management: controlling the flow of traffic and API calls between services, making calls more reliable, and enhancing network robustness in different environments. Observability: understanding the dependencies between services and the nature and flow of traffic between them provides the ability to identify problems quickly. Policy enforcement: controlling access policies between services by configuring the mesh rather than by changing the code. Service Identification and Security: providing service identifiability and security …","relpermalink":"/en/blog/service-mesh-an-integral-part-of-cloud-native-apps/","summary":"This article will explore the relationship between service mesh and cloud native.","title":"Service Mesh - an integral part of cloud-native applications"},{"content":"API gateways have been around for a long time as the entry point for clients to access the back-end, mainly to manage “north-south” traffic, In recent years, service mesh architectures have become popular, mainly for managing internal systems,(i.e. “east-west” traffic), while a service mesh like Istio also has built-in gateways that bring traffic inside and outside the system under unified control. This often creates confusion for first-time users of Istio. What is the relationship between the service mesh and the API gateway? How does Istio’s gateway work? What are the ways to expose the services in the Istio mesh? This article gives you the answer.\nKey Insights The service mesh was originally created to solve the problem of managing internal traffic for distributed systems, but API gateways existed long before it. While the Gateway is built into Istio, you can still use a custom Ingress Controller to proxy external traffic. API gateways and service mesh are converging. How do I expose services in the Istio mesh? The following diagram shows four approaches to expose services in the Istio mesh using Istio Gateway, Kubernetes Ingress, API Gateway, and NodePort/LB.\nExposing services through Istio Ingress Gateway The Istio mesh is shaded, and the traffic in the mesh is internal (east-west) traffic, while the traffic from clients accessing services within the Kubernetes cluster is external (north-south) traffic.\nApproach Controller Features NodePort/LoadBalancer Kubernetes Load balancing Kubernetes Ingress Ingress controller Load balancing, TLS, virtual host, traffic routing Istio Gateway Istio Load balancing, TLS, virtual host, advanced traffic routing, other advanced Istio features API Gateway API Gateway Load balancing, TLS, virtual host, advanced traffic routing, API lifecycle management, billing, rate limiting, policy enforcement, data aggregation Since NodePort/LoadBalancer is a basic way to expose services built into Kubernetes, this article will not discuss that option. Each of the other three approaches will be described below.\nUsing Kubernetes Ingress to expose traffic We all know that clients of a Kubernetes cluster cannot directly access the IP address of a pod because the pod is in a network plane built into Kubernetes. We can expose services inside Kubernetes outside the cluster using NodePort or Load Balancer Kubernetes service type. To support virtual hosting, hiding and saving IP addresses, you can use Ingress resources to expose services in Kubernetes.\nKubernetes Ingress to expose services Ingress is a Kubernetes resource that controls the behavior of an ingress controller that does the traffic touring, which is the equivalent of a load-balanced directional proxy server such as Nginx, Apache, etc., which also includes rule definitions, i.e., routing information for URLs, which is provided by the Ingress controller .\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: istio name: ingress spec: rules: - host: httpbin.example.com http: paths: - path: /status/* backend: serviceName: httpbin servicePort: 8000 The kubernetes.io/ingress.class: istio annotation in the example above indicates that the Ingress uses the Istio Ingress Controller which in fact uses Envoy proxy.\nUsing Istio Gateway to expose services Istio is a popular service mesh implementation that has evolved from Kubernetes that implements some features that Kubernetes doesn’t. (See What is Istio and why does Kubernetes need Istio? ) It makes traffic management transparent to the application, moving this functionality from the application to the platform layer and becoming a cloud-native infrastructure.\nIstio used Kubernetes Ingress as the traffic portal in versions prior to Istio 0.8, where Envoy was used as the Ingress Controller. From Istio 0.8 and later, Istio created the Gateway object. Gateway and VirtualService are used to represent the configuration model of Istio Ingress, and the default implementation of Istio Ingress uses the same Envoy proxy. In this way, the Istio control plane controls both the ingress gateway and the internal sidecar proxy with a consistent configuration model. These configurations include routing rules, policy enforcement, telemetry, and other service control functions.\nThe Istio Gateway resources function similarly to the Kubernetes Ingress in that it is responsible for north-south traffic to and from the cluster. The Istio Gateway acts as a load balancer to carry connections to and from the edge of the service mesh. The specification describes a set of open ports and the protocols used by those ports, as well as the SNI configuration for load balancing, etc.\nThe Istio Gateway resource itself can only be configured for L4 through L6, such as exposed ports, TLS settings, etc.; however, the Gateway can be bound to a VirtualService, where routing rules can be configured on L7, such as versioned traffic routing, fault injection, HTTP redirects, HTTP …","relpermalink":"/en/blog/istio-servicemesh-api-gateway/","summary":"What is the relationship between the service mesh and the API gateway? How does Istio’s gateway work? What are the ways to expose the services in the Istio mesh? This article gives you the answer.","title":"Using Istio service mesh as API Gateway"},{"content":"Do you have multiple Kubernetes clusters and a service mesh? Do your virtual machines and services in a Kubernetes cluster need to interact? This article will take you through the process and considerations of building a hybrid cloud using Kubernetes and an Istio Service Mesh. Together, Kubernetes and Istio can be used to bring hybrid workloads into a mesh and achieve interoperability for multicluster. But another layer of infrastructure — a management plane — is helpful for managing multicluster or multimesh deployments.\nKubernetes Using Kubernetes enables rapid deployment of a distributed environment that enables cloud interoperability and unifies the control plane on the cloud. It also provides resource objects, such as Service, Ingress and Gateway , to handle application traffic. The Kubernetes API Server communicates with the kube-proxy component on each node in the cluster, creates iptables rules for the node, and forwards requests to other pods.\nAssuming that a client now wants to access a service in Kubernetes, the request is first sent to the Ingress/Gateway, then forwarded to the backend service (Service A in the diagram below) based on the routing configuration in the Ingress/Gateway. Then Service A polls an instance of Service B for the traffic requested by Service B. Lastly, the traffic requested by Service A for Service B is polled forward to Service B’s instance.\nKubernetes Kubernetes Multicluster The most common usage scenarios for multicluster management include:\nservice traffic load balancing isolating development and production environments decoupling data processing and data storage cross-cloud backup and disaster recovery flexible allocation of compute resources low-latency access to services across regions avoiding vendor lock-in There are often multiple Kubernetes clusters within an enterprise; and the KubeFed implementation of Kubernetes cluster federation developed by Multicluster SIG enables multicluster management capabilities, which allows all Kubernetes clusters to be managed through the same interface.\nThere are several general issues that need to be addressed when using cluster federation:\nConfiguring which clusters need to be federated API resources need to be propagated across the clusters Configuring how API resources are distributed to different clusters Registering DNS records in clusters to enable service discovery across clusters The following is a multicluster architecture for KubeSphere — one of the most commonly used Kubernetes multicluster management architectures — where the Host Cluster serves as the control plane with two member clusters, West and East.\nMulticluster The Host Cluster needs to be able to access the API Server of the Member Cluster, but the network connectivity between Member Clusters is not required. The Host Cluster is independent of the Member Cluster it manages and the Member Cluster is not aware of the existence of the Host Cluster. The advantage of this is that when the control plane fails, the Member Cluster will not be affected and the deployed load can still operate normally without being affected.\nThe Host Cluster also assumes the role of API portal, and the Host Cluster forwards the resource requests to the Member Cluster — which is convenient for aggregation and also facilitates unified authority authentication. We see that there is a Federation Control Plane in the Host Cluster, where the Push Reconciler propagates the identity, role, and role binding from the Federation Cluster to all Member Clusters.\nIstio Service Mesh Consider using the Istio service mesh when we have multilingual, multiversion microservices running in Kubernetes and need finer-grained canary publishing and unified security policy management for inter-service observability. Istio enables intelligent application-aware load balancing from the application layer to other Service Mesh-enabled services in the cluster, by transparently intercepting all traffic to and from the application using IPTables, and bypassing the primary kube-proxy load balancing. The Istio control plane communicates with the Kubernetes API Server to obtain information about all registered services in the cluster.\nThe following diagram illustrates the basics of Istio, where all nodes belong to the same Kubernetes cluster.\nIstio Service Mesh You may end up with at least a few Kubernetes clusters, each hosting microservices. Multiple deployment models exist for Istio’s multicluster deployments — depending on network isolation, primary and backup — which can be specified by declaration when deploying using Istio Operator. Communication between these microservices in a cluster can be enhanced by a service mesh. Within the cluster, Istio provides common communication patterns to improve resiliency, security and observability.\nAll of the above is about application load management on Kubernetes, but for legacy applications on virtual machines: how can they be managed in the same plane? Istio supports …","relpermalink":"/en/blog/multicluster-management-with-kubernetes-and-istio/","summary":"This article explains three patterns/tools for debugging microservices in Kubernetes and the changes brought by the introduction of Istio for debugging microservices.","title":"Multicluster Management with Kubernetes and Istio"},{"content":"Kubernetes is arguably the best environment for running microservices so far, but the experience of debugging microservices in a Kubernetes environment may not be as user-friendly. This article will show you how to debug microservices in Kubernetes, introduce common tools, and explain how the introduction of Istio impacts debugging microservices.\nDebugging microservices is vastly different from traditional monolithic applications The debugging of microservices has been a long-standing problem for software developers. This challenge does not exist in traditional monolithic applications because developers can leverage the debugger in IDEs to add breakpoints, modify environment variables, single-step execution, etc. for their applications, all of which provide great help in software debugging. With the popularity of Kubernetes, the debugging of microservices becomes a thorny issue, where the following issues are more complicated than the debugging of traditional monolithic applications.\nMultiple dependencies A microservice often depends on multiple other microservices, some shared volumes across multiple microservices, and authorizations based on service accounts. When debugging a microservice, how do you deploy other dependent services to quickly build a latest set of staging environments?\nAccess from a local machine When microservices are running on a developer’s local computer, there is usually no direct access to the services in a Kubernetes cluster. How can you debug microservices deployed in a Kubernetes cluster as if they were local services?\nSlow development loop Usually, it takes a long process to update the code and build it into an image before pushing it to the cluster. How do you speed up the development cycle? Let’s look at the tools that address those challenges.\nTools The main solutions for debugging microservices in Kubernetes are:\nProxy: by building a VPN, deploying a proxy in the Kubernetes cluster, and adding local debug endpoints to make the services in Kubernetes directly accessible to local applications, your architecture will look like [ local service ] \u0026lt;-\u0026gt; [ proxy ] \u0026lt;-\u0026gt; [ app in Kubernetes ]. Sidecar: Inject a sidecar into the pod of the microservice to be debugged to intercept all traffic to and from the service, so that the service can be tracked and monitored, and the service can also be debugged in this sidecar. Service Mesh: To get an overall picture of the application, inject sidecars into all microservices so that you can get a dashboard that monitors global status. Here are three typical open source projects that implement the above solutions, each of which can help you debug microservices from a different perspective. You can apply them at different stages of software development and they can be said to be complementary to each other.\nProxy – debugging microservices with Telepresence Telepresence is essentially a local proxy that proxies data volumes, environment variables, and networks in a Kubernetes cluster locally. The following diagram shows the main usage scenarios for Telepresence.\nProxy mode: Telepresence Users need to manually execute the telepresence command locally, which will automatically deploy the agent to Kubernetes. Once the agent has been deployed,\nLocal services will have complete access to other services in the Kubernetes cluster, environment variables, Secret, ConfigMap, etc. Services in the cluster also have direct access to the locally exposed endpoints. However, this approach requires users to run multiple commands while debugging locally, and in some network environments it may not be possible to establish a VPN connection to the Kubernetes cluster.\nSidecar – debugging microservices with Nocalhost Nocalhost is a Kubernetes-based cloud development environment. To use it, you just need to install a plugin in your IDE – VS Code to extend Kubernetes and shorten the development feedback cycle. The development environment can be isolated by creating different namespaces for different users and using ServiceAccount when binding to different user corners. Nocalhost also provides a web console and API for administrators to manage different development environments.\nSidecar mode: Nocalhost As long as you have a Kubernetes cluster and have admin rights to the cluster, you can refer to the Nocalhost documentation to quickly start trying it out. To use the Nocalhost plugin in VS Code, you need to configure the Kubernetes cluster in the plugin first.\nSelect the Kubeconfig file you just exported or copy and paste the contents of the file directly into the configuration. Then select the service you need to test and select the corresponding Dev Container. VS Code will automatically open a new code window. Here is an example of the bookinfo sample provided by Istio. You can open the cloned code in your local IDE and click the hammer next to the code file to enter development mode. Selecting the corresponding DevContainer and Nocalhost will automatically inject a …","relpermalink":"/en/blog/how-to-debug-microservices-in-kubernetes-with-proxy-sidecar-or-service-mesh/","summary":"This article explains three patterns/tools for debugging microservices in Kubernetes and the changes brought by the introduction of Istio for debugging microservices.","title":"How to debug microservices in Kubernetes with proxy, sidecar or service mesh?"},{"content":"Istio was named by Tetrate founder Varun Talwar and Google lead engineer Louis Ryan in 2017 and was open sourced on May 24, 2017. Today is the fourth anniversary of Istio’s open source arrival. Let’s take a look back at Istio’s four years of development — and look forward to Istio’s future.\nIstio’s open source history In 2017, the year Kubernetes ended the container orchestration battle, Google took the opportunity to consolidate its dominance in the cloud native space and compensate for Kubernetes’ disadvantage in service-to-service traffic management by open-sourcing Istio. Istio released its 1.10 last week — but here are some of the most important releases in Istio’s history to date.\nDate Version Note May 24, 2017 0.1 Officially open source; established the architectural foundation of Control Plane, Data Plane and sidecar proxy. October 10, 2017 0.2 Started to support multiple runtime environments, such as virtual machines. June 1, 2018 0.8 API refactoring July 31, 2018 1.0 Production-ready, after which the Istio team underwent a massive reorganization. March 19, 2019 1.1 Enterprise-ready. Support for multiple Kubernetes clusters, with performance optimizations. March 3, 2020 1.5 Back to monolith, with microservice components merged into istiod, making Istio’s architecture cleaner and easier to maintain. Support for WebAssembly extension, making Istio’s ecology much stronger. November 18, 2020 1.8 Officially deprecated Mixer and focused on adding support for virtual machines. A year after its inception– and two months before the 1.0 release, version 0.8 was released with a massive refactoring of the API. In late July 2018, when 1.0 was released, Istio reached a production-ready tipping point. Since then, Google has massively reorganized the Istio team and several Istio-based service mesh startups were born, making 2018 the booming year of the service mesh industry.\nIstio 1.1 was released in March 2019, almost 9 months after 1.0 was released, which is far beyond the average release cycle of an open-source project. We know that the speed of iteration and evolution is a core competency of basic software. Since then, Istio has started a regular release cadence of one version per quarter and has become the #4 fastest growing project in GitHub’s top 10 in 2019 !\nThe Istio community In 2020, Istio’s project management began to mature and its governance reached a stage of evolution. We saw the first election of a steering committee for the Istio community and the transfer of the trademark to Open Usage Commons . The first IstioCon was successfully held in February 2021, with thousands of people attending the online conference. There is also a large Istio community in China , and face-to-face Istio community meetups will be held there in 2021. Stay tuned for more.\nAccording to the CNCF 2020 Survey, 46% of organizations were either using a service mesh in production or planning to use it in the next 12 months. Istio was the top used mesh among those using a mesh in production.\nThe future After 4 years of development, there is not only a large user base around Istio, but also several Istio vendors, as you can see on the homepage of the recently revamped Istio website. In the last few releases, Istio has shifted its development focus to improving the Day 2 Operation experience. We also expect to see more Istio adoption path recommendations, case studies, learning materials, training, and certifications (such as the industry’s first Certified Istio Administrator from Tetrate) that will facilitate the adoption of Istio.\n","relpermalink":"/en/blog/istio-4-year-birthday/","summary":"Today is Istio's 4 year birthday, let’s take a look back at Istio’s four years of development — and look forward to Istio’s future.","title":"Happy Istio 4th Anniversary -- Retrospect and Outlook"},{"content":"Istio, the most popular service mesh implementation , was developed on top of Kubernetes and has a different niche in the cloud native application ecosystem than Kubernetes. Rather than introduce you directly to what Istio has to offer, this article will explain how Istio came about and what it is in relation to Kubernetes.\nWhy Is There an Istio? To explain what Istio is, it’s also important to understand the context in which Istio came into being — i.e., why is there an Istio?\nMicroservices are a technical solution to an organizational problem. And Kubernetes/Istio are a technical solution to deal with the issues created by moving to microservices. As a deliverable for microservices, containers solve the problem of environmental consistency and allow for more granularity in limiting application resources. They are widely used as a vehicle for microservices.\nGoogle open-sourced Kubernetes in 2014, which grew exponentially over the next few years. It became a container scheduling tool to solve the deployment and scheduling problems of distributed applications — allowing you to treat many computers as though they were one computer. Because the resources of a single machine are limited and Internet applications may have traffic floods at different times (due to rapid expansion of user scale or different user attributes), the elasticity of computing resources needs to be high. A single machine obviously can’t meet the needs of a large-scale application; and conversely, it would be a huge waste for a very small-scale application to occupy the whole host.\nIn short, Kubernetes defines the final state of the service and enables the system to reach and stay in that state automatically. So how do you manage the traffic on the service after the application has been deployed? Below we will look at how service management is done in Kubernetes and how it has changed in Istio.\nHow Do You Do Service Management in Kubernetes? The following diagram shows the service model in Kubernetes:\nKubernetes Service Model From the above figure we can see that:\nDifferent instances of the same service may be scheduled to different nodes. Kubernetes combines multiple instances of a service through Service objects to unify external services. Kubernetes installs a kube-proxy component in each node to forward traffic, which has simple load balancing capabilities. Traffic from outside the Kubernetes cluster can enter the cluster via Ingress (Kubernetes has several other ways of exposing services; such as NodePort, LoadBalancer, etc.). Kubernetes is used as a tool for intensive resource management. However, after allocating resources to the application, Kubernetes doesn’t fully solve the problems of how to ensure the robustness and redundancy of the application, how to achieve finer-grained traffic division (not based on the number of instances of the service), how to guarantee the security of the service, or how to manage multiple clusters, etc.\nThe Basics of Istio The following diagram shows the service model in Istio, which supports both workloads and virtual machines in Kubernetes.\nIstio From the diagram we can see that:\nIstiod acts as the control plane, distributing the configuration to all sidecar proxies and gateways. (Note: for simplification, the connections between Istiod and sidecar are not drawn in the diagram.) Istio enables intelligent application-aware load balancing from the application layer to other mesh enabled services in the cluster, and bypasses the rudimentary kube-proxy load balancing. Application administrators can manipulate the behavior of traffic in the Istio mesh through a declarative API, in the same way they manage workloads in Kubernetes. It can take effects within seconds and they can do this without needing to redeploy. Ingress is replaced by Gateway resources, a special kind of proxy that is also a reused Sidecar proxy. A sidecar proxy can be installed in a virtual machine to bring the virtual machine into the Istio mesh. In fact, before Istio one could use SpringCloud, Netflix OSS, and other tools to programmatically manage the traffic in an application, by integrating the SDK in the application. Istio makes traffic management transparent to the application, moving this functionality out of the application and into the platform layer as a cloud native infrastructure.\nIstio complements Kubernetes, by enhancing its traffic management, observability and security for cloud native applications. The service mesh open source project — launched in 2017 by Google, IBM and Lyft — has come a long way in three years. A description of Istio’s core features can be found in the Istio documentation .\nSummary Service Mesh is the cloud native equivalent of TCP/IP, addressing application network communication, security and visibility issues. Istio is currently the most popular service mesh implementation, relying on Kubernetes but also scalable to virtual machine loads. Istio’s core consists of a control plane and a data …","relpermalink":"/en/blog/what-is-istio-and-why-does-kubernetes-need-it/","summary":"This article will explain how Istio came about and what it is in relation to Kubernetes.","title":"What Is Istio and Why Does Kubernetes Need it?"},{"content":"If you’ve heard of service mesh and tried Istio , you may have the following questions:\nWhy is Istio running on Kubernetes? What is the role of Kubernetes and a service mesh in the cloud native application architecture, respectively? What aspects of Kubernetes does Istio extend? What problems does it solve? What is the relationship between Kubernetes, Envoy, and Istio? This article will take you through the inner workings of Kubernetes and Istio. In addition, I will introduce the load balancing approach in Kubernetes, and explain why you need Istio when you have Kubernetes.\nKubernetes is essentially about application lifecycle management through declarative configuration, while a service mesh is essentially about providing inter-application traffic, security management and observability. If you have already built a stable application platform using Kubernetes, how do you set up load balancing and traffic control for calls between services? This is where a service mesh comes into the picture.\nEnvoy introduces the xDS protocol, which is supported by various open source software, such as Istio , MOSN , etc. Envoy contributes xDS to a service mesh or cloud native infrastructure. Envoy is essentially a modern version of a proxy that can be configured through APIs, based on which many different usage scenarios are derived — such as API Gateway, sidecar proxy in service mesh, and edge proxy.\nThis article contains the following:\nA description of the role of kube-proxy. The limitations of Kubernetes for microservice management. An introduction to the capabilities of Istio service mesh. A comparison of some of the concepts in Kubernetes, Envoy, and the Istio service mesh. Kubernetes vs Service Mesh The following diagram shows the service access relationship in Kubernetes and service mesh (one sidecar per pod model).\nKubernetes vs Service Mesh Traffic Forwarding Each node in a Kubernetes cluster deploys a kube-proxy component that communicates with the Kubernetes API Server, gets information about the services in the cluster, and then sets iptables rules to send requests for service directly to the corresponding Endpoint (a pod belonging to the same group of services).\nService Discovery Service Discovery Istio can follow the service registration in Kubernetes and can also interface with other service discovery systems via platform adapters in the control plane; and then generate data plane configurations (using CRD, which are stored in etcd) with transparent proxies for the data plane. The transparent proxy of the data plane is deployed as a sidecar container in the pod of each application service, and all these proxies need to request the control plane to synchronize the proxy configuration. The proxy is “transparent” because the application container is completely unaware of the presence of the proxy. The kube-proxy component in the process needs to intercept traffic as well, except that the kube-proxy intercepts traffic to and from the Kubernetes node — while the sidecar proxy intercepts traffic to and from the pod.\nDisadvantages of a Service Mesh Since Kubernetes has many pods running on each node, putting the original kube-proxy route forwarding function in each pod will increase the response latency — due to more hops when the sidecar intercepts the traffic — and consume more resources. In order to manage traffic in a fine-grained manner, a series of new abstractions will be added. This will further increase the learning cost for users, but as the technology becomes more popular this situation will be slowly alleviated.\nAdvantages of a Service Mesh The kube-proxy settings are global and cannot be controlled at a granular level for each service, while service mesh takes the traffic control out of the service layer in Kubernetes by means of sidecar proxy — allowing for more elasticity.\nShortcomings of Kube-Proxy First, it does not automatically try another pod if the forwarded pod is not serving properly. Each pod has a health check mechanism and when a pod has health problems, kubelet will restart the pod and kube-proxy will remove the corresponding forwarding rules. Also, nodePort-type services cannot add TLS or more complex message routing mechanisms.\nKube-proxy implements load balancing of traffic across multiple pod instances of a Kubernetes service, but how do you do fine-grained control of traffic between these services — such as dividing traffic by percentage to different application versions (which are all part of the same service but on different deployments), or doing canary releases (grayscale releases) and blue-green releases?\nThe Kubernetes community gives a way to do canary releases using Deployment , which is essentially a way to assign different pods to a deployment’s service by modifying the pod’s label.\nKubernetes Ingress vs. Istio Gateway As mentioned above, kube-proxy can only route traffic within a Kubernetes cluster. The pods of a Kubernetes cluster are located in a network created by CNI. …","relpermalink":"/en/blog/why-do-you-need-istio-when-you-already-have-kubernetes/","summary":"This article will take you through the inner workings of Kubernetes and Istio. In addition, I will introduce the load balancing approach in Kubernetes, and explain why you need Istio when you have Kubernetes.","title":"Why Do You Need Istio When You Already Have Kubernetes?"},{"content":"Tetrate Academy has recently released the Istio Fundamentals Course, which is now available for free. Sign up at Tetrate Academy now!\nCourse curriculum Here is the curriculum:\nService Mesh and Istio Overview Installing Istio Observability: Telemetry and Logs Traffic Management Security Advanced Features Troubleshooting Real World Examples There are self-assessment questions at the end of each course. I have passed the course, and here is the certificate after passing the course.\nTetrate Academy Istio Fundamentals Course More In the future, Tetrate will release the Certified Istio Administrator (CIA) exam and welcome all Istio users and administrators to follow and register for it.\n","relpermalink":"/en/notice/tetrate-istio-fundamental-courses/","summary":"Tetrate Academy has recently released the Istio Fundamentals Course, which is now available for free。","title":"Tetrate Academy Releases Free Istio Fundamentals Course"},{"content":"Different companies or software providers have devised countless ways to control user access to functions or resources, such as Discretionary Access Control (DAC), Mandatory Access Control (MAC), Role-Based Access Control (RBAC), and Attribute-Based Access Control (ABAC). In essence, whatever the type of access control model, three basic elements can be abstracted: user, system/application, and policy.\nIn this article, we will introduce ABAC, RBAC, and a new access control model — Next Generation Access Control (NGAC) — and compare the similarities and differences between the three, as well as why you should consider NGAC.\nWhat Is RBAC? RBAC, or Role-Based Access Control, takes an approach whereby users are granted (or denied) access to resources based on their role in the organization. Every role is assigned a collection of permissions and restrictions, which is great because you don’t need to keep track of every system user and their attributes. You just need to update appropriate roles, assign roles to users, or remove assignments. But this can be difficult to manage and scale. Enterprises that use the RBAC static role-based model have experienced role explosion: large companies may have tens of thousands of similar but distinct roles or users whose roles change over time, making it difficult to track roles or audit unneeded permissions. RBAC has fixed access rights, with no provision for ephemeral permissions or for considering attributes like location, time, or device. Enterprises using RBAC have had difficulty meeting the complex access control requirements to meet regulatory requirements of other organizational needs.\nRBAC Example Here’s an example Role in the “default” namespace in Kubernetes that can be used to grant read access to pods:\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [\u0026#34;v1\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] What Is ABAC? ABAC stands for Attribute-Based Access Control. At a high level, NIST defines ABAC as an access control method “where subject requests to perform operations on objects are granted or denied based on assigned attributes of the subject, environment conditions, and a set of policies that are specified in terms of those attributes and conditions.” ABAC is a fine-grained model since you can assign any attributes to the user, but at the same time it becomes a burden and hard to manage:\nWhen defining permissions, the relationship between users and objects cannot be visualized. If the rules are a little complex or confusingly designed, it will be troublesome for the administrator to maintain and trace. This can cause performance problems when there is a large number of permissions to process.\nABAC Example Kubernetes initially uses ABAC as access control and is configured via JSON Lines, for example:\nAlice can just read pods in namespace “foo”:\n{\u0026#34;apiVersion\u0026#34;: \u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: {\u0026#34;user\u0026#34;: \u0026#34;alice\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;pods\u0026#34;, \u0026#34;readonly\u0026#34;: true}} What Is NGAC? NGAC, or Next Generation Access Control, takes the approach of modeling access decision data as a graph. NGAC enables a systematic, policy-consistent approach to access control, granting or denying users administrative capabilities with a high level of granularity. NGAC was developed by NIST (National Institute of Standards and Technology) and is currently used in Tetrate Q and Tetrate Service Bridge .\nThere are several types of entities; they represent the resources you want to protect, the relationships between them, and the actors that interact with the system. The entities are:\nUsers Objects User attributes, such as organization unit Object attributes, such as folders Policy classes, such as file system access, location, and time NIST’s David Ferraiolo and Tetrate ‘s Ignasi Barrera shared how NGAC works at their presentation on Next Generation Access Control at Service Mesh Day 2019 in San Francisco.\nNGAC is based on the assumption that you can represent the system you want to protect in a graph that represents the resources you want to protect and your organizational structure, in a way that has meaning to you and that adheres to your organization semantics. On top of this model that is very particular to your organization, you can overlay policies. Between the resource model and the user model, the permissions are defined. This way NGAC provides an elegant way of representing the resources you want to protect, the different actors in the system, and how both worlds are tied together with permissions.\nNGAC DAG Image via Linear Time Algorithms to Restrict Insider Access using Multi-Policy Access Control Systems NGAC Example The following example shows a simple NGAC graph with a User DAG representing an organization structure, an Object DAG representing files and folders in a filesystem, a categorization of the files, and two different policies — file system and scope — …","relpermalink":"/en/blog/why-you-should-choose-ngac-as-your-access-control-model/","summary":"This article will introduce you to the next generation permission control model, NGAC, and compare ABAC, RABC, and explain why you should choose NGAC.","title":"Why You Should Choose NGAC as Your Access Control Model"},{"content":" IstioCon 2021 poster（Jimmy Song） Topic: Service Mesh in China Time: February 23rd, 10:00 - 10:10 am Beijing time How to participate: IstioCon 2021 website Cost: Free From February 22-25, Beijing time, the Istio community will be hosting the first IstioCon online, and registration is free to attend! I will be giving a lightning talk on Tuesday, February 23rd (the 12th day of the first month of the lunar calendar), as an evangelist and witness of Service Mesh technology in China, I will introduce the Service Mesh industry and community in China.\nI am a member of the inaugural IstioCon organizing committee with Zhonghu Xu (Huawei) and Shaojun Ding (Intel), as well as the organizer of the China region. Considering Istio’s large audience in China, we have arranged for Chinese presentations that are friendly to the Chinese time zone. There will be a total of 14 sharing sessions in Chinese, plus dozens more in English. The presentations will be in both lightning talk (10 minutes) and presentation (40 minutes) formats.\nJoin the Cloud Native Community Istio SIG to participate in the networking at this conference. For the schedule of IstioCon 2021, please visit IstioCon 2021 official website , or click for details.\n","relpermalink":"/en/notice/istiocon-2021/","summary":"IstioCon 2021, I'll be giving a lightning talk, February 22nd at 10am BST.","title":"IstioCon 2021 Lightning Talk Preview"},{"content":"The ServiceMesher website has lost connection with the webhook program on the web publishing server because the GitHub where the code is hosted has has been “lost” and the hosting server is temporarily unable to log in, so the site cannot be updated. Today I spent a day migrating all the blogs on ServiceMesher to the Cloud Native Community website cloudnative.to , and as of today, there are 354 blogs on the Cloud Native Community.\nServiceMesher blogs Now we plan to archive ServiceMesher official GitHub (all pages under the servicemesher.com domain) We are no longer accepting new PRs, so please submit them directly to the Cloud Native Community . Thank you all!\n","relpermalink":"/en/notice/servicemesher-blog-merged/","summary":"ServiceMesher website is no longer maintained, plan to archive the website code, the blog has been migrated to Cloud Native Community, please submit the new blog to Cloud Native Community.","title":"ServiceMesher website is no longer maintained, the original blog has been migrated to the cloud native community"},{"content":"In this article, I’ll give you an overview of Istio ‘s history of virtual machine integration support. In particular, the introduction of the smart DNS proxy and WorkloadGroup in Istio 1.8, which makes virtual machines and containers equivalent at the resource abstraction level.\nI will show you a tumultuous odyssey of Istio’s virtual machine integration. Tetrate, the enterprise service mesh company that made pushing Istio to run everywhere part of its founding mission, has used VM features extensively in customer deployments and has been instrumental in pushing VMs to Istio upstream.\nPreface In my previous article , I talked about how Istio 1.7 supported virtual machines. But at that time, late October, virtual machines were still not seamlessly integrated into Istio — there was still a lot of manual work required. Now, Istio 1.8 has added WorkloadGroup and smart DNS proxy, which allows non-Kubernetes workloads like VMs to become first-class citizens in Istio — just like pods.\nWith or without a sidecar installed for virtual machines, until 1.7 you could not resolve the DNS name of a Kubernetes service unless a kube-external DNS was configured — which is the last piece of virtual machine integration in Istio. This shortcoming has finally been fixed in Istio 1.8.\nWhy Is Virtual Machine Support Important? In the process of migrating our applications to cloud native architectures and continuously containerizing them, we will go through three phases as shown in the figure below.\nCloud Native Stages Stage 1: All applications are deployed on virtual machines Stage 2: Applications are deployed on both virtual machines and containers, are migrating from virtual machines to containers, and are using Kubernetes to manage containers. Stage 3: All applications are deployed in containers first, using Kubernetes to manage containers and Istio to manage service-to-service communication. The above diagram is artificially simplified: in reality, there might be multiple hybrid clouds, multiple regions, multiple clusters, etc. Plus, at stage 3 containers and virtual machines may remain in long-term coexistence, but the trend of containerization remains unchanged.\nIstio’s History of Virtual Machine Support Istio’s support for virtual machines is a long process, an odyssey of sorts.\n0.2: Istio Mesh Expansion As of version 0.2, Istio added virtual machines to the Mesh via Istio Mesh Expansion , provided that the following prerequisites were met.\nVirtual machines must have direct access to the application’s pods via IP address, which requires a flat network between the container and the VM via VPC or VPN; and virtual machines do not need access to the Cluster IP, but rather direct access to the service’s endpoints. Virtual machines must have access to Istio’s control plane services (Pilot, Mixer, CA, now being integrated as Istiod), which can expose the control plane endpoints to virtual machines by deploying load balancers in the Istio Mesh. (optional) the virtual machine has access to the DNS server inside the Mesh (deployed in Kubernetes). The steps to integrate a virtual machine are as follows.\nCreate an internal load balancer for the Istio control plane service and the DNS service for the Kubernetes cluster. Generate a configuration file for the Istio Service CIDR, Service Account token, security certificate, and IP of the Istio Control Plane Service (the IP exposed through the Internal Load Balancer) and send it to the virtual machine. Setup the Istio component, dnsmaq (for DNS discovery), in the virtual machine; so that the virtual machine can access the services in the mesh using FQDN, to ensure that the virtual machine can correctly resolve the Cluster IP of the services in the mesh. To run the service in a virtual machine, you need to configure the sidecar, add inbound ports to be intercepted, then restart Istio and also run istioctl to register the service. The following figure shows the detailed flow from integrating a virtual machine to accessing services in the virtual machine in a mesh.\nFigure 1 Figure 1\nThe DNS is hijacked by dnsmasq deployed in the virtual machine, which allows it to correctly obtain the Cluster IP of the Istio service (Kubernetes’ built-in DNS). Access to Kubernetes’ built-in DNS service (which is exposed outside the cluster via the Internal Load Balancer and can be accessed directly). Return the Cluster IP resolved by productpage.bookinfo.svc.cluster.local, noting that the IP address is not directly accessible, but failure to be DNS resolved will result in a failed VM request for the service. The virtual machine’s call to services in a mesh is hijacked by the sidecar proxy. Since the proxy is connected to the Istio control plane, the endpoints of the service can be queried via xDS, so traffic will be forwarded to one of the endpoints. To access VM services in mesh, you need to manually add VM services to mesh using the istioctl register command, which essentially registers the VM services to the …","relpermalink":"/en/blog/istio-18-a-virtual-machine-integration-odyssey/","summary":"In this article, I’ll give you an overview of Istio‘s history of virtual machine integration support. In particular, the introduction of the smart DNS proxy and WorkloadGroup in Istio 1.8, which makes virtual machines and containers equivalent at the resource abstraction level.","title":"Istio 1.8: A Virtual Machine Integration Odyssey"},{"content":"A service mesh is a relatively simple concept, consisting of a bunch of network proxies paired with each service in an application, plus a set of task management processes. The proxies are called the data plane and the management processes are called the control plane in the Service Mesh. The data plane intercepts calls between different services and “processes” them; the control plane is the brain of the mesh that coordinates the behavior of proxies and provides APIs for operations and maintenance personnel to manipulate and observe the entire network.\nThe diagram below shows the architecture of a service mesh.\nService Mesh Architecture Further, the service mesh is a dedicated infrastructure layer designed to enable reliable, fast, and secure inter-service invocation in microservices architectures. It is not a mesh of “services” but rather a mesh of “proxies” that services can plug into, thus abstracting the network from the application code. In a typical service mesh, these proxies are injected into each service deployment as a sidecar (and also may be deployed at the edge of the mesh). Instead of invoking services directly over the network, services invoke their local sidecar proxy, which in turn manages requests on behalf of the service, pushing the complexities of inter-service communications into a networking layer that can resolve them at scale. The set of interconnected sidecar proxies implements a so-called data plane, while on the other hand the service mesh control plane is used to configure proxies. The infrastructure introduced by a service mesh provides an opportunity, too, to collect metrics about the traffic that is flowing through the application.\nThe architecture of a service mesh The infrastructure layer of a service mesh is divided into two main parts: the control plane and the data plane.\nCharacteristics of the control plane\nDo not parse packets directly. Communicates with proxies in the control plane to issue policies and configurations. Visualizes network behavior. Typically provides APIs or command-line tools for configuration versioning and management for continuous integration and deployment. Characteristics of the data plane\nIs usually designed with the goal of statelessness (though in practice some data needs to be cached to improve traffic forwarding performance). Directly handles inbound and outbound packets, forwarding, routing, health checking, load balancing, authentication, authentication, generating monitoring data, etc. Is transparent to the application, i.e., can be deployed senselessly. Changes brought by the service mesh Decoupling of microservice governance from business logic\nA service mesh takes most of the capabilities in the SDK out of the application, disassembles them into separate processes, and deploys them in a sidecar model. By separating service communication and related control functions from the business process and synching them to the infrastructure layer, a service mesh mostly decouples them from the business logic, allowing application developers to focus more on the business itself.\nNote that the word “mostly” is mentioned here and that the SDK often needs to retain protocol coding and decoding logic, or even a lightweight SDK to implement fine-grained governance and monitoring policies in some scenarios. For example, to implement method-level call distributed tracing, the service mesh requires the business application to implement trace ID passing, and this part of the implementation logic can also be implemented through a lightweight SDK. Therefore, the service mesh is not zero-intrusive from a code level.\nUnified governance of heterogeneous environments\nWith the development of new technologies and staff turnover, there are often applications and services in different languages and frameworks in the same company, and in order to control these services uniformly, the previous practice was to develop a complete set of SDKs for each language and framework, which is very costly to maintain. With a service mesh, multilingual support is much easier by synching the main service governance capabilities to the infrastructure. By providing a very lightweight SDK, and in many cases, not even a separate SDK, it is easy to achieve unified traffic control and monitoring requirements for multiple languages and protocols.\nFeatures of service mesh Service mesh also has three major technical advantages over traditional microservice frameworks.\nObservability\nBecause the service mesh is a dedicated infrastructure layer through which all inter-service communication passes, it is uniquely positioned in the technology stack to provide uniform telemetry at the service invocation level. This means that all services are monitored as “black boxes.” The service mesh captures route data such as source, destination, protocol, URL, status codes, latency, duration, etc. This is essentially the same data that web server logs can provide, but the service mesh captures this data for …","relpermalink":"/en/blog/what-is-a-service-mesh/","summary":"This article will take you through what a service mesh is, as well as its architecture, features, and advantages and disadvantages.","title":"What is a service mesh?"},{"content":"1.8 is the last version of Istio to be released in 2020 and it has the following major updates:\nSupports installation and upgrades using Helm 3. Mixer was officially removed. Added Istio DNS proxy to transparently intercept DNS queries from applications. WorkloadGroup has been added to simplify the integration of virtual machines. WorkloadGroup is a new API object. It is intended to be used with non-Kubernetes workloads like Virtual Machines and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies.\nInstallation and Upgrades Istio starts to officially support the use of Helm v3 for installations and upgrades. In previous versions, the installation was done with the istioctl command-line tool or Operator. With version 1.8, Istio supports in-place and canary upgrades with Helm.\nEnhancing Istio’s Usability The istioctl command-line tool has a new bug reporting feature (istioctl bug-report ), which can be used to collect debugging information and get cluster status.\nThe way to install the add-on has changed: 1.7 istioctl is no longer recommended and has been removed in 1.8, to help solve the problem of add-on lagging upstream and to make it easier to maintain.\nTetrate is an enterprise service mesh company. Our flagship product, TSB, enables customers to bridge their workloads across bare metal, VMs, K8s, \u0026amp; cloud at the application layer and provide a resilient, feature-rich service mesh fabric powered by Istio, Envoy, and Apache SkyWalking.\nMixer, the Istio component that had been responsible for policy controls and telemetry collection, has been removed. Its functionalities are now being served by the Envoy proxies. For extensibility, service mesh experts recommend using WebAssembly (Wasm) to extend Envoy; and you can also try the GetEnvoy Toolkit , which makes it easier for developers to create Wasm extensions for Envoy. If you still want to use Mixer, you must use version 1.7 or older. Mixer continued receiving bug fixes and security fixes until Istio 1.7. Many features supported by Mixer have alternatives as specified in the Mixer Deprecation document, including the in-proxy extensions based on the Wasm sandbox API.\nSupport for Virtual Machines Istio’s recent upgrades have steadily focused on making virtual machines first-class citizens in the mesh. Istio 1.7 made progress to support virtual machines and Istio 1.8 adds a smart DNS proxy , which is an Istio sidecar agent written in Go. The Istio agent on the sidecar will come with a cache that is dynamically programmed by Istiod DNS Proxy. DNS queries from applications are transparently intercepted and served by an Istio proxy in a pod or VM that intelligently responds to DNS query requests, enabling seamless multicluster access from virtual machines to the service mesh.\nIstio 1.8 adds a WorkloadGroup , which describes a collection of workload instances. It provides a specification that the workload instances can use to bootstrap their proxies, including the metadata and identity. It is only intended to be used with non-k8s workloads like Virtual Machines, and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies. Using WorkloadGroups, Istio has started to help automate VM registration with istioctl experimental workload group .\nTetrate , the enterprise service mesh company, uses these VM features extensively in customers’ multicluster deployments, to enable sidecars to resolve DNS for hosts exposed at ingress gateways of all the clusters in a mesh; and to access them over mutual TLS.\nConclusion All in all, the Istio team has kept the promise made at the beginning of the year to maintain a regular release cadence of one release every three months since the 1.1 release in 2018, with continuous optimizations in performance and user experience for a seamless experience of brownfield and greenfield apps on Istio. We look forward to more progress from Istio in 2021.\n","relpermalink":"/en/blog/istio-1-8-a-smart-dns-proxy-takes-support-for-virtual-machines-a-step-further/","summary":"WorkloadGroup is a new API object. It is intended to be used with non-Kubernetes workloads like Virtual Machines and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies.","title":"Istio 1.8: A Smart DNS Proxy Takes Support for Virtual Machines a Step Further"},{"content":"Istio is a popular service mesh to connect, secure, control, and observe services. When it was first introduced as open source in 2017, Kubernetes was winning the container orchestration battle and Istio answered the needs of organizations moving to microservices. Although Istio claims to support heterogeneous environments such as Nomad, Consul, Eureka, Cloud Foundry, Mesos, etc., in reality, it has always worked best with Kubernetes — on which its service discovery is based.\nIstio was criticized for a number of issues early in its development, for the large number of components, the complexity of installation and maintenance, the difficulty of debugging, a steep learning curve due to the introduction of too many new concepts and objects (up to 50 CRDs), and the impact of Mixer components on performance. But these issues are gradually being overcome by the Istio team. As you can see from the roadmap released in early 2020, Istio has come a long way.\nBetter integration of VM-based workloads into the mesh is a major focus for the Istio team this year. Tetrate also offers seamless multicloud connectivity, security, and observability, including for VMs, via its product Tetrate Service Bridge . This article will take you through why Istio needs to integrate with virtual machines and how you can do so.\nWhy Should Istio Support Virtual Machines? Although containers and Kubernetes are now widely used, there are still many services deployed on virtual machines and APIs outside of the Kubernetes cluster that needs to be managed by Istio mesh. It’s a huge challenge to unify the management of the brownfield environment with the greenfield.\nWhat Is Needed to Add VMs to the Mesh? Before the “how,” I’ll describe what is needed to add virtual machines to the mesh. There are a couple of things that Istio must know when supporting virtual machine traffic: which VMs have services that should be part of the mesh, and how to reach the VMs. Each VM also needs an identity, in order to communicate securely with the rest of the mesh. These requirements could work with Kubernetes CRDs, as well as a full-blown Service Registry like Consul. And the service account based identity bootstrapping could work as a mechanism for assigning workload identities to VMs that do not have a platform identity. For VMs that do have a platform identity (like EC2, GCP, Azure, etc.), work is underway in Istio to exchange the platform identity with a Kubernetes identity for ease of setting up mTLS communication.\nHow Does Istio Support Virtual Machines? Istio’s support for virtual machines starts with its service registry mechanism. The information about services and instances in the Istio mesh comes from Istio’s service registries, which up to this point have only looked at or tracked pods. In newer versions, Istio now has resource types to track and watch VMs. The sidecars inside the mesh cannot observe and control traffic to services outside the mesh, because they do not have any information about them.\nThe Istio community and Tetrate have done a lot of work on Istio’s support for virtual machines. The 1.6 release included the addition of WorkloadEntry, which allows you to describe a VM exactly as you would a host running in Kubernetes. In 1.7, the release started to add the foundations for bootstrapping VMs into the mesh automatically through tokens, with Istio doing the heavy lifting. Istio 1.8 will debut another abstraction called WorkloadGroup, which is similar to a Kubernetes Deployment object — but for VMs.\nThe following diagram shows how Istio models services in the mesh. The predominant source of information comes from a platform service registry like Kubernetes, or a system like Consul. In addition, the ServiceEntry serves as a user-defined service registry, modeling services on VMs or external services outside the organization.\nWhy install Istio in a virtual machine when you can just use ServiceEntry to bring in the services in the VMs?\nUsing ServiceEntry, you can enable services inside the mesh to discover and access external services; and in addition, manage the traffic to those external services. In conjunction with VirtualService, you can also configure access rules for the corresponding external service — such as request timeouts, fault injection, etc. — to enable controlled access to the specified external service.\nEven so, it only controls the traffic on the client-side, not access to the introduced external service to other services. That is, it cannot control the behavior of the service as the call initiator. Deploying sidecars in a virtual machine and introducing the virtual machine workload via workload selector allows the virtual machine to be managed indiscriminately, like a pod in Kubernetes.\nFuture As you can see from the bookinfo demo , there is too much manual work involved in the process and it’s easy to go wrong. In the future, Istio will improve VM testing to be realistic, automate bootstrapping based on platform identity, …","relpermalink":"/en/blog/how-to-integrate-virtual-machines-into-istio-service-mesh/","summary":"Better integration of virtual machine-based workloads into the service mesh is a major focus for the Istio team this year, and Tetrate also provides seamless multi-cloud connectivity, security and observability, including for virtual machines, through its product Tetrate Service Bridge. This article will show you why Istio needs to integrate with virtual machines and how.","title":"How to Integrate Virtual Machines into Istio Service Mesh"},{"content":"Today is my 914th day and also the last day with Ant Group , tomorrow is September 1st, which is usually the day school starts, and everyone at Alibaba is known as “classmate”, tomorrow I will join Tetrate , and that’s kind of starting my new semester!\nAnt/Alibaba and the Cloud Native Community To date, Ant/Alibaba Group has had a profound impact on my career, especially its corporate culture and values, and the Alibaba recruiting philosophy of “finding like-minded people”, and isn’t the process of creating the Cloud Native Community also a process of finding like-minded people? Cloud Native Community is like a small society, I don’t want it to have much social value, but only want it to make a small but beautiful change to individuals, to enterprises and to society. I constantly think about myself as an individual and as an employee, especially as an initiator of the community. What is my mission as an individual, an employee, and especially as an initiator of a community? What role should I play in the company? Where is this community going? I’m fumbling along, but because of your support, it makes me stronger and more committed to the adoption and application of cloud native technology in China, outside of me I may have gone faster, but now with the community together we will go further!\n24 June 2019, Shanghai, KubeCon China 2019 June 24, 2019, Shanghai, KubeCon China 2019\nJoining Tetrate Over the past two years, I’ve been working hard to promote Istio and Service Mesh technology, and with funding from Ant Group, I started the ServiceMesher Community to bring Service Mesh technology to China. Next I want to bring Chinese practice to the world.\nAs a Developer Advocate, the most important thing is not to stop learning, but to listen and take stock. Over the past two years, I’ve seen a lot of people show interest in Service Mesh, but not enough to understand the risks and lack of knowledge about the new technology. I’m excited to join this Service Mesh-focused startup Tetrate , a global telecommuting startup with products built around open source Istio , [Envoy](https:/ /envoyproxy.io) and Apache SkyWalking , it aims to make it to be the cloud native network infrastructure. Here are several maintainers of these open source projects, such as Sheng Wu , Zack Butcher , Lizan Zhou , etc., and I believe that working with them can help you understand and apply Service Mesh quickly and effectively across cloud native.\nMore Earlier this year as I was preparing for the Cloud Native community, I set the course for the next three years - cloud native, open source and community. The road to pursue my dream is full of thorns, not only need courage and perseverance, but also need you to be my strong backing, I will overcome the thorns and move forward. Open source belongs to the world, to let the world understand us better, we must be more active into the world. I hope that China’s open source tomorrow will be better, I hope that Service Mesh technology will be better applied by the enterprises in China, I hope that cloud native can benefit the public, and I hope that we can all find our own mission.\nWe are hiring now, if you are interested with Tetrate , please send your resume to careers@tetrate.io .\n","relpermalink":"/en/blog/moving-on-from-ant-group/","summary":"Today is my last day at Ant and tomorrow I'm starting a new career at Tetrate.","title":"New Beginning - Goodbye Ant, Hello Tetrate"},{"content":"Just tonight, the jimmysong.io website was moved to the Alibaba Cloud Hong Kong node. This is to further optimize the user experience and increase access speed. I purchased an ECS on the Alibaba Cloud Hong Kong node, and now I have a public IP and can set subdomains. The website was previously deployed on GitHub Pages, the access speed is average, and it has to withstand GitHub instability. Impact (In recent years, GitHub downtime has occurred).\nMeanwhile, the blog has also done a lot to improve the site, thanks to Bai Jun away @baijunyao strong support, a lot of work for the revision of the site, including:\nChanged the theme color scheme and deepened the contrast Use aligolia to support full site search Optimized mobile display Articles in the blog have added zoom function Added table of contents to blog post This site is built on the theme of educenter .\nThanks to the majority of netizens who have supported this website for several years. The website has been in use for more than three years and has millions of visits. It has undergone two major revisions before and after, on January 31, 2020 and October 8, 2017, respectively. And changed the theme of the website. In the future, I will share more cloud-native content with you as always, welcome to collect, forward, and join the cloud-native community to communicate with the majority of cloud-native developers.\n","relpermalink":"/en/notice/migrating-to-alibaba-cloud/","summary":"Move the website to the Alibaba Cloud Hong Kong node to increase the speed of website access and the convenience of obtaining public IP and subdomain names.","title":"Move to Alibaba Cloud Hong Kong node"},{"content":" Just the other day, Java just celebrated its 25th birthday , and from the time of its birth it was called “write once, run everywhere”, but more than 20 years later, there is still a deep gap between programming and actual production delivery. the world of IT is never short of concepts, and if a concept doesn’t solve the problem, then it’s time for another layer of concepts. it’s been 6 years since Kubernetes was born, and it’s time for the post-Kubernetes era - the era of cloud-native applications!\nCloud Native Stage This white paper will take you on a journey to explore the development path of cloud-native applications in the post-Kubernetes era.\nHighlights of the ideas conveyed include.\nCloud-native has passed through a savage growth period and is moving towards uniform application of standards. Kubernetes’ native language does not fully describe the cloud-native application architecture, and the development and operation functions are heavily coupled in the configuration of resources. Operator’s expansion of the Kubernetes ecosystem has led to the fragmentation of cloud-native applications, and there is an urgent need for a unified application definition standard. The essence of OAM is to separate the R\u0026amp;D and O\u0026amp;M concerns in the definition of cloud-native applications, and to further abstract resource objects, simplify and encompass everything. “Kubernetes Next Generation” refers to the fact that after Kubernetes became the infrastructure layer standard, the focus of the cloud-native ecology is being overtaken by the application layer, and the last two years have been a powerful exploration of the hot Service Mesh process, and the era of cloud-native application architecture based on Kubernetes is coming. Kubernetes has become an established operating platform for cloud-native applications, and this white paper will expand with Kubernetes as the default platform, including an explanation of the OAM-based hierarchical model for cloud-native applications.\n","relpermalink":"/en/notice/guide-to-cloud-native-app/","summary":"Take you on a journey through the post-Kubernetes era of cloud-native applications.","title":"Guide to Cloud Native Application"},{"content":"At the beginning of 2020, due to the outbreak of the Crona-19 pandemic, employees around the world began to work at home. Though the distance between people grew farer, there was a group of people, who were us working in the cloud native area, gathered together for a common vision. During the past three months, we have set up the community management committee and used our spare time working together to complete the preparatory work for the community. Today we are here to announce the establishment of the Cloud Native Community.\nBackground Software is eating the world. —— Marc Andreessen\nThis sentence has been quoted countless times, and with the rise of Cloud Native, we’d like to talk about “Cloud Native is eating the software.” As more and more enterprises migrate their services to the cloud, the original development mode of enterprises cannot adapt to the application scenarios in the cloud, and it is being reshaped to conform to the cloud native standard.\nSo what is cloud native? Cloud native is a collection of best practices in architecture, r\u0026amp;d process and team culture to support faster innovation, superior user experience, stable and reliable user service and efficient r\u0026amp;d. The relationship between the open source community and the cloud native is inseparable. It is the existence of the open source community, especially the end user community, that greatly promotes the continuous evolution of cloud native technologies represented by container, service mesh and microservices.\nCNCF (Cloud Native Computing Foundation) holds Cloud Native conference every year in the international community, which has a wide audience and great influence. But it was not held in China for the first time until 2018, after several successful international events. However, there are no independent foundations or neutral open source communities in China. In recent years, many cloud native enthusiasts in China have set up many communication groups and held many meetups, which are very popular. Many excellent open source projects have emerged in the cloud native field, but there is no organized neutral community for overall management. Under this background, the Cloud Native Community emerges at the right moment.\nAbout Cloud Native Community is an open source community with technology, temperature and passion. It was founded spontaneously by a group of industry elites who love open source and uphold the principle of consensus, co-governance, co-construction and sharing. The aim of the community is: connection, neutral, open source. We are based in China, facing the world, enterprise neutrality, focusing on open source, and giving feedback to open source.\nIntroduction for the Steering Community：https://cloudnative.to/en/team/ .\nYou will gain the followings after joining the community:\nKnowledge and news closer to the source A more valuable network More professional and characteristic consultation Opportunities to get closer to opinion leaders Faster and more efficient personal growth More knowledge sharing and exposure opportunities More industry talent to be found Contact Contact with us.\nEmail: mailto:contact@cloudnative.to Twitter: https://twitter.com/CloudNativeTo ","relpermalink":"/en/notice/cloud-native-community-announecement/","summary":"Today the Community Steering Committee announced the official formation of the Cloud Native Community.","title":"Establishment of the Cloud Native Community"},{"content":"This article is a rework of previously written content and is included in the Istio Handbook of the ServiceMesher community . Other chapters are still being compiled.\nPeople who have just heard of Service Mesh and tried Istio may have the following questions:\nWhy does Istio bind Kubernetes? What roles do Kubernetes and Service Mesh play in cloud native? What aspects of Kubernetes has Istio extended? What problems have been solved? What is the relationship between Kubernetes, xDS protocols (Envoy , MOSN, etc) and Istio? Should I use Service Mesh? In this section, we will try to guide you through the internal connections between Kubernetes, the xDS protocol, and Istio Service Mesh. In addition, this section will also introduce the load balancing methods in Kubernetes, the significance of the xDS protocol for Service Mesh, and why Istio is needed in time for Kubernetes.\nUsing Service Mesh is not to say that it will break with Kubernetes, but that it will happen naturally. The essence of Kubernetes is to perform application lifecycle management through declarative configuration, while the essence of Service Mesh is to provide traffic and security management and observability between applications. If you have built a stable microservice platform using Kubernetes, how do you set up load balancing and flow control for calls between services?\nThe xDS protocol created by Envoy is supported by many open source software, such as Istio , Linkerd , MOSN, etc. Envoy’s biggest contribution to Service Mesh or cloud native is the definition of xDS. Envoy is essentially a proxy. It is a modern version of proxy that can be configured through APIs. Based on it, many different usage scenarios are derived, such as API Gateway, Service Mesh. Sidecar proxy and Edge proxy in.\nThis section contains the following\nExplain the role of kube-proxy. Kubernetes’ limitations in microservice management. Describe the features of Istio Service Mesh. Describe what xDS includes. Compare some concepts in Kubernetes, Envoy and Istio Service Mesh. Key takeaways If you want to know everything in advance, here are some of the key points from this article:\nThe essence of Kubernetes is application lifecycle management, specifically deployment and management (scaling, scaling, automatic recovery, release). Kubernetes provides a scalable and highly resilient deployment and management platform for microservices. The foundation of Service Mesh is a transparent proxy. After the traffic between microservices is intercepted through sidecar proxy, the behavior of microservices is managed through the control plane configuration. Service Mesh decoupled from Kubernetes traffic management, the internal flow without the need of Service Mesh kube-proxy supporting components, micro-services closer to abstract the application layer by, for traffic between management services, security and observability. xDS defines the protocol standards for Service Mesh configuration. Service Mesh is a higher-level abstraction of services in Kubernetes. Its next step is serverless. Kubernetes vs Service Mesh The following figure shows the service access relationship between Kubernetes and Service Mesh (one sidecar per pod mode).\nkubernetes vs service mesh Traffic forwarding\nEach node of the cluster Kubernetes a deployed kube-proxy assembly Kubernetes API Server may communicate with the cluster acquired service information, and then set iptables rules, sends a request for a service directly to the corresponding Endpoint (belonging to the same group service pod).\nService discovery\nService registration in Service Mesh Istio Service Mesh can use the service in Kubernetes for service registration. It can also connect to other service discovery systems through the platform adapter of the control plane, and then generate the configuration of the data plane (using CRD statements, stored in etcd), a transparent proxy for the data plane. (Transparent proxy) is deployed in the sidecar container in each application service pod. These proxy need to request the control plane to synchronize the proxy configuration. The reason why is a transparent proxy, because there is no application container fully aware agent, the process kube-proxy components like the need to block traffic, but kube-proxythat blocks traffic to Kubernetes node and sidecar proxy that blocks out of the Pod For more information, see Understanding Route Forwarding by the Envoy Sidecar Proxy in Istio Service Mesh .\nDisadvantages of Service Mesh\nBecause each node on Kubernetes many runs Pod, the original kube-proxyrouting forwarding placed in each pod, the distribution will lead to a lot of configuration, synchronization, and eventual consistency problems. In order to perform fine-grained traffic management, a series of new abstractions will be added, which will further increase the user’s learning costs. However, with the popularization of technology, this situation will gradually ease.\nAdvantages of Service Mesh\nkube-proxy The …","relpermalink":"/en/blog/service-mesh-the-microservices-in-post-kubernetes-era/","summary":"This article is a rework of previously written content and is included in the Istio Handbook of the ServiceMesher community . Other chapters are still being compiled.","title":"Service Mesh - the Microservices in Post Kubernetes Era"},{"content":"2020 is indeed a bad start. In less than a month, I hardly heard any good news:\nTrump assassinated Major General Sulaymani of Iran; this pneumonia outbreak in Wuhan; the news that my basketball icon Kobe died of a helicopter crash really shocked me, and the Lakers said goodbye on the 24th. This gave me another spiritual blow during the Spring Festival, which was originally lacking in interest.\n2020 is bound to be a year deeply remembered in all humankind. In the last few days of the first month of the year, I decided to revise the website. The first reason was that I could n’t go out during the extended Chinese New Year holiday, and it was boring at home. And many pictures on the website were saved on Weibo map beds. The picture bed is unstable, causing many photos to be irretrievable; coupled with a habit of organizing the website every long holiday (the last revision of the website was completed during the National Day holiday in 2018, completed at home for 7 days), so I decided to The website has been revised again and it has become what it is now.\nFeatures The website has the following features after this revision:\nReorganize the website content, the structure is more reasonable Support email subscription Images are stored on Github Responsive static website, card design, better user experience Everyone is welcome to enter your email address in the email input box at the bottom of the page. Once there is an important content update on this site, we will push you through the email as soon as possible.\n","relpermalink":"/en/notice/website-revision-notice/","summary":"In the last days of the first month of 2020, I decided to revamp the website.","title":"jimmysong.io website revision notice"},{"content":"A few days ago during the Mid-Autumn Festival, I translated Google’s Engineering Practices documentation , which is open source on Github. The original document Github address: https://github.com/google/eng-practices , the main content so far is summarized by Google How to conduct a Code Review guide, based on the title of the original Github repository, we will add more Google engineering practices in the future.\nGithub：https://github.com/rootsongjc/eng-practices Browse online: https://jimmysong.io/eng-practices The translation uses the same style and directory structure as the original document. In fact, if the original text has international requirements, it can be directly incorporated, but according to the translation suggestions submitted by several previous people, the author of this project does not recommend translation. The first is translation. The documents may be unmaintained, and the accuracy of the documents cannot be guaranteed.\nFor suggestions on Chinese integration, see:\nAdd Chinese translation #12 Add the link of Chinese version of code reviewer’s guide #8 ","relpermalink":"/en/notice/google-engineering-practices-zh/","summary":"Translated from Google's open source documentation on Github","title":"Chinese version of Google Engineering Practice Documents"},{"content":"The following paragraph is a release note from the Istio official blog https://istio.io/zh/blog/2019/announcing-1.1/ , which I translated.\nIstio was released at 4 a.m. Beijing time today and 1 p.m. Pacific time.\nSince the 1.0 release last July, we have done a lot to help people get Istio into production. We expected to release a lot of patches (six patches have been released so far!), But we are also working hard to add new features to the product.\nThe theme for version 1.1 is “Enterprise Ready”. We are happy to see more and more companies using Istio in production, but as some big companies join in, Istio also encounters some bottlenecks.\nThe main areas we focus on include performance and scalability. As people gradually put Istio into production and use larger clusters to run more services with higher capacity, there may be some scaling and performance issues. Sidecar takes up too much resources and adds too much latency. The control plane (especially Pilot) consumes excessive resources.\nWe put a lot of effort into making the data plane and control plane more efficient. In the 1.1 performance test, we observed that sidecars typically require 0.5 vCPU to process 1000 rps. A single Pilot instance can handle 1000 services (and 2000 pods) and consumes 1.5 vCPUs and 2GB of memory. Sidecar adds 5 milliseconds at the 50th percentile and 10 milliseconds at the 99th percentile (the execution strategy will increase latency).\nWe have also completed the work of namespace isolation. You can use the Kubernetes namespace to enforce control boundaries to ensure that teams do not interfere with each other.\nWe have also improved multi-cluster functionality and usability. We listened to the community and improved the default settings for flow control and policies. We introduced a new component called Galley. Galley validates YAML configuration, reducing the possibility of configuration errors. Galley is also used in multi-cluster setups-collecting service discovery information from each Kubernetes cluster. We also support other multi-cluster topologies, including single control planes and multiple synchronous control planes, without the need for flat network support.\nSee the release notes for more information and details .\nThere is more progress on this project. As we all know, Istio has many moving parts, and they take on too much work. To address this, we have recently established the Usability Working Group (available at any time). A lot happened in the community meeting (Thursday at 11 am) and in the working group. You can log in to discuss.istio.io with GitHub credentials to participate in the discussion!\nThanks to everyone who has contributed to Istio over the past few months-patching 1.0, adding features to 1.1, and extensive testing recently on 1.1. Special thanks to companies and users who work with us to install and upgrade to earlier versions to help us identify issues before they are released.\nFinally, go to the latest documentation and install version 1.1! Happy meshing!\nOfficial website The ServiceMesher community has been maintaining the Chinese page of the official Istio documentation since the 0.6 release of Istio . As of March 19, 2019, there have been 596 PR merges, and more than 310 documents have been maintained. Thank you for your efforts! Some documents may lag slightly behind the English version. The synchronization work is ongoing. For participation, please visit https://github.com/servicemesher/istio-official-translation. Istio official website has a language switch button on the right side of each page. You can always Switch between Chinese and English versions, you can also submit document modifications, report website bugs, etc.\nServiceMesher Community Website\nThe ServiceMesher community website http://www.servicemesher.com covers all technical articles in the Service Mesh field and releases the latest activities in a timely manner. It is your one-stop portal to learn about Service Mesh and participate in the community.\n","relpermalink":"/en/notice/istio-11/","summary":"Istio 1.1 was released at 4 am on March 20th, Beijing time. This version took 8 months! The ServiceMesher community also launched the Istio Chinese documentation.","title":"Istio 1.1 released"},{"content":"Istio handbook was originally an open source e-book I created (see https://jimmysong.io/istio-handbook ). It has been written for 8 months before donating to the ServiceMesher community. In order to further popularize Istio and Service Mesh technology, this book Donate to the community for co-authoring. The content of the original book was migrated to https://github.com/servicemesher/istio-handbook on March 10, 2019. The original book will no longer be updated.\nGitHub address: https://github.com/servicemesher/istio-handbook Reading address online: http://www.servicemesher.com/istio-handbook/ Conceptual picture of this book, cover photo of Shanghai Jing’an Temple at night , photo by Jimmy Song .\nThe publishing copyright of this book belongs to the blog post of Electronic Industry Press. Please do not print and distribute it without authorization.\nIstio is a service mesh framework jointly developed by Google, IBM, Lyft, etc., and began to enter the public vision in early 2017. As an important infrastructure layer that inherits Kubernetes and connects to the serverless architecture in the cloud-native era, Istio is of crucial importance important. The ServiceMesher community, as one of the earliest open source communities in China that is researching and promoting Service Mesh technology, decided to integrate community resources and co-author an open source e-book for readers.\nAbout this book This book originates from the rootsongjc / istio-handbook and the Istio knowledge map created by the ServiceMesher community .\nThis book is based on Istio 1.0+ and includes, but is not limited to , topics in the Istio Knowledge Graph .\nParticipate in the book Please refer to the writing guidelines of this book and join the Slack channel discussion after joining the ServiceMesher community .\n","relpermalink":"/en/notice/istio-handbook-by-servicemesher/","summary":"To further popularize Istio and Service Mesh technology, donate this book to the community for co-authoring.","title":"Donate Istio Handbook to the ServiceMesher community"},{"content":"Github: https://github.com/rootsongjc/cloud-native-sandbox Cloud Native Sandbox can help you setup a standalone Kubernetes and istio environment with Docker on you own laptop.\nThe sandbox integrated with the following components:\nKubernetes v1.10.3 Istio v1.0.4 Kubernetes dashboard v1.8.3 Differences with kubernetes-vagrant-centos-cluster As I have created the kubernetes-vagrant-centos-cluster to set up a Kubernetes cluster and istio service mesh with vagrantfile which consists of 1 master(also as node) and 3 nodes, but there is a big problem that it is so high weight and consume resources. So I made this light weight sandbox.\nFeatures\nNo VirtualBox or Vagrantfile required Light weight High speed, low drag Easy to operate Services\nAs the sandbox setup, you will get the following services.\nRecord with termtosvg .\nPrerequisite You only need a laptop with Docker Desktop installed and Kubernetes enabled .\nNote: Leave enough resources for Docker Desktop. At least 2 CPU, 4G memory.\nInstall To start the sandbox, you have to run the following steps.\nKubernetes dashboard(Optional) Install Kubernetes dashboard.\nkubectl apply -f install/dashbaord/ Get the dashboard token.\nkubectl -n kube-system describe secret default| awk \u0026#39;$1==\u0026#34;token:\u0026#34;{print $2}\u0026#39; Expose kubernetes-dashboard service.\nkubectl -n kube-system get pod -l k8s-app=kubernetes-dashboard -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39; Login to Kubernetes dashboard on http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login with the above token.\nIstio(Required) Install istio service mesh with the default add-ons.\n# Install istio kubectl apply -f install/istio/ To expose service grafana on http://localhost:3000 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 3000:3000 \u0026amp; To expose service prometheus on http://localhost:9090 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=prometheus -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 9090:9090 \u0026amp; To expose service jaeger on http://localhost:16686 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=jaeger -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 16686:16686 \u0026amp; To expose service servicegraph on http://localhost:8088/dotviz , http://localhost:8088/force/forcegraph.html .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=servicegraph -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 8088:8088 \u0026amp; Kiali Install kiali .\nkubectl -n istio-system apply -f install/kiali To expose service kiali on http://localhost:20001 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=kiali -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 20001:20001 \u0026amp; Bookinfo sample Deploy bookinfo sample .\n# Enable sidecar auto injection kubectl label namespace default istio-injection=enabled # Deploy bookinfo sample kubectl -n default apply -f sample/bookinfo Visit productpage on http://localhost/productpage .\nLet’s generate some loads.\nfor ((i=0;i\u0026lt;1000;i=i+1));do echo \u0026#34;Step-\u0026gt;$i\u0026#34;;curl http://localhost/productpage;done You can watch the service status through http://localhost:3000 .\nClient tools To operate the applications on Kubernetes, you should install the following tools.\nRequired\nkubectl - Deploy and manage applications on Kubernetes. istioctl - Istio configuration command line utility. Optional\nkubectx - Switch faster between clusters and namespaces in kubectl kube-ps1 - Kubernetes prompt info for bash and zsh ","relpermalink":"/en/blog/cloud-native-sandbox/","summary":"A standalone Kubernetes and Istio environment with Docker on you own laptop","title":"Cloud Native Sandbox"},{"content":"This video was recorded on the last day of 2018 and demonstrates the use of rootsongjc/kubernetes-vagrant-centos-cluster to automatically deploy a Kubernetes cluster and Istio Service Mesh.\nA few days ago I mentioned that kubernetes-vagrant-centos-cluster released v1.2.0 version to deploy a cloud-native experimental environment with one click. Someone in the Kubernetes and Service Mesh community asked me a long time ago to make a video to explain and demonstrate how to install Kubernetes and Istio Service Mesh, because I’m always busy, I’ve always made some time. Today, I will give a demo video, just don’t watch the video for a few minutes. In order to make this video, it took me half an hour to record, two hours to edit, and many years of shooting. , Editing, containers, virtual machines, Kubernetes, service grid experience. This is not so much a farewell as a new beginning.\nBecause the video was first posted on YouTube , it was explained in English (just a few supplementary instructions, it does n’t matter if you do n’t understand, just read the Chinese documentation on GitHub ).\nSkip to bilibli to watch . If you are interested in drone aerial photography, you can also take a look at Jimmy Song’s aerial photography works . Please support the coin or like, thank you.\nIf you have any questions, you can send a barrage or comment below the video.\nPS. Some people will ask why you chose to use bilibli, because there are no ads for watching videos on this platform, and most of them are uploaded by the Up master. Although the two-dimensional elements are mostly, the community atmosphere is still good.\nFor more exciting videos, visit Jimmy Song’s bibli homepage .\n","relpermalink":"/en/notice/cloud-native-kubernetes-service-mesh-local-demo-show/","summary":"This video was recorded by me on the last day of 2018 and demonstrates the use of rootsongjc/kubernetes-vagrant-centos-cluster to automatically deploy Kubernetes clusters and Istio Service Mesh.","title":"Kubernetes and Istio Service Mesh Cloud Native Local Video Demo Show"},{"content":"Updated at Mar 8, 2022\nThis article uses Istio’s official bookinfo sample to explain how Envoy performs routing forwarding after the traffic entering the Pod and forwarded to Envoy sidecar by iptables, detailing the inbound and outbound processing. For a detailed analysis of traffic interception, see Understanding Envoy Sidecar Proxy Injection and Traffic Interception in Istio Service Mesh .\nOverview of Sidecar Injection and Traffic Interception Steps Below is an overview of the steps from Sidecar injection, Pod startup to Sidecar proxy interception traffic and Envoy processing routing.\nKubernetes automatically injected through Admission Controller, or the user run istioctl command to manually inject sidecar container. Apply the YAML configuration deployment application. At this time, the service creation configuration file received by the Kubernetes API server already includes the Init container and the sidecar proxy. Before the sidecar proxy container and application container are started, the Init container started firstly. The Init container is used to set iptables (the default traffic interception method in Istio, and can also use BPF, IPVS, etc.) to Intercept traffic entering the pod to Envoy sidecar Proxy. All TCP traffic (Envoy currently only supports TCP traffic) will be Intercepted by sidecar, and traffic from other protocols will be requested as originally. Launch the Envoy sidecar proxy and application container in the Pod. Sidecar proxy and application container startup order issues\nStart the sidecar proxy and the application container. Which container is started first? Normally, Envoy Sidecar and the application container are all started up before receiving traffic requests. But we can’t predict which container will start first, so does the container startup order have an impact on Envoy hijacking traffic? The answer is yes, but it is divided into the following two situations.\nCase 1: The application container starts first, and the sidecar proxy is still not ready\nIn this case, the traffic is transferred to the 15001 port by iptables, and the port is not monitored in the Pod. The TCP link cannot be established and the request fails.\nCase 2: Sidecar starts first, the request arrives and the application is still not ready\nIn this case, the request will certainly fail. As for the step at which the failure begins, the reader is left to think.\nQuestion : If adding a readiness and living probe for the sidecar proxy and application container can solve the problem?\nTCP requests that are sent or received from the Pod will be hijacked by iptables. After the inbound traffic is hijacked, it is processed by the Inbound Handler and then forwarded to the application container for processing. The outbound traffic is hijacked by iptables and then forwarded to the Outbound Handler for processing. Upstream and Endpoint. Sidecar proxy requests Pilot to use the xDS protocol to synchronize Envoy configurations, including LDS, EDS, CDS, etc., but to ensure the order of updates, Envoy will use ADS to request configuration updates from Pilot directly. How Envoy handles route forwarding The following figure shows a productpageservice access request http://reviews.default.svc.cluster.local:9080/, when traffic enters reviews the internal services, reviews internal services Envoy Sidecar is how to do traffic blocked the route forward.\nIstio transparent traffic hijacking and traffic routing schematic Before the first step, productpage Envoy Sidecar Pod has been selected by EDS of a request to reviews a Pod service of its IP address, it sends a TCP connection request.\nThe Envoy configuration in the official website of Istio is to describe the process of Envoy doing traffic forwarding. The party considering the traffic of the downstream is to receive the request sent by the downstream. You need to request additional services, such as reviews service requests need Pod ratings service.\nreviews, there are three versions of the service, there is one instance of each version, three versions sidecar similar working steps, only to later reviews-v1-cb8655c75-b97zc Sidecar flow Pod forwarding this step will be described.\nUnderstanding the Inbound Handler The role of the inbound handler is to transfer the traffic from the downstream intercepted by iptables to localhost to establish a connection with the application container inside the Pod.\nLook reviews-v1-cb8655c75-b97zc at the Listener in the pod.\nRun istioctl pc listener reviews-v1-cb8655c75-b97zc to see what the Pod has a Listener.\nADDRESS PORT TYPE 172.33.3.3 9080 HTTP \u0026lt;--- Receives all inbound traffic on 9080 from listener 0.0.0.0_15006 10.254.0.1 443 TCP \u0026lt;--+ 10.254.4.253 80 TCP | 10.254.4.253 8080 TCP | 10.254.109.182 443 TCP | 10.254.22.50 15011 TCP | 10.254.22.50 853 TCP | 10.254.79.114 443 TCP | 10.254.143.179 15011 TCP | 10.254.0.2 53 TCP | Receives outbound non-HTTP traffic for relevant IP:PORT pair from listener 0.0.0.0_15001 10.254.22.50 443 TCP | 10.254.16.64 42422 TCP …","relpermalink":"/en/blog/understanding-how-envoy-sidecar-intercept-and-route-traffic-in-istio-service-mesh/","summary":"Details about Envoy sidecar with iptables rules.","title":"Understanding How Envoy Sidecar Intercept and Route Traffic in Istio Service Mesh"},{"content":"KubeCon \u0026amp; CloudNative in North America is the most worthy cloud-native event every year. This time it will be held in Seattle for four days, from December 10th to 13th, refer to the official website of the conference . This year, 8,000 people participated. You should notice that Kubernetes has become more and more low-level. Cloud-native application developers do not need to pay much attention to it. Major companies are publishing their own cloud-native technology stack layouts, including IBM, VMware, SAP, Startups around this ecosystem are still emerging. The PPT sharing address of the local conference: https://github.com/warmchang/KubeCon-North-America-2018 , thank you William Zhang for finishing and sharing the slides of this conference .\nSeattle scene Janet Kuo from Google describes the path to cloud-native technology adoption.\nThe same event of KubeCon \u0026amp; CloudNativeCon, the scene of the first EnvoyCon.\nAt KubeCon \u0026amp; CloudNativeCon Seattle, various directors, directors, directors, VPs, and Gartner analysts from IBM, Google, Mastercard, VMware, and Gartner are conducting live discussions on the topic of Scaling with Service Mesh and Istio. Why do we talk about Istio when we talk about Service Mesh? What is not suitable for Istio use case. . .\nThe PPT contains basic introduction, getting started, a total of more than 200 Deep Dive as well as practical application, we recommend you according to the General Assembly’s official website to choose topics of interest to look at the schedule, otherwise I might see, however.\nA little impression KubeCon \u0026amp; CloudNativeCon is held three times a year, Europe, China and North America. China is the first time this year, and it will be held in Shanghai in November. It is said that it will be held in June next year. Although everyone said that Kubernetes has become boring, the conference about Kubernetes There is still a lot of content, and the use of CRD to extend Kubernetes usage is increasing. Service Mesh has begun to become hot. As can be seen from the live pictures above, there are a large number of participants on the site and related topics are also increasing. It is known as a microservice in the post-Kubernetes era . This must be It will be an important development direction of cloud native after Kubernetes, and the ServiceMesher community pays close attention to it.\n","relpermalink":"/en/notice/kubecon-cloudnativecon-seattle-2018/","summary":"KubeCon \u0026 CloudNativeCon Seattle 2018 Data Sharing.","title":"Kubecon\u0026CloudNativeCon Seattle 2018"},{"content":"This is a postscript from the post- Kubernetes era. Just this evening I saw a post by Bilgin Ibryam Microservices in a Post-Kuberentes Era .\nOn April 9, 2017, the Kubernetes Handbook-Kubernetes Chinese Guide / Cloud Native Application Architecture Practice Manual was first submitted. In the past 16 months, 53 contributors participated, 1,088 commits, and a total of 23,9014 Chinese characters were written. At the same time , thousands of enthusiasts have gathered in the Kubernetes \u0026amp; Cloud Native combat group .\nIt has been more than 4 months since the previous version was released. During this period, Kubernetes and Prometheus graduated from CNCF respectively and have matured commercially. These two projects have basically taken shape and will not change much in the future. Kubernetes was originally developed for container orchestration. In order to solve the problem of microservice deployment, Kubernetes has gained popularity. The current microservices have gradually entered the post-Kubernetes era . Service Mesh and cloud native redefine microservices and distributed applications.\nWhen this version was released, the PDF size was 108M with a total of 239,014 Chinese characters. It is recommended to browse online , or clone the project and install the Gitbook command to compile it yourself.\nThis version has the following improvements:\nAdded Istio Service Mesh tutorial Increased use VirtualBox and Vagrant set up in a local cluster and distributed Kubernetes Istio Service Mesh Added cloud native programming language Ballerina and Pulumi introduced Added Quick Start Guide Added support for Kubernetes 1.11 Added enterprise-level service mesh adoption path guide Added SOFAMesh chapter Added vision for the cloud-native future Added CNCF charter and participation Added notes for Docker image repositories Added Envoy chapter Increased KCSP (Kubernetes certification service providers) and CKA (Certified Kubernetes administrator) instructions Updated some configuration files, YAML and reference links Updated CRI chapter Removed obsolete description Improved etcdctl command usage tutorial Fixed some typos Browse and download Browse online https://jimmysong.io/kubernetes-handbook To make it easy for everyone to download, I put a copy on Weiyun , which is available in PDF (108MB), MOBI (42MB), and EPUB (53MB). In this book, there are more practical tutorials. In order to better understand the principles of Kubernetes, I recommend studying ** In- depth analysis of Kubernetes by Zhang Lei, produced by Geek Time **.\nThank you Kubernetes for your support of this book. Thank you Contributors . In the months before this version was released, the ServiceMesher community was co-founded . As a force in the post-Kubernetes era , welcome to contact me to join the community and create cloud native New era .\nAt present , the WeChat group of the ServiceMesher community also has thousands of members. The Kubernete Handbook will continue, but Service Mesh is already a rising star. With Kubernetes in mind, welcome to join the ServiceMesher community and follow us. The public account of the community (also the one I manage).\n","relpermalink":"/en/notice/new-kubernetes-handbook-released-and-say-hello-to-post-kubernetes-era/","summary":"This is an obituary post-Kubernetes era. Kubernetes handbook by Jimmy Song v1.4 is released. The next focus of cloud native is Service Mesh!","title":"Kubernetes Handbook v1.4 is released"},{"content":"Today I am honored to announce that I have become a CNCF Ambassador . Here is my story with Cloud Native.\nOrigin The first time to attend the Cloud Native Computing Foundation is at the LC3 in Beijing 2017. I attended the meeting again this year, and in November of this year, CNCF will hold the KubeCon \u0026amp; CloudNativeCon for the first time in Shanghai, China. I’ll be there too.\nCloud Native Books My origins with the Cloud Native is originated from Kevin Hoffman’s book Cloud Native Go . I translated this book at the end of 2016. Since then, in China, the translation of the word Cloud Native has not been determined, we introduced it with 云原生 to China.\nAnd then I begin to write the kubernetes-handbook on GitHub. So far, it has more than 2000 stars. This book has written more than 200,000 Chinese characters, the first commit happened on April 14, 2017.\nSince the the book Cloud Native Go completed, the publisher recommended another Cloud Native book to me - Cloud Native Python by Manish Sethi.\nAnd the book Cloud Native Java by Josh Long and Kenny Bastani.\nIn March 2018, with the hope that Bring the world equal opportunities and Building a Financial Cloud Native Infrastructure, I joined the Ant Group .\nServiceMesher Community By the time of May 2018, I start to organize the ServiceMesher community.\nIn the last few months, we work with other open source communities in China, such as k8smeetup , Sharding-Sphere , Apache SkyWalking . Our community has grown to have 1,700 members and two round meetups in Hangzhou and Beijing till now.\nMore than 300 people participated in the scene and more than 20,000 people watched it live by IT大咖说 。\nFuture Here are some hopes of mine:\nOpen source culture become popular in China More and more people would like to be involved in open source projects Host one open source project into the CNCF A book related to Cloud Native or Service Mesh Strengthen cultural exchanges between China and the global Finally, welcome to China for traveling or share your topic with us on Cloud Native, and in the mean while we will share our experience on large scale web apps to the world. Hope to hear your voice!\n","relpermalink":"/en/blog/cloud-native-and-me-the-past-current-and-future/","summary":"Today I am honored to announce that I have become a CNCF Ambassador.","title":"Cloud Native and me - the past, current and future"},{"content":"Today, we are pleased to announce Istio 1.0 . It’s been over a year since the original 0.1 release. Since 0.1, Istio has grown rapidly with the help of a thriving community, contributors, and users. Many companies have successfully applied Istio to production today and have gained real value through the insight and control provided by Istio. We help large businesses and fast-growing startups such as eBay , Auto Trader UK , Descartes Labs , HP FitStation , Namely , PubNub and Trulia to connect, manage and protect their services from scratch with Istio. The release of this version as 1.0 recognizes that we have built a core set of features that users can rely on for their production.\nEcosystem Last year we saw a significant increase in the Istio ecosystem. Envoy continues its impressive growth and adds many features that are critical to a production-level service grid. Observability providers like Datadog , SolarWinds , Sysdig , Google Stackdriver, and Amazon CloudWatch have also written plugins to integrate Istio with their products. Tigera , Aporeto , Cilium and Styra have built extensions for our strategy implementation and network capabilities. Kiali built by Red Hat provides a good user experience for grid management and observability. Cloud Foundry is building the next-generation traffic routing stack for Istio, the recently announced Knative serverless project is doing the same, and Apigee has announced plans to use it in their API management solution. These are just a few of the projects that the community added last year.\nFeatures Since the 0.8 release, we have added some important new features, and more importantly, marked many existing features as Beta to indicate that they can be used in production. This is covered in more detail in the release notes , but it is worth mentioning:\nMultiple Kubernetes clusters can now be added to a single grid , enabling cross-cluster communication and consistent policy enforcement. Multi-cluster support is now Beta. The network API for fine-grained control of traffic through the grid is now Beta. Explicitly modeling ingress and egress issues with gateways allows operations personnel to control the network topology and meet access security requirements at the edge. Two-way TLS can now be launched incrementally without updating all clients of the service. This is a key feature that removes the barriers to deploying Istio on existing production. Mixer now supports developing out-of-process adapters . This will be the default way to extend Mixer in an upcoming release, which will make it easier to build adapters. Envoy now fully evaluates the authorization policies that control service access locally , improving their performance and reliability. Helm chart installation is now the recommended installation method, with a wealth of customization options to configure Istio to your needs. We put a lot of effort into performance, including continuous regression testing, large-scale environmental simulation, and target repair. We are very happy with the results and will share details in the coming weeks. Next step Although this is an important milestone for the project, much work remains to be done. When working with adopters, we’ve received a lot of important feedback about what to focus next. We’ve heard consistent topics about supporting hybrid clouds, installing modularity, richer network capabilities, and scalability for large-scale deployments. We have considered some feedback in the 1.0 release and we will continue to actively work on it in the coming months.\nQuick start If you are new to Istio and want to use it for deployment, we would love to hear from you. Check out our documentation , visit our chat forum or visit the mailing list . If you want to contribute more to the project, please join our community meeting and say hello.\nAt last The Istio team is grateful to everyone who contributed to the project. Without your help, it won’t have what it is today. Last year’s achievements were amazing, and we look forward to achieving even greater achievements with our community members in the future.\nThe ServiceMesher community is responsible for the translation and maintenance of Chinese content on Istio’s official website. At present, the Chinese content is not yet synchronized with the English content. You need to manually enter the URL to switch to Chinese ( https://istio.io/zh ). There is still a lot of work to do , Welcome everyone to join and participate.\n","relpermalink":"/en/notice/istio-v1-released/","summary":"Chinese documentation is released at the same time!","title":"Istio 1.0 is released"},{"content":" If there is a visual learning model and platform that provides infrastructure clusters for your operation, would you pay for it?\nTwo months ago, I met Jord in Kubernetes’s Slack channel, and later saw the link of MagicSandbox.io he (and possibly others) sent in the Facebook group of Taiwan Kubernetes User Group, and I clicked to apply for a trial Then, I received an email from Jord later, and he told me that he wanted to build a Kubernetes learning platform. That’s where the whole thing started, and then we had a couple of Zoom video chats for a long time.\nAbout MagicSandbox MagicSandbox is a startup company. Jord (Dutch) is also a serial entrepreneur. He has studied at Sichuan University in China for 4 years since he was 19, and then returned to Germany. He worked as a PM at Boston Consulting Group and now works at Entrepreneur. First (Europe’s top venture capital / enterprise incubator) is also located in Berlin, Germany. He met Mislav (Croatian). Mislav is a full-stack engineer and has several entrepreneurial experiences. They have similar odors, and they hit it off. Decided to be committed to the Internet education industry and create a world-class software engineer education platform. They want to start with Kubernetes, first provide Internet-based Kubernetes theory and practice teaching, and then expand the topic to ElasticSearch, GraphQL, and so on. topic.\nJord founded MagicSandbox in his home, and I became the face of MagicSandbox in China.\nNow we are going to release the MagicSandbox Alpha version. This version is an immature version and is provided for everyone to try for free. Positive feedback is also welcome.\nOfficial homepage: https://magicsandbox.com/ Chinese page: https://cn.magicsandbox.com/ (The content has not been finished yet, only the Chinese version homepage is currently provided) Follow us on Twitter: https://twitter.com/magicsandbox ","relpermalink":"/en/notice/magicsandbox-alpha-version-annoucement/","summary":"Online practical software engineering education platform.","title":"MagicSandbox Alpha released"},{"content":"Remember the cloud-native programming language I shared before finally appeared! Learn about Ballerina in one article! ? They are ready to attend the KubeCon \u0026amp; CloudNativeCon China Conference!\nKubeCon \u0026amp; CloudNativeCon China Conference will be held on November 14-15, 2018 (Wednesday, Thursday) in Shanghai . See: https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2018/ With Ballerina’s official authorization, I now need to help them find an “ambassador” in China, responsible for team guidance, Chinese and English translation, and familiarity with Cloud Native and microservices. It has influence in the industry and has no barriers to English communication.\nAmbassador duties\nTeam Leadership Responsible for Chinese and English translation of product declaration, PPT materials, etc. Help to arrange the booth The other party can provide\nConference tickets Travel expenses Accommodation during the conference Other compensation This is a photo of their team in front of their booth during the KubeCon \u0026amp; CloudNativeCon in Hagen in May this year.\nPS This is the most complete and best picture I have ever found of their team. (Photography needs to be strengthened)\nLet’s briefly introduce this startup called Ballerina. Their team is mainly from Sri Lanka. This is an island country next to India in the South Asian subcontinent. In ancient China, it was called “Lion Country” and rich in gemstones.\nThe capital of their country is Sri Lanka, which is pronounced in their own language: Si li jia ya wa de na pu la ke te\nIf you are interested, please contact me directly.\n","relpermalink":"/en/notice/a-ballerina-china-ambassador-required/","summary":"With official authorization from Ballerina, I now need to help them find an ambassador in China.","title":"Ballerina seeks Chinese ambassador"},{"content":"Envoy-Designed for cloud-native applications, open source edge and service proxy, Istio Service Mesh default data plane, Chinese version of the latest official document, dedicated by the ServiceMesher community, welcome everyone to learn and share together.\nTL;DR: http://www.servicemesher.com/envoy/ PDF download address: servicemesher/envoy This is the first time Community Service Mesh enthusiasts group activities, the document is based on Envoy latest (version 1.7) Official documents https://www.envoyproxy.io/docs/envoy/latest/ . A total of 120 articles with 26 participants took 13 days and 65,148 Chinese characters.\nVisit online address: http://www.servicemesher.com/envoy/ Note : This book does not include the v1 API reference and v2 API reference sections in the official documentation. Any links to API references in the book will jump directly to the official page.\nContributor See the contributor page: https://github.com/servicemesher/envoy/graphs/contributors Thanks to the above contributors for their efforts! Because the level of translators is limited, there are inevitably inadequacies in the text. I also ask readers to correct them. Also welcome more friends to join our GitHub organization: https://github.com/servicemesher ","relpermalink":"/en/notice/envoyproxy-docs-cn-17-release/","summary":"Translated by ServiceMesher community.","title":"Chinese version of the latest official document of Envoy released"},{"content":"Envoy is an open source L7 proxy and communication bus written in C ++ by Lyft. It is currently an open source project under CNCF . The code is hosted on GitHub. It is also the default data plane in the Istio service mesh. We found that it has very good performance, and there are also continuous open source projects based on Envoy, such as Ambassador , Gloo, etc. At present, the official documentation of Envoy has not been well finished, so we Service Service enthusiasts feel that they are launching the community The power of the co-translation of the latest (version 1.7) official documentation of Enovy and organization through GitHub.\nService Mesh enthusiasts have jointly translated the latest version of the official document of Envoy . The translated code is hosted at https://github.com/servicemesher/envoy . If you are also a Service Mesh enthusiast, you can join the SerivceMesher GitHub organization and participate together.\nThe official Envoy document excludes all articles in the two directories of the v1 API reference and the v2 API reference. There are more than 120 documents. The length of the documents varies. The original English official documents use the RST format. I manually converted them into Markdown format and compiled using Gitbook. A GitHub Issue was generated according to the path of the document relative to the home directory. Friends who want to participate in translation can contact me to join the ServiceMesher organization, and then select the article you want to translate in Issue , and then reply “Claim”.\nHere you can see all the contributors. In the future, we will also create a Service Mesh enthusiast website. The website uses static pages. All code will be hosted on Github. Welcome everyone to participate.\n","relpermalink":"/en/notice/enovy-doc-translation-start/","summary":"The SerivceMesher community is involved in translating the official documentation for the latest version of Envoy.","title":"Envoy's latest official document translation work started"},{"content":"TL; DR Click here to download the PDF of this book .\nRecently, Michael Hausenblas of Nginx released a booklet on container networks in docker and kubernetes. This 72-page material is a good introduction for everyone to understand the network in Docker and Kubernetes from shallow to deep.\nTarget audience Container Software Developer SRE Network Operation and Maintenance Engineer Architects who want to containerize traditional software ","relpermalink":"/en/notice/container-networking-from-docker-to-kubernetes-nginx/","summary":"Source from Nginx, published by O’Reilly.","title":"Docker container network book sharing"},{"content":"In 2017, we are facing a big era of architectural changes, such as Kubernetes ending the battle for container orchestration, Kafka release 1.0, serverless gradually gaining momentum, edge computing to replace cloud computing, Service Mesh ready to go, and artificial intelligence to empower business , Also brings new challenges to the architecture.\nI am about to participate in InfoQ’s ArchSummit Global Architects Summit on December 8-11 in Beijing . This conference also invited 100+ top technologists such as Dr. Ali Wangjian to share and summarize the architectural changes and reflections this year. I hope that you can build on this conference, summarize past practices, and look forward to a future-oriented architecture to transform this era of change into the common fortune of each of us.\nMy speech The content of my speech is from Kubernetes to Cloud Native-the road to cloud native applications . Link: From Kubernetes to Cloud Native-the road to cloud native applications . The time is Saturday, December 9, 9:30 am, in the fifth meeting room.\nAfter more than ten years of development of cloud computing, the new phase of cloud native has entered. Enterprise applications are preferentially deployed in cloud environments. How to adapt to the cloud native tide, use containers and Kubernetes to build cloud native platforms, and practice DevOps concepts and agility How IT, open source software, and the community can help IT transform, the solution to all these problems is the PaaS platform, which is self-evident to the enterprise.\nWe also prepared gifts for everyone: “Cloud Native Go-Building Cloud Native Web Applications Based on Go and React” and “Intelligent Data Era-Enterprise Big Data Strategy and Practice” There is also a book stand for the blog post of the Electronic Industry Press. Welcome to visit.\nArchSummit conference official website link: http://bj2017.archsummit.com/ For more details, please refer to the official website of the conference: http://bj2017.archsummit.com/ ","relpermalink":"/en/notice/archsummit-beijing-2017-from-kubernetes-to-cloud-native/","summary":"I will give a lecture at ArchSummit Beijing. From Kubernetes to Cloud Native, my path to cloud native applications.","title":"ArchSummit Beijing 2017 speech preview"},{"content":"Cloudinary-go is a Go client library and CLI tool to upload static assets to the Cloudinary service.\nInstallation Install the CLI tool and the library with:\ngo get github.com/rootsongjc/cloudinary-go/cloudinary Or download the release binary from release .\n","relpermalink":"/en/notice/cloudinary-go/","summary":"Cloudinary-go is a Go client library and CLI tool to upload static assets to Cloudinary service","title":"Cloudinary file upload tool written in Go released"},{"content":"Many people asked me how jimmysong.io made this website. I think it is necessary to write a book to popularize the knowledge of static website construction and Hugo as a tool.\nThis manual will guide you how to use Hugo to build a static website for personal blog or project display.\nTeach you how to build a static website from scratch. This does not require much programming and development experience and time investment, and basically does not require much cost (except for personalized domain names). You can quickly build and Launch a website.\nGithub address: https://github.com/rootsongjc/hugo-handbook Gitbook access address: https://jimmysong.io/hugo-handbook The content of this book will continue to improve over time and as my site improves, so stay tuned.\n","relpermalink":"/en/notice/building-static-website-with-hugo/","summary":"A manual for static website building, and a personal blog Gitbook using Hugo.","title":"Hugo Handbook is released"},{"content":"Kevin Hoffman(From Capital One, twitter @KevinHoffman ) was making a speech on TalkingData T11 Smart Data Summit.\nHe addressed that 15 Factors of Cloud Native which based on Heroku’s original Twelve-Factor App , but he add more 3 another factors on it.\nLet’s have a look at the 15 factors of Cloud Native.\n1. One codebase, one App Single version-controlled codebase, many deploys Multiple apps should not share code Microservices need separate release schedules Upgrade, deploy one without impacting others Tie build and deploy pipelines to single codebase 2. API first Service ecosystem requires a contract Public API Multiple teams on different schedulers Code to contract/API, not code dependencies Use well-documented contract standards Protobuf IDL, Swagger, Apiary, etc API First != REST first RPC can be more appropriate in some situations 3. Dependency Management Explicitly declare dependencies Include all dependencies with app release Create immutable build artifact (e.g. docker image) Rely on smallest docker image Base on scratch if possible App cannot rely on host for system tools or libraries 4. Design, Build, Release, Run Design part of iterative cycle Agile doesn’t mean random or undesigned Mature CI/CD pipeline and teams Design to production in days not months Build immutable artifacts Release automatically deploys to environment Environments contains config, not release artifact 5. Configuration, Credentials, Code “3 Cs” volatile substances that explode when combinded Password in a config file is as bad as password in code App must accept “3 Cs” from environment and only use harmless defaults Test - Could you expose code on Github and not reveal passwords, URLs, credentials? 6. Logs Emit formatted logs to stdout Code should not know about destination or purpose of log emissions Use downstream log aggregator collect, store, process, expose logs ELK, Splunk, Sumo, etc Use structured logs to allow query and analysis JSON, csv, KV, etc Logs are not metrics 7. Disposability App must start as quickly as possible App must stop quickly and gracefully Processes start and stop all the time in the cloud Every scale up/down disposes of processes Slow dispose == slow scale Slow dispose or startup can cause availability gaps 8. Backing Services Assume all resources supplied by backingservices Cannotassume mutable file system “Disk as a Service” (e.g. S3, virtual mounts, etc) Every backing service is bound resource URL, credentials, etc-\u0026gt; environment config Host does not satisfy NFRs Backing services and cloud infrastructure 9. Environment Parity “Works on my machine” Cloud-native anti-pattern. Must work everywhere Every commit is candidate for deployment Automated acceptance tests Provide no confidence if environments don’t match 10. Administrative Processes Database migrations Run-once scripts or jobs Avoid using for batch operations, consider instead: Event sourcing Schedulers Triggers from queues, etc Lambdas/functions 11. Port Binding In cloud, infrastructure determines port App must accept port assigned by platform Containers have internal/external ports App design must embrace this Never use reserved ports Beware of container “host mode” networking 12. Stateless Processes What is stateless? Long-term state handled by a backing service In-memory state lives onlyas long as request Requests from same client routed to different instances “Sticky sessions” cloud native anti-pattern 13. Concurency Scale horizontally using the process model Build disposable, stateless, share-nothing processes Avoid adding CPU/RAM to increase scale/throughput Where possible, let platform/libraries do threading Many single-threaded services \u0026gt; 1 multi-threaded monolith 14. Telemetry Monitor apps in the cloud like satellite in orbit No tether, no live debugger Application Perf Monitoring (APM) Domain Telemetry Health and system logs 15. Authentication \u0026amp; Authorization Security should never be an afterthought Auth should be explicit, documented decision Even if anonymous access is allowed Don’t allow anonymous access Bearer tokens/OAuth/OIDC best practices Audit all attempts to access Migrating Monoliths to the Cloud After this 15 factors, he also gave us some tips about how to migrate monoliths to the Cloud:\nMake a rule - stop adding to the monolith All new code must be cloud native Prioritize features Where will you get most benefit from cloud native? Come up with a plan Decompose monolith over time Fast, agile iterations toward ultimate goal Use multiple strategies and patterns Go - the Best Language for Building Cloud Native App At last, he advise us the programming language Go is the best language to build Cloud Native applications for these reasons below:\nLightweight Easily learning curve Compiles to native binaries Very fast Large, thriving, engaged community http://gopherize.me Kevin also wrote a book Cloud Native Go to show how to Building Web Applications and Microservices for the Cloud with Go and React. This book has been …","relpermalink":"/en/blog/high-level-cloud-native-from-kevin-hoffman/","summary":"Kevin Hoffman address that 15 Factors of Cloud Native.","title":"High Level Cloud Native From Kevin Hoffman"},{"content":"From now on I have my own independent domain name jimmysong.io , the website is still hosted on GitHub, the original URL https://jimmysong.io is still accessible.\nWhy use .io as the suffix? Because this is The First Step to Cloud Native!\nWhy choose today? Because today is August 18, the days are easy to remember.\nPS domain names are registered in namecheap and cost tens of dollars / year.\nProudly powered by hugo 🎉🎊🎉\n","relpermalink":"/en/notice/domain-name-jimmysong-io/","summary":"From now on I have my own independent domain name jimmysong.io.","title":"New domain name jimmysong.io"},{"content":"This is a list of software, tools, architecture and reference materials about Cloud Native. It is awesome-cloud-native , a project I opened on GitHub , and can also be browsed through the web page .\nIt is divided into the following areas:\nAwesome Cloud Native\nAI API gateway Big Data Container engine CI-CD Database Data Science Fault tolerant Logging Message broker Monitoring Networking Orchestration and scheduler Portability Proxy and load balancer RPC Security and audit Service broker Service mesh Service registry and discovery Serverless Storage Tracing Tools Tutorial This list will be continuously updated and improved in the future, not only for my usual research records, but also as a reference for the Cloud Native industry.\n","relpermalink":"/en/notice/awesome-cloud-native/","summary":"This is a list of software, tools, architecture, and reference materials about Cloud Native. It is a project I started on GitHub.","title":"Awesome Cloud Native list is released"},{"content":"This book is the Chinese version of Migrating to Cloud Native Application Architectures. The English version of this book was released in February 2015. The Chinese version was translated by Jimmy Song and published in July 2017.\nGitHub hosting address for this book: https://github.com/rootsongjc/migrating-to-cloud-native-application-architectures Gitbook reading address: https://jimmysong.io/migrating-to-cloud-native-application-architectures The application architectures discussed in this book include:\nTwelve-factor application: A collection of cloud-native application architecture patterns Microservices: independently deployed services, each service does one thing Self-service agile infrastructure: a platform that provides application environments and back-office services quickly, repeatably, and consistently API-based collaboration: published and versioned APIs that allow interactions between services in a cloud-native application architecture Pressure resistance: a system that becomes stronger under pressure ","relpermalink":"/en/notice/changes-needed-to-cloud-native-archtecture/","summary":"This book is translated from an eBook published by Matt Stine in February 2015.","title":"Migrating to Cloud Native Chinese version released"},{"content":"Istio 1.14 was released in June of this year, and one of the most notable features of this release is support for SPIRE , which is one of the implementations of SPIFFE , a CNCF incubation project. This article explains what SPIRE means for zero-trust architectures and why you would need SPIRE for authentication in Istio.\nAuthentication in Kubernetes We all know that Istio was built for and typically runs on Kubernetes, so before we talk about how to use SPIRE for authentication in Istio, let’s take a look at how Kubernetes handles authentication.\nLet’s look at an example of a pod’s token. Whenever a pod gets created in Kubernetes, it gets assigned a default service account from the namespace, assuming we didn’t explicitly assign a service account to it. Here is an example:\napiVersion: v1 data: ca.crt: {CA_CRT} namespace: ZGVmYXVsdA== token: {TOKEN_STRING} kind: Secret metadata: annotations: kubernetes.io/service-account.name: sleep kubernetes.io/service-account.uid: 2c0d00e8-13a2-48d0-9ff8-f987f3325ecf creationTimestamp: \u0026#34;2022-06-14T03:01:35Z\u0026#34; name: sleep-token-gwhwd namespace: default resourceVersion: \u0026#34;244535398\u0026#34; uid: b8822ceb-9553-4a17-96dc-d525bbaed0e0 type: kubernetes.io/service-account-token Kubernetes manages the identity of Pods with Service Accounts and then specifies the permissions of Pods with a Service Account to the Kubernetes API using RBAC. A service account’s token is stored in a secret, which does not include a declaration of the node or pod where the workload is running. When a malicious actor steals a token, they gain full access to the account and can steal information or carry out sabotage under the guise of that user.\nA token can only be used to identify a workload in one cluster, but Istio supports multiple clusters and meshes, as well as Kubernetes environments and virtual machines. A unified workload identity standard can help here.\nAn Introduction to SPIFFE and SPIRE SPIFFE’s (Secure Production Identity Framework for Everyone) goal is to create a zero-trust, fully-identified data center network by establishing an open, unified workload identity standard based on the concept of zero-trust. SPIRE can rotate X.509 SVID certificates and secret keys on a regular basis. Based on administrator-defined policies, SPIRE can dynamically provision workload certificates and Istio can consume them. I’ll go over some of the terms associated with SPIFFE in a little more detail below.\nSPIRE (SPIFFE Runtime Environment) is a SPIFFE implementation that is ready for production. SVID (SPIFFE Verifiable Identity Document) is the document that a workload uses to prove its identity to a resource or caller. SVID contains a SPIFFE ID that represents the service’s identity. It uses an X.509 certificate or a JWT token to encode the SPIFFE ID in a cryptographically verifiable document. The SPIFFE ID is a URI that looks like this: spiffe://trust_domain/workload_identifier.\nSPIFFE and Zero Trust Security The essence of Zero Trust is identity-centric dynamic access control. SPIFFE addresses the problem of identifying workloads.\nWe might identify a workload using an IP address and port in the era of virtual machines, but IP address-based identification is vulnerable to multiple services sharing an IP address, IP address forgery, and oversized access control lists. Because containers have a short lifecycle in the Kubernetes era, instead of an IP address, we rely on a pod or service name. However, different clouds and software platforms approach workload identity differently, and there are compatibility issues. This is especially true in heterogeneous hybrid clouds, where workloads run on both virtual machines and Kubernetes. It is critical to establish a fine-grained, interoperable identification system at this point.\nUsing SPIRE for Authentication in Istio With the introduction of SPIRE to Istio, we can give each workload a unique identity, which is used by workloads in the service mesh for peer authentication, request authentication, and authorization policies. The SPIRE Agent issues SVIDs for workloads by communicating with a shared UNIX Domain Socket in the workload. The Envoy proxy and the SPIRE agent communicate through the Envoy SDS (Secret Discovery Service) API. Whenever an Envoy proxy needs to access secrets (certificates, keys, or anything else needed to do secure communication), it will talk to the SPIRE agent through Envoy’s SDS API.\nThe most significant advantage of SDS is the ease with which certificates can be managed. Without this feature, certificates would have to be created as a secret and then mounted into the agent container in a Kubernetes deployment. The secret must be updated, and the proxy container must be re-deployed if the certificate expires. Using SDS, Istio can push the certificates to all Envoy instances in the service mesh. If the certificate expires, the server only needs to push the new certificate to the Envoy instance; Envoy will use the new certificate right away, and the …","relpermalink":"/en/blog/why-istio-need-spire/","summary":"This article explains what SPIRE means for zero-trust architectures and why you would need SPIRE for authentication in Istio.","title":"Why would you need SPIRE for authentication with Istio?"},{"content":" It’s been more than 5 years since Google, IBM, and Lyft unveiled the Istio open source project in May 2017. The Istio project has developed from a seed to a tall tree in these years. Many domestic books on the Istio service mesh were launched in the two years following the release of Istio 1.0 in 2018. My country is at the forefront of the world in the field of Istio book publishing.\nService mesh: one of the core technologies of cloud native Today, Istio is nearly synonymous with service mesh in China. The development of service mesh, as one of the core cloud-native technologies described by CNCF (Cloud Native Computing Foundation), has gone through the following stages.\n2017-2018: Exploratory Phase 2019-2020: Early Adopter Phase 2021 to present: Implementation on a large scale and ecological development stage Cloud native technology enables enterprises to design and deploy elastically scalable applications in new dynamic settings such as public, private, and hybrid clouds, according to the CNCF. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs are examples of cloud native technology.\nService mesh has been included to the CNCF definition of cloud native, indicating that it is one of the representative technologies of cloud native. Google is donating Istio to CNCF today, and we have reason to expect that as a CNCF project, Istio’s community will be more open, and its future development will be more smooth.\nService mesh and cloud native applications Cloud-native development is gaining traction. Despite the frequent emergence of new technologies and products, service mesh has maintained its place as “cloud-native network infrastructure” as part of the overall cloud-native technology stack throughout the past year. The cloud-native technology stack model is depicted in the diagram below, with representative technologies for each layer to define the standard. Service mesh and other cloud-native technologies complement each other as a new era of middleware emerges. Dapr (Distributed Application Runtime) defines the cloud-native middleware capability model, OAM defines the cloud-native application model, and so on, whereas service mesh Lattice defines a cloud-native seven-layer network model.\nCloud Native Application Model Why you need a service mesh Using a service mesh isn’t tantamount to abandoning Kubernetes; it just makes sense. The goal of Kubernetes is to manage application lifecycles through declarative configuration, whereas the goal of service mesh is to provide traffic control, security management, and observability amongst apps. How do you set up load balancing and flow management for calls between services after a robust microservice platform has been developed with Kubernetes?\nMany open source tools, including Istio, Linkerd, MOSN, and others, support Envoy’s xDS protocol. The specification of xDS is Envoy’s most significant contribution to service mesh or cloud native. Many various usage cases, such as API gateways, sidecar proxies in service meshes, and edge proxies, are derived from Envoy, which is simply a network proxy, a modern version of the proxy configured through the API.\nIn a nutshell, the move from Kubernetes to Istio was made for the following reasons.\nApplication life cycle management, specifically application deployment and management, is at the heart of Kubernetes (scaling, automatic recovery, and release). Kubernetes is a microservices deployment and management platform that is scalable and extremely elastic. Transparent proxy is the cornerstone of service mesh, which intercepts communication between microservices via sidecar proxy and then regulates microservice behavior via control plane settings. The deployment mode of service meshes has introduced new issues today. For service meshes, sidecar is no longer required, and an agentless service mesh based on gRPC is also being tested. xDS is a protocol standard for configuring service meshes, and a gRPC-based xDS is currently being developed. Kubernetes traffic management is decoupled with the service mesh. The kube-proxy component is not required to support traffic within the service mesh. The traffic between services is controlled by an abstraction close to the microservice application layer to achieve security and observability features. In Kubernetes, service mesh is an upper-level abstraction of service, and Serverless is the next stage, which is why Google released Knative based on Kubernetes and Istio following Istio. Open source in the name of the community The ServiceMesher community was founded in May 2018 with the help of Ant Financial. Following that, a tornado of service meshes erupted in China, and the community-led translation of Istio’s official documentation reached a fever pitch.\nI became aware of a dearth of Chinese resources for systematically teaching Istio over time, so in September 2018, I began to plan and create an Istio book, launching the Istio Handbook open source …","relpermalink":"/en/blog/istio-service-mesh-book/","summary":"By the Cloud Native Community(China)","title":"In-Depth Understanding of Istio: Announcing the Publication of a New Istio Book"},{"content":"This article will guide you on how to compile the Istio binaries and Docker images on macOS.\nBefore you begin Before we start, refer to the Istio Wiki , here is the information about my build environment.\nmacOS 12.3.1 Darwin AMD64 Docker Desktop 4.8.1(78998) Docker Engine v20.10.14 Start to compile First, download the Istio code from GitHub to the $GOPATH/src/istio.io/istio directory, and execute the commands below in that root directory.\nCompile into binaries Execute the following command to download the Istio dependent packages, which will be downloaded to the vendor directory.\ngo mod vendor Run the following command to build Istio:\nsudo make build If you do not run the command with sudo, you may encounter the following error.\nfatal: unsafe repository (\u0026#39;/work\u0026#39; is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work fatal: unsafe repository (\u0026#39;/work\u0026#39; is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work Makefile.core.mk:170: *** \u0026#34;TAG cannot be empty\u0026#34;. Stop. make: *** [build] Error 2 Even if you follow the prompts and run git config --global --add safe.directory /work, you will still get errors during compilation.\nThe compiled binary will be saved in out directory with the following directory structure.\nout ├── darwin_amd64 │ ├── bug-report │ ├── client │ ├── envoy │ ├── extauthz │ ├── install-cni │ ├── istio-cni │ ├── istio-cni-taint │ ├── istio-iptables │ ├── istio_is_init │ ├── istioctl │ ├── logs │ ├── operator │ ├── pilot-agent │ ├── pilot-discovery │ ├── release │ └── server └── linux_amd64 ├── envoy ├── envoy-centos ├── logs └── release It will build both the linux_amd64 and darwin_amd64 architectures binaries at the same time.\nCompile into Docker images Run the following command to compile Istio into a Docker image.\nsudo make build The compilation will take about 3 to 5 minutes depending on your network. Once the compilation is complete, you will see the Docker image of Istio by running the following command.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE localhost:5000/app_sidecar_centos_7 latest 2044037df94b 51 seconds ago 524MB localhost:5000/app_sidecar_ubuntu_jammy latest 5d8ae5ed55b7 About a minute ago 362MB localhost:5000/proxyv2 latest d4679412385f About a minute ago 243MB localhost:5000/install-cni latest 78f46d5771d2 About a minute ago 270MB localhost:5000/istioctl latest c38130a5adc8 About a minute ago 190MB localhost:5000/pilot latest 2aa9185ec202 About a minute ago 190MB localhost:5000/app latest 473adafaeb8d About a minute ago 188MB localhost:5000/operator latest 9ac1fedcdd12 About a minute ago 191MB localhost:5000/ext-authz latest 1fb5aaf20791 About a minute ago 117MB localhost:5000/app_sidecar_debian_11 latest 61376a02b95d 2 minutes ago 407MB localhost:5000/app_sidecar_ubuntu_xenial latest 7e8efe666611 2 minutes ago 418MB You can change the image name and push it into your own container registry.\nSummary This is how to build Istio on macOS. If you have already downloaded the Docker image you need to build, the build will take less than a minute. It also takes only a few minutes to build Docker images.\nReference Using the Code Base - github.com ","relpermalink":"/en/blog/how-to-build-istio/","summary":"This article will guide you on how to compile the Istio binaries on macOS.","title":"How to build Istio?"},{"content":"Updated on May 6, 2022\nBased on Istio version 1.13, this article will present the following.\nWhat is the sidecar pattern and what advantages does it have? How are the sidecar injections done in Istio? How does the sidecar proxy do transparent traffic hijacking? How is the traffic routed to upstream? The figure below shows how the productpage service requests access to http://reviews.default.svc.cluster.local:9080/ and how the sidecar proxy inside the reviews service does traffic blocking and routing forwarding when traffic goes inside the reviews service.\nIstio transparent traffic hijacking and traffic routing diagram At the beginning of the first step, the sidecar in the productpage pod has selected a pod of the reviews service to be requested via EDS, knows its IP address, and sends a TCP connection request.\nThere are three versions of the reviews service, each with an instance, and the sidecar work steps in the three versions are similar, as illustrated below only by the sidecar traffic forwarding step in one of the Pods.\nSidecar pattern Dividing the functionality of an application into separate processes running in the same minimal scheduling unit (e.g. Pod in Kubernetes) can be considered sidecar mode. As shown in the figure below, the sidecar pattern allows you to add more features next to your application without additional third-party component configuration or modifications to the application code.\nSidecar pattern The Sidecar application is loosely coupled to the main application. It can shield the differences between different programming languages and unify the functions of microservices such as observability, monitoring, logging, configuration, circuit breaker, etc.\nAdvantages of using the Sidecar pattern When deploying a service mesh using the sidecar model, there is no need to run an agent on the node, but multiple copies of the same sidecar will run in the cluster. In the sidecar deployment model, a companion container (such as Envoy or MOSN) is deployed next to each application’s container, which is called a sidecar container. The sidecar takes overall traffic in and out of the application container. In Kubernetes’ Pod, a sidecar container is injected next to the original application container, and the two containers share storage, networking, and other resources.\nDue to its unique deployment architecture, the sidecar model offers the following advantages.\nAbstracting functions unrelated to application business logic into a common infrastructure reduces the complexity of microservice code. Reduce code duplication in microservices architectures because it is no longer necessary to write the same third-party component profiles and code. The sidecar can be independently upgraded to reduce the coupling of application code to the underlying platform. iptables manipulation analysis In order to view the iptables configuration, we need to nsenter the sidecar container using the root user to view it, because kubectl cannot use privileged mode to remotely manipulate the docker container, so we need to log on to the host where the productpage pod is located.\nIf you use Kubernetes deployed by minikube, you can log directly into the minikube’s virtual machine and switch to root. View the iptables configuration that lists all the rules for the NAT (Network Address Translation) table because the mode for redirecting inbound traffic to the sidecar is REDIRECT in the parameters passed to the istio-iptables when the Init container is selected for the startup, so there will only be NAT table specifications in the iptables and mangle table configurations if TPROXY is selected. See the iptables command for detailed usage.\nWe only look at the iptables rules related to productpage below.\n# login to minikube, change user to root $ minikube ssh $ sudo -i # See the processes in the productpage pod\u0026#39;s istio-proxy container $ docker top `docker ps|grep \u0026#34;istio-proxy_productpage\u0026#34;|cut -d \u0026#34; \u0026#34; -f1` UID PID PPID C STIME TTY TIME CMD 1337 10576 10517 0 08:09 ? 00:00:07 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage.default --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istiod.istio-system.svc:15012 --zipkinAddress zipkin.istio-system:9411 --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --connectTimeout 10s --proxyAdminPort 15000 --concurrency 2 --controlPlaneAuthPolicy NONE --dnsRefreshRate 300s --statusPort 15020 --trust-domain=cluster.local --controlPlaneBootstrap=false 1337 10660 10576 0 08:09 ? 00:00:33 /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage.default --service-node sidecar~172.17.0.16~productpage-v1-7f44c4d57c-ksf9b.default~default.svc.cluster.local --max-obj-name-len 189 --local-address-ip-version v4 --log-format [Envoy (Epoch 0)] [%Y-%m-%d %T.%e][%t][%l][%n] …","relpermalink":"/en/blog/sidecar-injection-iptables-and-traffic-routing/","summary":"This blog describes the sidecar pattern, transparent traffic hijacking and routing.","title":"Sidecar injection, transparent traffic hijacking , and routing process in Istio explained in detail"},{"content":"This article will explain:\nThe sidecar auto-injection process in Istio The init container startup process in Istio The startup process of a Pod with Sidecar auto-injection enabled The following figure shows the components of a Pod in the Istio data plane after it has been started.\nIstio data plane pod Sidecar injection in Istio The following two sidecar injection methods are available in Istio.\nManual injection using istioctl. Kubernetes-based mutating webhook admission controller automatic sidecar injection method. Whether injected manually or automatically, SIDECAR’s injection process follows the following steps.\nKubernetes needs to know the Istio cluster to which the sidecar to be injected is connected and its configuration. Kubernetes needs to know the configuration of the sidecar container itself to be injected, such as the image address, boot parameters, etc. Kubernetes injects the above configuration into the side of the application container by the sidecar injection template and the configuration parameters of the above configuration-filled sidecar. The sidecar can be injected manually using the following command.\nistioctl kube-inject -f ${YAML_FILE} | kuebectl apply -f - This command is injected using Istio’s built-in sidecar configuration, see the Istio official website for details on how to use Istio below.\nWhen the injection is complete you will see that Istio has injected initContainer and sidecar proxy-related configurations into the original pod template.\nInit container The Init container is a dedicated container that runs before the application container is launched and is used to contain some utilities or installation scripts that do not exist in the application image.\nMultiple Init containers can be specified in a Pod, and if more than one is specified, the Init containers will run sequentially. The next Init container can only be run if the previous Init container must run successfully. Kubernetes only initializes the Pod and runs the application container when all the Init containers have been run.\nThe Init container uses Linux Namespace, so it has a different view of the file system than the application container. As a result, they can have access to Secret in a way that application containers cannot.\nDuring Pod startup, the Init container starts sequentially after the network and data volumes are initialized. Each container must be successfully exited before the next container can be started. If exiting due to an error will result in a container startup failure, it will retry according to the policy specified in the Pod’s restartPolicy. However, if the Pod’s restartPolicy is set to Always, the restartPolicy is used when the Init container failed.\nThe Pod will not become Ready until all Init containers are successful. The ports of the Init containers will not be aggregated in the Service. The Pod that is being initialized is in the Pending state but should set the Initializing state to true. The Init container will automatically terminate once it is run.\nSidecar injection example analysis For a detailed YAML configuration for the bookinfo applications, see bookinfo.yaml for the official Istio YAML of productpage in the bookinfo sample.\nThe following will be explained in the following terms.\nInjection of Sidecar containers Creation of iptables rules The detailed process of routing apiVersion: apps/v1 kind: Deployment metadata: name: productpage-v1 labels: app: productpage version: v1 spec: replicas: 1 selector: matchLabels: app: productpage version: v1 template: metadata: labels: app: productpage version: v1 spec: serviceAccountName: bookinfo-productpage containers: - name: productpage image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9080 volumeMounts: - name: tmp mountPath: /tmp volumes: - name: tmp emptyDir: {} Let’s see the productpage container’s Dockerfile .\nFROM python:3.7.4-slim COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt COPY test-requirements.txt ./ RUN pip install --no-cache-dir -r test-requirements.txt COPY productpage.py /opt/microservices/ COPY tests/unit/* /opt/microservices/ COPY templates /opt/microservices/templates COPY static /opt/microservices/static COPY requirements.txt /opt/microservices/ ARG flood_factor ENV FLOOD_FACTOR ${flood_factor:-0} EXPOSE 9080 WORKDIR /opt/microservices RUN python -m unittest discover USER 1 CMD [\u0026#34;python\u0026#34;, \u0026#34;productpage.py\u0026#34;, \u0026#34;9080\u0026#34;] We see that ENTRYPOINT is not configured in Dockerfile, so CMD’s configuration python productpage.py 9080 will be the default ENTRYPOINT, keep that in mind and look at the configuration after the sidecar injection.\n$ istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml We intercept only a portion of the YAML configuration that is part of the Deployment configuration associated with productpage.\ncontainers: - image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 # application image name: …","relpermalink":"/en/blog/istio-pod-process-lifecycle/","summary":"This article will explain Istio's Init container, Pod internal processes and the startup process.","title":"Istio data plane pod startup process explained"},{"content":"iptables is an important feature in the Linux kernel and has a wide range of applications. iptables is used by default in Istio for transparent traffic hijacking. Understanding iptables is very important for us to understand how Istio works. This article will give you a brief introduction to iptbles.\niptables introduction iptables is a management tool for netfilter, the firewall software in the Linux kernel. netfilter is located in the user space and is part of netfilter. netfilter is located in the kernel space and has not only network address conversion, but also packet content modification and packet filtering firewall functions.\nBefore learning about iptables for Init container initialization, let’s go over iptables and rule configuration.\nThe following figure shows the iptables call chain.\niptables 调用链 iptables The iptables version used in the Init container is v1.6.0 and contains 5 tables.\nRAW is used to configure packets. Packets in RAW are not tracked by the system. The filter is the default table used to house all firewall-related operations. NAT is used for network address translation (e.g., port forwarding). Mangle is used for modifications to specific packets (refer to corrupted packets). Security is used to force access to control network rules. Note: In this example, only the NAT table is used.\nThe chain types in the different tables are as follows.\nRule name raw filter nat mangle security PREROUTING ✓ ✓ ✓ INPUT ✓ ✓ ✓ OUTPUT ✓ ✓ ✓ ✓ ✓ POSTROUTING ✓ ✓ FORWARD ✓ ✓ ✓ Understand iptables rules View the default iptables rules in the istio-proxy container, the default view is the rules in the filter table.\n$ iptables -L -v Chain INPUT (policy ACCEPT 350K packets, 63M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 18M packets, 1916M bytes) pkts bytes target prot opt in out source destination We see three default chains, INPUT, FORWARD, and OUTPUT, with the first line of output in each chain indicating the chain name (INPUT/FORWARD/OUTPUT in this case), followed by the default policy (ACCEPT).\nThe following is a proposed structure diagram of iptables, where traffic passes through the INPUT chain and then enters the upper protocol stack, such as:\niptables chains Multiple rules can be added to each chain and the rules are executed in order from front to back. Let’s look at the table header definition of the rule.\nPKTS: Number of matched messages processed bytes: cumulative packet size processed (bytes) Target: If the message matches the rule, the specified target is executed. PROT: Protocols such as TDP, UDP, ICMP, and ALL. opt: Rarely used, this column is used to display IP options. IN: Inbound network interface. OUT: Outbound network interface. source: the source IP address or subnet of the traffic, the latter being anywhere. destination: the destination IP address or subnet of the traffic, or anywhere. There is also a column without a header, shown at the end, which represents the options of the rule, and is used as an extended match condition for the rule to complement the configuration in the previous columns. prot, opt, in, out, source and destination and the column without a header shown after destination together form the match rule. TARGET is executed when traffic matches these rules.\nTypes supported by TARGET\nTarget types include ACCEPT, REJECT, DROP, LOG, SNAT, MASQUERADE, DNAT, REDIRECT, RETURN or jump to other rules, etc. You can determine where the telegram is going by executing only one rule in a chain that matches in order, except for the RETURN type, which is similar to the return statement in programming languages, which returns to its call point and continues to execute the next rule.\nFrom the output, you can see that the Init container does not create any rules in the default link of iptables, but instead creates a new link.\nSummary With the above brief introduction to iptables, you have understood how iptables works, the rule chain and its execution order.\n","relpermalink":"/en/blog/understanding-iptables/","summary":"This article will give you a brief introduction to iptables, its tables and the order of execution.","title":"Understanding iptables"},{"content":"See the cloud native public library at: https://jimmysong.io/docs/ The cloud native public library project is a documentation project built using the Wowchemy theme, open sourced on GitHub .\nI have also adjusted the home page, menu and directory structure of the site, and the books section of the site will be maintained using the new theme.\nCloud native library positioning The cloud native public library is a collection of cloud native related books and materials published and translated by the author since 2017, and is a compendium and supplement to the dozen or so books already published. The original materials will continue to be published in the form of GitBooks, and the essence and related content will be sorted into the cloud native public library through this project.\nIn addition, the events section of this site has been revamped and moved to a new page .\n","relpermalink":"/en/notice/cloud-native-public-library/","summary":"A one-stop cloud native library that is a compendium of published materials.","title":"Cloud Native library launch"},{"content":"In my last two blogs:\nSidecar injection, transparent traffic hijacking , and routing process in Istio explained in detail Traffic types and iptables rules in Istio sidecar explained I gave you a detailed overview of the traffic in the Istio data plane, but the data plane does not exist in isolation. This article will show you the ports and their usages for each component of both the control plane and data plane in Istio, which will help you understand the relationship between these flows and troubleshoot them.\nOverview Firstly, I will show you a global schematic. The following figure shows the components of a sidecar in the Istio data plane, and the objects that interact with it.\nIstio components We can use the nsenter command to enter the namespace of the productpage Pod of the Bookinfo example and see the information about the ports it is listening on internally.\nIstio sidecar ports From the figure, we can see that besides the port 9080 that the productpage application listens to, the Sidecar container also listens to a large number of other ports, such as 15000, 15001, 15004, 15006, 15021, 15090, etc. You can learn about the ports used in Istio in the Istio documentation .\nLet’s go back into the productpage Pod and use the lsof -i command to see the ports it has open, as shown in the following figure.\nProductpage Pod ports We can see that there is a TCP connection established between the pilot-agent and istiod, the port in the listening described above, and the TCP connection established inside the Pod, which corresponds to the figure at the beginning of the article.\nThe root process of the Sidecar container (istio-proxy) is pilot-agent, and the startup command is shown below.\nInternal procecces in Sidecar As we can see from the figure, the PID of its pilot-agent process is 1, and it forked the Envoy process.\nCheck the ports it opens in Istiod, as shown in the figure below.\nIstiod ports We can see the ports that are listened to, the inter-process and remote communication connections.\nPorts usage overview These ports can play a pivotal role when you are troubleshooting. They are described below according to the component and function in which the port is located.\nPorts in Istiod The ports in Istiod are relatively few and single-function.\n9876: ControlZ user interface, exposing information about Istiod’s processes 8080: Istiod debugging port, through which the configuration and status information of the grid can be queried 15010: Exposes the xDS API and issues plain text certificates 15012: Same functionality as port 15010, but uses TLS communication 15014: Exposes control plane metrics to Prometheus 15017: Sidecar injection and configuration validation port Ports in sidecar From the above, we see that there are numerous ports in the sidecar.\n15000: Envoy admin interface, which you can use to query and modify the configuration of Envoy Proxy. Please refer to Envoy documentation for details. 15001: Used to handle outbound traffic. 15004: Debug port (explained further below). 15006: Used to handle inbound traffic. 15020: Summarizes statistics, perform health checks on Envoy and DNS agents, and debugs pilot-agent processes, as explained in detail below. 15021: Used for sidecar health checks to determine if the injected Pod is ready to receive traffic. We set up the readiness probe on the /healthz/ready path on this port, and Istio hands off the sidecar readiness checks to kubelet. 15053: Local DNS proxy for scenarios where the cluster’s internal domain names are not resolved by Kubernetes DNS. 15090: Envoy Prometheus query port, through which the pilot-agent will scratch metrics. The above ports can be divided into the following categories.\nResponsible for inter-process communication, such as 15001, 15006, 15053 Health check and information statistics, e.g. 150021, 15090 Debugging: 15000, 15004 Let’s look at the key ports in detail.\n15000 15000 is Envoy’s Admin interface, which allows us to modify Envoy and get a view and query metrics and configurations.\nThe Admin interface consists of a REST API with multiple endpoints and a simple user interface. You can enable the Envoy Admin interface view in the productpage Pod using the following command:\nkubectl -n default port-forward deploy/productpage-v1 15000 Visit http://localhost:15000 in your browser and you will see the Envoy Admin interface as shown below.\nEnvoy Admin interface 15004 With the pilot-agent proxy istiod debug endpoint on port 8080, you can access localhost’s port 15004 in the data plane Pod to query the grid information, which has the same effect as port 8080 below.\n8080 You can also forward istiod port 8080 locally by running the following command:\nkubectl -n istio-system port-forward deploy/istiod 8080 Visit http://localhost:8080/debug in your browser and you will see the debug endpoint as shown in the figure below.\nPilot Debug Console Of course, this is only one way to get the mesh information and debug the mesh, you can also use istioctl …","relpermalink":"/en/blog/istio-components-and-ports/","summary":"This article will introduce you to the various ports and functions of the Istio control plane and data plane.","title":" Istio component ports and functions in detail"},{"content":"As we know that Istio uses iptables for traffic hijacking, where the iptables rule chains has one called ISTIO_OUTPUT, which contains the following rules.\nRule target in out source destination 1 RETURN any lo 127.0.0.6 anywhere 2 ISTIO_IN_REDIRECT any lo anywhere !localhost owner UID match 1337 3 RETURN any lo anywhere anywhere !owner UID match 1337 4 RETURN any any anywhere anywhere owner UID match 1337 5 ISTIO_IN_REDIRECT any lo anywhere !localhost owner GID match 1337 6 RETURN any lo anywhere anywhere !owner GID match 1337 7 RETURN any any anywhere anywhere owner GID match 1337 8 RETURN any any anywhere localhost 9 ISTIO_REDIRECT any any anywhere anywhere The sidecar applies these rules to deal with different types of traffic. This article will show you the six types of traffic and their iptables rules in Istio sidecar.\niptables Traffic Routing in Sidecar The following list summarizes the six types of traffic in Sidecar.\nRemote service accessing local service: Remote Pod -\u0026gt; Local Pod Local service accessing remote service: Local Pod -\u0026gt; Remote Pod Prometheus crawling metrics of local service: Prometheus -\u0026gt; Local Pod Traffic between Local Pod service: Local Pod -\u0026gt; Local Pod Inter-process TCP traffic within Envoy Sidecar to Istiod traffic The following will explain the iptables routing rules within Sidecar for each scenario, which specifies which rule in ISTIO_OUTPUT is used for routing.\nType 1: Remote Pod -\u0026gt; Local Pod The following are the iptables rules for remote services, applications or clients accessing the local pod IP of the data plane.\nRemote Pod -\u0026gt; RREROUTING -\u0026gt; ISTIO_INBOUND -\u0026gt; ISTIO_IN_REDIRECT -\u0026gt; Envoy 15006 (Inbound) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 1 -\u0026gt; POSTROUTING -\u0026gt; Local Pod\nWe see that the traffic only passes through the Envoy 15006 Inbound port once. The following diagram shows this scenario of the iptables rules.\nRemote Pod to Local Pod Type 2: Local Pod -\u0026gt; Remote Pod The following are the iptables rules that the local pod IP goes through to access the remote service.\nLocal Pod-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 9 -\u0026gt; ISTIO_REDIRECT -\u0026gt; Envoy 15001 (Outbound) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 4 -\u0026gt; POSTROUTING -\u0026gt; Remote Pod\nWe see that the traffic only goes through the Envoy 15001 Outbound port.\nLocal Pod to Remote Pod The traffic in both scenarios above passes through Envoy only once because only one scenario occurs in that Pod, sending or receiving requests.\nType 3: Prometheus -\u0026gt; Local Pod Prometheus traffic that grabs data plane metrics does not have to go through the Envoy proxy.\nThese traffic pass through the following iptables rules.\nPrometheus-\u0026gt; RREROUTING -\u0026gt; ISTIO_INBOUND (traffic destined for ports 15020, 15090 will go to INPUT) -\u0026gt; INPUT -\u0026gt; Local Pod\nPrometheus to Local Pod Type 4: Local Pod -\u0026gt; Local Pod A Pod may simultaneously have two or more services. If the Local Pod accesses a service on the current Pod, the traffic will go through Envoy 15001 and Envoy 15006 ports to reach the service port of the Local Pod.\nThe iptables rules for this traffic are as follows.\nLocal Pod-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 9 -\u0026gt; ISTIO_REDIRECT -\u0026gt; Envoy 15001（Outbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 2 -\u0026gt; ISTIO_IN_REDIRECT -\u0026gt; Envoy 15006（Inbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 1 -\u0026gt; POSTROUTING -\u0026gt; Local Pod\nLocal Pod to Local Pod Type 5: Inter-process TCP traffic within Envoy Envoy internal processes with UID and GID 1337 will communicate with each other using lo NICs and localhost domains.\nThe iptables rules that these flows pass through are as follows.\nEnvoy process (Localhost) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 8 -\u0026gt; POSTROUTING -\u0026gt; Envoy process (Localhost)\nEnvoy inter-process TCP traffic Type 6: Sidecar to Istiod traffic Sidecar needs access to Istiod to synchronize its configuration so that Envoy will have traffic sent to Istiod.\nThe iptables rules that this traffic passes through are as follows.\npilot-agent process -\u0026gt; OUTPUT -\u0026gt; Istio_OUTPUT RULE 9 -\u0026gt; Envoy 15001 (Outbound Handler) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 4 -\u0026gt; POSTROUTING -\u0026gt; Istiod\nSidecar to Istiod Summary All the sidecar proxies that Istio injects into the Pod or installed in the virtual machine form the data plane of the service mesh, which is also the main workload location of Istio. In my next blog, I will take you through the ports of each component in Envoy and their functions, so that we can have a more comprehensive understanding of the traffic in Istio.\n","relpermalink":"/en/blog/istio-sidecar-traffic-types/","summary":"This article will show you the six traffic types and their iptables rules in Istio sidecar, and take you through the whole diagram in a schematic format.","title":"Traffic types and iptables rules in Istio sidecar explained"},{"content":"Istio 1.13 is the first release of 2022, and, not surprisingly, the Istio team will continue to release new versions every quarter. Overall, the new features in this release include:\nSupport for newer versions of Kubernetes New API – ProxyConfig, for configuring sidecar proxies Improved Telemetry API Support for hostname-based load balancers with multiple network gateways Support for Kubernetes Versions I often see people asking in the community which Istio supports Kubernetes versions. Istio’s website has a clear list of supported Kubernetes versions. You can see here that Istio 1.13 supports Kubernetes versions 1.20, 1.21, 1.22, and 1.23, and has been tested but not officially supported in Kubernetes 1.16, 1.17, 1.18, 1.19.\nWhen configuring Istio, there are a lot of checklists. I noted them all in the Istio cheatsheet . There are a lot of cheat sheets about configuring Istio, using resources, dealing with everyday problems, etc., in this project, which will be online soon, so stay tuned.\nThe following screenshot is from the Istio cheatsheet website, it shows the basic cheat sheet for setting up Istio.\nIstio cheatsheet Introducing the new ProxyConfig API Before Istio version 1.13, if you wanted to customize the configuration of the sidecar proxy, there were two ways to do it.\nMeshConfig\nUse MeshConfig and use IstioOperator to modify it at the Mesh level. For example, use the following configuration to alter the default discovery port for istiod.\napVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: defaultConfig: discoveryAddress: istiod:15012 Annotation in the Pods\nYou can also use annotation at the Pod level to customize the configuration. For example, you can add the following annotations to Pod to modify the default port for istiod of the workload:\nanannotations: proxy.istio.io/config: | discoveryAddress: istiod:15012 When you configure sidecar in either of these ways, the fields set in annotations will completely override the default fields in MeshConfig. Please refer to the Istio documentation for all configuration items of ProxyConfig.\nThe new API – ProxyConfig\nBut in 1.13, a new top-level custom resource, ProxyConfig, has been added, allowing you to customize the configuration of your sidecar proxy in one place by specifying a namespace and using a selector to select the scope of the workload, just like any other CRD. Istio currently has limited support for this API, so please refer to the Istio documentation for more information on the ProxyConfig API.\nHowever, no matter which way you customize the configuration of the sidecar proxy, it does not take effect dynamically and requires a workload restart to take effect. For example, for the above configuration, because you changed the default port of istiod, all the workloads in the mesh need to be restarted before connecting to the control plane.\nTelemetry API MeshConfig customized extensions and configurations in the Istio mesh. The three pillars of observability– Metrics, Telemetry, and Logging– can each be docked to different providers. The Telemetry API gives you a one-stop place for flexible configuration of them. Like the ProxyConfig API, the Telemetry API follows the configuration hierarchy of Workload Selector \u0026gt; Local Namespace \u0026gt; Root Configuration Namespace. The API was introduced in Istio 1.11 and has been further refined in that release to add support for OpenTelemetry logs, filtered access logs, and custom tracing service names. See Telemetry Configuration for details.\nAutomatic resolution of multi-network gateway hostnames In September 2021, a member of the Istio community reported an issue with the EKS load balancer failing to resolve when running multi-cluster Istio in AWS EKS. Workloads that cross cluster boundaries need to be communicated indirectly through a dedicated east-west gateway for a multi-cluster, multi-network mesh. You can follow the instructions on Istio’s website to configure a multi-network, primary-remote cluster, and Istio will automatically resolve the IP address of the load balancer based on the hostname.\nIstio 1.13.1 fixing the critical security vulnerabilities Istio 1.13.1 was released to fix a known critical vulnerability that could lead to an unauthenticated control plane denial of service attack.\nThe figure below shows a multi-cluster primary-remote mesh where istiod exposes port 15012 to the public Internet via a gateway so that a pod on another network can connect to it.\nMulti-network Mesh When installing a multi-network, primary-remote mode Istio mesh, for a remote Kubernetes cluster to access the control plane, an east-west Gateway needs to be installed in the Primary cluster, exposing port 15012 of the control plane istiod to the Internet. An attacker could send specially crafted messages to that port, causing the control plane to crash. If you set up a firewall to allow traffic from only some IPs to access this port, you will be able to reduce the impact of the problem. It is …","relpermalink":"/en/blog/what-is-new-in-istio-1-13/","summary":"In February 2022, Istio released 1.13.0 and 1.13.1. This blog will give you an overview of what’s new in these two releases.","title":"What's new in Istio 1.13?"},{"content":"Join a team of world-class engineers working on the next generation of networking services using Istio, Envoy, Apache SkyWalking and a few of the open projects to define the next generation of cloud native network service.\nIstio upstream contributor: Golang We are looking for engineers with strong distributed systems experience to join our team. We are building a secure, robust, and highly available service mesh platform for mission critical enterprise applications spanning both legacy and modern infrastructure. This is an opportunity to dedicate a significant amount of contribution to Istio upstream on a regular basis. If you are a fan of Istio and would like to increase your contribution on a dedicated basis, this would be an opportunity for you.\nRequirements\nFundamentals-based problem solving skills; Drive decision by function, first principles based mindset. Demonstrate bias-to-action and avoid analysis-paralysis; Drive action to the finish line and on time. You are ego-less when searching for the best ideasIntellectually curious with a penchant for seeing opportunities in ambiguity Understands the difference between attention to detail vs. detailed - oriented Values autonomy and results over process You contribute effectively outside of your specialty Experience building distributed system platforms using Golang Familiarity with Kubernetes, service mesh technologies such as Istio and Envoy Excellent understanding of networking protocols, concepts, consistency properties of distributed systems, techniques to identify and reconcile configuration drifts Experience contributing to open source projects is a plus. Familiarity with WebAssembly is a plus. Familiarity with Golang, hardware/software load balancers (F5, NGINX), HSM modules, Active Directory/LDAP is a plus. We encourage written and asynchronous communication in English, and proficient oral English is not required.\nDistributed Systems Engineer, Enterprise Infrastructure (Data plane) GoLang or C++ Developers Seeking backend software engineers experienced in building distributed systems using Golang and gRPC. We are building a secure, and highly available service mesh platform for mission-critical enterprise applications for Fortune 500 companies, spanning both the legacy and modern infrastructure. Should possess strong fundamentals in distributed systems and networking. Familiarity with technologies like Kubernetes, Istio, and Envoy, as well as open contributions would be a plus.\nRequirements\nExperience building distributed system platforms using C++, Golang, and gRPC. Familiarity with Kubernetes, service mesh technologies such as Istio and Envoy. Excellent understanding of networking protocols, concepts, consistency properties of distributed systems, techniques to identify and reconcile configuration drifts. Experience contributing to open source projects is a plus. Familiarity with the following is a plus: WebAssembly, Authorization: NGAC, RBAC, ABAC Familiarity with hardware/software load balancers (F5, NGINX), HSM modules, Active Directory/LDAP is a plus. Site Reliability Engineer, SRE Site Reliability Engineering (SRE) combines software and systems engineering to build and run scalable, massively distributed, fault-tolerant systems. As part of the team, you will be working on ensuring that Tetrate’s platform has reliability/uptime appropriate to users’ needs as well as a fast rate of improvement. Additionally, much of our engineering effort focuses on building infrastructure, improving the platform troubleshooting abilities, and eliminating toil through automation.\nRequirements\nSystematic problem-solving approach, coupled with excellent communication skills and a sense of ownership/finish and self-directed drive. Strong fundamentals in operating, debugging, and troubleshooting distributed systems(stateful and/or stateless) and networking. Familiarity with Kubernetes, service mesh technologies such as Istio and EnvoyAbility to debug, optimize code, and automate routine tasks. Experience programming in at least one of the following languages: C++, Rust, Python, Go. Familiarity with the concepts of quantifying failure and availability in a prescriptive manner using SLOs and SLIs. Experience in performance analysis and tuning is a plus. Location Worldwide\nWe are remote with presence in China, Indonesia, India, Japan, U.S., Canada, Ireland, the Netherlands, Spain, and Ukraine.\nPlease send GitHub or online links that showcase your code style along with your resume to careers@tetrate.io .\nAbout Tetrate Powered by Envoy and Istio, its ﬂagship product, Tetrate Service Bridge, enables bridging traditional and modern workloads. Customers can get consistent baked-in observability, runtime security and traffic management for all their workloads, in any environment.\nIn addition to the technology, Tetrate brings a world-class team that leads the open Envoy and Istio projects, providing best practices and playbooks that enterprises can use to modernize their …","relpermalink":"/en/notice/tetrate-recruit/","summary":"Remotely worldwide","title":"The Enterprise Service Mesh company Tetrate is hiring"},{"content":"As the service mesh architecture concept gains traction and the scenarios for its applications emerge, there is no shortage of discussions about it in the community. I have worked on service mesh with the community for 4 years now, and will summarize the development of service mesh in 2021 from this perspective. Since Istio is the most popular service mesh, this article will focus on the technical and ecological aspects of Istio.\nService mesh: a critical tech for Cloud Native Infrastructure As one of the vital technologies defined by CNCF for cloud native, Istio has been around for five years now. Their development has gone through the following periods.\nExploration phase: 2017-2018 Early adopter phase: 2019-2020 Large-scale landing and ecological development phase: 2021-present Service mesh has crossed the “chasm”(refer Crossing the Chasm theory) and is in between the “early majority” and “late majority” phases of adoption. Based on feedback from the audience of Istio Weekly, users are no longer blindly following new technologies for experimentation and are starting to consider whether they need them in their organization dialectically.\nCross the chasm While new technologies and products continue to emerge, the service mesh, as part of the cloud native technology stack, has continued to solidify its position as the “cloud native network infrastructure” over the past year. The diagram below illustrates the cloud native technology stack model, where each layer has several representative technologies that define the standard. As new-age middleware, the service mesh mirrors other cloud native technologies, such as Dapr (Distributed Application Runtime), which represents the capability model for cloud native middleware, OAM , which defines the cloud native application model, and the service mesh, which defines the L7 network model.\nCloud Native Stack A layered view of the cloud native application platform technology stack\nOptimizing the mesh for large scale production applications with different deployment models Over the past year, the community focused on the following areas.\nPerformance optimization: performance issues of service mesh in large-scale application scenarios. Protocol and extensions: enabling service mesh to support arbitrary L7 network protocols. Deployment models: Proxyless vs. Node model vs. Sidecar model. eBPF: putting some of the service mesh’s capabilities to the kernel layer. Performance optimization Istio was designed to serve service to service traffic by “proto-protocol forwarding”. The goal is making the service mesh as “transparent” as possible to applications. Thus using IPtables to hijack the traffic, according to the community-provided test results Istio 1.2 adds only 3 ms to the baseline latency for a mesh with 1000 RPS on 16 connections. However, because of issues inherent in the IPtables conntrack module, Istio’s performance issues begin to emerge as the mesh size increases. To optimize the performance of the Istio sidecar for resource usage and network latency, the community gave the following solutions.\nSidecar configuration: By configuring service dependencies manually or by adding an Operator to the control plane, the number of service configurations sent to Sidecar can be reduced, thus reducing the resource footprint of the data plane; for more automatic and intelligent configuration of Sidecar, the open source projects Slime and Aeraki both offer their innovative configuration loading solutions. The introduction of eBPF: eBPF can be a viable solution to optimize the performance of the service mesh. Some Cilium-based startups even radically propose to use eBPF to replace the Sidecar proxy completely. Still, the Envoy proxy/xDS protocol has become the proxy for the service mesh implementation and supports the Layer 7 protocol very well. We can use eBPF to improve network performance, but complex protocol negotiation, parsing, and user scaling remain challenging to implement on the user side. Protocol and extensions Extensibility of Istio has always been a significant problem, and there are two aspects to Istio’s extensibility.\nProtocol level: allowing Istio to support all L7 protocols Ecological: allowing Istio to run more extensions Istio uses Envoy as its data plane. Extending Istio is essentially an extension of Envoy’s functionality. Istio’s official solution is to use WebAssembly, and in Istio 1.12, the Wasm plugin configuration API was introduced to extend the Istio ecosystem. Istio’s extension mechanism uses the Proxy-Wasm Application Binary Interface (ABI) specification to provide a set of proxy-independent streaming APIs and utilities that can be implemented in any language with an appropriate SDK. Today, Proxy-Wasm’s SDKs are AssemblyScript (similar to TypeScript), C++, Rust, Zig, and Go (using the TinyGo WebAssembly System Interface).\nThere are still relatively few WebAssembly extensions available, and many enterprises choose to customize their CRD and build a …","relpermalink":"/en/blog/service-mesh-in-2021/","summary":"A review of the development of Service Mesh in 2021.","title":"Service Mesh in 2021: the ecosystem is emerging"},{"content":"It’s been more than four years since Istio launched in May 2017, and while the project has had a strong following on GitHub and 10+ releases, its growing open-source ecosystem is still in its infancy.\nRecently added support for WebAssembly extensions has made the most popular open source service mesh more extensible than ever. This table lists the open-source projects in the Istio ecosystem as of November 11, 2021, sorted by open-source date. These projects enhance the Istio service mesh with gateways, extensions, utilities, and more. In this article, I’ll highlight the two new projects in the category of extensions.\nProject Value Relationship with Istio Category Launch Date Dominant company Number of stars Envoy Cloud native high-performance edge/middle-service proxy The default data plane proxy September 2016 Lyft 18700 Istio Connection, secure, control, and observation services. Control plane service mesh May 2017 Google 29100 Emissary Gateway Kubernetes native API gateway for microservices, built on Envoy Connectable to Istio gateway February 2018 Ambassador 3600 APISIX Cloud native API gateways It can run as a data plane for Istio or as a gateway on its own gateway June 2019 API7 8100 MOSN Cloud native edge gateways \u0026amp; agents Available as Istio data plane proxy December 2019 Ant 3500 Slime Intelligent service mesh manager based on Istio Adding a management plane to Istio extensions January 2021 NetEase 236 GetMesh Istio integration and command-line management tools Utility for Istio multi-version management tools February 2021 Tetrate 95 Aeraki Manage any of Istio’s seven layers of load Extended multi-protocol support extensions March 2021 Tencent 330 Layotto Cloud native application runtime Using as a data plane for Istio runtime June 2021 Ant 393 Hango Gateway API gateways built on Envoy and Istio Integrates with Istio gateway August 2021 NetEase 253 Slime: an intelligent service mesh manager for Istio Slime is an Istio-based, intelligent mesh manager open-sourced by NetEase’s microservices team. Based on the Kubernetes Operator implementation, Slime can be used as a CRD manager that seamlessly interfaces with Istio without needing any customization or definition of dynamic service governance policies. This achieves automatic and convenient use of Istio and Envoy’s advanced features.\nSlime addresses the following issues:\nImplementing higher-level extensions in Istio. For example, extending the HTTP plugin; adaptive traffic limiting based on the resource usage of the service. Poor performance arising from Istio sending all the configurations within the mesh to each sidecar proxy. Slime solves these problems by building an Istio management plane. Its main purpose are\nto build a pluggable controller to facilitate the extension of new functions. to obtain data by listening to the data plane to intelligently generate the configuration for Istio. to build a higher-level CRD for the user to configure, which Slime converts into an Istio configuration. The following diagram shows the flow chart of Istio as an Istio management plane.\nSlime architecture The specific steps for Slime to manage Istio are as follows.\nSlime operator completes the initialization of Slime components in Kubernetes based on the administrator’s configuration. Developers create configurations that conform to the Slime CRD specification and apply them to Kubernetes clusters. Slime queries the monitoring data of the relevant service stored in Prometheus and converts the Slime CRD into an Istio CRD, in conjunction with the configuration of the adaptive part of the Slime CRD while pushing it to the Global Proxy. Istio listens for the creation of Istio CRDs. Istio pushes the configuration information of the Sidecar Proxy to the corresponding Sidecar Proxy in the data plane. The diagram below shows the internal architecture of Slime.\nSlime Internal We can divide Slime internally into three main components.\nslime-boot: operator for deploying Slime modules on Kubernetes. slime-controller: the core component of Slime that listens to the Slime CRD and converts it to an Istio CRD. slime-metric: the component used to obtain service metrics information. slime-controller dynamically adjusts service governance rules based on the information it receives. The following diagram shows the architecture of Slime Adaptive Traffic Limiting. Slime smart limiter Slime dynamically configures traffic limits by interfacing with the Prometheus metric server to obtain real-time monitoring.\nSlime’s adaptive traffic limitation process has two parts: one that converts SmartLimiter to EnvoyFilter and the other that monitors the data. Slime also provides an external monitoring data interface (Metric Discovery Server) that allows you to sync custom monitoring metrics to the traffic limiting component via MDS.\nThe CRD SmartLimiter created by Slime is used to configure adaptive traffic limiting. Its configuration is close to natural semantics, e.g., if you want to trigger an …","relpermalink":"/en/blog/istio-extensions-slime-and-aeraki/","summary":"In this article, I’ll introduce you two Istio extension projects: Aeraki and Slime.","title":"Introducing Slime and Aeraki in the evolution of Istio open-source ecosystem"},{"content":"You can use Istio to do multi-cluster management , API Gateway , and manage applications on Kubernetes or virtual machines . In my last blog , I talked about how service mesh is an integral part of cloud native applications. However, building infrastructure can be a big deal. There is no shortage of debate in the community about the practicability of service mesh and Istio– here’s a list of common questions and concerns, and how to address them.\nIs anyone using Istio in production? What is the impact on application performance due to the many resources consumed by injecting sidecar into the pod? Istio supports a limited number of protocols; is it scalable? Will Istio be manageable? – Or is it too complex, old services too costly to migrate, and the learning curve too steep? I will answer each of these questions below.\nIstio is architecturally stable, production-ready, and ecologically emerging Istio 1.12 was just released in November – and has evolved significantly since the explosion of service mesh in 2018 (the year Istio co-founders established Tetrate). Istio has a large community of providers and users . The Istio SIG of Cloud Native Community has held eight Istio Big Talk (Istio 大咖说) , with Baidu, Tencent, NetEase, Xiaohongshu(小红书), and Xiaodian Technology(小电科技) sharing their Istio practices. According to CNCF Survey Report 2020 , about 50% of the companies surveyed are using a service mesh in production or planning to in the next year, and about half (47%) of organizations using a service mesh in production are using Istio.\nMany companies have developed extensions or plugins for Istio, such as Ant, NetEase, eBay, and Airbnb. Istio’s architecture has been stable since the 1.5 release, and the release cycle is fixed quarterly, with the current project’s main task being Day-2 Operations.\nThe Istio community has also hosted various events, with the first IstioCon in March 2021, the Istio Meetup China in Beijing in July, and the Service Mesh Summit 2022 in Shanghai in January 2022.\nSo we can say that the Istio architecture is stable and production-ready, and the ecosystem is budding.\nThe impact of service mesh on application performance A service mesh uses iptables to do traffic hijacking by default to be transparent to applications. When the number of services is large, there are a lot of iptables rules that affect network performance. You can use techniques like eBPF to provide application performance, but the method requires a high version of the operating system kernel, which few enterprises can achieve.\nIstio DNS In the early days, Istio distributed the routing information of all services in the mesh to all proxy sidecars, which caused sidecar s to take up a lot of resources. Aeraki and Slime can achieve configuration lazy loading. We will introduce these two open-source projects in the Istio open-source ecosystem.\nFinally, there is a problem related to Sidecar proxy operation and maintenance: upgrading all Envoy proxies while ensuring constant traffic. A solution is using the SidecarSet resource in the open-source project OpenKruise .\nThe resource consumption and network latency associated with the introduction of Sidecar are also within reasonable limits, as you can see from the service mesh benchmark performance tests .\nExtending the Istio service mesh The next question is about extending the Istio service mesh. The current solution given by the Istio community is to use WebAssembly , an extension that is still relatively little used in production by now and has performance concerns. Most of the answers I’ve observed are CRDs that build a service mesh management plane based on Istio.\nAlso, making Istio support heterogeneous environments for all workloads, such as virtual machines and containers, is in strong demand for end-users. It allows them to migrate applications from traditional loads to cloud native easily. Finally, hybrid cloud traffic management for multiple clusters and meshes is a more advanced requirement.\nSteep learning curve Many people complain that Istio has too little learning material. Istio has been open source for four years, and there are a lot of learning resources now:\nIstio Documentation IstioCon 2021 Istio Big Talk/Istio Weekly Istio Fundamentals Course Certified Istio Administrator Yes, Istio is complex, but it’s been getting more and more manageable with every release. In my next blog, I will introduce you to two open source projects that extend Istio and give you some insight into what’s going on in the Istio community.\n","relpermalink":"/en/blog/the-debate-in-the-community-about-istio-and-service-mesh/","summary":"There is no shortage of debate in the community about the practicability of service mesh and Istio – here’s a list of common questions and concerns, and how to address them.","title":"The debate in the community about Istio and service mesh"},{"content":"If you don’t know what Istio is, you can read my previous articles below:\nWhat Is Istio and Why Does Kubernetes Need it? Why do you need Istio when you already have Kubernetes? This article will explore the relationship between service mesh and cloud native.\nService mesh – the product of the container orchestration war If you’ve been following the cloud-native space since its early days, you’ll remember the container orchestration wars of 2015 to 2017. Kubernetes won the container wars in 2017, the idea of microservices had taken hold, and the trend toward containerization was unstoppable. Kubernetes architecture matured and slowly became boring, and service mesh technologies, represented by Linkerd and Istio, entered the CNCF-defined cloud-native critical technologies on the horizon.\nKubernetes was designed with the concept of cloud-native in mind. A critical idea in cloud-native is the architectural design of microservices. When a single application is split into microservices, how can microservices be managed to ensure the SLA of the service as the number of services increases? The service mesh was born to solve this problem at the architectural level, free programmers’ creativity, and avoid tedious service discovery, monitoring, distributed tracing, and other matters.\nThe service mesh takes the standard functionality of microservices down to the infrastructure layer, allowing developers to focus more on business logic and thus speed up service delivery, which is consistent with the whole idea of cloud-native. You no longer need to integrate bulky SDKs in your application, develop and maintain SDKs for different languages, and just use the service mesh for Day 2 operations after the application is deployed.\nThe service mesh is regarded as the next generation of microservices. In the diagram, we can see that many of the concerns of microservices overlap with the functionality of Kubernetes. Kubernetes focuses on the application lifecycle, managing resources and deployments with little control over services. The service mesh fills this gap. The service mesh can connect, control, observe and protect microservices.\nKubernetes vs. xDS vs. Istio This diagram shows the layered architecture of Kubernetes and Istio.\nKubernetes vs xDS vs Istio The diagram indicates that the kube-proxy settings are global and cannot be controlled at a granular level for each service. All Kubernetes can do is topology-aware routing, routing traffic closer to the Pod, and setting network policies in and out of the Pod.\nIn contrast, the service mesh takes traffic control out of the service layer in Kubernetes through sidecar proxies, injects proxies into each Pod, and manipulates these distributed proxies through a control plane. It allows for more excellent resiliency.\nKube-proxy implements traffic load balancing between multiple pod instances of a Kubernetes service. But how do you finely control the traffic between these services — such as dividing the traffic by percentage to different application versions (which are all part of the same service, but on other deployments), or doing canary releases and blue-green releases?\nThe Kubernetes community gives a way to do canary releases using Deployment, assigning different pods to deployed services by modifying the pod’s label.\nEnvoy Architecture Currently, the most popular open-source implementation of service mesh in the world is Istio. From the CNCF Survey Report 2020 , we know that Istio is the most used service mesh in production today. Many companies have built their service mesh based on Istio, such as Ant, Airbnb, eBay, NetEase, Tencent, etc.\nCNCF Survey Report 2020 Figure from CNCF Survey Report 2020 Istio is developed based on Envoy, which has been used by default as its distributed proxy since the first day it was open-sourced. Envoy pioneered the creation of the xDS protocol for distributed gateway configuration, greatly simplifying the configuration of large-scale distributed networks. Ant Group open source MOSN also supported xDS In 2019. Envoy was also one of the first projects to graduate from CNCF, tested by large-scale production applications.\nService mesh – the cloud-native networking infrastructure With the above comparison between Kubernetes and service mesh in mind, we can see the place of service mesh in the cloud-native application architecture. That is, building a cloud-native network infrastructure specifically provides:\nTraffic management: controlling the flow of traffic and API calls between services, making calls more reliable, and enhancing network robustness in different environments. Observability: understanding the dependencies between services and the nature and flow of traffic between them provides the ability to identify problems quickly. Policy enforcement: controlling access policies between services by configuring the mesh rather than by changing the code. Service Identification and Security: providing service identifiability and security …","relpermalink":"/en/blog/service-mesh-an-integral-part-of-cloud-native-apps/","summary":"This article will explore the relationship between service mesh and cloud native.","title":"Service Mesh - an integral part of cloud-native applications"},{"content":"API gateways have been around for a long time as the entry point for clients to access the back-end, mainly to manage “north-south” traffic, In recent years, service mesh architectures have become popular, mainly for managing internal systems,(i.e. “east-west” traffic), while a service mesh like Istio also has built-in gateways that bring traffic inside and outside the system under unified control. This often creates confusion for first-time users of Istio. What is the relationship between the service mesh and the API gateway? How does Istio’s gateway work? What are the ways to expose the services in the Istio mesh? This article gives you the answer.\nKey Insights The service mesh was originally created to solve the problem of managing internal traffic for distributed systems, but API gateways existed long before it. While the Gateway is built into Istio, you can still use a custom Ingress Controller to proxy external traffic. API gateways and service mesh are converging. How do I expose services in the Istio mesh? The following diagram shows four approaches to expose services in the Istio mesh using Istio Gateway, Kubernetes Ingress, API Gateway, and NodePort/LB.\nExposing services through Istio Ingress Gateway The Istio mesh is shaded, and the traffic in the mesh is internal (east-west) traffic, while the traffic from clients accessing services within the Kubernetes cluster is external (north-south) traffic.\nApproach Controller Features NodePort/LoadBalancer Kubernetes Load balancing Kubernetes Ingress Ingress controller Load balancing, TLS, virtual host, traffic routing Istio Gateway Istio Load balancing, TLS, virtual host, advanced traffic routing, other advanced Istio features API Gateway API Gateway Load balancing, TLS, virtual host, advanced traffic routing, API lifecycle management, billing, rate limiting, policy enforcement, data aggregation Since NodePort/LoadBalancer is a basic way to expose services built into Kubernetes, this article will not discuss that option. Each of the other three approaches will be described below.\nUsing Kubernetes Ingress to expose traffic We all know that clients of a Kubernetes cluster cannot directly access the IP address of a pod because the pod is in a network plane built into Kubernetes. We can expose services inside Kubernetes outside the cluster using NodePort or Load Balancer Kubernetes service type. To support virtual hosting, hiding and saving IP addresses, you can use Ingress resources to expose services in Kubernetes.\nKubernetes Ingress to expose services Ingress is a Kubernetes resource that controls the behavior of an ingress controller that does the traffic touring, which is the equivalent of a load-balanced directional proxy server such as Nginx, Apache, etc., which also includes rule definitions, i.e., routing information for URLs, which is provided by the Ingress controller .\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: istio name: ingress spec: rules: - host: httpbin.example.com http: paths: - path: /status/* backend: serviceName: httpbin servicePort: 8000 The kubernetes.io/ingress.class: istio annotation in the example above indicates that the Ingress uses the Istio Ingress Controller which in fact uses Envoy proxy.\nUsing Istio Gateway to expose services Istio is a popular service mesh implementation that has evolved from Kubernetes that implements some features that Kubernetes doesn’t. (See What is Istio and why does Kubernetes need Istio? ) It makes traffic management transparent to the application, moving this functionality from the application to the platform layer and becoming a cloud-native infrastructure.\nIstio used Kubernetes Ingress as the traffic portal in versions prior to Istio 0.8, where Envoy was used as the Ingress Controller. From Istio 0.8 and later, Istio created the Gateway object. Gateway and VirtualService are used to represent the configuration model of Istio Ingress, and the default implementation of Istio Ingress uses the same Envoy proxy. In this way, the Istio control plane controls both the ingress gateway and the internal sidecar proxy with a consistent configuration model. These configurations include routing rules, policy enforcement, telemetry, and other service control functions.\nThe Istio Gateway resources function similarly to the Kubernetes Ingress in that it is responsible for north-south traffic to and from the cluster. The Istio Gateway acts as a load balancer to carry connections to and from the edge of the service mesh. The specification describes a set of open ports and the protocols used by those ports, as well as the SNI configuration for load balancing, etc.\nThe Istio Gateway resource itself can only be configured for L4 through L6, such as exposed ports, TLS settings, etc.; however, the Gateway can be bound to a VirtualService, where routing rules can be configured on L7, such as versioned traffic routing, fault injection, HTTP redirects, HTTP …","relpermalink":"/en/blog/istio-servicemesh-api-gateway/","summary":"What is the relationship between the service mesh and the API gateway? How does Istio’s gateway work? What are the ways to expose the services in the Istio mesh? This article gives you the answer.","title":"Using Istio service mesh as API Gateway"},{"content":"Do you have multiple Kubernetes clusters and a service mesh? Do your virtual machines and services in a Kubernetes cluster need to interact? This article will take you through the process and considerations of building a hybrid cloud using Kubernetes and an Istio Service Mesh. Together, Kubernetes and Istio can be used to bring hybrid workloads into a mesh and achieve interoperability for multicluster. But another layer of infrastructure — a management plane — is helpful for managing multicluster or multimesh deployments.\nKubernetes Using Kubernetes enables rapid deployment of a distributed environment that enables cloud interoperability and unifies the control plane on the cloud. It also provides resource objects, such as Service, Ingress and Gateway , to handle application traffic. The Kubernetes API Server communicates with the kube-proxy component on each node in the cluster, creates iptables rules for the node, and forwards requests to other pods.\nAssuming that a client now wants to access a service in Kubernetes, the request is first sent to the Ingress/Gateway, then forwarded to the backend service (Service A in the diagram below) based on the routing configuration in the Ingress/Gateway. Then Service A polls an instance of Service B for the traffic requested by Service B. Lastly, the traffic requested by Service A for Service B is polled forward to Service B’s instance.\nKubernetes Kubernetes Multicluster The most common usage scenarios for multicluster management include:\nservice traffic load balancing isolating development and production environments decoupling data processing and data storage cross-cloud backup and disaster recovery flexible allocation of compute resources low-latency access to services across regions avoiding vendor lock-in There are often multiple Kubernetes clusters within an enterprise; and the KubeFed implementation of Kubernetes cluster federation developed by Multicluster SIG enables multicluster management capabilities, which allows all Kubernetes clusters to be managed through the same interface.\nThere are several general issues that need to be addressed when using cluster federation:\nConfiguring which clusters need to be federated API resources need to be propagated across the clusters Configuring how API resources are distributed to different clusters Registering DNS records in clusters to enable service discovery across clusters The following is a multicluster architecture for KubeSphere — one of the most commonly used Kubernetes multicluster management architectures — where the Host Cluster serves as the control plane with two member clusters, West and East.\nMulticluster The Host Cluster needs to be able to access the API Server of the Member Cluster, but the network connectivity between Member Clusters is not required. The Host Cluster is independent of the Member Cluster it manages and the Member Cluster is not aware of the existence of the Host Cluster. The advantage of this is that when the control plane fails, the Member Cluster will not be affected and the deployed load can still operate normally without being affected.\nThe Host Cluster also assumes the role of API portal, and the Host Cluster forwards the resource requests to the Member Cluster — which is convenient for aggregation and also facilitates unified authority authentication. We see that there is a Federation Control Plane in the Host Cluster, where the Push Reconciler propagates the identity, role, and role binding from the Federation Cluster to all Member Clusters.\nIstio Service Mesh Consider using the Istio service mesh when we have multilingual, multiversion microservices running in Kubernetes and need finer-grained canary publishing and unified security policy management for inter-service observability. Istio enables intelligent application-aware load balancing from the application layer to other Service Mesh-enabled services in the cluster, by transparently intercepting all traffic to and from the application using IPTables, and bypassing the primary kube-proxy load balancing. The Istio control plane communicates with the Kubernetes API Server to obtain information about all registered services in the cluster.\nThe following diagram illustrates the basics of Istio, where all nodes belong to the same Kubernetes cluster.\nIstio Service Mesh You may end up with at least a few Kubernetes clusters, each hosting microservices. Multiple deployment models exist for Istio’s multicluster deployments — depending on network isolation, primary and backup — which can be specified by declaration when deploying using Istio Operator. Communication between these microservices in a cluster can be enhanced by a service mesh. Within the cluster, Istio provides common communication patterns to improve resiliency, security and observability.\nAll of the above is about application load management on Kubernetes, but for legacy applications on virtual machines: how can they be managed in the same plane? Istio supports …","relpermalink":"/en/blog/multicluster-management-with-kubernetes-and-istio/","summary":"This article explains three patterns/tools for debugging microservices in Kubernetes and the changes brought by the introduction of Istio for debugging microservices.","title":"Multicluster Management with Kubernetes and Istio"},{"content":"Kubernetes is arguably the best environment for running microservices so far, but the experience of debugging microservices in a Kubernetes environment may not be as user-friendly. This article will show you how to debug microservices in Kubernetes, introduce common tools, and explain how the introduction of Istio impacts debugging microservices.\nDebugging microservices is vastly different from traditional monolithic applications The debugging of microservices has been a long-standing problem for software developers. This challenge does not exist in traditional monolithic applications because developers can leverage the debugger in IDEs to add breakpoints, modify environment variables, single-step execution, etc. for their applications, all of which provide great help in software debugging. With the popularity of Kubernetes, the debugging of microservices becomes a thorny issue, where the following issues are more complicated than the debugging of traditional monolithic applications.\nMultiple dependencies A microservice often depends on multiple other microservices, some shared volumes across multiple microservices, and authorizations based on service accounts. When debugging a microservice, how do you deploy other dependent services to quickly build a latest set of staging environments?\nAccess from a local machine When microservices are running on a developer’s local computer, there is usually no direct access to the services in a Kubernetes cluster. How can you debug microservices deployed in a Kubernetes cluster as if they were local services?\nSlow development loop Usually, it takes a long process to update the code and build it into an image before pushing it to the cluster. How do you speed up the development cycle? Let’s look at the tools that address those challenges.\nTools The main solutions for debugging microservices in Kubernetes are:\nProxy: by building a VPN, deploying a proxy in the Kubernetes cluster, and adding local debug endpoints to make the services in Kubernetes directly accessible to local applications, your architecture will look like [ local service ] \u0026lt;-\u0026gt; [ proxy ] \u0026lt;-\u0026gt; [ app in Kubernetes ]. Sidecar: Inject a sidecar into the pod of the microservice to be debugged to intercept all traffic to and from the service, so that the service can be tracked and monitored, and the service can also be debugged in this sidecar. Service Mesh: To get an overall picture of the application, inject sidecars into all microservices so that you can get a dashboard that monitors global status. Here are three typical open source projects that implement the above solutions, each of which can help you debug microservices from a different perspective. You can apply them at different stages of software development and they can be said to be complementary to each other.\nProxy – debugging microservices with Telepresence Telepresence is essentially a local proxy that proxies data volumes, environment variables, and networks in a Kubernetes cluster locally. The following diagram shows the main usage scenarios for Telepresence.\nProxy mode: Telepresence Users need to manually execute the telepresence command locally, which will automatically deploy the agent to Kubernetes. Once the agent has been deployed,\nLocal services will have complete access to other services in the Kubernetes cluster, environment variables, Secret, ConfigMap, etc. Services in the cluster also have direct access to the locally exposed endpoints. However, this approach requires users to run multiple commands while debugging locally, and in some network environments it may not be possible to establish a VPN connection to the Kubernetes cluster.\nSidecar – debugging microservices with Nocalhost Nocalhost is a Kubernetes-based cloud development environment. To use it, you just need to install a plugin in your IDE – VS Code to extend Kubernetes and shorten the development feedback cycle. The development environment can be isolated by creating different namespaces for different users and using ServiceAccount when binding to different user corners. Nocalhost also provides a web console and API for administrators to manage different development environments.\nSidecar mode: Nocalhost As long as you have a Kubernetes cluster and have admin rights to the cluster, you can refer to the Nocalhost documentation to quickly start trying it out. To use the Nocalhost plugin in VS Code, you need to configure the Kubernetes cluster in the plugin first.\nSelect the Kubeconfig file you just exported or copy and paste the contents of the file directly into the configuration. Then select the service you need to test and select the corresponding Dev Container. VS Code will automatically open a new code window. Here is an example of the bookinfo sample provided by Istio. You can open the cloned code in your local IDE and click the hammer next to the code file to enter development mode. Selecting the corresponding DevContainer and Nocalhost will automatically inject a …","relpermalink":"/en/blog/how-to-debug-microservices-in-kubernetes-with-proxy-sidecar-or-service-mesh/","summary":"This article explains three patterns/tools for debugging microservices in Kubernetes and the changes brought by the introduction of Istio for debugging microservices.","title":"How to debug microservices in Kubernetes with proxy, sidecar or service mesh?"},{"content":"Istio was named by Tetrate founder Varun Talwar and Google lead engineer Louis Ryan in 2017 and was open sourced on May 24, 2017. Today is the fourth anniversary of Istio’s open source arrival. Let’s take a look back at Istio’s four years of development — and look forward to Istio’s future.\nIstio’s open source history In 2017, the year Kubernetes ended the container orchestration battle, Google took the opportunity to consolidate its dominance in the cloud native space and compensate for Kubernetes’ disadvantage in service-to-service traffic management by open-sourcing Istio. Istio released its 1.10 last week — but here are some of the most important releases in Istio’s history to date.\nDate Version Note May 24, 2017 0.1 Officially open source; established the architectural foundation of Control Plane, Data Plane and sidecar proxy. October 10, 2017 0.2 Started to support multiple runtime environments, such as virtual machines. June 1, 2018 0.8 API refactoring July 31, 2018 1.0 Production-ready, after which the Istio team underwent a massive reorganization. March 19, 2019 1.1 Enterprise-ready. Support for multiple Kubernetes clusters, with performance optimizations. March 3, 2020 1.5 Back to monolith, with microservice components merged into istiod, making Istio’s architecture cleaner and easier to maintain. Support for WebAssembly extension, making Istio’s ecology much stronger. November 18, 2020 1.8 Officially deprecated Mixer and focused on adding support for virtual machines. A year after its inception– and two months before the 1.0 release, version 0.8 was released with a massive refactoring of the API. In late July 2018, when 1.0 was released, Istio reached a production-ready tipping point. Since then, Google has massively reorganized the Istio team and several Istio-based service mesh startups were born, making 2018 the booming year of the service mesh industry.\nIstio 1.1 was released in March 2019, almost 9 months after 1.0 was released, which is far beyond the average release cycle of an open-source project. We know that the speed of iteration and evolution is a core competency of basic software. Since then, Istio has started a regular release cadence of one version per quarter and has become the #4 fastest growing project in GitHub’s top 10 in 2019 !\nThe Istio community In 2020, Istio’s project management began to mature and its governance reached a stage of evolution. We saw the first election of a steering committee for the Istio community and the transfer of the trademark to Open Usage Commons . The first IstioCon was successfully held in February 2021, with thousands of people attending the online conference. There is also a large Istio community in China , and face-to-face Istio community meetups will be held there in 2021. Stay tuned for more.\nAccording to the CNCF 2020 Survey, 46% of organizations were either using a service mesh in production or planning to use it in the next 12 months. Istio was the top used mesh among those using a mesh in production.\nThe future After 4 years of development, there is not only a large user base around Istio, but also several Istio vendors, as you can see on the homepage of the recently revamped Istio website. In the last few releases, Istio has shifted its development focus to improving the Day 2 Operation experience. We also expect to see more Istio adoption path recommendations, case studies, learning materials, training, and certifications (such as the industry’s first Certified Istio Administrator from Tetrate) that will facilitate the adoption of Istio.\n","relpermalink":"/en/blog/istio-4-year-birthday/","summary":"Today is Istio's 4 year birthday, let’s take a look back at Istio’s four years of development — and look forward to Istio’s future.","title":"Happy Istio 4th Anniversary -- Retrospect and Outlook"},{"content":"Istio, the most popular service mesh implementation , was developed on top of Kubernetes and has a different niche in the cloud native application ecosystem than Kubernetes. Rather than introduce you directly to what Istio has to offer, this article will explain how Istio came about and what it is in relation to Kubernetes.\nWhy Is There an Istio? To explain what Istio is, it’s also important to understand the context in which Istio came into being — i.e., why is there an Istio?\nMicroservices are a technical solution to an organizational problem. And Kubernetes/Istio are a technical solution to deal with the issues created by moving to microservices. As a deliverable for microservices, containers solve the problem of environmental consistency and allow for more granularity in limiting application resources. They are widely used as a vehicle for microservices.\nGoogle open-sourced Kubernetes in 2014, which grew exponentially over the next few years. It became a container scheduling tool to solve the deployment and scheduling problems of distributed applications — allowing you to treat many computers as though they were one computer. Because the resources of a single machine are limited and Internet applications may have traffic floods at different times (due to rapid expansion of user scale or different user attributes), the elasticity of computing resources needs to be high. A single machine obviously can’t meet the needs of a large-scale application; and conversely, it would be a huge waste for a very small-scale application to occupy the whole host.\nIn short, Kubernetes defines the final state of the service and enables the system to reach and stay in that state automatically. So how do you manage the traffic on the service after the application has been deployed? Below we will look at how service management is done in Kubernetes and how it has changed in Istio.\nHow Do You Do Service Management in Kubernetes? The following diagram shows the service model in Kubernetes:\nKubernetes Service Model From the above figure we can see that:\nDifferent instances of the same service may be scheduled to different nodes. Kubernetes combines multiple instances of a service through Service objects to unify external services. Kubernetes installs a kube-proxy component in each node to forward traffic, which has simple load balancing capabilities. Traffic from outside the Kubernetes cluster can enter the cluster via Ingress (Kubernetes has several other ways of exposing services; such as NodePort, LoadBalancer, etc.). Kubernetes is used as a tool for intensive resource management. However, after allocating resources to the application, Kubernetes doesn’t fully solve the problems of how to ensure the robustness and redundancy of the application, how to achieve finer-grained traffic division (not based on the number of instances of the service), how to guarantee the security of the service, or how to manage multiple clusters, etc.\nThe Basics of Istio The following diagram shows the service model in Istio, which supports both workloads and virtual machines in Kubernetes.\nIstio From the diagram we can see that:\nIstiod acts as the control plane, distributing the configuration to all sidecar proxies and gateways. (Note: for simplification, the connections between Istiod and sidecar are not drawn in the diagram.) Istio enables intelligent application-aware load balancing from the application layer to other mesh enabled services in the cluster, and bypasses the rudimentary kube-proxy load balancing. Application administrators can manipulate the behavior of traffic in the Istio mesh through a declarative API, in the same way they manage workloads in Kubernetes. It can take effects within seconds and they can do this without needing to redeploy. Ingress is replaced by Gateway resources, a special kind of proxy that is also a reused Sidecar proxy. A sidecar proxy can be installed in a virtual machine to bring the virtual machine into the Istio mesh. In fact, before Istio one could use SpringCloud, Netflix OSS, and other tools to programmatically manage the traffic in an application, by integrating the SDK in the application. Istio makes traffic management transparent to the application, moving this functionality out of the application and into the platform layer as a cloud native infrastructure.\nIstio complements Kubernetes, by enhancing its traffic management, observability and security for cloud native applications. The service mesh open source project — launched in 2017 by Google, IBM and Lyft — has come a long way in three years. A description of Istio’s core features can be found in the Istio documentation .\nSummary Service Mesh is the cloud native equivalent of TCP/IP, addressing application network communication, security and visibility issues. Istio is currently the most popular service mesh implementation, relying on Kubernetes but also scalable to virtual machine loads. Istio’s core consists of a control plane and a data …","relpermalink":"/en/blog/what-is-istio-and-why-does-kubernetes-need-it/","summary":"This article will explain how Istio came about and what it is in relation to Kubernetes.","title":"What Is Istio and Why Does Kubernetes Need it?"},{"content":"If you’ve heard of service mesh and tried Istio , you may have the following questions:\nWhy is Istio running on Kubernetes? What is the role of Kubernetes and a service mesh in the cloud native application architecture, respectively? What aspects of Kubernetes does Istio extend? What problems does it solve? What is the relationship between Kubernetes, Envoy, and Istio? This article will take you through the inner workings of Kubernetes and Istio. In addition, I will introduce the load balancing approach in Kubernetes, and explain why you need Istio when you have Kubernetes.\nKubernetes is essentially about application lifecycle management through declarative configuration, while a service mesh is essentially about providing inter-application traffic, security management and observability. If you have already built a stable application platform using Kubernetes, how do you set up load balancing and traffic control for calls between services? This is where a service mesh comes into the picture.\nEnvoy introduces the xDS protocol, which is supported by various open source software, such as Istio , MOSN , etc. Envoy contributes xDS to a service mesh or cloud native infrastructure. Envoy is essentially a modern version of a proxy that can be configured through APIs, based on which many different usage scenarios are derived — such as API Gateway, sidecar proxy in service mesh, and edge proxy.\nThis article contains the following:\nA description of the role of kube-proxy. The limitations of Kubernetes for microservice management. An introduction to the capabilities of Istio service mesh. A comparison of some of the concepts in Kubernetes, Envoy, and the Istio service mesh. Kubernetes vs Service Mesh The following diagram shows the service access relationship in Kubernetes and service mesh (one sidecar per pod model).\nKubernetes vs Service Mesh Traffic Forwarding Each node in a Kubernetes cluster deploys a kube-proxy component that communicates with the Kubernetes API Server, gets information about the services in the cluster, and then sets iptables rules to send requests for service directly to the corresponding Endpoint (a pod belonging to the same group of services).\nService Discovery Service Discovery Istio can follow the service registration in Kubernetes and can also interface with other service discovery systems via platform adapters in the control plane; and then generate data plane configurations (using CRD, which are stored in etcd) with transparent proxies for the data plane. The transparent proxy of the data plane is deployed as a sidecar container in the pod of each application service, and all these proxies need to request the control plane to synchronize the proxy configuration. The proxy is “transparent” because the application container is completely unaware of the presence of the proxy. The kube-proxy component in the process needs to intercept traffic as well, except that the kube-proxy intercepts traffic to and from the Kubernetes node — while the sidecar proxy intercepts traffic to and from the pod.\nDisadvantages of a Service Mesh Since Kubernetes has many pods running on each node, putting the original kube-proxy route forwarding function in each pod will increase the response latency — due to more hops when the sidecar intercepts the traffic — and consume more resources. In order to manage traffic in a fine-grained manner, a series of new abstractions will be added. This will further increase the learning cost for users, but as the technology becomes more popular this situation will be slowly alleviated.\nAdvantages of a Service Mesh The kube-proxy settings are global and cannot be controlled at a granular level for each service, while service mesh takes the traffic control out of the service layer in Kubernetes by means of sidecar proxy — allowing for more elasticity.\nShortcomings of Kube-Proxy First, it does not automatically try another pod if the forwarded pod is not serving properly. Each pod has a health check mechanism and when a pod has health problems, kubelet will restart the pod and kube-proxy will remove the corresponding forwarding rules. Also, nodePort-type services cannot add TLS or more complex message routing mechanisms.\nKube-proxy implements load balancing of traffic across multiple pod instances of a Kubernetes service, but how do you do fine-grained control of traffic between these services — such as dividing traffic by percentage to different application versions (which are all part of the same service but on different deployments), or doing canary releases (grayscale releases) and blue-green releases?\nThe Kubernetes community gives a way to do canary releases using Deployment , which is essentially a way to assign different pods to a deployment’s service by modifying the pod’s label.\nKubernetes Ingress vs. Istio Gateway As mentioned above, kube-proxy can only route traffic within a Kubernetes cluster. The pods of a Kubernetes cluster are located in a network created by CNI. …","relpermalink":"/en/blog/why-do-you-need-istio-when-you-already-have-kubernetes/","summary":"This article will take you through the inner workings of Kubernetes and Istio. In addition, I will introduce the load balancing approach in Kubernetes, and explain why you need Istio when you have Kubernetes.","title":"Why Do You Need Istio When You Already Have Kubernetes?"},{"content":"Tetrate Academy has recently released the Istio Fundamentals Course, which is now available for free. Sign up at Tetrate Academy now!\nCourse curriculum Here is the curriculum:\nService Mesh and Istio Overview Installing Istio Observability: Telemetry and Logs Traffic Management Security Advanced Features Troubleshooting Real World Examples There are self-assessment questions at the end of each course. I have passed the course, and here is the certificate after passing the course.\nTetrate Academy Istio Fundamentals Course More In the future, Tetrate will release the Certified Istio Administrator (CIA) exam and welcome all Istio users and administrators to follow and register for it.\n","relpermalink":"/en/notice/tetrate-istio-fundamental-courses/","summary":"Tetrate Academy has recently released the Istio Fundamentals Course, which is now available for free。","title":"Tetrate Academy Releases Free Istio Fundamentals Course"},{"content":"Different companies or software providers have devised countless ways to control user access to functions or resources, such as Discretionary Access Control (DAC), Mandatory Access Control (MAC), Role-Based Access Control (RBAC), and Attribute-Based Access Control (ABAC). In essence, whatever the type of access control model, three basic elements can be abstracted: user, system/application, and policy.\nIn this article, we will introduce ABAC, RBAC, and a new access control model — Next Generation Access Control (NGAC) — and compare the similarities and differences between the three, as well as why you should consider NGAC.\nWhat Is RBAC? RBAC, or Role-Based Access Control, takes an approach whereby users are granted (or denied) access to resources based on their role in the organization. Every role is assigned a collection of permissions and restrictions, which is great because you don’t need to keep track of every system user and their attributes. You just need to update appropriate roles, assign roles to users, or remove assignments. But this can be difficult to manage and scale. Enterprises that use the RBAC static role-based model have experienced role explosion: large companies may have tens of thousands of similar but distinct roles or users whose roles change over time, making it difficult to track roles or audit unneeded permissions. RBAC has fixed access rights, with no provision for ephemeral permissions or for considering attributes like location, time, or device. Enterprises using RBAC have had difficulty meeting the complex access control requirements to meet regulatory requirements of other organizational needs.\nRBAC Example Here’s an example Role in the “default” namespace in Kubernetes that can be used to grant read access to pods:\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [\u0026#34;v1\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] What Is ABAC? ABAC stands for Attribute-Based Access Control. At a high level, NIST defines ABAC as an access control method “where subject requests to perform operations on objects are granted or denied based on assigned attributes of the subject, environment conditions, and a set of policies that are specified in terms of those attributes and conditions.” ABAC is a fine-grained model since you can assign any attributes to the user, but at the same time it becomes a burden and hard to manage:\nWhen defining permissions, the relationship between users and objects cannot be visualized. If the rules are a little complex or confusingly designed, it will be troublesome for the administrator to maintain and trace. This can cause performance problems when there is a large number of permissions to process.\nABAC Example Kubernetes initially uses ABAC as access control and is configured via JSON Lines, for example:\nAlice can just read pods in namespace “foo”:\n{\u0026#34;apiVersion\u0026#34;: \u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: {\u0026#34;user\u0026#34;: \u0026#34;alice\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;pods\u0026#34;, \u0026#34;readonly\u0026#34;: true}} What Is NGAC? NGAC, or Next Generation Access Control, takes the approach of modeling access decision data as a graph. NGAC enables a systematic, policy-consistent approach to access control, granting or denying users administrative capabilities with a high level of granularity. NGAC was developed by NIST (National Institute of Standards and Technology) and is currently used in Tetrate Q and Tetrate Service Bridge .\nThere are several types of entities; they represent the resources you want to protect, the relationships between them, and the actors that interact with the system. The entities are:\nUsers Objects User attributes, such as organization unit Object attributes, such as folders Policy classes, such as file system access, location, and time NIST’s David Ferraiolo and Tetrate ‘s Ignasi Barrera shared how NGAC works at their presentation on Next Generation Access Control at Service Mesh Day 2019 in San Francisco.\nNGAC is based on the assumption that you can represent the system you want to protect in a graph that represents the resources you want to protect and your organizational structure, in a way that has meaning to you and that adheres to your organization semantics. On top of this model that is very particular to your organization, you can overlay policies. Between the resource model and the user model, the permissions are defined. This way NGAC provides an elegant way of representing the resources you want to protect, the different actors in the system, and how both worlds are tied together with permissions.\nNGAC DAG Image via Linear Time Algorithms to Restrict Insider Access using Multi-Policy Access Control Systems NGAC Example The following example shows a simple NGAC graph with a User DAG representing an organization structure, an Object DAG representing files and folders in a filesystem, a categorization of the files, and two different policies — file system and scope — …","relpermalink":"/en/blog/why-you-should-choose-ngac-as-your-access-control-model/","summary":"This article will introduce you to the next generation permission control model, NGAC, and compare ABAC, RABC, and explain why you should choose NGAC.","title":"Why You Should Choose NGAC as Your Access Control Model"},{"content":" IstioCon 2021 poster（Jimmy Song） Topic: Service Mesh in China Time: February 23rd, 10:00 - 10:10 am Beijing time How to participate: IstioCon 2021 website Cost: Free From February 22-25, Beijing time, the Istio community will be hosting the first IstioCon online, and registration is free to attend! I will be giving a lightning talk on Tuesday, February 23rd (the 12th day of the first month of the lunar calendar), as an evangelist and witness of Service Mesh technology in China, I will introduce the Service Mesh industry and community in China.\nI am a member of the inaugural IstioCon organizing committee with Zhonghu Xu (Huawei) and Shaojun Ding (Intel), as well as the organizer of the China region. Considering Istio’s large audience in China, we have arranged for Chinese presentations that are friendly to the Chinese time zone. There will be a total of 14 sharing sessions in Chinese, plus dozens more in English. The presentations will be in both lightning talk (10 minutes) and presentation (40 minutes) formats.\nJoin the Cloud Native Community Istio SIG to participate in the networking at this conference. For the schedule of IstioCon 2021, please visit IstioCon 2021 official website , or click for details.\n","relpermalink":"/en/notice/istiocon-2021/","summary":"IstioCon 2021, I'll be giving a lightning talk, February 22nd at 10am BST.","title":"IstioCon 2021 Lightning Talk Preview"},{"content":"The ServiceMesher website has lost connection with the webhook program on the web publishing server because the GitHub where the code is hosted has has been “lost” and the hosting server is temporarily unable to log in, so the site cannot be updated. Today I spent a day migrating all the blogs on ServiceMesher to the Cloud Native Community website cloudnative.to , and as of today, there are 354 blogs on the Cloud Native Community.\nServiceMesher blogs Now we plan to archive ServiceMesher official GitHub (all pages under the servicemesher.com domain) We are no longer accepting new PRs, so please submit them directly to the Cloud Native Community . Thank you all!\n","relpermalink":"/en/notice/servicemesher-blog-merged/","summary":"ServiceMesher website is no longer maintained, plan to archive the website code, the blog has been migrated to Cloud Native Community, please submit the new blog to Cloud Native Community.","title":"ServiceMesher website is no longer maintained, the original blog has been migrated to the cloud native community"},{"content":"In this article, I’ll give you an overview of Istio ‘s history of virtual machine integration support. In particular, the introduction of the smart DNS proxy and WorkloadGroup in Istio 1.8, which makes virtual machines and containers equivalent at the resource abstraction level.\nI will show you a tumultuous odyssey of Istio’s virtual machine integration. Tetrate, the enterprise service mesh company that made pushing Istio to run everywhere part of its founding mission, has used VM features extensively in customer deployments and has been instrumental in pushing VMs to Istio upstream.\nPreface In my previous article , I talked about how Istio 1.7 supported virtual machines. But at that time, late October, virtual machines were still not seamlessly integrated into Istio — there was still a lot of manual work required. Now, Istio 1.8 has added WorkloadGroup and smart DNS proxy, which allows non-Kubernetes workloads like VMs to become first-class citizens in Istio — just like pods.\nWith or without a sidecar installed for virtual machines, until 1.7 you could not resolve the DNS name of a Kubernetes service unless a kube-external DNS was configured — which is the last piece of virtual machine integration in Istio. This shortcoming has finally been fixed in Istio 1.8.\nWhy Is Virtual Machine Support Important? In the process of migrating our applications to cloud native architectures and continuously containerizing them, we will go through three phases as shown in the figure below.\nCloud Native Stages Stage 1: All applications are deployed on virtual machines Stage 2: Applications are deployed on both virtual machines and containers, are migrating from virtual machines to containers, and are using Kubernetes to manage containers. Stage 3: All applications are deployed in containers first, using Kubernetes to manage containers and Istio to manage service-to-service communication. The above diagram is artificially simplified: in reality, there might be multiple hybrid clouds, multiple regions, multiple clusters, etc. Plus, at stage 3 containers and virtual machines may remain in long-term coexistence, but the trend of containerization remains unchanged.\nIstio’s History of Virtual Machine Support Istio’s support for virtual machines is a long process, an odyssey of sorts.\n0.2: Istio Mesh Expansion As of version 0.2, Istio added virtual machines to the Mesh via Istio Mesh Expansion , provided that the following prerequisites were met.\nVirtual machines must have direct access to the application’s pods via IP address, which requires a flat network between the container and the VM via VPC or VPN; and virtual machines do not need access to the Cluster IP, but rather direct access to the service’s endpoints. Virtual machines must have access to Istio’s control plane services (Pilot, Mixer, CA, now being integrated as Istiod), which can expose the control plane endpoints to virtual machines by deploying load balancers in the Istio Mesh. (optional) the virtual machine has access to the DNS server inside the Mesh (deployed in Kubernetes). The steps to integrate a virtual machine are as follows.\nCreate an internal load balancer for the Istio control plane service and the DNS service for the Kubernetes cluster. Generate a configuration file for the Istio Service CIDR, Service Account token, security certificate, and IP of the Istio Control Plane Service (the IP exposed through the Internal Load Balancer) and send it to the virtual machine. Setup the Istio component, dnsmaq (for DNS discovery), in the virtual machine; so that the virtual machine can access the services in the mesh using FQDN, to ensure that the virtual machine can correctly resolve the Cluster IP of the services in the mesh. To run the service in a virtual machine, you need to configure the sidecar, add inbound ports to be intercepted, then restart Istio and also run istioctl to register the service. The following figure shows the detailed flow from integrating a virtual machine to accessing services in the virtual machine in a mesh.\nFigure 1 Figure 1\nThe DNS is hijacked by dnsmasq deployed in the virtual machine, which allows it to correctly obtain the Cluster IP of the Istio service (Kubernetes’ built-in DNS). Access to Kubernetes’ built-in DNS service (which is exposed outside the cluster via the Internal Load Balancer and can be accessed directly). Return the Cluster IP resolved by productpage.bookinfo.svc.cluster.local, noting that the IP address is not directly accessible, but failure to be DNS resolved will result in a failed VM request for the service. The virtual machine’s call to services in a mesh is hijacked by the sidecar proxy. Since the proxy is connected to the Istio control plane, the endpoints of the service can be queried via xDS, so traffic will be forwarded to one of the endpoints. To access VM services in mesh, you need to manually add VM services to mesh using the istioctl register command, which essentially registers the VM services to the …","relpermalink":"/en/blog/istio-18-a-virtual-machine-integration-odyssey/","summary":"In this article, I’ll give you an overview of Istio‘s history of virtual machine integration support. In particular, the introduction of the smart DNS proxy and WorkloadGroup in Istio 1.8, which makes virtual machines and containers equivalent at the resource abstraction level.","title":"Istio 1.8: A Virtual Machine Integration Odyssey"},{"content":"A service mesh is a relatively simple concept, consisting of a bunch of network proxies paired with each service in an application, plus a set of task management processes. The proxies are called the data plane and the management processes are called the control plane in the Service Mesh. The data plane intercepts calls between different services and “processes” them; the control plane is the brain of the mesh that coordinates the behavior of proxies and provides APIs for operations and maintenance personnel to manipulate and observe the entire network.\nThe diagram below shows the architecture of a service mesh.\nService Mesh Architecture Further, the service mesh is a dedicated infrastructure layer designed to enable reliable, fast, and secure inter-service invocation in microservices architectures. It is not a mesh of “services” but rather a mesh of “proxies” that services can plug into, thus abstracting the network from the application code. In a typical service mesh, these proxies are injected into each service deployment as a sidecar (and also may be deployed at the edge of the mesh). Instead of invoking services directly over the network, services invoke their local sidecar proxy, which in turn manages requests on behalf of the service, pushing the complexities of inter-service communications into a networking layer that can resolve them at scale. The set of interconnected sidecar proxies implements a so-called data plane, while on the other hand the service mesh control plane is used to configure proxies. The infrastructure introduced by a service mesh provides an opportunity, too, to collect metrics about the traffic that is flowing through the application.\nThe architecture of a service mesh The infrastructure layer of a service mesh is divided into two main parts: the control plane and the data plane.\nCharacteristics of the control plane\nDo not parse packets directly. Communicates with proxies in the control plane to issue policies and configurations. Visualizes network behavior. Typically provides APIs or command-line tools for configuration versioning and management for continuous integration and deployment. Characteristics of the data plane\nIs usually designed with the goal of statelessness (though in practice some data needs to be cached to improve traffic forwarding performance). Directly handles inbound and outbound packets, forwarding, routing, health checking, load balancing, authentication, authentication, generating monitoring data, etc. Is transparent to the application, i.e., can be deployed senselessly. Changes brought by the service mesh Decoupling of microservice governance from business logic\nA service mesh takes most of the capabilities in the SDK out of the application, disassembles them into separate processes, and deploys them in a sidecar model. By separating service communication and related control functions from the business process and synching them to the infrastructure layer, a service mesh mostly decouples them from the business logic, allowing application developers to focus more on the business itself.\nNote that the word “mostly” is mentioned here and that the SDK often needs to retain protocol coding and decoding logic, or even a lightweight SDK to implement fine-grained governance and monitoring policies in some scenarios. For example, to implement method-level call distributed tracing, the service mesh requires the business application to implement trace ID passing, and this part of the implementation logic can also be implemented through a lightweight SDK. Therefore, the service mesh is not zero-intrusive from a code level.\nUnified governance of heterogeneous environments\nWith the development of new technologies and staff turnover, there are often applications and services in different languages and frameworks in the same company, and in order to control these services uniformly, the previous practice was to develop a complete set of SDKs for each language and framework, which is very costly to maintain. With a service mesh, multilingual support is much easier by synching the main service governance capabilities to the infrastructure. By providing a very lightweight SDK, and in many cases, not even a separate SDK, it is easy to achieve unified traffic control and monitoring requirements for multiple languages and protocols.\nFeatures of service mesh Service mesh also has three major technical advantages over traditional microservice frameworks.\nObservability\nBecause the service mesh is a dedicated infrastructure layer through which all inter-service communication passes, it is uniquely positioned in the technology stack to provide uniform telemetry at the service invocation level. This means that all services are monitored as “black boxes.” The service mesh captures route data such as source, destination, protocol, URL, status codes, latency, duration, etc. This is essentially the same data that web server logs can provide, but the service mesh captures this data for …","relpermalink":"/en/blog/what-is-a-service-mesh/","summary":"This article will take you through what a service mesh is, as well as its architecture, features, and advantages and disadvantages.","title":"What is a service mesh?"},{"content":"1.8 is the last version of Istio to be released in 2020 and it has the following major updates:\nSupports installation and upgrades using Helm 3. Mixer was officially removed. Added Istio DNS proxy to transparently intercept DNS queries from applications. WorkloadGroup has been added to simplify the integration of virtual machines. WorkloadGroup is a new API object. It is intended to be used with non-Kubernetes workloads like Virtual Machines and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies.\nInstallation and Upgrades Istio starts to officially support the use of Helm v3 for installations and upgrades. In previous versions, the installation was done with the istioctl command-line tool or Operator. With version 1.8, Istio supports in-place and canary upgrades with Helm.\nEnhancing Istio’s Usability The istioctl command-line tool has a new bug reporting feature (istioctl bug-report ), which can be used to collect debugging information and get cluster status.\nThe way to install the add-on has changed: 1.7 istioctl is no longer recommended and has been removed in 1.8, to help solve the problem of add-on lagging upstream and to make it easier to maintain.\nTetrate is an enterprise service mesh company. Our flagship product, TSB, enables customers to bridge their workloads across bare metal, VMs, K8s, \u0026amp; cloud at the application layer and provide a resilient, feature-rich service mesh fabric powered by Istio, Envoy, and Apache SkyWalking.\nMixer, the Istio component that had been responsible for policy controls and telemetry collection, has been removed. Its functionalities are now being served by the Envoy proxies. For extensibility, service mesh experts recommend using WebAssembly (Wasm) to extend Envoy; and you can also try the GetEnvoy Toolkit , which makes it easier for developers to create Wasm extensions for Envoy. If you still want to use Mixer, you must use version 1.7 or older. Mixer continued receiving bug fixes and security fixes until Istio 1.7. Many features supported by Mixer have alternatives as specified in the Mixer Deprecation document, including the in-proxy extensions based on the Wasm sandbox API.\nSupport for Virtual Machines Istio’s recent upgrades have steadily focused on making virtual machines first-class citizens in the mesh. Istio 1.7 made progress to support virtual machines and Istio 1.8 adds a smart DNS proxy , which is an Istio sidecar agent written in Go. The Istio agent on the sidecar will come with a cache that is dynamically programmed by Istiod DNS Proxy. DNS queries from applications are transparently intercepted and served by an Istio proxy in a pod or VM that intelligently responds to DNS query requests, enabling seamless multicluster access from virtual machines to the service mesh.\nIstio 1.8 adds a WorkloadGroup , which describes a collection of workload instances. It provides a specification that the workload instances can use to bootstrap their proxies, including the metadata and identity. It is only intended to be used with non-k8s workloads like Virtual Machines, and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies. Using WorkloadGroups, Istio has started to help automate VM registration with istioctl experimental workload group .\nTetrate , the enterprise service mesh company, uses these VM features extensively in customers’ multicluster deployments, to enable sidecars to resolve DNS for hosts exposed at ingress gateways of all the clusters in a mesh; and to access them over mutual TLS.\nConclusion All in all, the Istio team has kept the promise made at the beginning of the year to maintain a regular release cadence of one release every three months since the 1.1 release in 2018, with continuous optimizations in performance and user experience for a seamless experience of brownfield and greenfield apps on Istio. We look forward to more progress from Istio in 2021.\n","relpermalink":"/en/blog/istio-1-8-a-smart-dns-proxy-takes-support-for-virtual-machines-a-step-further/","summary":"WorkloadGroup is a new API object. It is intended to be used with non-Kubernetes workloads like Virtual Machines and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies.","title":"Istio 1.8: A Smart DNS Proxy Takes Support for Virtual Machines a Step Further"},{"content":"Istio is a popular service mesh to connect, secure, control, and observe services. When it was first introduced as open source in 2017, Kubernetes was winning the container orchestration battle and Istio answered the needs of organizations moving to microservices. Although Istio claims to support heterogeneous environments such as Nomad, Consul, Eureka, Cloud Foundry, Mesos, etc., in reality, it has always worked best with Kubernetes — on which its service discovery is based.\nIstio was criticized for a number of issues early in its development, for the large number of components, the complexity of installation and maintenance, the difficulty of debugging, a steep learning curve due to the introduction of too many new concepts and objects (up to 50 CRDs), and the impact of Mixer components on performance. But these issues are gradually being overcome by the Istio team. As you can see from the roadmap released in early 2020, Istio has come a long way.\nBetter integration of VM-based workloads into the mesh is a major focus for the Istio team this year. Tetrate also offers seamless multicloud connectivity, security, and observability, including for VMs, via its product Tetrate Service Bridge . This article will take you through why Istio needs to integrate with virtual machines and how you can do so.\nWhy Should Istio Support Virtual Machines? Although containers and Kubernetes are now widely used, there are still many services deployed on virtual machines and APIs outside of the Kubernetes cluster that needs to be managed by Istio mesh. It’s a huge challenge to unify the management of the brownfield environment with the greenfield.\nWhat Is Needed to Add VMs to the Mesh? Before the “how,” I’ll describe what is needed to add virtual machines to the mesh. There are a couple of things that Istio must know when supporting virtual machine traffic: which VMs have services that should be part of the mesh, and how to reach the VMs. Each VM also needs an identity, in order to communicate securely with the rest of the mesh. These requirements could work with Kubernetes CRDs, as well as a full-blown Service Registry like Consul. And the service account based identity bootstrapping could work as a mechanism for assigning workload identities to VMs that do not have a platform identity. For VMs that do have a platform identity (like EC2, GCP, Azure, etc.), work is underway in Istio to exchange the platform identity with a Kubernetes identity for ease of setting up mTLS communication.\nHow Does Istio Support Virtual Machines? Istio’s support for virtual machines starts with its service registry mechanism. The information about services and instances in the Istio mesh comes from Istio’s service registries, which up to this point have only looked at or tracked pods. In newer versions, Istio now has resource types to track and watch VMs. The sidecars inside the mesh cannot observe and control traffic to services outside the mesh, because they do not have any information about them.\nThe Istio community and Tetrate have done a lot of work on Istio’s support for virtual machines. The 1.6 release included the addition of WorkloadEntry, which allows you to describe a VM exactly as you would a host running in Kubernetes. In 1.7, the release started to add the foundations for bootstrapping VMs into the mesh automatically through tokens, with Istio doing the heavy lifting. Istio 1.8 will debut another abstraction called WorkloadGroup, which is similar to a Kubernetes Deployment object — but for VMs.\nThe following diagram shows how Istio models services in the mesh. The predominant source of information comes from a platform service registry like Kubernetes, or a system like Consul. In addition, the ServiceEntry serves as a user-defined service registry, modeling services on VMs or external services outside the organization.\nWhy install Istio in a virtual machine when you can just use ServiceEntry to bring in the services in the VMs?\nUsing ServiceEntry, you can enable services inside the mesh to discover and access external services; and in addition, manage the traffic to those external services. In conjunction with VirtualService, you can also configure access rules for the corresponding external service — such as request timeouts, fault injection, etc. — to enable controlled access to the specified external service.\nEven so, it only controls the traffic on the client-side, not access to the introduced external service to other services. That is, it cannot control the behavior of the service as the call initiator. Deploying sidecars in a virtual machine and introducing the virtual machine workload via workload selector allows the virtual machine to be managed indiscriminately, like a pod in Kubernetes.\nFuture As you can see from the bookinfo demo , there is too much manual work involved in the process and it’s easy to go wrong. In the future, Istio will improve VM testing to be realistic, automate bootstrapping based on platform identity, …","relpermalink":"/en/blog/how-to-integrate-virtual-machines-into-istio-service-mesh/","summary":"Better integration of virtual machine-based workloads into the service mesh is a major focus for the Istio team this year, and Tetrate also provides seamless multi-cloud connectivity, security and observability, including for virtual machines, through its product Tetrate Service Bridge. This article will show you why Istio needs to integrate with virtual machines and how.","title":"How to Integrate Virtual Machines into Istio Service Mesh"},{"content":"Today is my 914th day and also the last day with Ant Group , tomorrow is September 1st, which is usually the day school starts, and everyone at Alibaba is known as “classmate”, tomorrow I will join Tetrate , and that’s kind of starting my new semester!\nAnt/Alibaba and the Cloud Native Community To date, Ant/Alibaba Group has had a profound impact on my career, especially its corporate culture and values, and the Alibaba recruiting philosophy of “finding like-minded people”, and isn’t the process of creating the Cloud Native Community also a process of finding like-minded people? Cloud Native Community is like a small society, I don’t want it to have much social value, but only want it to make a small but beautiful change to individuals, to enterprises and to society. I constantly think about myself as an individual and as an employee, especially as an initiator of the community. What is my mission as an individual, an employee, and especially as an initiator of a community? What role should I play in the company? Where is this community going? I’m fumbling along, but because of your support, it makes me stronger and more committed to the adoption and application of cloud native technology in China, outside of me I may have gone faster, but now with the community together we will go further!\n24 June 2019, Shanghai, KubeCon China 2019 June 24, 2019, Shanghai, KubeCon China 2019\nJoining Tetrate Over the past two years, I’ve been working hard to promote Istio and Service Mesh technology, and with funding from Ant Group, I started the ServiceMesher Community to bring Service Mesh technology to China. Next I want to bring Chinese practice to the world.\nAs a Developer Advocate, the most important thing is not to stop learning, but to listen and take stock. Over the past two years, I’ve seen a lot of people show interest in Service Mesh, but not enough to understand the risks and lack of knowledge about the new technology. I’m excited to join this Service Mesh-focused startup Tetrate , a global telecommuting startup with products built around open source Istio , [Envoy](https:/ /envoyproxy.io) and Apache SkyWalking , it aims to make it to be the cloud native network infrastructure. Here are several maintainers of these open source projects, such as Sheng Wu , Zack Butcher , Lizan Zhou , etc., and I believe that working with them can help you understand and apply Service Mesh quickly and effectively across cloud native.\nMore Earlier this year as I was preparing for the Cloud Native community, I set the course for the next three years - cloud native, open source and community. The road to pursue my dream is full of thorns, not only need courage and perseverance, but also need you to be my strong backing, I will overcome the thorns and move forward. Open source belongs to the world, to let the world understand us better, we must be more active into the world. I hope that China’s open source tomorrow will be better, I hope that Service Mesh technology will be better applied by the enterprises in China, I hope that cloud native can benefit the public, and I hope that we can all find our own mission.\nWe are hiring now, if you are interested with Tetrate , please send your resume to careers@tetrate.io .\n","relpermalink":"/en/blog/moving-on-from-ant-group/","summary":"Today is my last day at Ant and tomorrow I'm starting a new career at Tetrate.","title":"New Beginning - Goodbye Ant, Hello Tetrate"},{"content":"Just tonight, the jimmysong.io website was moved to the Alibaba Cloud Hong Kong node. This is to further optimize the user experience and increase access speed. I purchased an ECS on the Alibaba Cloud Hong Kong node, and now I have a public IP and can set subdomains. The website was previously deployed on GitHub Pages, the access speed is average, and it has to withstand GitHub instability. Impact (In recent years, GitHub downtime has occurred).\nMeanwhile, the blog has also done a lot to improve the site, thanks to Bai Jun away @baijunyao strong support, a lot of work for the revision of the site, including:\nChanged the theme color scheme and deepened the contrast Use aligolia to support full site search Optimized mobile display Articles in the blog have added zoom function Added table of contents to blog post This site is built on the theme of educenter .\nThanks to the majority of netizens who have supported this website for several years. The website has been in use for more than three years and has millions of visits. It has undergone two major revisions before and after, on January 31, 2020 and October 8, 2017, respectively. And changed the theme of the website. In the future, I will share more cloud-native content with you as always, welcome to collect, forward, and join the cloud-native community to communicate with the majority of cloud-native developers.\n","relpermalink":"/en/notice/migrating-to-alibaba-cloud/","summary":"Move the website to the Alibaba Cloud Hong Kong node to increase the speed of website access and the convenience of obtaining public IP and subdomain names.","title":"Move to Alibaba Cloud Hong Kong node"},{"content":" Just the other day, Java just celebrated its 25th birthday , and from the time of its birth it was called “write once, run everywhere”, but more than 20 years later, there is still a deep gap between programming and actual production delivery. the world of IT is never short of concepts, and if a concept doesn’t solve the problem, then it’s time for another layer of concepts. it’s been 6 years since Kubernetes was born, and it’s time for the post-Kubernetes era - the era of cloud-native applications!\nCloud Native Stage This white paper will take you on a journey to explore the development path of cloud-native applications in the post-Kubernetes era.\nHighlights of the ideas conveyed include.\nCloud-native has passed through a savage growth period and is moving towards uniform application of standards. Kubernetes’ native language does not fully describe the cloud-native application architecture, and the development and operation functions are heavily coupled in the configuration of resources. Operator’s expansion of the Kubernetes ecosystem has led to the fragmentation of cloud-native applications, and there is an urgent need for a unified application definition standard. The essence of OAM is to separate the R\u0026amp;D and O\u0026amp;M concerns in the definition of cloud-native applications, and to further abstract resource objects, simplify and encompass everything. “Kubernetes Next Generation” refers to the fact that after Kubernetes became the infrastructure layer standard, the focus of the cloud-native ecology is being overtaken by the application layer, and the last two years have been a powerful exploration of the hot Service Mesh process, and the era of cloud-native application architecture based on Kubernetes is coming. Kubernetes has become an established operating platform for cloud-native applications, and this white paper will expand with Kubernetes as the default platform, including an explanation of the OAM-based hierarchical model for cloud-native applications.\n","relpermalink":"/en/notice/guide-to-cloud-native-app/","summary":"Take you on a journey through the post-Kubernetes era of cloud-native applications.","title":"Guide to Cloud Native Application"},{"content":"At the beginning of 2020, due to the outbreak of the Crona-19 pandemic, employees around the world began to work at home. Though the distance between people grew farer, there was a group of people, who were us working in the cloud native area, gathered together for a common vision. During the past three months, we have set up the community management committee and used our spare time working together to complete the preparatory work for the community. Today we are here to announce the establishment of the Cloud Native Community.\nBackground Software is eating the world. —— Marc Andreessen\nThis sentence has been quoted countless times, and with the rise of Cloud Native, we’d like to talk about “Cloud Native is eating the software.” As more and more enterprises migrate their services to the cloud, the original development mode of enterprises cannot adapt to the application scenarios in the cloud, and it is being reshaped to conform to the cloud native standard.\nSo what is cloud native? Cloud native is a collection of best practices in architecture, r\u0026amp;d process and team culture to support faster innovation, superior user experience, stable and reliable user service and efficient r\u0026amp;d. The relationship between the open source community and the cloud native is inseparable. It is the existence of the open source community, especially the end user community, that greatly promotes the continuous evolution of cloud native technologies represented by container, service mesh and microservices.\nCNCF (Cloud Native Computing Foundation) holds Cloud Native conference every year in the international community, which has a wide audience and great influence. But it was not held in China for the first time until 2018, after several successful international events. However, there are no independent foundations or neutral open source communities in China. In recent years, many cloud native enthusiasts in China have set up many communication groups and held many meetups, which are very popular. Many excellent open source projects have emerged in the cloud native field, but there is no organized neutral community for overall management. Under this background, the Cloud Native Community emerges at the right moment.\nAbout Cloud Native Community is an open source community with technology, temperature and passion. It was founded spontaneously by a group of industry elites who love open source and uphold the principle of consensus, co-governance, co-construction and sharing. The aim of the community is: connection, neutral, open source. We are based in China, facing the world, enterprise neutrality, focusing on open source, and giving feedback to open source.\nIntroduction for the Steering Community：https://cloudnative.to/en/team/ .\nYou will gain the followings after joining the community:\nKnowledge and news closer to the source A more valuable network More professional and characteristic consultation Opportunities to get closer to opinion leaders Faster and more efficient personal growth More knowledge sharing and exposure opportunities More industry talent to be found Contact Contact with us.\nEmail: mailto:contact@cloudnative.to Twitter: https://twitter.com/CloudNativeTo ","relpermalink":"/en/notice/cloud-native-community-announecement/","summary":"Today the Community Steering Committee announced the official formation of the Cloud Native Community.","title":"Establishment of the Cloud Native Community"},{"content":"This article is a rework of previously written content and is included in the Istio Handbook of the ServiceMesher community . Other chapters are still being compiled.\nPeople who have just heard of Service Mesh and tried Istio may have the following questions:\nWhy does Istio bind Kubernetes? What roles do Kubernetes and Service Mesh play in cloud native? What aspects of Kubernetes has Istio extended? What problems have been solved? What is the relationship between Kubernetes, xDS protocols (Envoy , MOSN, etc) and Istio? Should I use Service Mesh? In this section, we will try to guide you through the internal connections between Kubernetes, the xDS protocol, and Istio Service Mesh. In addition, this section will also introduce the load balancing methods in Kubernetes, the significance of the xDS protocol for Service Mesh, and why Istio is needed in time for Kubernetes.\nUsing Service Mesh is not to say that it will break with Kubernetes, but that it will happen naturally. The essence of Kubernetes is to perform application lifecycle management through declarative configuration, while the essence of Service Mesh is to provide traffic and security management and observability between applications. If you have built a stable microservice platform using Kubernetes, how do you set up load balancing and flow control for calls between services?\nThe xDS protocol created by Envoy is supported by many open source software, such as Istio , Linkerd , MOSN, etc. Envoy’s biggest contribution to Service Mesh or cloud native is the definition of xDS. Envoy is essentially a proxy. It is a modern version of proxy that can be configured through APIs. Based on it, many different usage scenarios are derived, such as API Gateway, Service Mesh. Sidecar proxy and Edge proxy in.\nThis section contains the following\nExplain the role of kube-proxy. Kubernetes’ limitations in microservice management. Describe the features of Istio Service Mesh. Describe what xDS includes. Compare some concepts in Kubernetes, Envoy and Istio Service Mesh. Key takeaways If you want to know everything in advance, here are some of the key points from this article:\nThe essence of Kubernetes is application lifecycle management, specifically deployment and management (scaling, scaling, automatic recovery, release). Kubernetes provides a scalable and highly resilient deployment and management platform for microservices. The foundation of Service Mesh is a transparent proxy. After the traffic between microservices is intercepted through sidecar proxy, the behavior of microservices is managed through the control plane configuration. Service Mesh decoupled from Kubernetes traffic management, the internal flow without the need of Service Mesh kube-proxy supporting components, micro-services closer to abstract the application layer by, for traffic between management services, security and observability. xDS defines the protocol standards for Service Mesh configuration. Service Mesh is a higher-level abstraction of services in Kubernetes. Its next step is serverless. Kubernetes vs Service Mesh The following figure shows the service access relationship between Kubernetes and Service Mesh (one sidecar per pod mode).\nkubernetes vs service mesh Traffic forwarding\nEach node of the cluster Kubernetes a deployed kube-proxy assembly Kubernetes API Server may communicate with the cluster acquired service information, and then set iptables rules, sends a request for a service directly to the corresponding Endpoint (belonging to the same group service pod).\nService discovery\nService registration in Service Mesh Istio Service Mesh can use the service in Kubernetes for service registration. It can also connect to other service discovery systems through the platform adapter of the control plane, and then generate the configuration of the data plane (using CRD statements, stored in etcd), a transparent proxy for the data plane. (Transparent proxy) is deployed in the sidecar container in each application service pod. These proxy need to request the control plane to synchronize the proxy configuration. The reason why is a transparent proxy, because there is no application container fully aware agent, the process kube-proxy components like the need to block traffic, but kube-proxythat blocks traffic to Kubernetes node and sidecar proxy that blocks out of the Pod For more information, see Understanding Route Forwarding by the Envoy Sidecar Proxy in Istio Service Mesh .\nDisadvantages of Service Mesh\nBecause each node on Kubernetes many runs Pod, the original kube-proxyrouting forwarding placed in each pod, the distribution will lead to a lot of configuration, synchronization, and eventual consistency problems. In order to perform fine-grained traffic management, a series of new abstractions will be added, which will further increase the user’s learning costs. However, with the popularization of technology, this situation will gradually ease.\nAdvantages of Service Mesh\nkube-proxy The …","relpermalink":"/en/blog/service-mesh-the-microservices-in-post-kubernetes-era/","summary":"This article is a rework of previously written content and is included in the Istio Handbook of the ServiceMesher community . Other chapters are still being compiled.","title":"Service Mesh - the Microservices in Post Kubernetes Era"},{"content":"2020 is indeed a bad start. In less than a month, I hardly heard any good news:\nTrump assassinated Major General Sulaymani of Iran; this pneumonia outbreak in Wuhan; the news that my basketball icon Kobe died of a helicopter crash really shocked me, and the Lakers said goodbye on the 24th. This gave me another spiritual blow during the Spring Festival, which was originally lacking in interest.\n2020 is bound to be a year deeply remembered in all humankind. In the last few days of the first month of the year, I decided to revise the website. The first reason was that I could n’t go out during the extended Chinese New Year holiday, and it was boring at home. And many pictures on the website were saved on Weibo map beds. The picture bed is unstable, causing many photos to be irretrievable; coupled with a habit of organizing the website every long holiday (the last revision of the website was completed during the National Day holiday in 2018, completed at home for 7 days), so I decided to The website has been revised again and it has become what it is now.\nFeatures The website has the following features after this revision:\nReorganize the website content, the structure is more reasonable Support email subscription Images are stored on Github Responsive static website, card design, better user experience Everyone is welcome to enter your email address in the email input box at the bottom of the page. Once there is an important content update on this site, we will push you through the email as soon as possible.\n","relpermalink":"/en/notice/website-revision-notice/","summary":"In the last days of the first month of 2020, I decided to revamp the website.","title":"jimmysong.io website revision notice"},{"content":"A few days ago during the Mid-Autumn Festival, I translated Google’s Engineering Practices documentation , which is open source on Github. The original document Github address: https://github.com/google/eng-practices , the main content so far is summarized by Google How to conduct a Code Review guide, based on the title of the original Github repository, we will add more Google engineering practices in the future.\nGithub：https://github.com/rootsongjc/eng-practices Browse online: https://jimmysong.io/eng-practices The translation uses the same style and directory structure as the original document. In fact, if the original text has international requirements, it can be directly incorporated, but according to the translation suggestions submitted by several previous people, the author of this project does not recommend translation. The first is translation. The documents may be unmaintained, and the accuracy of the documents cannot be guaranteed.\nFor suggestions on Chinese integration, see:\nAdd Chinese translation #12 Add the link of Chinese version of code reviewer’s guide #8 ","relpermalink":"/en/notice/google-engineering-practices-zh/","summary":"Translated from Google's open source documentation on Github","title":"Chinese version of Google Engineering Practice Documents"},{"content":"The following paragraph is a release note from the Istio official blog https://istio.io/zh/blog/2019/announcing-1.1/ , which I translated.\nIstio was released at 4 a.m. Beijing time today and 1 p.m. Pacific time.\nSince the 1.0 release last July, we have done a lot to help people get Istio into production. We expected to release a lot of patches (six patches have been released so far!), But we are also working hard to add new features to the product.\nThe theme for version 1.1 is “Enterprise Ready”. We are happy to see more and more companies using Istio in production, but as some big companies join in, Istio also encounters some bottlenecks.\nThe main areas we focus on include performance and scalability. As people gradually put Istio into production and use larger clusters to run more services with higher capacity, there may be some scaling and performance issues. Sidecar takes up too much resources and adds too much latency. The control plane (especially Pilot) consumes excessive resources.\nWe put a lot of effort into making the data plane and control plane more efficient. In the 1.1 performance test, we observed that sidecars typically require 0.5 vCPU to process 1000 rps. A single Pilot instance can handle 1000 services (and 2000 pods) and consumes 1.5 vCPUs and 2GB of memory. Sidecar adds 5 milliseconds at the 50th percentile and 10 milliseconds at the 99th percentile (the execution strategy will increase latency).\nWe have also completed the work of namespace isolation. You can use the Kubernetes namespace to enforce control boundaries to ensure that teams do not interfere with each other.\nWe have also improved multi-cluster functionality and usability. We listened to the community and improved the default settings for flow control and policies. We introduced a new component called Galley. Galley validates YAML configuration, reducing the possibility of configuration errors. Galley is also used in multi-cluster setups-collecting service discovery information from each Kubernetes cluster. We also support other multi-cluster topologies, including single control planes and multiple synchronous control planes, without the need for flat network support.\nSee the release notes for more information and details .\nThere is more progress on this project. As we all know, Istio has many moving parts, and they take on too much work. To address this, we have recently established the Usability Working Group (available at any time). A lot happened in the community meeting (Thursday at 11 am) and in the working group. You can log in to discuss.istio.io with GitHub credentials to participate in the discussion!\nThanks to everyone who has contributed to Istio over the past few months-patching 1.0, adding features to 1.1, and extensive testing recently on 1.1. Special thanks to companies and users who work with us to install and upgrade to earlier versions to help us identify issues before they are released.\nFinally, go to the latest documentation and install version 1.1! Happy meshing!\nOfficial website The ServiceMesher community has been maintaining the Chinese page of the official Istio documentation since the 0.6 release of Istio . As of March 19, 2019, there have been 596 PR merges, and more than 310 documents have been maintained. Thank you for your efforts! Some documents may lag slightly behind the English version. The synchronization work is ongoing. For participation, please visit https://github.com/servicemesher/istio-official-translation. Istio official website has a language switch button on the right side of each page. You can always Switch between Chinese and English versions, you can also submit document modifications, report website bugs, etc.\nServiceMesher Community Website\nThe ServiceMesher community website http://www.servicemesher.com covers all technical articles in the Service Mesh field and releases the latest activities in a timely manner. It is your one-stop portal to learn about Service Mesh and participate in the community.\n","relpermalink":"/en/notice/istio-11/","summary":"Istio 1.1 was released at 4 am on March 20th, Beijing time. This version took 8 months! The ServiceMesher community also launched the Istio Chinese documentation.","title":"Istio 1.1 released"},{"content":"Istio handbook was originally an open source e-book I created (see https://jimmysong.io/istio-handbook ). It has been written for 8 months before donating to the ServiceMesher community. In order to further popularize Istio and Service Mesh technology, this book Donate to the community for co-authoring. The content of the original book was migrated to https://github.com/servicemesher/istio-handbook on March 10, 2019. The original book will no longer be updated.\nGitHub address: https://github.com/servicemesher/istio-handbook Reading address online: http://www.servicemesher.com/istio-handbook/ Conceptual picture of this book, cover photo of Shanghai Jing’an Temple at night , photo by Jimmy Song .\nThe publishing copyright of this book belongs to the blog post of Electronic Industry Press. Please do not print and distribute it without authorization.\nIstio is a service mesh framework jointly developed by Google, IBM, Lyft, etc., and began to enter the public vision in early 2017. As an important infrastructure layer that inherits Kubernetes and connects to the serverless architecture in the cloud-native era, Istio is of crucial importance important. The ServiceMesher community, as one of the earliest open source communities in China that is researching and promoting Service Mesh technology, decided to integrate community resources and co-author an open source e-book for readers.\nAbout this book This book originates from the rootsongjc / istio-handbook and the Istio knowledge map created by the ServiceMesher community .\nThis book is based on Istio 1.0+ and includes, but is not limited to , topics in the Istio Knowledge Graph .\nParticipate in the book Please refer to the writing guidelines of this book and join the Slack channel discussion after joining the ServiceMesher community .\n","relpermalink":"/en/notice/istio-handbook-by-servicemesher/","summary":"To further popularize Istio and Service Mesh technology, donate this book to the community for co-authoring.","title":"Donate Istio Handbook to the ServiceMesher community"},{"content":"Github: https://github.com/rootsongjc/cloud-native-sandbox Cloud Native Sandbox can help you setup a standalone Kubernetes and istio environment with Docker on you own laptop.\nThe sandbox integrated with the following components:\nKubernetes v1.10.3 Istio v1.0.4 Kubernetes dashboard v1.8.3 Differences with kubernetes-vagrant-centos-cluster As I have created the kubernetes-vagrant-centos-cluster to set up a Kubernetes cluster and istio service mesh with vagrantfile which consists of 1 master(also as node) and 3 nodes, but there is a big problem that it is so high weight and consume resources. So I made this light weight sandbox.\nFeatures\nNo VirtualBox or Vagrantfile required Light weight High speed, low drag Easy to operate Services\nAs the sandbox setup, you will get the following services.\nRecord with termtosvg .\nPrerequisite You only need a laptop with Docker Desktop installed and Kubernetes enabled .\nNote: Leave enough resources for Docker Desktop. At least 2 CPU, 4G memory.\nInstall To start the sandbox, you have to run the following steps.\nKubernetes dashboard(Optional) Install Kubernetes dashboard.\nkubectl apply -f install/dashbaord/ Get the dashboard token.\nkubectl -n kube-system describe secret default| awk \u0026#39;$1==\u0026#34;token:\u0026#34;{print $2}\u0026#39; Expose kubernetes-dashboard service.\nkubectl -n kube-system get pod -l k8s-app=kubernetes-dashboard -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39; Login to Kubernetes dashboard on http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login with the above token.\nIstio(Required) Install istio service mesh with the default add-ons.\n# Install istio kubectl apply -f install/istio/ To expose service grafana on http://localhost:3000 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 3000:3000 \u0026amp; To expose service prometheus on http://localhost:9090 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=prometheus -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 9090:9090 \u0026amp; To expose service jaeger on http://localhost:16686 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=jaeger -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 16686:16686 \u0026amp; To expose service servicegraph on http://localhost:8088/dotviz , http://localhost:8088/force/forcegraph.html .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=servicegraph -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 8088:8088 \u0026amp; Kiali Install kiali .\nkubectl -n istio-system apply -f install/kiali To expose service kiali on http://localhost:20001 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=kiali -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 20001:20001 \u0026amp; Bookinfo sample Deploy bookinfo sample .\n# Enable sidecar auto injection kubectl label namespace default istio-injection=enabled # Deploy bookinfo sample kubectl -n default apply -f sample/bookinfo Visit productpage on http://localhost/productpage .\nLet’s generate some loads.\nfor ((i=0;i\u0026lt;1000;i=i+1));do echo \u0026#34;Step-\u0026gt;$i\u0026#34;;curl http://localhost/productpage;done You can watch the service status through http://localhost:3000 .\nClient tools To operate the applications on Kubernetes, you should install the following tools.\nRequired\nkubectl - Deploy and manage applications on Kubernetes. istioctl - Istio configuration command line utility. Optional\nkubectx - Switch faster between clusters and namespaces in kubectl kube-ps1 - Kubernetes prompt info for bash and zsh ","relpermalink":"/en/blog/cloud-native-sandbox/","summary":"A standalone Kubernetes and Istio environment with Docker on you own laptop","title":"Cloud Native Sandbox"},{"content":"This video was recorded on the last day of 2018 and demonstrates the use of rootsongjc/kubernetes-vagrant-centos-cluster to automatically deploy a Kubernetes cluster and Istio Service Mesh.\nA few days ago I mentioned that kubernetes-vagrant-centos-cluster released v1.2.0 version to deploy a cloud-native experimental environment with one click. Someone in the Kubernetes and Service Mesh community asked me a long time ago to make a video to explain and demonstrate how to install Kubernetes and Istio Service Mesh, because I’m always busy, I’ve always made some time. Today, I will give a demo video, just don’t watch the video for a few minutes. In order to make this video, it took me half an hour to record, two hours to edit, and many years of shooting. , Editing, containers, virtual machines, Kubernetes, service grid experience. This is not so much a farewell as a new beginning.\nBecause the video was first posted on YouTube , it was explained in English (just a few supplementary instructions, it does n’t matter if you do n’t understand, just read the Chinese documentation on GitHub ).\nSkip to bilibli to watch . If you are interested in drone aerial photography, you can also take a look at Jimmy Song’s aerial photography works . Please support the coin or like, thank you.\nIf you have any questions, you can send a barrage or comment below the video.\nPS. Some people will ask why you chose to use bilibli, because there are no ads for watching videos on this platform, and most of them are uploaded by the Up master. Although the two-dimensional elements are mostly, the community atmosphere is still good.\nFor more exciting videos, visit Jimmy Song’s bibli homepage .\n","relpermalink":"/en/notice/cloud-native-kubernetes-service-mesh-local-demo-show/","summary":"This video was recorded by me on the last day of 2018 and demonstrates the use of rootsongjc/kubernetes-vagrant-centos-cluster to automatically deploy Kubernetes clusters and Istio Service Mesh.","title":"Kubernetes and Istio Service Mesh Cloud Native Local Video Demo Show"},{"content":"Updated at Mar 8, 2022\nThis article uses Istio’s official bookinfo sample to explain how Envoy performs routing forwarding after the traffic entering the Pod and forwarded to Envoy sidecar by iptables, detailing the inbound and outbound processing. For a detailed analysis of traffic interception, see Understanding Envoy Sidecar Proxy Injection and Traffic Interception in Istio Service Mesh .\nOverview of Sidecar Injection and Traffic Interception Steps Below is an overview of the steps from Sidecar injection, Pod startup to Sidecar proxy interception traffic and Envoy processing routing.\nKubernetes automatically injected through Admission Controller, or the user run istioctl command to manually inject sidecar container. Apply the YAML configuration deployment application. At this time, the service creation configuration file received by the Kubernetes API server already includes the Init container and the sidecar proxy. Before the sidecar proxy container and application container are started, the Init container started firstly. The Init container is used to set iptables (the default traffic interception method in Istio, and can also use BPF, IPVS, etc.) to Intercept traffic entering the pod to Envoy sidecar Proxy. All TCP traffic (Envoy currently only supports TCP traffic) will be Intercepted by sidecar, and traffic from other protocols will be requested as originally. Launch the Envoy sidecar proxy and application container in the Pod. Sidecar proxy and application container startup order issues\nStart the sidecar proxy and the application container. Which container is started first? Normally, Envoy Sidecar and the application container are all started up before receiving traffic requests. But we can’t predict which container will start first, so does the container startup order have an impact on Envoy hijacking traffic? The answer is yes, but it is divided into the following two situations.\nCase 1: The application container starts first, and the sidecar proxy is still not ready\nIn this case, the traffic is transferred to the 15001 port by iptables, and the port is not monitored in the Pod. The TCP link cannot be established and the request fails.\nCase 2: Sidecar starts first, the request arrives and the application is still not ready\nIn this case, the request will certainly fail. As for the step at which the failure begins, the reader is left to think.\nQuestion : If adding a readiness and living probe for the sidecar proxy and application container can solve the problem?\nTCP requests that are sent or received from the Pod will be hijacked by iptables. After the inbound traffic is hijacked, it is processed by the Inbound Handler and then forwarded to the application container for processing. The outbound traffic is hijacked by iptables and then forwarded to the Outbound Handler for processing. Upstream and Endpoint. Sidecar proxy requests Pilot to use the xDS protocol to synchronize Envoy configurations, including LDS, EDS, CDS, etc., but to ensure the order of updates, Envoy will use ADS to request configuration updates from Pilot directly. How Envoy handles route forwarding The following figure shows a productpageservice access request http://reviews.default.svc.cluster.local:9080/, when traffic enters reviews the internal services, reviews internal services Envoy Sidecar is how to do traffic blocked the route forward.\nIstio transparent traffic hijacking and traffic routing schematic Before the first step, productpage Envoy Sidecar Pod has been selected by EDS of a request to reviews a Pod service of its IP address, it sends a TCP connection request.\nThe Envoy configuration in the official website of Istio is to describe the process of Envoy doing traffic forwarding. The party considering the traffic of the downstream is to receive the request sent by the downstream. You need to request additional services, such as reviews service requests need Pod ratings service.\nreviews, there are three versions of the service, there is one instance of each version, three versions sidecar similar working steps, only to later reviews-v1-cb8655c75-b97zc Sidecar flow Pod forwarding this step will be described.\nUnderstanding the Inbound Handler The role of the inbound handler is to transfer the traffic from the downstream intercepted by iptables to localhost to establish a connection with the application container inside the Pod.\nLook reviews-v1-cb8655c75-b97zc at the Listener in the pod.\nRun istioctl pc listener reviews-v1-cb8655c75-b97zc to see what the Pod has a Listener.\nADDRESS PORT TYPE 172.33.3.3 9080 HTTP \u0026lt;--- Receives all inbound traffic on 9080 from listener 0.0.0.0_15006 10.254.0.1 443 TCP \u0026lt;--+ 10.254.4.253 80 TCP | 10.254.4.253 8080 TCP | 10.254.109.182 443 TCP | 10.254.22.50 15011 TCP | 10.254.22.50 853 TCP | 10.254.79.114 443 TCP | 10.254.143.179 15011 TCP | 10.254.0.2 53 TCP | Receives outbound non-HTTP traffic for relevant IP:PORT pair from listener 0.0.0.0_15001 10.254.22.50 443 TCP | 10.254.16.64 42422 TCP …","relpermalink":"/en/blog/understanding-how-envoy-sidecar-intercept-and-route-traffic-in-istio-service-mesh/","summary":"Details about Envoy sidecar with iptables rules.","title":"Understanding How Envoy Sidecar Intercept and Route Traffic in Istio Service Mesh"},{"content":"KubeCon \u0026amp; CloudNative in North America is the most worthy cloud-native event every year. This time it will be held in Seattle for four days, from December 10th to 13th, refer to the official website of the conference . This year, 8,000 people participated. You should notice that Kubernetes has become more and more low-level. Cloud-native application developers do not need to pay much attention to it. Major companies are publishing their own cloud-native technology stack layouts, including IBM, VMware, SAP, Startups around this ecosystem are still emerging. The PPT sharing address of the local conference: https://github.com/warmchang/KubeCon-North-America-2018 , thank you William Zhang for finishing and sharing the slides of this conference .\nSeattle scene Janet Kuo from Google describes the path to cloud-native technology adoption.\nThe same event of KubeCon \u0026amp; CloudNativeCon, the scene of the first EnvoyCon.\nAt KubeCon \u0026amp; CloudNativeCon Seattle, various directors, directors, directors, VPs, and Gartner analysts from IBM, Google, Mastercard, VMware, and Gartner are conducting live discussions on the topic of Scaling with Service Mesh and Istio. Why do we talk about Istio when we talk about Service Mesh? What is not suitable for Istio use case. . .\nThe PPT contains basic introduction, getting started, a total of more than 200 Deep Dive as well as practical application, we recommend you according to the General Assembly’s official website to choose topics of interest to look at the schedule, otherwise I might see, however.\nA little impression KubeCon \u0026amp; CloudNativeCon is held three times a year, Europe, China and North America. China is the first time this year, and it will be held in Shanghai in November. It is said that it will be held in June next year. Although everyone said that Kubernetes has become boring, the conference about Kubernetes There is still a lot of content, and the use of CRD to extend Kubernetes usage is increasing. Service Mesh has begun to become hot. As can be seen from the live pictures above, there are a large number of participants on the site and related topics are also increasing. It is known as a microservice in the post-Kubernetes era . This must be It will be an important development direction of cloud native after Kubernetes, and the ServiceMesher community pays close attention to it.\n","relpermalink":"/en/notice/kubecon-cloudnativecon-seattle-2018/","summary":"KubeCon \u0026 CloudNativeCon Seattle 2018 Data Sharing.","title":"Kubecon\u0026CloudNativeCon Seattle 2018"},{"content":"This is a postscript from the post- Kubernetes era. Just this evening I saw a post by Bilgin Ibryam Microservices in a Post-Kuberentes Era .\nOn April 9, 2017, the Kubernetes Handbook-Kubernetes Chinese Guide / Cloud Native Application Architecture Practice Manual was first submitted. In the past 16 months, 53 contributors participated, 1,088 commits, and a total of 23,9014 Chinese characters were written. At the same time , thousands of enthusiasts have gathered in the Kubernetes \u0026amp; Cloud Native combat group .\nIt has been more than 4 months since the previous version was released. During this period, Kubernetes and Prometheus graduated from CNCF respectively and have matured commercially. These two projects have basically taken shape and will not change much in the future. Kubernetes was originally developed for container orchestration. In order to solve the problem of microservice deployment, Kubernetes has gained popularity. The current microservices have gradually entered the post-Kubernetes era . Service Mesh and cloud native redefine microservices and distributed applications.\nWhen this version was released, the PDF size was 108M with a total of 239,014 Chinese characters. It is recommended to browse online , or clone the project and install the Gitbook command to compile it yourself.\nThis version has the following improvements:\nAdded Istio Service Mesh tutorial Increased use VirtualBox and Vagrant set up in a local cluster and distributed Kubernetes Istio Service Mesh Added cloud native programming language Ballerina and Pulumi introduced Added Quick Start Guide Added support for Kubernetes 1.11 Added enterprise-level service mesh adoption path guide Added SOFAMesh chapter Added vision for the cloud-native future Added CNCF charter and participation Added notes for Docker image repositories Added Envoy chapter Increased KCSP (Kubernetes certification service providers) and CKA (Certified Kubernetes administrator) instructions Updated some configuration files, YAML and reference links Updated CRI chapter Removed obsolete description Improved etcdctl command usage tutorial Fixed some typos Browse and download Browse online https://jimmysong.io/kubernetes-handbook To make it easy for everyone to download, I put a copy on Weiyun , which is available in PDF (108MB), MOBI (42MB), and EPUB (53MB). In this book, there are more practical tutorials. In order to better understand the principles of Kubernetes, I recommend studying ** In- depth analysis of Kubernetes by Zhang Lei, produced by Geek Time **.\nThank you Kubernetes for your support of this book. Thank you Contributors . In the months before this version was released, the ServiceMesher community was co-founded . As a force in the post-Kubernetes era , welcome to contact me to join the community and create cloud native New era .\nAt present , the WeChat group of the ServiceMesher community also has thousands of members. The Kubernete Handbook will continue, but Service Mesh is already a rising star. With Kubernetes in mind, welcome to join the ServiceMesher community and follow us. The public account of the community (also the one I manage).\n","relpermalink":"/en/notice/new-kubernetes-handbook-released-and-say-hello-to-post-kubernetes-era/","summary":"This is an obituary post-Kubernetes era. Kubernetes handbook by Jimmy Song v1.4 is released. The next focus of cloud native is Service Mesh!","title":"Kubernetes Handbook v1.4 is released"},{"content":"Today I am honored to announce that I have become a CNCF Ambassador . Here is my story with Cloud Native.\nOrigin The first time to attend the Cloud Native Computing Foundation is at the LC3 in Beijing 2017. I attended the meeting again this year, and in November of this year, CNCF will hold the KubeCon \u0026amp; CloudNativeCon for the first time in Shanghai, China. I’ll be there too.\nCloud Native Books My origins with the Cloud Native is originated from Kevin Hoffman’s book Cloud Native Go . I translated this book at the end of 2016. Since then, in China, the translation of the word Cloud Native has not been determined, we introduced it with 云原生 to China.\nAnd then I begin to write the kubernetes-handbook on GitHub. So far, it has more than 2000 stars. This book has written more than 200,000 Chinese characters, the first commit happened on April 14, 2017.\nSince the the book Cloud Native Go completed, the publisher recommended another Cloud Native book to me - Cloud Native Python by Manish Sethi.\nAnd the book Cloud Native Java by Josh Long and Kenny Bastani.\nIn March 2018, with the hope that Bring the world equal opportunities and Building a Financial Cloud Native Infrastructure, I joined the Ant Group .\nServiceMesher Community By the time of May 2018, I start to organize the ServiceMesher community.\nIn the last few months, we work with other open source communities in China, such as k8smeetup , Sharding-Sphere , Apache SkyWalking . Our community has grown to have 1,700 members and two round meetups in Hangzhou and Beijing till now.\nMore than 300 people participated in the scene and more than 20,000 people watched it live by IT大咖说 。\nFuture Here are some hopes of mine:\nOpen source culture become popular in China More and more people would like to be involved in open source projects Host one open source project into the CNCF A book related to Cloud Native or Service Mesh Strengthen cultural exchanges between China and the global Finally, welcome to China for traveling or share your topic with us on Cloud Native, and in the mean while we will share our experience on large scale web apps to the world. Hope to hear your voice!\n","relpermalink":"/en/blog/cloud-native-and-me-the-past-current-and-future/","summary":"Today I am honored to announce that I have become a CNCF Ambassador.","title":"Cloud Native and me - the past, current and future"},{"content":"Today, we are pleased to announce Istio 1.0 . It’s been over a year since the original 0.1 release. Since 0.1, Istio has grown rapidly with the help of a thriving community, contributors, and users. Many companies have successfully applied Istio to production today and have gained real value through the insight and control provided by Istio. We help large businesses and fast-growing startups such as eBay , Auto Trader UK , Descartes Labs , HP FitStation , Namely , PubNub and Trulia to connect, manage and protect their services from scratch with Istio. The release of this version as 1.0 recognizes that we have built a core set of features that users can rely on for their production.\nEcosystem Last year we saw a significant increase in the Istio ecosystem. Envoy continues its impressive growth and adds many features that are critical to a production-level service grid. Observability providers like Datadog , SolarWinds , Sysdig , Google Stackdriver, and Amazon CloudWatch have also written plugins to integrate Istio with their products. Tigera , Aporeto , Cilium and Styra have built extensions for our strategy implementation and network capabilities. Kiali built by Red Hat provides a good user experience for grid management and observability. Cloud Foundry is building the next-generation traffic routing stack for Istio, the recently announced Knative serverless project is doing the same, and Apigee has announced plans to use it in their API management solution. These are just a few of the projects that the community added last year.\nFeatures Since the 0.8 release, we have added some important new features, and more importantly, marked many existing features as Beta to indicate that they can be used in production. This is covered in more detail in the release notes , but it is worth mentioning:\nMultiple Kubernetes clusters can now be added to a single grid , enabling cross-cluster communication and consistent policy enforcement. Multi-cluster support is now Beta. The network API for fine-grained control of traffic through the grid is now Beta. Explicitly modeling ingress and egress issues with gateways allows operations personnel to control the network topology and meet access security requirements at the edge. Two-way TLS can now be launched incrementally without updating all clients of the service. This is a key feature that removes the barriers to deploying Istio on existing production. Mixer now supports developing out-of-process adapters . This will be the default way to extend Mixer in an upcoming release, which will make it easier to build adapters. Envoy now fully evaluates the authorization policies that control service access locally , improving their performance and reliability. Helm chart installation is now the recommended installation method, with a wealth of customization options to configure Istio to your needs. We put a lot of effort into performance, including continuous regression testing, large-scale environmental simulation, and target repair. We are very happy with the results and will share details in the coming weeks. Next step Although this is an important milestone for the project, much work remains to be done. When working with adopters, we’ve received a lot of important feedback about what to focus next. We’ve heard consistent topics about supporting hybrid clouds, installing modularity, richer network capabilities, and scalability for large-scale deployments. We have considered some feedback in the 1.0 release and we will continue to actively work on it in the coming months.\nQuick start If you are new to Istio and want to use it for deployment, we would love to hear from you. Check out our documentation , visit our chat forum or visit the mailing list . If you want to contribute more to the project, please join our community meeting and say hello.\nAt last The Istio team is grateful to everyone who contributed to the project. Without your help, it won’t have what it is today. Last year’s achievements were amazing, and we look forward to achieving even greater achievements with our community members in the future.\nThe ServiceMesher community is responsible for the translation and maintenance of Chinese content on Istio’s official website. At present, the Chinese content is not yet synchronized with the English content. You need to manually enter the URL to switch to Chinese ( https://istio.io/zh ). There is still a lot of work to do , Welcome everyone to join and participate.\n","relpermalink":"/en/notice/istio-v1-released/","summary":"Chinese documentation is released at the same time!","title":"Istio 1.0 is released"},{"content":" If there is a visual learning model and platform that provides infrastructure clusters for your operation, would you pay for it?\nTwo months ago, I met Jord in Kubernetes’s Slack channel, and later saw the link of MagicSandbox.io he (and possibly others) sent in the Facebook group of Taiwan Kubernetes User Group, and I clicked to apply for a trial Then, I received an email from Jord later, and he told me that he wanted to build a Kubernetes learning platform. That’s where the whole thing started, and then we had a couple of Zoom video chats for a long time.\nAbout MagicSandbox MagicSandbox is a startup company. Jord (Dutch) is also a serial entrepreneur. He has studied at Sichuan University in China for 4 years since he was 19, and then returned to Germany. He worked as a PM at Boston Consulting Group and now works at Entrepreneur. First (Europe’s top venture capital / enterprise incubator) is also located in Berlin, Germany. He met Mislav (Croatian). Mislav is a full-stack engineer and has several entrepreneurial experiences. They have similar odors, and they hit it off. Decided to be committed to the Internet education industry and create a world-class software engineer education platform. They want to start with Kubernetes, first provide Internet-based Kubernetes theory and practice teaching, and then expand the topic to ElasticSearch, GraphQL, and so on. topic.\nJord founded MagicSandbox in his home, and I became the face of MagicSandbox in China.\nNow we are going to release the MagicSandbox Alpha version. This version is an immature version and is provided for everyone to try for free. Positive feedback is also welcome.\nOfficial homepage: https://magicsandbox.com/ Chinese page: https://cn.magicsandbox.com/ (The content has not been finished yet, only the Chinese version homepage is currently provided) Follow us on Twitter: https://twitter.com/magicsandbox ","relpermalink":"/en/notice/magicsandbox-alpha-version-annoucement/","summary":"Online practical software engineering education platform.","title":"MagicSandbox Alpha released"},{"content":"Remember the cloud-native programming language I shared before finally appeared! Learn about Ballerina in one article! ? They are ready to attend the KubeCon \u0026amp; CloudNativeCon China Conference!\nKubeCon \u0026amp; CloudNativeCon China Conference will be held on November 14-15, 2018 (Wednesday, Thursday) in Shanghai . See: https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2018/ With Ballerina’s official authorization, I now need to help them find an “ambassador” in China, responsible for team guidance, Chinese and English translation, and familiarity with Cloud Native and microservices. It has influence in the industry and has no barriers to English communication.\nAmbassador duties\nTeam Leadership Responsible for Chinese and English translation of product declaration, PPT materials, etc. Help to arrange the booth The other party can provide\nConference tickets Travel expenses Accommodation during the conference Other compensation This is a photo of their team in front of their booth during the KubeCon \u0026amp; CloudNativeCon in Hagen in May this year.\nPS This is the most complete and best picture I have ever found of their team. (Photography needs to be strengthened)\nLet’s briefly introduce this startup called Ballerina. Their team is mainly from Sri Lanka. This is an island country next to India in the South Asian subcontinent. In ancient China, it was called “Lion Country” and rich in gemstones.\nThe capital of their country is Sri Lanka, which is pronounced in their own language: Si li jia ya wa de na pu la ke te\nIf you are interested, please contact me directly.\n","relpermalink":"/en/notice/a-ballerina-china-ambassador-required/","summary":"With official authorization from Ballerina, I now need to help them find an ambassador in China.","title":"Ballerina seeks Chinese ambassador"},{"content":"Envoy-Designed for cloud-native applications, open source edge and service proxy, Istio Service Mesh default data plane, Chinese version of the latest official document, dedicated by the ServiceMesher community, welcome everyone to learn and share together.\nTL;DR: http://www.servicemesher.com/envoy/ PDF download address: servicemesher/envoy This is the first time Community Service Mesh enthusiasts group activities, the document is based on Envoy latest (version 1.7) Official documents https://www.envoyproxy.io/docs/envoy/latest/ . A total of 120 articles with 26 participants took 13 days and 65,148 Chinese characters.\nVisit online address: http://www.servicemesher.com/envoy/ Note : This book does not include the v1 API reference and v2 API reference sections in the official documentation. Any links to API references in the book will jump directly to the official page.\nContributor See the contributor page: https://github.com/servicemesher/envoy/graphs/contributors Thanks to the above contributors for their efforts! Because the level of translators is limited, there are inevitably inadequacies in the text. I also ask readers to correct them. Also welcome more friends to join our GitHub organization: https://github.com/servicemesher ","relpermalink":"/en/notice/envoyproxy-docs-cn-17-release/","summary":"Translated by ServiceMesher community.","title":"Chinese version of the latest official document of Envoy released"},{"content":"Envoy is an open source L7 proxy and communication bus written in C ++ by Lyft. It is currently an open source project under CNCF . The code is hosted on GitHub. It is also the default data plane in the Istio service mesh. We found that it has very good performance, and there are also continuous open source projects based on Envoy, such as Ambassador , Gloo, etc. At present, the official documentation of Envoy has not been well finished, so we Service Service enthusiasts feel that they are launching the community The power of the co-translation of the latest (version 1.7) official documentation of Enovy and organization through GitHub.\nService Mesh enthusiasts have jointly translated the latest version of the official document of Envoy . The translated code is hosted at https://github.com/servicemesher/envoy . If you are also a Service Mesh enthusiast, you can join the SerivceMesher GitHub organization and participate together.\nThe official Envoy document excludes all articles in the two directories of the v1 API reference and the v2 API reference. There are more than 120 documents. The length of the documents varies. The original English official documents use the RST format. I manually converted them into Markdown format and compiled using Gitbook. A GitHub Issue was generated according to the path of the document relative to the home directory. Friends who want to participate in translation can contact me to join the ServiceMesher organization, and then select the article you want to translate in Issue , and then reply “Claim”.\nHere you can see all the contributors. In the future, we will also create a Service Mesh enthusiast website. The website uses static pages. All code will be hosted on Github. Welcome everyone to participate.\n","relpermalink":"/en/notice/enovy-doc-translation-start/","summary":"The SerivceMesher community is involved in translating the official documentation for the latest version of Envoy.","title":"Envoy's latest official document translation work started"},{"content":"TL; DR Click here to download the PDF of this book .\nRecently, Michael Hausenblas of Nginx released a booklet on container networks in docker and kubernetes. This 72-page material is a good introduction for everyone to understand the network in Docker and Kubernetes from shallow to deep.\nTarget audience Container Software Developer SRE Network Operation and Maintenance Engineer Architects who want to containerize traditional software ","relpermalink":"/en/notice/container-networking-from-docker-to-kubernetes-nginx/","summary":"Source from Nginx, published by O’Reilly.","title":"Docker container network book sharing"},{"content":"In 2017, we are facing a big era of architectural changes, such as Kubernetes ending the battle for container orchestration, Kafka release 1.0, serverless gradually gaining momentum, edge computing to replace cloud computing, Service Mesh ready to go, and artificial intelligence to empower business , Also brings new challenges to the architecture.\nI am about to participate in InfoQ’s ArchSummit Global Architects Summit on December 8-11 in Beijing . This conference also invited 100+ top technologists such as Dr. Ali Wangjian to share and summarize the architectural changes and reflections this year. I hope that you can build on this conference, summarize past practices, and look forward to a future-oriented architecture to transform this era of change into the common fortune of each of us.\nMy speech The content of my speech is from Kubernetes to Cloud Native-the road to cloud native applications . Link: From Kubernetes to Cloud Native-the road to cloud native applications . The time is Saturday, December 9, 9:30 am, in the fifth meeting room.\nAfter more than ten years of development of cloud computing, the new phase of cloud native has entered. Enterprise applications are preferentially deployed in cloud environments. How to adapt to the cloud native tide, use containers and Kubernetes to build cloud native platforms, and practice DevOps concepts and agility How IT, open source software, and the community can help IT transform, the solution to all these problems is the PaaS platform, which is self-evident to the enterprise.\nWe also prepared gifts for everyone: “Cloud Native Go-Building Cloud Native Web Applications Based on Go and React” and “Intelligent Data Era-Enterprise Big Data Strategy and Practice” There is also a book stand for the blog post of the Electronic Industry Press. Welcome to visit.\nArchSummit conference official website link: http://bj2017.archsummit.com/ For more details, please refer to the official website of the conference: http://bj2017.archsummit.com/ ","relpermalink":"/en/notice/archsummit-beijing-2017-from-kubernetes-to-cloud-native/","summary":"I will give a lecture at ArchSummit Beijing. From Kubernetes to Cloud Native, my path to cloud native applications.","title":"ArchSummit Beijing 2017 speech preview"},{"content":"Cloudinary-go is a Go client library and CLI tool to upload static assets to the Cloudinary service.\nInstallation Install the CLI tool and the library with:\ngo get github.com/rootsongjc/cloudinary-go/cloudinary Or download the release binary from release .\n","relpermalink":"/en/notice/cloudinary-go/","summary":"Cloudinary-go is a Go client library and CLI tool to upload static assets to Cloudinary service","title":"Cloudinary file upload tool written in Go released"},{"content":"Many people asked me how jimmysong.io made this website. I think it is necessary to write a book to popularize the knowledge of static website construction and Hugo as a tool.\nThis manual will guide you how to use Hugo to build a static website for personal blog or project display.\nTeach you how to build a static website from scratch. This does not require much programming and development experience and time investment, and basically does not require much cost (except for personalized domain names). You can quickly build and Launch a website.\nGithub address: https://github.com/rootsongjc/hugo-handbook Gitbook access address: https://jimmysong.io/hugo-handbook The content of this book will continue to improve over time and as my site improves, so stay tuned.\n","relpermalink":"/en/notice/building-static-website-with-hugo/","summary":"A manual for static website building, and a personal blog Gitbook using Hugo.","title":"Hugo Handbook is released"},{"content":"Kevin Hoffman(From Capital One, twitter @KevinHoffman ) was making a speech on TalkingData T11 Smart Data Summit.\nHe addressed that 15 Factors of Cloud Native which based on Heroku’s original Twelve-Factor App , but he add more 3 another factors on it.\nLet’s have a look at the 15 factors of Cloud Native.\n1. One codebase, one App Single version-controlled codebase, many deploys Multiple apps should not share code Microservices need separate release schedules Upgrade, deploy one without impacting others Tie build and deploy pipelines to single codebase 2. API first Service ecosystem requires a contract Public API Multiple teams on different schedulers Code to contract/API, not code dependencies Use well-documented contract standards Protobuf IDL, Swagger, Apiary, etc API First != REST first RPC can be more appropriate in some situations 3. Dependency Management Explicitly declare dependencies Include all dependencies with app release Create immutable build artifact (e.g. docker image) Rely on smallest docker image Base on scratch if possible App cannot rely on host for system tools or libraries 4. Design, Build, Release, Run Design part of iterative cycle Agile doesn’t mean random or undesigned Mature CI/CD pipeline and teams Design to production in days not months Build immutable artifacts Release automatically deploys to environment Environments contains config, not release artifact 5. Configuration, Credentials, Code “3 Cs” volatile substances that explode when combinded Password in a config file is as bad as password in code App must accept “3 Cs” from environment and only use harmless defaults Test - Could you expose code on Github and not reveal passwords, URLs, credentials? 6. Logs Emit formatted logs to stdout Code should not know about destination or purpose of log emissions Use downstream log aggregator collect, store, process, expose logs ELK, Splunk, Sumo, etc Use structured logs to allow query and analysis JSON, csv, KV, etc Logs are not metrics 7. Disposability App must start as quickly as possible App must stop quickly and gracefully Processes start and stop all the time in the cloud Every scale up/down disposes of processes Slow dispose == slow scale Slow dispose or startup can cause availability gaps 8. Backing Services Assume all resources supplied by backingservices Cannotassume mutable file system “Disk as a Service” (e.g. S3, virtual mounts, etc) Every backing service is bound resource URL, credentials, etc-\u0026gt; environment config Host does not satisfy NFRs Backing services and cloud infrastructure 9. Environment Parity “Works on my machine” Cloud-native anti-pattern. Must work everywhere Every commit is candidate for deployment Automated acceptance tests Provide no confidence if environments don’t match 10. Administrative Processes Database migrations Run-once scripts or jobs Avoid using for batch operations, consider instead: Event sourcing Schedulers Triggers from queues, etc Lambdas/functions 11. Port Binding In cloud, infrastructure determines port App must accept port assigned by platform Containers have internal/external ports App design must embrace this Never use reserved ports Beware of container “host mode” networking 12. Stateless Processes What is stateless? Long-term state handled by a backing service In-memory state lives onlyas long as request Requests from same client routed to different instances “Sticky sessions” cloud native anti-pattern 13. Concurency Scale horizontally using the process model Build disposable, stateless, share-nothing processes Avoid adding CPU/RAM to increase scale/throughput Where possible, let platform/libraries do threading Many single-threaded services \u0026gt; 1 multi-threaded monolith 14. Telemetry Monitor apps in the cloud like satellite in orbit No tether, no live debugger Application Perf Monitoring (APM) Domain Telemetry Health and system logs 15. Authentication \u0026amp; Authorization Security should never be an afterthought Auth should be explicit, documented decision Even if anonymous access is allowed Don’t allow anonymous access Bearer tokens/OAuth/OIDC best practices Audit all attempts to access Migrating Monoliths to the Cloud After this 15 factors, he also gave us some tips about how to migrate monoliths to the Cloud:\nMake a rule - stop adding to the monolith All new code must be cloud native Prioritize features Where will you get most benefit from cloud native? Come up with a plan Decompose monolith over time Fast, agile iterations toward ultimate goal Use multiple strategies and patterns Go - the Best Language for Building Cloud Native App At last, he advise us the programming language Go is the best language to build Cloud Native applications for these reasons below:\nLightweight Easily learning curve Compiles to native binaries Very fast Large, thriving, engaged community http://gopherize.me Kevin also wrote a book Cloud Native Go to show how to Building Web Applications and Microservices for the Cloud with Go and React. This book has been …","relpermalink":"/en/blog/high-level-cloud-native-from-kevin-hoffman/","summary":"Kevin Hoffman address that 15 Factors of Cloud Native.","title":"High Level Cloud Native From Kevin Hoffman"},{"content":"From now on I have my own independent domain name jimmysong.io , the website is still hosted on GitHub, the original URL https://jimmysong.io is still accessible.\nWhy use .io as the suffix? Because this is The First Step to Cloud Native!\nWhy choose today? Because today is August 18, the days are easy to remember.\nPS domain names are registered in namecheap and cost tens of dollars / year.\nProudly powered by hugo 🎉🎊🎉\n","relpermalink":"/en/notice/domain-name-jimmysong-io/","summary":"From now on I have my own independent domain name jimmysong.io.","title":"New domain name jimmysong.io"},{"content":"This is a list of software, tools, architecture and reference materials about Cloud Native. It is awesome-cloud-native , a project I opened on GitHub , and can also be browsed through the web page .\nIt is divided into the following areas:\nAwesome Cloud Native\nAI API gateway Big Data Container engine CI-CD Database Data Science Fault tolerant Logging Message broker Monitoring Networking Orchestration and scheduler Portability Proxy and load balancer RPC Security and audit Service broker Service mesh Service registry and discovery Serverless Storage Tracing Tools Tutorial This list will be continuously updated and improved in the future, not only for my usual research records, but also as a reference for the Cloud Native industry.\n","relpermalink":"/en/notice/awesome-cloud-native/","summary":"This is a list of software, tools, architecture, and reference materials about Cloud Native. It is a project I started on GitHub.","title":"Awesome Cloud Native list is released"},{"content":"This book is the Chinese version of Migrating to Cloud Native Application Architectures. The English version of this book was released in February 2015. The Chinese version was translated by Jimmy Song and published in July 2017.\nGitHub hosting address for this book: https://github.com/rootsongjc/migrating-to-cloud-native-application-architectures Gitbook reading address: https://jimmysong.io/migrating-to-cloud-native-application-architectures The application architectures discussed in this book include:\nTwelve-factor application: A collection of cloud-native application architecture patterns Microservices: independently deployed services, each service does one thing Self-service agile infrastructure: a platform that provides application environments and back-office services quickly, repeatably, and consistently API-based collaboration: published and versioned APIs that allow interactions between services in a cloud-native application architecture Pressure resistance: a system that becomes stronger under pressure ","relpermalink":"/en/notice/changes-needed-to-cloud-native-archtecture/","summary":"This book is translated from an eBook published by Matt Stine in February 2015.","title":"Migrating to Cloud Native Chinese version released"}]