<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jimmy Song - Cloud Native | Open Source | Community – service mesh</title>
    <link>https://jimmysong.io/en/tags/service-mesh/</link>
    <description>Recent content in service mesh on Jimmy Song - Cloud Native | Open Source | Community</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright &amp;copy; 2021 Jimmy Song All Right Reserved;&lt;/br&gt; Powerd by Hugo with [educenter](https://github.com/themefisher/educenter-hugo) theme</copyright>
    <lastBuildDate>Mon, 12 Jul 2021 22:22:00 +0800</lastBuildDate>
    
	  <atom:link href="https://jimmysong.io/en/tags/service-mesh/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Multicluster Management with Kubernetes and Istio</title>
      <link>https://jimmysong.io/en/blog/multicluster-management-with-kubernetes-and-istio/</link>
      <pubDate>Mon, 12 Jul 2021 22:22:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/en/blog/multicluster-management-with-kubernetes-and-istio/</guid>
      <description>
        
        
        &lt;p&gt;Do you have multiple Kubernetes clusters and a service mesh? Do your virtual machines and services in a Kubernetes cluster need to interact? This article will take you through the process and considerations of building a hybrid cloud using Kubernetes and an Istio Service Mesh. Together, Kubernetes and Istio can be used to bring hybrid workloads into a mesh and achieve interoperability for multicluster. But another layer of infrastructure — a management plane — is helpful for managing multicluster or multimesh deployments.&lt;/p&gt;
&lt;h2 id=&#34;kubernetes&#34;&gt;Kubernetes&lt;/h2&gt;
&lt;p&gt;Using Kubernetes enables rapid deployment of a distributed environment that enables cloud interoperability and unifies the control plane on the cloud. It also provides resource objects, such as Service, Ingress and &lt;a href=&#34;https://kubernetes.io/blog/2021/04/22/evolving-kubernetes-networking-with-the-gateway-api/&#34;&gt;Gateway&lt;/a&gt;, to handle application traffic. The Kubernetes API Server communicates with the kube-proxy component on each node in the cluster, creates iptables rules for the node, and forwards requests to other pods.&lt;/p&gt;
&lt;p&gt;Assuming that a client now wants to access a service in Kubernetes, the request is first sent to the Ingress/Gateway, then forwarded to the backend service (Service A in the diagram below) based on the routing configuration in the Ingress/Gateway. Then Service A polls an instance of Service B for the traffic requested by Service B. Lastly, the traffic requested by Service A for Service B is polled forward to Service B’s instance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008i3skNly1gsgg6a11l1j31lu0u042s.jpg&#34; alt=&#34;Kubernetes&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-multicluster&#34;&gt;Kubernetes Multicluster&lt;/h2&gt;
&lt;p&gt;The most common usage scenarios for multicluster management include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;service traffic load balancing&lt;/li&gt;
&lt;li&gt;isolating development and production environments&lt;/li&gt;
&lt;li&gt;decoupling data processing and data storage&lt;/li&gt;
&lt;li&gt;cross-cloud backup and disaster recovery&lt;/li&gt;
&lt;li&gt;flexible allocation of compute resources&lt;/li&gt;
&lt;li&gt;low-latency access to services across regions&lt;/li&gt;
&lt;li&gt;avoiding vendor lock-in&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are often multiple Kubernetes clusters within an enterprise; and the &lt;a href=&#34;https://github.com/kubernetes-sigs/kubefed&#34;&gt;KubeFed&lt;/a&gt; implementation of Kubernetes cluster federation developed by &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-multicluster/README.md&#34;&gt;Multicluster SIG&lt;/a&gt; enables multicluster management capabilities, which allows all Kubernetes clusters to be managed through the same interface.&lt;/p&gt;
&lt;p&gt;There are several general issues that need to be addressed when using cluster federation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Configuring which clusters need to be federated&lt;/li&gt;
&lt;li&gt;API resources need to be propagated across the clusters&lt;/li&gt;
&lt;li&gt;Configuring how API resources are distributed to different clusters&lt;/li&gt;
&lt;li&gt;Registering DNS records in clusters to enable service discovery across clusters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following is a multicluster architecture for &lt;a href=&#34;https://kubesphere.io/&#34;&gt;KubeSphere&lt;/a&gt; — one of the most commonly used Kubernetes multicluster management architectures — where the Host Cluster serves as the control plane with two member clusters, West and East.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008i3skNly1gsgg7a2ojvj31aa0u0491.jpg&#34; alt=&#34;Multicluster&#34;&gt;&lt;/p&gt;
&lt;p&gt;The Host Cluster needs to be able to access the API Server of the Member Cluster, but the network connectivity between Member Clusters is not required. The Host Cluster is independent of the Member Cluster it manages and the Member Cluster is not aware of the existence of the Host Cluster. The advantage of this is that when the control plane fails, the Member Cluster will not be affected and the deployed load can still operate normally without being affected.&lt;/p&gt;
&lt;p&gt;The Host Cluster also assumes the role of API portal, and the Host Cluster forwards the resource requests to the Member Cluster — which is convenient for aggregation and also facilitates unified authority authentication. We see that there is a Federation Control Plane in the Host Cluster, where the Push Reconciler propagates the identity, role, and role binding from the Federation Cluster to all Member Clusters.&lt;/p&gt;
&lt;h2 id=&#34;istio-service-mesh&#34;&gt;Istio Service Mesh&lt;/h2&gt;
&lt;p&gt;Consider using the Istio service mesh when we have multilingual, multiversion microservices running in Kubernetes and need finer-grained canary publishing and unified security policy management for inter-service observability. Istio enables intelligent application-aware load balancing from the application layer to other Service Mesh-enabled services in the cluster, by transparently intercepting all traffic to and from the application using IPTables, and bypassing the primary kube-proxy load balancing. The Istio control plane communicates with the Kubernetes API Server to obtain information about all registered services in the cluster.&lt;/p&gt;
&lt;p&gt;The following diagram illustrates the basics of Istio, where all nodes belong to the same Kubernetes cluster.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008i3skNly1gsgg6sdrk2j32v60u0qbb.jpg&#34; alt=&#34;Istio Service Mesh&#34;&gt;&lt;/p&gt;
&lt;p&gt;You may end up with at least a few Kubernetes clusters, each hosting microservices. Multiple &lt;a href=&#34;https://istio.io/latest/docs/setup/install/multicluster/&#34;&gt;deployment models&lt;/a&gt; exist for Istio’s multicluster deployments — depending on network isolation, primary and backup — which can be specified by declaration when deploying using Istio Operator. Communication between these microservices in a cluster can be enhanced by a service mesh. Within the cluster, Istio provides common communication patterns to improve resiliency, security and observability.&lt;/p&gt;
&lt;p&gt;All of the above is about application load management on Kubernetes, but for legacy applications on virtual machines: how can they be managed in the same plane? Istio supports applications on virtual machines, so why do we need a management plane?&lt;/p&gt;
&lt;h2 id=&#34;management-plane&#34;&gt;Management Plane&lt;/h2&gt;
&lt;p&gt;To manage gateways, traffic and security groupings, and apply them to different clusters and namespaces, you’ll need to add another layer of abstraction on top of Istio: a management plane. The diagram below shows the multitenant model of Tetrate Service Bridge (TSB). TSB uses Next Generation Access Control (NGAC) — a fine-grained authorization framework — to manage user access and also facilitate the construction of a zero-trust network.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008i3skNly1gsgg8ndcajj31il0u00z9.jpg&#34; alt=&#34;Management Plane&#34;&gt;&lt;/p&gt;
&lt;p&gt;Istio provides workload identification, protected by strong mTLS encryption. This zero-trust model is better than trusting workloads based on topology information, such as source IP. A common control plane for multicluster management is built on top of Istio. Then a management plane is added to manage multiple clusters — providing multitenancy, management configuration, observability, and more.&lt;/p&gt;
&lt;p&gt;The diagram below shows the architecture of Tetrate Service Bridge.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008i3skNly1gsgg951mknj314g0u0dnf.jpg&#34; alt=&#34;Tetrate Service Bridge&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Interoperability of heterogeneous clusters is achieved with Kubernetes. Istio brings containerized and virtual machine loads into a single control plane, to unify traffic, security and observability within the clusters. However, as the number of clusters, network environments and user permissions become more complex, there is a need to build another management plane above Istio’s control plane (for example, &lt;a href=&#34;https://www.tetrate.io/tetrate-service-bridge/&#34;&gt;Tetrate Service Bridge&lt;/a&gt;) for hybrid cloud management.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>How to debug microservices in Kubernetes with proxy, sidecar or service mesh?</title>
      <link>https://jimmysong.io/en/blog/how-to-debug-microservices-in-kubernetes-with-proxy-sidecar-or-service-mesh/</link>
      <pubDate>Mon, 05 Jul 2021 22:22:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/en/blog/how-to-debug-microservices-in-kubernetes-with-proxy-sidecar-or-service-mesh/</guid>
      <description>
        
        
        &lt;p&gt;Kubernetes is arguably the best environment for running microservices so far, but the experience of debugging microservices in a Kubernetes environment may not be as user-friendly. This article will show you how to debug microservices in Kubernetes, introduce common tools, and explain how the introduction of Istio impacts debugging microservices.&lt;/p&gt;
&lt;h2 id=&#34;debugging-microservices-is-vastly-different-from-traditional-monolithic-applications&#34;&gt;Debugging microservices is vastly different from traditional monolithic applications&lt;/h2&gt;
&lt;p&gt;The debugging of microservices has been a long-standing problem for software developers. This challenge does not exist in traditional monolithic applications because developers can leverage the debugger in IDEs to add breakpoints, modify environment variables, single-step execution, etc. for their applications, all of which provide great help in software debugging. With the popularity of Kubernetes, the debugging of microservices becomes a thorny issue, where the following issues are more complicated than the debugging of traditional monolithic applications.&lt;/p&gt;
&lt;h3 id=&#34;multiple-dependencies&#34;&gt;Multiple dependencies&lt;/h3&gt;
&lt;p&gt;A microservice often depends on multiple other microservices, some shared volumes across multiple microservices, and authorizations based on service accounts. When debugging a microservice, how do you deploy other dependent services to quickly build a latest set of staging environments?&lt;/p&gt;
&lt;h3 id=&#34;access-from-a-local-machine&#34;&gt;Access from a local machine&lt;/h3&gt;
&lt;p&gt;When microservices are running on a developer’s local computer, there is usually no direct access to the services in a Kubernetes cluster. How can you debug microservices deployed in a Kubernetes cluster as if they were local services?&lt;/p&gt;
&lt;h3 id=&#34;slow-development-loop&#34;&gt;Slow development loop&lt;/h3&gt;
&lt;p&gt;Usually, it takes a long process to update the code and build it into an image before pushing it to the cluster. How do you speed up the development cycle? Let’s look at the tools that address those challenges.&lt;/p&gt;
&lt;h2 id=&#34;tools&#34;&gt;Tools&lt;/h2&gt;
&lt;p&gt;The main solutions for debugging microservices in Kubernetes are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proxy: by building a VPN, deploying a proxy in the Kubernetes cluster, and adding local debug endpoints to make the services in Kubernetes directly accessible to local applications, your architecture will look like [ local service ] &amp;lt;-&amp;gt; [ proxy ] &amp;lt;-&amp;gt; [ app in Kubernetes ].&lt;/li&gt;
&lt;li&gt;Sidecar: Inject a sidecar into the pod of the microservice to be debugged to intercept all traffic to and from the service, so that the service can be tracked and monitored, and the service can also be debugged in this sidecar.&lt;/li&gt;
&lt;li&gt;Service Mesh: To get an overall picture of the application, inject sidecars into all microservices so that you can get a dashboard that monitors global status.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are three typical open source projects that implement the above solutions, each of which can help you debug microservices from a different perspective. You can apply them at different stages of software development and they can be said to be complementary to each other.&lt;/p&gt;
&lt;h3 id=&#34;proxy--debugging-microservices-with-telepresence&#34;&gt;Proxy – debugging microservices with Telepresence&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.telepresence.io/&#34;&gt;Telepresence&lt;/a&gt; is essentially a local proxy that proxies data volumes, environment variables, and networks in a Kubernetes cluster locally. The following diagram shows the main usage scenarios for Telepresence.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;telepresence.jpg&#34; alt=&#34;Proxy mode: Telepresence&#34;&gt;&lt;/p&gt;
&lt;p&gt;Users need to manually execute the telepresence command locally, which will automatically deploy the agent to Kubernetes. Once the agent has been deployed,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Local services will have complete access to other services in the Kubernetes cluster, environment variables, Secret, ConfigMap, etc.&lt;/li&gt;
&lt;li&gt;Services in the cluster also have direct access to the locally exposed endpoints.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, this approach requires users to run multiple commands while debugging locally, and in some network environments it may not be possible to establish a VPN connection to the Kubernetes cluster.&lt;/p&gt;
&lt;h3 id=&#34;sidecar--debugging-microservices-with-nocalhost&#34;&gt;Sidecar – debugging microservices with Nocalhost&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://nocalhost.dev/&#34;&gt;Nocalhost&lt;/a&gt; is a Kubernetes-based cloud development environment. To use it, you just need to install a plugin in your IDE – VS Code to extend Kubernetes and shorten the development feedback cycle. The development environment can be isolated by creating different namespaces for different users and using ServiceAccount when binding to different user corners. Nocalhost also provides a web console and API for administrators to manage different development environments.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;sidecar-nocalhost.jpg&#34; alt=&#34;Sidecar mode: Nocalhost&#34;&gt;&lt;/p&gt;
&lt;p&gt;As long as you have a Kubernetes cluster and have admin rights to the cluster, you can refer to the &lt;a href=&#34;https://nocalhost.dev/getting-started/&#34;&gt;Nocalhost documentation&lt;/a&gt; to quickly start trying it out. To use the Nocalhost plugin in VS Code, you need to configure the Kubernetes cluster in the plugin first.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Select the Kubeconfig file you just exported or copy and paste the contents of the file directly into the configuration.&lt;/li&gt;
&lt;li&gt;Then select the service you need to test and select the corresponding Dev Container. VS Code will automatically open a new code window.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here is an example of the &lt;a href=&#34;https://istio.io/latest/docs/examples/bookinfo/&#34;&gt;bookinfo sample&lt;/a&gt; provided by Istio. You can open the cloned code in your local IDE and click the hammer next to the code file to enter development mode. Selecting the corresponding DevContainer and Nocalhost will automatically inject a development container sidecar into the pod and automatically enter the container in the terminal, as shown in the following figure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;nocalhost-vs-code.jpg&#34; alt=&#34;Nocalhost VS code&#34;&gt;&lt;/p&gt;
&lt;p&gt;In development mode, the code is modified locally without rebuilding the image, and the remote development environment takes effect in real time, which can greatly accelerate the development speed. At the same time, Nocalhost also provides a server for managing the development environment and user rights, as shown in the following figure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;nocalhost-web-admin.jpg&#34; alt=&#34;Nocalhost Web&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;service-mesh--debugging-microservices-with-istio&#34;&gt;Service Mesh – debugging microservices with Istio&lt;/h3&gt;
&lt;p&gt;The above method of using proxy and sidecar can only debug one service at a time. You’ll need a mesh to get the global status of the application, such as the metrics of the service obtained, and debug the performance of the service by understanding the dependency and invocation process of the service through distributed tracing. These observability features need to be implemented by injecting sidecar uniformly for all services. And, when your services are in the process of migrating from VMs to Kubernetes, using Istio can bring VMs and Kubernetes into a single network plane (as shown below), making it easy for developers to debug and do incremental migrations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;istio-service-mesh.jpg&#34; alt=&#34;Serivce Mesh mode: Istio&#34;&gt;&lt;/p&gt;
&lt;p&gt;Of course, these benefits do not come without a “cost.” With the introduction of Istio, your Kubernetes services will need to adhere to the Istio naming convention and you’ll need to know how to debug microservices using the Istioctl command line and logging.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use the &lt;em&gt;istioctl analyze&lt;/em&gt; command to debug the deployment of microservices in your cluster, and you can use YAML files to examine the deployment of resources in a namespace or across your cluster.&lt;/li&gt;
&lt;li&gt;Use &lt;em&gt;istioctl proxy-config secret&lt;/em&gt; to ensure that the secret of a pod in a service mesh is loaded correctly and is valid.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In the process of microservicing applications and migrating from virtual machines to Kubernetes, developers need to make a lot of changes in their mindset and habits. By building a VPN between local and Kubernetes via proxy, developers can easily debug services in Kubernetes as if they were local services. By injecting a sidecar into the pod, you can achieve real-time debugging and speed up the development process. Finally, the Istio service mesh truly enables global observability, and you can also use tools like &lt;a href=&#34;https://www.tetrate.io/tetrate-service-bridge/&#34;&gt;Tetrate Service Bridge&lt;/a&gt; to manage heterogeneous platforms, helping you gradually move from monolithic applications to microservices.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Happy Istio 4th Anniversary -- Retrospect and Outlook</title>
      <link>https://jimmysong.io/en/blog/istio-4-year-birthday/</link>
      <pubDate>Mon, 24 May 2021 08:00:00 +0800</pubDate>
      
      <guid>https://jimmysong.io/en/blog/istio-4-year-birthday/</guid>
      <description>
        
        
        &lt;p&gt;Istio was named by &lt;a href=&#34;https://tetrate.io/&#34;&gt;Tetrate&lt;/a&gt; founder Varun Talwar and Google lead engineer Louis Ryan in 2017 and was open sourced on May 24, 2017. Today is the fourth anniversary of Istio’s open source arrival. Let’s take a look back at Istio’s four years of development — and look forward to Istio’s future.&lt;/p&gt;
&lt;h3 id=&#34;istios-open-source-history&#34;&gt;Istio’s open source history&lt;/h3&gt;
&lt;p&gt;In 2017, the year Kubernetes ended the container orchestration battle, Google took the opportunity to consolidate its dominance in the cloud native space and compensate for Kubernetes’ disadvantage in service-to-service traffic management by open-sourcing Istio. Istio released its 1.10 last week — but here are some of the most important releases in Istio’s history to date.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Date&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Version&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;May 24, 2017&lt;/td&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;td&gt;Officially open source; established the architectural foundation of Control Plane, Data Plane and sidecar proxy.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;October 10, 2017&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;Started to support multiple runtime environments, such as virtual machines.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;June 1, 2018&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;API refactoring&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;July 31, 2018&lt;/td&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;td&gt;Production-ready, after which the Istio team underwent a massive reorganization.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;March 19, 2019&lt;/td&gt;
&lt;td&gt;1.1&lt;/td&gt;
&lt;td&gt;Enterprise-ready. Support for multiple Kubernetes clusters, with performance optimizations.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;March 3, 2020&lt;/td&gt;
&lt;td&gt;1.5&lt;/td&gt;
&lt;td&gt;Back to monolith, with microservice components merged into istiod, making Istio’s architecture cleaner and easier to maintain. Support for WebAssembly extension, making Istio’s ecology much stronger.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;November 18, 2020&lt;/td&gt;
&lt;td&gt;1.8&lt;/td&gt;
&lt;td&gt;Officially deprecated Mixer and focused on adding support for virtual machines.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A year after its inception– and two months before the 1.0 release, version 0.8 was released with a massive refactoring of the API. In late July 2018, when 1.0 was released, Istio reached a production-ready tipping point. Since then, Google has massively reorganized the Istio team and several Istio-based service mesh startups were born, making 2018 the booming year of the service mesh industry.&lt;/p&gt;
&lt;p&gt;Istio 1.1 was released in March 2019, almost 9 months after 1.0 was released, which is far beyond the average release cycle of an open-source project. We know that the speed of iteration and evolution is a core competency of basic software. Since then, Istio has started a regular &lt;a href=&#34;https://istio.io/v1.7/about/release-cadence/&#34;&gt;release cadence&lt;/a&gt; of one version per quarter and has become the &lt;a href=&#34;https://octoverse.github.com/#fastest-growing-oss-projects-by-contributors&#34;&gt;#4 fastest growing project in GitHub’s top 10 in 2019&lt;/a&gt;!&lt;/p&gt;
&lt;h3 id=&#34;the-istio-community&#34;&gt;The Istio community&lt;/h3&gt;
&lt;p&gt;In 2020, Istio’s project management began to mature and its governance reached a stage of evolution. We saw the first &lt;a href=&#34;https://istio.io/latest/blog/2020/steering-election-results/&#34;&gt;election&lt;/a&gt; of a steering committee for the Istio community and the transfer of the trademark to &lt;a href=&#34;https://istio.io/latest/blog/2020/open-usage/&#34;&gt;Open Usage Commons&lt;/a&gt;. The first &lt;a href=&#34;https://events.istio.io/istiocon-2021/&#34;&gt;IstioCon&lt;/a&gt; was successfully held in February 2021, with thousands of people attending the online conference. There is also a &lt;a href=&#34;https://www.youtube.com/watch?v=6m-rhyfy8sg&amp;amp;list=PL7wB27eZmdffS-g_xh7X-b0echc_XZMKV&amp;amp;index=8&#34;&gt;large Istio community in China&lt;/a&gt;, and face-to-face Istio community meetups will be held there in 2021. Stay tuned for more.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;008i3skNly1gquicfqg14j31lw0smwl2.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;According to the CNCF 2020 Survey, 46% of organizations were either using a service mesh in production or planning to use it in the next 12 months. Istio was the top used mesh among those using a mesh in production.&lt;/p&gt;
&lt;h3 id=&#34;the-future&#34;&gt;The future&lt;/h3&gt;
&lt;p&gt;After 4 years of development, there is not only a large user base around Istio, but also several Istio vendors, as you can see on the &lt;a href=&#34;https://istio.io/&#34;&gt;homepage&lt;/a&gt; of the recently revamped Istio website. In the last few releases, Istio has shifted its development focus to improving the Day 2 Operation experience. We also expect to see more Istio adoption path recommendations, case studies, learning materials, training, and certifications (such as the industry’s first &lt;a href=&#34;https://academy.tetrate.io/courses/certified-istio-administrator&#34;&gt;Certified Istio Administrator&lt;/a&gt; from Tetrate) that will facilitate the adoption of Istio.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Why Do You Need Istio When You Already Have Kubernetes?</title>
      <link>https://jimmysong.io/en/blog/why-do-you-need-istio-when-you-already-have-kubernetes/</link>
      <pubDate>Wed, 07 Apr 2021 08:27:17 +0800</pubDate>
      
      <guid>https://jimmysong.io/en/blog/why-do-you-need-istio-when-you-already-have-kubernetes/</guid>
      <description>
        
        
        &lt;p&gt;If you’ve heard of service mesh and tried &lt;a href=&#34;https://istio.io/&#34;&gt;Istio&lt;/a&gt;, you may have the following questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Why is Istio running on Kubernetes?&lt;/li&gt;
&lt;li&gt;What is the role of Kubernetes and a service mesh in the cloud native application architecture, respectively?&lt;/li&gt;
&lt;li&gt;What aspects of Kubernetes does Istio extend? What problems does it solve?&lt;/li&gt;
&lt;li&gt;What is the relationship between Kubernetes, Envoy, and Istio?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This article will take you through the inner workings of Kubernetes and Istio. In addition, I will introduce the load balancing approach in Kubernetes, and explain why you need Istio when you have Kubernetes.&lt;/p&gt;
&lt;p&gt;Kubernetes is essentially about application lifecycle management through declarative configuration, while a service mesh is essentially about providing inter-application traffic, security management and observability. If you have already built a stable application platform using Kubernetes, how do you set up load balancing and traffic control for calls between services? This is where a service mesh comes into the picture.&lt;/p&gt;
&lt;p&gt;Envoy introduces the xDS protocol, which is supported by various open source software, such as &lt;a href=&#34;https://istio.io/&#34;&gt;Istio&lt;/a&gt;, &lt;a href=&#34;https://github.com/mosn/mosn&#34;&gt;MOSN&lt;/a&gt;, etc. Envoy contributes xDS to a service mesh or cloud native infrastructure. Envoy is essentially a modern version of a proxy that can be configured through APIs, based on which many different usage scenarios are derived — such as API Gateway, sidecar proxy in service mesh, and edge proxy.&lt;/p&gt;
&lt;p&gt;This article contains the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A description of the role of kube-proxy.&lt;/li&gt;
&lt;li&gt;The limitations of Kubernetes for microservice management.&lt;/li&gt;
&lt;li&gt;An introduction to the capabilities of Istio service mesh.&lt;/li&gt;
&lt;li&gt;A comparison of some of the concepts in Kubernetes, Envoy, and the Istio service mesh.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kubernetes-vs-service-mesh&#34;&gt;Kubernetes vs Service Mesh&lt;/h2&gt;
&lt;p&gt;The following diagram shows the service access relationship in Kubernetes and service mesh (one sidecar per pod model).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/008eGmZEly1gpb7knfo4dj31hk0redrz.jpg&#34; alt=&#34;Kubernetes vs Service Mesh&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;traffic-forwarding&#34;&gt;Traffic Forwarding&lt;/h3&gt;
&lt;p&gt;Each node in a Kubernetes cluster deploys a kube-proxy component that communicates with the Kubernetes API Server, gets information about the services in the cluster, and then sets iptables rules to send requests for service directly to the corresponding Endpoint (a pod belonging to the same group of services).&lt;/p&gt;
&lt;h3 id=&#34;service-discovery&#34;&gt;Service Discovery&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/008eGmZEly1gpb7knwb79j30kq0fcjs9.jpg&#34; alt=&#34;Service Discovery&#34;&gt;&lt;/p&gt;
&lt;p&gt;Istio can follow the service registration in Kubernetes and can also interface with other service discovery systems via platform adapters in the control plane; and then generate data plane configurations (using CRD, which are stored in etcd) with transparent proxies for the data plane. The transparent proxy of the data plane is deployed as a sidecar container in the pod of each application service, and all these proxies need to request the control plane to synchronize the proxy configuration. The proxy is “transparent” because the application container is completely unaware of the presence of the proxy. The kube-proxy component in the process needs to intercept traffic as well, except that the kube-proxy intercepts traffic to and from the Kubernetes node — while the sidecar proxy intercepts traffic to and from the pod.&lt;/p&gt;
&lt;h3 id=&#34;disadvantages-of-a-service-mesh&#34;&gt;Disadvantages of a Service Mesh&lt;/h3&gt;
&lt;p&gt;Since Kubernetes has many pods running on each node, putting the original kube-proxy route forwarding function in each pod will increase the response latency — due to more hops when the sidecar intercepts the traffic — and consume more resources. In order to manage traffic in a fine-grained manner, a series of new abstractions will be added. This will further increase the learning cost for users, but as the technology becomes more popular this situation will be slowly alleviated.&lt;/p&gt;
&lt;h3 id=&#34;advantages-of-a-service-mesh&#34;&gt;Advantages of a Service Mesh&lt;/h3&gt;
&lt;p&gt;The kube-proxy settings are global and cannot be controlled at a granular level for each service, while service mesh takes the traffic control out of the service layer in Kubernetes by means of sidecar proxy — allowing for more elasticity.&lt;/p&gt;
&lt;h3 id=&#34;shortcomings-of-kube-proxy&#34;&gt;Shortcomings of Kube-Proxy&lt;/h3&gt;
&lt;p&gt;First, it does not automatically try another pod if the forwarded pod is not serving properly. Each pod has a health check mechanism and when a pod has health problems, kubelet will restart the pod and kube-proxy will remove the corresponding forwarding rules. Also, nodePort-type services cannot add TLS or more complex message routing mechanisms.&lt;/p&gt;
&lt;p&gt;Kube-proxy implements load balancing of traffic across multiple pod instances of a Kubernetes service, but how do you do fine-grained control of traffic between these services — such as dividing traffic by percentage to different application versions (which are all part of the same service but on different deployments), or doing canary releases (grayscale releases) and blue-green releases?&lt;/p&gt;
&lt;p&gt;The Kubernetes community gives a way to &lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments&#34;&gt;do canary releases using Deployment&lt;/a&gt;, which is essentially a way to assign different pods to a deployment’s service by modifying the pod’s label.&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-ingress-vs-istio-gateway&#34;&gt;Kubernetes Ingress vs. Istio Gateway&lt;/h2&gt;
&lt;p&gt;As mentioned above, kube-proxy can only route traffic within a Kubernetes cluster. The pods of a Kubernetes cluster are located in a network created by CNI. An ingress — a resource object created in Kubernetes — is created for communication outside the cluster. It’s driven by an ingress controller located on Kubernetes edge nodes responsible for managing north-south traffic. Ingress must be docked to various Ingress Controllers, such as the &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34;&gt;nginx ingress controller&lt;/a&gt; and &lt;a href=&#34;https://traefik.io/&#34;&gt;traefik&lt;/a&gt;. Ingress is only applicable to HTTP traffic and is simple to use. It can only route traffic by matching a limited number of fields — such as service, port, HTTP path, etc. This makes it impossible to route TCP traffic such as MySQL, Redis, and various RPCs. This is why you see people writing nginx config language in ingress resource annotations.The only way to directly route north-south traffic is to use the service’s LoadBalancer or NodePort, the former requiring cloud vendor support and the latter requiring additional port management.&lt;/p&gt;
&lt;p&gt;Istio Gateway functions similarly to Kubernetes Ingress, in that it is responsible for north-south traffic to and from the cluster. Istio Gateway describes a load balancer for carrying connections to and from the edge of the mesh. The specification describes a set of open ports and the protocols used by those ports, the SNI configuration for load balancing, etc. Gateway is a CRD extension that also reuses the capabilities of the sidecar proxy; see the &lt;a href=&#34;https://istio.io/latest/docs/reference/config/networking/gateway/&#34;&gt;Istio website&lt;/a&gt; for detailed configuration.&lt;/p&gt;
&lt;h2 id=&#34;envoy&#34;&gt;Envoy&lt;/h2&gt;
&lt;p&gt;Envoy is the default sidecar proxy in Istio. Istio extends its control plane based on Enovy’s xDS protocol. We need to familiarize ourselves with Envoy’s basic terminology before talking about Envoy’s xDS protocol. The following is a list of basic terms and their data structures in Envoy; please refer to the &lt;a href=&#34;https://envoyproxy.io/&#34;&gt;Envoy documentation&lt;/a&gt; for more details.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/008eGmZEly1gpb7koah95j31450tetta.jpg&#34; alt=&#34;Envoy&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;basic-terminology&#34;&gt;Basic Terminology&lt;/h3&gt;
&lt;p&gt;The following are the basic terms in Enovy that you should know.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Downstream&lt;/strong&gt;: The downstream host connects to Envoy, sends the request, and receives the response; i.e., the host that sent the request.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upstream&lt;/strong&gt;: The upstream host receives connections and requests from Envoy and returns responses; i.e., the host that receives the requests.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Listener&lt;/strong&gt;: Listener is a named network address (for example, port, UNIX domain socket, etc.); downstream clients can connect to these listeners. Envoy exposes one or more listeners to the downstream hosts to connect.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cluster&lt;/strong&gt;: A cluster is a group of logically identical upstream hosts to which Envoy connects. Envoy discovers the members of a cluster through service discovery. Optionally, the health status of cluster members can be determined through proactive health checks. Envoy decides which member of the cluster to route requests through a load balancing policy.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Multiple listeners can be set in Envoy, each listener can set a filter chain (filter chain table), and the filter is scalable so that we can more easily manipulate the behavior of traffic — such as setting encryption, private RPC, etc.&lt;/p&gt;
&lt;p&gt;The xDS protocol was proposed by Envoy and is the default sidecar proxy in Istio, but as long as the xDS protocol is implemented, it can theoretically be used as a sidecar proxy in Istio — such as the &lt;a href=&#34;https://github.com/mosn/mosn&#34;&gt;MOSN&lt;/a&gt; open source by Ant Group.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cdn.thenewstack.io/media/2021/03/b800bf17-image3.png&#34;&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/008eGmZEly1gpb7kk7wk4j31060lqgqx.jpg&#34; alt=&#34;img&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Istio is a very feature-rich service mesh that includes the following capabilities.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Traffic Management: This is the most basic feature of Istio.&lt;/li&gt;
&lt;li&gt;Policy Control: Enables access control systems, telemetry capture, quota management, billing, etc.&lt;/li&gt;
&lt;li&gt;Observability: Implemented in the sidecar proxy.&lt;/li&gt;
&lt;li&gt;Security Authentication: The Citadel component does key and certificate management.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;traffic-management-in-istio&#34;&gt;Traffic Management in Istio&lt;/h2&gt;
&lt;p&gt;The following CRDs are defined in Istio to help users with traffic management.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gateway: Gateway describes a load balancer that runs at the edge of the network and is used to receive incoming or outgoing HTTP/TCP connections.&lt;/li&gt;
&lt;li&gt;VirtualService: VirtualService actually connects the Kubernetes service to the Istio Gateway. It can also perform additional operations, such as defining a set of traffic routing rules to be applied when a host is addressed.&lt;/li&gt;
&lt;li&gt;DestinationRule: The policy defined by the DestinationRule determines the access policy for the traffic after it has been routed. Simply put, it defines how traffic is routed. Among others, these policies can be defined as load balancing configurations, connection pool sizes, and external detection (for identifying and expelling unhealthy hosts in the load balancing pool) configurations.&lt;/li&gt;
&lt;li&gt;EnvoyFilter: The EnvoyFilter object describes filters for proxy services that can customize the proxy configuration generated by Istio Pilot. This configuration is generally rarely used by primary users.&lt;/li&gt;
&lt;li&gt;ServiceEntry: By default, services in the Istio service mesh are unable to discover services outside of the Mesh. ServiceEntry enables additional entries to be added to the service registry inside Istio, thus allowing automatically discovered services in the mesh to access and route to these manually added services.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kubernetes-vs-xds-vs-istio&#34;&gt;Kubernetes vs. xDS vs. Istio&lt;/h2&gt;
&lt;p&gt;Having reviewed the abstraction of traffic management in Kubernetes’ kube-proxy component, xDS, and Istio, let’s look now at a comparison of the three components/protocols in terms of traffic management only (note that the three are not exactly equivalent).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;xDS&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Istio service mesh&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Endpoint&lt;/td&gt;
&lt;td&gt;Endpoint&lt;/td&gt;
&lt;td&gt;WorkloadEntry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service&lt;/td&gt;
&lt;td&gt;Route&lt;/td&gt;
&lt;td&gt;VirtualService&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-proxy&lt;/td&gt;
&lt;td&gt;Route&lt;/td&gt;
&lt;td&gt;DestinationRule&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-proxy&lt;/td&gt;
&lt;td&gt;Listener&lt;/td&gt;
&lt;td&gt;EnvoyFilter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ingress&lt;/td&gt;
&lt;td&gt;Listener&lt;/td&gt;
&lt;td&gt;Gateway&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service&lt;/td&gt;
&lt;td&gt;Cluster&lt;/td&gt;
&lt;td&gt;ServiceEntry&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;takeaways&#34;&gt;Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The essence of Kubernetes is application lifecycle management, specifically deployment and management (scaling up and down, auto-recovery, release).&lt;/li&gt;
&lt;li&gt;Kubernetes provides a scalable and highly resilient deployment and management platform for microservices.&lt;/li&gt;
&lt;li&gt;A service mesh is based on transparent proxies that intercept traffic between services through sidecar proxies, and then manage the behavior of them through control plane configuration.&lt;/li&gt;
&lt;li&gt;A service mesh decouples traffic management from Kubernetes, eliminating the need for a kube-proxy component to support traffic within service mesh; and managing inter-service traffic, security and observability by providing an abstraction closer to the microservice application layer.&lt;/li&gt;
&lt;li&gt;xDS is one of the protocol standards for service mesh configuration.&lt;/li&gt;
&lt;li&gt;A service mesh is a higher-level abstraction of service in Kubernetes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;If the object managed by Kubernetes is a pod, then the object managed in service mesh is a service, so it’s just a matter of using Kubernetes to manage microservices and then applying service mesh. If you don’t even want to manage a service, then use a serverless platform like &lt;a href=&#34;https://knative.dev/&#34;&gt;Knative&lt;/a&gt; — but that’s an afterthought.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
