[{"content":"Recently, the CNCF released the KubeCon EU 2024 Transparency Report . The KubeCon + CloudNativeCon Europe 2024 held in Paris was the largest event in its history, and I had the fortune to attend in person. The beauty of Paris and the atmosphere of the conference were unforgettable.\nKubeCon + CloudNativeCon EU 2024 Transparency Report Cover We witnessed the power of the open-source and cloud-native communities and how they drive the next major technological shift — artificial intelligence. At this conference, CNCF announced the establishment of an AI working group and launched the first edition of the Cloud Native AI Whitepaper , followed by the creation of the Cloud Native AI Landscape .\nGrowth and Diversity The event welcomed over 12,000 participants, with the majority (78%) coming from Europe and 52% attending for the first time. Participants from different regions showed enthusiasm for cloud-native technologies. The diverse functional backgrounds of the participants, including business operations, developers, and architects, highlighted the wide application of cloud-native technology across fields.\nThe following table shows the top 10 countries by attendee numbers:\nCountry Number of Attendees France 1,956 Germany 1,879 USA 1,733 United Kingdom 816 Netherlands 797 Switzerland 368 Spain 355 Israel 312 Poland 303 Denmark 293 Innovation and Communication During the event, there were 331 sessions and 19 keynote speeches covering hot topics such as artificial intelligence, application development, and platform engineering. This edition of KubeCon also hosted poster sessions and project tours for researchers, helping attendees better understand various projects.\nCommunity and Inclusivity KubeCon is committed to supporting diversity, equity, and inclusion. Activities such as diversity lunches and Peer Group Mentoring provided platforms for participants to connect and support each other. (PS. The catering at the venue was not to my liking, especially the taste of the tea and the hard bread.)\nEnvironmental and Sustainability In terms of environmental and sustainability efforts, the organizers chose venues with an environmental consciousness and implemented measures to reduce food waste and recycle waste, demonstrating CNCF’s commitment to environmental protection.\nLooking Forward Looking forward, we hope to bring the magic of KubeCon to more regions globally. Whether it’s the upcoming KubeCon + CloudNativeCon China 2024 or other regional events, I am very much looking forward to meeting you. I have also submitted a panel session for KubeCon China, and if selected, we will meet in Hong Kong this August.\nConclusion KubeCon + CloudNativeCon Europe 2024 was not just a technical gathering, but a display of community strength. If you would like to know more details, please check out my previous recap of KubeCon + CloudNativeCon Europe .\nI hope this transparency report helps you better understand the impact of the event and inspires you to participate in future KubeCon events.\n","relpermalink":"/en/blog/kubecon-eu-2024-transparency-report/","summary":"A detailed interpretation of the key data and highlights from KubeCon Europe 2024.","title":"KubeCon EU 2024 Transparency Report Interpretation"},{"content":"In March 2024, during KubeCon EU, the Cloud Native Computing Foundation (CNCF) released its first detailed whitepaper on Cloud Native Artificial Intelligence (CNAI). This report thoroughly explores the current state, challenges, and future directions of integrating Cloud Native technologies with artificial intelligence. This article delves into the core content of this whitepaper.\nWhat is Cloud Native AI? Cloud Native AI refers to the approach of building and deploying artificial intelligence applications and workloads using Cloud Native technology principles. This includes leveraging microservices, containerization, declarative APIs, and continuous integration/continuous deployment (CI/CD) to enhance the scalability, reusability, and operability of AI applications.\nThe diagram below illustrates the architecture of Cloud Native AI, redrawn based on the whitepaper.\nCloud Native AI Architecture Relationship Between Cloud Native AI and Cloud Native Technologies Cloud Native technologies provide a flexible, scalable platform that makes the development and operation of AI applications more efficient. Through containerization and microservices architecture, developers can iterate and deploy AI models rapidly while ensuring high availability and scalability of systems. Kubernetes and other Cloud Native tools provide essential support such as resource scheduling, automatic scaling, and service discovery.\nThe whitepaper provides two examples illustrating the relationship between Cloud Native AI and Cloud Native technologies, namely running AI on Cloud Native infrastructure:\nHugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure OpenAI Scaling Kubernetes to 7,500 nodes Challenges of Cloud Native AI Despite providing a solid foundation for AI applications, Cloud Native technologies still face challenges when integrating AI workloads with Cloud Native platforms. These challenges include the complexity of data preparation, resource requirements for model training, and maintaining the security and isolation of models in multi-tenant environments. Additionally, resource management and scheduling in Cloud Native environments are crucial, especially for large-scale AI applications, and further optimization is needed to support efficient model training and inference.\nDevelopment Path of Cloud Native AI The whitepaper proposes several development paths for Cloud Native AI, including improving resource scheduling algorithms to better support AI workloads, developing new service mesh technologies to enhance the performance and security of AI applications, and driving innovation and standardization of Cloud Native AI technology through open-source projects and community collaboration.\nCloud Native AI Technology Landscape Cloud Native AI involves a variety of technologies, from containers and microservices to service meshes and serverless computing. Kubernetes is a key platform for deploying and managing AI applications, while service mesh technologies like Istio and Envoy provide powerful traffic management and security features. Additionally, monitoring tools like Prometheus and Grafana are essential for maintaining the performance and reliability of AI applications.\nBelow is the Cloud Native AI landscape provided in the whitepaper.\nGeneral Orchestration Kubernetes Volcano Armada Kuberay Nvidia NeMo Yunikorn Kueue Flame Distributed Training Kubeflow Training Operator Pytorch DDP TensorFlow Distributed Open MPI DeepSpeed Megatron Horovod Apla … ML Serving Kserve Seldon VLLM TGT Skypilot … CI/CD - Delivery Kubeflow Pipelines Mlflow TFX BentoML MLRun … Data Science Jupyter Kubeflow Notebooks PyTorch TensorFlow Apache Zeppelin … Workload Observability Prometheus Influxdb Grafana Weights and Biases (wandb) OpenTelemetry … AutoML Hyperopt Optuna Kubeflow Katib NNI … Governance \u0026amp; Policy Kyverno Kyverno-JSON OPA/Gatekeeper StackRox Minder … Data Architecture ClickHouse Apache Pinot Apache Druid Cassandra ScyllaDB Hadoop HDFS Apache HBase Presto Trino Apache Spark Apache Flink Kafka Pulsar Fluid Memcached Redis Alluxio Apache Superset … Vector Databases Milvus Chroma Weaviate Quadrant Pinecone Extensions Redis Postgres SQL ElasticSearch … Model/LLM Observability Trulens Langfuse Deepchecks OpenLLMetry … Summary Finally, let me summarizes the following key points:\nRole of the Open Source Community: The whitepaper clearly points out the role of the open-source community in advancing Cloud Native AI, including accelerating innovation and reducing costs through open-source projects and extensive collaboration.\nImportance of Cloud Native Technologies: Cloud Native AI is built and deployed according to Cloud Native principles, highlighting the importance of repeatability and scalability. Cloud Native technologies provide an efficient development and runtime environment for AI applications, especially in terms of resource scheduling and service scalability.\nChallenges Exist: Despite the many advantages brought …","relpermalink":"/en/blog/cloud-native-ai-whitepaper/","summary":"During KubeCon EU 2024, CNCF released its first Cloud Native Artificial Intelligence (CNAI) whitepaper. This article provides an in-depth analysis of the content of this whitepaper.","title":"In-depth Analysis of CNCF's Cloud Native AI Whitepaper"},{"content":"This article will introduce Tetrate’s newly launched tool – Tetrate Vulnerability Scanner (TVS) , a CVE scanner customized for Istio and Envoy. Before diving into the specific features of TVS, let’s briefly review the concept of CVE and its core role in software security.\nOverview of CVE CVE (Common Vulnerabilities and Exposures) is a public vulnerability database maintained by MITRE Corporation. It provides a standardized way to reference vulnerabilities in open-source software. Each CVE record has an identifier, description, and at least one public reference. CVE does not provide severity ratings for vulnerabilities. CVEs are not only an important resource for cybersecurity professionals but also a tool for developers and organizations to receive critical security updates. CVE Numbering Authorities (CNAs) are an integral part of the CVE program, assigning unique IDs to new CVEs. These IDs help find information related to vulnerabilities, including severity ratings (maintained by NIST’s NVD), affected software systems, and steps for remediation and damage control. For example, the well-known Log4j vulnerability in 2021 (CVE-2021-44228 ) had a severity rating of 10 due to its wide impact.\nPractical Scenarios for CVEs Integrating CVE scanning into CI/CD pipelines is a common practice aimed at automatically identifying and preventing code with known vulnerabilities from entering the main branch. This approach helps ensure that applications do not rely on third-party packages or libraries with security vulnerabilities, enhancing application security. For example, GitHub’s Dependabot can automatically detect CVEs in project dependencies and suggest fixes, making it an effective tool for maintaining project security. You may receive CVE notifications like jQuery Cross-Site Scripting vulnerability whenever there is a vulnerability in your PR or commit. Then, you can choose to tolerate the vulnerability or apply a patch.\nWhat is TVS? Istio often releases CVE notices on its official website, such as ISTIO-SECURITY-2024-001 . Previously, you had to track these notices manually, but now you can automate CVE scanning tasks with TVS, significantly reducing the workload of security teams.\nThe following figure shows the result of TVS.\nTVS CLI Currently, TVS only provides a command-line tool, and it will be integrated as a service into TIS in the future.\nThe following diagram illustrates the workflow of TVS.\nMermaid Diagram Begins after Istio installation. Collects SHA digests of installed Istio containers. Sends digests to Tetrate’s API. API detects CVE presence. If CVE is detected, logs SHA digests without personal information and notifies users. If no CVE is detected, no action is required. Upon receiving notification, users apply patches or mitigations. Process ends. TVS is available for free download and use by everyone. However, registration is required before performing CVE scans, as outlined in the TIS documentation .\nSupply Chain Security Recommendations A report indicates that even the latest versions of the most popular containers have hundreds of CVEs.\nHere are some recommendations for ensuring security:\nStart vulnerability scanning and remediation early, rather than waiting until the end. Integrate vulnerability scanning tools into CI/CD. Regularly update Istio and Envoy to the latest versions. Use Istio’s officially released distroless images or TID to reduce the attack surface and minimize vulnerabilities. Follow the best practices for the software supply chain introduced by CNCF. Follow Istio security best practices . Unique Value of TVS TVS provides a convenient CVE scanning operation through its command-line tool and is planned to be integrated into Kubernetes and Tetrate Istio Subscription (TIS) in the future to further simplify CVE management processes for Istio and Envoy. TIS provides CVE patches and backward compatibility support for up to 14 months from the Istio release, helping users get security updates on time while keeping the system stable.\nTVS is freely available for all users to download and use, with simple registration required before performing CVE scans. Additionally, you can register on the Istio and Envoy alerts and patches page to receive CVE notifications and patches as soon as they are available. For more information, refer to the TIS documentation .\nBy adopting TVS, an automated CVE scanning tool, enterprises can more effectively identify and address security vulnerabilities in Istio and Envoy, enhance infrastructure security, and reduce the burden on security teams, thereby promoting efficient security management processes.\nThis blog was initially published at tetrate.io .\n","relpermalink":"/en/blog/tetrate-vulnerability-scaner/","summary":"This article will introduce Tetrate’s newly launched tool – Tetrate Vulnerability Scanner (TVS), a CVE scanner customized for Istio and Envoy. ","title":"TVS: Istio and Envoy CVE Scanning Solution"},{"content":"Last week, I attended KubeCon EU 2024 in Paris, marking my first participation in KubeCon outside of China. The conference was unprecedentedly grand, reportedly attracting 12,000 attendees. In this article, I’ll share some observations from KubeCon, focusing mainly on the areas of service mesh and cloud-native infrastructure that caught my attention.\nIstio Contributors at the Istio booth during KubeCon EU Istio, Cilium, and Service Mesh Istio and Service Mesh were hot topics, showcasing the latest developments and applications of these technologies within the cloud-native ecosystem. The conference covered various areas, including infrastructure optimization, data locality, distributed tracing, and multi-cluster deployment, reflecting the widespread attention and continuous innovation in Service Mesh technology.\nData Locality and Global Request Routing Arthur Busser and Baudouin Herlicq from Pigment shared how Kubernetes and Istio can fulfill data locality requirements. They introduced methods using Istio for request routing based on custom headers, crucial for meeting data residency requirements such as GDPR and CCPA.\nDistributed Tracing and Enhanced Observability Chris Detsicas from ThousandEyes (part of Cisco) discussed configuring Istio with OpenTelemetry for effective distributed tracing, providing valuable visibility into microservices ecosystems for problem diagnosis and performance optimization.\nMulti-cluster Deployment and Traffic Management Haiwen Zhang and Yongxi Zhang from China Mobile introduced a new approach to simplify Istio multi-cluster deployment. This method utilizes a globally unique Istio control plane, achieving global service discovery through the main cluster’s Apiserver, automatically connecting the container networks of multiple clusters, and providing direct network connectivity for Pods. They highlighted the Kosmos project , offering a new solution to simplify service mesh deployment and management in multi-cluster environments.\nAmeer Abbas and John Howard from Google discussed building 99.99% reliability services with infrastructure reliability at 99.9%. They proposed a series of application architecture prototypes (Archetypes) to design and implement highly reliable multi-cluster applications.\nPrototype 1: Active-Passive Zones - Deploy all services in two zones of a single region, using read-only replicas of SQL databases, with fault tolerance within the zone achieved through L4 load balancers. Prototype 2: Multi-Zonal - Deploy all services in three zones of a single region, using highly available SQL databases, with fault tolerance within the zone achieved through global or zonal load balancers. Prototype 3: Active-Passive Region - Deploy all services in three zones of two regions, using SQL databases replicated across regions, with fault tolerance between regions achieved through DNS and load balancers. Prototype 4: Isolated Regions - Deploy all services in three zones of two regions, using global databases like Spanner or CockroachDB, with fault tolerance between regions achieved through zonal load balancers and DNS. Prototype 5: Global - Deploy all services in three zones of two or more regions, using global databases like Spanner or CockroachDB, with fault tolerance achieved globally through global load balancers. Security and Zero Trust Architecture Several sessions emphasized the importance of securing Istio in production environments. Discussions led by Microsoft’s Niranjan Shankar focused on steps and strategies to reinforce Istio’s security using network policies, third-party Kubernetes tools, and cloud-provided security services to build zero trust and defense-in-depth architectures.\nInfrastructure Compatibility and Future of Ambient Mesh Benjamin Leggett and Yuval Kohavi introduced an innovative approach to enable Istio’s Ambient mode to support any Kubernetes CNI, detailed in the Istio blog . This advancement addresses the limited CNI support in Ambient mesh, allowing applications to be included in Ambient mode without restarting Pods, thus simplifying operations and reducing infrastructure costs.\nThe Istio community announced that Ambient mode will become beta in the upcoming Istio 1.22 release, as described in the CNCF blog . Multiple presentations and discussions focused on the future of Istio Ambient Mesh, especially its potential to simplify workload operations and reduce infrastructure costs. The introduction of Istio Ambient Mesh signals a new direction for service mesh technology, offering a data plane architecture without sidecars, providing higher performance and lower resource consumption.\nInnovation in Sidecar-less Service Mesh Discussions at KubeCon EU 2024 evaluated and compared the pros and cons of using sidecar-based and sidecar-less (such as Istio’s Ambient Mesh) service mesh models. Christian Posta’s in-depth analysis of design decisions and trade-offs between Cilium and Istio in implementing sidecar-less service mesh highlighted the potential of …","relpermalink":"/en/blog/kubecon-eu-paris-recap/","summary":"Explore KubeCon EU 2024: From the latest developments in Istio and Cilium to an in-depth interpretation of cloud native trends such as AI convergence, the rise of Wasm, and enhanced observation.","title":"KubeCon EU 2024: Impressions and Recap from Paris"},{"content":"Changes and Reminders for ICA Certification Upcoming policy change: Please note that the ICA certification expiration policy will change on April 1, 2024, at 00:00UTC. Certifications obtained on or after this date will expire 24 months after meeting the certification requirements (including passing the exam). We encourage anyone interested and prepared to schedule and take the exam before the policy change. Please see more detailed information here .\nThe current certification validity period is 3 years, and certifications obtained after April 1st will be valid for 2 years.\nBackground on Tetrate Academy Tetrate Academy , operated by Tetrate, has been around for several years, launching Istio fundamentals tutorials, Envoy fundamentals tutorials, and CIAT certification exams, with over 13,000 people having taken Tetrate Academy courses. In September last year, Tetrate contributed CIAT to CNCF, renamed it ICA exam , and the certification exam was officially launched in November.\nICA domains by Linux Foundation\nExam Background ICA is derived from the CIAT exam contributed by Tetrate, providing validation of extensive knowledge and skills in Istio. The exam is conducted remotely online, with a time limit of 2 hours, during which candidates need to operate Kubernetes clusters and Istio in a virtual machine environment. The exam includes a series of problem-solving questions and some multiple-choice questions, requiring a passing score of 75%. The PSI system provides an online exam environment, and exam results will be emailed to candidates within 24 hours after the exam. The exam environment provides tools such as Kubernetes clusters, Istio installation, VS Code, kubectl, istioctl, etc. Access to Istio documentation is available during the exam, and questions and responses are in English only. Recommendations and Reminders Familiarize yourself with the Istio documentation before the exam, and prepare in advance, including checking the PSI system, identification, and exam environment. Take Tetrate’s free Istio fundamentals tutorial to deepen your understanding of Istio. Stay calm during the exam, answer familiar questions first, then unfamiliar ones, to ensure efficient completion of the exam. Reference Links ICA Certification FAQs Important Instructions for ICA Certification ","relpermalink":"/en/blog/ica-certificate/","summary":"Learn about ICA certification's origin, changes, exam prep. Evolved from CIAT, it assesses Istio skills. Familiarize with Istio docs and take Tetrate's tutorial for better prep.","title":"ICA Certification: Latest Changes and Exam Preparation Guide for Istio Skills"},{"content":"I am excited to announce that I will be attending this year’s KubeCon \u0026amp; CloudNativeCon event, and it will be my first time having the opportunity to attend KubeCon in Europe. I am greatly looking forward to meeting everyone in Paris and sharing insights and experiences about cloud-native technologies. In addition to participating in the main event, I will also be attending Istio Day and will be available at Tetrate’s booth (J14) to meet with everyone. I hope to spend this exciting time with you all and welcome you to come and chat with me!\n","relpermalink":"/en/notice/kubecon-eu-2024/","summary":"Hope to see and talk with you there.","title":"See you in KubeCon Paris!"},{"content":"This blog post analyzes the challenges of server-side obtaining the client source IP in the Istio service mesh and provides solutions. The following topics will be covered:\nReasons for the loss of source IP during packet transmission. How to identify the client source IP. Strategies for passing source IP in north-south and east-west requests. Handling methods for HTTP and TCP protocols. The Importance of Preserving Source IP The main reasons for preserving the client source IP include:\nAccess Control Policies: Performing authentication or security policies based on source IP. Load Balancing: Implementing request routing based on the client IP. Data Analysis: Access logs and monitoring metrics containing the actual source address, aiding developers in analysis. Meaning of Preserving Source IP Preserving the source IP refers to avoiding the situation where the actual client source IP is replaced as the request goes out from the client, and passes through a load balancer or reverse proxy.\nHere is an example process of source IP address lost:\nMermaid Diagram The above diagram represents the most common scenario. This article considers the following cases:\nNorth-South Traffic: Clients accessing servers through a load balancer (gateway) Single-tier gateway Multi-tier gateways East-West Traffic: Service-to-service communication within the mesh Protocols: HTTP and TCP How to Confirm Client Source IP? In the Istio service mesh, Envoy proxies typically add the client IP to the “X-Forwarded-For” header of HTTP requests. Here are the steps to confirm the client IP:\nCheck the X-Forwarded-For Header: It contains the IP addresses of various proxies along the request path. Select the Last IP: Usually, the last IP is the client IP closest to the server. Verify the IP’s Trustworthiness: Check the trustworthiness of the proxy servers. Use X-Envoy-External-Address: Envoy can set this header, which includes the real client IP. For more details, refer to the Envoy documentation on the x-forwarded-for header . For TCP/IP connections, you can parse the client IP from the protocol field.\nTesting Environment GKE\nClient Version: v1.28.4 Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 Server Version: v1.27.7-gke.1121000 Istio\nClient version: 1.20.1 Control plane version: 1.20.1 Data plane version: 1.20.1 (12 proxies) CNI\nWe use Cilium CNI but have not enabled the kube-proxy-less mode.\ncilium-cli: v0.15.18 compiled with go1.21.5 on darwin/amd64 cilium image (default): v1.14.4 cilium image (stable): unknown cilium image (running): 1.14.5 Node\nNode Name Internal IP Remarks gke-cluster1-default-pool-5e4152ba-t5h3 10.128.0.53 gke-cluster1-default-pool-5e4152ba-ubc9 10.128.0.52 gke-cluster1-default-pool-5e4152ba-yzbg 10.128.0.54 Ingress Gateway Pod Node Public IP of the local client computer used for testing: 123.120.247.15\nDeploying Test Example The following diagram illustrates the testing approach:\nMermaid Diagram First, deploy Istio according to the Istio documentation , and then enable sidecar auto-injection for the default namespace:\nkubectl label namespace default istio-injection=enabled Deploy the echo-server application in Istio:\nkubectl create deployment echo-server --image=registry.k8s.io/echoserver:1.4 kubectl expose deployment echo-server --name=clusterip --port=80 --target-port=8080 Create an Ingress Gateway:\ncat \u0026gt; config.yaml \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: clusterip-gateway spec: selector: istio: ingressgateway # Choose the appropriate selector for your environment servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;clusterip.jimmysong.io\u0026#34; # Replace with the desired hostname --- apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: clusterip-virtualservice spec: hosts: - \u0026#34;clusterip.jimmysong.io\u0026#34; # Replace with the same hostname as in the Gateway gateways: - clusterip-gateway # Use the name of the Gateway here http: - route: - destination: host: clusterip.default.svc.cluster.local # Replace with the actual hostname of your Service port: number: 80 # Port of the Service EOF kubectl apply -f config.yaml View the Envoy logs in the Ingress Gateway:\nkubectl logs -f deployment/istio-ingressgateway -n istio-system View the Envoy logs in the Sleep Pod:\nkubectl logs -f deployment/sleep -n default -c istio-proxy View the Envoy logs in the Echo Server:\nkubectl logs -f deployment/echo-server -n default -c istio-proxy Get the public IP of the gateway:\nexport GATEWAY_IP=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) Test locally using curl:\ncurl -H \u0026#34;Host: clusterip.jimmysong.io\u0026#34; $GATEWAY_IP Resource IP After deploying the test application, you need to obtain the IP addresses of the following resources, which will be used in the upcoming experiments.\nPod\nHere are the initial Pod IPs, but please note that as patches are applied to the Deployment, Pods may be recreated, and their names and IP …","relpermalink":"/en/blog/preserve-source-ip-in-istio/","summary":"This article focuses on how to maintain transparency of the client source IP in the Istio service mesh.","title":"Maintaining Traffic Transparency: Preserving Client Source IP in Istio"},{"content":"Introduction I often answer questions on Istio’s GitHub Discussions , and recently, I came across a discussion about Istio’s primary-remote deployment, specifically regarding how the remote cluster’s gateway initially authenticates to an external Istiod instance. This issue touches upon the core security mechanisms of service meshes in multi-cluster configurations, which I think merits more in-depth sharing in the community.\nIn the official Istio documentation on Installing Primary-Remote on different networks , one of the steps is to attach cluster2 as a remote cluster of cluster1 . This process creates a Secret containing a kubeconfig configuration, which includes the certificates and tokens required to access the remote cluster (cluster2).\n# This file is autogenerated, do not edit. apiVersion: v1 kind: Secret metadata: annotations: networking.istio.io/cluster: cluster2 creationTimestamp: null labels: istio/multiCluster: \u0026#34;true\u0026#34; name: istio-remote-secret-cluster2 namespace: istio-system stringData: cluster2: | apiVersion: v1 clusters: - cluster: certificate-authority-data: {CERTIFICATE} server: {CLUSTER2-APISERVER-ADDRESS} name: cluster2 contexts: - context: cluster: cluster2 user: cluster2 name: cluster2 current-context: cluster2 kind: Config preferences: {} users: - name: cluster2 user: token: {TOKEN} The key role of this Secret is to enable Istio in the primary cluster (cluster1) to access the API server of the remote cluster, thereby obtaining service information. Additionally, in the remote cluster (cluster2), the Istiod service points to the primary cluster’s Istiod service’s LoadBalancer IP (ports 15012 and 15017), allowing cluster2 to communicate with the primary cluster’s Istiod.\nVisualizing Authentication Since both clusters share a CA (provided by the primary cluster) and the remote cluster can access its own API server, the Istiod in the primary cluster can validate requests from the remote cluster (cluster2). The following sequence diagram clearly shows this process:\nMermaid Diagram This process is a key component of Istio’s multi-cluster configuration, ensuring secure cross-cluster communication within the service mesh. As seen in this discussion, both the remote gateway and the services depend on the primary cluster’s CA for initial mTLS authentication, providing a solid foundation for secure communication across the entire service mesh.\nConclusion In this blog, we explored how the gateway in a remote cluster initially authenticates to an external Istiod in Istio’s primary-remote deployment. We explained how to create a Secret containing a kubeconfig to allow Istio in the primary cluster to access the remote cluster’s API and how shared CA and service account tokens ensure the security of mTLS authentication. This process secures cross-cluster communication within the service mesh, providing key insights for understanding and implementing Istio’s multi-cluster configuration.\nThis blog was initially published at tetrate.io .\n","relpermalink":"/en/blog/primary-remote-istio-ingress-gateway-mtls/","summary":"This blog deeply analyzes the initial authentication and mTLS connection process of remote gateways in Istio's primary-remote deployment mode.","title":"Deciphering Istio Multi-Cluster Authentication \u0026 mTLS Connection"},{"content":"In this blog, I’ll guide you through installing Tetrate Istio Subscription (TIS) and activating its monitoring add-on.\nUnderstanding Tetrate Istio Subscription Tetrate Istio Subscription is a comprehensive, enterprise-grade product offered by Tetrate. It provides thoroughly tested Istio versions compatible with all major cloud platforms. Derived from the open-source Tetrate Istio Distro project, TIS adds extensive support to these builds, including optional FIPS-validated cryptographic modules and a range of tested add-ons and integrations.\nWhy Choose TIS? TIS is not a fork but an upstream distribution of Istio tailored for specific environments. Enhancements made to Istio are integrated upstream. Key benefits of TIS include:\nExtended Support: TIS offers 14 months of security update support. Commercial Support: TIS provides business support options for enterprise use cases, including compliance needs. Ease of Management: TIS simplifies installation and management processes. Multi-Environment Adaptability: TIS supports various cloud environments. FIPS Validation: TIS offers FIPS-validated versions for high-security requirements. Visit the TIS docs for more information › Pre-Installation Requirements Before installing TIS and its plugins, you’ll need:\nTerraform for importing dashboards into Grafana. Credentials from Tetrate for installing TIS. tis_username tis_password Installing Istio and Monitoring Addons First, check the supported Istio versions with TIS:\nhelm search repo tetratelabs/base --versions NAME CHART VERSION APP VERSION DESCRIPTION tetratelabs/base1.20.1+tetrate01.20.1-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.20.0+tetrate01.20.0-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.19.5+tetrate01.19.5-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.19.4+tetrate01.19.4-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.19.3+tetrate01.19.3-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.18.6+tetrate01.18.6-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.18.5+tetrate01.18.5-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.18.3+tetrate01.18.3-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.17.8+tetrate01.17.8-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.17.6+tetrate01.17.6-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.16.7+tetrate01.16.7-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.16.6+tetrate01.16.6-tetrate0Helm chart for deploying Istio cluster resource... We’ll install the latest Istio version, 1.20.1.\nexport TIS_USER=\u0026#34;\u0026lt;tis_username\u0026gt;\u0026#34; export TIS_PASS=\u0026#34;\u0026lt;tis_password\u0026gt;\u0026#34; # Helm chart version export VERSION=1.20.1+tetrate0 # Image tag export TAG=1.20.1-tetrate0 kubectl create namespace istio-system kubectl create secret docker-registry tetrate-tis-creds \\ --docker-server=\u0026#34;addon-containers.istio.tetratelabs.com\u0026#34; \\ --docker-username=${TIS_USER} \\ --docker-password=${TIS_PASS} \\ -n istio-system # Install Istio helm install istio-base tetratelabs/base -n istio-system \\ --set global.tag=${TAG} \\ --set global.hub=\u0026#34;addon-containers.istio.tetratelabs.com\u0026#34; \\ --set \u0026#34;global.imagePullSecrets[0]=tetrate-tis-creds\u0026#34; \\ --version ${VERSION} helm install istiod tetratelabs/istiod -n istio-system \\ --set global.tag=${TAG} \\ --set global.hub=\u0026#34;addon-containers.istio.tetratelabs.com\u0026#34; \\ --set \u0026#34;global.imagePullSecrets[0]=tetrate-tis-creds\u0026#34; \\ --version ${VERSION} \\ --wait # install ingress Gateway kubectl create namespace istio-ingress kubectl create secret docker-registry tetrate-tis-creds \\ --docker-server=\u0026#34;addon-containers.istio.tetratelabs.com\u0026#34; \\ --docker-username=${TIS_USER} \\ --docker-password=${TIS_PASS} \\ -n istio-ingress helm install istio-ingress tetratelabs/istio-ingress -n istio-ingress \\ --set global.tag=${TAG} \\ --set global.hub=\u0026#34;addon-containers.istio.tetratelabs.com\u0026#34; \\ --set \u0026#34;global.imagePullSecrets[0]=tetrate-tis-creds\u0026#34; \\ --version ${VERSION} \\ --wait # Install TIS addon helm install istio-monitoring-demo tis-addons/istio-monitoring-demo --namespace tis --create-namespace Port forward the Grafana service and open it in your local browser: http://localhost:3000 :\nkubectl port-forward --namespace tis svc/grafana 3000:3000 Note: Keep the port-forwarding command running, as we’ll need access to this port for importing the dashboard into Grafana.\nWhy TIS Monitoring Addon? The Tetrate Istio Subscription (TIS) Monitoring Addon stands out by offering a tailored and advanced monitoring experience compared to standard Istio dashboards. It focuses on modern needs with four specialized dashboards, replacing outdated elements with relevant metrics. Key enhancements include a unified panel design, detailed service insights, and critical metrics like sidecar performance. Reserved for TIS customers, these …","relpermalink":"/en/blog/enhancing-istio-with-tis-comprehensive-installation-and-monitoring-guide/","summary":"In this blog, I’ll guide you through installing Tetrate Istio Subscription (TIS) and activating its monitoring add-on.","title":"Enhancing Istio with TIS: Comprehensive Installation and Monitoring Guide"},{"content":"Experience it now: Istio Advisor Plus GPT Note: A ChatGPT Plus subscription is required.\nIntroduction Introducing Istio Advisor Plus GPT, a powerful integration of ChatGPT with Istio Advisor. This fusion of AI and Istio expertise offers many capabilities to enhance your Istio experience.\nWhat Can You Achieve with Istio Advisor Plus GPT? Istio Advisor Plus GPT empowers you with:\nComprehensive Istio Insights: Gain an in-depth understanding of Istio, covering core concepts, configurations, and best practices. Configuration Assistance: Get tailored guidance on configuring Istio components to suit your specific use cases. Troubleshooting Expertise: Quickly diagnose and resolve Istio issues with expert insights and recommendations. Performance Optimization: Optimize your Istio service mesh for efficiency and scalability. Security Enhancement: Implement robust security measures using Istio’s security features in your service mesh. Visual Representation: Visualize complex networking and service mesh processes with interactive diagrams. Upgrading Support: Plan and execute Istio upgrades seamlessly with expert advice. Bug Reporting Assistance: Compile detailed bug reports for effective issue resolution. Documentation Guidance: Access relevant Istio documentation sections for deeper insights. Ecosystem Integration: Explore the integration of Istio with key ecosystem tools. A Knowledge Repository at Your Disposal Istio Advisor Plus GPT offers an extensive knowledge base:\nIstio and Envoy Fundamentals: Delve into Istio and Envoy’s core principles, architecture, and components. Service Mesh Concepts: Understand the foundational concepts and advantages of service meshes. Tetrate Documentation: Learn about Tetrate’s products, including Tetrate Service Bridge (TSB). Zero-Trust Security: Master the principles of zero-trust security in the service mesh context. Real-World Use Cases: Explore real-world applications and case studies showcasing Istio’s impact. The Latest Open-Source Projects’ Documents: Istio, Envoy Gateway, Kubernetes Gateway API, Cilium, Envoy, Tetragon, SkyWalking Structured and Comprehensive Responses Our responses are structured for clarity and comprehensiveness:\nTLDR: A concise summary of key points. Explanation: Broader context or background information. Detailed Answer: In-depth analysis or step-by-step guidance. Example: Practical illustrations of concepts. Next Steps: Actionable advice for implementation. References: Access to relevant sources. Unlock the full potential of Istio Advisor GPT with ChatGPT integration and revolutionize your Istio journey.\n","relpermalink":"/en/blog/introducing-istio-advisor-plus-gpt/","summary":"Unlock the Power of Istio Advisor GPT with ChatGPT Integration.","title":"Introducing Istio Advisor Plus GPT"},{"content":"I’m delighted to present Istio’s most recent release—Istio 1.19 . This blog will provide an overview of the updates bundled in this release.\nGateway API: Revolutionizing Service Mesh Our previous blog highlighted the Gateway API’s potential to harmonize ingress gateways in Kubernetes and service mesh, opening doors to cross-namespace traffic support. With Istio’s official endorsement, the Gateway API takes center stage. While traditionally applied to north-south traffic (the ingress and egress of the mesh), it now extends its prowess to the realm of east-west traffic, the lifeblood within the mesh.\nIn Kubernetes, services wear multiple hats, handling tasks from service discovery and DNS to workload selection, routing and load balancing. Yet, control over these functions has been limited, with workload selection being the notable exception. The Gateway API changes the game, putting you in command of service routing. This introduces some overlap with Istio’s VirtualService, as both wield influence over traffic routing. Here’s a glimpse into three scenarios:\nInternal Kubernetes Requests: Without Istio, all internal traffic in Kubernetes takes the service route. North-South Traffic: By applying the Gateway API to the Ingress gateway, incoming traffic to Kubernetes follows xRoute (currently supporting HTTPRoute, TCPRoute and gRPCRoute) to services. East-West Traffic: Inside Istio, as traffic enters the data plane, xRoute of the Gateway API takes charge. It guides the traffic to either the original or a new destination service. Figure 1: Traffic routing. This dynamic fusion of the Gateway API with Istio not only refines service networking but also solidifies Istio’s significance in the Kubernetes ecosystem.\nGateway API for Service Mesh: A Deeper Dive At its current experimental stage (as of v0.8.0), the Gateway API for Service Mesh introduces a fresh approach to configuring service mesh support in Kubernetes. It directly links individual route resources (such as HTTPRoute) with Service resources, streamlining the configuration process.\nHere are some key takeaways:\nExperimental Stage: As of version v0.8.0, the Gateway API for Service Mesh is still experimental. It’s advised not to use it in production environments.\nService and Route Association: Unlike using Gateway and GatewayClass resources, individual route resources are linked directly with Service resources when configuring a service mesh.\nFrontend and Backend Aspects of Service: The Service’s frontend encompasses its name and cluster IP, while the backend consists of its collection of endpoint IPs. This distinction facilitates routing within a mesh without introducing redundant resources.\nRoute Attachment to Service: Routes are attached to a Service to apply configuration to any traffic directed to that Service. The traffic follows the mesh’s default behavior if no Routes are attached.\nNamespace Relationships:\nSame Namespace: A Route in the same Namespace as its Service, known as a producer route, is typically created by the workload creator to define acceptable usage. It affects all requests from any client of the workload across any Namespace. Different Namespaces: A Route in a different Namespace than its Service, termed a consumer route, refines how a consumer of a given workload makes requests. This Route only influences requests from workloads in the same Namespace as the Route. Figure 2: Producer Route and Consumer Route. Combining Routes: Multiple Routes for the same Service in a single Namespace, whether producer or consumer routes, will be merged according to the Gateway API Route merging rules. This means defining distinct consumer routes for multiple consumers in the same Namespace is impossible.\nRequest Flow:\nA client workload initiates a request for a specific Service in a Namespace. The mesh data plane intercepts the request and identifies the target Service. Based on associated Routes, the request is allowed, rejected, or forwarded to the appropriate workload based on matching rules. Bear in mind that, in the experimental stage, the Gateway API for Service Mesh may undergo further changes and is not recommended for production use.\nBut wait, there’s more! Our journey doesn’t end here – the support for ingress traffic using the API is rapidly heading toward General Availability, promising even more dynamic developments!\nLet’s delve further into additional enhancements in this release.\nAmbient Mesh Enhancements The Istio team has been tirelessly refining the ambient mesh, an innovative deployment model that offers an alternative to the traditional sidecar approach. If you haven’t explored ambient yet, now’s the perfect time to dive into the introduction blog post .\nWith this update, we’ve amplified support for ServiceEntry, WorkloadEntry, PeerAuthentication and DNS proxying. Alongside, bug fixes and reliability enhancements ensure a seamless experience.\nRemember, the ambient mesh is in its alpha phase for this release. The Istio community …","relpermalink":"/en/blog/istio-119-release/","summary":"I’m delighted to present Istio’s most recent release—Istio 1.19. This blog will provide an overview of the updates bundled in this release.","title":"What’s New in Istio 1.19: Gateway API and Beyond"},{"content":"Cloud computing professionals, it’s been a month since the Istio Certified Associate (ICA) Certification journey began. A collaboration between the Cloud Native Computing Foundation (CNCF), Linux Foundation Training \u0026amp; Certification, and Tetrate, this certification has set a new benchmark in microservices management.\nTetrate , a key contributor to the Istio project, initially crafted the Certified Istio Administrator by Tetrate (CIAT), which laid the groundwork for the ICA Certification. Since its introduction, the ICA has been an instrumental part of the Kubernetes ecosystem, helping thousands to harness the power of service mesh technology.\nThe certification curriculum covers essential domains such as installation, traffic management, and security, reflecting the comprehensive expertise needed in the field. In a short span, the ICA has gained significant momentum, underpinned by the vision and dedication of Tetrate to the cloud-native landscape.\nAs we celebrate one month of the ICA, Tetrate’s commitment to open source education continues through their Tetrate Academy, providing free, high-quality courses on service mesh and Kubernetes security. For those aiming to deploy Istio in production, Tetrate offers the Tetrate Istio Distribution (TID), a secure and supported Istio distribution.\nIf you haven’t yet, now is the time to join this growing community of Istio experts. Enhance your career and contribute to the evolution of cloud-native technologies.\nStart Your ICA Certification Today! This is just the beginning. Let’s look forward to more milestones in service mesh proficiency, with Tetrate and CNCF leading the charge.\n","relpermalink":"/en/blog/ica-review/","summary":"CNCF and Tetrate's Istio certification marks 1 month, boosting Kubernetes skills in microservices, security, and traffic management.","title":"A Month in Review: The Impact of Istio Certified Associate (ICA) Certification with Tetrate's Contribution"},{"content":"The development of cloud-native applications has led to a shift in development to the left and a higher frequency of application iteration, which has given rise to the need for GitOps. This article will introduce how to use the Argo project, including ArgoCD and Argo Rollouts, to achieve GitOps and canary deployment with Istio. There is also a demonstration in the article that shows how to achieve GitOps based on the Istio environment provided by Tetrate Service Express (also applicable to Tetrate Service Bridge).\nThe deployment architecture diagram of the demo in this article is shown in Figure 1. If you are already familiar with the deployment strategies and Argo projects introduced in this article, you can skip directly to the demo section.\nFigure 1: Architecture diagram of using Istio and Argo projects in TSE/TSB to achieve GitOps and canary release Deployment strategy First of all, I want to briefly introduce the two deployment strategies supported by Argo Rollouts, which can achieve zero-downtime deployment.\nThe steps of blue-green deployment and canary deployment are shown in Figure 2.\nFigure 2: Steps of blue-green deployment and canary deployment Blue-green deployment is a strategy that deploys the new version of the application in a separate environment in parallel without affecting the current production environment. In blue-green deployment, the current production environment is called the “blue environment,” and the environment where the new version of the application is deployed is called the “green environment.” Once the green environment is considered stable and has passed the test, the traffic will gradually switch from the blue environment to the green environment, allowing users to gradually access the new version. If problems occur during the switching process, it can be quickly rolled back to the blue environment to minimize the impact on users. The advantage of blue-green deployment is that it can provide high availability and zero-downtime deployment. Canary deployment is a strategy for gradually introducing new versions or features into the production environment. In canary deployment, the new version or feature is first deployed to a small number of users in the production environment, called “canary users.” By monitoring the feedback and performance indicators of canary users, the development team can evaluate the stability and reliability of the new version or feature. If there are no problems, more users can be gradually included in the canary deployment until all users use the new version. If a problem is found, it can be quickly rolled back or fixed to avoid negative effects on the entire user group. The advantage of canary deployment is that it can quickly identify problems and make adjustments in a small impact area. The main difference between blue-green deployment and canary deployment is the deployment method and the scale of the changes. Blue-green deployment deploys the entire application in a new environment and then switches, which is suitable for large-scale changes, such as major upgrades of the entire application. Canary deployment gradually introduces new versions or features, which is suitable for small-scale changes, such as adding or modifying a single feature.\nIn terms of application scenarios, blue-green deployment is suitable for systems with higher requirements for high availability and zero-downtime deployment. When deploying large-scale changes, blue-green deployment can ensure stability and reliability and can quickly roll back to cope with unexpected situations. Canary deployment is suitable for systems that need to quickly verify new features or versions. By gradually introducing changes, problems can be discovered early and adjustments can be made to minimize the impact on users.\nRelease strategy of Kubernetes Deployment In Kubernetes, the Deployment resource object is one of the main tools for managing the deployment and updating of applications. Deployment provides a declarative way to define the expected state of an application and implements the release strategy through the controller’s functionality. The architecture of Deployment is shown in Figure 3, where the colored squares represent pods of different versions.\nFigure 3: Architecture diagram of Kubernetes Deployment The release strategy can be configured in the spec field of Deployment. Here are some common release policy options:\nManagement of ReplicaSet: Deployment uses ReplicaSet to create and manage replicas of an application. The desired number of replicas can be specified by setting the spec.replicas field. During the release process, the Kubernetes controller ensures that the number of replicas of the new version’s ReplicaSet gradually increases when created, and the number of replicas of the old version’s ReplicaSet gradually decreases when deleted to achieve a smooth switch. Rolling update policy: Deployment supports multiple rolling update policies, which can be selected by setting …","relpermalink":"/en/blog/implementing-gitops-and-canary-deployment-with-argo-project-and-istio/","summary":"This article discusses how to use Deployment, ArgoCD, and Istio to implement GitOps and canary deployment. ","title":"Implementing GitOps and Canary Deployment with Argo Project and Istio"},{"content":"In this article, I will explain how to use GraphQL to query data from SkyWalking with Postman . It includes steps to obtain the bearer token, construct a query to retrieve load metrics for a specific service, and use GraphQL introspection to see the schema of SkyWalking GraphQL APIs. The article also provides references for further information.\nWhat Is GraphQL? GraphQL is a query language and runtime for APIs developed by Facebook. It provides a more efficient, powerful, and flexible alternative to traditional REST APIs by allowing clients to specify exactly what data they need and receive only that data in response. With GraphQL, clients can query multiple resources in a single request, reducing the number of roundtrips to the server and improving performance.\nWhat’s the Difference between GraphQL and REST APIs? GraphQL allows clients to request only the data they need, while REST APIs require clients to retrieve everything in a resource regardless of whether they need it or not. Additionally, GraphQL allows clients to query multiple resources in a single request, making it more efficient and less chatty than REST APIs.\nHow Do I Query Data from SkyWalking? SkyWalking defines the communication protocol for the query stage. The SkyWalking native UI and CLI use this protocol to consistently fetch data from the backend, without needing to worry about backend updates.\nThere are two methods for querying metrics from SkyWalking:\nGraphQL APIs PromQL APIs This article provides a guide on how to use GraphQL to query metrics from SkyWalking. If you are interested in the PromQL APIs, you can refer to the article Build Grafana dashboards for Apache SkyWalking — Native PromQL Support .Continuing with the following steps requires a TSB installation. If you don’t have one and still want to experience using GraphQL to query data in SkyWalking, you can use the free demo environment (username/password: skywalking/skywalking) provided by SkyWalking. Log in to the demo website and get a token for queries. Endpoint address for GraphQL queries is http://demo.skywalking.apache.org/graphql . The steps to construct the query are the same as described below.\nObserve GraphQL Queries in TSB Before we use Postman to construct our own GraphQL query, let’s first observe how TSB obtains data from SkyWalking.\nOpen Chrome DevTools and switch to the Network tab. Visit the Organization – Services tab on the website. Watch the network request list and right-click on the one of the graphql requests, like in the following image:\nFigure 1: Chrome DevTool The curl commands you see will look like this. Execute the command in your terminal, and you will get a list of services managed by TSB from SkyWalking.\ncurl \u0026#39;\u0026lt;https://saturn.tetrate.work/ui/graphql\u0026gt;\u0026#39; \\ -H \u0026#39;Accept-Language: en,zh-CN;q=0.9,zh;q=0.8,en-US;q=0.7,zh-TW;q=0.6\u0026#39; \\ -H \u0026#39;Cache-Control: no-cache\u0026#39; \\ -H \u0026#39;Client-Timestamp: 1686104776136\u0026#39; \\ -H \u0026#39;Connection: keep-alive\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -H \u0026#39;Cookie: ...\u0026#39; \\ -H \u0026#39;Origin: \u0026lt;https://saturn.tetrate.work\u0026gt;\u0026#39; \\ -H \u0026#39;Pragma: no-cache\u0026#39; \\ -H \u0026#39;Referer: \u0026lt;https://saturn.tetrate.work/mp/services\u0026gt;\u0026#39; \\ -H \u0026#39;Request-Id: ...\u0026#39; \\ -H \u0026#39;Sec-Fetch-Dest: empty\u0026#39; \\ -H \u0026#39;Sec-Fetch-Mode: cors\u0026#39; \\ -H \u0026#39;Sec-Fetch-Site: same-origin\u0026#39; \\ -H \u0026#39;User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\u0026#39; \\ -H \u0026#39;X-Bridge-Csrf-Token: IOmJszLAqY3TRIUNhTuGu7vQgnfQY1FtgYFm+l/+Mu4EmVQU5T8EaQ7bngkCv4hQ12ZGids+I21pHMdepE9/qQ==\u0026#39; \\ -H \u0026#39;X-Csrf-Token: xTbxZerD3t8N3PaS7nbjKCfxk1Q9dtvvrx4D+IJohHicb0VfB4iAZaP0zh1eXDWctQyCYZWaKLhAYT3M6Drk3A==\u0026#39; \\ -H \u0026#39;accept: application/json\u0026#39; \\ -H \u0026#39;sec-ch-ua: \u0026#34;Not.A/Brand\u0026#34;;v=\u0026#34;8\u0026#34;, \u0026#34;Chromium\u0026#34;;v=\u0026#34;114\u0026#34;, \u0026#34;Google Chrome\u0026#34;;v=\u0026#34;114\u0026#34;\u0026#39; \\ -H \u0026#39;sec-ch-ua-mobile: ?0\u0026#39; \\ -H \u0026#39;sec-ch-ua-platform: \u0026#34;macOS\u0026#34;\u0026#39; \\ --data-raw $\u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;query ServiceRegistryListMetrics(...)}\u0026#39; \\ --compressed Note: Some fields in the above example are too long and replaced with dots (…).\nNext, I will guide you through constructing a query to retrieve the load metrics for a specific service.\nObtain the Bearer Token Firstly, you need to obtain the bearer of the website. Log in to TSB UI, click on the user button in the upper right corner, and then click “Show token information”. In the pop-up window, you will see the Bearer Token, as shown in the following image.\nFigure 2: Get the bearer token from the TSB UI Note: The validity period of the bearer token is relatively short. When it expires, you need to log in to TSB again to obtain a new token.\nWe have already deployed the bookinfo application in advance and sent some test traffic. To query the load metrics of reviews using GraphQL in the Postman client, follow these steps:\nCreate a new GraphQL request and enter the request URL: $TSB_ADDRESS/graphql Add the Authorization header with the value Bearer $TOKEN Use GraphQL Introspection to see the schema of SkyWalking GraphQL APIs. Find and click the readMetricsValues item. You will see the variables on the right side. Fill in the …","relpermalink":"/en/blog/how-to-use-graphql-to-query-observability-data-from-skywalking-with-postman/","summary":"This article explains how to use GraphQL to query observability data from SkyWalking with Postman. It first introduces GraphQL and SkyWalking, then explains how to set up Postman to send GraphQL queries, and finally provides some example GraphQL queries that can be used to query observability data from SkyWalking.","title":"How to Use GraphQL to Query Observability Data from SkyWalking with Postman"},{"content":"Editor’s Recommendation The authors of this book are global experts in service mesh technology, providing comprehensive and detailed content. The translators of this book are technical experts from the domestic cloud-native community, striving to convey the content accurately and providing localization for difficult-to-understand concepts. The book covers comprehensive content, including the latest versions and core concepts of Istio, with rich and complete example codes. Istio in Action (Chinese edition: Best Practices with Istio) Overview As a representative product of service mesh technology, Istio has matured over the years and is increasingly favored by developers. This book focuses on Istio service mesh, covering basic concepts, core features, operations, and enterprise-level implementations. From basic installation and deployment to practical functions, from analyzing underlying principles to troubleshooting, and from advanced operations to enterprise-level practices, it provides a comprehensive introduction to various aspects of Istio service mesh.\nThis book is suitable for developers, operations engineers, architects, and other practitioners in the cloud-native field who are currently using or interested in Istio. Whether you are a beginner in service mesh technology or an expert in the field, you can find theoretical insights and practical guidance in this book.\nAbout the Authors Christian Posta is the Chief Technology Officer for Global Field at Solo.io. He is well-known in the cloud-native community as an author, blogger, speaker, and contributor to various open-source projects in the service mesh and cloud-native ecosystem. With experience working in both traditional enterprises and large internet companies, Christian now helps organizations create and deploy large-scale, cloud-native, resilient distributed architectures. He specializes in guiding, training, and leading teams to success in distributed systems concepts, microservices, DevOps, and cloud-native application design.\nRinor Maloku is an engineer at Solo.io, providing consulting services to clients adopting application networking solutions such as service mesh. Previously, he worked at Red Hat, where he developed middleware software that enabled development teams to ensure the high availability of their services. As a freelancer, he has served several DAX 30 members to fully leverage the potential of cloud computing technologies.\nAbout the Translators Ma Ruofei works as Chief Engineer at FreeWheel Beijing R\u0026amp;D Center, primarily responsible for microservices architecture design and cloud-native landing work. He is the author of “Istio Practical Guide,” columnist for GeekTime’s “Service Mesh in Action,” and the main author of “Cloud-Native Application Architecture: Best Practices for Microservices Development.” He is also an IT professional book expert consultant for People’s Posts and Telecommunications Press, as well as a member of the ServiceMesher technical community and the Cloud Native Community management committee. He has published and translated numerous articles on cutting-edge technologies in the cloud-native field and is passionate about technology sharing.\nSong Jingchao (Jimmy Song) is a Developer Advocate at Tetrate, the founder of the cloud-native community, and the former Developer Advocate and Open Source Manager at Ant Group. He is also a published author and independent writer. He is an early adopter and promoter of open-source technologies such as Kubernetes and Istio. He has authored books such as “Future Architecture: From Service to Cloud-Native” and “Understanding Istio: Advanced Practical Guide to Cloud-Native Service Mesh” and has participated in the translation of many works.\nLuo Guangming is an architect at ByteDance’s Service Framework Team, a member and Beijing stationmaster of the Cloud Native Community management committee. He has previously worked on cloud-native, microservices, and open-source-related projects at Ericsson and Baidu before joining ByteDance, where he is responsible for open-source projects related to microservices like CloudWeGo. He has been focusing on cutting-edge technologies, architectural evolution, and standardization processes in the cloud-native and microservices fields for a long time.\n","relpermalink":"/en/book/istio-in-action/","summary":"Authored by Christian Posta and Rinor Maloku.","title":"Istio in Action"},{"content":"In June, Istio 1.18 was released , marking the second release of Istio in 2023 and the first to offer official support for ambient mode . Tetrate’s Paul Merrison was one of the release managers for this version, and Tetrate’s contributions to this release included various customer-driven usability enhancements and important work in the underlying Envoy Proxy upon which Istio depends. When asked about the experience of working on Istio 1.18, Paul said “working as the lead release manager for Istio 1.18 gave me a fascinating insight into how a group of super talented people from around the world come together, organize themselves and ship software. There was a steep learning curve, but the Istio community is awesome and I was supported brilliantly. The biggest challenge was definitely learning and executing all the steps that are needed to bring a release to life, but the feeling of achievement when it finally made its way out into the world will stay with me for a while!” Istio first announced the introduction of Ambient mode in September last year, which was covered in detail by Zack in this blog post , where he explains the differences between ambient mode and sidecar mode.\nThis release introduces many new features and changes in addition to ambient mode, including enhanced Kubernetes Gateway API support, health checks for virtual machines that are not automatically registered, support for expired metrics, enhanced istioctl analyze, and more. See the release blog for details. The most significant of these are the ambient mode and Gateway API enhancements, detailed below.\n“Working as the lead release manager for Istio 1.18 gave me a fascinating insight into how a group of super talented people from around the world come together, organize themselves and ship software. There was a steep learning curve, but the Istio community is awesome and I was supported brilliantly. The biggest challenge was definitely learning and executing all the steps that are needed to bring a release to life, but the feeling of achievement when it finally made its way out into the world will stay with me for a while!”\nPaul Merrison, Tetrate Engineering and Istio Release Manager\nWhat Is Ambient Mode? Before discussing ambient mode, it is essential to understand the current “sidecar mode” used by Istio. Sidecar mode is the default data plane mode used by Istio, where each application pod comes equipped with a sidecar proxy (usually Envoy) that handles all network traffic in and out of the pod, providing Istio’s core functionality such as Zero Trust security, telemetry and traffic management. While sidecar mode is suitable for most users, ambient mode offers some advantages in specific circumstances. For more information on the differences between ambient mode and the standard sidecar mode, see our article on Ambient Mesh: What You Need to Know about This Experimental New Deployment Model for Istio .\nWhat Are the Design Goals of Ambient Mode? Non-intrusive: Ambient mode does not require injecting sidecar proxies into the application’s pods and only requires the application to be tagged to automatically join the mesh, potentially reducing the mesh’s impact on the application. Efficient resource utilization: Ambient mode can optimize resource utilization for some use cases by sharing the ztunnel proxy across the mesh. If certain L7 functionality of Istio is required, it can also be addressed by deploying Waypoints precisely for a ServiceAccount or Namespace, providing more control over resource consumption. Performance parity with sidecar mode: Ambient mode initially adopted a shared proxy model based on Envoy, but during development, issues such as complex Envoy configuration were discovered, leading the Istio community to develop its shared proxy (ztunnel ) based on Rust. In the future, ambient mode is expected to have comparable performance to traditional sidecar mode. Security: Ambient mode provides TLS support by running a shared proxy ztunnel on each node, and when users require the same security as sidecar mode, they also need to deploy one or more Waypoints in each namespace to handle L7 traffic in that namespace. Users can use istioctl x waypoint for Waypoint configuration management. For example, running the istioctl x waypoint generate command generates a Kubernetes Gateway API resource managed by Istio.\nOverall, ambient mode promises to offer additional flexibility to Istio’s deployment model which may prove helpful to some users. It should be noted that the ambient mode is still in the alpha stage and has yet to achieve production-level stability.\nEnhanced Kubernetes Gateway API Support Istio 1.18 introduces several vital improvements and modifications to its Kubernetes Gateway API support:\nSupport for v1beta1: When upgrading to the new version of Istio, Gateway API version greater than 0.6.0+ is required. Use the istioctl x precheck command to check for upgrade issues.\nGateway API automated deployment management upgrades: …","relpermalink":"/en/blog/istio-1-18-released-now-with-ambient-mode-available/","summary":"This article introduces Istio 1.18, the latest release of the service mesh platform. It highlights the new features and improvements, such as ambient mode, which allows Istio to run on any Kubernetes cluster without requiring a dedicated control plane. It also explains how to get started with Istio 1.18 using Tetrate’s distribution and support.","title":"Istio 1.18 Released, Now with Ambient Mode Available"},{"content":"In a previous blog post , I discussed how Istio Ambient Mesh uses iptables and Geneve tunnels to intercept traffic from application pods into Ztunnel. Many readers may not be familiar with this tunneling protocol, so this article will introduce the definition, packet structure and advantages of Geneve tunnels compared with the VXLAN protocol. Finally, this article will introduce how Istio Ambient Mesh applies Geneve tunnels to implement traffic interception and the new eBPF mode introduced in Istio 1.18.\nIntroduction to Geneve Tunnels In order to address the lack of flexibility and security in current data transmissions, the Geneve (Generic Network Virtualization Encapsulation) network virtualization encapsulation (tunneling) protocol was created. Geneve only defines a data encapsulation format, excluding control plane information. The key advantage of Geneve over VXLAN encapsulation is that it extends the types of encapsulated protocols by adding TLV format options.\nGeneve vs. VXLAN VXLAN and Geneve are both network virtualization protocols and they have many similarities. Virtualization protocols are technologies that separate virtual networks from physical networks. They allow network administrators to create multiple virtual networks in a virtual environment, each of which can have its own VLAN identifiers, IP addresses and routing. In addition, VXLAN and Geneve use UDP encapsulation, which enables them to be extended through existing network infrastructure. VXLAN and Geneve protocols are also flexible, can be used in different network topologies and are compatible with different virtualization platforms.\nFigure 1 shows the message structure of VXLAN and Geneve tunnels and the differences in their respective headers.\nFigure 1: VXLAN and Geneve packet format schematic diagram. From the figure, we can see that the message structure of VXLAN and Geneve tunneling protocols is similar, with the main difference being the use of different UDP port numbers and protocol headers. VXLAN uses port 4789, while Geneve uses port 6081. The Geneve protocol header is more extendable than VXLAN.\nThe Geneve tunneling protocol adds variable-length options that can contain zero or more option data in TLV format, making it more scalable than VXLAN. TLV stands for Type-Length-Value, which is a format for parsing and transmitting metadata in network packets. Each metadata information in the Geneve protocol is composed of a TLV format field, making it simple to flexibly add, delete and modify these metadata.\nThe TLV format field contains the following data:\nType: 8-bit type field. Length: 5-bit option length field, represented in multiples of 4 bytes, excluding the option header. Data: Variable-length option data field, which may not exist or may be between 4 and 128 bytes. The Geneve protocol can easily modify and extend metadata information while maintaining compatibility and flexibility by using the TLV format.\nPlease refer to RFC 7348 Virtual eXtensible Local Area Network (VXLAN): A Framework for Overlaying Virtualized Layer 2 Networks over Layer 3 Networks for more information about VXLAN. For more information about the Geneve tunnel packet format, please refer to RFC 8926 Geneve: Generic Network Virtualization Encapsulation .\nHow it Works The Geneve tunnel is mainly used in cloud computing and virtualization scenarios, and it can encapsulate packets in a new packet for transmission in a virtual network. The Geneve tunnel uses a 24-bit VNI (Virtual Network Identifier) to transmit packets from one physical network to another. The Geneve tunnel can also use security protocols such as IPsec and TLS to protect the transmission of packets.\nWhen a packet reaches the destination host, the Geneve tunnel protocol will de-encapsulate the packet from the Geneve protocol header and deliver it to the destination in the virtual network. During the de-encapsulation process, the VNI information in the Geneve protocol header is used to determine the destination of the packet, ensuring that the packet is correctly routed to the destination in the virtual network.\nAssuming there is a virtual network with a VNI of 1001. When a packet is transmitted from one physical network to another, a tunnel can be used to track the packet during transmission by setting the VNI between the source and target physical networks to 1001. When the packet reaches the target physical network, the VNI is removed from the packet and the packet is delivered to the target physical network.\nSecurity The Geneve tunnel protocol itself does not provide any security mechanisms, so packets transmitted in the Geneve tunnel can be subject to threats such as packet tampering, interception and replay.\nTo ensure the security of packets transmitted in the Geneve tunnel, some security protocols can be used. The following are some common security protocols:\nIPsec (Internet Protocol Security): IPsec is a network layer security protocol that can encrypt, authenticate and provide …","relpermalink":"/en/blog/traffic-interception-with-geneve-tunnel-with-istio-ambient-mesh/","summary":"This article introduces Geneve tunnels, a network virtualization protocol that can intercept Istio ambient mesh traffic more flexibly and securely than VXLAN. It also explains how Istio Ambient Mesh uses Geneve tunnels and the new eBPF mode in Istio 1.18.","title":"Using Geneve Tunnels to Implement Istio Ambient Mesh Traffic Interception"},{"content":"Envoy Gateway , the open-source API Gateway based on Envoy Proxy, has just released version 0.4.0 . This release is centered around customization, with the goal of enabling more use cases for end-users. In this blog post, we will discuss the new customizations available in this release and their significance for users.\nCustomizing Envoy Proxy Infrastructure One of the main customizations in this release is the ability to configure the exact type of EnvoyProxy (CRD) deployment. You can define the number of replicas, images, and resource limits that EnvoyProxy deploys. You can also add annotations to EnvoyProxy deployments and services. This makes different use cases possible, such as:\nLinking Envoy Gateway to external load balancers like AWS, NLB, ELB, and GCP. Injecting a sidecar alongside EnvoyProxy is very useful for managing the North-South traffic in the Envoy Gateway at the ingress layer and for managing the East-West traffic and enabling mutual TLS (mTLS) in the service mesh layer with the Envoy sidecar. This custom feature eliminates the need for users to create their own certificates, as it is based on Istio certificate management. Refer to the Envoy Gateway documentation for more customized features on Envoy Gateway.\nMulti-Tenant Deployment Modes Furthermore, Envoy Gateway has added support for other deployment modes in addition to the default Kubernetes single-tenant mode, such as multi-tenancy, as shown in Figure 1 below.\nFigure 1: Envoy Gateway multi-tenancy deployment mode. Deploy an Envoy Gateway Controller to each tenant’s namespace, which watches HTTPRoute and Service resources in Kubernetes, and creates and manages EnvoyProxy deployments in their respective namespaces.\nCustomizing Envoy xDS Bootstrap Another significant customization in this release is the ability to customize the Envoy xDS Bootstrap. With this feature, users can provide a bootstrap configuration to configure some static resources when starting up Envoy. A good case is configuring access logging, tracing and metrics to be sent to SkyWalking, which can work as an APM. Additionally, the release adds a lot of CLI tooling to help validate user configuration. Users can use the CLI as a dry run to change a specific field in Bootstrap, and it will fail if the config is not syntactically correct.\nExtending the Control Plane Envoy Gateway now provides the ability to allow vendors and extension developers to add gRPC hooks at different stages of the Envoy Gateway pipeline to further extend its functionality, allowing users to do things like enhance the xDS configuration being sent to Envoy, which was not possible before.\nSummary In conclusion, Envoy Gateway 0.4.0 extends the API for customization and enables more use cases for end-users. The new customizations include the ability to customize Envoy deployment, Envoy xDS Bootstrap, and the control plane. With the release of this version, Envoy Gateway is becoming more user-friendly and is positioning itself as a great alternative to ingress-nginx.\nThis blog was initially published at tetrate.io .\n","relpermalink":"/en/blog/envoy-gateway-customization/","summary":"In this blog post, we will discuss the new customizations available in this release and their significance for users.","title":"Envoy Gateway 0.4.0: Extending the API for Customization"},{"content":"Artificial Intelligence (AI) is changing our lives, making our work more efficient and intelligent. In this rapidly developing field, there are many practical AI tools that can help us better accomplish our work. In the future, mastering various AI tools to optimize your workflow and improve work efficiency will be a necessary skill for everyone. It’s time to gather some cheap and practical AI tools. Below are some recommended practical AI tools that are worth collecting. You can directly use these tools without additional programming knowledge. Most of these tools are free to use, or provide free usage quotas, or have low usage costs.\n1. ChatGPT ChatGPT ChatGPT is an intelligent chatbot based on GPT technology, which can interact with users in natural language. It uses deep learning technology and large-scale trained language models to understand users’ questions and provide useful answers.\nChatGPT can answer various questions, and users can directly input questions or topics on the website and get quick and accurate answers. It should be noted that ChatGPT is an online chatbot, and its answers may not be 100% accurate. In addition, the data that ChatGPT model is trained on is up to 2021, and it is still in the early development stage, and is constantly improving and optimizing.\nRecommendation Reasons\nChatGPT’s response speed is super fast, and you can get an answer in a few seconds, which is especially suitable for situations where quick replies are needed. You can also use OpenAI’s API to create your own tools. However, the free version of ChatGPT’s response speed is sometimes slow, and frequently refreshing the page is required when following up, and there are also limits on the number of characters for questions and answers.\nThe commonly used functions include:\nWriting code Translation Proofreading articles Summarizing an article (you can output a URL) Learning a knowledge area that you are not familiar with In addition, a ChatGPT desktop application is recommended: https://github.com/lencx/ChatGPT , as well as the ChatGPT prompt project https://github.com/f/awesome-chatgpt-prompts .\nChatGPT Plus ChatGPT Plus is an enhanced version of ChatGPT that utilizes GPT-3 technology to provide more powerful and intelligent natural language processing capabilities. It can assist users in completing various tasks such as generating articles, translation, Q\u0026amp;A, speech conversion, and chatbots.\nChatGPT Plus has a sleek and beautiful interface that is very intuitive and user-friendly. Users can input text using various methods such as keyboard input, voice input, and image input. ChatGPT Plus also offers multiple language and style options, making it easy for users to generate high-quality text and speech.\n2. Smodin Smodin Smodin.io is an online tool based on artificial intelligence and natural language processing technology that can help users generate content such as articles, press releases, blog posts, and social media posts. The website uses deep learning technology and language models to automatically generate high-quality text and offers a variety of language and style options.\nUsing Smodin.io is very simple. Simply enter the topic, keywords, desired language, and style you wish to generate, then click the “Generate” button to obtain a high-quality article. You can also make adjustments and edits as needed to meet your specific needs.\nIn addition to generating text, Smodin.io also offers other useful features such as grammar and spell check, SEO optimization suggestions, and real-time translation. The website is very flexible and convenient to use, making it suitable for individuals and businesses who require high-quality text, such as marketers, editors, writers, and bloggers.\nIt’s important to note that while Smodin.io can save you a lot of time and effort, since the text is generated automatically, there may be some syntax or logic errors. Therefore, appropriate review and editing are necessary when using it.\nRecommended Reasons\nThe website supports over 40 languages, limits input to 1000 characters per session, and returns rewritten results quickly. Free users have a limit on the number of calls they can make, while paid users have a limit. There are two payment tiers, both of which are relatively inexpensive.\n3. Bing Bing chat Bing is a search engine developed by Microsoft that integrates artificial intelligence technology to provide more intelligent search results. Recently, Microsoft announced a new feature called “Chat” added to Bing, where users can input questions in the chat box and Bing will automatically answer them, just like an intelligent chatbot. Bing also provides multiple conversation styles to choose from.\nIt should be noted that the chat function is currently in the testing phase and may have some issues and limitations. Additionally, Bing’s availability may be restricted in some countries and regions. However, it is still a very useful search engine that can help users quickly find the …","relpermalink":"/en/blog/ai-tools-collection/","summary":"Are you looking for ways to optimize your workflow and increase productivity? Here are some useful AI tools for you.","title":"Useful AI Tools to Optimize Your Workflow"},{"content":"In the previous blog post , I introduced how Istio manages certificates, and in this article, I will guide you on how to use an external certificate authority (CA) to achieve fine-grained certificate management and automatic certificate rotation through the integration of SPIRE and cert-manager .\nIf you are not familiar with SPIRE and what it’s used for, we recommend reading the following articles:\nWhy Would You Need SPIRE for Authentication with Istio? How to integrate SPIRE in Istio Introduction to the Certificate Issuance and Management Process Figure 1 shows the certificate trust chain used in this article based on cert-manager and SPIRE:\nFigure 1: Certificate trust chain based on cert-manager and SPIRE. cert-manager acts as the root CA to issue certificates to istiod and SPIRE. We use a self-signed issuer , but you can also configure it to use built-in issuers such as Let’s Encrypt, Vault, Venafi, or other external issuers. You can also choose to use other UpstreamAuthorities , such as Vault, SPIRE Federation, etc. SPIRE issues SVID certificates to the workloads and ingress Gateway and egress Gateway in the Istio mesh for mTLS between services. The certificates used when accessing the ingress Gateway from outside the mesh and when the egress Gateway access services outside the mesh need to be configured separately. Figure 2 shows the certificate issuance and update process after integrating SPIRE and cert-manager in Istio.\nFigure 2: Certificate issuance and update process after integrating SPIRE and \u0026lt;em\u0026gt;cert-manager\u0026lt;/em\u0026gt; in Istio. The Kubernetes Workload Registrar in the SPIRE Server automatically registers the workloads in Kubernetes and generates SPIFFE standard identities for all workloads. cert-manager issues and manages the CA certificates for istiod. The Envoy proxies in the workloads send certificate signing request (CSR) requests to the SPIRE Agent on the same node through the SDS API via a Unix domain socket (UDS). The SPIRE Agent sends the CSR to the SPIRE Server. The SPIRE Server returns the signed certificate to the SPIRE Agent. The SPIRE Agent returns the signed certificate to the workloads. SPIRE is responsible for the certificate management and updates for the workloads. Now that we have a general understanding of the process, let’s install the components manually.\nInstall cert-manager Run the following command to install cert-manager, which we will use for automatic certificate rotation:\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.10.1/cert-manager.yaml The root CA is a self-signed certificate. Run the following command to configure the root CA:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: selfsigned namespace: cert-manager spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: selfsigned-ca namespace: cert-manager spec: isCA: true duration: 21600h secretName: selfsigned-ca commonName: certmanager-ca subject: organizations: - cert-manager issuerRef: name: selfsigned kind: Issuer group: cert-manager.io --- apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: selfsigned-ca spec: ca: secretName: selfsigned-ca EOF Then configure certificates for istiod:\nkubectl create namespace istio-system cat \u0026lt;\u0026lt; EOF | kubectl apply -f - --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: cacerts namespace: istio-system spec: secretName: cacerts duration: 1440h renewBefore: 360h commonName: istiod.istio-system.svc isCA: true usages: - digital signature - key encipherment - cert sign dnsNames: - istiod.istio-system.svc issuerRef: name: selfsigned-ca kind: ClusterIssuer group: cert-manager.io EOF Now that we have installed cert-manager and created a ClusterIssuer named selfsigned-ca, let’s install SPIRE and use cert-manager as the UpstreamAuthority for SPIRE.\nInstalling SPIRE Quickly install SPIRE by running the following command:\nkubectl apply -f https://gist.githubusercontent.com/rootsongjc/5dac0518cc432cbf844114faca74aa40/raw/814587f94bbef8fb1dd376282249dcb2a8f7fa1b/spire-with-cert-manager-upstream-authority-quick-start.yaml This YAML file adapts to cert-manager compared to the samples/security/spire/spire-quickstart.yaml file in the Istio 1.16 installation package, such as:\nAdding permissions for the cert-manager.io API group to the spire-server-trust-role ClusterRole; Adding an UpstreamAuthority “cert-manager” configuration in the SPIRE Server configuration. Note: The trust_domain in the SPIRE Server configuration should be consistent with the TRUST_DOMAIN environment variable specified when installing Istio.\nThis command installs the Kubernetes Workload Registrar , automatically registering the workloads in Kubernetes. All workloads will be registered with the SPIFFE standard service identity format spiffe://\u0026lt;trust-domain\u0026gt;/ns/\u0026lt;namespace\u0026gt;/sa/\u0026lt;service-account\u0026gt; based on their service accounts.\nIf you want to adjust the TTL of the SPIRE CA and SVID …","relpermalink":"/en/blog/cert-manager-spire-istio/","summary":"In this blog, I will guide you on how to use an external certificate authority (CA) to achieve fine-grained certificate management and automatic certificate rotation through the integration of SPIRE and cert-manager.","title":"Managing Certificates in Istio with cert-manager and SPIRE"},{"content":"I mentioned in my last article on understanding mTLS traffic encryption in Istio that the key to traffic encryption is certificate management. We can use the built-in certificate authority (CA) in Istio or a custom CA to manage certificates within the mesh. This blog post will explain how Istio handles certificate management.\nWhat Is a Certificate? There are many types of certificates, but in this article, I am explicitly referring to X.509 V3 certificates. X.509 certificates are a standard digital format used to identify entities in computer networks. X.509 is the international standard for public key infrastructure (PKI) and is primarily used for identity authentication and information encryption, such as TLS.\nThe contents of a certificate are hashed using a hash function and then signed with the issuer’s private key. This way, when a certificate is received, the recipient can use the issuer’s public key to verify the certificate’s validity.\nA hash function is a function that maps an input of any length (also called a message) to a fixed-length output, also called a hash value or message digest. There are many hash functions, such as MD5, SHA-1, etc.\nA certificate is like a business card issued by an authoritative agency for the user to prove their identity, and it can also be used to encrypt information to ensure the security and integrity of communication. The following diagram shows the general steps of TLS communication, where the certificate proves the server’s identity and encrypts the communication.\nFigure 1 shows an example of an HTTP call to a website, issuing a digital certificate, authenticating, and encrypting the communication.\nFigure 1. TLS certificate issuance and validation process Here are the detailed steps:\nThe server (website owner) submits a certificate signing request to the CA; The CA verifies the server’s identity and the authenticity of the website and issues a digital certificate to the server, which the server installs so that visitors can verify the website’s security; The user sends a request to the website through a browser (client); The server returns the TLS certificate to the client; The client verifies the certificate’s validity with the CA and establishes the connection if the certificate is valid, or prompts the user to reject the connection if it is invalid; The client generates a pair of random public and private keys; The client sends its public key to the server; The server encrypts the message using the client’s public key; The server sends the encrypted data to the client; The client decrypts the data sent by the server using its private key. At this point, both parties have established a secure channel and can transmit encrypted data in both directions.\nHow to Generate Certificates You can generate X.509 certificates with the following open-source tools:\nEasy-RSA : A simple command-line tool maintained by the OpenVPN project, EasyRSA can easily generate secure certificates and keys for the OpenVPN network. OpenSSL : Originated by an individual in 1995 and now maintained by an independent organization, OpenSSL provides only a command-line tool. CFSSL : Developed and maintained by CloudFlare, CFSSL is not just a command-line tool for generating certificates but also can serve as a PKI server. BoringSSL : A branch of OpenSSL developed and maintained by Google, BoringSSL is used in the Chrome browser and Android operating system. Since most people are probably familiar with OpenSSL, we will use OpenSSL to create certificates in the following text.\nCertificate Trust Chain The validation of a certificate requires using a certificate trust chain (Certificate Trust Chain). A certificate trust chain refers to a series of certificates used for identity verification, which form a chain starting from a trusted root certificate issuing agency, connecting downward step by step, until a specific intermediate or terminal certificate is used for verifying a particular certificate. The trustworthiness of a digital certificate increases as the certificate level increases in the certificate trust chain.\nIn Figure 2, you can see four trust chains.\nFigure 2. Certificate trust chains Certificate trust chains are tree-like structures, where each CA can have one or more child CAs. There are three roles:\nRoot CA: The top-level CA can issue certificates to intermediate CAs. Intermediate CA: They can issue end-entity certificates. End entity: A device or service that holds an end-entity certificate. Root CAs are the top-level issuing authorities for digital certificates, so the certificates they issue are the most trustworthy. Root certificate authorities are usually operated and regulated by government agencies or other authoritative organizations such as the International Infrastructure Security Organization. Common root CAs include:\nSymantec/VeriSign Comodo DigiCert GlobalSign GoDaddy Entrust GeoTrust RapidSSL Baltimore CyberTrust Root Please note that the above list is just a sample. …","relpermalink":"/en/blog/istio-certificates-management/","summary":"This blog post will explain how Istio handles certificate management.","title":"How Are Certificates Managed in Istio?"},{"content":"In my last blog , I introduced transparent traffic intercepting and L4 routing in Ambient mode. In this blog, I will show you how L7 traffic is routed.\nThe figure below shows the L7 network traffic path in ambient mode.\nFigure 1: L7 network traffic in ambient mesh Note: The Waypoint proxy can be located on the same node as the application, and even all of the service and the Waypoint proxy can be on the same node. I draw them on three nodes for display purposes, but it has no significant impact on the actual traffic path, except that it is no longer sent to another node via eth0.\nIn the following section, we will explore the process in Figure 1 from a hands-on perspective.\nEnvironments for Waypoint Proxy Let’s continue to view the environment description using the ambient mode Istio deployed in the previous blog. To illustrate the L7 network routing, we need to create a Gateway on top of this.\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: gateway.networking.k8s.io/v1alpha2 kind: Gateway metadata: name: productpage annotations: istio.io/service-account: bookinfo-productpage spec: gatewayClassName: istio-mesh EOF After executing this command, a Waypoint proxy is created under the default namespace; in my environment, this pod is named bookinfo-productpage-waypoint-proxy-6f88c55d59-4dzdx and is specifically used to handle L7 traffic to the productpage service ( Service B), which I will call Waypoint proxy B.\nThe Waypoint proxy may be deployed in a different namespace, on a different node from the workload, or both. No matter where the Waypoint proxy is situated, the L7 traffic path is unaffected.\nWe will omit the sections of this blog dealing with intercepting inbound and outbound traffic because the way transparent traffic is handled in ambient mesh in L4 and L7 networks is similar. Details are available in the prior blog .\nWe will start directly with the traffic intercepted at Ztunnel A and then forward it to Envoy port 15006.\nOutbound Traffic Routing on Ztunnel A Use the following command to dump the Envoy proxy configuration on Ztunnel A:\nkubectl exec -n istio-system ztunnel-hptxk -c istio-proxy -- curl \u0026#34;127.0.0.1:15000/config_dump?include_eds\u0026#34;\u0026gt;ztunnel-a-all-include-eds.json 10.8.14.226 is the Cluster IP of the target service, and the service port is 9080. The traffic will be routed to the spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage cluster. Let’s look at the configuration of that cluster.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \u0026#34;10.8.14.226\u0026#34;: { \u0026#34;matcher\u0026#34;: { \u0026#34;matcher_tree\u0026#34;: { \u0026#34;input\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;port\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.matching.common_inputs.network.v3.DestinationPortInput\u0026#34; } }, \u0026#34;exact_match_map\u0026#34;: { \u0026#34;map\u0026#34;: { \u0026#34;9080\u0026#34;: { \u0026#34;action\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/google.protobuf.StringValue\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage\u0026#34; } } } } } } } } } 10.8.14.226 is the Cluster IP of the target service, and the service port is 9080. The traffic will be routed to the spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage cluster. Let’s look at the configuration of that cluster.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;version_info\u0026#34;: \u0026#34;2022-11-17T03:27:45Z/82\u0026#34;, \u0026#34;cluster\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.cluster.v3.Cluster\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;EDS\u0026#34;, \u0026#34;eds_cluster_config\u0026#34;: { \u0026#34;eds_config\u0026#34;: { \u0026#34;ads\u0026#34;: {}, \u0026#34;initial_fetch_timeout\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;resource_api_version\u0026#34;: \u0026#34;V3\u0026#34; } }, /* omit */ } The cluster is discovered using the EDS service. To view the EDS information for this cluster:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.endpoint.v3.ClusterLoadAssignment\u0026#34;, \u0026#34;endpoints\u0026#34;: [ { \u0026#34;locality\u0026#34;: {}, \u0026#34;lb_endpoints\u0026#34;: [ { \u0026#34;endpoint\u0026#34;: { \u0026#34;address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;10.4.3.14\u0026#34;, \u0026#34;port_value\u0026#34;: 15006 } }, \u0026#34;health_check_config\u0026#34;: {} }, \u0026#34;health_status\u0026#34;: \u0026#34;HEALTHY\u0026#34;, \u0026#34;load_balancing_weight\u0026#34;: 1 } ] } ], \u0026#34;policy\u0026#34;: { \u0026#34;overprovisioning_factor\u0026#34;: 140 } } Note: The output cluster_name field is still missing here. See the GitHub issue .\nTraffic is forwarded here directly to the Waypoint Proxy endpoint at 10.4.3.14:15006.\nTraffic Routing Using Waypoint Proxy Let’s dump the Envoy configuration into Waypoint Proxy B again.\nkubectl exec -n default bookinfo-productpage-waypoint-proxy-6f88c55d59-4dzdx -c istio-proxy -- curl \u0026#34;127.0.0.1:15000/config_dump?include_eds\u0026#34;\u0026gt;waypoint-a-all-include-eds.json Look into the configuration of inbound_CONNECT_terminate listener:\n1 2 3 4 5 6 …","relpermalink":"/en/blog/ambient-mesh-l7-traffic-path/","summary":"This article describes in detail the L7 traffic path in Ambient Mesh in both diagrammatic and hands-on form.","title":"L7 Traffic Path in Ambient Mesh"},{"content":"Istio’s new “ambient mode” is an experimental, “sidecar-less” deployment model for Istio . Instead of a sidecar proxy in front of every workload, ambient mode uses tproxy and HTTP Based Overlay Network Environment (HBONE) as key technologies for transparent traffic intercepting and routing that we covered in our recent article on transparent traffic intercepting and routing in the L4 network of Istio Ambient Mesh . In this article, we’ll take a closer look at tproxy and how it’s used.\nWhat Is a Proxy For? Proxies have a wide range of uses on the Internet, such as:\nRequest caching: to speed up network response, acting similarly to a CDN. Traffic filtering: used for network supervision, blocking or allowing access to specific hosts and websites. Traffic forwarding: used for load balancing or as a network relay. Traffic management: fine-grained management of traffic to and from the proxy, such as publishing to different backends by percentage, timeout and retry settings, circuit breaking, etc. Security auditing: logging and limiting client requests for billing or auditing purposes. Proxy Types There are a number of ways to classify proxies based on how they’re used. You can see two categories based on the location of the proxy in Figure 1:\nFigure 1: Forward proxy and reverse proxy. Forward proxies (like shadowsocks ) run on the client side and send requests to the server on behalf of the client. Reverse proxies (often in the form of a web server) accept Internet or external requests on behalf of the server and route them to the corresponding backends. Proxies may be located on the same node as the client or server or on a different node. We can classify them as transparent or non-transparent based on whether the client or server can see them. Figure 2 (below) shows the process of a client (A) sending a request to a server (C) through a proxy (B).\nFigure 2: Transparent vs. non-transparent proxies To use a non-transparent proxy, the client needs to explicitly change the destination address to that of the proxy server and use the proxy protocol to connect to the proxy server. To use a transparent proxy, the client and the server do not know the proxy is there, which means the client does not need to modify the destination address, and does not need to use the proxy protocol to connect to the proxy server; all the destination address conversion is done in the transparent proxy. Using the tproxy Transparent Proxy tproxy is a Linux kernel module (since Linux 2.2) that implements transparent proxies. To use tproxy, you must first use iptables to intercept the required packets at the required NIC, then listen for and forward the packet on that NIC.\nFollow these steps to use tproxy to implement a transparent proxy:\nFirst, you need to implement traffic interception: create a rule in the mangle table of the PREROUTING chain of iptables to intercept traffic and send it to tproxy for processing, for example, iptables -t mangle -A PREROUTING -p tcp -dport 9080 -j TPROXY --on-port 15001 --on-ip 127.0.0.1 --tproxy-mark 0x1/0x1, marking all TCP packets destined for port 9080 with a mark 1. You can specify the source IP address or IP set to further narrow the marking, with tproxy listening on port 15001. Create a routing rule that looks up all packets with mark 1 in a specific routing table: for example, add ip rule add fwmark 1 lookup 100, so that all packets with fwmark 1 look up to the routing table 100. Mapping packets to specific local addresses: for example, ip rule add local 0.0.0.0/0 dev lo table 100, which declares all IPv4 addresses as local in the routing table 100, but of course, this is just an example. In practice, you will need to forward packets with specific IPs to the local lo loopback NIC. The traffic has been intercepted on tproxy’s listening port 15001 (enter from Linux kernel space into user space). You can write a web application to process the packets or use tproxy-enabled software such as Squid or Envoy to process the packets.\nPros and Cons of Transparent Proxies Transparent proxies have the following advantages:\nHigher bandwidth and reduced transmission latency, thereby improving the quality of service. No need for users to configure networks and hosts. Control access to network services. Transparent proxies have the following disadvantages:\nIncorrectly configured, the transparent proxy may prevent connection to the Internet, leaving users unable to troubleshoot and fix errors. Security cannot be guaranteed, as intercepted user traffic may be tampered with by transparent proxies. The risk that transparent proxies may cache user information, leading to privacy leaks. Summary As a vital type of proxy, transparent proxies are used in a wide range of applications, whether in proxy software such as shadowsocks, Xray, or in the Istio service mesh. Understanding how they work helps us use proxies correctly, and whether or not you use a transparent proxy depends on how much you trust and know about it.\nThis …","relpermalink":"/en/blog/what-is-tproxy/","summary":"Tproxy is used to intercept traffic in Istio Ambient mode.","title":"How Istio's Ambient Mode Transparent Proxy - tproxy Works Under the Hood"},{"content":"In cloud native applications, a request often needs to be processed through a series of APIs or backend services, some of which are parallel and some serial and located on different platforms or nodes. How do we determine the service paths and nodes a call goes through to help us troubleshoot the problem? This is where distributed tracing comes into play.\nThis article covers:\nHow distributed tracing works How to choose distributed tracing software How to use distributed tracing in Istio How to view distributed tracing data using Bookinfo and SkyWalking as examples Distributed Tracing Basics Distributed tracing is a method for tracing requests in a distributed system to help users better understand, control, and optimize distributed systems. There are two concepts used in distributed tracing: TraceID and SpanID. You can see them in Figure 1 below.\nTraceID is a globally unique ID that identifies the trace information of a request. All traces of a request belong to the same TraceID, and the TraceID remains constant throughout the trace of the request. SpanID is a locally unique ID that identifies a request’s trace information at a certain time. A request generates different SpanIDs at different periods, and SpanIDs are used to distinguish trace information for a request at different periods. TraceID and SpanID are the basis of distributed tracing. They provide a uniform identifier for request tracing in distributed systems and facilitate users’ ability to query, manage, and analyze the trace information of requests.\nFigure 1: Trace and span The following is the process of distributed tracing:\nWhen a system receives a request, the distributed tracing system assigns a TraceID to the request, which is used to chain together the entire chain of invocations. The distributed trace system generates a SpanID and ParentID for each service call within the system for the request, which is used to record the parent-child relationship of the call; a Span without a ParentID is used as the entry point of the call chain. TraceID and SpanID are to be passed during each service call. When viewing a distributed trace, query the full process of a particular request by TraceID. How Istio Implements Distributed Tracing Istio’s distributed tracing is based on information collected by the Envoy proxy in the data plane. After a service request is intercepted by Envoy, Envoy adds tracing information as headers to the request forwarded to the destination workload. The following headers are relevant for distributed tracing:\nAs TraceID: x-request-id Used to establish parent-child relationships for Span in the LightStep trace: x-ot-span-context\u0026lt;/li Used for Zipkin, also for Jaeger, SkyWalking, see b3-propagation : x-b3-traceid x-b3-traceid x-b3-spanid x-b3-parentspanid x-b3-sampled x-b3-flags b3 For Datadog: x-datadog-trace-id x-datadog-parent-id x-datadog-sampling-priority For SkyWalking: sw8 For AWS X-Ray: x-amzn-trace-id For more information on how to use these headers, please see the Envoy documentation .\nRegardless of the language of your application, Envoy will generate the appropriate tracing headers for you at the Ingress Gateway and forward these headers to the upstream cluster. However, in order to utilize the distributed tracing feature, you must modify your application code to attach the tracing headers to upstream requests. Since neither the service mesh nor the application can automatically propagate these headers, you can integrate the agent for distributed tracing into the application or manually propagate these headers in the application code itself. Once the tracing headers are propagated to all upstream requests, Envoy will send the tracing data to the tracer’s back-end processing, and then you can view the tracing data in the UI.\nFor example, look at the code of the Productpage service in the Bookinfo application . You can see that it integrates the Jaeger client library and synchronizes the header generated by Envoy with the HTTP requests to the Details and Reviews services in the getForwardHeaders (request) function.\ndef getForwardHeaders(request): headers = {} # Using Jaeger agent to get the x-b3-* headers span = get_current_span() carrier = {} tracer.inject( span_context=span.context, format=Format.HTTP_HEADERS, carrier=carrier) headers.update(carrier) # Dealing with the non x-b3-* header manually if \u0026#39;user\u0026#39; in session: headers[\u0026#39;end-user\u0026#39;] = session[\u0026#39;user\u0026#39;] incoming_headers = [ \u0026#39;x-request-id\u0026#39;, \u0026#39;x-ot-span-context\u0026#39;, \u0026#39;x-datadog-trace-id\u0026#39;, \u0026#39;x-datadog-parent-id\u0026#39;, \u0026#39;x-datadog-sampling-priority\u0026#39;, \u0026#39;traceparent\u0026#39;, \u0026#39;tracestate\u0026#39;, \u0026#39;x-cloud-trace-context\u0026#39;, \u0026#39;grpc-trace-bin\u0026#39;, \u0026#39;sw8\u0026#39;, \u0026#39;user-agent\u0026#39;, \u0026#39;cookie\u0026#39;, \u0026#39;authorization\u0026#39;, \u0026#39;jwt\u0026#39;, ] for ihdr in incoming_headers: val = request.headers.get(ihdr) if val is not None: headers[ihdr] = val return headers For more information, the Istio documentation provides answers to frequently asked questions about distributed tracing in Istio.\nHow to Choose A Distributed Tracing System Distributed …","relpermalink":"/en/blog/distributed-tracing-with-skywalking-in-istio/","summary":"This blog will guide you to use SkyWalking for distributed tracing with Istio.","title":"How to Use SkyWalking for Distributed Tracing in Istio?"},{"content":"The Istio service mesh offers cloud native deployments a standard way to implement automatic mutual transport layer security (mTLS) . This reduces the attack surface of network communication by using strong identities to establish encrypted channels between workloads within the mesh that are both confidential and tamper-resistant. mTLS is a key component for building zero-trust application networks. To understand mTLS traffic encryption in Istio, this article will cover the following:\nAn overview of TLS, mTLS, and TLS termination An introduction to howTLS encryption works in Istio How to use Istio to implement mTLS in Kubernetes A discussion of when you do and don’t need mTLS What Is TLS and mTLS? TLS, the successor to Secure Sockets Layer (SSL), is a widely adopted security protocol used to create authenticated and encrypted connections between networked computers. For this reason, people often use the terms TLS and SSL interchangeably. In this article, we will refer to them collectively as TLS. TLS 1.0 was released in 1999, and the latest version is 1.3 (released in August 2018); versions 1.0 and 1.1 are deprecated.\nThe HTTPS we see when browsing the web uses TLS, as shown in Figure 1, which is built on top of TCP as the session layer in the OSI model. To ensure compatibility, TLS usually uses port 443, but you can use any port you want.\nFigure 1: HTTP vs. HTTPS TLS encryption is required when a client needs to confirm the identity of the server in order to guard against man-in-the-middle attacks and ensure communication security. Figure 2 shows how TLS-encrypted communication proceeds.\nFigure 2: simplified TLS handshake flow The server applies for and obtains a certificate (X.509 certificate) from a trusted certificate authority (CA). A request from the client to the server containing information such as the TLS version and password combination supported by the client. The server responds to the client request and attaches a digital certificate. The client verifies the status, validity, and digital signature of the certificate and confirms the identity of the server. Encrypted communication commences between the client and the server using a shared private key. The above is only an outline description of the TLS communication flow. If you’re interested in the details, please see this in-depth discussion of the complete TLS handshake process. From the above process, you will find that the certificate is the critical element representing the server’s identity. The server must use a certificate issued by an authoritatively certified CA in order to provide public services over the Internet. In contrast, you can manage certificates using your own public key infrastructure (PKI) for services inside of a private environment.\nMutual TLS, also referred to as mTLS, is the use of a two-way encrypted channel between a server and a client that necessitates certificate exchange and identity authentication between the parties.\nWhat Is TLS Termination? TLS termination is the process of decrypting TLS-encrypted traffic before it is forwarded to a web server. Offloading TLS traffic to an ingress gateway or specialized device improves web application performance while securing encrypted traffic. TLS termination is typically implemented at cluster ingress. All communication between the ingress and servers in the cluster will be conducted directly over HTTP in plaintext, enhancing service performance.\nFigure 3: TLS termination By default, Istio enables mTLS for mesh-based services and ends TLS at the ingress gateway. Furthermore, you can pass through traffic to back-end services for processing.\napiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: sample-gateway spec: servers: - port: number: 443 name: https protocol: HTTPS tls: mode: PASSTHROUGH See Gateway TLS Configuration for details.\nHow to Implement Automatic mTLS in Istio Figure 4 depicts the security architecture of Istio. This figure clearly shows that at the entry point, JSON Web Token (JWT) + TLS authentication and encryption are used, and that mTLS is enabled between all services within the Istio mesh.\nIstio 安全架构图 Istio includes a built-in CA, and Secret Discovery Service (SDS) —one of the discovery services in Envoy xDS —enables the issuance and rotation of SVID certificates. The mTLS flow in the Istio mesh is as follows:\nThe sidecar of every service requests a certificate from Istiod on behalf of the workload at startup, and Istiod issues the SVID certificate (the process is more complex, and I will explain it in a future blog). The sidecar of every workload intercepts all client requests within the pod. The client sidecar starts an mTLS handshake with the server sidecar. During the handshake, the JWT and authentication filter in the client sidecar will authenticate the identity of the request, and store the identity in the filter metadata after the authentication. Then the request will go through the authorization filter to determine if the …","relpermalink":"/en/blog/understanding-the-tls-encryption-in-istio/","summary":"This article introduces TLS and mTLS, and describes how to enable mTLS in Istio and its application scenarios.","title":"How Istio’s mTLS Traffic Encryption Works as Part of a Zero Trust Security Posture"},{"content":"Ambient mesh is an experimental new deployment model recently introduced to Istio. It splits the duties currently performed by the Envoy sidecar into two separate components: a node-level component for encryption (called “ztunnel”) and an L7 Envoy instance deployed per service for all other processing (called “waypoint”). The ambient mesh model is an attempt to gain some efficiencies in potentially improved lifecycle and resource management. You can learn more about what ambient mesh is and how it differs from the Sidecar pattern here .\nThis article takes you step-by-step through a hands-on approach to the transparent traffic intercepting and routing of L4 traffic paths in the Istio’s Ambient mode. If you don’t know what Ambient mode is, this article can help you understand.\nIf you want to skip the actual hands-on steps and just want to know the L4 traffic path in Ambient mode, please see the figure below, it shows a Pod of Service A calling a Pod of Service B on a different node below.\nFigure 1: Transparent traffic intercepting and routing in the L4 network of Istio Ambient Mesh Principles Ambient mode uses tproxy and HTTP Based Overlay Network Environment (HBONE) as key technologies for transparent traffic intercepting and routing:\nUsing tproxy to intercept the traffic from the host Pod into the Ztunnel (Envoy Proxy). Using HBONE to establish a tunnel for passing TCP traffic between Ztunnels. What Is tproxy? tproxy is a transparent proxy supported by the Linux kernel since version 2.2, where the t stands for transparent. You need to enable NETFILTER_TPROXY and policy routing in the kernel configuration. With tproxy, the Linux kernel can act as a router and redirect packets to user space. See the tproxy documentation for details.\nWhat Is HBONE? HBONE is a method of providing tunneling capabilities using the HTTP protocol. A client sends an HTTP CONNECT request (which contains the destination address) to an HTTP proxy server to establish a tunnel, and the proxy server establishes a TCP connection to the destination on behalf of the client, which can then transparently transport TCP data streams to the destination server through the proxy. In Ambient mode, Ztunnel (Envoy inside) acts as a transparent proxy, using Envoy Internal Listener to receive HTTP CONNECT requests and pass TCP streams to the upstream cluster.\nEnvironment Before starting the hands-on, it is necessary to explain the demo environment, and the corresponding object names in this article:\nItems Name IP Service A Pod sleep-5644bdc767-2dfg7 10.4.4.19 Service B Pod productpage-v1-5586c4d4ff-qxz9f 10.4.3.20 Ztunnel A Pod ztunnel-rts54 10.4.4.18 Ztunnel B Pod ztunnel-z4qmh 10.4.3.14 Node A gke-jimmy-cluster-default-pool-d5041909-d10i 10.168.15.222 Node B gke-jimmy-cluster-default-pool-d5041909-c1da 10.168.15.224 Service B Cluster productpage 10.8.14.226 Because these names will be used in subsequent command lines, the text will use pronouns, so that you can experiment in your own environment.\nFor the tutorial, I installed Istio Ambient mode in GKE. You can refer to this Istio blog post for installation instructions. Be careful not to install the Gateway, so as not to enable the L7 functionality; otherwise, the traffic path will be different from the descriptions in this blog.\nIn the following, we will experiment and dive into the L4 traffic path of a pod of sleep service to a pod of productpage service on different nodes. We will look at the outbound and inbound traffic of the Pods separately.\nOutbound Traffic Intercepting The transparent traffic intercepting process for outbound traffic from a pod in Ambient mesh is as follows:\nIstio CNI creates the istioout NIC and iptables rules on the node, adds the Pods’ IP in Ambient mesh to the IP set , and transparently intercepts outbound traffic from Ambient mesh to pistioout virtual NIC through Geneve (Generic Network Virtualization Encapsulation) tunnels with netfilter nfmark tags and routing rules. The init container in Ztunnel creates iptables rules that forward all traffic from the pistioout NIC to port 15001 of the Envoy proxy in Ztunnel. Envoy processes the packets and establishes an HBONE tunnel (HTTP CONNECT) with the upstream endpoints to forward the packets upstream. Check The Routing Rules On Node A Log in to Node A, where Service A is located, and use iptables-save to check the rules.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 $ iptables-save /* omit */ -A PREROUTING -j ztunnel-PREROUTING -A PREROUTING -m comment --comment \u0026#34;kubernetes service portals\u0026#34; -j KUBE-SERVICES -A ztunnel-POSTROUTING -m mark --mark 0x100/0x100 -j ACCEPT -A ztunnel-PREROUTING -m mark --mark 0x100/0x100 -j ACCEPT /* omit */ *mangle /* omit */ -A PREROUTING -j ztunnel-PREROUTING -A INPUT -j ztunnel-INPUT -A FORWARD -j ztunnel-FORWARD -A OUTPUT -j ztunnel-OUTPUT -A OUTPUT -s 169.254.169.254/32 -j DROP -A POSTROUTING -j ztunnel-POSTROUTING -A ztunnel-FORWARD -m mark …","relpermalink":"/en/blog/ambient-mesh-l4-traffic-path/","summary":"This article details transparent traffic intercepting and L4 traffic paths in Ambient Mesh in both diagrammatic and hands-on form.","title":"Transparent Traffic Intercepting and Routing in the L4 Network of Istio Ambient Mesh"},{"content":"In September 2022, Istio became a CNCF incubation project and launched the new Ambient Mesh . With CNCF’s strong community and marketing resources, and Ambient Mesh further lowering the barrier to trying Istio, the five year old open source project has been revitalized.\nIf you don’t know about service mesh and Istio, or are curious about the future of Istio, this eBook—The Current State and Future of the Istio Service Mesh will give you the answers. The following is an excerpt from the book. In my view, the future of Istio lies in being the infrastructure for zero-trust network and hybrid cloud.\nZero Trust Zero trust is an important topic, including when I spoke at IstioCon 2022. Istio is becoming an important part of zero trust, the most important element of which is identity-oriented control rather than network-oriented control.\nWhat Is Zero Trust? Zero trust is a security philosophy, not a best practice that all security teams follow in the same way. The concept of zero trust was proposed to bring a more secure network to the cloud-native world. Zero trust is a theoretical state where all consumers within a network not only have no authority but also have no awareness of the surrounding network. The main challenges of zero trust are the increasingly granular authorization and the need for a time limit for user authorization.\nAuthentication Istio 1.14 adds support for the SPIFFE Runtime Environment (SPIRE). SPIRE, a CNCF incubation project, is an implementation of the Secure Production Identity Framework for Everyone (SPIFFE), also a CNCF Incubation Project. In Kubernetes, we use ServiceAccount to provide identity information for workloads in Pods, and its core is based on Token (using Secret resource storage) to represent workload identity. A token is a resource in a Kubernetes cluster. How to unify the identities of multiple clusters and workloads running in non-Kubernetes environments (such as virtual machines)? That’s what SPIFFE is trying to solve.\nThe purpose of SPIFFE is to establish an open and unified workload identity standard based on the concept of zero trust, which helps to establish a fully identifiable data center network with zero trust. The core of SPIFFE is to define a short-lived encrypted identity document—SPIFFE Verifiable Identity Document (SVID)—through a simple API, which is used as an identity document (based on an X.509 certificate or JWT token) for workload authentication. SPIRE can automatically rotate SVID certificates and keys according to administrator-defined policies, dynamically provide workload identities, and Istio can dynamically consume these workload identities through SPIRE.\nThe Kubernetes-based SPIRE architecture diagram is shown below.\nFigure 1: SPIRE deployed in Kubernetes Istio originally used the Citadel service in Istiod to be responsible for certificate management in the service mesh, and issued the certificate to the data plane through the xDS (to be precise, SDS API) protocol. With SPIRE, the work of certificate management is handed over to SPIRE Server. SPIRE also supports the Envoy SDS API. After we enable SPIRE in Istio, the traffic entering the workload pod will be authenticated once after being transparently intercepted into the sidecar. The purpose of authentication is to compare the identity of the workload with the environment information it runs on (node, Pod’s ServiceAccount and Namespace, etc.) to prevent identity forgery. Please refer to How to Integrate SPIRE in Istio to learn how to use SPIRE for authentication in Istio.\nWe can deploy SPIRE in Kubernetes using the Kubernetes Workload Registrar, which automatically registers the workload in Kubernetes for us and generates an SVID. The registration machine is a Server-Agent architecture, which deploys a SPIRE Agent on each node, and the Agent communicates with the workload through a shared UNIX Domain Socket. The following diagram shows the process of using SPIRE for authentication in Istio.\nFigure 2: SPIRE-based workload authentication process in Istio The steps to using SPIRE for workload authentication in Istio are as follows:\nTo obtain the SIVD, the SPIRE Agent is referred to as a pilot-agent via shared UDS. The SPIRE Agent asks Kubernetes (to be precise, the kubelet on the node) for load information. The kubelet returns the information queried from the API server to the workload validator. The validator compares the result returned by the kubelet with the identity information shared by the sidecar. If it is the same, it returns the correct SVID cache to the workload. If it is different, the authentication fails. Please refer to the SPIRE documentation for the detailed process of registering and authenticating workloads.\nNGAC When each workload has an accurate identity, how can the permissions of these identities be restricted? Role-based access control (RBAC) is used by default in Kubernetes for access control. As the name suggests, this access control is based on roles. Although it is …","relpermalink":"/en/blog/the-future-of-istio/","summary":"The future of Istio lies in being the infrastructure for zero-trust network and hybrid cloud.","title":"The Future of Istio: the Path to Zero Trust Security"},{"content":"In this blog, you will learn about the Kubernetes Ingress Gateway, the Gateway API, and the emerging Gateway API trend, which enables the convergence of Kubernetes and service mesh.\nTakeaways Ingress, the original gateway for Kubernetes, has a resource model that is too simple to fit into today’s programmable networks. The Gateway API , the latest addition to the Kubernetes portal gateway, separates concerns through role delineation and provides cross-namespace support to make it more adaptable to multi-cloud environments. Most API gateways already support it. The Gateway API provides a new reference model for the convergence of ingress gateways (north-south) and service mesh (east-west, cross-cluster routing), where there is a partial functional overlap. History of the Kubernetes ingress gateway When Kubernetes was launched in June 2014, only NodePort and LoadBalancer-type Service objects were available to expose services within the cluster to the outside world. Later, Ingress was introduced to offer more control over incoming traffic.. To preserve its portability and lightweight design, the Ingress API matured more slowly than other Kubernetes APIs; it was not upgraded to GA until Kubernetes 1.19.\nIngress’ primary objective is to expose HTTP applications using a straightforward declarative syntax. When creating an Ingress or setting a default IngressClass in Kubernetes, you can deploy several Ingress Controllers and define the controller the gateway uses via IngressClass. Kubernetes currently supports only AWS, GCE, and Nginx Ingress controllers by default; many third-party ingress controllers are also supported.\nThe following diagram illustrates the workflow of Kubernetes Ingress.\nFigure 1: Kubernetes ingress workflow The detailed process is as follows:\nKubernetes cluster administrators deploy an Ingress Controller in Kubernetes. The Ingress Controller continuously monitors changes to IngressClass and Ingress objects in the Kubernetes API Server. Administrators apply IngressClass and Ingress to deploy the gateway. Ingress Controller creates the corresponding ingress gateway and configures the routing rules according to the administrator’s configuration. If in the cloud, the client accesses the load balancer for that ingress gateway. The gateway will route the traffic to the corresponding back-end service based on the host and path in the HTTP request. Istio supports both the Ingress and Gateway APIs. Below is an example configuration using the Istio Ingress Gateway, which will be created later using the Gateway API:\napiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: istio spec: controller: istio.io/ingress-controller --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress spec: ingressClassName: istio rules: - host: httpbin.example.com http: paths: - path: / pathType: Prefix backend: service: name: httpbin port: 8000 Note: You must specify the IngressClass in the ingressClassName field in the Ingress spec. Otherwise, the ingress gateway will not be created.\nLimitations of Kubernetes Ingress Although IngressClass decouples the ingress gateway from the back-end implementation, it still has significant limitations.\nIngress is too simple for most real-world use and it only supports HTTP protocol routing. It only supports host and path matching, and there is no standard configuration for advanced routing features, which can only be achieved through annotation, such as URL redirection using Nginx Ingress Controller, which requires configuration of nginx.ingress.kubernetes.io/rewrite-target annotation, which is no longer adaptable to the needs of a programmable proxy. The situation where services in different namespaces must be bound to the same gateway often arises in practical situations where the ingress gateway cannot be shared across multiple namespaces. No delineation of responsibilities for creating and managing ingress gateways, resulting in developers having to not only configure gateway routes but also create and manage gateways themselves. Kubernetes Gateway API The Gateway API is a collection of API resources: GatewayClass, Gateway, HTTPRoute, TCPRoute, ReferenceGrant, etc. The Gateway API exposes a more generic proxy API that can be used for more protocols than HTTP and models more infrastructure components, providing better deployment and management options for cluster operations.\nIn addition, the Gateway API achieves configuration decoupling by separating resource objects that people can manage in different roles. The following diagram shows the roles and objects in the Gateway API.\nFigure: Roles and componentes in Kubernetes Gateway API The following is an example of using the Gateway API in Istio.\napiVersion: gateway.networking.k8s.io/v1alpha2 kind: Gateway metadata: name: gateway namespace: istio-ingress spec: gatewayClassName: istio listeners: - name: default hostname: \u0026#34;*.example.com\u0026#34; port: 80 protocol: HTTP allowedRoutes: namespaces: from: All --- …","relpermalink":"/en/blog/why-gateway-api-is-the-future-of-ingress-and-mesh/","summary":"This article introduces the ingress gateway and Gateway API in Kubernetes, the new trend of them with service mesh.","title":"Why the Gateway API Is the Unified Future of Ingress for Kubernetes and Service Mesh"},{"content":"This article reviews the development of Istio open source in the past five years and looks forward to the future direction of Istio. The main points of view in this article are as follows:\nDue to the popularity of Kubernetes, microservices, DevOps, and cloud-native architectures, the rise of service mesh technology. The rise of Kubernetes and programmable proxies has laid the technical groundwork for Istio’s implementation. While eBPF can accelerate transparent traffic hijacking in Istio, it can not replace sidecars in service meshes. The future of Istio is to build a zero-trust network. Next, we start this article with the background of the birth of Istio.\nThe eve of the birth of Istio Since 2013, with the explosion of the mobile Internet, enterprises have had higher requirements for the efficiency of application iteration. Application architecture has begun to shift from monolithic to microservices, and DevOps has also become popular. In the same year, with the open source of Docker, the problems of application encapsulation and isolation were solved, making it easier to schedule applications in the orchestration system. In 2014, Kubernetes and Spring Boot were open-sourced, and Spring framework development of microservice applications became popular. In the next few years, a large number of RPC middleware open source projects appeared, such as Google released gRPC 1.0 in 2016. The service framework is in full bloom. In order to save costs, increase development efficiency, and make applications more flexible, more and more enterprises are migrating to the cloud, but this is not just as simple as moving applications to the cloud. In order to use cloud computing more efficiently, a set of “cloud native” methods and concepts are also on the horizon.\nIstio Open Source Timeline Let’s briefly review the major events of Istio open source:\nSeptember 2016: Since Envoy is an important part of Istio, Istio’s open source timeline should have an Envoy part. At first, Envoy was only used as an edge proxy inside Lyft, and it was verified in large-scale production inside Lyft before Envoy was open sourced. In fact, Envoy was open sourced before it was open sourced, and it got the attention of Google engineers. At that time, Google was planning to launch an open source project of service mesh, and initially planned to use Nginx as a proxy. In 2017, Envoy donated to CNCF . May 2017: Istio was announced as open source by Google, IBM, and Lyft. The microservices architecture was used from the beginning. The composition of the data plane, control plane, and sidecar pattern were determined. March 2018: Kubernetes successfully became the first project to graduate from CNCF, becoming more and more “boring”. The basic API has been finalized. In the second edition, CNCF officially wrote the service mesh into the cloud native first definition. The company I currently work for, Tetrate , was founded by the Google Istio team. July 2018: Istio 1.0 is released, billed as “production ready”. March 2020: Istio 1.5 was released, the architecture returned to a monolithic application, the release cycle was determined, a major version was released every three months, and the API became stable. From 2020 to the present: The development of Istio mainly focuses on Day 2 operation, performance optimization, and extensibility. Several open source projects in the Istio ecosystem have begun to emerge, such as Slime , Areaki , and Merbridge . Why did Istio come after Kubernetes? The emergence mentioned here refers to the birth of the concept of “service mesh”. After microservices and containerization, the increase in the use of heterogeneous languages, the surge in the number of services, and the shortened life cycle of containers are the fundamental reasons for the emergence of service meshes.\nTo make it possible for developers to manage traffic between services with minimal cost, Istio needs to solve three problems:\nTransparent traffic hijacks traffic between applications, which means that developers can quickly use the capabilities brought by Istio without modifying applications. Another point is the operation and maintenance level; how to inject the proxy into each application and manage these distributed sidecar proxies efficiently. An efficient and scalable sidecar proxy that can be configured through an API. The above three conditions are indispensable for the Istio service mesh, and we can see from them that these requirements are basically the requirements for the sidecar proxy. The choice of this proxy will directly affect the direction and success of the project.\nIn order to solve the three problems above, Istio chose:\nContainer Orchestration and Scheduling Platform: Kubernetes Programable proxy: Envoy From the figure below, we can see the transition of the service deployment architecture from Kubernetes to Istio, with many changes and constants.\nSchematic diagram of the architectural change from Kubernetes to Istio From Kubernetes to …","relpermalink":"/en/blog/beyond-istio-oss/","summary":"This article explains the background of Istio's birth, its position in the cloud-native technology stack, and the development direction of Istio.","title":"The Current State and Future of the Istio Service Mesh"},{"content":"Istio 1.14 was released in June of this year, and one of the most notable features of this release is support for SPIRE , which is one of the implementations of SPIFFE , a CNCF incubation project. This article explains what SPIRE means for zero-trust architectures and why you would need SPIRE for authentication in Istio.\nAuthentication in Kubernetes We all know that Istio was built for and typically runs on Kubernetes, so before we talk about how to use SPIRE for authentication in Istio, let’s take a look at how Kubernetes handles authentication.\nLet’s look at an example of a pod’s token. Whenever a pod gets created in Kubernetes, it gets assigned a default service account from the namespace, assuming we didn’t explicitly assign a service account to it. Here is an example:\napiVersion: v1 data: ca.crt: {CA_CRT} namespace: ZGVmYXVsdA== token: {TOKEN_STRING} kind: Secret metadata: annotations: kubernetes.io/service-account.name: sleep kubernetes.io/service-account.uid: 2c0d00e8-13a2-48d0-9ff8-f987f3325ecf creationTimestamp: \u0026#34;2022-06-14T03:01:35Z\u0026#34; name: sleep-token-gwhwd namespace: default resourceVersion: \u0026#34;244535398\u0026#34; uid: b8822ceb-9553-4a17-96dc-d525bbaed0e0 type: kubernetes.io/service-account-token Kubernetes manages the identity of Pods with Service Accounts and then specifies the permissions of Pods with a Service Account to the Kubernetes API using RBAC. A service account’s token is stored in a secret, which does not include a declaration of the node or pod where the workload is running. When a malicious actor steals a token, they gain full access to the account and can steal information or carry out sabotage under the guise of that user.\nA token can only be used to identify a workload in one cluster, but Istio supports multiple clusters and meshes, as well as Kubernetes environments and virtual machines. A unified workload identity standard can help here.\nAn Introduction to SPIFFE and SPIRE SPIFFE’s (Secure Production Identity Framework for Everyone) goal is to create a zero-trust, fully-identified data center network by establishing an open, unified workload identity standard based on the concept of zero-trust. SPIRE can rotate X.509 SVID certificates and secret keys on a regular basis. Based on administrator-defined policies, SPIRE can dynamically provision workload certificates and Istio can consume them. I’ll go over some of the terms associated with SPIFFE in a little more detail below.\nSPIRE (SPIFFE Runtime Environment) is a SPIFFE implementation that is ready for production. SVID (SPIFFE Verifiable Identity Document) is the document that a workload uses to prove its identity to a resource or caller. SVID contains a SPIFFE ID that represents the service’s identity. It uses an X.509 certificate or a JWT token to encode the SPIFFE ID in a cryptographically verifiable document. The SPIFFE ID is a URI that looks like this: spiffe://trust_domain/workload_identifier.\nSPIFFE and Zero Trust Security The essence of Zero Trust is identity-centric dynamic access control. SPIFFE addresses the problem of identifying workloads.\nWe might identify a workload using an IP address and port in the era of virtual machines, but IP address-based identification is vulnerable to multiple services sharing an IP address, IP address forgery, and oversized access control lists. Because containers have a short lifecycle in the Kubernetes era, instead of an IP address, we rely on a pod or service name. However, different clouds and software platforms approach workload identity differently, and there are compatibility issues. This is especially true in heterogeneous hybrid clouds, where workloads run on both virtual machines and Kubernetes. It is critical to establish a fine-grained, interoperable identification system at this point.\nUsing SPIRE for Authentication in Istio With the introduction of SPIRE to Istio, we can give each workload a unique identity, which is used by workloads in the service mesh for peer authentication, request authentication, and authorization policies. The SPIRE Agent issues SVIDs for workloads by communicating with a shared UNIX Domain Socket in the workload. The Envoy proxy and the SPIRE agent communicate through the Envoy SDS (Secret Discovery Service) API. Whenever an Envoy proxy needs to access secrets (certificates, keys, or anything else needed to do secure communication), it will talk to the SPIRE agent through Envoy’s SDS API.\nThe most significant advantage of SDS is the ease with which certificates can be managed. Without this feature, certificates would have to be created as a secret and then mounted into the agent container in a Kubernetes deployment. The secret must be updated, and the proxy container must be re-deployed if the certificate expires. Using SDS, Istio can push the certificates to all Envoy instances in the service mesh. If the certificate expires, the server only needs to push the new certificate to the Envoy instance; Envoy will use the new certificate right away, and the …","relpermalink":"/en/blog/why-istio-need-spire/","summary":"This article explains what SPIRE means for zero-trust architectures and why you would need SPIRE for authentication in Istio.","title":"Why Would You Need Spire for Authentication With Istio?"},{"content":" It’s been more than 5 years since Google, IBM, and Lyft unveiled the Istio open source project in May 2017. The Istio project has developed from a seed to a tall tree in these years. Many domestic books on the Istio service mesh were launched in the two years following the release of Istio 1.0 in 2018. My country is at the forefront of the world in the field of Istio book publishing.\nService mesh: one of the core technologies of cloud native Today, Istio is nearly synonymous with service mesh in China. The development of service mesh, as one of the core cloud-native technologies described by CNCF (Cloud Native Computing Foundation), has gone through the following stages.\n2017-2018: Exploratory Phase 2019-2020: Early Adopter Phase 2021 to present: Implementation on a large scale and ecological development stage Cloud native technology enables enterprises to design and deploy elastically scalable applications in new dynamic settings such as public, private, and hybrid clouds, according to the CNCF. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs are examples of cloud native technology.\nService mesh has been included to the CNCF definition of cloud native, indicating that it is one of the representative technologies of cloud native. Google is donating Istio to CNCF today, and we have reason to expect that as a CNCF project, Istio’s community will be more open, and its future development will be more smooth.\nService mesh and cloud native applications Cloud-native development is gaining traction. Despite the frequent emergence of new technologies and products, service mesh has maintained its place as “cloud-native network infrastructure” as part of the overall cloud-native technology stack throughout the past year. The cloud-native technology stack model is depicted in the diagram below, with representative technologies for each layer to define the standard. Service mesh and other cloud-native technologies complement each other as a new era of middleware emerges. Dapr (Distributed Application Runtime) defines the cloud-native middleware capability model, OAM defines the cloud-native application model, and so on, whereas service mesh Lattice defines a cloud-native seven-layer network model.\nCloud Native Application Model Why you need a service mesh Using a service mesh isn’t tantamount to abandoning Kubernetes; it just makes sense. The goal of Kubernetes is to manage application lifecycles through declarative configuration, whereas the goal of service mesh is to provide traffic control, security management, and observability amongst apps. How do you set up load balancing and flow management for calls between services after a robust microservice platform has been developed with Kubernetes?\nMany open source tools, including Istio, Linkerd, MOSN, and others, support Envoy’s xDS protocol. The specification of xDS is Envoy’s most significant contribution to service mesh or cloud native. Many various usage cases, such as API gateways, sidecar proxies in service meshes, and edge proxies, are derived from Envoy, which is simply a network proxy, a modern version of the proxy configured through the API.\nIn a nutshell, the move from Kubernetes to Istio was made for the following reasons.\nApplication life cycle management, specifically application deployment and management, is at the heart of Kubernetes (scaling, automatic recovery, and release). Kubernetes is a microservices deployment and management platform that is scalable and extremely elastic. Transparent proxy is the cornerstone of service mesh, which intercepts communication between microservices via sidecar proxy and then regulates microservice behavior via control plane settings. The deployment mode of service meshes has introduced new issues today. For service meshes, sidecar is no longer required, and an agentless service mesh based on gRPC is also being tested. xDS is a protocol standard for configuring service meshes, and a gRPC-based xDS is currently being developed. Kubernetes traffic management is decoupled with the service mesh. The kube-proxy component is not required to support traffic within the service mesh. The traffic between services is controlled by an abstraction close to the microservice application layer to achieve security and observability features. In Kubernetes, service mesh is an upper-level abstraction of service, and Serverless is the next stage, which is why Google released Knative based on Kubernetes and Istio following Istio. Open source in the name of the community The ServiceMesher community was founded in May 2018 with the help of Ant Financial. Following that, a tornado of service meshes erupted in China, and the community-led translation of Istio’s official documentation reached a fever pitch.\nI became aware of a dearth of Chinese resources for systematically teaching Istio over time, so in September 2018, I began to plan and create an Istio book, launching the Istio Handbook open source …","relpermalink":"/en/blog/istio-service-mesh-book/","summary":"By the Cloud Native Community(China)","title":"In-Depth Understanding of Istio: Announcing the Publication of a New Istio Book"},{"content":"Achieving Open Source in the Name of Community In May 2018, with the support of Ant Group, the ServiceMesher community was established. Subsequently, a whirlwind of service mesh swept across China, and the community-led translation work of the official Istio documentation entered a fervent stage.\nAs time went on, I felt the lack of comprehensive Chinese materials introducing Istio. Therefore, in September 2018, I began conceptualizing a book about Istio and initiated the open-source e-book project “Istio Handbook” on GitHub. Several months later, with the promotion of service mesh technology and the expansion of the ServiceMesher community, I met many friends who were also passionate about Istio and service mesh technology through online and offline activities in the community. We unanimously decided to write an open-source e-book about Istio together, compiling valuable articles and experiences accumulated by the community into a systematic text to share with developers.\nIn March 2019, under the organization of the community’s management committee, dozens of members volunteered to participate and jointly write this book. In May 2020, to better promote cloud native technology and enrich the technical content shared by the community, we established the Cloud Native Community and incorporated the original ServiceMesher community into it. The content operated by the community also expanded from service mesh technology to a more comprehensive range of cloud native technologies.\nIn October 2020, the main contributors to this book formed the editorial committee, including myself, Ma Ruofei, Wang Bai Ping, Wang Wei, Luo Guangming, Zhao Huabing, Zhong Hua, and Guo Xudong. With the guidance and assistance of the publisher, we carried out subsequent version upgrades, improvements, and optimizations for this book. After repeated iterations, this book “Understanding Istio: Advanced Practices in Cloud Native Service Mesh” finally met with everyone.\nBook cover About This Book After version 1.5, Istio underwent significant architectural changes and introduced or improved multiple features, such as introducing intelligent DNS proxy, new resource objects, and improving support for virtual machines.\nThis book is based on the new versions of Istio and strives to provide readers with the latest and most comprehensive content while continuously tracking the latest trends in the Istio community. Additionally, many authors of this book are frontline developers or operation engineers with rich practical experience in Istio, providing valuable reference cases for this book.\n","relpermalink":"/en/book/istio-in-depth/","summary":"This book was authored by the Cloud Native Community.","title":"Understanding Istio: Advanced Practices in Cloud Native Service Mesh"},{"content":"This article will guide you on how to compile the Istio binaries and Docker images on macOS.\nBefore you begin Before we start, refer to the Istio Wiki , here is the information about my build environment.\nmacOS 12.3.1 Darwin AMD64 Docker Desktop 4.8.1(78998) Docker Engine v20.10.14 Start to compile First, download the Istio code from GitHub to the $GOPATH/src/istio.io/istio directory, and execute the commands below in that root directory.\nCompile into binaries Execute the following command to download the Istio dependent packages, which will be downloaded to the vendor directory.\ngo mod vendor Run the following command to build Istio:\nsudo make build If you do not run the command with sudo, you may encounter the following error.\nfatal: unsafe repository (\u0026#39;/work\u0026#39; is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work fatal: unsafe repository (\u0026#39;/work\u0026#39; is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work Makefile.core.mk:170: *** \u0026#34;TAG cannot be empty\u0026#34;. Stop. make: *** [build] Error 2 Even if you follow the prompts and run git config --global --add safe.directory /work, you will still get errors during compilation.\nThe compiled binary will be saved in out directory with the following directory structure.\nout ├── darwin_amd64 │ ├── bug-report │ ├── client │ ├── envoy │ ├── extauthz │ ├── install-cni │ ├── istio-cni │ ├── istio-cni-taint │ ├── istio-iptables │ ├── istio_is_init │ ├── istioctl │ ├── logs │ ├── operator │ ├── pilot-agent │ ├── pilot-discovery │ ├── release │ └── server └── linux_amd64 ├── envoy ├── envoy-centos ├── logs └── release It will build both the linux_amd64 and darwin_amd64 architectures binaries at the same time.\nCompile into Docker images Run the following command to compile Istio into a Docker image.\nsudo make docker The compilation will take about 3 to 5 minutes depending on your network. Once the compilation is complete, you will see the Docker image of Istio by running the following command.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE localhost:5000/app_sidecar_centos_7 latest 2044037df94b 51 seconds ago 524MB localhost:5000/app_sidecar_ubuntu_jammy latest 5d8ae5ed55b7 About a minute ago 362MB localhost:5000/proxyv2 latest d4679412385f About a minute ago 243MB localhost:5000/install-cni latest 78f46d5771d2 About a minute ago 270MB localhost:5000/istioctl latest c38130a5adc8 About a minute ago 190MB localhost:5000/pilot latest 2aa9185ec202 About a minute ago 190MB localhost:5000/app latest 473adafaeb8d About a minute ago 188MB localhost:5000/operator latest 9ac1fedcdd12 About a minute ago 191MB localhost:5000/ext-authz latest 1fb5aaf20791 About a minute ago 117MB localhost:5000/app_sidecar_debian_11 latest 61376a02b95d 2 minutes ago 407MB localhost:5000/app_sidecar_ubuntu_xenial latest 7e8efe666611 2 minutes ago 418MB You can change the image name and push it into your own container registry.\nSummary This is how to build Istio on macOS. If you have already downloaded the Docker image you need to build, the build will take less than a minute. It also takes only a few minutes to build Docker images.\nReference Using the Code Base - github.com ","relpermalink":"/en/blog/how-to-build-istio/","summary":"This article will guide you on how to compile the Istio binaries on macOS.","title":"How to Build Istio?"},{"content":"Based on Istio version 1.13, this article will present the following.\nWhat is the sidecar pattern and what advantages does it have? How are the sidecar injections done in Istio? How does the sidecar proxy do transparent traffic intercepting? How is the traffic routed to upstream? The figure below shows how the productpage service requests access to http://reviews.default.svc.cluster.local:9080/ and how the sidecar proxy inside the reviews service does traffic blocking and routing forwarding when traffic goes inside the reviews service.\nIstio transparent traffic intercepting and traffic routing diagram At the beginning of the first step, the sidecar in the productpage pod has selected a pod of the reviews service to be requested via EDS, knows its IP address, and sends a TCP connection request.\nThere are three versions of the reviews service, each with an instance, and the sidecar work steps in the three versions are similar, as illustrated below only by the sidecar traffic forwarding step in one of the Pods.\nSidecar pattern Dividing the functionality of an application into separate processes running in the same minimal scheduling unit (e.g. Pod in Kubernetes) can be considered sidecar mode. As shown in the figure below, the sidecar pattern allows you to add more features next to your application without additional third-party component configuration or modifications to the application code.\nSidecar pattern The Sidecar application is loosely coupled to the main application. It can shield the differences between different programming languages and unify the functions of microservices such as observability, monitoring, logging, configuration, circuit breaker, etc.\nAdvantages of using the Sidecar pattern When deploying a service mesh using the sidecar model, there is no need to run an agent on the node, but multiple copies of the same sidecar will run in the cluster. In the sidecar deployment model, a companion container (such as Envoy or MOSN) is deployed next to each application’s container, which is called a sidecar container. The sidecar takes overall traffic in and out of the application container. In Kubernetes’ Pod, a sidecar container is injected next to the original application container, and the two containers share storage, networking, and other resources.\nDue to its unique deployment architecture, the sidecar model offers the following advantages.\nAbstracting functions unrelated to application business logic into a common infrastructure reduces the complexity of microservice code. Reduce code duplication in microservices architectures because it is no longer necessary to write the same third-party component profiles and code. The sidecar can be independently upgraded to reduce the coupling of application code to the underlying platform. iptables manipulation analysis In order to view the iptables configuration, we need to nsenter the sidecar container using the root user to view it, because kubectl cannot use privileged mode to remotely manipulate the docker container, so we need to log on to the host where the productpage pod is located.\nIf you use Kubernetes deployed by minikube, you can log directly into the minikube’s virtual machine and switch to root. View the iptables configuration that lists all the rules for the NAT (Network Address Translation) table because the mode for redirecting inbound traffic to the sidecar is REDIRECT in the parameters passed to the istio-iptables when the Init container is selected for the startup, so there will only be NAT table specifications in the iptables and mangle table configurations if TPROXY is selected. See the iptables command for detailed usage.\nWe only look at the iptables rules related to productpage below.\n# login to minikube, change user to root $ minikube ssh $ sudo -i # See the processes in the productpage pod\u0026#39;s istio-proxy container $ docker top `docker ps|grep \u0026#34;istio-proxy_productpage\u0026#34;|cut -d \u0026#34; \u0026#34; -f1` UID PID PPID C STIME TTY TIME CMD 1337 10576 10517 0 08:09 ? 00:00:07 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage.default --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istiod.istio-system.svc:15012 --zipkinAddress zipkin.istio-system:9411 --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --connectTimeout 10s --proxyAdminPort 15000 --concurrency 2 --controlPlaneAuthPolicy NONE --dnsRefreshRate 300s --statusPort 15020 --trust-domain=cluster.local --controlPlaneBootstrap=false 1337 10660 10576 0 08:09 ? 00:00:33 /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage.default --service-node sidecar~172.17.0.16~productpage-v1-7f44c4d57c-ksf9b.default~default.svc.cluster.local --max-obj-name-len 189 --local-address-ip-version v4 --log-format [Envoy (Epoch 0)] [%Y-%m-%d %T.%e][%t][%l][%n] %v -l warning …","relpermalink":"/en/blog/sidecar-injection-iptables-and-traffic-routing/","summary":"Learn the sidecar pattern, transparent traffic intercepting and routing in Istio.","title":"Understanding the Sidecar Injection, Traffic Intercepting \u0026 Routing Process in Istio"},{"content":"This article will explain:\nThe sidecar auto-injection process in Istio The init container startup process in Istio The startup process of a Pod with Sidecar auto-injection enabled The following figure shows the components of a Pod in the Istio data plane after it has been started.\nIstio data plane pod Sidecar injection in Istio The following two sidecar injection methods are available in Istio.\nManual injection using istioctl. Kubernetes-based mutating webhook admission controller automatic sidecar injection method. Whether injected manually or automatically, SIDECAR’s injection process follows the following steps.\nKubernetes needs to know the Istio cluster to which the sidecar to be injected is connected and its configuration. Kubernetes needs to know the configuration of the sidecar container itself to be injected, such as the image address, boot parameters, etc. Kubernetes injects the above configuration into the side of the application container by the sidecar injection template and the configuration parameters of the above configuration-filled sidecar. The sidecar can be injected manually using the following command.\nistioctl kube-inject -f ${YAML_FILE} | kuebectl apply -f - This command is injected using Istio’s built-in sidecar configuration, see the Istio official website for details on how to use Istio below.\nWhen the injection is complete you will see that Istio has injected initContainer and sidecar proxy-related configurations into the original pod template.\nInit container The Init container is a dedicated container that runs before the application container is launched and is used to contain some utilities or installation scripts that do not exist in the application image.\nMultiple Init containers can be specified in a Pod, and if more than one is specified, the Init containers will run sequentially. The next Init container can only be run if the previous Init container must run successfully. Kubernetes only initializes the Pod and runs the application container when all the Init containers have been run.\nThe Init container uses Linux Namespace, so it has a different view of the file system than the application container. As a result, they can have access to Secret in a way that application containers cannot.\nDuring Pod startup, the Init container starts sequentially after the network and data volumes are initialized. Each container must be successfully exited before the next container can be started. If exiting due to an error will result in a container startup failure, it will retry according to the policy specified in the Pod’s restartPolicy. However, if the Pod’s restartPolicy is set to Always, the restartPolicy is used when the Init container failed.\nThe Pod will not become Ready until all Init containers are successful. The ports of the Init containers will not be aggregated in the Service. The Pod that is being initialized is in the Pending state but should set the Initializing state to true. The Init container will automatically terminate once it is run.\nSidecar injection example analysis For a detailed YAML configuration for the bookinfo applications, see bookinfo.yaml for the official Istio YAML of productpage in the bookinfo sample.\nThe following will be explained in the following terms.\nInjection of Sidecar containers Creation of iptables rules The detailed process of routing apiVersion: apps/v1 kind: Deployment metadata: name: productpage-v1 labels: app: productpage version: v1 spec: replicas: 1 selector: matchLabels: app: productpage version: v1 template: metadata: labels: app: productpage version: v1 spec: serviceAccountName: bookinfo-productpage containers: - name: productpage image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9080 volumeMounts: - name: tmp mountPath: /tmp volumes: - name: tmp emptyDir: {} Let’s see the productpage container’s Dockerfile .\nFROM python:3.7.4-slim COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt COPY test-requirements.txt ./ RUN pip install --no-cache-dir -r test-requirements.txt COPY productpage.py /opt/microservices/ COPY tests/unit/* /opt/microservices/ COPY templates /opt/microservices/templates COPY static /opt/microservices/static COPY requirements.txt /opt/microservices/ ARG flood_factor ENV FLOOD_FACTOR ${flood_factor:-0} EXPOSE 9080 WORKDIR /opt/microservices RUN python -m unittest discover USER 1 CMD [\u0026#34;python\u0026#34;, \u0026#34;productpage.py\u0026#34;, \u0026#34;9080\u0026#34;] We see that ENTRYPOINT is not configured in Dockerfile, so CMD’s configuration python productpage.py 9080 will be the default ENTRYPOINT, keep that in mind and look at the configuration after the sidecar injection.\n$ istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml We intercept only a portion of the YAML configuration that is part of the Deployment configuration associated with productpage.\ncontainers: - image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 # application image name: …","relpermalink":"/en/blog/istio-pod-process-lifecycle/","summary":"This article will explain Istio's Init container, Pod internal processes and the startup process.","title":"Istio Data Plane Pod Startup Process Explained"},{"content":"iptables is an important feature in the Linux kernel and has a wide range of applications. iptables is used by default in Istio for transparent traffic hijacking. Understanding iptables is very important for us to understand how Istio works. This article will give you a brief introduction to iptbles.\niptables introduction iptables is a management tool for netfilter, the firewall software in the Linux kernel. netfilter is located in the user space and is part of netfilter. netfilter is located in the kernel space and has not only network address conversion, but also packet content modification and packet filtering firewall functions.\nBefore learning about iptables for Init container initialization, let’s go over iptables and rule configuration.\nThe following figure shows the iptables call chain.\niptables 调用链 iptables The iptables version used in the Init container is v1.6.0 and contains 5 tables.\nRAW is used to configure packets. Packets in RAW are not tracked by the system. The filter is the default table used to house all firewall-related operations. NAT is used for network address translation (e.g., port forwarding). Mangle is used for modifications to specific packets (refer to corrupted packets). Security is used to force access to control network rules. Note: In this example, only the NAT table is used.\nThe chain types in the different tables are as follows.\nRule name raw filter nat mangle security PREROUTING ✓ ✓ ✓ INPUT ✓ ✓ ✓ OUTPUT ✓ ✓ ✓ ✓ ✓ POSTROUTING ✓ ✓ FORWARD ✓ ✓ ✓ Understand iptables rules View the default iptables rules in the istio-proxy container, the default view is the rules in the filter table.\n$ iptables -L -v Chain INPUT (policy ACCEPT 350K packets, 63M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 18M packets, 1916M bytes) pkts bytes target prot opt in out source destination We see three default chains, INPUT, FORWARD, and OUTPUT, with the first line of output in each chain indicating the chain name (INPUT/FORWARD/OUTPUT in this case), followed by the default policy (ACCEPT).\nThe following is a proposed structure diagram of iptables, where traffic passes through the INPUT chain and then enters the upper protocol stack, such as:\niptables chains Multiple rules can be added to each chain and the rules are executed in order from front to back. Let’s look at the table header definition of the rule.\nPKTS: Number of matched messages processed bytes: cumulative packet size processed (bytes) Target: If the message matches the rule, the specified target is executed. PROT: Protocols such as TDP, UDP, ICMP, and ALL. opt: Rarely used, this column is used to display IP options. IN: Inbound network interface. OUT: Outbound network interface. source: the source IP address or subnet of the traffic, the latter being anywhere. destination: the destination IP address or subnet of the traffic, or anywhere. There is also a column without a header, shown at the end, which represents the options of the rule, and is used as an extended match condition for the rule to complement the configuration in the previous columns. prot, opt, in, out, source and destination and the column without a header shown after destination together form the match rule. TARGET is executed when traffic matches these rules.\nTypes supported by TARGET\nTarget types include ACCEPT, REJECT, DROP, LOG, SNAT, MASQUERADE, DNAT, REDIRECT, RETURN or jump to other rules, etc. You can determine where the telegram is going by executing only one rule in a chain that matches in order, except for the RETURN type, which is similar to the return statement in programming languages, which returns to its call point and continues to execute the next rule.\nFrom the output, you can see that the Init container does not create any rules in the default link of iptables, but instead creates a new link.\nSummary With the above brief introduction to iptables, you have understood how iptables works, the rule chain and its execution order.\n","relpermalink":"/en/blog/understanding-iptables/","summary":"This article will give you a brief introduction to iptables, its tables and the order of execution.","title":"Understanding IPTables"},{"content":"See the cloud native public library at: https://jimmysong.io/book/ I have also adjusted the home page, menu and directory structure of the site, and the books section of the site will be maintained using the new theme.\nCloud native library positioning The cloud native public library is a collection of cloud native related books and materials published and translated by the author since 2017, and is a compendium and supplement to the dozen or so books already published. The original materials will continue to be published in the form of GitBooks, and the essence and related content will be sorted into the cloud native public library through this project.\nIn addition, the events section of this site has been revamped and moved to a new page .\n","relpermalink":"/en/notice/cloud-native-public-library/","summary":"A one-stop cloud native library that is a compendium of published materials.","title":"Announcement of Cloud Native Library"},{"content":"In my last two blogs:\nSidecar injection, transparent traffic hijacking , and routing process in Istio explained in detail Traffic types and iptables rules in Istio sidecar explained I gave you a detailed overview of the traffic in the Istio data plane, but the data plane does not exist in isolation. This article will show you the ports and their usages for each component of both the control plane and data plane in Istio, which will help you understand the relationship between these flows and troubleshoot them.\nOverview Firstly, I will show you a global schematic. The following figure shows the components of a sidecar in the Istio data plane, and the objects that interact with it.\nIstio components We can use the nsenter command to enter the namespace of the productpage Pod of the Bookinfo example and see the information about the ports it is listening on internally.\nIstio sidecar ports From the figure, we can see that besides the port 9080 that the productpage application listens to, the Sidecar container also listens to a large number of other ports, such as 15000, 15001, 15004, 15006, 15021, 15090, etc. You can learn about the ports used in Istio in the Istio documentation .\nLet’s go back into the productpage Pod and use the lsof -i command to see the ports it has open, as shown in the following figure.\nProductpage Pod ports We can see that there is a TCP connection established between the pilot-agent and istiod, the port in the listening described above, and the TCP connection established inside the Pod, which corresponds to the figure at the beginning of the article.\nThe root process of the Sidecar container (istio-proxy) is pilot-agent, and the startup command is shown below.\nInternal procecces in Sidecar As we can see from the figure, the PID of its pilot-agent process is 1, and it forked the Envoy process.\nCheck the ports it opens in Istiod, as shown in the figure below.\nIstiod ports We can see the ports that are listened to, the inter-process and remote communication connections.\nPorts usage overview These ports can play a pivotal role when you are troubleshooting. They are described below according to the component and function in which the port is located.\nPorts in Istiod The ports in Istiod are relatively few and single-function.\n9876: ControlZ user interface, exposing information about Istiod’s processes 8080: Istiod debugging port, through which the configuration and status information of the grid can be queried 15010: Exposes the xDS API and issues plain text certificates 15012: Same functionality as port 15010, but uses TLS communication 15014: Exposes control plane metrics to Prometheus 15017: Sidecar injection and configuration validation port Ports in sidecar From the above, we see that there are numerous ports in the sidecar.\n15000: Envoy admin interface, which you can use to query and modify the configuration of Envoy Proxy. Please refer to Envoy documentation for details. 15001: Used to handle outbound traffic. 15004: Debug port (explained further below). 15006: Used to handle inbound traffic. 15020: Summarizes statistics, perform health checks on Envoy and DNS agents, and debugs pilot-agent processes, as explained in detail below. 15021: Used for sidecar health checks to determine if the injected Pod is ready to receive traffic. We set up the readiness probe on the /healthz/ready path on this port, and Istio hands off the sidecar readiness checks to kubelet. 15053: Local DNS proxy for scenarios where the cluster’s internal domain names are not resolved by Kubernetes DNS. 15090: Envoy Prometheus query port, through which the pilot-agent will scratch metrics. The above ports can be divided into the following categories.\nResponsible for inter-process communication, such as 15001, 15006, 15053 Health check and information statistics, e.g. 150021, 15090 Debugging: 15000, 15004 Let’s look at the key ports in detail.\n15000 15000 is Envoy’s Admin interface, which allows us to modify Envoy and get a view and query metrics and configurations.\nThe Admin interface consists of a REST API with multiple endpoints and a simple user interface. You can enable the Envoy Admin interface view in the productpage Pod using the following command:\nkubectl -n default port-forward deploy/productpage-v1 15000 Visit http://localhost:15000 in your browser and you will see the Envoy Admin interface as shown below.\nEnvoy Admin interface 15004 With the pilot-agent proxy istiod debug endpoint on port 8080, you can access localhost’s port 15004 in the data plane Pod to query the grid information, which has the same effect as port 8080 below.\n8080 You can also forward istiod port 8080 locally by running the following command:\nkubectl -n istio-system port-forward deploy/istiod 8080 Visit http://localhost:8080/debug in your browser and you will see the debug endpoint as shown in the figure below.\nPilot Debug Console Of course, this is only one way to get the mesh information and debug the mesh, you can also use istioctl …","relpermalink":"/en/blog/istio-components-and-ports/","summary":"This article will introduce you to the various ports and functions of the Istio control plane and data plane.","title":"Istio Component Ports and Functions in Details"},{"content":"As we know that Istio uses iptables for traffic hijacking, where the iptables rule chains has one called ISTIO_OUTPUT, which contains the following rules.\nRule target in out source destination 1 RETURN any lo 127.0.0.6 anywhere 2 ISTIO_IN_REDIRECT any lo anywhere !localhost owner UID match 1337 3 RETURN any lo anywhere anywhere !owner UID match 1337 4 RETURN any any anywhere anywhere owner UID match 1337 5 ISTIO_IN_REDIRECT any lo anywhere !localhost owner GID match 1337 6 RETURN any lo anywhere anywhere !owner GID match 1337 7 RETURN any any anywhere anywhere owner GID match 1337 8 RETURN any any anywhere localhost 9 ISTIO_REDIRECT any any anywhere anywhere The sidecar applies these rules to deal with different types of traffic. This article will show you the six types of traffic and their iptables rules in Istio sidecar.\niptables Traffic Routing in Sidecar The following list summarizes the six types of traffic in Sidecar.\nRemote service accessing local service: Remote Pod -\u0026gt; Local Pod Local service accessing remote service: Local Pod -\u0026gt; Remote Pod Prometheus crawling metrics of local service: Prometheus -\u0026gt; Local Pod Traffic between Local Pod service: Local Pod -\u0026gt; Local Pod Inter-process TCP traffic within Envoy Sidecar to Istiod traffic The following will explain the iptables routing rules within Sidecar for each scenario, which specifies which rule in ISTIO_OUTPUT is used for routing.\nType 1: Remote Pod -\u0026gt; Local Pod The following are the iptables rules for remote services, applications or clients accessing the local pod IP of the data plane.\nRemote Pod -\u0026gt; RREROUTING -\u0026gt; ISTIO_INBOUND -\u0026gt; ISTIO_IN_REDIRECT -\u0026gt; Envoy 15006 (Inbound) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 1 -\u0026gt; POSTROUTING -\u0026gt; Local Pod\nWe see that the traffic only passes through the Envoy 15006 Inbound port once. The following diagram shows this scenario of the iptables rules.\nRemote Pod to Local Pod Type 2: Local Pod -\u0026gt; Remote Pod The following are the iptables rules that the local pod IP goes through to access the remote service.\nLocal Pod-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 9 -\u0026gt; ISTIO_REDIRECT -\u0026gt; Envoy 15001 (Outbound) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 4 -\u0026gt; POSTROUTING -\u0026gt; Remote Pod\nWe see that the traffic only goes through the Envoy 15001 Outbound port.\nLocal Pod to Remote Pod The traffic in both scenarios above passes through Envoy only once because only one scenario occurs in that Pod, sending or receiving requests.\nType 3: Prometheus -\u0026gt; Local Pod Prometheus traffic that grabs data plane metrics does not have to go through the Envoy proxy.\nThese traffic pass through the following iptables rules.\nPrometheus-\u0026gt; RREROUTING -\u0026gt; ISTIO_INBOUND (traffic destined for ports 15020, 15090 will go to INPUT) -\u0026gt; INPUT -\u0026gt; Local Pod\nPrometheus to Local Pod Type 4: Local Pod -\u0026gt; Local Pod A Pod may simultaneously have two or more services. If the Local Pod accesses a service on the current Pod, the traffic will go through Envoy 15001 and Envoy 15006 ports to reach the service port of the Local Pod.\nThe iptables rules for this traffic are as follows.\nLocal Pod-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 9 -\u0026gt; ISTIO_REDIRECT -\u0026gt; Envoy 15001（Outbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 2 -\u0026gt; ISTIO_IN_REDIRECT -\u0026gt; Envoy 15006（Inbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 1 -\u0026gt; POSTROUTING -\u0026gt; Local Pod\nLocal Pod to Local Pod Type 5: Inter-process TCP traffic within Envoy Envoy internal processes with UID and GID 1337 will communicate with each other using lo NICs and localhost domains.\nThe iptables rules that these flows pass through are as follows.\nEnvoy process (Localhost) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 8 -\u0026gt; POSTROUTING -\u0026gt; Envoy process (Localhost)\nEnvoy inter-process TCP traffic Type 6: Sidecar to Istiod traffic Sidecar needs access to Istiod to synchronize its configuration so that Envoy will have traffic sent to Istiod.\nThe iptables rules that this traffic passes through are as follows.\npilot-agent process -\u0026gt; OUTPUT -\u0026gt; Istio_OUTPUT RULE 9 -\u0026gt; Envoy 15001 (Outbound Handler) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 4 -\u0026gt; POSTROUTING -\u0026gt; Istiod\nSidecar to Istiod Summary All the sidecar proxies that Istio injects into the Pod or installed in the virtual machine form the data plane of the service mesh, which is also the main workload location of Istio. In my next blog, I will take you through the ports of each component in Envoy and their functions, so that we can have a more comprehensive understanding of the traffic in Istio.\n","relpermalink":"/en/blog/istio-sidecar-traffic-types/","summary":"This article will show you the six traffic types and their iptables rules in Istio sidecar, and take you through the whole diagram in a schematic format.","title":"Traffic Types and Iptables Rules in Istio Sidecar Explained"},{"content":"Istio 1.13 is the first release of 2022, and, not surprisingly, the Istio team will continue to release new versions every quarter. Overall, the new features in this release include:\nSupport for newer versions of Kubernetes New API – ProxyConfig, for configuring sidecar proxies Improved Telemetry API Support for hostname-based load balancers with multiple network gateways Support for Kubernetes Versions I often see people asking in the community which Istio supports Kubernetes versions. Istio’s website has a clear list of supported Kubernetes versions. You can see here that Istio 1.13 supports Kubernetes versions 1.20, 1.21, 1.22, and 1.23, and has been tested but not officially supported in Kubernetes 1.16, 1.17, 1.18, 1.19.\nWhen configuring Istio, there are a lot of checklists. I noted them all in the Istio cheatsheet . There are a lot of cheat sheets about configuring Istio, using resources, dealing with everyday problems, etc., in this project, which will be online soon, so stay tuned.\nThe following screenshot is from the Istio cheatsheet website, it shows the basic cheat sheet for setting up Istio.\nIstio cheatsheet Introducing the new ProxyConfig API Before Istio version 1.13, if you wanted to customize the configuration of the sidecar proxy, there were two ways to do it.\nMeshConfig\nUse MeshConfig and use IstioOperator to modify it at the Mesh level. For example, use the following configuration to alter the default discovery port for istiod.\napVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: defaultConfig: discoveryAddress: istiod:15012 Annotation in the Pods\nYou can also use annotation at the Pod level to customize the configuration. For example, you can add the following annotations to Pod to modify the default port for istiod of the workload:\nanannotations: proxy.istio.io/config: | discoveryAddress: istiod:15012 When you configure sidecar in either of these ways, the fields set in annotations will completely override the default fields in MeshConfig. Please refer to the Istio documentation for all configuration items of ProxyConfig.\nThe new API – ProxyConfig\nBut in 1.13, a new top-level custom resource, ProxyConfig, has been added, allowing you to customize the configuration of your sidecar proxy in one place by specifying a namespace and using a selector to select the scope of the workload, just like any other CRD. Istio currently has limited support for this API, so please refer to the Istio documentation for more information on the ProxyConfig API.\nHowever, no matter which way you customize the configuration of the sidecar proxy, it does not take effect dynamically and requires a workload restart to take effect. For example, for the above configuration, because you changed the default port of istiod, all the workloads in the mesh need to be restarted before connecting to the control plane.\nTelemetry API MeshConfig customized extensions and configurations in the Istio mesh. The three pillars of observability– Metrics, Telemetry, and Logging– can each be docked to different providers. The Telemetry API gives you a one-stop place for flexible configuration of them. Like the ProxyConfig API, the Telemetry API follows the configuration hierarchy of Workload Selector \u0026gt; Local Namespace \u0026gt; Root Configuration Namespace. The API was introduced in Istio 1.11 and has been further refined in that release to add support for OpenTelemetry logs, filtered access logs, and custom tracing service names. See Telemetry Configuration for details.\nAutomatic resolution of multi-network gateway hostnames In September 2021, a member of the Istio community reported an issue with the EKS load balancer failing to resolve when running multi-cluster Istio in AWS EKS. Workloads that cross cluster boundaries need to be communicated indirectly through a dedicated east-west gateway for a multi-cluster, multi-network mesh. You can follow the instructions on Istio’s website to configure a multi-network, primary-remote cluster, and Istio will automatically resolve the IP address of the load balancer based on the hostname.\nIstio 1.13.1 fixing the critical security vulnerabilities Istio 1.13.1 was released to fix a known critical vulnerability that could lead to an unauthenticated control plane denial of service attack.\nThe figure below shows a multi-cluster primary-remote mesh where istiod exposes port 15012 to the public Internet via a gateway so that a pod on another network can connect to it.\nMulti-network Mesh When installing a multi-network, primary-remote mode Istio mesh, for a remote Kubernetes cluster to access the control plane, an east-west Gateway needs to be installed in the Primary cluster, exposing port 15012 of the control plane istiod to the Internet. An attacker could send specially crafted messages to that port, causing the control plane to crash. If you set up a firewall to allow traffic from only some IPs to access this port, you will be able to reduce the impact of the problem. It is …","relpermalink":"/en/blog/what-is-new-in-istio-1-13/","summary":"In February 2022, Istio released 1.13.0 and 1.13.1. This blog will give you an overview of what’s new in these two releases.","title":"What's New in Istio 1.13?"},{"content":"Join a team of world-class engineers working on the next generation of networking services using Istio, Envoy, Apache SkyWalking and a few of the open projects to define the next generation of cloud native network service.\nIstio upstream contributor: Golang We are looking for engineers with strong distributed systems experience to join our team. We are building a secure, robust, and highly available service mesh platform for mission critical enterprise applications spanning both legacy and modern infrastructure. This is an opportunity to dedicate a significant amount of contribution to Istio upstream on a regular basis. If you are a fan of Istio and would like to increase your contribution on a dedicated basis, this would be an opportunity for you.\nRequirements\nFundamentals-based problem solving skills; Drive decision by function, first principles based mindset. Demonstrate bias-to-action and avoid analysis-paralysis; Drive action to the finish line and on time. You are ego-less when searching for the best ideasIntellectually curious with a penchant for seeing opportunities in ambiguity Understands the difference between attention to detail vs. detailed - oriented Values autonomy and results over process You contribute effectively outside of your specialty Experience building distributed system platforms using Golang Familiarity with Kubernetes, service mesh technologies such as Istio and Envoy Excellent understanding of networking protocols, concepts, consistency properties of distributed systems, techniques to identify and reconcile configuration drifts Experience contributing to open source projects is a plus. Familiarity with WebAssembly is a plus. Familiarity with Golang, hardware/software load balancers (F5, NGINX), HSM modules, Active Directory/LDAP is a plus. We encourage written and asynchronous communication in English, and proficient oral English is not required.\nDistributed Systems Engineer, Enterprise Infrastructure (Data plane) GoLang or C++ Developers Seeking backend software engineers experienced in building distributed systems using Golang and gRPC. We are building a secure, and highly available service mesh platform for mission-critical enterprise applications for Fortune 500 companies, spanning both the legacy and modern infrastructure. Should possess strong fundamentals in distributed systems and networking. Familiarity with technologies like Kubernetes, Istio, and Envoy, as well as open contributions would be a plus.\nRequirements\nExperience building distributed system platforms using C++, Golang, and gRPC. Familiarity with Kubernetes, service mesh technologies such as Istio and Envoy. Excellent understanding of networking protocols, concepts, consistency properties of distributed systems, techniques to identify and reconcile configuration drifts. Experience contributing to open source projects is a plus. Familiarity with the following is a plus: WebAssembly, Authorization: NGAC, RBAC, ABAC Familiarity with hardware/software load balancers (F5, NGINX), HSM modules, Active Directory/LDAP is a plus. Site Reliability Engineer, SRE Site Reliability Engineering (SRE) combines software and systems engineering to build and run scalable, massively distributed, fault-tolerant systems. As part of the team, you will be working on ensuring that Tetrate’s platform has reliability/uptime appropriate to users’ needs as well as a fast rate of improvement. Additionally, much of our engineering effort focuses on building infrastructure, improving the platform troubleshooting abilities, and eliminating toil through automation.\nRequirements\nSystematic problem-solving approach, coupled with excellent communication skills and a sense of ownership/finish and self-directed drive. Strong fundamentals in operating, debugging, and troubleshooting distributed systems(stateful and/or stateless) and networking. Familiarity with Kubernetes, service mesh technologies such as Istio and EnvoyAbility to debug, optimize code, and automate routine tasks. Experience programming in at least one of the following languages: C++, Rust, Python, Go. Familiarity with the concepts of quantifying failure and availability in a prescriptive manner using SLOs and SLIs. Experience in performance analysis and tuning is a plus. Location Worldwide\nWe are remote with presence in China, Indonesia, India, Japan, U.S., Canada, Ireland, the Netherlands, Spain, and Ukraine.\nPlease send GitHub or online links that showcase your code style along with your resume to careers@tetrate.io .\nAbout Tetrate Powered by Envoy and Istio, its ﬂagship product, Tetrate Service Bridge, enables bridging traditional and modern workloads. Customers can get consistent baked-in observability, runtime security and traffic management for all their workloads, in any environment.\nIn addition to the technology, Tetrate brings a world-class team that leads the open Envoy and Istio projects, providing best practices and playbooks that enterprises can use to modernize their …","relpermalink":"/en/notice/tetrate-recruit/","summary":"Remotely worldwide","title":"The Enterprise Service Mesh company Tetrate is hiring"},{"content":"As the service mesh architecture concept gains traction and the scenarios for its applications emerge, there is no shortage of discussions about it in the community. I have worked on service mesh with the community for 4 years now, and will summarize the development of service mesh in 2021 from this perspective. Since Istio is the most popular service mesh, this article will focus on the technical and ecological aspects of Istio.\nService mesh: a critical tech for Cloud Native Infrastructure As one of the vital technologies defined by CNCF for cloud native, Istio has been around for five years now. Their development has gone through the following periods.\nExploration phase: 2017-2018 Early adopter phase: 2019-2020 Large-scale landing and ecological development phase: 2021-present Service mesh has crossed the “chasm”(refer Crossing the Chasm theory) and is in between the “early majority” and “late majority” phases of adoption. Based on feedback from the audience of Istio Weekly, users are no longer blindly following new technologies for experimentation and are starting to consider whether they need them in their organization dialectically.\nCross the chasm While new technologies and products continue to emerge, the service mesh, as part of the cloud native technology stack, has continued to solidify its position as the “cloud native network infrastructure” over the past year. The diagram below illustrates the cloud native technology stack model, where each layer has several representative technologies that define the standard. As new-age middleware, the service mesh mirrors other cloud native technologies, such as Dapr (Distributed Application Runtime), which represents the capability model for cloud native middleware, OAM , which defines the cloud native application model, and the service mesh, which defines the L7 network model.\nCloud Native Stack A layered view of the cloud native application platform technology stack\nOptimizing the mesh for large scale production applications with different deployment models Over the past year, the community focused on the following areas.\nPerformance optimization: performance issues of service mesh in large-scale application scenarios. Protocol and extensions: enabling service mesh to support arbitrary L7 network protocols. Deployment models: Proxyless vs. Node model vs. Sidecar model. eBPF: putting some of the service mesh’s capabilities to the kernel layer. Performance optimization Istio was designed to serve service to service traffic by “proto-protocol forwarding”. The goal is making the service mesh as “transparent” as possible to applications. Thus using IPtables to hijack the traffic, according to the community-provided test results Istio 1.2 adds only 3 ms to the baseline latency for a mesh with 1000 RPS on 16 connections. However, because of issues inherent in the IPtables conntrack module, Istio’s performance issues begin to emerge as the mesh size increases. To optimize the performance of the Istio sidecar for resource usage and network latency, the community gave the following solutions.\nSidecar configuration: By configuring service dependencies manually or by adding an Operator to the control plane, the number of service configurations sent to Sidecar can be reduced, thus reducing the resource footprint of the data plane; for more automatic and intelligent configuration of Sidecar, the open source projects Slime and Aeraki both offer their innovative configuration loading solutions. The introduction of eBPF: eBPF can be a viable solution to optimize the performance of the service mesh. Some Cilium-based startups even radically propose to use eBPF to replace the Sidecar proxy completely. Still, the Envoy proxy/xDS protocol has become the proxy for the service mesh implementation and supports the Layer 7 protocol very well. We can use eBPF to improve network performance, but complex protocol negotiation, parsing, and user scaling remain challenging to implement on the user side. Protocol and extensions Extensibility of Istio has always been a significant problem, and there are two aspects to Istio’s extensibility.\nProtocol level: allowing Istio to support all L7 protocols Ecological: allowing Istio to run more extensions Istio uses Envoy as its data plane. Extending Istio is essentially an extension of Envoy’s functionality. Istio’s official solution is to use WebAssembly, and in Istio 1.12, the Wasm plugin configuration API was introduced to extend the Istio ecosystem. Istio’s extension mechanism uses the Proxy-Wasm Application Binary Interface (ABI) specification to provide a set of proxy-independent streaming APIs and utilities that can be implemented in any language with an appropriate SDK. Today, Proxy-Wasm’s SDKs are AssemblyScript (similar to TypeScript), C++, Rust, Zig, and Go (using the TinyGo WebAssembly System Interface).\nThere are still relatively few WebAssembly extensions available, and many enterprises choose to customize their CRD and build a …","relpermalink":"/en/blog/service-mesh-in-2021/","summary":"A review of the development of Service Mesh in 2021.","title":"Service Mesh in 2021: The Ecosystem Is Emerging"},{"content":"It’s been more than four years since Istio launched in May 2017, and while the project has had a strong following on GitHub and 10+ releases, its growing open-source ecosystem is still in its infancy.\nRecently added support for WebAssembly extensions has made the most popular open source service mesh more extensible than ever. This table lists the open-source projects in the Istio ecosystem as of November 11, 2021, sorted by open-source date. These projects enhance the Istio service mesh with gateways, extensions, utilities, and more. In this article, I’ll highlight the two new projects in the category of extensions.\nProject Value Relationship with Istio Category Launch Date Dominant company Number of stars Envoy Cloud native high-performance edge/middle-service proxy The default data plane proxy September 2016 Lyft 18700 Istio Connection, secure, control, and observation services. Control plane service mesh May 2017 Google 29100 Emissary Gateway Kubernetes native API gateway for microservices, built on Envoy Connectable to Istio gateway February 2018 Ambassador 3600 APISIX Cloud native API gateways It can run as a data plane for Istio or as a gateway on its own gateway June 2019 API7 8100 MOSN Cloud native edge gateways \u0026amp; agents Available as Istio data plane proxy December 2019 Ant 3500 Slime Intelligent service mesh manager based on Istio Adding a management plane to Istio extensions January 2021 NetEase 236 GetMesh Istio integration and command-line management tools Utility for Istio multi-version management tools February 2021 Tetrate 95 Aeraki Manage any of Istio’s seven layers of load Extended multi-protocol support extensions March 2021 Tencent 330 Layotto Cloud native application runtime Using as a data plane for Istio runtime June 2021 Ant 393 Hango Gateway API gateways built on Envoy and Istio Integrates with Istio gateway August 2021 NetEase 253 Slime: an intelligent service mesh manager for Istio Slime is an Istio-based, intelligent mesh manager open-sourced by NetEase’s microservices team. Based on the Kubernetes Operator implementation, Slime can be used as a CRD manager that seamlessly interfaces with Istio without needing any customization or definition of dynamic service governance policies. This achieves automatic and convenient use of Istio and Envoy’s advanced features.\nSlime addresses the following issues:\nImplementing higher-level extensions in Istio. For example, extending the HTTP plugin; adaptive traffic limiting based on the resource usage of the service. Poor performance arising from Istio sending all the configurations within the mesh to each sidecar proxy. Slime solves these problems by building an Istio management plane. Its main purpose are\nto build a pluggable controller to facilitate the extension of new functions. to obtain data by listening to the data plane to intelligently generate the configuration for Istio. to build a higher-level CRD for the user to configure, which Slime converts into an Istio configuration. The following diagram shows the flow chart of Istio as an Istio management plane.\nSlime architecture The specific steps for Slime to manage Istio are as follows.\nSlime operator completes the initialization of Slime components in Kubernetes based on the administrator’s configuration. Developers create configurations that conform to the Slime CRD specification and apply them to Kubernetes clusters. Slime queries the monitoring data of the relevant service stored in Prometheus and converts the Slime CRD into an Istio CRD, in conjunction with the configuration of the adaptive part of the Slime CRD while pushing it to the Global Proxy. Istio listens for the creation of Istio CRDs. Istio pushes the configuration information of the Sidecar Proxy to the corresponding Sidecar Proxy in the data plane. The diagram below shows the internal architecture of Slime.\nSlime Internal We can divide Slime internally into three main components.\nslime-boot: operator for deploying Slime modules on Kubernetes. slime-controller: the core component of Slime that listens to the Slime CRD and converts it to an Istio CRD. slime-metric: the component used to obtain service metrics information. slime-controller dynamically adjusts service governance rules based on the information it receives. The following diagram shows the architecture of Slime Adaptive Traffic Limiting. Slime smart limiter Slime dynamically configures traffic limits by interfacing with the Prometheus metric server to obtain real-time monitoring.\nSlime’s adaptive traffic limitation process has two parts: one that converts SmartLimiter to EnvoyFilter and the other that monitors the data. Slime also provides an external monitoring data interface (Metric Discovery Server) that allows you to sync custom monitoring metrics to the traffic limiting component via MDS.\nThe CRD SmartLimiter created by Slime is used to configure adaptive traffic limiting. Its configuration is close to natural semantics, e.g., if you want to trigger an …","relpermalink":"/en/blog/istio-extensions-slime-and-aeraki/","summary":"In this article, I’ll introduce you two Istio extension projects: Aeraki and Slime.","title":"Introducing Slime and Aeraki in the Evolution of Istio Open-Source Ecosystem"},{"content":"You can use Istio to do multi-cluster management , API Gateway , and manage applications on Kubernetes or virtual machines . In my last blog , I talked about how service mesh is an integral part of cloud native applications. However, building infrastructure can be a big deal. There is no shortage of debate in the community about the practicability of service mesh and Istio– here’s a list of common questions and concerns, and how to address them.\nIs anyone using Istio in production? What is the impact on application performance due to the many resources consumed by injecting sidecar into the pod? Istio supports a limited number of protocols; is it scalable? Will Istio be manageable? – Or is it too complex, old services too costly to migrate, and the learning curve too steep? I will answer each of these questions below.\nIstio is architecturally stable, production-ready, and ecologically emerging Istio 1.12 was just released in November – and has evolved significantly since the explosion of service mesh in 2018 (the year Istio co-founders established Tetrate). Istio has a large community of providers and users . The Istio SIG of Cloud Native Community has held eight Istio Big Talk (Istio 大咖说) , with Baidu, Tencent, NetEase, Xiaohongshu(小红书), and Xiaodian Technology(小电科技) sharing their Istio practices. According to CNCF Survey Report 2020 , about 50% of the companies surveyed are using a service mesh in production or planning to in the next year, and about half (47%) of organizations using a service mesh in production are using Istio.\nMany companies have developed extensions or plugins for Istio, such as Ant, NetEase, eBay, and Airbnb. Istio’s architecture has been stable since the 1.5 release, and the release cycle is fixed quarterly, with the current project’s main task being Day-2 Operations.\nThe Istio community has also hosted various events, with the first IstioCon in March 2021, the Istio Meetup China in Beijing in July, and the Service Mesh Summit 2022 in Shanghai in January 2022.\nSo we can say that the Istio architecture is stable and production-ready, and the ecosystem is budding.\nThe impact of service mesh on application performance A service mesh uses iptables to do traffic hijacking by default to be transparent to applications. When the number of services is large, there are a lot of iptables rules that affect network performance. You can use techniques like eBPF to provide application performance, but the method requires a high version of the operating system kernel, which few enterprises can achieve.\nIstio DNS In the early days, Istio distributed the routing information of all services in the mesh to all proxy sidecars, which caused sidecar s to take up a lot of resources. Aeraki and Slime can achieve configuration lazy loading. We will introduce these two open-source projects in the Istio open-source ecosystem.\nFinally, there is a problem related to Sidecar proxy operation and maintenance: upgrading all Envoy proxies while ensuring constant traffic. A solution is using the SidecarSet resource in the open-source project OpenKruise .\nThe resource consumption and network latency associated with the introduction of Sidecar are also within reasonable limits, as you can see from the service mesh benchmark performance tests .\nExtending the Istio service mesh The next question is about extending the Istio service mesh. The current solution given by the Istio community is to use WebAssembly , an extension that is still relatively little used in production by now and has performance concerns. Most of the answers I’ve observed are CRDs that build a service mesh management plane based on Istio.\nAlso, making Istio support heterogeneous environments for all workloads, such as virtual machines and containers, is in strong demand for end-users. It allows them to migrate applications from traditional loads to cloud native easily. Finally, hybrid cloud traffic management for multiple clusters and meshes is a more advanced requirement.\nSteep learning curve Many people complain that Istio has too little learning material. Istio has been open source for four years, and there are a lot of learning resources now:\nIstio Documentation IstioCon 2021 Istio Big Talk/Istio Weekly Istio Fundamentals Course Certified Istio Administrator Yes, Istio is complex, but it’s been getting more and more manageable with every release. In my next blog, I will introduce you to two open source projects that extend Istio and give you some insight into what’s going on in the Istio community.\n","relpermalink":"/en/blog/the-debate-in-the-community-about-istio-and-service-mesh/","summary":"There is no shortage of debate in the community about the practicability of service mesh and Istio – here’s a list of common questions and concerns, and how to address them.","title":"The Debate in the Community About Istio and Service Mesh"},{"content":"If you don’t know what Istio is, you can read my previous articles below:\nWhat Is Istio and Why Does Kubernetes Need it? Why do you need Istio when you already have Kubernetes? This article will explore the relationship between service mesh and cloud native.\nService mesh – the product of the container orchestration war If you’ve been following the cloud-native space since its early days, you’ll remember the container orchestration wars of 2015 to 2017. Kubernetes won the container wars in 2017, the idea of microservices had taken hold, and the trend toward containerization was unstoppable. Kubernetes architecture matured and slowly became boring, and service mesh technologies, represented by Linkerd and Istio, entered the CNCF-defined cloud-native critical technologies on the horizon.\nKubernetes was designed with the concept of cloud-native in mind. A critical idea in cloud-native is the architectural design of microservices. When a single application is split into microservices, how can microservices be managed to ensure the SLA of the service as the number of services increases? The service mesh was born to solve this problem at the architectural level, free programmers’ creativity, and avoid tedious service discovery, monitoring, distributed tracing, and other matters.\nThe service mesh takes the standard functionality of microservices down to the infrastructure layer, allowing developers to focus more on business logic and thus speed up service delivery, which is consistent with the whole idea of cloud-native. You no longer need to integrate bulky SDKs in your application, develop and maintain SDKs for different languages, and just use the service mesh for Day 2 operations after the application is deployed.\nThe service mesh is regarded as the next generation of microservices. In the diagram, we can see that many of the concerns of microservices overlap with the functionality of Kubernetes. Kubernetes focuses on the application lifecycle, managing resources and deployments with little control over services. The service mesh fills this gap. The service mesh can connect, control, observe and protect microservices.\nKubernetes vs. xDS vs. Istio This diagram shows the layered architecture of Kubernetes and Istio.\nKubernetes vs xDS vs Istio The diagram indicates that the kube-proxy settings are global and cannot be controlled at a granular level for each service. All Kubernetes can do is topology-aware routing, routing traffic closer to the Pod, and setting network policies in and out of the Pod.\nIn contrast, the service mesh takes traffic control out of the service layer in Kubernetes through sidecar proxies, injects proxies into each Pod, and manipulates these distributed proxies through a control plane. It allows for more excellent resiliency.\nKube-proxy implements traffic load balancing between multiple pod instances of a Kubernetes service. But how do you finely control the traffic between these services — such as dividing the traffic by percentage to different application versions (which are all part of the same service, but on other deployments), or doing canary releases and blue-green releases?\nThe Kubernetes community gives a way to do canary releases using Deployment, assigning different pods to deployed services by modifying the pod’s label.\nEnvoy Architecture Currently, the most popular open-source implementation of service mesh in the world is Istio. From the CNCF Survey Report 2020 , we know that Istio is the most used service mesh in production today. Many companies have built their service mesh based on Istio, such as Ant, Airbnb, eBay, NetEase, Tencent, etc.\nCNCF Survey Report 2020 Figure from CNCF Survey Report 2020 Istio is developed based on Envoy, which has been used by default as its distributed proxy since the first day it was open-sourced. Envoy pioneered the creation of the xDS protocol for distributed gateway configuration, greatly simplifying the configuration of large-scale distributed networks. Ant Group open source MOSN also supported xDS In 2019. Envoy was also one of the first projects to graduate from CNCF, tested by large-scale production applications.\nService mesh – the cloud-native networking infrastructure With the above comparison between Kubernetes and service mesh in mind, we can see the place of service mesh in the cloud-native application architecture. That is, building a cloud-native network infrastructure specifically provides:\nTraffic management: controlling the flow of traffic and API calls between services, making calls more reliable, and enhancing network robustness in different environments. Observability: understanding the dependencies between services and the nature and flow of traffic between them provides the ability to identify problems quickly. Policy enforcement: controlling access policies between services by configuring the mesh rather than by changing the code. Service Identification and Security: providing service identifiability and security …","relpermalink":"/en/blog/service-mesh-an-integral-part-of-cloud-native-apps/","summary":"This article will explore the relationship between service mesh and cloud native.","title":"Service Mesh - An Integral Part of Cloud-Native Applications"},{"content":"API gateways have been around for a long time as the entry point for clients to access the back-end, mainly to manage “north-south” traffic, In recent years, service mesh architectures have become popular, mainly for managing internal systems,(i.e. “east-west” traffic), while a service mesh like Istio also has built-in gateways that bring traffic inside and outside the system under unified control. This often creates confusion for first-time users of Istio. What is the relationship between the service mesh and the API gateway? How does Istio’s gateway work? What are the ways to expose the services in the Istio mesh? This article gives you the answer.\nKey Insights The service mesh was originally created to solve the problem of managing internal traffic for distributed systems, but API gateways existed long before it. While the Gateway is built into Istio, you can still use a custom Ingress Controller to proxy external traffic. API gateways and service mesh are converging. How do I expose services in the Istio mesh? The following diagram shows four approaches to expose services in the Istio mesh using Istio Gateway, Kubernetes Ingress, API Gateway, and NodePort/LB.\nExposing services through Istio Ingress Gateway The Istio mesh is shaded, and the traffic in the mesh is internal (east-west) traffic, while the traffic from clients accessing services within the Kubernetes cluster is external (north-south) traffic.\nApproach Controller Features NodePort/LoadBalancer Kubernetes Load balancing Kubernetes Ingress Ingress controller Load balancing, TLS, virtual host, traffic routing Istio Gateway Istio Load balancing, TLS, virtual host, advanced traffic routing, other advanced Istio features API Gateway API Gateway Load balancing, TLS, virtual host, advanced traffic routing, API lifecycle management, billing, rate limiting, policy enforcement, data aggregation Since NodePort/LoadBalancer is a basic way to expose services built into Kubernetes, this article will not discuss that option. Each of the other three approaches will be described below.\nUsing Kubernetes Ingress to expose traffic We all know that clients of a Kubernetes cluster cannot directly access the IP address of a pod because the pod is in a network plane built into Kubernetes. We can expose services inside Kubernetes outside the cluster using NodePort or Load Balancer Kubernetes service type. To support virtual hosting, hiding and saving IP addresses, you can use Ingress resources to expose services in Kubernetes.\nKubernetes Ingress to expose services Ingress is a Kubernetes resource that controls the behavior of an ingress controller that does the traffic touring, which is the equivalent of a load-balanced directional proxy server such as Nginx, Apache, etc., which also includes rule definitions, i.e., routing information for URLs, which is provided by the Ingress controller .\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: istio name: ingress spec: rules: - host: httpbin.example.com http: paths: - path: /status/* backend: serviceName: httpbin servicePort: 8000 The kubernetes.io/ingress.class: istio annotation in the example above indicates that the Ingress uses the Istio Ingress Controller which in fact uses Envoy proxy.\nUsing Istio Gateway to expose services Istio is a popular service mesh implementation that has evolved from Kubernetes that implements some features that Kubernetes doesn’t. (See What is Istio and why does Kubernetes need Istio? ) It makes traffic management transparent to the application, moving this functionality from the application to the platform layer and becoming a cloud-native infrastructure.\nIstio used Kubernetes Ingress as the traffic portal in versions prior to Istio 0.8, where Envoy was used as the Ingress Controller. From Istio 0.8 and later, Istio created the Gateway object. Gateway and VirtualService are used to represent the configuration model of Istio Ingress, and the default implementation of Istio Ingress uses the same Envoy proxy. In this way, the Istio control plane controls both the ingress gateway and the internal sidecar proxy with a consistent configuration model. These configurations include routing rules, policy enforcement, telemetry, and other service control functions.\nThe Istio Gateway resources function similarly to the Kubernetes Ingress in that it is responsible for north-south traffic to and from the cluster. The Istio Gateway acts as a load balancer to carry connections to and from the edge of the service mesh. The specification describes a set of open ports and the protocols used by those ports, as well as the SNI configuration for load balancing, etc.\nThe Istio Gateway resource itself can only be configured for L4 through L6, such as exposed ports, TLS settings, etc.; however, the Gateway can be bound to a VirtualService, where routing rules can be configured on L7, such as versioned traffic routing, fault injection, HTTP redirects, HTTP …","relpermalink":"/en/blog/istio-servicemesh-api-gateway/","summary":"What is the relationship between the service mesh and the API gateway? How does Istio’s gateway work? What are the ways to expose the services in the Istio mesh? This article gives you the answer.","title":"Using Istio Service Mesh as API Gateway"},{"content":"Do you have multiple Kubernetes clusters and a service mesh? Do your virtual machines and services in a Kubernetes cluster need to interact? This article will take you through the process and considerations of building a hybrid cloud using Kubernetes and an Istio Service Mesh. Together, Kubernetes and Istio can be used to bring hybrid workloads into a mesh and achieve interoperability for multicluster. But another layer of infrastructure — a management plane — is helpful for managing multicluster or multimesh deployments.\nKubernetes Using Kubernetes enables rapid deployment of a distributed environment that enables cloud interoperability and unifies the control plane on the cloud. It also provides resource objects, such as Service, Ingress and Gateway , to handle application traffic. The Kubernetes API Server communicates with the kube-proxy component on each node in the cluster, creates iptables rules for the node, and forwards requests to other pods.\nAssuming that a client now wants to access a service in Kubernetes, the request is first sent to the Ingress/Gateway, then forwarded to the backend service (Service A in the diagram below) based on the routing configuration in the Ingress/Gateway. Then Service A polls an instance of Service B for the traffic requested by Service B. Lastly, the traffic requested by Service A for Service B is polled forward to Service B’s instance.\nKubernetes Kubernetes Multicluster The most common usage scenarios for multicluster management include:\nservice traffic load balancing isolating development and production environments decoupling data processing and data storage cross-cloud backup and disaster recovery flexible allocation of compute resources low-latency access to services across regions avoiding vendor lock-in There are often multiple Kubernetes clusters within an enterprise; and the KubeFed implementation of Kubernetes cluster federation developed by Multicluster SIG enables multicluster management capabilities, which allows all Kubernetes clusters to be managed through the same interface.\nThere are several general issues that need to be addressed when using cluster federation:\nConfiguring which clusters need to be federated API resources need to be propagated across the clusters Configuring how API resources are distributed to different clusters Registering DNS records in clusters to enable service discovery across clusters The following is a multicluster architecture for KubeSphere — one of the most commonly used Kubernetes multicluster management architectures — where the Host Cluster serves as the control plane with two member clusters, West and East.\nMulticluster The Host Cluster needs to be able to access the API Server of the Member Cluster, but the network connectivity between Member Clusters is not required. The Host Cluster is independent of the Member Cluster it manages and the Member Cluster is not aware of the existence of the Host Cluster. The advantage of this is that when the control plane fails, the Member Cluster will not be affected and the deployed load can still operate normally without being affected.\nThe Host Cluster also assumes the role of API portal, and the Host Cluster forwards the resource requests to the Member Cluster — which is convenient for aggregation and also facilitates unified authority authentication. We see that there is a Federation Control Plane in the Host Cluster, where the Push Reconciler propagates the identity, role, and role binding from the Federation Cluster to all Member Clusters.\nIstio Service Mesh Consider using the Istio service mesh when we have multilingual, multiversion microservices running in Kubernetes and need finer-grained canary publishing and unified security policy management for inter-service observability. Istio enables intelligent application-aware load balancing from the application layer to other Service Mesh-enabled services in the cluster, by transparently intercepting all traffic to and from the application using IPTables, and bypassing the primary kube-proxy load balancing. The Istio control plane communicates with the Kubernetes API Server to obtain information about all registered services in the cluster.\nThe following diagram illustrates the basics of Istio, where all nodes belong to the same Kubernetes cluster.\nIstio Service Mesh You may end up with at least a few Kubernetes clusters, each hosting microservices. Multiple deployment models exist for Istio’s multicluster deployments — depending on network isolation, primary and backup — which can be specified by declaration when deploying using Istio Operator. Communication between these microservices in a cluster can be enhanced by a service mesh. Within the cluster, Istio provides common communication patterns to improve resiliency, security and observability.\nAll of the above is about application load management on Kubernetes, but for legacy applications on virtual machines: how can they be managed in the same plane? Istio supports …","relpermalink":"/en/blog/multicluster-management-with-kubernetes-and-istio/","summary":"This article explains three patterns/tools for debugging microservices in Kubernetes and the changes brought by the introduction of Istio for debugging microservices.","title":"Multicluster Management With Kubernetes and Istio"},{"content":"Kubernetes is arguably the best environment for running microservices so far, but the experience of debugging microservices in a Kubernetes environment may not be as user-friendly. This article will show you how to debug microservices in Kubernetes, introduce common tools, and explain how the introduction of Istio impacts debugging microservices.\nDebugging microservices is vastly different from traditional monolithic applications The debugging of microservices has been a long-standing problem for software developers. This challenge does not exist in traditional monolithic applications because developers can leverage the debugger in IDEs to add breakpoints, modify environment variables, single-step execution, etc. for their applications, all of which provide great help in software debugging. With the popularity of Kubernetes, the debugging of microservices becomes a thorny issue, where the following issues are more complicated than the debugging of traditional monolithic applications.\nMultiple dependencies A microservice often depends on multiple other microservices, some shared volumes across multiple microservices, and authorizations based on service accounts. When debugging a microservice, how do you deploy other dependent services to quickly build a latest set of staging environments?\nAccess from a local machine When microservices are running on a developer’s local computer, there is usually no direct access to the services in a Kubernetes cluster. How can you debug microservices deployed in a Kubernetes cluster as if they were local services?\nSlow development loop Usually, it takes a long process to update the code and build it into an image before pushing it to the cluster. How do you speed up the development cycle? Let’s look at the tools that address those challenges.\nTools The main solutions for debugging microservices in Kubernetes are:\nProxy: by building a VPN, deploying a proxy in the Kubernetes cluster, and adding local debug endpoints to make the services in Kubernetes directly accessible to local applications, your architecture will look like [ local service ] \u0026lt;-\u0026gt; [ proxy ] \u0026lt;-\u0026gt; [ app in Kubernetes ]. Sidecar: Inject a sidecar into the pod of the microservice to be debugged to intercept all traffic to and from the service, so that the service can be tracked and monitored, and the service can also be debugged in this sidecar. Service Mesh: To get an overall picture of the application, inject sidecars into all microservices so that you can get a dashboard that monitors global status. Here are three typical open source projects that implement the above solutions, each of which can help you debug microservices from a different perspective. You can apply them at different stages of software development and they can be said to be complementary to each other.\nProxy – debugging microservices with Telepresence Telepresence is essentially a local proxy that proxies data volumes, environment variables, and networks in a Kubernetes cluster locally. The following diagram shows the main usage scenarios for Telepresence.\nProxy mode: Telepresence Users need to manually execute the telepresence command locally, which will automatically deploy the agent to Kubernetes. Once the agent has been deployed,\nLocal services will have complete access to other services in the Kubernetes cluster, environment variables, Secret, ConfigMap, etc. Services in the cluster also have direct access to the locally exposed endpoints. However, this approach requires users to run multiple commands while debugging locally, and in some network environments it may not be possible to establish a VPN connection to the Kubernetes cluster.\nSidecar – debugging microservices with Nocalhost Nocalhost is a Kubernetes-based cloud development environment. To use it, you just need to install a plugin in your IDE – VS Code to extend Kubernetes and shorten the development feedback cycle. The development environment can be isolated by creating different namespaces for different users and using ServiceAccount when binding to different user corners. Nocalhost also provides a web console and API for administrators to manage different development environments.\nSidecar mode: Nocalhost As long as you have a Kubernetes cluster and have admin rights to the cluster, you can refer to the Nocalhost documentation to quickly start trying it out. To use the Nocalhost plugin in VS Code, you need to configure the Kubernetes cluster in the plugin first.\nSelect the Kubeconfig file you just exported or copy and paste the contents of the file directly into the configuration. Then select the service you need to test and select the corresponding Dev Container. VS Code will automatically open a new code window. Here is an example of the bookinfo sample provided by Istio. You can open the cloned code in your local IDE and click the hammer next to the code file to enter development mode. Selecting the corresponding DevContainer and Nocalhost will automatically inject a …","relpermalink":"/en/blog/how-to-debug-microservices-in-kubernetes-with-proxy-sidecar-or-service-mesh/","summary":"This article explains three patterns/tools for debugging microservices in Kubernetes and the changes brought by the introduction of Istio for debugging microservices.","title":"How to Debug Microservices in Kubernetes With Proxy, Sidecar or Service Mesh?"},{"content":"Istio was named by Tetrate founder Varun Talwar and Google lead engineer Louis Ryan in 2017 and was open sourced on May 24, 2017. Today is the fourth anniversary of Istio’s open source arrival. Let’s take a look back at Istio’s four years of development — and look forward to Istio’s future.\nIstio’s open source history In 2017, the year Kubernetes ended the container orchestration battle, Google took the opportunity to consolidate its dominance in the cloud native space and compensate for Kubernetes’ disadvantage in service-to-service traffic management by open-sourcing Istio. Istio released its 1.10 last week — but here are some of the most important releases in Istio’s history to date.\nDate Version Note May 24, 2017 0.1 Officially open source; established the architectural foundation of Control Plane, Data Plane and sidecar proxy. October 10, 2017 0.2 Started to support multiple runtime environments, such as virtual machines. June 1, 2018 0.8 API refactoring July 31, 2018 1.0 Production-ready, after which the Istio team underwent a massive reorganization. March 19, 2019 1.1 Enterprise-ready. Support for multiple Kubernetes clusters, with performance optimizations. March 3, 2020 1.5 Back to monolith, with microservice components merged into istiod, making Istio’s architecture cleaner and easier to maintain. Support for WebAssembly extension, making Istio’s ecology much stronger. November 18, 2020 1.8 Officially deprecated Mixer and focused on adding support for virtual machines. A year after its inception– and two months before the 1.0 release, version 0.8 was released with a massive refactoring of the API. In late July 2018, when 1.0 was released, Istio reached a production-ready tipping point. Since then, Google has massively reorganized the Istio team and several Istio-based service mesh startups were born, making 2018 the booming year of the service mesh industry.\nIstio 1.1 was released in March 2019, almost 9 months after 1.0 was released, which is far beyond the average release cycle of an open-source project. We know that the speed of iteration and evolution is a core competency of basic software. Since then, Istio has started a regular release cadence of one version per quarter and has become the #4 fastest growing project in GitHub’s top 10 in 2019 !\nThe Istio community In 2020, Istio’s project management began to mature and its governance reached a stage of evolution. We saw the first election of a steering committee for the Istio community and the transfer of the trademark to Open Usage Commons . The first IstioCon was successfully held in February 2021, with thousands of people attending the online conference. There is also a large Istio community in China , and face-to-face Istio community meetups will be held there in 2021. Stay tuned for more.\nAccording to the CNCF 2020 Survey, 46% of organizations were either using a service mesh in production or planning to use it in the next 12 months. Istio was the top used mesh among those using a mesh in production.\nThe future After 4 years of development, there is not only a large user base around Istio, but also several Istio vendors, as you can see on the homepage of the recently revamped Istio website. In the last few releases, Istio has shifted its development focus to improving the Day 2 Operation experience. We also expect to see more Istio adoption path recommendations, case studies, learning materials, training, and certifications (such as the industry’s first Certified Istio Administrator from Tetrate) that will facilitate the adoption of Istio.\n","relpermalink":"/en/blog/istio-4-year-birthday/","summary":"Today is Istio's 4 year birthday, let’s take a look back at Istio’s four years of development — and look forward to Istio’s future.","title":"Happy Istio 4th Anniversary -- Retrospect and Outlook"},{"content":"Istio, the most popular service mesh implementation , was developed on top of Kubernetes and has a different niche in the cloud native application ecosystem than Kubernetes. Rather than introduce you directly to what Istio has to offer, this article will explain how Istio came about and what it is in relation to Kubernetes.\nWhy Is There an Istio? To explain what Istio is, it’s also important to understand the context in which Istio came into being — i.e., why is there an Istio?\nMicroservices are a technical solution to an organizational problem. And Kubernetes/Istio are a technical solution to deal with the issues created by moving to microservices. As a deliverable for microservices, containers solve the problem of environmental consistency and allow for more granularity in limiting application resources. They are widely used as a vehicle for microservices.\nGoogle open-sourced Kubernetes in 2014, which grew exponentially over the next few years. It became a container scheduling tool to solve the deployment and scheduling problems of distributed applications — allowing you to treat many computers as though they were one computer. Because the resources of a single machine are limited and Internet applications may have traffic floods at different times (due to rapid expansion of user scale or different user attributes), the elasticity of computing resources needs to be high. A single machine obviously can’t meet the needs of a large-scale application; and conversely, it would be a huge waste for a very small-scale application to occupy the whole host.\nIn short, Kubernetes defines the final state of the service and enables the system to reach and stay in that state automatically. So how do you manage the traffic on the service after the application has been deployed? Below we will look at how service management is done in Kubernetes and how it has changed in Istio.\nHow Do You Do Service Management in Kubernetes? The following diagram shows the service model in Kubernetes:\nKubernetes Service Model From the above figure we can see that:\nDifferent instances of the same service may be scheduled to different nodes. Kubernetes combines multiple instances of a service through Service objects to unify external services. Kubernetes installs a kube-proxy component in each node to forward traffic, which has simple load balancing capabilities. Traffic from outside the Kubernetes cluster can enter the cluster via Ingress (Kubernetes has several other ways of exposing services; such as NodePort, LoadBalancer, etc.). Kubernetes is used as a tool for intensive resource management. However, after allocating resources to the application, Kubernetes doesn’t fully solve the problems of how to ensure the robustness and redundancy of the application, how to achieve finer-grained traffic division (not based on the number of instances of the service), how to guarantee the security of the service, or how to manage multiple clusters, etc.\nThe Basics of Istio The following diagram shows the service model in Istio, which supports both workloads and virtual machines in Kubernetes.\nIstio From the diagram we can see that:\nIstiod acts as the control plane, distributing the configuration to all sidecar proxies and gateways. (Note: for simplification, the connections between Istiod and sidecar are not drawn in the diagram.) Istio enables intelligent application-aware load balancing from the application layer to other mesh enabled services in the cluster, and bypasses the rudimentary kube-proxy load balancing. Application administrators can manipulate the behavior of traffic in the Istio mesh through a declarative API, in the same way they manage workloads in Kubernetes. It can take effects within seconds and they can do this without needing to redeploy. Ingress is replaced by Gateway resources, a special kind of proxy that is also a reused Sidecar proxy. A sidecar proxy can be installed in a virtual machine to bring the virtual machine into the Istio mesh. In fact, before Istio one could use SpringCloud, Netflix OSS, and other tools to programmatically manage the traffic in an application, by integrating the SDK in the application. Istio makes traffic management transparent to the application, moving this functionality out of the application and into the platform layer as a cloud native infrastructure.\nIstio complements Kubernetes, by enhancing its traffic management, observability and security for cloud native applications. The service mesh open source project — launched in 2017 by Google, IBM and Lyft — has come a long way in three years. A description of Istio’s core features can be found in the Istio documentation .\nSummary Service Mesh is the cloud native equivalent of TCP/IP, addressing application network communication, security and visibility issues. Istio is currently the most popular service mesh implementation, relying on Kubernetes but also scalable to virtual machine loads. Istio’s core consists of a control plane and a data …","relpermalink":"/en/blog/what-is-istio-and-why-does-kubernetes-need-it/","summary":"This article will explain how Istio came about and what it is in relation to Kubernetes.","title":"What Is Istio and Why Does Kubernetes Need it?"},{"content":"If you’ve heard of service mesh and tried Istio , you may have the following questions:\nWhy is Istio running on Kubernetes? What is the role of Kubernetes and a service mesh in the cloud native application architecture, respectively? What aspects of Kubernetes does Istio extend? What problems does it solve? What is the relationship between Kubernetes, Envoy, and Istio? This article will take you through the inner workings of Kubernetes and Istio. In addition, I will introduce the load balancing approach in Kubernetes, and explain why you need Istio when you have Kubernetes.\nKubernetes is essentially about application lifecycle management through declarative configuration, while a service mesh is essentially about providing inter-application traffic, security management and observability. If you have already built a stable application platform using Kubernetes, how do you set up load balancing and traffic control for calls between services? This is where a service mesh comes into the picture.\nEnvoy introduces the xDS protocol, which is supported by various open source software, such as Istio , MOSN , etc. Envoy contributes xDS to a service mesh or cloud native infrastructure. Envoy is essentially a modern version of a proxy that can be configured through APIs, based on which many different usage scenarios are derived — such as API Gateway, sidecar proxy in service mesh, and edge proxy.\nThis article contains the following:\nA description of the role of kube-proxy. The limitations of Kubernetes for microservice management. An introduction to the capabilities of Istio service mesh. A comparison of some of the concepts in Kubernetes, Envoy, and the Istio service mesh. Kubernetes vs Service Mesh The following diagram shows the service access relationship in Kubernetes and service mesh (one sidecar per pod model).\nKubernetes vs Service Mesh Traffic Forwarding Each node in a Kubernetes cluster deploys a kube-proxy component that communicates with the Kubernetes API Server, gets information about the services in the cluster, and then sets iptables rules to send requests for service directly to the corresponding Endpoint (a pod belonging to the same group of services).\nService Discovery Service Discovery Istio can follow the service registration in Kubernetes and can also interface with other service discovery systems via platform adapters in the control plane; and then generate data plane configurations (using CRD, which are stored in etcd) with transparent proxies for the data plane. The transparent proxy of the data plane is deployed as a sidecar container in the pod of each application service, and all these proxies need to request the control plane to synchronize the proxy configuration. The proxy is “transparent” because the application container is completely unaware of the presence of the proxy. The kube-proxy component in the process needs to intercept traffic as well, except that the kube-proxy intercepts traffic to and from the Kubernetes node — while the sidecar proxy intercepts traffic to and from the pod.\nDisadvantages of a Service Mesh Since Kubernetes has many pods running on each node, putting the original kube-proxy route forwarding function in each pod will increase the response latency — due to more hops when the sidecar intercepts the traffic — and consume more resources. In order to manage traffic in a fine-grained manner, a series of new abstractions will be added. This will further increase the learning cost for users, but as the technology becomes more popular this situation will be slowly alleviated.\nAdvantages of a Service Mesh The kube-proxy settings are global and cannot be controlled at a granular level for each service, while service mesh takes the traffic control out of the service layer in Kubernetes by means of sidecar proxy — allowing for more elasticity.\nShortcomings of Kube-Proxy First, it does not automatically try another pod if the forwarded pod is not serving properly. Each pod has a health check mechanism and when a pod has health problems, kubelet will restart the pod and kube-proxy will remove the corresponding forwarding rules. Also, nodePort-type services cannot add TLS or more complex message routing mechanisms.\nKube-proxy implements load balancing of traffic across multiple pod instances of a Kubernetes service, but how do you do fine-grained control of traffic between these services — such as dividing traffic by percentage to different application versions (which are all part of the same service but on different deployments), or doing canary releases (grayscale releases) and blue-green releases?\nThe Kubernetes community gives a way to do canary releases using Deployment , which is essentially a way to assign different pods to a deployment’s service by modifying the pod’s label.\nKubernetes Ingress vs. Istio Gateway As mentioned above, kube-proxy can only route traffic within a Kubernetes cluster. The pods of a Kubernetes cluster are located in a network created by CNI. …","relpermalink":"/en/blog/why-do-you-need-istio-when-you-already-have-kubernetes/","summary":"This article will take you through the inner workings of Kubernetes and Istio. In addition, I will introduce the load balancing approach in Kubernetes, and explain why you need Istio when you have Kubernetes.","title":"Why Do You Need Istio When You Already Have Kubernetes?"},{"content":"Translator’s Foreword “Quarkus in Action: Java Solutions Optimized for Kubernetes” is authored by Alex Soto Bueno and Jason Porter, translated by Zhang Xiaoyu, Liu Yan, and Song Jingchao, and published by Machinery Industry Press in March 2021.\nQuarkus Cookbook in Action Quarkus is a new technology framework that differs from traditional Java architectures. It builds upon familiar technology stacks, utilizing mature technologies such as JPA, JAX-RS, Eclipse Vert.x, Eclipse MicroProfile, and CDI, tightly integrated with Kubernetes. Users can leverage Kubernetes’ efficient scheduling and operational capabilities to maximize resource savings.\nThe spark of cloud-native technology, ignited by the popularity of the Kubernetes community, has grown into a raging fire. Cloud-native-related technologies are emerging like mushrooms after the rain. Liu Yan, Song Jingchao, and I are all members of the cloud-native community and enthusiasts who love to promote various related technologies. One of our common interests is to keep an eye on the release of excellent books on foreign or mature technologies in this field.\nIn this process, we coincidentally discovered this book, which had not been translated in China yet. With great enthusiasm, we embarked on the journey of studying Quarkus.\nThis book portrays the technical aspects of Quarkus in a detailed and thorough manner by presenting questions, proposing solutions, and sparking discussions. Through this book, users can self-learn relevant content and improve the efficiency of Java-related development work with Quarkus, enabling them to stand firm in the fast-paced fields of microservice construction and cloud-based application development.\nThroughout the entire translation process, we received full support from HuaZhong Publishing House and Editor Li Zhongming, for which we express our heartfelt gratitude.\nFinally, thank you all for having the opportunity to read this book. We hope that our efforts can help you, who advocate for cloud-native technology, enjoy the same joy as us on the technical path of Quarkus.\n","relpermalink":"/en/book/quarkus-cookbook/","summary":"Authored by Alex Soto Bueno and Jason Porter.","title":"Quarkus in Action"},{"content":"Tetrate Academy has recently released the Istio Fundamentals Course, which is now available for free. Sign up at Tetrate Academy now!\nCourse curriculum Here is the curriculum:\nService Mesh and Istio Overview Installing Istio Observability: Telemetry and Logs Traffic Management Security Advanced Features Troubleshooting Real World Examples There are self-assessment questions at the end of each course. I have passed the course, and here is the certificate after passing the course.\nTetrate Academy Istio Fundamentals Course More In the future, Tetrate will release the Certified Istio Administrator (CIA) exam and welcome all Istio users and administrators to follow and register for it.\n","relpermalink":"/en/notice/tetrate-istio-fundamental-courses/","summary":"Tetrate Academy has recently released the Istio Fundamentals Course, which is now available for free.","title":"Tetrate Academy Releases Free Istio Fundamentals Course"},{"content":"Different companies or software providers have devised countless ways to control user access to functions or resources, such as Discretionary Access Control (DAC), Mandatory Access Control (MAC), Role-Based Access Control (RBAC), and Attribute-Based Access Control (ABAC). In essence, whatever the type of access control model, three basic elements can be abstracted: user, system/application, and policy.\nIn this article, we will introduce ABAC, RBAC, and a new access control model — Next Generation Access Control (NGAC) — and compare the similarities and differences between the three, as well as why you should consider NGAC.\nWhat Is RBAC? RBAC, or Role-Based Access Control, takes an approach whereby users are granted (or denied) access to resources based on their role in the organization. Every role is assigned a collection of permissions and restrictions, which is great because you don’t need to keep track of every system user and their attributes. You just need to update appropriate roles, assign roles to users, or remove assignments. But this can be difficult to manage and scale. Enterprises that use the RBAC static role-based model have experienced role explosion: large companies may have tens of thousands of similar but distinct roles or users whose roles change over time, making it difficult to track roles or audit unneeded permissions. RBAC has fixed access rights, with no provision for ephemeral permissions or for considering attributes like location, time, or device. Enterprises using RBAC have had difficulty meeting the complex access control requirements to meet regulatory requirements of other organizational needs.\nRBAC Example Here’s an example Role in the “default” namespace in Kubernetes that can be used to grant read access to pods:\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [\u0026#34;v1\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] What Is ABAC? ABAC stands for Attribute-Based Access Control. At a high level, NIST defines ABAC as an access control method “where subject requests to perform operations on objects are granted or denied based on assigned attributes of the subject, environment conditions, and a set of policies that are specified in terms of those attributes and conditions.” ABAC is a fine-grained model since you can assign any attributes to the user, but at the same time it becomes a burden and hard to manage:\nWhen defining permissions, the relationship between users and objects cannot be visualized. If the rules are a little complex or confusingly designed, it will be troublesome for the administrator to maintain and trace. This can cause performance problems when there is a large number of permissions to process.\nABAC Example Kubernetes initially uses ABAC as access control and is configured via JSON Lines, for example:\nAlice can just read pods in namespace “foo”:\n{\u0026#34;apiVersion\u0026#34;: \u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: {\u0026#34;user\u0026#34;: \u0026#34;alice\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;pods\u0026#34;, \u0026#34;readonly\u0026#34;: true}} What Is NGAC? NGAC, or Next Generation Access Control, takes the approach of modeling access decision data as a graph. NGAC enables a systematic, policy-consistent approach to access control, granting or denying users administrative capabilities with a high level of granularity. NGAC was developed by NIST (National Institute of Standards and Technology) and is currently used in Tetrate Q and Tetrate Service Bridge .\nThere are several types of entities; they represent the resources you want to protect, the relationships between them, and the actors that interact with the system. The entities are:\nUsers Objects User attributes, such as organization unit Object attributes, such as folders Policy classes, such as file system access, location, and time NIST’s David Ferraiolo and Tetrate ‘s Ignasi Barrera shared how NGAC works at their presentation on Next Generation Access Control at Service Mesh Day 2019 in San Francisco.\nNGAC is based on the assumption that you can represent the system you want to protect in a graph that represents the resources you want to protect and your organizational structure, in a way that has meaning to you and that adheres to your organization semantics. On top of this model that is very particular to your organization, you can overlay policies. Between the resource model and the user model, the permissions are defined. This way NGAC provides an elegant way of representing the resources you want to protect, the different actors in the system, and how both worlds are tied together with permissions.\nNGAC DAG Image via Linear Time Algorithms to Restrict Insider Access using Multi-Policy Access Control Systems NGAC Example The following example shows a simple NGAC graph with a User DAG representing an organization structure, an Object DAG representing files and folders in a filesystem, a categorization of the files, and two different policies — file system and scope — …","relpermalink":"/en/blog/why-you-should-choose-ngac-as-your-access-control-model/","summary":"This article will introduce you to the next generation permission control model, NGAC, and compare ABAC, RABC, and explain why you should choose NGAC.","title":"Why You Should Choose NGAC as Your Access Control Model"},{"content":" IstioCon 2021 poster (Jimmy Song) Topic: Service Mesh in China Time: February 23rd, 10:00 - 10:10 am Beijing time How to participate: IstioCon 2021 website Cost: Free From February 22-25, Beijing time, the Istio community will be hosting the first IstioCon online, and registration is free to attend! I will be giving a lightning talk on Tuesday, February 23rd (the 12th day of the first month of the lunar calendar), as an evangelist and witness of Service Mesh technology in China, I will introduce the Service Mesh industry and community in China.\nI am a member of the inaugural IstioCon organizing committee with Zhonghu Xu (Huawei) and Shaojun Ding (Intel), as well as the organizer of the China region. Considering Istio’s large audience in China, we have arranged for Chinese presentations that are friendly to the Chinese time zone. There will be a total of 14 sharing sessions in Chinese, plus dozens more in English. The presentations will be in both lightning talk (10 minutes) and presentation (40 minutes) formats.\nJoin the Cloud Native Community Istio SIG to participate in the networking at this conference. For the schedule of IstioCon 2021, please visit IstioCon 2021 official website , or click for details.\n","relpermalink":"/en/notice/istiocon-2021/","summary":"IstioCon 2021, I'll be giving a lightning talk, February 22nd at 10am BST.","title":"IstioCon 2021 Lightning Talk Preview"},{"content":"The ServiceMesher website has lost connection with the webhook program on the web publishing server because the GitHub where the code is hosted has has been “lost” and the hosting server is temporarily unable to log in, so the site cannot be updated. Today I spent a day migrating all the blogs on ServiceMesher to the Cloud Native Community website cloudnative.to , and as of today, there are 354 blogs on the Cloud Native Community.\nServiceMesher blogs Now we plan to archive ServiceMesher official GitHub (all pages under the servicemesher.com domain) We are no longer accepting new PRs, so please submit them directly to the Cloud Native Community . Thank you all!\n","relpermalink":"/en/notice/servicemesher-blog-merged/","summary":"ServiceMesher website is no longer maintained, plan to archive the website code, the blog has been migrated to Cloud Native Community, please submit the new blog to Cloud Native Community.","title":"ServiceMesher website is no longer maintained, the original blog has been migrated to the cloud native community"},{"content":"In this article, I’ll give you an overview of Istio ‘s history of virtual machine integration support. In particular, the introduction of the smart DNS proxy and WorkloadGroup in Istio 1.8, which makes virtual machines and containers equivalent at the resource abstraction level.\nI will show you a tumultuous odyssey of Istio’s virtual machine integration. Tetrate, the enterprise service mesh company that made pushing Istio to run everywhere part of its founding mission, has used VM features extensively in customer deployments and has been instrumental in pushing VMs to Istio upstream.\nPreface In my previous article , I talked about how Istio 1.7 supported virtual machines. But at that time, late October, virtual machines were still not seamlessly integrated into Istio — there was still a lot of manual work required. Now, Istio 1.8 has added WorkloadGroup and smart DNS proxy, which allows non-Kubernetes workloads like VMs to become first-class citizens in Istio — just like pods.\nWith or without a sidecar installed for virtual machines, until 1.7 you could not resolve the DNS name of a Kubernetes service unless a kube-external DNS was configured — which is the last piece of virtual machine integration in Istio. This shortcoming has finally been fixed in Istio 1.8.\nWhy Is Virtual Machine Support Important? In the process of migrating our applications to cloud native architectures and continuously containerizing them, we will go through three phases as shown in the figure below.\nCloud Native Stages Stage 1: All applications are deployed on virtual machines Stage 2: Applications are deployed on both virtual machines and containers, are migrating from virtual machines to containers, and are using Kubernetes to manage containers. Stage 3: All applications are deployed in containers first, using Kubernetes to manage containers and Istio to manage service-to-service communication. The above diagram is artificially simplified: in reality, there might be multiple hybrid clouds, multiple regions, multiple clusters, etc. Plus, at stage 3 containers and virtual machines may remain in long-term coexistence, but the trend of containerization remains unchanged.\nIstio’s History of Virtual Machine Support Istio’s support for virtual machines is a long process, an odyssey of sorts.\n0.2: Istio Mesh Expansion As of version 0.2, Istio added virtual machines to the Mesh via Istio Mesh Expansion , provided that the following prerequisites were met.\nVirtual machines must have direct access to the application’s pods via IP address, which requires a flat network between the container and the VM via VPC or VPN; and virtual machines do not need access to the Cluster IP, but rather direct access to the service’s endpoints. Virtual machines must have access to Istio’s control plane services (Pilot, Mixer, CA, now being integrated as Istiod), which can expose the control plane endpoints to virtual machines by deploying load balancers in the Istio Mesh. (optional) the virtual machine has access to the DNS server inside the Mesh (deployed in Kubernetes). The steps to integrate a virtual machine are as follows.\nCreate an internal load balancer for the Istio control plane service and the DNS service for the Kubernetes cluster. Generate a configuration file for the Istio Service CIDR, Service Account token, security certificate, and IP of the Istio Control Plane Service (the IP exposed through the Internal Load Balancer) and send it to the virtual machine. Setup the Istio component, dnsmaq (for DNS discovery), in the virtual machine; so that the virtual machine can access the services in the mesh using FQDN, to ensure that the virtual machine can correctly resolve the Cluster IP of the services in the mesh. To run the service in a virtual machine, you need to configure the sidecar, add inbound ports to be intercepted, then restart Istio and also run istioctl to register the service. The following figure shows the detailed flow from integrating a virtual machine to accessing services in the virtual machine in a mesh.\nFigure 1 Figure 1\nThe DNS is hijacked by dnsmasq deployed in the virtual machine, which allows it to correctly obtain the Cluster IP of the Istio service (Kubernetes’ built-in DNS). Access to Kubernetes’ built-in DNS service (which is exposed outside the cluster via the Internal Load Balancer and can be accessed directly). Return the Cluster IP resolved by productpage.bookinfo.svc.cluster.local, noting that the IP address is not directly accessible, but failure to be DNS resolved will result in a failed VM request for the service. The virtual machine’s call to services in a mesh is hijacked by the sidecar proxy. Since the proxy is connected to the Istio control plane, the endpoints of the service can be queried via xDS, so traffic will be forwarded to one of the endpoints. To access VM services in mesh, you need to manually add VM services to mesh using the istioctl register command, which essentially registers the VM services to the …","relpermalink":"/en/blog/istio-18-a-virtual-machine-integration-odyssey/","summary":"In this article, I’ll give you an overview of Istio‘s history of virtual machine integration support. In particular, the introduction of the smart DNS proxy and WorkloadGroup in Istio 1.8, which makes virtual machines and containers equivalent at the resource abstraction level.","title":"Istio 1.8: A Virtual Machine Integration Odyssey"},{"content":"A service mesh is a relatively simple concept, consisting of a bunch of network proxies paired with each service in an application, plus a set of task management processes. The proxies are called the data plane and the management processes are called the control plane in the Service Mesh. The data plane intercepts calls between different services and “processes” them; the control plane is the brain of the mesh that coordinates the behavior of proxies and provides APIs for operations and maintenance personnel to manipulate and observe the entire network.\nThe diagram below shows the architecture of a service mesh.\nService Mesh Architecture Further, the service mesh is a dedicated infrastructure layer designed to enable reliable, fast, and secure inter-service invocation in microservices architectures. It is not a mesh of “services” but rather a mesh of “proxies” that services can plug into, thus abstracting the network from the application code. In a typical service mesh, these proxies are injected into each service deployment as a sidecar (and also may be deployed at the edge of the mesh). Instead of invoking services directly over the network, services invoke their local sidecar proxy, which in turn manages requests on behalf of the service, pushing the complexities of inter-service communications into a networking layer that can resolve them at scale. The set of interconnected sidecar proxies implements a so-called data plane, while on the other hand the service mesh control plane is used to configure proxies. The infrastructure introduced by a service mesh provides an opportunity, too, to collect metrics about the traffic that is flowing through the application.\nThe architecture of a service mesh The infrastructure layer of a service mesh is divided into two main parts: the control plane and the data plane.\nCharacteristics of the control plane\nDo not parse packets directly. Communicates with proxies in the control plane to issue policies and configurations. Visualizes network behavior. Typically provides APIs or command-line tools for configuration versioning and management for continuous integration and deployment. Characteristics of the data plane\nIs usually designed with the goal of statelessness (though in practice some data needs to be cached to improve traffic forwarding performance). Directly handles inbound and outbound packets, forwarding, routing, health checking, load balancing, authentication, authentication, generating monitoring data, etc. Is transparent to the application, i.e., can be deployed senselessly. Changes brought by the service mesh Decoupling of microservice governance from business logic\nA service mesh takes most of the capabilities in the SDK out of the application, disassembles them into separate processes, and deploys them in a sidecar model. By separating service communication and related control functions from the business process and synching them to the infrastructure layer, a service mesh mostly decouples them from the business logic, allowing application developers to focus more on the business itself.\nNote that the word “mostly” is mentioned here and that the SDK often needs to retain protocol coding and decoding logic, or even a lightweight SDK to implement fine-grained governance and monitoring policies in some scenarios. For example, to implement method-level call distributed tracing, the service mesh requires the business application to implement trace ID passing, and this part of the implementation logic can also be implemented through a lightweight SDK. Therefore, the service mesh is not zero-intrusive from a code level.\nUnified governance of heterogeneous environments\nWith the development of new technologies and staff turnover, there are often applications and services in different languages and frameworks in the same company, and in order to control these services uniformly, the previous practice was to develop a complete set of SDKs for each language and framework, which is very costly to maintain. With a service mesh, multilingual support is much easier by synching the main service governance capabilities to the infrastructure. By providing a very lightweight SDK, and in many cases, not even a separate SDK, it is easy to achieve unified traffic control and monitoring requirements for multiple languages and protocols.\nFeatures of service mesh Service mesh also has three major technical advantages over traditional microservice frameworks.\nObservability\nBecause the service mesh is a dedicated infrastructure layer through which all inter-service communication passes, it is uniquely positioned in the technology stack to provide uniform telemetry at the service invocation level. This means that all services are monitored as “black boxes.” The service mesh captures route data such as source, destination, protocol, URL, status codes, latency, duration, etc. This is essentially the same data that web server logs can provide, but the service mesh captures this data for …","relpermalink":"/en/blog/what-is-a-service-mesh/","summary":"This article will take you through what a service mesh is, as well as its architecture, features, and advantages and disadvantages.","title":"What Is a Service Mesh?"},{"content":"1.8 is the last version of Istio to be released in 2020 and it has the following major updates:\nSupports installation and upgrades using Helm 3. Mixer was officially removed. Added Istio DNS proxy to transparently intercept DNS queries from applications. WorkloadGroup has been added to simplify the integration of virtual machines. WorkloadGroup is a new API object. It is intended to be used with non-Kubernetes workloads like Virtual Machines and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies.\nInstallation and Upgrades Istio starts to officially support the use of Helm v3 for installations and upgrades. In previous versions, the installation was done with the istioctl command-line tool or Operator. With version 1.8, Istio supports in-place and canary upgrades with Helm.\nEnhancing Istio’s Usability The istioctl command-line tool has a new bug reporting feature (istioctl bug-report ), which can be used to collect debugging information and get cluster status.\nThe way to install the add-on has changed: 1.7 istioctl is no longer recommended and has been removed in 1.8, to help solve the problem of add-on lagging upstream and to make it easier to maintain.\nTetrate is an enterprise service mesh company. Our flagship product, TSB, enables customers to bridge their workloads across bare metal, VMs, K8s, \u0026amp; cloud at the application layer and provide a resilient, feature-rich service mesh fabric powered by Istio, Envoy, and Apache SkyWalking.\nMixer, the Istio component that had been responsible for policy controls and telemetry collection, has been removed. Its functionalities are now being served by the Envoy proxies. For extensibility, service mesh experts recommend using WebAssembly (Wasm) to extend Envoy; and you can also try the GetEnvoy Toolkit , which makes it easier for developers to create Wasm extensions for Envoy. If you still want to use Mixer, you must use version 1.7 or older. Mixer continued receiving bug fixes and security fixes until Istio 1.7. Many features supported by Mixer have alternatives as specified in the Mixer Deprecation document, including the in-proxy extensions based on the Wasm sandbox API.\nSupport for Virtual Machines Istio’s recent upgrades have steadily focused on making virtual machines first-class citizens in the mesh. Istio 1.7 made progress to support virtual machines and Istio 1.8 adds a smart DNS proxy , which is an Istio sidecar agent written in Go. The Istio agent on the sidecar will come with a cache that is dynamically programmed by Istiod DNS Proxy. DNS queries from applications are transparently intercepted and served by an Istio proxy in a pod or VM that intelligently responds to DNS query requests, enabling seamless multicluster access from virtual machines to the service mesh.\nIstio 1.8 adds a WorkloadGroup , which describes a collection of workload instances. It provides a specification that the workload instances can use to bootstrap their proxies, including the metadata and identity. It is only intended to be used with non-k8s workloads like Virtual Machines, and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies. Using WorkloadGroups, Istio has started to help automate VM registration with istioctl experimental workload group .\nTetrate , the enterprise service mesh company, uses these VM features extensively in customers’ multicluster deployments, to enable sidecars to resolve DNS for hosts exposed at ingress gateways of all the clusters in a mesh; and to access them over mutual TLS.\nConclusion All in all, the Istio team has kept the promise made at the beginning of the year to maintain a regular release cadence of one release every three months since the 1.1 release in 2018, with continuous optimizations in performance and user experience for a seamless experience of brownfield and greenfield apps on Istio. We look forward to more progress from Istio in 2021.\n","relpermalink":"/en/blog/istio-1-8-a-smart-dns-proxy-takes-support-for-virtual-machines-a-step-further/","summary":"WorkloadGroup is a new API object. It is intended to be used with non-Kubernetes workloads like Virtual Machines and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies.","title":"Istio 1.8: A Smart DNS Proxy Takes Support For Virtual Machines A Step Further"},{"content":"Istio is a popular service mesh to connect, secure, control, and observe services. When it was first introduced as open source in 2017, Kubernetes was winning the container orchestration battle and Istio answered the needs of organizations moving to microservices. Although Istio claims to support heterogeneous environments such as Nomad, Consul, Eureka, Cloud Foundry, Mesos, etc., in reality, it has always worked best with Kubernetes — on which its service discovery is based.\nIstio was criticized for a number of issues early in its development, for the large number of components, the complexity of installation and maintenance, the difficulty of debugging, a steep learning curve due to the introduction of too many new concepts and objects (up to 50 CRDs), and the impact of Mixer components on performance. But these issues are gradually being overcome by the Istio team. As you can see from the roadmap released in early 2020, Istio has come a long way.\nBetter integration of VM-based workloads into the mesh is a major focus for the Istio team this year. Tetrate also offers seamless multicloud connectivity, security, and observability, including for VMs, via its product Tetrate Service Bridge . This article will take you through why Istio needs to integrate with virtual machines and how you can do so.\nWhy Should Istio Support Virtual Machines? Although containers and Kubernetes are now widely used, there are still many services deployed on virtual machines and APIs outside of the Kubernetes cluster that needs to be managed by Istio mesh. It’s a huge challenge to unify the management of the brownfield environment with the greenfield.\nWhat Is Needed to Add VMs to the Mesh? Before the “how,” I’ll describe what is needed to add virtual machines to the mesh. There are a couple of things that Istio must know when supporting virtual machine traffic: which VMs have services that should be part of the mesh, and how to reach the VMs. Each VM also needs an identity, in order to communicate securely with the rest of the mesh. These requirements could work with Kubernetes CRDs, as well as a full-blown Service Registry like Consul. And the service account based identity bootstrapping could work as a mechanism for assigning workload identities to VMs that do not have a platform identity. For VMs that do have a platform identity (like EC2, GCP, Azure, etc.), work is underway in Istio to exchange the platform identity with a Kubernetes identity for ease of setting up mTLS communication.\nHow Does Istio Support Virtual Machines? Istio’s support for virtual machines starts with its service registry mechanism. The information about services and instances in the Istio mesh comes from Istio’s service registries, which up to this point have only looked at or tracked pods. In newer versions, Istio now has resource types to track and watch VMs. The sidecars inside the mesh cannot observe and control traffic to services outside the mesh, because they do not have any information about them.\nThe Istio community and Tetrate have done a lot of work on Istio’s support for virtual machines. The 1.6 release included the addition of WorkloadEntry, which allows you to describe a VM exactly as you would a host running in Kubernetes. In 1.7, the release started to add the foundations for bootstrapping VMs into the mesh automatically through tokens, with Istio doing the heavy lifting. Istio 1.8 will debut another abstraction called WorkloadGroup, which is similar to a Kubernetes Deployment object — but for VMs.\nThe following diagram shows how Istio models services in the mesh. The predominant source of information comes from a platform service registry like Kubernetes, or a system like Consul. In addition, the ServiceEntry serves as a user-defined service registry, modeling services on VMs or external services outside the organization.\nWhy install Istio in a virtual machine when you can just use ServiceEntry to bring in the services in the VMs?\nUsing ServiceEntry, you can enable services inside the mesh to discover and access external services; and in addition, manage the traffic to those external services. In conjunction with VirtualService, you can also configure access rules for the corresponding external service — such as request timeouts, fault injection, etc. — to enable controlled access to the specified external service.\nEven so, it only controls the traffic on the client-side, not access to the introduced external service to other services. That is, it cannot control the behavior of the service as the call initiator. Deploying sidecars in a virtual machine and introducing the virtual machine workload via workload selector allows the virtual machine to be managed indiscriminately, like a pod in Kubernetes.\nFuture As you can see from the bookinfo demo , there is too much manual work involved in the process and it’s easy to go wrong. In the future, Istio will improve VM testing to be realistic, automate bootstrapping based on platform identity, …","relpermalink":"/en/blog/how-to-integrate-virtual-machines-into-istio-service-mesh/","summary":"Better integration of virtual machine-based workloads into the service mesh is a major focus for the Istio team this year, and Tetrate also provides seamless multi-cloud connectivity, security and observability, including for virtual machines, through its product Tetrate Service Bridge. This article will show you why Istio needs to integrate with virtual machines and how.","title":"How to Integrate Virtual Machines Into Istio Service Mesh"},{"content":"Today is my 914th day and also the last day with Ant Group , tomorrow is September 1st, which is usually the day school starts, and everyone at Alibaba is known as “classmate”, tomorrow I will join Tetrate , and that’s kind of starting my new semester!\nAnt/Alibaba and the Cloud Native Community To date, Ant/Alibaba Group has had a profound impact on my career, especially its corporate culture and values, and the Alibaba recruiting philosophy of “finding like-minded people”, and isn’t the process of creating the Cloud Native Community also a process of finding like-minded people? Cloud Native Community is like a small society, I don’t want it to have much social value, but only want it to make a small but beautiful change to individuals, to enterprises and to society. I constantly think about myself as an individual and as an employee, especially as an initiator of the community. What is my mission as an individual, an employee, and especially as an initiator of a community? What role should I play in the company? Where is this community going? I’m fumbling along, but because of your support, it makes me stronger and more committed to the adoption and application of cloud native technology in China, outside of me I may have gone faster, but now with the community together we will go further!\n24 June 2019, Shanghai, KubeCon China 2019 June 24, 2019, Shanghai, KubeCon China 2019\nJoining Tetrate Over the past two years, I’ve been working hard to promote Istio and Service Mesh technology, and with funding from Ant Group, I started the ServiceMesher Community to bring Service Mesh technology to China. Next I want to bring Chinese practice to the world.\nAs a Developer Advocate, the most important thing is not to stop learning, but to listen and take stock. Over the past two years, I’ve seen a lot of people show interest in Service Mesh, but not enough to understand the risks and lack of knowledge about the new technology. I’m excited to join this Service Mesh-focused startup Tetrate , a global telecommuting startup with products built around open source Istio , [Envoy](https:/ /envoyproxy.io) and Apache SkyWalking , it aims to make it to be the cloud native network infrastructure. Here are several maintainers of these open source projects, such as Sheng Wu , Zack Butcher , Lizan Zhou , etc., and I believe that working with them can help you understand and apply Service Mesh quickly and effectively across cloud native.\nMore Earlier this year as I was preparing for the Cloud Native community, I set the course for the next three years - cloud native, open source and community. The road to pursue my dream is full of thorns, not only need courage and perseverance, but also need you to be my strong backing, I will overcome the thorns and move forward. Open source belongs to the world, to let the world understand us better, we must be more active into the world. I hope that China’s open source tomorrow will be better, I hope that Service Mesh technology will be better applied by the enterprises in China, I hope that cloud native can benefit the public, and I hope that we can all find our own mission.\nWe are hiring now, if you are interested with Tetrate , please send your resume to careers@tetrate.io .\n","relpermalink":"/en/blog/moving-on-from-ant-group/","summary":"Today is my last day at Ant and tomorrow I'm starting a new career at Tetrate.","title":"New Beginning - Goodbye Ant, Hello Tetrate"},{"content":"“Cloud Native Patterns,” authored by Cornelia Davis, translated by Zhang Ruofei and Song Jingchao, published by Publishing House of Electronics Industry in August 2020.\nCover of the book Cloud Native Patterns What exactly are we talking about when we discuss cloud-native? I’ve been pondering this question for years, and perspectives may vary. Since translating the first book on cloud-native three years ago, I’ve been involved in translating and creating a series of works on cloud-native. Through participation in and observation of open-source projects, communities, foundations, and the process of application cloudification in the cloud-native field, I’ve come to the conclusion: cloud-native is a way of behavior and design philosophy. Essentially, any behavior or method that can improve resource utilization and application delivery efficiency in the cloud is cloud-native. The history of cloud computing is a history of cloud-native transformation. Cloud-native is the inevitable result of cloud computing adapting to social division of labor, entrusting system resources, underlying infrastructure, and application orchestration to cloud platforms, allowing developers to focus on business logic. Isn’t this what cloud computing has been striving for all along? Cloud-native applications pursue the rapid construction of highly fault-tolerant, elastic distributed applications, striving for the ultimate development efficiency and friendly online and operational experience. With the concept of cloud-native, born to be deployed in the cloud, they can maximize the dividends brought by cloud computing.\nBefore this, I translated several books on cloud-native topics, including “Cloud Native Go” by Kevin Hoffman and “Cloud Native Java” by Josh Long. They both come from Pivotal or have worked at Pivotal for many years. When I saw this book, I was surprised to find that the author, Cornelia Davis, also comes from this company. Pivotal is truly the cradle of cloud-native. The content of this book is different from previous cloud-native books, innovatively organizing patterns, so I immediately contacted Zhang Chunyu, the editor of Publishing House of Electronics Industry. He informed me that Zhang Ruofei was translating this book. Previously, I had cooperated with him to translate “Cloud Native Java.” This book is our second collaboration, and I truly admire his accuracy and efficiency in translating books. We each translated half of the content of this book.\nEveryone is discussing cloud-native, but how to implement it is still a matter of debate. This book lists 12 patterns for building cloud-native applications, mainly focusing on data, services, and interactions of cloud-native applications, namely, design patterns at the application level. These patterns are interspersed throughout the chapters of the second part of this book, covering various aspects of cloud-native applications, and combining theory with practice, guiding readers to implement a cloud-native application using Java.\nI also want to express my gratitude to the members and volunteers of the Cloud Native Community for their contributions to the development of cloud-native in China. Your encouragement and support are the driving force behind continuous efforts and exploration in the field of cloud-native. There may be some omissions in the translation process of this book, and I hope readers can point them out.\n","relpermalink":"/en/book/cloud-native-patterns/","summary":"Authored by Cornelia Davis.","title":"Cloud Native Patterns"},{"content":"Just tonight, the jimmysong.io website was moved to the Alibaba Cloud Hong Kong node. This is to further optimize the user experience and increase access speed. I purchased an ECS on the Alibaba Cloud Hong Kong node, and now I have a public IP and can set subdomains. The website was previously deployed on GitHub Pages, the access speed is average, and it has to withstand GitHub instability. Impact (In recent years, GitHub downtime has occurred).\nMeanwhile, the blog has also done a lot to improve the site, thanks to Bai Jun away @baijunyao strong support, a lot of work for the revision of the site, including:\nChanged the theme color scheme and deepened the contrast Use aligolia to support full site search Optimized mobile display Articles in the blog have added zoom function Added table of contents to blog post This site is built on the theme of educenter .\nThanks to the majority of netizens who have supported this website for several years. The website has been in use for more than three years and has millions of visits. It has undergone two major revisions before and after, on January 31, 2020 and October 8, 2017, respectively. And changed the theme of the website. In the future, I will share more cloud-native content with you as always, welcome to collect, forward, and join the cloud-native community to communicate with the majority of cloud-native developers.\n","relpermalink":"/en/notice/migrating-to-alibaba-cloud/","summary":"Move the website to the Alibaba Cloud Hong Kong node to increase the speed of website access and the convenience of obtaining public IP and subdomain names.","title":"Move to Alibaba Cloud Hong Kong node"},{"content":" Just the other day, Java just celebrated its 25th birthday , and from the time of its birth it was called “write once, run everywhere”, but more than 20 years later, there is still a deep gap between programming and actual production delivery. the world of IT is never short of concepts, and if a concept doesn’t solve the problem, then it’s time for another layer of concepts. it’s been 6 years since Kubernetes was born, and it’s time for the post-Kubernetes era - the era of cloud-native applications!\nCloud Native Stage This white paper will take you on a journey to explore the development path of cloud-native applications in the post-Kubernetes era.\nHighlights of the ideas conveyed include.\nCloud-native has passed through a savage growth period and is moving towards uniform application of standards. Kubernetes’ native language does not fully describe the cloud-native application architecture, and the development and operation functions are heavily coupled in the configuration of resources. Operator’s expansion of the Kubernetes ecosystem has led to the fragmentation of cloud-native applications, and there is an urgent need for a unified application definition standard. The essence of OAM is to separate the R\u0026amp;D and O\u0026amp;M concerns in the definition of cloud-native applications, and to further abstract resource objects, simplify and encompass everything. “Kubernetes Next Generation” refers to the fact that after Kubernetes became the infrastructure layer standard, the focus of the cloud-native ecology is being overtaken by the application layer, and the last two years have been a powerful exploration of the hot Service Mesh process, and the era of cloud-native application architecture based on Kubernetes is coming. Kubernetes has become an established operating platform for cloud-native applications, and this white paper will expand with Kubernetes as the default platform, including an explanation of the OAM-based hierarchical model for cloud-native applications.\n","relpermalink":"/en/notice/guide-to-cloud-native-app/","summary":"Take you on a journey through the post-Kubernetes era of cloud-native applications.","title":"Guide to Cloud Native Application"},{"content":"At the beginning of 2020, due to the outbreak of the Crona-19 pandemic, employees around the world began to work at home. Though the distance between people grew farer, there was a group of people, who were us working in the cloud native area, gathered together for a common vision. During the past three months, we have set up the community management committee and used our spare time working together to complete the preparatory work for the community. Today we are here to announce the establishment of the Cloud Native Community.\nBackground Software is eating the world. —— Marc Andreessen\nThis sentence has been quoted countless times, and with the rise of Cloud Native, we’d like to talk about “Cloud Native is eating the software.” As more and more enterprises migrate their services to the cloud, the original development mode of enterprises cannot adapt to the application scenarios in the cloud, and it is being reshaped to conform to the cloud native standard.\nSo what is cloud native? Cloud native is a collection of best practices in architecture, r\u0026amp;d process and team culture to support faster innovation, superior user experience, stable and reliable user service and efficient r\u0026amp;d. The relationship between the open source community and the cloud native is inseparable. It is the existence of the open source community, especially the end user community, that greatly promotes the continuous evolution of cloud native technologies represented by container, service mesh and microservices.\nCNCF (Cloud Native Computing Foundation) holds Cloud Native conference every year in the international community, which has a wide audience and great influence. But it was not held in China for the first time until 2018, after several successful international events. However, there are no independent foundations or neutral open source communities in China. In recent years, many cloud native enthusiasts in China have set up many communication groups and held many meetups, which are very popular. Many excellent open source projects have emerged in the cloud native field, but there is no organized neutral community for overall management. Under this background, the Cloud Native Community emerges at the right moment.\nAbout Cloud Native Community is an open source community with technology, temperature and passion. It was founded spontaneously by a group of industry elites who love open source and uphold the principle of consensus, co-governance, co-construction and sharing. The aim of the community is: connection, neutral, open source. We are based in China, facing the world, enterprise neutrality, focusing on open source, and giving feedback to open source.\nIntroduction for the Steering Community:https://cloudnative.to/en/team/ .\nYou will gain the followings after joining the community:\nKnowledge and news closer to the source A more valuable network More professional and characteristic consultation Opportunities to get closer to opinion leaders Faster and more efficient personal growth More knowledge sharing and exposure opportunities More industry talent to be found Contact Contact with us.\nEmail: mailto:contact@cloudnative.to Twitter: https://twitter.com/CloudNativeTo ","relpermalink":"/en/notice/cloud-native-community-announecement/","summary":"Today the Community Steering Committee announced the official formation of the Cloud Native Community.","title":"Establishment of the Cloud Native Community"},{"content":"This article is a rework of previously written content and is included in the Istio Handbook of the ServiceMesher community . Other chapters are still being compiled.\nPeople who have just heard of Service Mesh and tried Istio may have the following questions:\nWhy does Istio bind Kubernetes? What roles do Kubernetes and Service Mesh play in cloud native? What aspects of Kubernetes has Istio extended? What problems have been solved? What is the relationship between Kubernetes, xDS protocols (Envoy , MOSN, etc) and Istio? Should I use Service Mesh? In this section, we will try to guide you through the internal connections between Kubernetes, the xDS protocol, and Istio Service Mesh. In addition, this section will also introduce the load balancing methods in Kubernetes, the significance of the xDS protocol for Service Mesh, and why Istio is needed in time for Kubernetes.\nUsing Service Mesh is not to say that it will break with Kubernetes, but that it will happen naturally. The essence of Kubernetes is to perform application lifecycle management through declarative configuration, while the essence of Service Mesh is to provide traffic and security management and observability between applications. If you have built a stable microservice platform using Kubernetes, how do you set up load balancing and flow control for calls between services?\nThe xDS protocol created by Envoy is supported by many open source software, such as Istio , Linkerd , MOSN, etc. Envoy’s biggest contribution to Service Mesh or cloud native is the definition of xDS. Envoy is essentially a proxy. It is a modern version of proxy that can be configured through APIs. Based on it, many different usage scenarios are derived, such as API Gateway, Service Mesh. Sidecar proxy and Edge proxy in.\nThis section contains the following\nExplain the role of kube-proxy. Kubernetes’ limitations in microservice management. Describe the features of Istio Service Mesh. Describe what xDS includes. Compare some concepts in Kubernetes, Envoy and Istio Service Mesh. Key takeaways If you want to know everything in advance, here are some of the key points from this article:\nThe essence of Kubernetes is application lifecycle management, specifically deployment and management (scaling, scaling, automatic recovery, release). Kubernetes provides a scalable and highly resilient deployment and management platform for microservices. The foundation of Service Mesh is a transparent proxy. After the traffic between microservices is intercepted through sidecar proxy, the behavior of microservices is managed through the control plane configuration. Service Mesh decoupled from Kubernetes traffic management, the internal flow without the need of Service Mesh kube-proxy supporting components, micro-services closer to abstract the application layer by, for traffic between management services, security and observability. xDS defines the protocol standards for Service Mesh configuration. Service Mesh is a higher-level abstraction of services in Kubernetes. Its next step is serverless. Kubernetes vs Service Mesh The following figure shows the service access relationship between Kubernetes and Service Mesh (one sidecar per pod mode).\nkubernetes vs service mesh Traffic forwarding\nEach node of the cluster Kubernetes a deployed kube-proxy assembly Kubernetes API Server may communicate with the cluster acquired service information, and then set iptables rules, sends a request for a service directly to the corresponding Endpoint (belonging to the same group service pod).\nService discovery\nService registration in Service Mesh Istio Service Mesh can use the service in Kubernetes for service registration. It can also connect to other service discovery systems through the platform adapter of the control plane, and then generate the configuration of the data plane (using CRD statements, stored in etcd), a transparent proxy for the data plane. (Transparent proxy) is deployed in the sidecar container in each application service pod. These proxy need to request the control plane to synchronize the proxy configuration. The reason why is a transparent proxy, because there is no application container fully aware agent, the process kube-proxy components like the need to block traffic, but kube-proxythat blocks traffic to Kubernetes node and sidecar proxy that blocks out of the Pod For more information, see Understanding Route Forwarding by the Envoy Sidecar Proxy in Istio Service Mesh .\nDisadvantages of Service Mesh\nBecause each node on Kubernetes many runs Pod, the original kube-proxyrouting forwarding placed in each pod, the distribution will lead to a lot of configuration, synchronization, and eventual consistency problems. In order to perform fine-grained traffic management, a series of new abstractions will be added, which will further increase the user’s learning costs. However, with the popularization of technology, this situation will gradually ease.\nAdvantages of Service Mesh\nkube-proxy The …","relpermalink":"/en/blog/service-mesh-the-microservices-in-post-kubernetes-era/","summary":"This article is a rework of previously written content and is included in the Istio Handbook of the ServiceMesher community . Other chapters are still being compiled.","title":"Service Mesh - The Microservices in Post Kubernetes Era"},{"content":"2020 is indeed a bad start. In less than a month, I hardly heard any good news:\nTrump assassinated Major General Sulaymani of Iran; this pneumonia outbreak in Wuhan; the news that my basketball icon Kobe died of a helicopter crash really shocked me, and the Lakers said goodbye on the 24th. This gave me another spiritual blow during the Spring Festival, which was originally lacking in interest.\n2020 is bound to be a year deeply remembered in all humankind. In the last few days of the first month of the year, I decided to revise the website. The first reason was that I could n’t go out during the extended Chinese New Year holiday, and it was boring at home. And many pictures on the website were saved on Weibo map beds. The picture bed is unstable, causing many photos to be irretrievable; coupled with a habit of organizing the website every long holiday (the last revision of the website was completed during the National Day holiday in 2018, completed at home for 7 days), so I decided to The website has been revised again and it has become what it is now.\nFeatures The website has the following features after this revision:\nReorganize the website content, the structure is more reasonable Support email subscription Images are stored on Github Responsive static website, card design, better user experience Everyone is welcome to enter your email address in the email input box at the bottom of the page. Once there is an important content update on this site, we will push you through the email as soon as possible.\n","relpermalink":"/en/notice/website-revision-notice/","summary":"In the last days of the first month of 2020, I decided to revamp the website.","title":"jimmysong.io website revision notice"},{"content":"“Future Architecture: From Service-Oriented to Cloud-Native,” authored by Zhang Liang, Wu Sheng, Ao Xiao Jian, and Song Jingchao, published by Publishing House of Electronics Industry in April 2019.\nCover of the book Future Architecture The first author of this book is Zhang Liang, currently working at JD Finance. To enrich the content of the book, Zhang Liang invited his friends in the industry, Wu Sheng, Ao Xiao Jian, and myself, to co-author this ambitious work on “Future Architecture.” Below is an excerpt of his introduction to the book.\nOrigin of the Book As professionals in the internet industry, we have always been at the forefront of change, constantly chasing the waves of technological advancement to avoid falling behind the pace of the times. Especially in recent years, internet architecture has undergone continuous evolution, transitioning from centralized architecture to distributed architecture, and then to cloud-native architecture. Cloud-native architecture has gradually become the protagonist of this era because it can address issues such as slow application upgrades, bloated architectures, and inability to iterate quickly.\nIn the midst of this wave of change, I watched as it altered the course of internet architecture and brought new ideas and developments to more and more companies and individuals. I embarked on a journey to learn and understand it, integrating it into my knowledge system to update the architectural map in my mind with the experiences and insights accumulated over the years.\nIn 2017 and 2018, I experienced these changes firsthand, making projects like Elastic-Job and Sharding-Sphere widely recognized in the industry, internationalizing the open-source projects I was responsible for, and meeting more mentors and friends. These experiences inspired me to put what I heard, saw, knew, and felt onto paper, connecting them into this book: “Future Architecture: From Service-Oriented to Cloud-Native.”\nThis book covers everything you want to know about distributed systems, service-oriented architecture, service mesh, containers, orchestration, cloud-native, and cloud databases.\nIt contains my well-considered insights and experiences accumulated over the years, as well as my struggles of abandoning and picking up the pen again because I feel responsible for the content of the book.\nIt also features chapters contributed by seasoned experts in the field, including Wu Sheng, the founder of Apache Incubator project SkyWalking and an APM expert; Song Jingchao, a CNCF Ambassador, cloud-native evangelist, and founder of the Cloud-Native Community; and Ao Xiao Jian, a service mesh evangelist.\nTable of Contents Chapter 1: Cloud-Native Chapter 2: Remote Communication Chapter 3: Configuration Chapter 4: Service Governance Chapter 5: Observing Distributed Services Chapter 6: Intrusive Service Governance Solutions Chapter 7: Kubernetes: The Cornerstone of Cloud-Native Ecosystem Chapter 8: Cross-Language Service Governance Solutions: Service Mesh Chapter 9: Cloud-Native Data Architecture Chapter 10: Distributed Database Middleware Ecosystem: ShardingSphere Wings of Aspiration The back cover of the book features the Holy Trinity statue standing in front of Old Trafford Stadium, chosen by Professor Zhang Liang as the background image.\nOn February 6, 1958, while returning from Yugoslavia after securing a spot in the European Cup semi-finals, the Manchester United team encountered the Munich air disaster, disappearing in the night sky. To revive Manchester United, surviving manager Matt Busby rebuilt the team with blood, tears, and sweat. Exactly 10 years after the Munich air disaster, on May 29, 1968, Busby led his new team to finally lift the European Cup, comforting the souls of those who had passed away! The Holy Trinity statue became an eternal memorial!\nFaith, resilience, perseverance, miracles, and rebirth… These are the strengths I feel from this sculpture. Everyone will experience peaks and valleys, see mountains and deserts in their lifetime, and I hope this book not only brings practical knowledge of internet architecture but also embodies my blessings to you all: I hope that amidst the relentless struggles on this long journey, you can have your own faith and hope, and even after traversing countless deserts and oceans, you can see the light after experiencing numerous trials!\n","relpermalink":"/en/book/future-architecture/","summary":"Authors: Zhang Liang, Wu Sheng, Ao Xiao Jian, and Song Jingchao.","title":"Future Architecture: From Service-Oriented to Cloud-Native"},{"content":"The following paragraph is a release note from the Istio official blog https://istio.io/zh/blog/2019/announcing-1.1/ , which I translated.\nIstio was released at 4 a.m. Beijing time today and 1 p.m. Pacific time.\nSince the 1.0 release last July, we have done a lot to help people get Istio into production. We expected to release a lot of patches (six patches have been released so far!), But we are also working hard to add new features to the product.\nThe theme for version 1.1 is “Enterprise Ready”. We are happy to see more and more companies using Istio in production, but as some big companies join in, Istio also encounters some bottlenecks.\nThe main areas we focus on include performance and scalability. As people gradually put Istio into production and use larger clusters to run more services with higher capacity, there may be some scaling and performance issues. Sidecar takes up too much resources and adds too much latency. The control plane (especially Pilot) consumes excessive resources.\nWe put a lot of effort into making the data plane and control plane more efficient. In the 1.1 performance test, we observed that sidecars typically require 0.5 vCPU to process 1000 rps. A single Pilot instance can handle 1000 services (and 2000 pods) and consumes 1.5 vCPUs and 2GB of memory. Sidecar adds 5 milliseconds at the 50th percentile and 10 milliseconds at the 99th percentile (the execution strategy will increase latency).\nWe have also completed the work of namespace isolation. You can use the Kubernetes namespace to enforce control boundaries to ensure that teams do not interfere with each other.\nWe have also improved multi-cluster functionality and usability. We listened to the community and improved the default settings for flow control and policies. We introduced a new component called Galley. Galley validates YAML configuration, reducing the possibility of configuration errors. Galley is also used in multi-cluster setups-collecting service discovery information from each Kubernetes cluster. We also support other multi-cluster topologies, including single control planes and multiple synchronous control planes, without the need for flat network support.\nSee the release notes for more information and details .\nThere is more progress on this project. As we all know, Istio has many moving parts, and they take on too much work. To address this, we have recently established the Usability Working Group (available at any time). A lot happened in the community meeting (Thursday at 11 am) and in the working group. You can log in to discuss.istio.io with GitHub credentials to participate in the discussion!\nThanks to everyone who has contributed to Istio over the past few months-patching 1.0, adding features to 1.1, and extensive testing recently on 1.1. Special thanks to companies and users who work with us to install and upgrade to earlier versions to help us identify issues before they are released.\nFinally, go to the latest documentation and install version 1.1! Happy meshing!\nOfficial website The ServiceMesher community has been maintaining the Chinese page of the official Istio documentation since the 0.6 release of Istio . As of March 19, 2019, there have been 596 PR merges, and more than 310 documents have been maintained. Thank you for your efforts! Some documents may lag slightly behind the English version. The synchronization work is ongoing. For participation, please visit https://github.com/servicemesher/istio-official-translation . Istio official website has a language switch button on the right side of each page. You can always Switch between Chinese and English versions, you can also submit document modifications, report website bugs, etc.\nServiceMesher Community Website\nThe ServiceMesher community website covers all technical articles in the Service Mesh field and releases the latest activities in a timely manner. It is your one-stop portal to learn about Service Mesh and participate in the community.\n","relpermalink":"/en/notice/istio-11/","summary":"Istio 1.1 was released at 4 am on March 20th, Beijing time. This version took 8 months! The ServiceMesher community also launched the Istio Chinese documentation.","title":"Istio 1.1 released"},{"content":"Istio handbook was originally an open source e-book I created. It has been written for 8 months before donating to the ServiceMesher community. In order to further popularize Istio and Service Mesh technology, this book Donate to the community for co-authoring. The content of the original book was migrated to https://github.com/servicemesher/istio-handbook on March 10, 2019. The original book will no longer be updated.\nGitHub address: https://github.com/servicemesher/istio-handbook Conceptual picture of this book, cover photo of Shanghai Jing’an Temple at night , photo by Jimmy Song.\nThe publishing copyright of this book belongs to the blog post of Electronic Industry Press. Please do not print and distribute it without authorization.\nIstio is a service mesh framework jointly developed by Google, IBM, Lyft, etc., and began to enter the public vision in early 2017. As an important infrastructure layer that inherits Kubernetes and connects to the serverless architecture in the cloud-native era, Istio is of crucial importance important. The ServiceMesher community as one of the earliest open source communities in China that is researching and promoting Service Mesh technology, decided to integrate community resources and co-author an open source e-book for readers.\nAbout this book This book originates from the rootsongjc / istio-handbook and the Istio knowledge map created by the ServiceMesher community .\nThis book is based on Istio 1.0+ and includes, but is not limited to , topics in the Istio Knowledge Graph .\nParticipate in the book Please refer to the writing guidelines of this book and join the Slack channel discussion after joining the ServiceMesher community.\n","relpermalink":"/en/notice/istio-handbook-by-servicemesher/","summary":"To further popularize Istio and Service Mesh technology, donate this book to the community for co-authoring.","title":"Donate Istio Handbook to the ServiceMesher community"},{"content":"Github: https://github.com/rootsongjc/cloud-native-sandbox Cloud Native Sandbox can help you setup a standalone Kubernetes and istio environment with Docker on you own laptop.\nThe sandbox integrated with the following components:\nKubernetes v1.10.3 Istio v1.0.4 Kubernetes dashboard v1.8.3 Differences with kubernetes-vagrant-centos-cluster As I have created the kubernetes-vagrant-centos-cluster to set up a Kubernetes cluster and istio service mesh with vagrantfile which consists of 1 master(also as node) and 3 nodes, but there is a big problem that it is so high weight and consume resources. So I made this light weight sandbox.\nFeatures\nNo VirtualBox or Vagrantfile required Light weight High speed, low drag Easy to operate Services\nAs the sandbox setup, you will get the following services.\nRecord with termtosvg .\nPrerequisite You only need a laptop with Docker Desktop installed and Kubernetes enabled .\nNote: Leave enough resources for Docker Desktop. At least 2 CPU, 4G memory.\nInstall To start the sandbox, you have to run the following steps.\nKubernetes dashboard(Optional) Install Kubernetes dashboard.\nkubectl apply -f install/dashbaord/ Get the dashboard token.\nkubectl -n kube-system describe secret default| awk \u0026#39;$1==\u0026#34;token:\u0026#34;{print $2}\u0026#39; Expose kubernetes-dashboard service.\nkubectl -n kube-system get pod -l k8s-app=kubernetes-dashboard -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39; Login to Kubernetes dashboard on http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login with the above token.\nIstio(Required) Install istio service mesh with the default add-ons.\n# Install istio kubectl apply -f install/istio/ To expose service grafana on http://localhost:3000 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 3000:3000 \u0026amp; To expose service prometheus on http://localhost:9090 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=prometheus -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 9090:9090 \u0026amp; To expose service jaeger on http://localhost:16686 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=jaeger -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 16686:16686 \u0026amp; To expose service servicegraph on http://localhost:8088/dotviz , http://localhost:8088/force/forcegraph.html .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=servicegraph -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 8088:8088 \u0026amp; Kiali Install kiali .\nkubectl -n istio-system apply -f install/kiali To expose service kiali on http://localhost:20001 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=kiali -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 20001:20001 \u0026amp; Bookinfo sample Deploy bookinfo sample .\n# Enable sidecar auto injection kubectl label namespace default istio-injection=enabled # Deploy bookinfo sample kubectl -n default apply -f sample/bookinfo Visit productpage on http://localhost/productpage .\nLet’s generate some loads.\nfor ((i=0;i\u0026lt;1000;i=i+1));do echo \u0026#34;Step-\u0026gt;$i\u0026#34;;curl http://localhost/productpage;done You can watch the service status through http://localhost:3000 .\nClient tools To operate the applications on Kubernetes, you should install the following tools.\nRequired\nkubectl - Deploy and manage applications on Kubernetes. istioctl - Istio configuration command line utility. Optional\nkubectx - Switch faster between clusters and namespaces in kubectl kube-ps1 - Kubernetes prompt info for bash and zsh ","relpermalink":"/en/blog/cloud-native-sandbox/","summary":"A standalone Kubernetes and Istio environment with Docker on you own laptop","title":"Cloud Native Sandbox"},{"content":"This video was recorded on the last day of 2018 and demonstrates the use of rootsongjc/kubernetes-vagrant-centos-cluster to automatically deploy a Kubernetes cluster and Istio Service Mesh.\nA few days ago I mentioned that kubernetes-vagrant-centos-cluster released v1.2.0 version to deploy a cloud-native experimental environment with one click. Someone in the Kubernetes and Service Mesh community asked me a long time ago to make a video to explain and demonstrate how to install Kubernetes and Istio Service Mesh, because I’m always busy, I’ve always made some time. Today, I will give a demo video, just don’t watch the video for a few minutes. In order to make this video, it took me half an hour to record, two hours to edit, and many years of shooting. , Editing, containers, virtual machines, Kubernetes, service grid experience. This is not so much a farewell as a new beginning.\nBecause the video was first posted on YouTube , it was explained in English (just a few supplementary instructions, it does n’t matter if you do n’t understand, just read the Chinese documentation on GitHub ).\nSkip to bilibli to watch . If you are interested in drone aerial photography, you can also take a look at Jimmy Song’s aerial photography works . Please support the coin or like, thank you.\nIf you have any questions, you can send a barrage or comment below the video.\nPS. Some people will ask why you chose to use bilibli, because there are no ads for watching videos on this platform, and most of them are uploaded by the Up master. Although the two-dimensional elements are mostly, the community atmosphere is still good.\nFor more exciting videos, visit Jimmy Song’s bibli homepage .\n","relpermalink":"/en/notice/cloud-native-kubernetes-service-mesh-local-demo-show/","summary":"This video was recorded by me on the last day of 2018 and demonstrates the use of rootsongjc/kubernetes-vagrant-centos-cluster to automatically deploy Kubernetes clusters and Istio Service Mesh.","title":"Kubernetes and Istio Service Mesh Cloud Native Local Video Demo Show"},{"content":"This article uses Istio’s official bookinfo sample to explain how Envoy performs routing forwarding after the traffic entering the Pod and forwarded to Envoy sidecar by iptables, detailing the inbound and outbound processing. For a detailed analysis of traffic interception, see Understanding Envoy Sidecar Proxy Injection and Traffic Interception in Istio Service Mesh .\nOverview of Sidecar Injection and Traffic Interception Steps Below is an overview of the steps from Sidecar injection, Pod startup to Sidecar proxy interception traffic and Envoy processing routing.\nKubernetes automatically injected through Admission Controller, or the user run istioctl command to manually inject sidecar container. Apply the YAML configuration deployment application. At this time, the service creation configuration file received by the Kubernetes API server already includes the Init container and the sidecar proxy. Before the sidecar proxy container and application container are started, the Init container started firstly. The Init container is used to set iptables (the default traffic interception method in Istio, and can also use BPF, IPVS, etc.) to Intercept traffic entering the pod to Envoy sidecar Proxy. All TCP traffic (Envoy currently only supports TCP traffic) will be Intercepted by sidecar, and traffic from other protocols will be requested as originally. Launch the Envoy sidecar proxy and application container in the Pod. Sidecar proxy and application container startup order issues\nStart the sidecar proxy and the application container. Which container is started first? Normally, Envoy Sidecar and the application container are all started up before receiving traffic requests. But we can’t predict which container will start first, so does the container startup order have an impact on Envoy intercepting traffic? The answer is yes, but it is divided into the following two situations.\nCase 1: The application container starts first, and the sidecar proxy is still not ready\nIn this case, the traffic is transferred to the 15001 port by iptables, and the port is not monitored in the Pod. The TCP link cannot be established and the request fails.\nCase 2: Sidecar starts first, the request arrives and the application is still not ready\nIn this case, the request will certainly fail. As for the step at which the failure begins, the reader is left to think.\nQuestion : If adding a readiness and living probe for the sidecar proxy and application container can solve the problem?\nTCP requests that are sent or received from the Pod will be intercepted by iptables. After the inbound traffic is intercepted, it is processed by the Inbound Handler and then forwarded to the application container for processing. The outbound traffic is intercepted by iptables and then forwarded to the Outbound Handler for processing. Upstream and Endpoint. Sidecar proxy requests Pilot to use the xDS protocol to synchronize Envoy configurations, including LDS, EDS, CDS, etc., but to ensure the order of updates, Envoy will use ADS to request configuration updates from Pilot directly. How Envoy handles route forwarding The following figure shows a productpageservice access request http://reviews.default.svc.cluster.local:9080/, when traffic enters reviews the internal services, reviews internal services Envoy Sidecar is how to do traffic blocked the route forward.\nIstio transparent traffic intercepting and traffic routing schematic Before the first step, productpage Envoy Sidecar Pod has been selected by EDS of a request to reviews a Pod service of its IP address, it sends a TCP connection request.\nThe Envoy configuration in the official website of Istio is to describe the process of Envoy doing traffic forwarding. The party considering the traffic of the downstream is to receive the request sent by the downstream. You need to request additional services, such as reviews service requests need Pod ratings service.\nreviews, there are three versions of the service, there is one instance of each version, three versions sidecar similar working steps, only to later reviews-v1-cb8655c75-b97zc Sidecar flow Pod forwarding this step will be described.\nUnderstanding the Inbound Handler The role of the inbound handler is to transfer the traffic from the downstream intercepted by iptables to localhost to establish a connection with the application container inside the Pod.\nLook reviews-v1-cb8655c75-b97zc at the Listener in the pod.\nRun istioctl pc listener reviews-v1-cb8655c75-b97zc to see what the Pod has a Listener.\nADDRESS PORT TYPE 172.33.3.3 9080 HTTP \u0026lt;--- Receives all inbound traffic on 9080 from listener 0.0.0.0_15006 10.254.0.1 443 TCP \u0026lt;--+ 10.254.4.253 80 TCP | 10.254.4.253 8080 TCP | 10.254.109.182 443 TCP | 10.254.22.50 15011 TCP | 10.254.22.50 853 TCP | 10.254.79.114 443 TCP | 10.254.143.179 15011 TCP | 10.254.0.2 53 TCP | Receives outbound non-HTTP traffic for relevant IP:PORT pair from listener 0.0.0.0_15001 10.254.22.50 443 TCP | 10.254.16.64 42422 TCP | …","relpermalink":"/en/blog/understanding-how-envoy-sidecar-intercept-and-route-traffic-in-istio-service-mesh/","summary":"Details about Envoy sidecar with iptables rules.","title":"Understanding How Envoy Sidecar Intercept and Route Traffic in Istio Service Mesh"},{"content":"KubeCon \u0026amp; CloudNative in North America is the most worthy cloud-native event every year. This time it will be held in Seattle for four days, from December 10th to 13th, refer to the official website of the conference . This year, 8,000 people participated. You should notice that Kubernetes has become more and more low-level. Cloud-native application developers do not need to pay much attention to it. Major companies are publishing their own cloud-native technology stack layouts, including IBM, VMware, SAP, Startups around this ecosystem are still emerging. The PPT sharing address of the local conference: https://github.com/warmchang/KubeCon-North-America-2018 , thank you William Zhang for finishing and sharing the slides of this conference .\nSeattle scene Janet Kuo from Google describes the path to cloud-native technology adoption.\nThe same event of KubeCon \u0026amp; CloudNativeCon, the scene of the first EnvoyCon.\nAt KubeCon \u0026amp; CloudNativeCon Seattle, various directors, directors, directors, VPs, and Gartner analysts from IBM, Google, Mastercard, VMware, and Gartner are conducting live discussions on the topic of Scaling with Service Mesh and Istio. Why do we talk about Istio when we talk about Service Mesh? What is not suitable for Istio use case. . .\nThe PPT contains basic introduction, getting started, a total of more than 200 Deep Dive as well as practical application, we recommend you according to the General Assembly’s official website to choose topics of interest to look at the schedule, otherwise I might see, however.\nA little impression KubeCon \u0026amp; CloudNativeCon is held three times a year, Europe, China and North America. China is the first time this year, and it will be held in Shanghai in November. It is said that it will be held in June next year. Although everyone said that Kubernetes has become boring, the conference about Kubernetes There is still a lot of content, and the use of CRD to extend Kubernetes usage is increasing. Service Mesh has begun to become hot. As can be seen from the live pictures above, there are a large number of participants on the site and related topics are also increasing. It is known as a microservice in the post-Kubernetes era . This must be It will be an important development direction of cloud native after Kubernetes, and the ServiceMesher community pays close attention to it.\n","relpermalink":"/en/notice/kubecon-cloudnativecon-seattle-2018/","summary":"KubeCon \u0026 CloudNativeCon Seattle 2018 Data Sharing.","title":"Kubecon\u0026CloudNativeCon Seattle 2018"},{"content":"This is a postscript from the post- Kubernetes era. Just this evening I saw a post by Bilgin Ibryam Microservices in a Post-Kuberentes Era .\nOn April 9, 2017, the Kubernetes Handbook-Kubernetes Chinese Guide / Cloud Native Application Architecture Practice Manual was first submitted. In the past 16 months, 53 contributors participated, 1,088 commits, and a total of 23,9014 Chinese characters were written. At the same time , thousands of enthusiasts have gathered in the Kubernetes \u0026amp; Cloud Native combat group .\nIt has been more than 4 months since the previous version was released. During this period, Kubernetes and Prometheus graduated from CNCF respectively and have matured commercially. These two projects have basically taken shape and will not change much in the future. Kubernetes was originally developed for container orchestration. In order to solve the problem of microservice deployment, Kubernetes has gained popularity. The current microservices have gradually entered the post-Kubernetes era . Service Mesh and cloud native redefine microservices and distributed applications.\nWhen this version was released, the PDF size was 108M with a total of 239,014 Chinese characters. It is recommended to browse online , or clone the project and install the Gitbook command to compile it yourself.\nThis version has the following improvements:\nAdded Istio Service Mesh tutorial Increased use VirtualBox and Vagrant set up in a local cluster and distributed Kubernetes Istio Service Mesh Added cloud native programming language Ballerina and Pulumi introduced Added Quick Start Guide Added support for Kubernetes 1.11 Added enterprise-level service mesh adoption path guide Added SOFAMesh chapter Added vision for the cloud-native future Added CNCF charter and participation Added notes for Docker image repositories Added Envoy chapter Increased KCSP (Kubernetes certification service providers) and CKA (Certified Kubernetes administrator) instructions Updated some configuration files, YAML and reference links Updated CRI chapter Removed obsolete description Improved etcdctl command usage tutorial Fixed some typos Browse and download Browse online To make it easy for everyone to download, I put a copy on Weiyun , which is available in PDF (108MB), MOBI (42MB), and EPUB (53MB). In this book, there are more practical tutorials. In order to better understand the principles of Kubernetes, I recommend studying In-depth analysis of Kubernetes by Zhang Lei, produced by GeekTime.\nThank you Kubernetes for your support of this book. Thank you Contributors . In the months before this version was released, the ServiceMesher community was co-founded . As a force in the post-Kubernetes era , welcome to contact me to join the community and create cloud native New era .\nAt present , the WeChat group of the ServiceMesher community also has thousands of members. The Kubernete Handbook will continue, but Service Mesh is already a rising star. With Kubernetes in mind, welcome to join the ServiceMesher community and follow us. The public account of the community (also the one I manage).\n","relpermalink":"/en/notice/new-kubernetes-handbook-released-and-say-hello-to-post-kubernetes-era/","summary":"This is an obituary post-Kubernetes era. Kubernetes handbook by Jimmy Song v1.4 is released. The next focus of cloud native is Service Mesh!","title":"Kubernetes Handbook v1.4 is released"},{"content":"Today I am honored to announce that I have become a CNCF Ambassador . Here is my story with Cloud Native.\nOrigin The first time to attend the Cloud Native Computing Foundation is at the LC3 in Beijing 2017. I attended the meeting again this year, and in November of this year, CNCF will hold the KubeCon \u0026amp; CloudNativeCon for the first time in Shanghai, China. I’ll be there too.\nCloud Native Books My origins with the Cloud Native is originated from Kevin Hoffman’s book Cloud Native Go . I translated this book at the end of 2016. Since then, in China, the translation of the word Cloud Native has not been determined, we introduced it with 云原生 to China.\nAnd then I begin to write the kubernetes-handbook on GitHub. So far, it has more than 2000 stars. This book has written more than 200,000 Chinese characters, the first commit happened on April 14, 2017.\nSince the the book Cloud Native Go completed, the publisher recommended another Cloud Native book to me - Cloud Native Python by Manish Sethi.\nAnd the book Cloud Native Java by Josh Long and Kenny Bastani.\nIn March 2018, with the hope that Bring the world equal opportunities and Building a Financial Cloud Native Infrastructure, I joined the Ant Group .\nServiceMesher Community By the time of May 2018, I start to organize the ServiceMesher community.\nIn the last few months, we work with other open source communities in China, such as k8smeetup , Sharding-Sphere , Apache SkyWalking . Our community has grown to have 1,700 members and two round meetups in Hangzhou and Beijing till now.\nMore than 300 people participated in the scene and more than 20,000 people watched it live by IT 大咖说 。\nFuture Here are some hopes of mine:\nOpen source culture become popular in China More and more people would like to be involved in open source projects Host one open source project into the CNCF A book related to Cloud Native or Service Mesh Strengthen cultural exchanges between China and the global Finally, welcome to China for traveling or share your topic with us on Cloud Native, and in the mean while we will share our experience on large scale web apps to the world. Hope to hear your voice!\n","relpermalink":"/en/blog/cloud-native-and-me-the-past-current-and-future/","summary":"Today I am honored to announce that I have become a CNCF Ambassador.","title":"Cloud Native With Me - The Past, Current and Future"},{"content":"Today, we are pleased to announce Istio 1.0 . It’s been over a year since the original 0.1 release. Since 0.1, Istio has grown rapidly with the help of a thriving community, contributors, and users. Many companies have successfully applied Istio to production today and have gained real value through the insight and control provided by Istio. We help large businesses and fast-growing startups such as eBay , Auto Trader UK , Descartes Labs , HP FitStation , Namely , PubNub and Trulia to connect, manage and protect their services from scratch with Istio. The release of this version as 1.0 recognizes that we have built a core set of features that users can rely on for their production.\nEcosystem Last year we saw a significant increase in the Istio ecosystem. Envoy continues its impressive growth and adds many features that are critical to a production-level service grid. Observability providers like Datadog , SolarWinds , Sysdig , Google Stackdriver, and Amazon CloudWatch have also written plugins to integrate Istio with their products. Tigera , Aporeto , Cilium and Styra have built extensions for our strategy implementation and network capabilities. Kiali built by Red Hat provides a good user experience for grid management and observability. Cloud Foundry is building the next-generation traffic routing stack for Istio, the recently announced Knative serverless project is doing the same, and Apigee has announced plans to use it in their API management solution. These are just a few of the projects that the community added last year.\nFeatures Since the 0.8 release, we have added some important new features, and more importantly, marked many existing features as Beta to indicate that they can be used in production. This is covered in more detail in the release notes , but it is worth mentioning:\nMultiple Kubernetes clusters can now be added to a single grid , enabling cross-cluster communication and consistent policy enforcement. Multi-cluster support is now Beta. The network API for fine-grained control of traffic through the grid is now Beta. Explicitly modeling ingress and egress issues with gateways allows operations personnel to control the network topology and meet access security requirements at the edge. Two-way TLS can now be launched incrementally without updating all clients of the service. This is a key feature that removes the barriers to deploying Istio on existing production. Mixer now supports developing out-of-process adapters . This will be the default way to extend Mixer in an upcoming release, which will make it easier to build adapters. Envoy now fully evaluates the authorization policies that control service access locally , improving their performance and reliability. Helm chart installation is now the recommended installation method, with a wealth of customization options to configure Istio to your needs. We put a lot of effort into performance, including continuous regression testing, large-scale environmental simulation, and target repair. We are very happy with the results and will share details in the coming weeks. Next step Although this is an important milestone for the project, much work remains to be done. When working with adopters, we’ve received a lot of important feedback about what to focus next. We’ve heard consistent topics about supporting hybrid clouds, installing modularity, richer network capabilities, and scalability for large-scale deployments. We have considered some feedback in the 1.0 release and we will continue to actively work on it in the coming months.\nQuick start If you are new to Istio and want to use it for deployment, we would love to hear from you. Check out our documentation , visit our chat forum or visit the mailing list . If you want to contribute more to the project, please join our community meeting and say hello.\nAt last The Istio team is grateful to everyone who contributed to the project. Without your help, it won’t have what it is today. Last year’s achievements were amazing, and we look forward to achieving even greater achievements with our community members in the future.\nThe ServiceMesher community is responsible for the translation and maintenance of Chinese content on Istio’s official website. At present, the Chinese content is not yet synchronized with the English content. You need to manually enter the URL to switch to Chinese ( https://istio.io/zh ). There is still a lot of work to do , Welcome everyone to join and participate.\n","relpermalink":"/en/notice/istio-v1-released/","summary":"Chinese documentation is released at the same time!","title":"Istio 1.0 is released"},{"content":"“Cloud Native Java,” authored by Josh Long, translated by Zhang Ruofei and Song Jingchao, published by Publishing House of Electronics Industry in July 2017.\nCover of the book Cloud Native Java Jimmy Song with Josh Long Photo taken on November 3, 2018, in Beijing\nBook Introduction What distinguishes traditional enterprises from companies like Amazon, Netflix, and Etsy? These companies have perfected cloud-native development methods that enable them to stay ahead and lead the competition. This practical guide shows Java/JVM developers how to build software faster and better using Spring Boot, Spring Cloud, and Cloud Foundry.\nMany organizations have already ventured into cloud computing, test-driven development, microservices, and continuous integration and delivery. Authors Josh Long and Kenny Bastani will take you deep into these tools and practices, helping you transform traditional applications into true cloud-native ones.\nThe book consists of four major parts:\nFoundation: Understand the motivation behind cloud-native thinking; configure and test Spring Boot applications; migrate your traditional applications to the cloud Microservices: Build HTTP and RESTful services using Spring; route requests in distributed systems; establish edge services closer to data Data Integration: Manage data using Spring Data and integrate distributed services with Spring-supported event-driven, message-centric architectures Production: Make your system observable; use service proxies to connect stateful services; understand the important ideas behind continuous delivery If you’re building cloud-native applications, this book will serve as your essential guide to using the Java ecosystem. It covers everything—building resilient services, managing data flows (via REST and asynchronous events), testing, deployment, and observability.\n—Daniel Bryant, Software Developer and CTO at SpectoLabs\nWhether you’re just starting out on your cloud-native journey or nearing your cloud-native goals, everyone involved will benefit from the insights and experiences shared in this Cloud Native Java book.\n—Dr. Dave Syer, Contributor to the Spring Framework, and Co-founder of Spring Boot and Spring Cloud\nAbout the Authors Josh Long is a Spring Developer Advocate, an editor at InfoQ.com’s Java queue, and the primary author of several books, including the second edition of Spring Recipes (published by Apress). Josh has spoken at many international industry conferences, including TheServiceSide Java Symposium, SpringOne, OSCON, JavaZone, and Devoxx, among others. When he’s not writing code for SpringSource, he’s either at a Java User Group meeting or in a coffee shop. Josh loves solutions that push technology forward. His interests include scalability, BPM, grid computing, mobile computing, and so-called “smart” systems. You can browse his blogs at http://blog.springsource.org or http://joshlong.com .\nKenny Bastani is a Spring Developer Advocate at Pivotal. As an open-source contributor and blogger, Kenny focuses on graph databases, microservices, and attracting a passionate group of software developers. Kenny is also a regular attendee at industry conferences such as OSCON, SpringOne Platform, and GOTO. He maintains a personal blog about software architecture and provides tutorials and open-source reference examples for building event-driven microservices and serverless architectures.\nTable of Contents Foreword (James Watters) xvii\nForeword (Rod Johnson) xix\nPreface xxi\nPart I Foundations\nChapter 1 Cloud-Native Applications 3 Chapter 2 Bootcamp: Spring Boot and Cloud Foundry 21 Chapter 3 Configuration, as Twelve-Factor Style 67 Chapter 4 Testing 85 Chapter 5 Migrating Legacy Applications 115 Part II Web Services\nChapter 6 REST API 137 Chapter 7 Routing 179 Chapter 8 Edge Services 197 Part III Data Integration\nChapter 9 Data Management 251 Chapter 10 Messaging Systems 303 Chapter 11 Batch Processing and Tasks 325 Chapter 12 Data Integration 363 Part IV Production\nChapter 13 Observable Systems 411 Chapter 14 Service Proxies 469 Chapter 15 Continuous Delivery 497 Part V Appendix\nAppendix A Using Spring Boot in Java EE 527\n","relpermalink":"/en/book/cloud-native-java/","summary":"Authored by Josh Long.","title":"Cloud Native Java"},{"content":"“Cloud Native Python,” authored by Marish Sethi, translated by Song Jingchao, published by Publishing House of Electronics Industry in July 2018.\nCover of Cloud Native Python Introduction to Cloud Native Python With today’s rapid business development, relying solely on their own infrastructure is far from enough for enterprises to support their rapid expansion. Therefore, they have been pursuing the elasticity of the cloud to build platforms that support highly scalable applications.\nThis book can help you understand all the information about building cloud-native architectures with Python in one go. In this book, we first introduce you to cloud-native architectures and how they can help you solve various problems. Then you will learn how to build microservices using REST APIs and Python, construct the web layer in an event-driven manner. Next, you will learn how to interact with data services and build web views using React. After that, we will delve into application security and performance. Then, you will also learn how to containerize your services with Docker. Finally, you will learn how to deploy your applications on AWS and Azure platforms. After deploying your application, we will conclude this book with a series of concepts and techniques around application troubleshooting.\nWhat’s Covered in This Book Chapter 1: Introduction to cloud-native architectures and microservices, discussing the basic concepts of cloud-native architecture and setting up the application development environment. Chapter 2: Building microservices with Python, building your own microservices knowledge base and extending it based on your use cases. Chapter 3: Building web applications with Python, building an initial web application and integrating it with microservices. Chapter 4: Interacting with data services, teaching you how to migrate applications to different database services. Chapter 5: Building web views using React. Chapter 6: Creating scalable UI with Flux, helping you understand how to create scalable applications using Flux. Chapter 7: Event sourcing and CQRS, discussing how to store transactions in event form. Chapter 8: Securing web applications, keeping your applications safe from external threats. Chapter 9: Continuous delivery, knowledge related to frequent releases of applications. Chapter 10: Dockerizing your services, discussing container services and running applications in Docker. Chapter 11: Deploying applications to AWS platform, teaching you how to build infrastructure on AWS and establish the production environment of applications. Chapter 12: Deploying applications to Azure platform, discussing how to build infrastructure on Azure and establish the production environment of applications. Chapter 13: Monitoring cloud applications, understanding different monitoring tools for infrastructure and applications. Tools and Environment Required to Use This Book You need to have Python installed on your system. A text editor, preferably Vim, Sublime, or Notepad++. In one chapter, you will need to download POSTMAN, a powerful API testing suite that can be installed as a Chrome extension. You can download it here .\nAdditionally, it would be better if you have accounts on the following websites:\nJenkins Docker Amazon Web Services Terraform Target Audience This book is suitable for developers with basic Python knowledge, familiarity with the command line, and the basic principles of HTTP-based applications. It’s an ideal choice for those who want to understand how to build, test, and scale Python-based applications. No prior experience with building microservices using Python is required.\n","relpermalink":"/en/book/cloud-native-python/","summary":"Authored by Marish Sethi.","title":"Cloud Native Python"},{"content":" If there is a visual learning model and platform that provides infrastructure clusters for your operation, would you pay for it?\nTwo months ago, I met Jord in Kubernetes’s Slack channel, and later saw the link of MagicSandbox.io he (and possibly others) sent in the Facebook group of Taiwan Kubernetes User Group, and I clicked to apply for a trial Then, I received an email from Jord later, and he told me that he wanted to build a Kubernetes learning platform. That’s where the whole thing started, and then we had a couple of Zoom video chats for a long time.\nAbout MagicSandbox MagicSandbox is a startup company. Jord (Dutch) is also a serial entrepreneur. He has studied at Sichuan University in China for 4 years since he was 19, and then returned to Germany. He worked as a PM at Boston Consulting Group and now works at Entrepreneur. First (Europe’s top venture capital / enterprise incubator) is also located in Berlin, Germany. He met Mislav (Croatian). Mislav is a full-stack engineer and has several entrepreneurial experiences. They have similar odors, and they hit it off. Decided to be committed to the Internet education industry and create a world-class software engineer education platform. They want to start with Kubernetes, first provide Internet-based Kubernetes theory and practice teaching, and then expand the topic to ElasticSearch, GraphQL, and so on. topic.\nJord founded MagicSandbox in his home, and I became the face of MagicSandbox in China.\nNow we are going to release the MagicSandbox Alpha version. This version is an immature version and is provided for everyone to try for free. Positive feedback is also welcome.\nOfficial homepage: https://magicsandbox.com/ Chinese page: https://cn.magicsandbox.com/ (The content has not been finished yet, only the Chinese version homepage is currently provided) Follow us on Twitter: https://twitter.com/magicsandbox ","relpermalink":"/en/notice/magicsandbox-alpha-version-annoucement/","summary":"Online practical software engineering education platform.","title":"MagicSandbox Alpha released"},{"content":"Remember the cloud-native programming language I shared before finally appeared! Learn about Ballerina in one article! ? They are ready to attend the KubeCon \u0026amp; CloudNativeCon China Conference!\nKubeCon \u0026amp; CloudNativeCon China Conference will be held on November 14-15, 2018 (Wednesday, Thursday) in Shanghai . See: https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2018/ With Ballerina’s official authorization, I now need to help them find an “ambassador” in China, responsible for team guidance, Chinese and English translation, and familiarity with Cloud Native and microservices. It has influence in the industry and has no barriers to English communication.\nAmbassador duties\nTeam Leadership Responsible for Chinese and English translation of product declaration, PPT materials, etc. Help to arrange the booth The other party can provide\nConference tickets Travel expenses Accommodation during the conference Other compensation This is a photo of their team in front of their booth during the KubeCon \u0026amp; CloudNativeCon in Hagen in May this year.\nPS This is the most complete and best picture I have ever found of their team. (Photography needs to be strengthened)\nLet’s briefly introduce this startup called Ballerina. Their team is mainly from Sri Lanka. This is an island country next to India in the South Asian subcontinent. In ancient China, it was called “Lion Country” and rich in gemstones.\nThe capital of their country is Sri Lanka, which is pronounced in their own language: Si li jia ya wa de na pu la ke te\nIf you are interested, please contact me directly.\n","relpermalink":"/en/notice/a-ballerina-china-ambassador-required/","summary":"With official authorization from Ballerina, I now need to help them find an ambassador in China.","title":"Ballerina seeks Chinese ambassador"},{"content":"Envoy-Designed for cloud-native applications, open source edge and service proxy, Istio Service Mesh default data plane, Chinese version of the latest official document, dedicated by the ServiceMesher community, welcome everyone to learn and share together.\nPDF download address: servicemesher/envoy This is the first time Community Service Mesh enthusiasts group activities, the document is based on Envoy latest (version 1.7) Official documents https://www.envoyproxy.io/docs/envoy/latest/ . A total of 120 articles with 26 participants took 13 days and 65,148 Chinese characters.\nVisit online address: https://cloudnative.to/envoy/ Note : This book does not include the v1 API reference and v2 API reference sections in the official documentation. Any links to API references in the book will jump directly to the official page.\nContributor See the contributor page: https://github.com/servicemesher/envoy/graphs/contributors Thanks to the above contributors for their efforts! Because the level of translators is limited, there are inevitably inadequacies in the text. I also ask readers to correct them. Also welcome more friends to join our GitHub organization: https://github.com/servicemesher ","relpermalink":"/en/notice/envoyproxy-docs-cn-17-release/","summary":"Translated by ServiceMesher community.","title":"Chinese version of the latest official document of Envoy released"},{"content":"Envoy is an open source L7 proxy and communication bus written in C ++ by Lyft. It is currently an open source project under CNCF . The code is hosted on GitHub. It is also the default data plane in the Istio service mesh. We found that it has very good performance, and there are also continuous open source projects based on Envoy, such as Ambassador , Gloo, etc. At present, the official documentation of Envoy has not been well finished, so we Service Service enthusiasts feel that they are launching the community The power of the co-translation of the latest (version 1.7) official documentation of Enovy and organization through GitHub.\nService Mesh enthusiasts have jointly translated the latest version of the official document of Envoy . The translated code is hosted at https://github.com/servicemesher/envoy . If you are also a Service Mesh enthusiast, you can join the SerivceMesher GitHub organization and participate together.\nThe official Envoy document excludes all articles in the two directories of the v1 API reference and the v2 API reference. There are more than 120 documents. The length of the documents varies. The original English official documents use the RST format. I manually converted them into Markdown format and compiled using Gitbook. A GitHub Issue was generated according to the path of the document relative to the home directory. Friends who want to participate in translation can contact me to join the ServiceMesher organization, and then select the article you want to translate in Issue , and then reply “Claim”.\nHere you can see all the contributors. In the future, we will also create a Service Mesh enthusiast website. The website uses static pages. All code will be hosted on Github. Welcome everyone to participate.\n","relpermalink":"/en/notice/enovy-doc-translation-start/","summary":"The SerivceMesher community is involved in translating the official documentation for the latest version of Envoy.","title":"Envoy's latest official document translation work started"},{"content":"TL; DR Click here to download the PDF of this book .\nRecently, Michael Hausenblas of Nginx released a booklet on container networks in docker and kubernetes. This 72-page material is a good introduction for everyone to understand the network in Docker and Kubernetes from shallow to deep.\nTarget audience Container Software Developer SRE Network Operation and Maintenance Engineer Architects who want to containerize traditional software ","relpermalink":"/en/notice/container-networking-from-docker-to-kubernetes-nginx/","summary":"Source from Nginx, published by O’Reilly.","title":"Docker container network book sharing"},{"content":"In 2017, we are facing a big era of architectural changes, such as Kubernetes ending the battle for container orchestration, Kafka release 1.0, serverless gradually gaining momentum, edge computing to replace cloud computing, Service Mesh ready to go, and artificial intelligence to empower business , Also brings new challenges to the architecture.\nI am about to participate in InfoQ’s ArchSummit Global Architects Summit on December 8-11 in Beijing . This conference also invited 100+ top technologists such as Dr. Ali Wangjian to share and summarize the architectural changes and reflections this year. I hope that you can build on this conference, summarize past practices, and look forward to a future-oriented architecture to transform this era of change into the common fortune of each of us.\nMy speech The content of my speech is from Kubernetes to Cloud Native-the road to cloud native applications . Link: From Kubernetes to Cloud Native-the road to cloud native applications . The time is Saturday, December 9, 9:30 am, in the fifth meeting room.\nAfter more than ten years of development of cloud computing, the new phase of cloud native has entered. Enterprise applications are preferentially deployed in cloud environments. How to adapt to the cloud native tide, use containers and Kubernetes to build cloud native platforms, and practice DevOps concepts and agility How IT, open source software, and the community can help IT transform, the solution to all these problems is the PaaS platform, which is self-evident to the enterprise.\nWe also prepared gifts for everyone: “Cloud Native Go-Building Cloud Native Web Applications Based on Go and React” and “Intelligent Data Era-Enterprise Big Data Strategy and Practice” There is also a book stand for the blog post of the Electronic Industry Press. Welcome to visit.\nArchSummit conference official website link: http://bj2017.archsummit.com/ For more details, please refer to the official website of the conference: http://bj2017.archsummit.com/ ","relpermalink":"/en/notice/archsummit-beijing-2017-from-kubernetes-to-cloud-native/","summary":"I will give a lecture at ArchSummit Beijing. From Kubernetes to Cloud Native, my path to cloud native applications.","title":"ArchSummit Beijing 2017 speech preview"},{"content":"Cloudinary-go is a Go client library and CLI tool to upload static assets to the Cloudinary service.\nInstallation Install the CLI tool and the library with:\ngo get github.com/rootsongjc/cloudinary-go/cloudinary Or download the release binary from release .\n","relpermalink":"/en/notice/cloudinary-go/","summary":"Cloudinary-go is a Go client library and CLI tool to upload static assets to Cloudinary service","title":"Cloudinary file upload tool written in Go released"},{"content":"Kevin Hoffman(From Capital One, twitter @KevinHoffman ) was making a speech on TalkingData T11 Smart Data Summit.\nHe addressed that 15 Factors of Cloud Native which based on Heroku’s original Twelve-Factor App , but he add more 3 another factors on it.\nLet’s have a look at the 15 factors of Cloud Native.\n1. One codebase, one App Single version-controlled codebase, many deploys Multiple apps should not share code Microservices need separate release schedules Upgrade, deploy one without impacting others Tie build and deploy pipelines to single codebase 2. API first Service ecosystem requires a contract Public API Multiple teams on different schedulers Code to contract/API, not code dependencies Use well-documented contract standards Protobuf IDL, Swagger, Apiary, etc API First != REST first RPC can be more appropriate in some situations 3. Dependency Management Explicitly declare dependencies Include all dependencies with app release Create immutable build artifact (e.g. docker image) Rely on smallest docker image Base on scratch if possible App cannot rely on host for system tools or libraries 4. Design, Build, Release, Run Design part of iterative cycle Agile doesn’t mean random or undesigned Mature CI/CD pipeline and teams Design to production in days not months Build immutable artifacts Release automatically deploys to environment Environments contains config, not release artifact 5. Configuration, Credentials, Code “3 Cs” volatile substances that explode when combinded Password in a config file is as bad as password in code App must accept “3 Cs” from environment and only use harmless defaults Test - Could you expose code on Github and not reveal passwords, URLs, credentials? 6. Logs Emit formatted logs to stdout Code should not know about destination or purpose of log emissions Use downstream log aggregator collect, store, process, expose logs ELK, Splunk, Sumo, etc Use structured logs to allow query and analysis JSON, csv, KV, etc Logs are not metrics 7. Disposability App must start as quickly as possible App must stop quickly and gracefully Processes start and stop all the time in the cloud Every scale up/down disposes of processes Slow dispose == slow scale Slow dispose or startup can cause availability gaps 8. Backing Services Assume all resources supplied by backingservices Cannotassume mutable file system “Disk as a Service” (e.g. S3, virtual mounts, etc) Every backing service is bound resource URL, credentials, etc-\u0026gt; environment config Host does not satisfy NFRs Backing services and cloud infrastructure 9. Environment Parity “Works on my machine” Cloud-native anti-pattern. Must work everywhere Every commit is candidate for deployment Automated acceptance tests Provide no confidence if environments don’t match 10. Administrative Processes Database migrations Run-once scripts or jobs Avoid using for batch operations, consider instead: Event sourcing Schedulers Triggers from queues, etc Lambdas/functions 11. Port Binding In cloud, infrastructure determines port App must accept port assigned by platform Containers have internal/external ports App design must embrace this Never use reserved ports Beware of container “host mode” networking 12. Stateless Processes What is stateless? Long-term state handled by a backing service In-memory state lives onlyas long as request Requests from same client routed to different instances “Sticky sessions” cloud native anti-pattern 13. Concurency Scale horizontally using the process model Build disposable, stateless, share-nothing processes Avoid adding CPU/RAM to increase scale/throughput Where possible, let platform/libraries do threading Many single-threaded services \u0026gt; 1 multi-threaded monolith 14. Telemetry Monitor apps in the cloud like satellite in orbit No tether, no live debugger Application Perf Monitoring (APM) Domain Telemetry Health and system logs 15. Authentication \u0026amp; Authorization Security should never be an afterthought Auth should be explicit, documented decision Even if anonymous access is allowed Don’t allow anonymous access Bearer tokens/OAuth/OIDC best practices Audit all attempts to access Migrating Monoliths to the Cloud After this 15 factors, he also gave us some tips about how to migrate monoliths to the Cloud:\nMake a rule - stop adding to the monolith All new code must be cloud native Prioritize features Where will you get most benefit from cloud native? Come up with a plan Decompose monolith over time Fast, agile iterations toward ultimate goal Use multiple strategies and patterns Go - the Best Language for Building Cloud Native App At last, he advise us the programming language Go is the best language to build Cloud Native applications for these reasons below:\nLightweight Easily learning curve Compiles to native binaries Very fast Large, thriving, engaged community http://gopherize.me Kevin also wrote a book Cloud Native Go to show how to Building Web Applications and Microservices for the Cloud with Go and React. This book has been …","relpermalink":"/en/blog/high-level-cloud-native-from-kevin-hoffman/","summary":"Kevin Hoffman address that 15 Factors of Cloud Native.","title":"High Level Cloud Native From Kevin Hoffman"},{"content":"From now on I have my own independent domain name jimmysong.io , the website is still hosted on GitHub, the original URL jimmysong.io still accessible.\nWhy use .io as the suffix? Because this is The First Step to Cloud Native!\nWhy choose today? Because today is August 18, the days are easy to remember.\nPS domain names are registered in namecheap and cost tens of dollars / year.\nProudly powered by hugo 🎉🎊🎉\n","relpermalink":"/en/notice/domain-name-jimmysong-io/","summary":"From now on I have my own independent domain name jimmysong.io.","title":"New domain name jimmysong.io"},{"content":"“Cloud Native Go,” authored by Kevin Hoffman, translated by Song Jingchao, Wu Yingsong, Xu Bei, and Ma Chao, published by Publishing House of Electronics Industry in August 2017.\nCover of the book Cloud Native Go Author: Kevin Hoffman \u0026amp; Dan Nemeth Translators: Song Jingchao, Wu Yingsong, Xu Bei, Ma Chao Publisher: Publishing House of Electronics Industry Full Name: Cloud Native Go - A Guide to Building Web Cloud-Native Applications with Go and React This book has been published by the Publishing House of Electronics Industry and is available for purchase on JD.com .\nJimmy Song with Kevin Hoffman Photo taken on September 12, 2017, in Beijing\nIntroduction Cloud Native Go shows developers how to build large-scale cloud applications that can dynamically scale to handle almost any volume of data, traffic, or users while meeting the strong demands of today’s customers.\nKevin Hoffman and Dan Nemeth detail modern cloud-native applications, elucidating the factors, rules, and habits associated with rapid and reliable cloud-native development. They also introduce Go, a “simple and elegant” high-performance language particularly suited for cloud development.\nIn this book, you’ll create microservices using Go, add frontend web components using ReactJS and Flux, and master advanced cloud-native technologies based on Go. Hoffman and Nemeth demonstrate how to build continuous delivery pipelines using tools like Wercker, Docker, and Dockerhub; automatically push applications to platforms; and systematically monitor application performance in production.\nLearn the “way of the cloud”: why well-developed cloud software is essentially about mindset and rules Understand why using the Go language is the ideal choice for cloud-native microservices development Plan cloud applications that support continuous delivery and deployment Design service ecosystems and then build them test-first Push work in progress to the cloud Use event sourcing and CQRS patterns to respond to large-scale and high-throughput Secure cloud-based web applications: choices to make and avoid Create responsive cloud applications using third-party messaging providers Build large-scale, cloud-friendly GUIs using React and Flux Monitor dynamic scaling, failover, and fault tolerance in the cloud About the Authors Kevin Hoffman has helped businesses move their applications to the cloud in a modern and multi-language manner. He began programming at the age of 10, self-studying BASIC on a reassembled Commodore VIC-20. Since then, he has been passionate about building software and has spent a lot of time learning languages, frameworks, and patterns. He has built a range of software, from remote-controlled drones and biometric security systems to ultra-low-latency financial applications and mobile apps. He fell in love with the Go language while building custom components that work with Pivotal Cloud Foundry.\nKevin is the author of the popular fantasy book series (The Sigilord Chronicles ) and eagerly anticipates combining his love of building software with his love of building fantasy worlds.\nDan Nemeth is currently a Consulting Solution Architect at Pivotal, supporting Pivotal Cloud Foundry. He has been developing software since the days of the Commodore 64 and began coding professionally in 1995, writing CGI scripts in ANSIC for a local ISP. Since then, most of his career has been spent as an independent consultant providing solutions to industries ranging from finance to pharmaceuticals, using various languages and frameworks of the time. Dan recently adopted Go as his home and enthusiastically applies it to all his projects.\nWhen Dan isn’t in front of a computer, he can often be found sailing or fly-fishing near Annapolis.\nTable of Contents Chapter 1: The Way of the Cloud Chapter 2: Getting Started Chapter 3: Getting to Know Go Chapter 4: Continuous Delivery Chapter 5: Building Microservices in Go Chapter 6: Leveraging Backend Services Chapter 7: Building Data Services Chapter 8: Event Sourcing and CQRS Chapter 9: Building Web Applications with Go Chapter 10: Cloud Security Chapter 11: Using WebSockets Chapter 12: Building Web Views with React Chapter 13: Building Scalable UIs with Flux Chapter 14: Creating a Full Application: World of FluxCraft Chapter 15: Conclusion Appendix A: Troubleshooting Cloud Applications Index ","relpermalink":"/en/book/cloud-native-go/","summary":"Authored by Kevin Hoffman.","title":"Cloud Native Go"},{"content":"This is a list of software, tools, architecture and reference materials about Cloud Native. It is awesome-cloud-native , a project I opened on GitHub , and can also be browsed through the web page .\nThis list will be continuously updated and improved in the future, not only for my usual research records, but also as a reference for the Cloud Native industry.\n","relpermalink":"/en/notice/awesome-cloud-native/","summary":"This is a list of software, tools, architecture, and reference materials about Cloud Native. It is a project I started on GitHub.","title":"Awesome Cloud Native list is released"}]