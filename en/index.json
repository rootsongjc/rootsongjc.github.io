[{"content":"Three years ago, I traveled around northern Xinjiang with Addo in a camper van, an unforgettable journey especially for the Ili Valley. This Dragon Boat Festival, I decided to return to Ili for a road trip.\nAfter meticulous planning, I set off from Beijing, transferred at Urumqi, and arrived in the beautiful Yining. Here, we rented a car and embarked on a one-week, 1344-kilometer road trip.\nAerial Video In this video, you will see stunning aerial footage from my journey. From vast grasslands to clear lakes, from towering mountains to winding roads, these beautiful landscapes are perfectly captured in the video.\nKazanqi in Yining Our first stop was Kazanqi Folk Village in Yining, a town full of ethnic charm. The streets are lined with unique traditional houses, colorful walls, and blue doors, like a fairy tale world. The village exudes a strong sense of life, with locals going about their daily activities.\nFigure: Kazanqi in Yining Ili River From Kazanqi, we headed to the Ili River. The river’s clear water and lush greenery on both banks are mesmerizing. We watched a breathtaking sunset at the Ili Wetland Park, immersed in nature’s beauty.\nFigure: Ili River Kalajun Our third stop was Kalajun Grassland. The vast grasslands, with herds of cattle and sheep, and the distant snow-capped mountains under the blue sky, create a stunning natural painting. At an average altitude of over 2000 meters, the air is exceptionally fresh. Camping here, I experienced the tranquility of nature after a night of rain, with the golden sunrise illuminating the mountains.\nFigure: Kalajun Grassland Qiongkushitai Next, we arrived at Qiongkushitai, an original Kazakh village. Despite some commercialization, it retains its primitive charm. Newly opened homestays and restaurants attract many visitors. We rode horses to the back mountains, enjoying the purest natural beauty, especially at sunset, with rolling grasslands and distant snow-capped mountains creating a picturesque scene.\nFigure: Qiongkushitai Xiata Ancient Trail Xiata Ancient Trail was a significant part of our journey. This ancient trail was once part of the Silk Road, with spectacular mountain gorge scenery along the way. Though the scenic area was not yet open, we used a drone to capture the glacier valleys.\nFigure: Xiata Ancient Trail Yizhao Highway Yizhao Highway is known as one of the most beautiful roads in Xinjiang. The scenery changes from mountains to grasslands, from forests to lakes, each view captivating. Driving along the cliffs, I couldn’t help but admire the efforts of the road builders.\nFigure: Yizhao Highway Sayram Lake Our final stop was Sayram Lake, located in the Bortala Mongol Autonomous Prefecture. The lake’s clear water, as blue as Prussian blue, is surrounded by snow-capped mountains and grasslands, creating a poetic landscape. We strolled along the lake, spent the night on the west bank, and felt the serenity and grandeur of this place known as “the last tear of the Atlantic.”\nFigure: Sayram Lake This road trip not only allowed me to experience the magnificent natural scenery but also the unique ethnic culture of the region. I captured the beauty of the journey with aerial footage and created a video to share with everyone.\nLocal Delicacies Ili’s scenic beauty is matched by its delicious food. The tender roasted meat on red willow skewers, spicy and flavorful chili meat (a local breakfast paired with steamed buns), the numbing and spicy pepper chicken, and the fragrant fried rice with mountain salmon roe from Sayram Lake, all left a lasting impression.\nFigure: Clockwise from top left: fried rice with mountain salmon roe, roasted meat on red willow skewers, chili meat, pepper chicken Xinjiang’s dried fruits, cheese, and beef jerky also make great souvenirs.\nConclusion This journey will be a memorable chapter in my life. The beauty and charm of Xinjiang Ili have deeply captivated me. If given the chance, I will surely return to explore more unexplored landscapes.\nI hope this travelogue brings you some beautiful memories of Xinjiang Ili. If you have the opportunity to travel to Xinjiang, don’t miss these stunning spots.\n","date":"2024-06-10T10:38:16+08:00","relpermalink":"/en/blog/xinjiang-ili-trip/","section":"blog","summary":"A Dragon Boat Festival road trip through Ili, Xinjiang, exploring majestic landscapes and local delicacies, capturing unforgettable moments.","title":"Dragon Boat Festival Road Trip in Xinjiang: Experiencing Majestic Landscapes"},{"content":"I recently implemented instant search using Fuse and the entire site’s structured data (only 2MB after compression) exported from Hugo, and also set up a quick search URL at https://jimmysong.io/search/?q=keyword , customizing the result display page. This showcases the power of open source, allowing for personal customization anywhere.\nFigure: Search results display page Overview Below, I will share how to add instant search functionality to your Hugo website. The main steps are as follows:\nExport structured data from the Hugo site Build search JavaScript code using the Fuse library Add a frontend search template Automate the update of website structured data Further optimization Tip Readers can refer to the search implementation in the hugo-blox-builder project, here is some reference code:\nFrontend code: search.html Search implementation: wowchemy-search.js Styles: _search.scss 1. Export Hugo Website’s Structured Data First, you need to create a JSON file for your Hugo site, which will contain essential metadata of all pages such as titles, descriptions, and links. This can be achieved by adding a custom output format in your Hugo configuration file (usually config.toml or config.yaml):\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;RSS\u0026#34;, \u0026#34;JSON\u0026#34;] [outputFormats.JSON] mediaType = \u0026#34;application/json\u0026#34; baseName = \u0026#34;index\u0026#34; isPlainText = false Then, in your content template (like layouts/_default/list.json.json), define the JSON structure to be output:\n{ \u0026#34;data\u0026#34;: [ {{ range .Pages }} { \u0026#34;title\u0026#34;: \u0026#34;{{ .Title }}\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;{{ .Permalink }}\u0026#34;, \u0026#34;summary\u0026#34;: \u0026#34;{{ .Summary }}\u0026#34; } {{ if not (eq .Next nil) }},{{ end }} {{ end }} ] } This will generate an index.json file for your entire site, containing the basic information of all pages. Of course, you may not want to export all pages of your site; you can customize the export of specific sections or different types of pages using Hugo’s syntax.\n2. Build Search JavaScript Code Using Fuse Library Next, use the Fuse.js library to implement the front-end instant search functionality. First, include the Fuse.js library file in your website. You can load it from a CDN like jsDelivr :\n\u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/fuse.js/dist/fuse.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Then, in your JavaScript file, load and parse the index.json file and perform the search using Fuse.js:\nfetch(\u0026#39;/index.json\u0026#39;) .then(response =\u0026gt; response.json()) .then(data =\u0026gt; { const fuse = new Fuse(data.data, { keys: [\u0026#39;title\u0026#39;, \u0026#39;summary\u0026#39;], includeScore: true }); document.getElementById(\u0026#39;search-input\u0026#39;).addEventListener(\u0026#39;input\u0026#39;, function (e) { const results = fuse.search(e.target.value); displayResults(results); }); }); function displayResults(results) { const searchResults = document.getElementById(\u0026#39;search-results\u0026#39;); searchResults.innerHTML = \u0026#39;\u0026#39;; results.forEach(result =\u0026gt; { const elem = document.createElement(\u0026#39;div\u0026#39;); elem.innerHTML = `\u0026lt;a href=\u0026#34;${result.item.url}\u0026#34;\u0026gt;${result.item.title}\u0026lt;/a\u0026gt;`; searchResults.appendChild(elem); }); } The specific implementation can be referenced from wowchemy-search.js .\n3. Add Frontend Search Template Add a search box\nand results display area to your website:\n\u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;search-input\u0026#34; placeholder=\u0026#34;Enter search term\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;search-results\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; Additionally, you can add shortcut keys, usually ⌘/CTRL + K, to quickly open the search page.\nThe specific implementation can be referenced from this frontend template .\n4. Automate Updating Website Structured Data To ensure real-time search results, you can automate the Hugo website’s build and deployment process through GitHub Actions or other CI/CD tools, ensuring the index.json file is always up-to-date.\nCreate a .github/workflows/hugo_build.yml file, defining the automation process:\nname: Build and Deploy on: push: branches: - main jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Set up Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;latest\u0026#39; - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public Additionally, if your website supports multiple languages, you can generate index.json files for each language and load the corresponding index file on different language pages.\n5. Further Optimization Cache Optimization: Set a reasonable cache policy for the index.json to reduce server load and improve response speed. If you are using GitHub Pages as a static site, you can ignore this step. Data Compression: Compress your index.json file. You can choose to export part of the website’s data, such as certain sections, truncate the content, or compress the file into gz format, then decompress it on the front end after loading, reducing network data transfer. Search Result Highlighting: Add highlighting for keywords in the search results to improve user experience. Advanced Search Options: Allow users to filter searches by specific fields (like authors, categories). Network Optimization: Asynchronously load the JavaScript file used for searching, By following these steps, you can effectively add a highly efficient and customizable instant search feature to your Hugo website.\nConclusion This article introduces how to add instant search functionality to a Hugo website, and provides suggestions for further optimizing the search feature, including cache optimization, search result highlighting, and advanced search options. This not only showcases the powerful customization capabilities of open source technology but also enables website users to find needed information faster and more accurately.\n","date":"2024-05-31T10:21:36+08:00","relpermalink":"/en/blog/hugo-instant-search-guide/","section":"blog","summary":"This article introduces how to add instant search functionality to a Hugo website, and outlines ways to optimize it.","title":"How to Add Instant Search to Your Hugo Website"},{"content":"In modern microservice architectures, security is a crucial aspect. As the number of microservices increases, ensuring secure communication between services becomes a challenge. This article will introduce several common authentication methods in microservices, helping you choose the appropriate authentication scheme when designing and implementing microservice systems.\nCommon Authentication Methods in Microservices The table below lists several common authentication methods in microservices, comparing them from the perspectives of advantages, disadvantages, applicable scenarios, and real-world examples.\nAuthentication Method Advantages Disadvantages Deployment Location Applicable Scenarios Typical Uses Real-World Examples JWT Self-contained token, reduces server load Larger token size, may increase bandwidth overhead API Gateway, Between Services Stateless communication between microservices User authentication and authorization User authentication in microservices (e.g., Auth0, Firebase) OAuth 2.0 Widely supported, highly flexible Complex implementation, requires additional interactions API Gateway Third-party application authorization Third-party application access to user data GitHub OAuth for third-party applications accessing GitHub data and APIs mTLS High security, prevents man-in-the-middle attacks Complex certificate management, significant performance overhead Between Services Communication requiring high security Secure communication between sensitive services Service communication in banking systems Basic Authentication Simple and easy to implement Insecure, easily intercepted API Gateway, Between Services Simple API protection Simple internal services Basic authentication of Kubernetes API Server API Key Authentication Simple and easy to use Low security, easily abused API Gateway, Between Services Scenarios with low security requirements Simple service access control Various public APIs, such as OpenAI API Below we will introduce these common authentication methods in detail.\nJWT Authentication JWT (JSON Web Token) was first proposed by the IETF JSON Web Token (JWT) Working Group and was officially released as the RFC 7519 standard in 2015. The design goal of JWT is to provide a compact and self-contained way to securely transmit information between parties. Due to its ease of use and stateless nature, JWT quickly gained widespread adoption and became one of the standards for identity verification and information exchange, especially in microservices and modern web applications.\nThe following diagram shows the JWT authentication process.\nFigure: JWT Authentication Process JWT Authentication Process Explanation:\nUser provides credentials Client requests access token Authentication server returns JWT token Client requests resource server with JWT token Resource server validates JWT JWT Format and Example A JWT (JSON Web Token) consists of three parts: Header, Payload, and Signature, each encoded with Base64 and concatenated with dots (.).\nHeader: The header includes the type of token and the signing algorithm.\n{ \u0026#34;alg\u0026#34;: \u0026#34;HS256\u0026#34;, \u0026#34;typ\u0026#34;: \u0026#34;JWT\u0026#34; } Payload: The payload contains claims, which are assertions about the user or other data.\n{ \u0026#34;sub\u0026#34;: \u0026#34;1234567890\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;iat\u0026#34;: 1516239022 } Signature: The signature is generated by encoding the header and payload, and signing them with a secret using the algorithm specified in the header.\nHMACSHA256( base64UrlEncode(header) + \u0026#34;.\u0026#34; + base64UrlEncode(payload), secret ) Below is an example of a JWT token:\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c OAuth 2.0 OAuth (Open Authorization) protocol was first proposed by Blaine Cook and Chris Messina in 2006. Initially, the goal was to provide an open authorization standard for Twitter. OAuth 1.0 was released in 2007, offering a standardized way for users to authorize third-party applications to access their resources. However, due to its complex signature mechanism and other security issues, it faced some limitations.\nTo overcome these shortcomings, the IETF (Internet Engineering Task Force) established the OAuth Working Group to develop a simpler, more flexible authorization protocol. In 2012, OAuth 2.0 was officially released (RFC 6749 and RFC 6750). OAuth 2.0 simplifies the authorization process, adding multiple authorization modes, such as authorization code mode, implicit mode, resource owner password credentials mode, and client credentials mode.\nOAuth 2.0 quickly became an industry standard, widely used in various web services and applications such as Google, Facebook, GitHub, etc. Several extensions and complementary protocols (such as OpenID Connect) emerged based on it, further enhancing OAuth 2.0’s functionality and security.\nThe development and expansion of OAuth 2.0 have made it a cornerstone of modern internet authentication and authorization, providing a flexible and secure solution to meet the evolving needs of web applications.\nThe following diagram shows the OAuth 2.0 authentication process.\nFigure: OAuth 2.0 Authentication Process OAuth 2.0 Authentication Process Explanation:\nUser requests access to resource Client requests authentication User logs in and authorizes Authorization server returns authorization code Client exchanges authorization code for access token Client requests resource server with access token OAuth 2.0 Authorization Code In the OAuth 2.0 authorization code mode, the authorization code is a short-term credential obtained by the client from the authorization server after user authorization, used to exchange for an access token. The authorization code is a temporary string that can be passed between the authorization server and the client to obtain a more secure access token.\nOAuth 2.0 Extensions OAuth 2.0 has developed many extensions to adapt to different scenarios. The table below lists some commonly used extensions, their main functions, and applicable scenarios.\nExtension Name Main Function Applicable Scenarios Authorization Code PKCE Extension Enhances the security of the authorization code mode, prevents authorization code interception attacks Public clients (e.g., mobile apps, single-page apps) Dynamic Client Registration Protocol Allows clients to dynamically register and update client information Systems requiring high automation and flexibility Token Introspection Allows resource servers to verify and obtain detailed information about access tokens Scenarios requiring token validity verification and detailed information Token Revocation Provides a standard interface for token revocation Enhances system security and control Device Authorization Grant Allows devices with limited input capabilities to complete authentication via other devices Devices with limited input capabilities (e.g., smart TVs, game consoles) Mutual TLS Client Authentication Client authentication based on mutual TLS High-security application scenarios Resource Indicators Allows clients to specify the resource server to access in the authorization request Support for multiple resource servers Step-up Authentication Challenge Protocol Allows resource servers to request stronger authentication (e.g., multi-factor authentication) as needed Advanced authentication for high-risk operations OAuth 2.0 Authorization Process (Using GitHub as an Example) GitHub uses OAuth 2.\n0 to authorize third-party applications to access users’ GitHub data. OAuth 2.0 tokens on GitHub are called “access tokens” and are used for authentication and authorization to access GitHub APIs. It provides a secure, standardized way for third-party applications to access GitHub resources with user authorization. By using access tokens, applications can perform various operations on behalf of users, such as reading user information, accessing repositories, creating gists, etc. This process ensures user security and privacy while simplifying the authentication and authorization process for applications.\nThe detailed process and examples of using GitHub OAuth 2.0 tokens are as follows:\nUser Authorization: The user clicks the “Login with GitHub” button on the third-party application’s interface. The application redirects the user to GitHub’s authorization page.\nObtain Authorization Code: The user logs in and agrees to authorize on the GitHub authorization page. GitHub redirects the user back to the application with an authorization code in the URL parameter.\nExample:\nhttps://yourapp.com/callback?code=AUTHORIZATION_CODE Exchange Access Token: The application server requests an access token from GitHub’s authorization server using the authorization code.\nRequest Example:\nPOST https://github.com/login/oauth/access_token Content-Type: application/json Accept: application/json { \u0026#34;client_id\u0026#34;: \u0026#34;YOUR_CLIENT_ID\u0026#34;, \u0026#34;client_secret\u0026#34;: \u0026#34;YOUR_CLIENT_SECRET\u0026#34;, \u0026#34;code\u0026#34;: \u0026#34;AUTHORIZATION_CODE\u0026#34;, \u0026#34;redirect_uri\u0026#34;: \u0026#34;https://yourapp.com/callback\u0026#34; } GitHub Returns Access Token: GitHub validates the request and returns an access token.\nResponse Example:\n{ \u0026#34;access_token\u0026#34;: \u0026#34;YOUR_ACCESS_TOKEN\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;bearer\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;repo,gist\u0026#34; } Use Access Token to Access Resources: The application uses the obtained access token to access the GitHub API.\nRequest Example:\ncurl -H \u0026#34;Authorization: token YOUR_ACCESS_TOKEN\u0026#34; https://api.github.com/user Response Example:\n{ \u0026#34;login\u0026#34;: \u0026#34;github-user\u0026#34;, \u0026#34;id\u0026#34;: 1, \u0026#34;node_id\u0026#34;: \u0026#34;MDQ6VXNlcjE=\u0026#34;, \u0026#34;avatar_url\u0026#34;: \u0026#34;https://github.com/images/avatar.jpg\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Github User\u0026#34;, \u0026#34;company\u0026#34;: \u0026#34;GitHub\u0026#34;, \u0026#34;blog\u0026#34;: \u0026#34;https://example.com\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;Earth\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;github-user@example.com\u0026#34; } Characteristics and Use of Access Tokens Scopes: The scope of an access token’s permissions is specified by the user during authorization and can include reading user profiles, accessing user repositories, managing gists, etc. For example, in the example above, the scope includes repo and gist.\nValidity and Refreshing: Access tokens can be long-term or …","date":"2024-05-22T13:54:49+08:00","relpermalink":"/en/blog/microservice-auth-methods/","section":"blog","summary":"This article introduces several common authentication methods in microservices, helping you choose the appropriate authentication scheme when designing and implementing microservice systems.","title":"Detailed Explanation of Common Authentication Methods in Microservices"},{"content":"Recently, I spent over a month implementing the biggest revamp of this site in four years. The last major update was in 2020 . The key changes in this update include:\nContent Reorganization: Books from the original cloud-native resource library have been moved to the Resources directory. Visual Upgrade: The website theme and color scheme have been updated for a fresher and more modern look. Search Optimization: The search functionality has been enhanced, offering richer and more intuitive results with significantly faster response times. Mobile Adaptation: Comprehensive improvements have been made for better display and user experience on mobile devices. Performance Enhancements: Website performance has been improved, resulting in faster load times. Navigation Improvement: Page navigation and categorization have been optimized to help users quickly find the information they need. Mermaid Support: Added support for displaying Mermaid diagrams, making it easier to present flowcharts and graphs. Thank you for your continuous support and attention! I hope this revamp will provide you with a better user experience.\n","date":"2024-05-20T12:27:17+08:00","relpermalink":"/en/notice/website-revamp-notice/","section":"notice","summary":"The biggest update in nearly four years.","title":"Website Revamp Notice"},{"content":"This article delves into the design principles, implementation, and how Ambient Mode enhances security and permission management in Istio CNI plugins. The content includes:\nSecurity risks of Init containers and their solutions. Working principles and advantages of Istio CNI. Implementation mechanism of Ambient Mode and its integration with CNI. Overview of Istio Network Requirements and Solutions Istio service mesh intercepts and manages application traffic through the Sidecar mode. This mode injects a Sidecar Proxy and init containers into application pods and uses iptables rules to manage network traffic. For detailed deployment and operation processes, please refer to Understanding Sidecar Injection, Transparent Traffic Hijacking, and Traffic Routing in Istio . Although this method is effective on most Kubernetes platforms, the high dependency on privileges raises security concerns in multi-tenant environments.\nLimitations of Istio-init During its initial network configuration, Istio adopted the istio-init container to initialize traffic interception rules, requiring containers to have advanced permissions to modify network configurations like IPTables rules. While this method effectively manages traffic, it significantly increases permission requirements and security risks. According to the Istio documentation , the istio-init container is injected into pods within the Istio mesh by default to hijack network traffic to Istio’s Sidecar proxy. This process requires granting the Service Account deploying the pod the NET_ADMIN container permission , which may contradict the security policies of some organizations.\nIstio CNI Plugin In response to this challenge, the Istio community introduced the Istio CNI plugin, which avoids the need for init containers, allowing direct manipulation at the Kubernetes network layer, thereby reducing permission requirements and simplifying the deployment process, but with CNI compatibility issues.\nIntroduction of Ambient Mode Istio’s Ambient Mode is an innovative sidecar-less solution that enhances network flexibility and security by using Geneve tunnels or Istio CNI.\nOnly recently has the Istio community introduced a universal solution compatible with any CNI . This mode addresses compatibility issues with any CNI, enabling Istio to more effectively manage traffic between services without affecting existing network policies.\nSecurity Considerations for NET_ADMIN Permissions In containerized environments like Kubernetes and Docker, NET_ADMIN permissions allow processes within containers to perform extensive network-related operations, including modifying iptables rules, changing network interface configurations, managing IP routing tables, and controlling kernel parameters related to networking. However, the use of these permissions raises security concerns, especially regarding overprivileged access and potential attack surfaces.\nBest practices include:\nLimiting scope of use: Grant NET_ADMIN permissions only when necessary and restrict them through Kubernetes network policies. Continuous monitoring and auditing: Enforce strict logging and monitoring for containers using NET_ADMIN permissions. Working Principles of Istio CNI Plugin The Istio CNI plugin is a binary file installed as an agent in the file system of each node. The following flowchart illustrates the working principles of the Istio CNI node agent:\nFigure: Mermaid Diagram Istio CNI Node Agent acts as an agent installed on each node. It installs the Istio CNI plugin and updates the node’s CNI configuration. The agent monitors the CNI plugin and config paths for changes. In Sidecar Mode, it handles sidecar networking setups using iptables for pods. In Ambient Mode, it synchronizes pod events to an ambient watch server, which then configures iptables within pods. Nodes require elevated privileges like CAP_SYS_ADMIN, CAP_NET_ADMIN, and CAP_NET_RAW to function in either mode. Resolving Conflicts Between Istio Ambient Mode and Kubernetes CNI Istio’s Ambient Mode is designed to adapt to all CNIs, transparently handling traffic redirection within pods using ztunnel without affecting existing CNI configurations. In this mode, Ambient Mode manages traffic through ztunnel to flow through the Istio service mesh, while standard CNIs focus on providing standardized network access for pods.\nThe primary responsibilities of CNI are to address network connectivity between Kubernetes Pods, such as assigning IP addresses and forwarding packets. In contrast, Ambient Mode needs to import traffic into ztunnel, which may be incompatible with CNI’s network configuration. The main issues include:\nMainstream CNI network configurations may conflict with Istio’s CNI extensions, causing interruptions in traffic processing. Using Istio CNI may affect the execution of these policies if the deployed network policies depend on CNI. To address these issues, traffic redirection is managed by running ztunnel in the same user space as the pod, avoiding conflicts with the kernel space modified by CNI. Thus, pods can connect directly to ztunnel, bypassing the influence of CNI.\nThe following sequence diagram describes the process under Ambient mode:\nFigure: Mermaid Diagram Ambient CNI Agent initiates interactions by listening for UDS events signaling pod creations.\nAmbient Watch Server modifies iptables within pods to redirect traffic to ztunnel as needed.\nztunnel establishes connections and handles network traffic redirection within the Kubernetes cluster.\nResolving Conflicts Between Istio Ambient Mode and Kubernetes CNI To mitigate these conflicts, Istio’s Ambient Mode avoids dependencies on the kernel space modified by CNI:\nRun ztunnel in user space: This strategy allows ztunnel to run in the same user space as the pod, avoiding direct conflicts with CNI. Ensure CNI compatibility: Istio CNI configurations must be carried out without affecting existing CNI plugin configurations, ensuring normal communication between pods and traffic management. These measures help Istio’s Ambient Mode effectively manage traffic between services without disrupting existing CNI plugins.\nOptimized Traffic Management with Istio Ambient Mode Istio’s Ambient Mode employs an advanced traffic forwarding mechanism through node-local Ztunnel, allowing for the establishment of listening sockets within a Pod’s network namespace. This setup facilitates effective redirection of encrypted (mTLS) and plaintext traffic originating from the service mesh. Not only does this approach enhance the flexibility of traffic management, but it also prevents potential conflicts with existing CNI plugins. Below is a detailed implementation flow of this mode:\nFigure: Mermaid Diagram The specific steps involved are as follows:\nDetection of Tags: The Istio CNI node agent detects Pods tagged with istio.io/dataplane-mode=ambient. Triggering the CNI Plugin: Based on Pod events (either a new start or an existing Pod joining the mesh), the CNI plugin is triggered, leading the Istio CNI node agent to configure traffic redirection. Configuring Redirection Rules: Network redirection rules are set up within the Pod’s network namespace to intercept and redirect traffic to the node-local ztunnel proxy. Establishment of Listening Sockets: The node-local ztunnel creates listening sockets within the Pod’s network namespace to enable traffic redirection. Traffic Handling: The node-local ztunnel handles encrypted (mTLS) and plaintext traffic within the mesh, ensuring secure and efficient data transfer. Through this approach, Istio Ambient Mode provides a more effective and secure solution for managing inter-service traffic in Kubernetes environments.\nConclusion This article thoroughly analyzes the design principles, implementation, and advantages of the Istio CNI plugin, particularly how Istio CNI addresses the permission and security issues present in traditional istio-init methods. Through these innovations, Istio has made significant progress in network security and operational simplicity, providing a more flexible and efficient method for implementing Istio in Kubernetes environments.\nThis blog was initially published at tetrate.io .\n","date":"2024-05-19T18:54:49+08:00","relpermalink":"/en/blog/istio-cni-deep-dive/","section":"blog","summary":"This article provides a detailed explanation of the design principles, implementation methods, and how to enhance security and permission management through Ambient Mode in the Istio CNI plugin.","title":"Istio CNI Unveiled: Streamlining Service Mesh Connectivity"},{"content":"Effective management of networking is crucial in containerized environments. The Container Network Interface (CNI) is a standard that defines how containers should be networked. This article delves into the fundamentals of CNI and explores its relationship with CRI.\nWhat is CNI? The CNI (Container Network Interface) specification provides a common interface between container runtimes and network plugins, aiming to standardize container network configuration.\nThe CNI specification comprises several core components:\nNetwork configuration format: Defines how administrators define network configurations. Request protocol: Describes how container runtimes send network configuration or cleanup requests to network plugins. Plugin execution process: Details how plugins execute network setup or cleanup based on the provided configuration. Plugin delegation: Allows plugins to delegate specific functionalities to other plugins. Result return: Defines the data format for returning results to the runtime after plugin execution. By defining these core components, the CNI specification ensures that different container runtimes and network plugins can interact in a consistent manner, enabling automation and standardization of network configuration.\nKey points of the CNI specification CNI is a plugin-based containerized networking solution. CNI plugins are executable files. The responsibility of a single CNI plugin is singular. CNI plugins are invoked in a chained manner. The CNI specification defines a Linux network namespace for a container. Network definitions in CNI are stored in JSON format. Network definitions are transmitted to plugins via STDIN input streams, meaning network configuration files are not stored on the host, and other configuration parameters are passed to plugins via environment variables. CNI plugins receive network configuration parameters according to the operation type, perform network setup or cleanup tasks accordingly, and return the execution results. This process ensures dynamic configuration of container networks synchronized with container lifecycles.\nThe following diagram illustrates the multitude of network plugins encompassed by CNI.\nFigure: Mermaid Diagram According to the CNI specification , a CNI plugin is responsible for configuring a container’s network interface in some way. Plugins can be classified into two major categories:\n“Interface” plugins, responsible for creating network interfaces inside containers and ensuring their connectivity. “Chained” plugins, adjusting the configuration of already created interfaces (but may need to create more interfaces to accomplish this). Relationship Between CNI and CRI CNI and CRI (Container Runtime Interface) are two critical interfaces in Kubernetes, responsible for container network configuration and runtime management, respectively. In Kubernetes clusters, CRI invokes CNI plugins to configure or clean up container networks, ensuring tight coordination between the network configuration process and container creation and destruction processes.\nThe following diagram intuitively illustrates how CNI collaborates with CRI:\nFigure: Mermaid Diagram Kubelet to CRI: The Kubelet instructs the CRI to create the containers for a scheduled pod. CRI to Pod: The container runtime starts the container within the pod. Pod to CRI: Once the container is running, it signals back to the container runtime. CRI to Kubelet: The container runtime notifies the Kubelet that the containers are ready. Kubelet to CNI: With the containers up, the Kubelet calls the CNI to set up the network for the pod. CNI to Pod: The CNI configures the network for the pod, attaching it to the necessary network interface. Pod to CNI: After the network is configured, the pod confirms network setup to the CNI. CNI to Kubelet: The CNI informs the Kubelet that the pod’s network is ready. Kubelet to Pod: The pod is now fully operational, with both containers running and network configured. The following diagram shows the detailed steps involved in setting up networking for a pod in Kubernetes:\nFigure: Mermaid Diagram Pod scheduling: The Kubelet schedules a pod to run on a node. Request network setup: The scheduled pod requests network setup from the Kubelet. Invoke CNI: The Kubelet invokes the CNI to handle the network setup for the pod. Create network namespace: The CNI creates a network namespace for the pod, isolating its network environment. Allocate IP address: The CNI, through its IP Address Management (IPAM) plugin, allocates an IP address for the pod. Setup network interfaces: The CNI sets up the necessary network interfaces within the pod’s network namespace, attaching it to the network. Network setup complete: The pod notifies the Kubelet that its network setup is complete. Pod running with network: The pod is now running with its network configured and can communicate with other pods and services within the Kubernetes cluster. CNI Workflow The Container Network Interface (CNI) specification defines how containers should configure networks, including five operations: ADD, CHECK, DELETE, GC, and VERSION. Container runtimes execute these operations by calling various CNI plugins, enabling dynamic management and updates of container networks.\nFigure: Mermaid Diagram To elaborate on each step described in the sequence diagram, involving interactions between Kubelet, Pod, CNI plugins (both interface and chained), network setup, and IP address management (IPAM), let’s delve deeper into the process:\nSchedule Pod: Kubelet schedules a Pod to run on a node. This step initiates the lifecycle of Pods within the Kubernetes cluster. Request Network Setup: The Pod requests Kubelet for network setup. This request triggers the process of configuring the network for the Pod, ensuring its ability to communicate within the Kubernetes cluster. Call CNI Plugins: Kubelet invokes configured Container Network Interface (CNI) plugins. CNI defines a standardized way for container management systems to set up network interfaces within Linux containers. Kubelet passes necessary information to CNI plugins to initiate network setup. Call Interface Plugin: The CNI framework calls an interface CNI plugin responsible for setting up primary network interfaces for the Pod. This plugin may create a new network namespace, connect a pair of veth, or perform other actions to ensure the Pod has the required network interfaces. Set Network Interfaces: The interface CNI plugin configures network interfaces for the Pod. This setup includes assigning IP addresses, setting up routes, and ensuring interfaces are ready for communication. Call Chained Plugin: After setting up network interfaces, the interface CNI plugin or the CNI framework calls chained CNI plugins. These plugins perform additional network configuration tasks, such as setting up IP masquerading, configuring ingress/egress rules, or applying network policies. Allocate IP Address: As part of the chained process, one of the chained CNI plugins may involve IP Address Management (IPAM). The IPAM plugin is responsible for assigning an IP address to the Pod, ensuring each Pod has a unique IP within the cluster or namespace. IP Address Allocated: The IPAM plugin allocates an IP address and returns the allocation information to the calling plugin. This information typically includes the IP address itself, subnet mask, and possible gateway. Apply Network Policies: Chained CNI plugins apply any specified network policies to the Pod’s network interfaces. These policies may dictate allowed ingress and egress traffic, ensuring network security and isolation per cluster configuration requirements. Chained Configuration Complete: Once all chained plugins have completed their tasks, the overall network configuration for the Pod is considered complete. The CNI framework or the last plugin in the chain signals to Kubelet that network setup is complete. Network Setup Complete: Kubelet receives confirmation of network setup completion from the Pod. At this point, the Pod has fully configured network interfaces with IP addresses, route rules, and applied network policies. Pod Running with Network: The Pod is now running and has its network configured. It can communicate with other Pods within the Kubernetes cluster, access external resources per network policies, and perform its designated functions. The following are example sequence diagrams and detailed explanations for the ADD, CHECK, and DELETE operations based on the official CNI examples . Through these operations, interactions between the container runtime and CNI plugins facilitate dynamic management and updates of container network configurations.\nADD Operation Example Below is the example sequence diagram and detailed explanation for the ADD operation:\nFigure: Mermaid Diagram Container Runtime Calls Portmap Plugin: The container runtime executes the ADD operation by calling the Portmap plugin to configure port mapping for the container. Portmap Configuration Complete: The Portmap plugin completes the port mapping configuration and returns the result to the container runtime. Container Runtime Calls Tuning Plugin: The container runtime invokes the Tuning plugin to execute the ADD operation and configure network tuning parameters for the container. Tuning Configuration Complete: The Tuning plugin finishes configuring network tuning parameters and returns the result to the container runtime. Container Runtime Calls Bridge Plugin: The container runtime calls the Bridge plugin to execute the ADD operation and configure network interfaces and IP addresses for the container. Bridge Plugin Calls Host-local Plugin: Before completing its own configuration, the Bridge plugin calls the Host-local plugin to execute the ADD operation and configure IP addresses for the container. IPAM Configuration Complete: The Host-local plugin, acting as the authority for IP Address Management (IPAM), completes IP address allocation and returns the …","date":"2024-05-18T13:54:49+08:00","relpermalink":"/en/blog/cni-deep-dive/","section":"blog","summary":"This article provides an in-depth explanation of the basic concepts of Container Network Interface (CNI), its core components, and its relationship with Container Runtime Interface (CRI).","title":"CNI Essentials: Powering Kubernetes' Network"},{"content":"Selecting the right networking tool in a Kubernetes environment is crucial. According to Tetrate’s discussion , the choice depends on the type of network traffic: north-south or east-west. For services primarily handling external requests, Envoy Gateway is the ideal choice. It not only efficiently manages traffic but also seamlessly integrates as you transition to a microservices architecture.\nThis article explores the advantages of deploying Envoy Gateway on Kubernetes, its relationship with other service mesh components, and why it’s the ideal choice for exposing services to the public internet.\nOverview of Envoy Gateway and its Role in Service Mesh Envoy Gateway is a Kubernetes-native API gateway built around Envoy Proxy. It aims to lower the barrier for users adopting Envoy as an API gateway and lays the foundation for vendors to build value-added products like Tetrate Enterprise Gateway for Envoy .\nEnvoy Gateway is not only an ideal choice for managing north-south traffic but also serves as a crucial component for connecting and securing services within the service mesh. It enhances communication efficiency and security among microservices by providing features such as secure data transmission, traffic routing, load balancing, and fault recovery. Leveraging its built-in Envoy Proxy technology, Envoy Gateway can handle a large number of concurrent connections and complex traffic management policies while maintaining low latency and high throughput.\nFurthermore, the tight integration of Envoy Gateway with the Kubernetes Gateway API allows for declarative configuration and management, significantly simplifying the deployment and update processes of gateways within the service mesh. This integration not only improves operational efficiency but also enables Envoy Gateway to seamlessly collaborate with solutions like Istio without adding extra complexity.\nThe figure below illustrates the relationship between Envoy Gateway and the service mesh.\nFigure: Mermaid Diagram In a Kubernetes cluster, Envoy Gateway is responsible for managing north-south traffic, i.e., traffic entering and leaving the cluster, and is configured through the Kubernetes Gateway API, which defines routing specifications for services. Services within the cluster directly connect to pods. In the service mesh, the control plane (e.g., Istio or Linkerd) configures Envoy sidecars in the data plane, which handles east-west traffic within the cluster. In this system, Envoy Gateway can collaborate with the service mesh, but they independently manage traffic in different directions.\nThink of Envoy Gateway as the main entry point to a city (e.g., customs), where all traffic (like various vehicles) must pass through. It acts as a strict gatekeeper, responsible for inspection and guidance, ensuring each packet (like each passenger) is accurately delivered to its destination. In the city of Kubernetes, Envoy Gateway manages all inbound traffic, ensuring data flows securely and efficiently into the city and is accurately delivered to services within the city.\nOnce inside the city, the service mesh takes over, acting as a series of transportation networks within the city. Envoy sidecars in the service mesh are like taxis or buses within the city, responsible for transporting packets from the port to their specific destinations within the city. Envoy Gateway ensures smooth entry for external requests, and then the service mesh efficiently handles these requests within the cluster.\nThe support for Kubernetes Gateway API by Envoy Gateway can be seen as a significant upgrade to our city’s traffic signal system. It not only provides clearer and more personalized guidance for incoming data flows but also makes the entire city’s traffic operations more intelligent.\nCore Features and Advantages of Envoy Gateway Envoy Gateway offers several core features that make it a prominent choice for an API gateway:\nSimplified Configuration: Through direct integration with the Kubernetes Gateway API, Envoy Gateway allows developers to use Kubernetes custom resources to declaratively configure routing rules, security policies, and traffic management. Performance and Scalability: Built on battle-tested Envoy Proxy, it delivers outstanding performance and scalability, effortlessly handling thousands of services and millions of requests per second. Security Features: Built-in support for various security measures such as SSL/TLS termination, OAuth2, OIDC authentication, and fine-grained access control. Observability: Provides comprehensive monitoring capabilities including detailed metrics, logs, and tracing, crucial for diagnosing and understanding traffic behavior. Relationship with Gateway API The introduction of the Gateway API in Kubernetes provides a powerful new way to integrate and configure ingress gateways, offering higher flexibility and functionality compared to traditional ingress. As discussed in this blog , the Gateway API simplifies gateway management, allowing developers to define custom routing rules, TLS termination policies, and traffic policies using Kubernetes-native resources.\nThe Kubernetes Gateway API serves as the cornerstone of Envoy Gateway, providing a more expressive, flexible, and role-based approach to configuring gateways and routes within the Kubernetes ecosystem. This API offers custom resource definitions (CRDs) such as GatewayClass, Gateway, HTTPRoute, etc. Envoy Gateway utilizes these resources to create a user-friendly and consistent configuration model that aligns with Kubernetes’ native principles.\nWhat is an API Gateway? An API Gateway is a comprehensive management and hosting service for APIs. Serving as an intermediary layer between applications and backend services, it not only handles lifecycle events like creation, maintenance, deployment, running, and retiring but also performs additional critical functions. A robust API Gateway should provide the following features to enhance and extend its basic definition:\nTraffic Control: Ability to manage and control traffic to backend services, including request routing, load balancing, circuit breaking, and rate limiting, ensuring the stability and high availability of backend services. Security Assurance: Authentication, authorization, and encryption capabilities to effectively manage and protect API security. This involves authentication mechanisms, API key management, OAuth, JWT, mTLS, etc., ensuring that only authorized users and services can access the API. Monitoring and Analytics: Real-time monitoring and logging functionalities to track API usage, performance metrics, anomaly detection, and traffic pattern analysis, optimizing API performance and responsiveness. Change Management: Support for managing API changes, including version control and progressive deployment (such as blue-green or canary release), for seamless transition to new versions while minimizing impact on end users. Request and Response Transformation: Allow transformation of incoming and outgoing API calls, such as from REST to GraphQL, or adding, removing, and modifying request and response headers. Cross-Origin Resource Sharing (CORS) Support: Manage and control cross-origin requests, allowing frontend applications from different domains to securely call backend APIs. Quotas and Billing: Set quota limits for API usage, while also supporting billing functionalities for commercialized API offerings. Developer-Friendly Developer Portal: Provide a developer-facing portal, enabling third-party developers to easily discover, test, and integrate APIs. Protocol Support: Support for various network protocols, including HTTP/HTTPS, WebSocket, gRPC, etc., ensuring compatibility with a variety of clients and services. Plug-ability and Extensibility: Allow extension of API Gateway functionality through plugins or middleware, enabling flexible adaptation to various middleware services based on business requirements. Service Governance: Integration with service registration and discovery mechanisms to accommodate the dynamics of services in a microservices architecture. In conclusion, the role of an API Gateway extends far beyond simple API lifecycle management. It is a key component in realizing microservices architecture, ensuring service security, improving operational efficiency, and optimizing user experience. Through these extensive functionalities, the API Gateway becomes an indispensable part of modern cloud-native applications.\nOverview of Envoy Gateway Architecture The architecture of Envoy Gateway is designed to be lightweight and concise. It consists of a control plane that dynamically configures an Envoy proxy running as the data plane. This separation of concerns ensures that the gateway can scale with increasing traffic without affecting the efficiency of the control plane.\nThe architecture diagram of Envoy Gateway is shown below.\nFigure: Envoy Gateway Architecture Diagram At the core of this architecture is the Envoy Gateway, which is an instance of the Envoy proxy responsible for handling all traffic in and out of the Kubernetes cluster. Upon initial startup, Envoy Gateway provides static configuration through configuration files, establishing the basic parameters of its operation.\nThe dynamic aspect of Envoy Gateway configuration is handled by providers, which define the interaction between the gateway and Kubernetes or other dynamic configuration input sources. The resource monitor is responsible for monitoring changes to Kubernetes resources, with particular attention to CRUD operations related to custom resource definitions (CRDs).\nAs changes occur, resource transformers intervene to translate these external resources into a form understandable by Envoy Gateway. This transformation process is further facilitated by provider-specific infrastructure managers, which are responsible for managing resources related to specific clouds or infrastructure providers, shaping the infrastructure into an intermediate …","date":"2024-05-16T22:35:19+08:00","relpermalink":"/en/blog/envoy-gateway-introduction/","section":"blog","summary":"This article explores the advantages of deploying Envoy Gateway on Kubernetes, its relationship with other service mesh components, and why it's the ideal choice for exposing services to the public internet.","title":"Envoy Gateway Overview: Modern Kubernetes Ingress with Envoy Gateway and the Gateway API"},{"content":"Istio 1.22 marks the official beta release of Ambient mode, accompanied by a blog titled Say goodbye to your sidecars: Istio’s ambient mode reaches Beta in v1.22 , claiming that Layer 4 and Layer 7 features are now production-ready. This milestone was actually announced by the community at KubeCon EU a month earlier. Such exciting promotion seems to suggest that we can completely abandon the Sidecar mode, but is this really the case?\nWhy Not Hurry to Say Goodbye to Sidecar Mode? While I am open to new technologies, it may be premature to completely abandon the Sidecar mode. Each mode has its specific application scenarios, advantages, and disadvantages. Below, I will share in detail some of the limitations of the Ambient mode compared to the Sidecar mode, to help everyone better understand the differences between the two.\nKey Differences Between Ambient Mode and Sidecar Mode Traffic Management The L7 traffic management support in Ambient mode is not yet mature and production-ready. In contrast, Sidecar mode is more stable and reliable in this regard.\nSecurity In Ambient mode, mTLS is enforced at the namespace level, whereas Sidecar mode gives users more flexibility to choose whether to enable mTLS. This flexibility is particularly important for certain application scenarios.\nObservability For L7 layer telemetry data, it remains questionable whether Ambient mode can provide precise monitoring and tracing for each pod as effectively as Sidecar mode. Sidecar mode has been widely validated in terms of observability and is more mature.\nOperations In terms of deployment, Ambient mode recommends using Helm and only supports the Kubernetes platform, while Sidecar mode also supports VMs and hybrid cloud environments. Additionally, Ambient mode has not yet received official support from major cloud vendors. During upgrades, Ambient mode has a larger blast radius and currently does not support canary releases, recommending blue-green deployments instead. There is still a lack of best practices for migrating from Sidecar mode to Ambient mode or coexisting with both.\nExtensibility Currently, support for Wasm plugins in Ambient mode is still unclear, whereas Sidecar mode already has relatively complete support in this area.\nOther Functional Features While Dual Stack mode is still experimental in Sidecar mode, it has at least some implementation, whereas it remains unclear whether Ambient mode supports this feature.\nConclusion Although Istio 1.22 brings the exciting Ambient mode, we need to carefully consider these limitations and differences before completely saying goodbye to Sidecar mode. Each mode has its unique advantages and applicable scenarios, and users should make informed choices based on their own needs. I will continue to test and track Ambient mode, so stay tuned to this blog for more in-depth analysis.\n","date":"2024-05-16T11:28:34+08:00","relpermalink":"/en/blog/istio-ambient-mode-limitations/","section":"blog","summary":"In-depth discussion on the Ambient mode in Istio 1.22, comparison with the traditional Sidecar mode, and its limitations.","title":"Analysis of the Limitations of Istio Ambient Mode"},{"content":"Recently, the CNCF released the KubeCon EU 2024 Transparency Report . The KubeCon + CloudNativeCon Europe 2024 held in Paris was the largest event in its history, and I had the fortune to attend in person. The beauty of Paris and the atmosphere of the conference were unforgettable.\nFigure: KubeCon + CloudNativeCon EU 2024 Transparency Report Cover We witnessed the power of the open-source and cloud-native communities and how they drive the next major technological shift — artificial intelligence. At this conference, CNCF announced the establishment of an AI working group and launched the first edition of the Cloud Native AI Whitepaper , followed by the creation of the Cloud Native AI Landscape .\nGrowth and Diversity The event welcomed over 12,000 participants, with the majority (78%) coming from Europe and 52% attending for the first time. Participants from different regions showed enthusiasm for cloud-native technologies. The diverse functional backgrounds of the participants, including business operations, developers, and architects, highlighted the wide application of cloud-native technology across fields.\nThe following table shows the top 10 countries by attendee numbers:\nCountry Number of Attendees France 1,956 Germany 1,879 USA 1,733 United Kingdom 816 Netherlands 797 Switzerland 368 Spain 355 Israel 312 Poland 303 Denmark 293 Innovation and Communication During the event, there were 331 sessions and 19 keynote speeches covering hot topics such as artificial intelligence, application development, and platform engineering. This edition of KubeCon also hosted poster sessions and project tours for researchers, helping attendees better understand various projects.\nCommunity and Inclusivity KubeCon is committed to supporting diversity, equity, and inclusion. Activities such as diversity lunches and Peer Group Mentoring provided platforms for participants to connect and support each other. (PS. The catering at the venue was not to my liking, especially the taste of the tea and the hard bread.)\nEnvironmental and Sustainability In terms of environmental and sustainability efforts, the organizers chose venues with an environmental consciousness and implemented measures to reduce food waste and recycle waste, demonstrating CNCF’s commitment to environmental protection.\nLooking Forward Looking forward, we hope to bring the magic of KubeCon to more regions globally. Whether it’s the upcoming KubeCon + CloudNativeCon China 2024 or other regional events, I am very much looking forward to meeting you. I have also submitted a panel session for KubeCon China, and if selected, we will meet in Hong Kong this August.\nConclusion KubeCon + CloudNativeCon Europe 2024 was not just a technical gathering, but a display of community strength. If you would like to know more details, please check out my previous recap of KubeCon + CloudNativeCon Europe .\nI hope this transparency report helps you better understand the impact of the event and inspires you to participate in future KubeCon events.\n","date":"2024-05-07T21:11:49+08:00","relpermalink":"/en/blog/kubecon-eu-2024-transparency-report/","section":"blog","summary":"A detailed interpretation of the key data and highlights from KubeCon Europe 2024.","title":"KubeCon EU 2024 Transparency Report Interpretation"},{"content":"In March 2024, during KubeCon EU, the Cloud Native Computing Foundation (CNCF) released its first detailed whitepaper on Cloud Native Artificial Intelligence (CNAI). This report thoroughly explores the current state, challenges, and future directions of integrating Cloud Native technologies with artificial intelligence. This article delves into the core content of this whitepaper.\nWhat is Cloud Native AI? Cloud Native AI refers to the approach of building and deploying artificial intelligence applications and workloads using Cloud Native technology principles. This includes leveraging microservices, containerization, declarative APIs, and continuous integration/continuous deployment (CI/CD) to enhance the scalability, reusability, and operability of AI applications.\nThe diagram below illustrates the architecture of Cloud Native AI, redrawn based on the whitepaper.\nFigure: Cloud Native AI Architecture Relationship Between Cloud Native AI and Cloud Native Technologies Cloud Native technologies provide a flexible, scalable platform that makes the development and operation of AI applications more efficient. Through containerization and microservices architecture, developers can iterate and deploy AI models rapidly while ensuring high availability and scalability of systems. Kubernetes and other Cloud Native tools provide essential support such as resource scheduling, automatic scaling, and service discovery.\nThe whitepaper provides two examples illustrating the relationship between Cloud Native AI and Cloud Native technologies, namely running AI on Cloud Native infrastructure:\nHugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure OpenAI Scaling Kubernetes to 7,500 nodes Challenges of Cloud Native AI Despite providing a solid foundation for AI applications, Cloud Native technologies still face challenges when integrating AI workloads with Cloud Native platforms. These challenges include the complexity of data preparation, resource requirements for model training, and maintaining the security and isolation of models in multi-tenant environments. Additionally, resource management and scheduling in Cloud Native environments are crucial, especially for large-scale AI applications, and further optimization is needed to support efficient model training and inference.\nDevelopment Path of Cloud Native AI The whitepaper proposes several development paths for Cloud Native AI, including improving resource scheduling algorithms to better support AI workloads, developing new service mesh technologies to enhance the performance and security of AI applications, and driving innovation and standardization of Cloud Native AI technology through open-source projects and community collaboration.\nCloud Native AI Technology Landscape Cloud Native AI involves a variety of technologies, from containers and microservices to service meshes and serverless computing. Kubernetes is a key platform for deploying and managing AI applications, while service mesh technologies like Istio and Envoy provide powerful traffic management and security features. Additionally, monitoring tools like Prometheus and Grafana are essential for maintaining the performance and reliability of AI applications.\nBelow is the Cloud Native AI landscape provided in the whitepaper.\nGeneral Orchestration Kubernetes Volcano Armada Kuberay Nvidia NeMo Yunikorn Kueue Flame Distributed Training Kubeflow Training Operator Pytorch DDP TensorFlow Distributed Open MPI DeepSpeed Megatron Horovod Apla … ML Serving Kserve Seldon VLLM TGT Skypilot … CI/CD - Delivery Kubeflow Pipelines Mlflow TFX BentoML MLRun … Data Science Jupyter Kubeflow Notebooks PyTorch TensorFlow Apache Zeppelin … Workload Observability Prometheus Influxdb Grafana Weights and Biases (wandb) OpenTelemetry … AutoML Hyperopt Optuna Kubeflow Katib NNI … Governance \u0026amp; Policy Kyverno Kyverno-JSON OPA/Gatekeeper StackRox Minder … Data Architecture ClickHouse Apache Pinot Apache Druid Cassandra ScyllaDB Hadoop HDFS Apache HBase Presto Trino Apache Spark Apache Flink Kafka Pulsar Fluid Memcached Redis Alluxio Apache Superset … Vector Databases Milvus Chroma Weaviate Quadrant Pinecone Extensions Redis Postgres SQL ElasticSearch … Model/LLM Observability Trulens Langfuse Deepchecks OpenLLMetry … Summary Finally, let me summarizes the following key points:\nRole of the Open Source Community: The whitepaper clearly points out the role of the open-source community in advancing Cloud Native AI, including accelerating innovation and reducing costs through open-source projects and extensive collaboration.\nImportance of Cloud Native Technologies: Cloud Native AI is built and deployed according to Cloud Native principles, highlighting the importance of repeatability and scalability. Cloud Native technologies provide an efficient development and runtime environment for AI applications, especially in terms of resource scheduling and service scalability.\nChallenges Exist: Despite the many advantages brought by Cloud Native AI, there are still challenges in data preparation, model training resource requirements, and model security and isolation.\nFuture Development Directions: The whitepaper proposes development paths including optimizing resource scheduling algorithms to support AI workloads, developing new service mesh technologies to enhance performance and security, and leveraging open-source projects and community collaboration to further promote technological innovation and standardization.\nKey Technological Components: Key technologies involved in Cloud Native AI include containers, microservices, service meshes, and serverless computing. Kubernetes plays a central role in deploying and managing AI applications, while service mesh technologies such as Istio and Envoy provide necessary traffic management and security.\nFor more details, please download the Cloud Native AI Whitepaper .\n","date":"2024-04-16T12:54:49+08:00","relpermalink":"/en/blog/cloud-native-ai-whitepaper/","section":"blog","summary":"During KubeCon EU 2024, CNCF released its first Cloud Native Artificial Intelligence (CNAI) whitepaper. This article provides an in-depth analysis of the content of this whitepaper.","title":"In-depth Analysis of CNCF's Cloud Native AI Whitepaper"},{"content":"This article will introduce Tetrate’s newly launched tool – Tetrate Vulnerability Scanner (TVS) , a CVE scanner customized for Istio and Envoy. Before diving into the specific features of TVS, let’s briefly review the concept of CVE and its core role in software security.\nOverview of CVE CVE (Common Vulnerabilities and Exposures) is a public vulnerability database maintained by MITRE Corporation. It provides a standardized way to reference vulnerabilities in open-source software. Each CVE record has an identifier, description, and at least one public reference. CVE does not provide severity ratings for vulnerabilities. CVEs are not only an important resource for cybersecurity professionals but also a tool for developers and organizations to receive critical security updates. CVE Numbering Authorities (CNAs) are an integral part of the CVE program, assigning unique IDs to new CVEs. These IDs help find information related to vulnerabilities, including severity ratings (maintained by NIST’s NVD), affected software systems, and steps for remediation and damage control. For example, the well-known Log4j vulnerability in 2021 (CVE-2021-44228 ) had a severity rating of 10 due to its wide impact.\nPractical Scenarios for CVEs Integrating CVE scanning into CI/CD pipelines is a common practice aimed at automatically identifying and preventing code with known vulnerabilities from entering the main branch. This approach helps ensure that applications do not rely on third-party packages or libraries with security vulnerabilities, enhancing application security. For example, GitHub’s Dependabot can automatically detect CVEs in project dependencies and suggest fixes, making it an effective tool for maintaining project security. You may receive CVE notifications like jQuery Cross-Site Scripting vulnerability whenever there is a vulnerability in your PR or commit. Then, you can choose to tolerate the vulnerability or apply a patch.\nWhat is TVS? Istio often releases CVE notices on its official website, such as ISTIO-SECURITY-2024-001 . Previously, you had to track these notices manually, but now you can automate CVE scanning tasks with TVS, significantly reducing the workload of security teams.\nThe following figure shows the result of TVS.\nFigure: TVS CLI Currently, TVS only provides a command-line tool, and it will be integrated as a service into TIS in the future.\nThe following diagram illustrates the workflow of TVS.\nFigure: TVS Workflow Begins after Istio installation. Collects SHA digests of installed Istio containers. Sends digests to Tetrate’s API. API detects CVE presence. If CVE is detected, logs SHA digests without personal information and notifies users. If no CVE is detected, no action is required. Upon receiving notification, users apply patches or mitigations. Process ends. TVS is available for free download and use by everyone. However, registration is required before performing CVE scans, as outlined in the TIS documentation .\nSupply Chain Security Recommendations A report indicates that even the latest versions of the most popular containers have hundreds of CVEs.\nHere are some recommendations for ensuring security:\nStart vulnerability scanning and remediation early, rather than waiting until the end. Integrate vulnerability scanning tools into CI/CD. Regularly update Istio and Envoy to the latest versions. Use Istio’s officially released distroless images or TID to reduce the attack surface and minimize vulnerabilities. Follow the best practices for the software supply chain introduced by CNCF. Follow Istio security best practices . Unique Value of TVS TVS provides a convenient CVE scanning operation through its command-line tool and is planned to be integrated into Kubernetes and Tetrate Istio Subscription (TIS) in the future to further simplify CVE management processes for Istio and Envoy. TIS provides CVE patches and backward compatibility support for up to 14 months from the Istio release, helping users get security updates on time while keeping the system stable.\nTVS is freely available for all users to download and use, with simple registration required before performing CVE scans. Additionally, you can register on the Istio and Envoy alerts and patches page to receive CVE notifications and patches as soon as they are available. For more information, refer to the TIS documentation .\nBy adopting TVS, an automated CVE scanning tool, enterprises can more effectively identify and address security vulnerabilities in Istio and Envoy, enhance infrastructure security, and reduce the burden on security teams, thereby promoting efficient security management processes.\nThis blog was initially published at tetrate.io .\n","date":"2024-04-12T12:54:49+08:00","relpermalink":"/en/blog/tetrate-vulnerability-scaner/","section":"blog","summary":"This article will introduce Tetrate’s newly launched tool – Tetrate Vulnerability Scanner (TVS), a CVE scanner customized for Istio and Envoy. ","title":"TVS: Istio and Envoy CVE Scanning Solution"},{"content":"Last week, I attended KubeCon EU 2024 in Paris, marking my first participation in KubeCon outside of China. The conference was unprecedentedly grand, reportedly attracting 12,000 attendees. In this article, I’ll share some observations from KubeCon, focusing mainly on the areas of service mesh and cloud-native infrastructure that caught my attention.\nFigure: Istio Contributors at the Istio booth during KubeCon EU Istio, Cilium, and Service Mesh Istio and Service Mesh were hot topics, showcasing the latest developments and applications of these technologies within the cloud-native ecosystem. The conference covered various areas, including infrastructure optimization, data locality, distributed tracing, and multi-cluster deployment, reflecting the widespread attention and continuous innovation in Service Mesh technology.\nData Locality and Global Request Routing Arthur Busser and Baudouin Herlicq from Pigment shared how Kubernetes and Istio can fulfill data locality requirements. They introduced methods using Istio for request routing based on custom headers, crucial for meeting data residency requirements such as GDPR and CCPA.\nDistributed Tracing and Enhanced Observability Chris Detsicas from ThousandEyes (part of Cisco) discussed configuring Istio with OpenTelemetry for effective distributed tracing, providing valuable visibility into microservices ecosystems for problem diagnosis and performance optimization.\nMulti-cluster Deployment and Traffic Management Haiwen Zhang and Yongxi Zhang from China Mobile introduced a new approach to simplify Istio multi-cluster deployment. This method utilizes a globally unique Istio control plane, achieving global service discovery through the main cluster’s Apiserver, automatically connecting the container networks of multiple clusters, and providing direct network connectivity for Pods. They highlighted the Kosmos project , offering a new solution to simplify service mesh deployment and management in multi-cluster environments.\nAmeer Abbas and John Howard from Google discussed building 99.99% reliability services with infrastructure reliability at 99.9%. They proposed a series of application architecture prototypes (Archetypes) to design and implement highly reliable multi-cluster applications.\nPrototype 1: Active-Passive Zones - Deploy all services in two zones of a single region, using read-only replicas of SQL databases, with fault tolerance within the zone achieved through L4 load balancers. Prototype 2: Multi-Zonal - Deploy all services in three zones of a single region, using highly available SQL databases, with fault tolerance within the zone achieved through global or zonal load balancers. Prototype 3: Active-Passive Region - Deploy all services in three zones of two regions, using SQL databases replicated across regions, with fault tolerance between regions achieved through DNS and load balancers. Prototype 4: Isolated Regions - Deploy all services in three zones of two regions, using global databases like Spanner or CockroachDB, with fault tolerance between regions achieved through zonal load balancers and DNS. Prototype 5: Global - Deploy all services in three zones of two or more regions, using global databases like Spanner or CockroachDB, with fault tolerance achieved globally through global load balancers. Security and Zero Trust Architecture Several sessions emphasized the importance of securing Istio in production environments. Discussions led by Microsoft’s Niranjan Shankar focused on steps and strategies to reinforce Istio’s security using network policies, third-party Kubernetes tools, and cloud-provided security services to build zero trust and defense-in-depth architectures.\nInfrastructure Compatibility and Future of Ambient Mesh Benjamin Leggett and Yuval Kohavi introduced an innovative approach to enable Istio’s Ambient mode to support any Kubernetes CNI, detailed in the Istio blog . This advancement addresses the limited CNI support in Ambient mesh, allowing applications to be included in Ambient mode without restarting Pods, thus simplifying operations and reducing infrastructure costs.\nThe Istio community announced that Ambient mode will become beta in the upcoming Istio 1.22 release, as described in the CNCF blog . Multiple presentations and discussions focused on the future of Istio Ambient Mesh, especially its potential to simplify workload operations and reduce infrastructure costs. The introduction of Istio Ambient Mesh signals a new direction for service mesh technology, offering a data plane architecture without sidecars, providing higher performance and lower resource consumption.\nInnovation in Sidecar-less Service Mesh Discussions at KubeCon EU 2024 evaluated and compared the pros and cons of using sidecar-based and sidecar-less (such as Istio’s Ambient Mesh) service mesh models. Christian Posta’s in-depth analysis of design decisions and trade-offs between Cilium and Istio in implementing sidecar-less service mesh highlighted the potential of this model in improving performance, reducing resource consumption, and simplifying operational tasks. By analyzing the transition from Istio to Cilium at The New York Times, it further demonstrated the effectiveness of the sidecar-less model in handling complex, multi-region service meshes, while also pointing out challenges and implementation considerations in this transition. These discussions foreshadowed the potential evolution of service mesh technology towards more flexible and efficient architectures, where the sidecar-less approach may become a key strategy for optimizing the performance and resource utilization of cloud-native applications.\nIntersection of Cilium and Service Mesh Cilium was widely discussed at KubeCon EU 2024. As a technology based on eBPF, Cilium is not only seen as an efficient Container Network Interface (CNI) but also demonstrates strong potential in the service mesh domain. Through presentations by Isovalent and other organizations, Cilium was showcased as an advanced solution for connecting, observing, and securing service mesh. In particular, Cilium’s sidecar-less service mesh implementation was considered a future direction, leveraging eBPF technology to achieve secure communication and fine-grained traffic management among microservices without adding the burden of traditional sidecar proxies. Additionally, Cilium’s extensibility beyond service mesh, such as in multi-cloud networking and load balancing, highlights its position as a core component of the cloud-native ecosystem. These discussions and case studies of Cilium demonstrate its significant role in driving innovation in service mesh and cloud-native technologies.\nCloud Native Trends Several major trends are currently shaping the cloud-native landscape:\nEnhanced Sustainability and Environmental Awareness: For example, Deutsche Bahn is incorporating developers into its infrastructure greening process, highlighting a growing consideration of environmental factors in the design and operation of cloud-native solutions. This reflects a trend where companies are increasingly looking at reducing their environmental impact while pursuing technological advancements, achieving sustainable technology ecosystems through green computing and energy efficiency optimizations.\nIntegration of Artificial Intelligence with Cloud-Native Technologies: Artificial intelligence (AI) is emerging as the next major challenge for Kubernetes and the cloud-native ecosystem. Discussions by Nvidia on AI strategies, CNCF’s efforts to standardize AI in cloud-native futures, and various updates on tools and platforms integrating AI and machine learning (ML) underscore this trend. This trend indicates that seamlessly integrating AI and ML into cloud-native architectures can not only accelerate application development and deployment but also provide more intelligent and automated operational capabilities. CNCF has also announced the establishment of an AI Working Group and released an AI Whitepaper .\nRise of WebAssembly (Wasm): Support for the latest Wasm standards by Cosmonic, along with Fermyon’s donation of its open-source Wasm platform, SpinKube , to CNCF, demonstrates the growing importance of WebAssembly in cloud-native application development. Wasm provides an efficient and secure way to run client and server-side code outside the browser, which is crucial for building cross-platform, high-performance cloud-native applications.\nEnhanced Cloud-Native Observability: For instance, New Relic’s addition of native Kubernetes support to its observability platform highlights the increasing demand for monitoring, logging, and performance analysis of cloud-native applications. As the complexity of cloud-native architectures increases, enterprises need more robust tools to maintain system transparency and health, thereby optimizing performance and reliability.\nStrengthening Collaboration and Open Source Spirit in the Cloud Native Community: Initiatives such as CNCF’s establishment of the End User Technical Advisory Board , and collaboration between Red Hat and Docker to develop the Testcontainers Cloud framework, reflect the cloud-native community’s commitment to fostering a culture of collaboration and sharing. This open collaboration not only accelerates the development and adoption of new technologies but also provides a solid foundation for the healthy growth of the cloud-native ecosystem.\nThese trends collectively depict a diverse, continuously innovative, and increasingly mature cloud-native technology landscape, where sustainability, AI/ML integration, WebAssembly, enhanced observability, and community collaboration are key drivers of progress in this field.\nConclusion Insights from KubeCon EU 2024 have revealed several significant advancements and future directions in the cloud-native technology domain. From ongoing innovations in service meshes to the growing emphasis of the cloud-native ecosystem on environmental sustainability, and the …","date":"2024-03-27T16:54:49+08:00","relpermalink":"/en/blog/kubecon-eu-paris-recap/","section":"blog","summary":"Explore KubeCon EU 2024: From the latest developments in Istio and Cilium to an in-depth interpretation of cloud native trends such as AI convergence, the rise of Wasm, and enhanced observation.","title":"KubeCon EU 2024: Impressions and Recap from Paris"},{"content":"Changes and Reminders for ICA Certification Upcoming policy change: Please note that the ICA certification expiration policy will change on April 1, 2024, at 00:00UTC. Certifications obtained on or after this date will expire 24 months after meeting the certification requirements (including passing the exam). We encourage anyone interested and prepared to schedule and take the exam before the policy change. Please see more detailed information here .\nThe current certification validity period is 3 years, and certifications obtained after April 1st will be valid for 2 years.\nBackground on Tetrate Academy Tetrate Academy , operated by Tetrate, has been around for several years, launching Istio fundamentals tutorials, Envoy fundamentals tutorials, and CIAT certification exams, with over 13,000 people having taken Tetrate Academy courses. In September last year, Tetrate contributed CIAT to CNCF, renamed it ICA exam , and the certification exam was officially launched in November.\nFigure - ICA domains by Linux Foundation Exam Background ICA is derived from the CIAT exam contributed by Tetrate, providing validation of extensive knowledge and skills in Istio. The exam is conducted remotely online, with a time limit of 2 hours, during which candidates need to operate Kubernetes clusters and Istio in a virtual machine environment. The exam includes a series of problem-solving questions and some multiple-choice questions, requiring a passing score of 75%. The PSI system provides an online exam environment, and exam results will be emailed to candidates within 24 hours after the exam. The exam environment provides tools such as Kubernetes clusters, Istio installation, VS Code, kubectl, istioctl, etc. Access to Istio documentation is available during the exam, and questions and responses are in English only. Recommendations and Reminders Familiarize yourself with the Istio documentation before the exam, and prepare in advance, including checking the PSI system, identification, and exam environment. Take Tetrate’s free Istio fundamentals tutorial to deepen your understanding of Istio. Stay calm during the exam, answer familiar questions first, then unfamiliar ones, to ensure efficient completion of the exam. Reference Links ICA Certification FAQs Important Instructions for ICA Certification ","date":"2024-03-07T11:27:49+08:00","relpermalink":"/en/blog/ica-certificate/","section":"blog","summary":"Learn about ICA certification's origin, changes, exam prep. Evolved from CIAT, it assesses Istio skills. Familiarize with Istio docs and take Tetrate's tutorial for better prep.","title":"ICA Certification: Latest Changes and Exam Preparation Guide for Istio Skills"},{"content":"I am excited to announce that I will be attending this year’s KubeCon \u0026amp; CloudNativeCon event, and it will be my first time having the opportunity to attend KubeCon in Europe. I am greatly looking forward to meeting everyone in Paris and sharing insights and experiences about cloud-native technologies. In addition to participating in the main event, I will also be attending Istio Day and will be available at Tetrate’s booth (J14) to meet with everyone. I hope to spend this exciting time with you all and welcome you to come and chat with me!\n","date":"2024-03-06T08:27:49+08:00","relpermalink":"/en/notice/kubecon-eu-2024/","section":"notice","summary":"Hope to see and talk with you there.","title":"See you in KubeCon Paris!"},{"content":"This blog post analyzes the challenges of server-side obtaining the client source IP in the Istio service mesh and provides solutions. The following topics will be covered:\nReasons for the loss of source IP during packet transmission. How to identify the client source IP. Strategies for passing source IP in north-south and east-west requests. Handling methods for HTTP and TCP protocols. The Importance of Preserving Source IP The main reasons for preserving the client source IP include:\nAccess Control Policies: Performing authentication or security policies based on source IP. Load Balancing: Implementing request routing based on the client IP. Data Analysis: Access logs and monitoring metrics containing the actual source address, aiding developers in analysis. Meaning of Preserving Source IP Preserving the source IP refers to avoiding the situation where the actual client source IP is replaced as the request goes out from the client, and passes through a load balancer or reverse proxy.\nHere is an example process of source IP address lost:\nFigure: Mermaid Diagram The above diagram represents the most common scenario. This article considers the following cases:\nNorth-South Traffic: Clients accessing servers through a load balancer (gateway) Single-tier gateway Multi-tier gateways East-West Traffic: Service-to-service communication within the mesh Protocols: HTTP and TCP How to Confirm Client Source IP? In the Istio service mesh, Envoy proxies typically add the client IP to the “X-Forwarded-For” header of HTTP requests. Here are the steps to confirm the client IP:\nCheck the X-Forwarded-For Header: It contains the IP addresses of various proxies along the request path. Select the Last IP: Usually, the last IP is the client IP closest to the server. Verify the IP’s Trustworthiness: Check the trustworthiness of the proxy servers. Use X-Envoy-External-Address: Envoy can set this header, which includes the real client IP. For more details, refer to the Envoy documentation on the x-forwarded-for header . For TCP/IP connections, you can parse the client IP from the protocol field.\nTesting Environment GKE\nClient Version: v1.28.4 Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 Server Version: v1.27.7-gke.1121000 Istio\nClient version: 1.20.1 Control plane version: 1.20.1 Data plane version: 1.20.1 (12 proxies) CNI\nWe use Cilium CNI but have not enabled the kube-proxy-less mode.\ncilium-cli: v0.15.18 compiled with go1.21.5 on darwin/amd64 cilium image (default): v1.14.4 cilium image (stable): unknown cilium image (running): 1.14.5 Node\nNode Name Internal IP Remarks gke-cluster1-default-pool-5e4152ba-t5h3 10.128.0.53 gke-cluster1-default-pool-5e4152ba-ubc9 10.128.0.52 gke-cluster1-default-pool-5e4152ba-yzbg 10.128.0.54 Ingress Gateway Pod Node Public IP of the local client computer used for testing: 123.120.247.15\nDeploying Test Example The following diagram illustrates the testing approach:\nFigure: Mermaid Diagram First, deploy Istio according to the Istio documentation , and then enable sidecar auto-injection for the default namespace:\nkubectl label namespace default istio-injection=enabled Deploy the echo-server application in Istio:\nkubectl create deployment echo-server --image=registry.k8s.io/echoserver:1.4 kubectl expose deployment echo-server --name=clusterip --port=80 --target-port=8080 Create an Ingress Gateway:\ncat \u0026gt; config.yaml \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: clusterip-gateway spec: selector: istio: ingressgateway # Choose the appropriate selector for your environment servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;clusterip.jimmysong.io\u0026#34; # Replace with the desired hostname --- apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: clusterip-virtualservice spec: hosts: - \u0026#34;clusterip.jimmysong.io\u0026#34; # Replace with the same hostname as in the Gateway gateways: - clusterip-gateway # Use the name of the Gateway here http: - route: - destination: host: clusterip.default.svc.cluster.local # Replace with the actual hostname of your Service port: number: 80 # Port of the Service EOF kubectl apply -f config.yaml View the Envoy logs in the Ingress Gateway:\nkubectl logs -f deployment/istio-ingressgateway -n istio-system View the Envoy logs in the Sleep Pod:\nkubectl logs -f deployment/sleep -n default -c istio-proxy View the Envoy logs in the Echo Server:\nkubectl logs -f deployment/echo-server -n default -c istio-proxy Get the public IP of the gateway:\nexport GATEWAY_IP=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) Test locally using curl:\ncurl -H \u0026#34;Host: clusterip.jimmysong.io\u0026#34; $GATEWAY_IP Resource IP After deploying the test application, you need to obtain the IP addresses of the following resources, which will be used in the upcoming experiments.\nPod\nHere are the initial Pod IPs, but please note that as patches are applied to the Deployment, Pods may be recreated, and their names and IP addresses may change.\nPod Name Pod IP echo-server-6d9f5d97d7-fznrq 10.32.1.205 sleep-9454cc476-2dskx 10.32.3.202 istio-ingressgateway-6c96bdcd74-zh46d 10.32.1.221 Service\nService Name Cluster IP External IP clusterip 10.36.8.86 - sleep 10.36.14.12 - istio-ingressgateway 10.36.4.127 35.188.212.88 North-South Traffic Let’s first consider the scenario where the client is outside the Kubernetes cluster and accesses internal services through a load balancer.\nTest 1: Cluster Traffic Policy, iptables Traffic Hijacking This is the default situation after deploying the test application using the steps above, and it represents the commonly encountered scenario where the source IP address is said to be lost.\ncurl test:\ncurl -H \u0026#34;Host: clusterip.jimmysong.io\u0026#34; $GATEWAY_IP View Results 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 CLIENT VALUES: client_address=127.0.0.6 command=GET real path=/ query=nil request_version=1.1 request_uri=http://clusterip.jimmysong.io:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=clusterip.jimmysong.io user-agent=curl/8.4.0 x-b3-parentspanid=03c124c5f910001a x-b3-sampled=1 x-b3-spanid=103dc912ec14f3b4 x-b3-traceid=140ffa034822077f03c124c5f910001a x-envoy-attempt-count=1 x-envoy-internal=true x-forwarded-client-cert=By=spiffe://cluster.local/ns/default/sa/default;Hash=79253e34e1c28d389e9bfb1a62ffe8944b2c3c369b46bf4a9faf055b55dedb7f;Subject=\u0026#34;\u0026#34;;URI=spiffe://cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account x-forwarded-for=10.128.0.54 x-forwarded-proto=http x-request-id=b3c05e22-594e-98da-ab23-da711a8f53ec BODY: -no body in request- You only need to focus on the client_address and x-forwarded-for results. Other information in the curl test results will be omitted in the following curl test results.\nExplanation Meaning of fields in the results:\nclient_address: The client IP address obtained through TCP/IP protocol resolution, referred to as the remote address in Envoy. x-forwarded-for: x-forwarded-for (XFF) is a standard proxy header used to indicate the IP addresses that the request has passed through from the client to the server. A compliant proxy will add the IP address of the most recent client to the XFF list before proxying the request. See Envoy documentation for details. From the test results, we can see that the source IP address becomes the IP address of the Ingress Gateway Pod’s node (10.128.0.54).\nThe following diagram shows the packet flow paths between the two Pods.\nFigure: Mermaid Diagram For this scenario, preserving the source IP is straightforward and is also a standard option provided by Kubernetes.\nHow is the Source IP Lost? The following diagram shows how the source IP of the client is lost during the request process.\nFigure: Mermaid Diagram Because the load balancer sends packets to any node in the Kubernetes cluster, SNAT is performed during this process, resulting in the loss of the client’s source IP when it reaches the Server Pod.\nHow to Preserve the Client Source IP You can control the load balancer to preserve the source IP by setting the externalTrafficPolicy field in the service to Local.\nexternalTrafficPolicy\nexternalTrafficPolicy is a standard Service option that defines whether incoming traffic to Kubernetes nodes is load-balanced and how it’s load-balanced. Cluster is the default policy, but Local is typically used to preserve the source IP of incoming traffic to cluster nodes. Local effectively disables load balancing on the cluster nodes so that traffic received by local Pods sees the source IP address.\nFigure: Mermaid Diagram In other words, setting externalTrafficPolicy to Local allows packets to bypass kube-proxy on the nodes and reach the target Pod directly. However, most people do not set externalTrafficPolicy when creating a Service in Kubernetes, so the default Cluster policy is used.\nSince using the Local external traffic policy in Service can preserve the client’s source IP address, why isn’t it the default in Kubernetes?\nThe default setting of Kubernetes Service’s externalTrafficPolicy to Cluster instead of Local is primarily based on the following considerations:\nLoad Balancing: Ensures even distribution of traffic across all nodes, preventing overload on a single node. High Availability: Allows traffic to be received by any node in the cluster, enhancing service availability. Simplified Configuration: The Cluster mode reduces the complexity of network configurations. Performance Optimization: Avoids potential performance issues caused by preserving the client’s source IP. Universality: Compatible with a variety of network environments and cluster configurations, suitable for a broader range of scenarios. Test 2: Local Traffic Policy, iptables Traffic Hijacking Set the Ingress Gateway Service to use the Local external traffic policy:\nkubectl patch svc istio-ingressgateway -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;externalTrafficPolicy\u0026#34;:\u0026#34;Local\u0026#34;}}\u0026#39; -n istio-system Curl …","date":"2024-02-28T11:27:49+08:00","relpermalink":"/en/blog/preserve-source-ip-in-istio/","section":"blog","summary":"This article focuses on how to maintain transparency of the client source IP in the Istio service mesh.","title":"Maintaining Traffic Transparency: Preserving Client Source IP in Istio"},{"content":"Introduction I often answer questions on Istio’s GitHub Discussions , and recently, I came across a discussion about Istio’s primary-remote deployment, specifically regarding how the remote cluster’s gateway initially authenticates to an external Istiod instance. This issue touches upon the core security mechanisms of service meshes in multi-cluster configurations, which I think merits more in-depth sharing in the community.\nIn the official Istio documentation on Installing Primary-Remote on different networks , one of the steps is to attach cluster2 as a remote cluster of cluster1 . This process creates a Secret containing a kubeconfig configuration, which includes the certificates and tokens required to access the remote cluster (cluster2).\n# This file is autogenerated, do not edit. apiVersion: v1 kind: Secret metadata: annotations: networking.istio.io/cluster: cluster2 creationTimestamp: null labels: istio/multiCluster: \u0026#34;true\u0026#34; name: istio-remote-secret-cluster2 namespace: istio-system stringData: cluster2: | apiVersion: v1 clusters: - cluster: certificate-authority-data: {CERTIFICATE} server: {CLUSTER2-APISERVER-ADDRESS} name: cluster2 contexts: - context: cluster: cluster2 user: cluster2 name: cluster2 current-context: cluster2 kind: Config preferences: {} users: - name: cluster2 user: token: {TOKEN} The key role of this Secret is to enable Istio in the primary cluster (cluster1) to access the API server of the remote cluster, thereby obtaining service information. Additionally, in the remote cluster (cluster2), the Istiod service points to the primary cluster’s Istiod service’s LoadBalancer IP (ports 15012 and 15017), allowing cluster2 to communicate with the primary cluster’s Istiod.\nVisualizing Authentication Since both clusters share a CA (provided by the primary cluster) and the remote cluster can access its own API server, the Istiod in the primary cluster can validate requests from the remote cluster (cluster2). The following sequence diagram clearly shows this process:\nFigure: Mermaid Diagram This process is a key component of Istio’s multi-cluster configuration, ensuring secure cross-cluster communication within the service mesh. As seen in this discussion, both the remote gateway and the services depend on the primary cluster’s CA for initial mTLS authentication, providing a solid foundation for secure communication across the entire service mesh.\nConclusion In this blog, we explored how the gateway in a remote cluster initially authenticates to an external Istiod in Istio’s primary-remote deployment. We explained how to create a Secret containing a kubeconfig to allow Istio in the primary cluster to access the remote cluster’s API and how shared CA and service account tokens ensure the security of mTLS authentication. This process secures cross-cluster communication within the service mesh, providing key insights for understanding and implementing Istio’s multi-cluster configuration.\nThis blog was initially published at tetrate.io .\n","date":"2024-01-16T11:27:49+08:00","relpermalink":"/en/blog/primary-remote-istio-ingress-gateway-mtls/","section":"blog","summary":"This blog deeply analyzes the initial authentication and mTLS connection process of remote gateways in Istio's primary-remote deployment mode.","title":"Deciphering Istio Multi-Cluster Authentication \u0026 mTLS Connection"},{"content":"In this blog, I’ll guide you through installing Tetrate Istio Subscription (TIS) and activating its monitoring add-on.\nUnderstanding Tetrate Istio Subscription Tetrate Istio Subscription is a comprehensive, enterprise-grade product offered by Tetrate. It provides thoroughly tested Istio versions compatible with all major cloud platforms. Derived from the open-source Tetrate Istio Distro project, TIS adds extensive support to these builds, including optional FIPS-validated cryptographic modules and a range of tested add-ons and integrations.\nWhy Choose TIS? TIS is not a fork but an upstream distribution of Istio tailored for specific environments. Enhancements made to Istio are integrated upstream. Key benefits of TIS include:\nExtended Support: TIS offers 14 months of security update support. Commercial Support: TIS provides business support options for enterprise use cases, including compliance needs. Ease of Management: TIS simplifies installation and management processes. Multi-Environment Adaptability: TIS supports various cloud environments. FIPS Validation: TIS offers FIPS-validated versions for high-security requirements. Visit the TIS docs for more information › Pre-Installation Requirements Before installing TIS and its plugins, you’ll need:\nTerraform for importing dashboards into Grafana. Credentials from Tetrate for installing TIS. tis_username tis_password Installing Istio and Monitoring Addons First, check the supported Istio versions with TIS:\nhelm search repo tetratelabs/base --versions NAME CHART VERSION APP VERSION DESCRIPTION tetratelabs/base1.20.1+tetrate01.20.1-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.20.0+tetrate01.20.0-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.19.5+tetrate01.19.5-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.19.4+tetrate01.19.4-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.19.3+tetrate01.19.3-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.18.6+tetrate01.18.6-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.18.5+tetrate01.18.5-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.18.3+tetrate01.18.3-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.17.8+tetrate01.17.8-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.17.6+tetrate01.17.6-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.16.7+tetrate01.16.7-tetrate0Helm chart for deploying Istio cluster resource... tetratelabs/base1.16.6+tetrate01.16.6-tetrate0Helm chart for deploying Istio cluster resource... We’ll install the latest Istio version, 1.20.1.\nexport TIS_USER=\u0026#34;\u0026lt;tis_username\u0026gt;\u0026#34; export TIS_PASS=\u0026#34;\u0026lt;tis_password\u0026gt;\u0026#34; # Helm chart version export VERSION=1.20.1+tetrate0 # Image tag export TAG=1.20.1-tetrate0 kubectl create namespace istio-system kubectl create secret docker-registry tetrate-tis-creds \\ --docker-server=\u0026#34;addon-containers.istio.tetratelabs.com\u0026#34; \\ --docker-username=${TIS_USER} \\ --docker-password=${TIS_PASS} \\ -n istio-system # Install Istio helm install istio-base tetratelabs/base -n istio-system \\ --set global.tag=${TAG} \\ --set global.hub=\u0026#34;addon-containers.istio.tetratelabs.com\u0026#34; \\ --set \u0026#34;global.imagePullSecrets[0]=tetrate-tis-creds\u0026#34; \\ --version ${VERSION} helm install istiod tetratelabs/istiod -n istio-system \\ --set global.tag=${TAG} \\ --set global.hub=\u0026#34;addon-containers.istio.tetratelabs.com\u0026#34; \\ --set \u0026#34;global.imagePullSecrets[0]=tetrate-tis-creds\u0026#34; \\ --version ${VERSION} \\ --wait # install ingress Gateway kubectl create namespace istio-ingress kubectl create secret docker-registry tetrate-tis-creds \\ --docker-server=\u0026#34;addon-containers.istio.tetratelabs.com\u0026#34; \\ --docker-username=${TIS_USER} \\ --docker-password=${TIS_PASS} \\ -n istio-ingress helm install istio-ingress tetratelabs/istio-ingress -n istio-ingress \\ --set global.tag=${TAG} \\ --set global.hub=\u0026#34;addon-containers.istio.tetratelabs.com\u0026#34; \\ --set \u0026#34;global.imagePullSecrets[0]=tetrate-tis-creds\u0026#34; \\ --version ${VERSION} \\ --wait # Install TIS addon helm install istio-monitoring-demo tis-addons/istio-monitoring-demo --namespace tis --create-namespace Port forward the Grafana service and open it in your local browser: http://localhost:3000 :\nkubectl port-forward --namespace tis svc/grafana 3000:3000 Note: Keep the port-forwarding command running, as we’ll need access to this port for importing the dashboard into Grafana.\nWhy TIS Monitoring Addon? The Tetrate Istio Subscription (TIS) Monitoring Addon stands out by offering a tailored and advanced monitoring experience compared to standard Istio dashboards. It focuses on modern needs with four specialized dashboards, replacing outdated elements with relevant metrics. Key enhancements include a unified panel design, detailed service insights, and critical metrics like sidecar performance. Reserved for TIS customers, these dashboards reflect Tetrate’s commitment to providing value beyond basic support, ensuring a personalized and evolving monitoring solution. Choosing TIS Monitoring Addon means accessing a service that not only meets current needs but also adapts to future demands.\nInstalling Istio Monitoring Addons After logging in with the default user name and password admin/admin, select Administration-Service accounts in the left navigation bar, and follow the instructions in the Grafana documentation to create a Service account with admin privilege.\nFigure: Grafana Service Account Create a Service account for Grafana then use Terraform to import dashboards into Grafana:\ncat\u0026gt;~/.terraformrc\u0026lt;\u0026lt;EOF credentials \u0026#34;terraform.cloudsmith.io\u0026#34; { token = \u0026#34;tetrate/tis-containers/\u0026lt;tis_password\u0026gt;\u0026#34; } EOF # Create a terraform module file cat\u0026gt;istio-monitoring-grafana.tf\u0026lt;\u0026lt;EOF module \u0026#34;istio_monitoring_grafana\u0026#34; { source = \u0026#34;terraform.cloudsmith.io/tis-containers/istio-monitoring-grafana/tetrate\u0026#34; version = \u0026#34;v0.2.0\u0026#34; gf_url = \u0026#34;http://localhost:3000\u0026#34; gf_auth = \u0026#34;\u0026lt;grafana_service_account_token\u0026gt;\u0026#34; } EOF # Run the commands terraform init terraform plan terraform apply -auto-approve Congratulations, you have successfully imported four dashboards into Grafana:\nIstio Workload Dashboard Istio Service Dashboard Istio Wasm Extension Dashboard Istio Control Plan Dashboard However, some dashboards might not have data yet. Let’s generate some traffic in the mesh.\nTesting the Monitoring Deploy the Bookinfo application and ingress gateway:\nkubectl create secret docker-registry tetrate-tis-creds \\ --docker-server=\u0026#34;addon-containers.istio.tetratelabs.com\u0026#34; \\ --docker-username=${TIS_USER} \\ --docker-password=${TIS_PASS} \\ -n default kubectl label namespace default istio-injection=enabled kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.20/samples/bookinfo/platform/kube/bookinfo.yaml -n default kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.20/samples/bookinfo/platform/kube/bookinfo-gateway.yaml -n default Retrieve the ingress gateway IP and generate traffic:\nexport GATEWAY_IP=$(kubectl -n istio-ingress get service istio-ingressgateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) for i in $(seq 1 100);do curl http://$GATEWAY_IP/productpage ; sleep 3;done Now, when you visit the Grafana dashboard, you will see monitoring data.\nFigure: Grafana Dashboard In addition, while importing these dashboards, we also imported the following alert rules:\nFigure: Altering Rules You can also define alert rules in Grafana, such as integrating Telegram and Slack to send notifications.\nCleanup Run the following command to clean up the Bookinfo app and TIS:\nkubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml kubectl delete -f samples/bookinfo/networking/bookinfo-gateway.yaml helm uninstall istio-ingress -n istio-ingress helm uninstall istio-monitoring-demo -n tis helm uninstall istiod -n istio-system helm uninstall istio-base -n istio-system kubectl delete namespace tis kubectl delete namespace istio-ingress kubectl delete namespace istio-system Conclusion By following these steps, you have successfully set up and tested monitoring in Istio using TIS. Enjoy the insights and advantages of enhanced monitoring in your Istio environment!\nThis blog was initially published at tetrate.io .\n","date":"2024-01-10T11:27:49+08:00","relpermalink":"/en/blog/enhancing-istio-with-tis-comprehensive-installation-and-monitoring-guide/","section":"blog","summary":"In this blog, I’ll guide you through installing Tetrate Istio Subscription (TIS) and activating its monitoring add-on.","title":"Enhancing Istio with TIS: Comprehensive Installation and Monitoring Guide"},{"content":"Experience it now: Istio Advisor Plus GPT Note: A ChatGPT Plus subscription is required.\nIntroduction Introducing Istio Advisor Plus GPT, a powerful integration of ChatGPT with Istio Advisor. This fusion of AI and Istio expertise offers many capabilities to enhance your Istio experience.\nWhat Can You Achieve with Istio Advisor Plus GPT? Istio Advisor Plus GPT empowers you with:\nComprehensive Istio Insights: Gain an in-depth understanding of Istio, covering core concepts, configurations, and best practices. Configuration Assistance: Get tailored guidance on configuring Istio components to suit your specific use cases. Troubleshooting Expertise: Quickly diagnose and resolve Istio issues with expert insights and recommendations. Performance Optimization: Optimize your Istio service mesh for efficiency and scalability. Security Enhancement: Implement robust security measures using Istio’s security features in your service mesh. Visual Representation: Visualize complex networking and service mesh processes with interactive diagrams. Upgrading Support: Plan and execute Istio upgrades seamlessly with expert advice. Bug Reporting Assistance: Compile detailed bug reports for effective issue resolution. Documentation Guidance: Access relevant Istio documentation sections for deeper insights. Ecosystem Integration: Explore the integration of Istio with key ecosystem tools. A Knowledge Repository at Your Disposal Istio Advisor Plus GPT offers an extensive knowledge base:\nIstio and Envoy Fundamentals: Delve into Istio and Envoy’s core principles, architecture, and components. Service Mesh Concepts: Understand the foundational concepts and advantages of service meshes. Tetrate Documentation: Learn about Tetrate’s products, including Tetrate Service Bridge (TSB). Zero-Trust Security: Master the principles of zero-trust security in the service mesh context. Real-World Use Cases: Explore real-world applications and case studies showcasing Istio’s impact. The Latest Open-Source Projects’ Documents: Istio, Envoy Gateway, Kubernetes Gateway API, Cilium, Envoy, Tetragon, SkyWalking Structured and Comprehensive Responses Our responses are structured for clarity and comprehensiveness:\nTLDR: A concise summary of key points. Explanation: Broader context or background information. Detailed Answer: In-depth analysis or step-by-step guidance. Example: Practical illustrations of concepts. Next Steps: Actionable advice for implementation. References: Access to relevant sources. Unlock the full potential of Istio Advisor GPT with ChatGPT integration and revolutionize your Istio journey.\n","date":"2023-12-25T16:27:49+08:00","relpermalink":"/en/blog/introducing-istio-advisor-plus-gpt/","section":"blog","summary":"Unlock the Power of Istio Advisor GPT with ChatGPT Integration.","title":"Introducing Istio Advisor Plus GPT"},{"content":"I’m delighted to present Istio’s most recent release—Istio 1.19 . This blog will provide an overview of the updates bundled in this release.\nGateway API: Revolutionizing Service Mesh Our previous blog highlighted the Gateway API’s potential to harmonize ingress gateways in Kubernetes and service mesh, opening doors to cross-namespace traffic support. With Istio’s official endorsement, the Gateway API takes center stage. While traditionally applied to north-south traffic (the ingress and egress of the mesh), it now extends its prowess to the realm of east-west traffic, the lifeblood within the mesh.\nIn Kubernetes, services wear multiple hats, handling tasks from service discovery and DNS to workload selection, routing and load balancing. Yet, control over these functions has been limited, with workload selection being the notable exception. The Gateway API changes the game, putting you in command of service routing. This introduces some overlap with Istio’s VirtualService, as both wield influence over traffic routing. Here’s a glimpse into three scenarios:\nInternal Kubernetes Requests: Without Istio, all internal traffic in Kubernetes takes the service route. North-South Traffic: By applying the Gateway API to the Ingress gateway, incoming traffic to Kubernetes follows xRoute (currently supporting HTTPRoute, TCPRoute and gRPCRoute) to services. East-West Traffic: Inside Istio, as traffic enters the data plane, xRoute of the Gateway API takes charge. It guides the traffic to either the original or a new destination service. Figure: Figure 1: Traffic routing. This dynamic fusion of the Gateway API with Istio not only refines service networking but also solidifies Istio’s significance in the Kubernetes ecosystem.\nGateway API for Service Mesh: A Deeper Dive At its current experimental stage (as of v0.8.0), the Gateway API for Service Mesh introduces a fresh approach to configuring service mesh support in Kubernetes. It directly links individual route resources (such as HTTPRoute) with Service resources, streamlining the configuration process.\nHere are some key takeaways:\nExperimental Stage: As of version v0.8.0, the Gateway API for Service Mesh is still experimental. It’s advised not to use it in production environments.\nService and Route Association: Unlike using Gateway and GatewayClass resources, individual route resources are linked directly with Service resources when configuring a service mesh.\nFrontend and Backend Aspects of Service: The Service’s frontend encompasses its name and cluster IP, while the backend consists of its collection of endpoint IPs. This distinction facilitates routing within a mesh without introducing redundant resources.\nRoute Attachment to Service: Routes are attached to a Service to apply configuration to any traffic directed to that Service. The traffic follows the mesh’s default behavior if no Routes are attached.\nNamespace Relationships:\nSame Namespace: A Route in the same Namespace as its Service, known as a producer route, is typically created by the workload creator to define acceptable usage. It affects all requests from any client of the workload across any Namespace. Different Namespaces: A Route in a different Namespace than its Service, termed a consumer route, refines how a consumer of a given workload makes requests. This Route only influences requests from workloads in the same Namespace as the Route. Figure: Figure 2: Producer Route and Consumer Route. Combining Routes: Multiple Routes for the same Service in a single Namespace, whether producer or consumer routes, will be merged according to the Gateway API Route merging rules. This means defining distinct consumer routes for multiple consumers in the same Namespace is impossible.\nRequest Flow:\nA client workload initiates a request for a specific Service in a Namespace. The mesh data plane intercepts the request and identifies the target Service. Based on associated Routes, the request is allowed, rejected, or forwarded to the appropriate workload based on matching rules. Bear in mind that, in the experimental stage, the Gateway API for Service Mesh may undergo further changes and is not recommended for production use.\nBut wait, there’s more! Our journey doesn’t end here – the support for ingress traffic using the API is rapidly heading toward General Availability, promising even more dynamic developments!\nLet’s delve further into additional enhancements in this release.\nAmbient Mesh Enhancements The Istio team has been tirelessly refining the ambient mesh, an innovative deployment model that offers an alternative to the traditional sidecar approach. If you haven’t explored ambient yet, now’s the perfect time to dive into the introduction blog post .\nWith this update, we’ve amplified support for ServiceEntry, WorkloadEntry, PeerAuthentication and DNS proxying. Alongside, bug fixes and reliability enhancements ensure a seamless experience.\nRemember, the ambient mesh is in its alpha phase for this release. The Istio community eagerly awaits your feedback to propel it toward Beta.\nSimplified Virtual Machine and Multicluster Experiences Simplicity is key, especially when working with Virtual Machines and Multicluster setups. In this release, we’ve made the address field optional in the WorkloadEntry resources. This seemingly small adjustment promises to streamline your workflow significantly.\nElevated Security Configurations You can now configure OPTIONAL_MUTUAL for your Istio ingress gateway’s TLS settings, providing the flexibility of optional client certificate validation. Additionally, you can fine-tune your preferred cipher suites used for non-Istio mTLS traffic via MeshConfig.\nWith these updates, Istio 1.19 empowers you with greater control, flexibility and security in managing your service mesh.\nFeel free to explore these enhancements and share your experiences with the Istio community. For more details, refer to the official release notes .\nHappy meshing!\nThis blog was initially published at tetrate.io .\n","date":"2023-11-09T16:27:49+08:00","relpermalink":"/en/blog/istio-119-release/","section":"blog","summary":"I’m delighted to present Istio’s most recent release—Istio 1.19. This blog will provide an overview of the updates bundled in this release.","title":"What’s New in Istio 1.19: Gateway API and Beyond"},{"content":"Cloud computing professionals, it’s been a month since the Istio Certified Associate (ICA) Certification journey began. A collaboration between the Cloud Native Computing Foundation (CNCF), Linux Foundation Training \u0026amp; Certification, and Tetrate, this certification has set a new benchmark in microservices management.\nTetrate , a key contributor to the Istio project, initially crafted the Certified Istio Administrator by Tetrate (CIAT), which laid the groundwork for the ICA Certification. Since its introduction, the ICA has been an instrumental part of the Kubernetes ecosystem, helping thousands to harness the power of service mesh technology.\nThe certification curriculum covers essential domains such as installation, traffic management, and security, reflecting the comprehensive expertise needed in the field. In a short span, the ICA has gained significant momentum, underpinned by the vision and dedication of Tetrate to the cloud-native landscape.\nAs we celebrate one month of the ICA, Tetrate’s commitment to open source education continues through their Tetrate Academy, providing free, high-quality courses on service mesh and Kubernetes security. For those aiming to deploy Istio in production, Tetrate offers the Tetrate Istio Distribution (TID), a secure and supported Istio distribution.\nIf you haven’t yet, now is the time to join this growing community of Istio experts. Enhance your career and contribute to the evolution of cloud-native technologies.\nStart Your ICA Certification Today! This is just the beginning. Let’s look forward to more milestones in service mesh proficiency, with Tetrate and CNCF leading the charge.\n","date":"2023-11-08T11:27:49+08:00","relpermalink":"/en/blog/ica-review/","section":"blog","summary":"CNCF and Tetrate's Istio certification marks 1 month, boosting Kubernetes skills in microservices, security, and traffic management.","title":"A Month in Review: The Impact of Istio Certified Associate (ICA) Certification with Tetrate's Contribution"},{"content":"The development of cloud-native applications has led to a shift in development to the left and a higher frequency of application iteration, which has given rise to the need for GitOps. This article will introduce how to use the Argo project, including ArgoCD and Argo Rollouts, to achieve GitOps and canary deployment with Istio. There is also a demonstration in the article that shows how to achieve GitOps based on the Istio environment provided by Tetrate Service Express (also applicable to Tetrate Service Bridge).\nThe deployment architecture diagram of the demo in this article is shown in Figure 1. If you are already familiar with the deployment strategies and Argo projects introduced in this article, you can skip directly to the demo section.\nFigure: Figure 1: Architecture diagram of using Istio and Argo projects in TSE/TSB to achieve GitOps and canary release Deployment strategy First of all, I want to briefly introduce the two deployment strategies supported by Argo Rollouts, which can achieve zero-downtime deployment.\nThe steps of blue-green deployment and canary deployment are shown in Figure 2.\nFigure: Figure 2: Steps of blue-green deployment and canary deployment Blue-green deployment is a strategy that deploys the new version of the application in a separate environment in parallel without affecting the current production environment. In blue-green deployment, the current production environment is called the “blue environment,” and the environment where the new version of the application is deployed is called the “green environment.” Once the green environment is considered stable and has passed the test, the traffic will gradually switch from the blue environment to the green environment, allowing users to gradually access the new version. If problems occur during the switching process, it can be quickly rolled back to the blue environment to minimize the impact on users. The advantage of blue-green deployment is that it can provide high availability and zero-downtime deployment. Canary deployment is a strategy for gradually introducing new versions or features into the production environment. In canary deployment, the new version or feature is first deployed to a small number of users in the production environment, called “canary users.” By monitoring the feedback and performance indicators of canary users, the development team can evaluate the stability and reliability of the new version or feature. If there are no problems, more users can be gradually included in the canary deployment until all users use the new version. If a problem is found, it can be quickly rolled back or fixed to avoid negative effects on the entire user group. The advantage of canary deployment is that it can quickly identify problems and make adjustments in a small impact area. The main difference between blue-green deployment and canary deployment is the deployment method and the scale of the changes. Blue-green deployment deploys the entire application in a new environment and then switches, which is suitable for large-scale changes, such as major upgrades of the entire application. Canary deployment gradually introduces new versions or features, which is suitable for small-scale changes, such as adding or modifying a single feature.\nIn terms of application scenarios, blue-green deployment is suitable for systems with higher requirements for high availability and zero-downtime deployment. When deploying large-scale changes, blue-green deployment can ensure stability and reliability and can quickly roll back to cope with unexpected situations. Canary deployment is suitable for systems that need to quickly verify new features or versions. By gradually introducing changes, problems can be discovered early and adjustments can be made to minimize the impact on users.\nRelease strategy of Kubernetes Deployment In Kubernetes, the Deployment resource object is one of the main tools for managing the deployment and updating of applications. Deployment provides a declarative way to define the expected state of an application and implements the release strategy through the controller’s functionality. The architecture of Deployment is shown in Figure 3, where the colored squares represent pods of different versions.\nFigure: Figure 3: Architecture diagram of Kubernetes Deployment The release strategy can be configured in the spec field of Deployment. Here are some common release policy options:\nManagement of ReplicaSet: Deployment uses ReplicaSet to create and manage replicas of an application. The desired number of replicas can be specified by setting the spec.replicas field. During the release process, the Kubernetes controller ensures that the number of replicas of the new version’s ReplicaSet gradually increases when created, and the number of replicas of the old version’s ReplicaSet gradually decreases when deleted to achieve a smooth switch. Rolling update policy: Deployment supports multiple rolling update policies, which can be selected by setting the spec.strategy.type field. Common policies include: RollingUpdate: The default policy updates replicas gradually at a certain time interval. The number of replicas that are not available at the same time and the number of additional available replicas can be controlled by setting the spec.strategy.rollingUpdate.maxUnavailable and spec.strategy.rollingUpdate.maxSurge fields. Recreate: This policy first deletes all replicas of the old version during the update process and then creates replicas of the new version. This policy will cause the application to be temporarily unavailable during the update. Version control: Deployment sets labels for each version’s ReplicaSet through the spec.template.metadata.labels field so that the controller can track and manage them accurately. This way, multiple versions of ReplicaSet can coexist, and the number of replicas of each version can be accurately controlled. By using these configuration options, Deployment can achieve different release strategies. Updating the spec field of the Deployment object can trigger the release of a new version. The Kubernetes controller will automatically handle the creation, update, and deletion of replicas according to the specified policy to achieve smooth application updates and deployment strategies.\nImplementing GitOps with ArgoCD You can use Deployment to manually manage release strategies, but to achieve automation, we also need to use GitOps tools such as ArgoCD.\nArgoCD is a GitOps-based continuous delivery tool used to automate and manage the deployment of Kubernetes applications. It provides some key help to improve the efficiency and reliability of application deployment.\nHere are some of the help ArgoCD provides for Kubernetes application deployment:\nDeclarative configuration: ArgoCD uses a declarative way to define the expected state of an application and stores the application configuration in a Git repository. By versioning and continuous integration/continuous delivery (CI/CD) processes, it is easy to track and manage application configuration changes. Continuous deployment: ArgoCD can monitor configuration changes in the Git repository and automatically deploy the application to the Kubernetes environment. It provides customizable synchronization policies that can automatically trigger application deployment and updates, achieving continuous deployment. State comparison and automatic repair: ArgoCD periodically checks the current state of the application and compares it with the expected state. If inconsistencies are found, it will automatically try to repair and restore the application to the desired state to ensure consistency between the expected and actual states. Multi-environment management: ArgoCD supports managing multiple Kubernetes environments, such as development, testing, and production environments. It is easy to deploy and synchronize application configurations between different environments, ensuring consistency and controllability. Compared to Deployment resource objects, ArgoCD provides more advanced features and workflows that complement the capabilities of native Kubernetes resource objects:\nGitOps-based configuration management: ArgoCD stores application configuration in a Git repository, enabling GitOps-based configuration management. This approach ensures that configuration changes are traceable, auditable, and can be integrated with existing CI/CD pipelines.\nAutomated deployment and continuous delivery: ArgoCD can automatically detect configuration changes in the Git repository and deploy applications to Kubernetes environments, enabling automated deployment and continuous delivery.\nState management and automatic recovery: ArgoCD continuously monitors the state of applications and compares it to the expected state. If inconsistencies are detected, it automatically recovers and ensures that the application state remains consistent with the expected state.\nUsing Istio to achieve fine-grained traffic routing Although ArgoCD can implement GitOps, it essentially operates on Kubernetes Deployment and controls traffic routing through replica numbers. To achieve fine-grained traffic routing, services meshes like Istio are used.\nIstio achieves finer-grained traffic routing and application release through the following methods:\nVirtualService: Istio uses VirtualService to define traffic routing rules. By configuring VirtualService, traffic can be routed and distributed based on request attributes such as request headers, paths, weights, etc., directing requests to different service instances or versions.\nDestinationRule: Istio’s DestinationRule is used to define service version policies and load balancing settings. By specifying different traffic weights between service instances of different versions, advanced application release policies such as canary release or blue-green deployment can be implemented.\nTraffic control and policies: Istio provides rich traffic control and policy …","date":"2023-08-31T10:18:40+08:00","relpermalink":"/en/blog/implementing-gitops-and-canary-deployment-with-argo-project-and-istio/","section":"blog","summary":"This article discusses how to use Deployment, ArgoCD, and Istio to implement GitOps and canary deployment. ","title":"Implementing GitOps and Canary Deployment with Argo Project and Istio"},{"content":"In this article, I will explain how to use GraphQL to query data from SkyWalking with Postman . It includes steps to obtain the bearer token, construct a query to retrieve load metrics for a specific service, and use GraphQL introspection to see the schema of SkyWalking GraphQL APIs. The article also provides references for further information.\nWhat Is GraphQL? GraphQL is a query language and runtime for APIs developed by Facebook. It provides a more efficient, powerful, and flexible alternative to traditional REST APIs by allowing clients to specify exactly what data they need and receive only that data in response. With GraphQL, clients can query multiple resources in a single request, reducing the number of roundtrips to the server and improving performance.\nWhat’s the Difference between GraphQL and REST APIs? GraphQL allows clients to request only the data they need, while REST APIs require clients to retrieve everything in a resource regardless of whether they need it or not. Additionally, GraphQL allows clients to query multiple resources in a single request, making it more efficient and less chatty than REST APIs.\nHow Do I Query Data from SkyWalking? SkyWalking defines the communication protocol for the query stage. The SkyWalking native UI and CLI use this protocol to consistently fetch data from the backend, without needing to worry about backend updates.\nThere are two methods for querying metrics from SkyWalking:\nGraphQL APIs PromQL APIs This article provides a guide on how to use GraphQL to query metrics from SkyWalking. If you are interested in the PromQL APIs, you can refer to the article Build Grafana dashboards for Apache SkyWalking — Native PromQL Support .Continuing with the following steps requires a TSB installation. If you don’t have one and still want to experience using GraphQL to query data in SkyWalking, you can use the free demo environment (username/password: skywalking/skywalking) provided by SkyWalking. Log in to the demo website and get a token for queries. Endpoint address for GraphQL queries is http://demo.skywalking.apache.org/graphql . The steps to construct the query are the same as described below.\nObserve GraphQL Queries in TSB Before we use Postman to construct our own GraphQL query, let’s first observe how TSB obtains data from SkyWalking.\nOpen Chrome DevTools and switch to the Network tab. Visit the Organization – Services tab on the website. Watch the network request list and right-click on the one of the graphql requests, like in the following image:\nFigure: Figure 1: Chrome DevTool The curl commands you see will look like this. Execute the command in your terminal, and you will get a list of services managed by TSB from SkyWalking.\ncurl \u0026#39;\u0026lt;https://saturn.tetrate.work/ui/graphql\u0026gt;\u0026#39; \\ -H \u0026#39;Accept-Language: en,zh-CN;q=0.9,zh;q=0.8,en-US;q=0.7,zh-TW;q=0.6\u0026#39; \\ -H \u0026#39;Cache-Control: no-cache\u0026#39; \\ -H \u0026#39;Client-Timestamp: 1686104776136\u0026#39; \\ -H \u0026#39;Connection: keep-alive\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -H \u0026#39;Cookie: ...\u0026#39; \\ -H \u0026#39;Origin: \u0026lt;https://saturn.tetrate.work\u0026gt;\u0026#39; \\ -H \u0026#39;Pragma: no-cache\u0026#39; \\ -H \u0026#39;Referer: \u0026lt;https://saturn.tetrate.work/mp/services\u0026gt;\u0026#39; \\ -H \u0026#39;Request-Id: ...\u0026#39; \\ -H \u0026#39;Sec-Fetch-Dest: empty\u0026#39; \\ -H \u0026#39;Sec-Fetch-Mode: cors\u0026#39; \\ -H \u0026#39;Sec-Fetch-Site: same-origin\u0026#39; \\ -H \u0026#39;User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\u0026#39; \\ -H \u0026#39;X-Bridge-Csrf-Token: IOmJszLAqY3TRIUNhTuGu7vQgnfQY1FtgYFm+l/+Mu4EmVQU5T8EaQ7bngkCv4hQ12ZGids+I21pHMdepE9/qQ==\u0026#39; \\ -H \u0026#39;X-Csrf-Token: xTbxZerD3t8N3PaS7nbjKCfxk1Q9dtvvrx4D+IJohHicb0VfB4iAZaP0zh1eXDWctQyCYZWaKLhAYT3M6Drk3A==\u0026#39; \\ -H \u0026#39;accept: application/json\u0026#39; \\ -H \u0026#39;sec-ch-ua: \u0026#34;Not.A/Brand\u0026#34;;v=\u0026#34;8\u0026#34;, \u0026#34;Chromium\u0026#34;;v=\u0026#34;114\u0026#34;, \u0026#34;Google Chrome\u0026#34;;v=\u0026#34;114\u0026#34;\u0026#39; \\ -H \u0026#39;sec-ch-ua-mobile: ?0\u0026#39; \\ -H \u0026#39;sec-ch-ua-platform: \u0026#34;macOS\u0026#34;\u0026#39; \\ --data-raw $\u0026#39;{\u0026#34;query\u0026#34;:\u0026#34;query ServiceRegistryListMetrics(...)}\u0026#39; \\ --compressed Note: Some fields in the above example are too long and replaced with dots (…).\nNext, I will guide you through constructing a query to retrieve the load metrics for a specific service.\nObtain the Bearer Token Firstly, you need to obtain the bearer of the website. Log in to TSB UI, click on the user button in the upper right corner, and then click “Show token information”. In the pop-up window, you will see the Bearer Token, as shown in the following image.\nFigure: Figure 2: Get the bearer token from the TSB UI Note: The validity period of the bearer token is relatively short. When it expires, you need to log in to TSB again to obtain a new token.\nWe have already deployed the bookinfo application in advance and sent some test traffic. To query the load metrics of reviews using GraphQL in the Postman client, follow these steps:\nCreate a new GraphQL request and enter the request URL: $TSB_ADDRESS/graphql Add the Authorization header with the value Bearer $TOKEN Use GraphQL Introspection to see the schema of SkyWalking GraphQL APIs. Find and click the readMetricsValues item. You will see the variables on the right side. Fill in the condition and duration items, as shown in the following image.\nFigure: Figure 3: Postman query The variables look like this:\nquery ReadMetricsValues { readMetricsValues(condition: { name: \u0026#34;service_cpm\u0026#34;, entity: {scope: Service, serviceName: \u0026#34;reviews\u0026#34;, normal: true} }, duration: { start: \u0026#34;2023-06-05 0625\u0026#34;, end: \u0026#34;2023-06-05 0627\u0026#34;, step: MINUTE }) { label values { values { id value } } } } Click the Query button to get the result. It should look similar to this:\n{ \u0026#34;data\u0026#34;: { \u0026#34;readMetricsValues\u0026#34;: { \u0026#34;label\u0026#34;: null, \u0026#34;values\u0026#34;: { \u0026#34;values\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;service_cpm_202306050625_cmV2aWV3cw==.1\u0026#34;, \u0026#34;value\u0026#34;: 0 }, { \u0026#34;id\u0026#34;: \u0026#34;service_cpm_202306050626_cmV2aWV3cw==.1\u0026#34;, \u0026#34;value\u0026#34;: 0 }, { \u0026#34;id\u0026#34;: \u0026#34;service_cpm_202306050627_cmV2aWV3cw==.1\u0026#34;, \u0026#34;value\u0026#34;: 0 } ] } } } } The above is using the SkyWalking Demo environment to test GraphQL queries. GraphQL query support is also provided in TSE, and the endpoint address is https://$TSB_SERVER/graphql.\nNote: The query endpoint here is different from what we see in DevTool. The GraphQL query endpoint specific to the TSB UI is https://$TSB_SERVER/ui/graphql.For details about the SkyWalking GraphQL Query Protocol, please refer to GitHub .\nSummary In this article, I have introduced how to use the GraphQL query protocol in Postman to query data in SkyWalking. You can construct your own query conditions based on the GraphQL schema of SkyWalking. This feature is also available in TSB/TSE.\nReferences https://github.com/apache/skywalking-query-protocol SkyWalking Website This blog was originally published at tetrate.io .\n","date":"2023-07-20T16:27:49+08:00","relpermalink":"/en/blog/how-to-use-graphql-to-query-observability-data-from-skywalking-with-postman/","section":"blog","summary":"This article explains how to use GraphQL to query observability data from SkyWalking with Postman. It first introduces GraphQL and SkyWalking, then explains how to set up Postman to send GraphQL queries, and finally provides some example GraphQL queries that can be used to query observability data from SkyWalking.","title":"How to Use GraphQL to Query Observability Data from SkyWalking with Postman"},{"content":"Editor’s Recommendation The authors of this book are global experts in service mesh technology, providing comprehensive and detailed content. The translators of this book are technical experts from the domestic cloud-native community, striving to convey the content accurately and providing localization for difficult-to-understand concepts. The book covers comprehensive content, including the latest versions and core concepts of Istio, with rich and complete example codes. Figure: Istio in Action (Chinese edition) Overview As a representative product of service mesh technology, Istio has matured over the years and is increasingly favored by developers. This book focuses on Istio service mesh, covering basic concepts, core features, operations, and enterprise-level implementations. From basic installation and deployment to practical functions, from analyzing underlying principles to troubleshooting, and from advanced operations to enterprise-level practices, it provides a comprehensive introduction to various aspects of Istio service mesh.\nThis book is suitable for developers, operations engineers, architects, and other practitioners in the cloud-native field who are currently using or interested in Istio. Whether you are a beginner in service mesh technology or an expert in the field, you can find theoretical insights and practical guidance in this book.\nAbout the Authors Christian Posta is the Chief Technology Officer for Global Field at Solo.io. He is well-known in the cloud-native community as an author, blogger, speaker, and contributor to various open-source projects in the service mesh and cloud-native ecosystem. With experience working in both traditional enterprises and large internet companies, Christian now helps organizations create and deploy large-scale, cloud-native, resilient distributed architectures. He specializes in guiding, training, and leading teams to success in distributed systems concepts, microservices, DevOps, and cloud-native application design.\nRinor Maloku is an engineer at Solo.io, providing consulting services to clients adopting application networking solutions such as service mesh. Previously, he worked at Red Hat, where he developed middleware software that enabled development teams to ensure the high availability of their services. As a freelancer, he has served several DAX 30 members to fully leverage the potential of cloud computing technologies.\nAbout the Translators Ma Ruofei works as Chief Engineer at FreeWheel Beijing R\u0026amp;D Center, primarily responsible for microservices architecture design and cloud-native landing work. He is the author of “Istio Practical Guide,” columnist for GeekTime’s “Service Mesh in Action,” and the main author of “Cloud-Native Application Architecture: Best Practices for Microservices Development.” He is also an IT professional book expert consultant for People’s Posts and Telecommunications Press, as well as a member of the ServiceMesher technical community and the Cloud Native Community management committee. He has published and translated numerous articles on cutting-edge technologies in the cloud-native field and is passionate about technology sharing.\nSong Jingchao (Jimmy Song) is a Developer Advocate at Tetrate, the founder of the cloud-native community, and the former Developer Advocate and Open Source Manager at Ant Group. He is also a published author and independent writer. He is an early adopter and promoter of open-source technologies such as Kubernetes and Istio. He has authored books such as “Future Architecture: From Service to Cloud-Native” and “Understanding Istio: Advanced Practical Guide to Cloud-Native Service Mesh” and has participated in the translation of many works.\nLuo Guangming is an architect at ByteDance’s Service Framework Team, a member and Beijing stationmaster of the Cloud Native Community management committee. He has previously worked on cloud-native, microservices, and open-source-related projects at Ericsson and Baidu before joining ByteDance, where he is responsible for open-source projects related to microservices like CloudWeGo. He has been focusing on cutting-edge technologies, architectural evolution, and standardization processes in the cloud-native and microservices fields for a long time.\n","date":"2023-07-01T00:00:00Z","relpermalink":"/en/book/istio-in-action/","section":"book","summary":"Authored by Christian Posta and Rinor Maloku.","title":"Istio in Action"},{"content":"In June, Istio 1.18 was released , marking the second release of Istio in 2023 and the first to offer official support for ambient mode . Tetrate’s Paul Merrison was one of the release managers for this version, and Tetrate’s contributions to this release included various customer-driven usability enhancements and important work in the underlying Envoy Proxy upon which Istio depends. When asked about the experience of working on Istio 1.18, Paul said “working as the lead release manager for Istio 1.18 gave me a fascinating insight into how a group of super talented people from around the world come together, organize themselves and ship software. There was a steep learning curve, but the Istio community is awesome and I was supported brilliantly. The biggest challenge was definitely learning and executing all the steps that are needed to bring a release to life, but the feeling of achievement when it finally made its way out into the world will stay with me for a while!” Istio first announced the introduction of Ambient mode in September last year, which was covered in detail by Zack in this blog post , where he explains the differences between ambient mode and sidecar mode.\nThis release introduces many new features and changes in addition to ambient mode, including enhanced Kubernetes Gateway API support, health checks for virtual machines that are not automatically registered, support for expired metrics, enhanced istioctl analyze, and more. See the release blog for details. The most significant of these are the ambient mode and Gateway API enhancements, detailed below.\n“Working as the lead release manager for Istio 1.18 gave me a fascinating insight into how a group of super talented people from around the world come together, organize themselves and ship software. There was a steep learning curve, but the Istio community is awesome and I was supported brilliantly. The biggest challenge was definitely learning and executing all the steps that are needed to bring a release to life, but the feeling of achievement when it finally made its way out into the world will stay with me for a while!”\nPaul Merrison, Tetrate Engineering and Istio Release Manager\nWhat Is Ambient Mode? Before discussing ambient mode, it is essential to understand the current “sidecar mode” used by Istio. Sidecar mode is the default data plane mode used by Istio, where each application pod comes equipped with a sidecar proxy (usually Envoy) that handles all network traffic in and out of the pod, providing Istio’s core functionality such as Zero Trust security, telemetry and traffic management. While sidecar mode is suitable for most users, ambient mode offers some advantages in specific circumstances. For more information on the differences between ambient mode and the standard sidecar mode, see our article on Ambient Mesh: What You Need to Know about This Experimental New Deployment Model for Istio .\nWhat Are the Design Goals of Ambient Mode? Non-intrusive: Ambient mode does not require injecting sidecar proxies into the application’s pods and only requires the application to be tagged to automatically join the mesh, potentially reducing the mesh’s impact on the application. Efficient resource utilization: Ambient mode can optimize resource utilization for some use cases by sharing the ztunnel proxy across the mesh. If certain L7 functionality of Istio is required, it can also be addressed by deploying Waypoints precisely for a ServiceAccount or Namespace, providing more control over resource consumption. Performance parity with sidecar mode: Ambient mode initially adopted a shared proxy model based on Envoy, but during development, issues such as complex Envoy configuration were discovered, leading the Istio community to develop its shared proxy (ztunnel ) based on Rust. In the future, ambient mode is expected to have comparable performance to traditional sidecar mode. Security: Ambient mode provides TLS support by running a shared proxy ztunnel on each node, and when users require the same security as sidecar mode, they also need to deploy one or more Waypoints in each namespace to handle L7 traffic in that namespace. Users can use istioctl x waypoint for Waypoint configuration management. For example, running the istioctl x waypoint generate command generates a Kubernetes Gateway API resource managed by Istio.\nOverall, ambient mode promises to offer additional flexibility to Istio’s deployment model which may prove helpful to some users. It should be noted that the ambient mode is still in the alpha stage and has yet to achieve production-level stability.\nEnhanced Kubernetes Gateway API Support Istio 1.18 introduces several vital improvements and modifications to its Kubernetes Gateway API support:\nSupport for v1beta1: When upgrading to the new version of Istio, Gateway API version greater than 0.6.0+ is required. Use the istioctl x precheck command to check for upgrade issues.\nGateway API automated deployment management upgrades: All Kubernetes Gateway resources Istio manages will automatically configure Service and Deployment resources when created or updated. If the Gateway resource changes, the associated configuration will also be updated synchronously. In addition, the deployment of Gateway resources no longer depends on injection logic but has an independent creation process.\nRemoval of support for the proxy.istio.io/config annotation: The ProxyConfig resource only affects the Istio-managed Gateway.\nFixes to Istiod handling of configuration changes: If Service and Deployment configurations change, Istiod reprocesses them.\nIt no longer supports the Alpha version of the Gateway API by default: It can be re-enabled by setting PILOT_ENABLE_ALPHA_GATEWAY_API=true.\nIt is worth noting that when installing ambient mode, unlike previous Istio installations, IngressGateway is no longer included by default. In future development, Istio is leaning towards using the Gateway API to manage gateways. For more information on why the Gateway API is recommended over Ingress, please read my previous blog article on why the Gateway API Is the unified future of ingress for Kubernetes and service mesh .\nNext Steps to Ambient Mesh We’re excited that ambient mode is now available to users in an official Istio release, especially as it promises to make mesh adoption easier for all users. The easiest way to get started with Istio’s new ambient mode is Tetrate Istio Distro , Tetrate’s hardened, fully upstream Istio distribution, with FIPS-verified builds and support available . It’s a great way to get started with Istio knowing you have a trusted distribution, to begin with, an expert team supporting you, and also the option to get to FIPS compliance quickly if you need to.\nAs you add more apps to the mesh, you’ll need a unified way to manage those deployments and coordinate the mandates of the different teams involved. That’s where Tetrate Service Bridge comes in. Learn more about how Tetrate Service Bridge makes service mesh more secure, manageable, and resilient here , or contact us for a quick demo .\nThis blog was originally published at tetrate.io .\n","date":"2023-06-26T17:24:49+08:00","relpermalink":"/en/blog/istio-1-18-released-now-with-ambient-mode-available/","section":"blog","summary":"This article introduces Istio 1.18, the latest release of the service mesh platform. It highlights the new features and improvements, such as ambient mode, which allows Istio to run on any Kubernetes cluster without requiring a dedicated control plane. It also explains how to get started with Istio 1.18 using Tetrate’s distribution and support.","title":"Istio 1.18 Released, Now with Ambient Mode Available"},{"content":"In a previous blog post , I discussed how Istio Ambient Mesh uses iptables and Geneve tunnels to intercept traffic from application pods into Ztunnel. Many readers may not be familiar with this tunneling protocol, so this article will introduce the definition, packet structure and advantages of Geneve tunnels compared with the VXLAN protocol. Finally, this article will introduce how Istio Ambient Mesh applies Geneve tunnels to implement traffic interception and the new eBPF mode introduced in Istio 1.18.\nIntroduction to Geneve Tunnels In order to address the lack of flexibility and security in current data transmissions, the Geneve (Generic Network Virtualization Encapsulation) network virtualization encapsulation (tunneling) protocol was created. Geneve only defines a data encapsulation format, excluding control plane information. The key advantage of Geneve over VXLAN encapsulation is that it extends the types of encapsulated protocols by adding TLV format options.\nGeneve vs. VXLAN VXLAN and Geneve are both network virtualization protocols and they have many similarities. Virtualization protocols are technologies that separate virtual networks from physical networks. They allow network administrators to create multiple virtual networks in a virtual environment, each of which can have its own VLAN identifiers, IP addresses and routing. In addition, VXLAN and Geneve use UDP encapsulation, which enables them to be extended through existing network infrastructure. VXLAN and Geneve protocols are also flexible, can be used in different network topologies and are compatible with different virtualization platforms.\nFigure 1 shows the message structure of VXLAN and Geneve tunnels and the differences in their respective headers.\nFigure: Figure 1: VXLAN and Geneve packet format schematic diagram. From the figure, we can see that the message structure of VXLAN and Geneve tunneling protocols is similar, with the main difference being the use of different UDP port numbers and protocol headers. VXLAN uses port 4789, while Geneve uses port 6081. The Geneve protocol header is more extendable than VXLAN.\nThe Geneve tunneling protocol adds variable-length options that can contain zero or more option data in TLV format, making it more scalable than VXLAN. TLV stands for Type-Length-Value, which is a format for parsing and transmitting metadata in network packets. Each metadata information in the Geneve protocol is composed of a TLV format field, making it simple to flexibly add, delete and modify these metadata.\nThe TLV format field contains the following data:\nType: 8-bit type field. Length: 5-bit option length field, represented in multiples of 4 bytes, excluding the option header. Data: Variable-length option data field, which may not exist or may be between 4 and 128 bytes. The Geneve protocol can easily modify and extend metadata information while maintaining compatibility and flexibility by using the TLV format.\nPlease refer to RFC 7348 Virtual eXtensible Local Area Network (VXLAN): A Framework for Overlaying Virtualized Layer 2 Networks over Layer 3 Networks for more information about VXLAN. For more information about the Geneve tunnel packet format, please refer to RFC 8926 Geneve: Generic Network Virtualization Encapsulation .\nHow it Works The Geneve tunnel is mainly used in cloud computing and virtualization scenarios, and it can encapsulate packets in a new packet for transmission in a virtual network. The Geneve tunnel uses a 24-bit VNI (Virtual Network Identifier) to transmit packets from one physical network to another. The Geneve tunnel can also use security protocols such as IPsec and TLS to protect the transmission of packets.\nWhen a packet reaches the destination host, the Geneve tunnel protocol will de-encapsulate the packet from the Geneve protocol header and deliver it to the destination in the virtual network. During the de-encapsulation process, the VNI information in the Geneve protocol header is used to determine the destination of the packet, ensuring that the packet is correctly routed to the destination in the virtual network.\nAssuming there is a virtual network with a VNI of 1001. When a packet is transmitted from one physical network to another, a tunnel can be used to track the packet during transmission by setting the VNI between the source and target physical networks to 1001. When the packet reaches the target physical network, the VNI is removed from the packet and the packet is delivered to the target physical network.\nSecurity The Geneve tunnel protocol itself does not provide any security mechanisms, so packets transmitted in the Geneve tunnel can be subject to threats such as packet tampering, interception and replay.\nTo ensure the security of packets transmitted in the Geneve tunnel, some security protocols can be used. The following are some common security protocols:\nIPsec (Internet Protocol Security): IPsec is a network layer security protocol that can encrypt, authenticate and provide integrity protection to packets in the Geneve tunnel. IPsec can provide end-to-end security. TLS (Transport Layer Security): TLS is an encryption protocol based on the transport layer that can encrypt and authenticate packets in the Geneve tunnel. TLS can provide end-to-end security. MACSec (Media Access Control Security): MACSec is a data link layer security protocol that can encrypt and authenticate packets in the Geneve tunnel. MACSec can provide link-layer security. It should be noted that the above security protocols require corresponding configuration and deployment and may have a certain impact on performance. When choosing the appropriate security protocol, factors such as security, performance, manageability and other aspects need to be considered.\nWhy Choose Geneve? The following table compares the characteristics of VXLAN and Geneve in multiple aspects.\nFeature VXLAN Geneve Header format Fixed format Extensible format Scalability More focused on L2 extension Better support for emerging network services Operability Difficult to manage and extend Easier to manage and extend Performance Shorter header, higher performance Longer header, slightly lower performance Table 1: VXLAN vs Geneve characteristics.\nThe main reason for using the Geneve protocol is to combine the advantages of current network virtualization encapsulation technologies (such as VXLAN, NVGRE and STT) into one protocol. Through years of network virtualization development experience, we know that one of the most important requirements is scalability. The Geneve protocol encodes metadata using an extensible TLV structure, so it can independently develop the functionality of software and hardware endpoints to meet growing needs.\nHow Istio Ambient Mesh Applies Geneve Tunnels In the previous blog , I explained how Istio Ambient Mesh uses Ztunnel to implement L4 proxies and Figure 2 shows the L4 transparent traffic interception path using iptables and Geneve tunnels.\nFigure: Figure 2: L4 Transparent Traffic Interception Path Using Iptables and Geneve Tunnels. From the figure, we can see that:\nThe Istio CNI creates an istioout network card and iptables rules on the node, transparently intercepting the outbound traffic in the node to the pistioout virtual network card. The Istio CNI creates an istioin network card and iptables rules on the node, transparently intercepting the inbound traffic in the node to the pistioin virtual network card. The Istio CNI creates pistioin and pistioout network cards in ztunnel to receive data packets in the Geneve tunnel. The two network cards pistioin and pistioout are created by the init container or Istio CNI (see the CreateRulesWithinNodeProxyNS function in net_linux.go ), and their IP addresses and ports are fixed. The data packets sent by the application container need to pass through the istioout network card and be forwarded to the ztunnel container after being encapsulated in the Geneve tunnel. When the data packets are received by the ztunnel container, they are de-encapsulated and forwarded to the corresponding application containers through the pistioin network card.\nUsing eBPF for Transparent Traffic Interception eBPF (extended Berkeley Packet Filter) is a powerful technology that allows secure user-space programs to run within the Linux kernel. Initially developed as a technique for filtering network packets, eBPF has now been extended to other areas such as tracking system calls, performance analysis and security monitoring. The advantages of eBPF are its lightweight nature, efficiency, security and programmability. It can be used in real-time monitoring, network security, application debugging and optimization, container networking and various other fields.\nIstio Ambient Mesh also supports using the eBPF (extended Berkeley Packet Filter) mode for transparent traffic interception since 1.18. As shown in Figure 3, the eBPF program runs directly in the host kernel and forwards application traffic to ztunnel. Compared to the iptables-based approach, the eBPF mode can provide better network efficiency and scalability. However, it requires a higher version of the Linux kernel and is more difficult to implement.\nFigure: Figure 3: Intercepting the Traffic of Application Using eBPF. To use the eBPF mode to run Ambient Mesh, simply set the values.cni.ambient.redirectMode parameter to “ebpf” when installing Istio, as shown below:\nistioctl install --set profile=ambient --set values.cni.ambient.redirectMode=\u0026#34;ebpf\u0026#34; Summary This article introduced the working principle, security and comparison with VXLAN of the Geneve tunnel protocol. In addition, it also introduced how Istio Ambient Mesh uses Geneve tunnels to implement traffic interception and discussed the advantages and disadvantages of using eBPF for transparent traffic interception. The Geneve tunnel protocol is a universal tunneling protocol that can transmit packets in virtual networks, and it has more advantages than other tunneling protocols. …","date":"2023-05-29T15:27:49+08:00","relpermalink":"/en/blog/traffic-interception-with-geneve-tunnel-with-istio-ambient-mesh/","section":"blog","summary":"This article introduces Geneve tunnels, a network virtualization protocol that can intercept Istio ambient mesh traffic more flexibly and securely than VXLAN. It also explains how Istio Ambient Mesh uses Geneve tunnels and the new eBPF mode in Istio 1.18.","title":"Using Geneve Tunnels to Implement Istio Ambient Mesh Traffic Interception"},{"content":"Envoy Gateway , the open-source API Gateway based on Envoy Proxy, has just released version 0.4.0 . This release is centered around customization, with the goal of enabling more use cases for end-users. In this blog post, we will discuss the new customizations available in this release and their significance for users.\nCustomizing Envoy Proxy Infrastructure One of the main customizations in this release is the ability to configure the exact type of EnvoyProxy (CRD) deployment. You can define the number of replicas, images, and resource limits that EnvoyProxy deploys. You can also add annotations to EnvoyProxy deployments and services. This makes different use cases possible, such as:\nLinking Envoy Gateway to external load balancers like AWS, NLB, ELB, and GCP. Injecting a sidecar alongside EnvoyProxy is very useful for managing the North-South traffic in the Envoy Gateway at the ingress layer and for managing the East-West traffic and enabling mutual TLS (mTLS) in the service mesh layer with the Envoy sidecar. This custom feature eliminates the need for users to create their own certificates, as it is based on Istio certificate management. Refer to the Envoy Gateway documentation for more customized features on Envoy Gateway.\nMulti-Tenant Deployment Modes Furthermore, Envoy Gateway has added support for other deployment modes in addition to the default Kubernetes single-tenant mode, such as multi-tenancy, as shown in Figure 1 below.\nFigure: Figure 1: Envoy Gateway multi-tenancy deployment mode. Deploy an Envoy Gateway Controller to each tenant’s namespace, which watches HTTPRoute and Service resources in Kubernetes, and creates and manages EnvoyProxy deployments in their respective namespaces.\nCustomizing Envoy xDS Bootstrap Another significant customization in this release is the ability to customize the Envoy xDS Bootstrap. With this feature, users can provide a bootstrap configuration to configure some static resources when starting up Envoy. A good case is configuring access logging, tracing and metrics to be sent to SkyWalking, which can work as an APM. Additionally, the release adds a lot of CLI tooling to help validate user configuration. Users can use the CLI as a dry run to change a specific field in Bootstrap, and it will fail if the config is not syntactically correct.\nExtending the Control Plane Envoy Gateway now provides the ability to allow vendors and extension developers to add gRPC hooks at different stages of the Envoy Gateway pipeline to further extend its functionality, allowing users to do things like enhance the xDS configuration being sent to Envoy, which was not possible before.\nSummary In conclusion, Envoy Gateway 0.4.0 extends the API for customization and enables more use cases for end-users. The new customizations include the ability to customize Envoy deployment, Envoy xDS Bootstrap, and the control plane. With the release of this version, Envoy Gateway is becoming more user-friendly and is positioning itself as a great alternative to ingress-nginx.\nThis blog was initially published at tetrate.io .\n","date":"2023-05-16T13:19:28+08:00","relpermalink":"/en/blog/envoy-gateway-customization/","section":"blog","summary":"In this blog post, we will discuss the new customizations available in this release and their significance for users.","title":"Envoy Gateway 0.4.0: Extending the API for Customization"},{"content":"Artificial Intelligence (AI) is changing our lives, making our work more efficient and intelligent. In this rapidly developing field, there are many practical AI tools that can help us better accomplish our work. In the future, mastering various AI tools to optimize your workflow and improve work efficiency will be a necessary skill for everyone. It’s time to gather some cheap and practical AI tools. Below are some recommended practical AI tools that are worth collecting. You can directly use these tools without additional programming knowledge. Most of these tools are free to use, or provide free usage quotas, or have low usage costs.\n1. ChatGPT Figure: ChatGPT ChatGPT is an intelligent chatbot based on GPT technology, which can interact with users in natural language. It uses deep learning technology and large-scale trained language models to understand users’ questions and provide useful answers.\nChatGPT can answer various questions, and users can directly input questions or topics on the website and get quick and accurate answers. It should be noted that ChatGPT is an online chatbot, and its answers may not be 100% accurate. In addition, the data that ChatGPT model is trained on is up to 2021, and it is still in the early development stage, and is constantly improving and optimizing.\nRecommendation Reasons\nChatGPT’s response speed is super fast, and you can get an answer in a few seconds, which is especially suitable for situations where quick replies are needed. You can also use OpenAI’s API to create your own tools. However, the free version of ChatGPT’s response speed is sometimes slow, and frequently refreshing the page is required when following up, and there are also limits on the number of characters for questions and answers.\nThe commonly used functions include:\nWriting code Translation Proofreading articles Summarizing an article (you can output a URL) Learning a knowledge area that you are not familiar with In addition, a ChatGPT desktop application is recommended: https://github.com/lencx/ChatGPT , as well as the ChatGPT prompt project https://github.com/f/awesome-chatgpt-prompts .\nChatGPT Plus ChatGPT Plus is an enhanced version of ChatGPT that utilizes GPT-3 technology to provide more powerful and intelligent natural language processing capabilities. It can assist users in completing various tasks such as generating articles, translation, Q\u0026amp;A, speech conversion, and chatbots.\nChatGPT Plus has a sleek and beautiful interface that is very intuitive and user-friendly. Users can input text using various methods such as keyboard input, voice input, and image input. ChatGPT Plus also offers multiple language and style options, making it easy for users to generate high-quality text and speech.\n2. Smodin Figure: Smodin Smodin.io is an online tool based on artificial intelligence and natural language processing technology that can help users generate content such as articles, press releases, blog posts, and social media posts. The website uses deep learning technology and language models to automatically generate high-quality text and offers a variety of language and style options.\nUsing Smodin.io is very simple. Simply enter the topic, keywords, desired language, and style you wish to generate, then click the “Generate” button to obtain a high-quality article. You can also make adjustments and edits as needed to meet your specific needs.\nIn addition to generating text, Smodin.io also offers other useful features such as grammar and spell check, SEO optimization suggestions, and real-time translation. The website is very flexible and convenient to use, making it suitable for individuals and businesses who require high-quality text, such as marketers, editors, writers, and bloggers.\nIt’s important to note that while Smodin.io can save you a lot of time and effort, since the text is generated automatically, there may be some syntax or logic errors. Therefore, appropriate review and editing are necessary when using it.\nRecommended Reasons\nThe website supports over 40 languages, limits input to 1000 characters per session, and returns rewritten results quickly. Free users have a limit on the number of calls they can make, while paid users have a limit. There are two payment tiers, both of which are relatively inexpensive.\n3. Bing Figure: Bing chat Bing is a search engine developed by Microsoft that integrates artificial intelligence technology to provide more intelligent search results. Recently, Microsoft announced a new feature called “Chat” added to Bing, where users can input questions in the chat box and Bing will automatically answer them, just like an intelligent chatbot. Bing also provides multiple conversation styles to choose from.\nIt should be noted that the chat function is currently in the testing phase and may have some issues and limitations. Additionally, Bing’s availability may be restricted in some countries and regions. However, it is still a very useful search engine that can help users quickly find the information they need.\nRecommended Reasons\nThe newly integrated Chat feature in Bing is quite stable as it can connect to the internet and crawl real-time data. You can use it as an auxiliary tool for search engines or ask it for real-time information.\n4. GitHub Copilot Figure: GitHub Copilot GitHub Copilot is an AI code assistant developed jointly by OpenAI and GitHub, which can help developers write code faster and more accurately. It uses natural language processing and machine learning technologies to automatically generate high-quality code for users, improving development efficiency and code quality.\nGitHub Copilot can be integrated with various development tools and programming languages, such as VS Code, Visual Studio, Python, and JavaScript. Users only need to input a small amount of natural language description in the editor, and Copilot can automatically generate high-quality code for them. For example, when a user inputs “Create a function that takes two arrays as arguments and returns their dot product.” Copilot can automatically generate the following code:\nfunction dotProduct(arr1, arr2) { return arr1.map((n, i) =\u0026gt; n * arr2[i]).reduce((a, b) =\u0026gt; a + b); } It is worth noting that GitHub Copilot is currently in beta and is constantly being optimized and improved. Additionally, the quality and accuracy of code generated by GitHub Copilot may be impacted by various factors such as input descriptions, programming languages, coding standards, and more.\nRecommended Reasons\nGitHub Copilot’s code generation speed is very fast and can help developers save a lot of time and effort. Additionally, GitHub Copilot can learn from users’ code repositories, improving the quality and accuracy of code generation.\n5. Notion Figure: Notion Notion is a comprehensive tool that combines note-taking, to-do lists, project management, knowledge base, and team collaboration functions. It provides powerful editing and organizing features that can help users easily create various types of documents, including text, images, videos, tables, and databases.\nNotion has a clean and beautiful interface, making it very intuitive and user-friendly. Users can organize and find their documents in various ways, such as using tags, directories, and searches. In addition, Notion also provides rich collaboration and sharing features, making it easy for users to collaborate and share documents with team members.\nFor AI developers, Notion is a very useful tool that can help them easily manage projects, record experimental data, share notes, translations, and documents, etc. Notion also provides rich third-party applications and APIs, making it easy to integrate with other tools and services.\nIt should be noted that the free version of Notion has some limitations, such as file upload size limits and API call limits, etc. However, the paid version provides more features and services, such as unlimited file uploads, API calls, and team members, etc., suitable for team and enterprise users.\nRecommended Features\nNotion provides rich templates and plugins that can help users quickly create various types of documents and projects, such as product roadmaps, project plans, and work logs, etc. In addition, Notion’s API and webhook features are also very powerful, allowing users to easily integrate with other tools and services. Free users can create an unlimited number of documents, but only have 20 calls to Notion AI. If you exceed this limit, you need to subscribe for $10/month (Note: Notion AI functionality requires a separate subscription).\nNotion is my favorite AI text editor, and it supports Chinese very well. Moreover, it supports Markdown format, making it easy to export. It can replace some of the functions of Smodin, but the effect of text rewriting still needs to be verified.\n6. Grammarly Figure: Grammarly Grammarly is a writing assistant that uses artificial intelligence to help users improve their writing skills. It can recognize and correct grammar, spelling, and punctuation errors, as well as provide suggestions for improving writing clarity and style. Grammarly can be used as a browser extension, desktop application, and mobile application, and is suitable for various writing tasks, such as emails, social media posts, and academic papers. Grammarly has free and premium versions, which offer additional features.\nUsing Grammarly is simple. Users just need to install its extension in their browser or download the application on their computer or mobile device, and start typing in the text box where they need assistance. Grammarly will automatically check the text and provide suggestions and corrections when necessary. Users can also choose different writing styles and language settings to adapt to their writing needs.\nIn addition to basic spelling and grammar checks, Grammarly also offers advanced features such as vocabulary enhancement, sentence structure adjustment, text simplification, and style suggestions. These …","date":"2023-03-15T21:40:40+08:00","relpermalink":"/en/blog/ai-tools-collection/","section":"blog","summary":"Are you looking for ways to optimize your workflow and increase productivity? Here are some useful AI tools for you.","title":"Useful AI Tools to Optimize Your Workflow"},{"content":"In the previous blog post , I introduced how Istio manages certificates, and in this article, I will guide you on how to use an external certificate authority (CA) to achieve fine-grained certificate management and automatic certificate rotation through the integration of SPIRE and cert-manager .\nIf you are not familiar with SPIRE and what it’s used for, we recommend reading the following articles:\nWhy Would You Need SPIRE for Authentication with Istio? How to integrate SPIRE in Istio Introduction to the Certificate Issuance and Management Process Figure 1 shows the certificate trust chain used in this article based on cert-manager and SPIRE:\nFigure: Figure 1: Certificate trust chain based on cert-manager and SPIRE. cert-manager acts as the root CA to issue certificates to istiod and SPIRE. We use a self-signed issuer , but you can also configure it to use built-in issuers such as Let’s Encrypt, Vault, Venafi, or other external issuers. You can also choose to use other UpstreamAuthorities , such as Vault, SPIRE Federation, etc. SPIRE issues SVID certificates to the workloads and ingress Gateway and egress Gateway in the Istio mesh for mTLS between services. The certificates used when accessing the ingress Gateway from outside the mesh and when the egress Gateway access services outside the mesh need to be configured separately. Figure 2 shows the certificate issuance and update process after integrating SPIRE and cert-manager in Istio.\nFigure: Figure 2: Certificate issuance and update process after integrating SPIRE and \u0026lt;em\u0026gt;cert-manager\u0026lt;/em\u0026gt; in Istio. The Kubernetes Workload Registrar in the SPIRE Server automatically registers the workloads in Kubernetes and generates SPIFFE standard identities for all workloads. cert-manager issues and manages the CA certificates for istiod. The Envoy proxies in the workloads send certificate signing request (CSR) requests to the SPIRE Agent on the same node through the SDS API via a Unix domain socket (UDS). The SPIRE Agent sends the CSR to the SPIRE Server. The SPIRE Server returns the signed certificate to the SPIRE Agent. The SPIRE Agent returns the signed certificate to the workloads. SPIRE is responsible for the certificate management and updates for the workloads. Now that we have a general understanding of the process, let’s install the components manually.\nInstall cert-manager Run the following command to install cert-manager, which we will use for automatic certificate rotation:\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.10.1/cert-manager.yaml The root CA is a self-signed certificate. Run the following command to configure the root CA:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: selfsigned namespace: cert-manager spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: selfsigned-ca namespace: cert-manager spec: isCA: true duration: 21600h secretName: selfsigned-ca commonName: certmanager-ca subject: organizations: - cert-manager issuerRef: name: selfsigned kind: Issuer group: cert-manager.io --- apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: selfsigned-ca spec: ca: secretName: selfsigned-ca EOF Then configure certificates for istiod:\nkubectl create namespace istio-system cat \u0026lt;\u0026lt; EOF | kubectl apply -f - --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: cacerts namespace: istio-system spec: secretName: cacerts duration: 1440h renewBefore: 360h commonName: istiod.istio-system.svc isCA: true usages: - digital signature - key encipherment - cert sign dnsNames: - istiod.istio-system.svc issuerRef: name: selfsigned-ca kind: ClusterIssuer group: cert-manager.io EOF Now that we have installed cert-manager and created a ClusterIssuer named selfsigned-ca, let’s install SPIRE and use cert-manager as the UpstreamAuthority for SPIRE.\nInstalling SPIRE Quickly install SPIRE by running the following command:\nkubectl apply -f https://gist.githubusercontent.com/rootsongjc/5dac0518cc432cbf844114faca74aa40/raw/814587f94bbef8fb1dd376282249dcb2a8f7fa1b/spire-with-cert-manager-upstream-authority-quick-start.yaml This YAML file adapts to cert-manager compared to the samples/security/spire/spire-quickstart.yaml file in the Istio 1.16 installation package, such as:\nAdding permissions for the cert-manager.io API group to the spire-server-trust-role ClusterRole; Adding an UpstreamAuthority “cert-manager” configuration in the SPIRE Server configuration. Note: The trust_domain in the SPIRE Server configuration should be consistent with the TRUST_DOMAIN environment variable specified when installing Istio.\nThis command installs the Kubernetes Workload Registrar , automatically registering the workloads in Kubernetes. All workloads will be registered with the SPIFFE standard service identity format spiffe://\u0026lt;trust-domain\u0026gt;/ns/\u0026lt;namespace\u0026gt;/sa/\u0026lt;service-account\u0026gt; based on their service accounts.\nIf you want to adjust the TTL of the SPIRE CA and SVID certificates, you can modify ca_ttl (default 24h) and default_svid_ttl (default 1h) in the SPIRE Server configuration. See SPIRE Server configuration .\nInstall Istio Run the following command to install Istio and enable CA certificate rotation:\nistioctl operator init istioctl install --skip-confirmation -f - \u0026lt;\u0026lt;EOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system spec: profile: default meshConfig: trustDomain: example.org values: global: # This is used to customize the sidecar template sidecarInjectorWebhook: templates: spire: | spec: containers: - name: istio-proxy volumeMounts: - name: workload-socket mountPath: /run/secrets/workload-spiffe-uds readOnly: true volumes: - name: workload-socket csi: driver: \u0026#34;csi.spiffe.io\u0026#34; readOnly: true components: pilot: k8s: env: # If enabled, if user introduces new intermediate plug-in CA, user need not to restart istiod to pick up certs. Istiod picks newly added intermediate plug-in CA certs and updates it. Plug-in new Root-CA not supported. - name: AUTO_RELOAD_PLUGIN_CERTS value: \u0026#34;true\u0026#34; ingressGateways: - name: istio-ingressgateway enabled: true label: istio: ingressgateway k8s: overlays: - apiVersion: apps/v1 kind: Deployment name: istio-ingressgateway patches: - path: spec.template.spec.volumes.[name:workload-socket] value: name: workload-socket csi: driver: \u0026#34;csi.spiffe.io\u0026#34; readOnly: true - path: spec.template.spec.containers.[name:istio-proxy].volumeMounts.[name:workload-socket] value: name: workload-socket mountPath: \u0026#34;/run/secrets/workload-spiffe-uds\u0026#34; readOnly: true EOF Since we are going to deploy the workload using the spire template declared in the Istio Operator, we run the following command to deploy the Bookinfo application.\nistioctl kube-inject -f bookinfo-with-spire-template.yaml | kubectl apply -f - Note: The bookinfo-with-spire-template.yaml file used in the above command can be found in this Gist , and the only difference with the *samples/bookinfo/platform/kube/bookinfo.yaml file in the Istio installer is that the following annotation has been added to the template for each Deployment:\nannotations: inject.istio.io/templates: \u0026#34;sidecar,spire\u0026#34; Use the following command to check whether SPIRE has issued an identity to the workload:\nkubectl exec -i -t spire-server-0 -n spire -c spire-server -- /bin/sh -c \u0026#34;bin/spire-server entry show -socketPath /run/spire/sockets/server.sock\u0026#34; You can see the identity of the productpage service in the output:\nEntry ID : 68182621-aa9d-448d-9020-9b6ab3640b94 SPIFFE ID : spiffe://example.org/ns/default/sa/bookinfo-productpage Parent ID : spiffe://example.org/k8s-workload-registrar/demo-cluster/node/gke-jimmy-cluster-default-pool-d5041909-5tzd Revision : 1 TTL : default Selector : k8s:node-name:gke-jimmy-cluster-default-pool-d5041909-5tzd Selector : k8s:ns:default Selector : k8s:pod-uid:6244a82c-2862-4452-a592-f79a41e5ccff DNS name : productpage-v1-6999d7b9d9-7szxm DNS name : productpage.default.svc View the certificate trust chain for the productpage pod:\nistioctl proxy-config secret deployment/productpage-v1 -o json | jq -r \\ \u0026#39;.dynamicActiveSecrets[0].secret.tlsCertificate.certificateChain.inlineBytes\u0026#39; | base64 --decode \u0026gt; chain.pem View the root certificate:\nistioctl proxy-config secret deployment/productpage-v1 -o json | jq -r \\ \u0026#39;.dynamicActiveSecrets[1].secret.validationContext.trustedCa.inlineBytes\u0026#39; | base64 --decode \u0026gt; root.pem Open the chain.pem file with a text editor, and you will see two certificates. Save the two certificates in separate files and use the openssl command openssl x509 -noout -text -in $FILE to parse the certificate contents, and you will see the certificate trust chain in Figure 3.\nFigure: Figure 3: Certificate trust chain for the productpage service. View the certificate of istiod:\nistioctl proxy-config secret deployment/istiod -o json | jq -r \\ \u0026#39;.dynamicActiveSecrets[0].secret.tlsCertificate.certificateChain.inlineBytes\u0026#39; | base64 --decode \u0026gt; chain.pem From the certificate trust chain, we can see that\ncert-manager acts as the root node of the PKI to issue certificates for istiod. SPIRE acts as an intermediate CA and then issues certificates for individual workloads. The URI in the X.509 v3 principal alias of the workloads in the Istio mesh follows the SPIFFE identity specification. Setting up Automatic Certificate Rotation If you want to modify the rotation period for istiod certificates from 60 days (1440 hours) to 30 days (720 hours), run the following command:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: cacerts namespace: istio-system spec: secretName: cacerts duration: 720h renewBefore: 360h commonName: istiod.istio-system.svc isCA: true usages: - digital signature - key encipherment - cert sign dnsNames: - istiod.istio-system.svc issuerRef: name: selfsigned-ca kind: ClusterIssuer group: cert-manager.io EOF Run the following command to view istiod‘s logs:\nkubectl logs -l app=istiod -n …","date":"2023-01-30T18:39:40+08:00","relpermalink":"/en/blog/cert-manager-spire-istio/","section":"blog","summary":"In this blog, I will guide you on how to use an external certificate authority (CA) to achieve fine-grained certificate management and automatic certificate rotation through the integration of SPIRE and cert-manager.","title":"Managing Certificates in Istio with cert-manager and SPIRE"},{"content":"I mentioned in my last article on understanding mTLS traffic encryption in Istio that the key to traffic encryption is certificate management. We can use the built-in certificate authority (CA) in Istio or a custom CA to manage certificates within the mesh. This blog post will explain how Istio handles certificate management.\nWhat Is a Certificate? There are many types of certificates, but in this article, I am explicitly referring to X.509 V3 certificates. X.509 certificates are a standard digital format used to identify entities in computer networks. X.509 is the international standard for public key infrastructure (PKI) and is primarily used for identity authentication and information encryption, such as TLS.\nThe contents of a certificate are hashed using a hash function and then signed with the issuer’s private key. This way, when a certificate is received, the recipient can use the issuer’s public key to verify the certificate’s validity.\nA hash function is a function that maps an input of any length (also called a message) to a fixed-length output, also called a hash value or message digest. There are many hash functions, such as MD5, SHA-1, etc.\nA certificate is like a business card issued by an authoritative agency for the user to prove their identity, and it can also be used to encrypt information to ensure the security and integrity of communication. The following diagram shows the general steps of TLS communication, where the certificate proves the server’s identity and encrypts the communication.\nFigure 1 shows an example of an HTTP call to a website, issuing a digital certificate, authenticating, and encrypting the communication.\nFigure: Figure 1. TLS certificate issuance and validation process Here are the detailed steps:\nThe server (website owner) submits a certificate signing request to the CA; The CA verifies the server’s identity and the authenticity of the website and issues a digital certificate to the server, which the server installs so that visitors can verify the website’s security; The user sends a request to the website through a browser (client); The server returns the TLS certificate to the client; The client verifies the certificate’s validity with the CA and establishes the connection if the certificate is valid, or prompts the user to reject the connection if it is invalid; The client generates a pair of random public and private keys; The client sends its public key to the server; The server encrypts the message using the client’s public key; The server sends the encrypted data to the client; The client decrypts the data sent by the server using its private key. At this point, both parties have established a secure channel and can transmit encrypted data in both directions.\nHow to Generate Certificates You can generate X.509 certificates with the following open-source tools:\nEasy-RSA : A simple command-line tool maintained by the OpenVPN project, EasyRSA can easily generate secure certificates and keys for the OpenVPN network. OpenSSL : Originated by an individual in 1995 and now maintained by an independent organization, OpenSSL provides only a command-line tool. CFSSL : Developed and maintained by CloudFlare, CFSSL is not just a command-line tool for generating certificates but also can serve as a PKI server. BoringSSL : A branch of OpenSSL developed and maintained by Google, BoringSSL is used in the Chrome browser and Android operating system. Since most people are probably familiar with OpenSSL, we will use OpenSSL to create certificates in the following text.\nCertificate Trust Chain The validation of a certificate requires using a certificate trust chain (Certificate Trust Chain). A certificate trust chain refers to a series of certificates used for identity verification, which form a chain starting from a trusted root certificate issuing agency, connecting downward step by step, until a specific intermediate or terminal certificate is used for verifying a particular certificate. The trustworthiness of a digital certificate increases as the certificate level increases in the certificate trust chain.\nIn Figure 2, you can see four trust chains.\nFigure: Figure 2. Certificate trust chains Certificate trust chains are tree-like structures, where each CA can have one or more child CAs. There are three roles:\nRoot CA: The top-level CA can issue certificates to intermediate CAs. Intermediate CA: They can issue end-entity certificates. End entity: A device or service that holds an end-entity certificate. Root CAs are the top-level issuing authorities for digital certificates, so the certificates they issue are the most trustworthy. Root certificate authorities are usually operated and regulated by government agencies or other authoritative organizations such as the International Infrastructure Security Organization. Common root CAs include:\nSymantec/VeriSign Comodo DigiCert GlobalSign GoDaddy Entrust GeoTrust RapidSSL Baltimore CyberTrust Root Please note that the above list is just a sample. There are many other root CAs.\nWhen you open an HTTPS webpage in a Chrome browser, you can view the certificate information by clicking on the lock icon on the left side of the address bar, which includes the certificate trust chain, such as the certificate trust chain of https://tetrate.io shown in Figure 3.\nFigure: Figure 3. Certificate hierarchy of tetrate.io The certificate trust chain allows the client (such as a web browser) to verify each certificate step by step when verifying the terminal certificate to determine whether it is trustworthy. The principle of digital certificate issuance is that the CA binds the certificate owner’s public key and identity information together. Then the CA uses its proprietary private key to generate a formal digital signature to indicate that the CA issued this certificate. The digital signature on the certificate can be verified using the CA’s public key during certificate verification.\nHow to Incorporate Istio into the PKI Certificate Trust Chain Before using Istio, enterprises usually have their own internal public key infrastructure . How can Istio be incorporated into the PKI certificate trust chain?\nIstio has built-in certificate management functions that can be used out of the box. When Istio is started, it will create a self-signed certificate for istiod as the root certificate for all workloads in the mesh. The problem with this is that the built-in root certificate cannot achieve mutual trust between meshes if you have multiple meshes. The correct approach is not to use Istio’s self-signed certificate, but to incorporate Istio into your certificate trust chain and integrate Istio into your PKI, creating an intermediate certificate for each PKI mesh. In this way, two meshes have a shared trust root and can achieve mutual trust between meshes.\nIntegrate the Istio mesh into your internal PKI certificate trust chain by creating an intermediate CA, as shown in Figure 4.\nFigure: Figure 4: Create intermediate CAs for the multi-cluster mesh to enable Istio to be included in the on-premises PKI certificate trust chain There are many benefits to incorporating Istio into your company’s internal PKI certificate trust chain:\nSecure communication across grids/clusters: with a common trust root, clusters can verify each other’s identities and communicate with each other; More granular certificate revocation: you can revoke the certificate of an individual entity or intermediate CA to revoke the certificate for service or cluster; Easy implementation of certificate rotation: you can rotate certificates by cluster/mesh rather than rotating the root node certificate, which reduces downtime. It is recommended to use cert-manager to achieve automated large-scale CA certificate rotation in production. For more information, see Automate Istio CA Rotation in Production at Scale ; For detailed instructions on incorporating Istio into your company’s internal PKI certificate trust chain, see this blog post .\nSteps for Using a Custom CA in Istio By default, the Istio CA generates a self-signed root certificate and key and uses them to sign workload certificates. To protect the root CA key, you should use a root CA that runs offline on a secure machine and the root CA to issue intermediate certificates to the Istio CA running on each cluster. The Istio CA can then use the administrator-specified certificate and key to sign workload certificates and distribute the administrator-specified root certificate as the trusted root to workloads.\nFigure 5 shows the certificate issuance and mounting process in Istio.\nFigure: Figure 5. Certificate issuance and mounting process in Istio In Istio, the Envoy proxy injects two processes into the Pod: envoy and pilot-agent. The pilot-agent generates a private key and uses the Secret Discovery Service (SDS) via a Unix Domain Socket (UDS) to request a certificate signing request (CSR) from the Certificate Authority (CA) through the istiod, if you have not configured the CA plugin. istiod, which has a built-in CA, returns the certificate to the pilot-agent. The pilot-agent then sends the generated private key and the CA-returned certificate to the Envoy instance to mount. Istio defaults to using the CA built into istiod, but also supports injecting other CAs, see the Istio documentation . Suppose you want to create identities for your services using custom CA certificates and keys. In that case, you will need to:\nCreate a CA configuration file and use it to create self-signed CA root certificates and keys. Create a private key and signing request configuration files for the service. Create a certificate signing request (CSR) for the service. Use the root certificate and key, along with the service’s signing request file, to create a certificate for the service. Next, we will detail the process of creating and mounting a certificate for the Bookinfo productpage service using Istio’s built-in CA as an example.\nProcess for Istio’s Built-in CA …","date":"2023-01-30T09:09:40+08:00","relpermalink":"/en/blog/istio-certificates-management/","section":"blog","summary":"This blog post will explain how Istio handles certificate management.","title":"How Are Certificates Managed in Istio?"},{"content":"In my last blog , I introduced transparent traffic intercepting and L4 routing in Ambient mode. In this blog, I will show you how L7 traffic is routed.\nThe figure below shows the L7 network traffic path in ambient mode.\nFigure: Figure 1: L7 network traffic in ambient mesh Note: The Waypoint proxy can be located on the same node as the application, and even all of the service and the Waypoint proxy can be on the same node. I draw them on three nodes for display purposes, but it has no significant impact on the actual traffic path, except that it is no longer sent to another node via eth0.\nIn the following section, we will explore the process in Figure 1 from a hands-on perspective.\nEnvironments for Waypoint Proxy Let’s continue to view the environment description using the ambient mode Istio deployed in the previous blog. To illustrate the L7 network routing, we need to create a Gateway on top of this.\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: gateway.networking.k8s.io/v1alpha2 kind: Gateway metadata: name: productpage annotations: istio.io/service-account: bookinfo-productpage spec: gatewayClassName: istio-mesh EOF After executing this command, a Waypoint proxy is created under the default namespace; in my environment, this pod is named bookinfo-productpage-waypoint-proxy-6f88c55d59-4dzdx and is specifically used to handle L7 traffic to the productpage service ( Service B), which I will call Waypoint proxy B.\nThe Waypoint proxy may be deployed in a different namespace, on a different node from the workload, or both. No matter where the Waypoint proxy is situated, the L7 traffic path is unaffected.\nWe will omit the sections of this blog dealing with intercepting inbound and outbound traffic because the way transparent traffic is handled in ambient mesh in L4 and L7 networks is similar. Details are available in the prior blog .\nWe will start directly with the traffic intercepted at Ztunnel A and then forward it to Envoy port 15006.\nOutbound Traffic Routing on Ztunnel A Use the following command to dump the Envoy proxy configuration on Ztunnel A:\nkubectl exec -n istio-system ztunnel-hptxk -c istio-proxy -- curl \u0026#34;127.0.0.1:15000/config_dump?include_eds\u0026#34;\u0026gt;ztunnel-a-all-include-eds.json 10.8.14.226 is the Cluster IP of the target service, and the service port is 9080. The traffic will be routed to the spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage cluster. Let’s look at the configuration of that cluster.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \u0026#34;10.8.14.226\u0026#34;: { \u0026#34;matcher\u0026#34;: { \u0026#34;matcher_tree\u0026#34;: { \u0026#34;input\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;port\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.matching.common_inputs.network.v3.DestinationPortInput\u0026#34; } }, \u0026#34;exact_match_map\u0026#34;: { \u0026#34;map\u0026#34;: { \u0026#34;9080\u0026#34;: { \u0026#34;action\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/google.protobuf.StringValue\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage\u0026#34; } } } } } } } } } 10.8.14.226 is the Cluster IP of the target service, and the service port is 9080. The traffic will be routed to the spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage cluster. Let’s look at the configuration of that cluster.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;version_info\u0026#34;: \u0026#34;2022-11-17T03:27:45Z/82\u0026#34;, \u0026#34;cluster\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.cluster.v3.Cluster\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;spiffe://cluster.local/ns/default/sa/sleep_to_server_waypoint_proxy_spiffe://cluster.local/ns/default/sa/bookinfo-productpage\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;EDS\u0026#34;, \u0026#34;eds_cluster_config\u0026#34;: { \u0026#34;eds_config\u0026#34;: { \u0026#34;ads\u0026#34;: {}, \u0026#34;initial_fetch_timeout\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;resource_api_version\u0026#34;: \u0026#34;V3\u0026#34; } }, /* omit */ } The cluster is discovered using the EDS service. To view the EDS information for this cluster:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.endpoint.v3.ClusterLoadAssignment\u0026#34;, \u0026#34;endpoints\u0026#34;: [ { \u0026#34;locality\u0026#34;: {}, \u0026#34;lb_endpoints\u0026#34;: [ { \u0026#34;endpoint\u0026#34;: { \u0026#34;address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;10.4.3.14\u0026#34;, \u0026#34;port_value\u0026#34;: 15006 } }, \u0026#34;health_check_config\u0026#34;: {} }, \u0026#34;health_status\u0026#34;: \u0026#34;HEALTHY\u0026#34;, \u0026#34;load_balancing_weight\u0026#34;: 1 } ] } ], \u0026#34;policy\u0026#34;: { \u0026#34;overprovisioning_factor\u0026#34;: 140 } } Note: The output cluster_name field is still missing here. See the GitHub issue .\nTraffic is forwarded here directly to the Waypoint Proxy endpoint at 10.4.3.14:15006.\nTraffic Routing Using Waypoint Proxy Let’s dump the Envoy configuration into Waypoint Proxy B again.\nkubectl exec -n default bookinfo-productpage-waypoint-proxy-6f88c55d59-4dzdx -c istio-proxy -- curl \u0026#34;127.0.0.1:15000/config_dump?include_eds\u0026#34;\u0026gt;waypoint-a-all-include-eds.json Look into the configuration of inbound_CONNECT_terminate listener:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 { \u0026#34;name\u0026#34;: \u0026#34;inbound_CONNECT_terminate\u0026#34;, \u0026#34;active_state\u0026#34;: { \u0026#34;version_info\u0026#34;: \u0026#34;2022-11-17T03:27:45Z/82\u0026#34;, \u0026#34;listener\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.listener.v3.Listener\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;inbound_CONNECT_terminate\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port_value\u0026#34;: 15006 } }, \u0026#34;filter_chains\u0026#34;: [{ \u0026#34;filters\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;capture_tls\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/udpa.type.v1.TypedStruct\u0026#34;, \u0026#34;type_url\u0026#34;: \u0026#34;type.googleapis.com/istio.tls_passthrough.v1.CaptureTLS\u0026#34; } }, { \u0026#34;name\u0026#34;: \u0026#34;envoy.filters.network.http_connection_manager\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\u0026#34;, \u0026#34;stat_prefix\u0026#34;: \u0026#34;inbound_hcm\u0026#34;, \u0026#34;route_config\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;local_route\u0026#34;, \u0026#34;virtual_hosts\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;connect\u0026#34;, \u0026#34;domains\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;routes\u0026#34;: [{...}, { \u0026#34;match\u0026#34;: { \u0026#34;headers\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;:authority\u0026#34;, \u0026#34;exact_match\u0026#34;: \u0026#34;10.8.14.226:9080\u0026#34; }], \u0026#34;connect_matcher\u0026#34;: {} }, \u0026#34;route\u0026#34;: { \u0026#34;cluster\u0026#34;: \u0026#34;inbound-vip|9080|internal|productpage.default.svc.cluster.local\u0026#34;, \u0026#34;upgrade_configs\u0026#34;: [{ \u0026#34;upgrade_type\u0026#34;: \u0026#34;CONNECT\u0026#34;, \u0026#34;connect_config\u0026#34;: {} }] } } ] }], \u0026#34;validate_clusters\u0026#34;: false }, \u0026#34;http_filters\u0026#34;: [...], \u0026#34;tracing\u0026#34;: {...}, \u0026#34;http2_protocol_options\u0026#34;: { \u0026#34;allow_connect\u0026#34;: true }, \u0026#34;use_remote_address\u0026#34;: false, \u0026#34;upgrade_configs\u0026#34;: [{ \u0026#34;upgrade_type\u0026#34;: \u0026#34;CONNECT\u0026#34; }], \u0026#34;stream_idle_timeout\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;normalize_path\u0026#34;: true, \u0026#34;request_id_extension\u0026#34;: {...}, \u0026#34;path_with_escaped_slashes_action\u0026#34;: \u0026#34;KEEP_UNCHANGED\u0026#34; } } ], \u0026#34;transport_socket\u0026#34;: {...}, \u0026#34;name\u0026#34;: \u0026#34;inbound_CONNECT_terminate\u0026#34; }] }, \u0026#34;last_updated\u0026#34;: \u0026#34;2022-11-17T06:24:51.467Z\u0026#34; } } TCP traffic destined for 10.8.14.226:9080 will be forwarded to the inbound-vip|9080|internal|productpage.default.svc.cluster.local cluster, and the HTTP method will be changed to CONNECT. To view the configuration of this cluster.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 { \u0026#34;version_info\u0026#34;: \u0026#34;2022-11-17T03:27:45Z/82\u0026#34;, \u0026#34;cluster\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.cluster.v3.Cluster\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;inbound-vip|9080|internal|productpage.default.svc.cluster.local\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;STATIC\u0026#34;, \u0026#34;transport_socket\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;envoy.transport_sockets.internal_upstream\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.transport_sockets.internal_upstream.v3.InternalUpstreamTransport\u0026#34;, \u0026#34;passthrough_metadata\u0026#34;: [ { \u0026#34;kind\u0026#34;: { \u0026#34;cluster\u0026#34;: {} }, \u0026#34;name\u0026#34;: \u0026#34;istio\u0026#34; } ], \u0026#34;transport_socket\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;envoy.transport_sockets.raw_buffer\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.transport_sockets.raw_buffer.v3.RawBuffer\u0026#34; } } } }, \u0026#34;common_lb_config\u0026#34;: {}, \u0026#34;load_assignment\u0026#34;: { \u0026#34;cluster_name\u0026#34;: \u0026#34;inbound-vip|9080|internal|productpage.default.svc.cluster.local\u0026#34;, \u0026#34;endpoints\u0026#34;: [ { \u0026#34;lb_endpoints\u0026#34;: [ { \u0026#34;endpoint\u0026#34;: { \u0026#34;address\u0026#34;: { \u0026#34;envoy_internal_address\u0026#34;: { \u0026#34;server_listener_name\u0026#34;: \u0026#34;inbound-vip|9080||productpage.default.svc.cluster.local\u0026#34; } } } } ] } ] } }, \u0026#34;last_updated\u0026#34;: \u0026#34;2022-11-17T03:27:46.137Z\u0026#34; } The endpoint of the cluster is an Envoy internal listener inbound-vip|9080||productpage.default.svc.cluster.local：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 { \u0026#34;name\u0026#34;: \u0026#34;inbound-vip|9080||productpage.default.svc.cluster.local\u0026#34;, \u0026#34;active_state\u0026#34;: { \u0026#34;version_info\u0026#34;: \u0026#34;2022-11-17T03:27:45Z/82\u0026#34;, \u0026#34;listener\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.listener.v3.Listener\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;inbound-vip|9080||productpage.default.svc.cluster.local\u0026#34;, \u0026#34;filter_chains\u0026#34;: [{ \u0026#34;filters\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;restore_tls\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/udpa.type.v1.TypedStruct\u0026#34;, \u0026#34;type_url\u0026#34;: \u0026#34;type.googleapis.com/istio.tls_passthrough.v1.RestoreTLS\u0026#34; } }, { \u0026#34;name\u0026#34;: \u0026#34;envoy.filters.network.http_connection_manager\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\u0026#34;, \u0026#34;stat_prefix\u0026#34;: \u0026#34;inbound_0.0.0.0_9080\u0026#34;, \u0026#34;route_config\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;inbound-vip|9080|http|productpage.default.svc.cluster.local\u0026#34;, \u0026#34;virtual_hosts\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;inbound|http|9080\u0026#34;, \u0026#34;domains\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;routes\u0026#34;: [{ \u0026#34;match\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;/\u0026#34; }, \u0026#34;route\u0026#34;: { \u0026#34;cluster\u0026#34;: \u0026#34;inbound-vip|9080|http|productpage.default.svc.cluster.local\u0026#34;, \u0026#34;timeout\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;max_stream_duration\u0026#34;: { \u0026#34;max_stream_duration\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;grpc_timeout_header_max\u0026#34;: \u0026#34;0s\u0026#34; } }, \u0026#34;decorator\u0026#34;: { \u0026#34;operation\u0026#34;: \u0026#34;:9080/*\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34; }] }], \u0026#34;validate_clusters\u0026#34;: false } }, \u0026#34;server_name\u0026#34;: \u0026#34;istio-envoy\u0026#34;, \u0026#34;use_remote_address\u0026#34;: false, \u0026#34;forward_client_cert_details\u0026#34;: \u0026#34;APPEND_FORWARD\u0026#34;, \u0026#34;set_current_client_cert_details\u0026#34;: { \u0026#34;subject\u0026#34;: true, \u0026#34;dns\u0026#34;: true, \u0026#34;uri\u0026#34;: …","date":"2023-01-06T08:09:40+08:00","relpermalink":"/en/blog/ambient-mesh-l7-traffic-path/","section":"blog","summary":"This article describes in detail the L7 traffic path in Ambient Mesh in both diagrammatic and hands-on form.","title":"L7 Traffic Path in Ambient Mesh"},{"content":"Istio’s new “ambient mode” is an experimental, “sidecar-less” deployment model for Istio . Instead of a sidecar proxy in front of every workload, ambient mode uses tproxy and HTTP Based Overlay Network Environment (HBONE) as key technologies for transparent traffic intercepting and routing that we covered in our recent article on transparent traffic intercepting and routing in the L4 network of Istio Ambient Mesh . In this article, we’ll take a closer look at tproxy and how it’s used.\nWhat Is a Proxy For? Proxies have a wide range of uses on the Internet, such as:\nRequest caching: to speed up network response, acting similarly to a CDN. Traffic filtering: used for network supervision, blocking or allowing access to specific hosts and websites. Traffic forwarding: used for load balancing or as a network relay. Traffic management: fine-grained management of traffic to and from the proxy, such as publishing to different backends by percentage, timeout and retry settings, circuit breaking, etc. Security auditing: logging and limiting client requests for billing or auditing purposes. Proxy Types There are a number of ways to classify proxies based on how they’re used. You can see two categories based on the location of the proxy in Figure 1:\nFigure: Figure 1: Forward proxy and reverse proxy. Forward proxies (like shadowsocks ) run on the client side and send requests to the server on behalf of the client. Reverse proxies (often in the form of a web server) accept Internet or external requests on behalf of the server and route them to the corresponding backends. Proxies may be located on the same node as the client or server or on a different node. We can classify them as transparent or non-transparent based on whether the client or server can see them. Figure 2 (below) shows the process of a client (A) sending a request to a server (C) through a proxy (B).\nFigure: Figure 2: Transparent vs. non-transparent proxies To use a non-transparent proxy, the client needs to explicitly change the destination address to that of the proxy server and use the proxy protocol to connect to the proxy server. To use a transparent proxy, the client and the server do not know the proxy is there, which means the client does not need to modify the destination address, and does not need to use the proxy protocol to connect to the proxy server; all the destination address conversion is done in the transparent proxy. Using the tproxy Transparent Proxy tproxy is a Linux kernel module (since Linux 2.2) that implements transparent proxies. To use tproxy, you must first use iptables to intercept the required packets at the required NIC, then listen for and forward the packet on that NIC.\nFollow these steps to use tproxy to implement a transparent proxy:\nFirst, you need to implement traffic interception: create a rule in the mangle table of the PREROUTING chain of iptables to intercept traffic and send it to tproxy for processing, for example, iptables -t mangle -A PREROUTING -p tcp -dport 9080 -j TPROXY --on-port 15001 --on-ip 127.0.0.1 --tproxy-mark 0x1/0x1, marking all TCP packets destined for port 9080 with a mark 1. You can specify the source IP address or IP set to further narrow the marking, with tproxy listening on port 15001. Create a routing rule that looks up all packets with mark 1 in a specific routing table: for example, add ip rule add fwmark 1 lookup 100, so that all packets with fwmark 1 look up to the routing table 100. Mapping packets to specific local addresses: for example, ip rule add local 0.0.0.0/0 dev lo table 100, which declares all IPv4 addresses as local in the routing table 100, but of course, this is just an example. In practice, you will need to forward packets with specific IPs to the local lo loopback NIC. The traffic has been intercepted on tproxy’s listening port 15001 (enter from Linux kernel space into user space). You can write a web application to process the packets or use tproxy-enabled software such as Squid or Envoy to process the packets.\nPros and Cons of Transparent Proxies Transparent proxies have the following advantages:\nHigher bandwidth and reduced transmission latency, thereby improving the quality of service. No need for users to configure networks and hosts. Control access to network services. Transparent proxies have the following disadvantages:\nIncorrectly configured, the transparent proxy may prevent connection to the Internet, leaving users unable to troubleshoot and fix errors. Security cannot be guaranteed, as intercepted user traffic may be tampered with by transparent proxies. The risk that transparent proxies may cache user information, leading to privacy leaks. Summary As a vital type of proxy, transparent proxies are used in a wide range of applications, whether in proxy software such as shadowsocks, Xray, or in the Istio service mesh. Understanding how they work helps us use proxies correctly, and whether or not you use a transparent proxy depends on how much you trust and know about it.\nThis blog was originally published at tetrate.io .\n","date":"2023-01-05T11:27:49+08:00","relpermalink":"/en/blog/what-is-tproxy/","section":"blog","summary":"Tproxy is used to intercept traffic in Istio Ambient mode.","title":"How Istio's Ambient Mode Transparent Proxy - tproxy Works Under the Hood"},{"content":"In cloud native applications, a request often needs to be processed through a series of APIs or backend services, some of which are parallel and some serial and located on different platforms or nodes. How do we determine the service paths and nodes a call goes through to help us troubleshoot the problem? This is where distributed tracing comes into play.\nThis article covers:\nHow distributed tracing works How to choose distributed tracing software How to use distributed tracing in Istio How to view distributed tracing data using Bookinfo and SkyWalking as examples Distributed Tracing Basics Distributed tracing is a method for tracing requests in a distributed system to help users better understand, control, and optimize distributed systems. There are two concepts used in distributed tracing: TraceID and SpanID. You can see them in Figure 1 below.\nTraceID is a globally unique ID that identifies the trace information of a request. All traces of a request belong to the same TraceID, and the TraceID remains constant throughout the trace of the request. SpanID is a locally unique ID that identifies a request’s trace information at a certain time. A request generates different SpanIDs at different periods, and SpanIDs are used to distinguish trace information for a request at different periods. TraceID and SpanID are the basis of distributed tracing. They provide a uniform identifier for request tracing in distributed systems and facilitate users’ ability to query, manage, and analyze the trace information of requests.\nFigure: Figure 1: Trace and span The following is the process of distributed tracing:\nWhen a system receives a request, the distributed tracing system assigns a TraceID to the request, which is used to chain together the entire chain of invocations. The distributed trace system generates a SpanID and ParentID for each service call within the system for the request, which is used to record the parent-child relationship of the call; a Span without a ParentID is used as the entry point of the call chain. TraceID and SpanID are to be passed during each service call. When viewing a distributed trace, query the full process of a particular request by TraceID. How Istio Implements Distributed Tracing Istio’s distributed tracing is based on information collected by the Envoy proxy in the data plane. After a service request is intercepted by Envoy, Envoy adds tracing information as headers to the request forwarded to the destination workload. The following headers are relevant for distributed tracing:\nAs TraceID: x-request-id Used to establish parent-child relationships for Span in the LightStep trace: x-ot-span-context\u0026lt;/li Used for Zipkin, also for Jaeger, SkyWalking, see b3-propagation : x-b3-traceid x-b3-traceid x-b3-spanid x-b3-parentspanid x-b3-sampled x-b3-flags b3 For Datadog: x-datadog-trace-id x-datadog-parent-id x-datadog-sampling-priority For SkyWalking: sw8 For AWS X-Ray: x-amzn-trace-id For more information on how to use these headers, please see the Envoy documentation .\nRegardless of the language of your application, Envoy will generate the appropriate tracing headers for you at the Ingress Gateway and forward these headers to the upstream cluster. However, in order to utilize the distributed tracing feature, you must modify your application code to attach the tracing headers to upstream requests. Since neither the service mesh nor the application can automatically propagate these headers, you can integrate the agent for distributed tracing into the application or manually propagate these headers in the application code itself. Once the tracing headers are propagated to all upstream requests, Envoy will send the tracing data to the tracer’s back-end processing, and then you can view the tracing data in the UI.\nFor example, look at the code of the Productpage service in the Bookinfo application . You can see that it integrates the Jaeger client library and synchronizes the header generated by Envoy with the HTTP requests to the Details and Reviews services in the getForwardHeaders (request) function.\ndef getForwardHeaders(request): headers = {} # Using Jaeger agent to get the x-b3-* headers span = get_current_span() carrier = {} tracer.inject( span_context=span.context, format=Format.HTTP_HEADERS, carrier=carrier) headers.update(carrier) # Dealing with the non x-b3-* header manually if \u0026#39;user\u0026#39; in session: headers[\u0026#39;end-user\u0026#39;] = session[\u0026#39;user\u0026#39;] incoming_headers = [ \u0026#39;x-request-id\u0026#39;, \u0026#39;x-ot-span-context\u0026#39;, \u0026#39;x-datadog-trace-id\u0026#39;, \u0026#39;x-datadog-parent-id\u0026#39;, \u0026#39;x-datadog-sampling-priority\u0026#39;, \u0026#39;traceparent\u0026#39;, \u0026#39;tracestate\u0026#39;, \u0026#39;x-cloud-trace-context\u0026#39;, \u0026#39;grpc-trace-bin\u0026#39;, \u0026#39;sw8\u0026#39;, \u0026#39;user-agent\u0026#39;, \u0026#39;cookie\u0026#39;, \u0026#39;authorization\u0026#39;, \u0026#39;jwt\u0026#39;, ] for ihdr in incoming_headers: val = request.headers.get(ihdr) if val is not None: headers[ihdr] = val return headers For more information, the Istio documentation provides answers to frequently asked questions about distributed tracing in Istio.\nHow to Choose A Distributed Tracing System Distributed tracing systems are similar in principle. There are many such systems on the market, such as Apache SkyWalking , Jaeger , Zipkin , Lightstep , Pinpoint , and so on. For our purposes here, we will choose three of them and compare them in several dimensions. Here are our inclusion criteria:\nThey are currently the most popular open-source distributed tracing systems. All are based on the OpenTracing specification. They support integration with Istio and Envoy. Items Apache SkyWalking Jaeger Zipkin Implementations Language-based probes, service mesh probes, eBPF agent, third-party instrumental libraries (Zipkin currently supported) Language-based probes Language-based probes Database ES, H2, MySQL, TiDB, Sharding-sphere, BanyanDB ES, MySQL, Cassandra, Memory ES, MySQL, Cassandra, Memory Supported Languages Java, Rust, PHP, NodeJS, Go, Python, C++, .Net, Lua Java, Go, Python, NodeJS, C#, PHP, Ruby, C++ Java, Go, Python, NodeJS, C#, PHP, Ruby, C++ Initiator Personal Uber Twitter Governance Apache Foundation CNCF CNCF Version 9.3.0 1.39.0 2.23.19 Stars 20.9k 16.8k 15.8k Although Apache SkyWalking’s agent does not support as many languages as Jaeger and Zipkin, SkyWalking’s implementation is richer and compatible with Jaeger and Zipkin trace data, and development is more active, so it is one of the best choices for building a telemetry platform.\nDemo Refer to the Istio documentation to install and configure Apache SkyWalking.\nEnvironment Description The following is the environment for our demo:\nKubernetes 1.24.5 Istio 1.16 SkyWalking 9.1.0 Install Istio Before installing Istio, you can check the environment for any problems:\n$ istioctl experimental precheck ✔ No issues found when checking the cluster. Istio is safe to install or upgrade! To get started, check out https://istio.io/latest/docs/setup/getting-started/ Then install Istio and configure the destination for sending tracing messages as SkyWalking:\n# Initial Istio Operator istioctl operator init # Configure tracing destination kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system name: istio-with-skywalking spec: meshConfig: defaultProviders: tracing: - \u0026#34;skywalking\u0026#34; enableTracing: true extensionProviders: - name: \u0026#34;skywalking\u0026#34; skywalking: service: tracing.istio-system.svc.cluster.local port: 11800 EOF Deploy Apache SkyWalking Istio 1.16 supports distributed tracing using Apache SkyWalking. Install SkyWalking by executing the following code:\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.16/samples/addons/extras/skywalking.yaml It will install the following components under the istio-system namespace:\nSkyWalking Observability Analysis Platform (OAP) : Used to receive trace data, supports SkyWalking native data formats, Zipkin v1 and v2 and Jaeger format. UI : Used to query distributed trace data. For more information about SkyWalking, please refer to the SkyWalking documentation .\nDeploy the Bookinfo Application Execute the following command to install the bookinfo application:\nkubectl label namespace default istio-injection=enabled kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml Launch the SkyWalking UI:\nistioctl dashboard skywalking Figure 2 shows all the services available in the bookinfo application:\nFigure: Figure 2: SkyWalking General Service page You can also see information about instances, endpoints, topology, tracing, etc. For example, Figure 3 shows the service topology of the bookinfo application:\nFigure: Figure 3: Topology diagram of the Bookinfo application Tracing views in SkyWalking can be displayed in a variety of formats, including list, tree, table, and statistics. See Figure 4:\nFigure: Figure 4: SkyWalking General Service trace supports multiple display formats To facilitate our examination, set the sampling rate of the trace to 100%:\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: telemetry.istio.io/v1alpha1 kind: Telemetry metadata: name: mesh-default namespace: istio-system spec: tracing: - randomSamplingPercentage: 100.00 EOF Important: It’s generally not good practice to set the sampling rate to 100% in a production environment. To avoid the overhead of generating too many trace logs in production, please adjust the sampling strategy (sampling percentage).\nUninstall After experimenting, uninstall Istio and SkyWalking by executing the following command.\nsamples/bookinfo/platform/kube/cleanup.sh istioctl unintall --purge kubectl delete namespace istio-system Understanding the Bookinfo Tracing Information Navigate to the General Service tab in the Apache SkyWalking UI, and you can see the trace information for the most recent istio-ingressgateway service, as shown in Figure 5. Click on each span to see the details.\nFigure: Figure 5: The table view shows the basic information about each span. Switching to the list view, you can see the execution order and duration of each span, as …","date":"2023-01-04T10:09:40+08:00","relpermalink":"/en/blog/distributed-tracing-with-skywalking-in-istio/","section":"blog","summary":"This blog will guide you to use SkyWalking for distributed tracing with Istio.","title":"How to Use SkyWalking for Distributed Tracing in Istio?"},{"content":"The Istio service mesh offers cloud native deployments a standard way to implement automatic mutual transport layer security (mTLS) . This reduces the attack surface of network communication by using strong identities to establish encrypted channels between workloads within the mesh that are both confidential and tamper-resistant. mTLS is a key component for building zero-trust application networks. To understand mTLS traffic encryption in Istio, this article will cover the following:\nAn overview of TLS, mTLS, and TLS termination An introduction to howTLS encryption works in Istio How to use Istio to implement mTLS in Kubernetes A discussion of when you do and don’t need mTLS What Is TLS and mTLS? TLS, the successor to Secure Sockets Layer (SSL), is a widely adopted security protocol used to create authenticated and encrypted connections between networked computers. For this reason, people often use the terms TLS and SSL interchangeably. In this article, we will refer to them collectively as TLS. TLS 1.0 was released in 1999, and the latest version is 1.3 (released in August 2018); versions 1.0 and 1.1 are deprecated.\nThe HTTPS we see when browsing the web uses TLS, as shown in Figure 1, which is built on top of TCP as the session layer in the OSI model. To ensure compatibility, TLS usually uses port 443, but you can use any port you want.\nFigure: Figure 1: HTTP vs. HTTPS TLS encryption is required when a client needs to confirm the identity of the server in order to guard against man-in-the-middle attacks and ensure communication security. Figure 2 shows how TLS-encrypted communication proceeds.\nFigure: Figure 2: simplified TLS handshake flow The server applies for and obtains a certificate (X.509 certificate) from a trusted certificate authority (CA). A request from the client to the server containing information such as the TLS version and password combination supported by the client. The server responds to the client request and attaches a digital certificate. The client verifies the status, validity, and digital signature of the certificate and confirms the identity of the server. Encrypted communication commences between the client and the server using a shared private key. The above is only an outline description of the TLS communication flow. If you’re interested in the details, please see this in-depth discussion of the complete TLS handshake process. From the above process, you will find that the certificate is the critical element representing the server’s identity. The server must use a certificate issued by an authoritatively certified CA in order to provide public services over the Internet. In contrast, you can manage certificates using your own public key infrastructure (PKI) for services inside of a private environment.\nMutual TLS, also referred to as mTLS, is the use of a two-way encrypted channel between a server and a client that necessitates certificate exchange and identity authentication between the parties.\nWhat Is TLS Termination? TLS termination is the process of decrypting TLS-encrypted traffic before it is forwarded to a web server. Offloading TLS traffic to an ingress gateway or specialized device improves web application performance while securing encrypted traffic. TLS termination is typically implemented at cluster ingress. All communication between the ingress and servers in the cluster will be conducted directly over HTTP in plaintext, enhancing service performance.\nFigure: Figure 3: TLS termination By default, Istio enables mTLS for mesh-based services and ends TLS at the ingress gateway. Furthermore, you can pass through traffic to back-end services for processing.\napiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: sample-gateway spec: servers: - port: number: 443 name: https protocol: HTTPS tls: mode: PASSTHROUGH See Gateway TLS Configuration for details.\nHow to Implement Automatic mTLS in Istio Figure 4 depicts the security architecture of Istio. This figure clearly shows that at the entry point, JSON Web Token (JWT) + TLS authentication and encryption are used, and that mTLS is enabled between all services within the Istio mesh.\nFigure: Istio 安全架构图 Istio includes a built-in CA, and Secret Discovery Service (SDS) —one of the discovery services in Envoy xDS —enables the issuance and rotation of SVID certificates. The mTLS flow in the Istio mesh is as follows:\nThe sidecar of every service requests a certificate from Istiod on behalf of the workload at startup, and Istiod issues the SVID certificate (the process is more complex, and I will explain it in a future blog). The sidecar of every workload intercepts all client requests within the pod. The client sidecar starts an mTLS handshake with the server sidecar. During the handshake, the JWT and authentication filter in the client sidecar will authenticate the identity of the request, and store the identity in the filter metadata after the authentication. Then the request will go through the authorization filter to determine if the request is allowed. If the request is authenticated and authorized, the client and the server start to establish a connection for communication. In Istio, authentication and authorization between services can be configured using one of three resource objects:\nRequestAuthentication : To specify the service’s only currently supported request-level authentication method, JWT. PeerAuthentication : To enable mTLS or plaintext requests, set the transport authentication mode. AuthorizationPolicy : To specify who can do what when traffic between services is authorized? For instance, subject A either permits (ALLOW) or forbids (DENY) traffic from subject B. How to Enable Automatic mTLS in Istio In PeerAuthentication, you can specify the mTLS mode that will be used for the target workload. Peer authentication is supported in the following modes:\nPERMISSIVE: The workload’s default setting that allows it to accept either mTLS or plain text traffic. STRICT: The workload accepts only mTLS traffic. DISABLE: Disable mTLS. From a security perspective, mTLS should not be disabled unless you have your own security solution. UNSET: Inherited from the parent, with the following priority: service specific \u0026gt; namespace scope \u0026gt; mesh scope setting. Istio’s peer authentication uses PERMISSIVE mode by default, automatically sending mTLS traffic to these workloads and clear text traffic to workloads without a sidecar. After including Kubernetes services in the Istio mesh, we can use PERMISSIVE mode first to prevent services from failing mTLS. We can use one of two ways to enable strict mTLS mode for certain services:\nUse PeerAuthentication to define how traffic is transferred between sidecars. Use DestinationRule to define the TLS settings in the traffic routing policy. The reviews service’s mTLS configuration in the default namespace can be seen in the example below.\nUse PeerAuthentication to Set mTLS for Workloads For instance, the following configuration can be used to specify that a workload under a namespace has strict mTLS enabled.\napiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: foo-peer-policy namespace: default spec: selector: matchLabels: app: reviews mtls: mode: STRICT According to the Istio documentation , you can also enable strict mTLS for all services in the mesh by configuring strict mTLS for the namespace istio-system where Istio is installed.\nUse DestinationRule to Set up mTLS for Workloads Traffic routing policies, such as load balancing, anomaly detection, TLS settings, etc., are set using DestinationRule. In the TLS settings, there are various modes. As shown below, use ISTIO_MUTUAL mode to enable Istio’s workload-based automatic TLS.\napiVersion: networking.istio.io/v1beta1 kind: DestinationRule metadata: name: reviews namespace: default spec: host: reviews trafficPolicy: tls: mode: ISTIO_MUTUAL When Should You Use mTLS? The short answer is that you should use mTLS for network communication between application components that you have some control over—like between microservices in a cluster.\nOne-way TLS is typically used by Internet clients to connect to Web services, which means that only the server needs to show identification and is unconcerned with the identity of the client. One-way TLS allows you to use passwords, tokens, two-factor authentication, and other methods when you need to confirm the identity of the client. However, when using a service mesh, mTLS operates outside the application and doesn’t require many changes to the application logic, whereas such an authentication method requires internal application support.\nAs you can see from the example above, mTLS implementation calls for certificate exchange between services. As the number of services rises, managing numerous certificates becomes a laborious task. You can implement automatic mTLS and fix the certificate management issue with the aid of a service mesh.\nWhen Shouldn’t You Use mTLS? Although mTLS is the preferred protocol for securing inter-service communication in cloud-native applications, implementing mTLS necessitates a more complex, symmetric encryption and decryption process than one-way TLS. In some cases where there is high traffic volume or CPU utilization must be optimized, terminatingTLS at the traffic entry point and turning on mTLS internally for specific services only can help minimize request response times and decrease compute resource consumption for some traffic with lower security levels.\nAdditionally, it is necessary to disable probe rewriting for pods when using services that cannot obtain certificates, such as health checks performed via HTTP on Kubelet and the inability to access the service’s health check endpoint via TLS.\nFinally, when mesh services access some external services, mTLS is also not necessary.\nSummary mTLS is a crucial component of creating a zero-trust application network, which makes it possible to encrypt traffic within the mesh. Istio makes …","date":"2022-12-24T14:09:40+08:00","relpermalink":"/en/blog/understanding-the-tls-encryption-in-istio/","section":"blog","summary":"This article introduces TLS and mTLS, and describes how to enable mTLS in Istio and its application scenarios.","title":"How Istio’s mTLS Traffic Encryption Works as Part of a Zero Trust Security Posture"},{"content":"Ambient mesh is an experimental new deployment model recently introduced to Istio. It splits the duties currently performed by the Envoy sidecar into two separate components: a node-level component for encryption (called “ztunnel”) and an L7 Envoy instance deployed per service for all other processing (called “waypoint”). The ambient mesh model is an attempt to gain some efficiencies in potentially improved lifecycle and resource management. You can learn more about what ambient mesh is and how it differs from the Sidecar pattern here .\nThis article takes you step-by-step through a hands-on approach to the transparent traffic intercepting and routing of L4 traffic paths in the Istio’s Ambient mode. If you don’t know what Ambient mode is, this article can help you understand.\nIf you want to skip the actual hands-on steps and just want to know the L4 traffic path in Ambient mode, please see the figure below, it shows a Pod of Service A calling a Pod of Service B on a different node below.\nFigure: Figure 1: Transparent traffic intercepting and routing in the L4 network of Istio Ambient Mesh Principles Ambient mode uses tproxy and HTTP Based Overlay Network Environment (HBONE) as key technologies for transparent traffic intercepting and routing:\nUsing tproxy to intercept the traffic from the host Pod into the Ztunnel (Envoy Proxy). Using HBONE to establish a tunnel for passing TCP traffic between Ztunnels. What Is tproxy? tproxy is a transparent proxy supported by the Linux kernel since version 2.2, where the t stands for transparent. You need to enable NETFILTER_TPROXY and policy routing in the kernel configuration. With tproxy, the Linux kernel can act as a router and redirect packets to user space. See the tproxy documentation for details.\nWhat Is HBONE? HBONE is a method of providing tunneling capabilities using the HTTP protocol. A client sends an HTTP CONNECT request (which contains the destination address) to an HTTP proxy server to establish a tunnel, and the proxy server establishes a TCP connection to the destination on behalf of the client, which can then transparently transport TCP data streams to the destination server through the proxy. In Ambient mode, Ztunnel (Envoy inside) acts as a transparent proxy, using Envoy Internal Listener to receive HTTP CONNECT requests and pass TCP streams to the upstream cluster.\nEnvironment Before starting the hands-on, it is necessary to explain the demo environment, and the corresponding object names in this article:\nItems Name IP Service A Pod sleep-5644bdc767-2dfg7 10.4.4.19 Service B Pod productpage-v1-5586c4d4ff-qxz9f 10.4.3.20 Ztunnel A Pod ztunnel-rts54 10.4.4.18 Ztunnel B Pod ztunnel-z4qmh 10.4.3.14 Node A gke-jimmy-cluster-default-pool-d5041909-d10i 10.168.15.222 Node B gke-jimmy-cluster-default-pool-d5041909-c1da 10.168.15.224 Service B Cluster productpage 10.8.14.226 Because these names will be used in subsequent command lines, the text will use pronouns, so that you can experiment in your own environment.\nFor the tutorial, I installed Istio Ambient mode in GKE. You can refer to this Istio blog post for installation instructions. Be careful not to install the Gateway, so as not to enable the L7 functionality; otherwise, the traffic path will be different from the descriptions in this blog.\nIn the following, we will experiment and dive into the L4 traffic path of a pod of sleep service to a pod of productpage service on different nodes. We will look at the outbound and inbound traffic of the Pods separately.\nOutbound Traffic Intercepting The transparent traffic intercepting process for outbound traffic from a pod in Ambient mesh is as follows:\nIstio CNI creates the istioout NIC and iptables rules on the node, adds the Pods’ IP in Ambient mesh to the IP set , and transparently intercepts outbound traffic from Ambient mesh to pistioout virtual NIC through Geneve (Generic Network Virtualization Encapsulation) tunnels with netfilter nfmark tags and routing rules. The init container in Ztunnel creates iptables rules that forward all traffic from the pistioout NIC to port 15001 of the Envoy proxy in Ztunnel. Envoy processes the packets and establishes an HBONE tunnel (HTTP CONNECT) with the upstream endpoints to forward the packets upstream. Check The Routing Rules On Node A Log in to Node A, where Service A is located, and use iptables-save to check the rules.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 $ iptables-save /* omit */ -A PREROUTING -j ztunnel-PREROUTING -A PREROUTING -m comment --comment \u0026#34;kubernetes service portals\u0026#34; -j KUBE-SERVICES -A ztunnel-POSTROUTING -m mark --mark 0x100/0x100 -j ACCEPT -A ztunnel-PREROUTING -m mark --mark 0x100/0x100 -j ACCEPT /* omit */ *mangle /* omit */ -A PREROUTING -j ztunnel-PREROUTING -A INPUT -j ztunnel-INPUT -A FORWARD -j ztunnel-FORWARD -A OUTPUT -j ztunnel-OUTPUT -A OUTPUT -s 169.254.169.254/32 -j DROP -A POSTROUTING -j ztunnel-POSTROUTING -A ztunnel-FORWARD -m mark --mark 0x220/0x220 -j CONNMARK --save-mark --nfmask 0x220 --ctmask 0x220 -A ztunnel-FORWARD -m mark --mark 0x210/0x210 -j CONNMARK --save-mark --nfmask 0x210 --ctmask 0x210 -A ztunnel-INPUT -m mark --mark 0x220/0x220 -j CONNMARK --save-mark --nfmask 0x220 --ctmask 0x220 -A ztunnel-INPUT -m mark --mark 0x210/0x210 -j CONNMARK --save-mark --nfmask 0x210 --ctmask 0x210 -A ztunnel-OUTPUT -s 10.4.4.1/32 -j MARK --set-xmark 0x220/0xffffffff -A ztunnel-PREROUTING -i istioin -j MARK --set-xmark 0x200/0x200 -A ztunnel-PREROUTING -i istioin -j RETURN -A ztunnel-PREROUTING -i istioout -j MARK --set-xmark 0x200/0x200 -A ztunnel-PREROUTING -i istioout -j RETURN -A ztunnel-PREROUTING -p udp -m udp --dport 6081 -j RETURN -A ztunnel-PREROUTING -m connmark --mark 0x220/0x220 -j MARK --set-xmark 0x200/0x200 -A ztunnel-PREROUTING -m mark --mark 0x200/0x200 -j RETURN -A ztunnel-PREROUTING ! -i veth300a1d80 -m connmark --mark 0x210/0x210 -j MARK --set-xmark 0x40/0x40 -A ztunnel-PREROUTING -m mark --mark 0x40/0x40 -j RETURN -A ztunnel-PREROUTING ! -s 10.4.4.18/32 -i veth300a1d80 -j MARK --set-xmark 0x210/0x210 -A ztunnel-PREROUTING -m mark --mark 0x200/0x200 -j RETURN -A ztunnel-PREROUTING -i veth300a1d80 -j MARK --set-xmark 0x220/0x220 -A ztunnel-PREROUTING -p udp -j MARK --set-xmark 0x220/0x220 -A ztunnel-PREROUTING -m mark --mark 0x200/0x200 -j RETURN -A ztunnel-PREROUTING -p tcp -m set --match-set ztunnel-pods-ips src -j MARK --set-xmark 0x100/0x100 IPtables rule descriptions:\nLine 3: the PREROUTING chain is the first to run, and all packets will go to the ztunnel-PEROUTING chain first. Line 4: packets are sent to the KUBE-SERVICES chain, where the Cluster IP of the Kubernetes Service is DNAT’d to the Pod IP. Line 6: packets with 0x100/0x100 flags pass through the PREROUTING chain and no longer go through the KUBE-SERVICES chain. Line 35: this is the last rule added to the ztunnel-PREROUTING chain; all TCP packets entering the ztunnel-PREROUTING chain that are in the ztunnel-pods-ips IP set (created by the Istio CNI) are marked with 0x100/0x100, which overrides all previous marks. See the Netfilter documentation for more information about nfmark. By implementing the above iptables rules, we can ensure that ambient mesh only intercepts packets from the ztunnel-pods-ips IP set pods and marks the packets with 0x100/0x100 (nfmark, in value/mask format, both value and mask are 32-bit binary integers) without affecting other pods.\nLet’s look at the routing rules for this node.\n$ ip rule 0: from all lookup local 100: from all fwmark 0x200/0x200 goto 32766 101: from all fwmark 0x100/0x100 lookup 101 102: from all fwmark 0x40/0x40 lookup 102 103: from all lookup 100 32766: from all lookup main 32767: from all lookup default The routing table will be executed sequentially, with the first column indicating the priority of the routing table and the second column indicating the routing table to look for or jump to. You will see that all packets marked with 0x100/0x100 will look for the 101 routing table. Let’s look at that routing table.\n$ ip route show table 101 default via 192.168.127.2 dev istioout 10.4.4.18 dev veth52b75946 scope link You will see the 101 routing table with the keyword via, which indicates that the packets will be transmitted through the gateway, see the usage of the ip route command . All packets are sent through the istioout NIC to the gateway (IP is 192.168.127.2). The other line indicates the routing link for the ztunnel pod on the current node.\nLet’s look at the details of the istioout NIC.\n1 2 3 4 5 6 7 8 $ ip -d addr show istioout 24: istioout: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1410 qdisc noqueue state UNKNOWN group default link/ether 62:59:1b:ad:79:01 brd ff:ff:ff:ff:ff:ff geneve id 1001 remote 10.4.4.18 ttl auto dstport 6081 noudpcsum udp6zerocsumrx numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 192.168.127.1/30 brd 192.168.127.3 scope global istioout valid_lft forever preferred_lft forever inet6 fe80::6059:1bff:fead:7901/64 scope link valid_lft forever preferred_lft forever The istioout NIC in Pod A is connected to the pstioout NIC in ztunnel A through the Geneve tunnel.\nCheck The Routing Rules On Ztunnel A Go to the Ztunnel A Pod and use the ip -d a command to check its NIC information.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ ip -d a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 promiscuity 0 minmtu 0 maxmtu 0 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0@if16: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1460 qdisc noqueue state UP group default link/ether 06:3e:d1:5d:95:16 brd ff:ff:ff:ff:ff:ff link-netnsid 0 promiscuity 0 minmtu 68 maxmtu 65535 veth numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 10.4.2.1/24 brd 10.4.4.255 scope global eth0 …","date":"2022-12-15T12:09:40+08:00","relpermalink":"/en/blog/ambient-mesh-l4-traffic-path/","section":"blog","summary":"This article details transparent traffic intercepting and L4 traffic paths in Ambient Mesh in both diagrammatic and hands-on form.","title":"Transparent Traffic Intercepting and Routing in the L4 Network of Istio Ambient Mesh"},{"content":"In September 2022, Istio became a CNCF incubation project and launched the new Ambient Mesh . With CNCF’s strong community and marketing resources, and Ambient Mesh further lowering the barrier to trying Istio, the five year old open source project has been revitalized.\nIf you don’t know about service mesh and Istio, or are curious about the future of Istio, this eBook—The Current State and Future of the Istio Service Mesh will give you the answers. The following is an excerpt from the book. In my view, the future of Istio lies in being the infrastructure for zero-trust network and hybrid cloud.\nZero Trust Zero trust is an important topic, including when I spoke at IstioCon 2022. Istio is becoming an important part of zero trust, the most important element of which is identity-oriented control rather than network-oriented control.\nWhat Is Zero Trust? Zero trust is a security philosophy, not a best practice that all security teams follow in the same way. The concept of zero trust was proposed to bring a more secure network to the cloud-native world. Zero trust is a theoretical state where all consumers within a network not only have no authority but also have no awareness of the surrounding network. The main challenges of zero trust are the increasingly granular authorization and the need for a time limit for user authorization.\nAuthentication Istio 1.14 adds support for the SPIFFE Runtime Environment (SPIRE). SPIRE, a CNCF incubation project, is an implementation of the Secure Production Identity Framework for Everyone (SPIFFE), also a CNCF Incubation Project. In Kubernetes, we use ServiceAccount to provide identity information for workloads in Pods, and its core is based on Token (using Secret resource storage) to represent workload identity. A token is a resource in a Kubernetes cluster. How to unify the identities of multiple clusters and workloads running in non-Kubernetes environments (such as virtual machines)? That’s what SPIFFE is trying to solve.\nThe purpose of SPIFFE is to establish an open and unified workload identity standard based on the concept of zero trust, which helps to establish a fully identifiable data center network with zero trust. The core of SPIFFE is to define a short-lived encrypted identity document—SPIFFE Verifiable Identity Document (SVID)—through a simple API, which is used as an identity document (based on an X.509 certificate or JWT token) for workload authentication. SPIRE can automatically rotate SVID certificates and keys according to administrator-defined policies, dynamically provide workload identities, and Istio can dynamically consume these workload identities through SPIRE.\nThe Kubernetes-based SPIRE architecture diagram is shown below.\nFigure: Figure 1: SPIRE deployed in Kubernetes Istio originally used the Citadel service in Istiod to be responsible for certificate management in the service mesh, and issued the certificate to the data plane through the xDS (to be precise, SDS API) protocol. With SPIRE, the work of certificate management is handed over to SPIRE Server. SPIRE also supports the Envoy SDS API. After we enable SPIRE in Istio, the traffic entering the workload pod will be authenticated once after being transparently intercepted into the sidecar. The purpose of authentication is to compare the identity of the workload with the environment information it runs on (node, Pod’s ServiceAccount and Namespace, etc.) to prevent identity forgery. Please refer to How to Integrate SPIRE in Istio to learn how to use SPIRE for authentication in Istio.\nWe can deploy SPIRE in Kubernetes using the Kubernetes Workload Registrar, which automatically registers the workload in Kubernetes for us and generates an SVID. The registration machine is a Server-Agent architecture, which deploys a SPIRE Agent on each node, and the Agent communicates with the workload through a shared UNIX Domain Socket. The following diagram shows the process of using SPIRE for authentication in Istio.\nFigure: Figure 2: SPIRE-based workload authentication process in Istio The steps to using SPIRE for workload authentication in Istio are as follows:\nTo obtain the SIVD, the SPIRE Agent is referred to as a pilot-agent via shared UDS. The SPIRE Agent asks Kubernetes (to be precise, the kubelet on the node) for load information. The kubelet returns the information queried from the API server to the workload validator. The validator compares the result returned by the kubelet with the identity information shared by the sidecar. If it is the same, it returns the correct SVID cache to the workload. If it is different, the authentication fails. Please refer to the SPIRE documentation for the detailed process of registering and authenticating workloads.\nNGAC When each workload has an accurate identity, how can the permissions of these identities be restricted? Role-based access control (RBAC) is used by default in Kubernetes for access control. As the name suggests, this access control is based on roles. Although it is relatively simple to use, there is a role explosion problem for large-scale clusters—that is, there are too many roles, and the types are not static, making it difficult to track and audit role permission models. In addition, the access rights of roles in RBAC are fixed, and there is no provision for short-term use rights; nor does it take into account attributes such as location, time, or equipment. Enterprises using RBAC have difficulty meeting complex access control requirements to comply with the regulatory requirements that other organizations demand.\nNGAC, or Next Generation Access Control, takes the approach of modeling access decision data as a graph. NGAC enables a systematic, policy-consistent approach to access control, granting or denying user management capabilities with a high level of granularity. NGAC was developed by the U.S. National Institute of Standards and Technology (NIST) and is currently used for rights management in Tetrate Service Bridge (TSB). For more information, read our article on why you should choose NGAC over ABAC and RBAC .\nHybrid cloud In practical applications, we may deploy multiple Kubernetes clusters in various environments for reasons such as load balancing; isolation of development and production environments; decoupling of data processing and data storage; cross-cloud backup and disaster recovery; and avoiding vendor lock-in. The Kubernetes community provides a “cluster federation” function that can help us create a multi-cluster architecture, such as the common Kubernetes multi-cluster architecture shown in the figure below, in which the host cluster serves as the control plane and has two member clusters, namely West and East.\nFigure: Figure 3: Kubernetes cluster federation architecture Cluster federation requires that the networks between the host cluster and member clusters can communicate with each other, but it does not require network connectivity between member clusters. The host cluster serves as the API entry, and all resource requests from the outside world to the host cluster will be forwarded to the member clusters.\nThe control plane of the cluster federation is deployed in the host cluster, and the “Push Reconciler” in it will propagate the identities, roles, and role bindings in the federation to all member clusters. Cluster federation simply “connects” multiple clusters together, replicating workloads among multiple clusters, and the traffic between member clusters cannot be scheduled, nor can true multi-tenancy be achieved.\nCluster federation is not enough to realize hybrid clouds. In order to realize hybrid clouds in the true sense, it is necessary to achieve interconnection between clusters and realize multi-tenancy at the same time. Tetrate Service Bridge provides a general control plane for multi-cluster management on top of Istio and then adds a management plane to manage multiple clusters, providing functions such as multi-tenancy, management configuration, and observability. Below is a diagram of the multi-tenancy and API of the Istio management plane.\nFigure: Figure 4: TSB’s management plane built on top of Istio In order to manage the hybrid cloud, TSB uses a management plane based on Istio, creates tenant and workspace resources, and applies the gateway group, traffic group, and security group to the workloads in the corresponding cluster through selectors.\nTetrate provides an enterprise-level service mesh management plane that works across clusters, across clouds, and with on-premises deployments, including VMs and bare metal servers. At Tetrate, we are committed to building an application-aware network suitable for any environment and any workload and providing a zero-trust hybrid cloud platform. And we contribute to building needed infrastructure in open source wherever possible; only management-plane-specific functionality is reserved for the commercial software offering.\nThe figure shown below is the architecture diagram of Tetrate’s flagship product, Tetrate Service Bridge.\nFigure: Figure 5: Tetrate Service Bridge architecture For the detailed architecture of TSB, please refer to the TSB documentation .\nGet the eBook The above is just a summary of what we see for the future of Istio. To learn more about the historical motivation for the emergence of the service mesh, the evolution of Istio, and the Istio open source ecosystem, please download the eBook . This book provides detailed information on:\nThe rise of service mesh technology is due to the popularity of Kubernetes, microservices, DevOps, and cloud native architectures. The emergence of Kubernetes and programmable proxies, which laid a solid foundation for Istio. How eBPF can accelerate transparent traffic hijacking in Istio, but cannot replace the sidecar in the service mesh. How the future of Istio lies in building a zero trust network based on the hybrid cloud. In addition, there are also numerous illustrations in the book that will help you understand. With …","date":"2022-12-13T14:18:40+08:00","relpermalink":"/en/blog/the-future-of-istio/","section":"blog","summary":"The future of Istio lies in being the infrastructure for zero-trust network and hybrid cloud.","title":"The Future of Istio: the Path to Zero Trust Security"},{"content":"In this blog, you will learn about the Kubernetes Ingress Gateway, the Gateway API, and the emerging Gateway API trend, which enables the convergence of Kubernetes and service mesh.\nTakeaways Ingress, the original gateway for Kubernetes, has a resource model that is too simple to fit into today’s programmable networks. The Gateway API , the latest addition to the Kubernetes portal gateway, separates concerns through role delineation and provides cross-namespace support to make it more adaptable to multi-cloud environments. Most API gateways already support it. The Gateway API provides a new reference model for the convergence of ingress gateways (north-south) and service mesh (east-west, cross-cluster routing), where there is a partial functional overlap. History of the Kubernetes ingress gateway When Kubernetes was launched in June 2014, only NodePort and LoadBalancer-type Service objects were available to expose services within the cluster to the outside world. Later, Ingress was introduced to offer more control over incoming traffic.. To preserve its portability and lightweight design, the Ingress API matured more slowly than other Kubernetes APIs; it was not upgraded to GA until Kubernetes 1.19.\nIngress’ primary objective is to expose HTTP applications using a straightforward declarative syntax. When creating an Ingress or setting a default IngressClass in Kubernetes, you can deploy several Ingress Controllers and define the controller the gateway uses via IngressClass. Kubernetes currently supports only AWS, GCE, and Nginx Ingress controllers by default; many third-party ingress controllers are also supported.\nThe following diagram illustrates the workflow of Kubernetes Ingress.\nFigure: Figure 1: Kubernetes ingress workflow The detailed process is as follows:\nKubernetes cluster administrators deploy an Ingress Controller in Kubernetes. The Ingress Controller continuously monitors changes to IngressClass and Ingress objects in the Kubernetes API Server. Administrators apply IngressClass and Ingress to deploy the gateway. Ingress Controller creates the corresponding ingress gateway and configures the routing rules according to the administrator’s configuration. If in the cloud, the client accesses the load balancer for that ingress gateway. The gateway will route the traffic to the corresponding back-end service based on the host and path in the HTTP request. Istio supports both the Ingress and Gateway APIs. Below is an example configuration using the Istio Ingress Gateway, which will be created later using the Gateway API:\napiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: istio spec: controller: istio.io/ingress-controller --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress spec: ingressClassName: istio rules: - host: httpbin.example.com http: paths: - path: / pathType: Prefix backend: service: name: httpbin port: 8000 Note: You must specify the IngressClass in the ingressClassName field in the Ingress spec. Otherwise, the ingress gateway will not be created.\nLimitations of Kubernetes Ingress Although IngressClass decouples the ingress gateway from the back-end implementation, it still has significant limitations.\nIngress is too simple for most real-world use and it only supports HTTP protocol routing. It only supports host and path matching, and there is no standard configuration for advanced routing features, which can only be achieved through annotation, such as URL redirection using Nginx Ingress Controller, which requires configuration of nginx.ingress.kubernetes.io/rewrite-target annotation, which is no longer adaptable to the needs of a programmable proxy. The situation where services in different namespaces must be bound to the same gateway often arises in practical situations where the ingress gateway cannot be shared across multiple namespaces. No delineation of responsibilities for creating and managing ingress gateways, resulting in developers having to not only configure gateway routes but also create and manage gateways themselves. Kubernetes Gateway API The Gateway API is a collection of API resources: GatewayClass, Gateway, HTTPRoute, TCPRoute, ReferenceGrant, etc. The Gateway API exposes a more generic proxy API that can be used for more protocols than HTTP and models more infrastructure components, providing better deployment and management options for cluster operations.\nIn addition, the Gateway API achieves configuration decoupling by separating resource objects that people can manage in different roles. The following diagram shows the roles and objects in the Gateway API.\nFigure: Figure: Roles and componentes in Kubernetes Gateway API The following is an example of using the Gateway API in Istio.\napiVersion: gateway.networking.k8s.io/v1alpha2 kind: Gateway metadata: name: gateway namespace: istio-ingress spec: gatewayClassName: istio listeners: - name: default hostname: \u0026#34;*.example.com\u0026#34; port: 80 protocol: HTTP allowedRoutes: namespaces: from: All --- apiVersion: gateway.networking.k8s.io/v1alpha2 kind: HTTPRoute metadata: name: http namespace: default spec: parentRefs: - name: gateway namespace: istio-ingress hostnames: [\u0026#34;httpbin.example.com\u0026#34;] rules: - matches: - path: type: PathPrefix value: / backendRefs: - name: httpbin port: 8000 Similar to Ingress, Gateway uses gatewayClassName to declare the controller it uses, which needs to be created by the platform administrator and allows client requests for the *.example.com domain. Application developers can create routing rules in the namespace where their service resides, in this case, default, and bind to the Gateway via parentRefs, but only if the Gateway explicitly allows them to do so (via the rules set in the allowRoutes field).\nWhen you apply the above configuration, Istio will automatically create a load-balancing gateway for you. The following diagram shows the workflow of the Gateway API.\nFigure: Figure 3: Gateway API workflow The detailed process is as follows:\nThe infrastructure provider provides GatewayClass and Gateway Controller. Platform operator deploy Gateway (multiple deployments possible, or using different GatewayClasses). Gateway Controller continuously monitors changes to the GatewayClass and Gateway objects in the Kubernetes API Server. Gateway controller will create the corresponding gateway based on cluster operations and maintenance configuration. Application developers apply xRoutes and bind them to the service. If in the cloud, the client accesses the load balancer for that ingress gateway. The gateway will route to the corresponding back-end service based on the matching criteria in the traffic request. From the above steps, we can see that the Gateway API has a clear division of roles compared to Ingress and that routing rules can be decoupled from the gateway configuration, significantly increasing management flexibility.\nThe following diagram shows the route flow after it is accessed at the gateway and processed.\nFigure: Figure 4: Gateway API route flow From this figure, we can see that the route is bound to the gateway. The route is generally deployed in the same namespace as its backend services. Suppose the route is in a different namespace, and you need to explicitly give the route cross-namespace reference rights in ReferenceGrant, for example. In that case, the following HTTPRoute foo in the foo namespace can refer to the bar namespace bar service in the bar namespace.\nkind: HTTPRoute metadata: name: foo namespace: foo spec: rules: - matches: - path: /bar forwardTo: backend: - name: bar namespace: bar --- kind: ReferenceGrant metadata: name: bar namespace: bar spec: from: - group: networking.gateway.k8s.io kind: HTTPRoute namespace: foo to: - group: \u0026#34;\u0026#34; kind: Service Currently, the Gateway API only supports HTTPRoute, and the TCPRoute, UDPRoute, TLSRoute and GRCPRoute are still in the experimental stage. The Gateway API is already supported by a large number of gateway and service mesh projects, and please check the support status in the official Gateway documentation. Ingress Gateway and Service Mesh The service mesh focuses on east-west traffic, i.e., traffic within a Kubernetes cluster, but most service meshes, including Istio, also provide ingress gateway functionality. But, since Istio’s ingress functionality and API are more advanced than we need for this example, we’ll use Service Mesh Interface (SMI) to illustrate the relationship between the ingress gateway and the service mesh.\nSMI is a CNCF incubation project, open-sourced in 2019, that defines a common standard for vendor-independent service mesh running in Kubernetes.\nThe following diagram illustrates the overlap between the Gateway API and the SMI.\nFigure: Figure 5: Gateway API vs SMI From the diagram, we can see a clear overlap between the Gateway API and SMI in the traffic specification section. These overlaps result in the same functionality that needs to be implemented repeatedly in both the Gateway API and the service mesh.\nTraffic Management in the Istio Service Mesh Of course, not all service meshes are fully SMI-compliant. Istio, the most popular service mesh implementation, provides rich traffic management features but does not have a separate policy API for these features, instead coupling them in VirtualService and DestinationRule, as shown below.\nVirtualService Routing: canary release, matching routes based on HTTP header, URI, etc. Error injection: HTTP error code injection, HTTP delay injection. Traffic splitting: percentage-based traffic splitting. Traffic mirroring: mirroring a certain percentage of traffic to other clusters. Timeout: set the timeout period, after which the request will fail. Retry: set the retry policy, such as trigger conditions, number of retries, interval time, etc. DestinationRule Load balancing: setting up load balancing policies, such as simple load balancing, locality-aware load balancing, and area-weighted load balancing. Circuit Breaking: Outlier …","date":"2022-12-13T11:18:40+08:00","relpermalink":"/en/blog/why-gateway-api-is-the-future-of-ingress-and-mesh/","section":"blog","summary":"This article introduces the ingress gateway and Gateway API in Kubernetes, the new trend of them with service mesh.","title":"Why the Gateway API Is the Unified Future of Ingress for Kubernetes and Service Mesh"},{"content":"This article reviews the development of Istio open source in the past five years and looks forward to the future direction of Istio. The main points of view in this article are as follows:\nDue to the popularity of Kubernetes, microservices, DevOps, and cloud-native architectures, the rise of service mesh technology. The rise of Kubernetes and programmable proxies has laid the technical groundwork for Istio’s implementation. While eBPF can accelerate transparent traffic hijacking in Istio, it can not replace sidecars in service meshes. The future of Istio is to build a zero-trust network. Next, we start this article with the background of the birth of Istio.\nThe eve of the birth of Istio Since 2013, with the explosion of the mobile Internet, enterprises have had higher requirements for the efficiency of application iteration. Application architecture has begun to shift from monolithic to microservices, and DevOps has also become popular. In the same year, with the open source of Docker, the problems of application encapsulation and isolation were solved, making it easier to schedule applications in the orchestration system. In 2014, Kubernetes and Spring Boot were open-sourced, and Spring framework development of microservice applications became popular. In the next few years, a large number of RPC middleware open source projects appeared, such as Google released gRPC 1.0 in 2016. The service framework is in full bloom. In order to save costs, increase development efficiency, and make applications more flexible, more and more enterprises are migrating to the cloud, but this is not just as simple as moving applications to the cloud. In order to use cloud computing more efficiently, a set of “cloud native” methods and concepts are also on the horizon.\nIstio Open Source Timeline Let’s briefly review the major events of Istio open source:\nSeptember 2016: Since Envoy is an important part of Istio, Istio’s open source timeline should have an Envoy part. At first, Envoy was only used as an edge proxy inside Lyft, and it was verified in large-scale production inside Lyft before Envoy was open sourced. In fact, Envoy was open sourced before it was open sourced, and it got the attention of Google engineers. At that time, Google was planning to launch an open source project of service mesh, and initially planned to use Nginx as a proxy. In 2017, Envoy donated to CNCF . May 2017: Istio was announced as open source by Google, IBM, and Lyft. The microservices architecture was used from the beginning. The composition of the data plane, control plane, and sidecar pattern were determined. March 2018: Kubernetes successfully became the first project to graduate from CNCF, becoming more and more “boring”. The basic API has been finalized. In the second edition, CNCF officially wrote the service mesh into the cloud native first definition. The company I currently work for, Tetrate , was founded by the Google Istio team. July 2018: Istio 1.0 is released, billed as “production ready”. March 2020: Istio 1.5 was released, the architecture returned to a monolithic application, the release cycle was determined, a major version was released every three months, and the API became stable. From 2020 to the present: The development of Istio mainly focuses on Day 2 operation, performance optimization, and extensibility. Several open source projects in the Istio ecosystem have begun to emerge, such as Slime , Areaki , and Merbridge . Why did Istio come after Kubernetes? The emergence mentioned here refers to the birth of the concept of “service mesh”. After microservices and containerization, the increase in the use of heterogeneous languages, the surge in the number of services, and the shortened life cycle of containers are the fundamental reasons for the emergence of service meshes.\nTo make it possible for developers to manage traffic between services with minimal cost, Istio needs to solve three problems:\nTransparent traffic hijacks traffic between applications, which means that developers can quickly use the capabilities brought by Istio without modifying applications. Another point is the operation and maintenance level; how to inject the proxy into each application and manage these distributed sidecar proxies efficiently. An efficient and scalable sidecar proxy that can be configured through an API. The above three conditions are indispensable for the Istio service mesh, and we can see from them that these requirements are basically the requirements for the sidecar proxy. The choice of this proxy will directly affect the direction and success of the project.\nIn order to solve the three problems above, Istio chose:\nContainer Orchestration and Scheduling Platform: Kubernetes Programable proxy: Envoy From the figure below, we can see the transition of the service deployment architecture from Kubernetes to Istio, with many changes and constants.\nFigure: Schematic diagram of the architectural change from Kubernetes to Istio From Kubernetes to Istio, in a nutshell, the deployment architecture of the application has the following characteristics:\nKubernetes manages the life cycle of applications, specifically, application deployment and management (scaling, automatic recovery, and release). Automatic sidecar injection based on Kubernetes to achieve transparent traffic interception. First, the inter-service traffic is intercepted through the sidecar proxy, and then the behavior of the microservice is managed through the control plane configuration. Nowadays, the deployment mode of service mesh has also ushered in new challenges. A sidecar is no longer necessary for Istio service mesh. The proxyless service mesh based on gRPC is also being tested. The service mesh decouples traffic management from Kubernetes, and the traffic inside the service mesh does not need the support of the kube-proxy component. Through the abstraction close to the microservice application layer, the traffic between services is managed to achieve security and observability functions. The control plane issues proxy configuration to the data plane through the xDS protocol. The proxies that have implemented xDS include Envoy and the open source MOSN . When a client outside the Kubernetes cluster accesses the internal services of the cluster, it was originally through Kubernetes Ingress, but after Istio is available, it will be accessed through Gateway. Transparent traffic hijacking If you are using middleware such as gRPC to develop microservices, after integrating the SDK into the program, the interceptor in the SDK will automatically intercept the traffic for you, as shown in the following figure.\nFigure: Interceptor diagram of gRPC How to make the traffic in the Kubernetes pod go through the proxy? The answer is to inject a proxy into each application pod, share the network space with the application, and then modify the traffic path within the pod so that all traffic in and out of the pod goes through the sidecar. Its architecture is shown in the figure below.\nFigure: Diagram of transparent traffic hijacking in Istio From the figure, we can see that there is a very complex set of iptables traffic hijacking logic. The advantage of using iptables is that it is applicable to any Linux operating system. But this also has some side effects:\nAll services in the Istio mesh need to add a network hop when entering and leaving the pod. Although each hop may only be two or three milliseconds, as the dependencies between services and services in the mesh increase, this latency may increase significantly, which may not be suitable for service meshes for services that pursue low latency. Because Istio injects a large number of sidecars into the data plane, especially when the number of services increases, the control plane needs to deliver more Envoy proxy configurations to the data plane, which will cause the data plane to occupy a lot of system memory and network resources. How to optimize the service mesh in response to these two problems?\nUse proxyless mode: remove the sidecar proxy and go back to the SDK. Optimize the data plane: reduce the frequency and size of proxy configurations delivered to the data plane. eBPF: it can be used to optimize network hijacking. This article will explain these details in the section on performance optimization later on.\nSidecar operation and maintenance management Istio is built on top of Kubernetes, which can leverage Kubernetes’ container orchestration and lifecycle management to automatically inject sidecars into pods through admission controllers when Kubernetes creates pods.\nIn order to solve the resource consumption problem of Sidecar, some people have proposed four deployment modes for the service mesh, as shown in the following figure.\nFigure: Schematic diagram of four deployment modes of service mesh The following table compares these four deployment methods in detail. Each of them has advantages and disadvantages. The specific choice depends on the current situation.\nMode Memory overhead Security Fault domain Operation and maintenance Sidecar proxy The overhead is greatest because a proxy is injected per pod. Since the sidecar must be deployed with the workload, it is possible for the workload to bypass the sidecar. Pod-level isolation, if the proxy fails, only the workload in the Pod is affected. A workload’s sidecar can be upgraded independently without affecting other workloads. Node sharing proxy There is only one proxy on each node, shared by all workloads on that node, with low overhead. There are security risks in the management of encrypted content and private keys. Node-level isolation, if a version conflict, configuration conflict, or extension incompatibility occurs when a shared proxy is upgraded, it may affect all workloads on that node. There is no need to worry about injecting sidecars. Service Account / Node Sharing Proxy All workloads under the service account/identity use a shared proxy with little overhead. …","date":"2022-07-18T12:27:49+08:00","relpermalink":"/en/blog/beyond-istio-oss/","section":"blog","summary":"This article explains the background of Istio's birth, its position in the cloud-native technology stack, and the development direction of Istio.","title":"The Current State and Future of the Istio Service Mesh"},{"content":"Istio 1.14 was released in June of this year, and one of the most notable features of this release is support for SPIRE , which is one of the implementations of SPIFFE , a CNCF incubation project. This article explains what SPIRE means for zero-trust architectures and why you would need SPIRE for authentication in Istio.\nAuthentication in Kubernetes We all know that Istio was built for and typically runs on Kubernetes, so before we talk about how to use SPIRE for authentication in Istio, let’s take a look at how Kubernetes handles authentication.\nLet’s look at an example of a pod’s token. Whenever a pod gets created in Kubernetes, it gets assigned a default service account from the namespace, assuming we didn’t explicitly assign a service account to it. Here is an example:\napiVersion: v1 data: ca.crt: {CA_CRT} namespace: ZGVmYXVsdA== token: {TOKEN_STRING} kind: Secret metadata: annotations: kubernetes.io/service-account.name: sleep kubernetes.io/service-account.uid: 2c0d00e8-13a2-48d0-9ff8-f987f3325ecf creationTimestamp: \u0026#34;2022-06-14T03:01:35Z\u0026#34; name: sleep-token-gwhwd namespace: default resourceVersion: \u0026#34;244535398\u0026#34; uid: b8822ceb-9553-4a17-96dc-d525bbaed0e0 type: kubernetes.io/service-account-token Kubernetes manages the identity of Pods with Service Accounts and then specifies the permissions of Pods with a Service Account to the Kubernetes API using RBAC. A service account’s token is stored in a secret, which does not include a declaration of the node or pod where the workload is running. When a malicious actor steals a token, they gain full access to the account and can steal information or carry out sabotage under the guise of that user.\nA token can only be used to identify a workload in one cluster, but Istio supports multiple clusters and meshes, as well as Kubernetes environments and virtual machines. A unified workload identity standard can help here.\nAn Introduction to SPIFFE and SPIRE SPIFFE’s (Secure Production Identity Framework for Everyone) goal is to create a zero-trust, fully-identified data center network by establishing an open, unified workload identity standard based on the concept of zero-trust. SPIRE can rotate X.509 SVID certificates and secret keys on a regular basis. Based on administrator-defined policies, SPIRE can dynamically provision workload certificates and Istio can consume them. I’ll go over some of the terms associated with SPIFFE in a little more detail below.\nSPIRE (SPIFFE Runtime Environment) is a SPIFFE implementation that is ready for production. SVID (SPIFFE Verifiable Identity Document) is the document that a workload uses to prove its identity to a resource or caller. SVID contains a SPIFFE ID that represents the service’s identity. It uses an X.509 certificate or a JWT token to encode the SPIFFE ID in a cryptographically verifiable document. The SPIFFE ID is a URI that looks like this: spiffe://trust_domain/workload_identifier.\nSPIFFE and Zero Trust Security The essence of Zero Trust is identity-centric dynamic access control. SPIFFE addresses the problem of identifying workloads.\nWe might identify a workload using an IP address and port in the era of virtual machines, but IP address-based identification is vulnerable to multiple services sharing an IP address, IP address forgery, and oversized access control lists. Because containers have a short lifecycle in the Kubernetes era, instead of an IP address, we rely on a pod or service name. However, different clouds and software platforms approach workload identity differently, and there are compatibility issues. This is especially true in heterogeneous hybrid clouds, where workloads run on both virtual machines and Kubernetes. It is critical to establish a fine-grained, interoperable identification system at this point.\nUsing SPIRE for Authentication in Istio With the introduction of SPIRE to Istio, we can give each workload a unique identity, which is used by workloads in the service mesh for peer authentication, request authentication, and authorization policies. The SPIRE Agent issues SVIDs for workloads by communicating with a shared UNIX Domain Socket in the workload. The Envoy proxy and the SPIRE agent communicate through the Envoy SDS (Secret Discovery Service) API. Whenever an Envoy proxy needs to access secrets (certificates, keys, or anything else needed to do secure communication), it will talk to the SPIRE agent through Envoy’s SDS API.\nThe most significant advantage of SDS is the ease with which certificates can be managed. Without this feature, certificates would have to be created as a secret and then mounted into the agent container in a Kubernetes deployment. The secret must be updated, and the proxy container must be re-deployed if the certificate expires. Using SDS, Istio can push the certificates to all Envoy instances in the service mesh. If the certificate expires, the server only needs to push the new certificate to the Envoy instance; Envoy will use the new certificate right away, and the proxy container will not need to be re-deployed.\nThe architecture of using SPIRE for authentication in Istio is depicted in the diagram below.\nFigure: SPIRE Architecture with Istio Use StatefulSet resources to deploy the SPIRE Server and Kubernetes Workload Registrar in the spire namespace of the Kubernetes cluster, and DaemonSet resources to deploy a SPIRE Agent for each node. Assuming that you used the default DNS name cluster.local when you install Kubernetes, Kubernetes Workload Registrar creates identities for the workloads in the Istio mesh in the following format:\nSPRRE Server:spiffe://cluster.local/ns/spire/sa/server SPIRE Agent:spiffe://cluster.local/ns/spire/sa/spire-agent Kubernetes Node:spiffe://cluster.local/k8s-workload-registrar/demo-cluster/node/ Kubernetes Workload Pod:spiffe://cluster.local/{namespace}/spire/sa/{service_acount} This way, both the nodes and each workload have their own globally unique identity and can be scaled according to the cluster (trust domain).\nThe workload authentication process in Istio mesh is shown in the figure below.\nFigure: The workload authentication process in the Istio mesh The detailed process is as follows:\nThe pilot-agent in the sidecar of the workload calls the SPIRE agent via the shared UDS to get the SVID. SPIRE Agent asks Kubernetes (kubelet on the node to be precise) for information about the workload. The kubelet returns the information queries from the Kubernetes API server to the workload attestor. The attestor compares the results returned by the kubelet with the identity information shared by the sidecar. If they match, returns the SVID to the workload and caches it, if not, the attestation failed. Please refer to the Istio documentation to learn how to use SPIRE for authentication in Istio.\nSummary SPIFFE unifies identity standards in heterogeneous environments, which is the foundation of zero-trust networks. In Istio, whether we use SPIRE or not, authentication is not perceptible to workloads. By using SPIRE to provide authentication for workloads, we can effectively manage workload identity and lay the foundation for a zero-trust network.\n","date":"2022-06-29T11:27:49+08:00","relpermalink":"/en/blog/why-istio-need-spire/","section":"blog","summary":"This article explains what SPIRE means for zero-trust architectures and why you would need SPIRE for authentication in Istio.","title":"Why Would You Need Spire for Authentication With Istio?"},{"content":" It’s been more than 5 years since Google, IBM, and Lyft unveiled the Istio open source project in May 2017. The Istio project has developed from a seed to a tall tree in these years. Many domestic books on the Istio service mesh were launched in the two years following the release of Istio 1.0 in 2018. My country is at the forefront of the world in the field of Istio book publishing.\nService mesh: one of the core technologies of cloud native Today, Istio is nearly synonymous with service mesh in China. The development of service mesh, as one of the core cloud-native technologies described by CNCF (Cloud Native Computing Foundation), has gone through the following stages.\n2017-2018: Exploratory Phase 2019-2020: Early Adopter Phase 2021 to present: Implementation on a large scale and ecological development stage Cloud native technology enables enterprises to design and deploy elastically scalable applications in new dynamic settings such as public, private, and hybrid clouds, according to the CNCF. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs are examples of cloud native technology.\nService mesh has been included to the CNCF definition of cloud native, indicating that it is one of the representative technologies of cloud native. Google is donating Istio to CNCF today, and we have reason to expect that as a CNCF project, Istio’s community will be more open, and its future development will be more smooth.\nService mesh and cloud native applications Cloud-native development is gaining traction. Despite the frequent emergence of new technologies and products, service mesh has maintained its place as “cloud-native network infrastructure” as part of the overall cloud-native technology stack throughout the past year. The cloud-native technology stack model is depicted in the diagram below, with representative technologies for each layer to define the standard. Service mesh and other cloud-native technologies complement each other as a new era of middleware emerges. Dapr (Distributed Application Runtime) defines the cloud-native middleware capability model, OAM defines the cloud-native application model, and so on, whereas service mesh Lattice defines a cloud-native seven-layer network model.\nFigure: Cloud Native Application Model Why you need a service mesh Using a service mesh isn’t tantamount to abandoning Kubernetes; it just makes sense. The goal of Kubernetes is to manage application lifecycles through declarative configuration, whereas the goal of service mesh is to provide traffic control, security management, and observability amongst apps. How do you set up load balancing and flow management for calls between services after a robust microservice platform has been developed with Kubernetes?\nMany open source tools, including Istio, Linkerd, MOSN, and others, support Envoy’s xDS protocol. The specification of xDS is Envoy’s most significant contribution to service mesh or cloud native. Many various usage cases, such as API gateways, sidecar proxies in service meshes, and edge proxies, are derived from Envoy, which is simply a network proxy, a modern version of the proxy configured through the API.\nIn a nutshell, the move from Kubernetes to Istio was made for the following reasons.\nApplication life cycle management, specifically application deployment and management, is at the heart of Kubernetes (scaling, automatic recovery, and release). Kubernetes is a microservices deployment and management platform that is scalable and extremely elastic. Transparent proxy is the cornerstone of service mesh, which intercepts communication between microservices via sidecar proxy and then regulates microservice behavior via control plane settings. The deployment mode of service meshes has introduced new issues today. For service meshes, sidecar is no longer required, and an agentless service mesh based on gRPC is also being tested. xDS is a protocol standard for configuring service meshes, and a gRPC-based xDS is currently being developed. Kubernetes traffic management is decoupled with the service mesh. The kube-proxy component is not required to support traffic within the service mesh. The traffic between services is controlled by an abstraction close to the microservice application layer to achieve security and observability features. In Kubernetes, service mesh is an upper-level abstraction of service, and Serverless is the next stage, which is why Google released Knative based on Kubernetes and Istio following Istio. Open source in the name of the community The ServiceMesher community was founded in May 2018 with the help of Ant Financial. Following that, a tornado of service meshes erupted in China, and the community-led translation of Istio’s official documentation reached a fever pitch.\nI became aware of a dearth of Chinese resources for systematically teaching Istio over time, so in September 2018, I began to plan and create an Istio book, launching the Istio Handbook open source e-book project on GitHub. I met many friends who are also interested in Istio and service mesh technology in the online and offline events of the community a few months later, with the promotion of service mesh technology and the expansion of the ServiceMesher community. We unanimously agreed to collaborate on an open source Istio e-book, which will compile the community’s important writings and experience into a logical text and make it available to the majority of developers.\nHundreds of people volunteered and began co-authoring the book in March 2019 under the auspices of the Community Stewardship Council. In May 2020, we created a cloud-native community that incorporated the original ServiceMesher community in order to further promote cloud-native technology and expand the technical knowledge supplied by the community. The scope of community operations has also widened, moving away from service mesh to more extensive cloud-native tools.\nThe editorial board for this book, which includes me, Ma Ruofei, Wang Baiping, Wang Wei, Luo Guangming, Zhao Huabing, Zhong Hua, and Guo Xudong, was founded in October 2020. We performed further version updates, improvements, and optimizations to this book under the supervision and assistance of the publishing business. This book, “In-depth Understanding of Isito: Advanced Practice of Cloud Native Service Mesh,” finally met you after many iterations.\nFigure: The book cover About this book After version 1.5, Istio underwent considerable architectural modifications, and various new or better features were added, including the addition of a smart DNS proxy, additional resource objects, increased support for virtual machines, and more.\nThis book is based on the new edition of Istio, and it aims to provide readers with the most up-to-date and comprehensive content possible by following the newest trends in the Istio community. Furthermore, several of the book’s authors are front-line development or operation and maintenance engineers with extensive Istio expertise, offering detailed and useful reference cases for the book.\nThis book is currently available on the JD.com . Please read “In-depth Understanding of Isito: Advanced Practice of Cloud Native Service Mesh” if you want to learn more about Istio!\n","date":"2022-06-15T20:27:49+08:00","relpermalink":"/en/blog/istio-service-mesh-book/","section":"blog","summary":"By the Cloud Native Community(China)","title":"In-Depth Understanding of Istio: Announcing the Publication of a New Istio Book"},{"content":"Achieving Open Source in the Name of Community In May 2018, with the support of Ant Group, the ServiceMesher community was established. Subsequently, a whirlwind of service mesh swept across China, and the community-led translation work of the official Istio documentation entered a fervent stage.\nAs time went on, I felt the lack of comprehensive Chinese materials introducing Istio. Therefore, in September 2018, I began conceptualizing a book about Istio and initiated the open-source e-book project “Istio Handbook” on GitHub. Several months later, with the promotion of service mesh technology and the expansion of the ServiceMesher community, I met many friends who were also passionate about Istio and service mesh technology through online and offline activities in the community. We unanimously decided to write an open-source e-book about Istio together, compiling valuable articles and experiences accumulated by the community into a systematic text to share with developers.\nIn March 2019, under the organization of the community’s management committee, dozens of members volunteered to participate and jointly write this book. In May 2020, to better promote cloud native technology and enrich the technical content shared by the community, we established the Cloud Native Community and incorporated the original ServiceMesher community into it. The content operated by the community also expanded from service mesh technology to a more comprehensive range of cloud native technologies.\nIn October 2020, the main contributors to this book formed the editorial committee, including myself, Ma Ruofei, Wang Bai Ping, Wang Wei, Luo Guangming, Zhao Huabing, Zhong Hua, and Guo Xudong. With the guidance and assistance of the publisher, we carried out subsequent version upgrades, improvements, and optimizations for this book. After repeated iterations, this book “Understanding Istio: Advanced Practices in Cloud Native Service Mesh” finally met with everyone.\nFigure: Book cover About This Book After version 1.5, Istio underwent significant architectural changes and introduced or improved multiple features, such as introducing intelligent DNS proxy, new resource objects, and improving support for virtual machines.\nThis book is based on the new versions of Istio and strives to provide readers with the latest and most comprehensive content while continuously tracking the latest trends in the Istio community. Additionally, many authors of this book are frontline developers or operation engineers with rich practical experience in Istio, providing valuable reference cases for this book.\n","date":"2022-06-01T00:00:00Z","relpermalink":"/en/book/istio-in-depth/","section":"book","summary":"This book was authored by the Cloud Native Community.","title":"Understanding Istio: Advanced Practices in Cloud Native Service Mesh"},{"content":"This article will guide you on how to compile the Istio binaries and Docker images on macOS.\nBefore you begin Before we start, refer to the Istio Wiki , here is the information about my build environment.\nmacOS 12.3.1 Darwin AMD64 Docker Desktop 4.8.1(78998) Docker Engine v20.10.14 Start to compile First, download the Istio code from GitHub to the $GOPATH/src/istio.io/istio directory, and execute the commands below in that root directory.\nCompile into binaries Execute the following command to download the Istio dependent packages, which will be downloaded to the vendor directory.\ngo mod vendor Run the following command to build Istio:\nsudo make build If you do not run the command with sudo, you may encounter the following error.\nfatal: unsafe repository (\u0026#39;/work\u0026#39; is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work fatal: unsafe repository (\u0026#39;/work\u0026#39; is owned by someone else) To add an exception for this directory, call: git config --global --add safe.directory /work Makefile.core.mk:170: *** \u0026#34;TAG cannot be empty\u0026#34;. Stop. make: *** [build] Error 2 Even if you follow the prompts and run git config --global --add safe.directory /work, you will still get errors during compilation.\nThe compiled binary will be saved in out directory with the following directory structure.\nout ├── darwin_amd64 │ ├── bug-report │ ├── client │ ├── envoy │ ├── extauthz │ ├── install-cni │ ├── istio-cni │ ├── istio-cni-taint │ ├── istio-iptables │ ├── istio_is_init │ ├── istioctl │ ├── logs │ ├── operator │ ├── pilot-agent │ ├── pilot-discovery │ ├── release │ └── server └── linux_amd64 ├── envoy ├── envoy-centos ├── logs └── release It will build both the linux_amd64 and darwin_amd64 architectures binaries at the same time.\nCompile into Docker images Run the following command to compile Istio into a Docker image.\nsudo make docker The compilation will take about 3 to 5 minutes depending on your network. Once the compilation is complete, you will see the Docker image of Istio by running the following command.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE localhost:5000/app_sidecar_centos_7 latest 2044037df94b 51 seconds ago 524MB localhost:5000/app_sidecar_ubuntu_jammy latest 5d8ae5ed55b7 About a minute ago 362MB localhost:5000/proxyv2 latest d4679412385f About a minute ago 243MB localhost:5000/install-cni latest 78f46d5771d2 About a minute ago 270MB localhost:5000/istioctl latest c38130a5adc8 About a minute ago 190MB localhost:5000/pilot latest 2aa9185ec202 About a minute ago 190MB localhost:5000/app latest 473adafaeb8d About a minute ago 188MB localhost:5000/operator latest 9ac1fedcdd12 About a minute ago 191MB localhost:5000/ext-authz latest 1fb5aaf20791 About a minute ago 117MB localhost:5000/app_sidecar_debian_11 latest 61376a02b95d 2 minutes ago 407MB localhost:5000/app_sidecar_ubuntu_xenial latest 7e8efe666611 2 minutes ago 418MB You can change the image name and push it into your own container registry.\nSummary This is how to build Istio on macOS. If you have already downloaded the Docker image you need to build, the build will take less than a minute. It also takes only a few minutes to build Docker images.\nReference Using the Code Base - github.com ","date":"2022-05-15T14:18:40+08:00","relpermalink":"/en/blog/how-to-build-istio/","section":"blog","summary":"This article will guide you on how to compile the Istio binaries on macOS.","title":"How to Build Istio?"},{"content":"Based on Istio version 1.13, this article will present the following.\nWhat is the sidecar pattern and what advantages does it have? How are the sidecar injections done in Istio? How does the sidecar proxy do transparent traffic intercepting? How is the traffic routed to upstream? The figure below shows how the productpage service requests access to http://reviews.default.svc.cluster.local:9080/ and how the sidecar proxy inside the reviews service does traffic blocking and routing forwarding when traffic goes inside the reviews service.\nFigure: Istio transparent traffic intercepting and traffic routing diagram At the beginning of the first step, the sidecar in the productpage pod has selected a pod of the reviews service to be requested via EDS, knows its IP address, and sends a TCP connection request.\nThere are three versions of the reviews service, each with an instance, and the sidecar work steps in the three versions are similar, as illustrated below only by the sidecar traffic forwarding step in one of the Pods.\nSidecar pattern Dividing the functionality of an application into separate processes running in the same minimal scheduling unit (e.g. Pod in Kubernetes) can be considered sidecar mode. As shown in the figure below, the sidecar pattern allows you to add more features next to your application without additional third-party component configuration or modifications to the application code.\nFigure: Sidecar pattern The Sidecar application is loosely coupled to the main application. It can shield the differences between different programming languages and unify the functions of microservices such as observability, monitoring, logging, configuration, circuit breaker, etc.\nAdvantages of using the Sidecar pattern When deploying a service mesh using the sidecar model, there is no need to run an agent on the node, but multiple copies of the same sidecar will run in the cluster. In the sidecar deployment model, a companion container (such as Envoy or MOSN) is deployed next to each application’s container, which is called a sidecar container. The sidecar takes overall traffic in and out of the application container. In Kubernetes’ Pod, a sidecar container is injected next to the original application container, and the two containers share storage, networking, and other resources.\nDue to its unique deployment architecture, the sidecar model offers the following advantages.\nAbstracting functions unrelated to application business logic into a common infrastructure reduces the complexity of microservice code. Reduce code duplication in microservices architectures because it is no longer necessary to write the same third-party component profiles and code. The sidecar can be independently upgraded to reduce the coupling of application code to the underlying platform. iptables manipulation analysis In order to view the iptables configuration, we need to nsenter the sidecar container using the root user to view it, because kubectl cannot use privileged mode to remotely manipulate the docker container, so we need to log on to the host where the productpage pod is located.\nIf you use Kubernetes deployed by minikube, you can log directly into the minikube’s virtual machine and switch to root. View the iptables configuration that lists all the rules for the NAT (Network Address Translation) table because the mode for redirecting inbound traffic to the sidecar is REDIRECT in the parameters passed to the istio-iptables when the Init container is selected for the startup, so there will only be NAT table specifications in the iptables and mangle table configurations if TPROXY is selected. See the iptables command for detailed usage.\nWe only look at the iptables rules related to productpage below.\n# login to minikube, change user to root $ minikube ssh $ sudo -i # See the processes in the productpage pod\u0026#39;s istio-proxy container $ docker top `docker ps|grep \u0026#34;istio-proxy_productpage\u0026#34;|cut -d \u0026#34; \u0026#34; -f1` UID PID PPID C STIME TTY TIME CMD 1337 10576 10517 0 08:09 ? 00:00:07 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage.default --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istiod.istio-system.svc:15012 --zipkinAddress zipkin.istio-system:9411 --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --connectTimeout 10s --proxyAdminPort 15000 --concurrency 2 --controlPlaneAuthPolicy NONE --dnsRefreshRate 300s --statusPort 15020 --trust-domain=cluster.local --controlPlaneBootstrap=false 1337 10660 10576 0 08:09 ? 00:00:33 /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage.default --service-node sidecar~172.17.0.16~productpage-v1-7f44c4d57c-ksf9b.default~default.svc.cluster.local --max-obj-name-len 189 --local-address-ip-version v4 --log-format [Envoy (Epoch 0)] [%Y-%m-%d %T.%e][%t][%l][%n] %v -l warning --component-log-level misc:error --concurrency 2 # Enter the nsenter into the namespace of the sidecar container (any of the above is ok) $ nsenter -n --target 10660 View the process’s iptables rule chain under its namespace.\n# View the details of the rule configuration in the NAT table. $ iptables -t nat -L -v # PREROUTING chain: Used for Destination Address Translation (DNAT) to jump all incoming TCP traffic to the ISTIO_INBOUND chain. Chain PREROUTING (policy ACCEPT 2701 packets, 162K bytes) pkts bytes target prot opt in out source destination 2701 162K ISTIO_INBOUND tcp -- any any anywhere anywhere # INPUT chain: Processes incoming packets and non-TCP traffic will continue on the OUTPUT chain. Chain INPUT (policy ACCEPT 2701 packets, 162K bytes) pkts bytes target prot opt in out source destination # OUTPUT chain: jumps all outbound packets to the ISTIO_OUTPUT chain. Chain OUTPUT (policy ACCEPT 79 packets, 6761 bytes) pkts bytes target prot opt in out source destination 15 900 ISTIO_OUTPUT tcp -- any any anywhere anywhere # POSTROUTING CHAIN: All packets must first enter the POSTROUTING chain when they leave the network card, and the kernel determines whether they need to be forwarded out according to the packet destination. Chain POSTROUTING (policy ACCEPT 79 packets, 6761 bytes) pkts bytes target prot opt in out source destination # ISTIO_INBOUND CHAIN: Redirects all inbound traffic to the ISTIO_IN_REDIRECT chain, except for traffic destined for ports 15090 (used by Prometheus) and 15020 (used by Ingress gateway for Pilot health checks), and traffic sent to these two ports will return to the call point of the iptables rule chain, the successor POSTROUTING to the INPUT chain. Chain ISTIO_INBOUND (1 references) pkts bytes target prot opt in out source destination 0 0 RETURN tcp -- any any anywhere anywhere tcp dpt:ssh 2 120 RETURN tcp -- any any anywhere anywhere tcp dpt:15090 2699 162K RETURN tcp -- any any anywhere anywhere tcp dpt:15020 0 0 ISTIO_IN_REDIRECT tcp -- any any anywhere anywhere # ISTIO_IN_REDIRECT chain: jumps all inbound traffic to the local 15006 port, thus successfully blocking traffic to the sidecar. Chain ISTIO_IN_REDIRECT (3 references) pkts bytes target prot opt in out source destination 0 0 REDIRECT tcp -- any any anywhere anywhere redir ports 15006 # ISTIO_OUTPUT chain: see the details bellow Chain ISTIO_OUTPUT (1 references) pkts bytes target prot opt in out source destination 0 0 RETURN all -- any lo 127.0.0.6 anywhere 0 0 ISTIO_IN_REDIRECT all -- any lo anywhere !localhost owner UID match 1337 0 0 RETURN all -- any lo anywhere anywhere ! owner UID match 1337 15 900 RETURN all -- any any anywhere anywhere owner UID match 1337 0 0 ISTIO_IN_REDIRECT all -- any lo anywhere !localhost owner GID match 1337 0 0 RETURN all -- any lo anywhere anywhere ! owner GID match 1337 0 0 RETURN all -- any any anywhere anywhere owner GID match 1337 0 0 RETURN all -- any any anywhere localhost 0 0 ISTIO_REDIRECT all -- any any anywhere anywhere # ISTIO_REDIRECT chain: redirects all traffic to Sidecar (i.e. local) port 15001. Chain ISTIO_REDIRECT (1 references) pkts bytes target prot opt in out source destination 0 0 REDIRECT tcp -- any any anywhere anywhere redir ports 15001 The focus here is on the 9 rules in the ISTIO_OUTPUT chain. For ease of reading, I will show some of the above rules in the form of a table as follows.\nRule target in out source destination 1 RETURN any lo 127.0.0.6 anywhere 2 ISTIO_IN_REDIRECT any lo anywhere !localhost owner UID match 1337 3 RETURN any lo anywhere anywhere !owner UID match 1337 4 RETURN any any anywhere anywhere owner UID match 1337 5 ISTIO_IN_REDIRECT any lo anywhere !localhost owner GID match 1337 6 RETURN any lo anywhere anywhere !owner GID match 1337 7 RETURN any any anywhere anywhere owner GID match 1337 8 RETURN any any anywhere localhost 9 ISTIO_REDIRECT any any anywhere anywhere The following diagram shows the detailed flow of the ISTIO_ROUTE rule.\nFigure: ISTIO_ROUTE iptables rules I will explain the purpose of each rule, corresponding to the steps and details in the illustration at the beginning of the article, in the order in which they appear. Where rules 5, 6, and 7 are extensions of the application of rules 2, 3, and 4 respectively (from UID to GID), which serve similar purposes and will be explained together. Note that the rules therein are executed in order, meaning that the rule with the next highest order will be used as the default. When the outbound NIC (out) is lo (local loopback address, loopback interface), it means that the destination of the traffic is the local Pod, and traffic sent from the Pod to the outside, will not go through this interface. Only rules 4, 7, 8, and 9 apply to all outbound traffic from the review Pod.\nRule 1\nPurpose: To pass through traffic sent by the Envoy proxy to the local application container, so that it bypasses the Envoy proxy and goes directly to the application container. Corresponds to …","date":"2022-05-12T21:08:59+08:00","relpermalink":"/en/blog/sidecar-injection-iptables-and-traffic-routing/","section":"blog","summary":"Learn the sidecar pattern, transparent traffic intercepting and routing in Istio.","title":"Understanding the Sidecar Injection, Traffic Intercepting \u0026 Routing Process in Istio"},{"content":"This article will explain:\nThe sidecar auto-injection process in Istio The init container startup process in Istio The startup process of a Pod with Sidecar auto-injection enabled The following figure shows the components of a Pod in the Istio data plane after it has been started.\nFigure: Istio data plane pod Sidecar injection in Istio The following two sidecar injection methods are available in Istio.\nManual injection using istioctl. Kubernetes-based mutating webhook admission controller automatic sidecar injection method. Whether injected manually or automatically, SIDECAR’s injection process follows the following steps.\nKubernetes needs to know the Istio cluster to which the sidecar to be injected is connected and its configuration. Kubernetes needs to know the configuration of the sidecar container itself to be injected, such as the image address, boot parameters, etc. Kubernetes injects the above configuration into the side of the application container by the sidecar injection template and the configuration parameters of the above configuration-filled sidecar. The sidecar can be injected manually using the following command.\nistioctl kube-inject -f ${YAML_FILE} | kuebectl apply -f - This command is injected using Istio’s built-in sidecar configuration, see the Istio official website for details on how to use Istio below.\nWhen the injection is complete you will see that Istio has injected initContainer and sidecar proxy-related configurations into the original pod template.\nInit container The Init container is a dedicated container that runs before the application container is launched and is used to contain some utilities or installation scripts that do not exist in the application image.\nMultiple Init containers can be specified in a Pod, and if more than one is specified, the Init containers will run sequentially. The next Init container can only be run if the previous Init container must run successfully. Kubernetes only initializes the Pod and runs the application container when all the Init containers have been run.\nThe Init container uses Linux Namespace, so it has a different view of the file system than the application container. As a result, they can have access to Secret in a way that application containers cannot.\nDuring Pod startup, the Init container starts sequentially after the network and data volumes are initialized. Each container must be successfully exited before the next container can be started. If exiting due to an error will result in a container startup failure, it will retry according to the policy specified in the Pod’s restartPolicy. However, if the Pod’s restartPolicy is set to Always, the restartPolicy is used when the Init container failed.\nThe Pod will not become Ready until all Init containers are successful. The ports of the Init containers will not be aggregated in the Service. The Pod that is being initialized is in the Pending state but should set the Initializing state to true. The Init container will automatically terminate once it is run.\nSidecar injection example analysis For a detailed YAML configuration for the bookinfo applications, see bookinfo.yaml for the official Istio YAML of productpage in the bookinfo sample.\nThe following will be explained in the following terms.\nInjection of Sidecar containers Creation of iptables rules The detailed process of routing apiVersion: apps/v1 kind: Deployment metadata: name: productpage-v1 labels: app: productpage version: v1 spec: replicas: 1 selector: matchLabels: app: productpage version: v1 template: metadata: labels: app: productpage version: v1 spec: serviceAccountName: bookinfo-productpage containers: - name: productpage image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9080 volumeMounts: - name: tmp mountPath: /tmp volumes: - name: tmp emptyDir: {} Let’s see the productpage container’s Dockerfile .\nFROM python:3.7.4-slim COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt COPY test-requirements.txt ./ RUN pip install --no-cache-dir -r test-requirements.txt COPY productpage.py /opt/microservices/ COPY tests/unit/* /opt/microservices/ COPY templates /opt/microservices/templates COPY static /opt/microservices/static COPY requirements.txt /opt/microservices/ ARG flood_factor ENV FLOOD_FACTOR ${flood_factor:-0} EXPOSE 9080 WORKDIR /opt/microservices RUN python -m unittest discover USER 1 CMD [\u0026#34;python\u0026#34;, \u0026#34;productpage.py\u0026#34;, \u0026#34;9080\u0026#34;] We see that ENTRYPOINT is not configured in Dockerfile, so CMD’s configuration python productpage.py 9080 will be the default ENTRYPOINT, keep that in mind and look at the configuration after the sidecar injection.\n$ istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml We intercept only a portion of the YAML configuration that is part of the Deployment configuration associated with productpage.\ncontainers: - image: docker.io/istio/examples-bookinfo-productpage-v1:1.15.0 # application image name: productpage ports: - containerPort: 9080 - args: - proxy - sidecar - --domain - $(POD_NAMESPACE).svc.cluster.local - --configPath - /etc/istio/proxy - --binaryPath - /usr/local/bin/envoy - --serviceCluster - productpage.$(POD_NAMESPACE) - --drainDuration - 45s - --parentShutdownDuration - 1m0s - --discoveryAddress - istiod.istio-system.svc:15012 - --zipkinAddress - zipkin.istio-system:9411 - --proxyLogLevel=warning - --proxyComponentLogLevel=misc:error - --connectTimeout - 10s - --proxyAdminPort - \u0026#34;15000\u0026#34; - --concurrency - \u0026#34;2\u0026#34; - --controlPlaneAuthPolicy - NONE - --dnsRefreshRate - 300s - --statusPort - \u0026#34;15020\u0026#34; - --trust-domain=cluster.local - --controlPlaneBootstrap=false image: docker.io/istio/proxyv2:1.5.1 # sidecar proxy name: istio-proxy ports: - containerPort: 15090 name: http-envoy-prom protocol: TCP initContainers: - command: - istio-iptables - -p - \u0026#34;15001\u0026#34; - -z - \u0026#34;15006\u0026#34; - -u - \u0026#34;1337\u0026#34; - -m - REDIRECT - -i - \u0026#39;*\u0026#39; - -x - \u0026#34;\u0026#34; - -b - \u0026#39;*\u0026#39; - -d - 15090,15020 image: docker.io/istio/proxyv2:1.5.1 # init container name: istio-init Istio’s configuration for application Pod injection mainly includes:\nInit container istio-init: for setting iptables port forwarding in the pod Sidecar container istio-proxy: running a sidecar proxy, such as Envoy or MOSN The two containers will be parsed separately.\nInit container analysis The Init container that Istio injects into the pod is named istio-init, and we see in the YAML file above after Istio’s injection is complete that the init command for this container is.\nistio-iptables -p 15001 -z 15006 -u 1337 -m REDIRECT -i \u0026#39;*\u0026#39; -x \u0026#34;\u0026#34; -b \u0026#39;*\u0026#39; -d 15090,15020 Let’s check the container’s Dockerfile again to see how ENTRYPOINT determines what commands are executed at startup.\n# ommit # The pilot-agent will bootstrap Envoy. ENTRYPOINT [\u0026#34;/usr/local/bin/pilot-agent\u0026#34;] We see that the entrypoint of the istio-init container is the /usr/local/bin/istio-iptables command line, and the location of the code for this command-line tool is in the tools/istio-iptables directory of the Istio source code repository.\nInit container initiation The Init container’s entrypoint is the istio-iptables command line, which is used as follows.\nUsage: istio-iptables [flags] Flags: -n, --dry-run Do not call any external dependencies like iptables -p, --envoy-port string Specify the envoy port to which redirect all TCP traffic (default $ENVOY_PORT = 15001) -h, --help help for istio-iptables -z, --inbound-capture-port string Port to which all inbound TCP traffic to the pod/VM should be redirected to (default $INBOUND_CAPTURE_PORT = 15006) --iptables-probe-port string set listen port for failure detection (default \u0026#34;15002\u0026#34;) -m, --istio-inbound-interception-mode string The mode used to redirect inbound connections to Envoy, either \u0026#34;REDIRECT\u0026#34; or \u0026#34;TPROXY\u0026#34; -b, --istio-inbound-ports string Comma separated list of inbound ports for which traffic is to be redirected to Envoy (optional). The wildcard character \u0026#34;*\u0026#34; can be used to configure redirection for all ports. An empty list will disable -t, --istio-inbound-tproxy-mark string -r, --istio-inbound-tproxy-route-table string -d, --istio-local-exclude-ports string Comma separated list of inbound ports to be excluded from redirection to Envoy (optional). Only applies when all inbound traffic (i.e. \u0026#34;*\u0026#34;) is being redirected (default to $ISTIO_LOCAL_EXCLUDE_PORTS) -o, --istio-local-outbound-ports-exclude string Comma separated list of outbound ports to be excluded from redirection to Envoy -i, --istio-service-cidr string Comma separated list of IP ranges in CIDR form to redirect to envoy (optional). The wildcard character \u0026#34;*\u0026#34; can be used to redirect all outbound traffic. An empty list will disable all outbound -x, --istio-service-exclude-cidr string Comma separated list of IP ranges in CIDR form to be excluded from redirection. Only applies when all outbound traffic (i.e. \u0026#34;*\u0026#34;) is being redirected (default to $ISTIO_SERVICE_EXCLUDE_CIDR) -k, --kube-virt-interfaces string Comma separated list of virtual interfaces whose inbound traffic (from VM) will be treated as outbound --probe-timeout duration failure detection timeout (default 5s) -g, --proxy-gid string Specify the GID of the user for which the redirection is not applied. (same default value as -u param) -u, --proxy-uid string Specify the UID of the user for which the redirection is not applied. Typically, this is the UID of the proxy container -f, --restore-format Print iptables rules in iptables-restore interpretable format (default true) --run-validation Validate iptables --skip-rule-apply Skip iptables apply The above incoming parameters are reassembled into iptables rules. For more information on how to use this command, visit tools/istio-iptables/pkg/cmd/root.go.\nThe significance of the container’s existence is that it allows the sidecar agent to intercept all inbound and outbound traffic to the pod, redirect all inbound traffic to port 15006 (sidecar) except port 15090 (used by Prometheus) and port 15092 (Ingress …","date":"2022-05-12T19:18:40+08:00","relpermalink":"/en/blog/istio-pod-process-lifecycle/","section":"blog","summary":"This article will explain Istio's Init container, Pod internal processes and the startup process.","title":"Istio Data Plane Pod Startup Process Explained"},{"content":"iptables is an important feature in the Linux kernel and has a wide range of applications. iptables is used by default in Istio for transparent traffic hijacking. Understanding iptables is very important for us to understand how Istio works. This article will give you a brief introduction to iptbles.\niptables introduction iptables is a management tool for netfilter, the firewall software in the Linux kernel. netfilter is located in the user space and is part of netfilter. netfilter is located in the kernel space and has not only network address conversion, but also packet content modification and packet filtering firewall functions.\nBefore learning about iptables for Init container initialization, let’s go over iptables and rule configuration.\nThe following figure shows the iptables call chain.\nFigure: iptables 调用链 iptables The iptables version used in the Init container is v1.6.0 and contains 5 tables.\nRAW is used to configure packets. Packets in RAW are not tracked by the system. The filter is the default table used to house all firewall-related operations. NAT is used for network address translation (e.g., port forwarding). Mangle is used for modifications to specific packets (refer to corrupted packets). Security is used to force access to control network rules. Note: In this example, only the NAT table is used.\nThe chain types in the different tables are as follows.\nRule name raw filter nat mangle security PREROUTING ✓ ✓ ✓ INPUT ✓ ✓ ✓ OUTPUT ✓ ✓ ✓ ✓ ✓ POSTROUTING ✓ ✓ FORWARD ✓ ✓ ✓ Understand iptables rules View the default iptables rules in the istio-proxy container, the default view is the rules in the filter table.\n$ iptables -L -v Chain INPUT (policy ACCEPT 350K packets, 63M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 18M packets, 1916M bytes) pkts bytes target prot opt in out source destination We see three default chains, INPUT, FORWARD, and OUTPUT, with the first line of output in each chain indicating the chain name (INPUT/FORWARD/OUTPUT in this case), followed by the default policy (ACCEPT).\nThe following is a proposed structure diagram of iptables, where traffic passes through the INPUT chain and then enters the upper protocol stack, such as:\nFigure: iptables chains Multiple rules can be added to each chain and the rules are executed in order from front to back. Let’s look at the table header definition of the rule.\nPKTS: Number of matched messages processed bytes: cumulative packet size processed (bytes) Target: If the message matches the rule, the specified target is executed. PROT: Protocols such as TDP, UDP, ICMP, and ALL. opt: Rarely used, this column is used to display IP options. IN: Inbound network interface. OUT: Outbound network interface. source: the source IP address or subnet of the traffic, the latter being anywhere. destination: the destination IP address or subnet of the traffic, or anywhere. There is also a column without a header, shown at the end, which represents the options of the rule, and is used as an extended match condition for the rule to complement the configuration in the previous columns. prot, opt, in, out, source and destination and the column without a header shown after destination together form the match rule. TARGET is executed when traffic matches these rules.\nTypes supported by TARGET\nTarget types include ACCEPT, REJECT, DROP, LOG, SNAT, MASQUERADE, DNAT, REDIRECT, RETURN or jump to other rules, etc. You can determine where the telegram is going by executing only one rule in a chain that matches in order, except for the RETURN type, which is similar to the return statement in programming languages, which returns to its call point and continues to execute the next rule.\nFrom the output, you can see that the Init container does not create any rules in the default link of iptables, but instead creates a new link.\nSummary With the above brief introduction to iptables, you have understood how iptables works, the rule chain and its execution order.\n","date":"2022-05-12T12:18:40+08:00","relpermalink":"/en/blog/understanding-iptables/","section":"blog","summary":"This article will give you a brief introduction to iptables, its tables and the order of execution.","title":"Understanding IPTables"},{"content":"See the cloud native public library at: https://jimmysong.io/book/ I have also adjusted the home page, menu and directory structure of the site, and the books section of the site will be maintained using the new theme.\nCloud native library positioning The cloud native public library is a collection of cloud native related books and materials published and translated by the author since 2017, and is a compendium and supplement to the dozen or so books already published. The original materials will continue to be published in the form of GitBooks, and the essence and related content will be sorted into the cloud native public library through this project.\nIn addition, the events section of this site has been revamped and moved to a new page .\n","date":"2022-05-10T08:27:49+08:00","relpermalink":"/en/notice/cloud-native-public-library/","section":"notice","summary":"A one-stop cloud native library that is a compendium of published materials.","title":"Announcement of Cloud Native Library"},{"content":"In my last two blogs:\nSidecar injection, transparent traffic hijacking , and routing process in Istio explained in detail Traffic types and iptables rules in Istio sidecar explained I gave you a detailed overview of the traffic in the Istio data plane, but the data plane does not exist in isolation. This article will show you the ports and their usages for each component of both the control plane and data plane in Istio, which will help you understand the relationship between these flows and troubleshoot them.\nOverview Firstly, I will show you a global schematic. The following figure shows the components of a sidecar in the Istio data plane, and the objects that interact with it.\nFigure: Istio components We can use the nsenter command to enter the namespace of the productpage Pod of the Bookinfo example and see the information about the ports it is listening on internally.\nFigure: Istio sidecar ports From the figure, we can see that besides the port 9080 that the productpage application listens to, the Sidecar container also listens to a large number of other ports, such as 15000, 15001, 15004, 15006, 15021, 15090, etc. You can learn about the ports used in Istio in the Istio documentation .\nLet’s go back into the productpage Pod and use the lsof -i command to see the ports it has open, as shown in the following figure.\nFigure: Productpage Pod ports We can see that there is a TCP connection established between the pilot-agent and istiod, the port in the listening described above, and the TCP connection established inside the Pod, which corresponds to the figure at the beginning of the article.\nThe root process of the Sidecar container (istio-proxy) is pilot-agent, and the startup command is shown below.\nFigure: Internal procecces in Sidecar As we can see from the figure, the PID of its pilot-agent process is 1, and it forked the Envoy process.\nCheck the ports it opens in Istiod, as shown in the figure below.\nFigure: Istiod ports We can see the ports that are listened to, the inter-process and remote communication connections.\nPorts usage overview These ports can play a pivotal role when you are troubleshooting. They are described below according to the component and function in which the port is located.\nPorts in Istiod The ports in Istiod are relatively few and single-function.\n9876: ControlZ user interface, exposing information about Istiod’s processes 8080: Istiod debugging port, through which the configuration and status information of the grid can be queried 15010: Exposes the xDS API and issues plain text certificates 15012: Same functionality as port 15010, but uses TLS communication 15014: Exposes control plane metrics to Prometheus 15017: Sidecar injection and configuration validation port Ports in sidecar From the above, we see that there are numerous ports in the sidecar.\n15000: Envoy admin interface, which you can use to query and modify the configuration of Envoy Proxy. Please refer to Envoy documentation for details. 15001: Used to handle outbound traffic. 15004: Debug port (explained further below). 15006: Used to handle inbound traffic. 15020: Summarizes statistics, perform health checks on Envoy and DNS agents, and debugs pilot-agent processes, as explained in detail below. 15021: Used for sidecar health checks to determine if the injected Pod is ready to receive traffic. We set up the readiness probe on the /healthz/ready path on this port, and Istio hands off the sidecar readiness checks to kubelet. 15053: Local DNS proxy for scenarios where the cluster’s internal domain names are not resolved by Kubernetes DNS. 15090: Envoy Prometheus query port, through which the pilot-agent will scratch metrics. The above ports can be divided into the following categories.\nResponsible for inter-process communication, such as 15001, 15006, 15053 Health check and information statistics, e.g. 150021, 15090 Debugging: 15000, 15004 Let’s look at the key ports in detail.\n15000 15000 is Envoy’s Admin interface, which allows us to modify Envoy and get a view and query metrics and configurations.\nThe Admin interface consists of a REST API with multiple endpoints and a simple user interface. You can enable the Envoy Admin interface view in the productpage Pod using the following command:\nkubectl -n default port-forward deploy/productpage-v1 15000 Visit http://localhost:15000 in your browser and you will see the Envoy Admin interface as shown below.\nFigure: Envoy Admin interface 15004 With the pilot-agent proxy istiod debug endpoint on port 8080, you can access localhost’s port 15004 in the data plane Pod to query the grid information, which has the same effect as port 8080 below.\n8080 You can also forward istiod port 8080 locally by running the following command:\nkubectl -n istio-system port-forward deploy/istiod 8080 Visit http://localhost:8080/debug in your browser and you will see the debug endpoint as shown in the figure below.\nFigure: Pilot Debug Console Of course, this is only one way to get the mesh information and debug the mesh, you can also use istioctl command or Kiali to debug it, which will be more efficient and intuitive.\n15020 Port 15020 has three main usages.\nAggregating metrics: You can query port 15090 for Envoy’s metrics, or you can configure it to query the application’s metrics, aggregating Envoy, application, and its own metrics for Prometheus to collect. The corresponding debug endpoint is /stats/prometheus. Performing health checks on Envoy and DNS agent: the corresponding debug endpoints are /healthz/ready and /app-health. Debugging pilot-agent processes: the corresponding debug endpoints are /quitquitquit, debug/ndsz and /debug/pprof. The following figure shows the debugging information you see when you open http://localhost:15020/debug/pprof in your browser.\nFigure: pprof endpoint The information in the figure shows the stack information of the pilot-agent.\nSummary By understanding the component ports in Istio, you should have a better understanding of the relationship between the components in Istio and their internal traffic. Being familiar with the functions of these ports will help in troubleshooting the mesh.\n","date":"2022-05-08T10:18:40+08:00","relpermalink":"/en/blog/istio-components-and-ports/","section":"blog","summary":"This article will introduce you to the various ports and functions of the Istio control plane and data plane.","title":"Istio Component Ports and Functions in Details"},{"content":"As we know that Istio uses iptables for traffic hijacking, where the iptables rule chains has one called ISTIO_OUTPUT, which contains the following rules.\nRule target in out source destination 1 RETURN any lo 127.0.0.6 anywhere 2 ISTIO_IN_REDIRECT any lo anywhere !localhost owner UID match 1337 3 RETURN any lo anywhere anywhere !owner UID match 1337 4 RETURN any any anywhere anywhere owner UID match 1337 5 ISTIO_IN_REDIRECT any lo anywhere !localhost owner GID match 1337 6 RETURN any lo anywhere anywhere !owner GID match 1337 7 RETURN any any anywhere anywhere owner GID match 1337 8 RETURN any any anywhere localhost 9 ISTIO_REDIRECT any any anywhere anywhere The sidecar applies these rules to deal with different types of traffic. This article will show you the six types of traffic and their iptables rules in Istio sidecar.\niptables Traffic Routing in Sidecar The following list summarizes the six types of traffic in Sidecar.\nRemote service accessing local service: Remote Pod -\u0026gt; Local Pod Local service accessing remote service: Local Pod -\u0026gt; Remote Pod Prometheus crawling metrics of local service: Prometheus -\u0026gt; Local Pod Traffic between Local Pod service: Local Pod -\u0026gt; Local Pod Inter-process TCP traffic within Envoy Sidecar to Istiod traffic The following will explain the iptables routing rules within Sidecar for each scenario, which specifies which rule in ISTIO_OUTPUT is used for routing.\nType 1: Remote Pod -\u0026gt; Local Pod The following are the iptables rules for remote services, applications or clients accessing the local pod IP of the data plane.\nRemote Pod -\u0026gt; RREROUTING -\u0026gt; ISTIO_INBOUND -\u0026gt; ISTIO_IN_REDIRECT -\u0026gt; Envoy 15006 (Inbound) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 1 -\u0026gt; POSTROUTING -\u0026gt; Local Pod\nWe see that the traffic only passes through the Envoy 15006 Inbound port once. The following diagram shows this scenario of the iptables rules.\nFigure: Remote Pod to Local Pod Type 2: Local Pod -\u0026gt; Remote Pod The following are the iptables rules that the local pod IP goes through to access the remote service.\nLocal Pod-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 9 -\u0026gt; ISTIO_REDIRECT -\u0026gt; Envoy 15001 (Outbound) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 4 -\u0026gt; POSTROUTING -\u0026gt; Remote Pod\nWe see that the traffic only goes through the Envoy 15001 Outbound port.\nFigure: Local Pod to Remote Pod The traffic in both scenarios above passes through Envoy only once because only one scenario occurs in that Pod, sending or receiving requests.\nType 3: Prometheus -\u0026gt; Local Pod Prometheus traffic that grabs data plane metrics does not have to go through the Envoy proxy.\nThese traffic pass through the following iptables rules.\nPrometheus-\u0026gt; RREROUTING -\u0026gt; ISTIO_INBOUND (traffic destined for ports 15020, 15090 will go to INPUT) -\u0026gt; INPUT -\u0026gt; Local Pod\nFigure: Prometheus to Local Pod Type 4: Local Pod -\u0026gt; Local Pod A Pod may simultaneously have two or more services. If the Local Pod accesses a service on the current Pod, the traffic will go through Envoy 15001 and Envoy 15006 ports to reach the service port of the Local Pod.\nThe iptables rules for this traffic are as follows.\nLocal Pod-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 9 -\u0026gt; ISTIO_REDIRECT -\u0026gt; Envoy 15001（Outbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 2 -\u0026gt; ISTIO_IN_REDIRECT -\u0026gt; Envoy 15006（Inbound）-\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 1 -\u0026gt; POSTROUTING -\u0026gt; Local Pod\nFigure: Local Pod to Local Pod Type 5: Inter-process TCP traffic within Envoy Envoy internal processes with UID and GID 1337 will communicate with each other using lo NICs and localhost domains.\nThe iptables rules that these flows pass through are as follows.\nEnvoy process (Localhost) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 8 -\u0026gt; POSTROUTING -\u0026gt; Envoy process (Localhost)\nFigure: Envoy inter-process TCP traffic Type 6: Sidecar to Istiod traffic Sidecar needs access to Istiod to synchronize its configuration so that Envoy will have traffic sent to Istiod.\nThe iptables rules that this traffic passes through are as follows.\npilot-agent process -\u0026gt; OUTPUT -\u0026gt; Istio_OUTPUT RULE 9 -\u0026gt; Envoy 15001 (Outbound Handler) -\u0026gt; OUTPUT -\u0026gt; ISTIO_OUTPUT RULE 4 -\u0026gt; POSTROUTING -\u0026gt; Istiod\nFigure: Sidecar to Istiod Summary All the sidecar proxies that Istio injects into the Pod or installed in the virtual machine form the data plane of the service mesh, which is also the main workload location of Istio. In my next blog, I will take you through the ports of each component in Envoy and their functions, so that we can have a more comprehensive understanding of the traffic in Istio.\n","date":"2022-05-07T11:18:40+08:00","relpermalink":"/en/blog/istio-sidecar-traffic-types/","section":"blog","summary":"This article will show you the six traffic types and their iptables rules in Istio sidecar, and take you through the whole diagram in a schematic format.","title":"Traffic Types and Iptables Rules in Istio Sidecar Explained"},{"content":"Istio 1.13 is the first release of 2022, and, not surprisingly, the Istio team will continue to release new versions every quarter. Overall, the new features in this release include:\nSupport for newer versions of Kubernetes New API – ProxyConfig, for configuring sidecar proxies Improved Telemetry API Support for hostname-based load balancers with multiple network gateways Support for Kubernetes Versions I often see people asking in the community which Istio supports Kubernetes versions. Istio’s website has a clear list of supported Kubernetes versions. You can see here that Istio 1.13 supports Kubernetes versions 1.20, 1.21, 1.22, and 1.23, and has been tested but not officially supported in Kubernetes 1.16, 1.17, 1.18, 1.19.\nWhen configuring Istio, there are a lot of checklists. I noted them all in the Istio cheatsheet . There are a lot of cheat sheets about configuring Istio, using resources, dealing with everyday problems, etc., in this project, which will be online soon, so stay tuned.\nThe following screenshot is from the Istio cheatsheet website, it shows the basic cheat sheet for setting up Istio.\nFigure: Istio cheatsheet Introducing the new ProxyConfig API Before Istio version 1.13, if you wanted to customize the configuration of the sidecar proxy, there were two ways to do it.\nMeshConfig\nUse MeshConfig and use IstioOperator to modify it at the Mesh level. For example, use the following configuration to alter the default discovery port for istiod.\napVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: defaultConfig: discoveryAddress: istiod:15012 Annotation in the Pods\nYou can also use annotation at the Pod level to customize the configuration. For example, you can add the following annotations to Pod to modify the default port for istiod of the workload:\nanannotations: proxy.istio.io/config: | discoveryAddress: istiod:15012 When you configure sidecar in either of these ways, the fields set in annotations will completely override the default fields in MeshConfig. Please refer to the Istio documentation for all configuration items of ProxyConfig.\nThe new API – ProxyConfig\nBut in 1.13, a new top-level custom resource, ProxyConfig, has been added, allowing you to customize the configuration of your sidecar proxy in one place by specifying a namespace and using a selector to select the scope of the workload, just like any other CRD. Istio currently has limited support for this API, so please refer to the Istio documentation for more information on the ProxyConfig API.\nHowever, no matter which way you customize the configuration of the sidecar proxy, it does not take effect dynamically and requires a workload restart to take effect. For example, for the above configuration, because you changed the default port of istiod, all the workloads in the mesh need to be restarted before connecting to the control plane.\nTelemetry API MeshConfig customized extensions and configurations in the Istio mesh. The three pillars of observability– Metrics, Telemetry, and Logging– can each be docked to different providers. The Telemetry API gives you a one-stop place for flexible configuration of them. Like the ProxyConfig API, the Telemetry API follows the configuration hierarchy of Workload Selector \u0026gt; Local Namespace \u0026gt; Root Configuration Namespace. The API was introduced in Istio 1.11 and has been further refined in that release to add support for OpenTelemetry logs, filtered access logs, and custom tracing service names. See Telemetry Configuration for details.\nAutomatic resolution of multi-network gateway hostnames In September 2021, a member of the Istio community reported an issue with the EKS load balancer failing to resolve when running multi-cluster Istio in AWS EKS. Workloads that cross cluster boundaries need to be communicated indirectly through a dedicated east-west gateway for a multi-cluster, multi-network mesh. You can follow the instructions on Istio’s website to configure a multi-network, primary-remote cluster, and Istio will automatically resolve the IP address of the load balancer based on the hostname.\nIstio 1.13.1 fixing the critical security vulnerabilities Istio 1.13.1 was released to fix a known critical vulnerability that could lead to an unauthenticated control plane denial of service attack.\nThe figure below shows a multi-cluster primary-remote mesh where istiod exposes port 15012 to the public Internet via a gateway so that a pod on another network can connect to it.\nFigure: Multi-network Mesh When installing a multi-network, primary-remote mode Istio mesh, for a remote Kubernetes cluster to access the control plane, an east-west Gateway needs to be installed in the Primary cluster, exposing port 15012 of the control plane istiod to the Internet. An attacker could send specially crafted messages to that port, causing the control plane to crash. If you set up a firewall to allow traffic from only some IPs to access this port, you will be able to reduce the impact of the problem. It is recommended that you upgrade to Istio 1.13.1 immediately to resolve the issue completely.\nIstioCon 2022 Figure: IstioCon 2022 Finally, as a committee member for the last and current IstioCon, I call on everyone to register for IstioCon 2022 , which will be held online on April 25! It will be an industry-focused event, a platform to connect contributors and users to discuss the uses of Istio in different architectural setups, its limitations, and where to take the project next. The main focus on end-user companies, as we look forward to sharing a diversity of case studies showing how to use Istio in production.\n","date":"2022-03-28T16:43:27+08:00","relpermalink":"/en/blog/what-is-new-in-istio-1-13/","section":"blog","summary":"In February 2022, Istio released 1.13.0 and 1.13.1. This blog will give you an overview of what’s new in these two releases.","title":"What's New in Istio 1.13?"},{"content":"Join a team of world-class engineers working on the next generation of networking services using Istio, Envoy, Apache SkyWalking and a few of the open projects to define the next generation of cloud native network service.\nIstio upstream contributor: Golang We are looking for engineers with strong distributed systems experience to join our team. We are building a secure, robust, and highly available service mesh platform for mission critical enterprise applications spanning both legacy and modern infrastructure. This is an opportunity to dedicate a significant amount of contribution to Istio upstream on a regular basis. If you are a fan of Istio and would like to increase your contribution on a dedicated basis, this would be an opportunity for you.\nRequirements\nFundamentals-based problem solving skills; Drive decision by function, first principles based mindset. Demonstrate bias-to-action and avoid analysis-paralysis; Drive action to the finish line and on time. You are ego-less when searching for the best ideasIntellectually curious with a penchant for seeing opportunities in ambiguity Understands the difference between attention to detail vs. detailed - oriented Values autonomy and results over process You contribute effectively outside of your specialty Experience building distributed system platforms using Golang Familiarity with Kubernetes, service mesh technologies such as Istio and Envoy Excellent understanding of networking protocols, concepts, consistency properties of distributed systems, techniques to identify and reconcile configuration drifts Experience contributing to open source projects is a plus. Familiarity with WebAssembly is a plus. Familiarity with Golang, hardware/software load balancers (F5, NGINX), HSM modules, Active Directory/LDAP is a plus. We encourage written and asynchronous communication in English, and proficient oral English is not required.\nDistributed Systems Engineer, Enterprise Infrastructure (Data plane) GoLang or C++ Developers Seeking backend software engineers experienced in building distributed systems using Golang and gRPC. We are building a secure, and highly available service mesh platform for mission-critical enterprise applications for Fortune 500 companies, spanning both the legacy and modern infrastructure. Should possess strong fundamentals in distributed systems and networking. Familiarity with technologies like Kubernetes, Istio, and Envoy, as well as open contributions would be a plus.\nRequirements\nExperience building distributed system platforms using C++, Golang, and gRPC. Familiarity with Kubernetes, service mesh technologies such as Istio and Envoy. Excellent understanding of networking protocols, concepts, consistency properties of distributed systems, techniques to identify and reconcile configuration drifts. Experience contributing to open source projects is a plus. Familiarity with the following is a plus: WebAssembly, Authorization: NGAC, RBAC, ABAC Familiarity with hardware/software load balancers (F5, NGINX), HSM modules, Active Directory/LDAP is a plus. Site Reliability Engineer, SRE Site Reliability Engineering (SRE) combines software and systems engineering to build and run scalable, massively distributed, fault-tolerant systems. As part of the team, you will be working on ensuring that Tetrate’s platform has reliability/uptime appropriate to users’ needs as well as a fast rate of improvement. Additionally, much of our engineering effort focuses on building infrastructure, improving the platform troubleshooting abilities, and eliminating toil through automation.\nRequirements\nSystematic problem-solving approach, coupled with excellent communication skills and a sense of ownership/finish and self-directed drive. Strong fundamentals in operating, debugging, and troubleshooting distributed systems(stateful and/or stateless) and networking. Familiarity with Kubernetes, service mesh technologies such as Istio and EnvoyAbility to debug, optimize code, and automate routine tasks. Experience programming in at least one of the following languages: C++, Rust, Python, Go. Familiarity with the concepts of quantifying failure and availability in a prescriptive manner using SLOs and SLIs. Experience in performance analysis and tuning is a plus. Location Worldwide\nWe are remote with presence in China, Indonesia, India, Japan, U.S., Canada, Ireland, the Netherlands, Spain, and Ukraine.\nPlease send GitHub or online links that showcase your code style along with your resume to careers@tetrate.io .\nAbout Tetrate Powered by Envoy and Istio, its ﬂagship product, Tetrate Service Bridge, enables bridging traditional and modern workloads. Customers can get consistent baked-in observability, runtime security and traffic management for all their workloads, in any environment.\nIn addition to the technology, Tetrate brings a world-class team that leads the open Envoy and Istio projects, providing best practices and playbooks that enterprises can use to modernize their people and processes.\nVarun , co-founder was the initial founder of Istio and gRPC back in Google. We are a two-year-old start-up building compelling network products and services that we believe will result in a step function change in the industry. JJ , co-founder, started the cloud infrastructure team (working alongside Mesos, VM, OS and Provisioning infrastructure teams – under platform team) at Twitter. What got him excited to leave Twitter to start this company is the ability to create a change in how service development happens in enterprises.\nTetrate was founded in Silicon Valley with a large number of our team represented in Canada,, China, India, Indonesia, Ireland, Japan, New Zealand, Singapore, Spain, the Netherlands and Ukraine. Our recruiting goal is simple: find the best talent no matter the background and location. We don’t recruit to a title or a role, instead, we focus on problems that need to be solved. It would be great to discuss overlaps and interests over a live call. You choose where you want to work, flexible time away and work schedule. We have teams get-togethers about 3-4 times a year, most recently in SF, Seattle, Barcelona, San Diego, Washington D.C. and Bandung. This helps with building rapport which then helps with collaboration when you are working virtually.\nTo learn more, pleases visit tetrate.io .\n","date":"2022-03-15T07:28:18+08:00","relpermalink":"/en/notice/tetrate-recruit/","section":"notice","summary":"Remotely worldwide","title":"The Enterprise Service Mesh company Tetrate is hiring"},{"content":"As the service mesh architecture concept gains traction and the scenarios for its applications emerge, there is no shortage of discussions about it in the community. I have worked on service mesh with the community for 4 years now, and will summarize the development of service mesh in 2021 from this perspective. Since Istio is the most popular service mesh, this article will focus on the technical and ecological aspects of Istio.\nService mesh: a critical tech for Cloud Native Infrastructure As one of the vital technologies defined by CNCF for cloud native, Istio has been around for five years now. Their development has gone through the following periods.\nExploration phase: 2017-2018 Early adopter phase: 2019-2020 Large-scale landing and ecological development phase: 2021-present Service mesh has crossed the “chasm”(refer Crossing the Chasm theory) and is in between the “early majority” and “late majority” phases of adoption. Based on feedback from the audience of Istio Weekly, users are no longer blindly following new technologies for experimentation and are starting to consider whether they need them in their organization dialectically.\nFigure: Cross the chasm While new technologies and products continue to emerge, the service mesh, as part of the cloud native technology stack, has continued to solidify its position as the “cloud native network infrastructure” over the past year. The diagram below illustrates the cloud native technology stack model, where each layer has several representative technologies that define the standard. As new-age middleware, the service mesh mirrors other cloud native technologies, such as Dapr (Distributed Application Runtime), which represents the capability model for cloud native middleware, OAM , which defines the cloud native application model, and the service mesh, which defines the L7 network model.\nFigure: Cloud Native Stack A layered view of the cloud native application platform technology stack\nOptimizing the mesh for large scale production applications with different deployment models Over the past year, the community focused on the following areas.\nPerformance optimization: performance issues of service mesh in large-scale application scenarios. Protocol and extensions: enabling service mesh to support arbitrary L7 network protocols. Deployment models: Proxyless vs. Node model vs. Sidecar model. eBPF: putting some of the service mesh’s capabilities to the kernel layer. Performance optimization Istio was designed to serve service to service traffic by “proto-protocol forwarding”. The goal is making the service mesh as “transparent” as possible to applications. Thus using IPtables to hijack the traffic, according to the community-provided test results Istio 1.2 adds only 3 ms to the baseline latency for a mesh with 1000 RPS on 16 connections. However, because of issues inherent in the IPtables conntrack module, Istio’s performance issues begin to emerge as the mesh size increases. To optimize the performance of the Istio sidecar for resource usage and network latency, the community gave the following solutions.\nSidecar configuration: By configuring service dependencies manually or by adding an Operator to the control plane, the number of service configurations sent to Sidecar can be reduced, thus reducing the resource footprint of the data plane; for more automatic and intelligent configuration of Sidecar, the open source projects Slime and Aeraki both offer their innovative configuration loading solutions. The introduction of eBPF: eBPF can be a viable solution to optimize the performance of the service mesh. Some Cilium-based startups even radically propose to use eBPF to replace the Sidecar proxy completely. Still, the Envoy proxy/xDS protocol has become the proxy for the service mesh implementation and supports the Layer 7 protocol very well. We can use eBPF to improve network performance, but complex protocol negotiation, parsing, and user scaling remain challenging to implement on the user side. Protocol and extensions Extensibility of Istio has always been a significant problem, and there are two aspects to Istio’s extensibility.\nProtocol level: allowing Istio to support all L7 protocols Ecological: allowing Istio to run more extensions Istio uses Envoy as its data plane. Extending Istio is essentially an extension of Envoy’s functionality. Istio’s official solution is to use WebAssembly, and in Istio 1.12, the Wasm plugin configuration API was introduced to extend the Istio ecosystem. Istio’s extension mechanism uses the Proxy-Wasm Application Binary Interface (ABI) specification to provide a set of proxy-independent streaming APIs and utilities that can be implemented in any language with an appropriate SDK. Today, Proxy-Wasm’s SDKs are AssemblyScript (similar to TypeScript), C++, Rust, Zig, and Go (using the TinyGo WebAssembly System Interface).\nThere are still relatively few WebAssembly extensions available, and many enterprises choose to customize their CRD and build a service mesh management plane based on Istio. In addition, making Istio support heterogeneous environments for all workloads, such as virtual machines and containers, is also in strong demand for end-users. It allows them to migrate applications from traditional loads to service mesh easily. Finally, there is the hybrid cloud traffic management with multiple clusters and mesh, which is a more advanced requirement.\nDeployment models When the service mesh concept first emerged, there was a debate between the Per-node and Sidecar models, represented by Linkerd and Istio. eBPF later proposed a kernel to sink the service mesh, which led to more service mesh deployment models, as shown in the figure below.\nFigure: Service Mesh Deployment Models These four deployment methods have their own advantages and disadvantages, the specific choice of which depends on the actual situation.\nDevelopment of the Istio ecosystem and the projects that support Istio 2021 was also an exciting year for the Istio community, with a series of events and tutorials.\nFebruary, the first Istio distribution, Tetrate Istio Distro (TID) . February, the first IstioCon was held online, with over 2,000 participants. March, the first free online Istio Fundamentals Course is released. May, the first Certification Istio Administrator exam be released. May, ServiceMeshCon Europe was held online. July, Istio Meetup China was held in Beijing with more than 100 attendees. October, ServiceMeshCon North America was held in Los Angeles. There are also numerous open source projects related to Istio Service Mesh, as shown in the table below.\nProject Value Relationship with Istio Category Launch Date Dominant company Number of stars Envoy Cloud native high-performance edge/middle-service proxy The default data plane proxy September 2016 Lyft 18700 Istio Connection, secure, control, and observation services. Control plane service mesh May 2017 Google 29100 Emissary Gateway Kubernetes native API gateway for microservices, built on Envoy Connectable to Istio gateway February 2018 Ambassador 3600 APISIX Cloud native API gateways It can run as a data plane for Istio or as a gateway on its own gateway June 2019 API7 8100 MOSN Cloud native edge gateways \u0026amp; agents Available as Istio data plane proxy December 2019 Ant 3500 Slime Intelligent service mesh manager based on Istio Adding a management plane to Istio extensions January 2021 NetEase 236 GetMesh Istio integration and command-line management tools Utility for Istio multi-version management tools February 2021 Tetrate 95 Aeraki Manage any of Istio’s seven layers of load Extended multi-protocol support extensions March 2021 Tencent 330 Layotto Cloud native application runtime Using as a data plane for Istio runtime June 2021 Ant 393 Hango Gateway API gateways built on Envoy and Istio Integrates with Istio gateway August 2021 NetEase 253 Note: Data is as of January 6, 2022\nSummary Looking back, we can see that, unlike previous years where users were experimenting, users in 2021 looked for more practical uses for service mesh before implementing them. Their position as the infrastructure of cloud native networks is further strengthened, and more importantly, the service mesh ecosystem is emerging. Looking ahead, in 2022, two technologies to watch are eBPF and WebAssembly(Wasm). We believe that more good examples of service mesh practices will emerge, taking the ecology and standardization a step further.\n","date":"2022-01-12T16:43:27+08:00","relpermalink":"/en/blog/service-mesh-in-2021/","section":"blog","summary":"A review of the development of Service Mesh in 2021.","title":"Service Mesh in 2021: The Ecosystem Is Emerging"},{"content":"It’s been more than four years since Istio launched in May 2017, and while the project has had a strong following on GitHub and 10+ releases, its growing open-source ecosystem is still in its infancy.\nRecently added support for WebAssembly extensions has made the most popular open source service mesh more extensible than ever. This table lists the open-source projects in the Istio ecosystem as of November 11, 2021, sorted by open-source date. These projects enhance the Istio service mesh with gateways, extensions, utilities, and more. In this article, I’ll highlight the two new projects in the category of extensions.\nProject Value Relationship with Istio Category Launch Date Dominant company Number of stars Envoy Cloud native high-performance edge/middle-service proxy The default data plane proxy September 2016 Lyft 18700 Istio Connection, secure, control, and observation services. Control plane service mesh May 2017 Google 29100 Emissary Gateway Kubernetes native API gateway for microservices, built on Envoy Connectable to Istio gateway February 2018 Ambassador 3600 APISIX Cloud native API gateways It can run as a data plane for Istio or as a gateway on its own gateway June 2019 API7 8100 MOSN Cloud native edge gateways \u0026amp; agents Available as Istio data plane proxy December 2019 Ant 3500 Slime Intelligent service mesh manager based on Istio Adding a management plane to Istio extensions January 2021 NetEase 236 GetMesh Istio integration and command-line management tools Utility for Istio multi-version management tools February 2021 Tetrate 95 Aeraki Manage any of Istio’s seven layers of load Extended multi-protocol support extensions March 2021 Tencent 330 Layotto Cloud native application runtime Using as a data plane for Istio runtime June 2021 Ant 393 Hango Gateway API gateways built on Envoy and Istio Integrates with Istio gateway August 2021 NetEase 253 Slime: an intelligent service mesh manager for Istio Slime is an Istio-based, intelligent mesh manager open-sourced by NetEase’s microservices team. Based on the Kubernetes Operator implementation, Slime can be used as a CRD manager that seamlessly interfaces with Istio without needing any customization or definition of dynamic service governance policies. This achieves automatic and convenient use of Istio and Envoy’s advanced features.\nSlime addresses the following issues:\nImplementing higher-level extensions in Istio. For example, extending the HTTP plugin; adaptive traffic limiting based on the resource usage of the service. Poor performance arising from Istio sending all the configurations within the mesh to each sidecar proxy. Slime solves these problems by building an Istio management plane. Its main purpose are\nto build a pluggable controller to facilitate the extension of new functions. to obtain data by listening to the data plane to intelligently generate the configuration for Istio. to build a higher-level CRD for the user to configure, which Slime converts into an Istio configuration. The following diagram shows the flow chart of Istio as an Istio management plane.\nFigure: Slime architecture The specific steps for Slime to manage Istio are as follows.\nSlime operator completes the initialization of Slime components in Kubernetes based on the administrator’s configuration. Developers create configurations that conform to the Slime CRD specification and apply them to Kubernetes clusters. Slime queries the monitoring data of the relevant service stored in Prometheus and converts the Slime CRD into an Istio CRD, in conjunction with the configuration of the adaptive part of the Slime CRD while pushing it to the Global Proxy. Istio listens for the creation of Istio CRDs. Istio pushes the configuration information of the Sidecar Proxy to the corresponding Sidecar Proxy in the data plane. The diagram below shows the internal architecture of Slime.\nFigure: Slime Internal We can divide Slime internally into three main components.\nslime-boot: operator for deploying Slime modules on Kubernetes. slime-controller: the core component of Slime that listens to the Slime CRD and converts it to an Istio CRD. slime-metric: the component used to obtain service metrics information. slime-controller dynamically adjusts service governance rules based on the information it receives. The following diagram shows the architecture of Slime Adaptive Traffic Limiting. Figure: Slime smart limiter Slime dynamically configures traffic limits by interfacing with the Prometheus metric server to obtain real-time monitoring.\nSlime’s adaptive traffic limitation process has two parts: one that converts SmartLimiter to EnvoyFilter and the other that monitors the data. Slime also provides an external monitoring data interface (Metric Discovery Server) that allows you to sync custom monitoring metrics to the traffic limiting component via MDS.\nThe CRD SmartLimiter created by Slime is used to configure adaptive traffic limiting. Its configuration is close to natural semantics, e.g., if you want to trigger an access limit for Service A with a limit of 30QPS when the CPU exceeds 80%, the corresponding SmartLimiter is defined as follows.\napiVersion: microservice.netease.com/v1alpha1 kind: SmartLimiter metadata: name: a namespace: default spec: descriptors: - action: fill_interval: seconds: 1 quota: \u0026#34;30/{pod}\u0026#34; # 30 is the quota for this service. If there are three pods, the limit is 10 per pod. condition: \u0026#34;{cpu}\u0026gt;0.8\u0026#34; # Auto-fill the template based on the value of the monitor {cpu} Aeraki: A Non-Invasive Istio Extension Toolset Aeraki is a service mesh project open sourced by Tencent Cloud in March 2021. Aeraki provides an end-to-end cloud-native service mesh protocol extension solution that provides Istio with powerful third-party protocol extension capabilities in a non-intrusive way, supporting traffic management for Dubbo, Thrift, Redis, and private protocols in Istio. Aeraki’s architecture is shown in the following diagram.\nFigure: Aeraki architecture Aeraki architecture, source Istio blog .\nAs seen in the Aeraki architecture diagram, the Aeraki protocol extension solution consists of two components.\nAeraki: Aeraki runs as an Istio enhancement component on the control plane, providing user-friendly traffic rule configurations to operations via CRDs. Aeraki translates these traffic rule configurations into Envoy configurations distributed via Istio to sidecar proxies on the data plane. Aeraki also acts as an RDS server providing dynamic routing to the MetaProtocol Proxy on the data plane. The RDS provided by Aeraki differs from Envoy’s RDS in that Envoy RDS primarily offers dynamic routing for the HTTP protocol, while Aeraki RDS is designed to provide dynamic routing capabilities for all L7 protocols developed on the MetaProtocol framework. MetaProtocol Proxy: A generic L7 protocol proxy based on Envoy implementation. MetaProtocol Proxy is an extension of Envoy. It unifies the basic capabilities of service discovery, load balancing, RDS dynamic routing, traffic mirroring, fault injection, local/global traffic limiting, etc. for L7 protocols, which greatly reduces the difficulty of developing third-party protocols on Envoy and allows you to quickly create a third-party protocol plug-in based on MetaProtocol by only implementing the codec interface. Before the introduction of MetaProtocol Proxy, if you wanted to use Envoy to implement an L7 protocol to implement routing, traffic limiting, telemetry, etc., you needed to write a complete TCP filter, which would have required a lot of work. For most L7 protocols, the required traffic management capabilities are similar, so there is no need to duplicate this work in each L7 filter implementation. The Aeraki project uses a MetaProtocol Proxy to implement these unified capabilities, as shown in the following figure.\nFigure: MetaProtocol proxy MetaProtocol proxy, source Istio blog .\nBased on MetaProtocol Proxy, we only need to implement the codec interface part of the code to write a new L7 protocol Envoy Filter. In addition, without adding a single line of code, Aeraki can provide configuration distribution and RDS dynamic routing configuration for this L7 protocol at the control plane.\nMake Istio work for all environments and workloads We have seen that NetEase and Tencent are scaling Istio mainly by building Operator. However, this scaling is not enough for multi-cluster management. We know that much of our current infrastructure is transitioning to cloud native or containerized, which means containers, virtual machines, and other environments co-exist. How do we unify traffic management of these different environments? It is possible to do so using Istio.\nYou have to again build a management plane on top of Istio and add an abstraction layer to add CRDs that apply to cluster management, such as cluster traffic configuration, policy configuration, etc. Additionally, you have to deploy a Gateway in each cluster that connects uniformly to an edge proxy that interconnects all the groups.\nTo learn more about Tetrate Service Bridge (TSB), which provides this layer of infrastructure, you can go here . TSB is built on the open source Istio with enhancements, it follows the concept of the above two open source projects, and also builds a management plane to support heterogeneous environments.\nAs we can see, the Istio-based projects and the open source environment are booming and companies like Tetrate are doing useful jobs of productizing and making Istio available to all workloads.\n","date":"2022-01-10T16:43:27+08:00","relpermalink":"/en/blog/istio-extensions-slime-and-aeraki/","section":"blog","summary":"In this article, I’ll introduce you two Istio extension projects: Aeraki and Slime.","title":"Introducing Slime and Aeraki in the Evolution of Istio Open-Source Ecosystem"},{"content":"You can use Istio to do multi-cluster management , API Gateway , and manage applications on Kubernetes or virtual machines . In my last blog , I talked about how service mesh is an integral part of cloud native applications. However, building infrastructure can be a big deal. There is no shortage of debate in the community about the practicability of service mesh and Istio– here’s a list of common questions and concerns, and how to address them.\nIs anyone using Istio in production? What is the impact on application performance due to the many resources consumed by injecting sidecar into the pod? Istio supports a limited number of protocols; is it scalable? Will Istio be manageable? – Or is it too complex, old services too costly to migrate, and the learning curve too steep? I will answer each of these questions below.\nIstio is architecturally stable, production-ready, and ecologically emerging Istio 1.12 was just released in November – and has evolved significantly since the explosion of service mesh in 2018 (the year Istio co-founders established Tetrate). Istio has a large community of providers and users . The Istio SIG of Cloud Native Community has held eight Istio Big Talk (Istio 大咖说) , with Baidu, Tencent, NetEase, Xiaohongshu(小红书), and Xiaodian Technology(小电科技) sharing their Istio practices. According to CNCF Survey Report 2020 , about 50% of the companies surveyed are using a service mesh in production or planning to in the next year, and about half (47%) of organizations using a service mesh in production are using Istio.\nMany companies have developed extensions or plugins for Istio, such as Ant, NetEase, eBay, and Airbnb. Istio’s architecture has been stable since the 1.5 release, and the release cycle is fixed quarterly, with the current project’s main task being Day-2 Operations.\nThe Istio community has also hosted various events, with the first IstioCon in March 2021, the Istio Meetup China in Beijing in July, and the Service Mesh Summit 2022 in Shanghai in January 2022.\nSo we can say that the Istio architecture is stable and production-ready, and the ecosystem is budding.\nThe impact of service mesh on application performance A service mesh uses iptables to do traffic hijacking by default to be transparent to applications. When the number of services is large, there are a lot of iptables rules that affect network performance. You can use techniques like eBPF to provide application performance, but the method requires a high version of the operating system kernel, which few enterprises can achieve.\nFigure: Istio DNS In the early days, Istio distributed the routing information of all services in the mesh to all proxy sidecars, which caused sidecar s to take up a lot of resources. Aeraki and Slime can achieve configuration lazy loading. We will introduce these two open-source projects in the Istio open-source ecosystem.\nFinally, there is a problem related to Sidecar proxy operation and maintenance: upgrading all Envoy proxies while ensuring constant traffic. A solution is using the SidecarSet resource in the open-source project OpenKruise .\nThe resource consumption and network latency associated with the introduction of Sidecar are also within reasonable limits, as you can see from the service mesh benchmark performance tests .\nExtending the Istio service mesh The next question is about extending the Istio service mesh. The current solution given by the Istio community is to use WebAssembly , an extension that is still relatively little used in production by now and has performance concerns. Most of the answers I’ve observed are CRDs that build a service mesh management plane based on Istio.\nAlso, making Istio support heterogeneous environments for all workloads, such as virtual machines and containers, is in strong demand for end-users. It allows them to migrate applications from traditional loads to cloud native easily. Finally, hybrid cloud traffic management for multiple clusters and meshes is a more advanced requirement.\nSteep learning curve Many people complain that Istio has too little learning material. Istio has been open source for four years, and there are a lot of learning resources now:\nIstio Documentation IstioCon 2021 Istio Big Talk/Istio Weekly Istio Fundamentals Course Certified Istio Administrator Yes, Istio is complex, but it’s been getting more and more manageable with every release. In my next blog, I will introduce you to two open source projects that extend Istio and give you some insight into what’s going on in the Istio community.\n","date":"2021-12-17T16:43:27+08:00","relpermalink":"/en/blog/the-debate-in-the-community-about-istio-and-service-mesh/","section":"blog","summary":"There is no shortage of debate in the community about the practicability of service mesh and Istio – here’s a list of common questions and concerns, and how to address them.","title":"The Debate in the Community About Istio and Service Mesh"},{"content":"If you don’t know what Istio is, you can read my previous articles below:\nWhat Is Istio and Why Does Kubernetes Need it? Why do you need Istio when you already have Kubernetes? This article will explore the relationship between service mesh and cloud native.\nService mesh – the product of the container orchestration war If you’ve been following the cloud-native space since its early days, you’ll remember the container orchestration wars of 2015 to 2017. Kubernetes won the container wars in 2017, the idea of microservices had taken hold, and the trend toward containerization was unstoppable. Kubernetes architecture matured and slowly became boring, and service mesh technologies, represented by Linkerd and Istio, entered the CNCF-defined cloud-native critical technologies on the horizon.\nKubernetes was designed with the concept of cloud-native in mind. A critical idea in cloud-native is the architectural design of microservices. When a single application is split into microservices, how can microservices be managed to ensure the SLA of the service as the number of services increases? The service mesh was born to solve this problem at the architectural level, free programmers’ creativity, and avoid tedious service discovery, monitoring, distributed tracing, and other matters.\nThe service mesh takes the standard functionality of microservices down to the infrastructure layer, allowing developers to focus more on business logic and thus speed up service delivery, which is consistent with the whole idea of cloud-native. You no longer need to integrate bulky SDKs in your application, develop and maintain SDKs for different languages, and just use the service mesh for Day 2 operations after the application is deployed.\nThe service mesh is regarded as the next generation of microservices. In the diagram, we can see that many of the concerns of microservices overlap with the functionality of Kubernetes. Kubernetes focuses on the application lifecycle, managing resources and deployments with little control over services. The service mesh fills this gap. The service mesh can connect, control, observe and protect microservices.\nKubernetes vs. xDS vs. Istio This diagram shows the layered architecture of Kubernetes and Istio.\nFigure: Kubernetes vs xDS vs Istio The diagram indicates that the kube-proxy settings are global and cannot be controlled at a granular level for each service. All Kubernetes can do is topology-aware routing, routing traffic closer to the Pod, and setting network policies in and out of the Pod.\nIn contrast, the service mesh takes traffic control out of the service layer in Kubernetes through sidecar proxies, injects proxies into each Pod, and manipulates these distributed proxies through a control plane. It allows for more excellent resiliency.\nKube-proxy implements traffic load balancing between multiple pod instances of a Kubernetes service. But how do you finely control the traffic between these services — such as dividing the traffic by percentage to different application versions (which are all part of the same service, but on other deployments), or doing canary releases and blue-green releases?\nThe Kubernetes community gives a way to do canary releases using Deployment, assigning different pods to deployed services by modifying the pod’s label.\nFigure: Envoy Architecture Currently, the most popular open-source implementation of service mesh in the world is Istio. From the CNCF Survey Report 2020 , we know that Istio is the most used service mesh in production today. Many companies have built their service mesh based on Istio, such as Ant, Airbnb, eBay, NetEase, Tencent, etc.\nFigure: CNCF Survey Report 2020 Figure from CNCF Survey Report 2020 Istio is developed based on Envoy, which has been used by default as its distributed proxy since the first day it was open-sourced. Envoy pioneered the creation of the xDS protocol for distributed gateway configuration, greatly simplifying the configuration of large-scale distributed networks. Ant Group open source MOSN also supported xDS In 2019. Envoy was also one of the first projects to graduate from CNCF, tested by large-scale production applications.\nService mesh – the cloud-native networking infrastructure With the above comparison between Kubernetes and service mesh in mind, we can see the place of service mesh in the cloud-native application architecture. That is, building a cloud-native network infrastructure specifically provides:\nTraffic management: controlling the flow of traffic and API calls between services, making calls more reliable, and enhancing network robustness in different environments. Observability: understanding the dependencies between services and the nature and flow of traffic between them provides the ability to identify problems quickly. Policy enforcement: controlling access policies between services by configuring the mesh rather than by changing the code. Service Identification and Security: providing service identifiability and security protection in the mesh. ","date":"2021-12-12T16:43:27+08:00","relpermalink":"/en/blog/service-mesh-an-integral-part-of-cloud-native-apps/","section":"blog","summary":"This article will explore the relationship between service mesh and cloud native.","title":"Service Mesh - An Integral Part of Cloud-Native Applications"},{"content":"API gateways have been around for a long time as the entry point for clients to access the back-end, mainly to manage “north-south” traffic, In recent years, service mesh architectures have become popular, mainly for managing internal systems,(i.e. “east-west” traffic), while a service mesh like Istio also has built-in gateways that bring traffic inside and outside the system under unified control. This often creates confusion for first-time users of Istio. What is the relationship between the service mesh and the API gateway? How does Istio’s gateway work? What are the ways to expose the services in the Istio mesh? This article gives you the answer.\nKey Insights The service mesh was originally created to solve the problem of managing internal traffic for distributed systems, but API gateways existed long before it. While the Gateway is built into Istio, you can still use a custom Ingress Controller to proxy external traffic. API gateways and service mesh are converging. How do I expose services in the Istio mesh? The following diagram shows four approaches to expose services in the Istio mesh using Istio Gateway, Kubernetes Ingress, API Gateway, and NodePort/LB.\nFigure: Exposing services through Istio Ingress Gateway The Istio mesh is shaded, and the traffic in the mesh is internal (east-west) traffic, while the traffic from clients accessing services within the Kubernetes cluster is external (north-south) traffic.\nApproach Controller Features NodePort/LoadBalancer Kubernetes Load balancing Kubernetes Ingress Ingress controller Load balancing, TLS, virtual host, traffic routing Istio Gateway Istio Load balancing, TLS, virtual host, advanced traffic routing, other advanced Istio features API Gateway API Gateway Load balancing, TLS, virtual host, advanced traffic routing, API lifecycle management, billing, rate limiting, policy enforcement, data aggregation Since NodePort/LoadBalancer is a basic way to expose services built into Kubernetes, this article will not discuss that option. Each of the other three approaches will be described below.\nUsing Kubernetes Ingress to expose traffic We all know that clients of a Kubernetes cluster cannot directly access the IP address of a pod because the pod is in a network plane built into Kubernetes. We can expose services inside Kubernetes outside the cluster using NodePort or Load Balancer Kubernetes service type. To support virtual hosting, hiding and saving IP addresses, you can use Ingress resources to expose services in Kubernetes.\nFigure: Kubernetes Ingress to expose services Ingress is a Kubernetes resource that controls the behavior of an ingress controller that does the traffic touring, which is the equivalent of a load-balanced directional proxy server such as Nginx, Apache, etc., which also includes rule definitions, i.e., routing information for URLs, which is provided by the Ingress controller .\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: istio name: ingress spec: rules: - host: httpbin.example.com http: paths: - path: /status/* backend: serviceName: httpbin servicePort: 8000 The kubernetes.io/ingress.class: istio annotation in the example above indicates that the Ingress uses the Istio Ingress Controller which in fact uses Envoy proxy.\nUsing Istio Gateway to expose services Istio is a popular service mesh implementation that has evolved from Kubernetes that implements some features that Kubernetes doesn’t. (See What is Istio and why does Kubernetes need Istio? ) It makes traffic management transparent to the application, moving this functionality from the application to the platform layer and becoming a cloud-native infrastructure.\nIstio used Kubernetes Ingress as the traffic portal in versions prior to Istio 0.8, where Envoy was used as the Ingress Controller. From Istio 0.8 and later, Istio created the Gateway object. Gateway and VirtualService are used to represent the configuration model of Istio Ingress, and the default implementation of Istio Ingress uses the same Envoy proxy. In this way, the Istio control plane controls both the ingress gateway and the internal sidecar proxy with a consistent configuration model. These configurations include routing rules, policy enforcement, telemetry, and other service control functions.\nThe Istio Gateway resources function similarly to the Kubernetes Ingress in that it is responsible for north-south traffic to and from the cluster. The Istio Gateway acts as a load balancer to carry connections to and from the edge of the service mesh. The specification describes a set of open ports and the protocols used by those ports, as well as the SNI configuration for load balancing, etc.\nThe Istio Gateway resource itself can only be configured for L4 through L6, such as exposed ports, TLS settings, etc.; however, the Gateway can be bound to a VirtualService, where routing rules can be configured on L7, such as versioned traffic routing, fault injection, HTTP redirects, HTTP rewrites, and all other routing rules supported within the mesh.\nBelow is an example of a Gateway binding to a VirtualService. The pod with the “istio: ingressgateway” label will act as the Ingress controller and route HTTP traffic to port 80 of the httpbin.example.com virtual host. The biggest difference between this and using Kubernetes Ingress is that it requires us to manually bind the VirtualService to the Gateway and specify the pod where the Gateway is located. This configuration is equivalent to opening up an entry point to Kubernetes for external access.\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: httpbin-gateway spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;httpbin.example.com\u0026#34; The VirtualService below is bound to the gateway above via gateways to accept traffic from that gateway.\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: httpbin spec: hosts: - \u0026#34;httpbin.example.com\u0026#34; gateways: - httpbin-gateway http: - match: - uri: prefix: /status route: - destination: port: number: 8000 host: httpbin Using an API Gateway API gateways are API management tools that sit between the client and the back-end service and are widely used in microservices as a way to separate the client interface from the back-end implementation. When a client makes a request, the API gateway breaks it down into multiple requests, then routes them to the correct location, generates a response, and keeps track of everything.\nThe API Gateway is a special type of service in the microservices architecture that serves as the entry point for all microservices and is responsible for performing routing requests, protocol conversions, aggregating data, authentication, rate limiting, circuit breaking, and more. Most enterprise APIs are deployed through API Gateways, which typically handle common tasks across API service systems, such as TLS termination, authentication and authorization, rate limiting, and statistical information.\nThere can be one or more API Gateways in the mesh. The responsibilities of the API Gateway are\nRequest routing and version control Facilitating the transition of monolithic applications to microservices Permission authentication Data aggregation: monitoring and billing Protocol conversion Messaging and caching Security and alerting Many of the above basic functions such as routing and permission authentication can also be achieved through Istio Gateway, but some mature API gateways may be more advantageous in terms of feature richness and scalability.\nThe introduction of API Gateway requires consideration of the deployment, operation and maintenance, load balancing, and other scenarios of API Gateway itself, which increases the complexity of back-end services. An API Gateway carries a large number of interface adaptations, which makes it difficult to maintain. For some scenarios, the addition of a hop may lead to a reduction in performance. Currently, some API Gateway imitations are building their own service mesh by deploying them in the sidecar.\nSummary In the Istio mesh, you can use a variety of Kubernetes Ingress Controllers to act as entry gateways, but of course, you can also use Istio’s built-in Istio Gateway directly, for policy control, traffic management, and usage monitoring. The advantage of this is that the gateway can be managed directly through Istio’s control plane, without the need for additional tools. But for functions such as API statement cycle management, complex billing, protocol conversion, and authentication, a traditional API gateway may be a better fit for you. So, you can choose according to your needs, or you can use a combination.\nSome traditional reverse proxies are also moving towards Service Mesh, such as Nginx with Nginx Service Mesh and Traefik with Traefik Mesh, and some API gateway products are also moving towards Service Mesh, such as Kong with Kuma, and in the future, we will see more convergence of API gateways, reverse proxies, and service meshes.\n","date":"2021-08-06T10:22:00+08:00","relpermalink":"/en/blog/istio-servicemesh-api-gateway/","section":"blog","summary":"What is the relationship between the service mesh and the API gateway? How does Istio’s gateway work? What are the ways to expose the services in the Istio mesh? This article gives you the answer.","title":"Using Istio Service Mesh as API Gateway"},{"content":"Do you have multiple Kubernetes clusters and a service mesh? Do your virtual machines and services in a Kubernetes cluster need to interact? This article will take you through the process and considerations of building a hybrid cloud using Kubernetes and an Istio Service Mesh. Together, Kubernetes and Istio can be used to bring hybrid workloads into a mesh and achieve interoperability for multicluster. But another layer of infrastructure — a management plane — is helpful for managing multicluster or multimesh deployments.\nKubernetes Using Kubernetes enables rapid deployment of a distributed environment that enables cloud interoperability and unifies the control plane on the cloud. It also provides resource objects, such as Service, Ingress and Gateway , to handle application traffic. The Kubernetes API Server communicates with the kube-proxy component on each node in the cluster, creates iptables rules for the node, and forwards requests to other pods.\nAssuming that a client now wants to access a service in Kubernetes, the request is first sent to the Ingress/Gateway, then forwarded to the backend service (Service A in the diagram below) based on the routing configuration in the Ingress/Gateway. Then Service A polls an instance of Service B for the traffic requested by Service B. Lastly, the traffic requested by Service A for Service B is polled forward to Service B’s instance.\nFigure: Kubernetes Kubernetes Multicluster The most common usage scenarios for multicluster management include:\nservice traffic load balancing isolating development and production environments decoupling data processing and data storage cross-cloud backup and disaster recovery flexible allocation of compute resources low-latency access to services across regions avoiding vendor lock-in There are often multiple Kubernetes clusters within an enterprise; and the KubeFed implementation of Kubernetes cluster federation developed by Multicluster SIG enables multicluster management capabilities, which allows all Kubernetes clusters to be managed through the same interface.\nThere are several general issues that need to be addressed when using cluster federation:\nConfiguring which clusters need to be federated API resources need to be propagated across the clusters Configuring how API resources are distributed to different clusters Registering DNS records in clusters to enable service discovery across clusters The following is a multicluster architecture for KubeSphere — one of the most commonly used Kubernetes multicluster management architectures — where the Host Cluster serves as the control plane with two member clusters, West and East.\nFigure: Multicluster The Host Cluster needs to be able to access the API Server of the Member Cluster, but the network connectivity between Member Clusters is not required. The Host Cluster is independent of the Member Cluster it manages and the Member Cluster is not aware of the existence of the Host Cluster. The advantage of this is that when the control plane fails, the Member Cluster will not be affected and the deployed load can still operate normally without being affected.\nThe Host Cluster also assumes the role of API portal, and the Host Cluster forwards the resource requests to the Member Cluster — which is convenient for aggregation and also facilitates unified authority authentication. We see that there is a Federation Control Plane in the Host Cluster, where the Push Reconciler propagates the identity, role, and role binding from the Federation Cluster to all Member Clusters.\nIstio Service Mesh Consider using the Istio service mesh when we have multilingual, multiversion microservices running in Kubernetes and need finer-grained canary publishing and unified security policy management for inter-service observability. Istio enables intelligent application-aware load balancing from the application layer to other Service Mesh-enabled services in the cluster, by transparently intercepting all traffic to and from the application using IPTables, and bypassing the primary kube-proxy load balancing. The Istio control plane communicates with the Kubernetes API Server to obtain information about all registered services in the cluster.\nThe following diagram illustrates the basics of Istio, where all nodes belong to the same Kubernetes cluster.\nFigure: Istio Service Mesh You may end up with at least a few Kubernetes clusters, each hosting microservices. Multiple deployment models exist for Istio’s multicluster deployments — depending on network isolation, primary and backup — which can be specified by declaration when deploying using Istio Operator. Communication between these microservices in a cluster can be enhanced by a service mesh. Within the cluster, Istio provides common communication patterns to improve resiliency, security and observability.\nAll of the above is about application load management on Kubernetes, but for legacy applications on virtual machines: how can they be managed in the same plane? Istio supports applications on virtual machines, so why do we need a management plane?\nManagement Plane To manage gateways, traffic and security groupings, and apply them to different clusters and namespaces, you’ll need to add another layer of abstraction on top of Istio: a management plane. The diagram below shows the multitenant model of Tetrate Service Bridge (TSB). TSB uses Next Generation Access Control (NGAC) — a fine-grained authorization framework — to manage user access and also facilitate the construction of a zero-trust network.\nFigure: Management Plane Istio provides workload identification, protected by strong mTLS encryption. This zero-trust model is better than trusting workloads based on topology information, such as source IP. A common control plane for multicluster management is built on top of Istio. Then a management plane is added to manage multiple clusters — providing multitenancy, management configuration, observability, and more.\nThe diagram below shows the architecture of Tetrate Service Bridge.\nFigure: Tetrate Service Bridge Summary Interoperability of heterogeneous clusters is achieved with Kubernetes. Istio brings containerized and virtual machine loads into a single control plane, to unify traffic, security and observability within the clusters. However, as the number of clusters, network environments and user permissions become more complex, there is a need to build another management plane above Istio’s control plane (for example, Tetrate Service Bridge ) for hybrid cloud management.\n","date":"2021-07-12T22:22:00+08:00","relpermalink":"/en/blog/multicluster-management-with-kubernetes-and-istio/","section":"blog","summary":"This article explains three patterns/tools for debugging microservices in Kubernetes and the changes brought by the introduction of Istio for debugging microservices.","title":"Multicluster Management With Kubernetes and Istio"},{"content":"Kubernetes is arguably the best environment for running microservices so far, but the experience of debugging microservices in a Kubernetes environment may not be as user-friendly. This article will show you how to debug microservices in Kubernetes, introduce common tools, and explain how the introduction of Istio impacts debugging microservices.\nDebugging microservices is vastly different from traditional monolithic applications The debugging of microservices has been a long-standing problem for software developers. This challenge does not exist in traditional monolithic applications because developers can leverage the debugger in IDEs to add breakpoints, modify environment variables, single-step execution, etc. for their applications, all of which provide great help in software debugging. With the popularity of Kubernetes, the debugging of microservices becomes a thorny issue, where the following issues are more complicated than the debugging of traditional monolithic applications.\nMultiple dependencies A microservice often depends on multiple other microservices, some shared volumes across multiple microservices, and authorizations based on service accounts. When debugging a microservice, how do you deploy other dependent services to quickly build a latest set of staging environments?\nAccess from a local machine When microservices are running on a developer’s local computer, there is usually no direct access to the services in a Kubernetes cluster. How can you debug microservices deployed in a Kubernetes cluster as if they were local services?\nSlow development loop Usually, it takes a long process to update the code and build it into an image before pushing it to the cluster. How do you speed up the development cycle? Let’s look at the tools that address those challenges.\nTools The main solutions for debugging microservices in Kubernetes are:\nProxy: by building a VPN, deploying a proxy in the Kubernetes cluster, and adding local debug endpoints to make the services in Kubernetes directly accessible to local applications, your architecture will look like [ local service ] \u0026lt;-\u0026gt; [ proxy ] \u0026lt;-\u0026gt; [ app in Kubernetes ]. Sidecar: Inject a sidecar into the pod of the microservice to be debugged to intercept all traffic to and from the service, so that the service can be tracked and monitored, and the service can also be debugged in this sidecar. Service Mesh: To get an overall picture of the application, inject sidecars into all microservices so that you can get a dashboard that monitors global status. Here are three typical open source projects that implement the above solutions, each of which can help you debug microservices from a different perspective. You can apply them at different stages of software development and they can be said to be complementary to each other.\nProxy – debugging microservices with Telepresence Telepresence is essentially a local proxy that proxies data volumes, environment variables, and networks in a Kubernetes cluster locally. The following diagram shows the main usage scenarios for Telepresence.\nFigure: Proxy mode: Telepresence Users need to manually execute the telepresence command locally, which will automatically deploy the agent to Kubernetes. Once the agent has been deployed,\nLocal services will have complete access to other services in the Kubernetes cluster, environment variables, Secret, ConfigMap, etc. Services in the cluster also have direct access to the locally exposed endpoints. However, this approach requires users to run multiple commands while debugging locally, and in some network environments it may not be possible to establish a VPN connection to the Kubernetes cluster.\nSidecar – debugging microservices with Nocalhost Nocalhost is a Kubernetes-based cloud development environment. To use it, you just need to install a plugin in your IDE – VS Code to extend Kubernetes and shorten the development feedback cycle. The development environment can be isolated by creating different namespaces for different users and using ServiceAccount when binding to different user corners. Nocalhost also provides a web console and API for administrators to manage different development environments.\nFigure: Sidecar mode: Nocalhost As long as you have a Kubernetes cluster and have admin rights to the cluster, you can refer to the Nocalhost documentation to quickly start trying it out. To use the Nocalhost plugin in VS Code, you need to configure the Kubernetes cluster in the plugin first.\nSelect the Kubeconfig file you just exported or copy and paste the contents of the file directly into the configuration. Then select the service you need to test and select the corresponding Dev Container. VS Code will automatically open a new code window. Here is an example of the bookinfo sample provided by Istio. You can open the cloned code in your local IDE and click the hammer next to the code file to enter development mode. Selecting the corresponding DevContainer and Nocalhost will automatically inject a development container sidecar into the pod and automatically enter the container in the terminal, as shown in the following figure.\nFigure: Nocalhost VS code In development mode, the code is modified locally without rebuilding the image, and the remote development environment takes effect in real time, which can greatly accelerate the development speed. At the same time, Nocalhost also provides a server for managing the development environment and user rights, as shown in the following figure.\nFigure: Nocalhost Web Service Mesh – debugging microservices with Istio The above method of using proxy and sidecar can only debug one service at a time. You’ll need a mesh to get the global status of the application, such as the metrics of the service obtained, and debug the performance of the service by understanding the dependency and invocation process of the service through distributed tracing. These observability features need to be implemented by injecting sidecar uniformly for all services. And, when your services are in the process of migrating from VMs to Kubernetes, using Istio can bring VMs and Kubernetes into a single network plane (as shown below), making it easy for developers to debug and do incremental migrations.\nFigure: Serivce Mesh mode: Istio Of course, these benefits do not come without a “cost.” With the introduction of Istio, your Kubernetes services will need to adhere to the Istio naming convention and you’ll need to know how to debug microservices using the Istioctl command line and logging.\nUse the istioctl analyze command to debug the deployment of microservices in your cluster, and you can use YAML files to examine the deployment of resources in a namespace or across your cluster. Use istioctl proxy-config secret to ensure that the secret of a pod in a service mesh is loaded correctly and is valid. Summary In the process of microservicing applications and migrating from virtual machines to Kubernetes, developers need to make a lot of changes in their mindset and habits. By building a VPN between local and Kubernetes via proxy, developers can easily debug services in Kubernetes as if they were local services. By injecting a sidecar into the pod, you can achieve real-time debugging and speed up the development process. Finally, the Istio service mesh truly enables global observability, and you can also use tools like Tetrate Service Bridge to manage heterogeneous platforms, helping you gradually move from monolithic applications to microservices.\n","date":"2021-07-05T22:22:00+08:00","relpermalink":"/en/blog/how-to-debug-microservices-in-kubernetes-with-proxy-sidecar-or-service-mesh/","section":"blog","summary":"This article explains three patterns/tools for debugging microservices in Kubernetes and the changes brought by the introduction of Istio for debugging microservices.","title":"How to Debug Microservices in Kubernetes With Proxy, Sidecar or Service Mesh?"},{"content":"Istio was named by Tetrate founder Varun Talwar and Google lead engineer Louis Ryan in 2017 and was open sourced on May 24, 2017. Today is the fourth anniversary of Istio’s open source arrival. Let’s take a look back at Istio’s four years of development — and look forward to Istio’s future.\nIstio’s open source history In 2017, the year Kubernetes ended the container orchestration battle, Google took the opportunity to consolidate its dominance in the cloud native space and compensate for Kubernetes’ disadvantage in service-to-service traffic management by open-sourcing Istio. Istio released its 1.10 last week — but here are some of the most important releases in Istio’s history to date.\nDate Version Note May 24, 2017 0.1 Officially open source; established the architectural foundation of Control Plane, Data Plane and sidecar proxy. October 10, 2017 0.2 Started to support multiple runtime environments, such as virtual machines. June 1, 2018 0.8 API refactoring July 31, 2018 1.0 Production-ready, after which the Istio team underwent a massive reorganization. March 19, 2019 1.1 Enterprise-ready. Support for multiple Kubernetes clusters, with performance optimizations. March 3, 2020 1.5 Back to monolith, with microservice components merged into istiod, making Istio’s architecture cleaner and easier to maintain. Support for WebAssembly extension, making Istio’s ecology much stronger. November 18, 2020 1.8 Officially deprecated Mixer and focused on adding support for virtual machines. A year after its inception– and two months before the 1.0 release, version 0.8 was released with a massive refactoring of the API. In late July 2018, when 1.0 was released, Istio reached a production-ready tipping point. Since then, Google has massively reorganized the Istio team and several Istio-based service mesh startups were born, making 2018 the booming year of the service mesh industry.\nIstio 1.1 was released in March 2019, almost 9 months after 1.0 was released, which is far beyond the average release cycle of an open-source project. We know that the speed of iteration and evolution is a core competency of basic software. Since then, Istio has started a regular release cadence of one version per quarter and has become the #4 fastest growing project in GitHub’s top 10 in 2019 !\nThe Istio community In 2020, Istio’s project management began to mature and its governance reached a stage of evolution. We saw the first election of a steering committee for the Istio community and the transfer of the trademark to Open Usage Commons . The first IstioCon was successfully held in February 2021, with thousands of people attending the online conference. There is also a large Istio community in China , and face-to-face Istio community meetups will be held there in 2021. Stay tuned for more.\nAccording to the CNCF 2020 Survey, 46% of organizations were either using a service mesh in production or planning to use it in the next 12 months. Istio was the top used mesh among those using a mesh in production.\nThe future After 4 years of development, there is not only a large user base around Istio, but also several Istio vendors, as you can see on the homepage of the recently revamped Istio website. In the last few releases, Istio has shifted its development focus to improving the Day 2 Operation experience. We also expect to see more Istio adoption path recommendations, case studies, learning materials, training, and certifications (such as the industry’s first Certified Istio Administrator from Tetrate) that will facilitate the adoption of Istio.\n","date":"2021-05-24T08:00:00+08:00","relpermalink":"/en/blog/istio-4-year-birthday/","section":"blog","summary":"Today is Istio's 4 year birthday, let’s take a look back at Istio’s four years of development — and look forward to Istio’s future.","title":"Happy Istio 4th Anniversary -- Retrospect and Outlook"},{"content":"Istio, the most popular service mesh implementation , was developed on top of Kubernetes and has a different niche in the cloud native application ecosystem than Kubernetes. Rather than introduce you directly to what Istio has to offer, this article will explain how Istio came about and what it is in relation to Kubernetes.\nWhy Is There an Istio? To explain what Istio is, it’s also important to understand the context in which Istio came into being — i.e., why is there an Istio?\nMicroservices are a technical solution to an organizational problem. And Kubernetes/Istio are a technical solution to deal with the issues created by moving to microservices. As a deliverable for microservices, containers solve the problem of environmental consistency and allow for more granularity in limiting application resources. They are widely used as a vehicle for microservices.\nGoogle open-sourced Kubernetes in 2014, which grew exponentially over the next few years. It became a container scheduling tool to solve the deployment and scheduling problems of distributed applications — allowing you to treat many computers as though they were one computer. Because the resources of a single machine are limited and Internet applications may have traffic floods at different times (due to rapid expansion of user scale or different user attributes), the elasticity of computing resources needs to be high. A single machine obviously can’t meet the needs of a large-scale application; and conversely, it would be a huge waste for a very small-scale application to occupy the whole host.\nIn short, Kubernetes defines the final state of the service and enables the system to reach and stay in that state automatically. So how do you manage the traffic on the service after the application has been deployed? Below we will look at how service management is done in Kubernetes and how it has changed in Istio.\nHow Do You Do Service Management in Kubernetes? The following diagram shows the service model in Kubernetes:\nFigure: Kubernetes Service Model From the above figure we can see that:\nDifferent instances of the same service may be scheduled to different nodes. Kubernetes combines multiple instances of a service through Service objects to unify external services. Kubernetes installs a kube-proxy component in each node to forward traffic, which has simple load balancing capabilities. Traffic from outside the Kubernetes cluster can enter the cluster via Ingress (Kubernetes has several other ways of exposing services; such as NodePort, LoadBalancer, etc.). Kubernetes is used as a tool for intensive resource management. However, after allocating resources to the application, Kubernetes doesn’t fully solve the problems of how to ensure the robustness and redundancy of the application, how to achieve finer-grained traffic division (not based on the number of instances of the service), how to guarantee the security of the service, or how to manage multiple clusters, etc.\nThe Basics of Istio The following diagram shows the service model in Istio, which supports both workloads and virtual machines in Kubernetes.\nFigure: Istio From the diagram we can see that:\nIstiod acts as the control plane, distributing the configuration to all sidecar proxies and gateways. (Note: for simplification, the connections between Istiod and sidecar are not drawn in the diagram.) Istio enables intelligent application-aware load balancing from the application layer to other mesh enabled services in the cluster, and bypasses the rudimentary kube-proxy load balancing. Application administrators can manipulate the behavior of traffic in the Istio mesh through a declarative API, in the same way they manage workloads in Kubernetes. It can take effects within seconds and they can do this without needing to redeploy. Ingress is replaced by Gateway resources, a special kind of proxy that is also a reused Sidecar proxy. A sidecar proxy can be installed in a virtual machine to bring the virtual machine into the Istio mesh. In fact, before Istio one could use SpringCloud, Netflix OSS, and other tools to programmatically manage the traffic in an application, by integrating the SDK in the application. Istio makes traffic management transparent to the application, moving this functionality out of the application and into the platform layer as a cloud native infrastructure.\nIstio complements Kubernetes, by enhancing its traffic management, observability and security for cloud native applications. The service mesh open source project — launched in 2017 by Google, IBM and Lyft — has come a long way in three years. A description of Istio’s core features can be found in the Istio documentation .\nSummary Service Mesh is the cloud native equivalent of TCP/IP, addressing application network communication, security and visibility issues. Istio is currently the most popular service mesh implementation, relying on Kubernetes but also scalable to virtual machine loads. Istio’s core consists of a control plane and a data plane, with Envoy as the default data-plane agent. Istio acts as the network layer of the cloud native infrastructure and is transparent to applications. ","date":"2021-04-28T09:06:14+08:00","relpermalink":"/en/blog/what-is-istio-and-why-does-kubernetes-need-it/","section":"blog","summary":"This article will explain how Istio came about and what it is in relation to Kubernetes.","title":"What Is Istio and Why Does Kubernetes Need it?"},{"content":"If you’ve heard of service mesh and tried Istio , you may have the following questions:\nWhy is Istio running on Kubernetes? What is the role of Kubernetes and a service mesh in the cloud native application architecture, respectively? What aspects of Kubernetes does Istio extend? What problems does it solve? What is the relationship between Kubernetes, Envoy, and Istio? This article will take you through the inner workings of Kubernetes and Istio. In addition, I will introduce the load balancing approach in Kubernetes, and explain why you need Istio when you have Kubernetes.\nKubernetes is essentially about application lifecycle management through declarative configuration, while a service mesh is essentially about providing inter-application traffic, security management and observability. If you have already built a stable application platform using Kubernetes, how do you set up load balancing and traffic control for calls between services? This is where a service mesh comes into the picture.\nEnvoy introduces the xDS protocol, which is supported by various open source software, such as Istio , MOSN , etc. Envoy contributes xDS to a service mesh or cloud native infrastructure. Envoy is essentially a modern version of a proxy that can be configured through APIs, based on which many different usage scenarios are derived — such as API Gateway, sidecar proxy in service mesh, and edge proxy.\nThis article contains the following:\nA description of the role of kube-proxy. The limitations of Kubernetes for microservice management. An introduction to the capabilities of Istio service mesh. A comparison of some of the concepts in Kubernetes, Envoy, and the Istio service mesh. Kubernetes vs Service Mesh The following diagram shows the service access relationship in Kubernetes and service mesh (one sidecar per pod model).\nFigure: Kubernetes vs Service Mesh Traffic Forwarding Each node in a Kubernetes cluster deploys a kube-proxy component that communicates with the Kubernetes API Server, gets information about the services in the cluster, and then sets iptables rules to send requests for service directly to the corresponding Endpoint (a pod belonging to the same group of services).\nService Discovery Figure: Service Discovery Istio can follow the service registration in Kubernetes and can also interface with other service discovery systems via platform adapters in the control plane; and then generate data plane configurations (using CRD, which are stored in etcd) with transparent proxies for the data plane. The transparent proxy of the data plane is deployed as a sidecar container in the pod of each application service, and all these proxies need to request the control plane to synchronize the proxy configuration. The proxy is “transparent” because the application container is completely unaware of the presence of the proxy. The kube-proxy component in the process needs to intercept traffic as well, except that the kube-proxy intercepts traffic to and from the Kubernetes node — while the sidecar proxy intercepts traffic to and from the pod.\nDisadvantages of a Service Mesh Since Kubernetes has many pods running on each node, putting the original kube-proxy route forwarding function in each pod will increase the response latency — due to more hops when the sidecar intercepts the traffic — and consume more resources. In order to manage traffic in a fine-grained manner, a series of new abstractions will be added. This will further increase the learning cost for users, but as the technology becomes more popular this situation will be slowly alleviated.\nAdvantages of a Service Mesh The kube-proxy settings are global and cannot be controlled at a granular level for each service, while service mesh takes the traffic control out of the service layer in Kubernetes by means of sidecar proxy — allowing for more elasticity.\nShortcomings of Kube-Proxy First, it does not automatically try another pod if the forwarded pod is not serving properly. Each pod has a health check mechanism and when a pod has health problems, kubelet will restart the pod and kube-proxy will remove the corresponding forwarding rules. Also, nodePort-type services cannot add TLS or more complex message routing mechanisms.\nKube-proxy implements load balancing of traffic across multiple pod instances of a Kubernetes service, but how do you do fine-grained control of traffic between these services — such as dividing traffic by percentage to different application versions (which are all part of the same service but on different deployments), or doing canary releases (grayscale releases) and blue-green releases?\nThe Kubernetes community gives a way to do canary releases using Deployment , which is essentially a way to assign different pods to a deployment’s service by modifying the pod’s label.\nKubernetes Ingress vs. Istio Gateway As mentioned above, kube-proxy can only route traffic within a Kubernetes cluster. The pods of a Kubernetes cluster are located in a network created by CNI. An ingress — a resource object created in Kubernetes — is created for communication outside the cluster. It’s driven by an ingress controller located on Kubernetes edge nodes responsible for managing north-south traffic. Ingress must be docked to various Ingress Controllers, such as the nginx ingress controller and traefik . Ingress is only applicable to HTTP traffic and is simple to use. It can only route traffic by matching a limited number of fields — such as service, port, HTTP path, etc. This makes it impossible to route TCP traffic such as MySQL, Redis, and various RPCs. This is why you see people writing nginx config language in ingress resource annotations.The only way to directly route north-south traffic is to use the service’s LoadBalancer or NodePort, the former requiring cloud vendor support and the latter requiring additional port management.\nIstio Gateway functions similarly to Kubernetes Ingress, in that it is responsible for north-south traffic to and from the cluster. Istio Gateway describes a load balancer for carrying connections to and from the edge of the mesh. The specification describes a set of open ports and the protocols used by those ports, the SNI configuration for load balancing, etc. Gateway is a CRD extension that also reuses the capabilities of the sidecar proxy; see the Istio website for detailed configuration.\nEnvoy Envoy is the default sidecar proxy in Istio. Istio extends its control plane based on Enovy’s xDS protocol. We need to familiarize ourselves with Envoy’s basic terminology before talking about Envoy’s xDS protocol. The following is a list of basic terms and their data structures in Envoy; please refer to the Envoy documentation for more details.\nFigure: Envoy Basic Terminology The following are the basic terms in Enovy that you should know.\nDownstream: The downstream host connects to Envoy, sends the request, and receives the response; i.e., the host that sent the request. Upstream: The upstream host receives connections and requests from Envoy and returns responses; i.e., the host that receives the requests. Listener: Listener is a named network address (for example, port, UNIX domain socket, etc.); downstream clients can connect to these listeners. Envoy exposes one or more listeners to the downstream hosts to connect. Cluster: A cluster is a group of logically identical upstream hosts to which Envoy connects. Envoy discovers the members of a cluster through service discovery. Optionally, the health status of cluster members can be determined through proactive health checks. Envoy decides which member of the cluster to route requests through a load balancing policy. Multiple listeners can be set in Envoy, each listener can set a filter chain (filter chain table), and the filter is scalable so that we can more easily manipulate the behavior of traffic — such as setting encryption, private RPC, etc.\nThe xDS protocol was proposed by Envoy and is the default sidecar proxy in Istio, but as long as the xDS protocol is implemented, it can theoretically be used as a sidecar proxy in Istio — such as the MOSN open source by Ant Group.\nFigure: img Istio is a very feature-rich service mesh that includes the following capabilities.\nTraffic Management: This is the most basic feature of Istio. Policy Control: Enables access control systems, telemetry capture, quota management, billing, etc. Observability: Implemented in the sidecar proxy. Security Authentication: The Citadel component does key and certificate management. Traffic Management in Istio The following CRDs are defined in Istio to help users with traffic management.\nGateway: Gateway describes a load balancer that runs at the edge of the network and is used to receive incoming or outgoing HTTP/TCP connections. VirtualService: VirtualService actually connects the Kubernetes service to the Istio Gateway. It can also perform additional operations, such as defining a set of traffic routing rules to be applied when a host is addressed. DestinationRule: The policy defined by the DestinationRule determines the access policy for the traffic after it has been routed. Simply put, it defines how traffic is routed. Among others, these policies can be defined as load balancing configurations, connection pool sizes, and external detection (for identifying and expelling unhealthy hosts in the load balancing pool) configurations. EnvoyFilter: The EnvoyFilter object describes filters for proxy services that can customize the proxy configuration generated by Istio Pilot. This configuration is generally rarely used by primary users. ServiceEntry: By default, services in the Istio service mesh are unable to discover services outside of the Mesh. ServiceEntry enables additional entries to be added to the service registry inside Istio, thus allowing automatically discovered services in the mesh to access and route to these manually added services. Kubernetes vs. xDS vs. Istio Having reviewed the abstraction of traffic …","date":"2021-04-07T08:27:17+08:00","relpermalink":"/en/blog/why-do-you-need-istio-when-you-already-have-kubernetes/","section":"blog","summary":"This article will take you through the inner workings of Kubernetes and Istio. In addition, I will introduce the load balancing approach in Kubernetes, and explain why you need Istio when you have Kubernetes.","title":"Why Do You Need Istio When You Already Have Kubernetes?"},{"content":"Translator’s Foreword “Quarkus in Action: Java Solutions Optimized for Kubernetes” is authored by Alex Soto Bueno and Jason Porter, translated by Zhang Xiaoyu, Liu Yan, and Song Jingchao, and published by Machinery Industry Press in March 2021.\nFigure: Quarkus Cookbook in Action Quarkus is a new technology framework that differs from traditional Java architectures. It builds upon familiar technology stacks, utilizing mature technologies such as JPA, JAX-RS, Eclipse Vert.x, Eclipse MicroProfile, and CDI, tightly integrated with Kubernetes. Users can leverage Kubernetes’ efficient scheduling and operational capabilities to maximize resource savings.\nThe spark of cloud-native technology, ignited by the popularity of the Kubernetes community, has grown into a raging fire. Cloud-native-related technologies are emerging like mushrooms after the rain. Liu Yan, Song Jingchao, and I are all members of the cloud-native community and enthusiasts who love to promote various related technologies. One of our common interests is to keep an eye on the release of excellent books on foreign or mature technologies in this field.\nIn this process, we coincidentally discovered this book, which had not been translated in China yet. With great enthusiasm, we embarked on the journey of studying Quarkus.\nThis book portrays the technical aspects of Quarkus in a detailed and thorough manner by presenting questions, proposing solutions, and sparking discussions. Through this book, users can self-learn relevant content and improve the efficiency of Java-related development work with Quarkus, enabling them to stand firm in the fast-paced fields of microservice construction and cloud-based application development.\nThroughout the entire translation process, we received full support from HuaZhong Publishing House and Editor Li Zhongming, for which we express our heartfelt gratitude.\nFinally, thank you all for having the opportunity to read this book. We hope that our efforts can help you, who advocate for cloud-native technology, enjoy the same joy as us on the technical path of Quarkus.\n","date":"2021-03-31T00:00:00Z","relpermalink":"/en/book/quarkus-cookbook/","section":"book","summary":"Authored by Alex Soto Bueno and Jason Porter.","title":"Quarkus in Action"},{"content":"Tetrate Academy has recently released the Istio Fundamentals Course, which is now available for free. Sign up at Tetrate Academy now!\nCourse curriculum Here is the curriculum:\nService Mesh and Istio Overview Installing Istio Observability: Telemetry and Logs Traffic Management Security Advanced Features Troubleshooting Real World Examples There are self-assessment questions at the end of each course. I have passed the course, and here is the certificate after passing the course.\nFigure: Tetrate Academy Istio Fundamentals Course More In the future, Tetrate will release the Certified Istio Administrator (CIA) exam and welcome all Istio users and administrators to follow and register for it.\n","date":"2021-03-26T12:27:17+08:00","relpermalink":"/en/notice/tetrate-istio-fundamental-courses/","section":"notice","summary":"Tetrate Academy has recently released the Istio Fundamentals Course, which is now available for free.","title":"Tetrate Academy Releases Free Istio Fundamentals Course"},{"content":"Different companies or software providers have devised countless ways to control user access to functions or resources, such as Discretionary Access Control (DAC), Mandatory Access Control (MAC), Role-Based Access Control (RBAC), and Attribute-Based Access Control (ABAC). In essence, whatever the type of access control model, three basic elements can be abstracted: user, system/application, and policy.\nIn this article, we will introduce ABAC, RBAC, and a new access control model — Next Generation Access Control (NGAC) — and compare the similarities and differences between the three, as well as why you should consider NGAC.\nWhat Is RBAC? RBAC, or Role-Based Access Control, takes an approach whereby users are granted (or denied) access to resources based on their role in the organization. Every role is assigned a collection of permissions and restrictions, which is great because you don’t need to keep track of every system user and their attributes. You just need to update appropriate roles, assign roles to users, or remove assignments. But this can be difficult to manage and scale. Enterprises that use the RBAC static role-based model have experienced role explosion: large companies may have tens of thousands of similar but distinct roles or users whose roles change over time, making it difficult to track roles or audit unneeded permissions. RBAC has fixed access rights, with no provision for ephemeral permissions or for considering attributes like location, time, or device. Enterprises using RBAC have had difficulty meeting the complex access control requirements to meet regulatory requirements of other organizational needs.\nRBAC Example Here’s an example Role in the “default” namespace in Kubernetes that can be used to grant read access to pods:\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [\u0026#34;v1\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] What Is ABAC? ABAC stands for Attribute-Based Access Control. At a high level, NIST defines ABAC as an access control method “where subject requests to perform operations on objects are granted or denied based on assigned attributes of the subject, environment conditions, and a set of policies that are specified in terms of those attributes and conditions.” ABAC is a fine-grained model since you can assign any attributes to the user, but at the same time it becomes a burden and hard to manage:\nWhen defining permissions, the relationship between users and objects cannot be visualized. If the rules are a little complex or confusingly designed, it will be troublesome for the administrator to maintain and trace. This can cause performance problems when there is a large number of permissions to process.\nABAC Example Kubernetes initially uses ABAC as access control and is configured via JSON Lines, for example:\nAlice can just read pods in namespace “foo”:\n{\u0026#34;apiVersion\u0026#34;: \u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: {\u0026#34;user\u0026#34;: \u0026#34;alice\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;pods\u0026#34;, \u0026#34;readonly\u0026#34;: true}} What Is NGAC? NGAC, or Next Generation Access Control, takes the approach of modeling access decision data as a graph. NGAC enables a systematic, policy-consistent approach to access control, granting or denying users administrative capabilities with a high level of granularity. NGAC was developed by NIST (National Institute of Standards and Technology) and is currently used in Tetrate Q and Tetrate Service Bridge .\nThere are several types of entities; they represent the resources you want to protect, the relationships between them, and the actors that interact with the system. The entities are:\nUsers Objects User attributes, such as organization unit Object attributes, such as folders Policy classes, such as file system access, location, and time NIST’s David Ferraiolo and Tetrate ‘s Ignasi Barrera shared how NGAC works at their presentation on Next Generation Access Control at Service Mesh Day 2019 in San Francisco.\nNGAC is based on the assumption that you can represent the system you want to protect in a graph that represents the resources you want to protect and your organizational structure, in a way that has meaning to you and that adheres to your organization semantics. On top of this model that is very particular to your organization, you can overlay policies. Between the resource model and the user model, the permissions are defined. This way NGAC provides an elegant way of representing the resources you want to protect, the different actors in the system, and how both worlds are tied together with permissions.\nFigure: NGAC DAG Image via Linear Time Algorithms to Restrict Insider Access using Multi-Policy Access Control Systems NGAC Example The following example shows a simple NGAC graph with a User DAG representing an organization structure, an Object DAG representing files and folders in a filesystem, a categorization of the files, and two different policies — file system and scope — that can be combined to make access decisions. The association edges between the two DAGs define the permissions the actors have on the target resources.\nFigure: NGAC In this graph we can see a representation of two files, “resume” and “contract” in the “/hr-docs” folder, each linked to a category (“public/confidential”). There are also two policy classes, “File System” and “Scope,” where the objects in the graph are attached — these need to be satisfied in order to get access to each file.\nUser Allice has read and write access to both files in the example, because a path links Allice to each of the files and the paths grant permissions on both policy classes. However, user Bob only has access to the “resume” file, because although there exists a path from Bob to the “contract” file that satisfies the “File System” policy class with “read” permissions, there is no path granting permissions on the “Scope” policy class. So, access to the “contract” file is denied to Bob.\nWhy Choose NGAC? The need to keep track of attributes of all objects creates a manageability burden in the case of ABAC. RBAC reduces the burden since we extract all access information to roles, but this paradigm suffers from role explosion problems and can also become unmanageable. With NGAC we have everything we need in graphs — in a compact, centralized fashion.\nWhen access decisions are complex, processing times of ABAC can rise exponentially. RBAC becomes especially hard to manage at scale, while NGAC scales linearly.\nWhere NGAC really shines is in flexibility. It can be configured to allow or disallow access based not only on object attributes, but also on other conditions — time, location, phase of the moon, and so on.\nOther key advantages of NGAC include the ability to set policies consistently (to meet compliance requirements) and the ability to set ephemeral policies. For example, NGAC could grant a developer one-time access to resources during an outage, without leaving unnecessary permissions in place that could later lead to a security breach. NGAC can evaluate and combine multiple policies in a single access decision, while keeping its linear time complexity.\nSummary The following table compares ABAC, RBAC, and NGAC in several aspects.\nFigure: NGAC vs RBAC vs ABAC In conclusion:\nRBAC is simpler and has good performance, but can suffer at scale. ABAC is flexible, but performance and auditability are a problem. NGAC fixes those gaps by using a novel, elegant revolutionary approach: overlay access policies on top of an existing representation of the world, provided by the user. You can model RBAC and ABAC policies as well. References Guide to Attribute-Based Access Control (ABAC) Definition and Considerations Deploying ABAC policies using RBAC Systems RBAC vs. ABAC: What’s the Difference? Role Explosion: The Unintended Consequence of RBAC Exploring the Next Generation of Access Control Methodologies ","date":"2021-02-20T14:12:40+08:00","relpermalink":"/en/blog/why-you-should-choose-ngac-as-your-access-control-model/","section":"blog","summary":"This article will introduce you to the next generation permission control model, NGAC, and compare ABAC, RABC, and explain why you should choose NGAC.","title":"Why You Should Choose NGAC as Your Access Control Model"},{"content":" Figure: IstioCon 2021 poster (Jimmy Song) Topic: Service Mesh in China Time: February 23rd, 10:00 - 10:10 am Beijing time How to participate: IstioCon 2021 website Cost: Free From February 22-25, Beijing time, the Istio community will be hosting the first IstioCon online, and registration is free to attend! I will be giving a lightning talk on Tuesday, February 23rd (the 12th day of the first month of the lunar calendar), as an evangelist and witness of Service Mesh technology in China, I will introduce the Service Mesh industry and community in China.\nI am a member of the inaugural IstioCon organizing committee with Zhonghu Xu (Huawei) and Shaojun Ding (Intel), as well as the organizer of the China region. Considering Istio’s large audience in China, we have arranged for Chinese presentations that are friendly to the Chinese time zone. There will be a total of 14 sharing sessions in Chinese, plus dozens more in English. The presentations will be in both lightning talk (10 minutes) and presentation (40 minutes) formats.\nJoin the Cloud Native Community Istio SIG to participate in the networking at this conference. For the schedule of IstioCon 2021, please visit IstioCon 2021 official website , or click for details.\n","date":"2021-02-16T18:27:17+08:00","relpermalink":"/en/notice/istiocon-2021/","section":"notice","summary":"IstioCon 2021, I'll be giving a lightning talk, February 22nd at 10am BST.","title":"IstioCon 2021 Lightning Talk Preview"},{"content":"The ServiceMesher website has lost connection with the webhook program on the web publishing server because the GitHub where the code is hosted has has been “lost” and the hosting server is temporarily unable to log in, so the site cannot be updated. Today I spent a day migrating all the blogs on ServiceMesher to the Cloud Native Community website cloudnative.to , and as of today, there are 354 blogs on the Cloud Native Community.\nFigure: ServiceMesher blogs Now we plan to archive ServiceMesher official GitHub (all pages under the servicemesher.com domain) We are no longer accepting new PRs, so please submit them directly to the Cloud Native Community . Thank you all!\n","date":"2021-02-13T18:27:17+08:00","relpermalink":"/en/notice/servicemesher-blog-merged/","section":"notice","summary":"ServiceMesher website is no longer maintained, plan to archive the website code, the blog has been migrated to Cloud Native Community, please submit the new blog to Cloud Native Community.","title":"ServiceMesher website is no longer maintained"},{"content":"In this article, I’ll give you an overview of Istio ‘s history of virtual machine integration support. In particular, the introduction of the smart DNS proxy and WorkloadGroup in Istio 1.8, which makes virtual machines and containers equivalent at the resource abstraction level.\nI will show you a tumultuous odyssey of Istio’s virtual machine integration. Tetrate, the enterprise service mesh company that made pushing Istio to run everywhere part of its founding mission, has used VM features extensively in customer deployments and has been instrumental in pushing VMs to Istio upstream.\nPreface In my previous article , I talked about how Istio 1.7 supported virtual machines. But at that time, late October, virtual machines were still not seamlessly integrated into Istio — there was still a lot of manual work required. Now, Istio 1.8 has added WorkloadGroup and smart DNS proxy, which allows non-Kubernetes workloads like VMs to become first-class citizens in Istio — just like pods.\nWith or without a sidecar installed for virtual machines, until 1.7 you could not resolve the DNS name of a Kubernetes service unless a kube-external DNS was configured — which is the last piece of virtual machine integration in Istio. This shortcoming has finally been fixed in Istio 1.8.\nWhy Is Virtual Machine Support Important? In the process of migrating our applications to cloud native architectures and continuously containerizing them, we will go through three phases as shown in the figure below.\nFigure: Cloud Native Stages Stage 1: All applications are deployed on virtual machines Stage 2: Applications are deployed on both virtual machines and containers, are migrating from virtual machines to containers, and are using Kubernetes to manage containers. Stage 3: All applications are deployed in containers first, using Kubernetes to manage containers and Istio to manage service-to-service communication. The above diagram is artificially simplified: in reality, there might be multiple hybrid clouds, multiple regions, multiple clusters, etc. Plus, at stage 3 containers and virtual machines may remain in long-term coexistence, but the trend of containerization remains unchanged.\nIstio’s History of Virtual Machine Support Istio’s support for virtual machines is a long process, an odyssey of sorts.\n0.2: Istio Mesh Expansion As of version 0.2, Istio added virtual machines to the Mesh via Istio Mesh Expansion , provided that the following prerequisites were met.\nVirtual machines must have direct access to the application’s pods via IP address, which requires a flat network between the container and the VM via VPC or VPN; and virtual machines do not need access to the Cluster IP, but rather direct access to the service’s endpoints. Virtual machines must have access to Istio’s control plane services (Pilot, Mixer, CA, now being integrated as Istiod), which can expose the control plane endpoints to virtual machines by deploying load balancers in the Istio Mesh. (optional) the virtual machine has access to the DNS server inside the Mesh (deployed in Kubernetes). The steps to integrate a virtual machine are as follows.\nCreate an internal load balancer for the Istio control plane service and the DNS service for the Kubernetes cluster. Generate a configuration file for the Istio Service CIDR, Service Account token, security certificate, and IP of the Istio Control Plane Service (the IP exposed through the Internal Load Balancer) and send it to the virtual machine. Setup the Istio component, dnsmaq (for DNS discovery), in the virtual machine; so that the virtual machine can access the services in the mesh using FQDN, to ensure that the virtual machine can correctly resolve the Cluster IP of the services in the mesh. To run the service in a virtual machine, you need to configure the sidecar, add inbound ports to be intercepted, then restart Istio and also run istioctl to register the service. The following figure shows the detailed flow from integrating a virtual machine to accessing services in the virtual machine in a mesh.\nFigure: Figure 1 Figure 1\nThe DNS is hijacked by dnsmasq deployed in the virtual machine, which allows it to correctly obtain the Cluster IP of the Istio service (Kubernetes’ built-in DNS). Access to Kubernetes’ built-in DNS service (which is exposed outside the cluster via the Internal Load Balancer and can be accessed directly). Return the Cluster IP resolved by productpage.bookinfo.svc.cluster.local, noting that the IP address is not directly accessible, but failure to be DNS resolved will result in a failed VM request for the service. The virtual machine’s call to services in a mesh is hijacked by the sidecar proxy. Since the proxy is connected to the Istio control plane, the endpoints of the service can be queried via xDS, so traffic will be forwarded to one of the endpoints. To access VM services in mesh, you need to manually add VM services to mesh using the istioctl register command, which essentially registers the VM services to the service and endpoint in Kubernetes. Services in the mesh can be accessed using the VM-registered service name (FQDN, e.g. mysql.vm.svc.cluster.local). The above Istio support for virtual machines continued with Istio 1.0, which introduced a new API ServiceEntry with Istio 1.1, that allows additional entries to be added to Istio’s internal service registry so that services in the mesh can access/route to these manually specified services. The istioctl register command is no longer needed and will be deprecated in Istio 1.9.\nThe istioctl experimental add-to-mesh command has been added to Istio 1.5 to add services from a virtual machine to a mesh, and it works just like the istioctl register.\n1.6 to 1.7: New Resource Abstractions Istio introduced a new resource type, WorkloadEntry , in traffic management from version 1.6 , to abstract virtual machines so that they can be added to the mesh as equivalent loads to the pods in Kubernetes; with traffic management, security management, observability, etc. The mesh configuration process for virtual machines is simplified with WorkloadEntry, which selects multiple workload entries and Kubernetes pods based on the label selector specified in the service entry.\nIstio 1.8 adds a resource object for WorkloadGroup that provides a specification that can include both virtual machines and Kubernetes workloads, designed to mimic the existing sidecar injection and deployment specification model for Kubernetes workloads to bootstrap Istio agents on the VMs.\nBelow is a comparison of resource abstraction levels for virtual machines versus workloads in Kubernetes.\nItem Kubernetes Virtual Machine Basic schedule unit Pod WorkloadEntry Component Deployment WorkloadGroup Service register and discovery Service ServiceEntry From the above diagram, we can see that for virtual machine workloads there is a one-to-one correspondence with the workloads in Kubernetes.\nEverything seems perfect at this point. However, exposing the DNS server in the Kubernetes cluster directly is a big security risk , so we usually manually write the domain name and Cluster IP pair of the service the virtual machine needs to access to the local /etc/hosts — but this is not practical for a distributed cluster with a large number of nodes.\nThe process of accessing the services inside mesh by configuring the local /etc/hosts of the virtual machine is shown in the following figure.\nFigure: Figure 2 Figure 2\nRegistration of services in the virtual machine into the mesh. Manually write the domain name and Cluster IP pairs of the service to be accessed to the local /etc/hosts file in the virtual machine. Cluster IP where the virtual machine gets access to the service. The traffic is intercepted by the sidecar proxy and the endpoint address of the service to be accessed is resolved by Envoy. Access to designated endpoints of the service. In Kubernetes, we generally use the Service object for service registration and discovery; each service has a separate DNS name that allows applications to call each other by using the service name. We can use ServiceEntry to register a service in a virtual machine into Istio’s service registry, but a virtual machine cannot access a DNS server in a Kubernetes cluster to get the Cluster IP if the DNS server is not exposed externally to the mesh, which causes the virtual machine to fail to access the services in the mesh. Wouldn’t the problem be solved if we could add a sidecar to the virtual machine that would transparently intercept DNS requests and get the Cluster IP of all services in the mesh, similar to the role of dnsmasq in Figure 1?\nAs of Istio 1.8 — Smart DNS Proxy With the introduction of smart DNS proxy in Istio 1.8, virtual machines can access services within the mesh without the need to configure /etc/hosts, as shown in the following figure.\nFigure: Figure 3 Figure 3\nThe Istio agent on the sidecar will come with a cached DNS proxy dynamically programmed by Istiod. DNS queries from the application are transparently intercepted and served by the Istio proxy in the pod or VM, with the response to DNS query requests, enabling seamless access from the virtual machine to the service mesh.\nThe WorkloadGroup and smart DNS proxy introduced in Istio 1.8 provide powerful support for virtual machine workloads, making legacy applications deployed in virtual machines fully equivalent to pods in Kubernetes.\nSummary In this odyssey of Istio’s virtual machine support, we can see the gradual realization of unified management of virtual machines and pods — starting with exposing the DNS server in the mesh and setting up dnsmasq in the virtual machine, and ending with using smart DNS proxies and abstracting resources such as WorkloadEntry, WorkloadGroup and ServiceEntry. This article only focuses on the single cluster situation, which is not enough to be used in real production. We also need to deal with security, multicluster, multitenancy, etc.\nReferenced resources Tetrate …","date":"2021-01-23T08:27:17+08:00","relpermalink":"/en/blog/istio-vm-odysssey/","section":"blog","summary":"In this article, I’ll give you an overview of Istio‘s history of virtual machine integration support. In particular, the introduction of the smart DNS proxy and WorkloadGroup in Istio 1.8, which makes virtual machines and containers equivalent at the resource abstraction level.","title":"Istio 1.8: A Virtual Machine Integration Odyssey"},{"content":"A service mesh is a relatively simple concept, consisting of a bunch of network proxies paired with each service in an application, plus a set of task management processes. The proxies are called the data plane and the management processes are called the control plane in the Service Mesh. The data plane intercepts calls between different services and “processes” them; the control plane is the brain of the mesh that coordinates the behavior of proxies and provides APIs for operations and maintenance personnel to manipulate and observe the entire network.\nThe diagram below shows the architecture of a service mesh.\nFigure: Service Mesh Architecture Further, the service mesh is a dedicated infrastructure layer designed to enable reliable, fast, and secure inter-service invocation in microservices architectures. It is not a mesh of “services” but rather a mesh of “proxies” that services can plug into, thus abstracting the network from the application code. In a typical service mesh, these proxies are injected into each service deployment as a sidecar (and also may be deployed at the edge of the mesh). Instead of invoking services directly over the network, services invoke their local sidecar proxy, which in turn manages requests on behalf of the service, pushing the complexities of inter-service communications into a networking layer that can resolve them at scale. The set of interconnected sidecar proxies implements a so-called data plane, while on the other hand the service mesh control plane is used to configure proxies. The infrastructure introduced by a service mesh provides an opportunity, too, to collect metrics about the traffic that is flowing through the application.\nThe architecture of a service mesh The infrastructure layer of a service mesh is divided into two main parts: the control plane and the data plane.\nCharacteristics of the control plane\nDo not parse packets directly. Communicates with proxies in the control plane to issue policies and configurations. Visualizes network behavior. Typically provides APIs or command-line tools for configuration versioning and management for continuous integration and deployment. Characteristics of the data plane\nIs usually designed with the goal of statelessness (though in practice some data needs to be cached to improve traffic forwarding performance). Directly handles inbound and outbound packets, forwarding, routing, health checking, load balancing, authentication, authentication, generating monitoring data, etc. Is transparent to the application, i.e., can be deployed senselessly. Changes brought by the service mesh Decoupling of microservice governance from business logic\nA service mesh takes most of the capabilities in the SDK out of the application, disassembles them into separate processes, and deploys them in a sidecar model. By separating service communication and related control functions from the business process and synching them to the infrastructure layer, a service mesh mostly decouples them from the business logic, allowing application developers to focus more on the business itself.\nNote that the word “mostly” is mentioned here and that the SDK often needs to retain protocol coding and decoding logic, or even a lightweight SDK to implement fine-grained governance and monitoring policies in some scenarios. For example, to implement method-level call distributed tracing, the service mesh requires the business application to implement trace ID passing, and this part of the implementation logic can also be implemented through a lightweight SDK. Therefore, the service mesh is not zero-intrusive from a code level.\nUnified governance of heterogeneous environments\nWith the development of new technologies and staff turnover, there are often applications and services in different languages and frameworks in the same company, and in order to control these services uniformly, the previous practice was to develop a complete set of SDKs for each language and framework, which is very costly to maintain. With a service mesh, multilingual support is much easier by synching the main service governance capabilities to the infrastructure. By providing a very lightweight SDK, and in many cases, not even a separate SDK, it is easy to achieve unified traffic control and monitoring requirements for multiple languages and protocols.\nFeatures of service mesh Service mesh also has three major technical advantages over traditional microservice frameworks.\nObservability\nBecause the service mesh is a dedicated infrastructure layer through which all inter-service communication passes, it is uniquely positioned in the technology stack to provide uniform telemetry at the service invocation level. This means that all services are monitored as “black boxes.” The service mesh captures route data such as source, destination, protocol, URL, status codes, latency, duration, etc. This is essentially the same data that web server logs can provide, but the service mesh captures this data for all services, not just the web layer of individual services. It is important to note that collecting data is only part of the solution to the observability problem in microservice applications. Storing and analyzing this data needs to be complemented by mechanisms for additional capabilities, which then act as alerts or automatic instance scaling, for example.\nTraffic control\nWith a service mesh, services can be provided with various control capabilities such as intelligent routing (blue-green deployment, canary release, A/B test), timeout retries, circuit breaking, fault injection, traffic mirroring, etc. These are often features that are not available in traditional microservices frameworks but are critical to the system. For example, the service mesh carries the communication traffic between microservices, so it is possible to test the robustness of the whole application by simulating the failure of some microservices through rules for fault injection in the grid. Since the service mesh is designed to efficiently connect source request calls to their optimal destination service instances, these traffic control features are “destination-oriented.” This is a key feature of the service mesh’s traffic control capabilities.\nSecurity\nTo some extent, monolithic applications are protected by their single address space. However, once a monolithic application is broken down into multiple microservices, the network becomes a significant attack surface. More services mean more network traffic, which means more opportunities for hackers to attack the information flow. And service mesh provides the capabilities and infrastructure to protect network calls. The security-related benefits of service mesh are in three core areas: authentication of services, encryption of inter-service communications, and enforcement of security-related policies.\nService mesh has brought about tremendous change and has strong technical advantages, and has been called the second generation of “microservice architecture.” However, there is no silver bullet in software development. Traditional microservices architecture has many pain points, and service mesh is no exception. It has its limitations.\nIncreased complexity\nService mesh introduces sidecar proxies and other components into an already complex, distributed environment, which can greatly increase the overall chain and operational O\u0026amp;M complexity. Ops needs to be more specialized. Adding a service mesh such as Istio to a container orchestrator such as Kubernetes often requires Ops to become an expert in both technologies in order to fully utilize the capabilities of both and to troubleshoot the problems encountered in the environment.\nLatency\nAt the link level, a service mesh is an invasive, complex technology that can add significant latency to system calls. This latency is on the millisecond level, but it can also be intolerable in special business scenarios.\nPlatform adaptation\nThe intrusive nature of service mesh forces developers and operators to adapt to highly autonomous platforms and adhere to the platform’s rules.\nThe relationship between service mesh and Kubernetes Kubernetes is essentially application lifecycle management, specifically the deployment and management (scaling, auto-recovery, publishing) of containerized applications. Service mesh decouples traffic management from Kubernetes, eliminating the need for a kube-proxy component for internal traffic, and manages inter-service and ingress traffic, security, and observability through an abstraction closer to the microservice application layer. The xDS used by Istio and Envoy is one of the protocol standards for service mesh configuration.\nOrganizations that use Kubernetes often turn to a service mesh to address the networking issues that arise with containerization — but notably, a service mesh can work with a legacy or a modern workload, and can be put in place prior to containerization for a faster, safer path to modernization.\nSummary Readers should look dialectically at the advantages and disadvantages of a service mesh compared with traditional microservices architecture. A service mesh can be a critical part of the evolutionary path of application architecture, from the earliest monolith to distributed, to microservices, containerization, container orchestration, to hybrid workloads and multi-cloud.\nLooking ahead, Kubernetes is exploding, and it has become the container orchestration of choice for enterprise greenfield applications. If Kubernetes has completely won the market and the size and complexity of Kubernetes-based applications continue to grow, there will be a tipping point, and service mesh will be necessary to effectively manage these applications. As service mesh technology continues to evolve and the architecture and functionality of its implementation products, such as Istio, continue to be optimized, service mesh will completely replace traditional microservice architectures as the …","date":"2021-01-22T08:27:17+08:00","relpermalink":"/en/blog/what-is-a-service-mesh/","section":"blog","summary":"This article will take you through what a service mesh is, as well as its architecture, features, and advantages and disadvantages.","title":"What Is a Service Mesh?"},{"content":"1.8 is the last version of Istio to be released in 2020 and it has the following major updates:\nSupports installation and upgrades using Helm 3. Mixer was officially removed. Added Istio DNS proxy to transparently intercept DNS queries from applications. WorkloadGroup has been added to simplify the integration of virtual machines. WorkloadGroup is a new API object. It is intended to be used with non-Kubernetes workloads like Virtual Machines and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies.\nInstallation and Upgrades Istio starts to officially support the use of Helm v3 for installations and upgrades. In previous versions, the installation was done with the istioctl command-line tool or Operator. With version 1.8, Istio supports in-place and canary upgrades with Helm.\nEnhancing Istio’s Usability The istioctl command-line tool has a new bug reporting feature (istioctl bug-report ), which can be used to collect debugging information and get cluster status.\nThe way to install the add-on has changed: 1.7 istioctl is no longer recommended and has been removed in 1.8, to help solve the problem of add-on lagging upstream and to make it easier to maintain.\nTetrate is an enterprise service mesh company. Our flagship product, TSB, enables customers to bridge their workloads across bare metal, VMs, K8s, \u0026amp; cloud at the application layer and provide a resilient, feature-rich service mesh fabric powered by Istio, Envoy, and Apache SkyWalking.\nMixer, the Istio component that had been responsible for policy controls and telemetry collection, has been removed. Its functionalities are now being served by the Envoy proxies. For extensibility, service mesh experts recommend using WebAssembly (Wasm) to extend Envoy; and you can also try the GetEnvoy Toolkit , which makes it easier for developers to create Wasm extensions for Envoy. If you still want to use Mixer, you must use version 1.7 or older. Mixer continued receiving bug fixes and security fixes until Istio 1.7. Many features supported by Mixer have alternatives as specified in the Mixer Deprecation document, including the in-proxy extensions based on the Wasm sandbox API.\nSupport for Virtual Machines Istio’s recent upgrades have steadily focused on making virtual machines first-class citizens in the mesh. Istio 1.7 made progress to support virtual machines and Istio 1.8 adds a smart DNS proxy , which is an Istio sidecar agent written in Go. The Istio agent on the sidecar will come with a cache that is dynamically programmed by Istiod DNS Proxy. DNS queries from applications are transparently intercepted and served by an Istio proxy in a pod or VM that intelligently responds to DNS query requests, enabling seamless multicluster access from virtual machines to the service mesh.\nIstio 1.8 adds a WorkloadGroup , which describes a collection of workload instances. It provides a specification that the workload instances can use to bootstrap their proxies, including the metadata and identity. It is only intended to be used with non-k8s workloads like Virtual Machines, and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies. Using WorkloadGroups, Istio has started to help automate VM registration with istioctl experimental workload group .\nTetrate , the enterprise service mesh company, uses these VM features extensively in customers’ multicluster deployments, to enable sidecars to resolve DNS for hosts exposed at ingress gateways of all the clusters in a mesh; and to access them over mutual TLS.\nConclusion All in all, the Istio team has kept the promise made at the beginning of the year to maintain a regular release cadence of one release every three months since the 1.1 release in 2018, with continuous optimizations in performance and user experience for a seamless experience of brownfield and greenfield apps on Istio. We look forward to more progress from Istio in 2021.\n","date":"2020-11-19T16:43:27+08:00","relpermalink":"/en/blog/istio-1-8-a-smart-dns-proxy-takes-support-for-virtual-machines-a-step-further/","section":"blog","summary":"WorkloadGroup is a new API object. It is intended to be used with non-Kubernetes workloads like Virtual Machines and is meant to mimic the existing sidecar injection and deployment specification model used for Kubernetes workloads to bootstrap Istio proxies.","title":"Istio 1.8: A Smart DNS Proxy Takes Support For Virtual Machines A Step Further"},{"content":"Istio is a popular service mesh to connect, secure, control, and observe services. When it was first introduced as open source in 2017, Kubernetes was winning the container orchestration battle and Istio answered the needs of organizations moving to microservices. Although Istio claims to support heterogeneous environments such as Nomad, Consul, Eureka, Cloud Foundry, Mesos, etc., in reality, it has always worked best with Kubernetes — on which its service discovery is based.\nIstio was criticized for a number of issues early in its development, for the large number of components, the complexity of installation and maintenance, the difficulty of debugging, a steep learning curve due to the introduction of too many new concepts and objects (up to 50 CRDs), and the impact of Mixer components on performance. But these issues are gradually being overcome by the Istio team. As you can see from the roadmap released in early 2020, Istio has come a long way.\nBetter integration of VM-based workloads into the mesh is a major focus for the Istio team this year. Tetrate also offers seamless multicloud connectivity, security, and observability, including for VMs, via its product Tetrate Service Bridge . This article will take you through why Istio needs to integrate with virtual machines and how you can do so.\nWhy Should Istio Support Virtual Machines? Although containers and Kubernetes are now widely used, there are still many services deployed on virtual machines and APIs outside of the Kubernetes cluster that needs to be managed by Istio mesh. It’s a huge challenge to unify the management of the brownfield environment with the greenfield.\nWhat Is Needed to Add VMs to the Mesh? Before the “how,” I’ll describe what is needed to add virtual machines to the mesh. There are a couple of things that Istio must know when supporting virtual machine traffic: which VMs have services that should be part of the mesh, and how to reach the VMs. Each VM also needs an identity, in order to communicate securely with the rest of the mesh. These requirements could work with Kubernetes CRDs, as well as a full-blown Service Registry like Consul. And the service account based identity bootstrapping could work as a mechanism for assigning workload identities to VMs that do not have a platform identity. For VMs that do have a platform identity (like EC2, GCP, Azure, etc.), work is underway in Istio to exchange the platform identity with a Kubernetes identity for ease of setting up mTLS communication.\nHow Does Istio Support Virtual Machines? Istio’s support for virtual machines starts with its service registry mechanism. The information about services and instances in the Istio mesh comes from Istio’s service registries, which up to this point have only looked at or tracked pods. In newer versions, Istio now has resource types to track and watch VMs. The sidecars inside the mesh cannot observe and control traffic to services outside the mesh, because they do not have any information about them.\nThe Istio community and Tetrate have done a lot of work on Istio’s support for virtual machines. The 1.6 release included the addition of WorkloadEntry, which allows you to describe a VM exactly as you would a host running in Kubernetes. In 1.7, the release started to add the foundations for bootstrapping VMs into the mesh automatically through tokens, with Istio doing the heavy lifting. Istio 1.8 will debut another abstraction called WorkloadGroup, which is similar to a Kubernetes Deployment object — but for VMs.\nThe following diagram shows how Istio models services in the mesh. The predominant source of information comes from a platform service registry like Kubernetes, or a system like Consul. In addition, the ServiceEntry serves as a user-defined service registry, modeling services on VMs or external services outside the organization.\nWhy install Istio in a virtual machine when you can just use ServiceEntry to bring in the services in the VMs?\nUsing ServiceEntry, you can enable services inside the mesh to discover and access external services; and in addition, manage the traffic to those external services. In conjunction with VirtualService, you can also configure access rules for the corresponding external service — such as request timeouts, fault injection, etc. — to enable controlled access to the specified external service.\nEven so, it only controls the traffic on the client-side, not access to the introduced external service to other services. That is, it cannot control the behavior of the service as the call initiator. Deploying sidecars in a virtual machine and introducing the virtual machine workload via workload selector allows the virtual machine to be managed indiscriminately, like a pod in Kubernetes.\nFuture As you can see from the bookinfo demo , there is too much manual work involved in the process and it’s easy to go wrong. In the future, Istio will improve VM testing to be realistic, automate bootstrapping based on platform identity, improve DNS support and istioctl debugging, and more. You can follow the Istio Environment Working Group for more details about virtual machine support.\nReferences Virtual Machine Installation Virtual Machines in Single-Network Meshes Istio: Bringing VMs into the Mesh (with Cynthia Coan) Bridging Traditional and Modern Workloads ","date":"2020-11-02T16:43:27+08:00","relpermalink":"/en/blog/how-to-integrate-virtual-machines-into-istio-service-mesh/","section":"blog","summary":"Better integration of virtual machine-based workloads into the service mesh is a major focus for the Istio team this year, and Tetrate also provides seamless multi-cloud connectivity, security and observability, including for virtual machines, through its product Tetrate Service Bridge. This article will show you why Istio needs to integrate with virtual machines and how.","title":"How to Integrate Virtual Machines Into Istio Service Mesh"},{"content":"Today is my 914th day and also the last day with Ant Group , tomorrow is September 1st, which is usually the day school starts, and everyone at Alibaba is known as “classmate”, tomorrow I will join Tetrate , and that’s kind of starting my new semester!\nAnt/Alibaba and the Cloud Native Community To date, Ant/Alibaba Group has had a profound impact on my career, especially its corporate culture and values, and the Alibaba recruiting philosophy of “finding like-minded people”, and isn’t the process of creating the Cloud Native Community also a process of finding like-minded people? Cloud Native Community is like a small society, I don’t want it to have much social value, but only want it to make a small but beautiful change to individuals, to enterprises and to society. I constantly think about myself as an individual and as an employee, especially as an initiator of the community. What is my mission as an individual, an employee, and especially as an initiator of a community? What role should I play in the company? Where is this community going? I’m fumbling along, but because of your support, it makes me stronger and more committed to the adoption and application of cloud native technology in China, outside of me I may have gone faster, but now with the community together we will go further!\nFigure: 24 June 2019, Shanghai, KubeCon China 2019 June 24, 2019, Shanghai, KubeCon China 2019\nJoining Tetrate Over the past two years, I’ve been working hard to promote Istio and Service Mesh technology, and with funding from Ant Group, I started the ServiceMesher Community to bring Service Mesh technology to China. Next I want to bring Chinese practice to the world.\nAs a Developer Advocate, the most important thing is not to stop learning, but to listen and take stock. Over the past two years, I’ve seen a lot of people show interest in Service Mesh, but not enough to understand the risks and lack of knowledge about the new technology. I’m excited to join this Service Mesh-focused startup Tetrate , a global telecommuting startup with products built around open source Istio , [Envoy](https:/ /envoyproxy.io) and Apache SkyWalking , it aims to make it to be the cloud native network infrastructure. Here are several maintainers of these open source projects, such as Sheng Wu , Zack Butcher , Lizan Zhou , etc., and I believe that working with them can help you understand and apply Service Mesh quickly and effectively across cloud native.\nMore Earlier this year as I was preparing for the Cloud Native community, I set the course for the next three years - cloud native, open source and community. The road to pursue my dream is full of thorns, not only need courage and perseverance, but also need you to be my strong backing, I will overcome the thorns and move forward. Open source belongs to the world, to let the world understand us better, we must be more active into the world. I hope that China’s open source tomorrow will be better, I hope that Service Mesh technology will be better applied by the enterprises in China, I hope that cloud native can benefit the public, and I hope that we can all find our own mission.\nWe are hiring now, if you are interested with Tetrate , please send your resume to careers@tetrate.io .\n","date":"2020-08-31T08:27:17+08:00","relpermalink":"/en/blog/moving-on-from-ant-group/","section":"blog","summary":"Today is my last day at Ant and tomorrow I'm starting a new career at Tetrate.","title":"New Beginning - Goodbye Ant, Hello Tetrate"},{"content":"“Cloud Native Patterns,” authored by Cornelia Davis, translated by Zhang Ruofei and Song Jingchao, published by Publishing House of Electronics Industry in August 2020.\nFigure: Cover of the book Cloud Native Patterns What exactly are we talking about when we discuss cloud-native? I’ve been pondering this question for years, and perspectives may vary. Since translating the first book on cloud-native three years ago, I’ve been involved in translating and creating a series of works on cloud-native. Through participation in and observation of open-source projects, communities, foundations, and the process of application cloudification in the cloud-native field, I’ve come to the conclusion: cloud-native is a way of behavior and design philosophy. Essentially, any behavior or method that can improve resource utilization and application delivery efficiency in the cloud is cloud-native. The history of cloud computing is a history of cloud-native transformation. Cloud-native is the inevitable result of cloud computing adapting to social division of labor, entrusting system resources, underlying infrastructure, and application orchestration to cloud platforms, allowing developers to focus on business logic. Isn’t this what cloud computing has been striving for all along? Cloud-native applications pursue the rapid construction of highly fault-tolerant, elastic distributed applications, striving for the ultimate development efficiency and friendly online and operational experience. With the concept of cloud-native, born to be deployed in the cloud, they can maximize the dividends brought by cloud computing.\nBefore this, I translated several books on cloud-native topics, including “Cloud Native Go” by Kevin Hoffman and “Cloud Native Java” by Josh Long. They both come from Pivotal or have worked at Pivotal for many years. When I saw this book, I was surprised to find that the author, Cornelia Davis, also comes from this company. Pivotal is truly the cradle of cloud-native. The content of this book is different from previous cloud-native books, innovatively organizing patterns, so I immediately contacted Zhang Chunyu, the editor of Publishing House of Electronics Industry. He informed me that Zhang Ruofei was translating this book. Previously, I had cooperated with him to translate “Cloud Native Java.” This book is our second collaboration, and I truly admire his accuracy and efficiency in translating books. We each translated half of the content of this book.\nEveryone is discussing cloud-native, but how to implement it is still a matter of debate. This book lists 12 patterns for building cloud-native applications, mainly focusing on data, services, and interactions of cloud-native applications, namely, design patterns at the application level. These patterns are interspersed throughout the chapters of the second part of this book, covering various aspects of cloud-native applications, and combining theory with practice, guiding readers to implement a cloud-native application using Java.\nI also want to express my gratitude to the members and volunteers of the Cloud Native Community for their contributions to the development of cloud-native in China. Your encouragement and support are the driving force behind continuous efforts and exploration in the field of cloud-native. There may be some omissions in the translation process of this book, and I hope readers can point them out.\n","date":"2020-08-09T00:00:00Z","relpermalink":"/en/book/cloud-native-patterns/","section":"book","summary":"Authored by Cornelia Davis.","title":"Cloud Native Patterns"},{"content":"Just tonight, the jimmysong.io website was moved to the Alibaba Cloud Hong Kong node. This is to further optimize the user experience and increase access speed. I purchased an ECS on the Alibaba Cloud Hong Kong node, and now I have a public IP and can set subdomains. The website was previously deployed on GitHub Pages, the access speed is average, and it has to withstand GitHub instability. Impact (In recent years, GitHub downtime has occurred).\nMeanwhile, the blog has also done a lot to improve the site, thanks to Bai Jun away @baijunyao strong support, a lot of work for the revision of the site, including:\nChanged the theme color scheme and deepened the contrast Use aligolia to support full site search Optimized mobile display Articles in the blog have added zoom function Added table of contents to blog post This site is built on the theme of educenter .\nThanks to the majority of netizens who have supported this website for several years. The website has been in use for more than three years and has millions of visits. It has undergone two major revisions before and after, on January 31, 2020 and October 8, 2017, respectively. And changed the theme of the website. In the future, I will share more cloud-native content with you as always, welcome to collect, forward, and join the cloud-native community to communicate with the majority of cloud-native developers.\n","date":"2020-07-09T21:27:17+08:00","relpermalink":"/en/notice/migrating-to-alibaba-cloud/","section":"notice","summary":"Move the website to the Alibaba Cloud Hong Kong node to increase the speed of website access and the convenience of obtaining public IP and subdomain names.","title":"Move to Alibaba Cloud Hong Kong node"},{"content":" Just the other day, Java just celebrated its 25th birthday , and from the time of its birth it was called “write once, run everywhere”, but more than 20 years later, there is still a deep gap between programming and actual production delivery. the world of IT is never short of concepts, and if a concept doesn’t solve the problem, then it’s time for another layer of concepts. it’s been 6 years since Kubernetes was born, and it’s time for the post-Kubernetes era - the era of cloud-native applications!\nFigure: Cloud Native Stage This white paper will take you on a journey to explore the development path of cloud-native applications in the post-Kubernetes era.\nHighlights of the ideas conveyed include.\nCloud-native has passed through a savage growth period and is moving towards uniform application of standards. Kubernetes’ native language does not fully describe the cloud-native application architecture, and the development and operation functions are heavily coupled in the configuration of resources. Operator’s expansion of the Kubernetes ecosystem has led to the fragmentation of cloud-native applications, and there is an urgent need for a unified application definition standard. The essence of OAM is to separate the R\u0026amp;D and O\u0026amp;M concerns in the definition of cloud-native applications, and to further abstract resource objects, simplify and encompass everything. “Kubernetes Next Generation” refers to the fact that after Kubernetes became the infrastructure layer standard, the focus of the cloud-native ecology is being overtaken by the application layer, and the last two years have been a powerful exploration of the hot Service Mesh process, and the era of cloud-native application architecture based on Kubernetes is coming. Kubernetes has become an established operating platform for cloud-native applications, and this white paper will expand with Kubernetes as the default platform, including an explanation of the OAM-based hierarchical model for cloud-native applications.\n","date":"2020-05-25T22:27:17+08:00","relpermalink":"/en/notice/guide-to-cloud-native-app/","section":"notice","summary":"Take you on a journey through the post-Kubernetes era of cloud-native applications.","title":"Guide to Cloud Native Application"},{"content":"At the beginning of 2020, due to the outbreak of the Crona-19 pandemic, employees around the world began to work at home. Though the distance between people grew farer, there was a group of people, who were us working in the cloud native area, gathered together for a common vision. During the past three months, we have set up the community management committee and used our spare time working together to complete the preparatory work for the community. Today we are here to announce the establishment of the Cloud Native Community.\nBackground Software is eating the world. —— Marc Andreessen\nThis sentence has been quoted countless times, and with the rise of Cloud Native, we’d like to talk about “Cloud Native is eating the software.” As more and more enterprises migrate their services to the cloud, the original development mode of enterprises cannot adapt to the application scenarios in the cloud, and it is being reshaped to conform to the cloud native standard.\nSo what is cloud native? Cloud native is a collection of best practices in architecture, r\u0026amp;d process and team culture to support faster innovation, superior user experience, stable and reliable user service and efficient r\u0026amp;d. The relationship between the open source community and the cloud native is inseparable. It is the existence of the open source community, especially the end user community, that greatly promotes the continuous evolution of cloud native technologies represented by container, service mesh and microservices.\nCNCF (Cloud Native Computing Foundation) holds Cloud Native conference every year in the international community, which has a wide audience and great influence. But it was not held in China for the first time until 2018, after several successful international events. However, there are no independent foundations or neutral open source communities in China. In recent years, many cloud native enthusiasts in China have set up many communication groups and held many meetups, which are very popular. Many excellent open source projects have emerged in the cloud native field, but there is no organized neutral community for overall management. Under this background, the Cloud Native Community emerges at the right moment.\nAbout Cloud Native Community is an open source community with technology, temperature and passion. It was founded spontaneously by a group of industry elites who love open source and uphold the principle of consensus, co-governance, co-construction and sharing. The aim of the community is: connection, neutral, open source. We are based in China, facing the world, enterprise neutrality, focusing on open source, and giving feedback to open source.\nIntroduction for the Steering Community:https://cloudnative.to/en/team/ .\nYou will gain the followings after joining the community:\nKnowledge and news closer to the source A more valuable network More professional and characteristic consultation Opportunities to get closer to opinion leaders Faster and more efficient personal growth More knowledge sharing and exposure opportunities More industry talent to be found Contact Contact with us.\nEmail: mailto:contact@cloudnative.to Twitter: https://twitter.com/CloudNativeTo ","date":"2020-05-12T12:27:17+08:00","relpermalink":"/en/notice/cloud-native-community-announecement/","section":"notice","summary":"Today the Community Steering Committee announced the official formation of the Cloud Native Community.","title":"Establishment of the Cloud Native Community"},{"content":"This article is a rework of previously written content and is included in the Istio Handbook of the ServiceMesher community . Other chapters are still being compiled.\nPeople who have just heard of Service Mesh and tried Istio may have the following questions:\nWhy does Istio bind Kubernetes? What roles do Kubernetes and Service Mesh play in cloud native? What aspects of Kubernetes has Istio extended? What problems have been solved? What is the relationship between Kubernetes, xDS protocols (Envoy , MOSN, etc) and Istio? Should I use Service Mesh? In this section, we will try to guide you through the internal connections between Kubernetes, the xDS protocol, and Istio Service Mesh. In addition, this section will also introduce the load balancing methods in Kubernetes, the significance of the xDS protocol for Service Mesh, and why Istio is needed in time for Kubernetes.\nUsing Service Mesh is not to say that it will break with Kubernetes, but that it will happen naturally. The essence of Kubernetes is to perform application lifecycle management through declarative configuration, while the essence of Service Mesh is to provide traffic and security management and observability between applications. If you have built a stable microservice platform using Kubernetes, how do you set up load balancing and flow control for calls between services?\nThe xDS protocol created by Envoy is supported by many open source software, such as Istio , Linkerd , MOSN, etc. Envoy’s biggest contribution to Service Mesh or cloud native is the definition of xDS. Envoy is essentially a proxy. It is a modern version of proxy that can be configured through APIs. Based on it, many different usage scenarios are derived, such as API Gateway, Service Mesh. Sidecar proxy and Edge proxy in.\nThis section contains the following\nExplain the role of kube-proxy. Kubernetes’ limitations in microservice management. Describe the features of Istio Service Mesh. Describe what xDS includes. Compare some concepts in Kubernetes, Envoy and Istio Service Mesh. Key takeaways If you want to know everything in advance, here are some of the key points from this article:\nThe essence of Kubernetes is application lifecycle management, specifically deployment and management (scaling, scaling, automatic recovery, release). Kubernetes provides a scalable and highly resilient deployment and management platform for microservices. The foundation of Service Mesh is a transparent proxy. After the traffic between microservices is intercepted through sidecar proxy, the behavior of microservices is managed through the control plane configuration. Service Mesh decoupled from Kubernetes traffic management, the internal flow without the need of Service Mesh kube-proxy supporting components, micro-services closer to abstract the application layer by, for traffic between management services, security and observability. xDS defines the protocol standards for Service Mesh configuration. Service Mesh is a higher-level abstraction of services in Kubernetes. Its next step is serverless. Kubernetes vs Service Mesh The following figure shows the service access relationship between Kubernetes and Service Mesh (one sidecar per pod mode).\nFigure: kubernetes vs service mesh Traffic forwarding\nEach node of the cluster Kubernetes a deployed kube-proxy assembly Kubernetes API Server may communicate with the cluster acquired service information, and then set iptables rules, sends a request for a service directly to the corresponding Endpoint (belonging to the same group service pod).\nService discovery\nFigure: Service registration in Service Mesh Istio Service Mesh can use the service in Kubernetes for service registration. It can also connect to other service discovery systems through the platform adapter of the control plane, and then generate the configuration of the data plane (using CRD statements, stored in etcd), a transparent proxy for the data plane. (Transparent proxy) is deployed in the sidecar container in each application service pod. These proxy need to request the control plane to synchronize the proxy configuration. The reason why is a transparent proxy, because there is no application container fully aware agent, the process kube-proxy components like the need to block traffic, but kube-proxythat blocks traffic to Kubernetes node and sidecar proxy that blocks out of the Pod For more information, see Understanding Route Forwarding by the Envoy Sidecar Proxy in Istio Service Mesh .\nDisadvantages of Service Mesh\nBecause each node on Kubernetes many runs Pod, the original kube-proxyrouting forwarding placed in each pod, the distribution will lead to a lot of configuration, synchronization, and eventual consistency problems. In order to perform fine-grained traffic management, a series of new abstractions will be added, which will further increase the user’s learning costs. However, with the popularization of technology, this situation will gradually ease.\nAdvantages of Service Mesh\nkube-proxy The settings are globally effective, and fine-grained control of each service cannot be performed. Service Mesh uses sidecar proxy to extract the control of traffic in Kubernetes from the service layer, which can be further expanded.\nkube-proxy component In Kubernetes cluster, each Node to run a kube-proxy process. kube-proxy Responsible for the Service realization of a VIP (virtual IP) form. In Kubernetes v1.0, the proxy is implemented entirely in userspace. Kubernetes v1.1 adds the iptables proxy mode, but it is not the default operating mode. As of Kubernetes v1.2, the iptables proxy is used by default. In Kubernetes v1.8.0-beta.0, the ipvs proxy mode was added. More about kube-proxy component description please refer to kubernetes Description: service and kube-proxy principle and use IPVS achieve Kubernetes inlet flow load balancing.\nkube-proxy flaws The disadvantages of kube-proxy :\nFirst, if forwarded pod can not provide normal service, it does not automatically try another pod, of course, this can liveness probes be solved. Each pod has a health check mechanism. When there is a problem with the health of the pod, kube-proxy will delete the corresponding forwarding rule. In addition, nodePorttypes of services cannot add TLS or more sophisticated message routing mechanisms.\nKube-proxy implements load balancing of traffic among multiple pod instances of the Kubernetes service, but how to fine-grained control the traffic between these services, such as dividing the traffic into different application versions by percentage (these applications belong to the same service , But on a different deployment), do canary release and blue-green release? Kubernetes community gives the method using the Deployment do canary release , essentially by modifying the pod of the method label different pod to be classified into the Deployment of Service.\nKubernetes Ingress vs. Istio Gateway Speaking above kube-proxythe flow inside the only route Kubernetes clusters, and we know that Pod Kubernetes cluster located CNI outside the network created, external cluster is unable to communicate directly with, so Kubernetes created in the ingress of this resource object, which is located by the Kubernetes edge nodes (such nodes can be many or a group) are driven by the Ingress controller, which is responsible for managing north-south traffic . Ingress must be connected to various ingress controllers, such as nginx ingress controller and traefik . Ingress is only applicable to HTTP traffic, and its usage is also very simple. It can only route traffic by matching limited fields such as service, port, and HTTP path, which makes it unable to route TCP traffic such as MySQL, Redis, and various private RPCs. To directly route north-south traffic, you can only use Service’s LoadBalancer or NodePort. The former requires cloud vendor support, while the latter requires additional port management. Some Ingress controllers support exposing TCP and UDP services, but they can only be exposed using Services. Ingress itself does not support it, such as the nginx ingress controller . The exposed port of the service is configured by creating a ConfigMap.\nIstio Gateway is similar to Kubernetes Ingress in that it is responsible for north-south traffic to the cluster. GatewayThe load balancer described by Istio is used to carry connections in and out of the edge of the mesh. The specification describes a series of open ports and the protocols used by these ports, SNI configuration for load balancing, and so on. Gateway is a CRD extension. It also reuses the capability of sidecar proxy. For detailed configuration, please refer to Istio official website .\nxDS protocol You may have seen the following picture when you understand Service Mesh. Each block represents an instance of a service, such as a Pod in Kubernetes (which contains a sidecar proxy). The xDS protocol controls all traffic in Istio Service Mesh. The specific behavior is to link the squares in the figure below.\nFigure: Service Mesh diagram The xDS protocol was proposed by Envoy . The original xDS protocols in the Envoy v2 API refer to CDS (Cluster Discovery Service), EDS (Endpoint Discovery Service), LDS (Listener Discovery Service), and RDS (Route Discovery Service). Later, in the v3 version, Scoped Route Discovery Service (SRDS), Virtual Host Discovery Service (VHDS), Secret Discovery Service (SDS), and Runtime Discovery Service (RTDS) were developed. See the xDS REST and gRPC protocol for details .\nLet’s take a look at the xDS protocol with a service with two instances each.\nFigure: xDS protocol The arrow in the figure above is not the path or route after the traffic enters the proxy, nor is it the actual sequence. It is an imagined xDS interface processing sequence. In fact, there are cross references between xDS.\nAgents that support the xDS protocol dynamically discover resources by querying files or managing servers. In summary, the corresponding discovery service and its …","date":"2020-04-01T11:56:04+08:00","relpermalink":"/en/blog/service-mesh-the-microservices-in-post-kubernetes-era/","section":"blog","summary":"This article is a rework of previously written content and is included in the Istio Handbook of the ServiceMesher community . Other chapters are still being compiled.","title":"Service Mesh - The Microservices in Post Kubernetes Era"},{"content":"2020 is indeed a bad start. In less than a month, I hardly heard any good news:\nTrump assassinated Major General Sulaymani of Iran; this pneumonia outbreak in Wuhan; the news that my basketball icon Kobe died of a helicopter crash really shocked me, and the Lakers said goodbye on the 24th. This gave me another spiritual blow during the Spring Festival, which was originally lacking in interest.\n2020 is bound to be a year deeply remembered in all humankind. In the last few days of the first month of the year, I decided to revise the website. The first reason was that I could n’t go out during the extended Chinese New Year holiday, and it was boring at home. And many pictures on the website were saved on Weibo map beds. The picture bed is unstable, causing many photos to be irretrievable; coupled with a habit of organizing the website every long holiday (the last revision of the website was completed during the National Day holiday in 2018, completed at home for 7 days), so I decided to The website has been revised again and it has become what it is now.\nFeatures The website has the following features after this revision:\nReorganize the website content, the structure is more reasonable Support email subscription Images are stored on Github Responsive static website, card design, better user experience Everyone is welcome to enter your email address in the email input box at the bottom of the page. Once there is an important content update on this site, we will push you through the email as soon as possible.\n","date":"2020-01-30T12:27:17+08:00","relpermalink":"/en/notice/website-revision-notice/","section":"notice","summary":"In the last days of the first month of 2020, I decided to revamp the website.","title":"jimmysong.io website revision notice"},{"content":"“Future Architecture: From Service-Oriented to Cloud-Native,” authored by Zhang Liang, Wu Sheng, Ao Xiao Jian, and Song Jingchao, published by Publishing House of Electronics Industry in April 2019.\nFigure: Cover of the book Future Architecture The first author of this book is Zhang Liang, currently working at JD Finance. To enrich the content of the book, Zhang Liang invited his friends in the industry, Wu Sheng, Ao Xiao Jian, and myself, to co-author this ambitious work on “Future Architecture.” Below is an excerpt of his introduction to the book.\nOrigin of the Book As professionals in the internet industry, we have always been at the forefront of change, constantly chasing the waves of technological advancement to avoid falling behind the pace of the times. Especially in recent years, internet architecture has undergone continuous evolution, transitioning from centralized architecture to distributed architecture, and then to cloud-native architecture. Cloud-native architecture has gradually become the protagonist of this era because it can address issues such as slow application upgrades, bloated architectures, and inability to iterate quickly.\nIn the midst of this wave of change, I watched as it altered the course of internet architecture and brought new ideas and developments to more and more companies and individuals. I embarked on a journey to learn and understand it, integrating it into my knowledge system to update the architectural map in my mind with the experiences and insights accumulated over the years.\nIn 2017 and 2018, I experienced these changes firsthand, making projects like Elastic-Job and Sharding-Sphere widely recognized in the industry, internationalizing the open-source projects I was responsible for, and meeting more mentors and friends. These experiences inspired me to put what I heard, saw, knew, and felt onto paper, connecting them into this book: “Future Architecture: From Service-Oriented to Cloud-Native.”\nThis book covers everything you want to know about distributed systems, service-oriented architecture, service mesh, containers, orchestration, cloud-native, and cloud databases.\nIt contains my well-considered insights and experiences accumulated over the years, as well as my struggles of abandoning and picking up the pen again because I feel responsible for the content of the book.\nIt also features chapters contributed by seasoned experts in the field, including Wu Sheng, the founder of Apache Incubator project SkyWalking and an APM expert; Song Jingchao, a CNCF Ambassador, cloud-native evangelist, and founder of the Cloud-Native Community; and Ao Xiao Jian, a service mesh evangelist.\nTable of Contents Chapter 1: Cloud-Native Chapter 2: Remote Communication Chapter 3: Configuration Chapter 4: Service Governance Chapter 5: Observing Distributed Services Chapter 6: Intrusive Service Governance Solutions Chapter 7: Kubernetes: The Cornerstone of Cloud-Native Ecosystem Chapter 8: Cross-Language Service Governance Solutions: Service Mesh Chapter 9: Cloud-Native Data Architecture Chapter 10: Distributed Database Middleware Ecosystem: ShardingSphere Wings of Aspiration The back cover of the book features the Holy Trinity statue standing in front of Old Trafford Stadium, chosen by Professor Zhang Liang as the background image.\nOn February 6, 1958, while returning from Yugoslavia after securing a spot in the European Cup semi-finals, the Manchester United team encountered the Munich air disaster, disappearing in the night sky. To revive Manchester United, surviving manager Matt Busby rebuilt the team with blood, tears, and sweat. Exactly 10 years after the Munich air disaster, on May 29, 1968, Busby led his new team to finally lift the European Cup, comforting the souls of those who had passed away! The Holy Trinity statue became an eternal memorial!\nFaith, resilience, perseverance, miracles, and rebirth… These are the strengths I feel from this sculpture. Everyone will experience peaks and valleys, see mountains and deserts in their lifetime, and I hope this book not only brings practical knowledge of internet architecture but also embodies my blessings to you all: I hope that amidst the relentless struggles on this long journey, you can have your own faith and hope, and even after traversing countless deserts and oceans, you can see the light after experiencing numerous trials!\n","date":"2019-04-01T00:00:00Z","relpermalink":"/en/book/future-architecture/","section":"book","summary":"Authors: Zhang Liang, Wu Sheng, Ao Xiao Jian, and Song Jingchao.","title":"Future Architecture: From Service-Oriented to Cloud-Native"},{"content":"The following paragraph is a release note from the Istio official blog https://istio.io/zh/blog/2019/announcing-1.1/ , which I translated.\nIstio was released at 4 a.m. Beijing time today and 1 p.m. Pacific time.\nSince the 1.0 release last July, we have done a lot to help people get Istio into production. We expected to release a lot of patches (six patches have been released so far!), But we are also working hard to add new features to the product.\nThe theme for version 1.1 is “Enterprise Ready”. We are happy to see more and more companies using Istio in production, but as some big companies join in, Istio also encounters some bottlenecks.\nThe main areas we focus on include performance and scalability. As people gradually put Istio into production and use larger clusters to run more services with higher capacity, there may be some scaling and performance issues. Sidecar takes up too much resources and adds too much latency. The control plane (especially Pilot) consumes excessive resources.\nWe put a lot of effort into making the data plane and control plane more efficient. In the 1.1 performance test, we observed that sidecars typically require 0.5 vCPU to process 1000 rps. A single Pilot instance can handle 1000 services (and 2000 pods) and consumes 1.5 vCPUs and 2GB of memory. Sidecar adds 5 milliseconds at the 50th percentile and 10 milliseconds at the 99th percentile (the execution strategy will increase latency).\nWe have also completed the work of namespace isolation. You can use the Kubernetes namespace to enforce control boundaries to ensure that teams do not interfere with each other.\nWe have also improved multi-cluster functionality and usability. We listened to the community and improved the default settings for flow control and policies. We introduced a new component called Galley. Galley validates YAML configuration, reducing the possibility of configuration errors. Galley is also used in multi-cluster setups-collecting service discovery information from each Kubernetes cluster. We also support other multi-cluster topologies, including single control planes and multiple synchronous control planes, without the need for flat network support.\nSee the release notes for more information and details .\nThere is more progress on this project. As we all know, Istio has many moving parts, and they take on too much work. To address this, we have recently established the Usability Working Group (available at any time). A lot happened in the community meeting (Thursday at 11 am) and in the working group. You can log in to discuss.istio.io with GitHub credentials to participate in the discussion!\nThanks to everyone who has contributed to Istio over the past few months-patching 1.0, adding features to 1.1, and extensive testing recently on 1.1. Special thanks to companies and users who work with us to install and upgrade to earlier versions to help us identify issues before they are released.\nFinally, go to the latest documentation and install version 1.1! Happy meshing!\nOfficial website The ServiceMesher community has been maintaining the Chinese page of the official Istio documentation since the 0.6 release of Istio . As of March 19, 2019, there have been 596 PR merges, and more than 310 documents have been maintained. Thank you for your efforts! Some documents may lag slightly behind the English version. The synchronization work is ongoing. For participation, please visit https://github.com/servicemesher/istio-official-translation . Istio official website has a language switch button on the right side of each page. You can always Switch between Chinese and English versions, you can also submit document modifications, report website bugs, etc.\nServiceMesher Community Website\nThe ServiceMesher community website covers all technical articles in the Service Mesh field and releases the latest activities in a timely manner. It is your one-stop portal to learn about Service Mesh and participate in the community.\n","date":"2019-03-21T23:27:49+08:00","relpermalink":"/en/notice/istio-11/","section":"notice","summary":"Istio 1.1 was released at 4 am on March 20th, Beijing time. This version took 8 months! The ServiceMesher community also launched the Istio Chinese documentation.","title":"Istio 1.1 released"},{"content":"Istio handbook was originally an open source e-book I created. It has been written for 8 months before donating to the ServiceMesher community. In order to further popularize Istio and Service Mesh technology, this book Donate to the community for co-authoring. The content of the original book was migrated to https://github.com/servicemesher/istio-handbook on March 10, 2019. The original book will no longer be updated.\nGitHub address: https://github.com/servicemesher/istio-handbook Conceptual picture of this book, cover photo of Shanghai Jing’an Temple at night , photo by Jimmy Song.\nThe publishing copyright of this book belongs to the blog post of Electronic Industry Press. Please do not print and distribute it without authorization.\nIstio is a service mesh framework jointly developed by Google, IBM, Lyft, etc., and began to enter the public vision in early 2017. As an important infrastructure layer that inherits Kubernetes and connects to the serverless architecture in the cloud-native era, Istio is of crucial importance important. The ServiceMesher community as one of the earliest open source communities in China that is researching and promoting Service Mesh technology, decided to integrate community resources and co-author an open source e-book for readers.\nAbout this book This book originates from the rootsongjc / istio-handbook and the Istio knowledge map created by the ServiceMesher community .\nThis book is based on Istio 1.0+ and includes, but is not limited to , topics in the Istio Knowledge Graph .\nParticipate in the book Please refer to the writing guidelines of this book and join the Slack channel discussion after joining the ServiceMesher community.\n","date":"2019-03-10T16:55:11+08:00","relpermalink":"/en/notice/istio-handbook-by-servicemesher/","section":"notice","summary":"To further popularize Istio and Service Mesh technology, donate this book to the community for co-authoring.","title":"Donate Istio Handbook to the ServiceMesher community"},{"content":"Github: https://github.com/rootsongjc/cloud-native-sandbox Cloud Native Sandbox can help you setup a standalone Kubernetes and istio environment with Docker on you own laptop.\nThe sandbox integrated with the following components:\nKubernetes v1.10.3 Istio v1.0.4 Kubernetes dashboard v1.8.3 Differences with kubernetes-vagrant-centos-cluster As I have created the kubernetes-vagrant-centos-cluster to set up a Kubernetes cluster and istio service mesh with vagrantfile which consists of 1 master(also as node) and 3 nodes, but there is a big problem that it is so high weight and consume resources. So I made this light weight sandbox.\nFeatures\nNo VirtualBox or Vagrantfile required Light weight High speed, low drag Easy to operate Services\nAs the sandbox setup, you will get the following services.\nRecord with termtosvg .\nPrerequisite You only need a laptop with Docker Desktop installed and Kubernetes enabled .\nNote: Leave enough resources for Docker Desktop. At least 2 CPU, 4G memory.\nInstall To start the sandbox, you have to run the following steps.\nKubernetes dashboard(Optional) Install Kubernetes dashboard.\nkubectl apply -f install/dashbaord/ Get the dashboard token.\nkubectl -n kube-system describe secret default| awk \u0026#39;$1==\u0026#34;token:\u0026#34;{print $2}\u0026#39; Expose kubernetes-dashboard service.\nkubectl -n kube-system get pod -l k8s-app=kubernetes-dashboard -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39; Login to Kubernetes dashboard on http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login with the above token.\nIstio(Required) Install istio service mesh with the default add-ons.\n# Install istio kubectl apply -f install/istio/ To expose service grafana on http://localhost:3000 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 3000:3000 \u0026amp; To expose service prometheus on http://localhost:9090 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=prometheus -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 9090:9090 \u0026amp; To expose service jaeger on http://localhost:16686 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=jaeger -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 16686:16686 \u0026amp; To expose service servicegraph on http://localhost:8088/dotviz , http://localhost:8088/force/forcegraph.html .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=servicegraph -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 8088:8088 \u0026amp; Kiali Install kiali .\nkubectl -n istio-system apply -f install/kiali To expose service kiali on http://localhost:20001 .\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=kiali -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 20001:20001 \u0026amp; Bookinfo sample Deploy bookinfo sample .\n# Enable sidecar auto injection kubectl label namespace default istio-injection=enabled # Deploy bookinfo sample kubectl -n default apply -f sample/bookinfo Visit productpage on http://localhost/productpage .\nLet’s generate some loads.\nfor ((i=0;i\u0026lt;1000;i=i+1));do echo \u0026#34;Step-\u0026gt;$i\u0026#34;;curl http://localhost/productpage;done You can watch the service status through http://localhost:3000 .\nClient tools To operate the applications on Kubernetes, you should install the following tools.\nRequired\nkubectl - Deploy and manage applications on Kubernetes. istioctl - Istio configuration command line utility. Optional\nkubectx - Switch faster between clusters and namespaces in kubectl kube-ps1 - Kubernetes prompt info for bash and zsh ","date":"2019-01-18T19:06:14+08:00","relpermalink":"/en/blog/cloud-native-sandbox/","section":"blog","summary":"A standalone Kubernetes and Istio environment with Docker on you own laptop","title":"Cloud Native Sandbox"},{"content":"This video was recorded on the last day of 2018 and demonstrates the use of rootsongjc/kubernetes-vagrant-centos-cluster to automatically deploy a Kubernetes cluster and Istio Service Mesh.\nA few days ago I mentioned that kubernetes-vagrant-centos-cluster released v1.2.0 version to deploy a cloud-native experimental environment with one click. Someone in the Kubernetes and Service Mesh community asked me a long time ago to make a video to explain and demonstrate how to install Kubernetes and Istio Service Mesh, because I’m always busy, I’ve always made some time. Today, I will give a demo video, just don’t watch the video for a few minutes. In order to make this video, it took me half an hour to record, two hours to edit, and many years of shooting. , Editing, containers, virtual machines, Kubernetes, service grid experience. This is not so much a farewell as a new beginning.\nBecause the video was first posted on YouTube , it was explained in English (just a few supplementary instructions, it does n’t matter if you do n’t understand, just read the Chinese documentation on GitHub ).\nSkip to bilibli to watch . If you are interested in drone aerial photography, you can also take a look at Jimmy Song’s aerial photography works . Please support the coin or like, thank you.\nIf you have any questions, you can send a barrage or comment below the video.\nPS. Some people will ask why you chose to use bilibli, because there are no ads for watching videos on this platform, and most of them are uploaded by the Up master. Although the two-dimensional elements are mostly, the community atmosphere is still good.\nFor more exciting videos, visit Jimmy Song’s bibli homepage .\n","date":"2019-01-02T20:57:22+08:00","relpermalink":"/en/notice/cloud-native-kubernetes-service-mesh-local-demo-show/","section":"notice","summary":"This video was recorded by me on the last day of 2018 and demonstrates the use of rootsongjc/kubernetes-vagrant-centos-cluster to automatically deploy Kubernetes clusters and Istio Service Mesh.","title":"Kubernetes and Istio Service Mesh Cloud Native Local Video Demo Show"},{"content":"This article uses Istio’s official bookinfo sample to explain how Envoy performs routing forwarding after the traffic entering the Pod and forwarded to Envoy sidecar by iptables, detailing the inbound and outbound processing. For a detailed analysis of traffic interception, see Understanding Envoy Sidecar Proxy Injection and Traffic Interception in Istio Service Mesh .\nOverview of Sidecar Injection and Traffic Interception Steps Below is an overview of the steps from Sidecar injection, Pod startup to Sidecar proxy interception traffic and Envoy processing routing.\nKubernetes automatically injected through Admission Controller, or the user run istioctl command to manually inject sidecar container. Apply the YAML configuration deployment application. At this time, the service creation configuration file received by the Kubernetes API server already includes the Init container and the sidecar proxy. Before the sidecar proxy container and application container are started, the Init container started firstly. The Init container is used to set iptables (the default traffic interception method in Istio, and can also use BPF, IPVS, etc.) to Intercept traffic entering the pod to Envoy sidecar Proxy. All TCP traffic (Envoy currently only supports TCP traffic) will be Intercepted by sidecar, and traffic from other protocols will be requested as originally. Launch the Envoy sidecar proxy and application container in the Pod. Sidecar proxy and application container startup order issues\nStart the sidecar proxy and the application container. Which container is started first? Normally, Envoy Sidecar and the application container are all started up before receiving traffic requests. But we can’t predict which container will start first, so does the container startup order have an impact on Envoy intercepting traffic? The answer is yes, but it is divided into the following two situations.\nCase 1: The application container starts first, and the sidecar proxy is still not ready\nIn this case, the traffic is transferred to the 15001 port by iptables, and the port is not monitored in the Pod. The TCP link cannot be established and the request fails.\nCase 2: Sidecar starts first, the request arrives and the application is still not ready\nIn this case, the request will certainly fail. As for the step at which the failure begins, the reader is left to think.\nQuestion : If adding a readiness and living probe for the sidecar proxy and application container can solve the problem?\nTCP requests that are sent or received from the Pod will be intercepted by iptables. After the inbound traffic is intercepted, it is processed by the Inbound Handler and then forwarded to the application container for processing. The outbound traffic is intercepted by iptables and then forwarded to the Outbound Handler for processing. Upstream and Endpoint. Sidecar proxy requests Pilot to use the xDS protocol to synchronize Envoy configurations, including LDS, EDS, CDS, etc., but to ensure the order of updates, Envoy will use ADS to request configuration updates from Pilot directly. How Envoy handles route forwarding The following figure shows a productpageservice access request http://reviews.default.svc.cluster.local:9080/, when traffic enters reviews the internal services, reviews internal services Envoy Sidecar is how to do traffic blocked the route forward.\nFigure: Istio transparent traffic intercepting and traffic routing schematic Before the first step, productpage Envoy Sidecar Pod has been selected by EDS of a request to reviews a Pod service of its IP address, it sends a TCP connection request.\nThe Envoy configuration in the official website of Istio is to describe the process of Envoy doing traffic forwarding. The party considering the traffic of the downstream is to receive the request sent by the downstream. You need to request additional services, such as reviews service requests need Pod ratings service.\nreviews, there are three versions of the service, there is one instance of each version, three versions sidecar similar working steps, only to later reviews-v1-cb8655c75-b97zc Sidecar flow Pod forwarding this step will be described.\nUnderstanding the Inbound Handler The role of the inbound handler is to transfer the traffic from the downstream intercepted by iptables to localhost to establish a connection with the application container inside the Pod.\nLook reviews-v1-cb8655c75-b97zc at the Listener in the pod.\nRun istioctl pc listener reviews-v1-cb8655c75-b97zc to see what the Pod has a Listener.\nADDRESS PORT TYPE 172.33.3.3 9080 HTTP \u0026lt;--- Receives all inbound traffic on 9080 from listener 0.0.0.0_15006 10.254.0.1 443 TCP \u0026lt;--+ 10.254.4.253 80 TCP | 10.254.4.253 8080 TCP | 10.254.109.182 443 TCP | 10.254.22.50 15011 TCP | 10.254.22.50 853 TCP | 10.254.79.114 443 TCP | 10.254.143.179 15011 TCP | 10.254.0.2 53 TCP | Receives outbound non-HTTP traffic for relevant IP:PORT pair from listener 0.0.0.0_15001 10.254.22.50 443 TCP | 10.254.16.64 42422 TCP | 10.254.127.202 16686 TCP | 10.254.22.50 31400 TCP | 10.254.22.50 8060 TCP | 10.254.169.13 14267 TCP | 10.254.169.13 14268 TCP | 10.254.32.134 8443 TCP | 10.254.118.196 443 TCP \u0026lt;--+ 0.0.0.0 15004 HTTP \u0026lt;--+ 0.0.0.0 8080 HTTP | 0.0.0.0 15010 HTTP | 0.0.0.0 8088 HTTP | 0.0.0.0 15031 HTTP | 0.0.0.0 9090 HTTP | 0.0.0.0 9411 HTTP | Receives outbound HTTP traffic for relevant port from listener 0.0.0.0_15001 0.0.0.0 80 HTTP | 0.0.0.0 15030 HTTP | 0.0.0.0 9080 HTTP | 0.0.0.0 9093 HTTP | 0.0.0.0 3000 HTTP | 0.0.0.0 8060 HTTP | 0.0.0.0 9091 HTTP \u0026lt;--+ 0.0.0.0 15006 TCP \u0026lt;--- Receives all inbound and outbound traffic to the pod from IP tables and hands over to virtual listener As from productpage traffic arriving reviews Pods, downstream must clearly know the IP address of the Pod which is 172.33.3.3, so the request is 172.33.3.3:9080.\nVirtual Listener\nAs you can see from the Pod’s Listener list, the 0.0.0.0:15001/TCP Listener (the actual name is virtual) listens for all inbound traffic, and the following is the detailed configuration of the Listener.\n{ \u0026#34;name\u0026#34;: \u0026#34;virtual\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;socketAddress\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;portValue\u0026#34;: 15006 } }, \u0026#34;filterChains\u0026#34;: [ { \u0026#34;filters\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;envoy.tcp_proxy\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;cluster\u0026#34;: \u0026#34;BlackHoleCluster\u0026#34;, \u0026#34;stat_prefix\u0026#34;: \u0026#34;BlackHoleCluster\u0026#34; } } ] } ], \u0026#34;useOriginalDst\u0026#34;: true } UseOriginalDst : As can be seen from the configuration in useOriginalDstthe configuration as specified true, which is a Boolean value, the default is false, using iptables redirect connections, the proxy may receive port original destination address is not the same port, thus received at the proxy port It is 15001 and the original destination port is 9080. When this flag is set to true, the Listener redirects the connection to the Listener associated with the original destination address, here 172.33.3.3:9080. Listener If no relationship to the original destination address, the connection processing by the Listener to receive it, i.e. the virtualListener, after envoy.tcp_proxyforwarded to a filter process BlackHoleCluster, as the name implies, when no matching Envoy virtual listener when the effect of Cluster , will send the request to it and return 404. This will be referred to below Listener provided bindToPort echoes.\nNote : This parameter will be discarded, please use the Listener filter of the original destination address instead. The main purpose of this parameter is: Envoy listens to the 15201 port to intercept the traffic intercepted by iptables via other Listeners instead of directly forwarding it. See the Virtual Listener for details .\nListener 172.33.3.3_9080\nAs mentioned above, the traffic entering the inbound handler is virtual transferred to the 172.33.3.3_9080 Listener by the Listener. We are looking at the Listener configuration.\nRun istioctl pc listener reviews-v1-cb8655c75-b97zc --address 172.33.3.3 --port 9080 -o json view.\n[{ \u0026#34;name\u0026#34;: \u0026#34;172.33.3.3_9080\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;socketAddress\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;172.33.3.3\u0026#34;, \u0026#34;portValue\u0026#34;: 9080 } }, \u0026#34;filterChains\u0026#34;: [ { \u0026#34;filterChainMatch\u0026#34;: { \u0026#34;transportProtocol\u0026#34;: \u0026#34;raw_buffer\u0026#34; }, \u0026#34;filters\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;envoy.http_connection_manager\u0026#34;, \u0026#34;config\u0026#34;: { ... \u0026#34;route_config\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;inbound|9080||reviews.default.svc.cluster.local\u0026#34;, \u0026#34;validate_clusters\u0026#34;: false, \u0026#34;virtual_hosts\u0026#34;: [ { \u0026#34;domains\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;name\u0026#34;: \u0026#34;inbound|http|9080\u0026#34;, \u0026#34;routes\u0026#34;: [ { ... \u0026#34;route\u0026#34;: { \u0026#34;cluster\u0026#34;: \u0026#34;inbound|9080||reviews.default.svc.cluster.local\u0026#34;, \u0026#34;max_grpc_timeout\u0026#34;: \u0026#34;0.000s\u0026#34;, \u0026#34;timeout\u0026#34;: \u0026#34;0.000s\u0026#34; } } ] } ] }, \u0026#34;use_remote_address\u0026#34;: false, ... } } ]， \u0026#34;deprecatedV1\u0026#34;: { \u0026#34;bindToPort\u0026#34;: false } ... }, { \u0026#34;filterChainMatch\u0026#34;: { \u0026#34;transportProtocol\u0026#34;: \u0026#34;tls\u0026#34; }, \u0026#34;tlsContext\u0026#34;: {... }, \u0026#34;filters\u0026#34;: [... ] } ], ... }] bindToPort : Note that there are a bindToPort configuration that is false, the default value of the configuration true, showing Listener bind to the port, set here to false the process flow can Listener Listener transferred from the other, i.e., above said virtual Listener, where we see filterChains.filters in the envoy.http_connection_manager configuration section:\n\u0026#34;route_config\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;inbound|9080||reviews.default.svc.cluster.local\u0026#34;, \u0026#34;validate_clusters\u0026#34;: false, \u0026#34;virtual_hosts\u0026#34;: [ { \u0026#34;domains\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;name\u0026#34;: \u0026#34;inbound|http|9080\u0026#34;, \u0026#34;routes\u0026#34;: [ { ... \u0026#34;route\u0026#34;: { \u0026#34;cluster\u0026#34;: \u0026#34;inbound|9080||reviews.default.svc.cluster.local\u0026#34;, \u0026#34;max_grpc_timeout\u0026#34;: \u0026#34;0.000s\u0026#34;, \u0026#34;timeout\u0026#34;: \u0026#34;0.000s\u0026#34; } } ] } ] } This configuration indicates that traffic will be handed off to the Cluster for inbound|9080||reviews.default.svc.cluster.local processing.\nCluster inbound|9080||reviews.default.svc.cluster.local\nRun istioctl pc cluster reviews-v1-cb8655c75-b97zc --fqdn reviews.default.svc.cluster.local --direction inbound -o json to see the Cluster configuration is as follows.\n[ { \u0026#34;name\u0026#34;: \u0026#34;inbound|9080||reviews.default.svc.cluster.local\u0026#34;, \u0026#34;connectTimeout\u0026#34;: \u0026#34;1.000s\u0026#34;, \u0026#34;hosts\u0026#34;: [ { \u0026#34;socketAddress\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;portValue\u0026#34;: 9080 } } ], \u0026#34;circuitBreakers\u0026#34;: { \u0026#34;thresholds\u0026#34;: [ {} ] } } ] You can see that the Endpoint of the …","date":"2018-12-27T10:01:22+08:00","relpermalink":"/en/blog/understanding-how-envoy-sidecar-intercept-and-route-traffic-in-istio-service-mesh/","section":"blog","summary":"Details about Envoy sidecar with iptables rules.","title":"Understanding How Envoy Sidecar Intercept and Route Traffic in Istio Service Mesh"},{"content":"KubeCon \u0026amp; CloudNative in North America is the most worthy cloud-native event every year. This time it will be held in Seattle for four days, from December 10th to 13th, refer to the official website of the conference . This year, 8,000 people participated. You should notice that Kubernetes has become more and more low-level. Cloud-native application developers do not need to pay much attention to it. Major companies are publishing their own cloud-native technology stack layouts, including IBM, VMware, SAP, Startups around this ecosystem are still emerging. The PPT sharing address of the local conference: https://github.com/warmchang/KubeCon-North-America-2018 , thank you William Zhang for finishing and sharing the slides of this conference .\nSeattle scene Janet Kuo from Google describes the path to cloud-native technology adoption.\nThe same event of KubeCon \u0026amp; CloudNativeCon, the scene of the first EnvoyCon.\nAt KubeCon \u0026amp; CloudNativeCon Seattle, various directors, directors, directors, VPs, and Gartner analysts from IBM, Google, Mastercard, VMware, and Gartner are conducting live discussions on the topic of Scaling with Service Mesh and Istio. Why do we talk about Istio when we talk about Service Mesh? What is not suitable for Istio use case. . .\nThe PPT contains basic introduction, getting started, a total of more than 200 Deep Dive as well as practical application, we recommend you according to the General Assembly’s official website to choose topics of interest to look at the schedule, otherwise I might see, however.\nA little impression KubeCon \u0026amp; CloudNativeCon is held three times a year, Europe, China and North America. China is the first time this year, and it will be held in Shanghai in November. It is said that it will be held in June next year. Although everyone said that Kubernetes has become boring, the conference about Kubernetes There is still a lot of content, and the use of CRD to extend Kubernetes usage is increasing. Service Mesh has begun to become hot. As can be seen from the live pictures above, there are a large number of participants on the site and related topics are also increasing. It is known as a microservice in the post-Kubernetes era . This must be It will be an important development direction of cloud native after Kubernetes, and the ServiceMesher community pays close attention to it.\n","date":"2018-12-16T15:47:05+08:00","relpermalink":"/en/notice/kubecon-cloudnativecon-seattle-2018/","section":"notice","summary":"KubeCon \u0026 CloudNativeCon Seattle 2018 Data Sharing.","title":"Kubecon\u0026CloudNativeCon Seattle 2018"},{"content":"This is a postscript from the post- Kubernetes era. Just this evening I saw a post by Bilgin Ibryam Microservices in a Post-Kuberentes Era .\nOn April 9, 2017, the Kubernetes Handbook-Kubernetes Chinese Guide / Cloud Native Application Architecture Practice Manual was first submitted. In the past 16 months, 53 contributors participated, 1,088 commits, and a total of 23,9014 Chinese characters were written. At the same time , thousands of enthusiasts have gathered in the Kubernetes \u0026amp; Cloud Native combat group .\nIt has been more than 4 months since the previous version was released. During this period, Kubernetes and Prometheus graduated from CNCF respectively and have matured commercially. These two projects have basically taken shape and will not change much in the future. Kubernetes was originally developed for container orchestration. In order to solve the problem of microservice deployment, Kubernetes has gained popularity. The current microservices have gradually entered the post-Kubernetes era . Service Mesh and cloud native redefine microservices and distributed applications.\nWhen this version was released, the PDF size was 108M with a total of 239,014 Chinese characters. It is recommended to browse online , or clone the project and install the Gitbook command to compile it yourself.\nThis version has the following improvements:\nAdded Istio Service Mesh tutorial Increased use VirtualBox and Vagrant set up in a local cluster and distributed Kubernetes Istio Service Mesh Added cloud native programming language Ballerina and Pulumi introduced Added Quick Start Guide Added support for Kubernetes 1.11 Added enterprise-level service mesh adoption path guide Added SOFAMesh chapter Added vision for the cloud-native future Added CNCF charter and participation Added notes for Docker image repositories Added Envoy chapter Increased KCSP (Kubernetes certification service providers) and CKA (Certified Kubernetes administrator) instructions Updated some configuration files, YAML and reference links Updated CRI chapter Removed obsolete description Improved etcdctl command usage tutorial Fixed some typos Browse and download Browse online To make it easy for everyone to download, I put a copy on Weiyun , which is available in PDF (108MB), MOBI (42MB), and EPUB (53MB). In this book, there are more practical tutorials. In order to better understand the principles of Kubernetes, I recommend studying In-depth analysis of Kubernetes by Zhang Lei, produced by GeekTime.\nThank you Kubernetes for your support of this book. Thank you Contributors . In the months before this version was released, the ServiceMesher community was co-founded . As a force in the post-Kubernetes era , welcome to contact me to join the community and create cloud native New era .\nAt present , the WeChat group of the ServiceMesher community also has thousands of members. The Kubernete Handbook will continue, but Service Mesh is already a rising star. With Kubernetes in mind, welcome to join the ServiceMesher community and follow us. The public account of the community (also the one I manage).\n","date":"2018-09-04T10:23:23+08:00","relpermalink":"/en/notice/new-kubernetes-handbook-released-and-say-hello-to-post-kubernetes-era/","section":"notice","summary":"This is an obituary post-Kubernetes era. Kubernetes handbook by Jimmy Song v1.4 is released. The next focus of cloud native is Service Mesh!","title":"Kubernetes Handbook v1.4 is released"},{"content":"Today I am honored to announce that I have become a CNCF Ambassador . Here is my story with Cloud Native.\nOrigin The first time to attend the Cloud Native Computing Foundation is at the LC3 in Beijing 2017. I attended the meeting again this year, and in November of this year, CNCF will hold the KubeCon \u0026amp; CloudNativeCon for the first time in Shanghai, China. I’ll be there too.\nCloud Native Books My origins with the Cloud Native is originated from Kevin Hoffman’s book Cloud Native Go . I translated this book at the end of 2016. Since then, in China, the translation of the word Cloud Native has not been determined, we introduced it with 云原生 to China.\nAnd then I begin to write the kubernetes-handbook on GitHub. So far, it has more than 2000 stars. This book has written more than 200,000 Chinese characters, the first commit happened on April 14, 2017.\nSince the the book Cloud Native Go completed, the publisher recommended another Cloud Native book to me - Cloud Native Python by Manish Sethi.\nAnd the book Cloud Native Java by Josh Long and Kenny Bastani.\nIn March 2018, with the hope that Bring the world equal opportunities and Building a Financial Cloud Native Infrastructure, I joined the Ant Group .\nServiceMesher Community By the time of May 2018, I start to organize the ServiceMesher community.\nIn the last few months, we work with other open source communities in China, such as k8smeetup , Sharding-Sphere , Apache SkyWalking . Our community has grown to have 1,700 members and two round meetups in Hangzhou and Beijing till now.\nMore than 300 people participated in the scene and more than 20,000 people watched it live by IT 大咖说 。\nFuture Here are some hopes of mine:\nOpen source culture become popular in China More and more people would like to be involved in open source projects Host one open source project into the CNCF A book related to Cloud Native or Service Mesh Strengthen cultural exchanges between China and the global Finally, welcome to China for traveling or share your topic with us on Cloud Native, and in the mean while we will share our experience on large scale web apps to the world. Hope to hear your voice!\n","date":"2018-08-11T11:37:36+08:00","relpermalink":"/en/blog/cloud-native-and-me-the-past-current-and-future/","section":"blog","summary":"Today I am honored to announce that I have become a CNCF Ambassador.","title":"Cloud Native With Me - The Past, Current and Future"},{"content":"Today, we are pleased to announce Istio 1.0 . It’s been over a year since the original 0.1 release. Since 0.1, Istio has grown rapidly with the help of a thriving community, contributors, and users. Many companies have successfully applied Istio to production today and have gained real value through the insight and control provided by Istio. We help large businesses and fast-growing startups such as eBay , Auto Trader UK , Descartes Labs , HP FitStation , Namely , PubNub and Trulia to connect, manage and protect their services from scratch with Istio. The release of this version as 1.0 recognizes that we have built a core set of features that users can rely on for their production.\nEcosystem Last year we saw a significant increase in the Istio ecosystem. Envoy continues its impressive growth and adds many features that are critical to a production-level service grid. Observability providers like Datadog , SolarWinds , Sysdig , Google Stackdriver, and Amazon CloudWatch have also written plugins to integrate Istio with their products. Tigera , Aporeto , Cilium and Styra have built extensions for our strategy implementation and network capabilities. Kiali built by Red Hat provides a good user experience for grid management and observability. Cloud Foundry is building the next-generation traffic routing stack for Istio, the recently announced Knative serverless project is doing the same, and Apigee has announced plans to use it in their API management solution. These are just a few of the projects that the community added last year.\nFeatures Since the 0.8 release, we have added some important new features, and more importantly, marked many existing features as Beta to indicate that they can be used in production. This is covered in more detail in the release notes , but it is worth mentioning:\nMultiple Kubernetes clusters can now be added to a single grid , enabling cross-cluster communication and consistent policy enforcement. Multi-cluster support is now Beta. The network API for fine-grained control of traffic through the grid is now Beta. Explicitly modeling ingress and egress issues with gateways allows operations personnel to control the network topology and meet access security requirements at the edge. Two-way TLS can now be launched incrementally without updating all clients of the service. This is a key feature that removes the barriers to deploying Istio on existing production. Mixer now supports developing out-of-process adapters . This will be the default way to extend Mixer in an upcoming release, which will make it easier to build adapters. Envoy now fully evaluates the authorization policies that control service access locally , improving their performance and reliability. Helm chart installation is now the recommended installation method, with a wealth of customization options to configure Istio to your needs. We put a lot of effort into performance, including continuous regression testing, large-scale environmental simulation, and target repair. We are very happy with the results and will share details in the coming weeks. Next step Although this is an important milestone for the project, much work remains to be done. When working with adopters, we’ve received a lot of important feedback about what to focus next. We’ve heard consistent topics about supporting hybrid clouds, installing modularity, richer network capabilities, and scalability for large-scale deployments. We have considered some feedback in the 1.0 release and we will continue to actively work on it in the coming months.\nQuick start If you are new to Istio and want to use it for deployment, we would love to hear from you. Check out our documentation , visit our chat forum or visit the mailing list . If you want to contribute more to the project, please join our community meeting and say hello.\nAt last The Istio team is grateful to everyone who contributed to the project. Without your help, it won’t have what it is today. Last year’s achievements were amazing, and we look forward to achieving even greater achievements with our community members in the future.\nThe ServiceMesher community is responsible for the translation and maintenance of Chinese content on Istio’s official website. At present, the Chinese content is not yet synchronized with the English content. You need to manually enter the URL to switch to Chinese ( https://istio.io/zh ). There is still a lot of work to do , Welcome everyone to join and participate.\n","date":"2018-08-01T14:42:36+08:00","relpermalink":"/en/notice/istio-v1-released/","section":"notice","summary":"Chinese documentation is released at the same time!","title":"Istio 1.0 is released"},{"content":"“Cloud Native Java,” authored by Josh Long, translated by Zhang Ruofei and Song Jingchao, published by Publishing House of Electronics Industry in July 2017.\nFigure: Cover of the book Cloud Native Java Figure: Jimmy Song with Josh Long Photo taken on November 3, 2018, in Beijing\nBook Introduction What distinguishes traditional enterprises from companies like Amazon, Netflix, and Etsy? These companies have perfected cloud-native development methods that enable them to stay ahead and lead the competition. This practical guide shows Java/JVM developers how to build software faster and better using Spring Boot, Spring Cloud, and Cloud Foundry.\nMany organizations have already ventured into cloud computing, test-driven development, microservices, and continuous integration and delivery. Authors Josh Long and Kenny Bastani will take you deep into these tools and practices, helping you transform traditional applications into true cloud-native ones.\nThe book consists of four major parts:\nFoundation: Understand the motivation behind cloud-native thinking; configure and test Spring Boot applications; migrate your traditional applications to the cloud Microservices: Build HTTP and RESTful services using Spring; route requests in distributed systems; establish edge services closer to data Data Integration: Manage data using Spring Data and integrate distributed services with Spring-supported event-driven, message-centric architectures Production: Make your system observable; use service proxies to connect stateful services; understand the important ideas behind continuous delivery If you’re building cloud-native applications, this book will serve as your essential guide to using the Java ecosystem. It covers everything—building resilient services, managing data flows (via REST and asynchronous events), testing, deployment, and observability.\n—Daniel Bryant, Software Developer and CTO at SpectoLabs\nWhether you’re just starting out on your cloud-native journey or nearing your cloud-native goals, everyone involved will benefit from the insights and experiences shared in this Cloud Native Java book.\n—Dr. Dave Syer, Contributor to the Spring Framework, and Co-founder of Spring Boot and Spring Cloud\nAbout the Authors Josh Long is a Spring Developer Advocate, an editor at InfoQ.com’s Java queue, and the primary author of several books, including the second edition of Spring Recipes (published by Apress). Josh has spoken at many international industry conferences, including TheServiceSide Java Symposium, SpringOne, OSCON, JavaZone, and Devoxx, among others. When he’s not writing code for SpringSource, he’s either at a Java User Group meeting or in a coffee shop. Josh loves solutions that push technology forward. His interests include scalability, BPM, grid computing, mobile computing, and so-called “smart” systems. You can browse his blogs at http://blog.springsource.org or http://joshlong.com .\nKenny Bastani is a Spring Developer Advocate at Pivotal. As an open-source contributor and blogger, Kenny focuses on graph databases, microservices, and attracting a passionate group of software developers. Kenny is also a regular attendee at industry conferences such as OSCON, SpringOne Platform, and GOTO. He maintains a personal blog about software architecture and provides tutorials and open-source reference examples for building event-driven microservices and serverless architectures.\nTable of Contents Foreword (James Watters) xvii\nForeword (Rod Johnson) xix\nPreface xxi\nPart I Foundations\nChapter 1 Cloud-Native Applications 3 Chapter 2 Bootcamp: Spring Boot and Cloud Foundry 21 Chapter 3 Configuration, as Twelve-Factor Style 67 Chapter 4 Testing 85 Chapter 5 Migrating Legacy Applications 115 Part II Web Services\nChapter 6 REST API 137 Chapter 7 Routing 179 Chapter 8 Edge Services 197 Part III Data Integration\nChapter 9 Data Management 251 Chapter 10 Messaging Systems 303 Chapter 11 Batch Processing and Tasks 325 Chapter 12 Data Integration 363 Part IV Production\nChapter 13 Observable Systems 411 Chapter 14 Service Proxies 469 Chapter 15 Continuous Delivery 497 Part V Appendix\nAppendix A Using Spring Boot in Java EE 527\n","date":"2018-07-01T00:00:00Z","relpermalink":"/en/book/cloud-native-java/","section":"book","summary":"Authored by Josh Long.","title":"Cloud Native Java"},{"content":"“Cloud Native Python,” authored by Marish Sethi, translated by Song Jingchao, published by Publishing House of Electronics Industry in July 2018.\nFigure: Cover of Cloud Native Python Introduction to Cloud Native Python With today’s rapid business development, relying solely on their own infrastructure is far from enough for enterprises to support their rapid expansion. Therefore, they have been pursuing the elasticity of the cloud to build platforms that support highly scalable applications.\nThis book can help you understand all the information about building cloud-native architectures with Python in one go. In this book, we first introduce you to cloud-native architectures and how they can help you solve various problems. Then you will learn how to build microservices using REST APIs and Python, construct the web layer in an event-driven manner. Next, you will learn how to interact with data services and build web views using React. After that, we will delve into application security and performance. Then, you will also learn how to containerize your services with Docker. Finally, you will learn how to deploy your applications on AWS and Azure platforms. After deploying your application, we will conclude this book with a series of concepts and techniques around application troubleshooting.\nWhat’s Covered in This Book Chapter 1: Introduction to cloud-native architectures and microservices, discussing the basic concepts of cloud-native architecture and setting up the application development environment. Chapter 2: Building microservices with Python, building your own microservices knowledge base and extending it based on your use cases. Chapter 3: Building web applications with Python, building an initial web application and integrating it with microservices. Chapter 4: Interacting with data services, teaching you how to migrate applications to different database services. Chapter 5: Building web views using React. Chapter 6: Creating scalable UI with Flux, helping you understand how to create scalable applications using Flux. Chapter 7: Event sourcing and CQRS, discussing how to store transactions in event form. Chapter 8: Securing web applications, keeping your applications safe from external threats. Chapter 9: Continuous delivery, knowledge related to frequent releases of applications. Chapter 10: Dockerizing your services, discussing container services and running applications in Docker. Chapter 11: Deploying applications to AWS platform, teaching you how to build infrastructure on AWS and establish the production environment of applications. Chapter 12: Deploying applications to Azure platform, discussing how to build infrastructure on Azure and establish the production environment of applications. Chapter 13: Monitoring cloud applications, understanding different monitoring tools for infrastructure and applications. Tools and Environment Required to Use This Book You need to have Python installed on your system. A text editor, preferably Vim, Sublime, or Notepad++. In one chapter, you will need to download POSTMAN, a powerful API testing suite that can be installed as a Chrome extension. You can download it here .\nAdditionally, it would be better if you have accounts on the following websites:\nJenkins Docker Amazon Web Services Terraform Target Audience This book is suitable for developers with basic Python knowledge, familiarity with the command line, and the basic principles of HTTP-based applications. It’s an ideal choice for those who want to understand how to build, test, and scale Python-based applications. No prior experience with building microservices using Python is required.\n","date":"2018-07-01T00:00:00Z","relpermalink":"/en/book/cloud-native-python/","section":"book","summary":"Authored by Marish Sethi.","title":"Cloud Native Python"},{"content":" If there is a visual learning model and platform that provides infrastructure clusters for your operation, would you pay for it?\nTwo months ago, I met Jord in Kubernetes’s Slack channel, and later saw the link of MagicSandbox.io he (and possibly others) sent in the Facebook group of Taiwan Kubernetes User Group, and I clicked to apply for a trial Then, I received an email from Jord later, and he told me that he wanted to build a Kubernetes learning platform. That’s where the whole thing started, and then we had a couple of Zoom video chats for a long time.\nAbout MagicSandbox MagicSandbox is a startup company. Jord (Dutch) is also a serial entrepreneur. He has studied at Sichuan University in China for 4 years since he was 19, and then returned to Germany. He worked as a PM at Boston Consulting Group and now works at Entrepreneur. First (Europe’s top venture capital / enterprise incubator) is also located in Berlin, Germany. He met Mislav (Croatian). Mislav is a full-stack engineer and has several entrepreneurial experiences. They have similar odors, and they hit it off. Decided to be committed to the Internet education industry and create a world-class software engineer education platform. They want to start with Kubernetes, first provide Internet-based Kubernetes theory and practice teaching, and then expand the topic to ElasticSearch, GraphQL, and so on. topic.\nJord founded MagicSandbox in his home, and I became the face of MagicSandbox in China.\nNow we are going to release the MagicSandbox Alpha version. This version is an immature version and is provided for everyone to try for free. Positive feedback is also welcome.\nOfficial homepage: https://magicsandbox.com/ Chinese page: https://cn.magicsandbox.com/ (The content has not been finished yet, only the Chinese version homepage is currently provided) Follow us on Twitter: https://twitter.com/magicsandbox ","date":"2018-06-19T12:35:47+08:00","relpermalink":"/en/notice/magicsandbox-alpha-version-annoucement/","section":"notice","summary":"Online practical software engineering education platform.","title":"MagicSandbox Alpha released"},{"content":"Remember the cloud-native programming language I shared before finally appeared! Learn about Ballerina in one article! ? They are ready to attend the KubeCon \u0026amp; CloudNativeCon China Conference!\nKubeCon \u0026amp; CloudNativeCon China Conference will be held on November 14-15, 2018 (Wednesday, Thursday) in Shanghai . See: https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2018/ With Ballerina’s official authorization, I now need to help them find an “ambassador” in China, responsible for team guidance, Chinese and English translation, and familiarity with Cloud Native and microservices. It has influence in the industry and has no barriers to English communication.\nAmbassador duties\nTeam Leadership Responsible for Chinese and English translation of product declaration, PPT materials, etc. Help to arrange the booth The other party can provide\nConference tickets Travel expenses Accommodation during the conference Other compensation This is a photo of their team in front of their booth during the KubeCon \u0026amp; CloudNativeCon in Hagen in May this year.\nPS This is the most complete and best picture I have ever found of their team. (Photography needs to be strengthened)\nLet’s briefly introduce this startup called Ballerina. Their team is mainly from Sri Lanka. This is an island country next to India in the South Asian subcontinent. In ancient China, it was called “Lion Country” and rich in gemstones.\nThe capital of their country is Sri Lanka, which is pronounced in their own language: Si li jia ya wa de na pu la ke te\nIf you are interested, please contact me directly.\n","date":"2018-06-13T21:23:13+08:00","relpermalink":"/en/notice/a-ballerina-china-ambassador-required/","section":"notice","summary":"With official authorization from Ballerina, I now need to help them find an ambassador in China.","title":"Ballerina seeks Chinese ambassador"},{"content":"Envoy-Designed for cloud-native applications, open source edge and service proxy, Istio Service Mesh default data plane, Chinese version of the latest official document, dedicated by the ServiceMesher community, welcome everyone to learn and share together.\nPDF download address: servicemesher/envoy This is the first time Community Service Mesh enthusiasts group activities, the document is based on Envoy latest (version 1.7) Official documents https://www.envoyproxy.io/docs/envoy/latest/ . A total of 120 articles with 26 participants took 13 days and 65,148 Chinese characters.\nVisit online address: https://cloudnative.to/envoy/ Note : This book does not include the v1 API reference and v2 API reference sections in the official documentation. Any links to API references in the book will jump directly to the official page.\nContributor See the contributor page: https://github.com/servicemesher/envoy/graphs/contributors Thanks to the above contributors for their efforts! Because the level of translators is limited, there are inevitably inadequacies in the text. I also ask readers to correct them. Also welcome more friends to join our GitHub organization: https://github.com/servicemesher ","date":"2018-05-30T11:50:55+08:00","relpermalink":"/en/notice/envoyproxy-docs-cn-17-release/","section":"notice","summary":"Translated by ServiceMesher community.","title":"Chinese version of the latest official document of Envoy released"},{"content":"Envoy is an open source L7 proxy and communication bus written in C ++ by Lyft. It is currently an open source project under CNCF . The code is hosted on GitHub. It is also the default data plane in the Istio service mesh. We found that it has very good performance, and there are also continuous open source projects based on Envoy, such as Ambassador , Gloo, etc. At present, the official documentation of Envoy has not been well finished, so we Service Service enthusiasts feel that they are launching the community The power of the co-translation of the latest (version 1.7) official documentation of Enovy and organization through GitHub.\nService Mesh enthusiasts have jointly translated the latest version of the official document of Envoy . The translated code is hosted at https://github.com/servicemesher/envoy . If you are also a Service Mesh enthusiast, you can join the SerivceMesher GitHub organization and participate together.\nThe official Envoy document excludes all articles in the two directories of the v1 API reference and the v2 API reference. There are more than 120 documents. The length of the documents varies. The original English official documents use the RST format. I manually converted them into Markdown format and compiled using Gitbook. A GitHub Issue was generated according to the path of the document relative to the home directory. Friends who want to participate in translation can contact me to join the ServiceMesher organization, and then select the article you want to translate in Issue , and then reply “Claim”.\nHere you can see all the contributors. In the future, we will also create a Service Mesh enthusiast website. The website uses static pages. All code will be hosted on Github. Welcome everyone to participate.\n","date":"2018-05-16T14:47:19+08:00","relpermalink":"/en/notice/enovy-doc-translation-start/","section":"notice","summary":"The SerivceMesher community is involved in translating the official documentation for the latest version of Envoy.","title":"Envoy's latest official document translation work started"},{"content":"TL; DR Click here to download the PDF of this book .\nRecently, Michael Hausenblas of Nginx released a booklet on container networks in docker and kubernetes. This 72-page material is a good introduction for everyone to understand the network in Docker and Kubernetes from shallow to deep.\nTarget audience Container Software Developer SRE Network Operation and Maintenance Engineer Architects who want to containerize traditional software ","date":"2018-04-21T10:07:20+08:00","relpermalink":"/en/notice/container-networking-from-docker-to-kubernetes-nginx/","section":"notice","summary":"Source from Nginx, published by O’Reilly.","title":"Docker container network book sharing"},{"content":"In 2017, we are facing a big era of architectural changes, such as Kubernetes ending the battle for container orchestration, Kafka release 1.0, serverless gradually gaining momentum, edge computing to replace cloud computing, Service Mesh ready to go, and artificial intelligence to empower business , Also brings new challenges to the architecture.\nI am about to participate in InfoQ’s ArchSummit Global Architects Summit on December 8-11 in Beijing . This conference also invited 100+ top technologists such as Dr. Ali Wangjian to share and summarize the architectural changes and reflections this year. I hope that you can build on this conference, summarize past practices, and look forward to a future-oriented architecture to transform this era of change into the common fortune of each of us.\nMy speech The content of my speech is from Kubernetes to Cloud Native-the road to cloud native applications . Link: From Kubernetes to Cloud Native-the road to cloud native applications . The time is Saturday, December 9, 9:30 am, in the fifth meeting room.\nAfter more than ten years of development of cloud computing, the new phase of cloud native has entered. Enterprise applications are preferentially deployed in cloud environments. How to adapt to the cloud native tide, use containers and Kubernetes to build cloud native platforms, and practice DevOps concepts and agility How IT, open source software, and the community can help IT transform, the solution to all these problems is the PaaS platform, which is self-evident to the enterprise.\nWe also prepared gifts for everyone: “Cloud Native Go-Building Cloud Native Web Applications Based on Go and React” and “Intelligent Data Era-Enterprise Big Data Strategy and Practice” There is also a book stand for the blog post of the Electronic Industry Press. Welcome to visit.\nArchSummit conference official website link: http://bj2017.archsummit.com/ For more details, please refer to the official website of the conference: http://bj2017.archsummit.com/ ","date":"2017-12-07T12:11:29+08:00","relpermalink":"/en/notice/archsummit-beijing-2017-from-kubernetes-to-cloud-native/","section":"notice","summary":"I will give a lecture at ArchSummit Beijing. From Kubernetes to Cloud Native, my path to cloud native applications.","title":"ArchSummit Beijing 2017 speech preview"},{"content":"Cloudinary-go is a Go client library and CLI tool to upload static assets to the Cloudinary service.\nInstallation Install the CLI tool and the library with:\ngo get github.com/rootsongjc/cloudinary-go/cloudinary Or download the release binary from release .\n","date":"2017-10-29T15:24:19+08:00","relpermalink":"/en/notice/cloudinary-go/","section":"notice","summary":"Cloudinary-go is a Go client library and CLI tool to upload static assets to Cloudinary service","title":"Cloudinary file upload tool written in Go released"},{"content":"Kevin Hoffman(From Capital One, twitter @KevinHoffman ) was making a speech on TalkingData T11 Smart Data Summit.\nHe addressed that 15 Factors of Cloud Native which based on Heroku’s original Twelve-Factor App , but he add more 3 another factors on it.\nLet’s have a look at the 15 factors of Cloud Native.\n1. One codebase, one App Single version-controlled codebase, many deploys Multiple apps should not share code Microservices need separate release schedules Upgrade, deploy one without impacting others Tie build and deploy pipelines to single codebase 2. API first Service ecosystem requires a contract Public API Multiple teams on different schedulers Code to contract/API, not code dependencies Use well-documented contract standards Protobuf IDL, Swagger, Apiary, etc API First != REST first RPC can be more appropriate in some situations 3. Dependency Management Explicitly declare dependencies Include all dependencies with app release Create immutable build artifact (e.g. docker image) Rely on smallest docker image Base on scratch if possible App cannot rely on host for system tools or libraries 4. Design, Build, Release, Run Design part of iterative cycle Agile doesn’t mean random or undesigned Mature CI/CD pipeline and teams Design to production in days not months Build immutable artifacts Release automatically deploys to environment Environments contains config, not release artifact 5. Configuration, Credentials, Code “3 Cs” volatile substances that explode when combinded Password in a config file is as bad as password in code App must accept “3 Cs” from environment and only use harmless defaults Test - Could you expose code on Github and not reveal passwords, URLs, credentials? 6. Logs Emit formatted logs to stdout Code should not know about destination or purpose of log emissions Use downstream log aggregator collect, store, process, expose logs ELK, Splunk, Sumo, etc Use structured logs to allow query and analysis JSON, csv, KV, etc Logs are not metrics 7. Disposability App must start as quickly as possible App must stop quickly and gracefully Processes start and stop all the time in the cloud Every scale up/down disposes of processes Slow dispose == slow scale Slow dispose or startup can cause availability gaps 8. Backing Services Assume all resources supplied by backingservices Cannotassume mutable file system “Disk as a Service” (e.g. S3, virtual mounts, etc) Every backing service is bound resource URL, credentials, etc-\u0026gt; environment config Host does not satisfy NFRs Backing services and cloud infrastructure 9. Environment Parity “Works on my machine” Cloud-native anti-pattern. Must work everywhere Every commit is candidate for deployment Automated acceptance tests Provide no confidence if environments don’t match 10. Administrative Processes Database migrations Run-once scripts or jobs Avoid using for batch operations, consider instead: Event sourcing Schedulers Triggers from queues, etc Lambdas/functions 11. Port Binding In cloud, infrastructure determines port App must accept port assigned by platform Containers have internal/external ports App design must embrace this Never use reserved ports Beware of container “host mode” networking 12. Stateless Processes What is stateless? Long-term state handled by a backing service In-memory state lives onlyas long as request Requests from same client routed to different instances “Sticky sessions” cloud native anti-pattern 13. Concurency Scale horizontally using the process model Build disposable, stateless, share-nothing processes Avoid adding CPU/RAM to increase scale/throughput Where possible, let platform/libraries do threading Many single-threaded services \u0026gt; 1 multi-threaded monolith 14. Telemetry Monitor apps in the cloud like satellite in orbit No tether, no live debugger Application Perf Monitoring (APM) Domain Telemetry Health and system logs 15. Authentication \u0026amp; Authorization Security should never be an afterthought Auth should be explicit, documented decision Even if anonymous access is allowed Don’t allow anonymous access Bearer tokens/OAuth/OIDC best practices Audit all attempts to access Migrating Monoliths to the Cloud After this 15 factors, he also gave us some tips about how to migrate monoliths to the Cloud:\nMake a rule - stop adding to the monolith All new code must be cloud native Prioritize features Where will you get most benefit from cloud native? Come up with a plan Decompose monolith over time Fast, agile iterations toward ultimate goal Use multiple strategies and patterns Go - the Best Language for Building Cloud Native App At last, he advise us the programming language Go is the best language to build Cloud Native applications for these reasons below:\nLightweight Easily learning curve Compiles to native binaries Very fast Large, thriving, engaged community http://gopherize.me Kevin also wrote a book Cloud Native Go to show how to Building Web Applications and Microservices for the Cloud with Go and React. This book has been translated to Chinese by four guys from TalkingData with ❤️. 《Cloud Native Go 构建基于 Go 和 React 的云原生 Web 应用与微服务》published by PHEI publisher house.\nKevin was signing his name on the book\nFigure: kevin siging on the book This is his first visit to China, as a main translator of this book I an honored to be with him to take this photo.\nFigure: kevin hoffman with me ","date":"2017-09-15T20:32:47+08:00","relpermalink":"/en/blog/high-level-cloud-native-from-kevin-hoffman/","section":"blog","summary":"Kevin Hoffman address that 15 Factors of Cloud Native.","title":"High Level Cloud Native From Kevin Hoffman"},{"content":"From now on I have my own independent domain name jimmysong.io , the website is still hosted on GitHub, the original URL jimmysong.io still accessible.\nWhy use .io as the suffix? Because this is The First Step to Cloud Native!\nWhy choose today? Because today is August 18, the days are easy to remember.\nPS domain names are registered in namecheap and cost tens of dollars / year.\nProudly powered by hugo 🎉🎊🎉\n","date":"2017-08-18T23:48:30+08:00","relpermalink":"/en/notice/domain-name-jimmysong-io/","section":"notice","summary":"From now on I have my own independent domain name jimmysong.io.","title":"New domain name jimmysong.io"},{"content":"“Cloud Native Go,” authored by Kevin Hoffman, translated by Song Jingchao, Wu Yingsong, Xu Bei, and Ma Chao, published by Publishing House of Electronics Industry in August 2017.\nFigure: Cover of the book Cloud Native Go Author: Kevin Hoffman \u0026amp; Dan Nemeth Translators: Song Jingchao, Wu Yingsong, Xu Bei, Ma Chao Publisher: Publishing House of Electronics Industry Full Name: Cloud Native Go - A Guide to Building Web Cloud-Native Applications with Go and React This book has been published by the Publishing House of Electronics Industry and is available for purchase on JD.com .\nFigure: Jimmy Song with Kevin Hoffman Photo taken on September 12, 2017, in Beijing\nIntroduction Cloud Native Go shows developers how to build large-scale cloud applications that can dynamically scale to handle almost any volume of data, traffic, or users while meeting the strong demands of today’s customers.\nKevin Hoffman and Dan Nemeth detail modern cloud-native applications, elucidating the factors, rules, and habits associated with rapid and reliable cloud-native development. They also introduce Go, a “simple and elegant” high-performance language particularly suited for cloud development.\nIn this book, you’ll create microservices using Go, add frontend web components using ReactJS and Flux, and master advanced cloud-native technologies based on Go. Hoffman and Nemeth demonstrate how to build continuous delivery pipelines using tools like Wercker, Docker, and Dockerhub; automatically push applications to platforms; and systematically monitor application performance in production.\nLearn the “way of the cloud”: why well-developed cloud software is essentially about mindset and rules Understand why using the Go language is the ideal choice for cloud-native microservices development Plan cloud applications that support continuous delivery and deployment Design service ecosystems and then build them test-first Push work in progress to the cloud Use event sourcing and CQRS patterns to respond to large-scale and high-throughput Secure cloud-based web applications: choices to make and avoid Create responsive cloud applications using third-party messaging providers Build large-scale, cloud-friendly GUIs using React and Flux Monitor dynamic scaling, failover, and fault tolerance in the cloud About the Authors Kevin Hoffman has helped businesses move their applications to the cloud in a modern and multi-language manner. He began programming at the age of 10, self-studying BASIC on a reassembled Commodore VIC-20. Since then, he has been passionate about building software and has spent a lot of time learning languages, frameworks, and patterns. He has built a range of software, from remote-controlled drones and biometric security systems to ultra-low-latency financial applications and mobile apps. He fell in love with the Go language while building custom components that work with Pivotal Cloud Foundry.\nKevin is the author of the popular fantasy book series (The Sigilord Chronicles ) and eagerly anticipates combining his love of building software with his love of building fantasy worlds.\nDan Nemeth is currently a Consulting Solution Architect at Pivotal, supporting Pivotal Cloud Foundry. He has been developing software since the days of the Commodore 64 and began coding professionally in 1995, writing CGI scripts in ANSIC for a local ISP. Since then, most of his career has been spent as an independent consultant providing solutions to industries ranging from finance to pharmaceuticals, using various languages and frameworks of the time. Dan recently adopted Go as his home and enthusiastically applies it to all his projects.\nWhen Dan isn’t in front of a computer, he can often be found sailing or fly-fishing near Annapolis.\nTable of Contents Chapter 1: The Way of the Cloud Chapter 2: Getting Started Chapter 3: Getting to Know Go Chapter 4: Continuous Delivery Chapter 5: Building Microservices in Go Chapter 6: Leveraging Backend Services Chapter 7: Building Data Services Chapter 8: Event Sourcing and CQRS Chapter 9: Building Web Applications with Go Chapter 10: Cloud Security Chapter 11: Using WebSockets Chapter 12: Building Web Views with React Chapter 13: Building Scalable UIs with Flux Chapter 14: Creating a Full Application: World of FluxCraft Chapter 15: Conclusion Appendix A: Troubleshooting Cloud Applications Index ","date":"2017-08-01T00:00:00Z","relpermalink":"/en/book/cloud-native-go/","section":"book","summary":"Authored by Kevin Hoffman.","title":"Cloud Native Go"},{"content":"This is a list of software, tools, architecture and reference materials about Cloud Native. It is awesome-cloud-native , a project I opened on GitHub , and can also be browsed through the web page .\nThis list will be continuously updated and improved in the future, not only for my usual research records, but also as a reference for the Cloud Native industry.\n","date":"2017-07-18T11:21:14+08:00","relpermalink":"/en/notice/awesome-cloud-native/","section":"notice","summary":"This is a list of software, tools, architecture, and reference materials about Cloud Native. It is a project I started on GitHub.","title":"Awesome Cloud Native list is released"}]